0,1,label2,summary_sentences
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622–3631 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3622",text,[0],[0]
"Despite the massive success brought by neural machine translation (NMT, Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation systems (PBMT, Koehn et al., 2003), for low-resource language pairs (see, e.g., Koehn and Knowles, 2017).",1 Introduction,[0],[0]
"In the past few years, various approaches have been proposed to address this issue.",1 Introduction,[0],[0]
"The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre
* Equal contribution.
",1 Introduction,[0],[0]
"et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016).",1 Introduction,[0],[0]
"It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Lee et al., 2016; Johnson et al., 2016; Ha et al., 2016b).",1 Introduction,[0],[0]
"Its variant, transfer learning, was also proposed by Zoph et al. (2016), in which an NMT system is pretrained on a high-resource language pair before being finetuned on a target low-resource language pair.
",1 Introduction,[0],[0]
"In this paper, we follow up on these latest approaches based on multilingual NMT and propose a meta-learning algorithm for low-resource neural machine translation.",1 Introduction,[0],[0]
"We start by arguing that the recently proposed model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) could be applied to low-resource machine translation by viewing language pairs as separate tasks.",1 Introduction,[0],[0]
This view enables us to use MAML to find the initialization of model parameters that facilitate fast adaptation for a new language pair with a minimal amount of training examples (§3).,1 Introduction,[0],[0]
"Furthermore, the vanilla MAML however cannot handle tasks with mismatched input and output.",1 Introduction,[0],[0]
"We overcome this limitation by incorporating the universal lexical representation (Gu et al., 2018b) and adapting it for the meta-learning scenario (§3.3).
",1 Introduction,[0],[0]
We extensively evaluate the effectiveness and generalizing ability of the proposed meta-learning algorithm on low-resource neural machine translation.,1 Introduction,[0],[0]
"We utilize 17 languages from Europarl and Russian from WMT as the source tasks and test the meta-learned parameter initialization against five target languages (Ro, Lv, Fi, Tr and Ko), in all cases translating to English.",1 Introduction,[0],[0]
"Our experiments using only up to 160k tokens in each of the target task reveal that the proposed meta-learning approach outperforms the multilingual translation
approach across all the target language pairs, and the gap grows as the number of training examples decreases.",1 Introduction,[0],[0]
Neural Machine Translation (NMT),2 Background,[0],[0]
"Given a source sentence X = {x1, ..., xT 0}, a neural machine translation model factors the distribution over possible output sentences Y = {y1, ..., yT } into a chain of conditional probabilities with a leftto-right causal structure:
p(Y |X; ✓) = T+1Y
t=1
p(yt|y0:t 1, x1:T 0 ; ✓), (1)
where special tokens y0 (hbosi) and yT+1 (heosi) are used to represent the beginning and the end of a target sentence.",2 Background,[0],[0]
These conditional probabilities are parameterized using a neural network.,2 Background,[0],[0]
"Typically, an encoder-decoder architecture (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) with a RNN-based decoder is used.",2 Background,[0],[0]
"More recently, architectures without any recurrent structures (Gehring et al., 2017; Vaswani et al., 2017) have been proposed and shown to speed up training while achieving state-of-the-art performance.
",2 Background,[0],[0]
"Low Resource Translation NMT is known to easily over-fit and result in an inferior performance when the training data is limited (Koehn and Knowles, 2017).",2 Background,[0],[0]
"In general, there are two ways for handling the problem of low resource translation: (1) utilizing the resource of unlabeled monolingual data, and (2) sharing the knowledge between low- and high-resource language pairs.",2 Background,[0],[0]
"Many research efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning (Gulcehre et al., 2015; Zhang and Zong, 2016), back-translation (Sennrich et al., 2015), dual learning (He et al., 2016) and unsupervised machine translation with monolingual corpora only for both sides (Artetxe et al., 2017b; Lample et al., 2017; Yang et al., 2018).
",2 Background,[0],[0]
"For the second approach, prior researches have worked on methods to exploit the knowledge of auxiliary translations, or even auxiliary tasks.",2 Background,[0],[0]
"For instance, Cheng et al. (2016); Chen et al. (2017); Lee et al. (2017); Chen et al. (2018) investigate the use of a pivot to build a translation path between two languages even without any directed resource.",2 Background,[0],[0]
The pivot can be a third language or even an image in multimodal domains.,2 Background,[0],[0]
"When pivots are
not easy to obtain, Firat et al. (2016a); Lee et al. (2016); Johnson et al. (2016) have shown that the structure of NMT is suitable for multilingual machine translation.",2 Background,[0],[0]
"Gu et al. (2018b) also showed that such a multilingual NMT system could improve the performance of low resource translation by using a universal lexical representation to share embedding information across languages.
",2 Background,[0],[0]
"All the previous work for multilingual NMT assume the joint training of multiple high-resource languages naturally results in a universal space (for both the input representation and the model) which, however, is not necessarily true, especially for very low resource cases.
",2 Background,[0],[0]
"Meta Learning In the machine learning community, meta-learning, or learning-to-learn, has recently received interests.",2 Background,[0],[0]
Meta-learning tries to solve the problem of “fast adaptation on new training data.”,2 Background,[0],[0]
"One of the most successful applications of meta-learning has been on few-shot (or oneshot) learning (Lake et al., 2015), where a neural network is trained to readily learn to classify inputs based on only one or a few training examples.",2 Background,[0],[0]
"There are two categories of meta-learning:
1.",2 Background,[0],[0]
"learning a meta-policy for updating model parameters (see, e.g., Andrychowicz et al., 2016; Ha et al., 2016a; Mishra et al., 2017)
2.",2 Background,[0],[0]
"learning a good parameter initialization for fast adaptation (see, e.g., Finn et al., 2017; Vinyals et al., 2016; Snell et al., 2017).
",2 Background,[0],[0]
"In this paper, we propose to use a meta-learning algorithm for low-resource neural machine translation based on the second category.",2 Background,[0],[0]
"More specifically, we extend the idea of model-agnostic metalearning (MAML, Finn et al., 2017) in the multilingual scenario.",2 Background,[0],[0]
"The underlying idea of MAML is to use a set of source tasks T 1, . . .",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
", T K to find the initialization of parameters ✓0 from which learning a target task T 0 would require only a small number of training examples.",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"In the context of machine translation, this amounts to using many high-resource language pairs to find good initial parameters and training a new translation model on a low-resource language starting from the found initial parame-
ters.",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"This process can be understood as
✓⇤ = Learn(T 0;MetaLearn(T 1, . . .",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
", T K)).
",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"That is, we meta-learn the initialization from auxiliary tasks and continue to learn the target task.",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
We refer the proposed meta-learning method for NMT to MetaNMT.,3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
See Fig. 1 for the overall illustration.,3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"Given any initial parameters ✓0 (which can be either random or meta-learned),
the prior distribution of the parameters of a desired NMT model can be defined as an isotropic Guassian:
✓i ⇠ N (✓0i , 1/ ),
where 1/ is a variance.",3.1 Learn: language-specific learning,[0],[0]
"With this prior distribution, we formulate the language-specific learning process Learn(DT ; ✓0) as maximizing the logposterior of the model parameters given data DT :
Learn(DT ; ✓0) = argmax ✓ LDT (✓)
= argmax
✓
X
(X,Y )2DT
log p(Y |X, ✓) k✓ ✓0k2,
where we assume p(X|✓) to be uniform.",3.1 Learn: language-specific learning,[0],[0]
The first term above corresponds to the maximum likelihood criterion often used for training a usual NMT system.,3.1 Learn: language-specific learning,[0],[0]
"The second term discourages the newly learned model from deviating too much from the initial parameters, alleviating the issue of overfitting when there is not enough training data.",3.1 Learn: language-specific learning,[0],[0]
"In practice, we solve the problem above by maximizing the first term with gradient-based optimization and early-stopping after only a few update steps.
",3.1 Learn: language-specific learning,[0],[0]
"Thus, in the low-resource scenario, finding a good initialization ✓0 strongly correlates the final performance of the resulting model.",3.1 Learn: language-specific learning,[0],[0]
"We find the initialization ✓0 by repeatedly simulating low-resource translation scenarios using auxiliary, high-resource language pairs.",3.2 MetaLearn,[0],[0]
"Following Finn et al. (2017), we achieve this goal by defining the meta-objective function as
L(✓) =EkEDT k ,D0T k (2)2
64 X
(X,Y )2D0 T k
log p(Y |X;Learn(DT k ; ✓))
3
75 ,
where k ⇠ U({1, . . .",3.2 MetaLearn,[0],[0]
",K}) refers to one metalearning episode, and DT , D0T follow the uniform distribution over T ’s data.
",3.2 MetaLearn,[0],[0]
"We maximize the meta-objective function using stochastic approximation (Robbins and Monro, 1951) with gradient descent.",3.2 MetaLearn,[0],[0]
"For each episode, we uniformly sample one source task at random, T k.",3.2 MetaLearn,[0],[0]
"We then sample two subsets of training examples independently from the chosen task, DT k and D0T k .",3.2 MetaLearn,[0],[0]
We use the former to simulate languagespecific learning and the latter to evaluate its outcome.,3.2 MetaLearn,[0.9506083330978454],['We propose to use the word prediction mechanism to enhance the initial state generated by the encoder and extend the mechanism to control the hidden states of decoder as well.']
"Assuming a single gradient step is taken only the with learning rate ⌘, the simulation is:
✓0k = Learn(DT k ;",3.2 MetaLearn,[0],[0]
✓) =,3.2 MetaLearn,[0],[0]
"✓ ⌘r✓LDT k (✓).
",3.2 MetaLearn,[0],[0]
"Once the simulation of learning is done, we evaluate the updated parameters ✓0k on D 0 T k , The gradient computed from this evaluation, which we refer to as meta-gradient, is used to update the
meta model ✓.",3.2 MetaLearn,[0],[0]
"It is possible to aggregate multiple episodes of source tasks before updating ✓:
✓ ✓ ⌘0 X
k
r✓LD 0 T k (✓0k),
where ⌘0 is the meta learning rate.",3.2 MetaLearn,[0],[0]
"Unlike a usual learning scenario, the resulting model ✓0 from this meta-learning procedure is not necessarily a good model on its own.",3.2 MetaLearn,[0],[0]
It is however a good starting point for training a good model using only a few steps of learning.,3.2 MetaLearn,[0],[0]
"In the context of machine translation, this procedure can be understood as finding the initialization of a neural machine translation system that could quickly adapt to a new language pair by simulating such a fast adaptation scenario using many high-resource language pairs.
",3.2 MetaLearn,[0],[0]
"Meta-Gradient We use the following approximation property
H(x)v ⇡ r(x+ ⌫v) r(x) ⌫
to approximate the meta-gradient:1
r✓LD 0",3.2 MetaLearn,[0],[0]
(✓0),3.2 MetaLearn,[0],[0]
= r✓0LD 0,3.2 MetaLearn,[0],[0]
"(✓0)r✓(✓ ⌘r✓LD(✓))
",3.2 MetaLearn,[0],[0]
= r✓0LD 0,3.2 MetaLearn,[0],[0]
(✓0) ⌘,3.2 MetaLearn,[0],[0]
r✓0LD 0,3.2 MetaLearn,[0],[0]
"(✓0)H✓(LD(✓))
⇡ r✓0LD 0",3.2 MetaLearn,[0],[0]
"(✓0) ⌘
⌫
 r✓LD(✓)
",3.2 MetaLearn,[0],[0]
"✓̂ r✓LD(✓) ✓ ,
where ⌫ is a small constant and
ˆ✓ = ✓ + ⌫r",3.2 MetaLearn,[0],[0]
✓0LD 0,3.2 MetaLearn,[0],[0]
"(✓0).
",3.2 MetaLearn,[0],[0]
"In practice, we find that it is also possible to ignore the second-order term, ending up with the following simplified update rule:
r✓LD 0 (✓0) ⇡ r✓0LD 0",3.2 MetaLearn,[0],[0]
(✓0).,3.2 MetaLearn,[0],[0]
"(3)
1We omit the subscript k for simplicity.
",3.2 MetaLearn,[0],[0]
"Related Work: Multilingual Transfer Learning The proposed MetaNMT differs from the existing framework of multilingual translation (Lee et al., 2016; Johnson et al., 2016; Gu et al., 2018b) or transfer learning (Zoph et al., 2016).",3.2 MetaLearn,[0],[0]
"The latter can be thought of as solving the following problem:
max ✓ Lmulti(✓) =",3.2 MetaLearn,[0],[0]
"Ek
2 4 X
(X,Y )2Dk
log p(Y |X; ✓)
3
5 ,
where Dk is the training set of the k-th task, or language pair.",3.2 MetaLearn,[0],[0]
"The target low-resource language pair could either be a part of joint training or be trained separately starting from the solution ✓0 found from solving the above problem.
",3.2 MetaLearn,[0],[0]
"The major difference between the proposed MetaNMT and these multilingual transfer approaches is that the latter do not consider how learning happens with the target, low-resource language pair.",3.2 MetaLearn,[0],[0]
The former explicitly incorporates the learning process within the framework by simulating it repeatedly in Eq.,3.2 MetaLearn,[0],[0]
(2).,3.2 MetaLearn,[0],[0]
"As we will see later in the experiments, this results in a substantial gap in the final performance on the low-resource task.
",3.2 MetaLearn,[0],[0]
"Illustration In Fig. 2, we contrast transfer learning, multilingual learning and meta-learning using three source language pairs (Fr-En, Es-En and Pt-En) and two target pairs (Ro-En and Lv-En).",3.2 MetaLearn,[0],[0]
"Transfer learning trains an NMT system specifically for a source language pair (Es-En) and finetunes the system for each target language pair (RoEn, Lv-En).",3.2 MetaLearn,[0],[0]
"Multilingual learning often trains a single NMT system that can handle many different language pairs (Fr-En, Pt-En, Es-En), which may or may not include the target pairs (Ro-En, LvEn).",3.2 MetaLearn,[0],[0]
"If not, it finetunes the system for each target pair, similarly to transfer learning.",3.2 MetaLearn,[0],[0]
Both of these however aim at directly solving the source tasks.,3.2 MetaLearn,[0],[0]
"On the other hand, meta-learning trains the NMT system to be useful for fine-tuning on various tasks including the source and target tasks.",3.2 MetaLearn,[0],[0]
"This is done by repeatedly simulating the learning process on
low-resource languages using many high-resource language pairs (Fr-En, Pt-En, Es-En).",3.2 MetaLearn,[0],[0]
I/O mismatch across language pairs One major challenge that limits applying meta-learning for low resource machine translation is that the approach outlined above assumes the input and output spaces are shared across all the source and target tasks.,3.3 Unified Lexical Representation,[0],[0]
"This, however, does not apply to machine translation in general due to the vocabulary mismatch across different languages.",3.3 Unified Lexical Representation,[0],[0]
"In multilingual translation, this issue has been tackled by using a vocabulary of sub-words (Sennrich et al., 2015) or characters (Lee et al., 2016) shared across multiple languages.",3.3 Unified Lexical Representation,[0],[0]
"This surface-level sharing is however limited, as it cannot be applied to languages exhibiting distinct orthography (e.g., IndoEuroepan languages vs. Korean.)
",3.3 Unified Lexical Representation,[0],[0]
"Universal Lexical Representation (ULR) We tackle this issue by dynamically building a vocabulary specific to each language using a keyvalue memory network (Miller et al., 2016; Gulcehre et al., 2018), as was done successfully for low-resource machine translation recently by Gu et al. (2018b).",3.3 Unified Lexical Representation,[0],[0]
"We start with multilingual word embedding matrices ✏kquery 2 R|Vk|⇥d pretrained on large monolingual corpora, where Vk is the vocabulary of the k-th language.",3.3 Unified Lexical Representation,[0],[0]
"These embedding vectors can be obtained with small dictionaries of seed word pairs (Artetxe et al., 2017a; Smith et al., 2017) or in a fully unsupervised manner (Zhang et al., 2017; Conneau et al., 2018).",3.3 Unified Lexical Representation,[0],[0]
"We take one of these languages k0 to build universal lexical representation consisting of a universal embedding matrix ✏u 2 RM⇥d and a corresponding key matrix ✏key 2 RM⇥d, where M < |V 0k|.",3.3 Unified Lexical Representation,[0],[0]
Both ✏kquery and ✏key are fixed during meta-learning.,3.3 Unified Lexical Representation,[0],[0]
"We then compute the language-specific embedding of token x from the language k as the convex sum of the universal embedding vectors by
✏0[x] = MX
i=1
↵i✏u[i],
where ↵i / exp 1⌧ ✏key[i]",3.3 Unified Lexical Representation,[0],[0]
>A✏kquery[x] and ⌧ is set to 0.05.,3.3 Unified Lexical Representation,[0],[0]
"This approach allows us to handle languages with different vocabularies using a fixed number of shared parameters (✏u, ✏key and A.)
",3.3 Unified Lexical Representation,[0],[0]
"Learning of ULR It is not desirable to update the universal embedding matrix ✏u when fine-
tuning on a small corpus which contains a limited set of unique tokens in the target language, as it could adversely influence the other tokens’ embedding vectors.",3.3 Unified Lexical Representation,[0],[0]
"We thus estimate the change to each embedding vector induced by languagespecific learning by a separate parameter ✏k[x]:
✏k[x] = ✏0[x] + ✏k[x].
",3.3 Unified Lexical Representation,[0],[0]
"During language-specific learning, the ULR ✏0[x] is held constant, while only ✏k[x] is updated, starting from an all-zero vector.",3.3 Unified Lexical Representation,[0],[0]
"On the other hand, we hold ✏k[x]’s constant while updating ✏u and A during the meta-learning stage.",3.3 Unified Lexical Representation,[0],[0]
"Target Tasks We show the effectiveness of the proposed meta-learning method for low resource NMT with extremely limited training examples on five diverse target languages: Romanian (Ro) from WMT’16,2 Latvian (Lv), Finnish (Fi), Turkish (Tr) from WMT’17,3 and Korean (Ko) from Korean Parallel Dataset.4 We use the officially provided train, dev and test splits for all these languages.",4.1 Dataset,[0],[0]
The statistics of these languages are presented in Table 1.,4.1 Dataset,[0],[0]
"We simulate the low-resource translation scenarios by randomly sub-sampling the training set with different sizes.
",4.1 Dataset,[0],[0]
"Source Tasks We use the following languages from Europarl5: Bulgarian (Bg), Czech (Cs), Danish (Da), German (De), Greek (El), Spanish (Es), Estonian (Et), French (Fr), Hungarian (Hu), Italian (It), Lithuanian (Lt), Dutch (Nl), Polish (Pl), Portuguese (Pt), Slovak (Sk), Slovene (Sl) and
2 http://www.statmt.org/wmt16/translation-task.html 3 http://www.statmt.org/wmt17/translation-task.html 4 https://sites.google.com/site/koreanparalleldata/ 5 http://www.statmt.org/europarl/
Swedish (Sv), in addition to Russian (Ru)6 to learn the intilization for fine-tuning.",4.1 Dataset,[0],[0]
"In our experiments, different combinations of source tasks are explored to see the effects from the source tasks.
",4.1 Dataset,[0.9578048359176822],"['In the future, it might be helpful to analyze the benefits of jointly learning other related tasks together with machine translation, to provide further control of the learning process.']"
Validation We pick either Ro-En or Lv-En as a validation set for meta-learning and test the generalization capability on the remaining target tasks.,4.1 Dataset,[0],[0]
"This allows us to study the strict form of metalearning, in which target tasks are unknown during both training and model selection.
",4.1 Dataset,[0],[0]
"Preprocessing and ULR Initialization As described in §3.3, we initialize the query embedding vectors ✏kquery of all the languages.",4.1 Dataset,[0],[0]
"For each language, we use the monolingual corpora built from Wikipedia7 and the parallel corpus.",4.1 Dataset,[0],[0]
"The concatenated corpus is first tokenized and segmented using byte-pair encoding (BPE, Sennrich et al., 2016), resulting in 40, 000 subwords for each language.",4.1 Dataset,[0],[0]
"We then estimate word vectors using fastText (Bojanowski et al., 2016) and align them across all the languages in an unsupervised way
6 A subsample of approximately 2M pairs from WMT’17.",4.1 Dataset,[0],[0]
"7 We use the most recent Wikipedia dump (2018.5) from
https://dumps.wikimedia.org/backup-index.html.
using MUSE (Conneau et al., 2018) to get multilingual word vectors.",4.1 Dataset,[0],[0]
"We use the multilingual word vectors of the 20,000 most frequent words in English to form the universal embedding matrix ✏u.",4.1 Dataset,[0],[0]
"Model We utilize the recently proposed Transformer (Vaswani et al., 2017) as an underlying NMT system.",4.2 Model and Learning,[0],[0]
"We implement Transformer in this paper based on (Gu et al., 2018a)8 and modify it to use the universal lexical representation from §3.3.",4.2 Model and Learning,[0],[0]
"We use the default set of hyperparameters (dmodel = dhidden = 512, nlayer = 6, nhead = 8, nbatch = 4000, twarmup = 16000) for all the language pairs and across all the experimental settings.",4.2 Model and Learning,[0],[0]
"We refer the readers to (Vaswani et al., 2017; Gu et al., 2018a) for the details of the model.",4.2 Model and Learning,[0],[0]
"However, since the proposed metalearning method is model-agnostic, it can be easily extended to any other NMT architectures, e.g. RNN-based sequence-to-sequence models with attention (Bahdanau et al., 2015).
",4.2 Model and Learning,[0],[0]
8,4.2 Model and Learning,[0],[0]
"https://github.com/salesforce/nonauto-nmt
Learning We meta-learn using various sets of source languages to investigate the effect of source task choice.",4.2 Model and Learning,[0],[0]
"For each episode, by default, we use a single gradient step of language-specific learning with Adam (Kingma and Ba, 2014) per computing the meta-gradient, which is computed by the first-order approximation in Eq.",4.2 Model and Learning,[0],[0]
"(3).
",4.2 Model and Learning,[0],[0]
"For each target task, we sample training examples to form a low-resource task.",4.2 Model and Learning,[0],[0]
"We build tasks of 4k, 16k, 40k and 160k English tokens for each language.",4.2 Model and Learning,[0],[0]
We randomly sample the training set five times for each experiment and report the average score and its standard deviation.,4.2 Model and Learning,[0],[0]
"Each fine-tuning is done on a training set, early-stopped on a validation set and evaluated on a test set.",4.2 Model and Learning,[0],[0]
"In default without notation, datasets of 16k tokens are used.
",4.2 Model and Learning,[0],[0]
"Fine-tuning Strategies The transformer consists of three modules; embedding, encoder and decoder.",4.2 Model and Learning,[0],[0]
"We update all three modules during metalearning, but during fine-tuning, we can selectively tune only a subset of these modules.",4.2 Model and Learning,[0],[0]
"Following (Zoph et al., 2016), we consider three fine-tuning
strategies; (1) fine-tuning all the modules (all), (2) fine-tuning the embedding and encoder, but freezing the parameters of the decoder (emb+enc) and (3) fine-tuning the embedding only (emb).",4.2 Model and Learning,[0],[0]
vs. Multilingual Transfer Learning We metalearn the initial models on all the source tasks using either Ro-En or Lv-En as a validation task.,5 Results,[0],[0]
We also train the initial models to be multilingual translation systems.,5 Results,[0],[0]
"We fine-tune them using the four target tasks (Ro-En, Lv-En, Fi-En and Tr-En; 16k tokens each) and compare the proposed meta-learning strategy and the multilingual, transfer learning strategy.",5 Results,[0],[0]
"As presented in Fig. 3, the proposed learning approach significantly outperforms the multilingual, transfer learning strategy across all the target tasks regardless of which target task was used for early stopping.",5 Results,[0],[0]
We also notice that the emb+enc strategy is most effective for both meta-learning and transfer learning approaches.,5 Results,[0],[0]
"With the proposed meta-learning and emb+enc fine-tuning, the final NMT systems trained using only a fraction of all available training examples achieve 2/3 (Ro-En) and 1/2 (Lv-En, Fi-En and Tr-En) of the BLEU score achieved by the models trained with full training sets.
",5 Results,[0],[0]
"vs. Statistical Machine Translation We also test the same Ro-En datasets with 16, 000 target tokens using the default setting of Phrase-based MT (Moses) with the dev set for adjusting the parameters and the test set for calculating the final performance.",5 Results,[0],[0]
"We obtain 4.79(±0.234) BLEU point, which is higher than the standard NMT performance (0 BLEU).",5 Results,[0],[0]
"It is however still lower than both the multi-NMT and meta-NMT.
",5 Results,[0],[0]
"Impact of Validation Tasks Similarly to training any other neural network, meta-learning still requires early-stopping to avoid overfitting to a
specific set of source tasks.",5 Results,[0],[0]
"In doing so, we observe that the choice of a validation task has nonnegligible impact on the final performance.",5 Results,[0],[0]
"For instance, as shown in Fig. 3, Fi-En benefits more when Ro-En is used for validation, while the opposite happens with Tr-En.",5 Results,[0],[0]
"The relationship between the task similarity and the impact of a validation task must be investigated further in the future.
",5 Results,[0],[0]
"Training Set Size We vary the size of the target task’s training set and compare the proposed meta-learning strategy and multilingual, transfer learning strategy.",5 Results,[0],[0]
We use the emb+enc fine-tuning on Ro-En and Fi-En.,5 Results,[0],[0]
Fig. 4 demonstrates that the meta-learning approach is more robust to the drop in the size of the target task’s training set.,5 Results,[0],[0]
"The gap between the meta-learning and transfer learning grows as the size shrinks, confirming the effectiveness of the proposed approach on extremely lowresource language pairs.
",5 Results,[0],[0]
"Impact of Source Tasks In Table 2, we present the results on all five target tasks obtained while varying the source task set.",5 Results,[0],[0]
We first see that it is always beneficial to use more source tasks.,5 Results,[0],[0]
"Although the impact of adding more source tasks varies from one language to another, there is up to 2⇥ improvement going from one source task to 18 source tasks (Lv-En, Fi-En, Tr-En and Ko-En).",5 Results,[0],[0]
"The same trend can be observed even without any fine-tuning (i.e., unsupervised translation, (Lample et al., 2017; Artetxe et al., 2017b)).",5 Results,[0],[0]
"In addition, the choice of source languages has different implications for different target languages.",5 Results,[0],[0]
"For instance, Ro-En benefits more from {Es, Fr, It, Pt} than from {De, Ru}, while the opposite effect is observed with all the other target tasks.
",5 Results,[0],[0]
Training Curves,5 Results,[0],[0]
The benefit of meta-learning over multilingual translation is clearly demonstrated when we look at the training curves in Fig. 5.,5 Results,[0],[0]
"With the multilingual, transfer learning ap-
",5 Results,[0],[0]
"proach, we observe that training rapidly saturates and eventually degrades, as the model overfits to the source tasks.",5 Results,[0],[0]
MetaNMT,5 Results,[0],[0]
"on the other hand continues to improve and never degrades, as the metaobjective ensures that the model is adequate for fine-tuning on target tasks rather than for solving the source tasks.
",5 Results,[0.9540334151806725],"['Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality.']"
Sample Translations We present some sample translations from the tested models in Table 3.,5 Results,[0],[0]
Inspecting these examples provides the insight into the proposed meta-learning algorithm.,5 Results,[0],[0]
"For instance, we observe that the meta-learned model without any fine-tuning produces a word-by-word translation in the first example (Tr-En), which is due to the successful use of the universal lexcial representation and the meta-learned initialization.",5 Results,[0],[0]
"The system however cannot reorder tokens from Turkish to English, as it has not seen any training example of Tr-En.",5 Results,[0],[0]
"After seeing around 600 sentence pairs (16K English tokens), the model rapidly learns to correctly reorder tokens to form a better translation.",5 Results,[0],[0]
A similar phenomenon is observed in the Ko-En example.,5 Results,[0],[0]
These cases could be found across different language pairs.,5 Results,[0],[0]
"In this paper, we proposed a meta-learning algorithm for low-resource neural machine translation that exploits the availability of high-resource languages pairs.",6 Conclusion,[0],[0]
"We based the proposed algorithm on the recently proposed model-agnostic metalearning and adapted it to work with multiple languages that do not share a common vocabulary using the technique of universal lexcal representation, resulting in MetaNMT.",6 Conclusion,[0],[0]
"Our extensive evaluation, using 18 high-resource source tasks and 5 low-resource target tasks, has shown that the proposed MetaNMT significantly outperforms the existing approach of multilingual, transfer learning in low-resource neural machine translation across all the language pairs considered.
",6 Conclusion,[0],[0]
The proposed approach opens new opportunities for neural machine translation.,6 Conclusion,[0],[0]
"First, it is a principled framework for incorporating various extra sources of data, such as source- and targetside monolingual corpora.",6 Conclusion,[0],[0]
"Second, it is a generic framework that can easily accommodate existing and future neural machine translation systems.",6 Conclusion,[0],[0]
This research was supported in part by the Facebook Low Resource Neural Machine Translation Award.,Acknowledgement,[0],[0]
This work was also partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure).,Acknowledgement,[0],[0]
"KC thanks support by eBay, TenCent, NVIDIA and CIFAR.",Acknowledgement,[0],[0]
"In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) for lowresource neural machine translation (NMT).",abstractText,[0],[0]
"We frame low-resource translation as a metalearning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks.",abstractText,[0],[0]
"We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages.",abstractText,[0],[0]
"We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks.",abstractText,[0],[0]
"We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples.",abstractText,[0],[0]
"For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (⇠ 600 parallel sentences).",abstractText,[0],[0]
Meta-Learning for Low-Resource Neural Machine Translation,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 375–385 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Online platforms have revolutionized the way individuals collect and share information (O’Connor et al., 2010; Lee and Ma, 2012; Bakshy et al., 2015), but the vast bulk of online content is irrelevant or unpalatable to any given individual.",1 Introduction,[0],[0]
"A user interested in political discussion, for instance, might prefer content concerning a specific candidate or issue, and only then if discussed in a positive light without controversy (Adamic and Glance, 2005; Bakshy et al., 2015).
",1 Introduction,[0],[0]
"How do individuals facing such large quantities of superfluous material select which conversations to engage in, and how might we better algorithmically recommend conversations suited to individual users?",1 Introduction,[0],[0]
We approach this problem from a microblog conversation recommendation framework.,1 Introduction,[0],[0]
"Where prior work has focused on the content of individual posts for recommendation (Chen
et al., 2012; Yan et al., 2012; Vosecky et al., 2014; He and Tan, 2015), we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (Ritter et al.,",1 Introduction,[0],[0]
"2010).1 And where Backstrom et al. (2013) leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new
1In this paper, discourse mode refers to a certain type of dialogue act, e.g., agreement or argument.",1 Introduction,[0],[0]
"The discourse structure of a conversation means some combination (or a probability distribution) of discourse modes.
375
and repeated entry into conversations based on a combination of topical and discourse features.
",1 Introduction,[0],[0]
"To illustrate the interplay between topics and discourse, Figure 1 displays two snippets of conversations on Twitter collected during the 2016 United States presidential election.",1 Introduction,[0],[0]
User U1 participates in both conversations.,1 Introduction,[0],[0]
"The first conversation is centered around Clinton, and U1, who is more typically involved with conversations about candidate Sanders, does not return.",1 Introduction,[0],[0]
"In the second conversation, however, U1 is involved in a heated back-and-forth debate, and thus is drawn back to a conversation that they may otherwise have abandoned but for their enjoyment of adversarial discourse.
",1 Introduction,[0],[0]
"Effective conversation prediction and recommendation requires an understanding of both user interests and discourse behaviors, such as agreement, disagreement, inquiry, backchanneling, and emotional reactions.",1 Introduction,[0],[0]
"However, acquiring manual labels for both is a time-consuming process and hard to scale for new datasets.",1 Introduction,[0],[0]
"We instead propose a unified statistical learning framework for conversation recommendation, which jointly learns (1) hidden factors that reflect user interests based on conversation history, and (2) topics and discourse modes in ongoing conversations, as discovered by a novel probabilistic latent variable model.",1 Introduction,[0],[0]
"Our model is built on the success of collaborative filtering (CF) in recommendation systems, where latent dimensions of product ratings or movie reviews are extracted to better capture user preferences (Linden et al., 2003; Salakhutdinov and Mnih, 2008; Wang and Blei, 2011; McAuley and Leskovec, 2013).",1 Introduction,[0],[0]
"To the best of our knowledge, we are the first to model both topics and discourse modes as part of a CF framework and apply it to microblog conversation recommendation.2
Experimental results on two Twitter conversation datasets show that our proposed model yields significantly better performance than state-of-theart post-level recommendation systems.",1 Introduction,[0],[0]
"For example, by leveraging both topical content and discourse structure, our model achieves a mean average precision (MAP) of 0.76 on conversations about the U.S. presidential election, compared with 0.70 by McAuley and Leskovec (2013), which only considers topics.",1 Introduction,[0],[0]
"We further con-
2To ensure the general applicability of our approach to domains lacking such information, we do not utilize external features such as network structure, but it may certainly be added in future, more narrowly targeted applications.
ducted detailed analysis on the latent topics and discourse modes and find that our model can discover reasonable topic and discourse representations, which play an important role in characterizing reply behaviors.",1 Introduction,[0.9502337716005561],"['Instead of looking for other annotated data, we notice that the words in the target language sentence could be viewed as a natural annotation.']"
"Finally, we also provide a pilot study on recommendation for first time replies, which shows that our model outperforms comparable recommendation systems.
",1 Introduction,[0],[0]
The rest of this paper is structured as follows.,1 Introduction,[0],[0]
The related work is discussed in Section 2.,1 Introduction,[0],[0]
We then present our microblog conversation recommendation model in Section 3.,1 Introduction,[0],[0]
The experimental setup and results are described in Sections 4 and 5.,1 Introduction,[0],[0]
"Finally, we conclude in Section 6.",1 Introduction,[0],[0]
"Social media has attracted increasing attention in digital communication research (Agichtein et al., 2008; Kwak et al., 2010; Wu et al., 2011).",2 Related Work,[0],[0]
"The problem studied here is closely related to work on recommendation and response prediction in microblogs (Artzi et al., 2012; Hong et al., 2013), where the goal is to predict whether a user will share or reply to a given post.",2 Related Work,[0],[0]
"Existing methods focus on measuring features that reflect personalized user interests, including topics (Hong et al., 2013) and network structures (Pan et al., 2013; He and Tan, 2015).",2 Related Work,[0],[0]
"These features have been investigated under a learning to rank framework (Duan et al., 2010; Artzi et al., 2012), graph ranking models (Yan et al., 2012; Feng and Wang, 2013; Alawad et al., 2016), and neural network-based representation learning methods (Yu et al., 2016).
",2 Related Work,[0],[0]
"Distinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level.",2 Related Work,[0],[0]
"In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure.",2 Related Work,[0],[0]
"Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook et al., 2009; Ritter et al., 2010; Joty et al., 2011).
",2 Related Work,[0],[0]
"Our work is also in line with conversation modeling for social media discussions (Ritter et al., 2010; Budak and Agrawal, 2013; Louis and Cohen, 2015; Cheng et al., 2017).",2 Related Work,[0],[0]
"Topic modeling
has been employed to identify conversation content on Twitter (Ritter et al., 2010).",2 Related Work,[0],[0]
"In this work, we propose a probabilistic model to capture both topics and discourse modes as latent variables.",2 Related Work,[0],[0]
"A further line of work studies the reposting and reply structure of conversations (Gómez et al., 2011; Laniado et al., 2011; Backstrom et al., 2013; Budak and Agrawal, 2013).",2 Related Work,[0],[0]
"But none of this work distinguishes the rich discourse functions of replies, which is modeled and exploited in our work.",2 Related Work,[0],[0]
Our proposed microblog conversation recommendation framework is based on collaborative filtering and a novel probabilistic graphical model.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Concretely, our objective function takes the form:
minL+ µ ·NLL(C |Θ) (1)
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
This function encodes two types of information.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"First, L models user reply preference in a similar fashion to collaborative filtering (CF) (Hu et al., 2008; Pan et al., 2008).",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"It captures topics of interests and discourse structures users are commonly involved (e.g., argumentation), and takes the form of mean square error (MSE) based on user reply history.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"This part is detailed in Section 3.1.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The second term, NLL(C |Θ), denotes the negative log-likelihood of a set of conversations C, with Θ containing all parameters.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"A probabilistic model is described in Section 3.2 that shows how the topical content and discourse structures of conversations are captured by these latent variables.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
The hyperparameter µ controls the trade-off between the two effects.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"`2 regularization is also added for parameters to avoid model overfitting.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"For the rest of this section, we first present the construction of L andNLL(C |Θ) in Sections 3.1 and 3.2.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
We then discuss how these two components can be mutually informed by each other in Section 3.3.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Finally, the generative process and parameter learning are described in Section 3.4.
3.1 Reply Preference (L) Our user reply preference modeling is built on the success of collaborative filtering (CF) for product ratings.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"However, classic CF problems, such as product recommendation, generally rely on explicit user feedback.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Unlike user ratings on products, our input lacks explicit feedback from users about negative preferences and nonresponse.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, we follow one-class Collaborative Filtering (Hu et al., 2008; Pan et al., 2008),
which weights positive instances higher during training and is thus suited to our data.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Formally, for user u and conversation c, we measure reply preference based on the MSE between predicted preference score pu,c and reply history ru,c. ru,c equals 1 if u is in the conversation history; otherwise, it is 0.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The first term of objective (Eq. 1) takes the following form:
L = |U|∑
u=1
|C|∑
c=1
fu,c · (pu,c − ru,c)2 (2)
where U consists of users {u} and C is a set of conversations {c} in a dataset.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"fu,c is the corresponding weight for a conversation c and a target user u. Intuitively, it has a large value if positive feedback (user replied) is observed.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, we adapt the formulation from Pan et al. (2008):
fu,c = { s if ru,c = 1 (i.e., user replied) 1 if ru,c = 0
(3)
where s > 1, an integer hyperparameter to be tuned.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Inspired by prior models (Koren et al., 2009; McAuley and Leskovec, 2013), we propose the following latent factor model to describe pu,c:
pu,c = λ · γUu · γCc + (1− λ) ·",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
δUu · δCc,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"+ bu + bc + a (4)
γUu and γ C c are K-dimensional latent vectors that encode topic-specific information (where K is the number of latent topics) for users and conversations.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Specifically, γUu reflects the topical interests of u, with higher value γUu,k indicating greater interest by u in topic k. γCc captures the extents that topics are discussed in conversation c.
Similarly, D-dimensional vectors δUu and δ C c capture discourse structures in shaping reply behaviors (where D is the number of discourse clusters).",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"δUu reflects the discourse behaviors u prefers, such as u1 often enjoys arguments as in the second conversation of Figure 1, while δCc captures the discourse modes used throughout conversation c. By multiplying user and conversation factors, we can measure the corresponding similarity.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The predicted score pu,c thereby reflects the tendency for a user u to be involved in conversation c.
As pointed out by McAuley and Leskovec (2013), these latent vectors often encode hidden factors that are hard to interpret under a CF framework.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, in Section 3.2, we present a novel probabilistic model which can extract interpretable topics and discourse modes as word
distributions.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We then describe how they can be aligned with the latent vectors of γC and δU .
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Parameter a is an offset parameter, bu and bc are user and conversation biases, and λ ∈",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"[0, 1] serves as the weight for trading offs of topic and discourse factors in reply preference modeling.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
3.2 Corpus Likelihood NLL(C |Θ),3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Here we present a novel probabilistic model that learns coherent word distributions for latent topics and discourse modes of conversations.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Formally, we assume that each conversation c ∈ C contains Mc messages, and each message m has Nc,m words.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We distinguish three latent components – discourse, topic, and background – underlying conversations, each with their own type of word distribution.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"At the corpus level, there are K topics represented by word distribution φTk (k = 1, 2, ...,K), while φDd (d = 1, 2, ..., D) represents the D discourse modes embedded in corpus.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"In addition, we add a background word distribution φB to capture general information (e.g., common words), which do not indicate either discourse or topic information.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"φDd , φ T k , and φ
B are all multinomial word distributions over vocabulary size V .",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Below describes more details.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Message-level Modeling.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Our model assigns two types of message-level multinomial variables to each message: zc,m reflects its latent topic and dc,m represents its discourse mode.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Topic assignments.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Due to the short nature of microblog posts, we assume each message m in conversation c contains only one topic, indexed as zc,m. This strategy has been proven useful to alleviate data sparsity for topic inference (Quan et al., 2015).",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
We further assume messages in the same conversation would focus on similar topics.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We thus draw topic zc,m ∼ θc, where θc denotes the fractions of topics discussed in conversation c.
Discourse assignments.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"To capture discourse behaviors of u, distribution πu is used to represent the discourse modes in messages posted by u.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The discourse mode dc,m for message m is then generated from πuc,m , where uc,m is the author of m in c.
Word-level Modeling.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We aim to separate discourse, topic, and background information for conversations.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, for each word wc,m,n of message m, a ternary switcher xc,m,n ∈ {DISC, TOPIC,BACK} controls word wc,m,n to
fall into one of the three types: discourse, topic, and background.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Discourse words (DISC) are indicative of the discourse modes of messages.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"When xc,m,n = DISC (i.e., wc,m,n is assigned as a discourse word), word wc,m,n is generated from the discourse word distribution φDdc,m where dc,m is discourse assignment to",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"message m.
Topic words (TOPIC) describe the topical focus of a conversation.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"When xc,m,n = TOPIC, wc,m,n is assigned as a topic word and generated from φTzc,m – word distribution given topic of m.
Background words (BACK) capture the general information that is not related to discourse or topic.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"When word wc,m,n is assigned as a background word (xc,m,n = BACK), it is drawn from background distribution φB .
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Switching among Topic, Discourse, and Background.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We further assume the word type switcher xc,m,n is sampled from a multinomial distribution which depends on the current discourse mode dc,m. The intuition is that messages of different discourse modes may show different distributions of the three word types.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"For instance, a statement message may contain more content words than a rhetorical question.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Specifically, xc,m,n ∼ Multi(τdc,m), where τd is a 3-dimension stochastic vector that expresses the appearing probabilities of three kinds of words (DISC, TOPIC, BACK), when the discourse assignment is d. Stop words and punctuations are forced to be labeled as discourse or background.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"By explicitly distinguishing different types of words with switcher xc,m,n, we can thus separate word distributions that reflect discourse, topic, and background information.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Likelihood.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Based on the message-level and the word-level generation process, the probability of observing words in the given corpus is:
Pr(C |θ,π,φ, τ , z,d,x)
=
C∏
c=1
Mc∏
m=1
θc,zc,mπuc,m,dc,m
× ∏
xc,m,n=BACK
τdc,m,BACKφ B wc,m,n
× ∏
xc,m,n=DISC
τdc,m,DISCφ D dc,m,wc,m,n
× ∏
xc,m,n=TOPIC
τdc,m,TOPICφ T zc,m,wc,m,n
(5)
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"And we use negative log likelihood to model corpus likelihood effect in Eq. 1, i.e., NLL(C |Θ) =
− log(Pr(C |Θ), where parameters set Θ = {θ,π,φ, τ , z,d,x}.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Latent Variables
As mentioned above, the hidden factors discovered in Section 3.1 lack interpretability, which can be boosted by the learned latent topics and discourse modes in Section 3.2.",3.3 Mutually Informed User Preference and,[0],[0]
"However, it is nontrivial to link the topic-related parameters of γCc to the conversation topic distributions of θc, since the former takes real values from −∞ to +∞ while the latter is a stochastic vector.",3.3 Mutually Informed User Preference and,[0.950516555282178],['It is interesting to demonstrate the effectiveness of the proposed mechanism on other sequence to sequence learning tasks as well.']
"Therefore, we follow the strategy from McAuley and Leskovec (2013) to apply a softmax function over γCc :
θc,k = exp(κT γCc,k)∑K
k′=1 exp(κ T γCc,k′)
(6)
",3.3 Mutually Informed User Preference and,[0],[0]
"We further assume that the discourse mode preference by users, δUu , can also be informed by the discourse mode distribution captured by πu, i.e., a user who enjoys arguments may be willing to participate another.",3.3 Mutually Informed User Preference and,[0],[0]
"So similarly, we define:
πu,d = exp(κDδUu,d)∑D
d′=1 exp(κ DδUu,d′)
(7)
where κT and κD are learnable parameters that control the “peakiness” of the transformation.",3.3 Mutually Informed User Preference and,[0],[0]
"For example, a larger κT indicates a more focused conversation, while a smaller κT means users discuss diverse topics.
",3.3 Mutually Informed User Preference and,[0],[0]
"Finally, softmax transformation is also applied to φTk , φ D d , φ
B , and τd, as done in McAuley and Leskovec (2013), with additional parameters ψTk , ψDd , ψ
B , and χd (as shown in Figure 2).",3.3 Mutually Informed User Preference and,[0],[0]
This is to ensure that the distributions φ∗∗ and τd are stochastic vectors.,3.3 Mutually Informed User Preference and,[0],[0]
"In doing so, these distributions can be learned via optimizing ψ∗∗ and χd, which take any value and thus ensure that the cost function in Eq. 1 is optimized without considering any parameter constraints.",3.3 Mutually Informed User Preference and,[0],[0]
"Our word generation process is displayed in Figure 2 and described as follows:
• Compute topic distribution θc by Eq. 6 •",3.4 Generative Process and Model Learning,[0],[0]
"For message m = 1 to Mc:
– Compute discourse distribution πuc,m by Eq. 7 – Draw topic assignment zc,m ∼Multi(θc) – Draw discourse mode dc,m ∼Multi(πuc,m) –",3.4 Generative Process and Model Learning,[0],[0]
"For word index n = 1 to Nc,m: ∗ Draw word type xc,m,n ∼Multi(τd)
∗",3.4 Generative Process and Model Learning,[0],[0]
"if xc,m,n == BACK:",3.4 Generative Process and Model Learning,[0],[0]
"Draw word wc,m,n ∼Multi(φB) ∗",3.4 Generative Process and Model Learning,[0],[0]
"if xc,m,n == DISC:",3.4 Generative Process and Model Learning,[0],[0]
"Draw word wc,m,n ∼Multi(φDdc,m) ∗",3.4 Generative Process and Model Learning,[0],[0]
"if xc,m,n == TOPIC:",3.4 Generative Process and Model Learning,[0],[0]
"Draw word wc,m,n ∼Multi(φTzc,m)
Parameter Learning.",3.4 Generative Process and Model Learning,[0],[0]
"For learning, we randomly initialize all learnable parameters and then alternate between the following two steps: Step 1.",3.4 Generative Process and Model Learning,[0],[0]
"Fix topic and discourse assignments z and d, and word type switcher x, then optimize the remaining parameters in Eq. 1 by L-BFGS (Nocedal, 1980):
Update a, b, γ∗, δ∗, κ∗, ψ∗, χ = argminL+ µ ·NLL(C |Θ) (8)
Step 2.",3.4 Generative Process and Model Learning,[0],[0]
"Sample topic and discourse assignments z and d at the message level and word type switcher x at the word level, using the distributions, computed according to parameters optimized in step 1:
Sample zc,m, dc,m, xc,m,n with probabilities p(zc,m = k) =",3.4 Generative Process and Model Learning,[0],[0]
"θc,k
p(dc,m = d) = πuc,m,d p(xc,m,n = BACK) = φ B wc,m,nτdc,m,BACK p(xc,m,n = DISC) = φ D dc,m,wc,m,nτdc,m,DISC p(xc,m,n = TOPIC) = φ T zc,m,wc,m,nτdc,m,TOPIC
(9)
Step 2 is analogous to Gibbs Sampling (Griffiths, 2002) in probabilistic graphical models, such as LDA (Blei et al., 2003).",3.4 Generative Process and Model Learning,[0],[0]
"However, distinguishing from previous models, the multinomial distributions in our models are not drawn from a Dirichlet prior.",3.4 Generative Process and Model Learning,[0],[0]
"Instead, they are computed based on the parameters learned in Step 1.
",3.4 Generative Process and Model Learning,[0],[0]
"Our learning process stops when the change of parameters is small (i.e., below a pre-specified
threshold).",3.4 Generative Process and Model Learning,[0],[0]
"Multiple restarts are tried, and similar results are achieved.",3.4 Generative Process and Model Learning,[0],[0]
Datasets.,4 Experimental Setup,[0],[0]
"We collected two microblog conversation datasets from Twitter for experiments3: one contains discussions about the U.S. presidential election (henceforth US Election), the other gathers conversations of diverse topics based on the tweets released by TREC 2011 microblog track (henceforth TREC)4.",4 Experimental Setup,[0],[0]
"US Election was collected from January to June of 2016 using Twitter’s Streaming API5 with a small set of political keywords.6 To recover conversations, Tweet Search API7 was used to retrieve messages with the “inreply-to” relations to collect tweets in a recursive way until full conversations were recovered.
",4 Experimental Setup,[0],[0]
Statistics of the datasets are shown in Table 1.,4 Experimental Setup,[0],[0]
Figure 3 displays the number of conversations individual users participated in.,4 Experimental Setup,[0],[0]
"As can be seen, most users are involved in only a few conversations.",4 Experimental Setup,[0],[0]
"Simply leveraging personal chat history will not produce good performance for conversation
3The datasets are available at http://www.ccs. neu.edu/home/luwang/
4 http://trec.nist.gov/data/tweets/ 5https://developer.twitter.com/
en/docs/tweets/filter-realtime/ api-reference/post-statuses-filter.html
6Keyword list: “trump”, “hillary”, “clinton”, “president”, “politics”, and “election.”
",4 Experimental Setup,[0],[0]
"7https://developer.twitter.com/en/ docs/tweets/search/api-reference/ get-saved_searches-show-id
recommendation.",4 Experimental Setup,[0],[0]
"In our experiments, we predict whether a user will engage in a conversation given the previous messages in that conversation and past conversations the user is involved.",4 Experimental Setup,[0],[0]
"For model training and testing, we divide conversations into three ordered segments, corresponding to training, development, and test sets at 75%, 12.5%, and 12.5%.8
Preprocessing and Hyperparameter Tuning.",4 Experimental Setup,[0],[0]
"For preprocessing, links, mentions (i.e., @username), and hashtags in tweets were replaced with generic tags of “URL”, “MENTION”, and “HASHTAG”.",4 Experimental Setup,[0],[0]
We then utilized the Twitter NLP tool9,4 Experimental Setup,[0],[0]
"(Gimpel et al., 2011; Owoputi et al., 2013) for tokenization and non-alphabetic token removal.",4 Experimental Setup,[0],[0]
We removed stop words and punctuations for all comparisons to ensure comparable performance.,4 Experimental Setup,[0],[0]
"We maintain a vocabulary with the 5,000 most frequent words.
",4 Experimental Setup,[0],[0]
"Our model parameters are tuned on the development set based on grid search, i.e. the parameters that give the lowest value for our objective are selected.",4 Experimental Setup,[0],[0]
"Specifically, the number of discourse modes (D) and topics (K) are tuned to be 10.",4 Experimental Setup,[0],[0]
"The trade-off parameter µ between user preference and corpus negative log-likelihood takes value of 0.1, and λ, the parameter for balancing topic and discourse, is set to 0.5.",4 Experimental Setup,[0],[0]
"Finally, the confidence parameter s takes a value of 200 to give higher weight for positive instances, i.e., a user replied to a conversation.
",4 Experimental Setup,[0],[0]
Evaluation Metrics.,4 Experimental Setup,[0],[0]
"Following prior work on social media post recommendation (Chen et al., 2012; Yan et al., 2012), we treat our task on conversation recommendation as a ranking problem.",4 Experimental Setup,[0],[0]
"Therefore, popular information retrieval evaluation metrics, including precision at K (P@K), mean average precision (MAP) (Manning et al., 2008), and normalized Discounted Cumulative Gain at K (nDCG@K) (Järvelin and Kekäläinen, 2002) are reported.",4 Experimental Setup,[0],[0]
The metrics are computed per user in the dataset and then averaged over all users.,4 Experimental Setup,[0],[0]
"The values range from 0.0 to 1.0, with higher values indicating better performance.
",4 Experimental Setup,[0],[0]
Baselines and Comparisons.,4 Experimental Setup,[0],[0]
"For comparison, we first consider three baselines: 1) ranking
8At least one turn per conversation is retained for training.",4 Experimental Setup,[0],[0]
"It is possible that one user only replies in either development set or test set, but it is rather infrequent.
",4 Experimental Setup,[0],[0]
"9http://www.cs.cmu.edu/˜ark/TweetNLP/
conversations randomly (RANDOM); 2) longer conversations (i.e., more words) ranked higher (LENGTH); 3) conversations with more distinct users ranked higher (POPULARITY).
",4 Experimental Setup,[0],[0]
"We further compare results with three established recommendation models: • OCCF: one-class Collaborative Filtering (Pan et al., 2008), which only considers users’ reply history without modeling content in conversations.",4 Experimental Setup,[0],[0]
• RSVM:,4 Experimental Setup,[0],[0]
"ranking SVM (Joachims, 2002), which ranks conversations for each user with the content and Twitter features as in Duan et al. (2010).",4 Experimental Setup,[0],[0]
"• CTR: messages in one conversation are aggregated into one post and a state-of-the art Collaborative Filtering-based post recommendation model is applied (Chen et al., 2012).
",4 Experimental Setup,[0],[0]
"Finally, we also adapt the “hidden factors as topics” (HFT) model proposed in McAuley and Leskovec (2013) (henceforth ADAPTED HFT).",4 Experimental Setup,[0],[0]
"Because the original model leverages the ratings for all product reviews and does not handle implicit user feedback well, we replace their user preference objective function with ours (Eq. 2).",4 Experimental Setup,[0],[0]
"In this section, we first discuss our main evaluation in Section 5.1.",5 Experimental Results,[0],[0]
"A case study and corresponding discussion are provided in Section 5.2 to provide further insights, which is followed by an analysis of the topics and discourse modes discovered by our model (Section 5.3).",5 Experimental Results,[0],[0]
We also examine our performance on first time replies (Section 5.4).,5 Experimental Results,[0],[0]
"Experimental results are displayed in Table 2, where our model yields statistically significantly better results than baselines and comparisons
(paired t-tests, p < 0.01).",5.1 Conversation Recommendation Results,[0],[0]
"For P@K, we only report P@1, because a significant amount of users participate only in 1 or 2 conversations.",5.1 Conversation Recommendation Results,[0],[0]
"For nDCG@K, different K values are experimented, which results in similar trend, so only nDCG@5 is reported.
",5.1 Conversation Recommendation Results,[0],[0]
"We find that the baselines that rank conversations with simple features (e.g., length or popularity) perform poorly.",5.1 Conversation Recommendation Results,[0],[0]
"This implies that generic algorithms that do not consider conversation content or user preference cannot produce reasonable recommendations.
",5.1 Conversation Recommendation Results,[0],[0]
"Although some non-baseline systems capture content in one way or another, only ADAPTED HFT and our model exploit latent topic models to better represent content in tweets, and outperform other methods.
",5.1 Conversation Recommendation Results,[0],[0]
"Compared to ADAPTED HFT, which only considers latent topics under a collaborative filtering framework, our model extracts both topics and discourse modes as latent variables, and shows superior performance on both datasets.",5.1 Conversation Recommendation Results,[0],[0]
"Our discourse variables go beyond topical content to capture social behaviors that affect user engagement, such as
arguments, question-asking, agreement, and other discourse modes.
",5.1 Conversation Recommendation Results,[0],[0]
Training with Varying Conversation History.,5.1 Conversation Recommendation Results,[0],[0]
"To test the model performance based different levels of user engagement history, we further experiment with varying the length of conversations for training.",5.1 Conversation Recommendation Results,[0],[0]
"Specifically, in addition to using 75% of conversation history, we also extract the first 25% and 50% of history as training.",5.1 Conversation Recommendation Results,[0],[0]
The rest of a conversation is separated equally for development and test.,5.1 Conversation Recommendation Results,[0],[0]
Figure 4 shows the MAP scores for US Election and TREC datasets.,5.1 Conversation Recommendation Results,[0],[0]
"The increasing MAP for all methods as the training history increases indicates that generally, conversation history is essential for recommendation.",5.1 Conversation Recommendation Results,[0],[0]
"Our model performs consistently better over different lengths of conversation histories.
",5.1 Conversation Recommendation Results,[0],[0]
Results for Varying Degree of Data Sparsity.,5.1 Conversation Recommendation Results,[0],[0]
"From Table 1 and Figure 3, we observe that most users in our datasets are involved in only a few conversations.",5.1 Conversation Recommendation Results,[0],[0]
"In order to study the effects of data sparsity on recommendation models, we examine in Figure 5 the MAP scores for users engaged in a varying number of conversations, as measured on the TREC dataset.",5.1 Conversation Recommendation Results,[0],[0]
The results on the US Election dataset have similar distributions.,5.1 Conversation Recommendation Results,[0],[0]
"As we see, the prediction results become worse for users involved in fewer conversations.",5.1 Conversation Recommendation Results,[0],[0]
This indicates that data sparsity serves as a challenge for all recommendation models.,5.1 Conversation Recommendation Results,[0],[0]
We also observe that our model performs consistently better than other models over different degrees of sparsity.,5.1 Conversation Recommendation Results,[0],[0]
"This implies that effectively capturing discourse structure in conversation context is useful to mitigating the effects of
data sparsity on conversation recommendation.",5.1 Conversation Recommendation Results,[0],[0]
Here we present a case study based on the sample conversations in Figure 1.,5.2 Case Study and Discussion,[0],[0]
"Recall that user U1 is interested in conversations about Sanders, and also prefers more argumentative discourse, and thus returns in conversation c2 but not c1.
",5.2 Case Study and Discussion,[0],[0]
"Table 3 shows the predicted scores for the two conversations from OCCF, ADAPTED HFT, and our model (as in Eq. 2).",5.2 Case Study and Discussion,[0],[0]
"Both ADAPTED HFT and our model more accurately recommend c2 over c1, with our model producing a slightly higher recommendation score for c2.
",5.2 Case Study and Discussion,[0],[0]
Table 4 shows the latent dimension values for the learned topics and discourse modes for this user and these two conversations.,5.2 Case Study and Discussion,[0],[0]
"Based on human inspection, topic 1 appears to contain words about Sanders, which is the main topic in conversation c2.",5.2 Case Study and Discussion,[0],[0]
"Topic 2 is about Clinton, which is a dominating topic in conversation c1.",5.2 Case Study and Discussion,[0],[0]
"Our model also picks up user interest in topic 1 (Sanders), and thus assigns γUu1,1 a high value.",5.2 Case Study and Discussion,[0],[0]
"For discourse modes, our model also generates a high score for “argument” discourse (labeled via human inspection) for both the user and c2.",5.2 Case Study and Discussion,[0],[0]
Ablation Study.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"We have shown that joint modeling of topical content and discourse modes produces the superior performance for our model.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
Here we provide an ablation study to examine the relative contributions of those two aspects by setting the trade-off parameter λ to 1.0 (topic only) or 0.0 (discourse only).,5.3 Further Analysis of Topic and Discourse,[0],[0]
"Table 5 shows that topics or discourse individually improve slightly upon the comparison ADAPTED HFT, but only jointly do they improve significantly upon it.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
Topic Coherence.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"To examine the quality of topics found by our model, we use the CV topic coherence score measured via the open-source toolkit Palmetto10, which has been shown to produce evaluation performance comparable to human judgment (Röder et al., 2015).",5.3 Further Analysis of Topic and Discourse,[0],[0]
"Our model achieves topic coherence scores of 0.343 and 0.376 on TREC and US Election datasets, compared to 0.338 and 0.371 for the topics from ADAPTED HFT.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
Sample Discourse Modes.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"While our topic word distributions are relatively unsurprising, of greater interest are the discourse mode word distributions.",5.3 Further Analysis of Topic and Discourse,[0],[0]
Table 6 shows a sample of discourse modes as labeled by human.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"Although this is merely a qualitative human judgment at this point, there does appear to be a notable overlap in discourse modes between the two datasets even though they were learned separately.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
10https://github.com/AKSW/Palmetto/,5.3 Further Analysis of Topic and Discourse,[0],[0]
"From a recommendation perspective, users may be interested in joining new conversations.",5.4 First Time Reply Results,[0],[0]
We thus compare each recommendation system for first time replies.,5.4 First Time Reply Results,[0],[0]
"For each user, we only evaluate for conversations where they are newcomers.",5.4 First Time Reply Results,[0],[0]
"Table 7 shows that, unsurprisingly, all systems perform poorly on this task, though our model performs slightly better.",5.4 First Time Reply Results,[0],[0]
"This suggests that other features, e.g., network structures or other discussion thread features, could usefully be included in future studies that target new conversations.",5.4 First Time Reply Results,[0],[0]
This paper has presented a framework for microblog conversation recommendation via jointly modeling topics and discourse modes.,6 Conclusion,[0],[0]
Experimental results show that our method can outperform competitive approaches that omit user discourse behaviors.,6 Conclusion,[0],[0]
Qualitative analysis shows that our joint model yields meaningful topics and discourse representations.,6 Conclusion,[0],[0]
This work is partly supported by Innovation and Technology Fund (ITF) Project,Acknowledgements,[0],[0]
"No. 6904333, General Research Fund (GRF) Project No. 14232816 (12183516), and National Science Foundation Grant IIS-1566382.",Acknowledgements,[0],[0]
"We thank Shuming Shi, Yan Song, and the three anonymous reviewers for the insightful suggestions on various aspects of this work.",Acknowledgements,[0],[0]
Millions of conversations are generated every day on social media platforms.,abstractText,[0],[0]
"With limited attention, it is challenging for users to select which discussions they would like to participate in.",abstractText,[0],[0]
Here we propose a new method for microblog conversation recommendation.,abstractText,[0],[0]
"While much prior work has focused on postlevel recommendation, we exploit both the conversational context, and user content and behavior preferences.",abstractText,[0],[0]
"We propose a statistical model that jointly captures: (1) topics for representing user interests and conversation content, and (2) discourse modes for describing user replying behavior and conversation dynamics.",abstractText,[0],[0]
Experimental results on two Twitter datasets demonstrate that our system outperforms methods that only model content without considering discourse.,abstractText,[0],[0]
Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 102–112 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"One of the key advantages of word embeddings for natural language processing is that they enable generalization to words that are unseen in labeled training data, by embedding lexical features from large unlabeled datasets into a relatively low-dimensional Euclidean space.",1 Introduction,[0],[0]
"These low-dimensional embeddings are typically trained to capture distributional similarity, so that information can be shared among words that tend to appear in similar contexts.
",1 Introduction,[0],[0]
"However, it is not possible to enumerate the entire vocabulary of any language, and even large unlabeled datasets will miss terms that appear in later applications.",1 Introduction,[0],[0]
The issue of how to handle these out-of-vocabulary (OOV) words poses challenges for embedding-based methods.,1 Introduction,[0],[0]
"These challenges are particularly acute when working with lowresource languages, where even unlabeled data may be difficult to obtain at scale.",1 Introduction,[0],[0]
"A typical solution is to abandon hope, by assigning a single OOV embedding to all terms that do not appear in the unlabeled data.
",1 Introduction,[0],[0]
We approach this challenge from a quasigenerative perspective.,1 Introduction,[0],[0]
"Knowing nothing of a word except for its embedding and its written form, we attempt to learn the former from the latter.",1 Introduction,[0],[0]
"We train a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task.",1 Introduction,[0],[0]
"We call this model the MIMICK-RNN, for its ability to read a word’s spelling and mimick its distributional embedding.
",1 Introduction,[0],[0]
"Through nearest-neighbor analysis, we show that vectors learned via this method capture both word-shape features and lexical features.",1 Introduction,[0],[0]
"As a result, we obtain reasonable near-neighbors for OOV abbreviations, names, novel compounds, and orthographic errors.",1 Introduction,[0],[0]
"Quantitative evaluation on the Stanford RareWord dataset (Luong et al., 2013) provides more evidence that these character-based embeddings capture word similarity for rare and unseen words.
",1 Introduction,[0],[0]
"As an extrinsic evaluation, we conduct experiments on joint prediction of part-of-speech tags and morphosyntactic attributes for a diverse set of 23 languages, as provided in the Universal Dependencies dataset (De Marneffe et al., 2014).",1 Introduction,[0],[0]
"Our model shows significant improvement
102
across the board against a single UNK-embedding backoff method, and obtains competitive results against a supervised character-embedding model, which is trained end-to-end on the target task.",1 Introduction,[0],[0]
"In low-resource settings, our approach is particularly effective, and is complementary to supervised character embeddings trained from labeled data.",1 Introduction,[0],[0]
The MIMICK-RNN therefore provides a useful new tool for tagging tasks in settings where there is limited labeled data.,1 Introduction,[0],[0]
Models and code are available at www.github.com/ yuvalpinter/mimick .,1 Introduction,[0],[0]
Compositional models for embedding rare and unseen words.,2 Related Work,[0],[0]
"Several studies make use of morphological or orthographic information when training word embeddings, enabling the prediction of embeddings for unseen words based on their internal structure.",2 Related Work,[0],[0]
Botha and Blunsom (2014) compute word embeddings by summing over embeddings of the morphemes; Luong et al. (2013) construct a recursive neural network over each word’s morphological parse; Bhatia et al. (2016) use morpheme embeddings as a prior distribution over probabilistic word embeddings.,2 Related Work,[0],[0]
"While morphology-based approaches make use of meaningful linguistic substructures, they struggle with names and foreign language words, which include out-of-vocabulary morphemes.",2 Related Work,[0],[0]
"Character-based approaches avoid these problems: for example, Kim et al. (2016) train a recurrent neural network over words, whose embeddings are constructed by convolution over character embeddings; Wieting et al. (2016) learn embeddings of character ngrams, and then sum them into word embeddings.",2 Related Work,[0],[0]
"In all of these cases, the model for composing embeddings of subword units into word embeddings is learned by optimizing an objective over a large unlabeled corpus.",2 Related Work,[0],[0]
"In contrast, our approach is a post-processing step that can be applied to any set of word embeddings, regardless of how they were trained.",2 Related Work,[0],[0]
"This is similar to the “retrofitting” approach of Faruqui et al. (2015), but rather than smoothing embeddings over a graph, we learn a function to build embeddings compositionally.
",2 Related Work,[0],[0]
Supervised subword models.,2 Related Work,[0],[0]
Another class of methods learn task-specific character-based word embeddings within end-to-end supervised systems.,2 Related Work,[0],[0]
"For example, Santos and Zadrozny (2014) build word embeddings by convolution over char-
acters, and then perform part-of-speech (POS) tagging using a local classifier; the tagging objective drives the entire learning process.",2 Related Work,[0],[0]
"Ling et al. (2015) propose a multi-level long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997), in which word embeddings are built compositionally from an LSTM over characters, and then tagging is performed by an LSTM over words.",2 Related Work,[0],[0]
Plank et al. (2016) show that concatenating a character-level or bit-level LSTM network to a word representation helps immensely in POS tagging.,2 Related Work,[0],[0]
"Because these methods learn from labeled data, they can cover only as much of the lexicon as appears in their labeled training sets.",2 Related Work,[0],[0]
"As we show, they struggle in several settings: lowresource languages, where labeled training data is scarce; morphologically rich languages, where the number of morphemes is large, or where the mapping from form to meaning is complex; and in Chinese, where the number of characters is orders of magnitude larger than in non-logographic scripts.",2 Related Work,[0],[0]
"Furthermore, supervised subword models can be combined with MIMICK, offering additive improvements.
",2 Related Work,[0],[0]
Morphosyntactic attribute tagging.,2 Related Work,[0],[0]
"We evaluate our method on the task of tagging word tokens for their morphosyntactic attributes, such as gender, number, case, and tense.",2 Related Work,[0],[0]
"The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuruöz, 1994; Hajič and Hladká, 1998), and interest has been rejuvenated by the availability of large-scale multilingual morphosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014).",2 Related Work,[0],[0]
"For example, Faruqui et al. (2016) propose a graph-based technique for propagating typelevel morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger.",2 Related Work,[0],[0]
"In contrast, we apply a neural sequence labeling approach, inspired by the POS tagger of Plank et al. (2016).",2 Related Work,[0],[0]
"We approach the problem of out-of-vocabulary (OOV) embeddings as a generation problem: regardless of how the original embeddings were created, we assume there is a generative wordformbased protocol for creating these embeddings.",3 MIMICK Word Embeddings,[0],[0]
"By training a model over the existing vocabulary, we can later use that model for predicting the embedding of an unseen word.
",3 MIMICK Word Embeddings,[0],[0]
"Formally: given a language L, a vocabulary V ⊆ L of size V , and a pre-trained embeddings table W ∈ RV×d where each word {wk}Vk=1 is assigned a vector ek of dimension d, our model is trained to find the function f :",3 MIMICK Word Embeddings,[0],[0]
L → Rd such that the projected function f |V approximates the assignments f(wk),3 MIMICK Word Embeddings,[0],[0]
≈ ek.,3 MIMICK Word Embeddings,[0],[0]
"Given such a model, a new word wk∗ ∈ L \ V can now be assigned an embedding ek∗ = f(wk∗).
",3 MIMICK Word Embeddings,[0],[0]
Our predictive function of choice is a Word Type Character Bi-LSTM.,3 MIMICK Word Embeddings,[0],[0]
"Given a word with character sequence w = {ci}n1 , a forward-LSTM and a backward-LSTM are run over the corresponding character embeddings sequence {e(c)i }n1 .",3 MIMICK Word Embeddings,[0],[0]
"Let hnf represent the final hidden vector for the forward-LSTM, and let h0b represent the final hidden vector for the backward-LSTM.",3 MIMICK Word Embeddings,[0],[0]
"The word embedding is computed by a multilayer perceptron:
(1)f(w) = OT · g(Th ·",3 MIMICK Word Embeddings,[0],[0]
"[hnf ; h0b ] + bh) + bT ,
where Th, bh and OT , bT are parameters of affine transformations, and g is a nonlinear elementwise function.",3 MIMICK Word Embeddings,[0],[0]
"The model is presented in Figure 1.
",3 MIMICK Word Embeddings,[0],[0]
The training objective is similar to that of Yin and Schütze (2016).,3 MIMICK Word Embeddings,[0],[0]
"We match the predicted embeddings f(wk) to the pre-trained word embeddings ewk , by minimizing the squared Euclidean distance,
(2)L = ‖f(wk)− ewk‖22 .
",3 MIMICK Word Embeddings,[0],[0]
"By backpropagating from this loss, it is possible to obtain local gradients with respect to the parameters of the LSTMs, the character embeddings, and the output model.",3 MIMICK Word Embeddings,[0.9502594024743014],"['However, due to the large amount of parameters and relatively small training data set, the end-toend learning of an NMT model may not be able to learn the best solution.']"
"The ultimate output of the training phase is the character embeddings matrix C and the parameters of the neural network: M = {C,F,B,Th, bh,OT , bT }, where F,B are the forward and backward LSTM component parameters, respectively.",3 MIMICK Word Embeddings,[0],[0]
"The pretrained embeddings we use in our experiments are obtained from Polyglot (Al-Rfou et al., 2013), a multilingual word embedding effort.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Available for dozens of languages, each dataset contains 64-dimension embeddings for the 100,000 most frequent words in a language’s training corpus (of variable size), as well as an UNK embedding to be used for OOV words.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Even with this vocabulary size, querying words from respective UD corpora (train + dev + test) yields high
OOV rates: in at least half of the 23 languages in our experiments (see Section 5), 29.1% or more of the word types do not appear in the Polyglot vocabulary.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"The token-level median rate is 9.2%.1
Applying our MIMICK algorithm to Polyglot embeddings, we obtain a prediction model for each of the 23 languages.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Based on preliminary testing on randomly selected held-out development sets of 1% from each Polyglot vocabulary (with error calculated as in Equation 2), we set the following hyper-parameters for the remainder of the experiments: character embedding dimension = 20; one LSTM layer with 50 hidden units; 60 training epochs with no dropout; nonlinearity function g =",3.1 MIMICK Polyglot Embeddings,[0],[0]
"tanh.2 We initialize character embeddings randomly, and use DyNet to implement the model (Neubig et al., 2017).
",3.1 MIMICK Polyglot Embeddings,[0],[0]
Nearest-neighbor examination.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"As a preliminary sanity check for the validity of our protocol, we examined nearest-neighbor samples in languages for which speakers were available: English, Hebrew, Tamil, and Spanish.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Table 1 presents selected English OOV words with
1Some OOV counts, and resulting model performance, may be adversely affected by tokenization differences between Polyglot and UD.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Notably, some languages such as Spanish, Hebrew and Italian exhibit relational synthesis wherein words of separate grammatical phrases are joined into one form (e.g. Spanish del = de + el, ‘from the-masc.sg.’).",3.1 MIMICK Polyglot Embeddings,[0],[0]
"For these languages, the UD annotations adhere to the sub-token level, while Polyglot does not perform subtokenization.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"As this is a real-world difficulty facing users of out-of-the-box embeddings, we do not patch it over in our implementations or evaluation.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
"2Other settings, described below, were tuned on the supervised downstream tasks.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
their nearest in-vocabulary Polyglot words computed by cosine similarity.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"These examples demonstrate several properties: (a) word shape is learned well (acronyms, capitalizations, suffixes); (b) the model shows robustness to typos (e.g., developiong, corssing); (c) part-of-speech is learned across multiple suffixes (pesky – euphoric, ghastly); (d) word compounding is detected (e.g., lawnmower – bookmaker, postman); (e) semantics are not learned well (as is to be expected from the lack of context in training), but there are surprises (e.g., flatfish – slimy, watery).",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Table 2 presents examples from Hebrew that show learned properties can be extended to nominal morphosyntactic attributes (gender, number – first two examples) and even relational syntactic subword forms such as genetive markers (third example).",3.1 MIMICK Polyglot Embeddings,[0],[0]
Names are learned (fourth example) despite the lack of casing in the script.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"Spanish examples exhibit wordshape and part-of-speech learning patterns with some loose semantics: for example, the plural adjective form prenatales is similar to other familyrelated plural adjectives such as patrimoniales and generacionales.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Tamil displays some semantic similarities as well: e.g. enjineer (‘engineer’) predicts similarity to other professional terms such as kalviyiyal (‘education’), thozhilnutpa (‘technical’), and iraanuva (‘military’).
",3.1 MIMICK Polyglot Embeddings,[0],[0]
Stanford RareWords.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"The Stanford RareWord evaluation corpus (Luong et al., 2013) focuses on predicting word similarity between pairs involving low-frequency English words, predominantly ones with common morphological affixes.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"As these words are unlikely to be above the cutoff threshold for standard word embedding models, they emphasize the performance on OOV words.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
"For evaluation of our MIMICK model on the RareWord corpus, we trained the Variational Embeddings algorithm (VarEmbed; Bhatia et al., 2016) on a 20-million-token, 100,000- type Wikipedia corpus, obtaining 128-dimension
word embeddings for all words in the test corpus.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"VarEmbed estimates a prior distribution over word embeddings, conditional on the morphological composition.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"For in-vocabulary words, a posterior is estimated from unlabeled data; for outof-vocabulary words, the expected embedding can be obtained from the prior alone.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"In addition, we compare to FastText (Bojanowski et al., 2016), a high-vocabulary, high-dimensionality embedding benchmark.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
"The results, shown in Table 3, demonstrate that the MIMICK RNN recovers about half of the loss in performance incurred by the original Polyglot training model due to out-of-vocabulary words in the “All pairs” condition.",3.1 MIMICK Polyglot Embeddings,[0],[0]
MIMICK also outperforms VarEmbed.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"FastText can be considered an upper bound: with a vocabulary that is 25 times larger than the other models, it was missing words from only 44 pairs on this data.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"The Universal Dependencies (UD) scheme (De Marneffe et al., 2014) features a minimal set of 17 POS tags (Petrov et al., 2012) and supports tagging further language-specific features using attribute-specific inventories.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"For example, a verb in Turkish could be assigned a value for the evidentiality attribute, one which is absent from Danish.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"These additional morphosyntactic attributes are marked in the UD dataset as optional per-token attribute-value pairs.
",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Our approach for tagging morphosyntactic attributes is similar to the part-of-speech tagging model of Ling et al. (2015), who attach a projection layer to the output of a sentence-level bidirectional LSTM.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
We extend this approach to morphosyntactic tagging by duplicating this projection layer for each attribute type.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"The input to our multilayer perceptron (MLP) projection network is the hidden state produced for each token in the sentence by an underlying LSTM, and the output is
attribute-specific probability distributions over the possible values for each attribute on each token in the sequence.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Formally, for a given attribute a with possible values v ∈ Va, the tagging probability for the i’th word in a sentence is given by:
Pr(awi = v) =",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"(Softmax(φ(hi)))v , (3)
with
(4)φ(hi) = OaW · tanh(Wah ·",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"hi + bah) + baW ,
where hi is the i’th hidden state in the underlying LSTM, and φ(hi) is a two-layer feedforward neural network, with weights Wah and O a W .",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
We apply a softmax transformation to the output; the value at position v is then equal to the probability of attribute v applying to token wi.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"The input to the underlying LSTM is a sequence of word embeddings, which are initialized to the Polyglot vectors when possible, and to MIMICK vectors when necessary.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Alternative initializations are considered in the evaluation, as described in Section 5.2.
",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
Each tagged attribute sequence (including POS tags) produces a loss equal to the sum of negative log probabilities of the true tags.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
One way to combine these losses is to simply compute the sum loss.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"However, many languages have large differences in sparsity across morpho-syntactic attributes, as apparent from Table 4 (rightmost column).",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"We therefore also compute a weighted sum
loss, in which each attribute is weighted by the proportion of training corpus tokens on which it is assigned a non-NONE value.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Preliminary experiments on development set data were inconclusive across languages and training set sizes, and so we kept the simpler sum loss objective for the remainder of our study.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"In all cases, part-of-speech tagging was less accurate when learned jointly with morphosyntactic attributes.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
This may be because the attribute loss acts as POS-unrelated “noise” affecting the common LSTM layer and the word embeddings.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
The morphological complexity and compositionality of words varies greatly across languages.,5 Experimental Settings,[0],[0]
"While a morphologically-rich agglutinative language such as Hungarian contains words that carry many attributes as fully separable morphemes, a sentence in an analytic language such as Vietnamese may have not a single polymorphemic or inflected word in it.",5 Experimental Settings,[0],[0]
"To see whether this property is influential on our MIMICK model and its performance in the downstream tagging task, we select languages that comprise a sample of multiple morphological patterns.",5 Experimental Settings,[0],[0]
"Language family and script type are other potentially influential factors in an orthography-based approach such as ours, and so we vary along these parameters as well.",5 Experimental Settings,[0],[0]
"We also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agić (2017).
",5 Experimental Settings,[0],[0]
"As stated above, our approach is built on the Polyglot word embeddings.",5 Experimental Settings,[0],[0]
The intersection of the Polyglot embeddings and the UD dataset (version 1.4) yields 44 languages.,5 Experimental Settings,[0],[0]
"Of these, many are under-annotated for morphosyntactic attributes; we select twenty-three sufficiently-tagged languages, with the exception of Indonesian.3 Table 4 presents the selected languages and their typological properties.",5 Experimental Settings,[0],[0]
"As an additional proxy for mor-
3Vietnamese has no attributes by design; it is a pure analytic language.
",5 Experimental Settings,[0],[0]
"phological expressiveness, the rightmost column shows the proportion of UD tokens which are annotated with any morphosyntactic attribute.",5 Experimental Settings,[0],[0]
"As noted above, we use the UD datasets for testing our MIMICK algorithm on 23 languages4 with the supplied train/dev/test division.",5.1 Metrics,[0],[0]
"We measure partof-speech tagging by overall token-level accuracy.
",5.1 Metrics,[0],[0]
"For morphosyntactic attributes, there does not seem to be an agreed-upon metric for reporting performance.",5.1 Metrics,[0],[0]
Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene.,5.1 Metrics,[0],[0]
"Faruqui et al. (2016) report macro-averages of F1 scores of 11 languages from UD 1.1 for the various attributes (e.g., part-ofspeech, case, gender, tense); recall and precision were calculated for the full set of each attribute’s values, pooled together.5",5.1 Metrics,[0],[0]
Agić,5.1 Metrics,[0],[0]
"et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag.",5.1 Metrics,[0],[0]
"Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.g. ‘Ncmsh’ for “Noun short masculine singular definite”) in Bulgarian, reaching a tagset of size 680.",5.1 Metrics,[0],[0]
Müller et al. (2013) do the same for six other languages.,5.1 Metrics,[0],[0]
"We report micro F1: each token’s value for each attribute is compared separately with the gold labeling, where a correct prediction is a matching non-NONE attribute/value assignment.",5.1 Metrics,[0],[0]
"Recall and
4When several datasets are available for a language, we use the unmarked corpus.
5Details were clarified in personal communication with the authors.
precision are calculated over the entire set, with F1 defined as their harmonic mean.",5.1 Metrics,[0],[0]
"We implement and test the following models:
No-Char.",5.2 Models,[0],[0]
"Word embeddings are initialized from Polyglot models, with unseen words assigned the Polyglot-supplied UNK vector.",5.2 Models,[0],[0]
"Following tuning experiments on all languages with cased script, we found it beneficial to first back off to the lowercased form for an OOV word if its embedding exists, and only otherwise assign UNK.
MIMICK.",5.2 Models,[0],[0]
"Word embeddings are initialized from Polyglot, with OOV embeddings inferred from a MIMICK model (Section 3) trained on the Polyglot embeddings.",5.2 Models,[0],[0]
"Unlike the No-Char case, backing off to lowercased embeddings before using the MIMICK output did not yield conclusive benefits and thus we report results for the more straightforward no-backoff implementation.
CHAR→TAG.",5.2 Models,[0],[0]
"Word embeddings are initialized from Polyglot as in the No-Char model (with lowercase backoff), and appended with the output of a character-level LSTM updated during training (Plank et al., 2016).",5.2 Models,[0],[0]
"This additional module causes a threefold increase in training time.
",5.2 Models,[0],[0]
Both.,5.2 Models,[0],[0]
"Word embeddings are initialized as in MIMICK, and appended with the CHAR→TAG LSTM.
",5.2 Models,[0],[0]
Other models.,5.2 Models,[0],[0]
"Several non-Polyglot embedding models were examined, all performed substantially worse than Polyglot.",5.2 Models,[0],[0]
"Two of these
are notable: a random-initialization baseline, and a model initialized from FastText embeddings (tested on English).",5.2 Models,[0],[0]
"FastText supplies 300-dimension embeddings for 2.51 million lowercase-only forms, and no UNK vector.6 Both of these embedding models were attempted with and without CHAR→TAG concatenation.",5.2 Models,[0],[0]
"Another model, initialized from only MIMICK output embeddings, performed well only on the language with smallest Polyglot training corpus (Latvian).",5.2 Models,[0],[0]
"A Polyglot model where OOVs were initialized using an averaged embedding of all Polyglot vectors, rather than the supplied UNK vector, performed worse than our No-Char baseline on a great majority of the languages.
",5.2 Models,[0],[0]
"Last, we do not employ type-based tagset restrictions.",5.2 Models,[0],[0]
All tag inventories are computed from the training sets and each tag selection is performed over the full set.,5.2 Models,[0],[0]
"Based on development set experiments, we set the following hyperparameters for all models on all languages: two LSTM layers of hidden size 128, MLP hidden layers of size equal to the number of each attribute’s possible values; momentum stochastic gradient descent with 0.01 learning rate; 40 training epochs (80 for 5K settings) with a dropout rate of 0.5.",5.3 Hyperparameters,[0],[0]
The CHAR→TAG models use 20-dimension character embeddings and a single hidden layer of size 128.,5.3 Hyperparameters,[0],[0]
We report performance in both low-resource and full-resource settings.,6 Results,[0],[0]
"Low-resource training sets were obtained by randomly sampling training sentences, without replacement, until a predefined token limit was reached.",6 Results,[0],[0]
We report the results on the full sets and on N = 5000 tokens in Table 5 (partof-speech tagging accuracy) and Table 6 (morphosyntactic attribute tagging micro-F1).,6 Results,[0],[0]
"Results for additional training set sizes are shown in Figure 2; space constraints prevent us from showing figures for all languages.
MIMICK as OOV initialization.",6 Results,[0],[0]
"In nearly all experimental settings on both tasks, across languages and training corpus sizes, the MIMICK embeddings significantly improve over the Polyglot UNK embedding for OOV tokens on both
6Vocabulary type-level coverage for the English UD corpus: 55.6% case-sensitive, 87.9% case-insensitive.
",6 Results,[0],[0]
POS and morphosyntactic tagging.,6 Results,[0],[0]
"For POS, the largest margins are in the Slavic languages (Russian, Czech, Bulgarian), where word order is relatively free and thus rich word representations are imperative.",6 Results,[0],[0]
"Chinese also exhibits impressive improvement across all settings, perhaps due to the large character inventory (> 12,000), for which a model such as MIMICK can learn well-informed embeddings using the large Polyglot vocabulary dataset, overcoming both word- and characterlevel sparsity in the UD",6 Results,[0],[0]
"corpus.7 In morphosyntactic tagging, gains are apparent for Slavic languages and Chinese, but also for agglutinative languages — especially Tamil and Turkish — where the stable morpheme representation makes it easy for subword modeling to provide a type-level signal.8 To examine the effects on Slavic and agglutinative languages in a more fine-grained view, we present results of multiple training-set size experiments for each model, averaged over five repetitions (with different corpus samples), in Figure 2.
MIMICK vs. CHAR→TAG.",6 Results,[0],[0]
"In several languages, the MIMICK algorithm fares better than the CHAR→TAG model on part-of-speech tagging in low-resource settings.",6 Results,[0],[0]
"Table 7 presents the POS tagging improvements that MIMICK achieves over the pre-trained Polyglot models, with and without CHAR→TAG concatenation, with 10,000 tokens of training data.",6 Results,[0],[0]
"We obtain statistically significant improvements in most languages, even when CHAR→TAG is included.",6 Results,[0],[0]
"These improvements are particularly substantial for test-set tokens outside the UD training set, as shown in the right two columns.",6 Results,[0],[0]
"While test set OOVs are a strength of the CHAR→TAG model (Plank et al., 2016), in many languages there are still considerable improvements to be obtained from the application of MIMICK initialization.",6 Results,[0],[0]
"This suggests that with limited training data, the end-to-end CHAR→TAG model is unable to learn a sufficiently accurate representational mapping from orthography.",6 Results,[0],[0]
"We present a straightforward algorithm to infer OOV word embedding vectors from pre-trained,
7Character coverage in Chinese Polyglot is surprisingly good: only eight characters from the UD dataset are unseen in Polyglot, across more than 10,000 unseen word types.
",7 Conclusion,[0],[0]
8Persian is officially classified as agglutinative but it is mostly so with respect to derivations.,7 Conclusion,[0],[0]
"Its word-level inflections are rare and usually fusional.
limited-vocabulary models, without need to access the originating corpus.",7 Conclusion,[0],[0]
"This method is particularly useful for low-resource languages and tasks with little labeled data available, and in fact is task-agnostic.",7 Conclusion,[0],[0]
"Our method improves performance over word-based models on annotated sequence-tagging tasks for a large variety of languages across dimensions of family, orthography, and morphology.",7 Conclusion,[0],[0]
"In addition, we present a BiLSTM approach for tagging morphosyntactic attributes at the token level.",7 Conclusion,[0],[0]
"In this paper, the MIMICK model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters (Costa-jussà et al., 2017).",7 Conclusion,[0.9551310410114958],"['The prediction could be made from the initial state s0, without using extra resources such as word dictionaries, extracted phrases or frequent word lists, as in Mi et al. (2016).']"
"We thank Umashanthi Pavalanathan, Sandeep Soni, Roi Reichart, and our anonymous reviewers for their valuable input.",8 Acknowledgments,[0],[0]
We thank Manaal Faruqui and Ryan McDonald for their help in understanding the metrics for morphosyntactic tagging.,8 Acknowledgments,[0],[0]
The project was supported by project HDTRA1-15-10019 from the Defense Threat Reduction Agency.,8 Acknowledgments,[0],[0]
"Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data.",abstractText,[0],[0]
"However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist.",abstractText,[0],[0]
"In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings.",abstractText,[0],[0]
"Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level.",abstractText,[0],[0]
Intrinsic and extrinsic evaluations demonstrate the power of this simple approach.,abstractText,[0],[0]
"On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes.",abstractText,[0],[0]
It is competitive with (and complementary to) a supervised characterbased model in low-resource settings.,abstractText,[0],[0]
Mimicking Word Embeddings using Subword RNNs,title,[0],[0]
The sheer number and variety of online social networks (OSN) today is staggering.,1. Introduction,[0],[0]
"Although the purpose and the shaping of these networks vary generously, the majority of them has one aspect in common: the value of most OSNs is in its user data and the information that one can infer from the data.",1. Introduction,[0.9542165944481851],"['The intuition is that since the initial state is responsible for the translation of whole target sentence, it should at least contain information of each word in the target sentence.']"
"This, unfortunately, results in a big incentive for culprits to intrude OSNs and manipulate their data.",1. Introduction,[0],[0]
"One popular method of intruding and attacking an OSN is referred to as Sybil attack, where the intruder creates a whole bunch of fake (Sybil) accounts that are all under the attacker’s control.",1. Introduction,[0],[0]
"The intruder’s influence over the OSN
1MathPlan, 10587 Berlin, Germany 2Machine Learning Group, Berlin Institute of Technology, 10587 Berlin, Germany 3Berlin Big Data Center 4Max Planck Society 5Korea University.",1. Introduction,[0],[0]
Correspondence to: János,1. Introduction,[0],[0]
Höner <,1. Introduction,[0],[0]
"janos.hoener@campus.tuberlin.de>, Nico Görnitz <nico.goernitz@tu-berlin.de>, KlausRobert Müller <klaus-robert.mueller@tu-berlin.de>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
is multiplied by the number of accounts created, which opens possibilities of manipulation typically for gaining some monetary advantage in the end.
",1. Introduction,[0],[0]
"The term, Sybil attack, was coined by Douceur (2002) who showed that this kind of attack will be always possible unless a trusted agency certifies identities.",1. Introduction,[0],[0]
"Unfortunately, this approach is orthogonal to how OSNs grow.",1. Introduction,[0],[0]
The threshold of registration must be as low as possible to attract as many new users as possible.,1. Introduction,[0],[0]
"On the other hand, Sybil attacks can damage the value of OSNs significantly, which has been proved by the fact that Facebook shares dropped in 2012 after the company revealed that a significant share of its network is made up by Sybil accounts (The Associated Press, 2012).
",1. Introduction,[0],[0]
"There exists a number of “classic” feature-based solutions (Stein et al., 2011; Cao et al., 2012; Stringhini et al., 2010; Yang et al., 2014).",1. Introduction,[0],[0]
"However, up until now, it remains an unsolved problem as those methods can be evaded by cleverly designed attacking schemes (Bilge et al., 2009; Boshmaf et al., 2011; Wagner et al., 2012; Lowd & Meek, 2005) and manual detection is too expensive, time consuming, and simply unfeasible in large OSNs (Cao et al., 2012).
",1. Introduction,[0],[0]
More recent graph-based Sybil detection methods assume that honest (non-Sybil) nodes form a strongly connected subgraph and attackers can establish a limited amount of edges which leads to a sparse cut between the honest subraph and the Sybil nodes.,1. Introduction,[0],[0]
"The majority of the graph-based methods define trusted nodes, which the defender is sure to be honest, and use random walks (Yu et al., 2010; Danezis, 2009; Cao et al., 2012) or other typical graph-based algorithms like breadth-first-search (Tran et al., 2011) and belief propagation (Gong et al., 2014) to convey trust from the trusted nodes.",1. Introduction,[0],[0]
A node is identified as Sybil if sufficient ammount of trust is not delivered to it.,1. Introduction,[0],[0]
"Among random-walk based approaches, SybilRank is known to be the state-ofthe-art, of which the performance is theoretically guaranteed (Cao et al., 2012).",1. Introduction,[0],[0]
"However, the theory holds only under unrealistic topological assumptions of the network.",1. Introduction,[0],[0]
"In this paper, we show that the same theoretical guarantee can be obtained under more realistic situations.
",1. Introduction,[0],[0]
We further dicuss the robustness of the random walk approach against adversarial strategies.,1. Introduction,[0],[0]
"To this end, we formally introduce adversarial settings for graph-based Sybil
detection and derive an optimal attacking strategy that is based on the exploitation of trust leaks.",1. Introduction,[0],[0]
"Based on our analysis, we propose a transductive Sybil ranking (TSR), an integrated approach capable of adjusting edge weights based on sampled trust leaks.",1. Introduction,[0],[0]
We empirically show good performance of TSR against the state-of-the-art baselines on a variety of attacking scenarios using artificially generated data as well as real-world Facebook data.,1. Introduction,[0],[0]
"We are given a graph G = (V,E) consisting of nodes V and pairwise edges E between nodes.",2. Preliminaries,[0],[0]
"We denote GS = (VS , ES) the Sybil sub-graph, GH = (VH , EH) the disjunct honest sub-graph, and VT ✓ VH our trusted (verified nonSybil nodes) random walk seed nodes.",2. Preliminaries,[0],[0]
EA is the set of edges connecting any node in GS and any node in GH .,2. Preliminaries,[0],[0]
"Sybil Rank is considered the state-of-the-art graph-based method to detect Sybil accounts as it outperformed all its contestants (Cao et al., 2012).",2. Preliminaries,[0],[0]
It is also based on random walks and operates solely on the topology of the graph.,2. Preliminaries,[0],[0]
"Sybil Rank starts from the initial distribution {p(i)0 2 [0, 1]}|V |i=1",2. Preliminaries,[0],[0]
"(without superscript refers to a vector containing all elements), in which “trust” is assigned to the known honest nodes VT :
p (i) 0",2. Preliminaries,[0],[0]
"=
( 1
|VT",2. Preliminaries,[0],[0]
"| if vi 2 VT , 0 otherwise.
(1)
Then, it “propagates” the trust via a short (k steps) random walk:
p > k",2. Preliminaries,[0],[0]
= p >,2. Preliminaries,[0],[0]
k,2. Preliminaries,[0],[0]
"1Q = · · · = p>0 Qk , (2)
where Q 2 R|V |⇥|V",2. Preliminaries,[0],[0]
"| is the transition matrix through the edges with Qi,j = ( P j0 1[(i, j 0 ) 2 E]) 1, if (i, j) 2 E, and else 0.",2. Preliminaries,[0],[0]
"It is known that the stationary distribution ⇡ ⌘ p1 is the normalized degree distribution (Behrends, 2000)
⇡ > = ⇣ deg(v1) Vol(V ) , . . .",2. Preliminaries,[0],[0]
", deg(v|V |) Vol(V ) ⌘ , (3)
where deg(v) is the degree of node v, i.e., the number of all incident edges of v, and Vol(V ) = P v2V deg(v) is the sum of the degrees for all nodes in V .",2. Preliminaries,[0],[0]
"SybilRank conpensates the effect of degrees, and use the degree-normalized probability
p (i) = p",2. Preliminaries,[0],[0]
"(i) k /⇡ (i) (4)
as the ranking score, where a high ranking indicates a high probability of being an honest node.
",2. Preliminaries,[0],[0]
"Essentially, SybilRank relies on the assumption that the total number of attacking edges is bounded.",2. Preliminaries,[0],[0]
"Under this assumption, only a small fraction of the trust is propagated
through the sparse cut between the honest network and the Sybil nodes during the short random walk, while ”trust” go through the ”non-trusted” honest nodes through the dense connections within the the honest subgraph.
",2. Preliminaries,[0],[0]
Boshmaf et al. (2016) developed Integro to cope with a larger number of attacking edges.,2. Preliminaries,[0],[0]
"To this end, Integro introduces weights on the edges to bias the random walk, where the weights are determined after its pre-processing step to detect victims.",2. Preliminaries,[0],[0]
Here a victim is defined as a node that established a connection to one of the Sybil nodes.,2. Preliminaries,[0],[0]
"The set of all victim nodes is defined by Vv = {v 2 Vh : 9(v, s) 2 EA}.",2. Preliminaries,[0],[0]
"After the detection step, Integro lowers the weights to all incident edges to the detected victims, which prevents the trust to propagate through victim nodes.",2. Preliminaries,[0],[0]
"As the victims form a natural border between the honest and the Sybil graph, this reduces the overall flow of trust into the Sybil graph.",2. Preliminaries,[0],[0]
Boshmaf et al. found that traditional feature-based classification methods yield good and robust detection of victims.,2. Preliminaries,[0],[0]
"A notable advantage against the feature-based Sybil detection is that, unlike Sybils, victims generally do not behave adversarial, as they don’t have any incentive to ”hide”.",2. Preliminaries,[0],[0]
"More Realistic Assumptions
Cao et al. (2012) gave a security guarantee for SybilRank.",3. SybilRank’s Security Guarantee Under,[0],[0]
Let g := |EA| be the number of attacking edges and n := |V | be the number of all nodes in the graph.,3. SybilRank’s Security Guarantee Under,[0],[0]
their theory relies on the notion of trust leaks.,3. SybilRank’s Security Guarantee Under,[0],[0]
Definition 1.,3. SybilRank’s Security Guarantee Under,[0],[0]
(Trust leaks) Let rk0 = P i2VH p (i) k0 be the trust that remains in the honest graph after k0 random walk steps.,3. SybilRank’s Security Guarantee Under,[0],[0]
We call l = Pk k0=1(rk0+1 rk0) the absolute trust leak.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Assume that the attacking edges are created randomly, following a distribution ↵(EA).",3. SybilRank’s Security Guarantee Under,[0],[0]
We call CH(k 0 ),3. SybilRank’s Security Guarantee Under,[0],[0]
"= E↵(EA)[ rk0+1 rk0 rk0
] the expected relative trust leak.
",3. SybilRank’s Security Guarantee Under,[0],[0]
CH(k 0 ) is actually a constant with respect to k0 under reasonable assumptions on ↵(EA).,3. SybilRank’s Security Guarantee Under,[0],[0]
The following lemma has been proved: Lemma 1.,3. SybilRank’s Security Guarantee Under,[0],[0]
"(Cao et al., 2012) Assume that the graph G is created randomly, following the configuration model (Molloy & Reed, 1995).",3. SybilRank’s Security Guarantee Under,[0],[0]
"Then, the expected relative trust leak in each iteration is given by CH = gvol(VH) .
",3. SybilRank’s Security Guarantee Under,[0],[0]
This leads to a theoretical guarantee of SybilRank.,3. SybilRank’s Security Guarantee Under,[0],[0]
Theorem 1.,3. SybilRank’s Security Guarantee Under,[0],[0]
"(Cao et al., 2012) Assume that the graph G is created randomly, following the configuration model.",3. SybilRank’s Security Guarantee Under,[0],[0]
The total number of Sybils that are ranked higher than nonSybils by SybilRank is O(g log n).,3. SybilRank’s Security Guarantee Under,[0],[0]
"Theorem 1 implies good performance of SybilRank, but
it holds under the assumption that the attacking edges are created in the same process as the honest graph,1 which is not realistic.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Below, we show that the same guarantee is obtained under the following more realistic assumption: Assumption 1.",3. SybilRank’s Security Guarantee Under,[0],[0]
"The graph G is constructed by the following steps:
1.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Honest graph GH construction: GH is connected, nonbipartite, and scale free, i.e., the degree distribution follows the power law distribution.
",3. SybilRank’s Security Guarantee Under,[0],[0]
2.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Sybil graph GS construction: The topology of GS is arbitrary.
",3. SybilRank’s Security Guarantee Under,[0],[0]
3.,3. SybilRank’s Security Guarantee Under,[0],[0]
Attacking edges EA generation:,3. SybilRank’s Security Guarantee Under,[0],[0]
"The attacking edges are genarated on all possible edges EA ⇢ VS ⇥ VH between the honest and the Sybil subgraphs with equal propability.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Under Assumption 1, evaluating the expected trust leak is less straightforward.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Nevertheless, we can show that it results in the same formal security guarantee stated in Theorem 1.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"To properly compute the expected trust leak, the following random variables are defined.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Xv counts the number of attacking edges incident to node v, Yv = ⇡(v)
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Xv deg(v,GH)+Xv = ⇡(v) Xv deg(v,G) is the trust leak in node
v and Z = P
",3. SybilRank’s Security Guarantee Under,[0],[0]
v2VH Yv is the total trust leak.,3. SybilRank’s Security Guarantee Under,[0],[0]
Note that here ⇡(v) is the current amount of trust in node v and not the stationary distribution of the random walk.,3. SybilRank’s Security Guarantee Under,[0],[0]
This notation is used to avoid confusion with the probability mass function denoted by P .,3. SybilRank’s Security Guarantee Under,[0],[0]
"From Assumption 1 it follows that Xv is hypergeometrically distributed (Tuckwell, 1995) with the following parameters: the population size: N = |VH⇥VS |, successes: K = |{v}⇥ VS |, and the draws n = |EA|.",3. SybilRank’s Security Guarantee Under,[0],[0]
Let g := |EA| be the number of attacking edges.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Moreover, let nH := |VH",3. SybilRank’s Security Guarantee Under,[0],[0]
| and nS,3. SybilRank’s Security Guarantee Under,[0],[0]
":= |VS | denote the number of honest nodes and Sybil nodes, respectively.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"The probability mass function of Xv is given by P (Xv =
k) =
✓ K
k ◆✓ N K n k ◆ / ✓ N n",3. SybilRank’s Security Guarantee Under,[0],[0]
◆,3. SybilRank’s Security Guarantee Under,[0],[0]
"and according to Tuckwell
(1995), its expected value can be computed by E[Xv] = n
K N = |EA| |{v}⇥VS ||VH⇥VS | = |EA| |VH",3. SybilRank’s Security Guarantee Under,[0],[0]
"| = g nH
.",3. SybilRank’s Security Guarantee Under,[0],[0]
"The final goal is to compute the expected value of Z. The linearity of the expected value yields E[Z] =
P v2VH E[Yv] and for the
expected value of Yv we get
E[Yv] = ⇡(v)deg(v,G) P1 k=0 kP (Xv = k)
= ⇡(v) deg(v,G)E[Xv] = ⇡(v) deg(v,G) g nH .
",3. SybilRank’s Security Guarantee Under,[0],[0]
"1This assumption is not explicitly stated in Cao et al. (2012), but apparent from their derivation.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Using this result, the expected value of Z becomes E[Z]",3. SybilRank’s Security Guarantee Under,[0],[0]
"=P v2VH E[Yv] = g nH P v2VH ⇡(v) deg(v,G) , where the right hand side still contains a sum that needs to be evaluated individually for each node to compute its actual value.",3. SybilRank’s Security Guarantee Under,[0],[0]
"In order to “average out” this sum, we rely on the assumption that the honest nodes GH is power law-distributed (Barabási, 2009).",3. SybilRank’s Security Guarantee Under,[0],[0]
"To do this, a new random variable Dv is introduced which measures the degree of v. Then, the assumption results in the probability of a node having a degree of d being P (Dv = d) =",3. SybilRank’s Security Guarantee Under,[0],[0]
"d
⇣( ) , where ⇣ is the Riemann zeta function ⇣(s) := P1 n=0 n s (Barabási, 2009).
",3. SybilRank’s Security Guarantee Under,[0],[0]
"With this expression, it is possible to “average out” the exact topology of the graph by computing the expected value with respect to the newly defined random variable Dv:
E[Z] =",3. SybilRank’s Security Guarantee Under,[0],[0]
gnH P1 d=1 P v2VH ⇡(v) d P,3. SybilRank’s Security Guarantee Under,[0],[0]
(,3. SybilRank’s Security Guarantee Under,[0],[0]
"Dv = d)
= g nH P v2VH ⇡(v) P1 d=1 1 d d ⇣( )
= g nH P v2VH ⇡(v) ⇣( ) P1 d=1 d ( +1)
",3. SybilRank’s Security Guarantee Under,[0],[0]
= g nH ⇣( +1) ⇣,3. SybilRank’s Security Guarantee Under,[0],[0]
"( ) P v2VH ⇡(v).| {z }
Total trust in the honest graph
This yields the following lemma.",3. SybilRank’s Security Guarantee Under,[0],[0]
Lemma 2.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Under Assumption 1 the expected relative trust leak in each iteration of the random walk is given by
˜ CH = g
nH ⇣( +1) ⇣( )| {z } =:e
where e < 1 is a constant that depends on the parameter of the assumed power law distribution for the degree distribution.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Although Lemma 2 gives a different expected relative trust leak from Lemma 1, the fact that the maximum number of connection for each node is bounded in every OSN and therefore O(nH) = O(vol(VH)) leads to the same asymptotic behavior as Theorem 2: Theorem 2.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Under Assuption 1, the total number of Sybils that are ranked higher than non-Sybils by SybilRank is O(g log n).",3. SybilRank’s Security Guarantee Under,[0],[0]
"This result explicitly shows that, asymptotically, SybilRank’s security guarantee remains unchanged even under more realistic Assumption 1.",3. SybilRank’s Security Guarantee Under,[0],[0]
"In this section, we discuss adversarial strategies against graph-based Sybil detection methods.
",4. Adversarial Strategies,[0],[0]
"Attacker’s Action Although attackers in general can take variety of actions, we restricts their action to adding attacking edges.
",4. Adversarial Strategies,[0],[0]
Definition 2 (Attacking strategy).,4. Adversarial Strategies,[0],[0]
"Given an honest graph GH and a Sybil graph GS , an attacking strategy describes the set of attacking edges established by the intruder.
",4. Adversarial Strategies,[0],[0]
"The cost of action is measured by the number of attacking edges.
",4. Adversarial Strategies,[0],[0]
"Attacker’s Knowledge Generally, we focus on adversarial attacks against random walk based approaches.",4. Adversarial Strategies,[0],[0]
"That is, an attacker’s strategy for establishing edges from Sybil nodes to honest nodes in order to cloak an attacker’s Sybil sub-network.",4. Adversarial Strategies,[0],[0]
"For analysis, we assume different levels of knowledge that the attacker has on the defender’s strategy and information:
A.1 Strategy only.
",4. Adversarial Strategies,[0],[0]
"A.2 Strategy + topology.
",4. Adversarial Strategies,[0],[0]
"A.3 Strategy + topology + trusted nodes (positively labeled nodes).
",4. Adversarial Strategies,[0],[0]
"B.1 Strategy + topology + trusted nodes (positively labeled nodes) + untrusted nodes (negatively labeled nodes).
",4. Adversarial Strategies,[0],[0]
"Here, we divided the level of access to inside information for the attacker into two groups.",4. Adversarial Strategies,[0],[0]
"In group A (i.e., A.1, A.2, A.3) attackers are able to gather sophisticated information based on publicly available sources, whereas in group B (i.e., B.1) either some back-channel provides non-public information (e.g. defender marked Sybil nodes based on their analysis), or, the attackers are provided with all information visible to the defenders.
",4. Adversarial Strategies,[0],[0]
"Clearly, it is too hard, if not impossible, to have an out-ofthe-box solution for the setting described in group B and we therefore resort our analysis on the settings in group A.",4. Adversarial Strategies,[0],[0]
"In the first case (A.1), no efficient adversarial strategies for graph-based random walk approaches is possible.",4. Adversarial Strategies,[0],[0]
The attackers must build up sufficient attacking edges to trusted nodes in order to absorb enough trust.,4. Adversarial Strategies,[0],[0]
"In A.3 (and A.2 as a special case) on the other hand, the attacker gained enough information to guide the creation of attacking edges efficiently.",4. Adversarial Strategies,[0],[0]
This paper focuses on this most interesting situation.,4. Adversarial Strategies,[0],[0]
"More specifically, we assume the following: the intruder knows defender’s strategy (algorithm details), the topology of the honest graph, and the set of trusted nodes, i.e., she knows about GH = (VH , EH) and VT .",4. Adversarial Strategies,[0],[0]
Based on that knowledge the intruder can establish attacking edges to honest nodes of her choice with the goal to create an attacking scenario where the applied defense method fails.,4. Adversarial Strategies,[0],[0]
"Although the exact topology of the Sybil graph is not specified any further, for the following results it is assumed that it is designed in a way that suits the intruder well.
",4. Adversarial Strategies,[0],[0]
"Attacker’s Goal Attackers can have various final goals, e.g., spamming honest users to earn money, feeding wrong information to honest nodes, stealing nonpublic information, damaging countries/companies, etc.",4. Adversarial Strategies,[0],[0]
"Depending on the goal, the objective of the optimal strategy can differ.",4. Adversarial Strategies,[0],[0]
"We assume that attacker’s try to maximize their influence and hence, have an inherent need to increase the number of attacking edges.
",4. Adversarial Strategies,[0],[0]
"Random-walk based approaches such as SybilRank and Integro rely on the fact that the absolute trust leak l from the honest graph to the Sybil graph is small (i.e., below the amount needed to reach the stationary distribution within the Sybil sub-graph) which ensures low trust scores for the Sybil nodes.",4. Adversarial Strategies,[0],[0]
"However, if enough trust is being propagated to the Sybil graph, trust values will be close to the stationary distribution in the Sybil graph as well as in the honest graph.",4. Adversarial Strategies,[0],[0]
"Consequently, the degree-normalized ranking values will be similar to the ones in the honest graph, which makes Sybil nodes indistinguishable from honest nodes and therefore disables the detector.",4. Adversarial Strategies,[0],[0]
Definition 3 (Disabling Attacking Strategy).,4. Adversarial Strategies,[0],[0]
Let GH and GS be the honest graph and the Sybil graph.,4. Adversarial Strategies,[0],[0]
Let l : 2E !,4. Adversarial Strategies,[0],[0]
R be the absolute trust leak as a function of an attacking strategy.,4. Adversarial Strategies,[0],[0]
"Then, an attacking strategy EA ⇢ VH ⇥",4. Adversarial Strategies,[0],[0]
"VS is said to be disabling if
l(EA)",4. Adversarial Strategies,[0],[0]
"td, (5) where td is the disabling threshold, which depends on the topology of the Sybil graph and the detection algorithm.
",4. Adversarial Strategies,[0],[0]
"Surely, an attacker does not aim for just any disabling strategy but for the one that comes at the lowest cost.",4. Adversarial Strategies,[0],[0]
"As the cost of an attacking strategy is assumed to be increasing in the number of attacking edges, an optimal/minimal disabling strategy is given by the following definition.",4. Adversarial Strategies,[0],[0]
Definition 4 (Optimal Disabling Strategy).,4. Adversarial Strategies,[0],[0]
"An attacking strategy AE is said to be optimal if it is the solution to the following optimization problem:
min EA⇢VH⇥VS |EA| (6)
s. t. l(EA) td.
To solve this, the disabling threshold td and the trust leak function must be known to the attacker.",4. Adversarial Strategies,[0],[0]
"Ignoring the edge weights (which are unknown to the attacker) the amount of trust needed within the Sybil graph to reach the stationary distribution of the random walk is given by td =P
vi2VS ⇡i = vol(VS) vol(V ) .",4. Adversarial Strategies,[0],[0]
To exactly evaluate l(EA) the entire random walk needs to be simulated which is infeasible for the attacker without knowing its exact length and the edge weights.,4. Adversarial Strategies,[0],[0]
A useful estimate is to consider only the first iteration.,4. Adversarial Strategies,[0],[0]
"The computation of this value is feasible and the trust leak per attacking edge is by far the
largest in the first iteration because all the trust is concentrated in the relatively small subset of trusted nodes VT .",4. Adversarial Strategies,[0],[0]
"The trust leak in the first iteration ˜l(EA) is given by l(EA) = P v2VT (v) deg(v,GH)+(v)
, where (v) is the attacking degree (i.e., the number of attacking edges) of node v.",4. Adversarial Strategies,[0],[0]
This leads to a greedy strategy where the intruder iteratively adds those attacking edges which produce the largest increase in ˜l.,4. Adversarial Strategies,[0],[0]
In the following the term adversarial strategy/attacker refers to this greedy strategy.,4. Adversarial Strategies,[0],[0]
"In this section, we propose our new method and derive its efficient solver.",5. Proposed Method,[0],[0]
"Our method is specifically designed to cope with a large number of attacking edges by minimizing “trust leaks”, that is, minimizing a sampled trust leak by adjusting the edge weights—a missing mechanism for SybilRank and Integro.
",5. Proposed Method,[0],[0]
"Transductive Sybil Ranking Combining the approach of Backstrom & Leskovec (2011) and SybilRank (Cao et al., 2012), our proposed method, called transductive Sybil ranking (TSR), tries to leverage potential prior knowledge, negative labels, to bias a short random walk so that random walk methods work even with the existence of a large number of attacking edges.
",5. Proposed Method,[0],[0]
Assume that all nodes carry attributes and n  |V,5. Proposed Method,[0],[0]
"| nodes are additionally attached with label information, i.e., the defender knows a subset of nodes are honest, and another subset of nodes are sybil.",5. Proposed Method,[0],[0]
"More formally, the defender is given labeled nodes L := {(xi, yi) 2 X ⇥",5. Proposed Method,[0],[0]
"{+1, 1}}ni=1 and unlabeled nodes U := {xi 2 X}|V |i=n+1.",5. Proposed Method,[0],[0]
"Since only the honest nodes can be trusted, VT ✓ {vi 2 V ; yi = +1} holds.
",5. Proposed Method,[0],[0]
"We define an edge feature function u,v between nodes u and v as u,v : X ⇥ X !",5. Proposed Method,[0],[0]
Y .,5. Proposed Method,[0],[0]
"A corresponding parameterized, non-negative scoring function fw : Y !",5. Proposed Method,[0],[0]
"R+ is learned during training and applied as edge weight au,v = fw( u,v) in the weighted adjacency matrix Q 2 R|V |⇥|V",5. Proposed Method,[0],[0]
"|:
Qu,v =
( au,vP x au,x
if (u, v) 2 E, 0 otherwise.
(7)
",5. Proposed Method,[0],[0]
"Throughout our experiments, we restrict ourselves to the following differentiable edge feature function:
fw( u,v) =",5. Proposed Method,[0],[0]
"(1 exp( w> u,v))",5. Proposed Method,[0],[0]
1.,5. Proposed Method,[0],[0]
"(8) Once the transition matrix is fixed, The remaining procedure is the same as SybilRank.",5. Proposed Method,[0],[0]
"Namely, starting form the initial distribution (1), k-steps random walk (2) is applied with the transition matrix (7).",5. Proposed Method,[0],[0]
"After that, the degreenormalized ranking probability (4) is used for classification.",5. Proposed Method,[0],[0]
"However, we are also given negatively labeled nodes,
which are used to train the parameter w of the edge feature function (8), so that p(i) < p(j), 8 i, j 2 {1, . . .",5. Proposed Method,[0],[0]
", n} with yi = 1 and yj = +1.",5. Proposed Method,[0],[0]
"In the spirit of regularized risk minimization (Vapnik, 1999), this problem is formalized as follows:
Definition 5 (TSR optimization problem).",5. Proposed Method,[0],[0]
"TSR solves a quadratically regularized, non-convex optimization problem with generic loss-functions h :",5. Proposed Method,[0],[0]
"[0, 1]⇥{+1, 1} !",5. Proposed Method,[0],[0]
"R:
minimize w F (w) =
2
kwk2 + nX
i=1
h(p (i) (w), yi) .",5. Proposed Method,[0],[0]
"(9)
Using the notion of p(i)(w) visually indicates that node ranking probabilities p are (non-linearly) dependent on the parameter vector w.",5. Proposed Method,[0],[0]
"As for the choice of loss-functions, we examine the following:
• Wilcoxon-Mann-Whitney (WMW) loss (Yan et al., 2003).",5. Proposed Method,[0],[0]
"WMW maximizes the area under the ROC curve:
h(p, y) =
nX
j=1
1[y = +1^yj = 1] ⇣ 1 + exp p pjb ⌘ 1 .
",5. Proposed Method,[0],[0]
"• Smooth hinge-loss variant A smooth variant of the classical support vector machine hinge-loss with two additional parameters: a decision boundary b 2 R and a scaling parameter a 2 R:
h(p, y) =
8 ><
>: 1 2 y(ap b) if y(ap b)  0, 1 2 (1 y(ap b))2 if 0 < y(ap b)  1, 0 if 1 < y(ap b).
",5. Proposed Method,[0],[0]
"In this work, we focus on smooth, differentiable lossfunctions only, ensuring fast convergence to local optima via gradient-based methods, i.e., fast second-order methods (BFGS).",5. Proposed Method,[0],[0]
"A pivotal point is hence, to assess the gradient w.r.t.",5. Proposed Method,[0],[0]
"w.
Gradient Computation The remaining of this section is dedicated to the derivation of the gradient:
",5. Proposed Method,[0],[0]
"@F (w) @w = @ kwk2 @2w + Pn i @h(p(i)(w),yi) @w ,
where the loss-function h can be further split into @h(p(i)(w),yi)
@w = @h(p(i)(w),yi) @p(i)(w) @p(i)(w) @w .",5. Proposed Method,[0],[0]
"Since we con-
strained ourselves to differentiable loss-function h(p, y), the partial derivative w.r.t.",5. Proposed Method,[0],[0]
p can be calculated rather straightforward.,5. Proposed Method,[0],[0]
"More complicated is the evaluation of
@p(i)
",5. Proposed Method,[0],[0]
@w = @ @w p(i)k,5. Proposed Method,[0],[0]
⇡(i) =,5. Proposed Method,[0],[0]
✓ @p(i)k @w ⇡ (i) p(i)k @⇡ (i) @w ◆ ⇡ (i) 2 .,5. Proposed Method,[0],[0]
"(10)
The derivative of the i-th component of ⇡ is given by:
@⇡(i)
@w =
⇣ @deg⇤(vi) @w vol(V ) @vol(V )@w deg⇤(vi) ⌘ vol(V ) 2,
where @deg ⇤(vi) @w = P e2E vi2e @ae @w = P e2E vi2e @fw( e) @w and @vol(V )",5. Proposed Method,[0],[0]
@w = 2 P e2E @ae @w = 2 P e2E @fw( e) @w .,5. Proposed Method,[0],[0]
As fw is said to be differentiable the only part of Eq.,5. Proposed Method,[0],[0]
"(10) that remains is the Jacobian @pk/@w.
Theorem 3.",5. Proposed Method,[0],[0]
"The derivative @pk/@w for k 1 is given by:
@pk",5. Proposed Method,[0],[0]
@w = ✓ k 1P l=0 plQ k 1 l ◆ @Q @w .,5. Proposed Method,[0],[0]
"(11)
(the proof is given in Appendix A).",5. Proposed Method,[0],[0]
"The derivative of Q, defined in Eq.",5. Proposed Method,[0],[0]
"(7), is given by
@Quv @w =
8 <
:
@auv",5. Proposed Method,[0],[0]
@w P x aux auv P x @aux,5. Proposed Method,[0],[0]
"@w
( P x aux) 2",5. Proposed Method,[0],[0]
"if (u, v) 2 E, 0 otherwise.
",5. Proposed Method,[0],[0]
"This completes the computation of the gradient and enables the application of gradient-based methods, i.e., BFGS to find a (locally) optimal estimate ŵ.",5. Proposed Method,[0],[0]
"By using this estimate, TSR weights the whole graph, with which a short random walk is performed to obtain the final ranking p.
Robustness of TSR against Attacks By using the negative label information, our TSR, in principle, monitors “trust leak” through random walk, and adjusts the edge weights so that the leak is minimized.",5. Proposed Method,[0],[0]
"As a result, the weights tend to be lower on the attacking edges (to reduce the propagation), and higher on the Sybil edges (to boost the stationary distribution).",5. Proposed Method,[0],[0]
"Thus, we can expect that our TSR, which is an advanced integrated method, is more robust against attacks than the SybilRank and the two-step Integro approach.",5. Proposed Method,[0],[0]
"To assess the robustness of the proposed method and the baseline methods, we generate artificially network topology and edge and node attributes in order to have full control of the underlying ground truth.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"We separately create two graphs, the honest and the Sybil graph.",6. Empirical Evaluation on Synthetic Data,[0],[0]
Both use the generation method proposed by Holme & Kim (2002) for scale free networks.,6. Empirical Evaluation on Synthetic Data,[0],[0]
Node features are generated randomly and correlated through dependency injection.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"The edge features function u,v simply stacks node features of the two adjacent nodes xu",6. Empirical Evaluation on Synthetic Data,[0],[0]
and,6. Empirical Evaluation on Synthetic Data,[0],[0]
xv (see Appendix B for more details).,6. Empirical Evaluation on Synthetic Data,[0],[0]
"Connections between Sybil and honest graphs are established according to a random attacking strategy that iteratively adds attacking edges randomly, i.e., equally distributed on the set of all possible attacking edges VH ⇥",6. Empirical Evaluation on Synthetic Data,[0],[0]
"VS
or a adversarial attacking strategy that solves Problem (6) for optimal attacks.",6. Empirical Evaluation on Synthetic Data,[0],[0]
This strategy only chooses an honest node to be attacked next and the corresponding Sybil node is chosen randomly (equally distributed on the set of all Sybil nodes VS).,6. Empirical Evaluation on Synthetic Data,[0],[0]
"We test our method, TSR, using the proposed loss functions and compare against the stateof-the-art methods SybilRank and Integro.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"As Integro depends on a preceding victim prediction, we simulated one that achieves highest possible rankings (ROC-AUC close to 1.0).2
Random Attacking Strategy We generated a sample network (|VH | = 200 and |VS | = 30) and select 15 honest nodes and 8 Sybil nodes randomly, which will be used as labeled examples for our TSR.",6. Empirical Evaluation on Synthetic Data,[0],[0]
The labeled honest nodes are also used as the set VT of trusted seeding nodes for the random walks in all methods.,6. Empirical Evaluation on Synthetic Data,[0],[0]
We evaluate the performance in terms of ROC-AUC-values for the computed ranking.,6. Empirical Evaluation on Synthetic Data,[0],[0]
This procedure was repeated 20 times for varying number of attacking edges (10-200 edges).,6. Empirical Evaluation on Synthetic Data,[0],[0]
Figure 1 shows ROC-AUC curves for all methods under the random attacking setting.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"We can obsreve that our TSR, regardless of the choice of loss function, performs superior to the other methods.",6. Empirical Evaluation on Synthetic Data,[0],[0]
Integro’s accuracy deteriorates fast but still has an edge over SybilRank up to the point where the ROCAUC-value reaches 0.5.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"After that SybilRank and Integro essentially perform similar.
",6. Empirical Evaluation on Synthetic Data,[0],[0]
"Adversarial Attacking Strategy For the adversarial setting, we ran the same benchmarks but this time attacking edges were added according to the adversarial attacking strategy.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"Due to the much more aggressive setting, we varied the number of attacking edges from 1-40 and repeated this procedure 20 times to report averaged ROCAUC accuracies.",6. Empirical Evaluation on Synthetic Data,[0],[0]
The results are depicted in Figure 2.,6. Empirical Evaluation on Synthetic Data,[0],[0]
All choices of loss functions outperform SybilRank and Integro clearly.,6. Empirical Evaluation on Synthetic Data,[0],[0]
The results confirm our considerations that SybilRank’s performance drops fast and steep as soon as a certain amount of attacking edges is established.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"Integro behaves more robust than SybilRank, but, ultimately, must resign after a few more attacking edges.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"Again, our TSR is significantly more robust against adversarial attacks and can withstand higher number of attacking edges until its performance finally deteriorates.",6. Empirical Evaluation on Synthetic Data,[0],[0]
We also evaluated our method on a sample of the Facebook graph Leskovec & Mcauley collected from survey participants using the Facebook app.,7. Empirical Evaluation on Real-world Data,[0],[0]
"The dataset includes the topology (|V | = 4039 users and |E| = 88234 friend-
2SybiRank, Integro, and TSR rely on different information, and therefore, the fairness of comparison is not trivial.",7. Empirical Evaluation on Real-world Data,[0],[0]
"We discuss this issue in Appendix C.
ships) as well as node features for every node (see Table 1 for summary), Figure 3).",7. Empirical Evaluation on Real-world Data,[0],[0]
"Node features are comprised of obfuscated categorical features of users profiles including education, work, hometown, language, last name, etc.",7. Empirical Evaluation on Real-world Data,[0],[0]
"As with most of real world social graphs, the data exhibits strong multi-cluster structure, as seen in Figure 3 and Figure 4.",7. Empirical Evaluation on Real-world Data,[0],[0]
"These clusters pose additional challenges to the application of random walk-based methods as the trust propagation between two loosely inter-connected clusters is low (Cao et al., 2012; Boshmaf et al., 2016).",7. Empirical Evaluation on Real-world Data,[0],[0]
"Hence, trust seeds should be distributed among all clusters.",7. Empirical Evaluation on Real-world Data,[0],[0]
"Following SybilRank and Integro (Cao et al., 2012), we employ the Louvian clustering method (Blondel et al., 2008) first.
",7. Empirical Evaluation on Real-world Data,[0],[0]
"As common, the Sybil graph needs to be generated.",7. Empirical Evaluation on Real-world Data,[0],[0]
"For this purpose, a (small) subgraph was copied and declared as Sybil.",7. Empirical Evaluation on Real-world Data,[0],[0]
The attacking edges were created to link the honest and the Sybil graph following one of the attacking strategies (random or adversarial).,7. Empirical Evaluation on Real-world Data,[0],[0]
It was made sure that no Sybil node attacked one of the direct neighbors of its origin which is reasonable for most social graphs.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Edge features
for TSR are as follows: the number of shared features (in total), the number of shared friends, and the number of shared features within specific categories.",7. Empirical Evaluation on Real-world Data,[0],[0]
"The other experimental setup is the same as the previous section.
",7. Empirical Evaluation on Real-world Data,[0],[0]
Random Attacks,7. Empirical Evaluation on Real-world Data,[0],[0]
The trusted nodes |VT,7. Empirical Evaluation on Real-world Data,[0],[0]
| = 50 were randomly distributed among all clusters and a small subset of Sybils |VD| = 30 was chosen as known Sybil nodes.,7. Empirical Evaluation on Real-world Data,[0],[0]
Attacking edges EA were established following the random attacking strategy ranging from |EA| = 1 to |EA| = 1400.,7. Empirical Evaluation on Real-world Data,[0],[0]
Experiments were repeated 10 times.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Integro was run with
sarial attacking scenario on the Facebook graph.
",7. Empirical Evaluation on Real-world Data,[0],[0]
"two levels of accuracy in simulated victim detection, i.e., perfect (AUROC = 1) and almost perfect (AUROC = 0.9).",7. Empirical Evaluation on Real-world Data,[0],[0]
Figure 5 shows the AUROC-values.,7. Empirical Evaluation on Real-world Data,[0],[0]
The detection performance of SybilRank is the lowest and drops soon as attacking edges increase.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Integro with the perfect victim detection outperforms the other methods, but with just a slight reduction in the victim detection accuracy (AUROC = 0.9), its performance drops significantly.",7. Empirical Evaluation on Real-world Data,[0],[0]
All versions of TSR perform almost on par with perfect version of Integro in the lower range of attacking edges (1—800).,7. Empirical Evaluation on Real-world Data,[0],[0]
"In the higher range (800—1400), the hinge loss drop fast to end up with a performance similar to Integro with the almost perfect victim detection.",7. Empirical Evaluation on Real-world Data,[0],[0]
"However, the variant that uses the WMW-loss does not show this performance drop and stays close to the upper-bound of Integro.
",7. Empirical Evaluation on Real-world Data,[0],[0]
Adversarial Attacks The number of adversarial attack edges ranged from |EA| = 1 to |EA| = 45.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Figure 6 shows
the recorded average AUROC-values.",7. Empirical Evaluation on Real-world Data,[0],[0]
"Again, SybilRank’s performance drops the fastest and steepest and Integro is insignificantly better in this adversarial scenario.",7. Empirical Evaluation on Real-world Data,[0],[0]
Both variants of TSR performs better than the baselines.,7. Empirical Evaluation on Real-world Data,[0],[0]
"However, the WMW-loss variant performs only slightly better than SybilRank and Integro, while the hinge-loss variant keeps good performance even for a large number of attacking edges.",7. Empirical Evaluation on Real-world Data,[0],[0]
"As our future work, we will investigate which loss function should be chosen, depending on data and assumed attacker’s strategy.",7. Empirical Evaluation on Real-world Data,[0],[0]
"Overall, whereas SybilRank’s and Integro’s performance drops to an average AUROC-value below 0.5 at |EA| = 30, the hinge-loss variant of TSR still achieves an average value over 0.9 at the same amount of attacking edges.",7. Empirical Evaluation on Real-world Data,[0],[0]
"In this paper, we studied the problem of Sybil detection.",8. Conclusion & Outlook,[0],[0]
We first refined the security guarantees of random walk approaches towards more realistic assumptions.,8. Conclusion & Outlook,[0],[0]
"Then, we formalized and coined the adversarial setting and introduced optimal strategies for attackers.",8. Conclusion & Outlook,[0],[0]
"Further, we proposed a new method, transductive Sybil ranking (TSR), that leverages prior information, network topology as well as node and edge features.",8. Conclusion & Outlook,[0],[0]
"Unlike Integro, it is fused in a single optimization framework and can be solved efficiently by using gradient-based optimizer.",8. Conclusion & Outlook,[0],[0]
"In our empirical evaluation, we showed the advantages of our method and investigated the susceptibility of our method and baseline competitors to adversarial attacks.",8. Conclusion & Outlook,[0],[0]
"Further research will focus on the application of our method to real-world, large-scale OSNs.",8. Conclusion & Outlook,[0],[0]
"JH was supported by MathPlan GmbH and innoCampus, TU-Berlin.",9. Acknowledgments,[0],[0]
"SN, AB and KRM were supported by the German Ministry for Education and Research as Berlin Big Data Center BBDC, funding mark 01IS14013A. KRM thanks for the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (No.2017-0-00451).",9. Acknowledgments,[0],[0]
NG was supported by BMBF ALICE II grant 01IB15001B.,9. Acknowledgments,[0],[0]
Sybil detection is a crucial task to protect online social networks (OSNs) against intruders who try to manipulate automatic services provided by OSNs to their customers.,abstractText,[0],[0]
"In this paper, we first discuss the robustness of graph-based Sybil detectors SybilRank and Integro and refine theoretically their security guarantees towards more realistic assumptions.",abstractText,[0],[0]
"After that, we formally introduce adversarial settings for the graph-based Sybil detection problem and derive a corresponding optimal attacking strategy by exploitation of trust leaks.",abstractText,[0],[0]
"Based on our analysis, we propose transductive Sybil ranking (TSR), a robust extension to SybilRank and Integro that directly minimizes trust leaks.",abstractText,[0],[0]
Our empirical evaluation shows significant advantages of TSR over stateof-the-art competitors on a variety of attacking scenarios on artificially generated data and realworld datasets.,abstractText,[0],[0]
Minimizing Trust Leaks for Robust Sybil Detection,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1379–1388, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually. Data-driven methods have been proposed to mine formulas from KBs automatically, where random sampling and approximate calculation are common techniques to handle big data. Among a series of methods, Random Walk is believed to be suitable for knowledge graph data. However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference. Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas. To this end, we propose a novel goaldirected inference formula mining algorithm, which directs random walks by the specific inference target at each step. The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously. The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task.",text,[0],[0]
"Recently, various knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), WordNet (Miller, 1995), Yago (Hoffart et al., 2013), have been built, and researchers begin to explore how to make use of structural information to promote performances of several inference-based NLP applications, such as
text entailment, knowledge base completion, question and answering.",1 Introduction,[0],[0]
"Creating useful formulas is one of the most important steps in inference, and an accurate and high coverage formula set will bring a great promotion for an inference system.",1 Introduction,[0],[0]
"For example, Nationality(x, y) ∧ Nationality(z, y) ∧ Language(z, w)⇒ Language(x, w) is a high-quality formula, which means people with the same nationality probably speak the same language.",1 Introduction,[0],[0]
"However, it is a challenge to create formulas for open-domain KBs, where there are a great variety of relation types and it is impossible to construct a complete formula set by hand.
",1 Introduction,[0],[0]
"Several data-driven methods, such as Inductive Logic Programming (ILP) (Muggleton and De Raedt, 1994) and Markov Logic Network (MLN) (Richardson and Domingos, 2006), have been proposed to mine formulas automatically from KB data, which transform frequent sub-structures of KBs, e.g., paths or loops, into formulas.",1 Introduction,[0],[0]
"Figure 1.a shows a sub-graph extracted from Freebase, and the formula mentioned above about Language can be generated from the loop in Figure 1.d.",1 Introduction,[0],[0]
"However, the running time of these traditional probabilistic inference methods is unbearable over large-scale KBs.",1 Introduction,[0],[0]
"For example, MLN needs grounding for each candidate formula, i.e., it needs to enumerate all paths.",1 Introduction,[0],[0]
"Therefore, the computation complexity of MLN increases exponentially with the scale of a KB.
",1 Introduction,[0],[0]
"In order to handle large-scale KBs, the random walk is usually employed to replace enumerating all possible sub-structures.",1 Introduction,[0],[0]
"However, random walk is inefficient to find useful structures due to its completely randomized mechanism.",1 Introduction,[0],[0]
"For example in Fig-
1379
ure 1.b, the target path (yellow one) has a small probability to be visited, the reason is that the algorithm may select all the neighboring entity to transfer with an equal probability.",1 Introduction,[0],[0]
"This phenomenon is very common in KBs, e.g., each entity in Freebase has more than 30 neighbors in average, so there will be about 810,000 paths with length 4, and only several are useful.",1 Introduction,[0],[0]
"There have been two types of methods proposed to improve the efficiency of random walks, but they still meet serious problems, respectively.",1 Introduction,[0],[0]
1),1 Introduction,[0],[0]
Increasing rounds of random walks.,1 Introduction,[0],[0]
"More rounds of random walks will find more structures, but it will simultaneously introduce more noise and thus generate more false formulas.",1 Introduction,[0],[0]
"For example, the loop in Figure 1.c exists in Freebase, but it produces a false formula, Gender(x, y) ∧ Gender(z, y) ∧ Language(z, w)⇒ Language(x, w), which means people with the same gender speak the same language.",1 Introduction,[0],[0]
"This kind of structures frequently occur in KBs even the formulas are mined with a high confidence, because there are a lot of sparse structures in KBs which will lead to inaccurate confidence.",1 Introduction,[0],[0]
"According to our statistics, more than 90 percent of high-confidence formulas produced by random walk are noise.",1 Introduction,[0],[0]
2),1 Introduction,[0],[0]
Employing heuristic rules to direct random walks.,1 Introduction,[0],[0]
"This method directs random walks to find useful structures by rewriting the state transition probability matrix, but the artificial heuristic rules may only apply to a little part of formulas.",1 Introduction,[0],[0]
"For example, PRA (Lao and Cohen, 2010; Lao et al., 2011) assumes the more narrow distributions of elements in a formula are, the higher score the formula will obtain.",1 Introduction,[0],[0]
"However, formulas with high scores in PRA are not always true.",1 Introduction,[0],[0]
"For example, the formula in Figure 1.c has a high score in PRA, but it is not true.",1 Introduction,[0],[0]
"Oppositely, formulas with low scores in PRA are not always useless.",1 Introduction,[0],[0]
"For example, the formula, Father(x, y) ∧ Father(y, z) ⇒ Grandfather(x, t), has a low score when x and y both have several sons, but it obviously is the most effective to infer Grandfather.",1 Introduction,[0],[0]
"According to our investigations, the situations are common in KBs.",1 Introduction,[0],[0]
"In this paper, we propose a Goal-directed Random Walk algorithm to resolve the above problems.",1 Introduction,[0],[0]
The algorithm employs the specific inference target as the direction at each step in the random walk process.,1 Introduction,[0],[0]
"In more detail, to achieve such a goaldirected mechanism, at each step of random walk, the algorithm dynamically estimates the potentials for each neighbor by using the ultimate goal, and assigns higher probabilities to the neighbors with higher potentials.",1 Introduction,[0],[0]
"Therefore, the algorithm is more inclined to visit structures which are beneficial to infer
the target and avoid transferring to noise structures.",1 Introduction,[0],[0]
"For example in Figure 1, when the inference target is what language a person speaks, the algorithm is more inclined to walk along Nationality edge than Gender, because Nationality has greater potential than Gender to infer Language.",1 Introduction,[0],[0]
We build a real potential function based on low-rank distributional representations.,1 Introduction,[0],[0]
The reason of replacing symbols by distributional representations is that the distributional representations have less parameters and latent semantic relationship in them can contribute to estimate potentials more precisely.,1 Introduction,[0],[0]
"In summary, the contributions of this paper are as follows.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Compared with the basic random walk, our approach direct random walks by the inference target, which increases efficiency of mining useful formulas and has a great capability of resisting noise.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Compared with the heuristic methods, our approach can learn the strategy of random walk automatically and dynamically adjust the strategy for different inference targets, while the heuristic methods need to write heuristic rules by hand and follow the same rule all the time.",1 Introduction,[0],[0]
"• The experiments on link prediction task prove that our approach has a high efficiency on mining formulas and has a good performance on both WN18 and FB15K datasets.
",1 Introduction,[0],[0]
"The rest of this paper is structured as follows, Section 2 introduces the basic random walk for mining formulas.",1 Introduction,[0],[0]
Section 3 describes our approach in detail.,1 Introduction,[0],[0]
The experimental results and related discussions are shown in Section 4.,1 Introduction,[0],[0]
"Section 5 introduces some related works, and finally, Section 6 concludes this paper.",1 Introduction,[0],[0]
"Mining frequent patterns from source data is a problem that has a long history, and for different specific tasks, there are different types of source data and different definitions of pattern.",2.1 Frequent Pattern Mining,[0],[0]
"Mining formulas is more like frequent subgraph mining, which employs paths or loops as frequent patterns and mines them from a KB.",2.1 Frequent Pattern Mining,[0],[0]
"For each relation type R, the algorithm enumerates paths from entity H to entity T for each triplet R(H,T ).",2.1 Frequent Pattern Mining,[0],[0]
These paths are normalized to formulas by replacing entities to variables.,2.1 Frequent Pattern Mining,[0],[0]
"For example, the loop in Figure 1.d, National-
ity(Bob, America)",2.1 Frequent Pattern Mining,[0],[0]
"∧ Nationality(Stewart, America)",2.1 Frequent Pattern Mining,[0],[0]
"∧ Language(Bob, English) ⇒",2.1 Frequent Pattern Mining,[0],[0]
"Language(Stewart, English), can be normalized to the formula, Nationality(x, y) ∧ Nationality(z, y) ∧ Language(z, w) ⇒",2.1 Frequent Pattern Mining,[0],[0]
"Language(x, w).",2.1 Frequent Pattern Mining,[0],[0]
"Support and confidence are employed to estimate a formula, where the support value of a formula f : X ⇒ Y , noted as Sf , is defined as the proportion of paths in the KB which contains the body X , and the confidence value of X ⇒ Y , noted as Cf , is defined as the proportion of the paths that contains X which also meets X ⇒ Y .",2.1 Frequent Pattern Mining,[0],[0]
"Cf is calculated as follows,
Cf = Nf NX
(1)
",2.1 Frequent Pattern Mining,[0],[0]
whereNf is the total number of instantiated formula f and NX is the total number of instantiated X .,2.1 Frequent Pattern Mining,[0],[0]
Enumerating paths is a time consuming process and does not apply to large-scale KBs.,2.2 Random Walk on Knowledge Graph,[0],[0]
"Therefore, random walk on the graph is proposed to collect frequent paths instead of enumerating.",2.2 Random Walk on Knowledge Graph,[0],[0]
Random walk randomly chooses a neighbor to jump unlike enumerating which needs to search all neighbors.,2.2 Random Walk on Knowledge Graph,[0],[0]
"To estimate a formula f , the algorithm employs f ’s occurrence number during random walks N
′ f to approxi-
mate the total number Nf in Equation (1), and similarly employs N
′ X to approximate NX .",2.2 Random Walk on Knowledge Graph,[0],[0]
"Therefore,
f ’s confidence Cf can be approximatively estimated by N
′ f and N ′ X , noted as C ′ f .
",2.2 Random Walk on Knowledge Graph,[0],[0]
"Random walk maintains a state transition probability matrix P , and Pij means the probability of jumping from entity i to entity j. To make the confidence C
′",2.2 Random Walk on Knowledge Graph,[0],[0]
"f as close to the true confidence Cf as pos-
sible, the algorithm sets P as follows,
Pij = { 1/di, j ∈ Adj(i) 0, j /∈ Adj(i)",2.2 Random Walk on Knowledge Graph,[0],[0]
"(2)
where di is the out-degree of the entity i, Adj(i) is the set of adjacent entities of i, and ∑N j=1 Pij = 1.",2.2 Random Walk on Knowledge Graph,[0],[0]
Such a transition matrix means the algorithm may jump to all the neighboring entities with an equal probability.,2.2 Random Walk on Knowledge Graph,[0],[0]
"Such a random walk is independent from the inference target, so we call this type of random walk as a goalless random walk.",2.2 Random Walk on Knowledge Graph,[0],[0]
The goalless mechanism causes the inefficiency of mining useful structures.,2.2 Random Walk on Knowledge Graph,[0],[0]
"When we want to mine paths for R(H,T ), the algorithm cannot arrive at T from H
in the majority of rounds.",2.2 Random Walk on Knowledge Graph,[0],[0]
"Even though the algorithm recalls several paths for R(H,T ), some of them may generate noisy formulas for inferring R(H,T ).
",2.2 Random Walk on Knowledge Graph,[0],[0]
"To solve the above problem, several methods direct random walks by statically modifying P .",2.2 Random Walk on Knowledge Graph,[0],[0]
"For example, PRA sets Prij = P (j|i;r) |Ri| , P (j|i; r) = r(i,j) r(i,∗) , where P (j|i; r) is the probability of reaching node j from node i under the specific relation r, r(i, ∗) is the number of edges from i under r, and Ri is the number of relation types from i. Such a transition matrix implies the more narrow distributions of elements in a formula are, the higher score the formula will obtain, which can be viewed as the heuristic rule of PRA.",2.2 Random Walk on Knowledge Graph,[0],[0]
"We propose to use the inference target, ρ = R(H,T ), to direct random walks.",3.1 Goal-Directed Random Walk,[0],[0]
"When predicting ρ, our approach always directs random walks to find useful structures which may generate formulas to infer ρ.",3.1 Goal-Directed Random Walk,[0],[0]
"For different ρ, random walks are directed by modifying the transition matrix P in different ways.",3.1 Goal-Directed Random Walk,[0],[0]
"Our approach dynamically calculates Prij when jumping from entity i to entity j under relation r as follows,
",3.1 Goal-Directed Random Walk,[0],[0]
"Prij =    Φ(r(i, j), ρ)∑ k∈Adj(i) Φ(r(i, k), ρ) , j ∈ Adj(i)
0, j /∈ Adj(i)",3.1 Goal-Directed Random Walk,[0],[0]
"(3)
where Φ(r(i, j), ρ) is the r(i, j)’s potential which measures the potential contribution to infer ρ after walking to j.
Intuitively, if r(i, j) exits in a path from H to T and this path can generate a benefic formula to infer R(H,T ), the probability of jumping from i to j should larger and thus Φ(r(i, j), ρ) also should be larger.",3.1 Goal-Directed Random Walk,[0],[0]
"Reversely, if we cannot arrive at T within the maximal steps after jumping to j, or if the path produces a noisy formula leading to a wrong inference, Pij and Φ(r(i, j), ρ) should both be smaller.
",3.1 Goal-Directed Random Walk,[0],[0]
"To explicitly build a bridge between the potential Φ and the inference goal ρ, we maximize the likelihood of paths which can infer ρ.",3.1 Goal-Directed Random Walk,[0],[0]
"First, we recursively define the likelihood of a path from H to t
as PpHt = PpHs ·",3.1 Goal-Directed Random Walk,[0],[0]
"Prst , where Prst is defined in Equation (3).",3.1 Goal-Directed Random Walk,[0],[0]
"We then classify a path pHt into three separate categories: a) t = T and pHt can produce a benefic formula to infer R(H,T ); b) t 6=",3.1 Goal-Directed Random Walk,[0],[0]
T ; c) t = T but pHt may generate a noisy formula which misleads inference.,3.1 Goal-Directed Random Walk,[0],[0]
"Finally, we define the likelihood function as follows,
maxPP = ∏
pHt∈P P apHt(1− PpHt) b+c (4)
where P is all paths found in the process of performing random walks for R(H,T ), and t may be equal to T or not.",3.1 Goal-Directed Random Walk,[0],[0]
"a, b, c are three 0-1 variables corresponding to the above categories a), b), c).",3.1 Goal-Directed Random Walk,[0],[0]
"Only one in a, b, c can be 1 when PHt belongs to the corresponding category.",3.1 Goal-Directed Random Walk,[0],[0]
We then transform maximizing PP to minimizing Lrw = − logPP and employ SGD to train it.,3.1 Goal-Directed Random Walk,[0],[0]
"In practice, there is not a clear-cut boundary between a) and c), so we divide the loss into two parts:",3.1 Goal-Directed Random Walk,[0],[0]
Lrw = Ltrw + λL inf rw .,3.1 Goal-Directed Random Walk,[0],[0]
Ltrw is the loss of that t 6=,3.1 Goal-Directed Random Walk,[0],[0]
"T , and Linfrw is the loss of that pHT generates a noisy formula leading to a wrong inference.",3.1 Goal-Directed Random Walk,[0],[0]
λ is a super parameter to balance the two losses.,3.1 Goal-Directed Random Walk,[0],[0]
Ltrw and Linfrw have the same expression but are optimized in different stages.,3.1 Goal-Directed Random Walk,[0],[0]
"Ltrw can be optimized during random walks, while Linfrw should be optimized in the inference stage.",3.1 Goal-Directed Random Walk,[0],[0]
"We rewrite Lrw for a specific path p as follows,
Lrw(p) = −y logPp − (1− y) log (1− Pp) (5)
where y is the label of the path p and y = 1 if p is beneficial to infer ρ.",3.1 Goal-Directed Random Walk,[0],[0]
"To obtain the best Φ, we compute gradients of Lrw as follows,
∇Lrw(p) =",3.1 Goal-Directed Random Walk,[0],[0]
"(∇Lrw(r12),∇Lrw(r23), ...)
∇Lrw(rij)",3.1 Goal-Directed Random Walk,[0],[0]
"= ( ∂Lrw(rij) ∂Φrij , ∂Lrw(rij) ∂Φrik1 , ∂Lrw(rij) ∂Φrik2 , ...)
∂Lrw(rij)
",3.1 Goal-Directed Random Walk,[0],[0]
∂Φrij =,3.1 Goal-Directed Random Walk,[0],[0]
(Pp − y) · (1− Prij ),3.1 Goal-Directed Random Walk,[0],[0]
"Φrij · (1− Pp)
∂Lrw(rij) ∂Φrik",3.1 Goal-Directed Random Walk,[0],[0]
=,3.1 Goal-Directed Random Walk,[0],[0]
− (Pp − y) ·,3.1 Goal-Directed Random Walk,[0],[0]
"Prij
Φrij · (1− Pp) (6)
where ∇Lrw(rij) is the component of ∇Lrw(p) at rij .",3.1 Goal-Directed Random Walk,[0],[0]
"Φ(r(i,",3.1 Goal-Directed Random Walk,[0],[0]
"j), ρ) and Φ(r(i, k), ρ) are the potentials for all triplets r(i, j) ∈ p and r(i, k) /∈",3.1 Goal-Directed Random Walk,[0],[0]
"p, and rij is short for r(i, j).",3.1 Goal-Directed Random Walk,[0],[0]
"After iteratively updating Φrij and Φrik by the gradient of L t rw, the random walks can
be directed to find more paths fromH to T , and consequently it increases efficiency of the random walk.",3.1 Goal-Directed Random Walk,[0],[0]
"After updating Φrij and Φrik by the gradient ofL inf rw , random walk is more likely to find high-quality paths but not noise.",3.1 Goal-Directed Random Walk,[0],[0]
"Therefore, the goal-directed random walk increases efficiency of mining benefic formulas and has a great capability of resisting noise.",3.1 Goal-Directed Random Walk,[0],[0]
"The potential Φ(r(i, j), ρ) measures an implicit relationship between two triplets in the KB, so the total number of parameters is the square of the KB size.",3.2 Distributional Potential Function,[0],[0]
It is hard to precisely estimate all Φ because of the sparsity of training data.,3.2 Distributional Potential Function,[0],[0]
"To reduce the number of parameters, we represent each entity or relation in the KB as a low-rank numeric vector which is called embeddings (Bordes et al., 2013), and then we build a potential function Ψ on embeddings as Φ(r(i, j), ρ) = Ψ(Er(i,j), ER(H,T )), where Er(i,j) and ER(H,T ) are the embeddings of triplets.",3.2 Distributional Potential Function,[0],[0]
"In practice, we set Er(i,j) =",3.2 Distributional Potential Function,[0],[0]
"[Er, Ej ] and ER(H,T ) =",3.2 Distributional Potential Function,[0],[0]
"[ER, ET ] because Ei is the same for all triplets r(i, ∗), where [] is a concatenation operator.
",3.2 Distributional Potential Function,[0],[0]
"In the view of the neural network, our goaldirected mechanism is analogous to the attention mechanism.",3.2 Distributional Potential Function,[0],[0]
"At each step, the algorithm estimates attentions for each neighboring edges by Ψ. Therefore, there are several existing expressions of Ψ, e.g., the dot product (Sukhbaatar et al., 2015) and the single-layer perceptron (Bahdanau et al., 2015).",3.2 Distributional Potential Function,[0],[0]
"We will not compare different forms of Ψ, the detail comparison has been presented in the work (Luong et al., 2015).",3.2 Distributional Potential Function,[0],[0]
"We directly employ the simplest dot product for Ψ as follows,
Ψ(Er(i,j), ER(H,T ))",3.2 Distributional Potential Function,[0],[0]
"= σ(Er(i,j) · ER(H,T )) (7)
where σ is a nonlinear function and we set it as an exponential function.",3.2 Distributional Potential Function,[0],[0]
Ψ has no parameters beside KB embeddings which are updated during the training period.,3.2 Distributional Potential Function,[0],[0]
"To handle the dependence between goal-directed random walk and subsequent inference, we combine them into an integrated model and optimize them together.",3.3 Integrated Inference Model,[0],[0]
"To predict ρ = R(H,T ), the integrated model first collects formulas for R(H,T ), and then
Algorithm 1:",3.3 Integrated Inference Model,[0],[0]
"Train Integrated Inference Model
Input: KB, Ξ Output: Ψ, W , F 1: For ρ = R(H,T ) ∈ Ξ 2: Repeat ρ-directed Random Walk from H to t 3: Update Ψ by Ltrw 4: If t = T , then F = F ∩ fp 5: Calculate Linf and L inf rw by ρ 6: Update W by Linf 7: Update Ψ by Linfrw 8: Remove f ∈ F with little wf 9: Output Ψ, W , F
merges estimations of different formulas as features into a score function χ,
χ(ρ) =",3.3 Integrated Inference Model,[0],[0]
"∑
f∈Fρ δ(f) (8)
where Fρ is the formula set obtained by random walks for ρ, and δ(f) is an estimation of formula f .",3.3 Integrated Inference Model,[0],[0]
"The original frequent pattern mining algorithm employs formulas’ confidence as δ(f) directly, but it does not produce good results (Galárraga et al., 2013).",3.3 Integrated Inference Model,[0],[0]
"There are two ways to solve the problem: one is selecting another more suitable measure of f as δ(f) (Tan et al., 2002); the other is attaching a weight to each formula and learning weights with supervision, e.g., MLN (Richardson and Domingos, 2006) .",3.3 Integrated Inference Model,[0],[0]
We employ the latter method and set δ(f) =,3.3 Integrated Inference Model,[0],[0]
wf ·nf .,3.3 Integrated Inference Model,[0],[0]
"Finally, we employ a logistic regression classifier to predict R(H,T ), and the posterior probability of R(H,T ) is shown as follows,
P (ρ = y|χ) =",3.3 Integrated Inference Model,[0],[0]
"F(χ)y(1−F(χ))1−y
F(χ) = 1 1 + e−χ
(9)
where y is a 0-1 label of ρ.",3.3 Integrated Inference Model,[0],[0]
"Similar to Ltrw in Equation (5), we treat the negative logarithm of P (ρ = y|χ) as the loss of inference, Linf = − logP (ρ = y|χ), and turn to minimize it.",3.3 Integrated Inference Model,[0],[0]
"Moreover, the loss Linfrw of the above goal-directed random walk is influenced by the result of predicting R(H,T ), so Φrij and Φrik will be also updated.",3.3 Integrated Inference Model,[0],[0]
"Algorithm 1 shows the main process of training, where Ξ is the triplet set for training, Ψ is the potential function in Equation (7), F is the formula set, fp is
a formula generated from the path p, and H,T, t are entities in the KB.",3.3 Integrated Inference Model,[0],[0]
"To predict ρ = R(H,T ), the algorithm first performs multi rounds of random walks, and each random walk can find a path pHt (at line 2).",3.3 Integrated Inference Model,[0],[0]
"Then the algorithm decides to update Ψ by Ltrw based on whether t is T (at line 3), and adds the formula pf into the formula set when t = T (at line 4).",3.3 Integrated Inference Model,[0],[0]
"After random walks, the inference model predicts ρ, and computes Linf and L inf rw according to the prediction result (at line 5).",3.3 Integrated Inference Model,[0],[0]
"FinallyW and Ψ are updated by Linf and L inf rw (at line 6-7), respectively.",3.3 Integrated Inference Model,[0],[0]
"After training by all triplets in Ξ, the algorithm removes formulas with low weights from F (at line 8) and outputs the model (at line 9).",3.3 Integrated Inference Model,[0],[0]
"When we infer a new triplet by this model, the process is similar to Algorithm 1.",3.3 Integrated Inference Model,[0],[0]
We first compare our approach with several state-ofart methods on link prediction task to explore our approach’s overall ability of inference.,4 Experiments,[0],[0]
"Subsequently, we evaluate formulas mined by different random walk methods to explore whether the goal-directed mechanism can increase efficiency of mining useful structures.",4 Experiments,[0],[0]
"Finally, we dive deep into the formulas generated by our approach to analyze the characters of our approach.",4 Experiments,[0],[0]
"We conduct experiments on both WN18 and FB15K datasets which are subsets sampled from WordNet (Miller, 1995) and Freebase (Bollacker et al., 2008), respectively, and Table 1 shows the statistics of them.",4.1 Datasets and Evaluation Setup,[0],[0]
"For the link prediction task, we predict the missing h or t for a triplet r(h, t) in test set.",4.1 Datasets and Evaluation Setup,[0],[0]
"The detail evaluation method is that t in r(h, t) is replaced by all entities in the KB and methods need to rank the right answer at the top of the list, and so does h in r(h, t).",4.1 Datasets and Evaluation Setup,[0],[0]
"We report the mean of those true answer ranks and the Hits@10 under both ’raw’ and ’filter’ as TransE (Bordes et al., 2013) does, where Hits@10 is the proportion of correct entities ranked in the top 10.
sults on relation form of government in FB15K.",4.1 Datasets and Evaluation Setup,[0],[0]
We employ two types of baselines.,4.2 Baselines,[0],[0]
"One type is based on random walks including: a) the basic random walk algorithm whose state transition probability matrix is shown in Equation (2); b) PRA in (Lao et al., 2011) which is a typical heuristic random walk algorithm.",4.2 Baselines,[0],[0]
"The other type is based on KB embeddings including TransE (Bordes et al., 2013), Rescal (Nickel et al., 2011), TransH (Wang et al., 2014b), TransR",4.2 Baselines,[0],[0]
"(Lin et al., 2015b).",4.2 Baselines,[0],[0]
"These embedding-based methods have no explicit formulas, so we will not evaluate their performances on mining formulas.",4.2 Baselines,[0],[0]
We implement three random walk methods under a unified framework.,4.3 Settings,[0],[0]
"To predict r(h, ∗) quickly, we first select Top-K candidate instances, t1→K , by TransE as (Wei et al., 2015), and then the algorithm infers each r(h, ti) and ranks them by inference results.",4.3 Settings,[0],[0]
"We adjust parameters for our approach with the validate dataset, and the optimal configurations are set as follows.",4.3 Settings,[0],[0]
"The rounds of random walk is 10, learning rate is 0.0001, training epoch is 100, the size of candidate set is 500 for WN18 and 100 for FB15K, the embeddings have 50 dimensionalities for WN18 and 100 dimensionalities for FB15K, and the embeddings are initialized by TransE. For some relations, random walk truly finds no practicable formulas, so we employ TransE to improve per-
formance for these relations.",4.3 Settings,[0],[0]
"For embedding-based methods, we use reported results directly since the evaluation datasets are identical.",4.3 Settings,[0],[0]
"We show the results of link prediction for our approach and all baselines in Table 2 (* means the mean of ranks for random walk methods are evaluated in the Top-K subset), and we can obtain the following observations:
1)",4.4 Results on Link Prediction,[0],[0]
"Our approach achieves good performances on both WN18 and FB15K. On the FB15K, our approach outperforms all baselines.",4.4 Results on Link Prediction,[0],[0]
It indicates that our approach is effective for inference.,4.4 Results on Link Prediction,[0],[0]
"On the WN18, three random walk methods have similar performances.",4.4 Results on Link Prediction,[0],[0]
"The reason is that most entities in WN18 only have a small number of neighbors, so RW and PRA can also find useful structures in a few rounds.
2) For FB15K, the performances of RW and PRA are both poor and even worse than a part of embedding-based methods, but the performance of our approach is still the best.",4.4 Results on Link Prediction,[0],[0]
"The reason is that there are too many relation types in FB15K, so goalless random walks introduce lots of noise.",4.4 Results on Link Prediction,[0],[0]
"Oppositely, our approach has a great capability of resisting noise for the goal-directed mechanism.
3) RW and PRA have similar performances on both datasets, which indicates the heuristic rule of PRA does not apply to all relations and formulas.",4.4 Results on Link Prediction,[0],[0]
"To further explore whether the goal-directed mechanism can increase efficiency of mining paths, we compare the three random walk methods by the number of paths mined.",4.5 Paths Recall by Random Walks,[0],[0]
"For each triplet R(H,T )
in the training set, we perform 10 rounds of random walks fromH and record the number of times which arrive at T, noted as Arr@10.",4.5 Paths Recall by Random Walks,[0],[0]
We respectively select one relation type from WN18 and FB15K and show the comparison result in Figure 2.,4.5 Paths Recall by Random Walks,[0],[0]
"We can obtain the following observations:
1) With the increase of training epochs, Arr@10 of the goal-directed random walk first increases and then stays around a high value on both WN18 and FB15K, but the Arr@10 of RW and PRA always stay the same.",4.5 Paths Recall by Random Walks,[0],[0]
"This phenomenon indicates that the goal-directed random walk is a learnable model and can be trained to find more useful structures with epochs increasing, but RW and PRA are not.
2) RW and PRA always have similar Arr@10, which means PRA has not found more formulas.",4.5 Paths Recall by Random Walks,[0],[0]
This indicates that the heuristic rule of PRA is not always be beneficial to mining more structures for all relations.,4.5 Paths Recall by Random Walks,[0],[0]
"In Table 3, we show a small number of formulas mined by our approach from FB15K, and the formulas represent different types.",4.6 Example Formulas,[0],[0]
"Some formulas contain clear logic, e.g, Formula 1 means that if the writer x contributes a story to the film y and y is adapted from the book z, x is the writer of the book z.",4.6 Example Formulas,[0],[0]
"Some formulas have a high probability of being satisfied, e.g., Formula 3 means the wedding place probably is also the burial place for some people, and Formula 7 means the parent of the person x died of the disease and thus the person x has a high risk of suffering from the disease.",4.6 Example Formulas,[0],[0]
"Some formulas depend on synonyms, e.g., story by and works written have the similar meaning in Formula 2.",4.6 Example Formulas,[0],[0]
"However, there are still useless formulas, e.g, Formula 8 is useless be-
cause the body of the formula is same as the head.",4.6 Example Formulas,[0],[0]
"Such useless formula can be removed by a superrule, which is that the head of a formula cannot occur in its body.",4.6 Example Formulas,[0],[0]
"Our work has two aspects, which are related to mining formula automatically and inference on KBs, respectively.
",5 Related Work,[0],[0]
"Inductive Logic Programming (ILP) (Muggleton and De Raedt, 1994) and Association Rule Mining (ARM) (Agrawal et al., 1993) are both early works on mining formulas.",5 Related Work,[0],[0]
"FOIT (Quinlan, 1990) and SHERLOCK (Schoenmackers et al., 2010) are typical ILP systems, but the former one usually need a lot of negative facts and the latter one focuses on mining formulas from text.",5 Related Work,[0],[0]
"AMIE (Galárraga et al., 2013) is based on ARM and proposes a new measure for formulas instead of the confidence.",5 Related Work,[0],[0]
"Several structure learning algorithms (Kok and Domingos, 2005; Kok and Domingos, 2009; Kok and Domingos, 2010) based on Markov Logic Network (MLN) (Richardson and Domingos, 2006) can also learn first order logic formulas automatically, but they are too slow to run on large KBs.",5 Related Work,[0],[0]
"ProPPR (Wang et al., 2013; Wang et al., 2014a) performs structure learning by depth first searching on the knowledge graph, which is still not efficient enough to handle webscale KBs.",5 Related Work,[0],[0]
"PRA (Lao and Cohen, 2010; Lao et al., 2011) is a method based on random walks and employs heuristic rules to direct random walks.",5 Related Work,[0],[0]
"PRA is closely related to our approach, but unlike it, our approach dynamically calculates state transition prob-
abilities.",5 Related Work,[0],[0]
"Another method based on random walks (Wei et al., 2015) merges embedding similarities of candidates into the random walk as a priori, while our approach employs KB embeddings to calculate potentials for neighbors.
",5 Related Work,[0],[0]
"The majority of mining formula methods can perform inference on KBs, and besides them, a dozen methods based KB embeddings can also achieve the inference goal, and the typical ones of them are TransE (Bordes et al., 2013), Rescal (Nickel et al., 2011), TransH (Wang et al., 2014b), TransR",5 Related Work,[0],[0]
"(Lin et al., 2015b).",5 Related Work,[0],[0]
These embedding-based methods take advantage of the implicit relationship between elements of the KB and perform inference by calculating similarities.,5 Related Work,[0],[0]
"There are also methods which combine inference formulas and KB embeddings, such as PTransE (Lin et al., 2015a) and ProPPR+MF (Wang and Cohen, 2016).",5 Related Work,[0],[0]
"In this paper, we introduce a goal-directed random walk algorithm to increase efficiency of mining useful formulas and decrease noise simultaneously.",6 Conclusion and Future Works,[0],[0]
The approach employs the inference target as the direction at each steps in the random walk process and is more inclined to visit structures helpful to inference.,6 Conclusion and Future Works,[0],[0]
"In empirical studies, we show our approach achieves good performances on link prediction task over large-scale KBs.",6 Conclusion and Future Works,[0],[0]
"In the future, we are interested in exploring mining formulas directly in the distributional spaces which may resolve the sparsity of formulas.",6 Conclusion and Future Works,[0],[0]
"This work was supported by the Natural Science Foundation of China (No. 61533018), the National Basic Research Program of China (No. 2014CB340503) and the National Natural Science Foundation of China (No. 61272332).",7 Acknowledgments,[0],[0]
And this work was also supported by Google through focused research awards program.,7 Acknowledgments,[0],[0]
"Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually.",abstractText,[0],[0]
"Data-driven methods have been proposed to mine formulas from KBs automatically, where random sampling and approximate calculation are common techniques to handle big data.",abstractText,[0],[0]
"Among a series of methods, Random Walk is believed to be suitable for knowledge graph data.",abstractText,[0],[0]
"However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference.",abstractText,[0],[0]
"Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas.",abstractText,[0],[0]
"To this end, we propose a novel goaldirected inference formula mining algorithm, which directs random walks by the specific inference target at each step.",abstractText,[0],[0]
"The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously.",abstractText,[0],[0]
The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task.,abstractText,[0],[0]
Mining Inference Formulas by Goal-Directed Random Walks,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 22–31, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
Physically situated dialogue differs from traditional human-computer dialogue in that interactions will make use of reference to a dialogue agent’s surroundings.,1 Introduction,[0],[0]
"Tasks may fail due to dependencies on specific environment configurations, such as when a robot’s path to a goal is blocked.",1 Introduction,[0],[0]
"People will often help; in navigation dialogues they tend to ask proactive, task-related questions instead of simply signaling communication failure (Skantze, 2005).",1 Introduction,[0],[0]
They supplement the agent’s representation of the environment and allow it to complete tasks.,1 Introduction,[0],[0]
The current study establishes an empirical basis for grounding in physically situated contexts.,1 Introduction,[0],[0]
"We had people provide recovery strategies for a robot in various situations.
",1 Introduction,[0],[0]
"The focus of this work is on recovery from situated grounding problems, a type of miscommunication that occurs when an agent fails to uniquely map a person’s instructions to its surroundings (Marge and Rudnicky, 2013).",1 Introduction,[0],[0]
"A referential ambiguity is where an instruction resolves to more than one possibility (e.g., “Search the room on the left” when there are multiple rooms on the agent’s left); an impossible-to-execute problem
fails to resolve to any action (e.g., same instruction but there are no rooms on the agent’s left).",1 Introduction,[0],[0]
"A common strategy evidenced in human-human corpora is for people to ask questions to recover from situated grounding problems (Tenbrink et al., 2010).
",1 Introduction,[0],[0]
"Dialogue divides into two levels: that of managing the actual dialogue—determining who has the floor, that an utterance was recognized, etc.—and",1 Introduction,[0],[0]
"the dialogue that serves the main joint activities that dialogue partners are carrying out, like a human-robot team exploring a new area (Bangerter and Clark, 2003).",1 Introduction,[0],[0]
"Most approaches to grounding in dialogue systems are managing the dialogue itself, making use of spoken language input as an indicator of understanding (e.g., (Bohus, 2007; Skantze, 2007)).",1 Introduction,[0],[0]
Situated grounding problems are associated with the main joint activities; to resolve them we believe that the recovery model must be extended to include planning and environment information.,1 Introduction,[0],[0]
"Flexible recovery strategies make this possible by enabling dialogue partners to coordinate their joint activities and accomplish tasks.
",1 Introduction,[0],[0]
We cast the problem space as one where the agent aims to select the most efficient recovery strategy that would resolve a user’s intended referent.,1 Introduction,[0],[0]
We expect that this efficiency is tied to the cognitive load it takes to produce clarifications.,1 Introduction,[0],[0]
Viethen and Dale (2006) suggest a similar prediction in their study comparing human and automatically generated referring expressions of objects and their properties.,1 Introduction,[0],[0]
"We sought to answer the following questions in this work: • How good are people at detecting situated
grounding problems?",1 Introduction,[0],[0]
• How do people organize recovery strategies?,1 Introduction,[0],[0]
"• When resolving ambiguity, which properties do
people use to differentiate referents?",1 Introduction,[0],[0]
"• When resolving impossible-to-execute instruc-
tions, do people use active or passive ways to get the conversation back on track?
",1 Introduction,[0],[0]
"22
We determined the most common recovery strategies for referential ambiguity and impossible-toexecute problems.",1 Introduction,[0],[0]
Several patterns emerged that suggest ways that people expect agents to recover.,1 Introduction,[0],[0]
Ultimately we intend for dialogue systems to use such strategies in physically situated contexts.,1 Introduction,[0],[0]
Researchers have long observed miscommunication and recovery in human-human dialogue corpora.,2 Related Work,[0],[0]
"The HCRC MapTask had a direction giverdirection follower pair navigate two dimensional schematics with slightly different maps (Anderson et al., 1991).",2 Related Work,[0],[0]
Carletta (1992) proposed several recovery strategies following an analysis of this corpus.,2 Related Work,[0],[0]
"The SCARE corpus collected human-human dialogues in a similar scenario where the direction follower was situated in a three-dimensional virtual environment (Stoia et al., 2008).
",2 Related Work,[0],[0]
"The current study follows up an initial proposal set of recovery strategies for physically situated domains (Marge and Rudnicky, 2011).",2 Related Work,[0],[0]
Others have also developed recovery strategies for situated dialogue.,2 Related Work,[0],[0]
Kruijff et al. (2006) developed a framework for a robot mapping an environment that employed conversational strategies as part of the grounding process.,2 Related Work,[0],[0]
"A similar study focused on resolving misunderstandings in the humanrobot domain using the Wizard-of-Oz methodology (Koulouri and Lauria, 2009).",2 Related Work,[0],[0]
"A body of work on referring expression generation uses object attributes to generate descriptions of referents (e.g., (Guhe and Bard, 2008; Garoufi and Koller, 2014)).",2 Related Work,[0],[0]
"Viethen and Dale (2006) compared human-authored referring expressions of objects to existing natural language generation algorithms and found them to have very different content.
",2 Related Work,[0],[0]
Crowdsourcing has been shown to provide useful dialogue data:,2 Related Work,[0],[0]
Manuvinakurike and DeVault (2015) used the technique to collect gameplaying conversations.,2 Related Work,[0],[0]
"Wang et al. (2012) and Mitchell et al. (2014) have used crowdsourced data for training, while others have used it in real time systems (Lasecki et al., 2013; Huang et al., 2014).",2 Related Work,[0],[0]
"In this study, participants came up with phrases that a search-and-rescue robot should say in response to an operator’s command.",3 Method,[0],[0]
"The participant’s task was to view scenes in a virtual envi-
ronment then formulate the robot’s response to an operator’s request.",3 Method,[0],[0]
"Participants listened to an operator’s verbal command then typed in a response.
",3 Method,[0],[0]
"Scenes displayed one of three situations: referential ambiguity (more than one possible action), impossible-to-execute (zero possible actions), and executable (one possible action).",3 Method,[0],[0]
The instructions showed some example problems.,3 Method,[0],[0]
All situations involved one operator and one robot.,3 Method,[0],[0]
"After instructions and a practice trial, participants viewed scenes in one of 10 different environments (see Figure 1).",3.1 Experiment Design,[0],[0]
"They would first watch a flyover video of the robot’s environment, then view a screen showing labels for all possible referable objects in the scene.",3.1 Experiment Design,[0],[0]
The participant would then watch the robot enter the first scene.,3.1 Experiment Design,[0],[0]
"The practice trial and instructions did not provide any examples of questions.
",3.1 Experiment Design,[0],[0]
The robot would stop and a spoken instruction from the operator would be heard.,3.1 Experiment Design,[0],[0]
The participant was free to replay the instruction multiple times.,3.1 Experiment Design,[0],[0]
They would then enter a response (say an acknowledgment or a question).,3.1 Experiment Design,[0],[0]
"Upon completion of the trial, the robot would move to a different scene, where the process was repeated.
",3.1 Experiment Design,[0],[0]
Only self-contained questions that would allow the operator to answer without follow-up were allowed.,3.1 Experiment Design,[0],[0]
Thus generic questions like “which one?” would not allow the operator to give the robot enough useful information to proceed.,3.1 Experiment Design,[0],[0]
"In the instructions, we suggested that participants include some detail about the environment in their ques-
tions.",3.1 Experiment Design,[0],[0]
Participants used a web form1 to view situations and provide responses.,3.1 Experiment Design,[0],[0]
"We recorded demographic information (gender, age, native language, native country) and time on task.",3.1 Experiment Design,[0],[0]
"The instructions had several attention checks (Paolacci et al., 2010) to ensure that participants were focusing on the task.
",3.1 Experiment Design,[0],[0]
We created fifty trials across ten environments.,3.1 Experiment Design,[0],[0]
Each environment had five trials that represented waypoints the robot was to reach.,3.1 Experiment Design,[0],[0]
Participants viewed five different environments (totaling twenty-five trials).,3.1 Experiment Design,[0],[0]
Each command from the remote operator to the robot was a route instruction in the robot navigation domain.,3.1 Experiment Design,[0],[0]
Trials were assembled in two groups and participants were assigned randomly to one (see Table 1).,3.1 Experiment Design,[0],[0]
Trial order was randomized according to a Latin Square.,3.1 Experiment Design,[0],[0]
"Scenes were of a 3D virtual environment at eye level, with the camera one to two meters behind the robot.",3.1.1 Scenes and Environments,[0],[0]
"Camera angle issues with environment objects caused this variation.
",3.1.1 Scenes and Environments,[0],[0]
Participants understood that the fictional operator was not co-located with the robot.,3.1.1 Scenes and Environments,[0],[0]
The USARSim robot simulation toolkit and the UnrealEd game map editor were used to create the environment.,3.1.1 Scenes and Environments,[0],[0]
"Cepstral’s SwiftTalker was used for the operator voice.
",3.1.1 Scenes and Environments,[0],[0]
"Of the fifty scenes, twenty-five (50%) had referential ambiguities, fifteen (30%) were impossible-to-execute, and ten (20%) were executable controls.",3.1.1 Scenes and Environments,[0],[0]
"The selection was weighted to referential ambiguity, as these were expected to produce greater variety in recovery strategies.",3.1.1 Scenes and Environments,[0],[0]
"We randomly assigned each of fifty trials a stimulus type according to this distribution, then divided the list into ten environments.",3.1.1 Scenes and Environments,[0],[0]
"The environments featured objects and doorways appropriate to the trial type, as well as waypoints.
1See http://goo.gl/forms/ZGpK3L1nPh for an example.
",3.1.1 Scenes and Environments,[0],[0]
"Referential Ambiguity We arranged the sources of information participants could use to describe referents, to enable analysis of the relationship between context and recovery strategies.",3.1.1 Scenes and Environments,[0],[0]
"The sources of information (i.e., “situated dimensions”) were: (1) intrinsic properties (either color or size), (2) history (objects that the robot already encountered), (3) egocentric proximity of the robot to candidate referents around it (the robot’s perspective is always taken), and (4) object proximity (proximity of candidate referents to other objects).",3.1.1 Scenes and Environments,[0],[0]
"Table 2 provides additional details.
",3.1.1 Scenes and Environments,[0],[0]
Scenes with referential ambiguity had up to four sources of information available.,3.1.1 Scenes and Environments,[0],[0]
"Information sources were evenly distributed across five trial types: one that included all four sources, and four that included all but one source of information (e.g., one division excluded using history information but did allow proximity, spatial, and object properties, one excluded proximity, etc.).
",3.1.1 Scenes and Environments,[0],[0]
Impossible-to-Execute The impossible-to-execute trials divided into two broad types.,3.1.1 Scenes and Environments,[0],[0]
Nine of the fifteen scenes were impossible because the operator’s command did not match to any referent in the environment.,3.1.1 Scenes and Environments,[0],[0]
"The other six scenes were impossible because a path to get to the matching referent was not possible.
",3.1.1 Scenes and Environments,[0],[0]
Executable Ten scenes were executable for the study and served as controls.,3.1.1 Scenes and Environments,[0],[0]
"The operator’s command mentioned existing, unambiguous referents.",3.1.1 Scenes and Environments,[0],[0]
Participants were aware of the robot’s capabilities before the start of the experiment.,3.1.2 Robot Capabilities,[0],[0]
The instructions said that the robot knew the locations of all objects in the environment and whether doors were closed or open.,3.1.2 Robot Capabilities,[0],[0]
"The robot also knew the color and size of objects in the environment (intrinsic properties), where objects were relative to the robot itself and to other objects (proximity), when objects were right, left, in front, and behind it (spatial terms), the room and hallway locations of objects (location), and the places it has been (history, the robot kept track of which objects it had visited).",3.1.2 Robot Capabilities,[0],[0]
The robot could not pass through closed doors.,3.1.2 Robot Capabilities,[0],[0]
"We made five hypotheses about the organization and content of participant responses to situated grounding problems:
• Hypothesis 1: Participants will have more difficulty detecting impossible-to-execute scenes than ambiguous ones.",3.2 Hypotheses,[0],[0]
"Determining a robot’s tasks to be impossible requires good situation awareness (Nielsen et al., 2007) (i.e., an understanding of surroundings with respect to correctly completing tasks).",3.2 Hypotheses,[0],[0]
"Detecting referential ambiguity requires understanding the operator’s command and visually inspecting the space (Spivey et al., 2002); detecting impossible commands also requires recalling the robot’s capabilities and noticing obstacles.",3.2 Hypotheses,[0],[0]
"Previous research has noted that remote teleoperators have trouble establishing good situation awareness of a robot’s surroundings (Casper and Murphy, 2003; Burke et al., 2004).",3.2 Hypotheses,[0],[0]
"Moreover, obstacles near a robot can be difficult to detect with a restricted view as in the current study (Alfano and Michel, 1990; Arthur, 2000).",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
"Hypotheses 2a and 2b: Responses will more commonly be single, self-contained questions instead of a scene description followed by a question (2a for scenes with referential ambiguity, 2b for scenes that were impossible-toexecute).",3.2 Hypotheses,[0],[0]
"This should reflect the principle of least effort (Clark, 1996), and follow from Carletta’s (1992) observations in a similar dataset.",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
Hypothesis 3: Responses will use the situated dimensions that require the least cognitive effort when disambiguating referents.,3.2 Hypotheses,[0],[0]
Viethen and Dale (2006) suggest that minimizing cognitive load for the speaker or listener would produce more human-like referring expressions.,3.2 Hypotheses,[0],[0]
"We predict that responses will mention visually salient features of the scene, such as color or size of referents, more than history or object proximity.",3.2 Hypotheses,[0],[0]
"Desimone and Duncan (1995) found that color and shape draw more attention than other
properties in visual search tasks when they are highly distinguishable.",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
"Hypothesis 4: In cases of referential ambiguity where two candidate referents are present, responses will confirm one referent in the form of a yes-no question more than presenting a list.",3.2 Hypotheses,[0],[0]
"Results from an analysis of task-oriented dialogue suggests that people are efficient when asking clarification questions (Rieser and Moore, 2005).",3.2 Hypotheses,[0],[0]
"Additionally, Clark’s least effort principle (Clark, 1996) suggests that clarifying one referent using a yes-no confirmation would require less effort than presenting a list in two ways: producing a shorter question and constraining the range of responses to expect.",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
"Hypothesis 5: For impossible-to-execute instructions, responses will most commonly be ways for the robot to proactively work with the operator’s instruction, in an effort to get the conversation back on track.",3.2 Hypotheses,[0],[0]
"The other possible technique, to simply declare that the problem is not possible, will be less common.",3.2 Hypotheses,[0],[0]
This is because participants will believe such a strategy will not align with the task goal of having the robot say something that will allow it to proceed with the task.,3.2 Hypotheses,[0],[0]
"Skantze found that in human-human navigation dialogues, people would prefer to look for alternative ways to proceed rather than simply express nonunderstanding (Skantze, 2005).",3.2 Hypotheses,[0],[0]
"The key independent variable in this study was the stimulus type that the participant viewed (i.e., referential ambiguity, impossible-to-execute, or executable).",3.3 Measures,[0],[0]
"Dependent variables were observational measurements, presented below.",3.3 Measures,[0],[0]
"We report Fleiss’ kappa score for inter-annotator agreement
between three native English speaking annotators on a subset of the data.
",3.3 Measures,[0],[0]
Correctness (κ = 0.77):,3.3 Measures,[0],[0]
"Whether participants correctly determined the situation as ambiguous, impossible, or executable.",3.3 Measures,[0],[0]
Annotators labeled correctness based on the content of participant responses.,3.3 Measures,[0],[0]
This measure assessed participant accuracy for detecting situated grounding problems.,3.3 Measures,[0],[0]
"Either correct or incorrect.
",3.3 Measures,[0],[0]
"Sentence type (κ = 0.82): Either declarative, interrogative, imperative, or exclamatory (Cowan, 2008).
",3.3 Measures,[0],[0]
Question type (κ = 0.92): Sentences that needed an answer from the operator.,3.3 Measures,[0],[0]
"The three types were yes-no questions, alternative questions (which presented a list of options and includes wh- questions that used sources from Table 2), and generic wh- questions (Cowan, 2008).
",3.3 Measures,[0],[0]
Situated dimensions in response (κ = 0.75):,3.3 Measures,[0],[0]
The capability (or capabilities) that the participant mentioned when providing a response.,3.3 Measures,[0],[0]
"The types were intrinsic (color or size), object proximity, egocentric proximity, and history.
",3.3 Measures,[0],[0]
"Projected belief (impossible-to-execute trials only, κ = 0.80):",3.3 Measures,[0],[0]
"The participant’s belief about the next task, given the current operator instruction (projected onto the robot).",3.3 Measures,[0],[0]
"The types were unknown (response indicates participant is unsure what to do next), ask for more (ask for more details), propose alternative (propose alternative object), ask for help (ask operator to physically manipulate environment), and off topic.",3.3 Measures,[0],[0]
We recruited 30 participants.,3.4 Participation,[0],[0]
"All participants completed the web form through the Amazon Mechanical Turk (MTurk) web portal2, all were located in the United States and had a task approval rate ≥95%.",3.4 Participation,[0],[0]
The group included 29 self-reported native English speakers born in the United States; 1 self-reported as a native Bangla speaker born in Bangladesh.,3.4 Participation,[0],[0]
The gender distribution was 15 male to 15 female.,3.4 Participation,[0],[0]
"Participants ranged in age from 22 to 52 (mean: 33 years, std. dev.: 7.7).",3.4 Participation,[0],[0]
They were paid between $1 and $2 for their participation.,3.4 Participation,[0],[0]
"We
2https://www.mturk.com
collected a total of 750 responses.",3.4 Participation,[0],[0]
We analyzed the measures by tabulating frequencies for each possible value.,4 Results,[0],[0]
Table 3 presents some example responses.,4 Results,[0],[0]
"In general, participants were good at detecting situated grounding problems.",4.1 Correctness,[0],[0]
"Out of 750 responses, 667 (89%) implied the correct scene type.",4.1 Correctness,[0],[0]
"We analyzed correctness across actual stimulus types (ambiguous, impossible-to-execute, executable) using a mixed-effects analysis of variance model3, with participant included as a random effect and trial group as a fixed effect.
",4.1 Correctness,[0],[0]
Hypothesis 1 predicted that participants will do better detecting scenes with referential ambiguity than those that were impossible-to-execute; the results support this hypothesis.,4.1 Correctness,[0],[0]
"Actual stimulus type had a significant main effect on correctness (F[2, 58] = 12.3, p < 0.001); trial group did not (F[1, 28] = 0.1, p = 0.72).",4.1 Correctness,[0],[0]
Participants had significantly worse performance detecting impossible-to-execute scenes compared to ambiguous ones (p< 0.001; Tukey HSD test).,4.1 Correctness,[0],[0]
"In fact, they were four times worse; of the impossible-toexecute scenes, participants failed to detect that 22% (50/225) of them were impossible, compared to 5% (17/375) of scenes with referential ambiguity.",4.1 Correctness,[0],[0]
"Of the 150 instructions that were executable, participants failed to detect 11% (16/150) of them as such.",4.1 Correctness,[0],[0]
"We analyzed the 358 responses where participants correctly detected referential ambiguity.
",4.2 Referential Ambiguity,[0],[0]
"3This approach computed standard least squares regression using reduced maximum likelihood (Harville, 1977).
",4.2 Referential Ambiguity,[0],[0]
"Hypothesis 2a predicted that participants would more commonly ask single, self-contained questions instead of describing the scene and asking a question.",4.2 Referential Ambiguity,[0],[0]
We assessed this by counting sentence types within a response.,4.2 Referential Ambiguity,[0],[0]
Responses that had both a declarative sentence and an interrogative would fit this case.,4.2 Referential Ambiguity,[0],[0]
The results confirmed this hypothesis.,4.2 Referential Ambiguity,[0],[0]
"Only 4.5% (16/358) of possible responses had a declarative and an interrogative.
",4.2 Referential Ambiguity,[0],[0]
Hypothesis 3 predicted that participants would use the situated dimensions that require the least cognitive effort when disambiguating referents.,4.2 Referential Ambiguity,[0],[0]
"More specifically, the most common mentions will be those that are visually apparent (intrinsic properties like color and size), while those that require more processing would have fewer mentions (history and to a lesser extent object proximity and egocentric proximity).",4.2 Referential Ambiguity,[0],[0]
"We measured this by tabulating mentions of situated dimensions in all 358 correct participant responses, summarized in Figure 2.",4.2 Referential Ambiguity,[0],[0]
Multiple dimensions could occur in a single response.,4.2 Referential Ambiguity,[0],[0]
The results support this hypothesis.,4.2 Referential Ambiguity,[0],[0]
"By far, across all ambiguous scenarios, the most mentioned dimension was an intrinsic property.",4.2 Referential Ambiguity,[0],[0]
"More than half of all situated dimensions used were intrinsic (59%, 242/410 total mentions).",4.2 Referential Ambiguity,[0],[0]
"This was followed by the dimensions that we hypothesize require more cognitive effort: egocentric proximity had 30% (125/410) of mentions, object proximity 9.5% (39/410), and history 1% (4/410).",4.2 Referential Ambiguity,[0],[0]
"Of the intrinsic dimensions mentioned, most were only color (61%, 148/242), followed by size (33%, 81/242), and using both (5%, 13/242).
",4.2 Referential Ambiguity,[0],[0]
Hypothesis 4 predicted that participants would ask yes-no confirmation questions in favor of presenting lists when disambiguating a referent with exactly two candidates.,4.2 Referential Ambiguity,[0],[0]
"The results suggest that the opposite is true; people strongly preferred to
list options, even when a confirmation question about one would have been sufficient.",4.2 Referential Ambiguity,[0],[0]
"Of the 285 responses that were correctly detected as ambiguous and were for scenes of exactly two possible referents, 74% (212/285) presented a list of options.",4.2 Referential Ambiguity,[0],[0]
Only 14% (39/285) asked yes-no confirmation questions.,4.2 Referential Ambiguity,[0],[0]
The remaining 34 questions (12%) were generic wh-questions.,4.2 Referential Ambiguity,[0],[0]
These results held in scenes where three options were present.,4.2 Referential Ambiguity,[0],[0]
"Overall 72% (259/358) presented a list of options, while 16% (58/358) asked generic wh-questions and 11% (41/358) asked yes-no confirmations.",4.2 Referential Ambiguity,[0],[0]
"We analyzed the 175 responses where participants correctly identified impossible-to-execute situations.
",4.3 Impossible-to-Execute,[0],[0]
Hypothesis 2b predicted that participants would more often only ask a question than also describe the scene.,4.3 Impossible-to-Execute,[0],[0]
Results confirmed this hypothesis.,4.3 Impossible-to-Execute,[0],[0]
"42% (73/175) of responses simply asked a question, while 22% (39/175) used only a declarative.",4.3 Impossible-to-Execute,[0],[0]
"More than a third included a declarative as well (36%, 63/175).",4.3 Impossible-to-Execute,[0],[0]
"The general organization to these was to declare the problem then ask a question about it (89%, 56/63).
",4.3 Impossible-to-Execute,[0],[0]
"Hypothesis 5 predicted that responses for impossible-to-execute instructions will more commonly be proactive and make suggestions, instead of simply declaring that an action was not possible.",4.3 Impossible-to-Execute,[0],[0]
"Table 4 summarizes the results, which confirmed this hypothesis.",4.3 Impossible-to-Execute,[0],[0]
The most common belief that participants had for the robot was to have it propose an alternative referent to the impossible one specified by the operator.,4.3 Impossible-to-Execute,[0],[0]
The next-most common was to have the robot simply express uncertainty about what to do next.,4.3 Impossible-to-Execute,[0],[0]
"Though this belief occurred in about a third of responses, the remaining responses were all proactive ways for the robot to get the conversation back on track (i.e., propose alternative, ask for more, and ask for help).",4.3 Impossible-to-Execute,[0],[0]
"The results largely support the hypotheses, with the exception of Hypothesis 4.",5 Discussion,[0],[0]
"They also provide information about how people expect robots to recover from situated grounding problems.
",5 Discussion,[0],[0]
"Correctness Participants had the most trouble detecting impossible-to-execute scenes, supporting Hypothesis 1.",5 Discussion,[0],[0]
"An error analysis of the 50 responses for this condition had participants responding as if the impossible scenes were possible (62%, 31/50).",5 Discussion,[0],[0]
"The lack of good situation awareness was a factor, which agrees with previous findings in the human-robot interaction literature (Casper and Murphy, 2003; Burke et al., 2004).",5 Discussion,[0],[0]
We found that participants had trouble with a specific scene where they confused the front and back of the robot (9 of the 31 impossibleexecutable responses were for this scene).,5 Discussion,[0],[0]
"Note that all scenes showed the robot entering the room with the same perspective, facing forward.
",5 Discussion,[0],[0]
"Referential Ambiguity Results for Hypothesis 2a showed that participants overwhelmingly asked only a single, self-contained question as opposed to first stating that there was an ambiguity.",5 Discussion,[0],[0]
"Participants also preferred to present a list of options, despite the number of possible candidates.",5 Discussion,[0],[0]
"This contradicted Hypothesis 4. Rieser and Moore (2005) found that in task-oriented human-human dialogues, clarification requests aim to be as efficient as possible; they are mostly partially formed.",5 Discussion,[0],[0]
The results in our study were not of real-time dialogue; we isolated specific parts of what participants believed to be human-computer dialogue.,5 Discussion,[0],[0]
"Moreover, Rieser and Moore were observing clarifications at Bangerter and Clark’s (2003) dialogue management level; we were observing them in service of the joint activity of navigating the robot.",5 Discussion,[0],[0]
"We believe that this difference resulted in participants using caution by disambiguating with lists.
",5 Discussion,[0],[0]
"These results suggest that dialogue systems should present detection of referential ambiguity implicitly, and as a list.",5 Discussion,[0],[0]
"Generic wh- questions (e.g., “which one?” without presenting a followon list) are less desirable because they don’t constrain what the user can say, and don’t provide any indication of what the dialogue system can understand.",5 Discussion,[0],[0]
"A list offers several benefits: it grounds awareness of surroundings, presents a fixed set of options to the user, and constrains the range of
linguistic responses.",5 Discussion,[0],[0]
"This could also extend to general ambiguity, as in when there are a list of matches to a query, but that is outside the scope of this work.",5 Discussion,[0.9529072165731979],"['As a result, the values of initial state may not be exact during the translation process, leading to poor translation performances.']"
"Lists may be less useful as they grow in size; in our study they could not grow beyond three candidates.
",5 Discussion,[0],[0]
The data also supported Hypothesis 3.,5 Discussion,[0],[0]
Participants generally preferred to use situated dimensions that required less effort to describe.,5 Discussion,[0],[0]
"Intrinsic dimensions (color and size) had the greatest count, followed by egocentric proximity, object proximity, and finally using history.",5 Discussion,[0],[0]
"We attribute these results to the salient nature of intrinsic properties compared to ones that must be computed (i.e., egocentric and object proximity require spatial processing, while history requires thinking about previous exchanges).",5 Discussion,[0],[0]
This also speaks to a similar claim by Viethen and Dale (2006).,5 Discussion,[0],[0]
"Responses included color more than any other property, suggesting that an object’s color draws more visual attention than its size.",5 Discussion,[0],[0]
"Bright colors and big shapes stand out most in visual search tasks; we had more of the former than the latter (Desimone and Duncan, 1995).
",5 Discussion,[0],[0]
"For an ambiguous scene, participants appear to traverse a salience hierarchy (Hirst et al., 1994) whereby they select the most visually salient feature that also uniquely teases apart candidates.",5 Discussion,[0],[0]
"While the salience hierarchy varies depending on the current context of a referent, we anticipate such a hierarchy can be defined computationally.",5 Discussion,[0],[0]
"Others have proposed similar processes for referring expression generation (Van Der Sluis, 2005; Guhe and Bard, 2008).",5 Discussion,[0],[0]
One way to rank salience on the hierarchy could be predicted mental load; we speculate that this is a reason why history was barely mentioned to disambiguate.,5 Discussion,[0],[0]
"Another would be to model visual attention, which could explain why color was so dominant.
",5 Discussion,[0],[0]
"Note that only a few dimensions were “competing” at any given time, and their presence in the scenes was equal (save for history, which had slightly fewer due to task design constraints).",5 Discussion,[0],[0]
"Egocentric proximity, which uses spatial language to orient candidate referents relative to the robot, had a moderate presence.",5 Discussion,[0],[0]
"When intrinsic properties were unavailable in the scene, responses most often used this property.",5 Discussion,[0],[0]
"We found that sometimes participants would derive this property even if it wasn’t made prototypical in the scene (e.g., referring to a table as “left” when it was in front and
off to the left side of the robot).",5 Discussion,[0],[0]
This suggests that using egocentric proximity to disambiguate makes a good fallback strategy when nothing else works.,5 Discussion,[0],[0]
"Another situated dimension emerged from the responses, disambiguation by location (e.g., “Do you mean the box in this room or the other one?”).",5 Discussion,[0],[0]
"Though not frequent, it provides another useful technique to disambiguate when visually salient properties are not available.
",5 Discussion,[0],[0]
"Our findings differ from those of Carlson and Hill (2009) who found that salience is not as prominent as spatial relationships between a target (in the current study, this would be the robot) and other objects.",5 Discussion,[0],[0]
Our study did not direct participants to formulate spatial descriptions; they were free to compose responses.,5 Discussion,[0],[0]
"In addition, our work directly compares intrinsic properties for objects of the same broad type (e.g., disambiguation of a doors of different colors).",5 Discussion,[0],[0]
"Our findings suggest the opposite of Moratz et al. (2003), who found that when pointing out an object, describing its position may be better than describing its attributes in human-robot interactions.",5 Discussion,[0],[0]
"Their study only had one object type (cube) and did not vary color, size, or proximity to nearby objects.",5 Discussion,[0],[0]
"As a result, participants described objects using spatial terms.",5 Discussion,[0],[0]
"In our study, we explored variation of several attributes to determine participants’ preferences.
",5 Discussion,[0],[0]
Impossible-to-Execute Results supported Hypothesis 2b.,5 Discussion,[0],[0]
Most responses had a single sentence type.,5 Discussion,[0],[0]
"Although unanticipated, a useful strategy emerged: describe the problem that makes the scene impossible, then propose an alternative referent.",5 Discussion,[0],[0]
This type of strategy helped support Hypothesis 5.,5 Discussion,[0],[0]
"Responses for impossible scenes largely had the participant proactively presenting a way to move the task forward, similar to what Skantze (2005) observed in human-human dialogues.",5 Discussion,[0],[0]
This suggests that participants believed the robot should ask directed questions to recover.,5 Discussion,[0],[0]
These questions often took the form of posing alternative options.,5 Discussion,[0],[0]
We used the Amazon Mechanical Turk web portal to gather responses in this study.,5.1 Limitations,[0],[0]
"As such we could not control the participant environment when taking the study, but we did include attention checks.",5.1 Limitations,[0],[0]
"Participants did not interact with a
dialogue system.",5.1 Limitations,[0],[0]
Instead we isolated parts of the interaction that were instances of where the robot would have to say something in response to an instruction.,5.1 Limitations,[0],[0]
We asked participants to provide what they think the robot should say; there was no ongoing interaction.,5.1 Limitations,[0],[0]
"However, we maintained continuity by presenting videos of the robot navigating through the environment as participants completed the task.",5.1 Limitations,[0],[0]
"The robot was represented in a virtual environment, which prevents us from understanding if there are any influencing factors that may impact results if the robot were in physical form or co-present with the participant.",5.1 Limitations,[0],[0]
Recovery strategies allow situated agents like robots to recover from misunderstandings by using the human dialogue partner.,6 Conclusions,[0],[0]
We conducted a study that collected recovery strategies for physically situated dialogue with the goal of establishing an empirical basis for grounding in physically situated contexts.,6 Conclusions,[0],[0]
"We crowdsourced 750 written strategies across 30 participants and analyzed their situated properties and how they were organized.
",6 Conclusions,[0],[0]
We found that participants’ recovery strategies minimize cognitive effort and indicate a desire to successfully complete the task.,6 Conclusions,[0],[0]
"For disambiguation, there was a preference for strategies that use visually salient properties over ones that require additional mental processing, like spatial reasoning or memory recall.",6 Conclusions,[0],[0]
"For impossible-to-execute scenes, responses more often presented alternative referents than just noting non-understanding.",6 Conclusions,[0],[0]
"We should note that some differences between our findings and those of others may in part rest on differences in task and environment, though intrinsic variables such as mental effort will likely persist over different situations.
",6 Conclusions,[0],[0]
"In future work, we intend to use these data to model salience ranking in similar contexts.",6 Conclusions,[0],[0]
We will further assess the hypothesis that participants’ preferences in this study will enhance performance in a spoken dialogue system that deploys similar strategies.,6 Conclusions,[0],[0]
The authors thank Prasanna Kumar Muthukumar and Juneki Hong for helping to annotate recovery strategies.,Acknowledgments,[0],[0]
"We also thank Taylor Cassidy, Arthur William Evans, and the anonymous reviewers for their valuable comments.",Acknowledgments,[0],[0]
We describe an empirical study that crowdsourced human-authored recovery strategies for various problems encountered in physically situated dialogue.,abstractText,[0],[0]
The purpose was to investigate the strategies that people use in response to requests that are referentially ambiguous or impossible to execute.,abstractText,[0],[0]
"Results suggest a general preference for including specific kinds of visual information when disambiguating referents, and for volunteering alternative plans when the original instruction was not possible to carry out.",abstractText,[0],[0]
Miscommunication Recovery in Physically Situated Dialogue,title,[0],[0]
Feature selection is an important step in extracting interpretable patterns from data.,1. Introduction,[0],[0]
"It has numerous applications in a wide range of areas, including natural-language processing, genomics, and chemistry.",1. Introduction,[0],[0]
"Suppose that there are n or-
*These authors contributed equally and are listed alphabetically 1Department of Electrical Engineering, Stanford University, Stanford, California 2Department of Computer Science, Rice University, Houston, Texas 3Department of Electrical and Computer Engineering, Rice University, Houston, Texas.",1. Introduction,[0],[0]
"Correspondence to: Anshumali Shrivastava <anshumali@rice.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"dered pairs (Xi, yi)i∈[n], where Xi ∈ Rp are p-dimensional feature vectors, and yi ∈ R are scalar outputs.",1. Introduction,[0],[0]
"Feature selection aims to identify a small subset of features (coordinates of the p-dimensional feature vector) that best models the relationship between the data Xi and the output yi.
",1. Introduction,[0],[0]
A significant complication that is common in modern engineering and scientific applications is that the feature space p is ultra high-dimensional.,1. Introduction,[0],[0]
"For example, Weinberger introduced a dataset with 16 trillion (p = 1013) unique features (Weinberger et al., 2009).",1. Introduction,[0],[0]
A 16 trillion dimensional feature vector (of double 8 bytes) requires 128 terabytes of working memory.,1. Introduction,[0],[0]
Problems from modern genetics are even more challenging.,1. Introduction,[0],[0]
A particularly useful way to represent a long DNA sequence is by a feature vector that counts the occurrence frequency of all length-K sub-strings called K-mers.,1. Introduction,[0],[0]
"This representation plays an important role in large-scale regression problems in computational biology (Wood & Salzberg, 2014; Bray et al., 2015; Vervier et al., 2016; Aghazadeh et al., 2016).",1. Introduction,[0],[0]
"Typically, K is chosen to be larger than 12, and these strings are composed of all possible combinations of 16 characters ({A,T,C,G} in addition to 12 wild card characters).",1. Introduction,[0],[0]
"In this case, the feature vector dimension is p = 1612 = 248.",1. Introduction,[0],[0]
"A vector of size 248 single-precision variables requires approximately 1 petabyte of space!
",1. Introduction,[0],[0]
"For ultra large-scale feature selection problems, it is impossible to run standard explicit regularization-based methods like `1-regularization (Shalev-Shwartz & Tewari, 2011; Tan et al., 2014) or to select hyperparameters with a constrained amount of memory (Langford et al., 2009).",1. Introduction,[0],[0]
"This is not surprising, because these methods are not scalable in terms of memory and computational time (Duchi et al., 2008).",1. Introduction,[0],[0]
Another important operational concern is that most datasets represent features in the form of strings or tokens.,1. Introduction,[0],[0]
"For example, with DNA or n-gram datasets, features are represented by strings of characters.",1. Introduction,[0],[0]
"Even in click-through data (McMahan et al., 2013), features are indexed by textual tokens.",1. Introduction,[0],[0]
Observe that mapping each of these strings to a vector component requires maintaining a dictionary whose size equals the length of the feature vector.,1. Introduction,[0],[0]
"As a result, one does not even have the capability to create a numerical exact vector representation of the features.
",1. Introduction,[0],[0]
"Typically, when faced with such large machine learning tasks, the practitioner chooses to do feature hashing (Weinberger et al., 2009).",1. Introduction,[0],[0]
Consider a 3-gram string “abc”.,1. Introduction,[0],[0]
"With feature hashing, one uses a lossy, random hash function h : strings → {0, 1, 2, . . .",1. Introduction,[0],[0]
", R} to map “abc” to a feature number h(abc) in the range {0, 1, 2, . .",1. Introduction,[0],[0]
.,1. Introduction,[0],[0]
", R}.",1. Introduction,[0],[0]
This is extremely convenient because it enables one to avoid creating a large look-up dictionary.,1. Introduction,[0],[0]
"Furthermore, this serves as a dimensionality reduction technique, reducing the problem dimension to R. Unfortunately, this convenience comes at a cost.",1. Introduction,[0],[0]
"Given that useful dimensionality reduction is strictly surjective (i.e., R < p), we lose the identity of the original features.",1. Introduction,[0],[0]
"This is not a viable option if one cares about both feature selection and interpretability.
",1. Introduction,[0],[0]
"One reason to remain hopeful is that in such highdimensional problems, the data vectors Xi are extremely sparse (Wood & Salzberg, 2014).",1. Introduction,[0],[0]
"For instance, the DNA sequence of an organism contains only a small fraction (at most the length of the DNA sequence) of p = 1612 features.",1. Introduction,[0],[0]
"The situation is similar whether we are predicting clickthrough rates of users on a website or if we seek n-gram representations of text documents (Mikolov et al., 2013).",1. Introduction,[0],[0]
"In practice, ultra high-dimensional data is almost always ultrasparse.",1. Introduction,[0],[0]
"Thus, loading a sparse data vector into memory is usually not a concern.",1. Introduction,[0],[0]
"The problem arises in the intermediate stages of traditional methods, where dense iterates need to be tracked in the main memory.",1. Introduction,[0],[0]
"One popular approach is to use greedy thresholding methods (Maleki, 2009; Mikolov et al., 2013; Jain et al., 2014; 2017) combined with stochastic gradient descent (SGD) to prevent the feature vector β from becoming too dense and blowing up in memory.",1. Introduction,[0],[0]
"In these methods, the intermediate iterates are regularized at each step, and a full gradient update is never stored nor computed (since this is memory and computation intensive).",1. Introduction,[0],[0]
"However, it is well known that greedy thresholding can be myopic and can result in poor convergence.",1. Introduction,[0],[0]
We clearly observe this phenomenon in our evaluations.,1. Introduction,[0],[0]
"See Section 5 for details.
",1. Introduction,[0],[0]
"In this paper we tackle the ultra large-scale feature selection problem, i.e., feature selection with billions or more dimensions.",1. Introduction,[0],[0]
"We propose a novel feature selection algorithm called MISSION, a Memory-efficient, Iterative Sketching algorithm for Sparse feature selectION.",1. Introduction,[0],[0]
"MISSION, that takes on all the concerns outlined above.",1. Introduction,[0],[0]
"MISSION matches the accuracy performance of existing large-scale machine learning frameworks like Vowpal Wabbit (VW) (Agarwal et al., 2014) on real-world datasets.",1. Introduction,[0],[0]
"However, in contrast to VW, MISSION can perform feature selection exceptionally well.",1. Introduction,[0],[0]
"Furthermore, MISSION significantly surpasses the performance of classical algorithms such as Iterative Hard Thresholding (IHT), which is currently the popular feature selection alternative concerning the problem sizes we consider.
",1. Introduction,[0],[0]
Contributions:,1. Introduction,[0],[0]
"In this work, we show that the two-decade old Count-Sketch data structure (Charikar et al., 2002) from the streaming algorithms literature is ideally suited for ultra large-scale feature selection.",1. Introduction,[0],[0]
The Count-Sketch data structure enables us to retain the convenience of feature hashing along with the identity of important features.,1. Introduction,[0],[0]
"Moreover, Count-Sketch can accumulate gradients updates over several iterations because of linear aggregation.",1. Introduction,[0],[0]
This aggregation eliminates the problem of myopia associated with existing greedy thresholding approaches.,1. Introduction,[0],[0]
"The aggregation phenomenon also extends to recent parallel works which employ count sketches in streaming domain (Tai et al., 2018).
",1. Introduction,[0],[0]
"In particular, we force the parameters (or feature vector) to reside in a memory-efficient Count-Sketch data structure.",1. Introduction,[0],[0]
SGD gradient updates are easily applied to the Count-Sketch.,1. Introduction,[0],[0]
"Instead of moving in the gradient direction and then greedily projecting into a subspace defined by the regularizer (e.g., in the case of LASSO-based methods), MISSION adds the gradient directly into the Count-Sketch data structure, where it aggregates with all the past updates.",1. Introduction,[0],[0]
See Fig. 1 for the schematic.,1. Introduction,[0],[0]
"At any point of time in the iteration, this data structure stores a compressed, randomized, and noisy sketch of the sum of all the gradient updates, while preserving the information of the heavy-hitters—the coordinates that accumulate the highest amount of energy.",1. Introduction,[0],[0]
"In order to find an estimate of the feature vector, MISSION queries the CountSketch.",1. Introduction,[0],[0]
"The Count-Sketch is used in conjunction with a top-k heap, which explicitly stores the features with the heaviest weights.",1. Introduction,[0],[0]
"Only the features in the top-k heap are considered active, and the rest are set to zero.",1. Introduction,[0],[0]
"However, a representation for every weight is stored, in compressed form, inside the Count-Sketch.
",1. Introduction,[0],[0]
"We demonstrate that MISSION surpasses the sparse recovery performance of classical algorithms such as Iterative Hard Thresholding (IHT), which is the only other method we could run at our scale.",1. Introduction,[0],[0]
"In addition, experiments suggest that the memory requirements of MISSION scale well with the dimensionality p of the problem.",1. Introduction,[0],[0]
"MISSION matches the
accuracy of existing large-scale machine learning frameworks like Vowpal Wabbit (VW) on real-world, large-scale datasets.",1. Introduction,[0],[0]
"Moreover, MISSION achieves comparable or even better accuracy while using significantly fewer features.",1. Introduction,[0],[0]
"In the streaming setting, we are given a high-dimensional vector β ∈",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
Rp that is too costly to store in memory.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
We see only a very long sequence of updates over time.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"The only information available at time t is of the form (i,∆), which means that coordinate i is incremented (or decremented) by the amount ∆. We are given a limited amount of storage, on the order of O(log p), which means that we can never store the entire sequence of updates.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Sketching algorithms aim to estimate the value of current item i, after any number of updates using only O(log p) memory.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Accurate estimation of heavy coordinates is desirable.
",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
Count-Sketch is a popular algorithm for estimation in the streaming setting.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Count-Sketch keeps a matrix of counters (or bins) S of size d×w ∼ O(log p), where d andw are chosen based on the accuracy guarantees.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"The algorithm uses d random hash functions hj for j ∈ {1, 2, ..., d} to map the vector’s components to bins w, hj : {1, 2, ..., p} → {1, 2, ..., w} Every component i of the vector is hashed to d different bins.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"In particular, for any row j of sketch S, component i is hashed into bin S(j, hj(i)).",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"In addition to hj , Count-Sketch uses d random sign functions to map the components of the vectors randomly to {+1, −1}, i.e., si : {1, 2, ..., D} → {+1,−1}.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"An illustration of this sketch data structure with three hash functions in shown inside Fig. 1.
",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
The Count-Sketch supports two operations: UPDATE(item,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"i, increment ∆) and QUERY(item i).",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
The UPDATE operation updates the sketch with any observed increment.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"More formally, for an increment ∆ to an item i, the sketch is updated by adding sj(i)∆ to the cell S(j, hj(i))",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"∀j ∈ {1, 2, ..., d}.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"The QUERY operation returns an estimate for component i, the median of all the d different associated counters.
",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"It has been shown that, for any sequence of streaming updates (addition or subtraction) to the vector β, Count-Sketch provides an unbiased estimate of any component i, β̂i such that the following holds with high probability,
βi",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
− ||β||2 ≤ β̂i ≤,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
βi + ||β||2.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"(1)
It can be shown that the Eq. (1) is sufficient to achieve near-optimal guarantees for sparse recovery with the given space budget.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Furthermore, these guarantees also meet the best compressed sensing lower bounds in terms of the number of counters (or measurements) needed for sparse recovery (Indyk, 2013).",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Consider the feature selection problem in the ultra highdimensional setting: We are given the dataset (Xi, yi) for i ∈",3. Problem Formulation,[0],[0]
"[n] = {1, 2, . . .",3. Problem Formulation,[0],[0]
",",3. Problem Formulation,[0],[0]
"n},",3. Problem Formulation,[0],[0]
where Xi ∈,3. Problem Formulation,[0],[0]
Rp and yi ∈ R denote the ith measured and response variables.,3. Problem Formulation,[0],[0]
We are interested in finding the k-sparse (k non-zero entries) feature vector (or regressor),3. Problem Formulation,[0],[0]
β ∈,3. Problem Formulation,[0],[0]
"Rp from the optimization problem
min ‖β‖0=k
‖y −Xβ‖2, (2)
",3. Problem Formulation,[0],[0]
"where X = {X1,X2, . . .",3. Problem Formulation,[0],[0]
",Xn} and y =",3. Problem Formulation,[0],[0]
"[y1, y1, . . .",3. Problem Formulation,[0],[0]
", yn] denote the data matrix and response vector and the `0-norm ‖β‖0 counts the number of non-zero entries in β.
",3. Problem Formulation,[0],[0]
We are interested in solving the feature selection problem for ultra high-dimensional datasets where the number of features p is so large that a dense vector (or matrix) of size p cannot be stored explicitly in memory.,3. Problem Formulation,[0],[0]
"Among the menagerie of feature selection algorithms, the class of hard thresholding algorithms have the smallest memory footprint: Hard thresholding algorithms retain only the top-k values and indices of the entire feature vector using O(klog(p))",3.1. Hard Thresholding Algorithms,[0],[0]
"memory (Jain et al., 2014; Blumensath & Davies, 2009).",3.1. Hard Thresholding Algorithms,[0],[0]
"The iterative hard thresholding (IHT) algorithm generates the following iterates for the ith variable in an stochastic gradient descent (SGD) framework
βt+1",3.1. Hard Thresholding Algorithms,[0],[0]
← Hk(βt − 2λ,3.1. Hard Thresholding Algorithms,[0],[0]
"( yi −Xiβt )T Xi) (3)
The sparsity of the feature vector βt, enforced by the hard thresholding operator Hk, alleviates the need to store a vector of size O(p) in the memory in order to keep track of the changes of the features over the iterates.
",3.1. Hard Thresholding Algorithms,[0],[0]
"Unfortunately, because it only retains the top-k elements of β, the hard thresholding procedure greedily discards the information of the non top-k coordinates from the previous iteration.",3.1. Hard Thresholding Algorithms,[0],[0]
"In particular, it clips off coordinates that might add to the support set in later iterations.",3.1. Hard Thresholding Algorithms,[0],[0]
"This drastically affects the performance of hard thresholding algorithms in realworld scenarios where the design matrix X is not random, normalized, or well-conditioned.",3.1. Hard Thresholding Algorithms,[0],[0]
"In this regime, the gradient terms corresponding to the true support typically arrive in lagging order and are prematurely clipped in early iterations by Hk.",3.1. Hard Thresholding Algorithms,[0],[0]
"The effect of these lagging gradients is present even in the SGD framework, because the gradients are quite noisy, and only a small fraction of the energy of the true gradient is expressed in each iteration.",3.1. Hard Thresholding Algorithms,[0],[0]
"It is not difficult to see that these small energy, high noise signals can easily cause the greedy hard thresholding operator to make sub-optimal or incorrect decisions.",3.1. Hard Thresholding Algorithms,[0],[0]
"Ideally, we want to accumulate the gradients to get enough confidence in signal and to average out any noise.
",3.1. Hard Thresholding Algorithms,[0],[0]
"Algorithm 1 MISSION Initialize: β0 = 0, S (Count-Sketch), λ (Learning Rate) while not stopping criteria do
Find the gradient update gi = λ",3.1. Hard Thresholding Algorithms,[0],[0]
( 2 (yi −Xiβt) T Xi ),3.1. Hard Thresholding Algorithms,[0],[0]
Add the gradient update to the sketch gi → S Get the top-k heavy-hitters from the sketch βt+1,3.1. Hard Thresholding Algorithms,[0],[0]
"← S end while Return: The top-k heavy-hitters from the Count-Sketch
This aforementioned problem is in fact symptomatic of all other thresholding variants including the Iterative algorithm with inversion (ITI) (Maleki, 2009) and the Partial hard thresholding (PHT) algorithm (Jain et al., 2017).",3.1. Hard Thresholding Algorithms,[0],[0]
We now describe the MISSION algorithm.,4. The MISSION Algorithm,[0],[0]
"First, we initialize the Count-Sketch S and the feature vector βt=0 with zeros entries.",4. The MISSION Algorithm,[0],[0]
The Count-Sketch hashes a p-dimensional vector into O(log2p) buckets (Recall Fig. 1).,4. The MISSION Algorithm,[0],[0]
"We discuss this particular choice for the size of the Count-Sketch and the memory-accuracy trade offs of MISSION in Sections 5.3 and 6.1.
",4. The MISSION Algorithm,[0],[0]
"At iteration t, MISSION selects a random row Xi from the data matrix X and computes the stochastic gradient update term using the learning rate λ via gi = 2λ",4. The MISSION Algorithm,[0],[0]
(yi −Xiβt) T Xi i.e. the usual gradient update that minimizes the unconstrained quadratic loss ‖y−Xβ‖22.,4. The MISSION Algorithm,[0],[0]
The data vector Xi and the corresponding stochastic gradient term are sparse.,4. The MISSION Algorithm,[0],[0]
"We then add the non-zero entries of the stochastic gradient term {gij : ∀j gij > 0} to the Count-Sketch S. Next, MISSION queries the top-k values of the sketch to form βt+1.",4. The MISSION Algorithm,[0],[0]
We repeat the same procedure until convergence.,4. The MISSION Algorithm,[0],[0]
MISSION returns the top-k values of the Count-Sketch as the final output of the algorithm.,4. The MISSION Algorithm,[0],[0]
"The MISSION algorithm is detailed in Alg. 1. MISSION easily extends to other loss functions such as the hinge loss and logistic loss.
",4. The MISSION Algorithm,[0],[0]
MISSION is Different from Greedy Thresholding:,4. The MISSION Algorithm,[0],[0]
Denote the gradient vector update at any iteration t as ut.,4. The MISSION Algorithm,[0],[0]
"It is not difficult to see that starting with an all-zero vector β0, at any point of time t, the Count-Sketch state is equivalent to the sketch of the vector ∑t i=1",4. The MISSION Algorithm,[0],[0]
ut.,4. The MISSION Algorithm,[0],[0]
"In other words, the sketch aggregates the compressed aggregated vector.",4. The MISSION Algorithm,[0],[0]
"Thus, even if an individual SGD update is noisy and contains small signal energy, thresholding the Count-Sketch is based on the average update over time.",4. The MISSION Algorithm,[0],[0]
This averaging produces a robust signal that cancels out the noise.,4. The MISSION Algorithm,[0],[0]
"We can therefore expect MISSION to be superior over thresholding.
",4. The MISSION Algorithm,[0],[0]
"In the supplementary materials, we present initial theoretical results on the convergence of MISSION.",4. The MISSION Algorithm,[0],[0]
"Our results show
that, under certain assumptions, the full-gradient-descent version of MISSION converges geometrically to the true parameter β ∈",4. The MISSION Algorithm,[0],[0]
Rp up to some additive constants.,4. The MISSION Algorithm,[0],[0]
"The exploration of these assumptions and the extension to the SGD version of MISSION are exciting avenues for future work.
",4. The MISSION Algorithm,[0],[0]
"Feature Selection with the Ease of Feature Hashing: As argued earlier, the features are usually represented with strings, and we do not have the capability to map each string to a unique index in a vector without spendingO(p) memory.",4. The MISSION Algorithm,[0],[0]
"Feature hashing is convenient, because we can directly access every feature using hashes.",4. The MISSION Algorithm,[0],[0]
We can use any lossy hash function for strings.,4. The MISSION Algorithm,[0],[0]
MISSION only needs a few independent hash functions (3 in our Count-Sketch implementation) to access any component.,4. The MISSION Algorithm,[0],[0]
"The top-k estimation is done efficiently using a heap data structure of size k. Overall, we only access the data using efficient hash functions, which can be easily implemented in large-scale systems.",4. The MISSION Algorithm,[0],[0]
We designed a set of simulations to evaluate MISSION in a controlled setting.,5. Simulations,[0],[0]
"In contrast to the ultra large-scale, real-world experiments of Section 6, in the section the data matrices are drawn from a random Gaussian distribution and the ground truth features are known.",5. Simulations,[0],[0]
We first demonstrate the advantage of MISSION over greedy thresholding in feature selection.,5.1. Phase Transition,[0],[0]
"For this experiment, we modify MISSION slightly to find the root of the algorithmic advantage of MISSION: we replace the Count-Sketch with an “identity” sketch, or a sketch with a single hash function, h(i) = i.",5.1. Phase Transition,[0],[0]
"In doing so, we eliminate the complexity that Count-Sketch adds to the algorithm, so that the main difference between MISSION and IHT is that MISSION accumulates the gradients.",5.1. Phase Transition,[0],[0]
"To improve stability, we scale the non top-k elements of S by a factor γ ∈ (0, 1) that begins very near 1 and is gradually decreased until the algorithm converges.",5.1. Phase Transition,[0],[0]
"It is also possible to do this scaling in the CountSketch version of MISSION efficiently by exploiting the linearity of the sketch.
",5.1. Phase Transition,[0],[0]
Fig. 2 illustrates the empirical phase transition curves for sparse recovery using MISSION and the hard thresholding algorithms.,5.1. Phase Transition,[0],[0]
The phase transition curves show the points where the algorithm successfully recovers the features in > 50% of the random trails.,5.1. Phase Transition,[0],[0]
"MISSION shows a better phase transition curve compared to IHT by a considerable gap.
",5.1. Phase Transition,[0],[0]
Table 1.,5.1. Phase Transition,[0],[0]
Comparison of MISSION against hard thresholding algorithms in feature selection under adversarial effects.,5.1. Phase Transition,[0],[0]
We first report the percentage of instances in which the algorithms accurately find the solution (ACC) with no attenuation (α = 1) over 100 random trials.,5.1. Phase Transition,[0],[0]
"We then report the mean of the maximum level of attenuation α applied to the columns of design X before the algorithms fail to recover the support of β (over the trials that all algorithms can find the solution with α = 1).
",5.1. Phase Transition,[0],[0]
"(n, k) MISSION IHT ITI PHT",5.1. Phase Transition,[0],[0]
ACCα=1 α,5.1. Phase Transition,[0],[0]
ACCα=1 α,5.1. Phase Transition,[0],[0]
ACCα=1 α,5.1. Phase Transition,[0],[0]
"ACCα=1 α (100, 2) 100% 2.68 ± 0.37 100% 1.49 ± 0.33 91% 1.33 ± 0.23 64% 2.42 ± 0.87 (100, 3) 100% 2.52 ± 0.36 92% 1.36 ± 0.46 70% 1.15 ± 0.20 42% 2.05 ± 0.93 (100, 4) 100% 2.53 ± 0.23 72% 1.92 ± 0.91 37% 1.03 ± 0.09 39% 2.13 ± 1.07 (200, 5) 100% 4.07 ± 0.36 99",5.1. Phase Transition,[0],[0]
"% 2.34 ± 1.12 37% 1.15 ± 0.22 83% 2.75 ± 1.30 (200, 6) 100% 4.17 ± 0.24 97% 2.64 ± 1.14 23% 1.11 ± 0.12 73% 2.26 ± 1.33 (200, 7) 100% 4.07 ± 0.11 83% 1.64 ± 1.01 14% 1.11 ± 0.12 75% 3.39 ± 1.36
0.0 0.2 0.4 0.6 0.8 1.0
n/p
0.0
0.2
0.4
0.6
0.8
1.0
s/ n
MISSION IHT ITI",5.1. Phase Transition,[0],[0]
"PHT
Figure 2.",5.1. Phase Transition,[0],[0]
Empirical phase transition in recovering a binary feature vector β in p = 1000-dimensional space with a Gaussian data matrix X.,5.1. Phase Transition,[0],[0]
We illustrate the empirical 50% probability of success curves averaged over T = 20 trials.,5.1. Phase Transition,[0],[0]
MISSION outperforms the thresholding algorithms by a large margin.,5.1. Phase Transition,[0],[0]
"A major problem with the IHT algorithm, especially in largescale SGD settings, is with thresholding the coordinates with small gradients in the earlier iterations.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"IHT misses these coordinates, since they become prominent only after the gradients accumulate with the progression of the algorithm.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"The problem is amplified with noisy gradient updates such as SGD, which is unavoidable for large datasets.
",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
This phenomenon occurs frequently in sparse recovery problems.,5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"For example, when the coordinates that correspond to the columns of the data matrix with smaller energy lag in the iterations of gradient descent algorithm, IHT thresholds these lagging-gradient coordinates in first few iterations, and they never show up again in the support.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"In contrast, MISSION retains a footprint of the gradients of all the previous iterations in the Count-Sketch.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"When the total sum of the gradient of a coordinate becomes prominent, the coordinate joins the support after querying the top-k heavy hitters from the Count-Sketch.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
We illustrate this phenomena in sparse recovery using synthetic experiments.,5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"We recover sparse vector β from its random linear measurements y = Xβ, where the energy of X is imbalanced across its columns.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"In this case, the gradients corresponding to the
columns (coordinates) with smaller energy typically lag and are thresholded by IHT.
",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"To this end, we first construct a random Gaussian data matrix X ∈ R900×1000, pick a sparse vector β that is supported on an index set I , and then attenuate the energy of the columns of X supported by the indices in I by an attenuation factor of α = {1, 1.25, 1.5, 1.75, 2, . . .",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
", 5}.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
Note that α = 1 implies that no attenuation is applied to the matrix.,5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"In Table 1, we report the maximum attenuation level applied to a column of data matrix X before the algorithms fail to fully recover the support set I from y = βX.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"We observe that MISSION is consistently and up to three times more robust against adversarial attenuation of the columns of the data matrix in various design settings.
",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
The robustness of MISSION to the attenuation of the columns of X in sparse recovery task suggests that the Count-Sketch data structure enables gradient-based optimization methods such as IHT to store a footprint (or sketch) of all the gradients from the previous iterations and deliver them back when they become prominent.,5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"in MISSION
In this section we demonstrate that the memory requirements of MISSION grows polylogarithmically in the dimension of the problem p.",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
We conduct a feature selection experiment with a data matrix X ∈ R100×p whose entries are drawn from i.i.d. random Gaussian distributions with zero mean and unit variance.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"We run MISSION and IHT to recover the feature vector β from the output vector y = Xβ, where the feature vector β is a k = 5-sparse vector with random support.",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
We repeat the same experiment 1000 times with different realizations for the sparse feature vector β and report the results in Fig. 3.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
The left plot illustrates the feature selection accuracy of the algorithms as the dimension of the problem p grows.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"The right plot illustrates the minimum memory requirements of the algorithms to recover the features with 100% accuracy.
",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
The size of the Count-Sketch in MISSION scales only polylogarithmically with the dimension of the problem.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
This is surprising since the aggregate gradient in a classical SGD framework becomes typically dense in early iterations and thus requires a memory of order O(p).,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"MISSION, however, stores only the essential information of the features in the sketch using a poly-logarithmic sketch size.",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
Note that IHT sacrifices accuracy to achieve a small memory footprint.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
At every iteration IHT eliminates all the information except for the top-k features.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"We observe that, using only a logarithmic factor more memory, MISSION has a significant advantage over IHT in recovering the ground truth features.",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"All experiments were performed on a single machine, 2x Intel Xeon E5-2660 v4 processors (28 cores / 56 threads) with 512 GB of memory.",6. Experiments,[0],[0]
The code1 for training and running our randomized-hashing approach is available online.,6. Experiments,[0],[0]
"We designed the experiments to answer these questions:
1.",6. Experiments,[0],[0]
Does MISSION outperform IHT in terms of classification accuracy?,6. Experiments,[0],[0]
"In particular, how much does myopic thresholding affect IHT in practice?
2.",6. Experiments,[0],[0]
"How well does MISSION match the speed and accuracy of feature hashing (FH)?
3.",6. Experiments,[0],[0]
"How does changing the number of top-k features affect the accuracy and behaviour of the different methods?
4.",6. Experiments,[0],[0]
"What is the effect of changing the memory size of the Count-Sketch data structure on the classification accuracy of MISSION in read-world datasets?
5.",6. Experiments,[0],[0]
"Does MISSION scale well in comparison to the different methods on the ultra large-scale datasets (> 350 GB in size)?
1https://github.com/rdspring1/MISSION",6. Experiments,[0],[0]
"Datasets: We used four datasets in the experiments: 1) KDD2012, 2) RCV1, 3) Webspam–Trigram, 4) DNA2.",6.1. Large-scale Feature Extraction,[0],[0]
"The statistics of these datasets are summarized in Table 2.
",6.1. Large-scale Feature Extraction,[0],[0]
The DNA metagenomics dataset is a multi-class classification task where the model must classify 15 different bacteria species using DNA K-mers.,6.1. Large-scale Feature Extraction,[0],[0]
We sub-sampled the first 15 species from the original dataset containing 193 species.,6.1. Large-scale Feature Extraction,[0],[0]
We use all of the species in the DNA Metagenomics dataset for the large-scale experiments (See Section 6.2).,6.1. Large-scale Feature Extraction,[0],[0]
"Following standard procedures, each bacterial species is associated with a reference genome.",6.1. Large-scale Feature Extraction,[0],[0]
Fragments are sampled from the reference genome until each nucleotide is covered c times on average.,6.1. Large-scale Feature Extraction,[0],[0]
The fragments are then divided into K-mer sub-strings.,6.1. Large-scale Feature Extraction,[0],[0]
We used fragments of length 200 and K-mers of length 12.,6.1. Large-scale Feature Extraction,[0],[0]
"Each model was trained and tested with mean coverage c = {0.1, 1} respectively.",6.1. Large-scale Feature Extraction,[0],[0]
"For more details, see (Vervier et al., 2016).",6.1. Large-scale Feature Extraction,[0],[0]
"The feature extraction task is to find the DNA K-mers that best represent each bacteria class.
",6.1. Large-scale Feature Extraction,[0],[0]
"We implemented the following approaches to compare and contrast against our approach: For all methods, we used the logistic loss for binary classification and the cross-entropy loss for multi-class classification.
MISSION:",6.1. Large-scale Feature Extraction,[0],[0]
As described in Section 4.,6.1. Large-scale Feature Extraction,[0],[0]
"Iterative Hard Thresholding (IHT): An algorithm where, after each gradient update, a hard threshold is applied to the features.",6.1. Large-scale Feature Extraction,[0],[0]
"Only the top-k features are kept active, while the rest are set to zero.",6.1. Large-scale Feature Extraction,[0],[0]
"Since the features are strings or integers, we used a sorted heap to store and manipulate the top-k elements.",6.1. Large-scale Feature Extraction,[0],[0]
This was the only algorithm we could successfully run over the large datasets on our single machine.,6.1. Large-scale Feature Extraction,[0],[0]
Batch IHT: A modification to IHT that uses mini-batches such that the gradient sparsity is the same as the number of elements in the count-sketch.,6.1. Large-scale Feature Extraction,[0],[0]
We accumulate features and then sort and prune to find the top-k features.,6.1. Large-scale Feature Extraction,[0],[0]
"This accumulate, sort, prune process is repeated several times during training.",6.1. Large-scale Feature Extraction,[0],[0]
Note:,6.1. Large-scale Feature Extraction,[0],[0]
"This setup requires significantly more memory than MISSION, because it explicitly stores the feature strings.",6.1. Large-scale Feature Extraction,[0],[0]
The memory cost of maintaining a set of string features can be orders of magnitude more than the flat array used by MISSION.,6.1. Large-scale Feature Extraction,[0],[0]
"See Bloom Filters (Broder & Mitzenmacher, 2004) and related literature.",6.1. Large-scale Feature Extraction,[0],[0]
"This setup is not scalable to large-scale datasets.
",6.1. Large-scale Feature Extraction,[0],[0]
"2http://projects.cbio.mines-paristech.fr/ largescalemetagenomics/
Feature Hashing (FH): A standard machine learning algorithm for dimensionality reduction that reduces the memory cost associated with large datasets.",6.1. Large-scale Feature Extraction,[0],[0]
FH is not a feature selection algorithm and cannot identify important features.,6.1. Large-scale Feature Extraction,[0],[0]
"(Agarwal et al., 2014)
",6.1. Large-scale Feature Extraction,[0],[0]
Experimental Settings: The MISSION and IHT algorithms searched for the same number of top-k features.,6.1. Large-scale Feature Extraction,[0],[0]
"To ensure fair comparisons, the size of the Count-Sketch and the feature vector allocated for the FH model were equal.",6.1. Large-scale Feature Extraction,[0],[0]
The size of the MISSION and FH models were set to the nearest power of 2 greater than the number of features in the dataset.,6.1. Large-scale Feature Extraction,[0],[0]
"For all the experiments, the Count-Sketch data structure used 3 hash functions, and the model weights were divided equally among the hash arrays.",6.1. Large-scale Feature Extraction,[0],[0]
"For example, with the (Tiny) DNA metagenomics dataset, we allocated 24 bits or 16,777,216 weights for the FH model.",6.1. Large-scale Feature Extraction,[0],[0]
"Given 3 hash functions and 15 classes, roughly 372,827 elements were allocated for each class in the Count-Sketch.
MISSION, IHT, FH Comparison: Fig. 4 shows that MISSION surpasses IHT in classification accuracy in all four datasets, regardless of the number of features.",6.1. Large-scale Feature Extraction,[0],[0]
"In addition, MISSION closely matches FH, which is significant because FH is allowed to model a much larger set of features than MISSION or IHT.",6.1. Large-scale Feature Extraction,[0],[0]
"MISSION is 2–4× slower than FH, which is expected given that MISSION has the extra overhead of using a heap to track the top-k features.
MISSION’s accuracy rapidly rises with respect to the number of top-k features, while IHT’s accuracy plateaus and then grows slowly to match MISSION.",6.1. Large-scale Feature Extraction,[0],[0]
This observation corroborates our insight that the greedy nature of IHT hurts performance.,6.1. Large-scale Feature Extraction,[0],[0]
"When the number of top-k elements is small, the capacity of IHT is limited, so it picks the first set of features that provides good performance, ignoring the rest.",6.1. Large-scale Feature Extraction,[0],[0]
"On the other hand, MISSION decouples the memory from the top-k ranking, which is based on the aggregated gradients in the compressed sketch.",6.1. Large-scale Feature Extraction,[0],[0]
"By the linear property of the count-sketch, this ensures that the heavier entries occur in the top-k features with high probability.
",6.1. Large-scale Feature Extraction,[0],[0]
"Count-Sketch Memory Trade-Off: Fig. 5 shows how MISSION’s accuracy degrades gracefully, as the size of the Count-Sketch decreases.",6.1. Large-scale Feature Extraction,[0],[0]
"In this experiment, MISSION only used the top 500K features for classifying the Tiny DNA metagenomics dataset.",6.1. Large-scale Feature Extraction,[0],[0]
"When the top-k to Count-Sketch ratio is 1, then 500K weights were allocated for each class and hash array in the Count-Sketch data structure.",6.1. Large-scale Feature Extraction,[0],[0]
"The Batch IHT baseline was given 8,388,608 memory elements per class, enabling it to accumulate a significant number of features before thresholding to find the top-k features.",6.1. Large-scale Feature Extraction,[0],[0]
"This experiment shows that MISSION immediately outperforms IHT and Batch IHT, once the top-k to Count-Sketch ratio is 1:1.",6.1. Large-scale Feature Extraction,[0],[0]
"Thus, MISSION provides a unique memory-accuracy knob at any given value of top-k.",6.1. Large-scale Feature Extraction,[0],[0]
"Here we demonstrate that MISSION can extract features from three large-scale datasets: Criteo 1TB, Splice-Site, and DNA Metagenomics.
",6.2. Ultra Large-Scale Feature Selection,[0],[0]
Criteo 1TB: The Criteo 1TB3 dataset represents 24 days of click-through logs—23 days (training) + 1 day (testing).,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The task for this dataset is click-through rate (CTR) prediction— How likely is a user to click an ad?,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The dataset contains over 4 billion (training) and 175 million (testing) examples (2.5 TB of disk space).,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The performance metric is Area Under the ROC Curve (AUC).,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The VW baseline4 achieved 0.7570 AUC score.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
"MISSION and IHT scored close to the VW baseline with 0.751 AUC using only the top 250K features.
",6.2. Ultra Large-Scale Feature Selection,[0],[0]
Splice-Site: The task for this dataset is to distinguish between true and fake splice sites using the local context around the splice site in-question.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
"The dataset is highly skewed (few positive, many negative values), and so the performance metric is average precision (AP).",6.2. Ultra Large-Scale Feature Selection,[0],[0]
Average precision is the precision score averaged over all recall scores ranging from 0 to 1.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The dataset contains over 50 million (training) and 4.6 million (testing) examples (3.2 TB of disk space).,6.2. Ultra Large-Scale Feature Selection,[0],[0]
All the methods were trained for a single epoch with a learning rate of 0.5.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
"MISSION, Batch IHT, and SGD IHT tracked the top 16,384 features.",6.2. Ultra Large-Scale Feature Selection,[0],[0]
"FH, MISSION, and Batch IHT used 786,432 extra memory elements.",6.2. Ultra Large-Scale Feature Selection,[0],[0]
MISSION significantly outperforms Batch IHT and SGD IHT by 2.3%.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
"Also, unlike in Fig. 5, the extra memory did not help Batch IHT, since it performed the same as SGD IHT.",6.2. Ultra Large-Scale Feature Selection,[0],[0]
MISSION (17.5 hours) is 15% slower than FH (15 hours) in wall-clock running time.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
DNA Metagenomics:,AP 0.522 0.510 0.498 0.498,[0],[0]
This experiment evaluates MISSION’s performance on a medium-sized metagenomics dataset.,AP 0.522 0.510 0.498 0.498,[0],[0]
"The parameters from the Tiny (15 species) dataset in Section 6.1 are shared with this experiment, except the
3https://www.kaggle.com/c/criteo-display-ad-challenge 4https://github.com/rambler-digital-solutions/
criteo-1tb-benchmark
DNA - Tiny (15 Species) - Top-K: 500K
number of species is increased to 193.",AP 0.522 0.510 0.498 0.498,[0],[0]
The size of a sample batch with mean coverage c = 1 increased from 7 GB (Tiny) to 68 GB (Medium).,AP 0.522 0.510 0.498 0.498,[0],[0]
Each round (mean coverage c = 0.25) contains 3.45 million examples and about 16.93 million unique non-zero features (p).,AP 0.522 0.510 0.498 0.498,[0],[0]
MISSION and IHT tracked the top 2.5 million features per class.,AP 0.522 0.510 0.498 0.498,[0],[0]
"The FH baseline used 231 weights, about 11.1 million weights per class, and we allocated the same amount of space for the Count-Sketch.",AP 0.522 0.510 0.498 0.498,[0],[0]
"Each model was trained on a dataset with coverage c = 5.
",AP 0.522 0.510 0.498 0.498,[0],[0]
"Fig. 6 shows the evolution of classification accuracy over time for MISSION, IHT, and the FH baseline.",AP 0.522 0.510 0.498 0.498,[0],[0]
"After 5 epochs, MISSION closely matches the FH baseline.",AP 0.522 0.510 0.498 0.498,[0],[0]
"Note: MISSION converges faster than IHT such that MISSION is 1–4 rounds ahead of IHT, with the gap gradually increasing over time.",AP 0.522 0.510 0.498 0.498,[0],[0]
"On average, the running time of MISSION is 1–2× slower than IHT.",AP 0.522 0.510 0.498 0.498,[0],[0]
"However, this experiment demonstrates that since MISSION converges faster, it actually needs less time to reach a certain accuracy level.",AP 0.522 0.510 0.498 0.498,[0],[0]
"Therefore, MISSION is effectively faster and more accurate than IHT.",AP 0.522 0.510 0.498 0.498,[0],[0]
"Scalability and Parallelism: IHT finds the top-k features after each gradient update, which requires sorting the features based on their weights before thresholding.",7. Implementation Details and Discussion,[0],[0]
"The speed of the sorting process is improved by using a heap data
DNA - Medium (193 Species) - Top-K: 2.5M
structure, but it is still costly per update.",7. Implementation Details and Discussion,[0],[0]
"MISSION also uses a heap to store its top-k elements, but it achieves the same accuracy as IHT with far fewer top-k elements because of the Count-Sketch.",7. Implementation Details and Discussion,[0],[0]
"(Recall Section 4)
",7. Implementation Details and Discussion,[0],[0]
Another suggested improvement for the top-k heap is to use lazy updates.,7. Implementation Details and Discussion,[0],[0]
"Updating the weight of a feature does not change its position in the heap very often, but still requires an O(log n) operation.",7. Implementation Details and Discussion,[0],[0]
"With lazy updates, the heap is updated only if it the change is significant.",7. Implementation Details and Discussion,[0],[0]
|xt,7. Implementation Details and Discussion,[0],[0]
"− x0| ≥ , i.e. the new weight at time t exceeds the original value by some threshold.",7. Implementation Details and Discussion,[0],[0]
This tweak significantly reduces the number of heap updates at the cost of slightly distorting the heap.,7. Implementation Details and Discussion,[0],[0]
"In this paper, we presented MISSION, a new framework for ultra large-scale feature selection which maintains an efficient, approximate representation for the features using a Count-Sketch data structure.",8. Conclusion and Future Work,[0],[0]
"MISSION retains the simplicity of feature hashing without sacrificing the interpretability of the features.
",8. Conclusion and Future Work,[0],[0]
"Going forward, we are interested in leveraging our MISSION framework to explore pairwise or higher-order interaction features.",8. Conclusion and Future Work,[0],[0]
"Interaction features are important for scientific discovery, e.g., in genomics (Basu et al., 2018), however, the exponential dimensionality growth of interactions has hindered further progress.",8. Conclusion and Future Work,[0],[0]
We believe MISSION will enable more scientific discoveries from big data in future.,8. Conclusion and Future Work,[0],[0]
"AAA, DL, GD, and RB were supported by the DOD Vannevar Bush Faculty Fellowship grant N00014-18-1-2047, NSF grant CCF-1527501, ARO grant W911NF-15-1-0316, AFOSR grant FA9550-14-1-0088, ONR grant N00014-17-12551, DARPA REVEAL grant HR0011-16-C-0028, and an ONR BRC grant for Randomized Numerical Linear Algebra.",Acknowledgements,[0],[0]
"RS and AS were supported by NSF-1652131, AFOSR-YIP FA9550-18-1-0152, and ONR BRC grant for Randomized Numerical Linear Algebra.",Acknowledgements,[0],[0]
The authors would also like to thank NVIDIA and Amazon for gifting computing resources.,Acknowledgements,[0],[0]
Feature selection is an important challenge in machine learning.,abstractText,[0],[0]
It plays a crucial role in the explainability of machine-driven decisions that are rapidly permeating throughout modern society.,abstractText,[0],[0]
"Unfortunately, the explosion in the size and dimensionality of real-world datasets poses a severe challenge to standard feature selection algorithms.",abstractText,[0],[0]
"Today, it is not uncommon for datasets to have billions of dimensions.",abstractText,[0],[0]
"At such scale, even storing the feature vector is impossible, causing most existing feature selection methods to fail.",abstractText,[0],[0]
"Workarounds like feature hashing, a standard approach to large-scale machine learning, helps with the computational feasibility, but at the cost of losing the interpretability of features.",abstractText,[0],[0]
"In this paper, we present MISSION, a novel framework for ultra large-scale feature selection that performs stochastic gradient descent while maintaining an efficient representation of the features in memory using a Count-Sketch data structure.",abstractText,[0],[0]
MISSION retains the simplicity of feature hashing without sacrificing the interpretability of the features while using only O(log p) working memory.,abstractText,[0],[0]
"We demonstrate that MISSION accurately and efficiently performs feature selection on real-world, large-scale datasets with billions of dimensions.",abstractText,[0],[0]
MISSION: Ultra Large-Scale Feature Selection using Count-Sketches,title,[0],[0]
Many modern data sets consist of data that is gathered adaptively: the choice of whether to collect more data points of a given type depends on the data already collected.,1. Introduction,[0],[0]
"For example, it is common in industry to conduct “A/B” tests to make decisions about many things, including ad targeting, user interface design, and algorithmic modifications, and this A/B testing is often conducted using “bandit learning algorithms”",1. Introduction,[0],[0]
"(Bubeck et al., 2012), which adaptively select treatments to show to users in an effort to find the best treatment as quickly as possible.",1. Introduction,[0],[0]
"Similarly, sequen-
1Department of Statistics, The Wharton School, University of Pennsylvania 2Department of Computer Science, University of Pennsylvania.",1. Introduction,[0],[0]
Correspondence to: Seth Neel,1. Introduction,[0],[0]
"<sethneel93@gmail.com>, Aaron Roth <aaroth@cis.upenn.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"1This extended abstract is missing many details, proofs, and results that can be found in the full version (Neel & Roth, 2018).
",1. Introduction,[0],[0]
"tial clinical trials may halt or re-allocate certain treatment groups due to preliminary results, and empirical scientists may initially try and test multiple hypotheses and multiple treatments, but then decide to gather more data in support of certain hypotheses and not others, based on the results of preliminary statistical tests.
",1. Introduction,[0],[0]
"Unfortunately, as demonstrated by (Nie et al., 2017), the data that results from adaptive data gathering procedures will often exhibit substantial bias.",1. Introduction,[0],[0]
"As a result, subsequent analyses that are conducted on the data gathered by adaptive procedures will be prone to error, unless the bias is explicitly taken into account.",1. Introduction,[0],[0]
This can be difficult.,1. Introduction,[0],[0]
"(Nie et al., 2017) give a selective inference approach: in simple stochastic bandit settings, if the data was gathered by a specific stochastic algorithm that they design, they give an MCMC based procedure to perform maximum likelihood estimation to recover de-biased estimates of the underlying distribution means.",1. Introduction,[0],[0]
"In this paper, we give a related, but orthogonal approach whose simplicity allows for a substantial generalization beyond the simple stochastic bandits setting.",1. Introduction,[0],[0]
"We show that in very general settings, if the data is gathered by a differentially private procedure, then we can place strong bounds on the bias of the data gathered, without needing any additional de-biasing procedure.",1. Introduction,[0],[0]
"Via elementary techniques, this connection implies the existence of simple stochastic bandit algorithms with nearly optimal worst-case regret bounds, with very strong bias guarantees.",1. Introduction,[0],[0]
"By leveraging existing connections between differential privacy and adaptive data analysis (Dwork et al., 2015c; Bassily et al., 2016; Rogers et al., 2016), we can extend the generality of our approach to bound not just bias, but to correct for effects of adaptivity on arbitrary statistics of the gathered data.",1. Introduction,[0],[0]
"Since the data being gathered will generally be useful for some as yet unspecified scientific analysis, rather than just for the narrow problem of mean estimation, our technique allows for substantially broader possibilities compared to past approaches.",1. Introduction,[0],[0]
"This paper has three main contributions:
1.",1.1. Our Results,[0],[0]
"Using elementary techniques, we provide explicit bounds on the bias of empirical arm means maintained by bandit algorithms in the simple stochastic
setting that make their selection decisions as a differentially private function of their observations.",1.1. Our Results,[0],[0]
"Together with existing differentially private algorithms for stochastic bandit problems, this yields an algorithm that obtains an essentially optimal worst-case regret bound, and guarantees minimal bias (on the order of O(1/ √ K · T )) for the empirical mean maintained for every arm.",1.1. Our Results,[0],[0]
"In the full version (Neel & Roth, 2018), we also extend our results to the linear contextual bandit problem, proving new bounds for a private linear UCB algorithm along the way.
2.",1.1. Our Results,[0],[0]
"We then make a general observation, relating adaptive data gathering to an adaptive analysis of a fixed dataset (in which the choice of which query to pose to the dataset is adaptive).",1.1. Our Results,[0],[0]
This lets us apply the large existing literature connecting differential privacy to adaptive data analysis.,1.1. Our Results,[0],[0]
"In particular, it lets us apply the max-information bounds of (Dwork et al., 2015b; Rogers et al., 2016) to our adaptive data gathering setting.",1.1. Our Results,[0],[0]
"This allows us to give much more general guarantees about the data collected by differentially private collection procedures, that extend well beyond bias.",1.1. Our Results,[0],[0]
"For example, it lets us correct the p-values for arbitrary hypothesis tests run on the gathered data.
",1.1. Our Results,[0],[0]
3.,1.1. Our Results,[0],[0]
"Finally, we run a set of experiments that measure the bias incurred by the standard UCB algorithm in the stochastic bandit setting, contrast it with the low bias obtained by a private UCB algorithm, and show that there are settings of the privacy parameter that simultaneously can make bias statistically insignificant, while having competitive empirical regret with the non-private UCB algorithm.",1.1. Our Results,[0],[0]
We also demonstrate in the linear contextual bandit setting how failing to correct for adaptivity can lead to false discovery when applying t-tests for non-zero regression coefficients on an adaptively gathered dataset.,1.1. Our Results,[0],[0]
This paper bridges two recent lines of work.,1.2. Related Work,[0],[0]
"Our starting point is two recent papers: (Villar et al., 2015) empirically demonstrate in the context of clinical trials that a variety of simple stochastic bandit algorithms produce biased sample mean estimates (Similar results have been empirically observed in the context of contextual bandits (Dimakopoulou et al., 2017)).",1.2. Related Work,[0],[0]
"(Nie et al., 2017) prove that simple stochastic bandit algorithms that exhibit two natural properties (satisfied by most commonly used algorithms, including UCB and Thompson Sampling) result in empirical means that exhibit negative bias.",1.2. Related Work,[0],[0]
"They then propose a heuristic algorithm which computes a maximum likelihood estimator for the sample means from the empirical means gathered by a modified UCB algorithm which adds Gumbel noise to
the decision statistics.",1.2. Related Work,[0],[0]
"(Deshpande et al., 2017) propose a debiasing procedure for ordinary least-squares estimates computed from adaptively gathered data that trades off bias for variance, and prove a central limit theorem for their method.",1.2. Related Work,[0],[0]
"In contrast, the methods we propose in this paper are quite different.",1.2. Related Work,[0],[0]
"Rather than giving an ex-post debiasing procedure, we show that if the data were gathered in a differentially private manner, no debiasing is necessary.",1.2. Related Work,[0],[0]
"The strength of our method is both in its simplicity and generality: rather than proving theorems specific to particular estimators, we give methods to correct the p-values for arbitrary hypothesis tests that might be run on the adaptively gathered data.
",1.2. Related Work,[0],[0]
"The second line of work is the recent literature on adaptive data analysis (Dwork et al., 2015c;b; Hardt & Ullman, 2014; Steinke & Ullman, 2015; Russo & Zou, 2016; Wang et al., 2016; Bassily et al., 2016; Hardt & Blum, 2015; Cummings et al., 2016; Feldman & Steinke, 2017a;b) which draws a connection between differential privacy (Dwork et al., 2006) and generalization guarantees for adaptively chosen statistics.",1.2. Related Work,[0],[0]
"The adaptivity in this setting is dual to the setting we study in the present paper: In the adaptive data analysis literature, the dataset itself is fixed, and the goal is to find techniques that can mitigate bias due to the adaptive selection of analyses.",1.2. Related Work,[0],[0]
"In contrast, here, we study a setting in which the data gathering procedure is itself adaptive, and can lead to bias even for a fixed set of statistics of interest.",1.2. Related Work,[0],[0]
"However, we show that adaptive data gathering can be re-cast as an adaptive data analysis procedure, and so the results from the adaptive data analysis literature can be ported over.",1.2. Related Work,[0],[0]
"In a simple stochastic bandit problem, there are K unknown distributions Pi over the unit interval [0,1], each with (unknown) mean µi.",2.1. Simple Stochastic Bandit Problems,[0],[0]
"Over a series of rounds t ∈ {1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", T}, an algorithm A chooses an arm it ∈",2.1. Simple Stochastic Bandit Problems,[0],[0]
"[K], and observes a reward yit,t ∼ Pit .",2.1. Simple Stochastic Bandit Problems,[0],[0]
"Given a sequence of choices i1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", iT , the pseudo-regret of an algorithm is defined to be:
Regret((P1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", PK), i1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", iT )",2.1. Simple Stochastic Bandit Problems,[0],[0]
= T ·max i µi − T∑ t=1,2.1. Simple Stochastic Bandit Problems,[0],[0]
"µit
We say that regret is bounded if we can put a bound on the quantity Regret((P1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", PK), i1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", iT ) in the worst case over the choice of distributions P1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", PK , and with high probability or in expectation over the randomness of the algorithm and of the reward sampling.
",2.1. Simple Stochastic Bandit Problems,[0],[0]
"As an algorithm A interacts with a bandit problem, it generates a history Λ , which records the sequence of actions
taken and rewards observed thus far:",2.1. Simple Stochastic Bandit Problems,[0],[0]
"Λt = {(i`, yi`,`)} t−1 `=1.",2.1. Simple Stochastic Bandit Problems,[0],[0]
"We denote the space of histories of length T by HT = ([K]× R)T .
",2.1. Simple Stochastic Bandit Problems,[0],[0]
The definition of an algorithm A induces a sequence of T (possibly randomized) selection functions ft : Ht−1,2.1. Simple Stochastic Bandit Problems,[0],[0]
"→ [K], which map histories onto decisions of which arm to pull at each round.",2.1. Simple Stochastic Bandit Problems,[0],[0]
"In the contextual bandit problem, decisions are endowed with observable features.",2.2. Contextual Bandit Problems,[0],[0]
"Our algorithmic results in this paper focus on the linear contextual bandit problem, but our general connection between adaptive data gathering and differential privacy extends beyond the linear case.",2.2. Contextual Bandit Problems,[0],[0]
"For simplicity of exposition, we specialize to the linear case here.
",2.2. Contextual Bandit Problems,[0],[0]
"There are K arms i, each of which is associated with an unknown d-dimensional linear function represented by a vector of coefficients θi ∈ Rd with ||θi||2 ≤ 1.",2.2. Contextual Bandit Problems,[0],[0]
"In rounds t ∈ {1, . . .",2.2. Contextual Bandit Problems,[0],[0]
", T}, the algorithm is presented with a context xi,t ∈ Rd for each arm i with ||xi,t||2 ≤ 1, which may be selected by an adaptive adversary as a function of the past history of play.",2.2. Contextual Bandit Problems,[0],[0]
"We write xt to denote the set of all K contexts present at round t. As a function of these contexts, the algorithm then selects an arm it, and observes a reward yit,t. The rewards satisfy E [yi,t] = θi·xi,t and are bounded to lie in [0, 1].
",2.2. Contextual Bandit Problems,[0],[0]
"In the contextual setting, histories incorporate observed context information as well:",2.2. Contextual Bandit Problems,[0],[0]
"Λt = {(i`, x`, yi`,`)} t−1 `=1.
",2.2. Contextual Bandit Problems,[0],[0]
"Again, the definition of an algorithmA induces a sequence of T (possibly randomized) selection functions ft : Ht−1× Rd×K → [K], which now maps both a history and a set of contexts at round t to a choice of arm at round t.",2.2. Contextual Bandit Problems,[0],[0]
"Above we’ve characterized a bandit algorithmA as gathering data adaptively using a sequence of selection functions ft, which map the observed history",2.3. Data Gathering in the Query Model,[0],[0]
Λt ∈ Ht−1 to the index of the next arm pulled.,2.3. Data Gathering in the Query Model,[0],[0]
In this model only after the arm is chosen is a reward drawn from the appropriate distribution.,2.3. Data Gathering in the Query Model,[0],[0]
"Then the history is updated, and the process repeats.
",2.3. Data Gathering in the Query Model,[0],[0]
"In this section, we observe that whether the reward is drawn after the arm is “pulled,” or in advance, is a distinction without a difference.",2.3. Data Gathering in the Query Model,[0],[0]
"We cast this same interaction into the setting where an analyst asks an adaptively chosen sequence of queries to a fixed dataset, representing the arm rewards.",2.3. Data Gathering in the Query Model,[0],[0]
The process of running a bandit algorithm A up to time T can be formalized as the adaptive selection of T queries against a single database of size T - fixed in advance.,2.3. Data Gathering in the Query Model,[0],[0]
"The formalization consists of observing two things.
",2.3. Data Gathering in the Query Model,[0],[0]
"First, by the principle of deferred randomness, we can view any (simple or contextual) bandit algorithm as operating in a setting in the rewards available for every arm at every time step have been sampled before the start of the algorithm, rather than online as the algorithm makes its selections.",2.3. Data Gathering in the Query Model,[0],[0]
"Second, the choice of arm pulled at time t by the bandit algorithm can be viewed as the answer to an adaptively selected query against this fixed dataset of rewards.
",2.3. Data Gathering in the Query Model,[0],[0]
"Adaptive data analysis is formalized as an interaction in which a data analystA performs computations on a dataset D, observes the results, and then may choose the identity of the next computation to run as a function of previously computed results (Dwork et al., 2015c;a).",2.3. Data Gathering in the Query Model,[0],[0]
"A sequence of recent results shows that if the queries are differentially private in the dataset D, then they will not in general overfit D, in the sense that the distribution over results induced by computing q(D) will be “similar” to the distribution over results induced if q were run on a new dataset, freshly sampled from the same underlying distribution (Dwork et al., 2015c;a; Bassily et al., 2016; Dwork et al., 2015b; Rogers et al., 2016).",2.3. Data Gathering in the Query Model,[0],[0]
"We will be more precise about what these results say in Section 4.
",2.3. Data Gathering in the Query Model,[0],[0]
"Recall that histories Λ record the choices of the algorithm, in addition to its observations.",2.3. Data Gathering in the Query Model,[0],[0]
It will be helpful to introduce notation that separates out the choices of the algorithm from its observations.,2.3. Data Gathering in the Query Model,[0],[0]
"In the simple stochastic setting and the contextual setting, given a history",2.3. Data Gathering in the Query Model,[0],[0]
"Λt, an action history ΛAt =",2.3. Data Gathering in the Query Model,[0],[0]
"(i1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", it−1) ∈",2.3. Data Gathering in the Query Model,[0],[0]
"[K]t−1 denotes the portion of the history recording the actions of the algorithm.
",2.3. Data Gathering in the Query Model,[0],[0]
"In the simple stochastic setting, a bandit tableau is a T ×K matrix D ∈",2.3. Data Gathering in the Query Model,[0],[0]
"( [0, 1]K )T .",2.3. Data Gathering in the Query Model,[0],[0]
"Each row Dt of D is a vector of K real numbers, intuitively representing the rewards that would be available to a bandit algorithm at round t for each of the K arms.",2.3. Data Gathering in the Query Model,[0],[0]
"In the contextual setting, a bandit tableau is represented by a pair of T ×K matrices: D ∈",2.3. Data Gathering in the Query Model,[0],[0]
"( [0, 1]K
)T and C ∈ ( (Rd)K )T .",2.3. Data Gathering in the Query Model,[0],[0]
"Intuitively, C represents the contexts presented to a bandit algorithm",2.3. Data Gathering in the Query Model,[0],[0]
"A at each round: each row Ct corresponds to a set of K contexts, one for each arm.",2.3. Data Gathering in the Query Model,[0],[0]
"D again represents the rewards that would be available to the bandit algorithm at round t for each of the K arms.
",2.3. Data Gathering in the Query Model,[0],[0]
"We write Tab to denote a bandit tableau when the setting has not been specified: implicitly, in the simple stochastic case, Tab = D, and in the contextual case, Tab = (D,C).
",2.3. Data Gathering in the Query Model,[0],[0]
Given a bandit tableau and a bandit algorithm,2.3. Data Gathering in the Query Model,[0],[0]
"A, we have the following interaction:
We denote the subset of the reward tableau D corresponding to rewards that would have been revealed to a bandit algorithmA given action history ΛAt , by ΛAt (D).",2.3. Data Gathering in the Query Model,[0],[0]
"Concretely if ΛAt = (i1, . .",2.3. Data Gathering in the Query Model,[0],[0]
.,2.3. Data Gathering in the Query Model,[0],[0]
", it−1) then Λ A t (D) =",2.3. Data Gathering in the Query Model,[0],[0]
"{(i`, yi`,`)} t−1 `=1.",2.3. Data Gathering in the Query Model,[0],[0]
"Given a selection function ft and an action history ΛAt , de-
Interact Inputs: Time horizon T , bandit algorithm A, and bandit tableau Tab (D in the simple stochastic case, (D,C) in the contextual case)
1: for t = 1 to T do 2: (contextual case) ShowA contexts Ct,1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", Ct,K 3: Let A play action it 4: Show A reward Dt,it 5: end for 6: Return: (i1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", iT )
fine the query qΛAt as qΛAt (D) = ft(Λ A t (D)).
",2.3. Data Gathering in the Query Model,[0],[0]
We now define Algorithms Bandit and InteractQuery.,2.3. Data Gathering in the Query Model,[0],[0]
"Bandit is a standard contextual bandit algorithm defined by selection functions ft, and InteractQuery is the Interact routine that draws the rewards in advance, and at time t selects action it as the result of query qΛAt .",2.3. Data Gathering in the Query Model,[0],[0]
"With the above definitions in hand, it is straightforward to show that the two Algorithms are equivalent, in that they induce the same joint distribution on their outputs.",2.3. Data Gathering in the Query Model,[0],[0]
"In both algorithms for convenience we assume we are in the linear contextual setting, and we write ηit to denote the i.i.d. error distributions of the rewards, conditional on the contexts.
",2.3. Data Gathering in the Query Model,[0],[0]
"Bandit Inputs: T, k, {xit}, {θi}, ft,Λ0 = ∅
1: for t = 1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", T : do 2: Let it = ft(Λt−1) 3: Draw yit,t ∼ xit,t · θit + ηit 4: Update Λt = Λt−1 ∪ (it, yit,t) 5: end for 6: Return: ΛT
InteractQuery Inputs: T, k,D : Dit = θi · xit + ηit, ft
1: for t = 1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", T : do 2: Let qt = qΛAt−1 3: Let it = qt(D) 4: Update ΛAt = Λ",2.3. Data Gathering in the Query Model,[0],[0]
"A t−1 ∪ it 5: end for 6: Return: ΛAT
Claim 1.",2.3. Data Gathering in the Query Model,[0],[0]
"Let P1,t be the joint distribution induced by Algorithm Bandit on Λt at time t, and let P2,t be the joint distribution induced by Algorithm InteractQuery on Λt = Λ A t (D).",2.3. Data Gathering in the Query Model,[0],[0]
"Then ∀t P1,t = P2,t.
",2.3. Data Gathering in the Query Model,[0],[0]
"The upshot of this equivalence is that we can import existing results that hold in the setting in which the dataset
is fixed, and queries are adaptively chosen.",2.3. Data Gathering in the Query Model,[0],[0]
"There are a large collection of results of this form that apply when the queries are differentially private (Dwork et al., 2015c; Bassily et al., 2016; Rogers et al., 2016) which apply directly to our setting.",2.3. Data Gathering in the Query Model,[0],[0]
"In the next section we formally define differential privacy in the simple stochastic and contextual bandit setting, and leave the description of the more general transfer theorems to Section 4.",2.3. Data Gathering in the Query Model,[0],[0]
We will be interested in algorithms that are differentially private.,2.4. Differential Privacy,[0],[0]
"In the simple stochastic bandit setting, we will require differential privacy with respect to the rewards.",2.4. Differential Privacy,[0],[0]
"In the contextual bandit setting, we will also require differential privacy with respect to the rewards, but not necessarily with respect to the contexts.
",2.4. Differential Privacy,[0],[0]
"We now define the neighboring relation we need to define bandit differential privacy:
Definition 1.",2.4. Differential Privacy,[0],[0]
"In the simple stochastic setting, two bandit tableau’s D,D′ are reward neighbors if they differ in at most a single row: i.e. if there exists an index ` such that for all t 6=",2.4. Differential Privacy,[0],[0]
"`, Dt = D′t.
",2.4. Differential Privacy,[0],[0]
"In the contextual setting, two bandit tableau’s (D,C), (D′, C ′) are reward neighbors if C = C ′ and D and D′ differ in at most a single row: i.e. if there exists an index ` such that for all t 6=",2.4. Differential Privacy,[0],[0]
"`, Dt = D′t.
",2.4. Differential Privacy,[0],[0]
"Note that changing a context does not result in a neighboring tableau: this neighboring relation will correspond to privacy for the rewards, but not for the contexts.",2.4. Differential Privacy,[0],[0]
Remark 1.,2.4. Differential Privacy,[0],[0]
"Note that we could have equivalently defined reward neighbors to be tableaus that differ in only a single entry, rather than in an entire row.",2.4. Differential Privacy,[0],[0]
"The distinction is unimportant in a bandit setting, because a bandit algorithm will be able to observe only a single entry in any particular row.
",2.4. Differential Privacy,[0],[0]
Definition 2.,2.4. Differential Privacy,[0],[0]
"A bandit algorithm A is ( , δ) reward differentially private if for every time horizon T and every pair of bandit tableau Tab,Tab′ that are reward neighbors, and every subset S ⊆",2.4. Differential Privacy,[0],[0]
"[K]T :
P [Interact(T,A,Tab) ∈ S] ≤
e P [ Interact(T,A,Tab′) ∈ S ]",2.4. Differential Privacy,[0],[0]
"+ δ
",2.4. Differential Privacy,[0],[0]
"If δ = 0, we say that A is -differentially private.",2.4. Differential Privacy,[0],[0]
We begin by showing that differentially private algorithms that operate in the stochastic bandit setting compute empirical means for their arms that are nearly unbiased.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Together
with known differentially private algorithms for stochastic bandit problems, the result is an algorithm that obtains a nearly optimal (worst-case) regret guarantee while also guaranteeing that the collected data is nearly unbiased.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"We could (and do) obtain these results by combining the reduction to answering adaptively selected queries given by Theorem 1 with the standard generalization theorems in adaptive data analysis (e.g. Corollary 2 in its most general form), but we first prove these de-biasing results from first principles to build intuition.
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Theorem 1.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Let A be an ( , δ)-differentially private algorithm in the stochastic bandit setting.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Then, for all i ∈",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"[K], and all t, we have:∣∣∣E [Ŷ ti − µi]∣∣∣ ≤",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
(e − 1 + Tδ)µi Remark 2.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Note that since µi ∈,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"[0, 1], and for 1, e ≈ 1+ , this theorem bounds the bias by roughly +Tδ.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Often, we will have δ = 0 and so the bias will be bounded by roughly .
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Proof.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
First we fix some notation.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Fix any time horizon T , and let (ft)t∈[T ] be the sequence of selection functions induced by algorithm A. Let 1{ft(Λt)=i} be the indicator for the event that arm i is pulled at time t. We can write the random variable representing the sample mean of arm i at time T as
Ŷ Ti = T∑ t=1 1{ft(Λt)=i}∑T t′=1 1{ft′ (Λt′ )",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"=i} yit
where we recall that yi,t is the random variable representing the reward for arm",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"i at time t. Note that the numerator (ft(Λt) = i) is by definition independent of yi,t, but the denominator ( ∑T t′=1 1{ft′ (Λt′ )=i}) is not, because for t
′",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"> tΛt′ depends on yi,t.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"It is this dependence that leads to bias in adaptive data gathering procedures, and that we must argue is mitigated by differential privacy.
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
We recall that the random variable NTi represents the number of times arm i is pulled through round T : NTi =∑T t′=1 1{ft′ (Λt′ ),3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
=i}.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Using this notation, we write the sample mean of arm i at time T , as:
Ŷ Ti = T∑ t=1 1{ft(Λt)=i} NTi · yit
We can then calculate:
E[Ŷ ti ] = T∑ t=1 E[ 1{ft(Λt)=i} NTi yit]
= T∑ t=1 E yit∼Pi",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"[yit · E A [ 1{ft(Λt)=i} NTi |yit]]
where the first equality follows by the linearity of expectation, and the second follows by the law of iterated expectation.
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Our goal is to show that the conditioning in the inner expectation does not substantially change the value of the expectation.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Specifically, we want to show that all t, and any value yit, we have
E[ 1{ft(Λt)=i}
Ni |yit] ≥ e− E[
1{ft(Λt)=i}
NTi ]",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"− δ
If we can show this, then we will have
E[Ŷ Ti ] ≥ (e− T∑ t=1 E[ 1{ft(Λt)=i} NTi ]",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"− Tδ) · µi
= (e− E[ NTi NTi ]",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"− Tδ) · µi = (e− − Tδ) · µi
which is what we want (The reverse inequality is symmetric).
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
This is what we now show to complete the proof.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Observe that for all t, i, the quantity 1{ft(Λt)=i}Ni can be derived as a post-processing of the sequence of choices (f1(Λ1), . . .",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
", fT (ΛT )), and is therefore differentially private in the observed reward sequence.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Observe also that the quantity 1{ft(Λt)=i}
NTi is bounded in [0, 1].",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Hence (by a
lemma in the full version) for any pair of values yit, y′it, we have E[
1{ft(Λt)=i} NTi |yit] ≥ e− E[ 1{ft(Λt)=i} NTi |y′it]− δ.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"All
that remains is to observe that there must exist some value y′it such that E[ 1{ft(Λt)=i} Ni |y′it] ≥ E[ 1{ft(Λt)=i} Ni ].",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"(Otherwise, this would contradict",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Ey′it∼Pi,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"[E[ 1{ft(Λt)=i} Ni |y′it]] = E[ 1{ft(Λt)=i}
NTi ])",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Fixing any such y′it implies that for all yit
E[ 1{ft(Λt)=i}
Ni |yit] ≥ e− E[
1{ft(Λt)=i}
NTi |y′i,t]− δ
≥ e− E[ 1{ft(Λt)=i}
NTi ]",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"− δ
as desired.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
The upper bound on the bias follows symmetrically.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"There are existing differentially private variants of the classic UCB algorithm ((Auer et al., 2002; Agrawal, 1995; Lai & Robbins, 1985)), which give a nearly optimal tradeoff between privacy and regret (Mishra & Thakurta, 2014; Tossou & Dimitrakakis, 2017; 2016).",3.1. A Private UCB Algorithm,[0],[0]
"For completeness, we give a simple version of a private UCB algorithm in the full version which we use in our experiments.",3.1. A Private UCB Algorithm,[0],[0]
"Here, we simply quote the relevant theorem, which is a consequence of a theorem in (Tossou & Dimitrakakis, 2016):
Theorem 2.",3.1. A Private UCB Algorithm,[0],[0]
"(Tossou & Dimitrakakis, 2016)",3.1. A Private UCB Algorithm,[0],[0]
"There is an - differentially private algorithm that obtains expected regret bounded by:
O ( max ( lnT · (ln ln(T ) + ln(1/ )) , √ kT log T ))
",3.1. A Private UCB Algorithm,[0],[0]
"Thus, we can take to be as small as = O( ln 1.5 T√ kT )
while still having a regret bound of O( √ kT log T ), which is nearly optimal in the worst case (over instances) (Audibert & Bubeck, 2009).
",3.1. A Private UCB Algorithm,[0],[0]
"Combining the above bound with Theorem 1, and letting =",3.1. A Private UCB Algorithm,[0],[0]
"O( ln
1.5 T√ kT ), we have:
Corollary 1.",3.1. A Private UCB Algorithm,[0],[0]
"There exists a simple stochastic bandit algorithm that simultaneously guarantees that the bias of the empirical average for each arm i is bounded by O(µi · ln
1.5 T√ kT
) and guarantees expected regret bounded by O( √ kT log T ).
",3.1. A Private UCB Algorithm,[0],[0]
"Of course, other tradeoffs are possible using different values of .",3.1. A Private UCB Algorithm,[0],[0]
"For example, the algorithm of (Tossou & Dimitrakakis, 2016) obtains sub-linear regret so long as = ω( ln
2 T T ).",3.1. A Private UCB Algorithm,[0],[0]
"Thus, it is possible to obtain non-trivial regret while guaranteeing that the bias of the empirical means remains as low as polylog(T )/T .",3.1. A Private UCB Algorithm,[0],[0]
"Up through this point, we have focused our attention on showing how the private collection of data mitigates the effect that adaptivity has on bias, in both the stochastic and (in the full version) contextual bandit problems.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"In this section, we draw upon more powerful results from the adaptive data analysis literature to go substantially beyond bias: to correct the p-values of hypothesis tests applied to adaptively gathered data.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"These p-value corrections follow from the connection between differential privacy and a quantity called max information, which controls the extent to which the dependence of selected test on the dataset can distort the statistical validity of the test (Dwork et al., 2015b; Rogers et al., 2016).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"We briefly define max information, state the connection to differential privacy, and illustrate how max information bounds can be used to perform adaptive analyses in the private data gathering framework.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Definition 3 (Max-Information (Dwork et al., 2015b).).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let X,Z be jointly distributed random variables over domain (X ,Z).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let X ⊗ Z denote the random variable that draws independent copies of X,Z according to their marginal distributions.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"The β-approximate maxinformation between X,Z, denoted Iβ(X,Z), is defined
as:
Iβ(X,Z) = log sup O⊂(X×Z), P[(X,Z)∈O]>β",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
P,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"[(X,Z) ∈ O]−",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
β P,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
[X ⊗ Z ∈,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"O]
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Following (Rogers et al., 2016), define a test statistic t : D → R, where D is the space of all datasets.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"For D ∈ D, given an output a = t(D), the p-value associated with the test t on dataset D is p(a) =",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
PD∼P0,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"[t(D) ≥ a], where P0 is the null hypothesis distribution.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Consider an algorithm A, mapping a dataset to a test statistic.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Definition 4 (Valid p-value Correction Function (Rogers et al., 2016).).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
A function γ :,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"[0, 1] → [0, 1] is a valid p-value correction function for A if the procedure:
1.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Select a test statistic t = A(D)
2.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Reject the null hypothesis if p(t(D)) ≤ γ(α)
has probability at most α of rejection, when D ∼ P0.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Then the following theorem gives a valid p-value correction function when (D,A(D)) have bounded β-approximate max information.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Theorem 3 ((Rogers et al., 2016).).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let A be a datadependent algorithm for selecting a test statistics such that Iβ(X,A(X))",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
≤,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
k.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Then the following function γ is a valid p-value correction function for A: γ(α) = max(α−β
2k , 0)
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Finally, we can connect max information to differential privacy, which allows us to leverage private algorithms to perform arbitrary valid statistical tests.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Theorem 4 (Theorem 20 from (Dwork et al., 2015b).).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let A be an -differentially private algorithm, let P be an arbitrary product distribution over datasets of size n, and let D ∼ P .",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Then for every β > 0:
Iβ(D,A(D)) ≤ log(e)( 2n/2 + √ n log(2/β)/2)
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
Remark 3.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
We note that a hypothesis of this theorem is that the data is drawn from a product distribution.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"In the contextual bandit setting, this corresponds to rows in the bandit tableau being drawn from a product distribution.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"This will be the case if contexts are drawn from a distribution at each round, and then rewards are generated as some fixed stochastic function of the contexts.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Note that contexts (and even rewards) can be correlated with one another within a round, so long as they are selected independently across rounds.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
We now formalize the process of running a hypothesis test against an adaptively collected dataset.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
A bandit algorithm A generates a history ΛT ∈ HT .,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
Let the reward portion of the gathered dataset be denoted by DA.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"We define an adaptive test statistic selector as follows.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
Definition 5.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
Fix the reward portion of a bandit tableau D and bandit algorithmA.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"An adaptive test statistic selector is a function s from action histories to test statistics such that s(ΛAT ) is a real-valued function of the adaptively gathered dataset DA.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Importantly, the selection of the test statistic s(ΛAT ) can depend on the sequence of arms pulled by A (and in the contextual setting, on all contexts observed), but not otherwise on the reward portion of the tableau D. For example, tA = s(Λ",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"A T ) could be the t-statistic corresponding to the null hypothesis that the arm i∗ which was pulled the great-
est number of times has mean µ: tA(DA) = ∑NT i∗ t=1 yi∗t−µ√
NT i∗
By virtue of Theorems 3 and 4, and our view of adaptive data gathering as adaptively selected queries, we get the following corollary:
Corollary 2.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let A be an reward differentially private bandit algorithm, and let s be an adaptive test statistic selector.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Fix β > 0, and let γ(α) =
α
2log(e)( 2T/2+
√ T log(2/β)/2) , for α ∈",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"[0, 1].",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Then for any adaptively selected statistic tA = s(ΛAT ), and any product distribution P corresponding to the null hypothesis for tA
PD∼P,A [p(tA(D))",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
≤ γ(α)],4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"≤ α
If we set = O(1/ √ T ) in Corollary 2, then γ(α) = O(α)– i.e. a valid p-value correction that only scales α by a constant.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
We first validate our theoretical bounds on bias in the simple stochastic bandit setting.,5. Experiments,[0],[0]
"As expected the standard UCB algorithm underestimates the mean at each arm, while the private UCB algorithm of (?) obtains very low bias.",5. Experiments,[0],[0]
"While using the suggested by the theory effectively reduces bias and achieves near optimal asymptotic regret, the resulting private algorithm only achieves non-trivial regret for large T due to large constants and logarithmic factors in our bounds.",5. Experiments,[0],[0]
"This motivates a heuristic choice of that provides no theoretical guarantees on bias reduction, but leads to regret that is comparable to the non-private UCB algorithm.",5. Experiments,[0],[0]
We find empirically that even with this large choice of we achieve an 8 fold reduction in bias relative to UCB.,5. Experiments,[0],[0]
"This is consistent with the observation that our guarantees hold in the worst-case, and suggests that there is room for improvement in our theoretical bounds — both improving constants in the worst-case bounds on bias and on regret, and for proving instance specific bounds.",5. Experiments,[0],[0]
"Finally, we show that in the linear contextual bandit setting collecting data adaptively with a linear UCB algorithm and then conducting t-tests for regression coefficients yields incorrect inference (absent a p-value correction).",5. Experiments,[0],[0]
"These findings
confirm the necessity of our methods when drawing conclusions from adaptively gathered data.",5. Experiments,[0],[0]
In our first stochastic bandit experiment we set K = 20 and T = 500.,5.1. Stochastic Multi-Armed Bandit,[0],[0]
"The K arm means are equally spaced between 0 and 1 with gap ∆ = .05, with µ0 = 1.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"We run UCB and -private UCB for T rounds with = .05, and after each run compute the difference between the sample mean at each arm and the true mean.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"We repeat this process 10, 000 times, averaging to obtain high confidence estimates of the bias at each arm.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"The average absolute bias over all arms for private UCB was .00176, with the bias for every arm being statistically indistinguishable from 0 at 95% confidence (see Figure 1 for confidence intervals) while the average absolute bias (over arms) for UCB was .0698, or over 40 times higher.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"The most biased arm had a measured bias of roughly 0.14, and except for the top 4 arms, the bias of each arm was statistically significant.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"It is worth noting that private UCB achieves bias significantly lower than the = .05 guaranteed by the theory, indicating that the theoretical bounds on bias obtained from differential privacy are conservative.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"Figures 1, 2 show the bias at each arm for private UCB vs. UCB, with 95% confidence intervals around the bias at each arm.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"Not only is the bias for private UCB an order of magnitude smaller on average, it does not exhibit the systemic negative bias evident in Figure 2.
",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"Noting that the observed reduction in bias for = .05 exceeded that guaranteed by the theory, we run a second experiment withK = 5, T = 100000,∆ = .05, and = 400, averaging results over 1000 iterations.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
Figure 5 shows that private UCB achieves sub-linear regret comparable with UCB.,5.1. Stochastic Multi-Armed Bandit,[0],[0]
"While = 400 provides no meaningful theoretical guarantee, the average absolute bias at each arm mean obtained by the private algorithm was .0015 (statistically indistinguishable from 0 at 95% confidence for each arm), while the non-private UCB algorithm obtained average bias .011, 7.5 times larger.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"The bias reduction for the arm with the smallest mean (for which the bias is the worst with the non private algorithm) was by more than a factor of 10.
",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"Figures 3,4 show the bias at each arm for the private and non-private UCB algorithms together with 95% confidence intervals; again we observe a negative skew in the bias for UCB, consistent with the theory in (Nie et al., 2017).",5.1. Stochastic Multi-Armed Bandit,[0],[0]
Our second experiment confirms that adaptivity leads to bias in the linear contextual bandit setting in the context of hypothesis testing – and in particular can lead to false discovery in testing for non-zero regression coefficients.,5.2. Linear Contextual Bandits,[0],[0]
"The set up is as follows: for K = 5 arms, we observe rewards
yi,t ∼ N (θ′ixit, 1), where θi, xit ∈ R5, ||θi|| = ||xit|| = 1.",5.2. Linear Contextual Bandits,[0],[0]
"For each arm i, we set θi1 = 0.",5.2. Linear Contextual Bandits,[0],[0]
"Subject to these constraints, we pick the θ parameters uniformly at random (once per run), and select the contexts x uniformly at random (at each round).",5.2. Linear Contextual Bandits,[0],[0]
"We run a linear UCB algorithm (OFUL (?)) for T = 500 rounds, and identify the arm i∗ that has been selected most frequently.",5.2. Linear Contextual Bandits,[0],[0]
We then conduct a z-test for whether the first coordinate of θi∗ is equal to 0.,5.2. Linear Contextual Bandits,[0],[0]
"By construction the null hypothesis H0 : θi∗1 = 0 of the experiment is true, and absent adaptivity, the p-value should be distributed uniformly at random.",5.2. Linear Contextual Bandits,[0],[0]
"In particular, for any value of α the probability that the corresponding p-value is less than α is exactly α.",5.2. Linear Contextual Bandits,[0],[0]
"We record the observed p-value, and repeat the experiment 1000 times, displaying the histogram of observed p-values in Figure 6.",5.2. Linear Contextual Bandits,[0],[0]
"As expected, the adaptivity of the data gathering process leads the p-values to exhibit a strong downward skew.",5.2. Linear Contextual Bandits,[0],[0]
The dotted blue line demarcates α = .05.,5.2. Linear Contextual Bandits,[0],[0]
"Rather than probability .05 of falsely rejecting the null hypothesis at 95% confidence, we observe that 76% of the observed p-values fall below the .05 threshold.",5.2. Linear Contextual Bandits,[0],[0]
"This shows that a careful p-value correction in the style of Section 2.3 is essential even for simple testing of regression coefficients, lest bias lead to false discovery.",5.2. Linear Contextual Bandits,[0],[0]
"Data that is gathered adaptively — via bandit algorithms, for example — exhibits bias.",abstractText,[0],[0]
This is true both when gathering simple numeric valued data — the empirical means kept track of by stochastic bandit algorithms are biased downwards — and when gathering more complicated data — running hypothesis tests on complex data gathered via contextual bandit algorithms leads to false discovery.,abstractText,[0],[0]
"In this paper, we show that this problem is mitigated if the data collection procedure is differentially private.",abstractText,[0],[0]
"This lets us both bound the bias of simple numeric valued quantities (like the empirical means of stochastic bandit algorithms), and correct the p-values of hypothesis tests run on the adaptively gathered data.",abstractText,[0],[0]
"Moreover, there exist differentially private bandit algorithms with near optimal regret bounds: we apply existing theorems in the simple stochastic case, and give a new analysis for linear contextual bandits.",abstractText,[0],[0]
We complement our theoretical results with experiments validating our theory1.,abstractText,[0],[0]
Mitigating Bias in Adaptive Data Gathering via Differential Privacy,title,[0],[0]
Estimating generative models from unlabeled data is one of the challenges in unsupervised learning.,1. Introduction,[0],[0]
"Recently, several latent variable approaches have been proposed to learn flexible density estimators together with efficient sampling, such as generative adversarial networks (GANs) (Goodfellow et al., 2014), variational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014), iterative transformation of noise (Sohl-Dickstein et al., 2015), or non-volume preserving transformations (Dinh et al., 2017).
",1. Introduction,[0],[0]
"In this work we focus on GANs, currently the most con-
*Equal contribution 1Université Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France. 2Université Paris Sud, INRIA, équipe TAU, Gif-sur-Yvette, 91190, France.",1. Introduction,[0],[0]
"3Facebook Artificial Intelligence Research Paris, France.",1. Introduction,[0],[0]
"Correspondence to: Corentin Tallec <corentin.tallec@inria.fr>, Thomas Lucas <thomas.lucas@inria.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"vincing source of samples of natural images (Karras et al., 2018).",1. Introduction,[0],[0]
GANs consist of a generator and a discriminator network.,1. Introduction,[0],[0]
"The generator maps samples from a latent random variable with a basic prior, such as a multi-variate Gaussian, to the observation space.",1. Introduction,[0],[0]
This defines a probability distribution over the observation space.,1. Introduction,[0],[0]
A discriminator network is trained to distinguish between generated samples and true samples in the observation space.,1. Introduction,[0],[0]
"The generator, on the other hand, is trained to fool the discriminator.",1. Introduction,[0],[0]
"In an idealized setting with unbounded capacity of both networks and infinite training data, the generator should converge to the distribution from which the training data has been sampled.
",1. Introduction,[0],[0]
"In most adversarial setups, the discriminator classifies individual data samples.",1. Introduction,[0],[0]
"Consequently, it cannot directly detect discrepancies between the distribution of generated samples and global statistics of the training distribution, such as its moments or quantiles.",1. Introduction,[0],[0]
"For instance, if the generator models a restricted part of the support of the target distribution very well, this can fool the discriminator at the level of individual samples, a phenomenon known as mode dropping.",1. Introduction,[0],[0]
In such a case there is little incentive for the generator to model other parts of the support of the target distribution.,1. Introduction,[0],[0]
"A more thorough explanation of this effect can be found in (Salimans et al., 2016).
",1. Introduction,[0],[0]
"In order to access global distributional statistics, imagine a discriminator that could somehow take full probability distributions as its input.",1. Introduction,[0],[0]
This is impossible in practice.,1. Introduction,[0],[0]
"Still, it is possible to feed large batches of training or generated samples to the discriminator, as an approximation of the corresponding distributions.",1. Introduction,[0],[0]
The discriminator can compute statistics on those batches and detect discrepancies between the two distributions.,1. Introduction,[0],[0]
"For instance, if a large batch exhibits only one mode from a multimodal distribution, the discriminator would notice the discrepancy right away.",1. Introduction,[0],[0]
"Even though a single batch may not encompass all modes of the distribution, it will still convey more information about missing modes than an individual example.
",1. Introduction,[0],[0]
"Training the discriminator to discriminate “pure” batches with only real or only synthetic samples makes its task too easy, as a single bad sample reveals the whole batch as synthetic.",1. Introduction,[0],[0]
"Instead, we introduce a “mixed” batch discrimination task in which the discriminator needs to predict the ratio of real samples in a batch.
1
This use of batches differs from traditional minibatch learning.",1. Introduction,[0],[0]
"The batch is not used as a computational trick to increase parallelism, but as an approximate distribution, on which to compute global statistics.
",1. Introduction,[0],[0]
"A naive way of doing so would be to concatenate the samples in the batch, feeding the discriminator a single tensor containing all the samples.",1. Introduction,[0],[0]
"However, this is parameterhungry, and the computed statistics are not automatically invariant to the order of samples in the batch.",1. Introduction,[0],[0]
"To compute functions that depend on the samples only through their distribution, it is necessary to restrict the class of discriminator networks to permutation-invariant functions of the batch.",1. Introduction,[0],[0]
"For this, we adapt and extend an architecture from McGregor (2007) to compute symmetric functions of the input.",1. Introduction,[0],[0]
"We show this can be done with minimal modification to existing architectures, at a negligible computational overhead w.r.t.",1. Introduction,[0],[0]
"ordinary batch processing.
",1. Introduction,[0],[0]
"In summary, our contributions are the following:
• Naively training the discriminator to discriminate “pure” batches with only real or only synthetic samples makes its task way too easy.",1. Introduction,[0],[0]
"We introduce a discrimination loss based on mixed batches of true and fake samples, that avoids this pitfall.",1. Introduction,[0],[0]
We derive the associated optimal discriminator.,1. Introduction,[0],[0]
• We provide a principled way of defining neural networks that are permutation-invariant over a batch of samples.,1. Introduction,[0],[0]
"We formally prove that the resulting class of functions comprises all symmetric continuous functions, and only symmetric functions.",1. Introduction,[0],[0]
"• We apply these insights to GANs, with good experimental results, both qualitatively and quantitatively.
",1. Introduction,[0],[0]
"We believe that discriminating between distributions at the batch level provides an equally principled alternative to approaches to GANs based on duality formulas (Nowozin et al., 2016; Gulrajani et al., 2017; Arjovsky et al., 2017).",1. Introduction,[0],[0]
The training of generative models via distributional rather than pointwise information has been explored in several recent contributions.,2. Related work,[0],[0]
"Batch discrimination (Salimans et al., 2016) uses a handmade layer to compute batch statistics which are then combined with sample-specific features to enhance individual sample discrimination.",2. Related work,[0],[0]
Karras et al. (2018) directly compute the standard deviation of features and feed it as an additional feature to the last layer of the network.,2. Related work,[0],[0]
"Both methods use a single layer of handcrafted batch statistics, instead of letting the discriminator learn arbitrary batch statistics useful for discrimination as in our approach.",2. Related work,[0],[0]
"Moreover, in both methods the discriminator still assesses single samples, rather than entire batches.",2. Related work,[0],[0]
"Radford et al. (2015) reported improved results with batch normalization
in the discriminator, which may also be due to reliance on batch statistics.
",2. Related work,[0],[0]
"Other works, such as (Li et al., 2015) and (Dziugaite et al., 2015), replace the discriminator with a fixed distributional loss between true and generated samples, the maximum mean discrepancy, as the criterion to train the generative model.",2. Related work,[0],[0]
"This has the advantage of relieving the inherent instability of GANs, but lacks the flexibility of an adaptive discriminator.
",2. Related work,[0],[0]
The discriminator we introduce treats batches as sets of samples.,2. Related work,[0],[0]
Processing sets prescribes the use of permutation invariant networks.,2. Related work,[0],[0]
"There has been a large body of work around permutation invariant networks, e.g (McGregor, 2007; 2008; Qi et al., 2016; Zaheer et al., 2017; Vaswani et al., 2017).",2. Related work,[0],[0]
"Our processing is inspired by (McGregor, 2007; 2008) which designs a special kind of layer that provides the desired invariance property.",2. Related work,[0],[0]
The network from McGregor (2007) is a multi-layer perceptron in which the single hidden layer performs a batchwise computation that makes the result equivariant by permutation.,2. Related work,[0],[0]
"Here we show that stacking such hidden layers and reducing the final layer with a permutation invariant reduction, covers the whole space of continuous permutation invariant functions.
",2. Related work,[0],[0]
"Zaheer et al. (2017) first process each element of the set independently, then aggregate the resulting representation using a permutation invariant operation, and finally process the permutation invariant quantity.",2. Related work,[0],[0]
"Qi et al. (2016) process 3D point cloud data, and interleave layers that process points independently, and layers that apply equivariant transformations.",2. Related work,[0],[0]
"The output of their networks are either permutation equivariant for pointcloud segmentation, or permutation invariant for shape recognition.",2. Related work,[0],[0]
"In our approach we stack permutation equivariant layers that combine batch
information and sample information at every level, and aggregate these in the final layer using a permutation invariant operation.
",2. Related work,[0],[0]
"More complex approaches to permutation invariance or equivariance appear in (Guttenberg et al., 2016).",2. Related work,[0],[0]
"We prove, however, that our simpler architecture already covers the full space of permutation invariant functions.
",2. Related work,[0],[0]
Improving the training of GANs has received a lot of recent attention.,2. Related work,[0],[0]
"For instance, Arjovsky et al. (2017), Gulrajani et al. (2017) and Miyato et al. (2018) constrain the Lipschitz constant of the network and show that this stabilizes training and improves performance.",2. Related work,[0],[0]
Karras et al. (2018) achieved impressive results by gradually increasing the resolution of the generated images as training progresses.,2. Related work,[0],[0]
"Using a batch of samples rather than individual samples as input to the discriminator can provide global statistics about
the distributions of interest.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
Such statistics could be useful to avoid mode dropping.,3. Adversarial learning with permutation-invariant batch features,[0],[0]
"Adversarial learning (Goodfellow et al., 2014) can easily be extended to the batch discrimination case.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"For a fixed batch size B, the corresponding two-player optimization procedure becomes
min G max D Ex1,...,xB∼D",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"[logD(x1, . . .",3. Adversarial learning with permutation-invariant batch features,[0],[0]
", xB)]",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"+ (1)
Ez1,...,zB∼Z",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"[log(1−D(G(z1), . . .",3. Adversarial learning with permutation-invariant batch features,[0],[0]
", G(zB)))",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"]
with D the empirical distribution over data, Z a distribution over the latent variable that is the input of the generator, G a pointwise generator and D a batch discriminator.1 This leads to a learning procedure similar to the usual GAN algorithm, except that the loss encourages the discriminator to output 1 when faced with an entire batch of real data, and 0 when faced with an entire batch of generated data.
",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"Unfortunately, this basic procedure makes the work of the discriminator too easy.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"As the discriminator is only faced with batches that consist of either only training samples or only generated samples, it can base its prediction on any subset of these samples.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"For example, a single poor generated sample would be enough to reject a batch.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"To cope with this deficiency, we propose to sample batches that mix both training and generated data.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"The discriminator’s task is to predict the proportion of real images in the batch, which is clearly a permutation invariant quantity.",3. Adversarial learning with permutation-invariant batch features,[0.9503253261053575],"['Here the word prediction mechanism is a simpler sub-task of translation, where the order of words is not considered.']"
"A naive approach to sampling mixed batches would be, for each batch index, to pick a datapoint from either real or generated images with probability 12 .",3.1. Batch smoothing as a regularizer,[0],[0]
"This is necessarily ill behaved: as the batch size increases, the ratio of training data to generated data in the batch tends to 12 by the law of large numbers.",3.1. Batch smoothing as a regularizer,[0],[0]
"Consequently, a discriminator always predicting 12 would achieve very low error with large batch sizes, and provide no training signal to the generator.
",3.1. Batch smoothing as a regularizer,[0],[0]
"Instead, for each batch we sample a ratio p from a distribution P on [0, 1], and construct a batch by picking real samples with probability p and generated samples with probability 1− p.",3.1. Batch smoothing as a regularizer,[0],[0]
"This forces the discriminator to predict across an entire range of possible values of p.
Formally, suppose we are given a batch of training data x ∈ RB×n",3.1. Batch smoothing as a regularizer,[0],[0]
and a batch of generated data x̃ ∈ RB×n.,3.1. Batch smoothing as a regularizer,[0],[0]
"To mix x and x̃, a binary vector β is sampled from B (p)B , a B-dimensional Bernoulli distribution with parameter p.",3.1. Batch smoothing as a regularizer,[0],[0]
"The mixed batch with mixing vector β is denoted
mβ(x, x̃)",3.1. Batch smoothing as a regularizer,[0],[0]
:= x β + x̃ (1− β).,3.1. Batch smoothing as a regularizer,[0],[0]
"(2) 1The generator G could also be modified to produce batches of data, which can help to cover more modes per batch, but this deviates from the objective of learning a density estimator from which we can draw i.i.d.",3.1. Batch smoothing as a regularizer,[0],[0]
"samples.
",3.1. Batch smoothing as a regularizer,[0],[0]
"This apparently wastes some samples, but we can reuse the discarded samples by using 1− β in the next batch.
",3.1. Batch smoothing as a regularizer,[0],[0]
"The discriminator has to predict the ratio of real images, #βB where #β is the sum of the components of β.",3.1. Batch smoothing as a regularizer,[0],[0]
"As a loss on the predicted ratio, we use the Kullback–Leibler divergence between a Bernoulli distribution with the actual ratio of real images, and a Bernoulli distribution with the predicted ratio.",3.1. Batch smoothing as a regularizer,[0],[0]
"The divergence between Bernoulli distributions with parameters u and v is
KL(B (u) ||",3.1. Batch smoothing as a regularizer,[0],[0]
B (v)),3.1. Batch smoothing as a regularizer,[0],[0]
= u log u v +,3.1. Batch smoothing as a regularizer,[0],[0]
(1− u) log 1− u1− v .,3.1. Batch smoothing as a regularizer,[0],[0]
"(3)
Formally, the discriminator D will minimize the objective
Ep∼P, β∼B(p)B KL ( B (
#β B
)",3.1. Batch smoothing as a regularizer,[0],[0]
||,3.1. Batch smoothing as a regularizer,[0],[0]
"B (D(mβ(x, x̃))) ) ,
(4)
where the expectation is over sampling p from a distribution P , typically uniform on [0, 1], then sampling a mixed minibatch.",3.1. Batch smoothing as a regularizer,[0],[0]
"For clarity, we have omitted the expectation over the sampling of training and generated samples
The generator is trained with the loss
Ep∼P, β∼B(p)B log(D(mβ(x, x̃))).",3.1. Batch smoothing as a regularizer,[0],[0]
"(5)
This loss, which is not the generator loss associated to the min-max optimization problem, is known to saturate less (Goodfellow et al., 2014).
",3.1. Batch smoothing as a regularizer,[0],[0]
"In some experimental cases, using the discriminator loss (4) with P = U([0, 1]) made discriminator training too difficult.",3.1. Batch smoothing as a regularizer,[0],[0]
"To alleviate some of the difficulty, we sampled the mixing variable p from a reduced symmetric union of intervals [0, γ] ∪ [1 − γ, 1].",3.1. Batch smoothing as a regularizer,[0],[0]
"With low γ, all generated batches are nearly purely taken from either real or fake data.",3.1. Batch smoothing as a regularizer,[0],[0]
We refer to this training method as batch smoothing-γ.,3.1. Batch smoothing as a regularizer,[0],[0]
"Batch smoothing-0 corresponds to no mixing, while batch smoothing-0.5 corresponds to equation (4).",3.1. Batch smoothing as a regularizer,[0],[0]
"The optimal discriminator for batch smoothing can be computed explicitly, for p ∼ U([0, 1]), and extends the usual GAN discriminator when B = 1.",3.2. The optimal discriminator for batch smoothing,[0],[0]
Proposition 1.,3.2. The optimal discriminator for batch smoothing,[0],[0]
"The optimal discriminator for the loss (4),
given a batch y ∈ RB×N , is
D∗(y) = 12 punbalanced(y) pbalanced(y)
(6)
where the distribution pbalanced and punbalanced on batches are defined as
pbalanced(y)",3.2. The optimal discriminator for batch smoothing,[0],[0]
"= 1 B + 1 ∑
β∈{0,1}B
p1(y)βp2(y)1−β( B #β )
punbalanced(y) = 2 B + 1 ∑
β∈{0,1}B
p1(y)βp2(y)1−β( B #β )",3.2. The optimal discriminator for batch smoothing,[0],[0]
"#β B .
(7)
in which p1 is the data distribution and p2 the distribution of generated samples, and where p1(y)β is shorthand for p1(y1)β1 . . .",3.2. The optimal discriminator for batch smoothing,[0],[0]
"p1(yB)βB .
",3.2. The optimal discriminator for batch smoothing,[0],[0]
The proof is technical and is deferred to the supplementary material.,3.2. The optimal discriminator for batch smoothing,[0],[0]
"For non-uniform beta distributions on p, a similar result holds, with different coefficients depending on #β and B in the sum.
",3.2. The optimal discriminator for batch smoothing,[0],[0]
These heavy expressions can be interpreted easily.,3.2. The optimal discriminator for batch smoothing,[0],[0]
"First, in the case B = 1, the optimal discriminator reduces to the optimal discriminator for a standard GAN, D∗ = p1(y)p1(y)+p2(y) .
",3.2. The optimal discriminator for batch smoothing,[0],[0]
"Actually pbalanced(y) is simply the distribution of batches y under our procedure of sampling p uniformly, then sampling β ∼ B (p)B .",3.2. The optimal discriminator for batch smoothing,[0],[0]
"The binomial coefficients put on equal footing contributions with different true/fake ratios.
",3.2. The optimal discriminator for batch smoothing,[0],[0]
"The generator loss (5), when faced with the optimal discriminator, is the Kullback–Leibler divergence between pbalanced and punbalanced (up to sign and a constant log(2)).",3.2. The optimal discriminator for batch smoothing,[0],[0]
"Since punbalanced puts more weight on batches with higher #β (more true samples), this brings fake samples closer to true ones.
",3.2. The optimal discriminator for batch smoothing,[0],[0]
"Since pbalanced and punbalanced differ by a factor 2#β/B, the ratio D∗ = 12 punbalanced(y) pbalanced(y) is simply the expectation of #β/B under a probability distribution on β that is proportional to p1(y)βp2(y)1−β(
B #β ) .",3.2. The optimal discriminator for batch smoothing,[0],[0]
"But this is the posterior distribution on
β given the batch y and the uniform prior on the ratio p.",3.2. The optimal discriminator for batch smoothing,[0],[0]
"Thus, the optimal discriminator is just the posterior mean of the ratio of true samples, D∗(y) =",3.2. The optimal discriminator for batch smoothing,[0],[0]
IEβ|y [ #β B ] .,3.2. The optimal discriminator for batch smoothing,[0],[0]
This is standard when minimizing the expected divergence between Bernoulli distributions and the approach can therefore be extended to non-uniform priors on p as shown in section 9.,3.2. The optimal discriminator for batch smoothing,[0],[0]
Computing statistics of probability distributions from batches of i.i.d.,4. Permutation invariant networks,[0],[0]
"samples requires to compute quantities that
are invariant to permuting the order of samples within the batch.",4. Permutation invariant networks,[0],[0]
In this section we propose a permutation equivariant layer that can be used together with a permutation invariant aggregation operation to build networks that are permutation invariant.,4. Permutation invariant networks,[0],[0]
"We also provide a sketch of proof (fully developed in the supplementary material) that this architecture is able to reach all symmetric continuous functions, and only represents such functions.",4. Permutation invariant networks,[0],[0]
"A naive way of achieving invariance to batch permutations is to consider the batch dimension as a regular feature dimension, and to randomly reorder the batches at each step.",4.1. Building a permutation invariant architecture,[0],[0]
"This multiplies the input dimension by the batch size, and thus greatly increases the number of trainable parameters.",4.1. Building a permutation invariant architecture,[0],[0]
"Moreover, this only provides approximate invariance to batch permutation, as the network has to infer the invariance based on the training data.
",4.1. Building a permutation invariant architecture,[0],[0]
"Instead, we propose to directly build invariance into the architecture.",4.1. Building a permutation invariant architecture,[0],[0]
"This method drastically reduces the number of parameters compared to the naive approach, bringing it back in line with ordinary networks, and ensures strict invariance to batch permutation.
",4.1. Building a permutation invariant architecture,[0],[0]
Let us first formalize the notion of batch permutation invariance and equivariance.,4.1. Building a permutation invariant architecture,[0],[0]
"A function f from RB×l to RB×L is batch permutation equivariant if permuting samples in the batch results in the same permutation of the outputs: for any permutation σ of the inputs,
f(xσ(1), . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xσ(B))",4.1. Building a permutation invariant architecture,[0],[0]
"= f(x)σ(1), . . .",4.1. Building a permutation invariant architecture,[0],[0]
", f(x)σ(B).",4.1. Building a permutation invariant architecture,[0],[0]
"(8)
For instance, any regular neural network or other function treating the inputs x1, . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xB independently in parallel, is batch permutation equivariant.
",4.1. Building a permutation invariant architecture,[0],[0]
"A function f from RB×l to RL is batch permutation invariant if permuting the inputs in the batch does not change the output: for any permutation on batch indices σ,
f(xσ(1), . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xσ(B))",4.1. Building a permutation invariant architecture,[0],[0]
"= f(x1, . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xB).",4.1. Building a permutation invariant architecture,[0],[0]
"(9)
The mean, the max or the standard deviation along the batch axis are all batch permutation invariant.
",4.1. Building a permutation invariant architecture,[0],[0]
"Permutation equivariant and permutation invariant functions can be obtained by combining ordinary, parallel treatment of batch samples with an additional batch-averaging operation that performs an average of the activations across the batch direction.",4.1. Building a permutation invariant architecture,[0],[0]
"In our architecture, this averaging is the only form of interaction between different elements of the batch.",4.1. Building a permutation invariant architecture,[0],[0]
"It is one of our main results that such operations are sufficient to recover all invariant functions.
",4.1. Building a permutation invariant architecture,[0],[0]
"Formally, on a batch of data x ∈ RB×n, our proposed batch
permutation invariant network fθ is defined as
fθ(x) = 1 B B∑ b=1",4.1. Building a permutation invariant architecture,[0],[0]
(φθp ◦ φθp−1 ◦ . . .,4.1. Building a permutation invariant architecture,[0],[0]
"◦ φθ0(x))b (10)
where each φθi is a batch permutation equivariant function from RB×li−1 to RB×li , where the li’s are the layer sizes.
",4.1. Building a permutation invariant architecture,[0],[0]
"The equivariant layer operation φθ with l input features and L output features comprises an ordinary weight matrix Λ ∈ Rl×L that treats each data point of the batch independently (“non-batch-mixing”), a batch-mixing weight matrix Γ ∈ Rl×L, and a bias vector β ∈ RL.",4.1. Building a permutation invariant architecture,[0],[0]
"As in regular neural networks, Λ processes each data point in the batch independently.",4.1. Building a permutation invariant architecture,[0],[0]
"On the other hand, the weight matrix Γ operates after computing an average across the whole batch.",4.1. Building a permutation invariant architecture,[0],[0]
"Defining ρ as the batch average for each feature,
ρ(x1, . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xB)",4.1. Building a permutation invariant architecture,[0],[0]
:= 1 B B∑ b=1,4.1. Building a permutation invariant architecture,[0],[0]
"xb (11)
the permutation-equivariant layer φ is formally defined as φθ(x)b := µ ( β + xbΛ + ρ(x)Γ ) (12)
where µ is a nonlinearity, b is a batch index, and the parameter of the layer is θ = (β,Λ,Γ).",4.1. Building a permutation invariant architecture,[0],[0]
The networks constructed above are permutation invariant by construction.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"However, it is unclear a priori that all permutation invariant functions can be represented this way: the functions that can be approximated to arbitrary precision by those networks could be a strict subset of the set of permutation invariant functions.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0.954300331874318],"['The previously proposed word prediction mechanism could be used only as a extra training objective, which will not be computed during the translation.']"
"The optimal solution for the discriminator could lie outside this subset, making our construction too restrictive.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"We now show this is not the case: our architecture satisfies a universal approximation theorem for permutation-invariant functions.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
Theorem 1.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
The set of networks that can be constructed by stacking as in Eq.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
(10) the layers φ defined in Eq.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"(12), with sigmoid nonlinearities except on the output layer, is dense in the set of permutation-invariant functions (for the topology of uniform convergence on compact sets).
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"While the case of one-dimensional features is relatively simple, the multidimensional case is more intricate, and the detailed proof is given in the supplementary material.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"Let us describe the key ideas underlying the proof.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"The standard universal approximation theorem for neural networks proves the following: for any continuous function f , we can find a network that given a batch x = (x1, . . .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
", xB), computes (f(x1), . . .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
", f(xB)).",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"This is insufficient for our purpose as it provides no way of mixing information between samples in the batch.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"First, we prove that the set of functions that can be approximated to arbitrary precision by our networks is an algebra, i.e., a vector space stable under products.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"From this point on, it remains to be shown that this algebra contains a generative family of the continuous symmetric functions.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"To prove that we can compute the sum of two functions f1 and f2, compute f1 and f2 on different channels (this is possible even if f1 and f2 require different numbers of layers, by filling in with the identity if necessary).",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"Then sum across channels, which is possible in (12).
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"To compute products, first compute f1 and f2 on different channels, then apply the universal approximation theorem to turn this into log f1 and log f2, then add, then take the exponential thanks to the universal approximation theorem.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"The key point is then the following: the algebra of all permutation-invariant polynomials over the components of (x1, . . .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
", xB) is generated as an algebra by the averages 1 B (f(x1) + . .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
.+ f(xB)),4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
when f ranges over all functions of single batch elements.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"This non-trivial algebraic statement is proved in the supplementary material.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"By construction, such functions 1B (f(x1)+. .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
".+f(xB)) are readily available in our architecture, by computing f as in an ordinary network and then applying the batch-averaging operation ρ in the next layer.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
Further layers provide sums and products of those thanks to the algebra property.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
We can conclude with a symmetric version of the Stone–Weierstrass theorem (polynomials are dense in continuous functions).,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"In our experiments, we apply the constructions above to standard, deep convolutional neural networks.",4.3. Practical architecture,[0],[0]
"In practice, for the linear operations Λ and Γ in (12) we use convolutional kernels (of size 3× 3) acting over xb and ρ(x) respectively.",4.3. Practical architecture,[0],[0]
Weight tensors Λ and Γ are also reweighted like so that at the start of training ρ(x) does not contribute disproportionately compared with other features:,4.3. Practical architecture,[0],[0]
Λ̃ = |B||B|+1 Λ and Γ̃ = 1|B|+1 Γ where |B| denotes the size of batch B.,4.3. Practical architecture,[0],[0]
"While these coefficients could be learned, we have found this explicit initialization to improve training.",4.3. Practical architecture,[0],[0]
"Figure 1 shows how to modify standard CNN architectures to adapt each layer to our method.
",4.3. Practical architecture,[0],[0]
"In the first setup, which we refer to as BGAN, a permutation invariant reduction is done at the end of the discriminator, yielding a single prediction per batch, which is evaluated with the loss in (4).",4.3. Practical architecture,[0],[0]
"We also introduce a setup, M-BGAN, where we swap the order of averaging and applying the loss.",4.3. Practical architecture,[0],[0]
"2 Namely, letting y be the single target for the batch (in our case, the proportion of real samples), the BGAN case translates into
L((o1, . . .",4.3. Practical architecture,[0],[0]
", oB), y) = ` (
1 B B∑ i=1",4.3. Practical architecture,[0],[0]
"oi, y
) (13)
2This was initially a bug that worked.
while M-BGAN translates to
L((o1, . . .",4.3. Practical architecture,[0],[0]
", oB), y) = 1 B B∑ i=1",4.3. Practical architecture,[0],[0]
"`(oi, y) (14)
where L is the final loss function, ` is the KL loss function used in (4), (o1, . .",4.3. Practical architecture,[0],[0]
.,4.3. Practical architecture,[0],[0]
", ob) is the output of the last equivariant layer, and y is the target for the whole batch.
",4.3. Practical architecture,[0],[0]
Both these losses are permutation invariant.,4.3. Practical architecture,[0],[0]
A more detailled explanation of M-BGAN is given in Section 11.,4.3. Practical architecture,[0],[0]
The synthetic dataset from Zhang et al. (2017) is explicitly designed to test mode dropping.,5.1. Synthetic 2D distributions,[0],[0]
The data are sampled from a mixture of concentrated Gaussians in the 2D plane.,5.1. Synthetic 2D distributions,[0],[0]
"We compare standard GAN training, “mixup” training (Zhang et al., 2017), and batch smoothing using the BGAN from Section 4.3.
",5.1. Synthetic 2D distributions,[0],[0]
"In all cases, the generators and discriminators are three-layer ReLU networks with 512 units per layer.",5.1. Synthetic 2D distributions,[0],[0]
The latent variables of the generator are 2-dimensional standard Gaussians.,5.1. Synthetic 2D distributions,[0],[0]
"The models are trained on their respective losses using the Adam (Kingma & Ba, 2015) optimizer, with default parameters.",5.1. Synthetic 2D distributions,[0],[0]
"The discriminator is trained for five steps for each generator step.
",5.1. Synthetic 2D distributions,[0],[0]
The results are summarized in Figure 3.,5.1. Synthetic 2D distributions,[0],[0]
Batch smoothing and mixup have similar effects.,5.1. Synthetic 2D distributions,[0],[0]
Results for BGAN and M-BGAN are qualitatively similar on this dataset and we only display results for BGAN.,5.1. Synthetic 2D distributions,[0],[0]
"The standard GAN setting quickly diverges, due to its inability to fit several modes simultaneously, while both batch smoothing and mixup successfully fit the majority of modes of the distribution.",5.1. Synthetic 2D distributions,[0],[0]
"Next, we consider image generation on the CIFAR10 dataset.",5.2. Experimental results on CIFAR10,[0],[0]
"We use the simple architecture from (Miyato et al., 2018), minimally modified to obtain permutation invariance thanks to (12).",5.2. Experimental results on CIFAR10,[0],[0]
All other architectural choices are unchanged.,5.2. Experimental results on CIFAR10,[0],[0]
"The same Adam hyperparameters from (Miyato et al., 2018) are used for all models: α = 2e−4, β1 = 0.5, β2 = 0.999, and no learning rate decay.",5.2. Experimental results on CIFAR10,[0],[0]
"We performed hyperparameter search for the number of discrimination steps between each generation step, ndisc, over the range {1, . . .",5.2. Experimental results on CIFAR10,[0],[0]
", 5}, and for the batch smoothing parameter γ over [0.2, 0.5].",5.2. Experimental results on CIFAR10,[0],[0]
"All models are trained for 400, 000 iterations, counting both generation and discrimination steps.",5.2. Experimental results on CIFAR10,[0],[0]
"We compare smoothed BGAN and M-BGAN, and the same network trained with spectral normalization (Miyato et al., 2018) (SN), and gradient penalty (Gulrajani et al., 2017) on both the Wasserstein (Arjovsky et al., 2017) (WGP) and the standard loss (GP).",5.2. Experimental results on CIFAR10,[0],[0]
"We also compare to a model using the batch-discrimination layer from (Salimans et al., 2016), adding a final batch discrimination layer to the architecture of (Miyato et al., 2018).",5.2. Experimental results on CIFAR10,[0],[0]
"All models are evaluated by reporting the Inception Score and the Fréchet Inception Distance (Heusel et al., 2017) and results are summarized in Table 2.",5.2. Experimental results on CIFAR10,[0],[0]
"Figure 4 displays sample images generated with our best model.
",5.2. Experimental results on CIFAR10,[0],[0]
Figure 5.2 highlights the training dynamics of each model3.,5.2. Experimental results on CIFAR10,[0],[0]
"On this architecture, M-BGAN heavily outperforms both batch discrimination and our other variants, and yields results similar to, or slightly better than (Miyato et al., 2018).",5.2. Experimental results on CIFAR10,[0],[0]
"Model trained with batch smoothing display results on par with batch discrimination, and much better than without batch smoothing.
",5.2. Experimental results on CIFAR10,[0],[0]
"3For readability, a slight smoothing is performed on the curves.",5.2. Experimental results on CIFAR10,[0],[0]
"To check the effect of the batch smoothing parameter γ on the loss, we plot the discriminator and generator losses of the network for different γ’s.",5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
"The smaller the γ, the purer the batches.",5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
We would expect discriminator training to be more difficult with larger γ.,5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
The results corroborate this insight (Fig. 2).,5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
BGAN and M-BGAN behave similarly and we only report on BGAN in the figure.,5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
"The discriminator loss is not directly affected by an increase in γ, but the generator loss is lower for larger γ, revealing the relative advantage of the generator on the discriminator.
",5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
"This suggests to increase γ if the discriminator dominates learning, and to decrease γ if the discriminator is stuck at a high value in spite of poor generated samples.",5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
"Finally, on the celebA face dataset, we adapt the simple architecture of (Miyato et al., 2018) to the increased resolution by adding a layer to both networks.",5.4. Qualitative results on celebA,[0],[0]
"For optimization we use Adam with β1 = 0, β2 = 0.9, α = 1e",5.4. Qualitative results on celebA,[0],[0]
"− 4, and ndisc = 1.",5.4. Qualitative results on celebA,[0],[0]
"Fig. 5 dislays BGAN samples with pure batches, and BGAN and M-BGAN samples with γ = .5.",5.4. Qualitative results on celebA,[0],[0]
The visual quality of the samples is reasonable; we believe that an improvement is visible from pure batches to M-BGAN.,5.4. Qualitative results on celebA,[0],[0]
"We introduced a method to feed batches of samples to the discriminator of a GAN in an principled way, based on two observations: feeding all-fake or all-genuine batches to a discriminator makes its task too easy; second, a simple architectural trick makes it possible to provably recover all functions of the batch as an unordered set.",6. Conclusion,[0],[0]
"Experimentally, this provides a new, alternative method to reduce mode dropping and reach good quantitative scores in GAN training.",6. Conclusion,[0],[0]
This work has been partially supported by the grant ANR-16CE23-0006,ACKNOWLEDGMENTS,[0],[0]
“Deep in France” and LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01).,ACKNOWLEDGMENTS,[0],[0]
Generative adversarial networks (GANs) are powerful generative models based on providing feedback to a generative network via a discriminator network.,abstractText,[0],[0]
"However, the discriminator usually assesses individual samples.",abstractText,[0],[0]
"This prevents the discriminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution.",abstractText,[0],[0]
"We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch.",abstractText,[0],[0]
The latter score does not depend on the order of samples in a batch.,abstractText,[0],[0]
"Rather than learning this invariance, we introduce a generic permutation-invariant discriminator architecture.",abstractText,[0],[0]
This architecture is provably a universal approximator of all symmetric functions.,abstractText,[0],[0]
"Experimentally, our approach reduces mode collapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.",abstractText,[0],[0]
Mixed batches and symmetric discriminators for GAN training,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 438–443 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
438
Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance. Most existing approaches design sophisticated features or exploit various off-the-shelf tools, but achieve little success. In this paper, we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly 1.",text,[0],[0]
Discourse parsing aims to identify the structure and relationship between different element discourse units (EDUs).,1 Introduction,[0],[0]
"As a fundamental topic in natural language processing, discourse parsing can assist many down-stream applications such as summarization (Louis et al., 2010), sentiment analysis (Polanyi and van den Berg, 2011) and question-answering (Ferrucci et al., 2010).",1 Introduction,[0],[0]
"However, the performance of discourse parsing is still far from perfect, especially for EDUs that are distant to each other in the discourse.",1 Introduction,[0],[0]
"In fact, as found in (Jia et al., 2018), the discourse parsing performance drops quickly as the dependency span increases.",1 Introduction,[0],[0]
"The reason may be twofold:
1Code for replicating our experiments is available at https://github.com/PKUYeYuan/ACL2018 CFDP.
",1 Introduction,[0],[0]
"Firstly, as discussed in previous works (Joty et al., 2013), it is important to address discourse structure characteristics, e.g., through modeling lexical chains in a discourse, for discourse parsing, especially in dealing with long span scenarios.",1 Introduction,[0],[0]
"However, most existing approaches mainly focus on studying the semantic and syntactic aspects of EDU pairs, in a more local view.",1 Introduction,[0],[0]
"Discourse cohesion reflects the syntactic or semantic relationship between words or phrases in a discourse, and, to some extent, can indicate the topic changing or threads in a discourse.",1 Introduction,[0],[0]
"Discourse cohesion includes five situations, including reference, substitution, ellipsis, conjunction and lexical cohesion (Halliday and Hasan, 1989).",1 Introduction,[0],[0]
"Here, lexical cohesion reflects the semantic relationship of words, and can be modeled as the recurrence of words, synonym and contextual words.
",1 Introduction,[0],[0]
"However, previous works do not well model the discourse cohesion within the discourse parsing task, or do not even take this issue into account.",1 Introduction,[0],[0]
"Morris and Hirst (1991) proposes to utilize Roget thesauri to form lexical chains (sequences of semantically related words that can reflect the topic shifts within a discourse), which are used to extract features to characterize discourse structures.",1 Introduction,[0],[0]
"(Joty et al., 2013) uses lexical chain feature to model multi-sentential relation.",1 Introduction,[0],[0]
"Actually, these simplified cohesion features can already improve parsing performance, especially in long spans.
",1 Introduction,[0],[0]
"Secondly, in modern neural network methods, modeling discourse cohesion as part of the networks is not a trivial task.",1 Introduction,[0],[0]
"One can still use off-the-shell tools to obtain lexical chains, but these tools can not be jointly optimized with the main neural network parser.",1 Introduction,[0],[0]
"We argue that characterizing discourse cohesion implicitly within a unified framework would be more
straightforward and effective for our neural network based parser.",1 Introduction,[0],[0]
"As shown in Figure 1, the 12 EDUs in the given discourse talk about different topics, marked with 3 different colors, which could be captured by a memory network that maintains several memory slots.",1 Introduction,[0],[0]
"In discourse parsing, such an architecture may help to cluster topically similar or related EDUs into the same memory slot, and each slot could be considered as a representation that maintains a specific topic or thread within the current discourse.",1 Introduction,[0],[0]
"Intuitively, we could also treat such a mechanism as a way to capture the cohesion characteristics of the discourse, just like the lexical chain features used in previous works, but without relying on external tools or resources.
",1 Introduction,[0],[0]
"In this paper, we investigate how to exploit discourse cohesion to improve discourse parsing.",1 Introduction,[0],[0]
Our contribution includes: 1) we design a memory network method to capture discourse cohesion implicitly in order to improve discourse parsing.,1 Introduction,[0],[0]
2),1 Introduction,[0],[0]
"We choose bidirectional long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) with an attention mechanism to represent EDUs directly from embeddings, and use simple position features to capture shallow discourse structures, without relying on off-the-shelf tools or resources.",1 Introduction,[0],[0]
Experiments on the RST corpus show that the memory based discourse cohesion model can help better capture discourse structure information and lead to significant improvement over traditional feature based discourse parsing methods.,1 Introduction,[0],[0]
"Our parser is an arc-eager style transition system (Nivre, 2003) with 2 stacks and a queue as shown in Figure 2, which is similar in spirit with (Dyer et al., 2015; Ballesteros et al., 2015).",2 Model overview,[0],[0]
"We follow the conventional data structures in transition-based dependency parsing, i.e., a queue (B) of EDUs to be processed, a stack (S) to store the partially constructed discourse trees, and a stack (A) to represent the history of transitions (actions combined with discourse relations).
",2 Model overview,[0],[0]
"In our parser, the transition actions include Shift, Reduce, Left-arc and Right-arc.",2 Model overview,[0],[0]
"At each step, the parser chooses to take one of the four actions and pushes the selected transition into A. Shift pushes the first EDU in queue B to the top of the stack S, while Reduce pops the top item of S. Left-arc connects the first EDU (head) in B to the top EDU (dependent) in S and then pops the top item of S, while Right-arc connects the top EDU (head) of S to the first EDU (dependent) in B and then pushes B’s first EDU to the top of S. A parse tree can be finally constructed until B is empty and S only contains a complete discourse tree.",2 Model overview,[0],[0]
"For more details, please refer to (Nivre, 2003).
",2 Model overview,[0],[0]
"As shown in Figure 2, at time t, we characterize the current parsing process by preserving the top two elements in B, top three elements in A and the root EDU in the partially constructed tree at the top of S. We first concatenate the embeddings of the preserved elements in each data structure to obtain the embeddings of S, B and A.",2 Model overview,[0],[0]
"We then append the three representations with the position2 features (introduced in Section 2.1), respectively.",2 Model overview,[0],[0]
"We pass them through one ReLU layer and two fully connected layers with ReLU as their activation functions to obtain the final state representation pt at time t, which will be used to determine the best transition to take at t.
Next, we apply an affine transformation to pt and feed it to a softmax layer to get the distribution over all possible decisions (actions combined with discourse relations).",2 Model overview,[0],[0]
"We train our model using the automatically generated oracle action sequences as the gold-standard annotations, and utilize cross entropy as the loss function.",2 Model overview,[0],[0]
"We perform greedy search during decoding.
...
...
...
l1
l2
Pt
Word
...
a1 a2 an
POS Position1
slotj
match{ weighted sum
ReLU
FC1(ReLU)
FC2(ReLU)
S B
A
...
...
wi
softmax
RA(Li)SH ...
sloti
...
wi
} match weighted sum
Bi-LSTM
RA(Li)
SH
（1）
（2） （2）
Position2
Memory network1
Memory network2
SRefined BRefined
...",2 Model overview,[0],[0]
"Bi-LSTM
...
Figure 2: Our discourse parsing framework: (1) Basic EDU representation module; (2) Memory networks to capture the discourse cohesion so as to obtain the refined representations of S and B. RA(Li)",2 Model overview,[0],[0]
means that the chosen action is Right-arc and its relation is List.,2 Model overview,[0],[0]
SH means Shift.,2 Model overview,[0],[0]
a1 to an are weights for the attention mechanism of the bidirectional LSTM.,2 Model overview,[0],[0]
"As mentioned in previous work (Jia et al., 2018), when the top EDUs in S and B are far from each other in the discourse, i.e., with a long span, the parser will be prone to making wrong decisions.",2.1 Discourse Structures,[0],[0]
"To deal with these long-span cases, one should take discourse structures into account, e.g., extracting features from the structure of a long discourse or analyzing and characterizing different topics discussed in the discourse.
",2.1 Discourse Structures,[0],[0]
"We, therefore, choose two kinds of position features to reflect the structure information, which can be viewed as a shallow form of discourse cohesion.",2.1 Discourse Structures,[0],[0]
"The first one describes the position of an EDU alone, while the second represents the spatial relationship between the top EDUs of S and B. (1) Position1: the positions of the EDU in the sentence, paragraph and discourse, respectively.",2.1 Discourse Structures,[0],[0]
"(2) Position2: whether the top EDUs of S and B are in the same sentence/paragraph or not, and the distance between them.",2.1 Discourse Structures,[0],[0]
"Basic EDU representation: In our model, the EDUs in both S and B follow the same representation method, and we take an EDU in B as an example as shown in Figure 2.",3 Memory based Discourse Cohesion,[0],[0]
"The basic representation for an EDU is built by concatenating three components, i.e., word, POS and Position1.",3 Memory based Discourse Cohesion,[0],[0]
"Regarding word, we feed the
sequence of words in the EDU to a bi-directional Long Short Term Memory (LSTM) with attention mechanism and obtain the final word representation by concatenating the two final outputs from both directions.",3 Memory based Discourse Cohesion,[0],[0]
"Here, we use pre-trained Glove (Pennington et al., 2014) as the word embeddings.",3 Memory based Discourse Cohesion,[0],[0]
"We get the POS tags from Stanford CoreNLP toolkit (Manning et al., 2014), and similarly, send the POS tag sequence of the EDU to a bi-directional LSTM with attention mechanism to obtain the final POS representation.",3 Memory based Discourse Cohesion,[0],[0]
"For concise, we omit the bi-directional LSTM network structure for POS in Figure 2, which is the same as the one for word.",3 Memory based Discourse Cohesion,[0],[0]
"The Position1 feature vectors are randomly initialized and we expect them to work as a proxy to capture the shallow discourse structure information.
",3 Memory based Discourse Cohesion,[0],[0]
"Memory Refined Representation: Besides the shallow structure features, we design a memory network component to cluster EDUs with similar topics to the same memory slot to alleviate the long span issues, as illustrated in Figure 1.",3 Memory based Discourse Cohesion,[0],[0]
"We expect these memory slots can work as lexical chains, which can maintain different threads within the discourse.",3 Memory based Discourse Cohesion,[0],[0]
"Such a memory mechanism has the advantage that it can perform the clustering automatically and does not rely on extra tools or resources to train.
",3 Memory based Discourse Cohesion,[0],[0]
"Concretely, we match the representations of S and B with their corresponding memory networks, respectively, to get their discourse cohesion clues, which are used to improve the original representations.",3 Memory based Discourse Cohesion,[0],[0]
"Take B as an example, we first compute the similarity between the representation of B (Vb) and each memory slot mi in B’s memory.",3 Memory based Discourse Cohesion,[0],[0]
"We adopt the cosine similarity as our metric as below:
Sim[x, y] = x · y ‖x‖ · ‖y‖
(1)
",3 Memory based Discourse Cohesion,[0],[0]
"Then, we use this cosine similarity to produce a normalized weight wi for each memory slot.",3 Memory based Discourse Cohesion,[0],[0]
"We introduce a strength factor λ to improve the focus.
",3 Memory based Discourse Cohesion,[0],[0]
"wi = exp(λSim[Vb,mi])∑ j exp(λSim[Vb,mj ])
(2)
",3 Memory based Discourse Cohesion,[0],[0]
"Finally, we get the discourse cohesion clue of B (denoted by BCoh) from its memory according to the weighted sum of mi.
BCoh = ∑ i wimi (3)
",3 Memory based Discourse Cohesion,[0],[0]
"We concatenateBCoh (the discourse cohesion clue of B) and the original embedding of B to get the refined representation Brefined for B. Similarly, we concatenate SCoh and the embedding of S to get the refined representation Srefined for S, as shown in Figure 2.",3 Memory based Discourse Cohesion,[0],[0]
"In our experiments, each memory contains 20 slots, which are randomly initialized and optimized during training.",3 Memory based Discourse Cohesion,[0],[0]
Dataset:,4 Evaluation and Results,[0],[0]
"We use the RST Discourse Treebank (Carlson et al., 2001) with the same split as in (Li et al., 2014), i.e., 312 for training, 30 for development and 38 for testing.",4 Evaluation and Results,[0],[0]
"We experiment with two set of relations, the 111 types of fine-grained relations and the 19 types of coarse-grained relations, respectively.
",4 Evaluation and Results,[0],[0]
Evaluation Metrics:,4 Evaluation and Results,[0],[0]
"In the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), head is the core of a discourse, and a dependent gives supporting evidence to its head with certain relationship.",4 Evaluation and Results,[0],[0]
"We adopt unlabeled accuracy UAS (the ratio of EDUs that correctly identify their heads) and labeled accuracy LAS (the ratio of EDUs that have both correct heads and relations) as our evaluation metrics.
",4 Evaluation and Results,[0],[0]
"Baselines: We compare our method with the following baselines and models: (1) Perceptron: We re-implement the perceptron based arc-eager style dependency discourse parser as mentioned in (Jia et al., 2018) with coarse-grained relation.",4 Evaluation and Results,[0],[0]
"The Perceptron model chooses words, POS tags, positions and length features, totally 100 feature templates, with the early update strategy (Collins and Roark, 2004).",4 Evaluation and Results,[0],[0]
(2) Jia18:,4 Evaluation and Results,[0],[0]
"Jia et al. (2018) implement a transition-based discourse parser with stacked LSTM, where they choose a two-layer LSTM to represent EDUs by encoding four kinds of features including words, POS tags, positions and length features.",4 Evaluation and Results,[0],[0]
(3) Basic EDU representation (Basic): Our discourse parser with the basic EDU representation method mentioned in Section 3.,4 Evaluation and Results,[0],[0]
(4) Memory refined representation (Refined): Our full parser equipped with the basic EDU representation method and the memory networks to capture the discourse cohesion mentioned in Section 3.,4 Evaluation and Results,[0],[0]
"(5) MST-full (Li et al., 2014): a graph-based dependency discourse parser with carefully selected 6 sets of features including words, POS tags, positions,
length, syntactic and semantic similarity features, which achieves the state-of-art performance on the RST Treebank.",4 Evaluation and Results,[0],[0]
We list the overall discourse parsing performance in Table 1.,4.1 Results,[0],[0]
"Here, Jia18, a stack LSTM based method (Jia et al., 2018), outperforms the traditional Perceptron method, but falls behind our Basic model with word, POS tags and Position features.",4.1 Results,[0],[0]
"The reason may be that representing EDUs directly from the sequence of word/POS embeddings could probably capture the semantic meaning of EDUs, which is especially useful for taking into account synonyms or paraphrases that often confuse traditional feature-based methods.",4.1 Results,[0],[0]
"We can also see that Basic(word+pos+position) significantly outperforms Basic(word+pos), as the Position features may play a crucial role in providing useful structural clues to our parser.",4.1 Results,[0],[0]
"Such position information can also be considered as a shallow treatment to capture the discourse cohesion, especially for long span scenarios.",4.1 Results,[0],[0]
"When using the memory network, our Refined method achieves better performance than the Basic(word+pos+position) in both UAS and LAS.",4.1 Results,[0],[0]
"The reason may come from the ability of the memory networks in simulating the lexical chains within a discourse, where the memory networks can model the discourse cohesion so as to provide topical or structural clues to our parser.",4.1 Results,[0],[0]
"We use SIGF V2 (Padó, 2006) to perform significance test for the discussed models.",4.1 Results,[0],[0]
"We find that the Basic(word+pos+position) method significantly outperforms (Jia et al., 2018), and our Refined model performs significantly better than Basic(word+pos+position) (with p < 0.1).
",4.1 Results,[0],[0]
"However, when compared with MST-full (Li et al., 2014), our models still fall behind this state-of-the-art method.",4.1 Results,[0],[0]
"The main reason might be that MST-full follows a global graph-based dependency parsing framework, where their high order methods (in cubic time complexity) can directly analyze the relationship between any EDUs pairs in the discourse, while, we choose the transition-based local method with linear time complexity, which can only investigate the top EDUs in S and B according to the selected actions, thus usually has a lower performance than the global graph-based methods, but with a
lower (linear) time complexity.",4.1 Results,[0],[0]
"On the other hand, the neural network components help us maintain much fewer features than MST-full, which carefully selects 6 different sets of features that are usually obtained using extra tools and resources.",4.1 Results,[0],[0]
"And, the neural network design is flexible enough to incorporate various clues into a uniform framework, just like how we introduce the memory networks as a proxy to capture discourse cohesion.
",4.1 Results,[0],[0]
"In the RST corpus, when the distance between two EDUs is larger, there are usually fewer numbers of such EDU pairs, but the parsing performance for those long span cases drops more significantly.",4.1 Results,[0],[0]
"For example, the LAS is even lower than 5% for those dependencies that have a range of 6 EDUs.",4.1 Results,[0],[0]
We take a detailed look at the parsing performance for dependencies at different lengths (from 1 to 6 as an example) using coarse-grained relations.,4.1 Results,[0],[0]
"As shown in Table 2, compared with the Basic method, both UAS and LAS of the Refined method are improved significantly in almost all spans, where we observe more prominent improvement for the UAS in larger spans such as span 5 and span 6, with about 8.70% and 6.38%, respectively.
",4.1 Results,[0],[0]
"Finally, let us take a detailed comparison between Refined and Basic to investigate the advantages of capturing discourse cohesion.",4.1 Results,[0],[0]
"Note that, our Refined method wins Basic in almost all relations.",4.1 Results,[0],[0]
"Here, we discuss one typical relation List, which often indicates a long span
dependency between a pair of EDUs.",4.1 Results,[0],[0]
"In the test set of RST, the average span for List is 7.55, with the max span of 69.",4.1 Results,[0],[0]
"Our Refined can successfully identify 55 of them, with an average span of 9.02 and the largest one of 63, while, the Basic method can only identify 41 edges labeled with List, which are mostly shorter cases, with an average span of 1.32 and the largest one of 5.",4.1 Results,[0],[0]
"More detailedly, there are 18 edges that are correctly identified by our Refined but missed by the Basic method.",4.1 Results,[0],[0]
The average span of those dependencies is 25.39.,4.1 Results,[0],[0]
"It is easy to find that without further considerations in discourse structures, the Basic method has limited ability in correctly identifying longer span dependencies.",4.1 Results,[0],[0]
"And those comparisons prove again that our Refined can take better advantage of modeling discourse cohesion, which enables our model to perform better in long span scenarios.",4.1 Results,[0],[0]
"In this paper, we propose to utilize memory networks to model discourse cohesion automatically.",5 Conclusions,[0],[0]
"By doing so we could capture the topic change or threads within a discourse, which can further improve the discourse parsing performance, especially for long span scenarios.",5 Conclusions,[0],[0]
Experimental results on the RST Discourse Treebank show that our proposed method can characterize the discourse cohesion efficiently and archive significant improvement over traditional feature based discourse parsing methods.,5 Conclusions,[0],[0]
"We would like to thank our anonymous reviewers, Bingfeng Luo, and Sujian Li for their helpful comments and suggestions, which greatly improved our work.",Acknowledgments,[0],[0]
"This work is supported by National High Technology R&D Program of China (Grant No.2015AA015403), and Natural Science Foundation of China (Grant No. 61672057, 61672058).",Acknowledgments,[0],[0]
"For any correspondence, please contact Yansong Feng.",Acknowledgments,[0],[0]
Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance.,abstractText,[0],[0]
"Most existing approaches design sophisticated features or exploit various off-the-shelf tools, but achieve little success.",abstractText,[0],[0]
"In this paper, we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account.",abstractText,[0],[0]
"The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios.",abstractText,[0],[0]
"Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly 1.",abstractText,[0],[0]
Modeling Discourse Cohesion for Discourse Parsing via Memory Network,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4758–4765 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4758",text,[0],[0]
Over two decades after the seminal work by Picard (1997),1 Introduction,[0],[0]
"the quest of Affective Computing, to ease the interaction with computers by giving them a sense of how emotions shape our perception and behavior, is still far from being fulfilled.",1 Introduction,[0],[0]
"Undoubtedly, major progress has been made in NLP, with sentiment analysis being one of the most vivid and productive areas in recent years (Liu, 2015).
",1 Introduction,[0],[0]
"However, the vast majority of contributions has focused on polarity prediction, typically only distinguishing between positive and negative feeling
*",1 Introduction,[0],[0]
These authors contributed equally to this work.,1 Introduction,[0],[0]
"Anneke Buffone designed and supervised the crowdsourcing task and the survey described in Section 2, and provided psychological background knowledge.",1 Introduction,[0],[0]
"Sven Buechel was responsible for corpus creation, data analysis, and modeling.",1 Introduction,[0],[0]
"The technical set-up of the crowdsourcing task and the survey was done jointly by both first authors.
",1 Introduction,[0],[0]
"†Work conducted while being at the University of Pennsylvania.
or evaluation, usually in social media postings or product reviews (Rosenthal et al., 2017; Socher et al., 2013).",1 Introduction,[0],[0]
"Only very recently, researchers started exploring more sophisticated models of human emotion on a larger scale (Wang et al., 2016; Abdul-Mageed and Ungar, 2017; Mohammad and Bravo-Marquez, 2017a; Buechel and Hahn, 2017, 2018a,b).",1 Introduction,[0],[0]
"Yet such approaches, often rooted in psychological theory, also turned out to be more challenging in respect to annotation and modeling (Strapparava and Mihalcea, 2007).
",1 Introduction,[0],[0]
"Surprisingly, one of the most valuable affective phenomena for improving human-machine interaction has received surprisingly little attention: Empathy.",1 Introduction,[0],[0]
"Prior work focused mostly on spoken dialogue, commonly addressing conversational agents, psychological interventions, or call center applications (McQuiggan and Lester, 2007; Fung et al., 2016; Pérez-Rosas et al., 2017; Alam et al., 2017).
",1 Introduction,[0],[0]
"In contrast, to the best of our knowledge, only three contributions (Xiao et al., 2012; Gibson et al., 2015; Khanpour et al., 2017) previously addressed text-based empathy prediction1 (see Section 4 for details).",1 Introduction,[0],[0]
"Yet, all of them are limited in three ways: (a) neither of their corpora are available leaving the NLP community without shared data, (b) empathy ratings were provided by others than the one actually experiencing it which qualifies only as a weak form of ground truth, and (c) their notion of empathy is quite basic, falling short of current and past theory.
",1 Introduction,[0],[0]
1 Psychological studies commonly distinguish between state and trait empathy.,1 Introduction,[0],[0]
"While the former construct describes the amount of empathy a person experiences as a direct result of encountering a given stimulus, the latter refers to how empathetic one is on average and across situations.",1 Introduction,[0],[0]
This studies exclusively addresses state empathy.,1 Introduction,[0],[0]
"For a contribution addressing trait empathy from an NLP perspective, see AbdulMageed et al. (2017).
",1 Introduction,[0],[0]
In this contribution we present the first publicly available gold standard for text-based empathy prediction.,1 Introduction,[0],[0]
It is constructed using a novel annotation methodology which reliably captures empathy assessments via multi-item scales.,1 Introduction,[0],[0]
"The corpus as well as our work as a whole is also unique in being—to the best of our knowledge—the first computational approach differentiating multiple types of empathy, empathic concern and personal distress, a distinction well recognized throughout psychology and other disciplines.2",1 Introduction,[0],[0]
Background.,2 Corpus Design and Methodology,[0],[0]
Most psychological theories of empathic states are focused on reactions to negative rather than positive events.,2 Corpus Design and Methodology,[0],[0]
"Empathy for positive events remains less well understood and is thought to be regulated differently (Morelli et al., 2015).",2 Corpus Design and Methodology,[0],[0]
Thus we focus on empathetic reactions to need or suffering.,2 Corpus Design and Methodology,[0],[0]
"Despite the fact that everyone has an immediate, implicit understanding of empathy, research has been vastly inconsistent in its definition and operationalization (Cuff et al., 2016).",2 Corpus Design and Methodology,[0],[0]
"There is agreement, however, that there are multiple forms of empathy (see below).",2 Corpus Design and Methodology,[0],[0]
"The by far most widely cited state empathy scale is Batson’s Empathic Concern – Personal Distress Scale (Batson et al., 1987), henceforth empathy and distress.
",2 Corpus Design and Methodology,[0],[0]
"Distress is a self-focused, negative affective state that occurs when one feels upset due to witnessing an entity’s suffering or need, potentially via “catching” the suffering target’s negative emotions.",2 Corpus Design and Methodology,[0],[0]
"Empathy is a warm, tender, and compassionate feeling for a suffering target.",2 Corpus Design and Methodology,[0],[0]
"It is other-focused, retains self-other separation, and is marked by relatively more positive affect (Batson and Shaw, 1991; Goetz et al., 2010; Mikulincer and Shaver, 2010; Sober and Wilson, 1997).
",2 Corpus Design and Methodology,[0],[0]
Selection of News Stories.,2 Corpus Design and Methodology,[0],[0]
"Two research interns (psychology undergraduates) collected a total of 418 articles from popular online news platforms, selected to likely evoke empathic reactions, after being briefed on the goal and background of this study.",2 Corpus Design and Methodology,[0],[0]
"These articles were then used to elicit empathic responses in participants.
",2 Corpus Design and Methodology,[0],[0]
Acquiring Text and Ratings.,2 Corpus Design and Methodology,[0],[0]
The corpus acquisition was set up as a crowdsourcing task on MTurk.com pointing to a Qualtrics.com questionnaire.,2 Corpus Design and Methodology,[0],[0]
"The participants completed back-
2Data and code are available at: https://github. com/wwbp/empathic_reactions
ground measures on demographics and personality, and then proceeded to the main part of the survey where they read a random selection of five of the news articles.",2 Corpus Design and Methodology,[0],[0]
"After reading each of the articles, participants were asked to rate their level of empathy and distress before describing their thoughts and feelings about it in writing.
",2 Corpus Design and Methodology,[0],[0]
"In contrast to previous work, this set-up allowed us to acquire empathy scores of the actual writer of a text, instead of having to rely on an external evaluation by third parties (often student assistants with background in computer science).",2 Corpus Design and Methodology,[0],[0]
"Arguably, our proposed annotation methodology yields more appropriate gold data, yet also leads to more variance in the relationship between linguistic features and empathic state ratings.",2 Corpus Design and Methodology,[0],[0]
That is because each rating reflects a single individual’s feelings rather than a more stable average assessment by multiple raters.,2 Corpus Design and Methodology,[0],[0]
"To account for this, we use multi-item scales as is common practice in psychology.",2 Corpus Design and Methodology,[0],[0]
"I.e., participants give ratings for multiple items measuring the same construct (e.g., empathy) which are then averaged to obtain more reliable results.",2 Corpus Design and Methodology,[0],[0]
"As far as we know, this is the first time that multiitem scales are used in sentiment analysis.3
In our case, participants used Batson’s Empathic Concern – Personal Distress Scale (see above), i.e, rating 6 items for empathy (e.g., warm, tender, moved) and 8 items for distress (e.g., troubled, disturbed, alarmed) using a 7-point scale for each of those (see Appendix for details).",2 Corpus Design and Methodology,[0],[0]
"After rating their empathy, participants were asked to share their feelings about the article as they would with a friend in a private message or with a group of friends as a social media post in 300 to 800 characters.",2 Corpus Design and Methodology,[0],[0]
"Our final gold standard consists of these messages combined with the numeric ratings for empathy and distress.
",2 Corpus Design and Methodology,[0],[0]
"In sum, 403 participants completed the survey.",2 Corpus Design and Methodology,[0],[0]
"Median completion time was 32 minutes and each participant received 4 USD as compensation.
",2 Corpus Design and Methodology,[0],[0]
Post-Processing.,2 Corpus Design and Methodology,[0],[0]
Each message was manually reviewed by the authors.,2 Corpus Design and Methodology,[0],[0]
"Responses which deviated from the task description (e.g., mere copying from the articles at display) were removed (31 responses, 155 messages), leading to a total 1860 messages in our final corpus.",2 Corpus Design and Methodology,[0],[0]
"Gold ratings for empathy and distress were derived by averaging the respective items of the two multi-item scales.
",2 Corpus Design and Methodology,[0],[0]
"3 Here, we use sentiment as an umbrella term subsuming semantic orientation, emotion, as well as highly related concepts such as empathy.",2 Corpus Design and Methodology,[0],[0]
"For a first impression of the language of our new gold standard, we provide illustrative examples in Table 1.",3 Corpus Analysis,[0],[0]
"The participant in Example (1) displays higher empathy than distress, (2) displays higher distress than empathy, and (3) shows neither empathic state, but employs sarcasm, colloquialisms and social-media-style acronyms to express lack of emotional response to the article.",3 Corpus Analysis,[0],[0]
"As can be seen, the language of our corpus is diverse and authentic, featuring many phenomena of natural language which render its computational understanding difficult, thus constituting a sound but challenging gold standard for empathy prediction.
",3 Corpus Analysis,[0],[0]
Token Counts.,3 Corpus Analysis,[0],[0]
"We tokenized the 1860 messages using NLTK tools (Bird, 2006).",3 Corpus Analysis,[0],[0]
"In total, our corpus amounts to 173, 686 tokens.",3 Corpus Analysis,[0],[0]
"Individual message length varies between 52 and 198 tokens, the median being 84.",3 Corpus Analysis,[0],[0]
"See Appendix for details.
",3 Corpus Analysis,[0],[0]
Rating Distribution.,3 Corpus Analysis,[0],[0]
"Figure 1 displays the bivariate distribution of empathy and distress rat-
ings.",3 Corpus Analysis,[0],[0]
"As can be seen both target variables have a clear linear dependence, yet show only a moderate Pearson correlation of r=.451, similar to what was found in prior research (Batson et al., 1987, 1997).",3 Corpus Analysis,[0],[0]
"This finding supports that the two scales capture distinct affective phenomena and underscores the importance of our decision to describe empathic states in terms of multiple target variables, constituting a clear advancement over previous work.",3 Corpus Analysis,[0],[0]
"Both kinds of ratings show good coverage over the full range of the scales.
",3 Corpus Analysis,[0],[0]
Reliability of Ratings.,3 Corpus Analysis,[0],[0]
"Since each message is annotated by only one rater, its author, typical measures of inter-rater agreement are not applicable.",3 Corpus Analysis,[0],[0]
"Instead, we compute split-half reliability (SHR), a standard approach in psychology (Cronbach, 1947) which also becomes increasingly popular in sentiment analysis (Mohammad and BravoMarquez, 2017a; Buechel and Hahn, 2018a).",3 Corpus Analysis,[0],[0]
"SHR is computed by splitting the ratings for the individual scale items (e.g., warm, tender, etc. for empathy) of all participants randomly into two groups, averaging the individual item ratings for each group and participant, and then measuring the correlation between both groups.",3 Corpus Analysis,[0],[0]
"This process is repeated 100 times with random splits, before again averaging the results.",3 Corpus Analysis,[0],[0]
"Doing so for empathy and distress, we find very high4 SHR values of r=.875 and .924, respectively.",3 Corpus Analysis,[0],[0]
"In this section, we provide experimental results for modeling empathy and distress ratings based on the participants’ messages (see Section 2).",4 Modeling Empathy and Distress,[0],[0]
"We examine three different types of models, varying in
4 For a comparison against previously reported SHR values for different emotional categories, see Mohammad and Bravo-Marquez (2017b).
design complexity.",4 Modeling Empathy and Distress,[0],[0]
"Distinct models were trained for empathy and distress prediction.
",4 Modeling Empathy and Distress,[0],[0]
"First, ten percent of our newly created gold standard were randomly sampled to be used in development experiments.",4 Modeling Empathy and Distress,[0],[0]
"Then, the main experiment was conducted using 10-fold crossvalidation (CV), providing each model with identical train-test splits to increase reliability.",4 Modeling Empathy and Distress,[0],[0]
"The dev set was excluded for the CV experiment.
",4 Modeling Empathy and Distress,[0],[0]
Model performance is measured in terms of Pearson correlation r between predicted values and the human gold ratings.,4 Modeling Empathy and Distress,[0],[0]
"Thus, we phrase the prediction of empathy and distress as regression problems.
",4 Modeling Empathy and Distress,[0],[0]
"The input to our models is based on word embeddings, namely the publicly available FastText embeddings which were trained on Common Crawl (≈600B tokens) (Bojanowski et al., 2017; Mikolov et al., 2018).
",4 Modeling Empathy and Distress,[0],[0]
Ridge.,4 Modeling Empathy and Distress,[0],[0]
"Our first approach is Ridge regression, an `2-regularized version of linear regression.",4 Modeling Empathy and Distress,[0],[0]
The centroid of the word embeddings of the words in a message is used as features (embedding centroid).,4 Modeling Empathy and Distress,[0],[0]
"The regularization coefficient α is automatically chosen from {1, .5, .1, ..., .0001} during training.
FFN.",4 Modeling Empathy and Distress,[0],[0]
"Our second approach is a Feed-Forward Net with two hidden layers (256 and 128 units, respectively) with ReLU activation.",4 Modeling Empathy and Distress,[0],[0]
"Again, the embedding centroid is used as features.
CNN.",4 Modeling Empathy and Distress,[0],[0]
"The last approach is a Convolutional Neural Net.5 We use a single convolutional layer with filter sizes 1 to 3, each with 100 output channels, followed by an average pooling layer and a dense layer of 128 units.",4 Modeling Empathy and Distress,[0],[0]
"ReLUs were used for the convolutional and again for the dense layer.
",4 Modeling Empathy and Distress,[0],[0]
"Both deep learning models were trained using the Adam optimizer (Kingma and Ba, 2015) with a fixed learning rate of 10−3 and a batch size of 32.",4 Modeling Empathy and Distress,[0],[0]
We trained for a maximum of 200 epochs yet applied early stopping if the performance on the validation set did not improve for 20 consecutive epochs.,4 Modeling Empathy and Distress,[0],[0]
"We applied dropout with probabilities of .2, .5",4 Modeling Empathy and Distress,[0],[0]
and .5,4 Modeling Empathy and Distress,[0],[0]
"on input, dense and pooling layers, respectively.",4 Modeling Empathy and Distress,[0],[0]
Moreover `2 regularization of .001 was applied to the weights of conv and dense layers.,4 Modeling Empathy and Distress,[0],[0]
"Word embeddings were not updated.
",4 Modeling Empathy and Distress,[0],[0]
The results are provided in Table 2.,4 Modeling Empathy and Distress,[0],[0]
"As can be seen, all of our models achieve satisfying performance figures ranging between r=.379 and .444,
5 Recurrent models did not perform well during development due to high sequence length.
",4 Modeling Empathy and Distress,[0],[0]
given the assumed difficulty of the task (see Section 3).,4 Modeling Empathy and Distress,[0],[0]
"On average over the two target variables, the CNN performs best, followed by Ridge and the FFN.",4 Modeling Empathy and Distress,[0],[0]
"While the CNN significantly outperforms the other models in every case, the differences between Ridge and the FFN are not statistically significant for either empathy or distress.6",4 Modeling Empathy and Distress,[0],[0]
The improvements of the CNN over the other two approaches are much more pronounced for distress than for empathy.,4 Modeling Empathy and Distress,[0],[0]
"Since only the CNN is able to capture semantic effects from composition and word order, our data suggest that these phenomena are more important for predicting distress, whereas lexical features alone already perform quite well for empathy.
",4 Modeling Empathy and Distress,[0],[0]
Discussion.,4 Modeling Empathy and Distress,[0],[0]
"In comparison to closely related tasks such as emotion prediction (Mohammad and Bravo-Marquez, 2017a)",4 Modeling Empathy and Distress,[0],[0]
our performance figures for empathy and distress prediction are generally lower.,4 Modeling Empathy and Distress,[0],[0]
"However, given the small amount of previous work for the problem at hand, we argue that our results are actually quite strong.",4 Modeling Empathy and Distress,[0],[0]
"This becomes obvious, again, in comparison with emotion analysis where early work achieved correlation values around r=.3 at most (Strapparava and Mihalcea, 2007).",4 Modeling Empathy and Distress,[0],[0]
"Yet state-of-the-art performance literally doubled over the last decade (Beck, 2017), in part due to much larger training sets.
",4 Modeling Empathy and Distress,[0],[0]
"Comparison to the limited body of previous work in text-based empathy prediction is difficult for a number of reasons, e.g., differences in domain, evaluation metric, as well as methodology and linguistic level of annotation.",4 Modeling Empathy and Distress,[0],[0]
"Khanpour et al. (2017) annotate and model empathy in online health communities on the sentence-level, whereas the instances in our corpus are much longer and comprise multiple sentences.",4 Modeling Empathy and Distress,[0],[0]
"In contrast to our work, they treat empathy prediction as a classification problem.",4 Modeling Empathy and Distress,[0],[0]
"Their best performing model, a CNN-LSTM, achieves an F-score of .78.",4 Modeling Empathy and Distress,[0],[0]
"Gibson
6We use a two-tailed t-test for paired samples based on the results of the individual CV runs; p < .05.
et al. (2015) predict therapists’ empathy in motivational interviews.",4 Modeling Empathy and Distress,[0],[0]
Each therapy session transcript received one numeric score.,4 Modeling Empathy and Distress,[0],[0]
"Thus, each prediction is based on much more language data than our individual messages comprise.",4 Modeling Empathy and Distress,[0],[0]
"Their best model achieves a Spearman rank correlation of .61 using n-gram and psycholinguistic features.
",4 Modeling Empathy and Distress,[0],[0]
"Our contribution goes beyond both of these studies by, first, enriching empathy prediction with personal distress and, second, by annotating and modeling the empathic state actually felt by the writer, instead of relying on external assessments.",4 Modeling Empathy and Distress,[0],[0]
"This contribution was the first to attempt empathy prediction in terms of multiple target variables, empathic concern and personal distress.",5 Conclusion,[0],[0]
"We proposed a novel annotation methodology capturing empathic states actually felt by the author of a statement, instead of relying on third-party assessments.",5 Conclusion,[0],[0]
"To ensure high reliability in this singlerating setting, we employ multi-item scales in line with best practices in psychology.",5 Conclusion,[0],[0]
"Hereby we create the first publicly available gold standard for empathy prediction in written language, our survey being set-up and supervised by an expert psychologist.",5 Conclusion,[0],[0]
"Our analysis shows that the data set excels with high rating reliability and an authentic and diverse language, rich of challenging phenomena such as sarcasm.",5 Conclusion,[0],[0]
"We provide experimental results for three different predictive models, our CNN turning out superior.",5 Conclusion,[0],[0]
"Sven Buechel would like to thank his doctoral advisor Udo Hahn, JULIE Lab, for funding his research visit at the University of Pennsylvania.",Acknowledgments,[0],[0]
"Details on Stimulus and Instructions
Before being used in our survey, the selected news articles were categorized by the research interns who gathered them in terms of their intensity of suffering (major or minor), cause of suffering (political, human, nature or other), patient of suffering (humans, animals, environment, or other) and scale of suffering (individual or mass).",A Supplemental Material,[0],[0]
Research interns also provided a short list of key words for each article.,A Supplemental Material,[0],[0]
"This additional information was gathered to examine the influence of these factors on empathy elicitation and modeling performance in later studies.
",A Supplemental Material,[0],[0]
"At the beginning of the survey participants completed background items covering general demographics (including age, gender, and ethnicity), the most commonly used trait empathy scale, the Interpersonal Reactivity Index (Davis, 1980), a brief assessment of the Big 5 personality traits (Gosling et al., 2003), life satisfaction (Diener et al., 1985), as well as a brief measure of generalized trust.
",A Supplemental Material,[0],[0]
"After reading each of the articles, participants rated their level of empathic concern and personal distress using multi-item scales.",A Supplemental Material,[0],[0]
"Figure 2
shows a cropped screenshot of the survey hosted on Qualtrics.com.",A Supplemental Material,[0],[0]
"The first six items (warm, tender, sympathetic, softhearted, moved, and compassionate) refer to empathy.",A Supplemental Material,[0],[0]
"The last eight items (worried, upset, troubled, perturbed, grieved, disturbed, alarmed, and distressed) refer to distress.
",A Supplemental Material,[0],[0]
"After completing the rating items, participants were instructed to describe their reactions in writing as follows: Now that you have read this article, please write a message to a friend or friends about your feelings and thoughts regarding the article you just read.",A Supplemental Material,[0],[0]
This could be a private message to a friend or something you would post on social media.,A Supplemental Material,[0],[0]
Please do not identify your intended friend(s) — just write your thoughts about the article as if you were communicating with them.,A Supplemental Material,[0],[0]
"Please use between 300 and 800 characters.
",A Supplemental Material,[0],[0]
"Further Corpus Analyses
The word clouds in Figure 3 and Figure 4 show 1- grams of our corpus which correlate significantly (Benjamini-Hochberg corrected p < .05) with high empathy and high distress ratings, respectively.",A Supplemental Material,[0],[0]
"In the word clouds, larger size indicates higher correlation and the color scale, gray-bluered, indicates word frequency, dark red being most prevalent.",A Supplemental Material,[0],[0]
"The Differential Language Analysis Toolkit (Schwartz et al., 2017) was utilized for this analysis.",A Supplemental Material,[0],[0]
"As can be seen, the word clouds display high face-validity, giving further evidence for the soundness of our acquisition methodology.
",A Supplemental Material,[0],[0]
Figure 5 displays the distribution of the message length of our corpus in tokens.,A Supplemental Material,[0],[0]
As can be seen the majority of messages contain between 60 and 100 tokens.,A Supplemental Material,[0],[0]
Yet outliers go up to almost 200.,A Supplemental Material,[0],[0]
The introduction of a character cap for the writing task proved successful in comparison to a pilot study where this measure has not been in place.,A Supplemental Material,[0],[0]
"In the latter case, the maximum number of tokens was nearly twice as high due to even stronger outliers.",A Supplemental Material,[0],[0]
Computational detection and understanding of empathy is an important factor in advancing human-computer interaction.,abstractText,[0],[0]
"Yet to date, textbased empathy prediction has the following major limitations: It underestimates the psychological complexity of the phenomenon, adheres to a weak notion of ground truth where empathic states are ascribed by third parties, and lacks a shared corpus.",abstractText,[0],[0]
"In contrast, this contribution presents the first publicly available gold standard for empathy prediction.",abstractText,[0],[0]
It is constructed using a novel annotation methodology which reliably captures empathy assessments by the writer of a statement using multiitem scales.,abstractText,[0],[0]
"This is also the first computational work distinguishing between multiple forms of empathy, empathic concern, and personal distress, as recognized throughout psychology.",abstractText,[0],[0]
"Finally, we present experimental results for three different predictive models, of which a CNN performs the best.",abstractText,[0],[0]
Modeling Empathy and Distress in Reaction to News Stories,title,[0],[0]
"Proceedings of the SIGDIAL 2018 Conference, pages 20–31, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics
20",text,[0],[0]
"Every person is unique, yet they often share general patterns of behavior.",1 Introduction,[0],[0]
"Theories of personality aim to explain these patterns in terms of personality traits, e.g. the Big Five traits of extraversion or agreeableness.",1 Introduction,[0],[0]
"Previous work has shown: (1) the language that people generate includes linguistic features that express these personality traits; (2) it is possible to train models to automatically recognize a person’s personality from his language; and (3) it is possible to automatically train models for natural language generation that express personality traits (Pennebaker and King, 1999; Mairesse et al., 2007; Mairesse and Walker, 2011; Gill et al., 2012).
",1 Introduction,[0],[0]
"A distinct line of work has shown that people adapt to one another’s conversational behaviors and that conversants reliably re-use or mimic many
different aspects of their partner’s verbal and nonverbal behaviors, including lexical and syntactical traits, accent, speech rate, pause length, etc.",1 Introduction,[0],[0]
"(Coupland et al., 1988; Willemyns et al., 1997; Brennan and Clark, 1996; Branigan et al., 2010; Coupland et al., 1988; Parent and Eskenazi, 2010; Reitter et al., 2006a; Chartrand and Bargh, 1999; Hu et al., 2014).",1 Introduction,[0],[0]
"Previous work primarily focuses on developing methods on measuring adaptation in dialog, and studies have shown that adaptation measures are correlated with task success (Reitter and Moore, 2007), and that social variables such as power affect adaptation (Danescu-Niculescu-Mizil et al., 2012).
",1 Introduction,[0],[0]
We posit that it is crucial to enable adaptation in computer agents in order to make them more human-like.,1 Introduction,[0],[0]
"However, we need models to control the amount of adaptation in natural language generation.",1 Introduction,[0],[0]
"A primary challenge is that dialogs exhibit many different types of linguistic features, any or all of which, in principle, could be adapted.",1 Introduction,[0],[0]
"Previous work has often focused on individual features when measuring adaptation, and referring expressions have often been the focus, but the conversants in the dialog in Figure 1 from the ArtWalk Corpus appear to be adapting to the discourse marker okay in D98 and F98, the hedge kinda like in F100, and to the adjectival phrase like a vase in D101.
",1 Introduction,[0],[0]
"Therefore we propose a novel adaptation measure, Dialog Adaptation Score (DAS), which can model adaptation on any subset of linguistic features and can be applied on a turn by turn basis to any segment of dialog.",1 Introduction,[0],[0]
"Consider the example shown in Table 1, where the context (prime) is taken from an actual dialog.",1 Introduction,[0],[0]
"A response (target) with no adaptation makes the utterance stiff (DAS = 0), and too much adaptation (to all four discourse markers in prime, DAS = 1) makes the utterance unnatural.",1 Introduction,[0],[0]
"Our hypothesis is that we can learn models to approximate the appropriate amount of adaptation from the actual human response to the context (to discourse marker “okay”, DAS = 0.25).
",1 Introduction,[0],[0]
Conversants in dialogs express their own personality and adapt to their dialog partners simultaneously.,1 Introduction,[0],[0]
Our measure of adaptation produces models for adaptive natural language generation (NLG) for dialogs that integrates the predictions of both personality theories and adaptation theories.,1 Introduction,[0],[0]
"NLGs need to operate as a dialog unfolds on a turnby-turn basis, thus the requirements for a model of adaptation for NLG are different than simply measuring adaptation.
",1 Introduction,[0],[0]
Context:,1 Introduction,[0],[0]
okay alright,1 Introduction,[0],[0]
so,1 Introduction,[0],[0]
yeah,1 Introduction,[0],[0]
Im looking at 123 Locust right now Linguistic Features:,1 Introduction,[0],[0]
Discourse markers:,1 Introduction,[0],[0]
"okay, alright, so, yeah",1 Introduction,[0],[0]
"Referring expressions: 123 Locust Syntactic structures: VP->VBP+VP, VP->VBG+PP+ADVB ...
",1 Introduction,[0],[0]
We apply our method to multiple corpora to investigate how the dialog situation and speaker roles affect the level and type of adaptation to the other speaker.,1 Introduction,[0],[0]
"We show that:
• Different feature sets and conversational situations can have different adaptation models; • Speakers usually adapt more when they have
the initiative; • The degree of adaptation may vary over the
course of a dialog, and decreases as the adaptation window size increases.",1 Introduction,[0],[0]
Our goal is an algorithm for adaptive natural language generation (NLG) that controls the system output at each step of the dialog.,2 Method and Overview,[0],[0]
Our first aim therefore is a measure of dialog adaptation that can be applied on a turn by turn basis as a dialog unfolds.,2 Method and Overview,[0],[0]
"For this purpose, previous measures of dialog adaptation (Stenchikova and Stent, 2007; Danescu-Niculescu-Mizil et al., 2011) have two limitations: (1) their calculation require the complete dialog, and (2) they focus on single features and do not provide a model to control the interaction of multiple parameters in a single output, while our method measures adaptation with respect to any set of features.",2 Method and Overview,[0],[0]
"We further compare our method to existing measures in Section 6.
",2 Method and Overview,[0],[0]
"Measures of adaptation focus on prime-target pairs: (p, t), in which the prime contains linguistic features that the target may adapt to.",2 Method and Overview,[0],[0]
"While linguistic adaptation occur beyond the next turn, we simplify the calculation by using a window size of 1 for most experiments: for every utterance in the dialog (prime), we consider the next utterance by a different speaker as the target, if any.",2 Method and Overview,[0],[0]
We show the decay of adaptation with increasing window size in a separate experiment.,2 Method and Overview,[0],[0]
"When generating (p, t) pairs, it is possible to consider only speaker A adapting to speaker B (target=A), only speaker B adapting to speaker A (target=B), or both at the same time (target=Both).",2 Method and Overview,[0],[0]
"In the following definition, FCi(p) is the count of features in prime p of the i-th (p, t) pair, n is the total number of prime-target pairs in which FCi(p) 6= 0, similarly, FCi(p ∧ t) is the count of features in both prime p and target t.",2 Method and Overview,[0],[0]
"We define Dialog Adaptation Score (DAS) as:
DAS = 1
n n∑ i=1",2 Method and Overview,[0],[0]
"FCi(p ∧ t) FCi(p)
",2 Method and Overview,[0],[0]
"Within a feature set, DAS reflects the average probability that features in prime are adapted in target across all prime-target pairs in a dialog.",2 Method and Overview,[0],[0]
"Thus our Dialog Adaptation Score (DAS) models adaptation with respect to feature sets, providing a wholedialog adaptation model or a turn-by-turn adaptation model.",2 Method and Overview,[0],[0]
"The strength of DAS is the ability to model different classes of features related to individual differences such as personalities or social variables of interest such as status.
",2 Method and Overview,[0],[0]
DAS scores measured using various feature sets can be used as a vector model to control adaptation in Natural Language Generation (NLG).,2 Method and Overview,[0],[0]
"Although
we leave the application of DAS to NLG to future work, here we describe how we expect to use it.",2 Method and Overview,[0],[0]
"We consider the use of DAS with three NLG architectures: Overgeneration and Rank, Statistical Parameterized NLG, and Neural NLG.",2 Method and Overview,[0],[0]
Overgenerate and Rank.,2 Method and Overview,[0],[0]
"In this approach, different modules propose a possibly large set of next utterances in parallel, which are then fed to a (trained) ranker that outputs the top-ranked utterance.",2 Method and Overview,[0],[0]
"Previous work on adaptation/alignment in NLG has made use of this architecture (Brockmann, 2009; Buschmeier et al., 2010).",2 Method and Overview,[0],[0]
We can rank the generated responses based on the distances between their DAS vectors and learned DAS adaptation model.,2 Method and Overview,[0],[0]
The response with the smallest distance is the response with the best amount of adaptation.,2 Method and Overview,[0],[0]
We can also emphasize specific feature sets by giving weights to different dimensions of the vector and calculating weighted distance.,2 Method and Overview,[0],[0]
"For instance, in order to adapt more to personality and avoid too much lexical mimicry, one could prioritize related LIWC features, and adapt by using words from the same LIWC categories.",2 Method and Overview,[0],[0]
Statistical Parameterized NLG.,2 Method and Overview,[0],[0]
"Some NLG engines provide a list of parameters that can be controlled at generation time (Paiva and Evans, 2004; Lin and Walker, 2017).",2 Method and Overview,[0],[0]
DAS scores can be used as generation decision probabilities.,2 Method and Overview,[0],[0]
A DAS score of 0.48 for the LIWC feature set indicates that the probability of adapting to LIWC features in discourse context (prime) is 0.48.,2 Method and Overview,[0],[0]
"By mapping DAS scores to generation parameters, the generator could be directly controlled to exhibit the correct amount of adaptation for any feature set.",2 Method and Overview,[0],[0]
Neural NLG.,2 Method and Overview,[0],[0]
"Recent work in Neural NLG (NNLG) explores controlling stylistic variation in outputs using a vector to encode style parameters, possibly in combination with the use of a context vector to represent the dialog context (Ficler and Goldberg, 2017; Oraby et al., 2018).",2 Method and Overview,[0.9513763079070974],"['The gated recurrent unit (GRU) is used as the recurrent unit in each RNN, which is shown to have promising results in speech recognition and machine translation (Cho et al., 2014).']"
The vector based probabilities that are represented in the DAS adaptation model could be encoded into the context vector in NNLG.,2 Method and Overview,[0],[0]
"No other known adaptation measures could be used in this way.
",2 Method and Overview,[0],[0]
"We hypothesize that different conversational contexts may lead to more or less adaptive behavior, so we apply DAS on four human-human dialog corpora: two task-oriented dialog corpora that were designed to elicit adaptation (ArtWalk and Walking Around), one topic-centric spontaneous dialog corpus (Switchboard), and the MapTask Corpus used in much previous work.",2 Method and Overview,[0],[0]
"We obtain linguistic
features using fully automatic annotation tools, described in Section 4.",2 Method and Overview,[0],[0]
We learn models of adaptation from these dialogs on various feature sets.,2 Method and Overview,[0],[0]
We first validate the DAS measure by showing that DAS distinguishes original dialogs from dialogs where the orders of the turns have been randomized.,2 Method and Overview,[0],[0]
We then show how DAS varies as a function of the feature sets used and the dialog corpora.,2 Method and Overview,[0],[0]
"We also show how DAS can be used for fine-grained adaptation by applying DAS to individual dialog segments, and individual speakers, and illustrating the differences in adaptation as a function of these variables.",2 Method and Overview,[0],[0]
"Finally, we show how DAS scores decrease as the adaptation window size increases.",2 Method and Overview,[0],[0]
We develop models of adaptation using DAS on the following four corpora.,3 Corpora,[0],[0]
"ArtWalk Corpus (AWC).1 Figure 1 provides a sample of the Artwalk Corpus (Liu et al., 2016), a collection of mobile-to-Skype conversations between friend and stranger dyads performing a real world-situated task that was designed to elicit adaptation behaviors.",3 Corpora,[0],[0]
"Every dialog involves a stationary director on campus, and a follower downtown.",3 Corpora,[0],[0]
"The director provided directions to help the follower find 10 public art pieces such as sculptures, mosaics, or murals in downtown Santa Cruz.",3 Corpora,[0],[0]
The director had access to Google Earth views of the follower’s route and a map with locations and pictures of art pieces.,3 Corpora,[0],[0]
The corpus consists of transcripts of 24 friend and 24 stranger dyads (48 dialogs).,3 Corpora,[0],[0]
"In total, it contains approximately 185,000 words and 23,000 turns, from conversations that ranged from 24 to 55 minutes, or 197 to 691 turns.",3 Corpora,[0],[0]
"It includes referent negotiation, direction-giving, and small talk (non-task talk).2 Walking Around Corpus (WAC).3 The Walking Around Corpus",3 Corpora,[0],[0]
"(Brennan et al., 2013) consists of spontaneous spoken dialogs produced by 36 pairs of people, collected in order to elicit adaptation behaviors, as illustrated by Figure 2.",3 Corpora,[0],[0]
"In each dialog, a director navigates a follower using a mobile phone to 18 destinations on a medium-sized campus.",3 Corpora,[0],[0]
"Directors have access to a digital map marked with
1https://nlds.soe.ucsc.edu/artwalk 2For AWC and WAC, we remove annotations such as speech overlap, noises (laugh, cough) and indicators for short pauses, leaving only clean text.",3 Corpora,[0],[0]
"If more than one consecutive dialog turn has the same speaker, we merge them into one dialog turn.
",3 Corpora,[0],[0]
"3https://catalog.ldc.upenn.edu/ ldc2015s08
target destinations, labels (e.g. “Ship sculpture”), photos and followers’ real time location.",3 Corpora,[0],[0]
"Followers carry a cell phone with GPS, and a camera in order to take pictures of the destinations they visit.",3 Corpora,[0],[0]
Each dialog ranges from 175 to 885 turns.,3 Corpora,[0],[0]
"The major differences between AWC and WAC are (1) in order to elicit novel referring expressions and possible linguistic adaptation, destinations in AWC do not have provided labels; (2) AWC happens in a more open world setting (downtown) compared to WAC (university campus).",3 Corpora,[0],[0]
"Map Task Corpus (MPT).4 The Map Task Corpus (Anderson et al., 1991) is a set of 128 cooperative task-oriented dialogs involving two participants.",3 Corpora,[0],[0]
Each dialog ranges from 32 to 438 turns.,3 Corpora,[0],[0]
A director and a follower sit opposite one another.,3 Corpora,[0],[0]
Each has a paper map which the other cannot see (the maps are not identical).,3 Corpora,[0],[0]
The director has a route marked on their map; the follower has no route.,3 Corpora,[0],[0]
The participants’ goal is to reproduce the director’s route on the follower’s map.,3 Corpora,[0],[0]
"All maps consist of line drawing landmarks labelled with their names, such as “parked van”, “east lake”, or “white mountain”.",3 Corpora,[0],[0]
"Figure 3 shows an excerpt from the Map Task Corpus. Switchboard Corpus (SWBD).5 Switchboard (Godfrey et al., 1992) is a collection of two-speaker telephone conversations from all areas of the United States.",3 Corpora,[0],[0]
"An automatic operator handled the calls (giving recorded prompts, selecting and dialing another speaker, introducing discussion topics and recording the dialog).",3 Corpora,[0],[0]
"70 topics were provided, for example: pets, child care, music, and buying a car.",3 Corpora,[0],[0]
"Each topic has a corresponding prompt message played to the first speaker, e.g. “find out what kind of pets the
4http://groups.inf.ed.ac.uk/maptask/ 5https://catalog.ldc.upenn.edu/
ldc97s62
other caller has.”",3 Corpora,[0],[0]
"A subset of 200K utterances of Switchboard have also been tagged with dialog act tags (Jurafsky et al., 1997).",3 Corpora,[0],[0]
Each dialog contains 14 to 373 turns.,3 Corpora,[0],[0]
"Figure 1 provides an example of dialog act tags, such as b - Acknowledge (Backchannel), sv - Statement-opinion, sd - Statement-non-opinion, and % - Uninterpretable.",3 Corpora,[0],[0]
"We focus on this subset of the corpus.
",3 Corpora,[0],[0]
"Dialogs in SWBD have a different style from the three task-oriented, direction-giving corpora.",3 Corpora,[0],[0]
"Figure 4 illustrates how the SWBD dialogs are often lopsided: from utterance 14 to 18, speaker B states his opinion with verbose dialog turns, whereas speaker A only acknowledges and backchannels; from utterance 19 to 22, speaker A acts as the main speaker, whereas speaker B backchannels.",3 Corpora,[0],[0]
"Some theories of discourse define dialog turns as extending over backchannels, and we posit that this
would allow us to measure adaptation more faithfully, so we utilize the SWBD dialog act tags to filter turns that only contain backchannels, keeping only dialog turns with tags sd (Statement-nonopinion), sv (Statement-opinion), and bf (Summarize/reformulate).6",3 Corpora,[0],[0]
We then merge consecutive dialog turns from the same speaker.,3 Corpora,[0],[0]
"We consider the following feature sets: unigram, bigram, referring expressions, hedges/discourse markers, and Linguistic Inquiry and Word Count (LIWC) features.",4 Experimental Setup,[0],[0]
"Previous computational work on measuring linguistic adaptation in textual corpora have largely focused on lexical and syntactical features, which are included as baselines.",4 Experimental Setup,[0],[0]
"Referring expressions and discourse markers are key features that are commonly studied for adaptation behaviors in task-oriented dialogs, which are often hand annotated.",4 Experimental Setup,[0],[0]
Here we automatically extract these features by rules.,4 Experimental Setup,[0],[0]
"To model adaptation on the personality level, we draw features that correlate significantly with personality ratings from LIWC features.",4 Experimental Setup,[0],[0]
"We hypothesize that our feature sets will demonstrate different adaptation models.
",4 Experimental Setup,[0],[0]
"We lemmatize, POS tag and derive constituency structures using Stanford CoreNLP",4 Experimental Setup,[0],[0]
"(Manning et al., 2014).",4 Experimental Setup,[0],[0]
We then extract the following linguistic features from annotations and raw text.,4 Experimental Setup,[0],[0]
The following example features are based on D137 in Figure 2.,4 Experimental Setup,[0],[0]
Unigram Lemma/POS.,4 Experimental Setup,[0],[0]
We use lemma combined with POS tags to distinguish word senses.,4 Experimental Setup,[0],[0]
"E.g., lemmapos building/NN and lemmapos brick/NNS in D137.",4 Experimental Setup,[0],[0]
Bigram Lemma.,4 Experimental Setup,[0],[0]
"E.g., bigram the-brick and bigram side-of in D137.",4 Experimental Setup,[0],[0]
Syntactic Structure.,4 Experimental Setup,[0],[0]
"Following Reitter et al. (2006b), we take all the subtrees from a constituency parse tree (excluding the leaf nodes that contain words) as features.",4 Experimental Setup,[0],[0]
"E.g., syntax VP->VBP+PP and syntax ADJP-> DT+JJ in D137.",4 Experimental Setup,[0],[0]
The difference is that we use Stanford Parser rather than hand annotations.,4 Experimental Setup,[0],[0]
Referring Expression.,4 Experimental Setup,[0],[0]
Referring expressions are usually noun phrases.,4 Experimental Setup,[0],[0]
"We start by taking all constituency subtrees with root NP, then map the subtrees to their actual phrases in the text and remove all articles from the phrase, e.g., referexp little-concrete
6The filtering process removes 48.1% original dialog turns, but only 12.6% of the words.",4 Experimental Setup,[0],[0]
"Filtered dialogs have 3 to 85 dialog turns each.
and referexp math-building in D137.",4 Experimental Setup,[0],[0]
Hedge/Discourse Marker.,4 Experimental Setup,[0],[0]
"Hedges are mitigating words used to lessen the impact of an utterance, such as “actually” and “somewhat”.",4 Experimental Setup,[0],[0]
"Discourse markers are words or phrases that manage the flow and structure of discourse, such as “you know” and “I mean”.",4 Experimental Setup,[0],[0]
"We construct a dictionary of hedges and discourse markers, and use string matching to extract features, e.g., hedge you-know and hedge like in D137.",4 Experimental Setup,[0],[0]
LIWC.,4 Experimental Setup,[0],[0]
"Linguistic Inquiry and Word Count (Pennebaker et al., 2001) is a text analysis program that counts words in over 80 linguistic (e.g., pronouns, conjunctions), psychological (e.g., anger, positive emotion), and topical (e.g., leisure, money) categories.",4 Experimental Setup,[0],[0]
"E.g., liwc second-person and liwc informal in D137.",4 Experimental Setup,[0],[0]
"Because DAS features are binary, features such as Word Count and Number of New Lines are excluded.",4 Experimental Setup,[0],[0]
Personality LIWC.,4 Experimental Setup,[0],[0]
"Previous work reports for each LIWC feature whether it is significantly correlated with each Big Five trait (Mairesse et al., 2007) on conversational data (Mehl et al., 2006).",4 Experimental Setup,[0],[0]
"For each trait, we create feature sets consisting of such features.",4 Experimental Setup,[0],[0]
See Table 2.,4 Experimental Setup,[0],[0]
"In this section, we apply our DAS measure on the corpora introduced in Section 3.",5 Experiments on Modeling Adaptation,[0],[0]
"We first establish that our novel DAS measure is valid by testing whether it can distinguish dialogs in their original order vs. dialogs with randomly scrambled turns (the order of dialog turns are randomized within speakers), inspired by similar approaches in previous work (Gandhe and Traum, 2008; Ward and Litman, 2007; Barzilay and Lapata, 2005).",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"We calculate DAS scores for original dialogs and randomized dialogs using target=Both
(Sec. 2) to obtain overall adaptation scores for both speakers.
",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
We first test on lexical features (unigram and bigram) as in previous work.,5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"Then we add additional linguistic features (syntactic structure, referring expression, and discourse marker).",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
These five features (see Section 4) are referred to as “all but LIWC”.,5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"Finally, we test DAS validity using the higher level LIWC features.
",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"We perform paired t-tests on DAS scores for original dialogs and DAS scores for randomized dialogs, pairing every original dialog with its randomized dialog.",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"Table 3 shows the number of dialogs in each corpus, the average DAS scores of all dialogs within the corpus and p-values of corresponding t-tests.",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"Although the differences between the average scores are relatively small, the differences in almost all paired t-tests are extremely statistically significant (cells in bold, p < 0.0001).",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
The paired t-test on MPT using LIWC features shows a significant difference between the two test groups (p < 0.05).,5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
The original dialog corpora achieve higher average DAS scores than the randomized corpora for all 12 original-random pairs.,5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"The results show that DAS measure is sensitive to dialog turn order, as it should be if it is measuring dialog coherence and adaptation.",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
This experiment aims to broadly examine the differences in adaptation across different corpora and feature sets.,5.2 Adaptation across corpora and across features,[0],[0]
"We first compute DAS on the whole
dialog level for each feature set from Section 4, and then calculate the average across the corpus.",5.2 Adaptation across corpora and across features,[0],[0]
We use target=Both (Sec 2) to obtain an overall measure of adaptation and leave calculating finegrained DAS measures to Section 5.3.,5.2 Adaptation across corpora and across features,[0],[0]
Table 4 provides results.,5.2 Adaptation across corpora and across features,[0],[0]
"We will refer to features in row 1 to 6 as “linguistic features” and row 7 to 11 as “personality features”.
",5.2 Adaptation across corpora and across features,[0],[0]
"Comparing columns, we first examine the DAS scores across different corpora.",5.2 Adaptation across corpora and across features,[0],[0]
All p-values reported below are from paired t-tests.,5.2 Adaptation across corpora and across features,[0],[0]
"The two most similar corpora, the AWC and WAC, show no significant difference on linguistic features (p = 0.43).",5.2 Adaptation across corpora and across features,[0],[0]
"At the same time, the AWC and WAC do differ from the other two corpora.",5.2 Adaptation across corpora and across features,[0],[0]
This demonstrates that the DAS reflects real similarities and differences across corpora.,5.2 Adaptation across corpora and across features,[0],[0]
"MPT shows lower DAS scores on all linguistic features except for lemma (word repetition), where it achieves the highest DAS score.",5.2 Adaptation across corpora and across features,[0],[0]
"With respect to personality features, WAC has significantly higher DAS scores than AWC (p < 0.05), possibly because of the different experiment settings: college student participants are more comfortable around their own campus than in downtown.",5.2 Adaptation across corpora and across features,[0],[0]
MPT shows significantly lower DAS scores on personality features than AWC and WAC (p < 0.05).,5.2 Adaptation across corpora and across features,[0],[0]
This may be because the MPT setting is the most constrained of the four corpora: being fixed in topic and location means dialogs are less likely to be influenced by environmental factors or to contain social chit chat.,5.2 Adaptation across corpora and across features,[0],[0]
SWBD has the highest DAS scores in all feature sets except for referring expression.,5.2 Adaptation across corpora and across features,[0],[0]
The higher DAS in nonreferring features could be because the social chit chat allows more adaptation to occur.,5.2 Adaptation across corpora and across features,[0],[0]
"In addition, the dialogs we measure in SWBD are backchannelfiltered.",5.2 Adaptation across corpora and across features,[0],[0]
"The lower referring expression (respective to other SWBD scores) could be because SWBD does not require the referring expressions necessary
for the other three task-related corpora.",5.2 Adaptation across corpora and across features,[0],[0]
"We posit that the DAS adaptation models we present can be used in existing NLG architectures, described in Sec. 2.",5.2 Adaptation across corpora and across features,[0],[0]
"The AWC column in Table 4 shows adaptation model in the form of a DAS vector obtained from the ArtWalk Corpus.
",5.2 Adaptation across corpora and across features,[0],[0]
"Comparing rows, we then examine DAS scores among different features sets.",5.2 Adaptation across corpora and across features,[0],[0]
"LIWC has the highest DAS score among linguistic features, ranging from 0.48 to 0.71.",5.2 Adaptation across corpora and across features,[0],[0]
"While other linguistic features are largely content-specific, LIWC consists of higher level features that cover broader categories, thus its high DAS scores are expected.",5.2 Adaptation across corpora and across features,[0],[0]
"The DAS scores for the lemma feature range from 0.14 to 0.29, followed by Syntactic Structure (0.11 to 0.28), Hedge (0.17 to 0.25) and Bigram (0.01 to 0.07).",5.2 Adaptation across corpora and across features,[0],[0]
"Referring Expression has the lowest DAS score (0.01 to 0.03), possibly because our automatic extraction of referring expressions creates numerous subsets of one referring expression.",5.2 Adaptation across corpora and across features,[0],[0]
"Among personality features, Emotion Stability, Agreeableness, and Openness to Experience traits are adapted more than Extraversion and Conscientiousness.",5.2 Adaptation across corpora and across features,[0],[0]
We leave to future work the question of why these traits have higher DAS scores.,5.2 Adaptation across corpora and across features,[0],[0]
Our primary goal is to model adaptation at a finegrained level in order to provide fine-grained control of an NLG engine.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"To that end, we report results for adaptation models on a per dialog-segment and per-speaker basis.
",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"Reliable discourse segmentation is notoriously difficult (Passonneau and Litman, 1996), thus we heuristically divide each task-oriented dialog into segments based on number of destinations on the map: this effectively divides the dialog into subtasks.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"Since each dialog in SWBD only has one topic, we divide SWBD into 5 segments.7 We compute DAS for each segment, and take an average across all dialogs in the corpus for each segment.
",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
We compare all LIWC features vs. extraversion LIWC features because they provide high DAS scores across corpora.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
We also aim to explore the dynamics between two conversants on the extraversion scale.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
Figure 5 in Appendix illustrates how DAS varies as a function of speaker and dialog segment.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"In AWC, scores for all LIWC features
7To ensure two way adaptation exists in every segment (both speaker A adapting to B, and B adapting to A), the minimum length (number of turns) of each segment is 3.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"Thus we only work with dialogs longer than 15 turns in SWBD.
slightly decrease as dialogs progress (Fig. 5(a)), while extraversion features show a distinct increasing trend with correlation coefficients ranging from 0.7 to 0.86 (Fig. 5(b)), despite being a subset of all LIWC features.8 Average DAS displays the same decreasing trend in all and extraversion LIWC features for SWBD (Fig. 5(g) and 5(h)).",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"We speculate that this might be due to the setup of SWBD: as the dialogs progress, conversants have less to discuss about the topic and are less interested.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"We also calculate per segment adaptation in WAC and MPT, but their DAS scores do not show overall trends across the length of the dialog (Fig. 5(c) to 5(f)).
",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
We also explore whether speaker role and initiative affects adaptation.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"We use target=Both, target=D, and target=F to calculate DAS for each target.9",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
We hypothesize that directors and followers adapt differently in task-oriented dialogs.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"In all task-oriented corpora (AWC, WAC, and MPT), we observe generally higher DAS scores with target=D, indicating that in order to drive the dialogs, directors adapt more to followers.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"In SWBD, the speaker initiating the call (who brings up the discussion topic and may therefore drive the conversation) generally exhibits more adaptation.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
This experiment aims to examine the trend of DAS scores as the window size increases.,5.4 Adaptation on Different Window Sizes,[0],[0]
We begin with a window size of 1 and gradually increase it to 5.,5.4 Adaptation on Different Window Sizes,[0],[0]
"For a window size of n, the target utterance t is paired with the n-th utterance from a different speaker preceding t, if any.",5.4 Adaptation on Different Window Sizes,[0],[0]
"For example, in Figure 1, when window size is 3, target D100 is paired with prime F97; target D99 does not have any prime, thus no pair is formed.
",5.4 Adaptation on Different Window Sizes,[0],[0]
"Similar to Sec. 5.1, we compare DAS scores between dialogs in their original order vs. dialogs with randomly scrambled turns.",5.4 Adaptation on Different Window Sizes,[0],[0]
"We hypothesize that similar to the results of repetition decay measures (Reitter et al., 2006a; Ward and Litman, 2007; Pietsch et al., 2012), the DAS scores of original dialogs would decrease as the window size increases.",5.4 Adaptation on Different Window Sizes,[0],[0]
"We use target=both to obtain overall adaptation scores involving both speakers, and calculate DAS with all but the Personality LIWC feature sets introduced in Sec. 4.",5.4 Adaptation on Different Window Sizes,[0],[0]
"We first compute DAS on the whole dialog level for each window size, and then calculate the average DAS for each window size
8Using Simple Linear Regression in Weka 3.8.1.",5.4 Adaptation on Different Window Sizes,[0],[0]
"9In task-oriented dialogs, D stands for Director, F for Fol-
lower.",5.4 Adaptation on Different Window Sizes,[0],[0]
"In SWBD, D stands for the speaker initiating the call.
across the corpus.",5.4 Adaptation on Different Window Sizes,[0],[0]
"Results show that DAS scores for the original dialogs in all corpora decrease as window size increases, while DAS scores for the randomized dialogs stay relatively stable.",5.4 Adaptation on Different Window Sizes,[0],[0]
Figure 6 in Appendix shows plots of average DAS scores on different window sizes for original and randomized dialogs.,5.4 Adaptation on Different Window Sizes,[0],[0]
Plots of the AWC and WAC show similar trends.,5.4 Adaptation on Different Window Sizes,[0],[0]
Experiments with larger window sizes show that the original and random scores meet at window size 6 - 7 (with different versions of randomized dialogs).,5.4 Adaptation on Different Window Sizes,[0],[0]
"In MapTask, the original and random scores meet at window size 3 - 4.",5.4 Adaptation on Different Window Sizes,[0],[0]
"In SWBD, original and random scores meet at window size 2.",5.4 Adaptation on Different Window Sizes,[0],[0]
"Recent measures of linguistic adaptation fall into three categories: probabilistic measures, repetition decay measures, and document similarity measures (Xu and Reitter, 2015).",6 Related Work,[0],[0]
Probabilistic measures compute the probability of a single linguistic feature appearing in the target after its appearance in the prime.,6 Related Work,[0],[0]
"Some measures in this category focus more on comparing adaptation amongst features and do not handle turn by turn adaptation (Church, 2000; Stenchikova and Stent, 2007).",6 Related Work,[0],[0]
"Moreover, these measures produce scores for individual features, which need aggregation to reflect overall adaptivity (Danescu-Niculescu-Mizil et al., 2011, 2012).",6 Related Work,[0],[0]
"Document similarity measures calculate the similarity between prime and target by measuring the number of features that appear in both prime and target, normalized by the size of the two text sets (Wang et al., 2014).",6 Related Work,[0],[0]
"Both probabilistic measures and document similarity measures require the whole dialog to be complete before calculation.
",6 Related Work,[0],[0]
Repetition decay measures observe the decay rate of repetition probability of linguistic features.,6 Related Work,[0],[0]
"Previous work has fit the probability of linguistic feature repetition decrease with the distance between prime and target in logarithmic decay models (Reitter et al., 2006a,b; Reitter, 2008), linear decay models (Ward and Litman, 2007), and exponential decay models (Pietsch et al., 2012).
",6 Related Work,[0],[0]
Previous work on linguistic adaptation in natural language generation has also attempted to use adaptation models learned from human conversations.,6 Related Work,[0],[0]
"The alignment-capable microplanner SPUD prime (Buschmeier et al., 2009, 2010) uses the repetition decay model from Reitter (2008) as part of the activation functions for linguistic structures.",6 Related Work,[0],[0]
"However, the parameters are not learned from real
data.",6 Related Work,[0],[0]
"Repetition decay models do well in statistical parameterized NLG, but is hard to apply to overgenerate and rank NLG.",6 Related Work,[0],[0]
Isard et al. (2006) apply a pre-trained n-grams adaptation model to generate conversations.,6 Related Work,[0],[0]
"Hu et al. (2014) explore the effects of adaptation to various features by human evaluations, but their generator is not capable of deciding which features to adapt based on input context.",6 Related Work,[0],[0]
Dušek and Jurčı́ček (2016) use a seq2seq model to generate responses adapting to previous context.,6 Related Work,[0],[0]
They utilize an n-gram match ranker that promotes outputs with phrase overlap with context.,6 Related Work,[0],[0]
Our learned adaptation models could serve as a ranker.,6 Related Work,[0],[0]
"In addition to n-grams, DAS could produce models with any combinations of feature sets, providing more versatile adaptation behavior.",6 Related Work,[0],[0]
"To obtain models of linguistic adaptation, most measures could only measure an individual feature at a time, and need the whole dialog to calculate the measure (Church, 2000; Stenchikova and Stent, 2007; Danescu-Niculescu-Mizil et al., 2012; Pietsch et al., 2012; Reitter et al., 2006b; Ward and Litman, 2007).",7 Discussion and Future Work,[0],[0]
"This paper proposes the Dialog Adaptation Score (DAS) measure, which can be applied to NLG because it can be calculated on any segment of a dialog, and for any feature set.
",7 Discussion and Future Work,[0],[0]
"We first validate our measure by showing that the average DAS of original dialogs is significantly higher than randomized dialogs, indicating that it is sensitive to dialog priming as intended.",7 Discussion and Future Work,[0],[0]
"We then use DAS to show that feature sets such as LIWC, Syntactic Structure, and Hedge/Discourse Marker are adapted more than Bigram and Referring Expressions.",7 Discussion and Future Work,[0],[0]
We also demonstrate how we can use DAS to develop fine-grained models of adaptation: e.g. DAS applied to model adaptation in extraversion displays a distinct trend compared to all LIWC features in the task-oriented dialog corpus AWC.,7 Discussion and Future Work,[0],[0]
"Finally, we show that the degree of adaptation decreases as the window size increases.",7 Discussion and Future Work,[0],[0]
We leave to future work the implementation and evaluation of DAS adaptation models in natural language generation systems.,7 Discussion and Future Work,[0],[0]
"This research was supported by NSF CISE RI EAGER #IIS-1044693, NSF CISE CreativeIT #IIS1002921, NSF CHS #IIS-1115742, Nuance Foundation Grant SC-14-74, and auxiliary REU supplements.",Acknowledgement,[0],[0]
Previous work has shown that conversants adapt to many aspects of their partners’ language.,abstractText,[0],[0]
"Other work has shown that while every person is unique, they often share general patterns of behavior.",abstractText,[0],[0]
"Theories of personality aim to explain these shared patterns, and studies have shown that many linguistic cues are correlated with personality traits.",abstractText,[0],[0]
"We propose an adaptation measure for adaptive natural language generation for dialogs that integrates the predictions of both personality theories and adaptation theories, that can be applied as a dialog unfolds, on a turn by turn basis.",abstractText,[0],[0]
"We show that our measure meets criteria for validity, and that adaptation varies according to corpora and task, speaker, and the set of features used to model it.",abstractText,[0],[0]
"We also produce fine-grained models according to the dialog segmentation or the speaker, and demonstrate the decaying trend of adaptation.",abstractText,[0],[0]
Modeling Linguistic and Personality Adaptation for Natural Language Generation,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2289–2299 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2289",text,[0],[0]
"Understanding a story requires reasoning about the causal links between the events in the story and the mental states of the characters, even when those relationships are not explicitly stated.",1 Introduction,[0],[0]
"As shown by the commonsense story cloze shared task (Mostafazadeh et al., 2017)",1 Introduction,[0],[0]
", this reasoning is remarkably hard for both statistical and neural machine readers – despite being trivial for humans.",1 Introduction,[0],[0]
This stark performance gap between humans and machines is not surprising as most powerful language models have been designed to effectively learn local fluency patterns.,1 Introduction,[0],[0]
"Consequently, they generally lack the ability to abstract away from surface patterns in text to model more complex implied dynamics, such as intuiting characters’ mental states or predicting their plausible next actions.
",1 Introduction,[0],[0]
"In this paper, we construct a new annotation formalism to densely label commonsense short stories (Mostafazadeh et al., 2016) in terms of the mental states of the characters.",1 Introduction,[0],[0]
"The result-
The band instructor told the band to start playing.",1 Introduction,[0],[0]
"They grew tired and started playing worse after a while.
",He often stopped the music when players were off-tone.,[0],[0]
"The instructor was furious and threw his chair.
",He often stopped the music when players were off-tone.,[0],[0]
"He cancelled practice and expected us to perform
tomorrow.
",He often stopped the music when players were off-tone.,[0],[0]
"Instructor Players
E M E M
E M E M
E M E M
E M E M
E M E M
confide nt
[esteem]",He often stopped the music when players were off-tone.,[0],[0]
"[anger]
need re st
[esteem]
frustrate d
angry afraid
[disgust, fear]
",He often stopped the music when players were off-tone.,[0],[0]
"[esteem]
M EE
M [stability]
Figure 1: A story example with partial annotations for motivations (dashed) and emotional reactions (solid).",He often stopped the music when players were off-tone.,[0],[0]
"Open text explanations are in black (e.g., “frustrated”) and formal theory labels are in blue with brackets (e.g., “[esteem]”).
",He often stopped the music when players were off-tone.,[0],[0]
ing dataset offers three unique properties.,He often stopped the music when players were off-tone.,[0],[0]
"First, as highlighted in Figure 1, the dataset provides a fully-specified chain of motivations and emotional reactions for each story character as preand post-conditions of events.",He often stopped the music when players were off-tone.,[0],[0]
"Second, the annotations include state changes for entities even when they are not mentioned directly in a sentence (e.g., in the fourth sentence in Figure 1, players would feel afraid as a result of the instructor throwing a chair), thereby capturing implied effects unstated in the story.",He often stopped the music when players were off-tone.,[0],[0]
"Finally, the annotations encompass both formal labels from multiple theories of psychology (Maslow, 1943; Reiss, 2004; Plutchik, 1980) as well as open text descriptions of motivations and emotions, providing a comprehensive mapping between open text explanations and label categories (e.g., “to spend time with her son”
!",He often stopped the music when players were off-tone.,[0],[0]
Maslow’s category love).,He often stopped the music when players were off-tone.,[0],[0]
"Our corpus1 spans across 15k stories, amounting to 300k low-level annotations for around 150k character-line pairs.
",He often stopped the music when players were off-tone.,[0],[0]
"Using our new corpus, we present baseline performance on two new tasks focusing on mental state tracking of story characters: categorizing motivations and emotional reactions using theory labels, as well as describing motivations and emotional reactions using open text.",He often stopped the music when players were off-tone.,[0],[0]
Empirical results demonstrate that existing neural network models including those with explicit or latent entity representations achieve promising results.,He often stopped the music when players were off-tone.,[0],[0]
"Understanding people’s actions, motivations, and emotions has been a recurring research focus across several disciplines including philosophy and psychology (Schachter and Singer, 1962; Burke, 1969; Lazarus, 1991; Goldman, 2015).",2 Mental State Representations,[0],[0]
We draw from these prior works to derive a set of categorical labels for annotating the step-by-step causal dynamics between the mental states of story characters and the events they experience.,2 Mental State Representations,[0],[0]
"We use two popular theories of motivation: the “hierarchy of needs” of Maslow (1943) and the “basic motives” of Reiss (2004) to compile 5 coarse-grained and 19 fine-grained motivation categories, shown in Figure 2.",2.1 Motivation Theories,[0],[0]
"Maslow’s “hierarchy of needs” are comprised of five categories, ranging from physiological needs to spiritual growth, which we use as coarse-level categories.",2.1 Motivation Theories,[0],[0]
Reiss (2004) proposes 19 more fine-grained categories that provide a more informative range of motivations.,2.1 Motivation Theories,[0],[0]
"For example, even though they both relate
1We make our dataset publicly available at https:// uwnlp.github.io/storycommonsense/
to the physiological needs Maslow category, the food and rest motives from Reiss (2004) are very different.",2.1 Motivation Theories,[0],[0]
"While the Reiss theory allows for finergrained annotations of motivation, the larger set of abstract concepts can be overwhelming for annotators.",2.1 Motivation Theories,[0],[0]
"Motivated by Straker (2013), we design a hybrid approach, where Reiss labels are annotated as sub-categories of Maslow categories.",2.1 Motivation Theories,[0],[0]
"Among several theories of emotion, we work with the “wheel of emotions” of Plutchik (1980), as it has been a common choice in prior literature on emotion categorization (Mohammad and Turney, 2013; Zhou et al., 2016).",2.2 Emotion Theory,[0],[0]
We use the eight basic emotional dimensions as illustrated in Figure 2.,2.2 Emotion Theory,[0],[0]
"In addition to the motivation and emotion categories derived from psychology theories, we also obtain open text descriptions of character mental states.",2.3 Mental State Explanations,[0],[0]
"These open text descriptions allow learning computational models that can explain the mental states of characters in natural language, which is likely to be more accessible and informative to end users than having theory categories alone.",2.3 Mental State Explanations,[0],[0]
"Collecting both theory categories and open text also allows us to learn the automatic mappings between the two, which generalizes the previous work of Mohammad and Turney (2013) on emotion category mappings.",2.3 Mental State Explanations,[0],[0]
"In this study, we choose to annotate the simple commonsense stories introduced by Mostafazadeh et al. (2016).",3 Annotation Framework,[0],[0]
"Despite their simplicity, these stories pose a significant challenge to natural language understanding models (Mostafazadeh et al., 2017).
",3 Annotation Framework,[0],[0]
"In addition, they depict multiple interactions between story characters, presenting rich opportunities to reason about character motivations and reactions.",3 Annotation Framework,[0],[0]
"Furthermore, there are more than 98k such stories currently available covering a wide range of everyday scenarios.
",3 Annotation Framework,[0],[0]
"Unique Challenges While there have been a variety of annotated resources developed on the related topics of sentiment analysis (Mohammad and Turney, 2013; Deng and Wiebe, 2015), entity tracking (Hoffart et al., 2011; Weston et al., 2015), and story understanding (Goyal et al., 2010; Ouyang and McKeown, 2015; Lukin et al., 2016), our study is the first to annotate the full chains of mental state effects for story characters.",3 Annotation Framework,[0],[0]
"This poses several unique challenges as annotations require (1) interpreting discourse (2) understanding implicit causal effects, and (3) understanding formal psychology theory categories.",3 Annotation Framework,[0],[0]
"In prior literature, annotations of this complexity have typically been performed by experts (Deng and Wiebe, 2015; Ouyang and McKeown, 2015).",3 Annotation Framework,[0],[0]
"While reliable, these annotations are prohibitively expensive to scale up.",3 Annotation Framework,[0],[0]
"Therefore, we introduce a new annotation framework that pipelines a set of smaller isolated tasks as illustrated in Figure 3.",3 Annotation Framework,[0],[0]
All annotations were collected using crowdsourced workers from Amazon Mechanical Turk.,3 Annotation Framework,[0],[0]
We describe the components and workflow of the full annotation pipeline shown in Figure 3 below.,3.1 Annotation Pipeline,[0],[0]
"The example story in the figure is used to illustrate the output of various steps in the pipeline (full annotations for this example are in the appendix).
",3.1 Annotation Pipeline,[0],[0]
(1) Entity Resolution The first task in the pipeline aims to discover (1) the set of characters,3.1 Annotation Pipeline,[0],[0]
Ei in each story i and (2) the set of sentences Sij in which a specific character j 2,3.1 Annotation Pipeline,[0],[0]
"Ei is ex-
plicitly mentioned.",3.1 Annotation Pipeline,[0],[0]
"For example, in the story in Figure 3, the characters identified by annotators are “I/me” and “My cousin”, whom appear in sentences {1, 4, 5} and {1, 2, 3, 4, 5}, respectively.
",3.1 Annotation Pipeline,[0],[0]
We use Sij to control the workflow of later parts of the pipeline by pruning future tasks for sentences that are not tied to characters.,3.1 Annotation Pipeline,[0],[0]
"Because Sij is used to prune follow-up tasks, we take a high recall strategy to include all sentences that at least one annotator selected.
",3.1 Annotation Pipeline,[0],[0]
(2a) Action Resolution The next task identifies whether a character j appearing in a sentence k is taking any action to which a motivation can be attributed.,3.1 Annotation Pipeline,[0],[0]
We perform action resolution only for sentences k 2 Sij .,3.1 Annotation Pipeline,[0],[0]
"In the running example, we would want to know that the cousin in line 2 is not doing anything intentional, allowing us to omit this line in the next pipeline stage (3a) where a character’s motives are annotated.",3.1 Annotation Pipeline,[0],[0]
"Description of state (e.g., “Alex is feeling blue”) or passive event participation (e.g., “Alex trips”) are not considered volitional acts for which the character may have an underlying motive.",3.1 Annotation Pipeline,[0],[0]
"For each line and story character pair, we obtain 4 annotations.",3.1 Annotation Pipeline,[0],[0]
"Because pairs can still be filtered out in the next stage of annotation, we select a generous threshold where only 2 annotators must vote that an intentional action took place for the sentence to be used as an input to the motivation annotation task (3a).
",3.1 Annotation Pipeline,[0],[0]
(2b),3.1 Annotation Pipeline,[0],[0]
Affect Resolution This task aims to identify all of the lines where a story character j has an emotional reaction.,3.1 Annotation Pipeline,[0],[0]
"Importantly, it is often possible to infer the emotional reaction of a character j even when the character does not explicitly appear in a sentence k.",3.1 Annotation Pipeline,[0],[0]
"For instance, in Figure 3, we want to annotate the narrator’s reaction to line 2 even though they are not mentioned because their emotional response is inferrable.",3.1 Annotation Pipeline,[0],[0]
"We obtain 4 an-
notations per character per line.",3.1 Annotation Pipeline,[0],[0]
"The lines with at least 2 annotators voting are used as input for the next task: (3b) emotional reaction.
",3.1 Annotation Pipeline,[0],[0]
(3a) Motivation We use the output from the action resolution stage (2a) to ask workers to annotate character motives in lines where they intentionally initiate an event.,3.1 Annotation Pipeline,[0],[0]
"We provide 3 annotators a line from a story, the preceding lines, and a specific character.",3.1 Annotation Pipeline,[0],[0]
They are asked to produce a free response sentence describing what causes the character’s behavior in that line and to select the most related Maslow categories and Reiss subcategories.,3.1 Annotation Pipeline,[0],[0]
"In Figure 3, an annotator described the motivation of the narrator in line 1 as wanting “to have company” and then selected the love (Maslow) and family (Reiss) as categorical labels.",3.1 Annotation Pipeline,[0],[0]
"Because many annotators are not familiar with motivational theories, we require them to complete a tutorial the first time they attempt the task.
",3.1 Annotation Pipeline,[0],[0]
"(3b) Emotional Reaction Simultaneously, we use the output from the affect resolution stage (2b) to ask workers what the emotional response of a character is immediately following a line in which they are affected.",3.1 Annotation Pipeline,[0],[0]
"As with the motives, we give 3 annotators a line from a story, its previous context, and a specific character.",3.1 Annotation Pipeline,[0],[0]
We ask them to describe in open text how the character will feel following the event in the sentence (up to three emotions).,3.1 Annotation Pipeline,[0],[0]
"As a follow-up, we ask workers to compare their free responses against Plutchik categories by using 3-point likert ratings.",3.1 Annotation Pipeline,[0],[0]
"In Figure 3, we include a response for the emotional reaction of the narrator in line 1.",3.1 Annotation Pipeline,[0],[0]
"Even though the narrator was not mentioned directly in that line, an annotator recorded that they will react to their cousin being a slob by feeling “annoyed” and selected the Plutchik categories for sadness, disgust and anger.",3.1 Annotation Pipeline,[0],[0]
Cost The tasks corresponding to the theory category assignments are the hardest and most expensive in the pipeline (⇠$4 per story).,3.2 Dataset Statistics and Insights,[0],[0]
"Therefore, we obtain theory category labels only for a third of our annotated stories, which we assign to the development and test sets.",3.2 Dataset Statistics and Insights,[0],[0]
"The training data is annotated with a shortened pipeline with only open text descriptions of motivations and emotional reactions from two workers (⇠$1 per story).
",3.2 Dataset Statistics and Insights,[0],[0]
"Scale Our dataset to date includes a total of 300k low-level annotations for motivation and emotion across 15,000 stories (randomly selected from the ROC story training set).",3.2 Dataset Statistics and Insights,[0],[0]
"It covers over 150,000 character-line pairs, in which 56k character-line pairs have an annotated motivation and 105k have an annotated change in emotion (i.e. a label other than none).",3.2 Dataset Statistics and Insights,[0],[0]
"Table 1 shows the break down across training, development, and test splits.",3.2 Dataset Statistics and Insights,[0],[0]
"Figure 4 shows the frequency of different labels being selected for motivational and emotional categories in cases with positive change.
",3.2 Dataset Statistics and Insights,[0],[0]
"Agreements For quality control, we removed workers who consistently produced low-quality work, as discussed in the Appendix.",3.2 Dataset Statistics and Insights,[0],[0]
"In the categorization sets (Maslow, Reiss and Plutchik), we compare the performance of annotators by treating each individual category as a binary label (1
if they included the category in their set of responses) and averaging the agreement per category.",3.2 Dataset Statistics and Insights,[0],[0]
"For Plutchik scores, we count ‘moderately associated’ ratings as agreeing with ‘highly’ associated’ ratings.",3.2 Dataset Statistics and Insights,[0],[0]
The percent agreement and Krippendorff’s alpha are shown in Table 2.,3.2 Dataset Statistics and Insights,[0],[0]
"We also compute the percent agreement between the individual annotations and the majority labels.2
These scores are difficult to interpret by themselves, however, as annotator agreement in our categorization system has a number of properties that are not accounted for by these metrics (disagreement preferences – joy and trust are closer than joy and anger – that are difficult to quantify in a principled way, hierarchical categories map-
2Majority label for the motivation categories is what was agreed upon by at least two annotators per category.",3.2 Dataset Statistics and Insights,[0],[0]
"For emotion categories, we averaged the point-wise ratings and counted a category if the average rating was 2.
",3.2 Dataset Statistics and Insights,[0],[0]
"ping Reiss subcategories from Maslow categories, skewed category distributions that inflate PPA and deflate KA scores, and annotators that could select multiple labels for the same examples).
",3.2 Dataset Statistics and Insights,[0],[0]
"To provide a clearer understanding of agreement within this dataset, we create aggregated confusion matrices for annotator pairs.",3.2 Dataset Statistics and Insights,[0],[0]
"First, we sum the counts of combinations of answers between all paired annotations (excluding none labels).",3.2 Dataset Statistics and Insights,[0],[0]
"If an annotator selected multiple categories, we split the count uniformly among the selected categories.",3.2 Dataset Statistics and Insights,[0],[0]
We compute NPMI over the total confusion matrix.,3.2 Dataset Statistics and Insights,[0],[0]
"In Figure 5, we show the NPMI confusion matrix for motivational categories.
",3.2 Dataset Statistics and Insights,[0],[0]
"In the motivation annotations, we find the highest scores on the diagonal (i.e., Reiss agreement), with most confusions occurring between Reiss motives in the same Maslow category (outlined black in Figure 5).",3.2 Dataset Statistics and Insights,[0],[0]
"Other disagreements generally involve Reiss subcategories that are thematically similar, such as serenity (mental relaxation) and rest (physical relaxation).",3.2 Dataset Statistics and Insights,[0],[0]
"We provide this analysis for Plutchik categories in the appendix, finding high scores along the diagonal with disagreements typically occurring between categories in a “positive emotion” cluster (joy, trust) or a “negative emotion” cluster (anger, disgust,sadness).",3.2 Dataset Statistics and Insights,[0],[0]
The multiple modes covered by the annotations in this new dataset allow for multiple new tasks to be explored.,4 Tasks,[0],[0]
"We outline three task types below, covering a total of eight tasks on which to evaluate.
",4 Tasks,[0],[0]
"Differences between task type inputs and outputs are summarized in Figure 6.
",4 Tasks,[0],[0]
State Classification,4 Tasks,[0],[0]
"The three primary tasks involve categorizing the psychological states of story characters for each of the label sets (Maslow, Reiss, Plutchik) collected for the dev and test splits of our dataset.",4 Tasks,[0],[0]
"In each classification task, a model is given a line of the story (along with optional preceding context lines) and a character and predicts the motivation (or emotional reaction).",4 Tasks,[0],[0]
"A binary label is predicted for each of the Maslow needs, Reiss motives or Plutchik categories.
",4 Tasks,[0],[0]
"Annotation Classification Because the dev and test sets contain paired classification labels and free text explanations, we propose three tasks where a model must predict the correct Maslow/Reiss/Plutchik label given an emotional reaction or motivation explanation.
",4 Tasks,[0],[0]
"Explanation Generation Finally, we can use the free text explanations to train models to describe the psychological state of a character in free text (examples in Figure 4).",4 Tasks,[0],[0]
These explanations allow for two conditional generation tasks where the model must generate the words describing the emotional reaction or motivation of the character.,4 Tasks,[0],[0]
The general model architectures for the three tasks are shown in Figure 6.,5 Baseline Models,[0],[0]
We describe each model component below.,5 Baseline Models,[0],[0]
"The state classification and explanation generation models could be trained separately or in a multi-task set-up.
",5 Baseline Models,[0],[0]
"In the state classification and explanation generation tasks, a model is given a line from a story
x s containing N words {ws0, ws1, . . .",5 Baseline Models,[0],[0]
", wsN} from vocabulary V , a character in that story ej 2 E where E is the set of characters in the story, and (optionally) the preceding sentences in the story C = {x0 . . .",5 Baseline Models,[0],[0]
",xs 1} containing words from vocabulary V .",5 Baseline Models,[0],[0]
"A representation for a character’s psychological state is encoded as:
h e = Encoder(x s,C[ej ]) (1)
where C[ej ] corresponds to the concatenated subset of sentences in C where ej appears.",5 Baseline Models,[0],[0]
"While the end classifier or decoder is different for each task, we use the same set of encoders based on word embeddings, common neural network architectures, or memory networks to formulate a representation of the sentence and character, he.",5.1 Encoders,[0],[0]
"Unless specified, he is computed by encoding separate vector representations for the sentence (xs ! hs) and character-specific context (C[ej ] !",5.1 Encoders,[0],[0]
hc) and concatenating these encodings (he =,5.1 Encoders,[0],[0]
[hc;hs]).,5.1 Encoders,[0],[0]
"We describe the encoders below:
TF-IDF We learn a TD-IDF model on the full training corpus of Mostafazadeh et al. (2016) (excluding the stories in our dev/test sets).",5.1 Encoders,[0],[0]
"To encode the sentence, we extract TF-IDF features for its words, yielding vs 2 RV .",5.1 Encoders,[0],[0]
"A projection and nonlinearity is applied to these features to yield hs:
h s =",5.1 Encoders,[0],[0]
"(Wsv s + bs) (2)
where Ws 2 Rd⇥H .",5.1 Encoders,[0],[0]
"The character vector hc is encoded in the same way on sentences in the context pertaining to the character.
",5.1 Encoders,[0],[0]
"GloVe We extract pretrained Glove vectors (Pennington et al., 2014) for each word in V .",5.1 Encoders,[0],[0]
"The word embeddings are max-pooled, yielding embedding vs 2 RH , where H is the dimensionality of the Glove vectors.",5.1 Encoders,[0],[0]
"Using this max-pooled representation, hs and hc are extracted in the same manner as for TF-IDF features (Equation 2).
",5.1 Encoders,[0],[0]
CNN We implement a CNN text categorization model using the same configuration as Kim (2014) to encode the sentence words.,5.1 Encoders,[0],[0]
"A sentence is represented as a matrix, vs 2 RM⇥d where each row is a word embedding xsn for a word wsn 2 xs.
vs =",5.1 Encoders,[0],[0]
"[xs0, x s 1, . . .",5.1 Encoders,[0],[0]
",",5.1 Encoders,[0],[0]
"x s N ] (3)
h s = CNN(vs) (4)
where CNN represents the categorization model from (Kim, 2014).",5.1 Encoders,[0],[0]
The character vector hc is encoded in the same way with a separate CNN.,5.1 Encoders,[0],[0]
"Implementation details are provided in the appendix.
",5.1 Encoders,[0],[0]
LSTM,5.1 Encoders,[0],[0]
A two-layer bi-LSTM encodes the sentence words and concatenates the final time step hidden states from both directions to yield hs.,5.1 Encoders,[0],[0]
"The character vector hc is encoded the same way.
",5.1 Encoders,[0],[0]
REN We use the “tied” recurrent entity network from Henaff et al. (2017).,5.1 Encoders,[0],[0]
"A memory cell m is initialized for each of the J characters in the story, E = {e0, . . .",5.1 Encoders,[0],[0]
", eJ}.",5.1 Encoders,[0],[0]
The REN reads documents one sentence at a time and updates mj for ej 2 E after reading each sentence.,5.1 Encoders,[0],[0]
"Unlike the previous encoders, all sentences of the context C are given to the REN along with the sentence xs.",5.1 Encoders,[0],[0]
The model learns to distribute encoded information to the correct memory cells.,5.1 Encoders,[0],[0]
"The representation passed to the downstream model is:
h e = {mj}s (5)
where {mj}s is the memory vector in the cell corresponding to ej after reading xs.",5.1 Encoders,[0],[0]
"Implementation details are provided in the appendix.
",5.1 Encoders,[0],[0]
"NPN We also include the neural process network from Bosselut et al. (2018) with “tied” entities, but “untied” actions that are not grounded to particular concepts.",5.1 Encoders,[0],[0]
The memory is initialized and accessed similarly as the REN.,5.1 Encoders,[0],[0]
Exact implementation details are provided in the appendix.,5.1 Encoders,[0],[0]
"Once the sentence-character encoding he is extracted, the state classifier predicts a binary label ŷz for every category z 2 Z where Z is the set of category labels for a particular psychological theory (e.g., disgust, surprise, etc. in the Plutchik wheel).",5.2 State Classifier,[0],[0]
"We use logistic regression as a classifier:
ŷz =",5.2 State Classifier,[0],[0]
"(Wzh e + bz) (6)
where Wz and bz are a label-specific set of weights and biases for classifying each label z 2 Z .",5.2 State Classifier,[0],[0]
"The explanation generator is a single-layer LSTM (Hochreiter and Schmidhuber, 1997) that receives the encoded sentence-character representation he and predicts each word yt in the explanation using the same method from Sutskever et al. (2014).",5.3 Explanation Generator,[0],[0]
Implementation details are provided in the appendix.,5.3 Explanation Generator,[0],[0]
"For annotation classification tasks, words from open-text explanations are encoded with TF-IDF features.",5.4 Annotation Classifier,[0],[0]
The same classifier architecture from Section 5.2 is used to predict the labels.,5.4 Annotation Classifier,[0],[0]
State Classification,6.1 Training,[0],[0]
The dev set D is split into two portions of 80% (D1) and 20% (D2).,6.1 Training,[0],[0]
D1 is used to train the classifier and encoder.,6.1 Training,[0],[0]
D2 is used to tune hyperparameters.,6.1 Training,[0],[0]
"The model is trained to minimize the weighted binary cross entropy of predicting a class label yz for each class z:
L = ZX
z=1
zyz log ŷz+(1 z)(1 yz) log(1 ŷz)
(7) where Z is the number of labels in each of the three classifications tasks and z is defined as:
z = 1 e p P (yz) (8)
where P (yz) is the marginal class probability of a positive label for z in the training set.
",6.1 Training,[0],[0]
Annotation Classification,6.1 Training,[0],[0]
The dev set is split in the same manner as for state classification.,6.1 Training,[0],[0]
The TF-IDF features are trained on the set of training annotations Dt coupled with those from D1.,6.1 Training,[0],[0]
The model must minimize the same loss as in Equation 7.,6.1 Training,[0],[0]
"Details are provided in the appendix.
",6.1 Training,[0],[0]
Explanation Generation We use the training set of open annotations to train a model to predict explanations.,6.1 Training,[0],[0]
"The decoder is trained to minimize the negative loglikelihood of predicting each word in the explanation of a character’s state:
Lgen = TX
t=1
logP (yt|y0, ..., yt 1,he) (9)
where he is the sentence-character representation produced by an encoder from Section 5.1.",6.1 Training,[0],[0]
"Classification For the state and annotation classification task, we report the micro-averaged precision (P), recall (R), and F1 score of the Plutchik, Maslow, and Reiss prediction tasks.",6.2 Metrics,[0],[0]
We report the results of selecting a label at random in the top two rows of Table 3.,6.2 Metrics,[0],[0]
"Note that random is low because the distribution of positive instances for each
category is very uneven: macro-averaged positive class probabilities of 8.2, 1.7, and 9.9% per category for Maslow, Reiss, and Plutchik respectively.
",6.2 Metrics,[0],[0]
"Generation Because explanations tend to be short sequences (Figure 4) with high levels of synonymy, traditional metrics such as BLEU are inadequate for evaluating generation quality.",6.2 Metrics,[0],[0]
We use the vector average and vector extrema metrics from Liu et al. (2016) computed using the Glove vectors of generated and reference words.,6.2 Metrics,[0],[0]
We report results in Table 5 on the dev set and compare to a baseline that randomly samples an example from the dev set as a generated sequence.,6.2 Metrics,[0],[0]
Story Context vs. No Context,6.3 Ablations,[0],[0]
Our dataset is motivated by the importance of interpreting story context to categorize emotional reactions and motivations of characters.,6.3 Ablations,[0],[0]
"To test this importance, we ablate hc, the representation of the context sentences pertaining to the character, as an input to the state classifier for each encoder (except the REN and NPN).",6.3 Ablations,[0],[0]
"In Table 3, this ablation is the first row for each encoder presented.
",6.3 Ablations,[0],[0]
"Explanation Pretraining Because the state classification and explanation generation tasks use the same models to encode the story, we explore initializing a classification encoder with parameters trained on the generation task.",6.3 Ablations,[0],[0]
"For the CNN, LSTM, and REN encoders, we pretrain a generator to produce emotion or motivation explana-
tions.",6.3 Ablations,[0],[0]
We use the parameters from the emotion or motivation explanation generators to initialize the Plutchik or Maslow/Reiss classifiers respectively.,6.3 Ablations,[0],[0]
"State Classification We show results on the test set for categorizing Maslow, Reiss, and Plutchik states in Table 3.",7 Experimental Results,[0],[0]
"Despite the difficulty of the task, all models outperform the random baseline.",7 Experimental Results,[0],[0]
"Interestingly, the performance boost from adding entity-specific contextual information (i.e., not ablating hc) indicates that the models learn to condition on a character’s previous experience to classify its mental state at the current time step.",7 Experimental Results,[0],[0]
This effect can be seen in a story about a man whose flight is cancelled.,7 Experimental Results,[0],[0]
"The model without context predicts the same emotional reactions for the man, his wife and the pilot, but with context correctly predicts that the pilot will not have a reaction while predicting that the man and his wife will feel sad.
",7 Experimental Results,[0],[0]
"For the CNN, LSTM, REN, and NPN models, we also report results from pretraining encoder parameters using the free response annotations from the training set.",7 Experimental Results,[0],[0]
"This pretraining offers a clear performance boost for all models on all three prediction tasks, showing that the parameters of the encoder can be pretrained on auxiliary tasks providing emotional and motivational state signal.
",7 Experimental Results,[0],[0]
"The best performing models in each task are most effective at predicting Maslow physiological needs, Reiss food motives, and Plutchik reactions of joy.",7 Experimental Results,[0],[0]
"The relative ease of predicting motivations
related to food (and physiological needs generally) may be because they involve a more limited and concrete set of actions such as eating or cooking.
",7 Experimental Results,[0],[0]
Annotation Classification Table 4 shows that a simple model can learn to map open text responses to categorical labels.,7 Experimental Results,[0],[0]
"This further supports our hypothesis that pretraining a classification model on the free-response annotations could be helpful in boosting performance on the category prediction.
",7 Experimental Results,[0],[0]
"Explanation Generation Finally, we provide results for the task of generating explanations of motivations and emotions in Table 5.",7 Experimental Results,[0],[0]
"Because the explanations are closely tied to emotional and motivation states, the randomly selected explanation can often be close in embedding space to the reference explanations, making the random baseline fairly competitive.",7 Experimental Results,[0],[0]
"However, all models outperform the strong baseline on both metrics, indicating that the generated short explanations are closer semantically to the reference annotation.",7 Experimental Results,[0],[0]
Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects.,8 Related work,[0],[0]
"Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories.",8 Related work,[0],[0]
"More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017).",8 Related work,[0],[0]
"Similarly,
there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017).",8 Related work,[0],[0]
"Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions.",8 Related work,[0],[0]
"In our work, we collect mental state annotations for stories to used as a new resource in this space.
",8 Related work,[0],[0]
"Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state explicitly can boost performance while providing a preliminary interface for interpreting a model’s prediction.",8 Related work,[0],[0]
"Entity modeling in these works, however, was limited to tracking entity reference (Kiddon et al., 2016; Yang et al., 2016; Ji et al., 2017), recognizing entity state similarity (Henaff et al., 2017) or predicting simple attributes from entity states (Bosselut et al., 2018).",8 Related work,[0],[0]
Our work provides a new dataset for tracking emotional reactions and motivations of characters in stories.,8 Related work,[0],[0]
We present a large scale dataset as a resource for training and evaluating mental state tracking of characters in short commonsense stories.,9 Conclusion,[0],[0]
This dataset contains over 300k low-level annotations for character motivations and emotional reactions.,9 Conclusion,[0],[0]
We provide benchmark results on this new resource.,9 Conclusion,[0],[0]
"Importantly, we show that modeling character-specific context and pretraining on freeresponse data can boost labeling performance.",9 Conclusion,[0],[0]
"While our work only use information present in our dataset, we view our dataset as a future testbed for evaluating models trained on any number of resources for learning common sense about emotional reactions and motivations.",9 Conclusion,[0],[0]
We thank the reviewers for their insightful comments.,Acknowledgments,[0],[0]
"We also thank Bowen Wang, xlab members, Martha Palmer, Tim O’Gorman, Susan W. Brown, and Ghazaleh Kazeminejad for helpful discussions on inter-annotator agreement and the annotation pipeline.",Acknowledgments,[0],[0]
"This work was supported in part by NSF GRFP DGE-1256082, NSF IIS1714566, IIS-1524371, Samsung AI, and DARPA CwC (W911NF-15-1-0543).",Acknowledgments,[0],[0]
Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people’s mental states — a capability that is trivial for humans but remarkably hard for machines.,abstractText,[0],[0]
"To facilitate research addressing this challenge, we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions.",abstractText,[0],[0]
"Our work presents a new largescale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.",abstractText,[0],[0]
Modeling Naive Psychology of Characters in Simple Commonsense Stories,title,[0],[0]
"Reasoning about other agents’ intentions and being able to predict their behavior is important in multi-agent systems, in which the agents might have different, and sometimes competing, goals.",1. Introduction,[0],[0]
"In this paper, we introduce a new approach for estimating other agents’ unknown goals from their behavior and using those estimates to choose actions.",1. Introduction,[0],[0]
"We demonstrate that in the proposed tasks, using an explicit model of the other player leads to better performance than simply considering the other agent as part of the environment.
",1. Introduction,[0],[0]
"We frame the problem as a two-player stochastic game (Shapley, 1953), in which each agent is randomly assigned a different goal from a fixed set, which is shared between the agents.",1. Introduction,[0],[0]
"Players have full visibility of the environment, but no direct knowledge of the other’s goal and no communication channel.",1. Introduction,[0],[0]
"The reward obtained by each agent at the end of an episode depends on the goals of both agents, so an optimal policy must take into account both of their goals.
",1. Introduction,[0],[0]
"The key idea of this work is that as a first approximation
1New York University, New York City, USA 2Facebook AI Research, New York City, USA.",1. Introduction,[0],[0]
"Correspondence to: Roberta Raileanu <raileanu@cs.nyu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
of understanding what the other player is trying to achieve, an agent should ask itself “what would be my goal if I had acted as the other player had?”.",1. Introduction,[0],[0]
We instantiate this idea by parametrizing the agent’s action and value functions with a neural network that takes as input the observation state and a goal.,1. Introduction,[0],[0]
"As the agent plays the game, it uses its own policy (with the input expressed in the other agent’s frame of reference) to maximize the likelihood of the other’s observed actions and optimize directly over the goal representation to infer the other agent’s unknown goal.",1. Introduction,[0],[0]
"In contrast with the current literature, our approach does not require building any model of the other agent in order to infer its intention and predict its behavior.",1. Introduction,[0],[0]
"Background: A two-player Markov game is defined by a set of states S describing the possible configurations of all agents, a set of actions A1,A2 and a set of observations S1,S2 for each agent, and a transition function T : S × A1 ×A2 → S which gives the probability distribution on the next state as a function of current state and actions.",2. Approach,[0],[0]
"Each agent i chooses actions by sampling from a stochastic policy πi : S × Ai → [0, 1].",2. Approach,[0],[0]
The reward function of each agent is: ri : S ×A1 ×A2 → R.,2. Approach,[0],[0]
Each agent i aims to maximize its discounted return from time t onward: Rit = ∑∞ t=0,2. Approach,[0],[0]
"γ
trit, where rit is the reward obtained by agent i at time t and γ ∈ (0, 1] is the discount factor.",2. Approach,[0],[0]
"In this work, we consider both cooperative and adversarial settings.",2. Approach,[0],[0]
"In cooperative games, the agents have the same reward function: r1 = r2.
",2. Approach,[0],[0]
"We now describe Self Other-Modeling (SOM), a new approach for inferring other agents’ goals in an online fashion and using these estimates to choose actions.",2. Approach,[0],[0]
"To decide an action and to estimate the value of a state, we use a neural network f that takes as input its own goal zself , an estimate of the other player’s goal z̃other, and the observation state sself , and outputs a probability distribution over actions π and a value estimate V , such that for each agent i playing the game we have:[
πi V i ] = f(siself , z i self , z̃ i other; θ i) .
",2. Approach,[0],[0]
"Here θi are agent i’s parameters for f , which has one softmax output for the policy, one linear output for the value function, and all the non-output layers shared.",2. Approach,[0],[0]
"The actions
are sampled from policy πi.",2. Approach,[0],[0]
"The state siself contains the observation features from agent i’s viewpoint.
",2. Approach,[0],[0]
We propose that each agent models the behavior of the other player using its own policy.,2. Approach,[0],[0]
"Thus, each agent uses its own network f in two ways: acting mode, in which the agent uses f to choose its actions and inference mode, in which the agent uses f to infer the other agent’s goal.",2. Approach,[0],[0]
"For notation purposes, whenever f is used in acting mode (inference mode) we will refer to it as fself (fother):
acting mode: fself (sself , zself , z̃other; θ) (1)
inference mode: fother(sother, z̃other, zself ; θ).",2. Approach,[0],[0]
"(2)
The two modes have different relative placements of the network’s inputs zself and z̃other.",2. Approach,[0],[0]
"Additionally, since the environment is fully observed, the observation state of the two agents differs only by the specification of the agent’s identity on the map (i.e. each agent will be able to distinguish between its own location and the other’s location).",2. Approach,[0],[0]
"Hence, in acting mode, the network fself will take as input sself (with the identity of the acting agent at the location of the self ) and in inference mode, the network fother will take as input sother (with the identity of the acting agent at the location of the other).
",2. Approach,[0],[0]
"At each step, the agent uses equation (2) to output an estimate of the probability distribution over the other agent’s actions.",2. Approach,[0],[0]
"Then, the agent uses supervision of the other’s true action to backpropagate through fother (without updating its paramters) and directly optimize over its input z̃other, the estimate of the other agent’s goal.",2. Approach,[0],[0]
The number of optimization steps used to update z̃other is a hyperparameter that can vary with the game.,2. Approach,[0],[0]
The new estimate z̃other is used as input to fself in (1) for choosing the self agent’s next action.,2. Approach,[0],[0]
"Figure 1 illustrates this technique.
",2. Approach,[0],[0]
"Note that the network f is never updated during inference mode (i.e. using supervision of the other agent’s actions), f ’s parameters θ are updated only at the end of each episode using Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) with reward signal obtained by the self agent.",2. Approach,[0],[0]
"In contrast, z̃other is updated (multiple times) at each step in the game.
",2. Approach,[0],[0]
Algorithm 1 represents the pseudo-code for training a SOM agent for one episode.,2. Approach,[0],[0]
The procedure is formulated from the viewpoint of a single agent.,2. Approach,[0],[0]
"Since the goals are discrete in all the tasks considered here, the agent’s goal zself is encoded as a one-hot vector of dimension equal to the total number of possible goals in the game.",2. Approach,[0],[0]
"In line 6, siself is the self’s observation state from the perspective of agent i, which is the same as the other’s observation state from the perspective of agent j, sjother.
",2. Approach,[0],[0]
"We consider a continuous vector z̃other of the same dimension as zself , such that the estimate of the other agent’s
Algorithm 1 SOM training for one episode 1: procedure SELF OTHER-MODELING 2: for k := 1, num players do 3: z̃kother ← 1ngoals1ngoals 4: game.reset() 5: for step := 1, episode length do 6: siself = s j other ← game.get state()
7: z̃OH,iother = one hot(argmax(softmax(z̃",2. Approach,[0],[0]
"i other)) 8: πiself , V i self ← f iself (siself , ziself , z̃ OH,i other; θ i)
9: aiself ∼ πiself 10: game.action(aiself ) 11: for k : = 1, num inference steps do 12: z̃GS,jother = gumbel soft(softmax(z̃",2. Approach,[0],[0]
j other)),2. Approach,[0],[0]
13:,2. Approach,[0],[0]
"π̃jother← f j other(s j other, z̃ GS,j other, z j self ; θ j) 14: loss = cross entropy loss(π̃jother, a i self ) 15: loss.backward() 16: update(z̃jother) 17: for k := 1, num players do 18: policy.update(θk)
goal is a sample from the Categorical distribution with class probabilities softmax(z̃other).",2. Approach,[0],[0]
"Thus, the estimate of the other’s goal is given by the one-hot vector z̃OHother, as shown in line 7.",2. Approach,[0],[0]
"At the beginning of each game, the estimate of the other’s goal z̃OHother is randomly initialized, as illustrated in line 3, where 1ngoals represents a vector of all ones with the size equal to the number of possible goals.
",2. Approach,[0],[0]
"In inference mode, the estimate of the other agent’s goal is expressed as a sample from the Gumbel-Softmax distribution (Jang et al., 2016; Maddison et al., 2016), z̃GSother, as shown in line 12, where gumbel soft(p) = softmax[g + log(p)]), with g sampled from the Gumbel distribution and the softmax temperature τ = 1.",2. Approach,[0],[0]
"To update the estimate of the other’s goal, we directly optimize z̃other by using the cross-entropy loss to backpropagate through fother (lines 14, 15, 16).
",2. Approach,[0],[0]
"The agents’ policies are parametrized by long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997)
with two fully-connected linear layers, and exponential linear unit (ELU) (Clevert et al., 2015) activations.",2. Approach,[0],[0]
"The weights of the networks are initialized with semi-orthogonal matrices, as described in Saxe et al. (2013) and zero bias.",2. Approach,[0],[0]
Multi-Agent Learning.,3. Related Work,[0],[0]
"Recent work in deep multi-agent RL focuses on partially visible, fully cooperative settings (Foerster et al., 2016a;b; Omidshafiei et al., 2017) and emergent communication (Lazaridou et al., 2016; Foerster et al., 2016a; Sukhbaatar et al., 2016; Das et al., 2017; Mordatch & Abbeel, 2017).",3. Related Work,[0],[0]
"Lerer & Peysakhovich (2017) design RL agents that are able to maintain cooperation in complex social dilemmas by generalizing a well-known game theoretic strategy called tit-for-tat (Axelrod, 2006), to multiagent Markov games.",3. Related Work,[0],[0]
Leibo et al. (2017) considers semicooperative multi-agent environments in which the agents develop cooperative or competitive strategies depending on the task type and reward structure.,3. Related Work,[0],[0]
"Similarly, Lowe et al. (2017) proposes a centralized actor-critic architecture for efficient training in settings with such mixed strategies.",3. Related Work,[0],[0]
"Our setting is different since we do not allow communication between the agents, so the players have to indirectly reason about others’ intentions from their observed behavior.
",3. Related Work,[0],[0]
Intent Recognition.,3. Related Work,[0],[0]
"Research on plan, activity, and intent recognition has a long history, but it usually assumes domain knowledge or a form of rationality and uses techniques such as Bayesian inference or Hidden Markov Models (Sukthankar et al., 2014).",3. Related Work,[0],[0]
"The field of inverse reinforcement learning (IRL) (Russell, 1998; Ng et al., 2000; Abbeel & Ng, 2004) is also related to the problem considered here.",3. Related Work,[0],[0]
"IRL’s aim is to infer the reward function of an agent by observing its behavior, which is assumed to be nearly optimal.",3. Related Work,[0],[0]
"In contrast, our approach uses the observed actions of the other player to directly infer its goal in an online manner, which is then used by the agent when acting in the environment.",3. Related Work,[0],[0]
"This avoids the need for collecting offline samples of the other’s (state, action) pairs in order to estimate its reward function and use it to learn a policy.",3. Related Work,[0],[0]
"The more recent papers by Hadfield-Menell et al. (2016; 2017) are also concerned with the problem of inferring intentions, but their focus is on human-robot interaction and value alignment.",3. Related Work,[0],[0]
"Motivated by similar goals, Chandrasekaran et al. (2017) consider the problem of building a theory of AI’s mind, in order to improve human-AI interaction and the interpretability of AI systems.",3. Related Work,[0],[0]
"Recent work in cognitive science attempts to understand human decision-making by using a hierarchical model of social agency that infers human intentions for choosing a strategy (Kleiman-Weiner et al., 2016).",3. Related Work,[0],[0]
"However, none of these papers design algorithms that explicitly model other artificial agents in the environment or estimate their intentions, with the purpose of improving their decision
making.
",3. Related Work,[0],[0]
Modeling Other Agents.,3. Related Work,[0],[0]
Opponent modeling has been extensively studied in games of imperfect information.,3. Related Work,[0],[0]
Yet most previous approaches focuses on developing models with domain-specific probabilistic priors or strategy parametrizations.,3. Related Work,[0],[0]
"In contrast, our work proposes a more general framework for opponent modeling.",3. Related Work,[0],[0]
"Davidson (1999) uses an MLP to predict opponent actions given a game history, but the agents cannot adapt to their opponents’ behavior online.",3. Related Work,[0],[0]
"Lockett et al. (2007) designs a neural network architecture to identify the opponent type by learning a mixture of weights over a given set of cardinal opponents, but the game does not unfold within the RL framework.
",3. Related Work,[0],[0]
The closest work to ours is Foerster et al. (2017) and He et al. (2016).,3. Related Work,[0],[0]
Foerster et al. (2017) designs RL agents that take into account the learning of other agents in the environment when updating their own policies.,3. Related Work,[0],[0]
This enables the agents to discover self-interested yet collaborative strategies such as tit-for-tat in the iterated prisoner’s dilemma.,3. Related Work,[0],[0]
"While our work does not explicitly attempt to shape the learning of other agents, it has the advantage that agents can update their beliefs during an episode and change their strategies online to gain more reward.",3. Related Work,[0],[0]
"Our setting is also different in that it considers that each agent has some hidden information needed by the other player to maximize its return.
",3. Related Work,[0],[0]
"Our work is very much in line with He et al. (2016), where the authors build a general framework for modeling other agents in the reinforcement learning setting.",3. Related Work,[0],[0]
He et al. (2016) proposes a model that jointly learns a policy and the behavior of opponents by encoding observations of the opponent into a DQN.,3. Related Work,[0],[0]
Their Mixture of Experts architecture is able to discover different opponent strategy patterns in two competitive tasks.,3. Related Work,[0],[0]
"In our approach, rather than using hand designed features of the other agent’s behavior, the agent models others using its own policy.",3. Related Work,[0],[0]
"Another difference is that in this work, the agent runs an optimization over the input vector to infer the other agent’s hidden goal, rather than using a feed-forward network.",3. Related Work,[0],[0]
"In the experiments below, we show that SOM outperforms an adaptation of the method of He et al. (2016) to our setting.",3. Related Work,[0],[0]
"In this section, we evaluate our model SOM on three tasks:
•",4. Experiments,[0],[0]
"The coin game, in Section 4.2, which is a fully cooperative task where the agents’ roles are symmetric.
",4. Experiments,[0],[0]
"• The recipe game, in Section 4.3, which is adversarial, but with symmetric roles.
",4. Experiments,[0],[0]
"• The door game, in Section 4.4, which is fully cooperative but has asymmetric roles for the two players.
",4. Experiments,[0],[0]
We compare SOM to three other baselines and to a model that has access to the ground truth of the other agent’s goal.,4. Experiments,[0],[0]
"All the tasks considered are created in the Mazebase gridworld environment (Sukhbaatar et al., 2015).",4. Experiments,[0],[0]
"TRUE-OTHER-GOAL (TOG): We provide an upper bound on the performance of our model given by a policy network which takes the other agent’s true goal as input zother, as well as the state features sself and its own goal zself .",4.1. Baselines,[0],[0]
"Since this model has direct access to the true goal of the other agent, it does not need a separate network to model the behavior of the other agent.",4.1. Baselines,[0],[0]
"The architecture of TOG is the same as the one of SOM’s policy network, f .
",4.1. Baselines,[0],[0]
NO-OTHER-MODEL (NOM): The first baseline we use only takes as inputs the observation states sself and its own goal zself .,4.1. Baselines,[0],[0]
"NOM has the same architecture as the one used for SOM’s policy network, fself .",4.1. Baselines,[0],[0]
"This baseline does not explicitly model the other agent’s policy, goal, or actions.
",4.1. Baselines,[0],[0]
"INTEGRATED-POLICY-PREDICTOR (IPP): Starting with the architecture and inputs of NOM, we construct a stronger baseline, IPP, which has an additional final linear layer that outputs a probability distribution over the next action of the other agent.",4.1. Baselines,[0],[0]
"Besides the A3C loss used to train the policy of this network, we also add a cross-entropy loss to train the prediction of the other agent’s action, using observations of its true actions.
",4.1. Baselines,[0],[0]
SEPARATE-POLICY-PREDICTOR (SPP): He et al. (2016) propose an opponent modeling framework based on DQN.,4.1. Baselines,[0],[0]
"In their approach, a neural network (separate from the learned Q-network) is trained to predict the opponents actions given hand crafted state information specific to the opponent.",4.1. Baselines,[0],[0]
"An intermediate hidden representation from this network is given as input to the Q-network.
",4.1. Baselines,[0],[0]
We adapt the model of He et al. (2016) to our setting.,4.1. Baselines,[0],[0]
"In particular, we use A3C instead of DQN",4.1. Baselines,[0],[0]
"and we do not use the task-specific features used to represent the hidden goal of the opponent.
",4.1. Baselines,[0],[0]
"The resulting model, SPP, consists of two separate networks, a policy network for deciding the agent’s actions, and an opponent network for predicting the other agent’s actions.",4.1. Baselines,[0],[0]
"The opponent network takes as input its own state observation sself and goal zself , and outputs a probability distribution for the action taken by the other agent at the next step, as well as its hidden (recurrent) state.",4.1. Baselines,[0],[0]
"As in IPP, we train the opponent policy predictor with a cross-entropy loss using the true actions of the other agent.",4.1. Baselines,[0],[0]
"At each step, the hidden (recurrent) state outputted by this network is taken as input by the agent’s policy network, along with the observation state and its own goal.",4.1. Baselines,[0],[0]
"Both the policy network and the opponent policy predictor are LSTMs with the same
architecture as SOM.
",4.1. Baselines,[0],[0]
"In contrast to SOM, SPP does not explicitly infer the other agent’s goal.",4.1. Baselines,[0],[0]
"Rather, it builds an implicit model of the opponent by predicting the agent’s actions at each time step.",4.1. Baselines,[0],[0]
"In SOM, an inferred goal is given as additional input to the policy network.",4.1. Baselines,[0],[0]
"The analog of the inferred goal in SPP is the hidden (recurrent) state obtained from the opponent policy predictor which is given as an additional input to the policy network.
",4.1. Baselines,[0],[0]
Training Details.,4.1. Baselines,[0],[0]
"In all our experiments, we train the agents’ policies using A3C (Mnih et al., 2016) with an entropy coefficient of 0.01, a value loss coefficient of 0.5, and a discount factor of 0.99.",4.1. Baselines,[0],[0]
"The parameters of the agents’ policies are optimized using Adam (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999, = 1×10−8, and weight decay 0.",4.1. Baselines,[0],[0]
"SGD with a learning rate of 0.1 was used for inferring the other agent’s goal, z̃other.
",4.1. Baselines,[0],[0]
The hidden layer dimension of the policy network was 64 for the Coin and Recipe Games and 128 for the Door Game.,4.1. Baselines,[0],[0]
"We use a learning rate of 1×10−4 for all games and models.
",4.1. Baselines,[0],[0]
The observation state s is represented by few-hot vectors indicating the locations of all the objects in the environment (including the other player).,4.1. Baselines,[0],[0]
"The dimension of this input state is 1 × nfeatures, where the number of features is 384, 192, and 900 for the Coin, Recipe, and Door games, respectively.
",4.1. Baselines,[0],[0]
"For each experiment, we trained the models using 5 different random seeds.",4.1. Baselines,[0],[0]
"All the results shown are for 10 optimization updates of z̃other at each step in the game, unless mentioned otherwise.",4.1. Baselines,[0],[0]
"First, we evaluate the model on a fully cooperative task, in which the agents can gain more reward when using both of their goals rather than only their own goal.",4.2. Coin Game.,[0],[0]
So it is in the best interest of each agent to estimate the other player’s goal and use that information when taking actions.,4.2. Coin Game.,[0],[0]
"The game, shown in the left diagram of Figure 4, takes place on a 8× 8 grid containing 12 coins of 3 different colors (4 coins of each color).",4.2. Coin Game.,[0],[0]
"At the beginning of each episode, the agents are randomly assigned one of the three colors.",4.2. Coin Game.,[0],[0]
"The action space consists of: go up, down, left, right, or pass.",4.2. Coin Game.,[0],[0]
"Once an agent steps on a coin, that coin disappears from the grid.",4.2. Coin Game.,[0],[0]
The game ends after 20 steps.,4.2. Coin Game.,[0],[0]
"The reward received by both agents at the end of the game is given by the formula below:
R(cself , cother) =",4.2. Coin Game.,[0],[0]
(n self Cself + notherCself ),4.2. Coin Game.,[0],[0]
"2 + (nselfCother + n other Cother )2
− (nselfCneither + n other Cneither )2,
where notherCself is the number of coins of the self’s goal-color, which were collected by the other agents, and nselfCneither is
the number of coins corresponding to neither of the agents’ goals, collected by the self.",4.2. Coin Game.,[0],[0]
"For the example in Figure 4, agent 1 has Cself = orange and Cother = cyan, while agent 2’s Cself is cyan and Cother is orange.",4.2. Coin Game.,[0],[0]
"Cneither is red for both agents.
",4.2. Coin Game.,[0],[0]
"The role of the penalty for collecting coins that do not correspond to any of the agents’ goals is to avoid convergence to a greedy policy in which the agents can gain a non-negligible amount of reward by collecting all the coins in their proximity, without any regard to their color.
",4.2. Coin Game.,[0],[0]
"To maximize its return, each agent needs maximize the number of collected coins of its own or its collaborator’s color, and minimize the number of coins of the remaining color.",4.2. Coin Game.,[0],[0]
"Hence, when both agents are able to infer their collaborators’ goals with high accuracy and as early as possible in the game, they can use that information to maximize their
shared utility.
",4.2. Coin Game.,[0],[0]
Figure 3 shows the mean and standard deviation of the reward across 5 runs with different random seeds obtained by SOM.,4.2. Coin Game.,[0],[0]
Our model clearly outperforms all other baselines on this task.,4.2. Coin Game.,[0],[0]
"We also show the empirical upper bound on the reward using the model which takes as input the true color assigned to the other agent.
",4.2. Coin Game.,[0],[0]
Figure 2 analyzes the strategies of the different models by looking at the proportion of coins of each type collected by the agents.,4.2. Coin Game.,[0],[0]
"The optimal strategy is for each agent to maximize nselfCself + n self Cother
and minimize nselfCneither .",4.2. Coin Game.,[0],[0]
"Due to the randomness in the initial conditions (in particular, the locations of coins in the environment), this amounts to each agent collecting an equal number of coins of its own and of the other’s color on average, across a large number of episodes (i.e. n̄selfCself = n̄ self Cother ).
",4.2. Coin Game.,[0],[0]
"Indeed, this is the strategy learned by the model with perfect information of the other agent’s goal (TOG).",4.2. Coin Game.,[0],[0]
"SOM also learns to collect significantly more Other than Neither coins (although not as many as Self coins), indicating its ability to distinguish between the two types, at least during some of the episodes.",4.2. Coin Game.,[0],[0]
"This means that SOM can accurately infer the other agent’s goal early enough during the episode and use that information to collect more Other Coins, thus gaining more reward than if it were only using its own goal to direct its actions.
",4.2. Coin Game.,[0],[0]
"In contrast, the agents trained with the three baseline models collect significantly more Self coins, and as many Other as Neither coins on average.",4.2. Coin Game.,[0],[0]
"This shows that they learn to use their own goal for gaining reward, but they are unable to use the hidden goal of the other agent for further increasing their returns.",4.2. Coin Game.,[0],[0]
"Even if IPP and SPP are able to predict the actions of the other player with an accuracy of about 50%, they do not learn to distinguish between the coins that would increase (Other) and those that would decrease (Neither)
their reward.",4.2. Coin Game.,[0],[0]
This shows the weaknesses of using an implicit model of the other agent to maximize reward on certain tasks.,4.2. Coin Game.,[0],[0]
"Agents in adversarial scenarios have competing goals, so the ability of inferring the opponent’s goal could better inform the agent’s actions.",4.3. Recipe Game.,[0],[0]
"With this motivation in mind, we evaluate our model on a game in which the agents have to craft certain compositional recipes, each containing multiple items found in the environment.",4.3. Recipe Game.,[0],[0]
"The agents are given as input the names of their goal-recipes, without the corresponding components needed to make it.",4.3. Recipe Game.,[0],[0]
"The resources in the environment are scarce, so only one of the agents can craft its recipe within one episode.
",4.3. Recipe Game.,[0],[0]
"As illustrated in Figure 4 (center), there are 4 types of items: {sun, star, moon, lightning} and 4 recipes: {sun, sun, star}; {star, star, moon}; {moon, moon, lightning}; {lightning, lightning, sun}.",4.3. Recipe Game.,[0],[0]
"The game is played in a 4× 6 grid, which contains 8 items in total, 2 of each type.
",4.3. Recipe Game.,[0],[0]
"At the beginning of each episode, we randomly assign a recipe to one of the agents, and then we randomly pick a recipe for the other agent so that it has overlapping items with the recipe of the first agent.",4.3. Recipe Game.,[0],[0]
This ensures that the agents are competing for resources within each episode.,4.3. Recipe Game.,[0],[0]
"At the end of the episode, each agent receives a reward of +1 for crafting its own recipe and a penalty of -0.1 for each item it picked up not needed for making its recipe.
",4.3. Recipe Game.,[0],[0]
We designed the layout of the grid so that neither agent has an initial advantage by being closer to the scarce resource.,4.3. Recipe Game.,[0],[0]
"At the beginning of each episode, one of the agents starts on the left-most column of the grid, while the other one starts on the right-most column, at the same y-coordinate.",4.3. Recipe Game.,[0],[0]
Their initial y-coordinate as well as which agent starts on the left/right is randomized.,4.3. Recipe Game.,[0],[0]
"Similarly, one item of each of the 4 different types is placed at random in the grid formed
by the second and third columns of the maze, from left to right.",4.3. Recipe Game.,[0],[0]
"The rest of the items are placed in the forth and fifth columns, so that the symmetry with respect to the vertical axis is preserved (i.e. items of the same type are placed at the same y-coordinate, and symmetric x-coordinates).
",4.3. Recipe Game.,[0],[0]
"Agents have six actions to choose from: pass, go up, down, left, right, or pick up (for picking up an item, which then disappears from the grid).",4.3. Recipe Game.,[0],[0]
The first agent to take an action is randomized.,4.3. Recipe Game.,[0],[0]
"The game ends after 50 steps.
",4.3. Recipe Game.,[0],[0]
"We pretrain all baselines on a version of the game which does not have overlapping recipes, in order to ensure that all the models learn to pick up the corresponding items, given a recipe as goal.",4.3. Recipe Game.,[0],[0]
All of the models learn to craft their assigned recipes ∼ 90% of the time on this simpler task.,4.3. Recipe Game.,[0],[0]
"Then, we continue training the models on the adversarial task in which their recipes overlap in each episode.",4.3. Recipe Game.,[0],[0]
"SOM is initialized with a pretrained NOM network.
",4.3. Recipe Game.,[0],[0]
Figure 5 shows the winning fraction for different pairs played against each other in the Recipe game.,4.3. Recipe Game.,[0],[0]
"For the first 100k episodes, the models are not being trained.",4.3. Recipe Game.,[0],[0]
"We can see that SOM significantly outperfroms NOM, IPP, and SPP, winning ∼ 75 − 80% of the time, while the baselines can only win ∼ 15− 20% of the games.",4.3. Recipe Game.,[0],[0]
"SPP ties against NOM, and TOG outperforms SOM by a large margin.",4.3. Recipe Game.,[0],[0]
We also played the same types of agents against each other and they all win ∼ 40− 50% of the games.,4.3. Recipe Game.,[0],[0]
"In this section, we show that on a collaborative task with asymmetric roles and multiple possible partners, the agents can learn to figure out what role they should be playing in each game based on their partners’ actions.
",4.4. Door Game.,[0],[0]
"In the Door game, two agents are located in a 5 × 9 grid, with 5 goals behind 5 doors on the left wall, and 5 switches on the right wall of the grid.",4.4. Door Game.,[0],[0]
"The game starts with the two players in random squares on the grid, except for the ones occupied by the goals, doors, or switches, as illustrated in Figure 4.",4.4. Door Game.,[0],[0]
"Agents can take any of the five actions: go up, down, left, right or pass.",4.4. Door Game.,[0],[0]
An action is invalid if it moves the player outside of the border or to a square occupied by a block or closed door.,4.4. Door Game.,[0],[0]
Both agents receive +3 reward when either one of them steps on its goal and they are penalized -0.1 for each step they take.,4.4. Door Game.,[0],[0]
The game ends when one of them gets to its goal or after 22 steps.,4.4. Door Game.,[0],[0]
"All the goals are behind doors which are open only as long as one of the agents sits on the corresponding switch for that door.
",4.4. Door Game.,[0],[0]
"At the beginning of an episode, each of the two players is randomly selected from a pool of 5 agents and receives as input a random number from 1 to 5 corresponding to its goal.",4.4. Door Game.,[0],[0]
Each of the 5 agents has its own policy which gets updated at the end of each episode they play.,4.4. Door Game.,[0],[0]
"Note that the agents’
identities are not visible (i.e. there is no indication in the state features that specifies the id’s of the agents playing during a given episode).",4.4. Door Game.,[0],[0]
"This restriction is important in order to ensure that the agents cannot gain advantage by specializing into the two roles needed to win (i.e. goal-goer and switch-puller) and identifying the specialization of the other player by simply observing its unique id.
The agents need to cooperate in order to receive reward.",4.4. Door Game.,[0],[0]
"In contrast to our previous tasks, the two players must take different roles.",4.4. Door Game.,[0],[0]
"In fact, the player who sits on the switch should ignore its own goal and instead infer the other’s goal, while the player who goes to its goal does not need to infer the other’s goal, but only use its own.",4.4. Door Game.,[0],[0]
"In order to sit on the correct switch, an agent has to infer the other player’s goal from their observed actions.",4.4. Door Game.,[0],[0]
"The only way in which an agent can use its own policy to model the other player is if each agent learns to play both roles of the game, i.e. go to its own goal and also open its collaborator’s door by sitting on the corresponding switch.",4.4. Door Game.,[0],[0]
"Indeed, we see that the agents learn to play both roles and they are able to use their own policies to infer the other player’s goals when needed.
",4.4. Door Game.,[0],[0]
Fig 6 shows the mean and standard deviation of the winning fraction obtained by one of the agents on the Door game.,4.4. Door Game.,[0],[0]
"While our model is still able to outperform the three baselines, the gap between the performance of our model and that of IPP or SPP (an approximate version of (He et al., 2016)) is smaller than in the previous tasks.",4.4. Door Game.,[0],[0]
"However, this is a more difficult task for our model since it needs the agent to learn both roles before effectively using its own policy to infer the other agent’s goal.",4.4. Door Game.,[0],[0]
"The plot shows that SOM actually performs worse than IPP and SPP during the initial part of training, before outperforming them.",4.4. Door Game.,[0],[0]
"Nevertheless, we see that SOM training allows the agents to play both roles in an asymmetric cooperative game, and to infer the goal and role of the other player.",4.4. Door Game.,[0],[0]
"In this section we further analyze the ability of the SOM models to infer other’s intended goals.
",4.5. Analyzing the goal inference,[0],[0]
Figure 7 shows the fraction of episodes in which the goal of the other agent is correctly inferred.,4.5. Analyzing the goal inference,[0],[0]
"We consider that the goal is correctly inferred only when the estimate of the other’s goal remains accurate until the end of the game, so that we avoid counting the episodes in which the agent might infer the correct goal by chance at some intermediate step in the game.",4.5. Analyzing the goal inference,[0],[0]
"In all the games, the SOM agent learns to infer the other player’s goal with a mean accuracy ranging
from ∼",4.5. Analyzing the goal inference,[0],[0]
60− 80%.,4.5. Analyzing the goal inference,[0],[0]
"Comparing the second plot in Figure 2 with the left plot in Figure 7, one can observe that the SOM agent starts distinguishing Other from Neither coins after approximately 2M training episodes, which coincides with the time when the mean accuracy of the inferred goal converges to ∼ 75%.",4.5. Analyzing the goal inference,[0],[0]
"The Door Game (right) presents higher variance since the agents learn to use and infer the other’s goal at different stages during training.
",4.5. Analyzing the goal inference,[0],[0]
Figure 8 shows the cumulative distribution of the step at which the goal of the other player is correctly inferred (and remains the same until the end of the game).,4.5. Analyzing the goal inference,[0],[0]
The cumulative distribution is computed over the episodes in which the goal is correctly inferred before the end of the game.,4.5. Analyzing the goal inference,[0],[0]
"In the Coin (blue) and Recipe (red) games, 80% of the times the agent correctly infers the goal of the other, it does so in the first five steps.",4.5. Analyzing the goal inference,[0],[0]
The distribution for the Door (green) game indicates that the agent needs more steps on average to correctly infer the goal.,4.5. Analyzing the goal inference,[0],[0]
This explains in part why the SOM agent only slightly outperforms the SPP baseline.,4.5. Analyzing the goal inference,[0],[0]
"If the agent does not infer the other’s goal early enough in the episode, it cannot efficiently use it to maximize its return.
",4.5. Analyzing the goal inference,[0],[0]
"Figure 9 shows how the performance of the agent varies with
the number of optimization updates performed on z̃other at each step in the game.",4.5. Analyzing the goal inference,[0],[0]
"As expected, the agent’s reward (blue) generally increases with the number of inference steps, as does the fraction of episodes in which the goal is correctly inferred.",4.5. Analyzing the goal inference,[0],[0]
"One should note that increasing the number of inference steps from 10 to 20 only translates into less than 0.45% performance gain, while increasing it from 1 to 5 translates into a performance gain of 6.9% on the Coin game, suggesting that there is a certain threshold above which increasing the number of inference steps will not significantly improve performance.",4.5. Analyzing the goal inference,[0],[0]
Summary.,5. Discussion,[0],[0]
"In this paper, we introduced a new approach for inferring other agents’ hidden goals from their behavior and using those estimates to choose actions.",5. Discussion,[0],[0]
"We demonstrated that the agents are able to estimate others’ hidden goals in both cooperative and competitive settings, which enables them to converge to better policies.",5. Discussion,[0],[0]
"In the proposed tasks, using an explicit model of the other player led to better performance than simply considering the other agent as part of the environment.
Strengths.",5. Discussion,[0],[0]
Some of the main advantages of our method are its simplicity and flexibility.,5. Discussion,[0],[0]
"This method does not require any extra parameters to model other agents in the environment, can be trained with any reinforcement learning algorithm, and can be easily integrated with any network architecture.",5. Discussion,[0],[0]
"SOM can also be adapted to settings with more than two players, since the agent can use its own policy to model the behavior of any number of agents and infer their goals.",5. Discussion,[0],[0]
"Moreover, it can be easily generalized to numerous other environments and tasks.
Limitations.",5. Discussion,[0],[0]
Our approach is based on the assumption that the agents are identical or that their transition functions are independent and identically distributed.,5. Discussion,[0],[0]
"Hence, the framework is expected to be more suitable for symmetric games, in which the agents share a fixed set of goals and have similar abilities, and we expect a degradation of performance for asymmetric games.",5. Discussion,[0],[0]
Our experiments confirm this observation.,5. Discussion,[0],[0]
"Another limitation of SOM is that it requires a longer training time than other baselines, since we backpropagate through the network at each step.",5. Discussion,[0],[0]
"However, their online nature is essential in adapting to the behavior of other agents in the environment.
",5. Discussion,[0],[0]
Future Work.,5. Discussion,[0],[0]
"We plan to extend this work by evaluating the models on more complex environments and model deviations from the assumption that the players have identical policies, given a certain goal and state of the world.",5. Discussion,[0],[0]
"Another important avenue for future research is to design models that can adapt to non-stationary strategies of others in the environment, as well as to tasks with hierarchical goals.",5. Discussion,[0],[0]
This work was partially supported by ONR grant N0001413-1-0646.,Acknowledgements,[0],[0]
The authors wish to thank Alex Peysakhovich and Adam Lerer for helpful discussions.,Acknowledgements,[0],[0]
We consider the multi-agent reinforcement learning setting with imperfect information.,abstractText,[0],[0]
"The reward function depends on the hidden goals of both agents, so the agents must infer the other players’ goals from their observed behavior in order to maximize their returns.",abstractText,[0],[0]
"We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent’s actions and update its belief of their hidden goal in an online manner.",abstractText,[0],[0]
"We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players’ goals, in both cooperative and competitive settings.",abstractText,[0],[0]
Modeling Others using Oneself in Multi-Agent Reinforcement Learning,title,[0],[0]
"Being able to anticipate upcoming content is a core property of human language processing (Kutas et al., 2011; Kuperberg and Jaeger, 2016) that has received a lot of attention in the psycholinguistic literature in recent years.",1 Introduction,[0],[0]
Expectations about upcoming words help humans comprehend language in noisy settings and deal with ungrammatical input.,1 Introduction,[0],[0]
"In this paper, we use a computational model to address the question of how different layers of knowledge (linguistic knowledge as well as common-sense knowledge) influence human anticipation.
",1 Introduction,[0],[0]
Here we focus our attention on semantic predictions of discourse referents for upcoming noun phrases.,1 Introduction,[0],[0]
"This task is particularly interesting because it allows us to separate the semantic task of antic-
ipating an intended referent and the processing of the actual surface form.",1 Introduction,[0],[0]
"For example, in the context of I ordered a medium sirloin steak with fries.",1 Introduction,[0],[0]
"Later, the waiter brought . . .",1 Introduction,[0],[0]
", there is a strong expectation of a specific discourse referent, i.e., the referent introduced by the object NP of the preceding sentence, while the possible referring expression could be either the steak I had ordered, the steak, our food, or it.",1 Introduction,[0],[0]
Existing models of human prediction are usually formulated using the informationtheoretic concept of surprisal.,1 Introduction,[0],[0]
"In recent work, however, surprisal is usually not computed for DRs, which represent the relevant semantic unit, but for the surface form of the referring expressions, even though there is an increasing amount of literature suggesting that human expectations at different levels of representation have separable effects on prediction and, as a consequence, that the modelling of only one level (the linguistic surface form) is insufficient (Kuperberg and Jaeger, 2016; Kuperberg, 2016; Zarcone et al., 2016).",1 Introduction,[0],[0]
"The present model addresses this shortcoming by explicitly modelling and representing common-sense knowledge and conceptually separating the semantic (discourse referent) and the surface level (referring expression) expectations.
",1 Introduction,[0],[0]
"Our discourse referent prediction task is related to the NLP task of coreference resolution, but it substantially differs from that task in the following ways: 1) we use only the incrementally available left context, while coreference resolution uses the full text; 2) coreference resolution tries to identify the DR for a given target NP in context, while we look at the expectations of DRs based only on the context
ar X
iv :1
70 2.
",1 Introduction,[0],[0]
"03 12
1v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
0 Fe
b 20
before the target NP is seen.",1 Introduction,[0],[0]
The distinction between referent prediction and prediction of referring expressions also allows us to study a closely related question in natural language generation: the choice of a type of referring expression based on the predictability of the DR that is intended by the speaker.,1 Introduction,[0],[0]
"This part of our work is inspired by a referent guessing experiment by Tily and Piantadosi (2009), who showed that highly predictable referents were more likely to be realized with a pronoun than unpredictable referents, which were more likely to be realized using a full NP.",1 Introduction,[0],[0]
"The effect they observe is consistent with a Gricean point of view, or the principle of uniform information density (see Section 5.1).",1 Introduction,[0],[0]
"However, Tily and Piantadosi do not provide a computational model for estimating referent predictability.",1 Introduction,[0],[0]
"Also, they do not include selectional preference or common-sense knowledge effects in their analysis.
",1 Introduction,[0],[0]
"We believe that script knowledge, i.e., commonsense knowledge about everyday event sequences, represents a good starting point for modelling conversational anticipation.",1 Introduction,[0],[0]
This type of common-sense knowledge includes temporal structure which is particularly relevant for anticipation in continuous language processing.,1 Introduction,[0],[0]
"Furthermore, our approach can build on progress that has been made in recent years in methods for acquiring large-scale script knowledge; see Section 1.1.",1 Introduction,[0],[0]
Our hypothesis is that script knowledge may be a significant factor in human anticipation of discourse referents.,1 Introduction,[0],[0]
"Explicitly modelling this knowledge will thus allow us to produce more human-like predictions.
",1 Introduction,[0],[0]
"Script knowledge enables our model to generate anticipations about discourse referents that have already been mentioned in the text, as well as anticipations about textually new discourse referents which have been activated due to script knowledge.",1 Introduction,[0],[0]
"By modelling event sequences and event participants, our model captures many more long-range dependencies than normal language models are able to.",1 Introduction,[0],[0]
"As an example, consider the following two alternative text passages:
We got seated, and had to wait for 20 minutes.",1 Introduction,[0],[0]
"Then, the waiter brought the ...
We ordered, and had to wait for 20 minutes.",1 Introduction,[0],[0]
"Then, the waiter brought the ...
Preferred candidate referents for the object posi-
tion of the waiter brought the ... are instances of the food, menu, or bill participant types.",1 Introduction,[0],[0]
"In the context of the alternative preceding sentences, there is a strong expectation of instances of a menu and a food participant, respectively.
",1 Introduction,[0],[0]
This paper represents foundational research investigating human language processing.,1 Introduction,[0],[0]
"However, it also has the potential for application in assistant technology and embodied agents.",1 Introduction,[0],[0]
"The goal is to achieve human-level language comprehension in realistic settings, and in particular to achieve robustness in the face of errors or noise.",1 Introduction,[0],[0]
"Explicitly modelling expectations that are driven by common-sense knowledge is an important step in this direction.
",1 Introduction,[0],[0]
"In order to be able to investigate the influence of script knowledge on discourse referent expectations, we use a corpus that contains frequent reference to script knowledge, and provides annotations for coreference information, script events and participants (Section 2).",1 Introduction,[0],[0]
"In Section 3, we present a large-scale experiment for empirically assessing human expectations on upcoming referents, which allows us to quantify at what points in a text humans have very clear anticipations vs. when they do not.",1 Introduction,[0],[0]
"Our goal is to model human expectations, even if they turn out to be incorrect in a specific instance.",1 Introduction,[0],[0]
The experiment was conducted via Mechanical Turk and follows the methodology of Tily and Piantadosi (2009).,1 Introduction,[0],[0]
"In section 4, we describe our computational model that represents script knowledge.",1 Introduction,[0],[0]
"The model is trained on the gold standard annotations of the corpus, because we assume that human comprehenders usually will have an analysis of the preceding discourse which closely corresponds to the gold standard.",1 Introduction,[0],[0]
"We compare the prediction accuracy of this model to human predictions, as well as to two baseline models in Section 4.3.",1 Introduction,[0],[0]
One of them uses only structural linguistic features for predicting referents; the other uses general script-independent selectional preference features.,1 Introduction,[0],[0]
"In Section 5, we test whether surprisal (as estimated from human guesses vs. computational models) can predict the type of referring expression used in the original texts in the corpus (pronoun vs. full referring expression).",1 Introduction,[0],[0]
"This experiment also has wider implications with respect to the on-going discussion of whether the referring expression choice is dependent on predictability, as predicted by the uniform information density hy-
pothesis.",1 Introduction,[0],[0]
"The contributions of this paper consist of:
• a large dataset of human expectations, in a variety of texts related to every-day activities.",1 Introduction,[0],[0]
"• an implementation of the conceptual distinction
between the semantic level of referent prediction and the type of a referring expression.",1 Introduction,[0],[0]
"• a computational model which significantly im-
proves modelling of human anticipations.",1 Introduction,[0],[0]
"• showing that script knowledge is a significant
factor in human expectations.",1 Introduction,[0],[0]
"• testing the hypothesis of Tily and Piantadosi
that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent.",1 Introduction,[0],[0]
"Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant.",1.1 Scripts,[0],[0]
"Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest.",1.1 Scripts,[0],[0]
"Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014).
",1.1 Scripts,[0],[0]
"Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language
comprehension (Schütz-Bosbach and Prinz, 2007).",1.1 Scripts,[0],[0]
"Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012).",1.1 Scripts,[0],[0]
"Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation.",2 Data: The InScript Corpus,[0],[0]
"They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge.
",2 Data: The InScript Corpus,[0],[0]
"We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge.",2 Data: The InScript Corpus,[0],[0]
InScript is a crowdsourced corpus of simple narrative texts.,2 Data: The InScript Corpus,[0],[0]
"Participants were asked to write about a specific activity (e.g., a restaurant visit, a bus ride, or a grocery shopping event) which they personally experienced, and they were instructed to tell the story as if explaining the activity to a child.",2 Data: The InScript Corpus,[0],[0]
This resulted in stories that are centered around a specific scenario and that explicitly mention mundane details.,2 Data: The InScript Corpus,[0],[0]
"Thus, they generally realize longer event chains associated with a single script, which makes them particularly appropriate to our purpose.
",2 Data: The InScript Corpus,[0],[0]
"The InScript corpus is labelled with event-type, participant-type, and coreference information.",2 Data: The InScript Corpus,[0],[0]
"Full verbs are labeled with event type information, heads of all noun phrases with participant types, using scenario-specific lists of event types (such as enter bathroom, close drain and fill water for the “taking a bath” scenario) and participant types (such as bather, water and bathtub).",2 Data: The InScript Corpus,[0],[0]
"On average, each template offers a choice of 20 event types and 18 participant
to guess the upcoming referent (indicated by XXXXXX above).",2 Data: The InScript Corpus,[0],[0]
"They can either choose from the previously activated referents, or they can write something new.
types.",2 Data: The InScript Corpus,[0],[0]
The InScript corpus consists of 910 stories addressing 10 scenarios (about 90 stories per scenario).,2 Data: The InScript Corpus,[0],[0]
"The corpus has 200,000 words, 12,000 verb instances with event labels, and 44,000 head nouns with participant instances.",2 Data: The InScript Corpus,[0],[0]
"Modi et al. (2016) report an inter-annotator agreement of 0.64 for event types and 0.77 for participant types (Fleiss’ kappa).
",2 Data: The InScript Corpus,[0],[0]
We use gold-standard event- and participant-type annotation to study the influence of script knowledge on the expectation of discourse referents.,2 Data: The InScript Corpus,[0],[0]
"In addition, InScript provides coreference annotation, which makes it possible to keep track of the mentioned discourse referents at each point in the story.",2 Data: The InScript Corpus,[0],[0]
We use this information in the computational model of DR prediction and in the DR guessing experiment described in the next section.,2 Data: The InScript Corpus,[0],[0]
An example of an annotated InScript story is shown in Figure 1.,2 Data: The InScript Corpus,[0],[0]
"We use the InScript corpus to develop computational models for the prediction of discourse refer-
ents (DRs) and to evaluate their prediction accuracy.",3 Referent Cloze Task,[0],[0]
"This can be done by testing how often our models manage to reproduce the original discourse referent (cf. also the “narrative cloze” task by (Chambers and Jurafsky, 2008) which tests whether a verb together with a role can be correctly guessed by a model).",3 Referent Cloze Task,[0],[0]
"However, we do not only want to predict the “correct” DRs in a text but also to model human expectation of DRs in context.",3 Referent Cloze Task,[0],[0]
"To empirically assess human expectation, we created an additional database of crowdsourced human predictions of discourse referents in context using Amazon Mechanical Turk.",3 Referent Cloze Task,[0],[0]
"The design of our experiment closely resembles the guessing game of (Tily and Piantadosi, 2009) but extends it in a substantial way.
",3 Referent Cloze Task,[0],[0]
"Workers had to read stories of the InScript corpus 1 and guess upcoming participants: for each target NP, workers were shown the story up to this NP excluding the NP itself, and they were asked to guess the next person or object most likely to be referred to.",3 Referent Cloze Task,[0],[0]
"In case they decided in favour of a discourse referent already mentioned, they had to choose among the available discourse referents by clicking an NP in the preceding text, i.e., some noun with a specific, coreference-indicating color; see Figure 2.",3 Referent Cloze Task,[0],[0]
"Otherwise, they would click the “New” button, and would in turn be asked to give a short description of the new person or object they expected to be mentioned.",3 Referent Cloze Task,[0],[0]
"The percentage of guesses that agree with the actually referred entity was taken as a basis for estimating the surprisal.
",3 Referent Cloze Task,[0],[0]
"The experiment was done for all stories of the test set: 182 stories (20%) of the InScript corpus, evenly taken from all scenarios.",3 Referent Cloze Task,[0],[0]
"Since our focus is on the effect of script knowledge, we only considered those NPs as targets that are direct dependents of script-related events.",3 Referent Cloze Task,[0],[0]
Guessing started from the third sentence only in order to ensure that a minimum of context information was available.,3 Referent Cloze Task,[0],[0]
"To keep the complexity of the context manageable, we restricted guessing to a maximum of 30 targets and skipped the rest of the story (this applied to 12% of the stories).",3 Referent Cloze Task,[0],[0]
"We collected 20 guesses per NP for 3346 noun phrase instances, which amounts to a total of around 67K guesses.",3 Referent Cloze Task,[0],[0]
"Workers selected a con-
1The corpus is available at : http://www.sfb1102.",3 Referent Cloze Task,[0],[0]
"uni-saarland.de/?page_id=2582
text NP in 68% of cases and “New” in 32% of cases.",3 Referent Cloze Task,[0],[0]
Our leading hypothesis is that script knowledge substantially influences human expectation of discourse referents.,3 Referent Cloze Task,[0],[0]
The guessing experiment provides a basis to estimate human expectation of already mentioned DRs (the number of clicks on the respective NPs in text).,3 Referent Cloze Task,[0],[0]
"However, we expect that script knowledge has a particularly strong influence in the case of first mentions.",3 Referent Cloze Task,[0],[0]
"Once a script is evoked in a text, we assume that the full script structure, including all participants, is activated and available to the reader.
",3 Referent Cloze Task,[0],[0]
Tily and Piantadosi (2009) are interested in second mentions only and therefore do not make use of the worker-generated noun phrases classified as “New”.,3 Referent Cloze Task,[0],[0]
"To study the effect of activated but not explicitly mentioned participants, we carried out a subsequent annotation step on the worker-generated noun phrases classified as “New”.",3 Referent Cloze Task,[0],[0]
"We presented annotators with these noun phrases in their contexts (with co-referring NPs marked by color, as in the MTurk experiment) and, in addition, displayed all participant types of the relevant script (i.e., the script associated with the text in the InScript corpus).",3 Referent Cloze Task,[0],[0]
Annotators did not see the “correct” target NP.,3 Referent Cloze Task,[0],[0]
"We asked annotators to either (1) select the participant type instantiated by the NP (if any), (2) label the NP as unrelated to the script, or (3), link the NP to an overt antecedent in the text, in the case that the NP is actually a second mention that had been erroneously labeled as new by the worker.",3 Referent Cloze Task,[0],[0]
Option (1) provides a basis for a fine-grained estimation of first-mention DRs.,3 Referent Cloze Task,[0],[0]
"Option (3), which we added when we noticed the considerable number of overlooked antecedents, serves as correction of the results of the M-Turk experiment.",3 Referent Cloze Task,[0],[0]
"Out of the 22K annotated “New” cases, 39% were identified as second mentions, 55% were linked to a participant type, and 6% were classified as really novel.",3 Referent Cloze Task,[0],[0]
"In this section, we describe the model we use to predict upcoming discourse referents (DRs).",4 Referent Prediction Model,[0],[0]
"Our model should not only assign probabilities to DRs already explicitly introduced in the preceding text fragment (e.g., “bath” or “bathroom” for the
cloze task in Figure 2) but also reserve some probability mass for ‘new’ DRs, i.e., DRs activated via the script context or completely novel ones not belonging to the script.",4.1 Model,[0],[0]
"In principle, different variants of the activation mechanism must be distinguished.",4.1 Model,[0],[0]
"For many participant types, a single participant belonging to a specific semantic class is expected (referred to with the bathtub or the soap).",4.1 Model,[0],[0]
"In contrast, the “towel” participant type may activate a set of objects, elements of which then can be referred to with a towel or another towel.",4.1 Model,[0],[0]
"The “bath means” participant type may even activate a group of DRs belonging to different semantic classes (e.g., bubble bath and salts).",4.1 Model,[0],[0]
"Since it is not feasible to enumerate all potential participants, for ‘new’ DRs we only predict their participant type (“bath means” in our example).",4.1 Model,[0],[0]
"In other words, the number of categories in our model is equal to the number of previously introduced DRs plus the number of participant types of the script plus 1, reserved for a new DR not corresponding to any script participant (e.g., cellphone).",4.1 Model,[0],[0]
"In what follows, we slightly abuse the terminology and refer to all these categories as discourse referents.
",4.1 Model,[0],[0]
"Unlike standard co-reference models, which predict co-reference chains relying on the entire document, our model is incremental, that is, when predicting a discourse referent d(t) at a given position t, it can look only in the history h(t) (i.e., the preceding part of the document), excluding the referring expression (RE) for the predicted DR.",4.1 Model,[0],[0]
We also assume that past REs are correctly resolved and assigned to correct participant types (PTs).,4.1 Model,[0],[0]
"Typical NLP applications use automatic coreference resolution systems, but since we want to model human behavior, this might be inappropriate, since an automated system would underestimate human performance.",4.1 Model,[0],[0]
"This may be a strong assumption, but for reasons explained above, we use gold standard past REs.
",4.1 Model,[0],[0]
"We use the following log-linear model (“softmax regression”):
p(d(t) = d|h(t))",4.1 Model,[0],[0]
"= exp(w T f(d, h(t)))∑
d′ exp(w T f(d′, h(t)))
,
where f is the feature function we will discuss in the following subsection, w are model parameters, and the summation in the denominator is over the
set of categories described above.",4.1 Model,[0],[0]
Some of the features included in f are a function of the predicate syntactically governing the unobservable target RE (corresponding to the DR being predicted).,4.1 Model,[0],[0]
"However, in our incremental setting, the predicate is not available in the history h(t) for subject NPs.",4.1 Model,[0],[0]
"In this case, we use an additional probabilistic model, which estimates the probability of the predicate v given the context h(t), and marginalize out its predictions:
p(d(t)=d|h(t))= ∑ v p(v|h(t))",4.1 Model,[0],[0]
"exp(w T f(d, h(t), v))∑ d′ exp(w T f(d′, h(t), v))
",4.1 Model,[0],[0]
"The predicate probabilities p(v|h(t)) are computed based on the sequence of preceding predicates (i.e., ignoring any other words) using the recurrent neural network language model estimated on our training set.2",4.1 Model,[0],[0]
"The expression f(d, h(t), v) denotes the feature function computed for the referent d, given the history composed of h(t) and the predicate v.",4.1 Model,[0],[0]
Our features encode properties of a DR as well as characterize its compatibility with the context.,4.2 Features,[0],[0]
We face two challenges when designing our features.,4.2 Features,[0],[0]
"First, although the sizes of our datasets are respectable from the script annotation perspective, they are too small to learn a richly parameterized model.",4.2 Features,[0],[0]
"For many of our features, we address this challenge by using external word embeddings3 and associate parameters with some simple similarity measures computed using these embeddings.",4.2 Features,[0],[0]
"Con-
",4.2 Features,[0],[0]
"2We used RNNLM toolkit (Mikolov et al., 2011; Mikolov et al., 2010) with default settings.
",4.2 Features,[0],[0]
"3We use 300-dimensional word embeddings estimated on Wikipedia with the skip-gram model of Mikolov et al. (2013): https://code.google.com/p/word2vec/
sequently, there are only a few dozen parameters which need to be estimated from scenario-specific data.",4.2 Features,[0],[0]
"Second, in order to test our hypothesis that script information is beneficial for the DR prediction task, we need to disentangle the influence of script information from general linguistic knowledge.",4.2 Features,[0],[0]
"We address this by carefully splitting the features apart, even if it prevents us from modeling some interplay between the sources of information.",4.2 Features,[0],[0]
We will describe both classes of features below; also see a summary in Table 1.,4.2 Features,[0],[0]
These features are based on Tily and Piantadosi (2009).,4.2.1 Shallow Linguistic Features,[0],[0]
"In addition, we consider a selectional preference feature.",4.2.1 Shallow Linguistic Features,[0],[0]
Recency feature.,4.2.1 Shallow Linguistic Features,[0],[0]
"This feature captures the distance lt(d) between the position t and the last occurrence of the candidate DR d. As a distance measure, we use the number of sentences from the last mention and exponentiate this number to make the dependence more extreme; only very recent DRs will receive a noticeable weight: exp(−lt(d)).",4.2.1 Shallow Linguistic Features,[0],[0]
This feature is set to 0 for new DRs.,4.2.1 Shallow Linguistic Features,[0],[0]
Frequency.,4.2.1 Shallow Linguistic Features,[0],[0]
The frequency feature indicates the number of times the candidate discourse referent d has been mentioned so far.,4.2.1 Shallow Linguistic Features,[0],[0]
We do not perform any bucketing.,4.2.1 Shallow Linguistic Features,[0],[0]
Grammatical function.,4.2.1 Shallow Linguistic Features,[0],[0]
This feature encodes the dependency relation assigned to the head word of the last mention of the DR or a special none label if the DR is new.,4.2.1 Shallow Linguistic Features,[0],[0]
Previous subject indicator.,4.2.1 Shallow Linguistic Features,[0],[0]
This binary feature indicates whether the candidate DR d is coreferential with the subject of the previous verbal predicate.,4.2.1 Shallow Linguistic Features,[0],[0]
Previous object indicator.,4.2.1 Shallow Linguistic Features,[0],[0]
The same but for the object position.,4.2.1 Shallow Linguistic Features,[0],[0]
Previous RE type.,4.2.1 Shallow Linguistic Features,[0],[0]
"This three-valued feature indicates whether the previous mention of the candidate DR d is a pronoun, a non-pronominal noun phrase, or has never been observed before.",4.2.1 Shallow Linguistic Features,[0],[0]
The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v.,4.2.2 Selectional Preferences Feature,[0],[0]
"It is computed as the cosine similarity simcos(xTd ,xv,r) of a vector-space representation of the DR xd and a structured vector-space representation of the pred-
icate xv,r.",4.2.2 Selectional Preferences Feature,[0],[0]
The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010).,4.2.2 Selectional Preferences Feature,[0],[0]
"Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task.
",4.2.2 Selectional Preferences Feature,[0],[0]
"The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010).",4.2.2 Selectional Preferences Feature,[0],[0]
"This is a count-based, third-order cooccurrence tensor whose indices are a word w0, a second word w1, and a complex syntactic relation r, which is used as a stand-in for a semantic link.",4.2.2 Selectional Preferences Feature,[0],[0]
"The values for each (w0, r, w1) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of large corpora (ukWaC, BNC, and Wikipedia).
",4.2.2 Selectional Preferences Feature,[0],[0]
Our procedure has some differences with that of Baroni and Lenci.,4.2.2 Selectional Preferences Feature,[0],[0]
"For example, for estimating the fit of an alternative new DR (in other words, xd based on no previous mentions), we use an average over head words of all REs in the training set, a “null referent.”",4.2.2 Selectional Preferences Feature,[0],[0]
"xv,r is calculated as the average of the top 20 (by LMI) r-fillers for v in TypeDM; in other words, the prototypical instrument of rub may be represented by summing vectors like towel, soap, eraser, coin. . .",4.2.2 Selectional Preferences Feature,[0],[0]
"If the predicate has not yet been encountered (as for subject positions), scores for all scenario-relevant verbs are emitted for marginalization.",4.2.2 Selectional Preferences Feature,[0],[0]
"In this section, we describe features which rely on script information.",4.2.3 Script Features,[0],[0]
Our goal will be to show that such common-sense information is beneficial in performing DR prediction.,4.2.3 Script Features,[0],[0]
"We consider only two script features.
",4.2.3 Script Features,[0],[0]
Participant type fit This feature characterizes how well the participant type (PT) of the candidate DR d fits a specific syntactic role r of the governing predicate v; it can be regarded as a generalization of the selectional preference feature to participant types and also its specialisation to the considered scenario.,4.2.3 Script Features,[0],[0]
"Given the candidate DR d, its participant type p, and the syntactic
relation r, we collect all the predicates in the training set which have the participant type p in the position r.",4.2.3 Script Features,[0],[0]
"The embedding of the DR xp,r is given by the average embedding of these predicates.",4.2.3 Script Features,[0],[0]
"The feature is computed as the dot product of xp,r and the word embedding of the predicate v.
Predicate schemas The following feature captures a specific aspect of knowledge about prototypical sequences of events.",4.2.3 Script Features,[0],[0]
This knowledge is called predicate schemas in the recent co-reference modeling work of Peng et al. (2015).,4.2.3 Script Features,[0],[0]
"In predicate schemas, the goal is to model pairs of events such that if a DR d participated in the first event (in a specific role), it is likely to participate in the second event (again, in a specific role).",4.2.3 Script Features,[0],[0]
"For example, in the restaurant scenario, if one observes a phrase John ordered, one is likely to see John waited somewhere later in the document.",4.2.3 Script Features,[0],[0]
"Specific arguments are not that important (where it is John or some other DR), what is important is that the argument is reused across the predicates.",4.2.3 Script Features,[0],[0]
"This would correspond to the rule X-subject-of-order → X-subject-of-eat.4 Unlike the previous work, our dataset is small, so we cannot induce these rules directly as there will be very few rules, and the model would not generalize to new data well enough.",4.2.3 Script Features,[0],[0]
"Instead, we again encode this intuition using similarities in the real-valued embedding space.
",4.2.3 Script Features,[0],[0]
"Recall that our goal is to compute a feature ϕ(d, h(t))",4.2.3 Script Features,[0],[0]
"indicating how likely a potential DR d is to follow, given the history h(t).",4.2.3 Script Features,[0],[0]
"For example, imag-
4In this work, we limit ourselves to rules where the syntactic function is the same on both sides of the rule.",4.2.3 Script Features,[0],[0]
"In other words, we can, in principle, encode the pattern X pushed Y → X apologized but not the pattern X pushed Y → Y cried.
",4.2.3 Script Features,[0],[0]
ine that the model is asked to predict the DR marked by XXXXXX in Figure 4.,4.2.3 Script Features,[0],[0]
"Predicate-schema rules can only yield previously introduced DRs, so the score ϕ(d, h(t))",4.2.3 Script Features,[0],[0]
= 0 for any new DR d. Let us use “soap” as an example of a previously introduced DR and see how the feature is computed.,4.2.3 Script Features,[0],[0]
"In order to choose which inference rules can be applied to yield “soap”, we can inspect Figure 4.",4.2.3 Script Features,[0],[0]
"There are only two preceding predicates which have DR “soap” as their object (rubbed and grabbed), resulting in two potential rules X-object-of-grabbed→ X-object-of-rinsed and X-object-of-rubbed → X-object-of-rinsed.",4.2.3 Script Features,[0],[0]
"We define the score ϕ(d, h(t))",4.2.3 Script Features,[0],[0]
as the average of the rule scores.,4.2.3 Script Features,[0],[0]
"More formally, we can write
ϕ(d, h(t))= 1 |N(d, h(t))| ∑
(u,v,r)∈N(d,h(t))
ψ(u, v, r), (1)
where ψ(u, v, r) is the score for a rule X-r-of-u → X-r-of-v, N(d, h(t))",4.2.3 Script Features,[0],[0]
"is the set of applicable rules, and |N(d, h(t))| denotes its cardinality.5",4.2.3 Script Features,[0],[0]
"We define ϕ(d, h(t))",4.2.3 Script Features,[0],[0]
"as 0, when the set of applicable rules is empty (i.e. |N(d, h(t))| = 0).
",4.2.3 Script Features,[0],[0]
"The scoring function ψ(u, v, r) as a linear func-
5In all our experiments, rather than considering all potential predicates in the history to instantiate rules, we take into account only 2 preceding verbs.",4.2.3 Script Features,[0],[0]
"In other words, u and v can be interleaved by at most one verb and |N(d, h(t))| is in {0, 1, 2}.
tion of a joint embedding xu,v of verbs u and v:
ψ(u, v, r) =",4.2.3 Script Features,[0],[0]
"αTr xu,v.
The two remaining questions are (1) how to define the joint embeddings xu,v, and (2) how to estimate the parameter vectorαr.",4.2.3 Script Features,[0],[0]
"The joint embedding of two predicates, xu,v, can, in principle, be any composition function of embeddings of u and v, for example their sum or component-wise product.",4.2.3 Script Features,[0],[0]
"Inspired by Bordes et al. (2013), we use the difference between the word embeddings:
ψ(u, v, r) = αTr (xu − xv),
where xu and xv are external embeddings of the corresponding verbs.",4.2.3 Script Features,[0],[0]
Encoding the succession relation as translation in the embedding space has one desirable property: the scoring function will be largely agnostic to the morphological form of the predicates.,4.2.3 Script Features,[0],[0]
"For example, the difference between the embeddings of rinsed and rubbed is very similar to that of rinse and rub (Botha and Blunsom, 2014), so the corresponding rules will receive similar scores.",4.2.3 Script Features,[0],[0]
"Now, we can rewrite the equation (1) as
ϕ(d, h(t))= αT r(h(t))
",4.2.3 Script Features,[0],[0]
"∑ (u,v,r)∈N(d,h(t))",4.2.3 Script Features,[0],[0]
"(xu − xv)
|N(d, h(t))| (2)
where r(h(t)) denotes the syntactic function corresponding to the DR being predicted (object in our example).
",4.2.3 Script Features,[0],[0]
"As for the parameter vector αr, there are again a number of potential ways how it can be estimated.",4.2.3 Script Features,[0],[0]
"For example, one can train a discriminative classifier to estimate the parameters.",4.2.3 Script Features,[0],[0]
"However, we opted for a simpler approach—we set it equal to the empirical estimate of the expected feature vector xu,v on the training set:6
αr = 1
Dr ∑ l,t δr(r(h",4.2.3 Script Features,[0],[0]
"(l,t)))",4.2.3 Script Features,[0],[0]
"∑ (u,v,r′)∈N(d(l,t),h(l,t)) (xu − xv), (3)
where l refers to a document in the training set, t is (as before) a position in the document, h(l,t) and
6This essentially corresponds to using the Naive Bayes model with the simplistic assumption that the score differences are normally distributed with spherical covariance matrices.
",4.2.3 Script Features,[0],[0]
"d(l,t) are the history and the correct DR for this position, respectively.",4.2.3 Script Features,[0],[0]
"The term δr(r′) is the Kronecker delta which equals 1 if r = r′ and 0, otherwise.",4.2.3 Script Features,[0],[0]
"Dr is the total number of rules for the syntactic function r in the training set:
Dr = ∑ l,t δr(r(h",4.2.3 Script Features,[0],[0]
"(l,t)))× |N(d(l,t), h(l,t))|.
",4.2.3 Script Features,[0],[0]
Let us illustrate the computation with an example.,4.2.3 Script Features,[0],[0]
"Imagine that our training set consists of the document in Figure 1, and the trained model is used to predict the upcoming DR in our referent cloze example (Figure 4).",4.2.3 Script Features,[0],[0]
"The training document includes the pair X-object-of-scrubbed→ X-object-of-rinsing, so the corresponding term (xscrubbed - xrinsing) participates in the summation (3) for αobj .",4.2.3 Script Features,[0],[0]
"As we rely on external embeddings, which encode semantic similarities between lexical items, the dot product of this term and (xrubbed - xrinsed) will be high.7 Consequently, ϕ(d, h(t)) is expected to be positive for d = “soap”, thus, predicting “soap” as the likely forthcoming DR.",4.2.3 Script Features,[0],[0]
"Unfortunately, there are other terms (xu − xv) both in expression (3) for αobj and in expression (2) for ϕ(d, h(t)).",4.2.3 Script Features,[0],[0]
"These terms may be
7The score would have been even higher, should the predicate be in the morphological form rinsing rather than rinsed.",4.2.3 Script Features,[0],[0]
"However, embeddings of rinsing and rinsed would still be sufficiently close to each other for our argument to hold.
irrelevant to the current prediction, as X-object-ofplugged → X-object-of-filling from Figure 1, and may not even encode any valid regularities, as Xobject-of-got → X-object-of-scrubbed (again from Figure 1).",4.2.3 Script Features,[0],[0]
This may suggest that our feature will be too contaminated with noise to be informative for making predictions.,4.2.3 Script Features,[0],[0]
"However, recall that independent random vectors in high dimensions are almost orthogonal, and, assuming they are bounded, their dot products are close to zero.",4.2.3 Script Features,[0],[0]
"Consequently, the products of the relevant (“non-random”) terms, in our example (xscrubbed - xrinsing) and (xrubbed - xrinsed), are likely to overcome the (“random”) noise.",4.2.3 Script Features,[0],[0]
"As we will see in the ablation studies, the predicateschema feature is indeed predictive of a DR and contributes to the performance of the full model.",4.2.3 Script Features,[0],[0]
"We would like to test whether our model can produce accurate predictions and whether the model’s guesses correlate well with human predictions for the referent cloze task.
",4.3 Experiments,[0],[0]
"In order to be able to evaluate the effect of script knowledge on referent predictability, we compare three models: our full Script model uses all of the features introduced in section 4.2; the Linguistic model relies only on the ‘linguistic features’ but not the script-specific ones; and the Base model includes all the shallow linguistic features.",4.3 Experiments,[0],[0]
The Base model differs from the linguistic model in that it does not model selectional preferences.,4.3 Experiments,[0],[0]
"Table 2 summarizes features used in different models.
",4.3 Experiments,[0],[0]
"The data set was randomly divided into training (70%), development (10%, 91 stories from 10 sce-
narios), and test (20%, 182 stories from 10 scenarios) sets.",4.3 Experiments,[0],[0]
"The feature weights were learned using L-BFGS (Byrd et al., 1995) to optimize the loglikelihood.",4.3 Experiments,[0],[0]
Evaluation against original referents.,4.3 Experiments,[0],[0]
We calculated the percentage of correct DR predictions.,4.3 Experiments,[0],[0]
See Table 3 for the averages across 10 scenarios.,4.3 Experiments,[0],[0]
We can see that the task appears hard for humans: their average performance reaches only 73% accuracy.,4.3 Experiments,[0],[0]
"As expected, the Base model is the weakest system (the accuracy of 31%).",4.3 Experiments,[0],[0]
Modeling selectional preferences yields an extra 18% in accuracy (Linguistic model).,4.3 Experiments,[0],[0]
"The key finding is that incorporation of script knowledge increases the accuracy by further 13%, although still far behind human performance (62% vs. 73%).",4.3 Experiments,[0],[0]
"Besides accuracy, we use perplexity, which we computed not only for all our models but also for human predictions.",4.3 Experiments,[0],[0]
This was possible as each task was solved by multiple humans.,4.3 Experiments,[0],[0]
We used unsmoothed normalized guess frequencies as the probabilities.,4.3 Experiments,[0],[0]
"As we can see from Table 3, the perplexity scores are consistent with the accuracies: the script model again outperforms other methods, and, as expected, all the models are weaker than humans.
",4.3 Experiments,[0],[0]
"As we used two sets of script features, capturing different aspects of script knowledge, we performed extra ablation studies (Table 4).",4.3 Experiments,[0],[0]
The experiments confirm that both feature sets were beneficial.,4.3 Experiments,[0],[0]
Evaluation against human expectations.,4.3 Experiments,[0],[0]
"In the previous subsection, we demonstrated that the incorporation of selectional preferences and, perhaps more interestingly, the integration of automatically acquired script knowledge lead to improved accuracy in predicting discourse referents.",4.3 Experiments,[0],[0]
Now we turn to another question raised in the introduction: does incorporation of this knowledge make our predictions more human-like?,4.3 Experiments,[0],[0]
"In other words, are we able to accurately estimate human expectations?",4.3 Experiments,[0],[0]
"This includes not only being sufficiently accurate but also making the same kind of incorrect predictions.
",4.3 Experiments,[0],[0]
"In this evaluation, we therefore use human guesses collected during the referent cloze task as our target.",4.3 Experiments,[0],[0]
We then calculate the relative accuracy of each computational model.,4.3 Experiments,[0],[0]
"As can be seen in Figure 5, the Script model, at approx.",4.3 Experiments,[0],[0]
"53% accuracy, is a lot more accurate in predicting human guesses than the Linguistic model and the Base model.",4.3 Experiments,[0],[0]
"We can also
observe that the margin between the Script model and the Linguistic model is a lot larger in this evaluation than between the Base model and the Linguistic model.",4.3 Experiments,[0],[0]
"This indicates that the model which has access to script knowledge is much more similar to human prediction behavior in terms of top guesses than the script-agnostic models.
",4.3 Experiments,[0],[0]
Now we would like to assess if our predictions are similar as distributions rather than only yielding similar top predictions.,4.3 Experiments,[0],[0]
"In order to compare the distributions, we use the Jensen-Shannon divergence (JSD), a symmetrized version of the KullbackLeibler divergence.
",4.3 Experiments,[0],[0]
"Intuitively, JSD measures the distance between two probability distributions.",4.3 Experiments,[0],[0]
A smaller JSD value is indicative of more similar distributions.,4.3 Experiments,[0],[0]
"Figure 6 shows that the probability distributions resulting from the Script model are more similar to human predictions than those of the Linguistic and Base models.
",4.3 Experiments,[0],[0]
"In these experiments, we have shown that script knowledge improves predictions of upcoming referents and that the script model is the best among our models in approximating human referent predictions.",4.3 Experiments,[0],[0]
"Using the referent prediction models, we next attempt to replicate Tily and Piantadosi’s findings that
the choice of the type of referring expression (pronoun or full NP) depends in part on the predictability of the referent.",5 Referring Expression Type Prediction Model (RE Model),[0],[0]
"The uniform information density (UID) hypothesis suggests that speakers tend to convey information at a uniform rate (Jaeger, 2010).",5.1 Uniform Information Density hypothesis,[0],[0]
"Applied to choice of referring expression type, it would predict that a highly predictable referent should be encoded using a short code (here: a pronoun), while an unpredictable referent should be encoded using a longer form (here: a full NP).",5.1 Uniform Information Density hypothesis,[0],[0]
"Information density is measured using the information-theoretic measure of the surprisal S of a message mi:
S(mi) =",5.1 Uniform Information Density hypothesis,[0],[0]
− logP (mi | context) UID has been very successful in explaining a variety of linguistic phenomena; see Jaeger et al. (2016).,5.1 Uniform Information Density hypothesis,[0],[0]
"There is, however, controversy about whether UID affects pronominalization.",5.1 Uniform Information Density hypothesis,[0],[0]
Tily and Piantadosi (2009) report evidence that writers are more likely to refer using a pronoun or proper name when the referent is easy to guess and use a full NP when readers have less certainty about the upcoming referent; see also Arnold (2001).,5.1 Uniform Information Density hypothesis,[0],[0]
"But other experiments (using highly controlled stimuli) have failed to find an effect of predictability on pronominalization (Stevenson et al., 1994; Fukumura and van Gompel, 2010; Rohde and Kehler, 2014).",5.1 Uniform Information Density hypothesis,[0],[0]
The present study hence contributes to the debate on whether UID affects referring expression choice.,5.1 Uniform Information Density hypothesis,[0],[0]
Our goal is to determine whether referent predictability (quantified in terms of surprisal) is correlated with the type of referring expression used in the text.,5.2 A model of Referring Expression Choice,[0],[0]
Here we focus on the distinction between pronouns and full noun phrases.,5.2 A model of Referring Expression Choice,[0],[0]
Our data also contains a small percentage (ca.,5.2 A model of Referring Expression Choice,[0],[0]
1%) of proper names (like “John”).,5.2 A model of Referring Expression Choice,[0],[0]
"Due to this small class size and earlier findings that proper nouns behave much like pronouns (Tily and Piantadosi, 2009), we combined pronouns and proper names into a single class of short encodings.
",5.2 A model of Referring Expression Choice,[0],[0]
"For the referring expression type prediction task, we estimate the surprisal of the referent from each of our computational models from Section 4 as well as the human cloze task.",5.2 A model of Referring Expression Choice,[0],[0]
"The surprisal of an upcoming discourse referent d(t) based on the previous context
h(t) is thereby estimated as: S(d(t)) =",5.2 A model of Referring Expression Choice,[0],[0]
"− log p(d(t) | h(t))
",5.2 A model of Referring Expression Choice,[0],[0]
"In order to determine whether referent predictability has an effect on referring expression type over and above other factors that are known to affect the choice of referring expression, we train a logistic regression model with referring expression type as a response variable and discourse referent predictability as well as a large set of other linguistic factors (based on Tily and Piantadosi, 2009) as explanatory variables.",5.2 A model of Referring Expression Choice,[0],[0]
"The model is defined as follows:
p(n(t) = n|d(t), h(t))",5.2 A model of Referring Expression Choice,[0],[0]
=,5.2 A model of Referring Expression Choice,[0],[0]
"exp(v Tg(n, dt, h(t)))∑
n′",5.2 A model of Referring Expression Choice,[0],[0]
"exp(v Tg(n′, dt, h(t)))
,
where d(t) and h(t) are defined as before, g is the feature function, and v is the vector of model parameters.",5.2 A model of Referring Expression Choice,[0],[0]
The summation in the denominator is over NP types (full NP vs. pronoun/proper noun).,5.2 A model of Referring Expression Choice,[0],[0]
We ran four different logistic regression models.,5.3 RE Model Experiments,[0],[0]
These models all contained exactly the same set of linguistic predictors but differed in the estimates used for referent type surprisal and residual entropy.,5.3 RE Model Experiments,[0.9532251148168281],"['As all our models with fixed vocabulary size have exactly the same number of parameters for decoding (extra mechanism is used only for training), we only plot the decoding time of the WPED for comparison.']"
"One logistic regression model used surprisal estimates based on the human referent cloze task, while the three other models used estimates based on the three computational models (Base, Linguistic and Script).",5.3 RE Model Experiments,[0],[0]
"For our experiment, we are interested in the choice of referring expression type for those occurrences of references, where a “real choice” is possible.",5.3 RE Model Experiments,[0],[0]
We therefore exclude for our analysis reported below all first mentions as well as all first and second person pronouns (because there is no optionality in how to refer to first or second person).,5.3 RE Model Experiments,[0],[0]
This subset contains 1345 data points.,5.3 RE Model Experiments,[0],[0]
The results of all four logistic regression models are shown in Table 5.,5.4 Results,[0],[0]
We first take a look at the results for the linguistic features.,5.4 Results,[0],[0]
"While there is a bit of variability in terms of the exact coefficient estimates between the models (this is simply due to small correlations between these predictors and the predictors for surprisal), the effect of all of these features is largely consistent across models.",5.4 Results,[0],[0]
"For instance, the positive coefficients for the recency feature means that when a previous mention happened
very recently, the referring expression is more likely to be a pronoun (and not a full NP).
",5.4 Results,[0],[0]
"The coefficients for the surprisal estimates of the different models are, however, not significantly different from zero.",5.4 Results,[0],[0]
Model comparison shows that they do not improve model fit.,5.4 Results,[0],[0]
We also used the estimated models to predict referring expression type on new data and again found that surprisal estimates from the models did not improve prediction accuracy.,5.4 Results,[0],[0]
This effect even holds for our human cloze data.,5.4 Results,[0],[0]
"Hence, it cannot be interpreted as a problem with the models—even human predictability estimates are, for this dataset, not predictive of referring expression type.
",5.4 Results,[0],[0]
We also calculated regression models for the full dataset including first and second person pronouns as well as first mentions (3346 data points).,5.4 Results,[0],[0]
"The results for the full dataset are fully consistent with the findings shown in Table 5: there was no significant effect of surprisal on referring expression type.
",5.4 Results,[0],[0]
"This result contrasts with the findings by Tily and Piantadosi (2009), who reported a significant effect of surprisal on RE type for their data.",5.4 Results,[0],[0]
"In order to replicate their settings as closely as possible, we also included residualEntropy as a predictor in our model (see last predictor in Table 5); however, this did not change the results.",5.4 Results,[0],[0]
Our study on incrementally predicting discourse referents showed that script knowledge is a highly important factor in determining human discourse expectations.,6 Discussion and Future Work,[0],[0]
"Crucially, the computational modelling approach allowed us to tease apart the different factors that affect human prediction as we cannot manipulate this in humans directly (by asking them to “switch off” their common-sense knowledge).
",6 Discussion and Future Work,[0],[0]
"By modelling common-sense knowledge in terms of event sequences and event participants, our model captures many more long-range dependencies than normal language models.",6 Discussion and Future Work,[0],[0]
"The script knowledge is automatically induced by our model from crowdsourced scenario-specific text collections.
",6 Discussion and Future Work,[0],[0]
"In a second study, we set out to test the hypothesis that uniform information density affects referring expression type.",6 Discussion and Future Work,[0],[0]
"This question is highly controversial in the literature: while Tily and Piantadosi (2009) find a significant effect of surprisal on referring expression type in a corpus study very similar to ours, other studies that use a more tightly controlled experimental approach have not found an effect of predictability on RE type (Stevenson et al., 1994; Fukumura and van Gompel, 2010; Rohde and Kehler, 2014).",6 Discussion and Future Work,[0],[0]
"The present study, while replicating exactly the setting of T&P in terms of features and analysis, did not find support for a UID effect on RE type.",6 Discussion and Future Work,[0],[0]
"The difference in results between T&P 2009 and our results could be due to the different corpora and text sorts that were used; specifically, we would expect that larger predictability effects might be observable at script boundaries, rather than within a script, as is the case in our stories.
",6 Discussion and Future Work,[0],[0]
A next step in moving our participant prediction model towards NLP applications would be to replicate our modelling results on automatic textto-script mapping instead of gold-standard data as done here (in order to approximate human level of processing).,6 Discussion and Future Work,[0],[0]
"Furthermore, we aim to move to more complex text types that include reference to several scripts.",6 Discussion and Future Work,[0],[0]
"We plan to consider the recently published ROC Stories corpus (Mostafazadeh et al., 2016), a large crowdsourced collection of topically unrestricted short and simple narratives, as a basis for these next steps in our research.",6 Discussion and Future Work,[0],[0]
We thank the editors and the anonymous reviewers for their insightful suggestions.,Acknowledgments,[0],[0]
We would like to thank Florian Pusse for helping with the Amazon Mechanical Turk experiment.,Acknowledgments,[0],[0]
We would also like to thank Simon Ostermann and Tatjana Anikina for helping with the InScript corpus.,Acknowledgments,[0],[0]
"This research was partially supported by the German Research Foundation (DFG) as part of SFB 1102 ‘Information Density and Linguistic Encoding’, European Research Council (ERC) as part of ERC Starting Grant BroadSem (#678254), the Dutch National Science Foundation as part of NWO VIDI 639.022.518, and the DFG once again as part of the MMCI Cluster of Excellence (EXC 284).",Acknowledgments,[0],[0]
Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content.,abstractText,[0],[0]
Prediction also affects perception and might be a key to robustness in human language processing.,abstractText,[0],[0]
"In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts.",abstractText,[0],[0]
We find that script knowledge significantly improves model estimates of human predictions.,abstractText,[0],[0]
"In a second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect.",abstractText,[0],[0]
Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 303–308 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Intuitively, a man can swallow a candy or paintball but not a desk.",1 Introduction,[0],[0]
"Equally so, one cannot plausibly eat a cake and then hold it.",1 Introduction,[0],[0]
What kinds of semantic knowledge are necessary for distinguishing a physically plausible event (or event sequence) from an implausible one?,1 Introduction,[0],[0]
"Semantic plausibility stands in stark contrast to the familiar selectional preference (Erk and Padó, 2010; Van de Cruys, 2014) which is concerned with the typicality of events (Table 1).",1 Introduction,[0],[0]
"For example, candy is a typical entity for man-swallow-* but paintball is not, even though both events are plausible physically.",1 Introduction,[0],[0]
"Also, some events are physically plausible but are never stated because humans avoid stating the obvious.",1 Introduction,[0],[0]
"Critically, semantic plausibility is sensitive to certain properties such as relative object size that are not explicitly encoded by selectional preferences (Bagherinezhad et al., 2016).",1 Introduction,[0],[0]
"Therefore, it is crucial that we learn to model these dimensions in addition to using classical distributional signals.
",1 Introduction,[0],[0]
"Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017).",1 Introduction,[0],[0]
"Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance.",1 Introduction,[0],[0]
"Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem.
",1 Introduction,[0],[0]
"In this work, we show that world knowledge injection is necessary and effective for the semantic plausibility task, for which we create a robust, high-agreement dataset (details in section 3).",1 Introduction,[0],[0]
"Employing methods inspired by the recent work on world knowledge propagation through distributional context (Forbes and Choi, 2017; Wang et al., 2017), we accomplish the goal with minimal effort in manual annotation.",1 Introduction,[0],[0]
"Finally, we perform an indepth error analysis to point to future directions of work on semantic plausibility.
303",1 Introduction,[0],[0]
Simple events (i.e. S-V-O) have seen thorough investigation from the angle of selectional preference.,2 Related Work,[0],[0]
"While early works are resourcebased (Resnik, 1996; Clark and Weir, 2001), later work shows that unsupervised learning with distributional data yields strong performance (O’Seaghdha, 2010; Erk and Padó, 2010), which has recently been further improved upon with neural approaches (Van de Cruys, 2014; Tilk et al., 2016).",2 Related Work,[0],[0]
"Distribution-only models however, as will be shown, fail on the semantic plausibility task we propose.
",2 Related Work,[0],[0]
Physical world knowledge modeling appears frequently in more closely related work.,2 Related Work,[0],[0]
Bagherinezhad et al. (2016) combine computer vision and text-based information extraction to learn the relative sizes of objects; Forbes and Choi (2017) crowdsource physical knowledge along specified dimensions and employ belief propagation to learn relative physical attributes of object pairs.,2 Related Work,[0],[0]
"Wang et al. (2017) propose a multimodal LDA to learn the definitional properties (e.g. animal, fourlegged) of entities.",2 Related Work,[0],[0]
"Zhang et al. (2017) study the role of common-sense knowledge in natural language inference, which is inherently betweenevents rather than single-event focused.",2 Related Work,[0],[0]
"Prior work does not specifically handles the (singleevent) semantic plausibility task and related efforts do not necessarily adapt well to this task, as we will show, suggesting that new approaches are needed.",2 Related Work,[0],[0]
"To study the semantic plausibility of S-V-O events, specifically physical semantic plausibility, we create a dataset1 through Amazon Mechanical Turk with the following criteria in mind: (i) Robustness: Strong inter-annotator agreement; (ii) Diversity: A wide range of typical/atypical, plausible/implausible events; (iii) Balanced: Equal number of plausible and implausible events.
",3 Data,[0],[0]
"In creating physical events, we work with a fixed vocabulary of 150 concrete verbs and 450 concrete nouns from Brysbaert et al. (2014)’s word list, with a concreteness threshold of 4.95 (scale: 0-5).",3 Data,[0],[0]
"We take the following steps:
1Link: https://github.com/suwangcompling/ Modeling-Semantic-Plausibility-NAACL18/ tree/master/data.
",3 Data,[0],[0]
"(a) Have Turkers write down plausible or implausible S-V and V-O selections;
(b) Randomly generate S-V-O triples from collected S-V and V-O pairs;
(c) Send resulting S-V-O triples to Turkers to filter for ones with high agreement (by majority vote).
",3 Data,[0],[0]
(a) ensures diversity and the cleanness of data (compared with noisy selectional preference data collected unsupervised from free text): the Turkers are instructed (with examples) to (i) consider both typical and atypical selections (e.g. manswallow-* with candy or paintball); (ii) disregard metaphorical uses (e.g. feel-blue or fish-idea).,3 Data,[0],[0]
"2,000 pairs are collected in the step, balancing typical and atypical pairs.",3 Data,[0],[0]
"In (b), we manually filter error submissions in triple generation.",3 Data,[0],[0]
"For (c), 5 Turkers provide labels, and we only keep the ones that have ≥ 3 majority votes, resulting with 3,062 triples (of 4,000 annotated triples, plausibleimplausible balanced), with 100% ≥ 3 agreement, 95% ≥ 4 agreement, and 90% 5 agreement.
",3 Data,[0],[0]
"To empirically show the failure of distributiononly methods, we run Van de Cruys (2014)’s neural net classifier (hereforth NN), which is one of the strongest models designed for selectional preference (Figure 1, left-box).",3 Data,[0],[0]
Let x be the concatenation of the embeddings of the three words in an S-V-O triple.,3 Data,[0],[0]
"The prediction P (y|x) is computed as follows:
P (y = 1|x) = σ2(W2σ1(W1x))",3 Data,[0],[0]
"(1) where σ is a nonlinearity, W are weights, and we use 300D pretrained GloVe vectors (Pennington et al., 2014).",3 Data,[0],[0]
"The model achieves an accuracy of 68% (logistic regression baseline: 64%) after finetuning, verifying the intuition that distributional data alone cannot satisfactorily capture the semantics of physical plausibility.",3 Data,[0],[0]
"Recognizing that a distribution-alone method lacks necessary information, we collect a set of world knowledge features.",4 World Knowledge Features,[0],[0]
The feature types derive from inspecting the high agreement event triples for knowledge missing in distributional selection (e.g. relative sizes in man-swallowpaintball/desk).,4 World Knowledge Features,[0],[0]
"Previously, Forbes and Choi (2017) proposed a three level (3-LEVEL) featurization scheme, where an object-pair can take 3
values for, e.g. relative size: {−1, 0, 1} (i.e. lesser, similar, greater).",4 World Knowledge Features,[0],[0]
"This method, however, does not explain many cases we observed.",4 World Knowledge Features,[0],[0]
"For instance, man-hug-cat/ant, man is larger than both cat and ant, but the latter event is implausible.",4 World Knowledge Features,[0],[0]
3-LEVEL is also inefficient: k objects incur O(k2) elicitations.,4 World Knowledge Features,[0],[0]
"We thus propose a binning-by-landmark method, which is sufficiently fine-grained while still being efficient and easy for the annotator: given an entity n, the Turker decides to which of the landmarks n is closest to.",4 World Knowledge Features,[0],[0]
"E.g., for SIZE, we have the landmarks {watch, book, cat, person, jeep, stadium}, in ascending sizes.",4 World Knowledge Features,[0],[0]
"If n = dog, the Turker may put n in the bin corresponding to cat.",4 World Knowledge Features,[0],[0]
"The features2 are listed with their landmarks as follows: • SENTIENCE: rock, tree, ant, cat, chimp, man.",4 World Knowledge Features,[0],[0]
•,4 World Knowledge Features,[0],[0]
MASS-COUNT:,4 World Knowledge Features,[0],[0]
"milk, sand, pebbles, car.",4 World Knowledge Features,[0],[0]
"• PHASE: smoke, milk, wood.",4 World Knowledge Features,[0],[0]
•,4 World Knowledge Features,[0],[0]
"SIZE: watch, book, cat, person, jeep, stadium.",4 World Knowledge Features,[0],[0]
"• WEIGHT: watch, book, dumbbell, man, jeep, stadium.",4 World Knowledge Features,[0],[0]
"• RIGIDITY: water, skin, leather, wood, metal.
5",4 World Knowledge Features,[0],[0]
"Turkers provide annotations for all 450 nouns, and we obtained 93% ≥ 3 agreement, 85% ≥ 4 agreement, and 79% 5 agreement.
",4 World Knowledge Features,[0],[0]
"Our binning is sufficiently granular, which is crucial for semantic plausibility of an event in many cases.",4 World Knowledge Features,[0],[0]
"E.g. for man-hug-cat/ant, man, cat and ant fall in the 4th, 3rd and 1st bin, which suffices to explain why man-hug-cat is plausible while man-hug-ant is not.",4 World Knowledge Features,[0],[0]
"Compared to past work (Forbes and Choi, 2017), it is efficient.",4 World Knowledge Features,[0],[0]
"Each entity only needs one assignment in comparison to the landmarks to be located in a “global scale” (e.g. from the smallest to the largest objects), and even for extreme granularity, it only takes O(k log k) comparisons.",4 World Knowledge Features,[0],[0]
It is also intuitive: differences in bins capture the intuition that one can hug smaller objects as long as those objects are not too small.,4 World Knowledge Features,[0],[0]
We answer two questions: (i) Does world knowledge improve the accuracy of semantic plausibility classification?,5 Models,[0],[0]
"(ii) Can we minimize effort in knowledge feature annotation by learning from a
2We experimented with numerous feature types, e.g. size, temperature, shape, etc. and kept the subset that contributes most substantially to semantic plausibility classification.",5 Models,[0],[0]
"More details on the feature types in supplementary material (https://github.com/suwangcompling/ Modeling-Semantic-Plausibility-NAACL18/ tree/master/supplementary).
",5 Models,[0],[0]
small amount of training data?,5 Models,[0],[0]
"For question (i), we experiment with various methods to incorporate the features on top of the embedding-only NN (Section 3).",5 Models,[0],[0]
"Our architecture3 is outlined in Figure 1, where we ensemble the NN (left-box) and another feedforward net for features (WK, right-box) to produce the final prediction.",5 Models,[0],[0]
"For the feature net, the relative physical attributes of the subject-object pair can be encoded in 3-LEVEL (Section 4) or the bin difference (BINDIFF) scheme.4 For BIN-DIFF, given the two entities in an S-V-O event (i.e. S, O) ant and man, which are in the bins of the landmark watch (i.e. the 1st) and that of person (i.e. the 4th), the pair ant-man gets a BIN-DIFF value of 1−4 = −3.",5 Models,[0],[0]
"Exemplifying the featurization function f(s, o) with SIZE:
f3-L(SIZE(s), SIZE(o)) ∈ {−1, 0, 1} (2) fBIN(SIZE(s), SIZE(o))",5 Models,[0],[0]
"= BIN(s)− BIN(o) (3)
Then, given a featurization scheme, we may feed raw feature values (RAW VEC, for 3-LEVEL, e.g. concatenation of -1, 0 or 1 of all feature types, in that order, and in one-hot format), or feature embeddings (EMBEDDING, e.g. concatenation of embeddings looked up with feature values).",5 Models,[0],[0]
"Fi-
3More configuration details in supplementary material.",5 Models,[0],[0]
"4We also tried using bin numbers directly, however it does not produce ideal results (classification accuracy between 3- LEVEL and BIN-DIFF).",5 Models,[0],[0]
"Thus for brevity we drop this setup.
nally, let aNN,aWK be the penultimate-layer vectors of NN and WK (see Figure 1), we affine transform their concatenation to predict label ŷ with argmax on the final softmax layer:
ŷ",5 Models,[0],[0]
= argmax y softmax(σ(W,5 Models,[0],[0]
"[aNN;aWK] + b)) (4)
where σ is a ReLU nonlinearity.",5 Models,[0],[0]
"We will only report the results from the best-performing model configuration, which has BIN-DIFF + EMBEDDING.",5 Models,[0],[0]
"The model will be listed below as NN + WK-GOLD (i.e. with GOLD, Turker-annotated World Knowledge features).
",5 Models,[0],[0]
"For question (ii), we select a data-efficient feature learning model.",5 Models,[0],[0]
Following Forbes and Choi (2017) we evaluate the models with 5% or 20% of training data.,5 Models,[0],[0]
We experiment with several previously proposed techniques: (a) label spreading; (b) factor graph; (c) multi-LDA.,5 Models,[0],[0]
As a baseline we employ a simple but well-tuned logistic regressor (LR).,5 Models,[0],[0]
"We also initialize the factor graph with this LR, on account of its unexpectedly strong performance.5 Finally, observing that the feature types are inherently ordinal (e.g. SIZE from small to large), we also run ordinal logistic regression (Adeleke and Adepoju, 2010).",5 Models,[0],[0]
"For model selection we first evaluate the object-pair attribute data collected by Forbes and Choi (2017), 2.5k pairs labeled in the 3-LEVEL scheme.",5 Models,[0],[0]
We then compared the the LR and Ordinal-LR (our strongest models6 in this experiment) on 10k randomly generated object-pairs from our annotated nouns.,5 Models,[0],[0]
"The results are summarized in Table 2, where we see
5We verified our setup with the authors and they attributed the higher performance of our LR to hyperparameter choices.",5 Models,[0],[0]
"6Because the factor graph + LR gives very slight improvement, for simplicity we choose LR instead.
",5 Models,[0],[0]
"(i) 3-LEVEL propagation is much easier; (ii) our object-pairs are more challenging, likely due to sparsity with larger vocabulary size; (iii) ordinality information contributes substantially to performance.",5 Models,[0],[0]
The model that uses propagated features (w/ Ordinal-LR) will be listed as NN + WK-PROP.,5 Models,[0],[0]
"We evaluate the models on the task of classifying our 3,062 S-V-O triples by semantic plausibility (10-fold CV, taking the average over 20 runs with the same random seed).",6 Semantic Plausibility Results,[0],[0]
"We compare our three models in the 3-LEVEL and BIN-DIFF schemes, with NN + WK-PROP evaluated in 5% and 20% training conditions.",6 Semantic Plausibility Results,[0],[0]
The results are outlined in Table 3.,6 Semantic Plausibility Results,[0],[0]
Summarizing our findings: (i) world knowledge undoubtedly leads to a strong performance boost (∼8%); (ii) BIN-DIFF scheme works much better than 3-LEVEL — it manages to outperform the latter even with much weaker propagation accuracy; (iii) the accuracy loss with propagated features seems rather mild with 20% labeled training and the best scheme.,6 Semantic Plausibility Results,[0],[0]
"To understand what challenges remain in this task, we run the models above 200 times (10-fold CV, random shuffle at each run), and inspect the top 200 most frequently misclassified cases.",7 Error Analysis,[0],[0]
"The percentage statistics below are from counting the error cases.
",7 Error Analysis,[0],[0]
"In the cases where NN misclassifies while NN + WK-GOLD correctly classifies, 60% relates to SIZE and WEIGHT (e.g. missing man-hug-ant
(bad) or dog-pull-paper (good)).",7 Error Analysis,[0],[0]
PHASE takes up 18% (e.g. missing monkey-puff-smoke (good)).,7 Error Analysis,[0],[0]
"This validates the intuition that distributional contexts do not encode these types of world knowledge.
",7 Error Analysis,[0],[0]
"For cases often misclassified by all the models, we observe two main types of errors: (i) data sparsity; (ii) highly-specific attributes.
",7 Error Analysis,[0],[0]
Data sparsity (32%).,7 Error Analysis,[0],[0]
"man-choke-ant, e.g., is a singleton big-object-choke-small-object instance, and there are no distributionally similar verbs that can help (e.g. suffocate);",7 Error Analysis,[0],[0]
"For sun-heat-water, because the majority of the actions in the data are limited to solid objects, the models tend to predict implausible for whenever a gas/liquid appears as the object.
Highly-specific attributes (68%).",7 Error Analysis,[0],[0]
“long-tailed” physical attributes which are absent from our feature set are required.,7 Error Analysis,[0],[0]
"To exemplify a few:7
• edibility (21%).",7 Error Analysis,[0],[0]
"*-fry-egg (plausible) and *-fry-cup (implausible) are hard to distinguish because egg and cup are similar in SIZE/WEIGHT/..., however introducing large free-text data to help learn edibility misguides our model to mind selectional preference, causing mislabeling of other events.",7 Error Analysis,[0],[0]
• natural vs. artificial (18%).,7 Error Analysis,[0],[0]
"Turkers of-
ten think creating natural objects like moon or mountain is implausible but creating an equally big (but artificial) object like skyscraper is plausible.",7 Error Analysis,[0],[0]
• hollow objects (15%).,7 Error Analysis,[0],[0]
"plane-contain-shell
and purse-contain-scissors are plausible, but the hollow-object-can-contain-things attribute is failed to be captured.",7 Error Analysis,[0],[0]
• forefoot dexterity (5%).,7 Error Analysis,[0],[0]
"horse-hug-man is
implausible but bear-hug-man is plausible; For *-snatch-watch, girl is a plausible subject, but not pig.",7 Error Analysis,[0],[0]
"Obviously the dexterity of the forefoot of the agent matters here.
",7 Error Analysis,[0],[0]
"The analysis shows that the task and the dataset highlights the necessity for more sophisticated knowledge featurization and cleverer learning techniques (e.g. features from computer vision, propagation methods with stronger capacity to generalize) to reduce the cost of manual annotation.",7 Error Analysis,[0],[0]
7Percentages calculated with the 68% as the denominator.,7 Error Analysis,[0],[0]
Full list in supplementary material.,7 Error Analysis,[0],[0]
"We present the novel task of semantic plausibility, which forms the foundation of various interesting and complex NLP tasks in event semantics (Bowman et al., 2016; Mostafazadeh et al., 2016; Li and Jurafsky, 2017).",8 Conclusion,[0],[0]
"We collected a high-quality dedicated dataset, showed empirically that the conventional, distribution data only model fails on the task, and that clever world knowledge injection can help substantially with little annotation cost, which lends initial empirical support for the scalability of our approach in practical applications, i.e. labeling little but propagating well approximates performance with full annotation.",8 Conclusion,[0],[0]
"Granted that annotation-based injection method does not cover the full spectrum of leverageable world knowledge information (alternative/complementary sources being images and videos, e.g. Bagherinezhad et al. 2016), it is indeed irreplaceable in some cases (e.g. features such as WEIGHT or RIGIDITY are not easily learnable through visual modality), and in other cases presents a low-cost and effective option.",8 Conclusion,[0],[0]
"Finally, we also discovered the limitation of existing methods through a detailed error analysis, and thereby invite cross-area effort (e.g. multimodal knowledge features) in the future exploration in automated methods for semantic plausibility learning.",8 Conclusion,[0],[0]
This research was supported by NSF grant IIS 1523637.,Acknowledgments,[0],[0]
"Further, this material is based on research sponsored by DARPA under agreement number FA8750-18- 2-0017.",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgments,[0],[0]
We acknowledge the Texas Advanced Computing Center for providing grid resources that contributed to these results.,Acknowledgments,[0],[0]
We would also like to thank our reviewers for their insightful comments.,Acknowledgments,[0],[0]
"Distributional data tells us that a man can swallow candy, but not that a man can swallow a paintball, since this is never attested.",abstractText,[0],[0]
However both are physically plausible events.,abstractText,[0],[0]
This paper introduces the task of semantic plausibility: recognizing plausible but possibly novel events.,abstractText,[0],[0]
We present a new crowdsourced dataset of semantic plausibility judgments of single events such as man swallow paintball.,abstractText,[0],[0]
"Simple models based on distributional representations perform poorly on this task, despite doing well on selection preference, but injecting manually elicited knowledge about entity properties provides a substantial performance boost.",abstractText,[0],[0]
Our error analysis shows that our new dataset is a great testbed for semantic plausibility models: more sophisticated knowledge representation and propagation could address many of the remaining errors.,abstractText,[0],[0]
Modeling Semantic Plausibility by Injecting World Knowledge,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1064",text,[0],[0]
"Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT)
∗Work done at Huawei Noah’s Ark Lab, HongKong.
on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015).",1 Introduction,[0],[0]
"However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus.",1 Introduction,[0],[0]
"Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT.",1 Introduction,[0],[0]
"As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax.",1 Introduction,[0],[0]
"In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy.
",1 Introduction,[0],[0]
"In principle, syntax is a promising avenue for translation modeling.",1 Introduction,[0],[0]
"This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008;
688
Shen et al., 2008; Li et al., 2013).",1 Introduction,[0],[0]
"While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax.",1 Introduction,[0],[0]
Figure 1 (a) shows a Chinese-to-English translation example of NMT.,1 Introduction,[0],[0]
"In this example, the NMT seq2seq model incorrectly translates the Chinese noun phrase (i.e., 新 生/xinsheng 银行/yinhang) into a discontinuous phrase in English (i.e., new ... bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence.",1 Introduction,[0],[0]
"Statistics on our development set show that one forth of Chinese noun phrases are translated into discontinuous phrases in English, indicating the substantial disrespect of syntax in NMT",1 Introduction,[0],[0]
"translation.1 Figure 1 (b) shows another example with over translation, where the noun phrase 两/liang 个/ge 女孩/nvhai is translated twice in English.",1 Introduction,[0],[0]
"Similar to discontinuous translation, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence.
",1 Introduction,[0],[0]
"In this paper we are not aiming at solving any particular issue, either the discontinuous translation or the over translation.",1 Introduction,[0],[0]
"Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general.",1 Introduction,[0],[0]
"Specifically, rather than directly assigning each source word with manually designed syntactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information.",1 Introduction,[0],[0]
"On the basis, we systematically propose and compare several different approaches to incorporating the label sequence into the seq2seq NMT model.",1 Introduction,[0],[0]
Experimentation on Chinese-to-English translation demonstrates that all proposed approaches are able to improve the translation accuracy.,1 Introduction,[0],[0]
"As a background and a baseline, in this section, we briefly describe the NMT model with an attention mechanism by Bahdanau et al. (2015), which mainly consists of an encoder and a decoder, as shown in Figure 2.
",2 Attention-based NMT,[0],[0]
"Encoder The encoding of a source sentence is for1Manually examining 200 random such discontinuously translated noun phrases, we find that 90% of them should be continuously translated according to the reference translation.
mulated using a pair of neural networks, i.e., two recurrent neural networks (denoted bi-RNN): one reads an input sequence x =",2 Attention-based NMT,[0],[0]
"(x1, ..., xm) from left to right and outputs a forward sequence of hidden states ( −→ h1, ..., −→ hm), while the other operates from right to left and outputs a backward sequence ( ←− h1, ..., ←− hm).",2 Attention-based NMT,[0],[0]
Each source word xj is represented as hj (also referred to as word annotation vector): the concatenation of hidden states −→ hj and ←− hj .,2 Attention-based NMT,[0],[0]
"Such bi-RNN encodes not only the word itself but also its left and right context, which can provide important evidence for its translation.
",2 Attention-based NMT,[0],[0]
"Decoder The decoder is also an RNN that predicts a target sequence y = (y1, ..., yn).",2 Attention-based NMT,[0],[0]
"Each target word yi is predicted via a multi-layer perceptron (MLP) component which is based on a recurrent hidden state si, the previous predicted word yi−1, and a source-side context vector ci.",2 Attention-based NMT,[0],[0]
"Here, ci is calculated as a weighted sum over source annotation vectors (h1, ..., hm).",2 Attention-based NMT,[0],[0]
The weight vector αi ∈,2 Attention-based NMT,[0],[0]
"Rm over source annotation vectors is obtained by an attention model, which captures the correspondences between the source and the target languages.",2 Attention-based NMT,[0],[0]
The attention weight αij is computed based on the previous recurrent hidden state si−1 and source annotation vector hj .,2 Attention-based NMT,[0],[0]
"The conventional NMT models treat a sentence as a sequence of words and ignore external knowledge, failing to effectively capture various kinds of inherent structure of the sentence.",3 NMT with Source Syntax,[0],[0]
"To leverage external knowledge, specifically the syntax in the source side, we focus on the parse tree of a sentence and propose three different NMT models that explicitly consider the syntactic structure into encoding.",3 NMT with Source Syntax,[0],[0]
"Our purpose is to inform the NMT model the structural context of each word in its corresponding parse tree with the goal that the learned annotation vectors (h1, ..., hm) encode not
I love dogs
only the information of words and their surroundings, but also structural context in the parse tree.",3 NMT with Source Syntax,[0],[0]
"In the rest of this section, we use English sentences as examples to explain our methods.",3 NMT with Source Syntax,[0],[0]
"To obtain the structural context of a word in its parse tree, ideally the model should not only capture and remember the whole parse tree structure, but also discriminate the contexts of any two different words.",3.1 Syntax Representation,[0],[0]
"However, considering the lack of efficient way to directly model structural information, an alternative way is to linearize the phrase parse tree into a sequence of structural labels and learn the structural context through the sequence.",3.1 Syntax Representation,[0],[0]
"For example, Figure 3(c) shows the structural label sequence of Figure 3(b) in a simple way following a depth-first traversal order.",3.1 Syntax Representation,[0],[0]
"Note that linearizing a parse tree in a depth-first traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), suggesting that the linearized sequence can be viewed as an alternative to its tree structure.2
2We have also tried to include the ending brackets in the structural label sequence, as what (Vinyals et al., 2015; Choe
hwj and
←−−
hwj are the forward and
backward hidden states for word wj ,
−→
hli and
←−
hli are for structural label li, ewj is the word embedding for word wj , and ⊕ is for concatenation operator.
",3.1 Syntax Representation,[0],[0]
There is no doubt that the structural label sequence is much longer than its word sequence.,3.1 Syntax Representation,[0],[0]
"In order to obtain the structural label annotation vector for wi in word sequence, we simply look for wi’s part-of-speech (POS) tag in the label sequence and view the tag’s annotation vector as wi’s label annotation vector.",3.1 Syntax Representation,[0],[0]
This is because wi’s POS tag location can also represent wi’s location in the parse tree.,3.1 Syntax Representation,[0],[0]
"For example, in Figure 3, word w1 in (a) maps to l3 in (c) since l3 is the POS tag of w1.",3.1 Syntax Representation,[0],[0]
"Likewise, w2 maps to l5 and w3 to l7.",3.1 Syntax Representation,[0],[0]
"That is to say, we use l3’s learned annotation vector as w1’s label annotation vector.
and Charniak, 2016) do.",3.1 Syntax Representation,[0],[0]
"However, the performance gap is very small by adding the ending brackets or not.",3.1 Syntax Representation,[0],[0]
"In the next, we first propose two different encoders to augment word annotation vector with its corresponding label annotation vector, each of which consists of two RNNs 3: in one encoder, the two RNNs work independently (i.e., Parallel RNN Encoder) while in another encoder the two RNNs work in a hierarchical way (i.e., Hierarchical RNN Encoder).",3.2 RNN Encoders with Source Syntax,[0],[0]
The difference between the two encoders lies in how the two RNNs interact.,3.2 RNN Encoders with Source Syntax,[0],[0]
"Then, we propose the third encoder with a single RNN, which learns word and label annotation vectors stitchingly (i.e., Mixed RNN Encoder).",3.2 RNN Encoders with Source Syntax,[0],[0]
"Since any of the above three approaches focuses only on the encoder as to generate source annotation vectors along with structural information, we keep the rest part of the NMT models unchanged.
",3.2 RNN Encoders with Source Syntax,[0],[0]
"Parallel RNN Encoder Figure 4 (a) illustrates our Parallel RNN encoder, which includes two parallel RNNs: i.e., a word RNN and a structural label RNN.",3.2 RNN Encoders with Source Syntax,[0],[0]
"On the one hand, the word RNN, as in conventional NMT models, takes a word sequence as input and output a word annotation vector for each word.",3.2 RNN Encoders with Source Syntax,[0],[0]
"On the other hand, the structural label RNN takes the structural label sequence of the word sequence as input and obtains a label annotation vector for each label.",3.2 RNN Encoders with Source Syntax,[0],[0]
"Besides, we concatenate each word’s word annotation vector and its POS tag’s label annotation vector as the final annotation vector for the word.",3.2 RNN Encoders with Source Syntax,[0],[0]
"For example, the final annotation vector for word love in Figure 4 (a) is [ −−→ hw2; ←−− hw2; −→ hl5; ←− hl5], where the first two subitems [ −−→ hw2; ←−− hw2] are the word annotation vector and the rest two subitems",3.2 RNN Encoders with Source Syntax,[0],[0]
"[ −→ hl5; ←− hl5] are its POS tag VBP’s label annotation vector.
",3.2 RNN Encoders with Source Syntax,[0],[0]
"Hierarchical RNN Encoder Partially inspired by the model architecture of GNMT (Wu et al., 2016) which consists of multiple layers of LSTM RNNs, we propose a two-layer model architecture in which the lower layer is the structural label RNN while the upper layer is the word RNN, as shown in Figure 4 (b).",3.2 RNN Encoders with Source Syntax,[0],[0]
"We put the word RNN in the upper layer because each item in the word sequence can map into an item in the structural label sequence, while this does not hold if the order of the two RNNs is reversed.",3.2 RNN Encoders with Source Syntax,[0],[0]
"As shown in Figure 4 (b), for example, the POS tag VBP’s label annotation vector [ −→ hl5, ←− hl5] is concatenated with word
3Hereafter, we simplify bi-RNN as RNN.
",3.2 RNN Encoders with Source Syntax,[0],[0]
"love’s word embedding ew2 to feed as the input to the word RNN.
",3.2 RNN Encoders with Source Syntax,[0],[0]
Mixed RNN Encoder Figure 5 presents our Mixed RNN encoder.,3.2 RNN Encoders with Source Syntax,[0],[0]
"Similarly, the sequence of input is the linearization of its parse tree (as in Figure 3 (b)) following a depth-first traversal order, but being mixed with both words and structural labels in a stitching way.",3.2 RNN Encoders with Source Syntax,[0],[0]
"It shows that the RNN learns annotation vectors for both the words and the structural labels, though only the annotation vectors of words are further fed to decoding (e.g., ([ −→ h4, ←− h4], [ −→ h7, ←− h7], [ −→ h10, ←− h10])).",3.2 RNN Encoders with Source Syntax,[0],[0]
"Even though the annotation vectors of structural labels are not directly fed forward for decoding, the error signal is back propagated along the word sequence and allows the annotation vectors of structural labels being updated accordingly.",3.2 RNN Encoders with Source Syntax,[0],[0]
"Though all the three encoders model both word sequence and structural label sequence, the differences lie in their respective model architecture with respect to the degree of coupling the two sequences:
• In the Parallel RNN encoder, the word RNN and structural label RNN work in a parallel way.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"That is to say, the error signal back propagated from the word sequence would not affect the structural label RNN, and vice versa.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"In contrast, in the Hierarchical RNN encoder, the error signal back propagated from the word sequence has a direct impact on the structural label annotation vectors, and thus on the structural label embeddings.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"Finally, the Mixed RNN encoder ties the structural label sequence and word sequence together in the closest way.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"Therefore, the degrees of coupling the word and structural
label sequences in these three encoders are like this: Mixed RNN encoder > Hierarchical RNN encoder >",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"Parallel RNN encoder.
",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
• Figure 4 and Figure 5 suggest that the Mixed RNN encoder is the simplest.,3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders.,3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
We have presented our approaches to incorporating the source syntax into NMT encoders.,4 Experimentation,[0],[0]
"In this section, we evaluate their effectiveness on Chinese-to-English translation.",4 Experimentation,[0],[0]
"Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4",4.1 Experimental Settings,[0],[0]
"We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5",4.1 Experimental Settings,[0],[0]
"To get the source syntax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser 6 (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005).",4.1 Experimental Settings,[0],[0]
"We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.
",4.1 Experimental Settings,[0],[0]
"For efficient training of neural networks, we limit the maximum sentence length on both source and target sides to 50.",4.1 Experimental Settings,[0],[0]
"We also limit both the source and target vocabularies to the most frequent 16K words in Chinese and English, covering approximately 95.8% and 98.2% of the two corpora respectively.",4.1 Experimental Settings,[0],[0]
All the out-of-vocabulary words are mapped to a special token UNK.,4.1 Experimental Settings,[0],[0]
"Besides, the word embedding dimension is 620 and the size of a hidden layer is 1000.",4.1 Experimental Settings,[0],[0]
"All the other settings are the same as in Bahdanau et al.(2015).
",4.1 Experimental Settings,[0],[0]
"4The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
5http://www.itl.nist.gov/iad/mig/ tests/mt/
6https://github.com/slavpetrov/ berkeleyparser
The inventory of structural labels includes 16 phrase labels and 32 POS tags.",4.1 Experimental Settings,[0],[0]
"In both our Parallel RNN encoder and Hierarchical RNN encoder, we set the embedding dimension of these labels as 100 and the size of a hidden layer as 100.",4.1 Experimental Settings,[0],[0]
"Besides, the maximum structural label sequence length is set to 100.",4.1 Experimental Settings,[0],[0]
"In our Mixed RNN encoder, since we only have one input sequence, we equally treat the structural labels and words (i.e., a structural label is also initialized with 620 dimension embedding).",4.1 Experimental Settings,[0],[0]
"Compared to the baseline NMT model, the only different setting is that we increase the maximum sentence length on source-side from 50 to 150.
",4.1 Experimental Settings,[0],[0]
"We compare our method with two state-of-theart models of SMT and NMT:
• cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7
• RNNSearch: a re-implementation of the attentional NMT system (Bahdanau et al., 2015) with slight changes taken from dl4mt tutorial.8 For the activation function f of an RNN, RNNSearch uses the gated recurrent unit (GRU) recently proposed by (Cho et al., 2014b).",4.1 Experimental Settings,[0],[0]
"It incorporates dropout (Hinton et al., 2012) on the output layer and improves the attention model by feeding the lastly generated word.",4.1 Experimental Settings,[0],[0]
"We use AdaDelta (Zeiler, 2012) to optimize model parameters in training with the mini-batch size of 80.",4.1 Experimental Settings,[0],[0]
"For translation, a beam search with size 10 is employed.",4.1 Experimental Settings,[0],[0]
Table 1 shows the translation performances measured in BLEU score.,4.2 Experiment Results,[0],[0]
"Clearly, all the proposed NMT models with source syntax improve the translation accuracy over all test sets, although there exist considerable differences among different variants.
",4.2 Experiment Results,[0],[0]
Parameters The three proposed models introduce new parameters in different ways.,4.2 Experiment Results,[0],[0]
"As a baseline model, RNNSearch has 60.6M parameters.",4.2 Experiment Results,[0],[0]
"Due to the infrastructure similarity, the Parallel RNN system and the Hierarchical RNN system introduce
7https://github.com/redpony/cdec 8https://github.com/nyu-dl/
dl4mt-tutorial
the similar size of additional parameters, resulting from the RNN model for structural label sequences (about 0.1M parameters) and catering either the augmented annotation vectors (as shown in Figure 4 (a)) or the augmented word embeddings (as shown in Figure 4 (b))",4.2 Experiment Results,[0],[0]
(the remain parameters).,4.2 Experiment Results,[0],[0]
"It is not surprising that the Mixed RNN system does not require any additional parameters since though the input sequence becomes longer, we keep the vocabulary size unchanged, resulting in no additional parameters.
",4.2 Experiment Results,[0],[0]
Speed Introducing the source syntax slightly slows down the training speed.,4.2 Experiment Results,[0],[0]
"When running on a single GPU GeForce GTX 1080, the baseline model speeds 153 minutes per epoch with 14K updates while the proposed structural label RNNs in both Parallel RNN and Hierarchical RNN systems only increases the training time by about 6% (thanks to the small size of structural label embeddings and annotation vectors), and the Mixed RNN system spends 26% more training time to cater the triple sized input sequence.
",4.2 Experiment Results,[0],[0]
"Comparison with the baseline NMT model (RNNSearch) While all the three proposed NMT models outperform RNNSearch, the Parallel RNN system and the Hierarchical RNN system achieve similar accuracy (e.g., 36.6 v.s. 36.7).",4.2 Experiment Results,[0],[0]
"Besides, the Mixed RNN system achieves the best accuracy overall test sets with the only exception of NIST MT 02.",4.2 Experiment Results,[0],[0]
"Over all test sets, it outperforms RNNSearch by 1.4 BLEU points and outperforms the other two improved NMT models by 0.3∼0.4 BLEU points, suggesting the benefits of high degree of coupling the word sequence and the structural label sequence.",4.2 Experiment Results,[0],[0]
"This is very encouraging since the Mixed RNN encoder is the simplest, without introducing new parameters and with only slight additional training time.
",4.2 Experiment Results,[0],[0]
Comparison with the SMT model (cdec),4.2 Experiment Results,[0],[0]
Table 1 also shows that all NMT systems outperform the SMT system.,4.2 Experiment Results,[0],[0]
"This is very consistent with other studies on Chinese-to-English translation (Mi et al., 2016; Tu et al., 2017b; Wang et al., 2017).",4.2 Experiment Results,[0],[0]
"As the proposed Mixed RNN system achieves the best performance, we further look at the RNNSearch system and the Mixed RNN system to explore more on how syntactic information helps in translation.",5 Analysis,[0],[0]
"Following Bahdanau et al. (2015), we group sentences of similar lengths together and compute BLEU scores.",5.1 Effects on Long Sentences,[0],[0]
Figure 6 presents the BLEU scores over different lengths of input sentences.,5.1 Effects on Long Sentences,[0],[0]
It shows that Mixed RNN system outperforms RNNSearch over sentences with all different lengths.,5.1 Effects on Long Sentences,[0],[0]
"It also shows that the performance drops substantially
when the length of input sentences increases.",5.1 Effects on Long Sentences,[0],[0]
"This performance trend over the length is consistent with the findings in (Cho et al., 2014a; Tu et al., 2016, 2017a).",5.1 Effects on Long Sentences,[0],[0]
"We also observe that the NMT systems perform surprisingly bad on sentences over 50 in length, especially compared to the performance of SMT system (i.e., cdec).",5.1 Effects on Long Sentences,[0],[0]
"We think that the bad behavior of NMT systems towards long sentences (e.g., length of 50) is due to the following two reasons: (1) the maximum source sentence length limit is set as 50 in training, 9 making the learned models not ready to translate sentences over the maximum length limit; (2) NMT systems tend to stop early for long input sentences.",5.1 Effects on Long Sentences,[0],[0]
"Due to the capability of carrying syntactic information in source annotation vectors, we conjecture that our model with source syntax is also beneficial for alignment.",5.2 Analysis on Word Alignment,[0],[0]
"To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs.",5.2 Analysis on Word Alignment,[0],[0]
"We force the decoder to output reference translations, as to get automatic alignments between input sentences and their reference translations.",5.2 Analysis on Word Alignment,[0],[0]
"To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) in Table 2.
",5.2 Analysis on Word Alignment,[0],[0]
Table 2 shows that source syntax information improves the attention model as expected by maintaining an annotation vector summarizing structural information on each source word.,5.2 Analysis on Word Alignment,[0],[0]
The above subsection examines the alignment performance at the word level.,5.3 Analysis on Phrase Alignment,[0],[0]
"In this subsection, we turn to phrase alignment analysis by moving from word unit to phrase unit.",5.3 Analysis on Phrase Alignment,[0],[0]
"Given a source phrase XP, we use word alignments to examine if the phrase is translated continuously (Cont.), or dis-
9Though the maximum source length limit in Mixed RNN system is set to 150, it approximately contains 50 words in maximum.
continuously (Dis.), or if it is not translated at all (Un.).
",5.3 Analysis on Phrase Alignment,[0],[0]
"There are some phrases, such as noun phrases (NPs), prepositional phrases (PPs) that we usually expect to have a continuous translation.",5.3 Analysis on Phrase Alignment,[0],[0]
"With respect to several such types of phrases, Table 3 shows how these phrases are translated.",5.3 Analysis on Phrase Alignment,[0],[0]
"From the table, we see that translations of RNNSearch system do not respect source syntax very well.",5.3 Analysis on Phrase Alignment,[0],[0]
"For example, in RNNSearch translations, 57.3%, 33.6%, and 9.1% of PPs are translated continuously, discontinuously, and untranslated, respectively.",5.3 Analysis on Phrase Alignment,[0],[0]
"Fortunately, our Mixed RNN system is able to have more continuous translation for those phrases.",5.3 Analysis on Phrase Alignment,[0],[0]
Table 3 also suggests that there is still much room for NMT to show more respect to syntax.,5.3 Analysis on Phrase Alignment,[0],[0]
"To estimate the over translation generated by NMT, we propose ratio of over translation (ROT):
ROT =
∑ wi t(wi)
|w| (1)
where |w| is the number of words in consideration, t(wi) is the times of over translation for word wi.",5.4 Analysis on Over Translation,[0],[0]
Given a word w and its translation e = e1e2 . . .,5.4 Analysis on Over Translation,[0],[0]
"en, we have:
t(w) =",5.4 Analysis on Over Translation,[0],[0]
"|e| − |uniq(e)| (2)
where |e| is the number of words in w’s translation e, while |uniq(e)| is the number of unique words in e.",5.4 Analysis on Over Translation,[0],[0]
"For example, if a source word 香
港/xiangkang is translated as hong kong hong kong, we say it being over translated 2 times.
",5.4 Analysis on Over Translation,[0],[0]
Table 4 presents ROT grouped by some typical POS tags.,5.4 Analysis on Over Translation,[0],[0]
It is not surprising that RNNSearch system has high ROT with respect to POS tags of NR (proper noun) and CD (cardinal number): this is due to the fact that the two POS tags include high percentage of unknown words which tend to be translated multiple times in translation.,5.4 Analysis on Over Translation,[0],[0]
Words of DT (determiner) are another source of over translation since they are usually translated to multiple the in English.,5.4 Analysis on Over Translation,[0],[0]
"It also shows that by introducing source syntax, Mixed RNN system alleviates the over translation issue by 18%: ROT drops from 5.5% to 4.5%.",5.4 Analysis on Over Translation,[0],[0]
We analyze the translation of source-side rare words that are mapped to a special token UNK.,5.5 Analysis on Rare Word Translation,[0],[0]
"Given a rare word w, we examine if it is translated into a non-UNK word (non-UNK), UNK (UNK), or if it is not translated at all (Un.).
",5.5 Analysis on Rare Word Translation,[0],[0]
Table 5 shows how source-side rare words are translated.,5.5 Analysis on Rare Word Translation,[0],[0]
The four POS tags listed in the table account for about 90% of all rare words in the test sets.,5.5 Analysis on Rare Word Translation,[0],[0]
It shows that in Mixed RNN system is more likely to translate source-side rare words into UNK on the target side.,5.5 Analysis on Rare Word Translation,[0],[0]
This is reasonable since the source side rare words tends to be translated into rare words in the target side.,5.5 Analysis on Rare Word Translation,[0],[0]
"Moreover, it is hard to obtain its correct non-UNK translation when a source-side rare word is replaced as UNK.
",5.5 Analysis on Rare Word Translation,[0],[0]
Note that our approach is compatible with with approaches of open vocabulary.,5.5 Analysis on Rare Word Translation,[0],[0]
"Taking the sub-
word approach (Sennrich et al., 2016) as an example, for a word on the source side which is divided into several subword units, we can synthesize subPOS nodes that cover these units.",5.5 Analysis on Rare Word Translation,[0],[0]
"For example, if misunderstand/VB is divided into units of mis and understand, we construct substructure (VB (VB-F mis) (VB-I understand)).",5.5 Analysis on Rare Word Translation,[0],[0]
"While there has been substantial work on linguistically motivated SMT, approaches that leverage syntax for NMT start to shed light very recently.",6 Related Work,[0],[0]
"Generally speaking, NMT can provide a flexible mechanism for adding linguistic knowledge, thanks to its strong capability of automatically learning feature representations.
",6 Related Work,[0],[0]
"Eriguchi et al. (2016) propose a tree-tosequence model that learns annotation vectors not only for terminal words, but also for non-terminal nodes.",6 Related Work,[0],[0]
They also allow the attention model to align target words to non-terminal nodes.,6 Related Work,[0],[0]
Our approach is similar to theirs by using source-side phrase parse tree.,6 Related Work,[0],[0]
"However, our Mixed RNN system, for example, incorporates syntax information by learning annotation vectors of syntactic labels and words stitchingly, but is still a sequenceto-sequence model, with no extra parameters and with less increased training time.
",6 Related Work,[0],[0]
Sennrich and Haddow (2016) define a few linguistically motivated features that are attached to each individual words.,6 Related Work,[0],[0]
"Their features include lemmas, subword tags, POS tags, dependency labels, etc.",6 Related Work,[0],[0]
"They concatenate feature embeddings with word embeddings and feed the concatenated em-
beddings into the NMT encoder.",6 Related Work,[0],[0]
"On the contrast, we do not specify any feature, but let the model implicitly learn useful information from the structural label sequence.
",6 Related Work,[0],[0]
Shi et al. (2016) design a few experiments to investigate if the NMT system without external linguistic input is capable of learning syntactic information on the source-side as a by-product of training.,6 Related Work,[0],[0]
"However, their work is not focusing on improving NMT with linguistic input.",6 Related Work,[0],[0]
"Moreover, we analyze what syntax is disrespected in translation from several new perspectives.
",6 Related Work,[0],[0]
Garcı́a-Martı́nez et al. (2016) generalize NMT outputs as lemmas and morphological factors in order to alleviate the issues of large vocabulary and out-of-vocabulary word translation.,6 Related Work,[0],[0]
The lemmas and corresponding factors are then used to generate final words in target language.,6 Related Work,[0],[0]
"Though they use linguistic input on the target side, they are limited to the word level features.",6 Related Work,[0],[0]
"Phrase level, or even sentence level linguistic features are harder to obtain for a generation task such as machine translation, since this would require incremental parsing of the hypotheses at test time.",6 Related Work,[0],[0]
"In this paper, we have investigated whether and how source syntax can explicitly help NMT to improve its translation accuracy.
",7 Conclusion,[0],[0]
"To obtain syntactic knowledge, we linearize a parse tree into a structural label sequence and let the model automatically learn useful information through it.",7 Conclusion,[0],[0]
"Specifically, we have described three different models to capture the syntax knowledge, i.e., Parallel RNN, Hierarchical RNN, and Mixed RNN.",7 Conclusion,[0],[0]
Experimentation on Chinese-to-English translation shows that all proposed models yield improvements over a state-ofthe-art baseline NMT system.,7 Conclusion,[0],[0]
"It is also interesting to note that the simplest model (i.e., Mixed RNN) achieves the best performance, resulting in obtaining significant improvements of 1.4 BLEU points on NIST MT 02 to 05.
",7 Conclusion,[0],[0]
"In this paper, we have also analyzed the translation behavior of our improved system against the state-of-the-art NMT baseline system from several perspectives.",7 Conclusion,[0],[0]
Our analysis shows that there is still much room for NMT translation to be consistent with source syntax.,7 Conclusion,[0],[0]
"In our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syn-
tactic features (e.g., features showing the syntactic role that a word is playing) for NMT, and employing the source syntax to constrain and guild the attention models.",7 Conclusion,[0],[0]
"The authors would like to thank three anonymous reviewers for providing helpful comments, and also acknowledge Xing Wang, Xiangyu Duan, Zhengxian Gong for useful discussions.",Acknowledgments,[0],[0]
"This work was supported by National Natural Science Foundation of China (Grant No. 61525205, 61331011, 61401295).",Acknowledgments,[0],[0]
"Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements.",abstractText,[0],[0]
"Specifically, we linearize parse trees of source sentences to obtain structural label sequences.",abstractText,[0],[0]
"On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed.",abstractText,[0],[0]
Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy.,abstractText,[0],[0]
"It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points.",abstractText,[0],[0]
"Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.",abstractText,[0],[0]
Modeling Source Syntax for Neural Machine Translation,title,[0],[0]
"In many real-world domains, data acquisition is costly.",1. Introduction,[0],[0]
"For instance, magnetic resonance imaging (MRI) requires scan times proportional to the number of measurements, which can be significant for patients (Lustig et al., 2008).",1. Introduction,[0],[0]
"Geophysical applications like oil drilling require expensive simulation of seismic waves (Qaisar et al., 2013).",1. Introduction,[0],[0]
"Such appli-
1Computer Science Department, Stanford University, CA, USA.",1. Introduction,[0],[0]
"Correspondence to: Manik Dhar <dmanik@cs.stanford.edu>, Aditya Grover",1. Introduction,[0],[0]
"<adityag@cs.stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
cations, among many others, can benefit significantly from compressed sensing techniques to acquire signals efficiently (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).
",1. Introduction,[0],[0]
"In compressed sensing, we wish to acquire an n-dimensional signal x ∈ Rn using only m n measurements linear in x.",1. Introduction,[0],[0]
"The measurements could potentially be noisy, but even in the absence of any noise we need to impose additional structure on the signal to guarantee unique recovery.",1. Introduction,[0],[0]
"Classical results on compressed sensing impose structure by assuming the underlying signal to be approximately l-sparse in some known basis, i.e., the l-largest entries dominate the rest.",1. Introduction,[0],[0]
"For instance, images and audio signals are typically sparse in the wavelet and Fourier basis respectively (Mallat, 2008).",1. Introduction,[0],[0]
"If the matrix of linear vectors relating the signal and measurements satisfies certain mild conditions, then one can provably recover x with only m = O(l log nl ) measurements using LASSO (Tibshirani, 1996; Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006; Bickel et al., 2009).
",1. Introduction,[0],[0]
"Alternatively, structural assumptions on the signals being sensed can be learned from data, e.g., using a dataset of typical signals (Baraniuk et al., 2010; Peyre, 2010; Chen et al., 2010; Yu & Sapiro, 2011).",1. Introduction,[0],[0]
"Particularly relevant to this work, Bora et al. (2017) proposed an approach where structure is provided by a deep generative model learned from data.",1. Introduction,[0],[0]
"Specifically, the underlying signal x being sensed is assumed to be close to the range of a deterministic function expressed by a pretrained, latent variable modelG : Rk → Rn such that x ≈ G(z) where z ∈ Rk denote the latent variables.",1. Introduction,[0],[0]
"Consequently, the signal x is recovered by optimizing for a latent vector z that minimizes the `2 distance between the measurements corresponding to G(z) and the actual ones.",1. Introduction,[0],[0]
"Even though the objective being optimized in this case is non-convex, empirical results suggest that the reconstruction error decreases much faster than LASSO-based recovery as we increase the number of measurements.
",1. Introduction,[0],[0]
"A limitation of the above approach is that the recovered signal is constrained to be in the range of the generator function G. Hence, if the true signal being sensed is not in the range of G, the algorithm cannot drive the reconstruction error to zero even when m ≥ n",1. Introduction,[0],[0]
(even if we ignore error due to measurement noise and non-convex optimization).,1. Introduction,[0],[0]
"This is also observed empirically, as the reconstruction error of generative model-based recovery saturates as we keep
increasing the number of measurements m. On the other hand, LASSO-based recovery continues to shrink the error with increasing number of measurements, eventually outperforming the generative model-based recovery.
",1. Introduction,[0],[0]
"To overcome this limitation, we propose a framework that allows recovery of signals with sparse deviations from the set defined by the range of the generator function.",1. Introduction,[0],[0]
"The recovered signals have the general form of G(ẑ) + ν̂, where ν̂ ∈ Rn is a sparse vector.",1. Introduction,[0],[0]
This allows the recovery algorithm to consider signals away from the range of the generator function.,1. Introduction,[0],[0]
"Similar to LASSO, we relax the hardness in optimizing for sparse vectors by minimizing the `1 norm of the deviations.",1. Introduction,[0],[0]
"Unlike LASSO-based recovery, we can exploit the rich structure imposed by a (deep) generative model (at the expense of solving a hard optimization problem if G is non-convex).",1. Introduction,[0],[0]
"In fact, we show that LASSO-based recovery is a special case of our framework if the generator function G maps all z to the origin.",1. Introduction,[0],[0]
"Unlike generative model-based recovery, the signals recovered by our algorithm are not constrained to be in the range of the generator function.
",1. Introduction,[0],[0]
"Our proposed algorithm, referred to as Sparse-Gen, has desirable theoretical properties and empirical performance.",1. Introduction,[0],[0]
"Theoretically, we derive upper bounds on the reconstruction error for an optimal decoder with respect to the proposed model and show that this error vanishes with m = n measurements.",1. Introduction,[0],[0]
"We confirm our theory empirically, wherein we find that recovery using Sparse-Gen with variational autoencoders (Kingma & Welling, 2014) as the underlying generative model outperforms both LASSO-based and generative model-based recovery in terms of the reconstruction errors for the same number of measurements for MNIST and Omniglot datasets.",1. Introduction,[0],[0]
"Additionally, we observe significant improvements in the more practical and novel task of transfer compressed sensing where a generative model on a data-rich, source domain provides a prior for sensing a data-scarce, target domain.",1. Introduction,[0],[0]
"In this section, we review the necessary background and prior work in modeling domain specific structure in compressed sensing.",2. Preliminaries,[0],[0]
"We are interested in solving the following system of equations,
y = Ax (1)
where x ∈ Rn is the signal of interest being sensed through measurements y ∈ Rm, and A ∈ Rm×n is a measurement matrix.",2. Preliminaries,[0],[0]
"For efficient acquisition of signals, we will design measurement matrices such that m n. However, the system is under-determined whenever rank(A) <",2. Preliminaries,[0],[0]
"n. Hence, unique recovery requires additional assumptions on x. We now discuss two ways to model the structure of x.
Sparsity.",2. Preliminaries,[0],[0]
Sparsity in a well-chosen basis is natural in many domains.,2. Preliminaries,[0],[0]
"For instance, natural images are sparse in the wavelet basis whereas audio signals exhibit sparsity in the Fourier basis (Mallat, 2008).",2. Preliminaries,[0],[0]
"Hence, it is natural to assume the domain of signals x we are interested in recovering is
Sl(0) = {x : ‖x− 0‖0 ≤",2. Preliminaries,[0],[0]
l}.,2. Preliminaries,[0],[0]
"(2)
This is the set of l-sparse vectors with the `0 distance measured from the origin.",2. Preliminaries,[0],[0]
"Such assumptions dominate the prior literature in compressed sensing and can be further relaxed to recover approximately sparse signals (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).
",2. Preliminaries,[0],[0]
Latent variable generative models.,2. Preliminaries,[0],[0]
"A latent variable model specifies a joint distribution Pθ(x, z) over the observed data x (e.g., images) and a set of latent variables z ∈",2. Preliminaries,[0],[0]
"Rk (e.g., features).",2. Preliminaries,[0],[0]
"Given a training set of signals {x1, · · · , xM}, we can learn the parameters θ of such a model, e.g., via maximum likelihood.",2. Preliminaries,[0],[0]
"When Pθ(x, z) is parameterized using deep neural networks, such generative models can effectively model complex, high-dimensional signal distributions for modalities such as images and audio (Kingma & Welling, 2014; Goodfellow et al., 2014).
",2. Preliminaries,[0],[0]
"Given a pretrained latent variable generative model with parameters θ, we can associate a generative model function G : Rk → Rn mapping a latent vector z to the mean of the conditional distribution Pθ(x|z).",2. Preliminaries,[0],[0]
"Thereafter, the space of signals that can be recovered with such a model is given by the range of the generator function,
SG = {G(z) : z ∈ Rk}.",2. Preliminaries,[0],[0]
"(3)
Note that the set is defined with respect to the latent vectors z, and we omit the dependence of G on the parameters θ (which are fixed for a pretrained model) for brevity.",2. Preliminaries,[0],[0]
"Signal recovery in compressed sensing algorithm typically involves solving an optimization problem consistent with the modeling assumptions on the domain of the signals being sensed.
",2.1. Recovery algorithms,[0],[0]
Sparse vector recovery using LASSO.,2.1. Recovery algorithms,[0],[0]
"Under the assumptions of sparsity, the signal x can be recovered by solving an `0 minimization problem (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).
",2.1. Recovery algorithms,[0],[0]
"min x ‖x‖0
s.t.",2.1. Recovery algorithms,[0],[0]
"Ax = y. (4)
",2.1. Recovery algorithms,[0],[0]
"The objective above is however NP-hard to optimize, and hence, it is standard to consider a convex relaxation,
min x ‖x‖1
s.t.",2.1. Recovery algorithms,[0],[0]
"Ax = y. (5)
",2.1. Recovery algorithms,[0],[0]
"In practice, it is common to solve the Lagrangian of the above problem.",2.1. Recovery algorithms,[0],[0]
We refer to this method as LASSO-based recovery due to similarities of the objective in Eq.,2.1. Recovery algorithms,[0],[0]
"(5) to the LASSO regularization used broadly in machine learning (Tibshirani, 1996).",2.1. Recovery algorithms,[0],[0]
"LASSO-based recovery is the predominant technique for recovering sparse signals since it involves solving a tractable convex optimization problem.
",2.1. Recovery algorithms,[0],[0]
In order to guarantee unique recovery to the underdetermined system in Eq.,2.1. Recovery algorithms,[0],[0]
"(1), the measurement matrix A is designed to satisfy the Restricted Isometry Property (RIP) or the Restricted Eigenvalue Condition (REC) for l-sparse matrices with high probability (Candès & Tao, 2005; Bickel et al., 2009).",2.1. Recovery algorithms,[0],[0]
We define these conditions below.,2.1. Recovery algorithms,[0],[0]
Definition 1.,2.1. Recovery algorithms,[0],[0]
Let Sl(0) ⊂,2.1. Recovery algorithms,[0],[0]
Rn be the set of l-sparse vectors.,2.1. Recovery algorithms,[0],[0]
"For some parameter α ∈ (0, 1), a matrix A ∈ Rm×n is said to satisfy RIP(l, α) if ∀ x ∈ Sl(0),
(1− α)‖x‖2 ≤ ‖Ax‖2 ≤ (1 + α)‖x‖2.
",2.1. Recovery algorithms,[0],[0]
Definition 2.,2.1. Recovery algorithms,[0],[0]
Let Sl(0) ⊂,2.1. Recovery algorithms,[0],[0]
Rn be the set of l-sparse vectors.,2.1. Recovery algorithms,[0],[0]
"For some parameter γ > 0, a matrix A ∈ Rm×n is said to satisfy REC(l, γ) if ∀ x ∈ Sl(0),
‖Ax‖2 ≥ γ‖x‖2.
",2.1. Recovery algorithms,[0],[0]
"Intuitively, RIP implies that A approximately preserves Euclidean norms for sparse vectors and REC implies that sparse vectors are far from the nullspace of A. Many classes of matrices satisfy these conditions with high probability, including random Gaussian and Bernoulli matrices where every entry of the matrix is sampled from a standard normal and uniform Bernoulli distribution respectively (Baraniuk et al., 2008).
",2.1. Recovery algorithms,[0],[0]
Generative model vector recovery using gradient descent.,2.1. Recovery algorithms,[0],[0]
If the signals being sensed are assumed to lie close to the range SG of a generative model function G as defined in Eq.,2.1. Recovery algorithms,[0],[0]
"(3) , then we can recover the best approximation to the true signal by `2-minimization over z,
min z ‖AG(z)− y‖22.",2.1. Recovery algorithms,[0],[0]
"(6)
The function G is typically expressed as a deep neural network which makes the overall objective non-convex, but differentiable almost everywhere w.r.t z.",2.1. Recovery algorithms,[0],[0]
"In practice, good reconstructions can be recovered by gradient-based optimization methods.",2.1. Recovery algorithms,[0],[0]
"We refer to this method proposed by Bora et al. (2017) as generative model-based recovery.
",2.1. Recovery algorithms,[0],[0]
"To guarantee unique recovery, generative model-based recovery makes two key assumptions.",2.1. Recovery algorithms,[0],[0]
"First, the generator functionG is assumed to be L-Lipschitz, i.e., ∀ z1, z2 ∈ Rk,
‖G(z1)−G(z2)‖2 ≤ L‖z1",2.1. Recovery algorithms,[0],[0]
"− z2‖2.
",2.1. Recovery algorithms,[0],[0]
"Secondly, the measurement matrix A is designed to satisfy the Set-Restricted Eigenvalue Condition (S-REC) with high probability (Bora et al., 2017).
",2.1. Recovery algorithms,[0],[0]
Definition 3.,2.1. Recovery algorithms,[0],[0]
Let S ⊆ Rn.,2.1. Recovery algorithms,[0],[0]
"For some parameters γ > 0, δ ≥ 0, a matrix A ∈ Rm×n is said to satisfy the SREC(S, γ, δ) if ∀ x1, x2 ∈ S,
‖A(x1 − x2)‖2 ≥",2.1. Recovery algorithms,[0],[0]
γ‖x1,2.1. Recovery algorithms,[0],[0]
− x2‖2,2.1. Recovery algorithms,[0],[0]
"− δ.
",2.1. Recovery algorithms,[0],[0]
S-REC generalizes REC to an arbitrary set of vectors S as opposed to just considering the set of approximately sparse vectors Sl(0) and allowing an additional slack term δ.,2.1. Recovery algorithms,[0],[0]
"In particular, S is chosen to be the range of the generator function G for generative model-based recovery.",2.1. Recovery algorithms,[0],[0]
The modeling assumptions based on sparsity and generative modeling discussed in the previous section can be limiting in many cases.,3. The Sparse-Gen framework,[0],[0]
"On one hand, sparsity assumes a relatively weak prior over the signals being sensed.",3. The Sparse-Gen framework,[0],[0]
"Empirically, we observe that the recovered signals xL have large reconstruction error ‖xL− x‖22 especially when the number of measurements m is small.",3. The Sparse-Gen framework,[0],[0]
"On the other hand, generative models imposes a very strong, but rigid prior which works well when the number of measurements is small.",3. The Sparse-Gen framework,[0],[0]
"However, the performance of the corresponding recovery methods saturates with increasing measurements since the recovered signal xG = G(zG) is constrained to lie in the range of the generator function G. If zG ∈ Rk is the optimum value returned by an optimization procedure for Eq.",3. The Sparse-Gen framework,[0],[0]
"(6), then the reconstruction error ‖xG − x‖22 is limited by the dimensionality of the latent space and the quality of the generator function.
",3. The Sparse-Gen framework,[0],[0]
"To sidestep the above limitations, we consider a strictly more expressive class of signals by allowing sparse deviations from the range of a generator function.",3. The Sparse-Gen framework,[0],[0]
"Formally, the domain of the recovered signals is given by,
Sl,G = ∪z∈Dom(G)Sl(G(z)) (7)
where Sl(G(z)) denotes the set of sparse vectors centered on G(z) and z varies over the domain of G (typically Rk).",3. The Sparse-Gen framework,[0],[0]
"We refer to this modeling assumption and the consequent algorithmic framework for recovery as Sparse-Gen.
Based on this modeling assumption, we will recover signals of the form G(z) + ν for some ν ∈",3. The Sparse-Gen framework,[0],[0]
Rn that is preferably sparse.,3. The Sparse-Gen framework,[0],[0]
"Specifically, we consider the optimization of a hybrid objective,
min z,ν ‖ν‖0
s.t.",3. The Sparse-Gen framework,[0],[0]
"A (G(z) + ν) = y. (8)
In the above optimization problem the objective is nonconvex and non-differentiable, while the constraint is nonconvex (for general G), making the above optimization
problem hard to solve.",3. The Sparse-Gen framework,[0],[0]
"To ease the optimization problem, we propose two modifications.",3. The Sparse-Gen framework,[0],[0]
"First, we relax the `0 minimization to an `1 minimization similar to LASSO.
",3. The Sparse-Gen framework,[0],[0]
"min z,ν",3. The Sparse-Gen framework,[0],[0]
"‖ν‖1
s.t.",3. The Sparse-Gen framework,[0],[0]
"A (G(z) + ν) = y. (9)
",3. The Sparse-Gen framework,[0],[0]
"Next, we square the non-convex constraint on both sides and consider the Lagrangian of the above problem to get the final unconstrained optimization problem for Sparse-Gen,
min z,ν ‖ν‖1 + λ‖A (G(z) + ν)− y‖22 (10)
where λ is the Lagrange multiplier.
",3. The Sparse-Gen framework,[0],[0]
The above optimization problem is non-differentiable w.r.t.,3. The Sparse-Gen framework,[0],[0]
ν and non-convex w.r.t.,3. The Sparse-Gen framework,[0],[0]
z,3. The Sparse-Gen framework,[0],[0]
(if G is non-convex).,3. The Sparse-Gen framework,[0],[0]
"In practice, it can be solved in practice using gradient descent (since the non-differentiability is only at a finite number of points) or using sequential convex programming (SCP).",3. The Sparse-Gen framework,[0],[0]
"SCP is an effective heuristic for non-convex problems where the convex portions of the problem are solved using a standard convex optimization technique (Boyd & Vandenberghe, 2004).",3. The Sparse-Gen framework,[0],[0]
"In the case of Eq. (10), the optimization w.r.t. ν (for fixed z) is a convex optimization problem whereas the non-convexity typically involves differentiable terms (w.r.t. z) if G is a deep neural network.",3. The Sparse-Gen framework,[0],[0]
"Empirically, we find excellent recovery by standard first order gradient-based methods (Duchi et al., 2011; Tieleman & Hinton, 2012; Kingma & Ba, 2015).
",3. The Sparse-Gen framework,[0],[0]
"Unlike LASSO-based recovery which recovers only sparse signals, Sparse-Gen can impose a stronger domain-specific prior using a generative model.",3. The Sparse-Gen framework,[0],[0]
"If we fix the generator function to map all z to the origin, we recover LASSO-based recovery as a special case of Sparse-Gen. Additionally, Sparse-Gen is not constrained to recover signals over the range of G, as in the case of generative model-based recovery.",3. The Sparse-Gen framework,[0],[0]
"In fact, it can recover signals with sparse deviations from the range of G. Note that the sparse deviations can be
defined in a basis different from the canonical basis.",3. The Sparse-Gen framework,[0],[0]
"In such cases, we consider the following optimization problem,
min z,ν ‖Bν‖1 + λ‖A",3. The Sparse-Gen framework,[0],[0]
"(G(z) + ν)− y‖22 (11)
where B is a change of basis matrix that promotes sparsity of the vector Bν",3. The Sparse-Gen framework,[0],[0]
.,3. The Sparse-Gen framework,[0],[0]
Figure 1 illustrates the differences in modeling assumptions between Sparse-Gen and other frameworks.,3. The Sparse-Gen framework,[0],[0]
The proofs for all results in this section are given in the Appendix.,4. Theoretical Analysis,[0],[0]
"Our analysis and experiments account for measurement noise in compressed sensing, i.e.,
y = Ax+ .",4. Theoretical Analysis,[0],[0]
"(12)
Let ∆ :",4. Theoretical Analysis,[0],[0]
Rm → Rn denote an arbitrary decoding function used to recover the true signal x from the measurements y ∈ Rm.,4. Theoretical Analysis,[0],[0]
"Our analysis will upper bound the `2-error in recovery incurred by our proposed framework using mixed norm guarantees (in particular, `2/`1).",4. Theoretical Analysis,[0],[0]
"To this end, we first state some key definitions.",4. Theoretical Analysis,[0],[0]
"Define the least possible `1 error for recovering x under the Sparse-Gen modeling as,
σSl,G(x) = inf x̂∈Sl,G
‖x− x̂‖1
where the optimal x̂ is the closest point to x in the allowed domain Sl,G. We now state the main lemma guiding the theoretical analysis.",4. Theoretical Analysis,[0],[0]
Lemma 1.,4. Theoretical Analysis,[0],[0]
"Given a function G : Rk → Rn and measurement noise with ‖ ‖2 ≤ max, let A be any matrix that satisfies S-REC(S1.5l,G, (1− α), δ) and RIP(2l, α) for some α ∈ (0, 1), l > 0.",4. Theoretical Analysis,[0],[0]
"Then, there exists a decoder ∆ :",4. Theoretical Analysis,[0],[0]
"Rm → Rn such that,
‖x−∆(Ax+ )‖2 ≤ (2l)−1/2C0σl,G(x) +",4. Theoretical Analysis,[0],[0]
"C1 max + δ′
for all x ∈ Rn, where C0 = 2((1+α)(1−α)−1 +1), C1 = 2(1− α)−1, and δ′ = δ(1− α)−1.
",4. Theoretical Analysis,[0],[0]
The above lemma shows that there exists a decoder such that the error in recovery can be upper bounded for measurement matrices satisfying S-REC and RIP.,4. Theoretical Analysis,[0],[0]
Note that Lemma 1 only guarantees the existence of such a decoder and does not prescribe an optimization algorithm for recovery.,4. Theoretical Analysis,[0],[0]
"Apart from the errors due to the bounded measurement noise max and a scaled slack term appearing in the S-REC condition δ′, the major term in the upper bound corresponds to (up to constants) the minimum possible error incurred by the best possible recovery vector in Sl,G given by σl,G(x).",4. Theoretical Analysis,[0],[0]
"Similar terms appear invariably in the compressed sensing literature and are directly related to the modeling assumptions regarding x (for example, Theorem 8.3 in Cohen et al. (2009)).
",4. Theoretical Analysis,[0],[0]
"Our next lemma shows that random Gaussian matrices satisfy the S-REC (over the range of Lipschitz generative model functions) and RIP conditions with high probability for G with bounded domain, both of which together are sufficient conditions for Lemma 1 to hold.
",4. Theoretical Analysis,[0],[0]
Lemma 2.,4. Theoretical Analysis,[0],[0]
LetG : Bk(r)→ Rn be anL-Lipschitz function where Bk(r) = {z | z ∈,4. Theoretical Analysis,[0],[0]
"Rk, ‖z‖2 ≤ r} is the `2-norm ball in Rk.",4. Theoretical Analysis,[0],[0]
"For α ∈ (0, 1), if
m = O
( 1
α2
( k log ( Lr
δ
)",4. Theoretical Analysis,[0],[0]
+,4. Theoretical Analysis,[0],[0]
l log(n/l) )),4. Theoretical Analysis,[0],[0]
"then a random matrix A ∈ Rm×n with i.i.d. entries such that Aij ∼ N ( 0, 1m ) satisfies the S-REC(S1.5l,G, 1− α, δ) and RIP(2l, α) with 1− e−Ω(α2m) probability.
",4. Theoretical Analysis,[0],[0]
"Using Lemma 1 and Lemma 2, we can bound the error due to decoding with generative models and random Gaussian measurement matrices in the following result.
",4. Theoretical Analysis,[0],[0]
Theorem 1.,4. Theoretical Analysis,[0],[0]
Let G : Bk(r)→,4. Theoretical Analysis,[0],[0]
Rn be an L-Lipschitz function.,4. Theoretical Analysis,[0],[0]
"For any α ∈ (0, 1), l > 0, let A ∈ Rm×n be a random Gaussian matrix with
m = O
( 1
α2
( k log ( Lr
δ
) +",4. Theoretical Analysis,[0],[0]
l log(n/l) )),4. Theoretical Analysis,[0],[0]
rows of i.i.d.,4. Theoretical Analysis,[0],[0]
"entries scaled such that Ai,j ∼ N(0, 1/m).",4. Theoretical Analysis,[0],[0]
Let ∆ be the decoder satisfying Lemma 1.,4. Theoretical Analysis,[0],[0]
"Then, we have with 1− e−Ω(α2m) probability,
‖x−∆(Ax+ )",4. Theoretical Analysis,[0],[0]
"‖2 ≤ (2l)−1/2C0σl,G(x) +",4. Theoretical Analysis,[0],[0]
"C1 max + δ′
for all x ∈ Rn, ‖ ‖2 ≤ max, where C0, C1, γ, δ′ are constants defined in Lemma 1.
",4. Theoretical Analysis,[0],[0]
"From the above lemma, we see that the number of measurements needed to guarantee upper bounds on the reconstruction error of any signal with high probability depends on two terms.",4. Theoretical Analysis,[0],[0]
"The first term includes dependence on the Lipschitz constant L of the generative model function G. A high Lipschitz constant makes recovery harder (by requiring a larger
number of measurements), but only contributes logarithmically.",4. Theoretical Analysis,[0],[0]
"The second term, typical of results in sparse vector recovery, shows a logarithmic growth on the dimensionality n of the signals.",4. Theoretical Analysis,[0],[0]
"Ignoring logarithmic dependences and constants, recovery using Sparse-Gen requires about O(k + l) measurements for recovery.",4. Theoretical Analysis,[0],[0]
Note that Theorem 1 assumes access to an optimization oracle for decoding.,4. Theoretical Analysis,[0],[0]
"In practice, we consider the solutions returned by gradient-based optimization methods to a non-convex objective defined in Eq.",4. Theoretical Analysis,[0],[0]
"(11) that are not guaranteed to correspond to the optimal decoding in general.
",4. Theoretical Analysis,[0],[0]
"Finally, we obtain tighter bounds for the special case when G is expressed using a neural network with only ReLU activations.",4. Theoretical Analysis,[0],[0]
"These bounds do not rely explicitly on the Lipschitz constant L or require the domain of G to be bounded.
",4. Theoretical Analysis,[0],[0]
Theorem 2.,4. Theoretical Analysis,[0],[0]
"If G : Rk → Rn is a neural network of depth d with only ReLU activations and at most c nodes in each layer, then the guarantees of Theorem 1 hold for
m = O
( 1
α2
( (k + l)d log",4. Theoretical Analysis,[0],[0]
c+,4. Theoretical Analysis,[0],[0]
(k + l),4. Theoretical Analysis,[0],[0]
"log(n/l) )) .
",4. Theoretical Analysis,[0],[0]
"Our theoretical analysis formalizes the key properties of recovering signals using Sparse-Gen. As shown in Lemma 1, there exists a decoder for recovery based on such modeling assumptions that extends recovery guarantees based on vanilla sparse vector recovery and generative model-based recovery.",4. Theoretical Analysis,[0],[0]
Such recovery requires measurement matrices that satisfy both the RIP and S-REC conditions over the set of vectors that deviate in sparse directions from the range of a generative model function.,4. Theoretical Analysis,[0],[0]
"In Theorems 1-2, we observed that the number of measurements required to guarantee recovery with high probability grow almost linearly (with some logarithmic terms) with the latent space dimensionality k of the generative model and the permissible sparsity l for deviating from the range of the generative model.",4. Theoretical Analysis,[0],[0]
We evaluated Sparse-Gen for compressed sensing of highdimensional signals from the domain of benchmark image datasets.,5. Experimental Evaluation,[0],[0]
"Specifically, we considered the MNIST dataset of handwritten digits (LeCun et al., 2010) and the OMNIGLOT dataset of handwritten characters (Lake et al., 2015).",5. Experimental Evaluation,[0],[0]
"Both these datasets have the same data dimensionality (28× 28), but significantly different characteristics.",5. Experimental Evaluation,[0],[0]
The MNIST dataset has fewer classes (10 digits from 0-9) as opposed to Omniglot which shows greater diversity (1623 characters across 50 alphabets).,5. Experimental Evaluation,[0],[0]
"Additional experiments with generative adversarial networks on the CelebA dataset are reported in the Appendix.
Baselines.",5. Experimental Evaluation,[0],[0]
"We considered methods based on sparse vector recovery using LASSO (Tibshirani, 1996; Candès & Tao,
2005) and generative model based recovery using variational autoencoders (VAE) (Kingma & Welling, 2014; Bora et al., 2017).",5. Experimental Evaluation,[0],[0]
"For VAE training, we used the standard train/held-out splits of both datasets.",5. Experimental Evaluation,[0],[0]
Compressed sensing experiments that we report were performed on the entire test set of images.,5. Experimental Evaluation,[0],[0]
"The architecture and other hyperparameter details are given in the Appendix.
",5. Experimental Evaluation,[0],[0]
Experimental setup.,5. Experimental Evaluation,[0],[0]
"For the held-out set of instances, we artificially generated measurements y through a random matrix A ∈ Rm×n with entries sampled i.i.d.",5. Experimental Evaluation,[0],[0]
from a Gaussian with zero mean and standard deviation of 1/m. Measurement noise is sampled from zero mean and diagonal scalar covariance matrix with entries as 0.01.,5. Experimental Evaluation,[0],[0]
"For evaluation, we report the reconstruction error measured as ‖x̂− x‖p where x̂ is the recovered signal and p is a norm of interest, varying the number of measurementsm from 50 to the highest value of 750.",5. Experimental Evaluation,[0],[0]
"We report results for the p = {1, 2,∞} norms.
",5. Experimental Evaluation,[0],[0]
"We evaluated sensing of both continuous signals (MNIST) with pixel values in range [0, 1] and discrete signals (Omniglot) with binary pixel values {0, 1}.",5. Experimental Evaluation,[0],[0]
"For all algorithms considered, recovery was performed by optimizing over a continuous space.",5. Experimental Evaluation,[0],[0]
"In the case of sparse recovery methods (including Sparse-Gen) it is possible that unconstrained optimization returns signals outside the domain of interest, in which case they are projected to the required domain by simple clipping, i.e., any signal less than zero is clipped to 0 and similarly any signal greater than one is clipped to 1.
Results and Discussion.",5. Experimental Evaluation,[0],[0]
The reconstruction errors for varying number of measurements are given in Figure 2.,5. Experimental Evaluation,[0],[0]
"Consistent with the theory, the strong prior in generative modelbased recovery methods outperforms the LASSO-based methods for sparse vector recovery.",5. Experimental Evaluation,[0],[0]
"In the regime of low measurements, the performance of algorithms that can incorporate the generative model prior dominates over methods modeling sparsity using LASSO.",5. Experimental Evaluation,[0],[0]
"The performance of plain generative model-based methods however saturates with increasing measurements, unlike Sparse-Gen and LASSO which continue to shrink the error.",5. Experimental Evaluation,[0],[0]
"The trends are consistent for both MNIST and Omniglot, although we observe the relative magnitudes of errors in the case of Omniglot are much higher than that of MNIST.",5. Experimental Evaluation,[0],[0]
This is expected due to the increased diversity and variations of the structure of the signals being sensed in the case of Omniglot.,5. Experimental Evaluation,[0],[0]
We also observe the trends to be consistent across the various norms considered.,5. Experimental Evaluation,[0],[0]
One of the primary motivations for compressive sensing is to directly acquire the signals using few measurements.,5.1. Transfer compressed sensing,[0],[0]
"On the contrary, learning a deep generative model requires access to large amounts of training data.",5.1. Transfer compressed sensing,[0],[0]
"In several applications, getting the data for training a generative model might not be feasible.",5.1. Transfer compressed sensing,[0],[0]
"Hence, we test the generative model-based recovery on the novel task of transfer compressed sensing.
",5.1. Transfer compressed sensing,[0],[0]
Experimental setup.,5.1. Transfer compressed sensing,[0],[0]
We train the generative model on a source domain (assumed to be data-rich) and related to a data-hungry target domain we wish to sense.,5.1. Transfer compressed sensing,[0],[0]
"Given the matching dimensions of MNIST and Omniglot, we conduct experiments transferring from MNIST (source) to Omniglot (target) and vice versa.
Results and Discussion.",5.1. Transfer compressed sensing,[0],[0]
The reconstruction errors for the norms considered are given in Figure 3.,5.1. Transfer compressed sensing,[0],[0]
"For both the sourcetarget pairs, we observe that the Sparse-Gen consistently performs well.",5.1. Transfer compressed sensing,[0],[0]
Vanilla generative model-based recovery shows hardly an improvements with increasing measurements.,5.1. Transfer compressed sensing,[0],[0]
We can qualitatively see this phenomena for transferring from MNIST (source) to Omniglot (target) in Figure 4.,5.1. Transfer compressed sensing,[0],[0]
"With only m = 100 measurements, all models perform poorly and generative model based methods particularly continue to sense images similar to MNIST.",5.1. Transfer compressed sensing,[0],[0]
"On the other hand, there is a noticeable transition at m = 200 measurements for SparseVAE where it adapts better to the domain being sensed than plain generative model-based recovery and achieves lower reconstruction error.",5.1. Transfer compressed sensing,[0],[0]
"Since the introduction of compressed sensing over a decade ago, there has been a vast body of research studying various extensions and applications (Candès & Tao, 2005; Donoho,
2006; Candès et al., 2006).",6. Related Work,[0],[0]
"This work explores the effect of modeling different structural assumptions on signals in theory and practice.
",6. Related Work,[0],[0]
Themes around sparsity in a well-chosen basis has driven much of the research in this direction.,6. Related Work,[0],[0]
"For instance, the paradigm of model-based compressed sensing accounts for the interdependencies between the dimensions of a sparse data signal (Baraniuk et al., 2010; Duarte & Eldar, 2011; Gilbert et al., 2017).",6. Related Work,[0],[0]
"Alternatively, adaptive selection of basis vectors from a dictionary that best capture the structure of the particular signal being sensed has also been explored (Peyre, 2010; Tang et al., 2013).",6. Related Work,[0],[0]
"Many of these methods have been extended to recovery of structured tensors (Zhang et al., 2013; 2014).",6. Related Work,[0],[0]
"In another prominent line of research involving Bayesian compressed sensing, the sparseness assumption is formalized by placing sparsenesspromoting priors on the signals (Ji et al., 2008; He & Carin, 2009; Babacan et al., 2010; Baron et al., 2010).
",6. Related Work,[0],[0]
Research exploring structure beyond sparsity is relatively scarce.,6. Related Work,[0],[0]
Early works in this direction can be traced to Baraniuk & Wakin (2009) who proposed algorithms for recovering signals lying on a smooth manifold.,6. Related Work,[0],[0]
The generative model-based recovery methods consider functions that do not necessarily define manifolds since the range of a generator function could intersect with itself.,6. Related Work,[0],[0]
"Yu & Sapiro (2011) coined the term statistical compressed sensing and proposed
algorithms for efficient sensing of signals from a mixture of Gaussians.",6. Related Work,[0],[0]
The recent work in deep generative model-based recovery differs in key theoretical aspects as well in the use of a more expressive family of models based on neural networks.,6. Related Work,[0],[0]
A related recent work by Hand & Voroninski (2017) provides theoretical guarantees on the solution recovered for solving non-convex linear inverse problems with deep generative priors.,6. Related Work,[0],[0]
"Empirical advances based on well-designed deep neural network architectures that sacrifice many of the theoretical guarantees have been proposed for applications such as MRI (Mardani et al., 2017; 2018).",6. Related Work,[0],[0]
"Many recent methods propose to learn mappings of signals to measurements using neural networks, instead of restricting them to be linear, random matrices (Mousavi et al., 2015; Kulkarni et al., 2016; Chang et al., 2017; Lu et al., 2018).
",6. Related Work,[0],[0]
"Our proposed framework bridges the gap between algorithms that model structure using sparsity and enjoy good theoretical properties with advances in deep generative models, in particular their use for compressed sensing.",6. Related Work,[0],[0]
"The use of deep generative models as priors for compressed sensing presents a new outlook on algorithms for inexpen-
sive data acquisition.",7. Conclusion and Future Work,[0],[0]
"In this work, we showed that these priors can be used in conjunction with classical modeling assumptions based on sparsity.",7. Conclusion and Future Work,[0],[0]
"Our proposed framework, Sparse-Gen, generalizes both sparse vector recovery and recovery using generative models by allowing for sparse deviations from the range of a generative model function.",7. Conclusion and Future Work,[0],[0]
"The benefits of using such modeling assumptions are observed both theoretically and empirically.
",7. Conclusion and Future Work,[0],[0]
"In the future, we would like to design algorithms that can better model the structure within sparse deviations.",7. Conclusion and Future Work,[0],[0]
"Followup work in this direction can benefit from the vast body of prior work in structured sparse vector recovery (Duarte & Eldar, 2011).",7. Conclusion and Future Work,[0],[0]
"From a theoretical perspective, a better understanding of the non-convexity resulting from generative model-based recovery can lead to stronger guarantees and consequently better optimization algorithms for recovery.",7. Conclusion and Future Work,[0],[0]
"Finally, it would be interesting to extend Sparse-Gen for compressed sensing of other data modalities such as graphs for applications in network tomography and reconstruction (Xu et al., 2011).",7. Conclusion and Future Work,[0],[0]
"Real-world graph networks are typically sparse in the canonical basis and can be modeled effectively using deep generative models (Grover et al., 2018), which is consistent with the modeling assumptions of the Sparse-Gen framework.",7. Conclusion and Future Work,[0],[0]
"We are thankful to Tri Dao, Jonathan Kuck, Daniel Levy, Aditi Raghunathan, and Yang Song for helpful comments on early drafts.",Acknowledgements,[0],[0]
"This research was supported by Intel Corporation, TRI, a Hellman Faculty Fellowship, ONR, NSF (#1651565, #1522054, #1733686 ) and FLI (#2017-158687).",Acknowledgements,[0],[0]
AG is supported by a Microsoft Research PhD Fellowship.,Acknowledgements,[0],[0]
"In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal.",abstractText,[0],[0]
"Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model.",abstractText,[0],[0]
A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements.,abstractText,[0],[0]
"However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined.",abstractText,[0],[0]
"We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals.",abstractText,[0],[0]
"Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior.",abstractText,[0],[0]
"Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.",abstractText,[0],[0]
Modeling Sparse Deviations for Compressed Sensing using Generative Models,title,[0],[0]
"Many languages exhibit fluency phenomena that are discontinuous in the surface string, and are thus not modelled well by traditional n-gram language models.",1 Introduction,[0],[0]
"Examples include morphological agreement, e.g. subject-verb agreement in languages that do not (exclusively) follow SVO word order, subcategorisation, and collocations involving distant, but syntactically linked words.
",1 Introduction,[0],[0]
Syntactic language models try to overcome the limitation to a local n-gram context by using syntactically related words (and non-terminals) as context information.,1 Introduction,[0],[0]
"Despite their theoretical attractiveness, it has proven difficult to improve SMT with parsers as language models (Och et al., 2004; Post and Gildea, 2008).
",1 Introduction,[0],[0]
"This paper describes an effective method to model, train, decode with, and weight a syntactic language model for SMT.",1 Introduction,[0],[0]
"While all these aspects are important for successfully applying a syntactic language model, our primary contributions are a novel dependency language model which improves over prior work by making relational modelling assumptions, which we argue are better suited for languages with a (relatively) free word order, and the use of a syntactic evaluation metric for optimizing the loglinear parameters of the SMT model.
",1 Introduction,[0],[0]
"While language models that operate on words linked through a dependency chain – called syntactic n-grams (Sidorov et al., 2013) – can improve translation, some of the improvement is invisible to an n-gram metric such as BLEU.",1 Introduction,[0],[0]
"As a result, tuning to BLEU does not show the full value of a syntactic language model.",1 Introduction,[0],[0]
"What does show its value is an optimization metric that operates on the same syntactic n-grams that are modelled by the dependency LM.
",1 Introduction,[0],[0]
The paper is structured as follows.,1 Introduction,[0],[0]
"Section 2 describes our relational dependency language model; section 3 describes our neural network training procedure, and the integration of the model into an SMT decoder.",1 Introduction,[0],[0]
We describe the syntactic evaluation metric we use for tuning in Section 4.,1 Introduction,[0],[0]
"The language models are evaluated on the basis of perplexity and SMT
169
Transactions of the Association for Computational Linguistics, vol. 3, pp.",1 Introduction,[0],[0]
"169–182, 2015.",1 Introduction,[0],[0]
Action Editor: Philipp Koehn.,1 Introduction,[0],[0]
"Submission batch: 11/2014; Revision batch 2/2015; Published 3/2015.
",1 Introduction,[0],[0]
c©2015 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY-NC-SA 4.0 license.
performance in section 5.",1 Introduction,[0],[0]
"We discuss related work in section 6, and finish with concluding remarks in section 7.",1 Introduction,[0],[0]
"As motivation, and working example for the model description, consider the dependency tree in Figure 1, which is taken from the output of our baseline string-to-tree SMT system.1 The output contains two errors:
• a morphological agreement error between the subject Ergebnisse (plural) and the finite verb wird (singular).
• a subcategorisation error: überraschen is transitive, but the translation has a prepositional phrase instead of an object.
",2 A Relational Dependency Language Model,[0],[0]
"While these errors might not have occurred if the words involved were adjacent to one another here and throughout the training set, non-adjacency is common, especially where the distance between subject and finite verb, or between a full verb and its arguments can be arbitrarily long.
",2 A Relational Dependency Language Model,[0],[0]
"Prior work on syntactic language modelling has typically focused on English, and we argue that some modelling decisions do not transfer well to other languages.",2 A Relational Dependency Language Model,[0],[0]
The dependency models proposed by Shen et al. (2010) and Zhang (2009) rely heavily on structural information such as the direction and distance of the dependent from the parent.,2 A Relational Dependency Language Model,[0],[0]
"In a language where the order of syntactic dependents is more flexible than in English, such as German2, grammatical function (and thus the inflection) is hard to predict from the dependent order.",2 A Relational Dependency Language Model,[0],[0]
"Instead, we make dependency labels, which encode grammatical relations, a core element of our model.3
1The tree is converted into constituency format for compatibility with SCFG decoding algorithms, with dependency edges represented as non-terminal nodes.
",2 A Relational Dependency Language Model,[0],[0]
"2German has a strict word order within noun phrases and for the placement of verbs, but has different word order for main clauses and subordinated clauses, and some flexibility in the order of dependents of a verb.
",2 A Relational Dependency Language Model,[0],[0]
"3Tsarfaty (2010) classifies parsing approaches into configurational approaches that rely on structural information, and relational ones that take grammatical relations as primitives.",2 A Relational Dependency Language Model,[0],[0]
"While she uses dependency syntax as a prototypical example of
Shen et al. (2010) propose a model that estimates probability of each token given its parent and/or preceding siblings.",2 A Relational Dependency Language Model,[0],[0]
"We start with a variant of their model that does not hard-code configurational modelling assumptions, and then extend it by including dependency labels.",2 A Relational Dependency Language Model,[0],[0]
"Let S be a sequence of terminal symbols w1, w2, ..., wn with a dependency topology T , and let hs(i) and ha(i) be lists of heads of preceding siblings and ancestors of wi according to T , from closest to furthest.",2.1 Unlabelled Model,[0],[0]
"In our example in Figure 1:
• w4 = jüngsten
• hs(4) = (der)
• ha(4) =",2.1 Unlabelled Model,[0],[0]
"(Umfrage,Ergebnisse,wird, )",2.1 Unlabelled Model,[0],[0]
Note that ha and its subsequences are instances of syntactic,2.1 Unlabelled Model,[0],[0]
n-grams.,2.1 Unlabelled Model,[0],[0]
"For this model, we follow related work and assume that T is available (Popel and Marecek, 2010), approximating P (S) as P (S|T ).",2.1 Unlabelled Model,[0],[0]
"We make the Markov assumption that the probability of each word only depends on its preceding siblings4 and ancestors, and decompose the probability of a sentence like this:
P (S) = P (w1, w2, ..., wn)
≈ n∏
i=1
P (wi|hs(i), ha(i))",2.1 Unlabelled Model,[0],[0]
"(1)
We further make the Markov assumption that only a fixed window of the closest q siblings, and the closest r ancestors, affect the probability of a word.
",2.1 Unlabelled Model,[0],[0]
"P (S) ≈ n∏
i=1
P (wi|hs(i)q1, ha(i)r1) (2)
",2.1 Unlabelled Model,[0],[0]
"Equation 2 represents our basic, unlabelled model.",2.1 Unlabelled Model,[0],[0]
"It differs from that of Shen et al. (2010) in two ways.
relational approaches, the dependency LM by Shen et al. (2010) would fall into the configurational category, while ours is relational.
",2.1 Unlabelled Model,[0],[0]
"4Shen et al. (2010) use the siblings that are between the word and its parent, i.e. the following siblings if the word comes before its parent.",2.1 Unlabelled Model,[0],[0]
"We believe both preceding and following siblings are potentially useful, but leave expansion of the context to future work.
",2.1 Unlabelled Model,[0],[0]
"die Ergebnisse der jüngsten Umfrage wird für viele überraschen .
",2.1 Unlabelled Model,[0],[0]
"root root
det
subj
det
attr
gmod
pp
pn
aux
sent
punct
$.
vroot
aux
VAFIN
subj
First, it uses separate context windows for siblings and ancestors.",2.1 Unlabelled Model,[0],[0]
"In contrast, Shen et al. (2010) treat the ancestor as the first symbol in a context window that is shared between the ancestor and siblings.",2.1 Unlabelled Model,[0],[0]
"Our formulation encodes our belief that the model should always assume dependence on the r nearest ancestor nodes, regardless of the number of siblings.",2.1 Unlabelled Model,[0],[0]
"Secondly, Shen et al. (2010) separate dependents to the left and to the right of the parent.",2.1 Unlabelled Model,[0],[0]
"While the fixed SVO verb order in English is compatible with such a separation, allowing PL to model subjects, PR to model objects, most arguments can occur before or after the head verb in German main clauses.",2.1 Unlabelled Model,[0],[0]
We thus argue that left and right dependents should be modelled by a single model to allow for sharing of statistical strength.5,2.1 Unlabelled Model,[0],[0]
The motivation for the inclusion of dependency labels is twofold.,2.2 Labelled Model,[0],[0]
"Firstly, having dependency labels in the context serves as a strong signal for the prediction of the correct inflectional form.",2.2 Labelled Model,[0],[0]
"Secondly, dependency labels are the appropriate level of ab-
5Similar arguments have been made for parsing of (relatively) free word-order languages, e.g. by Tsarfaty et al. (2009).
straction to model subcategorisation frames.",2.2 Labelled Model,[0],[0]
"Let D be a sequence of dependency labels l1, l2, ..., ln, with each label li being the label of the incoming arc at position i in T , and ls(i) and la(i) the list of dependency labels of the siblings and ancestors of wi, respectively.",2.2 Labelled Model,[0],[0]
"Continuing the example for w4, these are:
• l4 = attr
• ls(4) =",2.2 Labelled Model,[0],[0]
"(det)
• la(4) =",2.2 Labelled Model,[0],[0]
"(gmod, subj, vroot, sent)
",2.2 Labelled Model,[0],[0]
"We predict both the terminal symbols S and dependency labels D. The latter lets us model subcategorisation by penalizing unlikely relations, e.g. objects whose parent is an intransitive verb.",2.2 Labelled Model,[0],[0]
"We decompose P (S,D) into P (D)× P (S|D) to obtain:
P (S,D) = P (D)× P (S|D)
",2.2 Labelled Model,[0],[0]
"≈ n∏
i=1
",2.2 Labelled Model,[0],[0]
"Pl(i)× Pw(i)
Pl(i) =",2.2 Labelled Model,[0],[0]
"P (li|hs(i)q1, ls(i)q1, ha(i)r1, la(i)r1) Pw(i) =P (wi|hs(i)q1, ls(i)q1, ha(i)r1, la(i)r1, li)
(3)",2.2 Labelled Model,[0],[0]
We here discuss some details for the extraction of the context hs and ha.,2.3 Head and Label Extraction,[0],[0]
"Dependency structures require no language-specific head extraction rules, even in a converted constituency representation.",2.3 Head and Label Extraction,[0],[0]
"In the constituency representation shown in Figure 1, each non-terminal node in the tree that is not a preterminal has exactly one pre-terminal child.",2.3 Head and Label Extraction,[0],[0]
"The head of a non-terminal node can thus be extracted by identifying the pre-terminal child, and taking its terminal symbol as head.",2.3 Head and Label Extraction,[0],[0]
"An exception is the virtual node sent, which is added to the root of the tree to combine subtrees that are not connected in the original grammar, e.g. the main tree and the punctuation symbol.",2.3 Head and Label Extraction,[0],[0]
"If a node has no pre-terminal child, we use a special token as its head.
",2.3 Head and Label Extraction,[0],[0]
"If the sibling of a node is a pre-terminal node, we represent this through a special token in hs and ls.",2.3 Head and Label Extraction,[0],[0]
"We also use special out-of-bound tokens (separate for hs, ha, ls and la) to fill up the context window if the window is larger than the number of siblings and/or ancestors.
",2.3 Head and Label Extraction,[0],[0]
The context extraction rules are languageindependent and can be applied to any dependency structure.,2.3 Head and Label Extraction,[0],[0]
Language-specific or grammar-specific rules are possible in principle.,2.3 Head and Label Extraction,[0],[0]
"For instance, for verbal heads in German, one could consider separable verb prefixes part of the head, and thus model differences in subcategorisation between schlagen (Engl. beat) and schlagen ...",2.3 Head and Label Extraction,[0],[0]
vor (Engl. suggest).,2.3 Head and Label Extraction,[0],[0]
"The model in equation 3 still assumes the topology of the dependency tree to be given, and we remedy this by also predicting pre-terminal nodes, and a virtual STOP node as the last child of each node.",2.4 Predicting the Tree Topology,[0],[0]
"This models the position of the head in a subtree (through the prediction of pre-terminal nodes), and the probability that a word has no more dependents (by assigning probability mass to the STOP node).
",2.4 Predicting the Tree Topology,[0],[0]
"Instead of generating all n terminal symbols as in equation 3, we generate all m nodes in the dependency tree in top-down, depth-first order, with li being PT for pre-terminals, and the node label otherwise, and wi being either the head of the node, or if the node has no pre-terminal child.",2.4 Predicting the Tree Topology,[0],[0]
"Our final model is given in equation 4.
",2.4 Predicting the Tree Topology,[0],[0]
"P (S,D, T )",2.4 Predicting the Tree Topology,[0],[0]
"≈ m∏
i=1
{",2.4 Predicting the Tree Topology,[0],[0]
"Pl(i)× Pw(i), if wi 6= Pl(i), otherwise
(4) Figure 2 illustrates the prediction of a subtree of the dependency tree in Figure 1.",2.4 Predicting the Tree Topology,[0],[0]
"Note that T is encoded implicitly, and can be retrieved from D through a stack to which all nodes (except for preterminal and STOP nodes) are pushed after prediction, and from which the last node is popped when predicting a STOP node.",2.4 Predicting the Tree Topology,[0],[0]
"We extract all training instances from automatically parsed training text, and perform training with a standard feed-forward neural network (Bengio et al., 2003), using the NPLM toolkit (Vaswani et al., 2013).",3 Neural Network Training and SMT Decoding,[0],[0]
"Back-off smoothing schemes are unsatisfactory because it is unclear which part of the context should be forgotten first, and neural networks elegantly solve this problem.",3 Neural Network Training and SMT Decoding,[0],[0]
"We use two separate networks, one for Pw and one for Pl.",3 Neural Network Training and SMT Decoding,[0],[0]
"Both networks share the same input vocabulary, but are trained and applied independently.",3 Neural Network Training and SMT Decoding,[0],[0]
"The model input is a (2q+2r)-word context vector (+1 for Pw to encode li), each word being mapped to a shared embedding layer.",3 Neural Network Training and SMT Decoding,[0],[0]
"We use a single hidden layer with rectifiedlinear activation function, and noise-contrastive estimation (NCE).
",3 Neural Network Training and SMT Decoding,[0],[0]
We integrate our dependency language models into a string-to-tree SMT system as additional feature functions that score each translation hypothesis.,3 Neural Network Training and SMT Decoding,[0],[0]
"The model in equation 4 predicts P (S,D, T ).
",3 Neural Network Training and SMT Decoding,[0],[0]
"Obtaining the probability of the translation hypothesis P (S) would require the (costly) marginalization over all sequences of dependency labels D and topologies T , but like the SMT decoder itself, we approximate the search for the best translation by searching for the highest-scoring derivation, meaning that we directly integrate Pw and Pl as two features into the log-linear SMT model.",3 Neural Network Training and SMT Decoding,[0.9526435233402725],"['When we start the experiments with top 1k vocabulary (1/30 of the baseline settings), the translation quality of both WPE-V and WPED-V are already higher than the baseNMT; while their decoding time is less than 1/3 of an NMT system with 30k vocabulary.']"
"We use selfnormalized neural networks with precomputation of the hidden layer, which makes the integration into decoding reasonably fast.
",3 Neural Network Training and SMT Decoding,[0],[0]
"The decoder builds the translation bottom-up, and the full context is not available for all symbols in the hypothesis.",3 Neural Network Training and SMT Decoding,[0],[0]
"Vaswani et al. (2013) propose to use a special null word for unavailable context, their embedding being the weighted average of the input embeddings of all other words.",3 Neural Network Training and SMT Decoding,[0],[0]
"We adopt this strategy, with the difference that we use separate null words for each position in the context window in order to reflect distributional differences between the different positions, e.g. between ancestor labels and sibling labels.",3 Neural Network Training and SMT Decoding,[0],[0]
"Symbols are re-scored as more context becomes available in decoding, but poor approximations could affect pruning and thus lead to search errors.",3 Neural Network Training and SMT Decoding,[0],[0]
"In Table 1, we illustrate the use of null words with a 5-gram and a bigram NNLM model.",3 Neural Network Training and SMT Decoding,[0],[0]
"We observe a small increase in entropy when querying the 5-gram model with bigrams, compared to querying a bigram model directly.
",3 Neural Network Training and SMT Decoding,[0],[0]
Some hierarchical SMT systems allow glue rules which concatenate two subtrees.,3 Neural Network Training and SMT Decoding,[0],[0]
"Since the resulting glue structures do not occur in the training data, we do not estimate their probability in our model.",3 Neural Network Training and SMT Decoding,[0],[0]
"When encountering the root of a glue rule in our language model, we recursively evaluate its children, but ignore the glue node itself.",3 Neural Network Training and SMT Decoding,[0],[0]
This could introduce a bias towards using more glue rules during translation.,3 Neural Network Training and SMT Decoding,[0],[0]
"To counter this, and encourage the production of linguistically plausible trees, we assign a fixed, high cost to glue rules.",3 Neural Network Training and SMT Decoding,[0],[0]
"Glue rules thus play a small
role in our systems, with about 100 glue rule applications per 3000 sentences, and could be abandoned entirely.6",3 Neural Network Training and SMT Decoding,[0],[0]
"N-gram based metrics such as BLEU (Papineni et al., 2002) are still predominantly used to optimize the log-linear parameters of SMT systems, and (to a lesser extent) to evaluate the final translation systems.",4 Optimizing Syntactic N-grams,[0],[0]
"However, n-gram metrics are not well suited to measure fluency phenomena with string-level gaps, and there is a danger that BLEU underestimates the modelling power of dependency language models, resulting in a suboptimal assignment of loglinear weights.",4 Optimizing Syntactic N-grams,[0],[0]
"As an alternative metric that operates on the level of syntactic n-grams, we use a variant of the head-word chain metric (HWCM) (Liu and Gildea, 2005).
",4 Optimizing Syntactic N-grams,[0],[0]
"HWCM is a precision metric similar to BLEU, but instead of counting n-gram matches between the translation output and the reference, it compares head-word chains, or syntactic n-grams.",4 Optimizing Syntactic N-grams,[0],[0]
"HWCM is not only suitable for our task because it operates on the same structures as the dependency language models, but also because our string-to-tree SMT architecture produces trees that can be evaluated directly, without requiring a separate parse of the translation output, a task for which few parsers are optimized.",4 Optimizing Syntactic N-grams,[0],[0]
"For extracting syntactic n-grams from the reference translations of the respective development and test sets, we automatically parse them, using the same preprocessing as for training.
",4 Optimizing Syntactic N-grams,[0],[0]
"We count syntactic n-grams of sizes 1 to 4, mirroring the typical usage of BLEU.",4 Optimizing Syntactic N-grams,[0],[0]
"Banerjee and Lavie (2005) have demonstrated the importance of recall in MT evaluation, and we compute the harmonic mean of precision and recall, which we denote HWCMf , instead of the original, precision-based metric.",4 Optimizing Syntactic N-grams,[0],[0]
We perform three evaluations of our dependency language models.,5 Evaluation,[0],[0]
"Our perplexity evaluation measures model perplexity on the 1-best output of a
6For efficiency reasons, our experimental systems only perform SCFG parsing for spans of up to 50 words, and use glue rules to concatenate partial derivations in longer sentences.",5 Evaluation,[0],[0]
"Better decoding algorithms have reduced the need for this limit (Sennrich, 2014).
baseline SMT system and a human reference translation.",5 Evaluation,[0],[0]
Our SMT evaluation integrates the model as a feature function in a string-to-tree SMT system and evaluates its impact on translation quality.,5 Evaluation,[0],[0]
"Finally, we quantify the effect of different language models on grammaticality by measuring the number of agreement errors of our SMT systems.
",5 Evaluation,[0],[0]
"We refer to the unlabelled variant of our model (equation 2) as DLM, and to the labelled variant (equation 4) as RDLM, emphasizing that the latter is a relational dependency LM.",5 Evaluation,[0],[0]
"We perform our experiments on English→German data from the WMT 2014 shared translation task (Bojar et al., 2014), consisting of about 4.5 million sentence pairs of parallel data and 120 million sentences of monolingual German data.",5.1 Data and Methods,[0],[0]
We train all language models on the German side of the parallel text and the monolingual data.,5.1 Data and Methods,[0],[0]
"We also perform some experiments on the English→Russian data from the same translation task, with 2 million sentence pairs of parallel data and 34 million sentences of monolingual Russian data.
",5.1 Data and Methods,[0],[0]
"For a 5-gram Neural Network LM baseline (NNLM), and the dependency language models, we train feed-forward Neural Network language models with the NPLM toolkit.",5.1 Data and Methods,[0],[0]
"We use 150 dimensions for the input embeddings, and a single hidden layer with 750 dimensions.",5.1 Data and Methods,[0],[0]
"We use a vocabulary of 500 000 words (70 for the output vocabulary of Pl), from which we draw 100 noise samples for NCE (50 for Pl).",5.1 Data and Methods,[0],[0]
"We train for two epochs, each epoch being a full traversal of the training text.",5.1 Data and Methods,[0],[0]
"For unknown words, we back-off to a special unk token for the sequence models and Pl, and to the pre-terminal symbol for the other dependency models.",5.1 Data and Methods,[0],[0]
"We report perplexity values with softmax normalization, but disable normalization during decoding, relying on the selfnormalization of NCE for efficiency.",5.1 Data and Methods,[0],[0]
"For the translation experiments with DLM and RDLM, we set the sibling window size q to 1, and the ancestor window size r to 2.7
We train baseline language models with interpolated modified Kneser-Ney smoothing with SRILM
7On our test set, a node has an average of 4.6 ancestors (σ = 2.5), and 1.2 left siblings (σ = 1.3).
",5.1 Data and Methods,[0],[0]
"(Stolcke, 2002).",5.1 Data and Methods,[0],[0]
The model in the SMT baseline uses the full vocabulary and a linear interpolation of component models for domain adaptation.,5.1 Data and Methods,[0],[0]
"For the perplexity evaluation, we use the same vocabulary and training data as for the Neural Network models.
",5.1 Data and Methods,[0],[0]
"For the English→German SMT evaluation, our baseline system is a string-to-tree SMT system with Moses (Koehn et al., 2007), with dependency parsing of the German texts (Sennrich et al., 2013).",5.1 Data and Methods,[0],[0]
"It is described in more detail in (Williams et al., 2014).",5.1 Data and Methods,[0],[0]
This setup was ranked 1–2 (out of 18) in the WMT 2014 shared translation task and is stateof-the art.,5.1 Data and Methods,[0],[0]
"Our biggest deviation from this setup is that we do not enforce the morphological agreement constraints that are provided by a unification grammar (Williams and Koehn, 2011), but use them for analysis instead.",5.1 Data and Methods,[0],[0]
"For English→Russian, we copy the language-independent settings from the the English→German set-up, and perform dependency parsing with a Russian model for the Maltparser (Nivre et al., 2006; Sharoff and Nivre, 2011), applying projectivization after parsing.
",5.1 Data and Methods,[0],[0]
"We tune our system on a development set of 2000 sentences with k-best batch MIRA (Cherry and Foster, 2012) on BLEU and a linear interpolation of BLEU and HWCMf , and report both scores for evaluation.",5.1 Data and Methods,[0],[0]
"We also report METEOR (Denkowski and Lavie, 2011) for German and TER (Snover et al., 2006).",5.1 Data and Methods,[0],[0]
"We control for optimizer instability by running the optimization three times per system and performing significance testing with Multeval (Clark et al., 2011), which we enhanced to also perform significance testing for HWCMf .
",5.1 Data and Methods,[0],[0]
"5.2 Implementation notes on model by Shen et al. (2010)
",5.1 Data and Methods,[0],[0]
We reimplement the model by Shen et al. (2010) for our evaluation.,5.1 Data and Methods,[0],[0]
"The authors did not specify training and smoothing of their model, so we only adopt their definition of the context window, and use the same neural network architecture as for our other models.",5.1 Data and Methods,[0],[0]
"Specifically, we use two neural networks: one for left dependents, and one for right dependents.",5.1 Data and Methods,[0],[0]
"We use maximum-likelihood estimation for the head of root nodes, ignoring unseen events.",5.1 Data and Methods,[0],[0]
"To distinguish between parents and siblings in the context window, we double the input vocabulary and mark parents with a suffix.",5.1 Data and Methods,[0],[0]
"Like Shen et al. (2010), we ignore the
prediction of STOP labels, meaning that our implementation assumes the dependency topology to be given.",5.1 Data and Methods,[0],[0]
We use a trigram model like the original authors.,5.1 Data and Methods,[0],[0]
"Peter et al. (2012) experiment with higher orders variants, but do not consider grandparent nodes.",5.1 Data and Methods,[0],[0]
"We consider scalability to a larger ancestor context a real concern, since another duplication of the vocabulary may be necessary for each ancestor level.",5.1 Data and Methods,[0],[0]
There are a number of factors that make a direct comparison of the reference set perplexity unfair.,5.3 Perplexity,[0],[0]
"Mainly, the unlabelled dependency model DLM and the one by Shen et al. (2010) assume that the dependency topology is given; Pw even assumes this for the dependency labels D. Conversely, the full RDLM predicts the terminal sequence, the dependency labels, and the dependency topology, and we thus expect it to have a higher perplexity.8 Also note that we compare 5-gram n-gram models to 3- and 4- gram dependency models.",5.3 Perplexity,[0],[0]
"A more minor difference is that n-gram models also predict end-of-sentence tokens, which the dependency models do not.
",5.3 Perplexity,[0],[0]
"Rather than directly comparing perplexity between different models, our focus lies on a perplexity comparison between a human reference translation and the 1-best SMT output of a baseline transla-
8For better comparability, we measure perplexity per surface word, not per prediction.
tion system.",5.3 Perplexity,[0],[0]
"Our basic assumption is that the difference in perplexity (or cross-entropy) tells us whether a model contains information that is not already part of the baseline model, and if incorporating it into our SMT system can nudge the system towards producing a translation that is more similar to the reference.
",5.3 Perplexity,[0],[0]
Results for English→German are shown in table 2.,5.3 Perplexity,[0],[0]
"The baseline 5-gram language model with Kneser-Ney smoothing prefers the SMT output over the reference translation, which is natural given that this language model is part of the system producing the SMT output.",5.3 Perplexity,[0],[0]
"The 5-gram NNLM improves over the Kneser-Ney models, and happens to assign almost the same perplexity score to both texts.",5.3 Perplexity,[0],[0]
"This still means that it is less biased towards the SMT output than the baseline model, and can be a valuable addition to the model.
",5.3 Perplexity,[0],[0]
"The dependency language models all show a preference for the reference translation, with DLM having a stronger preference than the model by Shen et al. (2010), and RDLM having the strongest preference.",5.3 Perplexity,[0],[0]
"The direct comparison of DLM and Pw, which is the component of RDLM that predicts the terminal symbols, shows that dependency labels serve as a strong signal for predicting the terminals, confirming our initial hypothesis.",5.3 Perplexity,[0],[0]
The prediction of the dependency topology and labels through Pl means that the full RDLM has the highest perplexity of all models.,5.3 Perplexity,[0],[0]
"However, it also strongly prefers the human reference text over the baseline SMT output.",5.3 Perplexity,[0],[0]
Translation results for English→German with different language models added to our baseline are shown in Table 3.,5.4 Translation Quality,[0],[0]
"Considering the systems tuned on BLEU, we observe that the 5-gram NNLM and RDLM are best in terms of BLEU and TER, but that RDLM is the only winner9 according to HWCMf and METEOR.",5.4 Translation Quality,[0],[0]
"In particular, we observe a sizable gap of 0.6 HWCMf points between the NNLM and the RDLM systems, despite similar BLEU scores.",5.4 Translation Quality,[0],[0]
"The unlabelled DLM and the dependency LM by Shen et al. (2010), which are generally weaker than RDLM, also tend to improve HWCMf more than BLEU.",5.4 Translation Quality,[0],[0]
"This reflects the fact that the dependency
9We denote a system a winner if no other system [in the group of systems under consideration] is significantly better according to significance testing with Multeval.
LMs improve fluency along the syntactic n-grams that HWCM measures, whereas NNLM only improves local fluency, to which BLEU is most sensitive.",5.4 Translation Quality,[0],[0]
"The fact that the models cover different phenomena is also reflected in the fact that we see further gains from combining the 5-gram NNLM with the strongest dependency LM, RDLM, for a total improvement of 0.9–1.1 BLEU over the baseline.
",5.4 Translation Quality,[0],[0]
"If we use BLEU+HWCMf as our tuning objective, the difference between the models increases.",5.4 Translation Quality,[0],[0]
"Compared to the 5-gram NNLM, the RDLM system gains 0.8–0.9 points in HWCMf and 0.3–0.5 points in BLEU.",5.4 Translation Quality,[0],[0]
"Compared to the original baseline, tuned only on BLEU, the system with RDLM that is tuned on BLEU+HWCMf yields an improvement of 1.1– 1.3 BLEU and 1.3–1.4 HWCMf .
",5.4 Translation Quality,[0],[0]
"If we compare the same system being trained on both tuning objectives, we observe that tuning on BLEU+HWCMf , unsurprisingly, yields higher HWCMf scores than tuning on BLEU only.",5.4 Translation Quality,[0],[0]
What is more surprising is that adding HWCMf as a tuning objective also yields significantly higher BLEU on the test sets for 9 out of 10 data points.,5.4 Translation Quality,[0],[0]
The gap is larger for the two systems with RDLM (0.3–0.6 BLEU) than for the baseline or the NNLM system (0.1–0.2 BLEU).,5.4 Translation Quality,[0],[0]
"We hypothesize that the inclusion of HWCMf as a tuning metric reduces overfitting and encourages the production of more grammatically well-formed constructions, which we expect to be a robust objective across different texts, espe-
cially when coupled with a strong dependency language model such as RDLM.
",5.4 Translation Quality,[0],[0]
Some example translations are shown in table 4.,5.4 Translation Quality,[0],[0]
"They illustrate three error types in the baseline system:
1.",5.4 Translation Quality,[0],[0]
"an error in subject-verb agreement.
",5.4 Translation Quality,[0],[0]
2.,5.4 Translation Quality,[0],[0]
"a subcategorisation error: gelten is a valid translation of the intransitive meaning of apply, but cannot be used for transitive constructions, where anwenden is correct.
3.",5.4 Translation Quality,[0],[0]
"a collocation error: two separate collocations are conflated in the baseline translation:
• reach a decision on [...] eine Entscheidung über",5.4 Translation Quality,[0],[0]
"[...] treffen
• reach an agreement on [...] eine Einigung über",5.4 Translation Quality,[0],[0]
"[...] erzielen
All errors are due to inter-dependencies in the sentence that have string-level gaps, but which can be modelled through syntactic n-grams, and are corrected by the system with RDLM and tuning on BLEU+HWCMf .
",5.4 Translation Quality,[0],[0]
We evaluate a subset of the systems on an English→Russian task to test whether the improvements from adding RDLM and tuning on BLEU+HWCMf apply to other language pairs.,5.4 Translation Quality,[0],[0]
Results are shown in Table 5.,5.4 Translation Quality,[0],[0]
"The system with RDLM
is the consistent winner, and significantly outperforms the baseline for all metrics and test sets.",5.4 Translation Quality,[0],[0]
Tuning on BLEU+HWCMf results in further improvements in HWCMf and TER.,5.4 Translation Quality,[0],[0]
"Looking at the combined effect of adding RDLM and changing the tuning objective, we observe gains in BLEU by 0.5–0.9 points, and gains in HWCMf by 2.1–3.4 points.",5.4 Translation Quality,[0],[0]
"We argue that the dependency language models and HWCMf as a tuning metric improve grammaticality, and we are able to quantify one aspect thereof, morphological agreement, for English→German.",5.5 Morphological Agreement,[0],[0]
"Williams and Koehn (2011) introduce a unification grammar with hand-crafted agreement constraints to identify and suppress selected morphological agreement violations in German, namely in regards to noun phrase agreement, prepositional phrase agreement, and subject-verb agreement.",5.5 Morphological Agreement,[0],[0]
We can use their grammar to analyse the effect of different models on morphological agreement by counting the number of translations that violate at least one agreement constraint.,5.5 Morphological Agreement,[0],[0]
"We assume that the number of false posi-
tives (i.e. correct analyses that trigger an agreement violation) remains roughly constant throughout all systems, so that a reduction in the number of agreement violations is an indicator of better grammatical agreement.
",5.5 Morphological Agreement,[0],[0]
Table 6 shows the results.,5.5 Morphological Agreement,[0],[0]
"While the 5-gram NNLM reduces the number of agreement errors somewhat compared to the baseline (-18%), the reduction is greater for DLM (-34%) and RDLM (-46%).",5.5 Morphological Agreement,[0],[0]
"Neither the baseline nor the 5-gram NNLM
profits strongly from tuning on HWCMf , while the number of agreement errors is further reduced for the system with DLM (-41%) and RDLM (-54%).",5.5 Morphological Agreement,[0],[0]
"Adding the 5-gram NNLM to the RDLM system yields no further reduction on the number of agreement errors.
",5.5 Morphological Agreement,[0],[0]
"Enforcing the agreement constraints on the baseline system (tuned on BLEU+HWCMf ) provides us with a gain of 0.3 in both BLEU and HWCMf ; on the RDLM system, only 0.03.",5.5 Morphological Agreement,[0],[0]
"The fact that the benefit of enforcing the agreement constraints drops off more sharply than the number of constraint violations indicates that the remaining violations tend to be harder for the model to correct, e.g. because the translation model has not learned to produce the required inflection of a word, or because some of the remaining violations are false positives.",5.5 Morphological Agreement,[0],[0]
"While the dependency language models’ effect of improving morphological agreement is not (fully) cumulative with the benefit from enforcing the unification constraints formulated by Williams and Koehn (2011), our model has the advantage of being languageindependent, learning from the data itself rather than relying on manually developed, grammar-specific constraints, and covering a wider range of phenomena such as subcategorisation and syntactic collocations.
",5.5 Morphological Agreement,[0],[0]
"The results confirm that the RDLM is more effective at reducing morphological agreement errors than a similarly trained n-gram NNLM and the unlabelled DLM, and that adding HWCMf to the training objective is beneficial.",5.5 Morphological Agreement,[0],[0]
"On a a meta-evaluation level, we compare the rank correlation between the automatic metrics and the numer of agreement errors with Kendall’s τ correlation, and observe that he number of agreement errors is more strongly (negatively) correlated with HWCMf (τ = −0.92) than with BLEU (τ = −0.77), METEOR (τ = −0.54) or TER (τ = 0.69).",5.5 Morphological Agreement,[0],[0]
"This supports our theoretical expectation that HWCMf is more sensitive to morphological agreement, which is enforced along syntactic n-grams, than n-gram metrics such as BLEU, or the unigram metric METEOR.",5.5 Morphological Agreement,[0],[0]
"While there has been a wide range of dependency language models proposed (e.g. (Chelba et al., 1997;
Quirk et al., 2004; Shen et al., 2010; Zhang, 2009; Popel and Marecek, 2010)), there are vast differences in modelling assumptions.",6 Related Work,[0],[0]
"Our work is most similar to the dependency language model described in Shen et al. (2010), or the h-gram model proposed by Zhang (2009), both of which have been used for SMT.",6 Related Work,[0],[0]
"We make different modelling assumptions, relying less on configurational information, but including the prediction of dependency labels in the model.",6 Related Work,[0],[0]
"We argue that our relational modelling assumptions are more suitable for languages with a relatively free word order such as German.
",6 Related Work,[0],[0]
"To a lesser extent, our work is similar to other parsing models that have been used for language modelling, such as lexicalized PCFGs (Charniak, 2001; Collins, 2003; Charniak et al., 2003), or structured language models (Chelba and Jelinek, 2000); previous efforts to include them in the translation process failed to improve translation performance (Och et al., 2004; Post and Gildea, 2008).",6 Related Work,[0],[0]
"Differences in our work that could explain why we see improvements include the use of Neural Networks for training the model on the automatically parsed training text, instead of re-using existing parser models, which could be seen as a form of self-training (McClosky et al., 2006), and the integration of the language model into the decoder instead of n-best reranking.",6 Related Work,[0],[0]
"Also, there are major differences in the parsing models themselves.",6 Related Work,[0],[0]
"For instance, note that the structured LM by Chelba and Jelinek (2000) uses a binary branching structure, and that complex label sets would be required to encode subcategorisation frames in binary trees (Hockenmaier and Steedman, 2002).
",6 Related Work,[0],[0]
Our neural network is a standard feed-forward neural network as introduced by Bengio et al. (2003).,6 Related Work,[0],[0]
"Recently, recursive neural networks have been proposed for syntactic parsing (Socher et al., 2010; Le and Zuidema, 2014).",6 Related Work,[0],[0]
"The recursive nature of such models allows for the encoding of more context; for an efficient integration into the dynamic programming search of SMT decoding, we deem our model, which makes stronger Markov assumptions, more suitable.
",6 Related Work,[0],[0]
"While BLEU has been the standard objective function for tuning the log-linear parameters in SMT systems, recent work has investigated alternative objective functions.",6 Related Work,[0],[0]
"Some authors concluded that none of
the tested alternatives could consistently outperform BLEU (Cer et al., 2010; Callison-Burch et al., 2011).",6 Related Work,[0],[0]
"Liu et al. (2011) report that tuning on the TESLA metric gives better results than tuning on BLEU; Lo et al. (2013) do the same for MEANT.
",6 Related Work,[0],[0]
"There is related work on improving morphological agreement and subcategorisation through postediting (Rosa et al., 2012) or independent models for inflection generation (Toutanova et al., 2008; Weller et al., 2013).",6 Related Work,[0],[0]
"The latter models initially produce a stemmed translation, then predict the inflection through feature-rich sequence models.",6 Related Work,[0],[0]
Such a pipeline of prediction steps is less powerful than our joint prediction of stems and inflection.,6 Related Work,[0],[0]
"For instance, in example 2 in Table 4, our model chooses a different stem to match the subcategorisation frame of the translation; it is not possible to fix the baseline translation with inflection changes alone.",6 Related Work,[0],[0]
The main contribution of this paper is the description of a relational dependency language,7 Conclusion,[0],[0]
"model.10 We show that it is a valuable asset to a state-of-the-art SMT system by comparing perplexity values with other types of languages models, and by its integration into decoding, which results in improvements according to automatic MT metrics and reduces the number of agreement errors.",7 Conclusion,[0],[0]
"We show that the disfluencies that our model captures are qualitatively different from an n-gram Neural Network language model, with our model being more effective at modelling fluency phenomena along syntactic n-grams.
",7 Conclusion,[0],[0]
A second important contribution is the optimization of the log-linear parameters of an SMT system based on syntactic n-grams.,7 Conclusion,[0],[0]
We are to our knowledge the first to tune an SMT system on a non-shallow syntactic similarity metric.,7 Conclusion,[0],[0]
"Apart from showing improvements by tuning on HWCMf , our results also shed light on the interaction between models and tuning metrics.",7 Conclusion,[0],[0]
"With n-gram language models, the choice of tuning metric only had a small effect on the English→German translation results.",7 Conclusion,[0],[0]
"Only with dependency language models, which are able to model the syntactic n-grams that HWCM scores, did we see large improvements from adding
10We have released an implementation of RDLM and tuning on HWCMf as part of the Moses decoder.
HWCMf to the objective function.",7 Conclusion,[0],[0]
"On the one hand, this has implications when evaluating new model components: using an objective function that cannot capture the impact of a model component can result in false negatives because the model component will not receive an appropriate weight, and the model may thus seem to be of little use, even in a human evaluation.",7 Conclusion,[0],[0]
"On the other hand, it is an important finding for the evaluation of objective functions: the performance of an objective function is tied to the power of the underlying model.",7 Conclusion,[0],[0]
"Without a model that is able to model syntactic n-grams, we might have concluded that HWCM is of little help as an objective function.",7 Conclusion,[0],[0]
"Now, we hypothesize that HWCM is well-suited to optimize dependency language models because both operate on syntactic ngrams, just like BLEU and n-gram models are natural counterparts.
",7 Conclusion,[0],[0]
"The approach we present is languageindependent, and we evaluated it on SMT into German and Russian.",7 Conclusion,[0],[0]
"While we have no empirical data on the model’s effectiveness for other target languages, we suspect that syntactic n-grams are especially suited for modelling and evaluating translations into languages with inter-dependencies between distant words and relatively free word order, such as German, Czech, or Russian.
",7 Conclusion,[0],[0]
"In this work, we relied on parse hypotheses being provided by a string-to-tree SMT decoder, but other settings are conceivable for future work, such as performing n-best string reranking by coupling the relational dependency LM with a monolingual parse algorithm.",7 Conclusion,[0],[0]
"Another obvious extension of the relational dependency LM is the inclusion of more context, for instance through larger windows for siblings and ancestors, or source-context as in (Devlin et al., 2014).",7 Conclusion,[0],[0]
"Also, we believe that the model can benefit from further advances in neural network modelling, for instance recent findings that ensembles of networks outperform a single network (Mikolov et al., 2011; Devlin et al., 2014)",7 Conclusion,[0],[0]
I thank Bonnie Webber and the anonymous reviewers for their helpful suggestions and feedback.,Acknowledgements,[0],[0]
This research was funded by the Swiss National Science Foundation under grant P2ZHP1_148717.,Acknowledgements,[0],[0]
"The role of language models in SMT is to promote fluent translation output, but traditional n-gram language models are unable to capture fluency phenomena between distant words, such as some morphological agreement phenomena, subcategorisation, and syntactic collocations with string-level gaps.",abstractText,[0],[0]
Syntactic language models have the potential to fill this modelling gap.,abstractText,[0],[0]
We propose a language model for dependency structures that is relational rather than configurational and thus particularly suited for languages with a (relatively) free word order.,abstractText,[0],[0]
"It is trainable with Neural Networks, and not only improves over standard n-gram language models, but also outperforms related syntactic language models.",abstractText,[0],[0]
We empirically demonstrate its effectiveness in terms of perplexity and as a feature function in string-to-tree SMT from English to German and Russian.,abstractText,[0],[0]
We also show that using a syntactic evaluation metric to tune the log-linear parameters of an SMT system further increases translation quality when coupled with a syntactic language model.,abstractText,[0],[0]
Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 360–369, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Humans appear to organize and remember everyday experiences by imposing a narrative structure on them (Nelson, 1989; Thorne and Nam, 2009; Bruner, 1991; McAdams et al., 2006), and many genres of natural language text are therefore narratively structured, e.g. dinner table conversations, news articles, user reviews and blog posts (Polanyi, 1989; Jurafsky et al., 2014; Bell, 2005; Gordon et al., 2011).",1 Introduction,[0],[0]
"Moreover, there is broad consensus that understanding a narrative involves activating a representation, early in the narrative, of the protagonist and her goals and desires, and then maintaining that representation as the narrative evolves, as a vehicle for explaining the protagonist’s actions and tracking narrative outcomes (Elson, 2012; Rapp and Gerrig, 2006; Trabasso
and van den Broek, 1985; Lehnert, 1981).",1 Introduction,[0],[0]
"To date, there has been limited work on computational models for recognizing the expression of the protagonist’s goals and desires in narrative texts, and tracking their corresponding narrative outcomes.",1 Introduction,[0],[0]
"We introduce a new corpus DesireDB of∼3,500 first-person informal narratives with annotations for desires and their fulfillment status, available online.1 Because first-person narratives often revolve around the narrator’s private states and goals (Labov, 1972), this corpus is highly suitable as a testbed for identifying human desires and their outcomes.",1 Introduction,[0],[0]
"Moreover, first-person narratives allow the narrative protagonist (first-person) to be easily identified and tracked.",1 Introduction,[0],[0]
"Figure 1 illustrates examples of desire and goal expressions in our corpus.
DesireDB is open domain.",1 Introduction,[0],[0]
It contains a broad range of expressions of desires and goal statements in personal narratives.,1 Introduction,[0],[0]
It also includes the narrative context for each desire statement as shown in Figure 2.,1 Introduction,[0],[0]
"We include both prior and
1https://nlds.soe.ucsc.edu/DesireDB
360
post context of the desire expressions, since theories of narrative structure suggest that the evaluation points of a narrative can precede the expression of the events, goals and desires of the narrator (Labov, 1972; Swanson et al., 2014).
",1 Introduction,[0],[0]
"Our approach builds on seminal work on a computational model of Lehnert’s plot units, that applied modern NLP tools to tracking narrative affect states in Aesop’s Fables (Goyal et al., 2010; Lehnert, 1981; Goyal and Riloff, 2013).",1 Introduction,[0],[0]
"Our framing of the problem is also inspired by recent work that identifies three forms of desire expressions in short narratives from MCTest and SimpleWiki and develops models to predict whether desires are fulfilled or unfulfilled (Chaturvedi et al., 2016).",1 Introduction,[0],[0]
"However DesireDB’s narrative and sentence structure is more complex than either MCTest or SimpleWiki (Richardson et al., 2013; Coster and Kauchak, 2011).
",1 Introduction,[0],[0]
"We propose new features (Sec 4.1), as well as testing features used in previous work, and apply different classifiers to model desire fulfillment in our corpus.",1 Introduction,[0],[0]
We also directly compare to results on MCTest and SimpleWiki (Sec 4.4).,1 Introduction,[0],[0]
We apply LSTM models that distinguish between prior and post context and capture the flow of the narrative.,1 Introduction,[0],[0]
"Our best system, a Skip-Thought RNN model, achieves an F-measure of 0.70, while a logistic regression system achieves 0.66.",1 Introduction,[0],[0]
"Our models and features outperform Chaturvedi et al. (2016) on MCTest and SimpleWiki, while providing new results for a new corpus for tracking desires in first-person narratives.",1 Introduction,[0],[0]
"Moreover, analysis of our results shows that features representing the discourse structure (such as overt discourse relation markers) are the best predictors of fulfillment status of a desire or goal.",1 Introduction,[0],[0]
"We also show that both prior and post context are important for this task.
",1 Introduction,[0],[0]
We discuss related work in Sec. 2 and describe our corpus and annotations in Sec. 3.,1 Introduction,[0],[0]
Section 4 presents our features and methods for modeling desire fulfillment in narratives along with the experiments and results including comparison to previous work.,1 Introduction,[0],[0]
"Finally, we present conclusions and future directions in Sec. 5.",1 Introduction,[0],[0]
"There has recently been an upsurge in interest in computational models of narrative structure (Lehnert, 1981; Wilensky, 1982) and story understanding (Rahimtoroghi et al., 2016; Swanson
et al., 2014; Ouyang and McKeown, 2015, 2014).",2 Related Work,[0],[0]
"However there has been limited work on computational models for recognizing the expression of the protagonist’s goals and desires in narrative genres.
",2 Related Work,[0],[0]
"Our approach builds on work by Goyal and Riloff (2013) that applied modern NLP tools to track narrative affect states in Aesop’s Fables (Goyal et al., 2010).",2 Related Work,[0],[0]
They present a system called AESOP that uses a number of existing resources to identify affect states of the characters as part of deriving plot units.,2 Related Work,[0],[0]
"The motivation of modeling plot units is the idea that emotional reactions are central to the notion of a narrative and the main plot of a story can be modeled by tracking the transition between the affect states (Lehnert, 1981).",2 Related Work,[0],[0]
The AESOP system identifies affect states and creates links between them to model plot units and is evaluated on a small set of two-character fables.,2 Related Work,[0],[0]
They performed a manual annotation to examine different types of affect expressions in the narratives.,2 Related Work,[0],[0]
"Their study shows that many affect states arise from events where a character is acted upon in positive or negative ways, not explicit expression of emotions.",2 Related Work,[0],[0]
They also show that most of the affect states emerge by the expression of goals and plans and goal completion.,2 Related Work,[0],[0]
"Some of our features are motivated by the idea that implicit sentiment polarity can represent success or failure of goals and can be used to better model desire and goal
fulfillment in a narrative (Reed et al., 2017), although we cannot directly compare our findings to theirs because their annotations are not publicly available.
",2 Related Work,[0],[0]
"Chaturvedi et al. (2016) exploit two deliberately simplified datasets in order to model desire and its fulfillment: MCTest which contains 660 stories limited to content understandable by 7-year old children, and, SimpleWiki created from a dump of the Simple English Wikipedia discarding all the lists, tables and titles.",2 Related Work,[0],[0]
"They use desire statements matching a list of three verb phrases, wanted to, hoped to, and wished to.",2 Related Work,[0],[0]
Their context representation consists of five or fewer sentences following the desire expression.,2 Related Work,[0],[0]
They use BOW (Bag of Words) as baseline and apply unstructured and structured models for desire fulfillment modeling with different features motivated by narrative structure.,2 Related Work,[0],[0]
Their best result is achieved with a structured prediction model called Latent Structured Narrative Model (LSNM) which models the evolution of the narrative by associating a latent variable with each fragment of the context in the data.,2 Related Work,[0],[0]
"Their best unstructured model is a Logistic Regression classifier that uses all of their features.
",2 Related Work,[0],[0]
"Recent work on computational models of semantics provides an evaluation test for story understanding (Mostafazadeh et al., 2017).",2 Related Work,[0],[0]
"The task includes four-sentence stories, each with two possible endings where only one is correct.",2 Related Work,[0],[0]
"The goal is for each system to select the correct ending of the story by modeling different levels of semantics in narratives, such as lexical, sentential and discourse-level.",2 Related Work,[0],[0]
"The highest performing model with 75% accuracy used a linear regression classifier with several features such as neural language models and stylistic features to model the story coherence (Schwartz et al., 2017).",2 Related Work,[0],[0]
The results from other systems showed that sentiment is an important factor and using only sentiment features could achieve about 65% accuracy on the test.,2 Related Work,[0],[0]
DesireDB aims to provide a testbed for modeling desire and goals in personal narrative and predicting their fulfillment status.,3 DesireDB Corpus,[0],[0]
"We develop a systematic method to identify desire and goal statements, and then collect annotations to create goldstandard labels of fulfillment status as well as spans of text marked as evidence.",3 DesireDB Corpus,[0],[0]
"Our corpus is a subset of the Spinn3r corpus (Burton et al., 2011, 2009), consisting of firstperson narratives from six personal blog domains: livejournal.com, wordpress.com, blogspot.com, spaces.live.com, typepad.com, travelpod.com.",3.1 Identifying Desires and Goals,[0],[0]
"To create our dataset, we select only desire expressions involving some version of the first-person.",3.1 Identifying Desires and Goals,[0],[0]
"In first-person narratives, the narrator and protagonist naturally align which makes it much easier to identify and track the protagonist than in fiction or historical genre.",3.1 Identifying Desires and Goals,[0],[0]
"Thus, selecting narrative passages with expressions of desire relating to the first-person are very likely to discuss subsequent behaviors to achieve that desire and the end result.",3.1 Identifying Desires and Goals,[0],[0]
"Put simply, zooming in on first-person desires means that desire and its aftermath are more likely to be highly topical for the narrative.",3.1 Identifying Desires and Goals,[0],[0]
"This corpus, then, is highly suitable as a testbed for modeling human desires and their fulfillment.
",3.1 Identifying Desires and Goals,[0],[0]
"Human desires and goals can be expressed linguistically in many different ways, including both explicit verbal and nominal markers of desire or necessity (e.g., want, hope) and more general markers of urges (e.g., craving, hunger, thirst).",3.1 Identifying Desires and Goals,[0],[0]
"To systematically discover predicates that specify desires, we browsed FrameNet 1.7 (Baker et al., 1998) selecting frames that seemed likely to contain lexical units specifying desires: Beingnecessary, Desiring, Have-as-a-demand, Needing, Offer, Purpose, Request, Required-event, Scheduling, Seeking, Seeking-to-achieve, Stimulus-focus, Stimulate-emotion, and Worry.",3.1 Identifying Desires and Goals,[0],[0]
"We then selected 100 representative instances of that frame in English Gigaword (Parker et al., 2011) by first selecting the 10 most frequent lexical units in that frame, and then selecting 10 random instances per lexical unit.",3.1 Identifying Desires and Goals,[0],[0]
"One of the authors examined each set of 100 instances, estimating for each sentence whether the predicate specifies a goal that the surrounding text picks up on.",3.1 Identifying Desires and Goals,[0],[0]
"Because we were looking for predicates that reliably specify desires that motivate a protagonist’s actions, we eliminated frames where less than 80% of the sentences showed this characteristic.
",3.1 Identifying Desires and Goals,[0],[0]
"This resulted in a downsample to the following four frames: Desiring, Needing, Purpose, and Request.",3.1 Identifying Desires and Goals,[0],[0]
We selected only the verbal lexical units because we found that verbs were more likely to introduce goals than nouns or adjectives.,3.1 Identifying Desires and Goals,[0],[0]
"We examined 100 instances for each verbal lex-
ical unit, discarding as before.",3.1 Identifying Desires and Goals,[0],[0]
This resulted in 37 verbs.,3.1 Identifying Desires and Goals,[0],[0]
"For each verb, we systematically constructed and coded all past forms of the verb (e.g., was [verb]ing, had [verb]ed, had been [verb]ing, [verb]ed, didn’t",3.1 Identifying Desires and Goals,[0],[0]
"[verb], etc.)",3.1 Identifying Desires and Goals,[0],[0]
"because we posited that morphological form itself may convey likelihood of fulfillment (e.g., a past perfect I had wanted to ... signals that something changed, either the desire or fulfillment).",3.1 Identifying Desires and Goals,[0],[0]
"We initially experimented with both past and (historical) present, but past tense verb patterns resulted in much higher precision.",3.1 Identifying Desires and Goals,[0],[0]
"We counted the instances of these patterns in our dataset, and retained only those lemmas with at least 1000 instances across the corpus.
",3.1 Identifying Desires and Goals,[0],[0]
"We extract stories containing the verbal patterns of desire, with five sentences before and after the desire expression sentence as context (See Fig. 2).",3.1 Identifying Desires and Goals,[0],[0]
Our annotation results provide support that the evidence of desire fulfillment can be expressed before the desire statement.,3.1 Identifying Desires and Goals,[0],[0]
We also study the effect of prior and post context in understanding desire fulfillment in our experiments (Section 4) and show that using the narrative context preceding the desire statement improves the results.,3.1 Identifying Desires and Goals,[0],[0]
"We extracted ∼600K desire expressions with their context, and then sample 3,680 instances for annotation.",3.2 Data Annotation,[0],[0]
"This subset consists of 16 verbal patterns (when collapsing all morphological forms to their
head word).",3.2 Data Annotation,[0],[0]
A group of pre-qualified Mechanical Turkers then labelled each instance.,3.2 Data Annotation,[0],[0]
"The annotators labelled the fulfillment status of the desire expression sentence based on the prior and post context, by choosing from three labels: Fulfilled, Unfulfilled, and Unknown from the context.",3.2 Data Annotation,[0],[0]
They were also asked to mark the evidence for the label they had chosen by specifying a span of text in the narrative.,3.2 Data Annotation,[0],[0]
"For each data instance, we asked the Turkers to mark the subject of the desire expression and determine if the expressed desire is hypothetical (e.g., a conditional sentence) or not.
",3.2 Data Annotation,[0],[0]
The annotators were selected from a list of prequalified workers who had successfully passed a test on a textual entailment task with 100% correct answers.,3.2 Data Annotation,[0],[0]
They were provided with detailed instructions and examples as to how to label the desires and mark the evidence.,3.2 Data Annotation,[0],[0]
We also specified the desire expression verbal pattern using square brackets (as shown in Fig. 1 and 2) for more clarity.,3.2 Data Annotation,[0],[0]
Three annotators were assigned to work on each data instance.,3.2 Data Annotation,[0],[0]
"To generate the gold-standard labels we used majority vote and the cases with no agreement were labeled as ‘None’.
",3.2 Data Annotation,[0],[0]
"Table 1 reports the distribution of data and goldstandard labels (Ful:Fulfilled, Unf:Unfulfilled, Unk:",3.2 Data Annotation,[0],[0]
Unknown from the context).,3.2 Data Annotation,[0],[0]
About half of the desire expressions (53%) were labeled Fulfilled and about one third (31%) were labeled Unfulfilled.,3.2 Data Annotation,[0],[0]
"The annotators didn’t agree on about 2% of the instances, that were labeled None.",3.2 Data Annotation,[0],[0]
"As Tabel 1 shows, the distribution of labels is not uniform across different verbal patterns.",3.2 Data Annotation,[0],[0]
"For in-
stance, decided to and couldn’t wait are highly skewed towards Fulfilled as opposed to hoped to which includes 68% Unfulfilled instances.",3.2 Data Annotation,[0],[0]
"Some patterns seem to be harder to annotate, like wished to, which has the highest rate of Unknown (30%) and None (8%) among all.
",3.2 Data Annotation,[0],[0]
"Other than fulfillment status, for each data instance in our corpus we include the agreementscore which is the number of annotators that agreed on the assigned label.",3.2 Data Annotation,[0],[0]
"In addition, we provide the evidence as a part of the DesireDB data, by merging the text spans marked by the annotators as evidence.",3.2 Data Annotation,[0],[0]
"We compared the evidence spans pairwise to measure the overlap-score, indicating the number of pairs of annotators with overlapping responses.",3.2 Data Annotation,[0],[0]
An example is shown in Figure 3.,3.2 Data Annotation,[0],[0]
"The first part is the extracted data including the desire expression with prior and post context, and the second part is the gold-standard annotations.
",3.2 Data Annotation,[0],[0]
"To assess inter-annotator agreement for Fulfillment, we calculated Krippendorff-alpha Kappa (Krippendorff, 1970, 2004) for pairwise inter-annotator reliability, and, the average of Kappa between each annotator and the majority vote.",3.2 Data Annotation,[0],[0]
These two metrics are 0.63 and 0.88 respectively.,3.2 Data Annotation,[0],[0]
"Overall, 66% of the data was labeled with total agreement (where all three annotators agreed on the same label) and about 32% of data was labeled by two agreements and one disagreement.",3.2 Data Annotation,[0],[0]
We also examined the agreements across each label separately.,3.2 Data Annotation,[0],[0]
"For Fulfilled class, total agreement rate is 75%, which for Unfulfilled is 67%, and on Unknown from the context is 41%.",3.2 Data Annotation,[0],[0]
We believe this indicates that annotating unfulfilled desires was harder than fulfilled cases.,3.2 Data Annotation,[0],[0]
"For evidence marking, in 79% of the data all three annotators marked overlapping spans.",3.2 Data Annotation,[0],[0]
"We conducted a range of experiments on predicting fulfillment status of desires and goals, using different features and models, including LSTM architectures that can encode the sequential structure of the narratives.",4 Modeling Desire Fulfillment,[0],[0]
We first describe our features and models.,4 Modeling Desire Fulfillment,[0],[0]
"Then, we present our feature analysis study to examine their importance in modeling fulfillment.",4 Modeling Desire Fulfillment,[0],[0]
Finally we provide results of direct comparison to previous work on the existing corpora.,4 Modeling Desire Fulfillment,[0],[0]
"In our original informal examination of the DesireDB development data, we noticed several ways that a writer can signal (lack of) fulfillment of a desire like “I hoped to pick up a dictionary”.",4.1 Features Description,[0],[0]
"First, they may mention an outcome that entails (“The book I bought was...”) or strongly implies fulfillment (“I went back home happily.”).",4.1 Features Description,[0],[0]
"However, we noticed that in many cases of fulfillment, the ‘marker’ was simply the absence of any mention that things went wrong.",4.1 Features Description,[0],[0]
"For lack of fulfillment, while we found cases where writers explicitly state that their desire wasn’t met, we noted many instances where evidence came from mentioning that an enabling condition for fulfillment wasn’t met (“The bookstore was closed.”).
",4.1 Features Description,[0],[0]
"True machine understanding of these kinds of narrative structures requires robust models of the complex interplay of semantics (including negation) as well as world knowledge about the scripts for tasks like buying books, including what count as enabling conditions and entailers for fulfillment.",4.1 Features Description,[0],[0]
"While we hope to explore more articulated models in the future, for our experiments we considered reasonable proxies for the conditions mentioned above using existing resources (note that we also tested LSTM models described below, which may implicitly learn such relationships with sufficient data).",4.1 Features Description,[0],[0]
"One set (Desire Features) indexes properties of the desire expression (e.g., the desire verb) as well as overlap between the desired object/event and the surrounding context.",4.1 Features Description,[0],[0]
"The remaining features attempt to find general markers
for success or failure.",4.1 Features Description,[0],[0]
"One set (Discourse Features) looks for overt discourse relation markers that signal violation of expectation (e.g., ‘but’, ‘however’) or its opposite (e.g., ‘so’).",4.1 Features Description,[0],[0]
"Another uses the Connotation Lexicon (Feng et al., 2013) to model whether the context provides a positive or negative event.",4.1 Features Description,[0],[0]
All of these features are inspired by Chaturvedi et al. (2016).,4.1 Features Description,[0],[0]
"Finally, motivated by the AESOP modeling of affect states for identifying plot units (Goyal and Riloff, 2013), one set of features (Sentiment-Flow-Features) indexes whether there has been a change in sentiment in the surrounding context (which might be the mention of a thwarted effort or a hard won victory).",4.1 Features Description,[0],[0]
"Figure 4 provides an example of this.
",4.1 Features Description,[0],[0]
"In addition to a BOW (Bag of Words) baseline, we extracted the four types of features mentioned above.",4.1 Features Description,[0],[0]
"For features that examine the context around the desire expression, our experiments used the pre-context, the post-context, or both, as discussed below; context features are computed per sentence i of the context.",4.1 Features Description,[0],[0]
We also tested various ablations of these features described below as well.,4.1 Features Description,[0],[0]
"We now describe the full set of features in more detail.
",4.1 Features Description,[0],[0]
Desire-Features.,4.1 Features Description,[0],[0]
"From a desire expression of the form ‘X Ved S’, we extract the lexical feature",4.1 Features Description,[0],[0]
"Desire-Verb, the lemma for V. We also extract a list of focal words, the content words in embedded sentence S.",4.1 Features Description,[0],[0]
"In Figure 4, these are ‘do’, ‘go’, and ‘run’.",4.1 Features Description,[0],[0]
"The features Focal{Word,Synonym,Antonym}-Mention-i counts how many times each word, its synonyms, or its antonyms in WordNet (Fellbaum, 1998) are in the context, respectively.",4.1 Features Description,[0],[0]
"Similarly, Desire-SubjectMention-i marks if subject X is mentioned in the context.",4.1 Features Description,[0],[0]
"Finally, boolean First-Person-Subject indicates if X is first person (‘I’, ‘we’).
",4.1 Features Description,[0],[0]
Discourse-Features.,4.1 Features Description,[0],[0]
This class of features count how many of two classes of discourse relation markers (Violated-Expectation–i vs. MeetingExpectation–i) occur in the context.,4.1 Features Description,[0],[0]
"For the classes, we manually coded all overt discourse relation markers in the Penn Discourse",4.1 Features Description,[0],[0]
"Treebank three ways(violation, meeting, or neutral), leading to 15 meeting markers (‘accordingly’, ‘so’, ‘ultimately’, ‘finally’) and 31 violating (‘although’, ‘rather’, ‘yet’, ‘but’).",4.1 Features Description,[0],[0]
"In addition, we also tracked the presence of the most frequent of these (‘so’ and ‘but’, respectively) in the desire sentence itself by the booleans So-Present and But-Present.",4.1 Features Description,[0],[0]
Connotation-Features.,"1,366 953 380 70 2,780",[0],[0]
"Beyond the use of WordNet expansion for Focal-Word-Mention-i, we also used the Connotation Lexicon (Feng et al., 2013), a lexical resource marking very general connotation polarities (positive or negative) of words (as opposed to more specific sentiment lexicons).","1,366 953 380 70 2,780",[0],[0]
Connotation-Agree-i counts for each word w in focal words the number of words in the context that have the same connotation polarity as w. Connotation-Disgree-i is defined similarly.,"1,366 953 380 70 2,780",[0],[0]
Sentiment-Flow-Features.,"1,366 953 380 70 2,780",[0],[0]
"To model affect states, we compute a sentiment score for the desire expression sentence as well as each sentence in the context.","1,366 953 380 70 2,780",[0],[0]
"Then for each sentence of the context, the booleans Sentiment-Agree-i and SentimentDisagree-i mark whether that sentence and the desire expression sentence have the same sentiment polarity (see Figure 4).","1,366 953 380 70 2,780",[0],[0]
"While there is evidence suggesting that models of implicit sentiment (e.g., (Goyal et al., 2010; Reed et al., 2017)) could do much better at tracking affect states, here we use the Stanford Sentiment system (Socher et al., 2013).","1,366 953 380 70 2,780",[0],[0]
Our features are motivated by narrative characteristics but do not directly capture the sequential structure of the narratives.,4.2 LSTM Models,[0],[0]
"We thus apply neural network models suitable for sequence learning, in order to directly encode the order of the sentences in the story and distinguish between prior and post context.",4.2 LSTM Models,[0],[0]
"We use two different architectures of LSTM (Long Short-Term Memory) (Hochreiter and Schmidhuber, 1997) models to generate sentence embeddings and then apply a three-layer RNN (Recurrent Neural Network) for classification.",4.2 LSTM Models,[0],[0]
"We used Keras (Chollet, 2015) as a deep learning toolkit for implementing our experiments.",4.2 LSTM Models,[0],[0]
Skip-Thoughts.,4.2 LSTM Models,[0],[0]
"This is a sequential model that uses pre-trained skip-thoughts model (Kiros et al., 2015) as the embedding of sentences.",4.2 LSTM Models,[0],[0]
"It first concatenates features, if any, with embeddings, and then uses LSTM to generate a single representation for the context sequence, which is the output of the last unit.",4.2 LSTM Models,[0],[0]
"That single representation is then
concatenated with embedding-feature concatenation of desire sentence and is fed into a multi-layer network to yield a single binary output.",4.2 LSTM Models,[0],[0]
CNN-RNN.,4.2 LSTM Models,[0],[0]
"The only difference between the CNN-RNN model and Skip-Thought is that it uses the 1-dimensional convolution with maxover-time pooling introduced in (Kim, 2014) to generate the sentence embedding from word embedding, instead of using skip-thoughts.",4.2 LSTM Models,[0],[0]
"We use Google News Vectors (Mikolov et al., 2013) for the word embedding with different sizes from 1 to 7 for the kernel.
",4.2 LSTM Models,[0],[0]
"For our experiments, we first constructed a subset of DesireDB that we will call SimpleDesireDB, in order to be able to compare more directly to the models and data used in previous work.",4.2 LSTM Models,[0],[0]
"Chaturvedi et al. (2016) used three verb phrases to identify desire expressions (wanted to, hoped to, and wished to), so we selected a portion of our corpus including these patterns along with two other expressions (couldn’t wait to and decided to) to have sufficient data for experiments.",4.2 LSTM Models,[0],[0]
Table 2 shows the distribution of labels in this subset.,4.2 LSTM Models,[0],[0]
"For classification experiments we use data labeled as Fulfilled and Unfulfilled, thus the majority class accuracy is 59%.",4.2 LSTM Models,[0],[0]
"We split the data into Train (1,656), Dev (327), and Test (336) sets for the experiments.
",4.2 LSTM Models,[0],[0]
"Results of our two LSTM models for Fulfilled (Ful) and Unfulfilled (Unf) classes and the overall classification task (P:precision, R:recall) on Simple-DesireDB are presented in Table 3.",4.2 LSTM Models,[0],[0]
ALL feature set includes all the features described in Sec. 4.1 (without BOW).,4.2 LSTM Models,[0],[0]
"The results indicate that our features can considerably improve the model, compared to the BOW baseline (F1 improved from
0.65 to 0.70 for Skip-Thought).",4.2 LSTM Models,[0],[0]
"We also conducted 4 sets of experiments to study the importance of prior, post and the whole context in predicting fulfillment status, using our best model.",4.2 LSTM Models,[0],[0]
The results of Skip-Thought using different contextual representations are in Table 4 with ALL features.,4.2 LSTM Models,[0],[0]
The results indicate that adding features from prior context alone improves the results.,4.2 LSTM Models,[0],[0]
"The best results are obtained by including the whole context and desire sentence.
",4.2 LSTM Models,[0],[0]
We then experimented with our best model on all of DesireDB.,4.2 LSTM Models,[0],[0]
"We also trained Naive Bayes, SVM and Logistic Regression (LR) classifiers as baselines, with the best results on the Dev set achieved by Logistic Regression.",4.2 LSTM Models,[0],[0]
Table 5 shows the results of Skip-Thought and LR on DesireDB for different features on the test set.,4.2 LSTM Models,[0],[0]
"Our feature ablation study on the Dev set, discussed in Sec. 4.3, indicates that Discourse features are better predictors of fulfillment status, so we present results using only Discourse features in addition to BOW and ALL.
",4.2 LSTM Models,[0],[0]
All of the results indicate that similar features and methods achieve better results for the Fulfilled class as compared to Unfulfilled.,4.2 LSTM Models,[0],[0]
"We believe the reason is that identifying unfulfillment of a desire or goal is a more difficult task, as discussed in the annotation description in Section 3.2.",4.2 LSTM Models,[0],[0]
"To further our analysis on the annotation disagreements, we examined the cases where only two annotators agreed on the assigned label.",4.2 LSTM Models,[0],[0]
"From the expressions labeled Fulfilled by two annotators, 64% were labeled Unknown from the context by the disagreeing annotator, and only 36% were labeled Unfulfilled.",4.2 LSTM Models,[0],[0]
"However, these numbers for the Unfulfilled class are respectively 49% and 51%, indicating a
stronger disagreement between annotators when labeling Unfulfilled expressions.",4.2 LSTM Models,[0],[0]
We used the InfoGain measure to rank features based on their importance in modeling desire fulfillment.,4.3 Feature Selection Experiments,[0],[0]
"The top 5 features are: But-Present, Post-Context-Connotation-Disagree, Post-Context-Violated-Expectation, Desire-Verb, Is-First-Person.",4.3 Feature Selection Experiments,[0],[0]
We also tested different feature sets separately.,4.3 Feature Selection Experiments,[0],[0]
"We describe our experiment results below.
",4.3 Feature Selection Experiments,[0],[0]
The results of the feature ablation experiments using LR model are shown in Table 6.,4.3 Feature Selection Experiments,[0],[0]
The ALL feature set includes all the features described in Sec. 4.1 (without BOW).,4.3 Feature Selection Experiments,[0],[0]
We obtained high precision and F-measure using the Discourse features.,4.3 Feature Selection Experiments,[0],[0]
"We also experimented with our top feature from the InfoGain analysis, But-Present, which surprisingly achieves a high F-measure, compared to using ALL and Discourse feature sets.",4.3 Feature Selection Experiments,[0],[0]
The last row of Table 6 shows the results of using ALL features excluding But-Present.,4.3 Feature Selection Experiments,[0],[0]
This indicates that features motivated by narrative structure are primarily driving improvement.,4.3 Feature Selection Experiments,[0],[0]
"In previous work Chaturvedi et al. (2016) show that a model representing narrative structure could beat the BOW baseline, but they performed no systematic feature ablation.",4.3 Feature Selection Experiments,[0],[0]
"Our results suggest that ultimately, the presence of “but” is likely a central driver for their improvements as well.",4.3 Feature Selection Experiments,[0],[0]
"We directly compare our methods and features to the most relevant previous work (Chaturvedi et al., 2016).",4.4 Comparison to Previous Work,[0],[0]
They applied their models on two datasets and reported the results for the Fulfilled class.,4.4 Comparison to Previous Work,[0],[0]
"We present the same metrics in Table 7, using our best model Skip-Thought (SkipTh).",4.4 Comparison to Previous Work,[0],[0]
"We also present results of our LR model with our Discourse features, Discourse-LR, trained and tested on their corpora to compare to their features.",4.4 Comparison to Previous Work,[0],[0]
The first three rows show the results from Chaturvedi et al. (2016) for comparison.,4.4 Comparison to Previous Work,[0],[0]
"As described in Sec. 2, they used BOW as baseline, LSNM is their best model, and Unstruct-LR is their unstructured model that uses all of their features with LR.
",4.4 Comparison to Previous Work,[0],[0]
"On both corpora, Discourse-LR outperforms Unstruct-LR, showing that the Discourse features are stronger indicators of the desire fulfillment status when used with LR classifier.",4.4 Comparison to Previous Work,[0],[0]
"In addition, on SimpleWiki, LR-Discourse outperforms their structured model, LSNM (0.46 vs. 0.27 on F-1).",4.4 Comparison to Previous Work,[0],[0]
"We created a novel dataset, DesireDB, for studying the expression of desires and their fulfillment in narrative discourse.",5 Conclusion and Future Work,[0],[0]
"We show that contextual
features help with classification, and that both prior and post context are useful.",5 Conclusion and Future Work,[0],[0]
"Finally, we show that exploiting narrative structure is helpful, both directly in terms of the utility of discourse relation features and indirectly via the superior performance of a Skip-Thought LSTM model.
",5 Conclusion and Future Work,[0],[0]
"In future work, we plan to explore richer features and models for semantic and discourse-based features, as well as the utility of more narrativelyaware features.",5 Conclusion and Future Work,[0],[0]
"For instance, the sentiment flow features roughly track the notion that the arc of a narrative may implicitly reveal resolution of a goal via changes in affect states.",5 Conclusion and Future Work,[0],[0]
"We hope to examine whether there are other similar rough-grained measures of change over the entire narrative that can improve the results.
",5 Conclusion and Future Work,[0],[0]
DesireDB contains annotator-labeled spans for evidence for the annotator’s conclusions.,5 Conclusion and Future Work,[0],[0]
"While we have not used this labeling, we plan to use it in future work.",5 Conclusion and Future Work,[0],[0]
"Finally, we hope to turn to automatically detecting instances of desire expressions that give rise to the kind of goal-oriented narratives DesireDB contains.",5 Conclusion and Future Work,[0],[0]
"Here we have used highprecision search patterns but our annotations show that such patterns still admitted 134 hypothetical desires (e.g., ‘If I had wanted to buy a book’).",5 Conclusion and Future Work,[0],[0]
It would appear that distinguishing hypothetical vs. real desires itself could be an interesting problem.,5 Conclusion and Future Work,[0],[0]
"This research was supported by Nuance Foundation Grant SC-14-74, NSF Grant IIS-1302668-002 and IIS-1321102.",Acknowledgments,[0],[0]
"Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives.",abstractText,[0],[0]
There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes.,abstractText,[0],[0]
"However, to date, there has been limited work on computational models for this problem.",abstractText,[0],[0]
"We introduce a new dataset, DesireDB, which includes goldstandard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context.",abstractText,[0],[0]
"We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves F-measure of 0.7 on our corpus.",abstractText,[0],[0]
Modelling Protagonist Goals and Desires in First-Person Narrative,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1128–1137 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1128
We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate highquality abstractive conversation responses in accordance with designated emotions.",text,[0],[0]
A critical research problem for artificial intelligence is to design intelligent agents that can perceive and generate human emotions.,1 Introduction,[0],[0]
"In the past decade, there has been significant progress in sentiment analysis (Pang et al., 2002, 2008; Liu, 2012) and natural language understanding—e.g., classifying the sentiment of online reviews.",1 Introduction,[0],[0]
"To build empathetic conversational agents, machines must also have the ability to learn to generate emotional sentences.
",1 Introduction,[0],[0]
"One of the major challenges is the lack of largescale, manually labeled emotional text datasets.",1 Introduction,[0],[0]
"Due to the cost and complexity of manual annotation, most prior research studies primarily focus on small-sized labeled datasets (Pang et al., 2002; Maas et al., 2011; Socher et al., 2013), which are not ideal for training deep learning models with a large number of parameters.
",1 Introduction,[0],[0]
"In recent years, a handful of medium to large scale, emotional corpora in the area of emotion analysis (Go et al., 2016) and dialog (Li et al., 2017b) are proposed.",1 Introduction,[0],[0]
"However, all of them are limited to a traditional, small set of labels, for example, “happiness,” “sadness,” “anger,” etc. or simply binary “positive” and “negative.”",1 Introduction,[0],[0]
"Such coarse-grained classification labels make it difficult to capture the nuances of human emotion.
",1 Introduction,[0],[0]
"To avoid the cost of human annotation, we propose the use of naturally-occurring emoji-rich Twitter data.",1 Introduction,[0],[0]
We construct a dataset using Twitter conversations with emojis in the response.,1 Introduction,[0],[0]
"The fine-grained emojis chosen by the users in the response can be seen as the natural label for the emotion of the response.
",1 Introduction,[0],[0]
We assume that the emotions and nuances of emojis are established through the extensive usage by Twitter users.,1 Introduction,[0],[0]
"If we can create agents that
are able to imitate Twitter users’ language style when using those emojis, we claim that, to some extent, we have captured those emotions.",1 Introduction,[0],[0]
"Using a large collection of Twitter conversations, we then trained a conditional generative model to automatically generate the emotional responses.",1 Introduction,[0],[0]
"Figure 1 shows an example.
",1 Introduction,[0],[0]
"To generate emotional responses in dialogs, another technical challenge is to control the target emotion labels.",1 Introduction,[0],[0]
"In contrast to existing work (Huang et al., 2017) that uses information retrieval to generate emotional responses, the research question we are pursuing in this paper, is to design novel techniques that can generate abstractive responses of any given arbitrary emotions, without having human annotators to label a huge amount of training data.
",1 Introduction,[0],[0]
"To control the target emotion of the response, we investigate several encoder-decoder generation models, including a standard attention-based SEQ2SEQ model as the base model, and a more sophisticated CVAE model (Kingma and Welling, 2013; Sohn et al., 2015), as VAE is recently found convenient in dialog generation (Zhao et al., 2017).
",1 Introduction,[0],[0]
"To explicitly improve emotion expression, we then experiment with several extensions to the CVAE model, including a hybrid objective with policy gradient.",1 Introduction,[0],[0]
"The performance in emotion expression is automatically evaluated by a separate sentence-to-emoji classifier (Felbo et al., 2017).",1 Introduction,[0],[0]
"Additionally, we conducted a human evaluation to assess the quality of the generated emotional text.
",1 Introduction,[0],[0]
Results suggest that our method is capable of generating state-of-the-art emotional text at scale.,1 Introduction,[0],[0]
"Our main contributions are three-fold:
• We provide a publicly available, large-scale dataset of Twitter conversation-pairs naturally labeled with fine-grained emojis.
",1 Introduction,[0],[0]
"• We are the first to use naturally labeled emojis for conducting large-scale emotional response generation for dialog.
",1 Introduction,[0],[0]
"• We apply several state-of-the-art generative models to train an emotional response generation system, and analysis confirms that our models deliver strong performance.
",1 Introduction,[0],[0]
"In the next section, we outline related work on sentiment analysis and emoji on Twitter data, as well as neural generative models.",1 Introduction,[0],[0]
"Then, we will
introduce our new emotional research dataset and formalize the task.",1 Introduction,[0],[0]
"Next, we will describe the neural models we applied for the task.",1 Introduction,[0],[0]
"Finally, we will show automatic evaluation and human evaluation results, and some generated examples.",1 Introduction,[0],[0]
Experiment details can be found in supplementary materials.,1 Introduction,[0],[0]
"In natural language processing, sentiment analysis (Pang et al., 2002) is an area that involves designing algorithms for understanding emotional text.",2 Related Work,[0],[0]
Our work is aligned with some recent studies on using emoji-rich Twitter data for sentiment classification.,2 Related Work,[0],[0]
"Eisner et al. (2016) proposes a method for training emoji embedding EMOJI2VEC, and combined with word2vec (Mikolov et al., 2013), they apply the embeddings for sentiment classification.",2 Related Work,[0],[0]
"DeepMoji (Felbo et al., 2017) is closely related to our study: It makes use of a large, naturally labeled Twitter emoji dataset, and train an attentive bi-directional long short-term memory network (Hochreiter and Schmidhuber, 1997) model for sentiment analysis.",2 Related Work,[0],[0]
"Instead of building a sentiment classifier, our work focuses on generating emotional responses, given the context and the target emoji.
",2 Related Work,[0],[0]
"Our work is also in line with the recent progress of the application of Variational Autoencoder (VAE) (Kingma and Welling, 2013) in dialog generation.",2 Related Work,[0],[0]
"VAE (Kingma and Welling, 2013) encodes data in a probability distribution, and then samples from the distribution to generate examples.",2 Related Work,[0],[0]
"However, the original frameworks do not support end-to-end generation.",2 Related Work,[0],[0]
"Conditional VAE (CVAE) (Sohn et al., 2015; Larsen et al., 2015) was proposed to incorporate conditioning option in the generative process.",2 Related Work,[0],[0]
"Recent research in dialog generation shows that language generated by VAE models enjoy significantly greater diversity than traditional SEQ2SEQ models (Zhao et al., 2017), which is a preferable property toward building a true-to-life dialog agents.
",2 Related Work,[0],[0]
"In dialog research, our work aligns with recent advances in sequence-to-sequence models (Sutskever et al., 2014) using long shortterm memory networks (Hochreiter and Schmidhuber, 1997).",2 Related Work,[0],[0]
A slightly altered version of this model serves as our base model.,2 Related Work,[0],[0]
Our modification enabled it to condition on single emojis.,2 Related Work,[0],[0]
"Li
et al. (2016) use a reinforcement learning algorithm to improve the vanilla sequence-to-sequence model for non-task-oriented dialog systems, but their reinforced and its follow-up adversarial models (Li et al., 2017a) also do not model emotions or conditional labels.",2 Related Work,[0],[0]
"Zhao et al. (2017) recently introduced conditional VAE for dialog modeling, but neither did they model emotions in the conversations, nor explore reinforcement learning to improve results.",2 Related Work,[0],[0]
"Given a dialog history, Xie et.",2 Related Work,[0],[0]
al.’s work recommends suitable emojis for current conversation.,2 Related Work,[0],[0]
Xie et.,2 Related Work,[0],[0]
"al. (2016)compress the dialog history to vector representation through a hierarchical RNN and then map it to a emoji by a classifier, while in our model, the representation for original tweet, combined with the emoji embedding, is used to generate a response.",2 Related Work,[0],[0]
We start by describing our dataset and approaches to collecting and processing the data.,3 Dataset,[0],[0]
"Social media is a natural source of conversations, and people use emojis extensively within their posts.",3 Dataset,[0],[0]
"However, not all emojis are used to express emotion and frequency of emojis are unevenly distributed.",3 Dataset,[0],[0]
"Inspired by DeepMoji (Felbo et al., 2017), we use 64 common emojis as labels (see Table 1), and collect a large corpus of Twitter conversations con-
taining those emojis.",3 Dataset,[0],[0]
Note that emojis with the difference only in skin tone are considered the same emoji.,3 Dataset,[0],[0]
"We crawled conversation pairs consisting of an original post and a response on Twitter from 12th to 14th of August, 2017.",3.1 Data Collection,[0],[0]
The response to a conversation must include at least one of the 64 emoji labels.,3.1 Data Collection,[0],[0]
"Due to the limit of Twitter streaming API, tweets are filtered on the basis of words.",3.1 Data Collection,[0],[0]
"In our case, a tweet can be reached only if at least one of the 64 emojis is used as a word, meaning it has to be a single character separated by blank space.",3.1 Data Collection,[0],[0]
"However, this kind of tweets is arguably cleaner, as it is often the case that this emoji is used to wrap up the whole post and clusters of repeated emojis are less likely to appear in such tweets.
",3.1 Data Collection,[0],[0]
"For both original tweets and responses, only English tweets without multimedia contents (such as URL, image or video) are allowed, since we assume that those contents are as important as the text itself for the machine to understand the conversation.",3.1 Data Collection,[0],[0]
"If a tweet contains less than three alphabetical words, the conversation is not included in the dataset.",3.1 Data Collection,[0],[0]
Then we label responses with emojis.,3.2 Emoji Labeling,[0],[0]
"If there are multiple types of emoji in a response, we use the emoji with most occurrences inside the response.",3.2 Emoji Labeling,[0],[0]
"Among those emojis with same occurrences, we choose the least frequent one across the whole corpus, on the hypothesis that less frequent tokens better represent what the user wants to express.",3.2 Emoji Labeling,[0],[0]
See Figure 2 for example.,3.2 Emoji Labeling,[0],[0]
"During preprocessing, all mentions and hashtags are removed, and punctuation1 and emojis are separated if they are adjacent to words.",3.3 Data Preprocessing,[0],[0]
"Words with digits are all treated as the same special token.
",3.3 Data Preprocessing,[0],[0]
"In some cases, users use emojis and symbols in a cluster to express emotion extensively.",3.3 Data Preprocessing,[0],[0]
"To normalize the data, words with more than two repeated letters, symbol strings of more than one repeated punctuation symbols or emojis are shortened, for example, ‘!!!!’",3.3 Data Preprocessing,[0],[0]
"is shortened to ‘!’, and ‘yessss’ to ‘yess’.",3.3 Data Preprocessing,[0],[0]
Note that we do not reduce duplicate letters completely and convert the word to the ‘correct’ spelling (‘yes’ in the example) since the length of repeated letters represents the intensity of emotion.,3.3 Data Preprocessing,[0],[0]
"By distinguishing ‘yess’ from ‘yes’, the emotional intensity is partially preserved in our dataset.
",3.3 Data Preprocessing,[0],[0]
"Then all symbols, emojis, and words are tokenized.",3.3 Data Preprocessing,[0],[0]
"Finally, we build a vocabulary of size 20K according to token frequency.",3.3 Data Preprocessing,[0],[0]
"Any tokens outside the vocabulary are replaced by the same special token.
",3.3 Data Preprocessing,[0],[0]
"We randomly split the corpus into 596,959 /32,600/32,600",3.3 Data Preprocessing,[0],[0]
conversation pairs for train /validation/test set2.,3.3 Data Preprocessing,[0],[0]
Distribution of emoji labels within the corpus is presented in Table 1.,3.3 Data Preprocessing,[0],[0]
"In this work, our goal is to generate emotional responses to tweets with the emotion specified by an emoji label.",4 Generative Models,[0],[0]
We assembled several generative models and trained them on our dataset.,4 Generative Models,[0],[0]
"Traditional studies use deep recurrent architecture and encoder-decoder models to generate conversation responses, mapping original texts to target responses.",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"Here we use a sequence-to-sequence (SEQ2SEQ) model (Sutskever et al., 2014) with global attention mechanism (Luong et al., 2015) as our base model (See Figure 3).
",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
We use randomly initialized embedding vectors to represent each word.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"To specifically model the
1Emoticons (e.g. ‘:)’, ‘(-:’) are made of mostly punctuation marks.",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
They are not examined in this paper.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"Common emoticons are treated as words during preprocessing.
",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
2We will release the dataset with all tweets in its original form before preprocessing.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"To comply with Twitter’s policy, we will include the tweet IDs in our release, and provide a script for downloading the tweets using the official API.",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"No information of the tweet posters is collected.
",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"emotion, we compute the embedding vector of the emoji label the same way as word embeddings.",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
The emoji embedding is further reduced to smaller size vector ve through a dense layer.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"We pass the embeddings of original tweets through a bidirectional RNN encoder of GRU cells (Schuster and Paliwal, 1997; Chung et al., 2014).",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
The encoder outputs a vector vo that represents the original tweet.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
Then vo and ve are concatenated and fed to a 1-layer RNN decoder of GRU cells.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
A response is then generated from the decoder.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"Having similar encoder-decoder structures, SEQ2SEQ can be easily extended to a Conditional Variational Autoencoder (CVAE) (Sohn et al., 2015).",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Figure 3 illustrates the model: response encoder, recognition network, and prior network
are added on top of the SEQ2SEQ model.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Response encoder has the same structure to original tweet encoder, but it has separate parameters.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"We use embeddings to represent Twitter responses and pass them through response encoder.
",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Mathematically, CVAE is trained by maximizing a variational lower bound on the conditional likelihood of x given c, according to:
p(x|c) = ∫ p(x|z, c)p(z|c)dz (1)
z, c and x are random variables.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
z is the latent variable.,4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"In our case, the condition c =",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"[vo; ve], target x represents the response.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Decoder is used to approximate p(x|z, c), denoted as pD(x|z, c).",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Prior network is introduced to approximate p(z|c), denoted as pP (z|c).",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Recognition network qR(z|x, c) is introduced to approximate true posterior p(z|x, c) and will be absent during generation phase.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"By assuming that the latent variable has a multivariate Gaussian distribution with a diagonal covariance matrix, the lower bound to log p(x|c) can then be written by:
−L(θD, θP , θR;x, c) =",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"KL(qR(z|x, c)||pP (z|c)) −EqR(z|x,c)(log pD(x|z, c))
",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"(2)
θD, θP , θR are parameters of those networks.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"In recognition/prior network, we first pass the variables through an MLP to get the mean and log variance of z’s distribution.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Then we run a reparameterization trick (Kingma and Welling, 2013) to sample latent variables.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"During training, z by the recognition network is passed to the decoder and trained to approximate z′ by the prior network.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"While during testing, the target response is absent, and z′ by the prior network is passed to the decoder.
",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Our CVAE inherits the same attention mechanism from the base model connecting the original tweet encoder to the decoder, which makes our model deviate from previous works of CVAE on text data.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Based on the attention memory as well as c and z, a response is finally generated from the decoder.
",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"When handling text data, the VAE models that apply recurrent neural networks as the structure of their encoders/decoders may first learn to ignore the latent variable, and explain the data with the more easily optimized decoder.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"The latent
variables lose its functionality, and the VAE deteriorates to a plain SEQ2SEQ model mathematically (Bowman et al., 2015).",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
Some previous methods effectively alleviate this problem.,4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Such methods are also important to keep a balance between the two items of the loss, namely KL loss and reconstruction loss.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"We use techniques of KL annealing, early stopping (Bowman et al., 2015) and bag-of-word loss (Zhao et al., 2017) in our models.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"The general loss with bag-of-word loss (see supplementary materials for details) is rewritten as:
L′ =",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
L+ Lbow (3),4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"In order to further control the emotion of our generation more explicitly, we combine policy gradient techniques on top of the CVAE above and proposed Reinforced CVAE model for our task.",4.3 Reinforced CVAE,[0],[0]
We first train an emoji classifier on our dataset separately and fix its parameters thereafter.,4.3 Reinforced CVAE,[0],[0]
The classifier is used to produce reward for the policy training.,4.3 Reinforced CVAE,[0],[0]
"It is a skip connected model of Bidirectional GRU-RNN layers (Felbo et al., 2017).
",4.3 Reinforced CVAE,[0],[0]
"During the policy training, we first get the generated response x′ by passing x and c through the CVAE, then feeding generation x′ to classifier and get the probability of the emoji label as reward R. Let θ be parameters of our network, REINFORCE algorithm (Williams, 1992) is used to maximize the expected reward of generated responses:
J (θ) = Ep(x|c)(Rθ(x, c))",4.3 Reinforced CVAE,[0],[0]
"(4)
The gradient of Equation 4 is approximated using the likelihood ratio trick (Glynn, 1990; Williams, 1992):
∇J (θ) =",4.3 Reinforced CVAE,[0],[0]
"(R− r)∇ |x|∑ t log p(xt|c, x1:t−1) (5)
r is the baseline value to keep estimate unbiased and reduce its variance.",4.3 Reinforced CVAE,[0],[0]
"In our case, we directly pass x through emoji classifier and compute the probability of the emoji label as r. The model then encourages response generation that has R > r.
As REINFORCE objective is unrelated to response generation, it may make the generation model quickly deteriorate to some generic responses.",4.3 Reinforced CVAE,[0],[0]
"To stabilize the training process, we propose two straightforward techniques to constrain the policy training:
1.",4.3 Reinforced CVAE,[0],[0]
Adjust rewards according to the position of the emoji label when all labels are ranked from high to low in order of the probability given by the emoji classifier.,4.3 Reinforced CVAE,[0],[0]
"When the probability of the emoji label is of high rank among all possible emojis, we assume that the model has succeeded in emotion expression, thus there is no need to adjust parameters toward higher probability in this response.",4.3 Reinforced CVAE,[0],[0]
"Modified policy gradient is written as:
∇J ′(θ) =",4.3 Reinforced CVAE,[0],[0]
"α(R− r)∇ |x|∑ t log p(xt|c, x1:t−1)
(6)
",4.3 Reinforced CVAE,[0],[0]
where α ∈,4.3 Reinforced CVAE,[0],[0]
"[0, 1] is a variant coefficient.",4.3 Reinforced CVAE,[0],[0]
"The higher R ranks in all types of emoji label, the closer α is to 0.
2.",4.3 Reinforced CVAE,[0],[0]
"Train Reinforced CVAE by a hybrid objective of REINFORCE and variational lower bound objective, learning towards both emotion accuracy and response appropriateness:
minθL′′ = L′",4.3 Reinforced CVAE,[0],[0]
− λJ ′,4.3 Reinforced CVAE,[0],[0]
"(7)
λ is a balancing coefficient, which is set to 1 in our experiments.
",4.3 Reinforced CVAE,[0],[0]
The algorithm outlining the training process of Reinforced CVAE can be found in the supplementary materials.,4.3 Reinforced CVAE,[0],[0]
We conducted several experiments to finalize the hyper-parameters of our models (Table 2).,5 Experimental Results and Analyses,[0],[0]
"During training, fully converged base SEQ2SEQ model is used to initialize its counterparts in CVAE models.",5 Experimental Results and Analyses,[0],[0]
Pretraining is vital to the success of our models since it is essentially hard for them to learn a latent variable space from total randomness.,5 Experimental Results and Analyses,[0],[0]
"For more details, please refer to the supplementary materials.
",5 Experimental Results and Analyses,[0],[0]
"In this section, we first report and analyze the general results of our models, including perplexity, loss and emotion accuracy.",5 Experimental Results and Analyses,[0],[0]
Then we take a closer look at the generation quality as well as our models’ capability of expressing emotion.,5 Experimental Results and Analyses,[0],[0]
"To generally evaluate the performance of our models, we use generation perplexity and top-1/top-5
emoji accuracy on the test set.",5.1 General,[0],[0]
Perplexity indicates how much difficulty the model is having when generating responses.,5.1 General,[0],[0]
"We also use top-5 emoji accuracy, since the meaning of different emojis may overlap with only a subtle difference.",5.1 General,[0],[0]
"The machine may learn that similarity and give multiple possible labels as the answer.
",5.1 General,[0],[0]
Note that we use the same emoji classifier for evaluation.,5.1 General,[0],[0]
"Its accuracy (see supplementary materials) may not seem perfect, but it is the stateof-the-art emoji classifier given so many classes.",5.1 General,[0],[0]
"Also, it’s reasonable to use the same classifier in training for automated evaluation, as is in (Hu et al., 2017).",5.1 General,[0],[0]
"We can obtain meaningful results as long as the classifier is able to capture the semantic relationship between emojis (Felbo et al., 2017).
",5.1 General,[0],[0]
"As is shown in Table 2, CVAE significantly reduces the perplexity and increases the emoji accuracy over base model.",5.1 General,[0],[0]
Reinforced CVAE also adds to the emoji accuracy at the cost of a slight increase in perplexity.,5.1 General,[0],[0]
"These results confirm that proposed methods are effective toward the generation of emotional responses.
",5.1 General,[0],[0]
"When converged, the KL loss is 27.0/25.5 for the CVAE/Reinforced CVAE respectively, and reconstruction loss 42.2/40.0.",5.1 General,[0],[0]
"The models achieved a balance between the two items of loss, confirming that they have successfully learned a meaningful latent variable.",5.1 General,[0],[0]
"SEQ2SEQ generates in a monotonous way, as several generic responses occur repeatedly, while the generation of CVAE models is of much more diversity.",5.2 Generation Diversity,[0],[0]
"To showcase this disparity, we calculated the type-token ratios of unigrams/bigrams/trigrams in generated responses as
the order of frequencies in the dataset.",5.2 Generation Diversity,[0],[0]
"No.0 is , for instance, No.1 and so on.",5.2 Generation Diversity,[0],[0]
Top: CVAE v. Base.,5.2 Generation Diversity,[0],[0]
Bottom: Reinforced CVAE v. CVAE.,5.2 Generation Diversity,[0],[0]
"If Reinforced CVAE scores higher, the margin is marked in orange.",5.2 Generation Diversity,[0],[0]
"If lower, in black.
",5.2 Generation Diversity,[0],[0]
the diversity score.,5.2 Generation Diversity,[0],[0]
"As shown in Table 3, results show that CVAE models beat the base models by a large margin.",5.2 Generation Diversity,[0],[0]
Diversity scores of Reinforced CVAE are reasonably compromised since it’s generating more emotional responses.,5.2 Generation Diversity,[0],[0]
There are potentially multiple types of emotion in reaction to an utterance.,5.3 Controllability of Emotions,[0],[0]
Our work makes it possible to generate a response to an arbitrary emotion by conditioning the generation on a specific type of emoji.,5.3 Controllability of Emotions,[0],[0]
"In this section, we generate one response in reply to each original tweet in the dataset and condition on each emoji of the selected 64 emo-
jis.",5.3 Controllability of Emotions,[0],[0]
"We may have recorded some original tweets with different replies in the dataset, but an original tweet only need to be used once for each emoji, so we eliminate duplicate original tweets in the dataset.",5.3 Controllability of Emotions,[0],[0]
"There are 30,299 unique original tweets in the test set.
",5.3 Controllability of Emotions,[0],[0]
Figure 4 shows the top-5 accuracy of each type of the first 32 emoji labels when the models generates responses from the test set conditioning on the same emoji.,5.3 Controllability of Emotions,[0],[0]
The results show that CVAE models increase the accuracy over every type of emoji label.,5.3 Controllability of Emotions,[0],[0]
"Reinforced CVAE model sees a bigger increase on the less common emojis, confirming the effect of the emoji-specified policy training.",5.3 Controllability of Emotions,[0],[0]
"We employed crowdsourced judges to evaluate a random sample of 100 items (Table 4), each being assigned to 5 judges on the Amazon Mechanical Turk.",5.4 Human Evaluation,[0],[0]
We present judges original tweets and generated responses.,5.4 Human Evaluation,[0],[0]
"In the first setting of human evaluation, judges are asked to decide which one of the two generated responses better reply the original tweet.",5.4 Human Evaluation,[0],[0]
"In the second setting, the emoji label is presented with the item discription, and judges are asked to pick one of the two generated responses that they decide better fits this emoji.",5.4 Human Evaluation,[0],[0]
(These two settings of evaluation are conducted separately so that it will not affect judges’ verdicts.),5.4 Human Evaluation,[0],[0]
Order of two generated responses under one item is permuted.,5.4 Human Evaluation,[0],[0]
"Ties are permitted for an-
swers.",5.4 Human Evaluation,[0],[0]
We batch five items as one assignment and insert an item with two identical outputs as the sanity check.,5.4 Human Evaluation,[0],[0]
"Anyone who failed to choose “tie” for that item is considered as a careless judge and is therefore rejected from our test.
",5.4 Human Evaluation,[0],[0]
We then conducted a simplified Turing test.,5.4 Human Evaluation,[0],[0]
"Each item we present judges an original tweet, its reply by a human, and its response generated from Reinforced CVAE model.",5.4 Human Evaluation,[0],[0]
We ask judges to decide which of the two given responses is written by a human.,5.4 Human Evaluation,[0],[0]
Other parts of the setting are similar to above-mentioned tests.,5.4 Human Evaluation,[0],[0]
"It turned out 18% of the test subjects mistakenly chose machine-generated responses as human written, and 27% stated that
they were not able to distinguish between the two responses.
",5.4 Human Evaluation,[0],[0]
"In regard of the inter-rater agreement, there are four cases.",5.4 Human Evaluation,[0],[0]
"The ideal situation is that all five judges choose the same answer for a item, and in the worst-case scenario, at most two judges choose the same answer.",5.4 Human Evaluation,[0],[0]
"In light of this, we have counted that 32%/33%/31%/5% of all items have 5/4/3/2 judges in agreement, showing that our experiment has a reasonably reliable inter-rater agreement.",5.4 Human Evaluation,[0],[0]
"We sampled some generated responses from all three models, and list them in Figure 5.",5.5 Case Study,[0],[0]
"Given
an original tweet, we would like to generate responses with three different target emotions.
",5.5 Case Study,[0],[0]
"SEQ2SEQ only chooses to generate most frequent expressions, forming a predictable pattern for its generation (See how every sampled response by the base model starts with “I’m”).",5.5 Case Study,[0],[0]
"On the contrary, generation from the CVAE model is diverse, which is in line with previous quantitative analysis.",5.5 Case Study,[0],[0]
"However, the generated responses are sometimes too diversified and unlikely to reply to the original tweet.
",5.5 Case Study,[0],[0]
Reinforced CVAE somtetimes tends to generate a lengthy response by stacking up sentences (See the responses to the first tweet when conditioning on the ‘folded hands’ emoji and the ‘sad face’ emoji).,5.5 Case Study,[0],[0]
"It learns to break the length limit of sequence generation during hybrid training, since the variational lower bound objective is competing with REINFORCE objective.",5.5 Case Study,[0],[0]
The situation would be more serious is λ in Equation 7 is set higher.,5.5 Case Study,[0],[0]
"However, this phenomenon does not impair the fluency of generated sentences, as can be seen in
Figure 5.",5.5 Case Study,[0],[0]
"In this paper, we investigate the possibility of using naturally annotated emoji-rich Twitter data for emotional response generation.",6 Conclusion and Future Work,[0],[0]
"More specifically, we collected more than half a million Twitter conversations with emoji in the response and assumed that the fine-grained emoji label chosen by the user expresses the emotion of the tweet.",6 Conclusion and Future Work,[0],[0]
We applied several state-of-the-art neural models to learn a generation system that is capable of giving a response with an arbitrarily designated emotion.,6 Conclusion and Future Work,[0],[0]
We performed automatic and human evaluations to understand the quality of generated responses.,6 Conclusion and Future Work,[0],[0]
We trained a large scale emoji classifier and ran the classifier on the generated responses to evaluate the emotion accuracy of the generated response.,6 Conclusion and Future Work,[0],[0]
"We performed an Amazon Mechanical Turk experiment, by which we compared our models with a baseline sequence-to-sequence model on metrics of relevance and emotion.",6 Conclusion and Future Work,[0],[0]
"Experimentally, it is shown that our model is capable of generating high-quality emotional responses, without the need of laborious human annotations.",6 Conclusion and Future Work,[0],[0]
Our work is a crucial step towards building intelligent dialog agents.,6 Conclusion and Future Work,[0],[0]
"We are also looking forward to transferring the idea of naturally-labeled emojis to task-oriented dialog and multi-turn dialog
generation problems.",6 Conclusion and Future Work,[0],[0]
"Due to the nature of social media text, some emotions, such as fear and disgust, are underrepresented in the dataset, and the distribution of emojis is unbalanced to some extent.",6 Conclusion and Future Work,[0],[0]
"We will keep accumulating data and increase the ratio of underrepresented emojis, and advance toward more sophisticated abstractive generation methods.",6 Conclusion and Future Work,[0],[0]
Generating emotional language is a key step towards building empathetic natural language processing agents.,abstractText,[0],[0]
"However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels.",abstractText,[0],[0]
"Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult.",abstractText,[0],[0]
"In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis.",abstractText,[0],[0]
We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence.,abstractText,[0],[0]
"We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text.",abstractText,[0],[0]
"Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate highquality abstractive conversation responses in accordance with designated emotions.",abstractText,[0],[0]
MOJITALK: Generating Emotional Responses at Scale,title,[0],[0]
"Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al.,
2011).",1 Introduction,[0],[0]
"Most prominent word representation techniques are grounded in the distributional hypothesis (Harris, 1954), relying on word co-occurrence information in large textual corpora (Curran, 2004; Turney and Pantel, 2010; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Schwartz et al., 2015, i.a.).
",1 Introduction,[0],[0]
"Morphologically rich languages, in which “substantial grammatical information. .",1 Introduction,[0],[0]
.,1 Introduction,[0],[0]
"is expressed at word level” (Tsarfaty et al., 2010), pose specific challenges for NLP.",1 Introduction,[0],[0]
"This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology.",1 Introduction,[0],[0]
"In the case of distributional vector space models, morphological complexity brings two challenges to the fore:
1.",1 Introduction,[0],[0]
Estimating Rare Words: A single lemma can have many different surface realisations.,1 Introduction,[0],[0]
Naively treating each realisation as a separate word leads to sparsity problems and a failure to exploit their shared semantics.,1 Introduction,[0],[0]
"On the other hand, lemmatising the entire corpus can obfuscate the differences that exist between different word forms even though they share some aspects of meaning.
2.",1 Introduction,[0],[0]
Embedded Semantics:,1 Introduction,[0],[0]
"Morphology can encode semantic relations such as antonymy (e.g. literate and illiterate, expensive and inexpensive) or (near-)synonymy (north, northern, northerly).
",1 Introduction,[0],[0]
"In this work, we tackle the two challenges jointly by introducing a resource-light vector space finetuning procedure termed morph-fitting.",1 Introduction,[0],[0]
The proposed method does not require curated knowledge bases or gold lexicons.,1 Introduction,[0],[0]
"Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the
ar X
iv :1
70 6.
00 37
",1 Introduction,[0],[0]
"7v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
J un
2 01
7
proliferation of word forms in morphologically rich languages.",1 Introduction,[0],[0]
"Formalised as an instance of the post-processing semantic specialisation paradigm (Faruqui et al., 2015; Mrkšić et al., 2016), morphfitting is steered by a set of linguistic constraints derived from simple language-specific rules which describe (a subset of) morphological processes in a language.",1 Introduction,[0],[0]
"The constraints emphasise similarity on one side (e.g., by extracting morphological synonyms), and antonymy on the other (by extracting morphological antonyms), see Fig. 1 and Tab. 2.
",1 Introduction,[0],[0]
"The key idea of the fine-tuning process is to pull synonymous examples described by the constraints closer together in the transformed vector space, while at the same time pushing antonymous examples away from each other.",1 Introduction,[0],[0]
"The explicit post-hoc injection of morphological constraints enables: a) the estimation of more accurate vectors for lowfrequency words which are linked to their highfrequency forms by the constructed constraints;1 this tackles the data sparsity problem; and b) specialising the distributional space to distinguish between similarity and relatedness (Kiela et al., 2015), thus supporting language understanding applications such as dialogue state tracking (DST).2
As a post-processor, morph-fitting allows the integration of morphological rules with any distributional vector space in any language: it treats an input distributional word vector space as a black box and fine-tunes it so that the transformed space reflects the knowledge coded in the input morphological constraints (e.g., Italian words rispettoso and irrispetosa should be far apart in the trans-
1For instance, the vector for the word katalanischem which occurs only 9 times in the German Wikipedia will be pulled closer to the more reliable vectors for katalanisch and katalanischer, with frequencies of 2097 and 1383 respectively.
2Representation models that do not distinguish between synonyms and antonyms may have grave implications in downstream language understanding applications such as spoken dialogue systems: a user looking for ‘an affordable Chinese restaurant in west Cambridge’ does not want a recommendation for ‘an expensive Thai place in east Oxford’.
",1 Introduction,[0],[0]
"formed vector space, see Fig. 1).",1 Introduction,[0],[0]
"Tab. 1 illustrates the effects of morph-fitting by qualitative examples in three languages: the vast majority of nearest neighbours are “morphological” synonyms.
",1 Introduction,[0],[0]
"We demonstrate the efficacy of morph-fitting in four languages (English, German, Italian, Russian), yielding large and consistent improvements on benchmarking word similarity evaluation sets such as SimLex-999 (Hill et al., 2015), its multilingual extension (Leviant and Reichart, 2015), and SimVerb-3500 (Gerz et al., 2016).",1 Introduction,[0],[0]
"The improvements are reported for all four languages, and with a variety of input distributional spaces, verifying the robustness of the approach.
",1 Introduction,[0],[0]
"We then show that incorporating morph-fitted vectors into a state-of-the-art neural-network DST model results in improved tracking performance, especially for morphologically rich languages.",1 Introduction,[0],[0]
"We report an improvement of 4% on Italian, and 6% on German when using morph-fitted vectors instead of the distributional ones, setting a new state-of-theart DST performance for the two datasets.3
3There are no readily available DST datasets for Russian.",1 Introduction,[0],[0]
"Preliminaries In this work, we focus on four languages with varying levels of morphological complexity: English (EN), German (DE), Italian (IT), and Russian (RU).",2 Morph-fitting: Methodology,[0],[0]
These correspond to languages in the Multilingual SimLex-999 dataset.,2 Morph-fitting: Methodology,[0],[0]
"Vocabularies Wen, Wde, Wit, Wru are compiled by retaining all word forms from the four Wikipedias with word frequency over 10, see Tab. 3.",2 Morph-fitting: Methodology,[0],[0]
"We then extract sets of linguistic constraints from these (large) vocabularies using a set of simple language-specific if-then-else rules, see Tab. 2.4",2 Morph-fitting: Methodology,[0],[0]
These constraints (Sect. 2.2) are used as input for the vector space post-processing ATTRACT-REPEL algorithm (outlined in Sect. 2.1).,2 Morph-fitting: Methodology,[0],[0]
"The ATTRACT-REPEL model, proposed by Mrkšić et al. (2017b), is an extension of the PARAGRAM procedure proposed by Wieting et al. (2015).",2.1 The ATTRACT-REPEL Model,[0],[0]
It provides a generic framework for incorporating similarity (e.g. successful and accomplished) and antonymy constraints (e.g. nimble and clumsy) into pre-trained word vectors.,2.1 The ATTRACT-REPEL Model,[0],[0]
"Given the initial vector space and collections of ATTRACT and REPEL constraints A and R, the model gradually modifies the space to bring the designated word vectors closer together or further apart.",2.1 The ATTRACT-REPEL Model,[0],[0]
The method’s cost function consists of three terms.,2.1 The ATTRACT-REPEL Model,[0],[0]
"The first term pulls the ATTRACT examples (xl, xr) ∈",2.1 The ATTRACT-REPEL Model,[0],[0]
A closer together.,2.1 The ATTRACT-REPEL Model,[0],[0]
"If BA denotes the current mini-batch of ATTRACT examples, this term can be expressed as:
A(BA) =",2.1 The ATTRACT-REPEL Model,[0],[0]
"∑
(xl,xr)∈BA
(ReLU (δatt + xltl",2.1 The ATTRACT-REPEL Model,[0],[0]
"− xlxr)
+ ReLU (δatt + xrtr − xlxr))
where δatt is the similarity margin which determines how much closer synonymous vectors should be to each other than to each of their respective negative examples.",2.1 The ATTRACT-REPEL Model,[0],[0]
"ReLU(x) = max(0, x) is the standard rectified linear unit (Nair and Hinton, 2010).",2.1 The ATTRACT-REPEL Model,[0],[0]
The ‘negative’ example ti for each word xi in any ATTRACT pair is the word vector closest to xi among the examples in the current minibatch (distinct from its target synonym and xi itself).,2.1 The ATTRACT-REPEL Model,[0],[0]
"This means that this term forces synonymous
4A native speaker can easily come up with these sets of morphological rules (or at least with a reasonable subset of them) without any linguistic training.",2.1 The ATTRACT-REPEL Model,[0],[0]
"What is more, the rules for DE, IT, and RU were created by non-native, non-fluent speakers with a limited knowledge of the three languages, exemplifying the simplicity and portability of the approach.
",2.1 The ATTRACT-REPEL Model,[0],[0]
"words from the in-batch ATTRACT constraints to be closer to one another than to any other word in the current mini-batch.
",2.1 The ATTRACT-REPEL Model,[0],[0]
The second term pushes antonyms away from each other.,2.1 The ATTRACT-REPEL Model,[0],[0]
"If (xl, xr) ∈ BR is the current minibatch of REPEL constraints, this term can be expressed as follows:
R(BR) =",2.1 The ATTRACT-REPEL Model,[0],[0]
"∑
(xl,xr)∈BR
(ReLU (δrpl + xlxr − xltr)
+ ReLU (δrpl + xlxr − xrtr))
",2.1 The ATTRACT-REPEL Model,[0],[0]
"In this case, each word’s ‘negative’ example is the (in-batch) word vector furthest away from it (and distinct from the word’s target antonym).",2.1 The ATTRACT-REPEL Model,[0],[0]
"The intuition is that we want antonymous words from the input REPEL constraints to be further away from each other than from any other word in the current mini-batch; δrpl is now the repel margin.
",2.1 The ATTRACT-REPEL Model,[0],[0]
The final term of the cost function serves to retain the abundance of semantic information encoded in the starting distributional space.,2.1 The ATTRACT-REPEL Model,[0],[0]
"If xiniti is the initial distributional vector and V (B) is the set of all vectors present in the given mini-batch, this term (per mini-batch) is expressed as follows:
R(BA,BR) = ∑
xi∈V (BA∪BR)
",2.1 The ATTRACT-REPEL Model,[0],[0]
"λreg ∥∥∥xiniti − xi∥∥∥ 2
where λreg is the L2 regularisation constant.5",2.1 The ATTRACT-REPEL Model,[0],[0]
"This term effectively pulls word vectors towards their initial (distributional) values, ensuring that relations encoded in initial vectors persist as long as they do not contradict the newly injected ones.",2.1 The ATTRACT-REPEL Model,[0],[0]
"Semantic Specialisation with Constraints The fine-tuning ATTRACT-REPEL procedure is entirely driven by the input ATTRACT and REPEL sets of
5We use hyperparameter values δatt = 0.6, δrpl = 0.0, λreg = 10
−9 from prior work without fine-tuning.",2.2 Language-Specific Rules and Constraints,[0],[0]
"We train all models for 10 epochs with AdaGrad (Duchi et al., 2011).
constraints.",2.2 Language-Specific Rules and Constraints,[0],[0]
"These can be extracted from a variety of semantic databases such as WordNet (Fellbaum, 1998), the Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015), or BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) as done in prior work (Faruqui et al., 2015; Wieting et al., 2015; Mrkšić et al., 2016, i.a.).",2.2 Language-Specific Rules and Constraints,[0],[0]
"In this work, we investigate another option: extracting constraints without curated knowledge bases in a spectrum of languages by exploiting inherent language-specific properties related to linguistic morphology.",2.2 Language-Specific Rules and Constraints,[0],[0]
"This relaxation ensures a wider portability of ATTRACTREPEL to languages and domains without readily available or adequate resources.
",2.2 Language-Specific Rules and Constraints,[0],[0]
Extracting ATTRACT Pairs,2.2 Language-Specific Rules and Constraints,[0],[0]
"The core difference between inflectional and derivational morphology can be summarised in a few lines as follows: the former refers to a set of processes through which the word form expresses meaningful syntactic information, e.g., verb tense, without any change to the semantics of the word.",2.2 Language-Specific Rules and Constraints,[0],[0]
"On the other hand, the latter refers to the formation of new words with semantic shifts in meaning (Schone and Jurafsky, 2001; Haspelmath and Sims, 2013; Lazaridou et al., 2013; Zeller et al., 2013; Cotterell and Schütze, 2017).
",2.2 Language-Specific Rules and Constraints,[0],[0]
"For the ATTRACT constraints, we focus on inflectional rather than on derivational morphology rules as the former preserve the full meaning of a word, modifying it only to reflect grammatical roles such as verb tense or case markers (e.g., (en_read, en_reads) or (de_katalanisch, de_katalanischer)).",2.2 Language-Specific Rules and Constraints,[0],[0]
"This choice is guided by our intent to fine-tune the original vector space in order to improve the embedded semantic relations.
",2.2 Language-Specific Rules and Constraints,[0],[0]
"We define two rules for English, widely recognised as morphologically simple (Avramidis and Koehn, 2008; Cotterell et al., 2016b).",2.2 Language-Specific Rules and Constraints,[0],[0]
"These are: (R1) if w1, w2 ∈Wen, where w2 = w1 + ing/ed/s, then add (w1, w2) and (w2, w1) to the set of ATTRACT constraints A.",2.2 Language-Specific Rules and Constraints,[0],[0]
"This rule yields pairs such as (look, looks), (look, looking), (look, looked).
",2.2 Language-Specific Rules and Constraints,[0],[0]
"If w[: −1] is a function which strips the last character from word w, the second rule is: (R2)
if w1 ends with the letter e and w1 ∈ Wen and w2 ∈ Wen, where w2 = w1",2.2 Language-Specific Rules and Constraints,[0],[0]
"[: −1] + ing/ed, then add (w1, w2) and (w2, w1) to A.",2.2 Language-Specific Rules and Constraints,[0],[0]
"This creates pairs such as (create, creating) and (create, created).",2.2 Language-Specific Rules and Constraints,[0],[0]
"Naturally, introducing more sophisticated rules is possible in order to cover for other special cases and morphological irregularities (e.g., sweep / swept), but in all our EN experiments, A is based on the two simple EN rules R1 and R2.
",2.2 Language-Specific Rules and Constraints,[0],[0]
"The other three languages, with more complicated morphology, yield a larger number of rules.",2.2 Language-Specific Rules and Constraints,[0],[0]
"In Italian, we rely on the sets of rules spanning: (1) regular formation of plural (libro / libri); (2) regular verb conjugation (aspettare / aspettiamo); (3) regular formation of past participle (aspettare / aspettato); and (4) rules regarding grammatical gender (bianco / bianca).",2.2 Language-Specific Rules and Constraints,[0],[0]
"Besides these, another set of rules is used for German and Russian: (5) regular declension (e.g., asiatisch / asiatischem).
",2.2 Language-Specific Rules and Constraints,[0],[0]
"Extracting REPEL Pairs As another source of implicit semantic signals, W also contains words which represent derivational antonyms: e.g., two words that denote concepts with opposite meanings, generated through a derivational process.",2.2 Language-Specific Rules and Constraints,[0],[0]
"We use a standard set of EN “antonymy” prefixes: APen = {dis, il, un, in, im, ir, mis, non, anti} (Fromkin et al., 2013).",2.2 Language-Specific Rules and Constraints,[0],[0]
"If w1, w2 ∈ Wen, where w2 is generated by adding a prefix from APen to w1, then (w1, w2) and (w2, w1) are added to the set of REPEL constraints R. This rule generates pairs such as (advantage, disadvantage) and (regular, irregular).",2.2 Language-Specific Rules and Constraints,[0],[0]
"An additional rule replaces the suffix -ful with -less, extracting antonyms such as (careful, careless).
",2.2 Language-Specific Rules and Constraints,[0],[0]
"Following the same principle, we use APde = {un, nicht, anti, ir, in, miss}, APit = {in, ir, im, anti}, and APru = {не, анти}.",2.2 Language-Specific Rules and Constraints,[0],[0]
"For instance, this generates an IT pair (rispettoso, irrispettoso)",2.2 Language-Specific Rules and Constraints,[0],[0]
(see Fig. 1).,2.2 Language-Specific Rules and Constraints,[0],[0]
"For DE, we use another rule targeting suffix replacement: -voll is replaced by -los.
",2.2 Language-Specific Rules and Constraints,[0],[0]
We further expand the set of REPEL constraints by transitively combining antonymy pairs from the previous step with inflectional ATTRACT pairs.,2.2 Language-Specific Rules and Constraints,[0],[0]
"This step yields additional constraints such as (rispettosa, irrispettosi) (see Fig. 1).",2.2 Language-Specific Rules and Constraints,[0],[0]
The final A andR constraint counts are given in Tab. 3.,2.2 Language-Specific Rules and Constraints,[0],[0]
The full sets of rules are available as supplemental material.,2.2 Language-Specific Rules and Constraints,[0],[0]
"Training Data and Setup For each of the four languages we train the skip-gram with negative sampling (SGNS) model (Mikolov et al., 2013)
on the latest Wikipedia dump of each language.",3 Experimental Setup,[0],[0]
"We induce 300-dimensional word vectors, with the frequency cut-off set to 10.",3 Experimental Setup,[0],[0]
The vocabulary sizes |W | for each language are provided in Tab. 3.6,3 Experimental Setup,[0],[0]
"We label these collections of vectors SGNS-LARGE.
",3 Experimental Setup,[0],[0]
Other Starting Distributional Vectors We also analyse the impact of morph-fitting on other collections of well-known EN word vectors.,3 Experimental Setup,[0],[0]
These vectors have varying vocabulary coverage and are trained with different architectures.,3 Experimental Setup,[0],[0]
"We test standard distributional models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015).",3 Experimental Setup,[0],[0]
"We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014).
",3 Experimental Setup,[0],[0]
"We also experiment with standard well-known distributional spaces in other languages (IT and DE), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vulić and Korhonen, 2016a).
",3 Experimental Setup,[0],[0]
"Morph-fixed Vectors A baseline which utilises an equal amount of knowledge as morph-fitting, termed morph-fixing, fixes the vector of each word to the distributional vector of its most frequent inflectional synonym, tying the vectors of lowfrequency words to their more frequent inflections.",3 Experimental Setup,[0],[0]
"For each word w1, we construct a set of M + 1 words Ww1 = {w1, w′1, . . .",3 Experimental Setup,[0],[0]
", w′M} consisting of the word w1 itself and all M words which cooccur with w1 in the ATTRACT constraints.",3 Experimental Setup,[0],[0]
"We then choose the word w′max from the set Ww1 with the maximum frequency in the training data, and fix all other word vectors in Ww1 to its word vector.",3 Experimental Setup,[0],[0]
"The morph-fixed vectors (MFIX) serve as our primary baseline, as they outperformed another straightforward baseline based on stemming across
6Other SGNS parameters were set to standard values (Baroni et al., 2014; Vulić and Korhonen, 2016b): 15 epochs, 15 negative samples, global learning rate: .025, subsampling rate: 1e− 4.",3 Experimental Setup,[0],[0]
"Similar trends in results persist with d = 100, 500.
",3 Experimental Setup,[0],[0]
"all of our intrinsic and extrinsic experiments.
",3 Experimental Setup,[0],[0]
"Morph-fitting Variants We analyse two variants of morph-fitting: (1) using ATTRACT constraints only (MFIT-A), and (2) using both ATTRACT and REPEL constraints (MFIT-AR).",3 Experimental Setup,[0],[0]
"Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity ratings for 3,500 verb pairs.7 SimLex-999 was translated to DE, IT, and RU by Leviant and Reichart (2015), and they crowdsourced similarity scores from native speakers.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"We use this dataset for our multilingual evaluation.8
Morph-fitting EN Word Vectors As the first experiment, we morph-fit a wide spectrum of EN distributional vectors induced by various architectures (see Sect. 3).",4 Intrinsic Evaluation: Word Similarity,[0],[0]
The results on SimLex and SimVerb are summarised in Tab. 4.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
The results with EN SGNS-LARGE vectors are shown in Fig. 3a.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
"Morphfitted vectors bring consistent improvement across all experiments, regardless of the quality of the initial distributional space.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
This finding confirms that the method is robust: its effectiveness does not depend on the architecture used to construct the initial space.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
"To illustrate the improvements, note that the best score on SimVerb for a model trained on running text is achieved by Context2vec (ρ = 0.388); injecting morphological constraints into this vector space results in a gain of 7.1 ρ points.
",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"Experiments on Other Languages We next extend our experiments to other languages, testing both morph-fitting variants.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"The results are summarised in Tab. 5, while Fig. 3a-3d show results for the morph-fitted SGNS-LARGE vectors.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"These scores confirm the effectiveness and robustness of morph-fitting across languages, suggesting that the idea of fitting to morphological constraints is indeed language-agnostic, given the set of languagespecific rule-based constraints.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"Fig. 3 also demon-
7Unlike other gold standard resources such as WordSim353 (Finkelstein et al., 2002) or MEN (Bruni et al., 2014), SimLex and SimVerb provided explicit guidelines to discern between semantic similarity and association, so that related but non-similar words (e.g. cup and coffee) have a low rating.
",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"8Since Leviant and Reichart (2015) re-scored the original EN SimLex, we use their EN SimLex version for consistency.
strates that the morph-fitted vector spaces consistently outperform the morph-fixed ones.
",4 Intrinsic Evaluation: Word Similarity,[0],[0]
The comparison between MFIT-A and MFITAR indicates that both sets of constraints are important for the fine-tuning process.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
"MFIT-A yields consistent gains over the initial spaces, and (consistent) further improvements are achieved by also incorporating the antonymous REPEL constraints.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
This demonstrates that both types of constraints are useful for semantic specialisation.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
We also tried using other post-processing specialisation models from the literature in lieu of ATTRACT-REPEL using the same set of “morphological” synonymy and antonymy constraints.,Comparison to Other Specialisation Methods,[0],[0]
"We compare ATTRACT-REPEL to the retrofitting model
of (Faruqui et al., 2015) and counter-fitting (Mrkšić et al., 2017a).",Comparison to Other Specialisation Methods,[0],[0]
The two baselines were trained for 20 iterations using suggested settings.,Comparison to Other Specialisation Methods,[0],[0]
"The results for EN, DE, and IT are summarised in Fig. 2.",Comparison to Other Specialisation Methods,[0],[0]
They clearly indicate that MFIT-AR outperforms the two other post-processors for each language.,Comparison to Other Specialisation Methods,[0],[0]
We hypothesise that the difference in performance mainly stems from context-sensitive vector space updates performed by ATTRACT-REPEL.,Comparison to Other Specialisation Methods,[0],[0]
"Conversely, the other two models perform pairwise updates which do not consider what effect each update has on the example pair’s relation to other word vectors (for a detailed comparison, see (Mrkšić et al., 2017b)).
",Comparison to Other Specialisation Methods,[0],[0]
"Besides their lower performance, the two other specialisation models have additional disadvantages compared to the proposed morph-fitting model.",Comparison to Other Specialisation Methods,[0],[0]
"First, retrofitting is able to incorporate only synonymy/ATTRACT pairs, while our results demonstrate the usefulness of both types of constraints, both for intrinsic evaluation (Tab. 5) and downstream tasks (see later Fig. 3).",Comparison to Other Specialisation Methods,[0],[0]
"Second, counter-fitting is computationally intractable with SGNS-LARGE vectors, as its regularisation term involves the computation of all pairwise distances between words in the vocabulary.
",Comparison to Other Specialisation Methods,[0],[0]
"Further Discussion The simplicity of the used language-specific rules does come at a cost of occasionally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress).",Comparison to Other Specialisation Methods,[0],[0]
"In future work, we will study how to fur-
ther refine extracted sets of constraints.",Comparison to Other Specialisation Methods,[0],[0]
"We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.).",Comparison to Other Specialisation Methods,[0],[0]
Goal-oriented dialogue systems provide conversational interfaces for tasks such as booking flights or finding restaurants.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In slot-based systems, application domains are specified using ontologies that define the search constraints which users can express.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
An ontology consists of a number of slots and their assorted slot values.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In a restaurant search domain, sets of slot-values could include PRICE =",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"[cheap, expensive] or FOOD =",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"[Thai, Indian, ...].
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The DST model is the first component of modern dialogue pipelines (Young, 2010).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
It serves to capture the intents expressed by the user at each dialogue turn and update the belief state.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
This probability distribution over the possible dialogue states (defined by the domain ontology) is the system’s internal estimate of the user’s goals.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"It is used by the downstream dialogue manager component to choose the subsequent system response (Su et al., 2016).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The following example shows the true dialogue state in a multi-turn dialogue:
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
User: What’s good in the southern part of town?,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
inform(area=south) System: Vedanta is the top-rated Indian place.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
User: How about something cheaper?,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"inform(area=south, price=cheap) System: Seven Days is very popular.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
Great hot pot.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
User:,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
What’s the address?,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"inform(area=south, price=cheap); request(address) System: Seven Days is at 66 Regent Street.
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The Dialogue State Tracking Challenge (DSTC) shared task series formalised the evaluation and provided labelled DST datasets (Henderson et al., 2014a,b; Williams et al., 2016).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"While a plethora of DST models are available based on, e.g., handcrafted rules (Wang et al., 2014) or conditional random fields (Lee and Eskenazi, 2013), the recent DST methodology has seen a shift towards neural-
network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkšić et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkšić et al., 2017a, i.a.).
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkšić et al., 2017a).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
The NBT learns to compose these vectors into intermediate utterance and context representations.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
These are then used to decide which of the ontology-defined intents (goals) have been expressed by the user.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The NBT model keeps word vectors fixed during training, so that unseen, yet related words can be mapped to the right intent at test time (e.g. northern to north).
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
Data: Multilingual WOZ 2.0 Dataset,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Our DST evaluation is based on the WOZ dataset, released by Wen et al. (2017).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In this Wizard-of-Oz setup, two Amazon Mechanical Turk workers assumed the role of the user and the system asking/providing information about restaurants in Cambridge (operating over the same ontology and database used for DSTC2 (Henderson et al., 2014a)).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Users typed instead of speaking, removing the need to deal with noisy speech recognition.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In DSTC datasets, users would quickly adapt to the system’s inability to deal with complex queries.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Conversely, the WOZ setup allowed them to use sophisticated language.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The WOZ 2.0 release expanded the dataset to 1,200 dialogues (Mrkšić et al., 2017a).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In this work, we use translations of this dataset to Italian and German, released by Mrkšić et al. (2017b).
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Evaluation Setup The principal metric we use to measure DST performance is the joint goal accuracy, which represents the proportion of test set dialogue turns where all user goals expressed up to that point of the dialogue were decoded correctly (Henderson et al., 2014a).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The NBT models for EN, DE and IT are trained using four variants of the SGNS-LARGE vectors: 1) the initial distributional vectors; 2) morph-fixed vectors; 3) and 4) the two variants of morph-fitted vectors (see Sect. 3).
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"As shown by Mrkšić et al. (2017b), semantic specialisation of the employed word vectors ben-
efits DST performance across all three languages.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"However, large gains on SimLex-999 do not always induce correspondingly large gains in downstream performance.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In our experiments, we investigate the extent to which morph-fitting improves DST performance, and whether these gains exhibit stronger correlation with intrinsic performance.
Results and Discussion The dark bars (against the right axes) in Fig. 3 show the DST performance of NBT models making use of the four vector collections.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"IT and DE benefit from both kinds of morph-fitting: IT performance increases from 74.1→ 78.1 (MFIT-A) and DE performance rises even more: 60.6→ 66.3 (MFIT-AR), setting a new state-of-the-art score for both datasets.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The morph-fixed vectors do not enhance DST performance, probably because fixing word vectors to their highest frequency inflectional form eliminates useful semantic content encoded in the original vectors.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"On the other hand, morph-fitting makes use of this information, supplementing it with semantic relations between different morphological forms.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"These conclusions are in line with the SimLex gains, where morph-fitting outperforms both distributional and morph-fixed vectors.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
English performance shows little variation across the four word vector collections investigated here.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"This corroborates our intuition that, as a morphologically simpler language, English stands to gain less from fine-tuning the morphological variation for downstream applications.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
This result again points at the discrepancy between intrinsic and extrinsic evaluation: the considerable gains in SimLex performance do not necessarily induce similar gains in downstream performance.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
Additional discrepancies between SimLex and downstream DST performance are detected for German and Italian.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"While we observe a slight drop in SimLex performance with the DE MFIT-AR vectors compared to the MFIT-A ones, their relative performance is reversed in the DST task.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"On the other hand, we see the opposite trend in Italian, where the MFITA vectors score lower than the MFIT-AR vectors on SimLex, but higher on the DST task.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In summary, we believe these results show that SimLex is not a perfect proxy for downstream performance in language understanding tasks.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Regardless, its performance does correlate with downstream performance to a large extent, providing a useful indicator for the usefulness of specific word vector
spaces for extrinsic tasks such as DST.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together.,6 Related Work,[0],[0]
"Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016).",6 Related Work,[0],[0]
"Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkšić et al., 2016).",6 Related Work,[0],[0]
Morph-fitting falls into the latter category.,6 Related Work,[0],[0]
"However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words.
",6 Related Work,[0],[0]
Word Vectors and Morphology,6 Related Work,[0],[0]
The use of morphological resources to improve the representations of morphemes and words is an active area of research.,6 Related Work,[0],[0]
"The majority of proposed architectures encode morphological information, provided either as gold standard morphological resources (SylakGlassman et al., 2015) such as CELEX (Baayen et al., 1995) or as an external analyser such as Morfessor (Creutz and Lagus, 2007), along with distributional information jointly at training time in the language modelling (LM) objective (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schütze, 2015; Bhatia et al., 2016, i.a.).",6 Related Work,[0],[0]
"The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes.",6 Related Work,[0],[0]
"Contrary to our work, these models typically coalesce all lexical relations.
",6 Related Work,[0],[0]
"Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters) (dos Santos and Zadrozny, 2014; Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016;
",6 Related Work,[0],[0]
"Wieting et al., 2016; Verwimp et al., 2017, i.a.).",6 Related Work,[0],[0]
"In contrast to prior work, our model decouples the use of morphological information, now provided in the form of inflectional and derivational rules transformed into constraints, from the actual training.",6 Related Work,[0],[0]
"This pipelined approach results in a simpler, more portable model.",6 Related Work,[0],[0]
"In spirit, our work is similar to Cotterell et al. (2016b), who formulate the idea of post-training specialisation in a generative Bayesian framework.",6 Related Work,[0],[0]
Their work uses gold morphological lexicons; we show that competitive performance can be achieved using a non-exhaustive set of simple rules.,6 Related Work,[0],[0]
"Our framework facilitates the inclusion of antonyms at no extra cost and naturally extends to constraints from other sources (e.g., WordNet) in future work.",6 Related Work,[0],[0]
Another practical difference is that we focus on similarity and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures.,6 Related Work,[0],[0]
We have presented a novel morph-fitting method which injects morphological knowledge in the form of linguistic constraints into word vector spaces.,7 Conclusion and Future Work,[0],[0]
The method makes use of implicit semantic signals encoded in inflectional and derivational rules which describe the morphological processes in a language.,7 Conclusion and Future Work,[0],[0]
The results in intrinsic word similarity tasks show that morph-fitting improves vector spaces induced by distributional models across four languages.,7 Conclusion and Future Work,[0],[0]
"Finally, we have shown that the use of morph-fitted vectors boosts the performance of downstream language understanding models which rely on word representations as features, especially for morphologically rich languages such as German.
",7 Conclusion and Future Work,[0],[0]
"Future work will focus on other potential sources of morphological knowledge, porting the framework to other morphologically rich languages and downstream tasks, and on further refinements of the post-processing specialisation algorithm and the constraint selection.",7 Conclusion and Future Work,[0],[0]
This work is supported by the ERC Consolidator Grant LEXICAL:,Acknowledgments,[0],[0]
Lexical Acquisition Across Languages (no 648909).,Acknowledgments,[0],[0]
RR is supported by the IntelICRI grant: Hybrid Models for Minimally Supervised Information Extraction from Conversations.,Acknowledgments,[0],[0]
The authors are grateful to the anonymous reviewers for their helpful suggestions.,Acknowledgments,[0],[0]
"In this supplemental material, we provide a short comprehensive overview of simple languagespecific morphological rules in English (EN), German (DE), Italian (UT), and Russian (RU).",Morphological Rules,[0],[0]
These rules were used to build the sets of synonymous ATTRACT and antonymous REPEL constraints for our morph-fitting fine-tuning procedure.,Morphological Rules,[0],[0]
"As discussed in the paper, the linguistic constraints extracted from the rules require only a comprehensive list of vocabulary words in each language.",Morphological Rules,[0],[0]
A native speaker of each language used in our experiments is able to easily come up with these sets of morphological rules (or at least with a reasonable subset of rules) without any linguistic training.,Morphological Rules,[0],[0]
"What is more, the rules for German, Italian, and Russian were created by non-native and non-fluent speakers who have only a passive or limited knowledge of the three languages, exemplifying the simplicity and portability of the fine-tuning approach based on the shallow “morphological supervision”.",Morphological Rules,[0],[0]
"The simplicity is also confirmed by the short time used to compile the rules, ranging from a few minutes for English to approximately two hours for Russian.
",Morphological Rules,[0],[0]
"Different languages differ in their “morphological richness” (e.g., declension, verb conjugation, plural forming, gender) which consequently leads to the varying number of rules in each language.",Morphological Rules,[0],[0]
"However, all four languages in our study display morphological regularities described by simple morphological rules that are exploited to build sets of ATTRACT and REPEL linguistic constraints in each language from scratch.9
Vocabularies W in all four languages are labeled Wen, Wde, Wit, Wru.",Morphological Rules,[0],[0]
"We add the pairs (w1, w2) and (w2, w1) generated by the rules to the sets of constraints iff both w1, w2 ∈W .",Morphological Rules,[0],[0]
"After we generate all such constraints, since some constraints may have been generated by more than one rule, we remove all duplicates from the respective sets of ATTRACT and REPEL constraints.
",Morphological Rules,[0],[0]
"Before we start, we will define two simple functions: (i) the function",Morphological Rules,[0],[0]
"w[: −N ] strips the last
9Note that the rules for extracting ATTRACT constraints were additionally used to generate the Morph-SimLex evaluation set, also provided as supplemental material.
",Morphological Rules,[0],[0]
"N characters from the word w, (ii) the function w.ew(sub) tests if the word w ends with a sequence of characters sub.",Morphological Rules,[0],[0]
"For instance, create[: −1] returns creat, while create.ew(’s’) returns False and create.ew(’e’) returns True.",Morphological Rules,[0],[0]
"Inflectional Synonymy: ATTRACT As discussed in the paper, we rely on only two simple inflectional morphological rules in English: - w2 = w1 + ’s’/’ed’/’ing’.",English Rules,[0],[0]
"This rule yields constraints such as (speak, speaking), (turtle, turtles), or (clean, cleaned).",English Rules,[0],[0]
-,English Rules,[0],[0]
"If w1.ew(’e’), then w2 = w1[: −1] + ’ed’/’ing’.",English Rules,[0],[0]
"This rule yields constraints such as (create, creating), or (generate, generated).
",English Rules,[0],[0]
"Derivational Antonymy: REPEL We assume the following set of standard “antonymy” prefixes in English: APen = {’dis’, ’il’, ’un’, ’in’, ’im’,
’ir’, ’mis’, ’non’, ’anti’}.",English Rules,[0],[0]
"We rely on the following derivational rules to extract REPEL pairs: -w2 = ap +w1, where ap ∈ APen.",English Rules,[0],[0]
"This rule yields constraints such as (mature, immature), (allow, disallow) or (regularity, irregularity).",English Rules,[0],[0]
-,English Rules,[0],[0]
"If w1.ew(’ful’), then w2 = w1[: −3] + ’less’.",English Rules,[0],[0]
"This rule yields constraints such as (cheerful, cheerless).
",English Rules,[0],[0]
"As mentioned in the paper, for all four languages we further expand the set of REPEL constraints by transitively combining antonymy pairs with inflectional ATTRACT pairs.",English Rules,[0],[0]
"In simple words, the friend of my enemy is my enemy.",English Rules,[0],[0]
"This means that, given an ATTRACT pair (allow, allows) and a REPEL pair (allow, disallow), we extract another REPEL pair (allows, disallow).",English Rules,[0],[0]
"Inflectional Synonymy: ATTRACT Being morphologically richer than English, the German language naturally requires more rules to describe its (inflectional) morphological richness and variation.",German Rules,[0],[0]
"First, we capture the regular declension of nouns and adjectives by the following heuristic: - Generate a set of words Ww1 = {w1, w2|w2 = w1 + ’e’/’em’/’en’/’er’/’es’}; take the Cartesian product on Ww1 ×Ww1 and then exclude (wi, wi)
pairs with identical words.",German Rules,[0],[0]
"This rule generates pairs such as (schottisch, schottische), (schottischem, schottischen).
",German Rules,[0],[0]
"The second set of rules describes regular verb morphology, i.e., verb conjugation in the present and past tense, and the formation of regular past participles.",German Rules,[0],[0]
"This set of rules may be expressed as: - If w1.ew(’en’), then w′1 = w1[: −2].",German Rules,[0],[0]
"If w′1.ew(’t’), then generate a set of words Ww1 = {w1, w2|w2 = w1 + ’e’/’st’/’ete’/’etest’/’etet’/’eten’, w2 = ’ge’+w′1+ ’et’}, else (if not w′1.ew(’t’)), generate a set of words Ww1 = {w1, w2|w2 = w1 + ’e’/’st’/’te’/’test’/’tet’/’ten’, w2 = ’ge’+w′1+ ’t’}.",German Rules,[0],[0]
We then take the Cartesian product on Ww1 ×Ww1 .,German Rules,[0],[0]
"Again, all pairs with identical words were discarded.",German Rules,[0],[0]
"This rule yields pairs such as (machen, machten), (mache, gemacht), (kaufst, kauft), or (arbeite, arbeitete) and (arbeiten, gearbeitet).
",German Rules,[0],[0]
"Another set of rules targets the regular formation of plural nouns: - If w1.ew(’ei’) or w1.ew(’heit’) or w1.ew(’keit’) or w1.ew(’schaft’) or w1.ew(’ung’), then w2 = w1 + ’en’.",German Rules,[0],[0]
"This rule yields pairs such as (wahrheit, wahrheiten) or (gemeinschaft, gemeinschaften).",German Rules,[0],[0]
-,German Rules,[0],[0]
"If w1.ew(’in’), then w2 = w1 + ’nen’.",German Rules,[0],[0]
"This rule generates pairs such as (lehrerin, lehrerinnen) or (lektorin, lektorinnen).",German Rules,[0],[0]
- Ifw1.ew(’a’/’i’/’o’/’u’/’y’),German Rules,[0],[0]
thenw2 = w1 + ’s’.,German Rules,[0],[0]
"This rule yields pairs such as (auto, autos).",German Rules,[0],[0]
-,German Rules,[0],[0]
"If w1.ew(’e’), then w2 = w1 + ’n’.",German Rules,[0],[0]
"This rule yields pairs such as (postkarte, postkarten).",German Rules,[0],[0]
- w2 = lumlaut(w1) +,German Rules,[0],[0]
"er, where the function lumlaut(w) replaces the last occurrence of the letter ’a’,’o’ or ’u’ with ’ä’,’ö’ or ’ü’.",German Rules,[0],[0]
"This rule generates pairs such as (wörterbuch, wörterbücher) or (stadt, städter).
",German Rules,[0],[0]
"Derivational Antonymy: REPEL We assume the following set of standard “antonymy” prefixes in German: APde = {’un’, ’nicht’, ’anti’, ’ir’, ’in’,
’miss’}.",German Rules,[0],[0]
"We rely on the following derivational rules to extract REPEL pairs in German: - w2 = ap + w1, where ap ∈",German Rules,[0],[0]
APde.,German Rules,[0],[0]
"This rule yields constraints such as (aktiv, inaktiv), (wandelbar, unwandelbar) or (zyklone, antizyklone).",German Rules,[0],[0]
-,German Rules,[0],[0]
"If w1.ew(’voll’), then w2 = w1[: −4] + ’los’.",German Rules,[0],[0]
"This rule yields constraints such as (geschmackvoll, geschmacklos).
",German Rules,[0],[0]
"The set of REPEL is then again transitively expanded yielding pairs such as (relevant, irrelevanter) or (aktivem, inaktiv).",German Rules,[0],[0]
"Inflectional Synonymy: ATTRACT The first set of rules aims at capturing the regular plural forming in Italian (e.g., libro, libri) and regular differences in gender (e.g., rapido, rapida).",Italian Rules,[0],[0]
"We rely on the simple heuristic which can be expressed as follows: - If w1.ew(’a’/’e’/’o’/’i’), then generate a set of words Ww1 =",Italian Rules,[0],[0]
{w2|w2 = w1,Italian Rules,[0],[0]
"[: −1] + ’a’/’e’/’o’/’i’}, and take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Italian Rules,[0],[0]
"This rule yields pairs such as (nero, neri) or (generazione, generazioni).",Italian Rules,[0],[0]
-,Italian Rules,[0],[0]
"If w1.ew(’ga’/’ca’), then w2 = w1 + ’he’.",Italian Rules,[0],[0]
"This rule generates pairs such as (tartaruga, tartarughe) or (bianca, bianche).",Italian Rules,[0],[0]
-,Italian Rules,[0],[0]
"If w1.ew(’go’), then w2 = w1 + ’hi’.",Italian Rules,[0],[0]
"This rule generates pairs such as (albergo, alberghi).
",Italian Rules,[0],[0]
The second set of rules targets regular verb conjugation in Italian and the formation of regular past participles.,Italian Rules,[0],[0]
"The following rules are used: - If w1.ew(’are’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −3] + ’iamo’/’ate’/’ano’/’o’/’i’/’a’/’ato’/’ata’/’ati’/’ate’}; take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Italian Rules,[0],[0]
"This rule results in pairs such as (aspettare, aspettiamo).",Italian Rules,[0],[0]
-,Italian Rules,[0],[0]
"If w1.ew(’ere’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −3] + ’iamo’/’ete’/’ono’/’o’/’i’/’e’/’uto’/’uta’/’uti’/’ute’}; take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Italian Rules,[0],[0]
"This rule results in pairs such as (ricevere, ricevete) or (riceve, ricevuto).",Italian Rules,[0],[0]
-,Italian Rules,[0],[0]
"If w1.ew(’ire’), then generate a set of words Ww1 = {w1, w2|w2 = w1",Italian Rules,[0],[0]
[: −3] + ’iamo’/’ite’/’ono’/’o’/’i’/’e’/’ito’/’ita’/’iti’/’ite’}; take the Cartesian product on Ww1 × Ww1 discarding pairs with identical words.,Italian Rules,[0],[0]
"This rule results in pairs such as (dormire, dormono) or (dormi, dormita).
",Italian Rules,[0],[0]
"Derivational Antonymy: REPEL We assume the following set of standard “antonymy” prefixes: APit = {’in’, ’ir’, ’im’, ’anti’}.",Italian Rules,[0],[0]
"The following derivational rule is used to extract REPEL pairs: - w2 = ap + w1, where ap ∈ APit.",Italian Rules,[0],[0]
"This rule yields constraints such as (attivo, inattivo) or (rispettosa, irrispettosa).
",Italian Rules,[0],[0]
"The set of REPEL was then expanded as before, e.g., with additional pairs such as (rispettosa, irrispettosi) generated.",Italian Rules,[0],[0]
Inflectional Synonymy: ATTRACT The first set of rules in Russian targets the regular forming of plural in Russian.,Russian Rules,[0],[0]
A few simple heuristics are used as follows: - w2 = w1 + ’и’/’ы’.,Russian Rules,[0],[0]
"This rule yields pairs such as (aльбом, aльбомы), transliterated as: (al’bom, al’bomy).",Russian Rules,[0],[0]
"- if w1.ew(’a’/’я’/’ь’), then w2 = w1",Russian Rules,[0],[0]
[: −1] + ’и’/’ы’.,Russian Rules,[0],[0]
"This rule generates pairs such as (песня, песни): (pesnja, pesni).",Russian Rules,[0],[0]
"- if w1.ew(’o’), then w2 = w1",Russian Rules,[0],[0]
[: −1] + ’a’.,Russian Rules,[0],[0]
"This rule generates pairs such as (письмо, письма): (pis’mo, pis’ma).",Russian Rules,[0],[0]
"- if w1.ew(’e’), then w2 = w1[: −1] + ’я’.",Russian Rules,[0],[0]
"This rule generates pairs such as (платье, платья): (plat’e, plat’ja).
",Russian Rules,[0],[0]
The next set of rules targets regular verb conjugation of Russian verbs as well as the regular formation of past participles.,Russian Rules,[0],[0]
"We again build a simple heuristic to extract ATTRACT pairs: - if w1.ew(’ти’/’ть’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −2] + ’у’/’ю’/’ешь’/’ишь’/’ет’/’ит’/’ем’/’им’, w2 = w1[: −2]+ ’ете’/ите’/’ут’/’ют’/’ат’/’ят’, w2 = w1[: −2] + ’нный’/’нная’} and take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (варить, варите) or (заканчиваю, заканчивают), transliterated as: (varit’, varite), (zakanchivaju, zakanchivajut).
",Russian Rules,[0],[0]
"Following that, we also utilise the regularities regarding declension processes in Russian, captured by the following rules: - if w1.ew(’a’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −1] + ’e’/’y’/’ой’} and take the Cartesian product on Ww1 × Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (работа, работой): (rabota, rabotoj).",Russian Rules,[0],[0]
"- if w1.ew(’я’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −1] + ’e’/’ю’/’ей’} and take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (линия, линию): (linija, liniju).",Russian Rules,[0],[0]
"- if w1.ew(’ы’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −1] + ’ам’/’ами’/’ах’} and take the Cartesian product
on Ww1 × Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (работам, работами): (rabotam, rabotami).",Russian Rules,[0],[0]
"- if w1.ew(’и’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −1] + ’ь’/’ям’/’ями’/’ях’} and take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (работам, работами): (rabotam, rabotami).
",Russian Rules,[0],[0]
"Yet another set of rules targets regular adjective comparison and gender: - if w1.ew(’ый’/’ой’/’ий’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −2] + ’ь’/’ее’/’ые’}.",Russian Rules,[0],[0]
"This rule yields pairs such as (быстрый, быстрее): (bystryj, bystree).",Russian Rules,[0],[0]
"- if w1.ew(’ая’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −2] + ’ее’/’ыe’/’ый’}.",Russian Rules,[0],[0]
"This rule yields pairs such as (новая, новыe): (novaja, novye).",Russian Rules,[0],[0]
"- if w1.ew(’oe’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −2] + ’ый’/’ыe’/’ая’}.",Russian Rules,[0],[0]
"This rule yields pairs such as (новое, новый): (novoe, novyj).
",Russian Rules,[0],[0]
Derivational Antonymy: REPEL We assume the following set of standard “antonymy” prefixes in Russian:,Russian Rules,[0],[0]
"APru = {не, анти’}, and simply use the following rule: - w2 = ap + w1, where ap ∈ APru.",Russian Rules,[0],[0]
"This rule yields constraints such as (адекватный, неадекватный) or (вирусная, антивирусная), transliterated as: (adekvatnyj, neadekvatnyj) and (virusnaja, antivirusnaja).
",Russian Rules,[0],[0]
"The further expansion of REPEL constraints yields pairs such as (адекватный, неадекватная): (adekvatnyj, neadekvatnaja).",Russian Rules,[0],[0]
We stress that the listed rules for all four languages are non-exhaustive and do not cover all possible inflectional and derivational morphological phenomena.,Further Discussion,[0],[0]
"More linguistic constraints may be extracted by resorting to more sophisticated rules covering finer-grained morphological processes (e.g., covering irregular plural forming or irregular verb conjugation and past participle forming, or non-standard declensions).",Further Discussion,[0],[0]
"Further, the listed rules, written by non-native speakers without any linguistic training in a very short time span, do not necessarily rely on established linguistic theories in each language, but are rather simple heuristics aiming to capture morphological regularities.",Further Discussion,[0],[0]
Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures.,abstractText,[0],[0]
"These effects are detrimental for language understanding systems, which may infer that inexpensive is a rephrasing for expensive or may not associate acquire with acquires.",abstractText,[0],[0]
"In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces.",abstractText,[0],[0]
"Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart.",abstractText,[0],[0]
"In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection.",abstractText,[0],[0]
"Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.",abstractText,[0],[0]
Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"SMT from a morphologically poor language like English into a language with richer morphology continues to be a problem, in particular when training data is sparse and/or the SMT system has insufficient modeling capabilities for morphological variation in the target language.",1 Introduction,[0],[0]
"Most previous approaches to this problem have utilized a translate-and-inflect method, where a first-pass SMT system is trained on lemmatized forms, and the correct inflection for every word is predicted in a second pass by statistical classifiers trained on a combination of source and target language features.",1 Introduction,[0],[0]
"This paper looks at morphological modeling from a different perspective, namely to improve SMT in a real-time speech-
to-speech translation system.",1 Introduction,[0],[0]
Our focus is on resolving those morphological translation errors that are most likely to cause confusions and misunderstandings in machine-translation mediated human-human dialogs.,1 Introduction,[0],[0]
"Due to the constraints imposed by a realtime system, previous approaches that rely on elaborate feature sets and multi-pass processing strategies are unsuitable for this problem.",1 Introduction,[0],[0]
The language pair of interest in this study is English and Iraqi Arabic (IA).,1 Introduction,[0],[0]
The latter is a spoken dialect of Arabic with few existing linguistic resources.,1 Introduction,[0],[0]
We therefore develop a low-resource approach that relies on sourceside dependency parses only.,1 Introduction,[0],[0]
We analyze its performance in combination with different types of parsers and different translation models.,1 Introduction,[0],[0]
Results show a significant improvement in translation performance in both automatic and manual evaluations.,1 Introduction,[0],[0]
"Moreover, the proposed method is sufficiently fast for a realtime system.",1 Introduction,[0],[0]
"Much work in SMT has addressed the issue of translating from morphologically-rich languages by preprocessing the source and/or target data by e.g., stemming and morphological decomposition (Popovic and Ney, 2004; Goldwater and McClosky, 2005), compound splitting (Koehn and Knight, 2003), or various forms of tokenization (Lee, 2004; Habash and Sadat, 2006).",2 Prior Work,[0],[0]
"In (Minkov et al., 2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages.",2 Prior Work,[0],[0]
"A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the
995
machine translation output from a first-pass system, conditioned on a set of lexical, morphological and syntactic features.",2 Prior Work,[0],[0]
"More recently, (Chahuneau et al., 2013) applied a similar translate-and-inflect approach, utilizing unsupervised in addition to supervised morphological analyses.",2 Prior Work,[0],[0]
"Inflection generation models were also used by (Fraser et al., 2012; Weller et al., 2013) for translation into German, and by (El Kholy and Habash, 2012) for Modern Standard Arabic.",2 Prior Work,[0],[0]
"(Sultan, 2011) added both syntactic information on the source side that was used in filtering the phrase table, plus postprocessing on the target side for English-Arabic translation.",2 Prior Work,[0],[0]
"Still other approaches enrich the translation system with morphology-aware feature functions or specific agreement models (Koehn and Hoang, 2007; Green and DeNero, 2012; Williams and Koehn, 2011).
",2 Prior Work,[0],[0]
"In contrast to the above studies, which have concentrated on text translation, this paper focuses on spoken language translation within a bilingual human-human dialog system.",2 Prior Work,[0],[0]
"Thus, our main goal is not to predict the correct morphological form of every word, but to prevent communication errors resulting from the mishandling of morphology.",2 Prior Work,[0],[0]
The intended use in a real-time dialog system imposes additional constraints on morphological modeling: any proposed approach should not add a significant computational burden to the overall system that might result in delays in translation or response generation.,2 Prior Work,[0],[0]
"Our goal is also complicated by the fact that our target language is a spoken dialect of Arabic, for which few linguistic resources (training data, lexicons, morphological analyzers) exist.",2 Prior Work,[0],[0]
"Lastly, Arabic written forms are morphologically highly ambiguous due to the lack of short vowel markers that signal grammatical categories.",2 Prior Work,[0],[0]
The first step in the dialog system used for this study consists of an automatic speech recognition (ASR) component that produces ASR hypotheses for the user’s speech input.,3 Dialog System and Analysis,[0],[0]
Several error detection modules then identify likely out-of-vocabulary and misrecognized words.,3 Dialog System and Analysis,[0],[0]
"This information is used by a clarification module that asks the user to rephrase these error segments; another module then combines the user’s answers into a merged, corrected representa-
tion before sending it to the translation engine.",3 Dialog System and Analysis,[0],[0]
"A machine translation error detection module analyzes the translation to check for errors, such as unknown words.",3 Dialog System and Analysis,[0],[0]
"If an error is found, another clarification subdialog is initiated; otherwise, the translation is sent to a text-to-speech engine to produce the acoustic output in the other language.",3 Dialog System and Analysis,[0],[0]
A schematic representation is shown in Figure 1.,3 Dialog System and Analysis,[0],[0]
"More details about the system can be found in (et al., 2013).",3 Dialog System and Analysis,[0],[0]
The system was evaluated in live mode with native IA speakers as part of the DARPA BOLT Phase-II benchmark evaluations.,3 Dialog System and Analysis,[0],[0]
The predefined scenarios included military and humanitarian assistance/disaster relief scenarios as well as general topics.,3 Dialog System and Analysis,[0],[0]
"All system interactions were logged and evaluated by bilingual human assessors.
",3 Dialog System and Analysis,[0],[0]
"During debriefing sessions with the users, some users voiced dissatisfaction with the translation quality, and a subsequent detailed error analysis was conducted on the logs of 30 interactions.",3 Dialog System and Analysis,[0],[0]
"Similar to previous studies (Condon et al., 2010)",3 Dialog System and Analysis,[0],[0]
we found that a frequently recurring problem was wrong morphological verb forms in the IA output.,3 Dialog System and Analysis,[0],[0]
Some examples are shown in Table 1.,3 Dialog System and Analysis,[0],[0]
"In Example 1, to make sure should be translated by a first-person plural verb but it is translated by a second-person plural form, changing the meaning to (you (pl.) make sure).",3 Dialog System and Analysis,[0],[0]
The desired verb form would be ntAkd.,3 Dialog System and Analysis,[0],[0]
"Similarly, in Example 2 the translation of transport should agree with the translations of someone and the preceding
auxiliary verb can (yqdr).",3 Dialog System and Analysis,[0],[0]
The correct form would be yqlk (he/she transports you) instead of nqlk (we transport you).,3 Dialog System and Analysis,[0],[0]
Such translation errors are confusing to users as they affect the understanding of basic semantic roles.,3 Dialog System and Analysis,[0],[0]
They tend to occur when translating English infinitival constructions (to+verb) or other syntactic constructions where English base verb forms need to be translated by a finite verb in IA.,3 Dialog System and Analysis,[0],[0]
"In these cases, explicit morphological features like person and number are required in Arabic but they are lacking in the English input.",3 Dialog System and Analysis,[0],[0]
An analysis of the SMT component showed that morphological translation errors primarily occur when a head word and its dependent (such as a verbal head and its subject noun dependent) are translated as part of different phrases or rules.,4 Approach,[0],[0]
"In that case, insufficient context is available to produce the correct translation.",4 Approach,[0],[0]
Our approach is to annotate syntactic dependencies on the source side using a statistical parser.,4 Approach,[0],[0]
"Based on the resulting dependency structures the source-side data is then tagged with explicit morphological verbal features using deterministic rules (e.g., subject nouns assign their person/number features to their verbal heads), and a new translation model is trained on this data.",4 Approach,[0],[0]
Our assumption is that words tagged with explicit morphological features will be aligned with their correct translations during training and will thus produce correctly inflected forms during testing even when the syntactic context is not available in the same phrase/rule.,4 Approach,[0],[0]
"For instance, the input sentence in Example 1 in Table 1 would be annotated as: you need-2sg to tell-2sg the locals to evacuate-3pl the area",4 Approach,[0],[0]
so we can-1pl secure-1pl the area to make1pl sure no one gets-3sg hurt.,4 Approach,[0],[0]
"This approach avoids the costly extraction of multiple features, subsequent statistical classification, and inflection generation during run time; moreover, it
does not require target-side annotation tools, an advantage when dealing with under-resourced spoken dialects.",4 Approach,[0],[0]
"There are, however, several potential issues with this approach.",4 Approach,[0],[0]
"First, introducing tags fragments the training data: the same word may receive multiple different tags, either due to genuine ambiguity or because of parser errors.",4 Approach,[0],[0]
"As a result, word alignment and phrase extraction may suffer from data sparsity.",4 Approach,[0],[0]
"Second, new word-tag combinations in the test data that were not observed in the training data will not have an existing translation.",4 Approach,[0],[0]
"Third, the performance of the model is highly dependent on the accuracy of the parser.",4 Approach,[0],[0]
"Finally, we make the assumption that the expression of person and number categories are matched across source and target language – in practice, we have indeed seen very few mismatched cases where e.g., a singular noun phrase in English is translated by a plural noun phrase in IA (see Section 6 below).
",4 Approach,[0],[0]
To address the first point the morph-tagged translation model can be used in a backoff procedure rather than as an alternative model.,4 Approach,[0],[0]
"In this case the baseline model is used by default, and the morphtagged model is only used whenever heads and dependents are translated as part of different phrases.",4 Approach,[0],[0]
Unseen translations for particular word-tag combinations in the test set could in principle be addressed by using a morphological analyzer to generate novel word forms with the desired inflections.,4 Approach,[0],[0]
"However, this would require identifying the correct stem for the word in question, generating all possible morphological forms, and either selecting one or providing all options to the SMT system, which again increases system load.",4 Approach,[0],[0]
We analyzed unseen word-tag combination in the test data but found that their percentage was very small (< 1%).,4 Approach,[0],[0]
"Thus, for these forms we back off to the untagged counterparts rather than generating new inflected forms.",4 Approach,[0],[0]
"To obtain better insight into the effect of parsing accuracy we compared the performance of two parsers in our
annotation pipeline: the Stanford parser (de Marneffe et al., 2006) (version 3.3.1) and the Macaon parser (Nasr et al., 2014).",4 Approach,[0],[0]
"The latter is an implementation of graph-based parsing (McDonald et al., 2005) where a projective dependency tree maximizing a score function is sought in the graph of all possible trees using dynamic programming.",4 Approach,[0],[0]
"It uses a 1st-order decoder, which is more robust to speech input as well as out-of-domain training data.",4 Approach,[0],[0]
"The features implemented reflect those of (Bohnet, 2010) (based on lexemes and part-of-speech tags).",4 Approach,[0],[0]
"The parser was trained on Penn-Treebank data transformed to match speech (lower-cased, no punctuation), with one iteration of self-training on the Transtac training set.",4 Approach,[0],[0]
"We also use the combination of both parsers, where source words are only tagged if the tags derived independently from each parser agree with each other.",4 Approach,[0],[0]
Development experiments were carried out on the Transtac corpus of dialogs in the military and medical domain.,5 Data and Baseline Systems,[0],[0]
"The number of sentence pairs is 762k for the training set, 6.9k for the dev set, 2.8k for eval set 1, and 1.8k for eval set 2.",5 Data and Baseline Systems,[0],[0]
"Eval set 1 has one reference per sentence, eval set 2 has four references.",5 Data and Baseline Systems,[0],[0]
"For the development experiments we used a phrase-based Moses SMT system with a hierarchical reordering model, tested on Eval set 1.",5 Data and Baseline Systems,[0],[0]
The language model was a backoff 6-gram model trained using Kneser-Ney discounting and interpolation of higher- and lower-order n-grams.,5 Data and Baseline Systems,[0],[0]
In addition to automatic evaluation we performed manual analyses of the accuracy of verbal features in the IA translations on a subset of 65 sentences (containing 143 verb forms) from the live evaluations described above.,5 Data and Baseline Systems,[0],[0]
"This analysis counts a verb form as correct if its morphological features for person and number are correct, although it may have the wrong lemma (e.g., wrong word sense).",5 Data and Baseline Systems,[0],[0]
The development experiments were designed to identify the setup that produces the highest verbal inflection accuracy.,5 Data and Baseline Systems,[0],[0]
"For final testing we used a more advanced SMT engine on Eval set 2.This system is the one used in the real-time dialog system; it contains a hierarchical phrase-based translation model, sparse features, and a neural network joint model (NNJM) (Devlin et al., 2014).",5 Data and Baseline Systems,[0],[0]
"Results in Table 2 show the comparison between the baseline, different parsers, and the combined system.",6 Experiments and Results,[0],[0]
We see that verbal inflection accuracy increases substantially from the baseline performance and is best for the Macaon parser.,6 Experiments and Results,[0],[0]
"Improvements over the baseline system without morphology are statistically significant; differences between the individual parsers are not (not, however, that the sample size for manual evaluation was quite small).
",6 Experiments and Results,[0],[0]
"BLEU is not affected negatively but even increases slightly - thus, data fragmentation does not seem to be a problem overall.",6 Experiments and Results,[0],[0]
"This may be due to the nature of the task and domain, which is results in fairly short, simple sentence constructions that can be adequately translated by a concatenation of shorter phrases rather than requiring longer phrases.",6 Experiments and Results,[0],[0]
Back-off systems (indicated by bo) and the combined system improve BLEU only trivially while decreasing verbal inflection accuracy by varying amounts.,6 Experiments and Results,[0],[0]
For testing within the dialog system we thus choose the Macaon parser and utilize a standard translation model rather than a backoff model.,6 Experiments and Results,[0],[0]
An added benefit is that the Macaon parser is already used in other components in the dialog system.,6 Experiments and Results,[0],[0]
"Using this setup we ran two experiments with dialog system’s SMT engine: first, we re-extracted phrases and rules based on the morph-tagged data and reoptimized the feature weights.",6 Experiments and Results,[0],[0]
"In the second experiment, we additionally applied the NNJM to the morph-tagged source text.",6 Experiments and Results,[0],[0]
To this end we include all the morphological variants of the original vocabulary that was used for the NNJM in the untagged baseline system.,6 Experiments and Results,[0],[0]
Table 3 shows the results.,6 Experiments and Results,[0],[0]
"The morph-tagged data improves the BLEU score under both conditions: in Experiment 1, the improve-
ment is almost a full BLEU point (0.91); in Experiment 2 the improvement is even larger (1.13), even though the baseline performance is stronger.",6 Experiments and Results,[0],[0]
"Both results are statistically significant at p = 0.05, using a paired bootstrap resampling test.",6 Experiments and Results,[0],[0]
"The combination of morph-tagged data and the more advanced modeling options (sparse features, NNJM) in this system seem to be beneficial.",6 Experiments and Results,[0],[0]
Improved translation performance may also be captured by the four reference translations as opposed to one in Eval set 1.,6 Experiments and Results,[0],[0]
"In order to assess the added computation cost
of our procedure we computed the decoding speed of the MT component in the dialog system for both the baseline and the morpho-tag systems.",6 Experiments and Results,[0],[0]
"In the baseline MT system (with NNJM) without morphotags, decoding takes 0.01572 seconds per word or 0.15408 seconds per sentence – these numbers were obtained on a Dell Precision M4800",6 Experiments and Results,[0],[0]
Laptop with a quad-core Intel i7-4930MX Processor and 32GB of RAM.,6 Experiments and Results,[0],[0]
Morpho-tagging only adds 0.00031 seconds per word or 0.0024 seconds per sentence.,6 Experiments and Results,[0],[0]
"Thus, our procedure is extremely efficient.
",6 Experiments and Results,[0],[0]
"An analysis of the remaining morphological translation errors not captured by our approach showed that in about 34% of all cases these were due to part-of-speech tagging or parser errors, i.e. verbs were mistagged as nouns rather than verbs and thus did not receive any morphological tags, or the parser hypothesized wrong dependency relations.",6 Experiments and Results,[0],[0]
In 53% of the cases the problem is the lack of more extensive discourse or contextual knowledge.,6 Experiments and Results,[0],[0]
"This includes constructions where there is no overt subject for a verb in the current utterance, and the appropriate underlying subject must be inferred from the preceding discourse or from knowledge of the situational context.",6 Experiments and Results,[0],[0]
"This is an instance of the more general problem of control (see e.g.,(Landau, 2013) for an overview of research in this area).",6 Experiments and Results,[0],[0]
It is exemplified by cases such as the following: 1.,6 Experiments and Results,[0],[0]
"The first step is to make sure that all personnel
are in your debrief.",6 Experiments and Results,[0],[0]
"Here, the underlying subject of “to make sure” could be a range of different candidates (I, you, we, etc.) and must be inferred from context.",6 Experiments and Results,[0],[0]
2.,6 Experiments and Results,[0],[0]
I can provide up to one platoon to help you guys cordon off the area.,6 Experiments and Results,[0],[0]
"In this case the statistical parser identified I as the subject of help, but platoon is more likely to be the controller and was in fact identified as the underlying subject by the annotator.",6 Experiments and Results,[0],[0]
"Such cases could potentially be resolved during the parsing step by integrating semantic information, e.g. as in (Bansal et al., 2014).",6 Experiments and Results,[0],[0]
"However, initial investigations with semantic features in the Macaon parser resulted in a significant slow-down of the parser.",6 Experiments and Results,[0],[0]
"In other cases, more sophisticated modeling of the entities and their relationships in the situational context will be required.",6 Experiments and Results,[0],[0]
"This clearly is an area for future study.
",6 Experiments and Results,[0],[0]
"Finally, in 13% of the cases, mistranslations are caused by a mismatch of number features across languages (e.g. number features for nouns such as family or people).",6 Experiments and Results,[0],[0]
We have shown that significant gains in BLEU and verbal inflection accuracy in speech-to-speech translation for English-IA can be achieved by incorporating morphological tags derived from dependency parse information in the source language.,7 Conclusion,[0],[0]
"The proposed method is fast, low-resource, and can easily be incorporated into a real-time dialog system.",7 Conclusion,[0],[0]
It adds negligible computational cost and does not require any target-language specific annotation tools.,7 Conclusion,[0],[0]
"Possible areas for future study include the use of discourse or and other contextual information to determine morphological agreement, application to other languages pairs/morphological agreement types, and learning the annotation rules from data.",7 Conclusion,[0],[0]
This study was funded by the Defense Advanced Research Projects Agency (DARPA) under contract HR0011-12-C-0016 - subcontract 19-000234.,Acknowledgments,[0],[0]
This paper addresses the problem of morphological modeling in statistical speech-tospeech translation for English to Iraqi Arabic.,abstractText,[0],[0]
An analysis of user data from a real-time MT-based dialog system showed that generating correct verbal inflections is a key problem for this language pair.,abstractText,[0],[0]
We approach this problem by enriching the training data with morphological information derived from sourceside dependency parses.,abstractText,[0],[0]
We analyze the performance of several parsers as well as the effect on different types of translation models.,abstractText,[0],[0]
"Our method achieves an improvement of more than a full BLEU point and a significant increase in verbal inflection accuracy; at the same time, it is computationally inexpensive and does not rely on target-language linguistic tools.",abstractText,[0],[0]
Morphological Modeling for Machine Translation of English-Iraqi Arabic Spoken Dialogs,title,[0],[0]
"ar X
iv :1
91 1.
04 91
6v 2
[ cs
.C L
] 1
2 Fe
b 20
21 Appeared in the proceedings of EMNLP 2016 (Austin, November). This version was
Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure— especially in the case of derivational morphology. In this work, we introduce a discriminative, joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.1",text,[0],[0]
"In NLP, supervised morphological segmentation has typically been viewed as either a sequence-labeling or a segmentation task (Ruokolainen et al., 2016).",1 Introduction,[0],[0]
"In contrast, we consider a hierarchical approach, employing a context-free grammar (CFG).",1 Introduction,[0],[0]
"CFGs provide a richer model of morphology: They capture (i) the intuition that words themselves have internal constituents, which belong to different categories, as well as (ii) the order in which affixes are attached.",1 Introduction,[0],[0]
"Moreover, many morphological processes, e.g., compounding and reduplication, are best modeled as hierarchical; thus, context-free models are expressively more appropriate.
",1 Introduction,[0],[0]
"The purpose of morphological segmentation is to decompose words into smaller units, known as morphemes, which are typically taken to be the smallest meaning-bearing units in language.
",1 Introduction,[0],[0]
"1We found post publication that CELEX (Baayen et al., 1993) has annotated words for hierarchical morphological segmentation as well.
",1 Introduction,[0],[0]
This work concerns itself with modeling hierarchical structure over these morphemes.,1 Introduction,[0],[0]
Note a simple flat morphological segmentation can also be straightforwardly derived from the CFG parse tree.,1 Introduction,[0],[0]
"Segmentations have found use in a diverse set of NLP applications, e.g., automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and Çetinoğlu, 2015).",1 Introduction,[0],[0]
"In contrast to prior work, we focus on canonical segmentation, i.e., we seek to jointly model orthographic changes and segmentation.",1 Introduction,[0],[0]
"For instance, the canonical segmentation of untestably is un+test+able+ly, where we map ably to able+ly, restoring the letters le.
",1 Introduction,[0],[0]
We make two contributions: (i) We introduce a joint model for canonical segmentation with a CFG backbone.,1 Introduction,[0],[0]
We experimentally show that this model outperforms a semi-Markov model on flat segmentation.,1 Introduction,[0],[0]
"(ii) We release the first morphology treebank, consisting of 7454 English word types, each annotated with a full constituency parse.",1 Introduction,[0],[0]
Why should we analyze morphology hierarchically?,2 The Case For Hierarchical Structure,[0],[0]
"It is true that we can model much of morphology with finite-state machinery (Beesley and Karttunen, 2003), but there are, nevertheless, many cases where hierarchical structure appears requisite.",2 The Case For Hierarchical Structure,[0],[0]
"For instance, the flat segmentation of the word untestably7→un+test+able+ly is missing important information about how the word was derived.",2 The Case For Hierarchical Structure,[0],[0]
"The correct parse [[un[[test]able]]ly], on the other hand, does tell us that this is the order in which the complex form was derived:
test able 7−−→testable un 7−→untestable ly 7−→untestably.
",2 The Case For Hierarchical Structure,[0],[0]
"This gives us insight into the structure of the
lexicon—we expect that the segment testable exists as an independent word, but ably does not.
",2 The Case For Hierarchical Structure,[0],[0]
"Moreover, a flat segmentation is often semantically ambiguous.",2 The Case For Hierarchical Structure,[0],[0]
There are two potentially valid readings of untestably depending on how the negative prefix un scopes.,2 The Case For Hierarchical Structure,[0],[0]
The correct tree (see Figure 1) yields the reading “in the manner of not able to be tested.”,2 The Case For Hierarchical Structure,[0],[0]
A second—likely infelicitous reading—where the segment untest forms a constituent yields the reading “in a manner of being able to untest.”,2 The Case For Hierarchical Structure,[0],[0]
"Recovering the hierarchical structure allows us to select the correct reading; note there are even cases of true ambiguity; e.g., unlockable has two readings: “unable to be locked” and “able to be unlocked.”
",2 The Case For Hierarchical Structure,[0],[0]
"We also note that theoretical linguists often implicitly assume a context-free treatment of word formation, e.g., by employing brackets to indicate different levels of affixation.",2 The Case For Hierarchical Structure,[0],[0]
"Others have explicitly modeled word-internal structure with grammars (Selkirk, 1982; Marvin, 2002).",2 The Case For Hierarchical Structure,[0],[0]
"A novel component of this work is the development of a discriminative parser (Finkel et al., 2008; Hall et al., 2014) for morphology.",3 Parsing the Lexicon,[0],[0]
"The goal is to define a probability distribution over all trees that could arise from the input word, after reversal of orthographic and phonological processes.",3 Parsing the Lexicon,[0],[0]
We employ the simple grammar shown in Table 1.,3 Parsing the Lexicon,[0],[0]
"Despite its simplicity, it models the order in which morphemes are attached.
",3 Parsing the Lexicon,[0],[0]
"More formally, our goal is to map a surface form w (e.g., w=untestably) into its underlying canonical form u (e.g., u=untestablely) and then into a parse tree t over its morphemes.",3 Parsing the Lexicon,[0],[0]
"We assume u,w ∈ Σ∗, for some discrete alphabet Σ.2 Note
2For efficiency, we assume u ∈ Σ|w|+k , k = 5.
that a parse tree over the string implicitly defines a flat segmentation given our grammar—one can simply extract the characters spanned by all preterminals in the resulting tree.",3 Parsing the Lexicon,[0],[0]
"Before describing the joint model in detail, we first consider its pieces individually.",3 Parsing the Lexicon,[0],[0]
"To extract a canonical segmentation (Naradowsky and Goldwater, 2009; Cotterell et al., 2016), we restore orthographic changes that occur during word formation.",3.1 Restoring Orthographic Changes,[0],[0]
"To this end, we define the score function
scoreη(u, a,w) = exp ( g(u, a,w)⊤η )
(1)
where a is a monotonic alignment between the strings u and w.",3.1 Restoring Orthographic Changes,[0],[0]
"The goal is for scoreη to assign higher values to better matched pairs, e.g., (w=untestably, u=untestablely).",3.1 Restoring Orthographic Changes,[0],[0]
"We refer to Dreyer et al. (2008) for a thorough exposition.
",3.1 Restoring Orthographic Changes,[0],[0]
"For ease of computation, we can encode this function as a weighted finite-state machine (WFST) (Mohri et al., 2002).",3.1 Restoring Orthographic Changes,[0],[0]
"This requires, however, that the feature function g factors over the topology of the finite-state encoding.",3.1 Restoring Orthographic Changes,[0],[0]
"Since our model conditions on the word w, the feature function g can extract features from any part of this string.",3.1 Restoring Orthographic Changes,[0],[0]
"Features on the output string, u, however, are more restricted.",3.1 Restoring Orthographic Changes,[0],[0]
"In this work, we employ a bigram model over output characters.",3.1 Restoring Orthographic Changes,[0],[0]
This implies that each state remembers exactly one character: the previous one.,3.1 Restoring Orthographic Changes,[0],[0]
See Cotterell et al. (2014) for details.,3.1 Restoring Orthographic Changes,[0],[0]
We can compute the score for two strings u and w using a weighted generalization of the Levenshtein algorithm.,3.1 Restoring Orthographic Changes,[0],[0]
"Computing the partition function requires a different dynamic program, which runs in O(|w|2 · |Σ|2) time.",3.1 Restoring Orthographic Changes,[0],[0]
"Note that since |Σ| ≈ 26 (lower case English letters), it takes
roughly 262 = 676 times longer to compute the partition function than to score a pair of strings.
",3.1 Restoring Orthographic Changes,[0],[0]
"Our model includes several simple feature tem-
plates, including features that fire on individual edit actions as well as conjunctions of edit actions and characters in the surrounding context.",3.1 Restoring Orthographic Changes,[0],[0]
See Cotterell et al. (2016) for details.,3.1 Restoring Orthographic Changes,[0],[0]
"Next, we need to score an underlying canonical form (e.g., u=untestablely) together with a parse tree (e.g., t=[[un[[test]able]]ly]).",3.2 Morphological Analysis as Parsing,[0],[0]
"Thus, we define the parser score with the following function
scoreω(t, u) = exp


∑
π∈Π(t)
f(π, u)⊤ω

 (2)
where Π(t) is the set of anchored productions in the tree t. An anchored production π is a grammar rule in Chomsky normal form attached to a span, e.g., Ai,k → Bi,jCj,k.",3.2 Morphological Analysis as Parsing,[0],[0]
"Each π is then assigned a weight by the linear function f(π, u)⊤ω, where the function f extracts relevant features from the anchored production as well as the corresponding span of the underlying form u.",3.2 Morphological Analysis as Parsing,[0],[0]
"This model is typically referred to as a weighted CFG (WCFG) (Smith and Johnson, 2007) or a CRF parser.
",3.2 Morphological Analysis as Parsing,[0],[0]
"For f , we define three span features: (i) indicator features on the span’s segment, (ii) an indicator feature that fires if the segment appears in an external corpus3 and (iii) the conjunction of the segment with the label (e.g., PREFIX) of the subtree root.",3.2 Morphological Analysis as Parsing,[0],[0]
"Following Hall et al. (2014), we employ an indicator feature for each production as well as production backoff features.",3.2 Morphological Analysis as Parsing,[0],[0]
"Our complete model is a joint CRF (Koller and Friedman, 2009) where each of
3We use the Wikipedia dump from 2016-05-01.
",4 A Joint Model,[0],[0]
the above scores are factors.,4 A Joint Model,[0],[0]
"We define the following probability distribution over trees, canonical forms and their alignments to the original word
pθ(t,a, u | w) = (3)
1
Zθ(w) scoreω(t, u) · scoreη(u, a,w)
where θ = {ω,η} is the parameter vector and the normalizing partition function as
Zθ(w) = ∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
(4)
∑
t′∈T (u′)
scoreω(t ′, u′) · scoreη(u ′, a, w)
where T (u) is the set of all parse trees for the string u.",4 A Joint Model,[0],[0]
"This involves a sum over all possible underlying orthographic forms and all parse trees for those forms.
",4 A Joint Model,[0],[0]
The joint approach has the advantage that it allows both factors to work together to influence the choice of the underlying form u.,4 A Joint Model,[0],[0]
This is useful as the parser now has access to which words are attested in the language; this helps guide the relatively weak transduction model.,4 A Joint Model,[0],[0]
"On the downside, the partition function Zθ now involves a sum over all strings in Σ|w|+k and all possible parses of each string!",4 A Joint Model,[0],[0]
"Finally, we define the marginal distribution over trees and underlying forms as
pθ(t, u | w) = ∑
a∈A(u,w)
pθ(t, a, u | w) (5)
where A(u,w) is the set of all monotonic alignments between u and w.",4 A Joint Model,[0],[0]
The marginalized form in eq. (5) is our final model of morphological segmentation since we are not interested in the latent alignments a.,4 A Joint Model,[0],[0]
"We use stochastic gradient descent to optimize the log-probability of the training data ∑N
n=1 log pθ(t (n), u(n) | w(n)); this requires the computation of the gradient of the partition function ∇θ logZθ.",4.1 Learning and Inference,[0],[0]
"We may view this gradient as an expectation:
∇θ logZθ(w) =",4.1 Learning and Inference,[0],[0]
"(6)
E(t,a,u)∼pθ(·|w)


∑
π∈Π(t)
f(π, u)⊤ + g(u, a,w)⊤


We provide the full derivation in Appendix A with an additional Rao-Blackwellization step that we make use of in the implementation.",4.1 Learning and Inference,[0],[0]
"While the sum over all underlying forms and trees in eq. (6) may be achieved in polynomial time (using the Bar-Hillel construction), we make use of an importance-sampling estimator, derived by Cotterell et al. (2016), which is faster in practice.",4.1 Learning and Inference,[0],[0]
"Roughly speaking, we approximate the hard-tosample-from distribution pθ by taking samples from an easy-to-sample-from proposal distribution q. Specifically, we employ a pipeline model for q consisting of WFST and then a WCFG sampled from consecutively.",4.1 Learning and Inference,[0],[0]
We then reweight the samples using the unnormalized score from pθ.,4.1 Learning and Inference,[0],[0]
"Importance sampling has found many uses in NLP ranging from language modeling (Bengio et al., 2003) and neural MT (Jean et al., 2015) to parsing (Dyer et al., 2016).",4.1 Learning and Inference,[0],[0]
"Due to a lack of space, we omit the derivation of the importance-sampled approximate gradient.",4.1 Learning and Inference,[0],[0]
We also decode by importance sampling.,4.2 Decoding,[0],[0]
"Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree.",4.2 Decoding,[0],[0]
We believe our attempt to train discriminative grammars for morphology is novel.,5 Related Work,[0],[0]
"Nevertheless, other researchers have described parsers for morphology.",5 Related Work,[0],[0]
Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation.,5 Related Work,[0],[0]
"Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013).",5 Related Work,[0],[0]
"Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algo-
rithm (Baker, 1979).",5 Related Work,[0],[0]
"Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014).",5 Related Work,[0],[0]
"Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure.",6 Morphological Treebank,[0],[0]
A core reason behind this is that—to the best of our knowledge— there are no hierarchically annotated corpora for the task.,6 Morphological Treebank,[0],[0]
"To remedy this, we provide tree annotations for a subset of the English portion of CELEX (Baayen et al., 1993).",6 Morphological Treebank,[0],[0]
We reannotated 7454 English types with a full constituency parse.4,6 Morphological Treebank,[0],[0]
The resource will be freely available for future research.,6 Morphological Treebank,[0],[0]
The annotation of the morphology treebank was guided by three core principles.,6.1 Annotation Guidelines,[0],[0]
The first principle concerns productivity: we exclusively annotate productive morphology.,6.1 Annotation Guidelines,[0],[0]
"In the context of morphology, productivity refers to the degree that native speakers actively employ the affix to create new words (Aronoff, 1976).",6.1 Annotation Guidelines,[0],[0]
"We believe that for NLP applications, we should focus on productive affixation.",6.1 Annotation Guidelines,[0],[0]
"Indeed, this sets our corpus apart from many existing morphologically annotated corpora such as CELEX.",6.1 Annotation Guidelines,[0],[0]
"For example, CELEX contains warmth7→warm+th, but th is not a productive suffix and cannot be used to create new words.",6.1 Annotation Guidelines,[0],[0]
"Thus, we do not want to analyze hearth7→hear+th or, in general, allow wug7→wug+th.",6.1 Annotation Guidelines,[0],[0]
"Second, we annotate for semantic coherence.",6.1 Annotation Guidelines,[0],[0]
"When there are several candidate parses, we choose the one that is best compatible with the compositional semantics of the derived form.
",6.1 Annotation Guidelines,[0],[0]
"Interestingly, multiple trees can be considered valid depending on the linguistic tier of interest.",6.1 Annotation Guidelines,[0],[0]
Consider the word unhappier.,6.1 Annotation Guidelines,[0],[0]
"From a semantic
4In many cases, we corrected the flat segmentation as well.
perspective, we have the parse",6.1 Annotation Guidelines,[0],[0]
[[un [happy]] er] which gives us the correct meaning “not happy to a greater degree.”,6.1 Annotation Guidelines,[0],[0]
"However, since the suffix er only attaches to mono- and bisyllabic words, we get [un[[happy] er]] from a phonological perspective.",6.1 Annotation Guidelines,[0],[0]
"In the linguistics literature, this problem is known as the bracketing paradox (Pesetsky, 1985; Embick, 2015).",6.1 Annotation Guidelines,[0],[0]
"We annotate exclusively at the syntactic-semantic tier.
",6.1 Annotation Guidelines,[0],[0]
"Thirdly, in the context of derivational morphology, we force spans to be words themselves.",6.1 Annotation Guidelines,[0],[0]
"Since derivational morphology—by definition—forms new words from existing words (Lieber and Štekauer, 2014), it follows that each span rooted with WORD or ROOT in the correct parse corresponds to a word in the lexicon.",6.1 Annotation Guidelines,[0],[0]
"For example, consider unlickable.",6.1 Annotation Guidelines,[0],[0]
"The correct parse, under our scheme, is [un [[lick] able]].",6.1 Annotation Guidelines,[0],[0]
"Each of the spans (lick, lickable and unlickable) exists as a word.",6.1 Annotation Guidelines,[0],[0]
"By contrast, the parse [[un [lick]] able] contains the span unlick, which is not a word in the lexicon.",6.1 Annotation Guidelines,[0],[0]
"The span in the segmented form may involve changes, e.g., [un [[achieve] able]], where achieveable is not a word, but achievable (after deleting e) is.",6.1 Annotation Guidelines,[0],[0]
We run a simple experiment to show the empirical utility of parsing words—we compare a WCFG-based canonical segmenter with the semiMarkov segmenter introduced in Cotterell et al. (2016).,7 Experiments,[0],[0]
We divide the corpus into 10 distinct train/dev/test splits with 5454 words for train and 1000 for each of dev and test.,7 Experiments,[0],[0]
"We report three evaluation metrics: full form accuracy, morpheme F1 (Van den Bosch and Daelemans, 1999) and average edit distance to the gold segmentation with boundaries marked by a distinguished symbol.",7 Experiments,[0],[0]
"For the WCFG model, we also report constituent F1— typical for sentential constituency parsing— as a baseline for future systems.",7 Experiments,[0],[0]
This F1 measures how well we predict the whole tree (not just a segmentation).,7 Experiments,[0],[0]
"For all models, we use L2 regularization and run 100 epochs of ADAGRAD (Duchi et al., 2011) with early stopping.",7 Experiments,[0],[0]
"We tune the regularization coefficient by grid search considering λ ∈ {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}.",7 Experiments,[0],[0]
Table 2 shows the results.,7.1 Results and Discussion,[0],[0]
"The hierarchical WCFG model outperforms the flat semi-Markov model on
all metrics on the segmentation task.",7.1 Results and Discussion,[0],[0]
"This shows that modeling structure among the morphemes, indeed, does help segmentation.",7.1 Results and Discussion,[0],[0]
The largest improvements are found under the morpheme F1 metric (≈ 6.5 points).,7.1 Results and Discussion,[0],[0]
"In contrast, accuracy improves by < 1%.",7.1 Results and Discussion,[0],[0]
Edit distance is in between with an improvement of 0.2 characters.,7.1 Results and Discussion,[0],[0]
"Accuracy, in general, is an all or nothing metric since it requires getting every canonical segment correct.",7.1 Results and Discussion,[0],[0]
"Morpheme F1, on the other hand, gives us partial credit.",7.1 Results and Discussion,[0],[0]
"Thus, what this shows us is that the WCFG gets a lot more of the morphemes in the held-out set correct, even if it only gets a few more complete forms correct.",7.1 Results and Discussion,[0],[0]
We provide additional results evaluating the entire tree with constituency F1 as a future baseline.,7.1 Results and Discussion,[0],[0]
We presented a discriminative CFG-based model for canonical morphological segmentation and showed empirical improvements on its ability to segment words under three metrics.,8 Conclusion,[0],[0]
We argue that our hierarchical approach to modeling morphemes is more often appropriate than the traditional flat segmentation.,8 Conclusion,[0],[0]
"Additionally, we have annotated 7454 words with a morphological constituency parse.",8 Conclusion,[0],[0]
"The corpus is available online at
http://ryancotterell.github.io/data/morphological-treebank to allow for exact comparison and to spark future research.",8 Conclusion,[0],[0]
The first author was supported by a DAAD LongTerm Research Grant and an NDSEG fellowship.,Acknowledgements,[0],[0]
The third author was supported by DFG (SCHU 2246/10-1).,Acknowledgements,[0],[0]
"Here we provide the gradient of the log-partition function as an expectation:
∇θ logZθ(w) = 1
Zθ(w) ∇θZθ(w) (7)
= 1
Zθ(w) ∇θ


∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
scoreω(t ′, u′) · scoreη(u ′, a, w)


",A Derivation of Eq. 6,[0],[0]
"= 1
Zθ(w)
∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
∇θ",A Derivation of Eq. 6,[0],[0]
"( scoreω(t ′, u′) · scoreη(u ′, a, w) )
= 1
Zθ(w)
∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
(
scoreη(u ′, a, w) ·",A Derivation of Eq. 6,[0],[0]
"∇ωscoreω(t ′, u′)
+ scoreω(t ′, u′) · ∇ηscoreη(u ′, a, w) )
",A Derivation of Eq. 6,[0],[0]
"= 1
Zθ(w)
∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
scoreη(u ′, a, w) ·",A Derivation of Eq. 6,[0],[0]
"scoreω(t ′, u′)


∑
π∈Π(t′)
f(π, u′)⊤ + g(u′, a, w)⊤


= ∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
scoreη(u ′, a, w) · scoreω(t ′, u′)
Zθ(w)


∑
π∈Π(t′)
f(π, u)⊤ + g(u′, a, w)⊤


= E(t,a,u)∼pθ(·|w)


∑
π∈Π(t)
f(π, u)⊤ + g(u, a,w)⊤

 (8)
The result above can be further improved through Rao-Blackwellization.",A Derivation of Eq. 6,[0],[0]
"In this case, when we sample a tree–underlying form pair (t, u), we marginalize out all alignments that could have given rise to the sampled pair.",A Derivation of Eq. 6,[0],[0]
"The final derivation is show below:
∇θ logZθ(w) = E(t,a,u)∼pθ(·|w)


∑
π∈Π(t)
f(π, u)⊤ + g(u, a,w)⊤


= E(t,u)∼pθ(·|w)


∑
π∈Π(t)
f(π, u)⊤ + ∑
a∈A(u,w)
pθ(a | u,w)g(u, a,w) ⊤

 (9)
",A Derivation of Eq. 6,[0],[0]
This estimator in eq. (9) will have lower variance than eq.,A Derivation of Eq. 6,[0],[0]
(8).,A Derivation of Eq. 6,[0],[0]
"Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output.",abstractText,[0],[0]
"In many cases, however, proper morphological analysis requires hierarchical structure— especially in the case of derivational morphology.",abstractText,[0],[0]
"In this work, we introduce a discriminative, joint model of morphological segmentation along with the orthographic changes that occur during word formation.",abstractText,[0],[0]
"To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model.",abstractText,[0],[0]
"Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.",abstractText,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1066–1076, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
In this paper we study the task of movie script summarization, which we argue could enhance script browsing, give readers a rough idea of the script’s plotline, and speed up reading time. We formalize the process of generating a shorter version of a screenplay as the task of finding an optimal chain of scenes. We develop a graph-based model that selects a chain by jointly optimizing its logical progression, diversity, and importance. Human evaluation based on a question-answering task shows that our model produces summaries which are more informative compared to competitive baselines.",text,[0],[0]
"Each year, about 50,000 screenplays are registered with the WGA1, the Writers Guild of America.",1 Introduction,[0],[0]
Only a fraction of these make it through to be considered for production and an even smaller fraction to the big screen.,1 Introduction,[0],[0]
How do producers and directors navigate through this vast number of scripts available?,1 Introduction,[0],[0]
"Typically, production companies, agencies, and studios hire script readers, whose job is to analyze screenplays that come in, sorting the hopeful from the hopeless.",1 Introduction,[0],[0]
"Having read the script, a reader will generate a coverage report consisting of a logline (one or two sentences describing the story in a nutshell), a synopsis (a two- to three-page long summary of the script), comments explaining its appeal or problematic aspects, and a final verdict as to whether the script merits further consideration.",1 Introduction,[0],[0]
"A script excerpt
1The WGA is a collective term representing US TV and film writers.
from “Silence of the Lambs”, an American thriller released in 1991, is shown in Figure 1.
",1 Introduction,[0],[0]
"Although there are several screenwriting tools for authors (e.g., Final Draft is a popular application which automatically formats scripts to industry standards, keeps track of revisions, allows insertion of notes, and writing collaboratively online), there is a lack of any kind of script reading aids.",1 Introduction,[0],[0]
"Features of such a tool could be to automatically grade the quality of the script (e.g., thumbs up or down), generate
1066
synopses and loglines, identify main characters and their stories, or facilitate browsing (e.g., “show me every scene where there is a shooting”).",1 Introduction,[0],[0]
In this paper we explore whether current NLP technology can be used to address some of these tasks.,1 Introduction,[0],[0]
"Specifically, we focus on script summarization, which we conceptualize as the process of generating a shorter version of a screenplay, ideally encapsulating its most informative scenes.",1 Introduction,[0],[0]
"The resulting summaries can be used to enhance script browsing, give readers a rough idea of the script’s content and plotline, and speed up reading time.
",1 Introduction,[0],[0]
"So, what makes a good script summary?",1 Introduction,[0],[0]
"According to modern film theory, “all films are about nothing — nothing but character” (Monaco, 1982).",1 Introduction,[0],[0]
"Beyond characters, a summary should also highlight major scenes representative of the story and its progression.",1 Introduction,[0],[0]
"With this in mind, we define a script summary as a chain of scenes which conveys a narrative and smooth transitions from one scene to the next.",1 Introduction,[0],[0]
"At the same time, a good chain should incorporate some diversity (i.e., avoid redundancy), and focus on important scenes and characters.",1 Introduction,[0],[0]
We formalize the problem of selecting a good summary chain using a graph-theoretic approach.,1 Introduction,[0],[0]
"We represent scripts as (directed) bipartite graphs with vertices corresponding to scenes and characters, and edge weights to their strength of correlation.",1 Introduction,[0],[0]
"Intuitively, if two scenes are connected, a random walk starting from one would reach the other frequently.",1 Introduction,[0],[0]
"We find a chain of highly connected scenes by jointly optimizing logical progression, diversity, and importance.
",1 Introduction,[0],[0]
"Our contributions in this work are three-fold: we introduce a novel summarization task, on a new text genre, and formalize scene selection as the problem of finding a chain that represents a film’s story; we propose several novel methods for analyzing script content (e.g., identifying important characters and their interactions); and perform a large-scale human evaluation study using a question-answering task.",1 Introduction,[0],[0]
Experimental results show that our method produces summaries which are more informative compared to several competitive baselines.,1 Introduction,[0],[0]
"Computer-assisted analysis of literary text has a long history, with the first studies dating back to the
1960s (Mosteller and Wallace, 1964).",2 Related Work,[0],[0]
"More recently, the availability of large collections of digitized books and works of fiction has enabled researchers to observe cultural trends, address questions about language use and its evolution, study how individuals rise to and fall from fame, perform gender studies, and so on (Michel et al., 2010).",2 Related Work,[0],[0]
"Most existing work focuses on low-level analysis of word patterns, with a few notable exceptions.",2 Related Work,[0],[0]
Elson et al. (2010) analyze 19th century British novels by constructing a conversational network with vertices corresponding to characters and weighted edges corresponding to the amount of conversational interaction.,2 Related Work,[0],[0]
"Elsner (2012) analyzes characters and their emotional trajectories, whereas Nalisnick and Baird (2013) identify a character’s enemies and allies in plays based on the sentiment of their utterances.",2 Related Work,[0],[0]
"Other work (Bamman et al., 2013, 2014) automatically infers latent character types (e.g., villains or heroes) in novels and movie plot summaries.
",2 Related Work,[0],[0]
"Although we are not aware of any previous approaches to summarize screenplays, the field of computer vision is rife with attempts to summarize video (see Reed 2004 for an overview).",2 Related Work,[0],[0]
"Most techniques are based on visual information and rely on low-level cues such as motion, color, or audio (e.g., Rasheed et al. 2005).",2 Related Work,[0],[0]
Movie summarization is a special type of video summarization which poses many challenges due to the large variety of film styles and genres.,2 Related Work,[0],[0]
"A few recent studies (Weng et al., 2009; Lin et al., 2013) have used concepts from social network analysis to identify lead roles and role communities in order to segment movies into scenes (containing one or more shots) and create more informative summaries.",2 Related Work,[0],[0]
A surprising fact about this line of work is that it does not exploit the movie script in any way.,2 Related Work,[0],[0]
Characters are typically identified using face recognition techniques and scene boundaries are presumed unknown and are automatically detected.,2 Related Work,[0],[0]
"A notable exception are Sang and Xu (2010) who generate video summaries for movies, while taking into account character interaction features which they estimate from the corresponding screenplay.
",2 Related Work,[0],[0]
Our own approach is inspired by work in egocentric video analysis.,2 Related Work,[0],[0]
"An egocentric video offers a first-person view of the world and is captured from a wearable camera focusing on the user’s activities,
social interactions, and interests.",2 Related Work,[0],[0]
"Lu and Grauman (2013) present a summarization model which extracts subshot sequences while finding a balance of important subshots that are both diverse and provide a natural progression through the video, in terms of prominent visual objects (e.g., bottle, mug, television).",2 Related Work,[0],[0]
"We adapt their technique to our task, and show how to estimate character-scene correlations based on linguistic analysis.",2 Related Work,[0],[0]
We also interpret movies as social networks and extract a rich set of features from character interactions and their sentiment which we use to guide the summarization process.,2 Related Work,[0],[0]
"We compiled ScriptBase, a collection of 1,276 movie scripts, by automatically crawling web-sites which host or link entire movie scripts (e.g., imsdb.com).",3 ScriptBase: A Movie Script Corpus,[0],[0]
"The retrieved scripts were then cross-matched against Wikipedia2 and IMDB3 and paired with corresponding user-written summaries, plot sections, loglines and taglines (taglines are short snippets used by marketing departments to promote a movie).",3 ScriptBase: A Movie Script Corpus,[0],[0]
"We also collected metainformation regarding the movie’s genre, its actors, the production year, etc.",3 ScriptBase: A Movie Script Corpus,[0],[0]
"ScriptBase contains movies comprising 23 genres; each movie is on average accompanied by 3 user summaries, 3 loglines, and 3 taglines.",3 ScriptBase: A Movie Script Corpus,[0],[0]
The corpus spans years 1909–2013.,3 ScriptBase: A Movie Script Corpus,[0],[0]
"Some corpus statistics are shown in Figure 2.
",3 ScriptBase: A Movie Script Corpus,[0],[0]
"The scripts were further post-processed with the Stanford CoreNLP pipeline (Manning et al., 2014) to perform tagging, parsing, named entity recognition and coreference resolution.",3 ScriptBase: A Movie Script Corpus,[0],[0]
"They were also annotated with semantic roles (e.g., ARG0, ARG1), using the MATE tools (Björkelund et al., 2009).",3 ScriptBase: A Movie Script Corpus,[0],[0]
Our summarization experiments focused on comedies and thrillers.,3 ScriptBase: A Movie Script Corpus,[0],[0]
"We randomly selected 30 movies
2http://en.wikipedia.org 3http://www.imdb.com/
for training/development and 65 movies for testing.",3 ScriptBase: A Movie Script Corpus,[0],[0]
"As mentioned earlier, we define script summarization as the task of selecting a chain of scenes representing the movie’s most important content.",4 The Scene Extraction Model,[0],[0]
We interpret the term scene in the screenplay sense.,4 The Scene Extraction Model,[0],[0]
A scene is a unit of action that takes place in one location at one time (see Figure 1).,4 The Scene Extraction Model,[0],[0]
"We therefore need not be concerned with scene segmentation; scene boundaries are clearly marked, and constitute the basic units over which our model operates.
",4 The Scene Extraction Model,[0],[0]
"Let M = (S,C) represent a screenplay consisting of a set S = {s1,s2, . . .",4 The Scene Extraction Model,[0],[0]
",sn} of scenes, and a set C = {c1, . . .",4 The Scene Extraction Model,[0],[0]
",cm} of characters.",4 The Scene Extraction Model,[0],[0]
"We are interested in finding a list S′ = {si, . .",4 The Scene Extraction Model,[0],[0]
.sk},4 The Scene Extraction Model,[0],[0]
"of ordered, consecutive scenes subject to a compression rate m (see the example in Figure 3).",4 The Scene Extraction Model,[0],[0]
A natural interpretation of m in our case is the percentage of scenes from the original script retained in the summary.,4 The Scene Extraction Model,[0],[0]
"The extracted chain should contain (a) important scenes (i.e., critical for comprehending the story and its development); (b) diverse scenes that cover different aspects of the story; and (c) scenes which highlight the story’s progression from beginning to end.",4 The Scene Extraction Model,[0],[0]
"We therefore find the chain S′ maximizing the objective function Q(S′) which is the weighted sum of three terms: the story progression P, scene diversity D, and scene importance I:
S∗ = argmax S′⊂S Q(S′)",4 The Scene Extraction Model,[0],[0]
"(1) Q(S′) = λPP(S′)+λDD(S′)+λII(S′) (2)
In the following, we define each of the three terms.
",4 The Scene Extraction Model,[0],[0]
Scene-to-scene Progression The first term in the objective is responsible for selecting chains representing a logically coherent story.,4 The Scene Extraction Model,[0],[0]
"Intuitively, this means that if our chain includes a scene where a character commits an action, then scenes involving affected parties or follow-up actions should also be included.",4 The Scene Extraction Model,[0],[0]
"We operationalize this idea of progression in a story in terms of how strongly the characters in a selected scene si influence the transition to the next scene si+1:
P(S′)",4 The Scene Extraction Model,[0],[0]
"= |S′|−1 ∑ i=0 ∑ c∈Ci INF(si,si+1|c) (3)
We represent screenplays as weighted, bipartite graphs connecting scenes and characters:
B = (V,E) : V = C∪S
E = {(s,c,ws,c)|s ∈ S, c ∈C, ws,c ∈",4 The Scene Extraction Model,[0],[0]
"[0,1]}∪ {(c,s,wc,s)|c ∈C, s ∈ S, wc,s ∈",4 The Scene Extraction Model,[0],[0]
"[0,1]}
The set of vertices V corresponds to the union of characters C and scenes S. We therefore add to the bipartite graph one node per scene and one node per character, and two directed edges for each scene-character and character-scene pair.",4 The Scene Extraction Model,[0],[0]
An example of a bipartite graph is shown in Figure 4.,4 The Scene Extraction Model,[0],[0]
"We further assume that two scenes si and si+1 are tightly connected in such a graph if a random walk with restart (RWR; Tong et al. 2006; Kim et al. 2014) which starts in si has a high probability of ending in si+1.
",4 The Scene Extraction Model,[0],[0]
"In order to calculate the random walk stationary distributions, we must estimate the weights between a character and a scene.",4 The Scene Extraction Model,[0],[0]
"We are interested in how important a character is generally in the movie, and
specifically in a particular scene.",4 The Scene Extraction Model,[0],[0]
"For wc,s, we consider the probability of a character being important, i.e., of them belonging to the set of main characters:
wc,s = P(c ∈ main(M)), ∀(c,s,wc,s) ∈ E (4)
where P(c ∈main(M)) is some probability score associated with c being a main character in script M. For ws,c, we take the number of interactions a character is involved in relative to the total number of interactions in a specific scene as indicative of the character’s importance in that scene.",4 The Scene Extraction Model,[0],[0]
"Interactions refer to conversational interactions as well as relations between characters (e.g., who does what to whom):
ws,c = ∑
c′∈Cs inter(c,c′)
∑ c1,c2∈Cs
inter(c1,c2) , ∀(s,c,ws,c) ∈ E (5)
",4 The Scene Extraction Model,[0],[0]
We defer discussion of how we model probability P(c ∈Main(M)) and obtain interaction counts to Section 5.,4 The Scene Extraction Model,[0],[0]
"Weights ws,c and wc,s are normalized:
ws,c = ws,c
∑(s,c′,w′s,c) w ′",4 The Scene Extraction Model,[0],[0]
"s,c
, ∀(s,c,ws,c) ∈ E (6)
wc,s = wc,s
∑(c,s′,w′c,s) w ′",4 The Scene Extraction Model,[0],[0]
"c,s
, ∀(c,s,wc,s) ∈ E (7)
",4 The Scene Extraction Model,[0],[0]
"We calculate the stationary distributions of a random walk on a transition matrix T , enumerating over all vertices v (i.e., characters and scenes) in the bipartite graph B:
T (i, j) = { wi, j if (vi,v j,wi, j ∈ EB) 0",4 The Scene Extraction Model,[0],[0]
"otherwise
(8)
We measure the influence individual characters have on scene-to-scene transitions as follows.",4 The Scene Extraction Model,[0],[0]
"The stationary distribution rk for a RWR walker starting at node k is a vector that satisfies:
rk = (1− ε)Trk + εek (9)
where T is the transition matrix of the graph, ek is a seed vector, with all elements 0, except for element k which is set to 1, and ε is a restart probability parameter.",4 The Scene Extraction Model,[0],[0]
"In practice, our vectors rk and ek are indexed by the scenes and characters in a movie, i.e., they have length |S|+ |C|, and their nth element corresponds either to a known scene or character.",4 The Scene Extraction Model,[0],[0]
"In cases where
graphs are relatively small, we can compute r directly4 by solving:
rk = ε(I− (1− ε)T )−1ek (10)
",4 The Scene Extraction Model,[0],[0]
The lth element of r then equals the probability of the random walker being in state l in the stationary distribution.,4 The Scene Extraction Model,[0],[0]
"Let rck be the same as rk, but with the character node c of the bipartite graph being turned into a sink, i.e., all entries for c in the transition matrix T are 0.",4 The Scene Extraction Model,[0],[0]
"We can then define how a single character influences the transition between scenes si and si+1 as:
INF(si,si+1|c) = rsi",4 The Scene Extraction Model,[0],[0]
"[si+1]− rcsi [si+1] (11)
where rsi",4 The Scene Extraction Model,[0],[0]
[si+1] is shorthand for that element in the vector rsi that corresponds to scene si+1.,4 The Scene Extraction Model,[0],[0]
"We use the INF score directly in Equation (3) to determine the progress score of a candidate chain.
",4 The Scene Extraction Model,[0],[0]
Diversity The diversity term D(S′),4 The Scene Extraction Model,[0],[0]
"in our objective should encourage chains which consist of more dissimilar scenes, thereby avoiding redundancy.",4 The Scene Extraction Model,[0],[0]
"The diversity of chain S′ is the sum of the diversities of its successive scenes:
D(S′) = |S′|−1",4 The Scene Extraction Model,[0],[0]
"∑ i=1 d(si,si+1) (12)
",4 The Scene Extraction Model,[0],[0]
"The diversity d(si,si+1) of two scenes si and si+1 is estimated taking into account two factors: (a) do they have any characters in common, and (b) does the sentiment change from one scene to the next:
d(si,si+1) = dchar(si,si+1)+dsen(si,si+1)
2 (13)
where dchar(si,si+1) and dsen(si,si+1) respectively denote character and sentiment similarity between scenes.",4 The Scene Extraction Model,[0],[0]
"Specifically, dchar(si,si+1) is the relative character overlap between scenes si and si+1:
dchar(si,si+1)",4 The Scene Extraction Model,[0],[0]
=,4 The Scene Extraction Model,[0],[0]
"1− |Csi ∩Csi+1 ||Csi ∪Csi+1 | (14)
dchar will be 0 if two scenes share the same characters and 1 if no characters are shared.",4 The Scene Extraction Model,[0],[0]
"Analogously,
4We could also solve for r recursively which would be preferable for large graphs, since the performed matrix inversion is computationally expensive.
",4 The Scene Extraction Model,[0],[0]
"we define dsen, the sentiment overlap between two scenes as:
dsen(si,si+1)",4 The Scene Extraction Model,[0],[0]
"=1− k ·di f (si,si+1)k− k ·di f (si,si+1)+1 (15) di f (si,si+1) = 1
1+ |sen(si)− sen(si+1)| (16)
where the sentiment sen(s) of scene s is the aggregate sentiment score of all interactions in s:
sen(s) = ∑ c,c′∈Cs
sen(inter(c,c′))",4 The Scene Extraction Model,[0],[0]
"(17)
We explain how interactions and their sentiment are computed in Section 5.",4 The Scene Extraction Model,[0],[0]
"Again, dsen is larger if two scenes have a less similar sentiment.",4 The Scene Extraction Model,[0],[0]
"di f (si,si+1) becomes 1 if the sentiments are identical, and increasingly smaller for more dissimilar sentiments.",4 The Scene Extraction Model,[0],[0]
"The sigmoid-like function in Equation (15) scales dsen within range [0,1] to take smaller values for larger sentiment differences (factor k adjusts the curve’s smoothness).
",4 The Scene Extraction Model,[0],[0]
Importance The score I(S′) captures whether a chain contains important scenes.,4 The Scene Extraction Model,[0],[0]
"We define I(S′) as the sum of all scene-specific importance scores imp(si) of scenes contained in the chain:
I(S′) =",4 The Scene Extraction Model,[0],[0]
|S′|,4 The Scene Extraction Model,[0],[0]
"∑ i=1 imp(si) (18)
",4 The Scene Extraction Model,[0],[0]
"The importance imp(si) of a scene si is the ratio of lead to support characters within that scene:
imp(si) = ∑c: c∈Csi∧c∈main(M) 1
∑c: c∈Csi 1 (19)
where Csi is the set of characters present in scene si, and main(M) is the set of main characters in the movie.5 I(si) is 0 if a scene does not contain any main characters, and 1 if it contains only main characters (see Section 5 for how main(M) is inferred).
",4 The Scene Extraction Model,[0],[0]
Optimal Chain Selection We use Linear Programming to efficiently find a good chain.,4 The Scene Extraction Model,[0],[0]
"The objective is to maximize Equation (2), i.e., the sum of the terms for progress, diversity and importance,
5Whether scenes are important if they contain many main characters is an empirical question in its own right.",4 The Scene Extraction Model,[0],[0]
"For our purposes, we assume that this relation holds.
subject to their weights λ.",4 The Scene Extraction Model,[0],[0]
"We add a constraint corresponding to the compression rate, i.e., the number of scenes to be selected and enforce their linear order by disallowing non-consecutive combinations.",4 The Scene Extraction Model,[0],[0]
We use GLPK6 to solve the linear problem.,4 The Scene Extraction Model,[0],[0]
In this section we discuss several aspects of the implementation of the model presented in the previous section.,5 Implementation,[0],[0]
We explain how interactions are extracted and how sentiment is calculated.,5 Implementation,[0],[0]
"We also present our method for identifying main characters and estimating the weights ws,c and wc,s in the bipartite graph.
",5 Implementation,[0],[0]
Interactions The notion of interaction underlies many aspects of the model defined in the previous section.,5 Implementation,[0],[0]
"For instance, interaction counts are required to estimate the weights ws,c in the bipartite graph of the progression term (see Equation (5)), and in defining diversity (see Equations (15)–(17)).",5 Implementation,[0],[0]
"As we shall see below, interactions are also important for identifying main characters in a screenplay.
",5 Implementation,[0],[0]
"We use the term interaction to refer to conversations between two characters, as well as their relations (e.g., if a character kills another).",5 Implementation,[0],[0]
"For conversational interactions, we simply need to identify the speaker generating an utterance and the listener.",5 Implementation,[0],[0]
"Speaker attribution comes for free in our case, as speakers are clearly marked in the text (see Figure 1).",5 Implementation,[0],[0]
"Listener identification is more involved, especially when there are multiple characters in a scene.",5 Implementation,[0],[0]
We rely on a few simple heuristics.,5 Implementation,[0],[0]
"We assume that the previous speaker in the same scene, who is different from the current speaker, is the listener.",5 Implementation,[0],[0]
"If there is no previous speaker, we assume that the listener is the closest character mentioned in the speaker’s utterance (e.g., via a coreferring proper name or a pronoun).",5 Implementation,[0],[0]
"In cases where we cannot find a suitable listener, we assume the current speaker is the listener.
",5 Implementation,[0],[0]
We obtain character relations from the output of a semantic role labeler.,5 Implementation,[0],[0]
Relations are denoted by verbs whose ARG0 and ARG1 roles are character names.,5 Implementation,[0],[0]
We extract relations from the dialogue but also from scene descriptions.,5 Implementation,[0],[0]
"For example, in Figure 1 the description Suddenly, [...] he
6https://www.gnu.org/software/glpk/
clubs her over the head contains the relation clubs(MAN,CATHERINE).",5 Implementation,[0],[0]
"Pronouns are resolved to their antecedent using the Stanford coreference resolution system (Lee et al., 2011).
",5 Implementation,[0],[0]
"Sentiment We labeled lexical items in screenplays with sentiment values using the AFINN-96 lexicon (Nielsen, 2011), which is essentially a list of words scored with sentiment strength within the range",5 Implementation,[0],[0]
"[−5,+5].",5 Implementation,[0],[0]
The list also contains obscene words (which are often used in movies) and some Internet slang.,5 Implementation,[0],[0]
"By summing over the sentiment scores of individual words, we can work out the sentiment of an interaction between two characters, the sentiment of a scene (see Equation (17)), and even the sentiment between characters (e.g., who likes or dislikes whom in the movie in general).
",5 Implementation,[0],[0]
Main Characters,5 Implementation,[0],[0]
"The progress term in our summarization objective crucially relies on characters and their importance (see the weight wc,s in Equation (4)).",5 Implementation,[0],[0]
"Previous work (Weng et al., 2009; Lin et al., 2013) extracts social networks where nodes correspond to roles in the movie, and edges to their co-occurrence.",5 Implementation,[0],[0]
"Leading roles (and their communities) are then identified by measuring their centrality in the network (i.e., number of edges terminating in a given node).
",5 Implementation,[0],[0]
It is relatively straightforward to obtain a social network from a screenplay.,5 Implementation,[0],[0]
"Formally, for each movie we define a weighted and undirected graph:
G = {C,E}, : C = {c1, . .",5 Implementation,[0],[0]
.cn},5 Implementation,[0],[0]
", E = {(ci,c j,w)|ci,c j ∈C, w ∈",5 Implementation,[0],[0]
"N>0}
where vertices correspond to movie characters7, and edges denote character-to-character interactions.",5 Implementation,[0],[0]
Figure 5 shows an example of a social network for “The Silence of the Lambs”.,5 Implementation,[0],[0]
"Due to lack of space, only main characters are displayed, however the actual graph contains all characters (42 in this case).",5 Implementation,[0],[0]
"Importantly, edge weights are not normalized, but directly reflect the strength of association between different characters.
",5 Implementation,[0],[0]
We do not solely rely on the social network to identify main characters.,5 Implementation,[0],[0]
"We estimate P(c ∈ main(M)), the probability of c being a leading character in movie M, using a Multi Layer
7We assume one node per speaking role in the script.
",5 Implementation,[0],[0]
Perceptron (MLP) and several features pertaining to the structure of the social network and the script text itself.,5 Implementation,[0],[0]
"A potential stumbling block in treating character identification as a classification task is obtaining training data, i.e., a list of main characters for each movie.",5 Implementation,[0],[0]
"We generate a gold-standard by assuming that the characters listed under Wikipedia’s Cast section (or an equivalent section, e.g., Characters) are the main characters in the movie.
",5 Implementation,[0],[0]
"Examples of the features we used for the classification task include the barycenter of a character (i.e., the sum of its distance to all other characters),",5 Implementation,[0],[0]
"PageRank (Page et al., 1999), an eigenvectorbased centrality measure, absolute/relative interaction weight (the sum of all interactions a character is involved in, divided by the sum of all interactions in the network), absolute/relative number of sentences uttered by a character, number of times a character is described by other characters (e.g., He is a monster or She is nice), number of times a character talks about other characters, and type-tokenratio of sentences uttered by the character (i.e., rate of unique words in a character’s speech).",5 Implementation,[0],[0]
"Using these features, the MLP achieves an F1 of 79.0% on the test set.",5 Implementation,[0],[0]
It outperforms other classification methods such as Naive Bayes or logistic regression.,5 Implementation,[0],[0]
"Using the full-feature set, the MLP also obtains performance superior to any individual measure of graph connectivity.
",5 Implementation,[0],[0]
"Aside from Equation (4), lead characters also appear in Equation (19), which determines scene importance.",5 Implementation,[0],[0]
We assume a character c ∈ main(M) if it is predicted by the MLP with a probability ≥ 0.5.,5 Implementation,[0],[0]
Gold Standard Chains The development and tuning of the chain extraction model presented in Section 4 necessitates access to a gold standard of key scene chains representing the movie’s most important content.,6 Experimental Setup,[0],[0]
Our experiments concentrated on a sample of 95 movies (comedies and thrillers) from the ScriptBase corpus (Section 3).,6 Experimental Setup,[0],[0]
Performing the scene selection task for such a big corpus manually would be both time consuming and costly.,6 Experimental Setup,[0],[0]
"Instead, we used distant supervision based on Wikipedia to automatically generate a gold standard.
",6 Experimental Setup,[0],[0]
"Specifically, we assume that Wikipedia plots are representative of the most important content in a movie.",6 Experimental Setup,[0],[0]
"Using the alignment algorithm presented in Nelken and Shieber (2006), we align script sentences to Wikipedia plot sentences and assume that scenes with at least one alignment are part of the gold chain of scenes.",6 Experimental Setup,[0],[0]
We obtain many-to-many alignments using features such as lemma overlap and word stem similarity.,6 Experimental Setup,[0],[0]
"When evaluated on four movies8 (from the training set) whose content was manually aligned to Wikipedia plots, the aligner achieved a precision of .53 at a recall rate of .82 at deciding whether a scene should be aligned.",6 Experimental Setup,[0],[0]
Scenes are ranked according to the number of alignments they contain.,6 Experimental Setup,[0],[0]
"When creating gold chains at different compression rates, we start with the best-ranked scenes and then successively add lower ranked ones until we reach the desired compression rate.
",6 Experimental Setup,[0],[0]
System Comparison In our experiments we compared our scene extraction model (SceneSum) against three baselines.,6 Experimental Setup,[0],[0]
The first baseline was based on the minimum overlap (MinOv) of characters in consecutive scenes and corresponds closely to the diversity term in our objective.,6 Experimental Setup,[0],[0]
The second baseline was based on the maximum overlap (MaxOv) of characters and approximates the importance term in our objective.,6 Experimental Setup,[0],[0]
"The third baseline selects scenes at random (averaged over 1,000 runs).",6 Experimental Setup,[0],[0]
"Parameters for our models were tuned on the training set, weights for the terms in the objective were optimized to the following values: λP = 1.0, λD = 0.3, and λI = 0.1.",6 Experimental Setup,[0],[0]
"We set the restart probability of our random walker
8“Cars 2”, “Shrek”, “Swordfish”, and “The Silence of the Lambs”.
to ε = 0.5, and the sigmoid scaling factor in our diversity term to k =−1.2.
",6 Experimental Setup,[0],[0]
Evaluation We assessed the output of our model (and comparison systems) automatically against the gold chains described above.,6 Experimental Setup,[0],[0]
We performed experiments with compression rates in the range of 10% to 50% and measured performance in terms of F1.,6 Experimental Setup,[0],[0]
"In addition, we also evaluated the quality of the extracted scenes as perceived by humans, which is necessary, given the approximate nature of our gold standard.",6 Experimental Setup,[0],[0]
"We adopted a question-answering (Q&A) evaluation paradigm which has been used previously to evaluate summaries and document compression (Morris et al., 1992; Mani et al., 2002; Clarke and Lapata, 2010).",6 Experimental Setup,[0],[0]
"Under the assumption that the summary is to function as a replacement for the full script, we can measure the extent to which it can be used to find answers to questions which have been derived from the entire script and are representative of its core content.",6 Experimental Setup,[0],[0]
"The more questions a hypothetical system can answer, the better it is at summarizing the script as a whole.
",6 Experimental Setup,[0],[0]
Two annotators were independently instructed to read scripts (from our test set) and create Q&A pairs.,6 Experimental Setup,[0],[0]
"The annotators generated questions relating to the plot of the movie and the development of its characters, requiring an unambiguous answer.",6 Experimental Setup,[0],[0]
They compared and revised their Q&A pairs until a common agreed-upon set of five questions per movie was reached (see Table 1 for an example).,6 Experimental Setup,[0],[0]
"In addition, for every movie we asked subjects to name the main characters, and summarize its plot (in no more than four sentences).",6 Experimental Setup,[0],[0]
"Using Amazon Mechanical Turk (AMT)9, we elicited answers for eight scripts (four comedies and thrillers) in four summarization con-
9https://www.mturk.com/
ditions: using our model, the two baselines based on minimum and maximum character overlap, and the random system.",6 Experimental Setup,[0],[0]
"All models were assessed at the same compression rate of 20% which seems realistic in an actual application environment, e.g., computer aided summarization.",6 Experimental Setup,[0],[0]
The scripts were preselected in an earlier AMT study where participants were asked to declare whether they had seen the movies in our test set (65 in total).,6 Experimental Setup,[0],[0]
We chose the screenplays which had received the least viewings so as to avoid eliciting answers based on familiarity with the movie.,6 Experimental Setup,[0],[0]
"A total of 29 participants, all self-reported native English speakers, completed the Q&A task.",6 Experimental Setup,[0],[0]
The answers provided by the subjects were scored against an answer key.,6 Experimental Setup,[0],[0]
"A correct answer was marked with a score of one, and zero otherwise.",6 Experimental Setup,[0],[0]
"In cases where more answers were required per question, partial scores were awarded to each correct answer (e.g., 0.5).",6 Experimental Setup,[0],[0]
The score for a summary is the average of its question scores.,6 Experimental Setup,[0],[0]
"Table 2 shows the performance of SceneSum, our scene extraction model, and the three comparison systems (MaxOv, MinOv, Random) on the automatic gold standard at five compression rates.",7 Results,[0],[0]
"As can be seen, MaxOv performs best in terms of F1, followed by SceneSum.",7 Results,[0],[0]
We believe this is an artifact due to the way the gold standard was created.,7 Results,[0],[0]
Scenes with large numbers of main characters are more likely to figure in Wikipedia plot summaries and will thus be more frequently aligned.,7 Results,[0],[0]
"A chain based on maximum character overlap will focus on such scenes and will agree with the gold standard better compared to chains which take additional script properties into account.
",7 Results,[0],[0]
We further analyzed the scenes selected by SceneSum and the comparison systems with respect to their position in the script.,7 Results,[0],[0]
"Table 3 shows the av-
erage percentage of scenes selected from the beginning, middle, and end of the movie (based on an equal division of the number of scenes in the screenplay).",7 Results,[0],[0]
"As can be seen, the number of selected scenes tends to be evenly distributed across the entire movie.",7 Results,[0],[0]
"SceneSum has a slight bias towards the beginning of the movie which is probably natural, since leading characters appear early on, as well as important scenes introducing essential story elements (e.g., setting, points of view).
",7 Results,[0],[0]
The results of our human evaluation study are summarized in Table 4.,7 Results,[0],[0]
We observe that SceneSum summaries are overall more informative compared to those created by the baselines.,7 Results,[0],[0]
"In other words, AMT participants are able to answer more questions regarding the story of the movie when reading SceneSum summaries.",7 Results,[0],[0]
"In two instances (“A Nightmare on Elm Street 3” and “Mumford”), the overlap models score better, however, in this case the movies largely consist of scenes with the same characters and relatively little variation (“A Nightmare on Elm Street 3”), or the camera follows the main lead in his interactions with other characters (“Mumford”).",7 Results,[0],[0]
"Since our model is not so character-centric, it might be thrown off by non-character-based terms in its objective, leading to the selection of unfavorable scenes.",7 Results,[0],[0]
Table 4 also presents a break down of the different types of questions answered by our participants.,7 Results,[0],[0]
"Again, we see that in most cases a larger percentage is answered correctly when reading SceneSum summaries.
",7 Results,[0],[0]
"Overall, we observe that SceneSum extracts chains which encapsulate important movie content across the board.",7 Results,[0],[0]
"We should point out that although our movies are broadly classified as comedies and thrillers, they have very different structure and content.",7 Results,[0],[0]
"For example, “Little Athens” has a very loose plotline, “Living in Oblivion” has multi-
ple dream sequences, whereas “While She was Out” contains only a few characters and a series of important scenes towards the end.",7 Results,[0],[0]
"Despite this variety, SceneSum performs consistently better in our taskbased evaluation.",7 Results,[0],[0]
In this paper we have developed a graph-based model for script summarization.,8 Conclusions,[0],[0]
"We formalized the process of generating a shorter version of a screenplay as the task of finding an optimal chain of scenes, which are diverse, important, and exhibit logical progression.",8 Conclusions,[0],[0]
A large-scale evaluation based on a question-answering task revealed that our method produces more informative summaries compared to several baselines.,8 Conclusions,[0],[0]
"In the future, we plan to explore model performance in a wider range of movie genres as well as its applicability to other NLP tasks (e.g., book summarization or event extraction).",8 Conclusions,[0],[0]
We would also like to automatically determine the compression rate which should presumably vary according to the movie’s length and content.,8 Conclusions,[0],[0]
"Finally, our long-term goal is to be able to generate loglines as well as movie plot summaries.
",8 Conclusions,[0],[0]
"Acknowledgments We would like to thank Rik Sarkar, Jon Oberlander and Annie Louis for their valuable feedback.",8 Conclusions,[0],[0]
"Special thanks to Bharat Ambati, Lea Frermann, and Daniel Renshaw for their help with system evaluation.",8 Conclusions,[0],[0]
"In this paper we study the task of movie script summarization, which we argue could enhance script browsing, give readers a rough idea of the script’s plotline, and speed up reading time.",abstractText,[0],[0]
We formalize the process of generating a shorter version of a screenplay as the task of finding an optimal chain of scenes.,abstractText,[0],[0]
"We develop a graph-based model that selects a chain by jointly optimizing its logical progression, diversity, and importance.",abstractText,[0],[0]
Human evaluation based on a question-answering task shows that our model produces summaries which are more informative compared to competitive baselines.,abstractText,[0],[0]
Movie Script Summarization as Graph-based Scene Extraction,title,[0],[0]
Support vector machines (SVMs) and Boosting have been two mainstream learning approaches during the past decade.,1. Introduction,[0],[0]
"The former (Cortes & Vapnik, 1995) roots in the statistical learning theory (Vapnik, 1995) with the central idea of searching a large margin separator, i.e., maximizing the smallest distance from the instances to the classification boundary in a RKHS (reproducing kernel Hilbert space).",1. Introduction,[0],[0]
"It is noteworthy that there is also a long history of applying margin theory to explain the latter (Freund & Schapire, 1995; Schapire et al., 1998), due to its tending to be empirically resistant to over-fitting (Reyzin & Schapire, 2006; Wang et al., 2011; Zhou, 2012).
",1. Introduction,[0],[0]
"1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China.",1. Introduction,[0],[0]
"Correspondence to: Zhi-Hua Zhou <zhouzh@lamda.nju.edu.cn>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Recently, the margin theory for Boosting has finally been defended (Gao & Zhou, 2013), and has disclosed that the margin distribution rather than a single margin is more crucial to the generalization performance.",1. Introduction,[0],[0]
It suggests that there may still exist large space to further enhance for SVMs.,1. Introduction,[0],[0]
"Inspired by this recognition, (Zhang & Zhou, 2014; 2016) proposed a binary classification method to optimize margin distribution by characterizing it through the firstand second-order statistics, which achieves quite satisfactory experimental results.",1. Introduction,[0],[0]
"Later, (Zhou & Zhou, 2016) extends the idea to an approach which is able to exploit unlabeled data and handle unequal misclassification cost.",1. Introduction,[0],[0]
"A brief summary of this line of early research can be found in (Zhou, 2014).
",1. Introduction,[0],[0]
"Although it has been shown that for binary classification, optimizing the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously can get superior performance, it still remains open for multi-class classification.",1. Introduction,[0],[0]
"Moreover, the margin for multiclass classification is much more complicated than that for binary class classification, which makes the resultant optimization be a difficult non-differentiable non-convex programming.",1. Introduction,[0],[0]
"In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine) to solve this problem efficiently.",1. Introduction,[0],[0]
"For optimization, we relax mcODM into a series of convex quadratic programming (QP), and extend the Block Coordinate Descent (BCD) algorithm (Tseng, 2001) to solve the dual of each QP.",1. Introduction,[0],[0]
The sub-problem of each iteration of BCD is also a QP.,1. Introduction,[0],[0]
"By exploiting its special structure, we derive a sorting algorithm to solve it which is much faster than general QP solvers.",1. Introduction,[0],[0]
"We further provide a generalization error bound based on Rademacher complexity, and further present the analysis of the relationship between generalization error and margin distribution for multi-class classification.",1. Introduction,[0],[0]
"Extensive experiments on twenty two data sets show the superiority of our method to all four versions of multi-class SVMs.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows.,1. Introduction,[0],[0]
Section 2 introduces some preliminaries.,1. Introduction,[0],[0]
Section 3 formulates the problem.,1. Introduction,[0],[0]
Section 4 presents the proposed algorithm.,1. Introduction,[0],[0]
Section 5 discusses some theoretical analyses.,1. Introduction,[0],[0]
Section 6 reports on our experimental studies and empirical observations.,1. Introduction,[0],[0]
Finally Section 7 concludes with future work.,1. Introduction,[0],[0]
We denote by X ∈ Rd the instance space and Y =,2. Preliminaries,[0],[0]
"[k] the label set, where [k] = {1, . . .",2. Preliminaries,[0],[0]
", k}.",2. Preliminaries,[0],[0]
Let D be an unknown (underlying) distribution over X × Y .,2. Preliminaries,[0],[0]
"A training set S = {(x1, y1), (x2, y2), . . .",2. Preliminaries,[0],[0]
", (xm, ym)} ∈",2. Preliminaries,[0],[0]
(X × Y)m is drawn identically and independently (i.i.d.),2. Preliminaries,[0],[0]
according to distribution D. Let ϕ,2. Preliminaries,[0],[0]
: X 7→ H be a feature mapping associated to some positive definite kernel κ.,2. Preliminaries,[0],[0]
"For multi-class classification setting, the hypothesis is defined based on k weight vectors w1, . . .",2. Preliminaries,[0],[0]
",wk ∈ H, where each weight vector wl, l ∈ Y defines a scoring function x 7→ w⊤l ϕ(x) and the label of instance x is the one resulting in the largest score, i.e., h(x) =",2. Preliminaries,[0],[0]
argmaxl∈Y w ⊤ l ϕ(x).,2. Preliminaries,[0],[0]
"This decision function naturally leads to the following definition of the margin for a labeled instance (x, y):
γh(x, y) = w ⊤ y ϕ(x)−max l ̸=y w⊤l ϕ(x).
",2. Preliminaries,[0],[0]
"Thus h misclassifies (x, y) if and only if it produces a negative margin for this instance.
",2. Preliminaries,[0],[0]
"Given a hypothesis set H of functions mapping X to Y and the labeled training set S, our goal is to learn a function h ∈ H such that the generalization error R(h) = E(x,y)∼D[1h(x)̸=y] is small.",2. Preliminaries,[0],[0]
"To design optimal margin distribution machine for multiclass classification, we need to understand how to optimize the margin distribution.",3. Formulation,[0],[0]
"(Gao & Zhou, 2013) proved that, to characterize the margin distribution, it is important to consider",3. Formulation,[0],[0]
not only the margin mean,3. Formulation,[0],[0]
but also the margin variance.,3. Formulation,[0],[0]
"Inspired by this idea, (Zhang & Zhou, 2014; 2016) proposed the optimal margin distribution machine for binary classification, which characterizes the margin distribution according to the first- and second-order statistics, that is, maximizing the margin mean and minimizing the margin variance simultaneously.",3. Formulation,[0],[0]
"Specifically, let γ̄ denote the margin mean, and the optimal margin distribution machine can be formulated as:
min w,γ̄,ξi,ϵi Ω(w)− ηγ̄",3. Formulation,[0],[0]
+,3. Formulation,[0],[0]
λ m m∑ i=1,3. Formulation,[0],[0]
(ξ2i + ϵ,3. Formulation,[0],[0]
2,3. Formulation,[0],[0]
"i )
s.t. γh(xi, yi) ≥ γ̄",3. Formulation,[0],[0]
"− ξi, γh(xi, yi) ≤ γ̄ + ϵi, ∀i,
where Ω(w) is the regularization term to penalize the model complexity, η and λ are trading-off parameters, ξi and ϵi are the deviation of the margin γh(xi, yi) to the margin mean.",3. Formulation,[0],[0]
It’s evident that ∑m i=1(ξ 2,3. Formulation,[0],[0]
i + ϵ 2,3. Formulation,[0],[0]
i )/m,3. Formulation,[0],[0]
"is exactly the margin variance.
",3. Formulation,[0],[0]
"By scaling w which doesn’t affect the final classification results, the margin mean can be fixed as 1, then the de-
viation of the margin of (xi, yi) to the margin mean is |γh(xi, yi)",3. Formulation,[0],[0]
"− 1|, and the optimal margin distribution machine can be reformulated as
min w,ξi,ϵi
Ω(w)",3. Formulation,[0],[0]
"+ λ
m m∑ i=1",3. Formulation,[0],[0]
"ξ2i + µϵ 2 i (1− θ)2
s.t. γh(xi, yi) ≥ 1− θ − ξi, γh(xi, yi) ≤ 1 + θ + ϵi, ∀i.
where µ ∈ (0, 1] is a parameter to trade off two different kinds of deviation (larger or less than margin mean).",3. Formulation,[0],[0]
θ ∈,3. Formulation,[0],[0]
"[0, 1) is a parameter of the zero loss band, which can control the number of support vectors, i.e., the sparsity of the solution, and (1− θ)2 in the denominator is to scale the second term to be a surrogate loss for 0-1 loss.
",3. Formulation,[0],[0]
"For multi-class classification, let the regularization term Ω(w)",3. Formulation,[0],[0]
= ∑k l=1 ∥wl∥2H/2,3. Formulation,[0],[0]
"and combine with the definition of margin, and we arrive at the formulation of mcODM,
min wl,ξi,ϵi
1
2 k∑ l=1",3. Formulation,[0],[0]
∥wl∥2H +,3. Formulation,[0],[0]
λ m m∑ i=1,3. Formulation,[0],[0]
"ξ2i + µϵ 2 i (1− θ)2 (1)
s.t. w⊤yiϕ(xi)−maxl ̸=yi w⊤l ϕ(xi) ≥ 1− θ − ξi,
w⊤yiϕ(xi)−maxl ̸=yi w⊤l ϕ(xi) ≤ 1 + θ + ϵi, ∀i.
where λ, µ and θ are the parameters for trading-off described previously.",3. Formulation,[0],[0]
"Due to the max operator in the second constraint, mcODM is a non-differentiable non-convex programming, which is quite difficult to solve directly.
",4. Optimization,[0],[0]
"In this section, we first relax mcODM into a series of convex quadratic programming (QP), which can be much easier to handle.",4. Optimization,[0],[0]
"Specifically, at each iteration, we recast the first constraint as k − 1 linear inequality constraints:
w⊤yiϕ(xi)−w ⊤ l ϕ(xi) ≥ 1− θ − ξi, l ̸= yi,
and replace the second constraint with
w⊤yiϕ(xi)−Mi ≤ 1 + θ + ϵi,
where Mi = maxl ̸=yi w̄ ⊤ l ϕ(xi) and w̄l is the solution to the previous iteration.",4. Optimization,[0],[0]
"Then we can repeatedly solve the following convex QP problem until convergence:
min wl,ξi,ϵi
1
2 k∑ l=1",4. Optimization,[0],[0]
∥wl∥2H +,4. Optimization,[0],[0]
λ m m∑ i=1,4. Optimization,[0],[0]
"ξ2i + µϵ 2 i (1− θ)2 (2)
s.t. w⊤yiϕ(xi)−w ⊤ l ϕ(xi) ≥ 1− θ − ξi, ∀l ̸= yi,
w⊤yiϕ(xi)−Mi ≤ 1 + θ + ϵi, ∀i.
",4. Optimization,[0],[0]
"Introduce the lagrange multipliers ζli ≥ 0, l ̸= yi for the first k − 1 constraints and βi ≥ 0 for the last constraint respectively, the Lagrangian function of Eq. 2 leads to
L(wl, ξi, ϵi, ζ",4. Optimization,[0],[0]
"l i , βi)
= 1
2 k∑ l=1",4. Optimization,[0],[0]
∥wl∥2H +,4. Optimization,[0],[0]
λ m m∑ i=1,4. Optimization,[0],[0]
"ξ2i + µϵ 2 i (1− θ)2
− m∑ i=1",4. Optimization,[0],[0]
∑,4. Optimization,[0],[0]
"l ̸=yi ζli(w ⊤ yiϕ(xi)−w ⊤ l ϕ(xi)− 1 + θ + ξi)
+",4. Optimization,[0],[0]
m∑ i=1 βi(w,4. Optimization,[0],[0]
⊤ yiϕ(xi)−Mi,4. Optimization,[0],[0]
"− 1− θ − ϵi),
By setting the partial derivations of variables {wl, ξi, ϵi} to zero, we have
wl = m∑ i=1",4. Optimization,[0],[0]
"δyi,l ∑ s̸=yi ζsi",4. Optimization,[0],[0]
"− (1− δyi,l)ζli − δyi,lβi ϕ(xi), ξi = m(1− θ)2
2λ
∑ l ̸=yi ζli , ϵi = m(1− θ)2 2λµ βi.",4. Optimization,[0],[0]
"(3)
where δyi,l equals 1 when yi = l and 0 otherwise.",4. Optimization,[0],[0]
"We further simplify the expression of wl as
wl = m∑ i=1",4. Optimization,[0],[0]
"(αli − δyi,lβi)ϕ(xi), (4)
by defining αli ≡ −ζli for ∀l ̸= yi and α yi",4. Optimization,[0],[0]
i ≡,4. Optimization,[0],[0]
"∑ s̸=yi
ζsi and substituting Eq.",4. Optimization,[0],[0]
"4 and Eq. 3 into the Lagrangian function, then we have the following dual problem
min αli,α yi",4. Optimization,[0],[0]
"i ,βi
1
2 k∑ l=1",4. Optimization,[0],[0]
∥wl∥2H + m(1− θ)2 4λ,4. Optimization,[0],[0]
m∑ i=1,4. Optimization,[0],[0]
"(αyii ) 2
+ m(1− θ)2
4λµ
m∑ i=1 β2i",4. Optimization,[0],[0]
+ (1− θ) m∑ i=1,4. Optimization,[0],[0]
∑,4. Optimization,[0],[0]
"l ̸=yi αli
+ (Mi + 1 + θ) m∑ i=1",4. Optimization,[0],[0]
"βi (5)
s.t. k∑
l=1
αli = 0, ∀i,
αli ≤ 0, ∀i,∀l ̸= yi, βi ≥ 0, ∀i.
",4. Optimization,[0],[0]
"The objective function in Eq. 5 involves m(k + 1) variables in total, so it is not easy to optimize with respect to all the variables simultaneously.",4. Optimization,[0],[0]
"Note that all the constraints can be partitioned into m disjoint sets, and the i-th set only involves α1i , . . .",4. Optimization,[0],[0]
", α k i , βi, so the variables can be divided into m decoupled groups and an efficient block coordinate descent algorithm (Tseng, 2001) can be applied.
",4. Optimization,[0],[0]
"Specifically, we sequentially select a group of k + 1 variables α1i , . . .",4. Optimization,[0],[0]
", α k i , βi associated with instance xi to minimize, while keeping other variables as constants, and repeat this procedure until convergence.
",4. Optimization,[0],[0]
"Algorithm 1 below details the kenrel mcODM.
",4. Optimization,[0],[0]
Algorithm 1 Kenrel mcODM 1: Input: Data set S. 2: Initialize α⊤ =,4. Optimization,[0],[0]
"[α11, . . .",4. Optimization,[0],[0]
", α k 1 , . . .",4. Optimization,[0],[0]
", α 1 m, . . .",4. Optimization,[0],[0]
", α k m] and
β⊤ =",4. Optimization,[0],[0]
"[β1, . . .",4. Optimization,[0],[0]
", βm] as zero vector. 3: while α and β not converge do 4: for i = 1, . . .",4. Optimization,[0],[0]
",m do 5: Mi ← maxl ̸=yi ∑m j=1(α",4. Optimization,[0],[0]
"l j − δyj ,lβj)κ(xj ,xi).",4. Optimization,[0],[0]
6: end for 7: Solve Eq. 5 by block coordinate descent method.,4. Optimization,[0],[0]
"8: end while 9: Output: α, β.",4. Optimization,[0],[0]
"The sub-problem in step 7 of Algorithm 1 is also a convex QP with k + 1 variables, which can be accomplished by some standard QP solvers.",4.1. Solving the sub-problem,[0],[0]
"However, by exploiting its special structure, i.e., only a small quantity of cross terms are involved, we can derive an algorithm to solve this subproblem just by sorting, which can be much faster than general QP solvers.
",4.1. Solving the sub-problem,[0],[0]
"Note that all variables except α1i , . . .",4.1. Solving the sub-problem,[0],[0]
", α k",4.1. Solving the sub-problem,[0],[0]
"i , βi are fixed, so we have the following sub-problem:
min αli,α yi",4.1. Solving the sub-problem,[0],[0]
"i ,βi ∑ l ̸=yi A 2 (αli) 2 + ∑ l ̸=yi",4.1. Solving the sub-problem,[0],[0]
"Blα l i + D 2 (αyii ) 2 −Aαyii βi
+Byiα yi i +
E 2 β2i + Fβi
s.t. k∑
l=1
αli = 0, (6)
αli ≤ 0, ∀l ̸= yi, βi ≥ 0.
where A = κ(xi,xi), Bl = ∑ j ̸=i κ(xi,xj)(α",4.1. Solving the sub-problem,[0],[0]
l j,4.1. Solving the sub-problem,[0],[0]
"−
δyj ,lβj)+1− θ for ∀l ̸= yi, Byi = ∑ j ̸=i κ(xi,xj)(α yi",4.1. Solving the sub-problem,[0],[0]
"j − δyj ,yiβj), D = A + m(1−θ)2 2λ , E = A + m(1−θ)2
2λµ and F ≡Mi + 1 + θ −Byi .
",4.1. Solving the sub-problem,[0],[0]
"The KKT conditions of Eq. 6 indicate that there are scalars ν, ρl and η such that
k∑ l=1 αli = 0, (7) αli ≤ 0, ∀l ̸= yi, (8)
βi ≥ 0, (9) ρlα l i = 0, ρl ≥ 0, ∀l ̸= yi, (10)
",4.1. Solving the sub-problem,[0],[0]
Aαli +,4.1. Solving the sub-problem,[0],[0]
"Bl − ν + ρl = 0, ∀l ̸= yi, (11) ηβi = 0, η ≥ 0, (12) −Aαyii +",4.1. Solving the sub-problem,[0],[0]
Eβi + F,4.1. Solving the sub-problem,[0],[0]
"− η = 0, (13) Dαyii −Aβi +Byi − ν = 0.",4.1. Solving the sub-problem,[0],[0]
"(14)
",4.1. Solving the sub-problem,[0],[0]
"According to Eq. 8, Eq. 10 and Eq. 11 are equivalent to
Aαli",4.1. Solving the sub-problem,[0],[0]
"+Bl − ν = 0, if αli < 0, ∀l ̸= yi, (15)",4.1. Solving the sub-problem,[0],[0]
Bl,4.1. Solving the sub-problem,[0],[0]
"− ν ≤ 0, if αli = 0, ∀l ̸= yi.",4.1. Solving the sub-problem,[0],[0]
"(16)
In the same way, Eq. 12 and Eq. 13 are equivalent to
−Aαyii",4.1. Solving the sub-problem,[0],[0]
+,4.1. Solving the sub-problem,[0],[0]
"Eβi + F = 0, if βi > 0, (17) −Aαyii + F ≥ 0, if βi = 0.",4.1. Solving the sub-problem,[0],[0]
"(18)
Thus KKT conditions turn to Eq. 7 - Eq. 9 and Eq. 14 - Eq. 18.",4.1. Solving the sub-problem,[0],[0]
"Note that
αli ≡ min ( 0,
ν",4.1. Solving the sub-problem,[0],[0]
"−Bl A
) , ∀l ̸= yi, (19)
satisfies KKT conditions Eq. 8 and Eq. 15 - Eq. 16",4.1. Solving the sub-problem,[0],[0]
"and βi ≡ max ( 0,
Aαyii − F E
) , (20)
satisfies KKT conditions Eq. 9 and Eq. 17 - Eq. 18.",4.1. Solving the sub-problem,[0],[0]
"By substituting Eq. 20 into Eq. 14, we obtain
Dαyii +Byi − ν = max ( 0, A
E (Aαyii − F )
) .",4.1. Solving the sub-problem,[0],[0]
"(21)
Let’s consider the following two cases in turn.
",4.1. Solving the sub-problem,[0],[0]
"Case 1: Aαyii ≤ F , according to Eq. 20 and Eq. 21, we have βi = 0 and α yi i = ν−Byi D .",4.1. Solving the sub-problem,[0],[0]
"Thus, A ν−Byi D ≤ F , which implies that ν ≤",4.1. Solving the sub-problem,[0],[0]
Byi + DFA .,4.1. Solving the sub-problem,[0],[0]
"Case 2: Aαyii > F , according to Eq. 20 and Eq. 21, we have βi = Aα yi",4.1. Solving the sub-problem,[0],[0]
i −F E > 0 and α yi i = Eν−AF−EByi DE−A2 .,4.1. Solving the sub-problem,[0],[0]
"Thus, A Eν−AF−EByi
DE−A2 > F , which implies that ν",4.1. Solving the sub-problem,[0],[0]
>,4.1. Solving the sub-problem,[0],[0]
"Byi + DF A .
",4.1. Solving the sub-problem,[0],[0]
The remaining task is to find ν such that Eq. 7 holds.,4.1. Solving the sub-problem,[0],[0]
"With Eq. 7 and Eq. 19, it can be shown that
ν =
AByi D + ∑ l:αli<0
Bl A D + |{l|α l i < 0}| , Case 1, (22)
ν",4.1. Solving the sub-problem,[0],[0]
"=
AEByi+A 2F DE−A2 + ∑",4.1. Solving the sub-problem,[0],[0]
"l:αli<0 Bl
AE DE−A2 + |{l|α l i < 0}|
, Case 2. (23)
",4.1. Solving the sub-problem,[0],[0]
"In both cases, the optimal ν takes the form of (P +∑ l:αli<0 Bl)/(Q+ |{l|αli < 0}|), where P and Q are some
constants.",4.1. Solving the sub-problem,[0],[0]
"(Fan et al., 2008) showed that it can be found by sorting {Bl} for ∀l ̸= yi in decreasing order and then sequentially adding them into an empty set Φ, until
ν∗ = P +
∑",4.1. Solving the sub-problem,[0],[0]
"l∈Φ Bl
Q+ |Φ| ≥",4.1. Solving the sub-problem,[0],[0]
"max l ̸∈Φ Bl. (24)
Note that the Hessian matrix of the objective function of Eq. 6 is positive definite, which guarantees the existence and uniqueness of the optimal solution, so only one of Eq. 22 and Eq. 23 can hold.",4.1. Solving the sub-problem,[0],[0]
"We can first compute ν∗ according to Eq. 24 for Case 1, and then check whether the constraint of ν is satisfied.",4.1. Solving the sub-problem,[0],[0]
"If not, we further compute ν∗ for Case 2.",4.1. Solving the sub-problem,[0],[0]
"Algorithm 2 summarizes the pseudo-code for solving the sub-problem.
",4.1. Solving the sub-problem,[0],[0]
"Algorithm 2 Solving the sub-problem 1: Input: Parameters A, B = {B1, . . .",4.1. Solving the sub-problem,[0],[0]
", Bk}, D,E, F .",4.1. Solving the sub-problem,[0],[0]
"2: Initialize B̂ ← B, then swap B̂1 and B̂yi , and sort
B̂\{B̂1} in decreasing order.",4.1. Solving the sub-problem,[0],[0]
"3: i← 2, ν ← AByi/D. 4: while i ≤ k and ν/(i− 2 +A/D) < B̂i do 5: ν ← ν + B̂i.",4.1. Solving the sub-problem,[0],[0]
6: i← i+ 1. 7: end while 8: if ν ≤,4.1. Solving the sub-problem,[0],[0]
"Byi +DF/A then 9: αli ← min(0, (ν −Bl)/A), l ̸= yi.
10: αyii ← (ν −Byi)/D. 11: βi ← 0. 12: else 13: i← 2, ν ← (AEB̂1 +A2F )/(DE",4.1. Solving the sub-problem,[0],[0]
−A2).,4.1. Solving the sub-problem,[0],[0]
14: while i ≤ k and ν/(i− 2+AE/(DE−A2)),4.1. Solving the sub-problem,[0],[0]
< B̂i do 15: ν ← ν + B̂i.,4.1. Solving the sub-problem,[0],[0]
"16: i← i+ 1. 17: end while 18: αli ← min(0, (ν −Bl)/A), l ̸= yi. 19: αyii ← (Eν −AF − EByi)/(DE −A2).",4.1. Solving the sub-problem,[0],[0]
20: βi ← (Aαyii − F ),4.1. Solving the sub-problem,[0],[0]
/E. 21: end if 22:,4.1. Solving the sub-problem,[0],[0]
"Output: α1i , . . .",4.1. Solving the sub-problem,[0],[0]
", αki , βi.",4.1. Solving the sub-problem,[0],[0]
"In section 4.1, the proposed method can efficiently deal with kernel mcODM.",4.2. Speedup for linear kernel,[0],[0]
"However, the computation of Mi in step 5 of Algorithm 1 and the computation of parameters B̄l in Algorithm 2 both involve the kernel matrix, whose inherent computational cost takes O(m2) time, so it might be computational prohibitive for large scale problems.
",4.2. Speedup for linear kernel,[0],[0]
"When linear kernel is used, these problems can be alleviated.",4.2. Speedup for linear kernel,[0],[0]
"According to Eq. 4, w is spanned by the training instance so it lies in a finite dimensional space under this
circumstance.",4.2. Speedup for linear kernel,[0],[0]
"By storing w1, . . .",4.2. Speedup for linear kernel,[0],[0]
",wk explicitly, the computational cost of Mi = maxl ̸=yi w ⊤",4.2. Speedup for linear kernel,[0],[0]
l xi can be much less.,4.2. Speedup for linear kernel,[0],[0]
"Moreover, note that B̄l = ∑ j ̸=i x ⊤",4.2. Speedup for linear kernel,[0],[0]
"i xj(α
",4.2. Speedup for linear kernel,[0],[0]
l j,4.2. Speedup for linear kernel,[0],[0]
"− δyj ,lβj)",4.2. Speedup for linear kernel,[0],[0]
"=∑m
j=1 x ⊤",4.2. Speedup for linear kernel,[0],[0]
"i xj(ᾱ l j−δyj ,lβ̄j)−x⊤i xi(ᾱli−δyi,lβ̄i) =",4.2. Speedup for linear kernel,[0],[0]
"w⊤l xi−
A(αli − δyi,lβi), so B̄l can also be computed efficiently.",4.2. Speedup for linear kernel,[0],[0]
"In this section, we study the statistical property of mcODM.",5. Analysis,[0],[0]
"To present the generalization bound of mcODM, we need to introduce the following loss function Φ,
Φ(z) = 1z≤0 + (z − 1 + θ)2
(1− θ)2 10<z≤1−θ,
γh,θ(x, y) = w ⊤ y ϕ(x)−max l∈Y {w⊤l ϕ(x)− (1− θ)1l=y},
where 1(·) is the indicator function that returns 1 when the argument holds, and 0 otherwise.",5. Analysis,[0],[0]
"As can be seen, γh,θ(x, y) is a lower bound of γh(x, y) and Φ(γh,θ(x, y))",5. Analysis,[0],[0]
"= Φ(γh(x, y)).
",5. Analysis,[0],[0]
Theorem 1.,5. Analysis,[0],[0]
"Let H = {(x, y) ∈ X",5. Analysis,[0],[0]
×,5. Analysis,[0],[0]
[k] 7→,5. Analysis,[0],[0]
w⊤y ϕ(x)| ∑k l=1,5. Analysis,[0],[0]
"∥wl∥2H ≤ Λ2} be the hypothesis space of mcODM, where ϕ :",5. Analysis,[0],[0]
X 7→ H is a feature mapping induced by some positive definite kernel κ.,5. Analysis,[0],[0]
"Assume that S ⊆ {x : κ(x,x) ≤ r2}, then for any δ > 0, with probability at least 1 − δ, the following generalization bound holds for any h ∈ H ,
R(h) ≤ 1 m m∑ i=1",5. Analysis,[0],[0]
"Φ(γh(xi, yi))",5. Analysis,[0],[0]
"+ 16rΛ 1− θ
√ 2πk
m + 3 √ ln 2δ 2m .
",5. Analysis,[0],[0]
Proof.,5. Analysis,[0],[0]
"Let H̃θ be the family of hypotheses mapping X × Y 7→ R defined by H̃θ = {(x, y) 7→ γh,θ(x, y) :",5. Analysis,[0],[0]
"h ∈ H}, with McDiarmid inequality (McDiarmid, 1989), yields the following inequality with probability at least 1− δ,
E[Φ(γh,θ(x, y))]",5. Analysis,[0],[0]
"≤ 1
m m∑ i=1 Φ(γh,θ(xi, yi))
",5. Analysis,[0],[0]
"+ 2RS(Φ ◦ H̃θ) + 3 √ ln 2δ 2m ,∀h ∈ H̃θ.
",5. Analysis,[0],[0]
"Note that Φ(γh,θ) = Φ(γh), R(h) = E[1γh(x,y)≤0] ≤",5. Analysis,[0],[0]
"E[1γh,θ(x,y)≤0] ≤ E[Φ(γh,θ(x, y))]",5. Analysis,[0],[0]
"and Φ(z) is 21−θ - Lipschitz function, by using Talagrand’s lemma (Mohri et al., 2012), we have
R(h) ≤ 1 m m∑ i=1",5. Analysis,[0],[0]
"Φ(γh(xi, yi))",5. Analysis,[0],[0]
"+ 4RS(H̃θ) 1− θ + 3 √ ln 2δ 2m .
",5. Analysis,[0],[0]
"According to Theorem 7 of (Lei et al., 2015), we have RS(H̃θ) ≤ 4rΛ",5. Analysis,[0],[0]
"√ 2πk/m and proves the stated result.
",5. Analysis,[0],[0]
Theorem 1 shows that we can get a tighter generalization bound for smaller rΛ and smaller θ.,5. Analysis,[0],[0]
"Since γ ≤ 2rΛ, so the former can be viewed as an upper bound of the margin.",5. Analysis,[0],[0]
"Besides, 1 − θ is the lower bound of the zero loss band of mcODM.",5. Analysis,[0],[0]
"This verifies that better margin distribution (i.e., larger margin mean and smaller margin variance) can yield better generalization performance, which is also consistent with the work of (Gao & Zhou, 2013).",5. Analysis,[0],[0]
"In this section, we empirically evaluate the effectiveness of our method on a broad range of data sets.",6. Empirical Study,[0],[0]
"We first introduce the experimental settings and compared methods in Section 6.1, and then in Section 6.2, we compare our method with four versions of multi-class SVMs, i.e., mcSVM (Weston & Watkins, 1999; Crammer & Singer, 2001; 2002), one-versus-all SVM (ovaSVM), one-versusone SVM (ovoSVM) (Ulrich, 1998) and error-correcting output code SVM (ecocSVM) (Dietterich & Bakiri, 1995).",6. Empirical Study,[0],[0]
"In addition, we also study the influence of the number of classes on generalization performance and margin distribution in Section 6.3.",6. Empirical Study,[0],[0]
"Finally, the computational cost is presented in Section 6.4.",6. Empirical Study,[0],[0]
We evaluate the effectiveness of our proposed methods on twenty two data sets.,6.1. Experimental Setup,[0],[0]
Table 1 summarizes the statistics of these data sets.,6.1. Experimental Setup,[0],[0]
"The data set size ranges from 150 to more than 581,012, and the dimensionality ranges from 4 to more than 62,061.",6.1. Experimental Setup,[0],[0]
"Moreover, the number of class ranges from 3 to 1,000, so these data sets cover a broad range of properties.",6.1. Experimental Setup,[0],[0]
All features are normalized into the interval,6.1. Experimental Setup,[0],[0]
"[0, 1].",6.1. Experimental Setup,[0],[0]
"For each data set, eighty percent of the instances are randomly selected as training data, and the rest are used as testing data.",6.1. Experimental Setup,[0],[0]
"For each data set, experiments are repeated for 10 times with random data partitions, and the average accuracies as well as the standard deviations are recorded.
mcODM is compared with four versions of multi-class SVMs, i.e., ovaSVM, ovoSVM, ecocSVM and mcSVM.",6.1. Experimental Setup,[0],[0]
These four methods can be roughly classified into two groups.,6.1. Experimental Setup,[0],[0]
The first group includes the first three methods by converting the multi-class classification problem into a set of binary classification problems.,6.1. Experimental Setup,[0],[0]
"Specially, ovaSVM consists of learning k scoring functions hl : X 7→ {−1,+1}, l ∈ Y , each seeking to discriminate one class l ∈ Y from all the others, as can be seen it need train k SVM models.",6.1. Experimental Setup,[0],[0]
"Alternatively, ovoSVM determines the scoring functions for all the combinations of class pairs, so it need train k(k − 1)/2 SVM models.",6.1. Experimental Setup,[0],[0]
"Finally, ecocSVM is a generalization of the former two methods.",6.1. Experimental Setup,[0],[0]
"This technique assigns to each class l ∈ Y a code word with length c, which serves as a signature for this class.",6.1. Experimental Setup,[0],[0]
"After training
c binary SVM models h1(·), . . .",6.1. Experimental Setup,[0],[0]
", hc(·), the class predicted for each testing instance is the one whose signatures is the closest to [h1(x), . . .",6.1. Experimental Setup,[0],[0]
", hc(x)] in Hamming distance.",6.1. Experimental Setup,[0],[0]
"The weakness of these methods is that they may produce unclassifiable regions and their computational costs are usually quite large in practice, which can be observed in the following experiments.",6.1. Experimental Setup,[0],[0]
"On the other hand, mcSVM belongs to the second group.",6.1. Experimental Setup,[0],[0]
"It directly determines all the scroing functions at once, so the time cost is usually less than the former methods.",6.1. Experimental Setup,[0],[0]
"In addition, the unclassifiable regions are also resolved.
",6.1. Experimental Setup,[0],[0]
"For all the methods, the regularization parameter λ for mcODM or C for binary SVM and mcSVM is selected by 5-fold cross validation from [20, 22, . . .",6.1. Experimental Setup,[0],[0]
", 220].",6.1. Experimental Setup,[0],[0]
"For mcODM, the regularization parameters µ and θ are both selected from [0.2, 0.4, 0.6, 0.8].",6.1. Experimental Setup,[0],[0]
"For ecocSVM, the exhaustive codes strategy is employed, i.e., for each class, we construct a code of length 2k−1 − 1 as the signature.",6.1. Experimental Setup,[0],[0]
All the selections for parameters are performed on training sets.,6.1. Experimental Setup,[0],[0]
Table 2 summarizes the detailed results on twenty two data sets.,6.2. Results,[0],[0]
"As can be seen, the overall performance of our method is superior or highly competitive to the other compared methods.",6.2. Results,[0],[0]
"Specifically, mcODM performs significantly better than mcSVM/ovaSVM/ovoSVM/ecocSVM on 17/19/18/17 over 22 data sets respectively, and achieves the best accuracy on 20 data sets.",6.2. Results,[0],[0]
"In addition, as can be seen, in comparing with other four methods which don’t consider margin distribution, the win/tie/loss counts show that mcODM is always better or comparable, almost never worse than it.",6.2. Results,[0],[0]
"In this section we study the influence of the number of classes on generalization performance and margin distribution, respectively.",6.3. Influence of the Number of Classes,[0],[0]
"Figure 1 plots the generalization performance of all the five methods on data set segment, and similar observation can be found for other data sets.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"As can be seen, when the number of classes is less than four, all methods perform quite well.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"However, as the fifth class is added, the generalization performance of other four methods decreases drastically.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"This might be attributable to the introduction of some noisy data, which SVM-style algorithms are very sensitive to since they optimize the minimum margin.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"On the other hand, our method considers the whole margin distribution, so it can be robust to noise and relatively more stable.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"Figure 2 plots the frequency histogram of margin distribution produced by mcSVM, ovaSVM and mcODM on data set segment as the number of classes increases from two to seven.",6.3.2. MARGIN DISTRIBUTION,[0],[0]
"As can be seen, when the number of classes is less than four, all methods can achieve good margin distribution, whereas with the increase of the number of classes, the other two methods begin to produce negative margins.",6.3.2. MARGIN DISTRIBUTION,[0],[0]
"At the same time, the distribution of our method becomes
“sharper”, which prevents the instance with small margin, so our method can still perform relatively well as the number of classes increases, which is also consistent with the observation from Figure 1.",6.3.2. MARGIN DISTRIBUTION,[0],[0]
"We compare the single iteration time cost of our method with mcSVM, ovaSVM, ovoSVM on all the data sets except aloi, on which ovoSVM could not return results in 48 hours.",6.4. Time Cost,[0],[0]
All the experiments are performed with MATLAB 2012b on a machine with 8×2.60 GHz CPUs and 32GB main memory.,6.4. Time Cost,[0],[0]
The average CPU time (in seconds) on each data set is shown in Figure 3.,6.4. Time Cost,[0],[0]
"The binary SVM used in ovaSVM, ovoSVM and mcSVM are both implemented by the LIBLINEAR (Fan et al., 2008) package.",6.4. Time Cost,[0],[0]
"It can be seen that for small data sets, the efficiency of all the methods are similar, however, for data sets with more than ten classes, e.g., sector and rcv1, mcSVM and mcODM, which learn all the scroing functions at once, are much faster than ovaSVM and ovoSVM, owing to the inefficiency of binarydecomposition as discussed in Section 6.1.",6.4. Time Cost,[0],[0]
"Note that LIBLINEAR are very fast implementations of binary SVM and mcSVM, and this shows that our method is also computationally efficient.",6.4. Time Cost,[0],[0]
"Recent studies disclose that for binary class classification, maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution.",7. Conclusions,[0],[0]
"However, it remains open to the influence of margin distribution for multi-class classification.",7. Conclusions,[0],[0]
We try to answer this question in this paper.,7. Conclusions,[0],[0]
"After maximizing the margin mean and minimizing the margin variance simultaneously, the resultant optimization is a difficult non-differentiable non-convex programming.",7. Conclusions,[0],[0]
We propose mcODM to solve this problem efficiently.,7. Conclusions,[0],[0]
Extensive experiments on twenty two data sets validate the superiority of our method to four versions of multi-class SVMs.,7. Conclusions,[0],[0]
"In the future it will be interesting to extend mcODM to more general learning settings, i.e., multilabel learning and structured learning.",7. Conclusions,[0],[0]
This research was supported by the NSFC (61333014) and the Collaborative Innovation Center of Novel Software Technology and Industrialization.,Acknowledgements,[0],[0]
"Authors want to thank reviewers for helpful comments, and thank Dr. Wei Gao for reading a draft.",Acknowledgements,[0],[0]
"Recent studies disclose that maximizing the minimum margin like support vector machines does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution.",abstractText,[0],[0]
"Although it has been shown that for binary classification, characterizing the margin distribution by the firstand second-order statistics can achieve superior performance.",abstractText,[0],[0]
"It still remains open for multiclass classification, and due to the complexity of margin for multi-class classification, optimizing its distribution by mean and variance can also be difficult.",abstractText,[0],[0]
"In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine), which can solve this problem efficiently.",abstractText,[0],[0]
"We also give a theoretical analysis for our method, which verifies the significance of margin distribution for multi-class classification.",abstractText,[0],[0]
Empirical study further shows that mcODM always outperforms all four versions of multi-class SVMs on all experimental data sets.,abstractText,[0],[0]
Multi-Class Optimal Margin Distribution Machine,title,[0],[0]
"Many tasks in scientific and engineering applications can be framed as bandit optimisation problems, where we need to sequentially evaluate a noisy black-box function f : X → R with the goal of finding its optimum.",1. Introduction,[0],[0]
"Some applications include hyper-parameter tuning in machine learning (Hutter et al., 2011; Snoek et al., 2012), optimal policy search (Lizotte et al., 2007; Martinez-Cantin et al., 2007) and scientific experiments (Gonzalez et al., 2014; Parkinson et al., 2006).
",1. Introduction,[0],[0]
"*Equal contribution 1Carnegie Mellon University, Pittsburgh PA, USA 2Rice University, Houston TX, USA.",1. Introduction,[0],[0]
"Correspondence to: Kirthevasan Kandasamy <kandasamy@cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Typically, in such applications, each function evaluation is expensive, and conventionally, the bandit literature has focused on developing methods for finding the optimum while keeping the number of evaluations to f at a minimum.
",1. Introduction,[0],[0]
"However, with increasingly expensive function evaluations, conventional methods have become infeasible as a significant cost needs to be expended before we can learn anything about f .",1. Introduction,[0],[0]
"As a result, multi-fidelity optimisation methods have recently gained attention (Cutler et al., 2014; Kandasamy et al., 2016a; Li et al., 2016).",1. Introduction,[0],[0]
"As the name suggests, these methods assume that we have access to lower fidelity approximations to f which can be evaluated instead of f .",1. Introduction,[0],[0]
"The lower the fidelity, the cheaper the evaluation, but it provides less accurate information about f .",1. Introduction,[0],[0]
"For example, when optimising the configuration of an expensive real world robot, its performance can be approximated using cheaper computer simulations.",1. Introduction,[0],[0]
"The goal is to use the cheap approximations to guide search for the optimum of f , and reduce the overall cost of optimisation.",1. Introduction,[0],[0]
"However, most multi-fidelity work assume only a finite number of approximations.",1. Introduction,[0],[0]
"In this paper, we study multi-fidelity optimisation when there is access to a continuous spectrum of approximations.
",1. Introduction,[0],[0]
"To motivate this set up, consider tuning a classification algorithm over a space of hyper-parameters X by maximising a validation set accuracy.",1. Introduction,[0],[0]
The algorithm is to be trained using N• data points via an iterative algorithm for T• iterations.,1. Introduction,[0],[0]
"However, we wish to use fewer training points N <",1. Introduction,[0],[0]
N• and/or fewer iterations T < T• to approximate the validation accuracy.,1. Introduction,[0],[0]
We can view validation accuracy as a function g :,1. Introduction,[0],[0]
"[1, N•]",1. Introduction,[0],[0]
"× [1, T•]",1. Introduction,[0],[0]
"× X → R where evaluating g(N,T, x) requires training the algorithm with N points for T iterations with the hyper-parameters x.",1. Introduction,[0],[0]
"If the training complexity of the algorithm is quadratic in data size and linear in the number of iterations, then the cost of this evaluation is λ(N,T ) = O(N2T ).",1. Introduction,[0],[0]
"Our goal is to find the optimum when N = N•, and T = T•, i.e. we wish to maximise f(x) = g(N•, T•, x).
",1. Introduction,[0],[0]
"In this setting, while N,T are technically discrete choices, they are more naturally viewed as coming from a continuous 2 dimensional fidelity space, [1, N•]",1. Introduction,[0],[0]
×,1. Introduction,[0],[0]
"[1, T•].",1. Introduction,[0],[0]
"One might hope that cheaper queries to g(N,T, ·) with N,T less than N•, T• can be used to learn about g(N•, T•, ·) and consequently optimise it using less overall cost.",1. Introduction,[0],[0]
"Indeed, this
is the case with many machine learning algorithms where cross validation performance tends to vary smoothly with data set size and number of iterations.",1. Introduction,[0],[0]
"Therefore, one may use cheap low fidelity experiments with small (N,T ) to discard bad hyper-parameters and deploy expensive high fidelity experiments with large (N,T ) only in a small but promising region.",1. Introduction,[0],[0]
"The main theoretical result of this paper (Theorem 1) shows that our proposed algorithm, BOCA, exhibits precisely this behaviour.
",1. Introduction,[0],[0]
"Continuous approximations also arise in simulation studies: where simulations can be carried out at varying levels of granularity, on-line advertising: where an ad can be controlled by continuous parameters such as display time or target audience, and several other experiment design tasks.",1. Introduction,[0],[0]
"In fact, in many multi-fidelity papers, the finite approximations were obtained by discretising a continuous space (Huang et al., 2006; Kandasamy et al., 2016a).",1. Introduction,[0],[0]
"Here, we study a Bayesian optimisation technique that is directly designed for continuous fidelity spaces and is potentially applicable to more general spaces.",1. Introduction,[0],[0]
"Our main contributions are,
1.",1. Introduction,[0],[0]
A novel setting and model for multi-fidelity optimisation with continuous approximations using Gaussian process (GP) assumptions.,1. Introduction,[0],[0]
"We develop a novel algorithm, BOCA, for this setting.
2.",1. Introduction,[0],[0]
A theoretical analysis characterising the behaviour and regret bound for BOCA. 3.,1. Introduction,[0],[0]
"An empirical study which demonstrates that BOCA outperforms alternatives, both multi-fidelity and otherwise, on a series of synthetic problems and real examples in hyper-parameter tuning and astrophysics.
",1. Introduction,[0],[0]
"Related Work Bayesian optimisation (BO), refers to a suite of techniques for bandit optimisation which use a prior belief distribution for f .",1. Introduction,[0],[0]
"While there are several techniques for BO (de Freitas et al., 2012; Hernández-Lobato et al., 2014; Jones et al., 1998; Mockus, 1994; Thompson, 1933), our work will build on the Gaussian process upper confidence bound (GP-UCB) algorithm of Srinivas et al. (2010).",1. Introduction,[0],[0]
"GP-UCB models f as a GP and uses upper confidence bound (UCB) (Auer, 2003) techniques to determine the next point for evaluation.
",1. Introduction,[0],[0]
"BO techniques have been used in developing multi-fidelity optimisation methods in various applications such as hyperparameter tuning and industrial design (Forrester et al., 2007; Huang et al., 2006; Klein et al., 2015; Lam et al., 2015; Poloczek et al., 2016; Swersky et al., 2013).",1. Introduction,[0],[0]
"However, these methods are either problem specific and/or only use a finite number of fidelities.",1. Introduction,[0],[0]
"Further, none of them come with theoretical underpinnings.",1. Introduction,[0],[0]
"Recent work has studied multi-fidelity methods for specific problems such as hyperparameter tuning, active learning and reinforcement learning (Agarwal et al., 2011; Cutler et al., 2014; Li et al., 2016; Sabharwal et al., 2015; Zhang & Chaudhuri, 2015).",1. Introduction,[0],[0]
"While
some of the above tasks can be framed as optimisation problems, the methods themselves are specific to the problem considered.",1. Introduction,[0],[0]
"Our method is more general as it applies to any bandit optimisation task.
",1. Introduction,[0],[0]
Perhaps the closest work to us is that of Kandasamy et al. (2016a;b;c) who developed MF-GP-UCB assuming a finite number of approximations to f .,1. Introduction,[0],[0]
"While this line of work was the first to provide theoretical guarantees for multifidelity optimisation, it has two important shortcomings.",1. Introduction,[0],[0]
"First, they make strong assumptions, particularly a uniform bound on the difference between the expensive function and an approximation.",1. Introduction,[0],[0]
This does not allow for instances where an approximation might be good at certain regions but not at the other.,1. Introduction,[0],[0]
"In contrast, our probabilistic treatment between fidelities is is robust to such cases.",1. Introduction,[0],[0]
"Second, their model does not allow sharing information between fidelities; each approximation is treated independently.",1. Introduction,[0],[0]
"Not only is this wasteful as lower fidelities can provide useful information about higher fidelities, it also means that the algorithm might perform poorly if the fidelities are not designed properly.",1. Introduction,[0],[0]
We demonstrate this with an experiment in Section 4.,1. Introduction,[0],[0]
"On the other hand, our model allows sharing information across the fidelity space in a natural way.",1. Introduction,[0],[0]
"In addition, we can also handle continuous approximations whereas their method is strictly for a finite number of approximations.",1. Introduction,[0],[0]
"That said, BOCA inherits a key intuition from MF-GP-UCB, which is to choose a fidelity only if we have sufficiently reduced the uncertainty at all lower fidelities.",1. Introduction,[0],[0]
"Besides this, there are considerable differences in the mechanics of the algorithm and proof techniques.",1. Introduction,[0],[0]
"As we proceed, we will draw further comparisons to Kandasamy et al. (2016a).",1. Introduction,[0],[0]
Gaussian processes: A GP over a space X is a random process from X to R. GPs are typically used as a prior for functions in Bayesian nonparametrics.,2.1. Some Background Material,[0],[0]
It is characterised by a mean function µ : X → R and a covariance function (or kernel) κ :,2.1. Some Background Material,[0],[0]
"X 2 → R. If f ∼ GP(µ, κ), then f(x) is distributed normally N (µ(x), κ(x, x)) for all x ∈ X .",2.1. Some Background Material,[0],[0]
"Suppose that we are given n observations Dn = {(xi, yi)}ni=1 from this GP, where xi ∈ X , yi = f(xi)",2.1. Some Background Material,[0],[0]
+,2.1. Some Background Material,[0],[0]
i ∈ R,2.1. Some Background Material,[0],[0]
"and i ∼ N (0, η2).",2.1. Some Background Material,[0],[0]
"Then the posterior process f |Dn is also a GP with mean µn and covariance κn given by
µn(x) =",2.1. Some Background Material,[0],[0]
k >,2.1. Some Background Material,[0],[0]
"(K + η2I)−1Y, (1)
κn(x, x ′) = κ(x, x′)− k>(K +",2.1. Some Background Material,[0],[0]
"η2I)−1k′.
Here",2.1. Some Background Material,[0],[0]
Y ∈,2.1. Some Background Material,[0],[0]
"Rn is a vector with Yi = yi, and k, k′ ∈",2.1. Some Background Material,[0],[0]
"Rn are such that ki = κ(x, xi), k′i = κ(x
′, xi).",2.1. Some Background Material,[0],[0]
"The matrix K ∈ Rn×n is given by Ki,j = κ(xi, xj).",2.1. Some Background Material,[0],[0]
"We refer the reader to chapter 2 of Rasmussen & Williams (2006) for more on the basics of GPs and their use in regression.
",2.1. Some Background Material,[0],[0]
Radial kernels:,2.1. Some Background Material,[0],[0]
The prior covariance functions of GPs are typically taken to be radial kernels; some examples are the squared exponential (SE) and Matérn kernels.,2.1. Some Background Material,[0],[0]
"Using a radial kernel means that the prior covariance can be written as κ(x, x′) = κ0φ(‖x−x′‖) and depends only on the distance between x and x′. Here, the scale parameter κ0 captures the magnitude f could deviate from µ. The function φ : R+ → R+ is a decreasing function with ‖φ‖∞ = φ(0) = 1.",2.1. Some Background Material,[0],[0]
"In this paper, we will use the SE kernel in a running example to convey the intuitions in our methods.",2.1. Some Background Material,[0],[0]
"For the SE kernel, φ(r) = φh(r) = exp(−r2/(2h2)), where h ∈ R+, called the bandwidth of the kernel, controls the smoothness of the GP.",2.1. Some Background Material,[0],[0]
"When h is large, the samples drawn from the GP tend to be smoother as illustrated in Fig. 1.",2.1. Some Background Material,[0],[0]
"We will reference this observation frequently in the text.
",2.1. Some Background Material,[0],[0]
"GP-UCB: The Gaussian Process Upper Confidence Bound (GP-UCB) algorithm of Srinivas et al. (2010) is a method for bandit optimisation, which, like many other BO methods, models f as a sample from a Gaussian process.",2.1. Some Background Material,[0],[0]
"At time t, the next point xt for evaluating f is chosen via the following procedure.",2.1. Some Background Material,[0],[0]
"First, we construct an upper confidence bound ϕt(x) = µt−1(x) + β 1/2 t σt−1(x) for the GP.",2.1. Some Background Material,[0],[0]
µt−1 is the posterior mean of the GP conditioned on the previous t− 1 evaluations and σt−1 is the posterior standard deviation.,2.1. Some Background Material,[0],[0]
"Following other UCB algorithms (Auer, 2003), the next point is chosen by maximising ϕt, i.e. xt = argmaxx∈X ϕt(x).",2.1. Some Background Material,[0],[0]
The µt−1 term encourages an exploitative strategy – in that we want to query regions where we already believe f is high – and σt−1 encourages an exploratory strategy – in that we want to query where we are uncertain about f so that we do not miss regions which have not been queried yet.,2.1. Some Background Material,[0],[0]
"βt, which is typically increasing with t, controls the trade-off between exploration and exploitation.",2.1. Some Background Material,[0],[0]
We have provided a brief review of GP-UCB in Appendix A.1.,2.1. Some Background Material,[0],[0]
"Our goal in bandit optimisation is to maximise a function f : X → R, over a domain X .",2.2. Problem Set Up,[0],[0]
When we evaluate f at x ∈ X we observe y = f(x) + where E[ ] = 0.,2.2. Problem Set Up,[0],[0]
Let,2.2. Problem Set Up,[0],[0]
x? ∈,2.2. Problem Set Up,[0],[0]
argmaxx∈X f(x) be a maximiser of f and f? = f(x?),2.2. Problem Set Up,[0],[0]
be the maximum value.,2.2. Problem Set Up,[0],[0]
"An algorithm for bandit optimisation is a sequence of points {xt}t≥0, where, at time t, the algorithm chooses to evaluate f at xt based on previous queries and
observations {(xi, yi)}t−1i=1 .",2.2. Problem Set Up,[0],[0]
"After n queries to f , its goal is to achieve small simple regret Sn, as defined below.
",2.2. Problem Set Up,[0],[0]
"Sn = min t=1,...,n
f?",2.2. Problem Set Up,[0],[0]
− f(xt).,2.2. Problem Set Up,[0],[0]
"(2)
Continuous Approximations: In this work, we will let f be a slice of a function g that lies in a larger space.",2.2. Problem Set Up,[0],[0]
"Precisely, we will assume the existence of a fidelity space Z and a function g : Z×X → R defined on the product space of the fidelity space and domain.",2.2. Problem Set Up,[0],[0]
"The function f which we wish to maximise is related to g via f(·) = g(z•, ·), where z• ∈ Z .",2.2. Problem Set Up,[0],[0]
"For instance, in the hyper-parameter tuning example from Section 1, Z = [1, N•] ×",2.2. Problem Set Up,[0],[0]
"[1, T•]",2.2. Problem Set Up,[0],[0]
and z• =,2.2. Problem Set Up,[0],[0]
"[N•, T•].",2.2. Problem Set Up,[0],[0]
"Our goal is to find a maximiser x? ∈ argmaxx f(x) = argmaxx g(z•, x).",2.2. Problem Set Up,[0],[0]
We have illustrated this setup in Fig. 2.,2.2. Problem Set Up,[0],[0]
"In the rest of the manuscript, the term “fidelities” will refer to points z in the fidelity space Z .
",2.2. Problem Set Up,[0],[0]
"The multi-fidelity framework is attractive when the following two conditions are true about the problem.
",2.2. Problem Set Up,[0],[0]
1.,2.2. Problem Set Up,[0],[0]
"There exist fidelities z ∈ Z where evaluating g is cheaper than evaluating at z•. To this end, we will associate a known cost function λ : Z → R+.",2.2. Problem Set Up,[0],[0]
"In the hyper-parameter tuning example, λ(z) = λ(N,T ) = O(N2T ).",2.2. Problem Set Up,[0],[0]
"It is helpful to think of z• as being the most expensive fidelity, i.e. maximiser of λ, and that λ(z) decreases as we move away from z•. However, this notion is strictly not necessary for our algorithm or results.
",2.2. Problem Set Up,[0],[0]
2.,2.2. Problem Set Up,[0],[0]
"The cheap g(z, ·) evaluation gives us information about g(z•, ·).",2.2. Problem Set Up,[0],[0]
This is true if g is smooth across the fidelity space as illustrated in Fig. 2.,2.2. Problem Set Up,[0],[0]
"As we will describe shortly, this smoothness can be achieved by modelling g as a GP with an appropriate kernel for the fidelity space Z .
",2.2. Problem Set Up,[0],[0]
"In the above setup, a multi-fidelity algorithm is a sequence of query-fidelity pairs {(zt, xt)}t≥0 where, at time t, the algorithm chooses zt ∈ Z and xt ∈ X , and observes yt =
g(zt, xt) + where E[ ] = 0.",2.2. Problem Set Up,[0],[0]
"The choice of (zt, xt) can of course depend on the previous fidelity-query-observation triples {(zi, xi, yi)}t−1i=1 .
",2.2. Problem Set Up,[0],[0]
Multi-fidelity Simple Regret: We provide bounds on the simple regret S(Λ) of a multi-fidelity optimisation method after it has spent capital Λ of a resource.,2.2. Problem Set Up,[0],[0]
"Following Kandasamy et al. (2016a); Srinivas et al. (2010), we will aim to provide any capital bounds, meaning that an algorithm would be expected to do well for all values of (sufficiently large)",2.2. Problem Set Up,[0],[0]
"Λ. Say we have made N queries to g within capital Λ, i.e. N is the random quantity such that N = max{n",2.2. Problem Set Up,[0],[0]
≥ 1 : ∑n t=1 λ(zt) ≤ Λ}.,2.2. Problem Set Up,[0],[0]
"While the cheap evaluations at z 6= z• are useful in guiding search for the optimum of g(z•, ·), there is no reward for optimising a cheaper g(z, ·).",2.2. Problem Set Up,[0],[0]
"Accordingly, we define the simple regret after capital Λ as,
S(Λ) =  min t∈{1,...,N} s.t zt=z• f?",2.2. Problem Set Up,[0],[0]
"− f(xt) if we have queried at z•,
+∞ otherwise.
",2.2. Problem Set Up,[0],[0]
"This definition reduces to the single fidelity definition (2) when we only query g at z•. It is also similar to the definition in Kandasamy et al. (2016a), but unlike them, we do not impose additional boundedness constraints on f or g.
Before we proceed, we note that it is customary in the bandit literature to analyse cumulative regret.",2.2. Problem Set Up,[0],[0]
"However, the definition of cumulative regret depends on the application at hand (Kandasamy et al., 2016c) and the results in this paper can be extended to to many sensible notions of cumulative regret.",2.2. Problem Set Up,[0],[0]
"However, both to simplify exposition and since our focus in this paper is optimisation, we stick to simple regret.
",2.2. Problem Set Up,[0],[0]
"Assumptions: As we will be primarily focusing on continuous and compact domains and fidelity spaces, going forward we will assume, without any loss of generality, that X =",2.2. Problem Set Up,[0],[0]
"[0, 1]d and Z =",2.2. Problem Set Up,[0],[0]
"[0, 1]p.",2.2. Problem Set Up,[0],[0]
We discuss non-continuous settings briefly at the end of Section 3.,2.2. Problem Set Up,[0],[0]
"In keeping with similar work in the Bayesian optimisation literature, we will assume g ∼ GP(0, κ) and upon querying at (z, x) we observe y = g(z, x) + where ∼ N (0, η2).",2.2. Problem Set Up,[0],[0]
κ :,2.2. Problem Set Up,[0],[0]
(Z × X )2 → R is the prior covariance defined on the product space.,2.2. Problem Set Up,[0],[0]
"In this work, we will study exclusively κ of the following form,
κ([z, x], [z′, x′]) = κ0 φZ(‖z",2.2. Problem Set Up,[0],[0]
− z′‖)φX (‖x− x′‖).,2.2. Problem Set Up,[0],[0]
"(3)
Here, κ0 ∈ R+ is the scale parameter and φZ , φX are radial kernels defined on Z,X respectively.",2.2. Problem Set Up,[0],[0]
The fidelity space kernel φZ is an important component in this work.,2.2. Problem Set Up,[0],[0]
"It controls the smoothness of g across the fidelity space and hence determines how much information the lower fidelities provide about g(z•, ·).",2.2. Problem Set Up,[0],[0]
"For example, suppose that φZ was a SE kernel.",2.2. Problem Set Up,[0],[0]
"A favourable setting for a multi-fidelity method would be for φZ to have a large bandwidth hZ as that would imply
that g is very smooth across Z .",2.2. Problem Set Up,[0],[0]
We will see that hZ determines the behaviour and theoretical guarantees of BOCA in a natural way when φZ is the SE kernel.,2.2. Problem Set Up,[0],[0]
"To formalise this notion, we will define the following function ξ : Z →",2.2. Problem Set Up,[0],[0]
"[0, 1].
ξ(z) = √ 1− φZ(‖z",2.2. Problem Set Up,[0],[0]
"− z•‖)2 (4)
One interpretation of ξ(z) is that it measures the gap in information about g(z•, ·) when we query at z 6=",2.2. Problem Set Up,[0],[0]
"z•. That is, it is the price we have to pay, in information, for querying at a cheap fidelity.",2.2. Problem Set Up,[0],[0]
Observe that ξ increases when we move away from z• in the fidelity space.,2.2. Problem Set Up,[0],[0]
"For the SE kernel, it can be shown1 ξ(z)",2.2. Problem Set Up,[0],[0]
≈ ‖z−z•‖hZ .,2.2. Problem Set Up,[0],[0]
"For large hZ , g is smoother across Z and we can expect the lower fidelities to be more informative about f ; as expected the information gap ξ is small for large hZ .",2.2. Problem Set Up,[0],[0]
"If hZ is small and g is not smooth, the gap ξ is large and lower fidelities are not as informative.
",2.2. Problem Set Up,[0],[0]
"Before we present our algorithm for the above setup, we will introduce notation for the posterior GPs for g and f .",2.2. Problem Set Up,[0],[0]
"Let Dn = {(zi, xi, yi)}ni=1 be n fidelity, query, observation values from the GP g, where yi was observed when evaluating g(zi, xi).",2.2. Problem Set Up,[0],[0]
"We will denote the posterior mean and standard deviation of g conditioned on Dn by νn and τn respectively (νn, τn can be computed from (1) by replacing x←",2.2. Problem Set Up,[0],[0]
"[z, x]).",2.2. Problem Set Up,[0],[0]
"Therefore g(z, x)|Dn ∼ N (νn(z, x), τ2n(z, x)) for all (z, x) ∈ Z × X .",2.2. Problem Set Up,[0],[0]
"We will further denote
µn(·) = νn(z•, ·), σn(·) = τn(z•, ·), (5)
to be the posterior mean and standard deviation of g(z•, ·) = f(·).",2.2. Problem Set Up,[0],[0]
"It follows that f |Dn is also a GP and satisfies f(x)|Dn ∼ N (µn(x), σ2n(x)) for all x ∈ X .
3.",2.2. Problem Set Up,[0],[0]
"BOCA: Bayesian Optimisation with Continuous Approximations
BOCA is a sequential strategy to select a domain point xt ∈ X and fidelity zt ∈ Z at time t based on previous observations.",2.2. Problem Set Up,[0],[0]
"At time t, we will first construct an upper confidence bound ϕt for the function f we wish to optimise.",2.2. Problem Set Up,[0],[0]
"It takes the form,
ϕt(x) = µt−1(x) + β 1/2 t σt−1(x).",2.2. Problem Set Up,[0],[0]
"(6)
Recall from (5) that µt−1 and σt−1 are the posterior mean and standard deviation of f using the observations from the previous t−1 time steps at all fidelities, i.e. the entireZ×X space.",2.2. Problem Set Up,[0],[0]
"We will specify βt in Theorems 1, 8.",2.2. Problem Set Up,[0],[0]
"Following other UCB algorithms, our next point xt in the domain X for evaluating g is a maximiser of ϕt, i.e. xt ∈ argmaxx∈X ϕt(x).
",2.2. Problem Set Up,[0],[0]
"Next, we need to determine the fidelity zt ∈ Z to query g. 1Strictly, ξ(z) ≤",2.2. Problem Set Up,[0],[0]
"‖z− z•‖/hZ , but the inequality is tighter for larger hZ .",2.2. Problem Set Up,[0],[0]
"In any case, ξ is strictly decreasing with hZ .
",2.2. Problem Set Up,[0],[0]
"For this we will first select a subset Zt(xt) of Z as follows,
Zt(xt) = { z ∈ Z : λ(z) < λ(z•), τt−1(z, xt) > γ(z),
ξ(z) >",2.2. Problem Set Up,[0],[0]
β −1/2,2.2. Problem Set Up,[0],[0]
"t ‖ξ‖∞ } , (7)
where γ(z) = √ κ0 ξ(z)
( λ(z)
λ(z•)
)q .
",2.2. Problem Set Up,[0],[0]
"Here, ξ is the information gap function in (4) and τt−1 is the posterior standard deviation of g, and p, d are the dimensionalities of Z,X .",2.2. Problem Set Up,[0],[0]
The exponent q depends on the kernel used for φZ .,2.2. Problem Set Up,[0],[0]
"For e.g., for the SE kernel, q = 1/(p+ d + 2).",2.2. Problem Set Up,[0],[0]
We filter out the fidelities we consider at time t using three conditions as specified above.,2.2. Problem Set Up,[0],[0]
We elaborate on these conditions in more detail in Section 3.1.,2.2. Problem Set Up,[0],[0]
"If Zt is not empty, we choose the cheapest fidelity in this set, i.e. zt ∈ argminz∈Zt λ(z).",2.2. Problem Set Up,[0],[0]
"If Zt is empty, we choose zt = z•.
We have summarised the resulting procedure below in Algorithm 1.",2.2. Problem Set Up,[0],[0]
An important advantage of BOCA is that it only requires specifying the GP hyper-parameters for g such as the kernel κ.,2.2. Problem Set Up,[0],[0]
"In practice, this can be achieved by various effective heuristics such as maximising the GP marginal likelihood or cross validation which are standard in most BO methods.",2.2. Problem Set Up,[0],[0]
"In contrast, MF-GP-UCB of Kandasamy et al. (2016a) requires tuning several other hyper-parameters.
",2.2. Problem Set Up,[0],[0]
Algorithm 1 BOCA Input: kernel κ. •,2.2. Problem Set Up,[0],[0]
"Set ν0(·)← 0, τ0(·)← κ(·, ·)1/2, D0 ← ∅. • for t = 1, 2, . . .
",2.2. Problem Set Up,[0],[0]
1. xt,2.2. Problem Set Up,[0],[0]
← argmaxx∈X ϕt(x).,2.2. Problem Set Up,[0],[0]
See (6) 2.,2.2. Problem Set Up,[0],[0]
zt ← argminz∈Zt(xt)∪{z•} λ(z).,2.2. Problem Set Up,[0],[0]
See (7) 3.,2.2. Problem Set Up,[0],[0]
yt,2.2. Problem Set Up,[0],[0]
"← Query g at (zt, xt).",2.2. Problem Set Up,[0],[0]
4.,2.2. Problem Set Up,[0],[0]
"Dt ← Dt−1 ∪ {(zt, xt, yt)}.",2.2. Problem Set Up,[0],[0]
"Update posterior mean νt, and standard deviation τt for g conditioned on Dt.",2.2. Problem Set Up,[0],[0]
"We will now provide an intuitive justification for the three conditions in the selection criterion for zt, i.e., equation (7).",3.1. Fidelity Selection Criterion,[0],[0]
"The first condition, λ(z) < λ(z•) is fairly obvious; since we wish to optimise g(z•, ·) and since we are not rewarded for queries at other fidelities, there is no reason to consider fidelities that are more expensive than z•.
The second condition, τt−1(z, xt) > γ(z) says that we will only consider fidelities where the posterior variance is larger than a threshold γ(z) = √ κ0ξ(z)(λ(z)/λ(z•))
q , which depends critically on two quantities, the cost function λ and the information gap ξ.",3.1. Fidelity Selection Criterion,[0],[0]
"As a first step towards parsing this condition, observe that a reasonable multi-fidelity strategy should be inclined to query cheap fidelities and learn about
g before querying expensive fidelities.",3.1. Fidelity Selection Criterion,[0],[0]
"As γ(z) is monotonically increasing in λ(z), it becomes easier for a cheap z to satisfy τt−1(z, xt) > γ(z) and be included in Zt at time",3.1. Fidelity Selection Criterion,[0],[0]
"t. Moreover, since we choose zt to be the minimiser of λ in Zt, a cheaper fidelity will always be chosen over expensive ones if included in Zt.",3.1. Fidelity Selection Criterion,[0],[0]
"Second, if a particular fidelity z is far away from z•, it probably contains less information about g(z•, ·).",3.1. Fidelity Selection Criterion,[0],[0]
"Again, a reasonable multi-fidelity strategy should be discouraged from making such queries.",3.1. Fidelity Selection Criterion,[0],[0]
This is precisely the role of the information gap ξ which is increasing with ‖z,3.1. Fidelity Selection Criterion,[0],[0]
"− z•‖. As z moves away from z•, γ(z) increases and it becomes harder to satisfy τt−1(z, xt) > γ(z).",3.1. Fidelity Selection Criterion,[0],[0]
"Therefore, such a z is less likely to be included in Zt(xt) and be considered for evaluation.",3.1. Fidelity Selection Criterion,[0],[0]
"Our analysis reveals that setting γ as in (7) is a reasonable trade off between cost and information in the approximations available to us; cheaper fidelities cost less, but provide less accurate information about the function f we wish to optimise.",3.1. Fidelity Selection Criterion,[0],[0]
It is worth noting that the second condition is similar in spirit to Kandasamy et al. (2016a) who proceed from a lower to higher fidelity only when the lower fidelity variance is smaller than a threshold.,3.1. Fidelity Selection Criterion,[0],[0]
"However, while they treat the threshold as a hyper-parameter, we are able to explicitly specify theoretically motivated values.
",3.1. Fidelity Selection Criterion,[0],[0]
The third condition in (7) is ξ(z) > ‖ξ‖∞/β1/2t .,3.1. Fidelity Selection Criterion,[0],[0]
"Since ξ is increasing as we move away from z•, it says we should exclude fidelities inside a (small) neighbourhood of z•. Recall that if Zt is empty, BOCA will choose z• by default.",3.1. Fidelity Selection Criterion,[0],[0]
"But when it is not empty, we want to prevent situations where we get arbitrarily close to z• but not actually query at z•. Such pathologies can occur when we are dealing with a continuum of fidelities and this condition forces BOCA to pick z• instead of querying very close to it.",3.1. Fidelity Selection Criterion,[0],[0]
"Observe that since βt is increasing with t, this neighborhood is shrinking with time and therefore the algorithm will eventually have the opportunity to evaluate fidelities close to z•.",3.1. Fidelity Selection Criterion,[0],[0]
We now present our main theoretical contributions.,3.2. Theoretical Results,[0],[0]
"In order to simplify the exposition and convey the gist of our results, we will only present a simplified version of our theorems.",3.2. Theoretical Results,[0],[0]
"We will suppress constants, polylog terms, and other technical details that arise due to a covering argument in our proofs.",3.2. Theoretical Results,[0],[0]
"A rigorous treatment is available in Appendix B.
Maximum Information Gain: Up until this point, we have not discussed much about the kernel φX of the domain X .",3.2. Theoretical Results,[0],[0]
"Since we are optimising f over X , it is natural to expect that this will appear in the bounds.",3.2. Theoretical Results,[0],[0]
Srinivas et al. (2010) showed that the statistical difficulty of GP bandits is determined by the Maximum Information Gain (MIG) which measures the maximum information a subset of observations have about f .,3.2. Theoretical Results,[0],[0]
We denote it by Ψn(A) where A is a subset of X and n is the number of queries to f .,3.2. Theoretical Results,[0],[0]
"We refer the reader
to Appendix B for a formal definition of MIG.",3.2. Theoretical Results,[0],[0]
"For the current exposition however, it suffices to know that for radial kernels, Ψn(A) increases with n and the volume vol(A) of A.",3.2. Theoretical Results,[0],[0]
"For instance, when we use an SE kernel for φX , we have Ψn(A) ∝",3.2. Theoretical Results,[0],[0]
"vol(A) log(n)d+1and for a Matérn kernel with smoothness parameter ν, Ψn(A) ∝ vol(A)n1− ν 2ν+d(d+1) .",3.2. Theoretical Results,[0],[0]
"(Srinivas et al., 2010).",3.2. Theoretical Results,[0],[0]
"Let nΛ = bΛ/λ(z•)c denote the number of queries by a single fidelity algorithm within capital Λ. Srinivas et al. (2010) showed that the simple regret S(Λ) for GP-UCB after capital Λ can be bounded by,
Simple Regret for GP-UCB: S(Λ) .",3.2. Theoretical Results,[0],[0]
√ ΨnΛ(X ) nΛ .,3.2. Theoretical Results,[0],[0]
"(8)
In our analysis of BOCA we show that most queries to g at fidelity z• will be confined to a small subset of X which contains the optimum x?.",3.2. Theoretical Results,[0],[0]
"Precisely, after capital Λ, for any α ∈ (0, 1), we show there exists ρ > 0",3.2. Theoretical Results,[0],[0]
"such that the number of queries outside the following set Xρ is less than nαΛ.
Xρ = { x ∈ X",3.2. Theoretical Results,[0],[0]
: f? − f(x) ≤,3.2. Theoretical Results,[0],[0]
2ρ,3.2. Theoretical Results,[0],[0]
√ κ0 ‖ξ‖∞ } .,3.2. Theoretical Results,[0],[0]
"(9)
Here, ξ is from (4).",3.2. Theoretical Results,[0],[0]
"While it is true that any optimisation algorithm would eventually query extensively in a neighbourhood around the optimum, a strong result of the above form is not always possible.",3.2. Theoretical Results,[0],[0]
"For instance, for GP-UCB, the best achievable bound on the number of queries in any set that does not contain x? is n 1/2 Λ .",3.2. Theoretical Results,[0],[0]
"The fact that Xρ exists relies crucially on the multi-fidelity assumptions and that our algorithm leverages information from lower fidelities when querying at z•. As ξ is small when g is smooth across Z , the set Xρ will be small when the approximations are highly informative about g(z•, ·).",3.2. Theoretical Results,[0],[0]
"For e.g., when φZ is a SE kernel, we haveXρ ≈ {x ∈ X : f?−f(x) ≤ 2ρ √ κ0p/hZ}.",3.2. Theoretical Results,[0],[0]
"When hZ is large and g is smooth across Z , Xρ is small as the right side of the inequality is smaller.",3.2. Theoretical Results,[0],[0]
"As BOCA confines most of its evaluations to this small set containing x?, we will be able to achieve much better regret than GP-UCB.",3.2. Theoretical Results,[0],[0]
"When hZ is small and g is not smooth across Z , the set Xρ becomes large and the advantage of multi-fidelity optimisation diminishes.",3.2. Theoretical Results,[0],[0]
"One can similarly argue that for the Matérn kernel, as the parameter ν increases, g will be smoother across Z , and Xρ becomes smaller yielding better bounds on the regret.",3.2. Theoretical Results,[0],[0]
"Below, we provide an informal statement of our main theoretical result. .",3.2. Theoretical Results,[0],[0]
", will denote inequality and equality ignoring constant and polylog terms.
",3.2. Theoretical Results,[0],[0]
"Theorem 1 (Informal, Regret of BOCA).",3.2. Theoretical Results,[0],[0]
"Let g ∼ GP(0, κ) where κ satisfies (3).",3.2. Theoretical Results,[0],[0]
Choose βt d log(t/δ).,3.2. Theoretical Results,[0],[0]
"Then, for sufficiently large Λ and for all α ∈ (0, 1), there exists ρ depending on α such that the following bound holds with probability at least 1− δ.
S(Λ) .",3.2. Theoretical Results,[0],[0]
√ ΨnΛ(Xρ) nΛ + √ ΨnαΛ(X ),3.2. Theoretical Results,[0],[0]
"n2−αΛ
In the above bound, the latter term vanishes fast due to the n−(1−α/2)Λ dependence.",3.2. Theoretical Results,[0],[0]
"When comparing this with (8), we see that we outperform GP-UCB by a factor of √ ΨnΛ(Xρ)/ΨnΛ(X ) √
vol(Xρ)/vol(X ) asymptotically.",3.2. Theoretical Results,[0],[0]
"If g is smooth across the fidelity space, Xρ is small and the gains over GP-UCB are significant.",3.2. Theoretical Results,[0],[0]
"If g becomes less smooth across Z , the bound decays gracefully, but we are never worse than GP-UCB up to constant factors.
",3.2. Theoretical Results,[0],[0]
Theorem 1 also has similarities to the bounds of Kandasamy et al. (2016a) who also demonstrate better regret than GPUCB by showing that it is dominated by queries inside a set X ′,3.2. Theoretical Results,[0],[0]
which contains the optimum.,3.2. Theoretical Results,[0],[0]
"However, their bounds depend critically on certain threshold hyper-parameters which determine the volume of X ′",3.2. Theoretical Results,[0],[0]
among other terms in their regret.,3.2. Theoretical Results,[0],[0]
"The authors of that paper note that their bounds will suffer if these hyper-parameters are not chosen appropriately, but do not provide theoretically justified methods to make this choice.",3.2. Theoretical Results,[0],[0]
"In contrast, many of the design choices for BOCA fall out naturally of our modeling assumptions.",3.2. Theoretical Results,[0],[0]
"Beyond this analogue, our results are not comparable to Kandasamy et al. (2016a) as the assumptions are different.
",3.2. Theoretical Results,[0],[0]
"Extensions: While we have focused on continuousZ , many of the ideas here can be extended to other settings.",3.2. Theoretical Results,[0],[0]
"If Z is a discrete subset of [0, 1]p our work extends straightforwardly.",3.2. Theoretical Results,[0],[0]
We reiterate that this will not be the same as the finite fidelity MF-GP-UCB algorithm as the assumptions are different.,3.2. Theoretical Results,[0],[0]
"In particular, Kandasamy et al. (2016a) are not able to effectively share information across fidelities as we do.",3.2. Theoretical Results,[0],[0]
We also believe that Algorithm 1 can be extended to arbitrary fidelity spaces Z provided that a kernel can be defined on Z .,3.2. Theoretical Results,[0],[0]
Our results can also be extended to discrete domains X and various other kernels for φX by adopting techniques from Srinivas et al. (2010).,3.2. Theoretical Results,[0],[0]
"We compare BOCA to the following four baselines: (i) GPUCB, (ii) the GP-EI criterion in BO (Jones et al., 1998), (iii) MF-GP-UCB (Kandasamy et al., 2016a) and (iv) MF-SKO, the multi-fidelity sequential kriging optimisation method from Huang et al. (2006).",4. Experiments,[0],[0]
All methods are based on GPs and we use the SE kernel for both the fidelity space and domain.,4. Experiments,[0],[0]
"The first two are not multi-fidelity methods, while the last two are finite multi-fidelity methods2.",4. Experiments,[0],[0]
Kandasamy et al. (2016a) also study some naive multi-fidelity algorithms and demonstrate that they do not perform well; as such we will not consider such alternatives here.,4. Experiments,[0],[0]
"In all our experiments, the fidelity space was designed to be Z =",4. Experiments,[0],[0]
"[0, 1]p with z• = 1p =",4. Experiments,[0],[0]
"[1, . .",4. Experiments,[0],[0]
.,4. Experiments,[0],[0]
", 1] ∈",4. Experiments,[0],[0]
"Rp being the most expensive fi-
2To our knowledge, the only other work that applies to continuous approximations is Klein et al. (2015) which was developed specifically for hyper-parameter tuning.",4. Experiments,[0],[0]
"Further, their implementation is not made available and is not straightforward to implement.
delity.",4. Experiments,[0],[0]
"For MF-GP-UCB and MF-SKO, we used 3 fidelities (2 approximations) where the approximations were obtained at z = 0.3331p and z = 0.6671p in Z .",4. Experiments,[0],[0]
"Empirically, we found that both algorithms did reasonably well with 1-3 approximations, but did not perform well with a large number of approximations (> 5); even the original papers restrict experiments to 1-3 approximations.",4. Experiments,[0],[0]
Implementation details for all methods are given in Appendix C.1.,4. Experiments,[0],[0]
The results for the first set of synthetic experiments are given in Fig. 3.,4.1. Synthetic Experiments,[0],[0]
"The title of each figure states the function used, and the dimensionalities p, d of the fidelity space and domain.",4.1. Synthetic Experiments,[0],[0]
"To reflect the setting in our theory, we add Gaussian noise to the function value when observing g at any (z, x).",4.1. Synthetic Experiments,[0],[0]
This makes the problem more challenging than standard global optimisation problems where function evaluations are not noisy.,4.1. Synthetic Experiments,[0],[0]
"The functions g, the cost functions λ and the noise variances η2 are given in Appendix C.2.
",4.1. Synthetic Experiments,[0],[0]
The first two panels in Fig. 3 are simple sanity checks.,4.1. Synthetic Experiments,[0],[0]
"In both cases, Z = [0, 1], X = [0, 1] and the functions were sampled from GPs.",4.1. Synthetic Experiments,[0],[0]
"The GP was made known to all methods, i.e. all methods used the true GP in picking the next point.",4.1. Synthetic Experiments,[0],[0]
"In the first panel, we used an SE kernel with bandwidth 0.1 for φX and 1.0 for φZ .",4.1. Synthetic Experiments,[0],[0]
"g is smooth across Z in this setting, and BOCA outperforms other baselines.",4.1. Synthetic Experiments,[0],[0]
The curve starts mid-way as BOCA is yet to query at z• up until that point.,4.1. Synthetic Experiments,[0],[0]
"The second panel uses the same set up as the first except
we used bandwidth 0.01 for φZ .",4.1. Synthetic Experiments,[0],[0]
"Even though g is highly un-smooth across Z , BOCA does not perform poorly.",4.1. Synthetic Experiments,[0],[0]
This corroborates a claim that we made earlier that BOCA can naturally adapt to the smoothness of the approximations.,4.1. Synthetic Experiments,[0],[0]
"The other multi-fidelity methods suffer in this setting.
",4.1. Synthetic Experiments,[0],[0]
"In the remaining experiments, we use some standard benchmarks for global optimisation.",4.1. Synthetic Experiments,[0],[0]
We modify them to obtain g and add noise to the observations.,4.1. Synthetic Experiments,[0],[0]
"As the kernel and other GP hyper-parameters are unknown, we learn them by maximising the marginal likelihood every 25 iterations.",4.1. Synthetic Experiments,[0],[0]
We outperform all methods on all problems except in the case of the Borehole function where MF-GP-UCB does better.,4.1. Synthetic Experiments,[0],[0]
The last synthetic experiment is the Branin function given in Fig. 4(a).,4.1. Synthetic Experiments,[0],[0]
"We used the same set up as above, but use 10 fidelities for MF-GP-UCB and MF-SKO where the kth fidelity is obtained at z = k101p in the fidelity space.",4.1. Synthetic Experiments,[0],[0]
Notice that the performance of finite fidelity methods deteriorate.,4.1. Synthetic Experiments,[0],[0]
"In particular, as MF-GP-UCB does not share information across fidelities, the approximations need to be designed carefully for the algorithm to work well.",4.1. Synthetic Experiments,[0],[0]
Our more natural modelling assumptions prevent such pitfalls.,4.1. Synthetic Experiments,[0],[0]
We next present two real examples in astrophysics and hyper-parameter tuning.,4.1. Synthetic Experiments,[0],[0]
"We do not add noise to the observations, but treat it as optimisation tasks, where the goal is to maximise the function.",4.1. Synthetic Experiments,[0],[0]
"We use data on TypeIa supernova for maximum likelihood inference on 3 cosmological parameters, the Hubble con-
stant H0 ∈ (60, 80), the dark matter fraction ΩM ∈ (0, 1) and dark energy fraction ΩΛ ∈ (0, 1); hence d = 3.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"The likelihood is given by the Robertson-Walker metric, the computation of which requires a one dimensional numerical integration for each point in the dataset.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"Unlike typical maximum likelihood problems, here the likelihood is only accessible via point evaluations.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
We use the dataset from Davis et al (2007) which has data on 192 supernovae.,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
We construct a p = 2 dimensional multi-fidelity problem where we can choose between data set size N ∈,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"[50, 192] and perform the integration on grids of size G ∈",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"[102, 106] via the trapezoidal rule.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"As the cost function for fidelity selection, we used λ(N,G) = NG as the computation time is linear in both parameters.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
Our goal is to maximise the average log likelihood at z• =,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"[192, 106].",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
For the finite fidelity methods we use three fidelities with the approximations available at z =,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"[97, 2.15× 103] and z = [145, 4.64× 104] (which correspond to 0.3331p and 0.6671p after rescaling as in Section 4.1).",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
The results are given in Fig. 4(b) where we plot the maximum average log likelihood against wall clock time as that is the cost in this experiment.,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
The plot includes the time taken by each method to tune the GPs and determine the next points/fidelities for evaluation.,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"We use the 20 news groups dataset (Joachims, 1996) in a text classification task.",4.3. Support Vector Classification with 20 news groups,[0],[0]
"We obtain the bag of words representation for each document, convert them to tf-idf features and feed them to a support vector classifier.",4.3. Support Vector Classification with 20 news groups,[0],[0]
"The goal is to tune the regularisation penalty and the temperature of the rbf kernel both in the range [10−2, 103]; hence d = 2.",4.3. Support Vector Classification with 20 news groups,[0],[0]
The support vector implementation was taken from scikit-learn.,4.3. Support Vector Classification with 20 news groups,[0],[0]
We set this up as a 2 dimensional multi-fidelity problem where we can choose a dataset size N ∈,4.3. Support Vector Classification with 20 news groups,[0],[0]
"[5000, 15000] and the number of training iterations T ∈ [20, 100].",4.3. Support Vector Classification with 20 news groups,[0],[0]
Each evaluation takes the given dataset of size N and splits it up into 5 to perform 5-fold cross validation.,4.3. Support Vector Classification with 20 news groups,[0],[0]
"As the cost function for fidelity selection, we used λ(N,T ) =",4.3. Support Vector Classification with 20 news groups,[0],[0]
"NT as
the training/validation complexity is linear in both parameters.",4.3. Support Vector Classification with 20 news groups,[0],[0]
Our goal is to maximise the cross validation accuracy at z• =,4.3. Support Vector Classification with 20 news groups,[0],[0]
"[15000, 100].",4.3. Support Vector Classification with 20 news groups,[0],[0]
For the finite fidelity methods we use three fidelities with the approximations available at z =,4.3. Support Vector Classification with 20 news groups,[0],[0]
"[8333, 47] and z =",4.3. Support Vector Classification with 20 news groups,[0],[0]
"[11667, 73].",4.3. Support Vector Classification with 20 news groups,[0],[0]
The results are given in Fig. 4(c) where we plot the average cross validation accuracy against wall clock time.,4.3. Support Vector Classification with 20 news groups,[0],[0]
"We studied Bayesian optimisation with continuous approximations, by treating the approximations as arising out of a continuous fidelity space.",5. Conclusion,[0],[0]
"While previous multi-fidelity literature has predominantly focused on a finite number of approximations, BOCA applies to continuous fidelity spaces and can potentially be extended to arbitrary spaces.",5. Conclusion,[0],[0]
We bound the simple regret for BOCA and demonstrate that it is better than methods such as GP-UCB which ignore the approximations and that the gains are determined by the smoothness of the fidelity space.,5. Conclusion,[0],[0]
"When compared to existing multi-fidelity methods, BOCA is able to share information across fidelities effectively, has more natural modelling assumptions and has fewer hyper-parameters to tune.",5. Conclusion,[0],[0]
"Empirically, we demonstrate that BOCA is competitive with other baselines in synthetic and real problems.",5. Conclusion,[0],[0]
"Another nice feature of using continuous approximations is that it relieves the practitioner from having to design the approximations; she/he can specify the available approximations and let the algorithm decide how to choose them.
",5. Conclusion,[0],[0]
"Going forward, we wish to extend our theoretical results to more general settings.",5. Conclusion,[0],[0]
"For instance, we believe a stronger bound on the regret might be possible if φZ is a finite dimensional kernel.",5. Conclusion,[0],[0]
"Since finite dimensional kernels are typically not radial (Sriperumbudur et al., 2016), our analysis techniques will not carry over straightforwardly.",5. Conclusion,[0],[0]
Another line of work that we have alluded to is to study more general fidelity spaces with an appropriately defined kernel φZ .,5. Conclusion,[0],[0]
We would like to thank Renato Negrinho for reviewing an initial draft of this paper.,Acknowledgements,[0],[0]
This research is supported in part by DOE grant DESC0011114 and NSF grant IIS1563887.,Acknowledgements,[0],[0]
KK is supported by a Facebook Ph.D. fellowship.,Acknowledgements,[0],[0]
"Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design.",abstractText,[0],[0]
"Recently, multifidelity methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications.",abstractText,[0],[0]
Multifidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process.,abstractText,[0],[0]
"However, most multi-fidelity methods assume only a finite number of approximations.",abstractText,[0],[0]
"On the other hand, in many practical applications, a continuous spectrum of approximations might be available.",abstractText,[0],[0]
"For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data N and/or few training iterations T .",abstractText,[0],[0]
"Here, the approximations are best viewed as arising out of a continuous two dimensional space (N,T ).",abstractText,[0],[0]
"In this work, we develop a Bayesian optimisation method, BOCA, for this setting.",abstractText,[0],[0]
We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations.,abstractText,[0],[0]
BOCA outperforms several other baselines in synthetic and real experiments.,abstractText,[0],[0]
Multi-fidelity Bayesian Optimisation with Continuous Approximations,title,[0],[0]
"Motivated by settings such as hyper-parameter tuning and physical simulations, we consider the problem of black-box optimization of a function. Multi-fidelity techniques have become popular for applications where exact function evaluations are expensive, but coarse (biased) approximations are available at much lower cost. A canonical example is that of hyper-parameter selection in a learning algorithm. The learning algorithm can be trained for fewer iterations – this results in a lower cost, but its validation error is only coarsely indicative of the same if the algorithm had been trained till completion. We incorporate the multi-fidelity setup into the powerful framework of black-box optimization through hierarchical partitioning. We develop tree-search based multi-fidelity algorithms with theoretical guarantees on simple regret. We finally demonstrate the performance gains of our algorithms on both real and synthetic datasets.",text,[0],[0]
"Optimizing a black-box function f over a Euclidean domain X is a classical problem studied in several disciplines including computer science, mathematics, and operations research.",1. Introduction,[0],[0]
"It finds applications in many real world scientific and engineering tasks including scientific experimentation, industrial design, and model selection in statistics and machine learning (Martinez-Cantin et al., 2007; Parkinson et al., 2006; Snoek et al., 2012).",1. Introduction,[0],[0]
"Given a budget of n evaluations, an optimization algorithm operates sequentially – at time t, it chooses to evaluate f at xt based on its previous evaluations {xi, f(xi)} t 1 i=1 .",1. Introduction,[0],[0]
"At the end of n evaluations, it makes a recommendation x(n) and its performance is measured by its 1Univerity of Texas as Austin 2Carnegie Mellon University.",1. Introduction,[0],[0]
"Correspondence to: Rajat Sen <rajat.sen@utexas.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"simple regret Rn,
Rn = sup x2X f(x) f(x(n)).
",1. Introduction,[0],[0]
Our study focuses on applications where exact evaluation of the function f is expensive.,1. Introduction,[0],[0]
"As an example, in the case of model selection, training and validating large neural networks can take several hours to days.",1. Introduction,[0],[0]
"Similarly, the simulation of an astrophysical process typically takes multiple days even on a cluster of super computers.",1. Introduction,[0],[0]
Traditional methods for black-box optimization are poorly suited for such applications because we need to invest a considerable number of evaluations to optimize f .,1. Introduction,[0],[0]
"This motivates studying the multi-fidelity setting where we have cheaper, but potentially biased approximations to the function (Cutler et al., 2014; Huang et al., 2006a; Kandasamy et al., 2016c; 2017).",1. Introduction,[0],[0]
"As an illustration, in a hyper-parameter tuning scenario, the task is to find the best set of hyper-parameters for training a machine learning model.",1. Introduction,[0],[0]
"In this setting, the black-box function that needs to be optimized is the validation error after training the learning algorithm to completion (a certain number of maximum iterations)",1. Introduction,[0],[0]
i.e X represents the allowed set of hyper-parameters while the function represents the validation error after training to completion.,1. Introduction,[0],[0]
"However, as training the algorithm till completion is expensive, we may choose to train the learning algorithm for a few iterations at chosen hyper-parameters and then test it on the validation set.",1. Introduction,[0],[0]
"These inexpensive validation errors act as the cheap approximations (fidelities) to the function value and can indeed provide valuable information regarding the quality of the hyper-parameters.
",1. Introduction,[0],[0]
"The multi-fidelity setup for black-box function optimization has been popularly studied in the Bayesian optimization setting (Huang et al., 2006b; Kandasamy et al., 2016b; 2017).",1. Introduction,[0],[0]
"However, in this paper we focus on another powerful framework for sequential black-box optimization that works with hierarchical partitioning of the function domain X .",1. Introduction,[0],[0]
"These tree-search based methods were initially motivated by an empirically successful heuristic UCT (Kocsis & Szepesvári, 2006), which subsequently lead to several theoretically founded algorithms for black-box optimization through hierarchical partitioning (Bubeck et al., 2011; Kleinberg et al., 2008; Munos, 2011; Valko et al., 2013).
",1. Introduction,[0],[0]
"In this work, we incorporate cheap approximations or fideli-
ties with tree-search based methods for black-box optimization.",1. Introduction,[0],[0]
"We assume access to a tree-like partitioning of the domain X similar to (Bubeck et al., 2011; Grill et al., 2015; Munos, 2011).",1. Introduction,[0],[0]
"The partitioning of the domain X is denoted as P and it contains hierarchical cells {Ph,i}, where h denotes the height of the cell and i denotes the index.",1. Introduction,[0],[0]
"A cell Ph,i at height h has K children {Ph+1,ik}Kk=1, which are distinct partitions of Ph,i. At height 0, there is only one partition P0,1 = X .",1. Introduction,[0],[0]
An example of such an hierarchical partition for X =,1. Introduction,[0],[0]
"[0, 1] and K = 2 would be:",1. Introduction,[0],[0]
"P0,1 =",1. Introduction,[0],[0]
"[0, 1], P1,1,P1,2 =",1. Introduction,[0],[0]
"[0, 0.5], (0.5, 1], P2,1 =",1. Introduction,[0],[0]
"[0, 0.25]... and so on.",1. Introduction,[0],[0]
Most of the prior work assume some smoothness property about the function and hierarchical partitioning.,1. Introduction,[0],[0]
"We follow a similar path adopting the smoothness assumptions in (Grill et al., 2015).",1. Introduction,[0],[0]
"This assumption states that there exists ⌫ and ⇢ 2 (0, 1) such that,
8h 0, 8x 2",1. Introduction,[0],[0]
"Ph,i⇤h , f(x) f(x ⇤) ⌫⇢h, (1)
where x⇤ is assumed to be the unique point in X such that f(x⇤) = supx2X f(x).",1. Introduction,[0],[0]
"This assumption basically says that the diameter of the function is bounded for all cells that contain the optima, and that this diameter goes down at a geometric rate with the height of the cell.",1. Introduction,[0],[0]
"We also adopt the definition of the well-known near-optimality dimension d(⌫, ⇢) which restricts the number of cells at height h that contain points close to the optima.",1. Introduction,[0],[0]
"This is an important quantity in the analysis of many tree-search based methods (Bubeck et al., 2011; Grill et al., 2015; Munos, 2011; Valko et al., 2013).
",1. Introduction,[0],[0]
In addition we also model that the function can be accessed at a continuous range of fidelities within Z =,1. Introduction,[0],[0]
"[0, 1],",1. Introduction,[0],[0]
where z = 0 is the cheapest fidelity and z = 1 is the most expensive one.,1. Introduction,[0],[0]
"For instance, in our hyper-parameter tuning example, z = 1 may correspond to training the learning algorithm for 1000 iterations while z = 0 represents training the algorithm to 50 iterations.",1. Introduction,[0],[0]
"When a function is evaluated at a point x 2 X with a fidelity z 2 Z , a value fz(x) is revealed such that |f(x) fz(x)| < ⇣(z) where ⇣(.) is a fixed bias function.",1. Introduction,[0],[0]
The bias function is monotonically decreasing in z with ⇣(1) = 0.,1. Introduction,[0],[0]
There is also a cost associated with these evaluations which is captured by a cost function : Z !,1. Introduction,[0],[0]
R+.,1. Introduction,[0],[0]
The cost function is assumed to be monotonically increasing in z.,1. Introduction,[0],[0]
"For instance, in hyper-parameter tuning the cost increases linearly with the number of iterations.",1. Introduction,[0],[0]
"The objective is to locate a point x such that f(x) is as close as possible to supx2X f(x) given a finite cost budget ⇤.
",1. Introduction,[0],[0]
"The following are the main contributions of this work:
(i)",1. Introduction,[0],[0]
We incorporate multiple fidelities/cheap approximations in black-box function optimization through hierarchical partitioning.,1. Introduction,[0],[0]
We propose and analyze two algorithms in this setting.,1. Introduction,[0],[0]
"Our first algorithm is known as MFDOO (Algorithm 1) which requires knowledge about the smoothness
parameter (⌫, ⇢).",1. Introduction,[0],[0]
"This algorithm is similar to DOO (Munos, 2011), however it is designed to explore coarser partitions at lower fidelities while exploring finer partitions at higher fidelities, when the algorithm zooms in on a promising area of the function domain.",1. Introduction,[0],[0]
"Motivated by recent work (Grill et al., 2015), we also propose a second algorithm MFPDOO (Algorithm 2), which does not require knowledge about the smoothness.",1. Introduction,[0],[0]
"This algorithm spawns several instances of MFDOO (Algorithm 1) with carefully selected parameters, at least one of which is bound to perform nearly as well as MFDOO when the smoothness parameters are known.
",1. Introduction,[0],[0]
"(ii) We provide simple regret bounds for both of our algorithms, given a fixed cost budget ⇤ for performing evaluations.",1. Introduction,[0],[0]
"First we show that when the smoothness parameters are known, MFDOO has simple regret of O(⇤ 1/d(⌫,⇢)+1) under some conditions on the bias and cost function.",1. Introduction,[0],[0]
"Here, d(⌫, ⇢) is the near-optimality dimension of the function with respect to parameters (⌫, ⇢).",1. Introduction,[0],[0]
"On the other hand a naive application of DOO (Munos, 2011)1 only using the highest fidelity z = 1 would yield a regret bound of O((⇤/ (1)) 1/d(⌫,⇢)).",1. Introduction,[0],[0]
"We also show that our second algorithm MFPDOO can obtain a simple regret bound of O((⇤/ log⇤) 1/d(⌫,⇢)+1) even when the smoothness parameters are not known.",1. Introduction,[0],[0]
"The precise details about our theoretical guarantees can be found in Section 5.
(iii) Finally, we compare the performance of our algorithms with several state of the art algorithms (Grill et al., 2015; Huang et al., 2006b; Jones et al., 1998; Kandasamy et al., 2016b;b; Srinivas et al., 2009) for black-box optimization in the multi-fidelity setting, on real and synthetic data-sets.",1. Introduction,[0],[0]
We demonstrate that our algorithms outperform the state of the art in most of these experiments.,1. Introduction,[0],[0]
"We build on a line of work on bandits and black-box optimization with hierarchical partitions (Bubeck et al., 2011; Kleinberg et al., 2008; Munos, 2011; Valko et al., 2013).",2. Related Work,[0],[0]
"These methods rely on the principle of optimism i.e they build upper bounds on the value of the functions inside different partitions using the already explored points x1..., xt.",2. Related Work,[0],[0]
"Then at time t+ 1, a point is chosen from the partition that has the highest value of this upper-bound.",2. Related Work,[0],[0]
"We closely follow the line of work initiated in (Munos, 2011) that was later extended to noisy function evaluations in (Grill et al., 2015; Valko et al., 2013).",2. Related Work,[0],[0]
"In (Munos, 2011) it was assumed that the function follows a local Lipschitz condition with respect to a semi-metric `, and the diameter of the hierarchical partitions with respect to this semi-metric decrease geometrically with height.",2. Related Work,[0],[0]
"Grill et al. (Grill et al., 2015) later merged these
1Note that DOO also requires knowledge of the smoothness parameters of the function.
",2. Related Work,[0],[0]
"two assumptions into one, by having a single condition that related the smoothness of the function with the hierarchical partition.",2. Related Work,[0],[0]
"In this work we adapt the regime of (Grill et al., 2015).",2. Related Work,[0],[0]
"However, we also model cheap approximations to the functions through a one-dimensional fidelity space.
",2. Related Work,[0],[0]
"Multi-fidelity optimization has had a rich history in many settings (Agarwal et al., 2011; Forrester et al., 2007; Huang et al., 2006a; Klein et al., 2016; Lam et al., 2015; Li et al., 2016; Poloczek et al., 2016; Sabharwal et al., 2015; Zhang & Chaudhuri, 2015), with those that are not applicationspecific focusing on a Bayesian framework without formal guarantees (we refer to (Kandasamy et al., 2017) for additional discussion).",2. Related Work,[0],[0]
Kandasamy et al. (2016c) propose and analyse a UCB style multi-fidelity algorithm for the K-armed bandit setting assuming a finite number of approximations to the K arms.,2. Related Work,[0],[0]
"They then extend this work to develop UCB algorithms for black-box optimization under Bayesian Gaussian process assumptions on f , both with a finite number of approximations and a continuous spectrum of approximations (Kandasamy et al., 2016a;b; 2017).",2. Related Work,[0],[0]
"In all these works, the relation between the approximations and the true function is known and appears in the form of uniform bounds on the approximation or a smoothness assumption arising out of the kernel of the Gaussian process.",2. Related Work,[0],[0]
In our work we merge the multi-fidelity setting with the hierarchical partitions framework.,2. Related Work,[0],[0]
We consider the problem of optimizing a function f : X !,3. Problem Setting,[0],[0]
R with black-box access at different fidelities.,3. Problem Setting,[0],[0]
"The aim is to locate a point x such that f(x) is as close as possible to supx2X f(x), given a finite budget for performing evaluations.
",3. Problem Setting,[0],[0]
"We assume that the function can be queried at a continuous range of fidelities in Z , [0, 1].",3. Problem Setting,[0],[0]
"When the function is queried at a point x 2 X with fidelity z 2 Z , a value fz(x) is revealed.",3. Problem Setting,[0],[0]
"We assume that |fz(x) f(x)|  ⇣(z), where ⇣ : Z ! R+ is a known bias function.",3. Problem Setting,[0],[0]
"It is also assumed that a single query at fidelity z incurs a cost (z), where : Z !",3. Problem Setting,[0],[0]
R+ is a known cost function.,3. Problem Setting,[0],[0]
"We assume there is a unique point x⇤ 2 X at which supx2X f(x) is achieved.
",3. Problem Setting,[0],[0]
Bias and Cost Functions: The bias function ⇣ is assumed to be bounded and monotonically decreasing in z.,3. Problem Setting,[0],[0]
The optimal fidelity z is assumed to have zero bias i.e. ⇣(1) = 0.,3. Problem Setting,[0],[0]
"The cost function is assumed to be bounded and monotonically increasing in z.
The multi-fidelity setting is motivated by engineering applications where cheap approximations are available.",3. Problem Setting,[0],[0]
"One promising use case is that of hyper-parameter tuning, where the validation performance of a learning algorithm at different hyper-parameters can be observed.",3. Problem Setting,[0],[0]
"The aim is to locate
the best hyper-parameter.",3. Problem Setting,[0],[0]
"In such a setting, cheap approximations are available for instance instead of evaluating the learning algorithm after a maximum of T iterations, one may choose to evaluate it after t < T iterations.",3. Problem Setting,[0],[0]
In this case T can be mapped to z = 1,3. Problem Setting,[0],[0]
and t can be mapped to a z < 1.,3. Problem Setting,[0],[0]
The cost function in this setting is proportional to the O(t) computation required.,3. Problem Setting,[0],[0]
"The bias function is monotonically decreasing with z, however may not be known exactly in practice.",3. Problem Setting,[0],[0]
"However, prior works in multi-fidelity setup (Kandasamy et al., 2016a;c; Kleinberg et al., 2008) have all assumed access to a known bias function for the theoretical guarantees.",3. Problem Setting,[0],[0]
"Even though we assume the bias function is known in theory, we shall see in our experiments in Section 6 that a simple parametric form of the bias function can be assumed and the parameters can be updated online during the course of our algorithm (similar to (Kandasamy et al., 2017)).
",3. Problem Setting,[0],[0]
Simple Regret:,3. Problem Setting,[0],[0]
The objective is to locate a point x such that f(x) is as close as possible to supx2X f(x) given a finite cost budget.,3. Problem Setting,[0],[0]
Let ⇤ be the total cost budget allowed.,3. Problem Setting,[0],[0]
"Consider an optimization policy that queries a sequence of points {x1, ..., xn(⇤)} at fidelities {z1, ..., zn(⇤)} respectively and finally returns a recommendation x⇤.",3. Problem Setting,[0],[0]
"Our main quantity of interest is the simple regret which is defined as,
R⇤ = sup x2X f(x) f (x⇤) , (2)
such that Pn(⇤)
i=1",3. Problem Setting,[0],[0]
(zi)  ⇤.,3. Problem Setting,[0],[0]
Note that the simple regret is always measured at the highest fidelity as we are only interested in optimizing the actual function.,3. Problem Setting,[0],[0]
"In this section we define the hierarchical partitions of the domain X that we assume access to and then provide our technical assumptions about the function and the hierarchical partitions.
Hierarchical Partitions: We assume access to a tree-like hierarchical partitioning P = {Ph,i} of the domain X , where, h denotes a depth parameter.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"For any depth h 0, the cells {Ph,i}1iIh denote a partitioning of the space X , where Ih is the number of cells at depth h. At depth 0",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"there is a single cell P0,1 = X .",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"A cell Ph,i can be split into K child nodes at depth h + 1.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"In what follows, querying a cell Ph,i would refer to evaluating the function at a fixed representative point xh,i 2 Ph,i at a chosen fidelity.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"The fixed representative point is usually chosen to be the coordinate wise mid-point for any given cell.
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
As an illustrative example let us consider a hierarchical black-box optimization problem over the domain X =,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[0, 1] ⇥",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[0, 1].",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Let us consider a hierarchical partition of this domain where the cells are of the form {x 2 X : b1,1  x1 < b1,2, b2,1  ",3.1. Hierarchical Partitions and Assumptions,[0],[0]
x2 <,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"b2,2}.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Such a
cell will be denoted by the notation",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[b1,1, b1,2], [b2,1, b2,2]].",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Then a hierarchical partition with K = 2 starts with the root node P0,1 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0, 1], [0, 1]].",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"This can be further sub-divided into children cells at h = 1 given by P1,1 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0, 0.5], [0, 1]] and P1,2 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0.5, 1], [0, 1]].",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"P1,2 can be further partitioned into P2,1 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0.5, 1], [0, 0.5]] and P2,2 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0.5, 1], [0.5, 1]] and so on.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
The fixed representative point for a cell,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[b1,1, b1,2], [b2,1, b2,2]] is chosen as the point [(b1,1 + b1,2)/2, (b2,1 + b2,2)/2].
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
Black-box optimization is akin to a needle in a haystack problem without any conditions on the function f(x).,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Therefore, similar to prior work (Grill et al., 2015) we make the following smoothness assumption which depends on the properties of both the function f and the hierarchical partitioning P .",3.1. Hierarchical Partitions and Assumptions,[0],[0]
Assumption 1 (Smoothness Decay).,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"There exists ⌫ and ⇢ 2 (0, 1) such that,
8h 0, 8x 2",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Ph,i⇤h , f(x) f(x ⇤) ⌫⇢h, (3)
where Ph,i⇤h is the unique partition of height h which contains x⇤.
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"We also adopt the definition of the near-optimalitydimension for parameters (⌫, ⇢) from (Grill et al., 2015).",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"This is a quantity that depends on the choice of parameters, the partitioning and the function itself.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
Definition 1.,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"The near-optimality dimension of f with respect to parameters (⌫, ⇢) is given by,
d(⌫, ⇢) , inf d 0 2 R+ : 9C(⌫, ⇢), 8h 0,
Nh(2⌫⇢ h)  ",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"C(⌫, ⇢)⇢",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"d
0h o
(4)
where Nh(✏) is the number of cells",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Ph,i such that supx2Ph,i f(x) f(x ⇤) ✏.
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Let (⌫⇤, ⇢⇤) be the parameters with the minimum near optimality dimension d(⌫⇤, ⇢⇤).
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Discussion: Access to hierarchical partitions have been assumed in a string of previous works on black-box optimization (Bubeck et al., 2011; Grill et al., 2015; Kleinberg et al., 2008; Munos, 2011; Slivkins, 2011; Valko et al., 2013).",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Many of these prior works assume a semi-metric ` over the domain X (Bubeck et al., 2011; Munos, 2011; Valko et al., 2013).",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"In (Bubeck et al., 2011), it is assumed that the function satisfies a weak-Lipschitzness condition.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"More recent works (Munos, 2011; Valko et al., 2013) have assumed a local-smoothness property w.r.t the metric given by 8x 2 X , f(x⇤) f(x)  ",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"`(x, x⇤).",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"However, recently Grill et al. (Grill et al., 2015) have observed that Assumption 1 is sufficient to combine several assumptions about the semi-metric, the function and the hierarchical partitions into one combinatorial condition and similarly have adapted
the definition of the near-optimality dimension without the semi-metric.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"It was depicted in (Grill et al., 2015) that prior algorithms like (Bubeck et al., 2011; Munos, 2011; Valko et al., 2013) can be shown to have good regret guarantees with this new set of assumptions, and therefore we adopt these assumptions in our work.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
In this section we present two algorithms for black-box optimization using different fidelities and the hierarchical partitioning provided.,4. Algorithms,[0],[0]
"In Section 4.1, we provide Algorithm 1 which requires the knowledge of the optimal smoothness decay parameters (⌫⇤, ⇢⇤).",4. Algorithms,[0],[0]
"Then in Section 4.2, we provide Algorithm 2 that searches for the optimal smoothness by spawning O(log⇤) instances of Algorithm 1 with a carefully designed sequence of smoothness parameters (⌫, ⇢) as arguments.",4. Algorithms,[0],[0]
"In this section we provide an algorithm which takes as an argument the smoothness parameters (⌫, ⇢).","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"We show in Section 5 that if the parameters provided match with the optimal parameters (⌫⇤, ⇢⇤) then the algorithm enjoys strong theoretical guarantees under some conditions on the bias and cost functions ⇣(z) and (z) respectively.
","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"Algorithm 1 MFDOO: Multi-Fidelity Deterministic Optimistic Optimization
1: Arguments: (⌫, ⇢), ⇣(z), (z), P , ⇤ 2: Define zh = ⇣ 1(⌫⇢h) 3: Let T = {(0, 1)} be the tree initialized (root node
evaluated with fidelity z0).","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
Set of leaves at time t: Lt. 4: Time: t = 1; Cost: C = (z0).,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"5: while C  ⇤ do 6: Select the leaf (h, j) 2 Lt with maximum bh,j ,
fzh(xh,j) + ⌫⇢ h + ⇣(zh).
7: Expand this node; add to Tt the K children of (h, j).","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
8: Evaluate the children at the fidelity level zh+1.,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
t =,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
t+ 1. C = C +K (zh+1).,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
9: end while 10: Let h(⇤) be the height of the tree.,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"Return x⇤ = argmax(h(⇤),i)","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"fzh(⇤)(xh(⇤),i).
","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"In Algorithm 1, with some abuse of notation we define for all h 0, zh = ⇣ 1(⌫⇢h) i.e the fidelity at which the bias becomes less than or equal to the smoothness decay parameter at height h.","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
All cells at height h are evaluated at the fidelity zh.,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"The intuition is that if x⇤ belongs to a cell Ph,i⇤ at height h that has been evaluated, then by Assumption 1 we have that all points in the cell are at least ⌫⇢
h optimal.","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"Ideally, beyond this point we would only like to expand leaf nodes that are at least O(⌫⇢h) optimal,
which can only be achieved if the error due to the fidelities is O(⌫⇢h).","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"At each step, a leaf node with the highest upper bound parameter bh,i, is expanded and the children cells are evaluated.","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"In this section we describe an algorithm that does not require the optimal parameters (⌫⇤, ⇢⇤).","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"Algorithm 2 just requires ⇢max, ⌫max which are loose upper-bounds of ⇢⇤ and ⌫⇤ respectively.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"The algorithm proceeds by spawning O(log⇤) MFDOO instances with different (⌫, ⇢)’s which have been carefully designed.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"Similar ideas were explored in a setting without fidelities in (Grill et al., 2015).","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"In Section 5, we show that Algorithm 2 does almost as well as Algorithm 1 without requiring the optimal parameters as input.
","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
Algorithm 2,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"MFPDOO: Multi-Fidelity Parallel Deterministic Optimistic Optimization
1: Arguments: (⌫max, ⇢max), ⇣(z), (z), P , ⇤ 2: Let N = (1/2)Dmax log(⇤/ log(⇤)) where Dmax =
logK/ log(1/⇢max) 3: for i = 0 to N 1 do 4: Spawn MFDOO (Algorithm 1) with parameters
(⌫max, ⇢ N/(N","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"i) max ) with budget (⇤ N (1))/N
5: end for 6: Let x(i)⇤ be the point returned by the i
th MFDOO instance for i 2 {0, .., N 1}.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
Evaluate all {x(i)⇤ }i at the z = 1.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"Return the point x⇤ = x (i⇤) ⇤ where i ⇤ = argmaxi f(x (i) ⇤ ).
","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
Algorithm 2 proceeds by spawning N different MFDOO instances with the parameters specified in step 4 of the algorithm.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"We will show in Theorem 2 that at least one of the MFDOO instances will have a performance comparable to Algorithm 1 supplied with parameters (⌫⇤, ⇢⇤) with a budget of O(⇤/N).","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"Step 6 of the algorithm obtains the exact value of the points returned by all the MFDOO instances by evaluating them at the highest fidelity, and then chooses the one with the maximum value.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"This ensures that the highest performing MFDOO instance is selected.
","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
Remark 1.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
Our algorithms and the theoretical results assume that the bias function ⇣(.) is known.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"However, in practice we do not assume perfect knowledge about the bias function.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"We assume a simple parametric form of the bias function and update the parameters online, when the bias assumptions are violated.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"We provide more details in Section 6 and show that even without this knowledge, the algorithms perform better than other benchmarks.
","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"It should be noted that the different MFDOO instances created by Algorithm 2 can share information among each other, when multiple instances query the same partition at
very similar fidelities.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
This leads to huge improvements in practice in terms of effectively using the cost budget.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"In this section we first prove a general result about the simple regret of Algorithm 1 which assumes access to the optimal parameters (⌫, ⇢).",5. Theoretical Results,[0],[0]
"This naturally implies a simple regret bound on Algorithm 1 when it is supplied with the parameters (⌫⇤, ⇢⇤) i.e. the ones that have the minimum near-optimality dimension.",5. Theoretical Results,[0],[0]
Then we refine these guarantees under some natural conditions on the bias and cost functions.,5. Theoretical Results,[0],[0]
"Finally, we show that Algorithm 2 can achieve guarantees close to Algorithm 1 with the optimal parameters, without having access to them.
",5. Theoretical Results,[0],[0]
"We first present the following general result about Algorithm 1.
Theorem 1.",5. Theoretical Results,[0],[0]
"Let h0 be the biggest number h such
hX
l=0
C(⌫, ⇢)K (zl)⇢ d(⌫,⇢)l  ",5. Theoretical Results,[0],[0]
"⇤.
",5. Theoretical Results,[0],[0]
Let h(⇤) = h0 + 1.,5. Theoretical Results,[0],[0]
"Then Algorithm 1 run with parameters (⌫, ⇢) (s.t ⌫ ⌫⇤, ⇢ ⇢⇤), incurs a simple regret of at most 2⌫⇢h(⇤) and terminates using a total cost of at most ⇤+K (1).
",5. Theoretical Results,[0],[0]
We defer the proof of Theorem 1 to Section A in the appendix.,5. Theoretical Results,[0],[0]
"Note that the guarantee in Theorem 1 is the tightest when the parameters (⌫⇤, ⇢⇤) are supplied as the near optimality dimension d(⌫⇤, ⇢⇤) is the lowest.
",5. Theoretical Results,[0],[0]
"Now, we impose some natural conditions on the cost and bias functions.",5. Theoretical Results,[0],[0]
"We provide more specialized versions of the guarantees in Theorem 1 under these two conditions separately, which are described below.
Assumption 2.",5. Theoretical Results,[0],[0]
We assume that ⇣(.),5. Theoretical Results,[0],[0]
"and (.) are such that (z⇤h)  min{ h,⇤(1)} for some positive constant .",5. Theoretical Results,[0],[0]
"Here, z⇤h = ⇣ 1(⌫⇤⇢h⇤).
",5. Theoretical Results,[0],[0]
Motivation: The above assumption is motivated by the following hyper-parameter tuning scenario.,5. Theoretical Results,[0],[0]
Consider training a learning algorithm with a particular hyper-parameter that involves optimizing a strongly convex and smooth function with gradient descent.,5. Theoretical Results,[0],[0]
Let the fidelity denote a rescaled version of the number of steps in gradient descent n.,5. Theoretical Results,[0],[0]
We assume that at the optimal fidelity (N steps) we reach the optimal value of the function up to an error of ✏⇤.,5. Theoretical Results,[0],[0]
Let zn = n/N .,5. Theoretical Results,[0],[0]
"At fidelity zn the error decays to ⇣(zn) = O(rn) for some r 2 (0, 1).",5. Theoretical Results,[0],[0]
The cost incurred is linear in the number of steps say (zn) =,5. Theoretical Results,[0],[0]
sn for s > 0.,5. Theoretical Results,[0],[0]
"In this setting it can be shown that if ⇣(zn) ⇠ ⌫⇤⇢h⇤ , then n = O(h) and therefore (zn) = O(h).
",5. Theoretical Results,[0],[0]
"The second assumption under which we provide specialized guarantees is as follows.
",5. Theoretical Results,[0],[0]
Assumption 3.,5. Theoretical Results,[0],[0]
We assume that ⇣(.),5. Theoretical Results,[0],[0]
"and (.) are such that (z⇤h)  min{ h ,⇤(1)} for some constant 2 (⇢, 1).",5. Theoretical Results,[0],[0]
"Here, z⇤h = ⇣ 1(⌫⇤⇢h⇤).
",5. Theoretical Results,[0],[0]
Motivation: Assumption 3 is motivated by a similar hyperparameter tuning scenario as above.,5. Theoretical Results,[0],[0]
Consider training a learning algorithm with a particular hyper-parameter that involves optimizing a smooth convex function with accelerated gradient descent.,5. Theoretical Results,[0],[0]
Let the fidelity denote a rescaled version of the number of steps in gradient descent n as above.,5. Theoretical Results,[0],[0]
At fidelity zn the error decays to ⇣(zn) = O(1/n2).,5. Theoretical Results,[0],[0]
The cost incurred is linear in the number of steps say (zn) =,5. Theoretical Results,[0],[0]
sn for s > 0.,5. Theoretical Results,[0],[0]
"In this setting it can be shown that if ⇣(zn) ⇠ ⌫⇤⇢h⇤ , then n = O( h) for = O( p ⇢).
",5. Theoretical Results,[0],[0]
"We are now at a position to introduce a specialized corollary of Theorem 1.
",5. Theoretical Results,[0],[0]
Corollary 1.,5. Theoretical Results,[0],[0]
"Algorithm 1 with parameters (⌫, ⇢) (s.t ⌫ ⌫⇤, ⇢ ⇢⇤) run with a total budget of ⇤ terminates with a total cost of at most ⇤ + K (1) and has the following properties: (i)",5. Theoretical Results,[0],[0]
Under Assumption 2: R⇤  2⌫ ⇣,5. Theoretical Results,[0],[0]
"C(⌫,⇢)K ⇤(1 ⇢d(⌫,⇢))",5. Theoretical Results,[0],[0]
"⌘ 1 d(⌫,⇢)+✏ for some small ✏ > 0, provided ⇤ is large enough.
",5. Theoretical Results,[0],[0]
"(ii) Under Assumption 3:
R⇤  2 ⌫ ⇢
⇣ 2C(⌫,⇢)K
⇤( 1⇢ d(⌫,⇢) 1)
⌘ 1 d(⌫,⇢)+1
.
",5. Theoretical Results,[0],[0]
"Comparison with DOO (Munos, 2011):",5. Theoretical Results,[0],[0]
"The above result can be directly compared to DOO (Munos, 2011) which is in the noiseless black-box optimization regime, without access to fidelities.",5. Theoretical Results,[0],[0]
"The simple regret of DOO under the same assumptions would scale as O ⇣ (⇤/ (1)) 1/d(⌫⇤,⇢⇤) ⌘
when all the evaluations are performed at the highest fidelity.",5. Theoretical Results,[0],[0]
"In contrast our bounds under Assumption 2 scales as O ⇣ (⇤/ ) 1/(d(⌫⇤,⇢⇤)+✏) ⌘ , where ✏ is a constant close
to zero.",5. Theoretical Results,[0],[0]
"Note that (z⇤h)  (1), and therefore = (1) trivially satisfies the inequality in Assumption 2.",5. Theoretical Results,[0],[0]
"Typically, is expected to be much less as compared to the highest fidelity cost (1).",5. Theoretical Results,[0],[0]
"For example in our hyper-parameter tuning example where the fidelity is the number of iterations (a maximum of N iterations), is a small constant (see the discussion on Assumption 2), while (1) can be O(N).",5. Theoretical Results,[0],[0]
This can lead to significant gains in simple regret as we show in our empirical results in Section 6.,5. Theoretical Results,[0],[0]
"Similarly, under Assumption 3 our simple regret scales as O ⇤ 1/(d(⌫⇤,⇢⇤)+1) , which can be much better than that of DOO (Munos, 2011) as the total budget is not divided by (1).
",5. Theoretical Results,[0],[0]
"Now, we will provide one of our main results which states that Algorithm 2 can recover simple regret bounds which
are close to that of Algorithm 1 even when the optimal smoothness parameters are not known.
",5. Theoretical Results,[0],[0]
Theorem 2.,5. Theoretical Results,[0],[0]
"Algorithm 2 when run with upper-bounds ⌫max and ⇢max with a total cost budget of ⇤ terminates after using up a cost of at most ⇤+O(K (1) log⇤) and has the following regret guarantees:
(i)",5. Theoretical Results,[0],[0]
"Under Assumption 2 the simple regret is O ⇣ (⌫max/⌫⇤) Dmax ✏+d(⌫⇤,⇢⇤)⇥",5. Theoretical Results,[0],[0]
"⇣ 2⇤
K Dmax log(⇤/ log⇤)
(1) K
⌘ 1✏+d(⌫⇤,⇢⇤) ◆
(ii)",5. Theoretical Results,[0],[0]
"Under Assumption 3 if ⇢max the simple regret is O ⇣ (⌫max/⌫⇤) 2Dmax 1+d(⌫⇤,⇢⇤)⇥",5. Theoretical Results,[0],[0]
"⇣ 2⇤
KDmax log(⇤/ log⇤)
(1) K
⌘ 11+d(⌫⇤,⇢⇤) ◆
.
",5. Theoretical Results,[0],[0]
"We defer the proof of this theorem to Appendix C.
Comparison with POO (Grill et al., 2015):",5. Theoretical Results,[0],[0]
"It is worthwhile to compare our result with that of POO (Grill et al., 2015) which uses only the highest fidelity.",5. Theoretical Results,[0],[0]
"It should be noted that POO is in a noisy setting, which gives rise to extra polylog factors in the bounds.",5. Theoretical Results,[0],[0]
"However, ignoring polylog factors the simple regret bound of POO would scale as O (⇤/(log(⇤/ (1))",5. Theoretical Results,[0],[0]
⇤ (1))),5. Theoretical Results,[0],[0]
"1/(d(⌫⇤,⇢⇤)+2) .",5. Theoretical Results,[0],[0]
In contrast our bounds scale as O ⇣ (⇤/( log(⇤))),5. Theoretical Results,[0],[0]
"1/(d(⌫⇤,⇢⇤)+✏) ⌘
and O (⇤/ log(⇤))",5. Theoretical Results,[0],[0]
"1/(d(⌫⇤,⇢⇤)+1) under assumptions 2 and 3 respectively.",5. Theoretical Results,[0],[0]
This can lead to much better performance at the same cost budget.,5. Theoretical Results,[0],[0]
We demonstrate this in our empirical results in Section 6.,5. Theoretical Results,[0],[0]
In this section we provide empirical results on synthetic and real datasets.,6. Empirical Results,[0],[0]
"We compare our algorithm with the following related works: (i) BOCA (Kandasamy et al., 2017) which is a multi-fidelity Gaussian Process (GP) based algorithm that can handle continuous fidelity spaces, (ii) MFGP-UCB (Kandasamy et al., 2016c) which is a GP based multi-fidelity method that can handle finite fidelities, (iii) GP-EI criterion in bayesian optimization (Jones et al., 1998), (iv) MF-SKO, the multi-fidelity sequential kriging optimisation method (Huang et al., 2006b), (v) GP-UCB (Srinivas et al., 2009) and (vi) MFPDOO(z = 1) which is a version of our algorithm that uses only the highest fidelity; this is very similar to POO (Grill et al., 2015) but in a noiseless setting.",6. Empirical Results,[0],[0]
"This algorithm is referred to as PDOO in the figures, which is essentially DOO (Munos, 2011) with the smoothness parameters tuned according to the scheme in POO (Grill et al., 2015).
",6. Empirical Results,[0],[0]
For our theoretical guarantees the bias function ⇣ is assumed to be known.,6. Empirical Results,[0],[0]
"However, in practice we assume a parametric
form for the bias function that is ⇣(z) = c(1 z) where c is initially set to a very small constant like 0.001 in our experiments.",6. Empirical Results,[0],[0]
The nature of Algorithm 2 is such that the same cells are queried at different fidelities by the different MFDOO instances spawned.,6. Empirical Results,[0],[0]
"If a cell is queried at two different fidelities z1 and z2 and the function values obtained are f1 and f2, then we update c to 2c whenever c|z1 z2| < |f1 f2|.",6. Empirical Results,[0],[0]
The above update is only performed if |z1 z2| is greater than a specified threshold (0.0001 in our experiments).,6. Empirical Results,[0],[0]
"The hierarchical partitioning is performed according to a scheme similar to that of the DIRECT algorithm (Finkel, 2003), where each time a cell is split into K children the dimension that has the biggest width is split into K regions.",6. Empirical Results,[0],[0]
We set K = 2 in all our experiments.,6. Empirical Results,[0],[0]
"Now, we will present the results of our synthetic experiments.
",6. Empirical Results,[0],[0]
Our implementation can be found at https://github.com/rajatsen91/MFTREE DET.,6. Empirical Results,[0],[0]
We evaluate all the algorithms on standard benchmark functions used in global optimization.,6.1. Synthetic Experiments,[0],[0]
The functions have been modified to incorporate the fidelity space Z =,6.1. Synthetic Experiments,[0],[0]
"[0, 1].",6.1. Synthetic Experiments,[0],[0]
"The setup followed is identical to the one in (Kandasamy et al., 2017), except that we only work in a one dimensional fidelity space.",6.1. Synthetic Experiments,[0],[0]
"Also, we perform our experiments in a noiseless setting and therefore no Gaussian noise is added to the function evaluations, unlike in (Kandasamy et al., 2017).",6.1. Synthetic Experiments,[0],[0]
Note that MF-GP-UCB and MF-SKO are finite fidelity methods.,6.1. Synthetic Experiments,[0],[0]
The approximations for these methods are obtained at z = 0.333 and z = 0.667.,6.1. Synthetic Experiments,[0],[0]
"We provide more details about the synthetic functions and the fidelities in Appendix D. Our experiments are performed under the deterministic setting, where no noise is added to the approximations.",6.1. Synthetic Experiments,[0],[0]
"However, several of the algorithms that we compare to have a randomized component.",6.1. Synthetic Experiments,[0],[0]
"For these algorithms, the results are averaged over 10 experiments and the corresponding error bars are shown.",6.1. Synthetic Experiments,[0],[0]
In our algorithm we set the number of MFDOO instances spawned to be N = 0.1Dmax,6.1. Synthetic Experiments,[0],[0]
"log(⇤/ (1)), given
a total budget ⇤.",6.1. Synthetic Experiments,[0],[0]
"We set ⇢max = 0.95 and ⌫max = 2.0.
",6.1. Synthetic Experiments,[0],[0]
"The results of the synthetic experiments are shown in Figure 1(a)-(e), where the title of each figure shows the name of the function, the dimension of the domain (d) and the dimension of the fidelity space (p).",6.1. Synthetic Experiments,[0],[0]
We have p = 1 in all our experiments.,6.1. Synthetic Experiments,[0],[0]
"It can be observed the tree based methods outperform the other algorithms by a large margin, except in the experiments with the CurinExp function (Fig. 1c).",6.1. Synthetic Experiments,[0],[0]
"Tree-based methods can handle higher dimensions better, as we can see in the Hartman6 (Fig. 1b) and Borehole (Fig. 1e) function experiments.",6.1. Synthetic Experiments,[0],[0]
Note that MFPDOO also beats PDOO by a large margin which only uses the highest fidelity.,6.1. Synthetic Experiments,[0],[0]
"PDOO is essentially DOO (Munos, 2011) where the smoothness decay parameters are tuned according to the scheme in (Grill et al., 2015).",6.1. Synthetic Experiments,[0],[0]
"MFPDOO can effectively explore the space at cheaper fidelities and then expend the higher fidelities in promising regions of the domain, unlike PDOO.",6.1. Synthetic Experiments,[0],[0]
In this section we describe our experiments that involve tuning hyper-parameters for text classification.,6.2. Tuning SVM for News Group Classification,[0],[0]
"For this purpose we use a subset of the 20 news group dataset (Joachims, 1996).",6.2. Tuning SVM for News Group Classification,[0],[0]
"All the algorithms are used for tuning two hyperparameters: (i) the regularization penalty and (ii) the temperature of the rbf kernel both in the range of [10 2, 103].",6.2. Tuning SVM for News Group Classification,[0],[0]
"For our experiments, we use the scikit-learn implementation of SVM classifier and also the inbuilt KFold function for crossvalidation.",6.2. Tuning SVM for News Group Classification,[0],[0]
The bag of words in each of the text document is converted into tf-idf features before applying the classification models.,6.2. Tuning SVM for News Group Classification,[0],[0]
"We use a one-dimensional fidelity space, where the fidelity denotes the number of samples used to obtain 5-fold cross-validation accuracy.",6.2. Tuning SVM for News Group Classification,[0],[0]
z = 1 corresponds to 5000 samples which is the maximum number of samples in the subset of the data used.,6.2. Tuning SVM for News Group Classification,[0],[0]
z = 0 corresponds to 100 samples.,6.2. Tuning SVM for News Group Classification,[0],[0]
"Note that for the finite fidelity methods MF-SKO and MF-GP-UCB, approximations are obtained at z = 0.33 and z = 0.667.
",6.2. Tuning SVM for News Group Classification,[0],[0]
For our algorithms we set ⌫max = 1.0 and ⇢max = 0.9.,6.2. Tuning SVM for News Group Classification,[0],[0]
At the beginning of the experiment some of the budget is expended to obtain the function values at a point x with two different fidelities z1 = 0.8 and z2 = 0.2.,6.2. Tuning SVM for News Group Classification,[0],[0]
Thus the total budget spent in the initialization is ⇤(0.8) + (0.2).,6.2. Tuning SVM for News Group Classification,[0],[0]
The function values obtained are then used to initialize c in the bias function ⇣(z) = c(1 z).,6.2. Tuning SVM for News Group Classification,[0],[0]
The initial value of c is set to 2|f1 f2|/|z1 z2|.,6.2. Tuning SVM for News Group Classification,[0],[0]
"Thereafter, c is updated online according to the method detailed above.",6.2. Tuning SVM for News Group Classification,[0],[0]
"We set N = 0.5Dmax log(⇤/ (1)).
",6.2. Tuning SVM for News Group Classification,[0],[0]
The cross-validation accuracy obtained as a function of time is plotted in Fig. 1f for all the candidate algorithms.,6.2. Tuning SVM for News Group Classification,[0],[0]
"It can be observed that MFPDOO outperforms the other algorithms, especially in low-budget settings.",6.2. Tuning SVM for News Group Classification,[0],[0]
We considered the problem of black-box function optimization using hierarchical partitions in the presence of cheap approximations or fidelities.,7. Conclusion,[0],[0]
We propose two tree-search based algorithms which can navigate the domain effectively using cheaper fidelities for coarser partitions and more expensive ones while zeroing in on finer partitions.,7. Conclusion,[0],[0]
We analyze our algorithms under standard smoothness assumptions and provide simple regret guarantees given a cost budget ⇤.,7. Conclusion,[0],[0]
Our simple regret guarantees scale much better with respect to ⇤ as compared to other hierarchical partitioning based algorithms that do not use cheaper fidelities.,7. Conclusion,[0],[0]
"Our first algorithm (MFDOO) requires the knowledge of the smoothness parameters (⌫⇤, ⇢⇤) and has a simple regret bound of O(⇤ 1/(d(⌫⇤,⇢⇤)+1))",7. Conclusion,[0],[0]
"where d(⌫⇤, ⇢⇤) is the near-optimality dimension.",7. Conclusion,[0],[0]
"Our second algorithm (MFPDOO) can obtain a simple regret bound of O((⇤/ log⇤) 1/(d(⌫⇤,⇢⇤)+1))",7. Conclusion,[0],[0]
even when the smoothness parameter are unknown.,7. Conclusion,[0],[0]
"Finally, we empirically validate the performance of our algorithms on real and synthetic datasets, where they outperform the stateof-the art multi-fidelity algorithms.
",7. Conclusion,[0],[0]
This work opens up several interesting research directions.,7. Conclusion,[0],[0]
The theoretical guarantees of our algorithms assume some nice properties about the bias and cost functions.,7. Conclusion,[0],[0]
We believe it is possible to design more robust algorithms that have similar guarantees even for the bias and cost functions that are not well-designed.,7. Conclusion,[0],[0]
Our setting is also restricted to a one dimensional fidelity space.,7. Conclusion,[0],[0]
"However, in many application the fidelity space may be multi-dimensional.",7. Conclusion,[0],[0]
"For instance, in the hyper-parameter tuning one can choose to use less samples or train for lesser iterations.",7. Conclusion,[0],[0]
It is an interesting research direction to incorporate a multi-dimensional fidelity space with tree-search based algorithms.,7. Conclusion,[0],[0]
"Finally, in this work we work in the noise-less setting where the function and the approximations are deterministic.",7. Conclusion,[0],[0]
We believe it is possible to extend our results to a setting where zero-mean noise is added to the function and its approximations.,7. Conclusion,[0],[0]
"This work is partially supported by NSF grant 1320175, ARO grant W911NF-17-1-0359, and the US DoT supported D-STOP Tier 1 University Transportation Center.",Acknowledgment,[0],[0]
"Motivated by settings such as hyper-parameter tuning and physical simulations, we consider the problem of black-box optimization of a function.",abstractText,[0],[0]
"Multi-fidelity techniques have become popular for applications where exact function evaluations are expensive, but coarse (biased) approximations are available at much lower cost.",abstractText,[0],[0]
A canonical example is that of hyper-parameter selection in a learning algorithm.,abstractText,[0],[0]
"The learning algorithm can be trained for fewer iterations – this results in a lower cost, but its validation error is only coarsely indicative of the same if the algorithm had been trained till completion.",abstractText,[0],[0]
We incorporate the multi-fidelity setup into the powerful framework of black-box optimization through hierarchical partitioning.,abstractText,[0],[0]
We develop tree-search based multi-fidelity algorithms with theoretical guarantees on simple regret.,abstractText,[0],[0]
We finally demonstrate the performance gains of our algorithms on both real and synthetic datasets.,abstractText,[0],[0]
Multi-Fidelity Black-Box Optimization with Hierarchical Partitions,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3243–3253 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3243
Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained onehop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",text,[0],[0]
"Large-scale knowledge graphs (KGs) support a variety of downstream NLP applications such as semantic search (Berant et al., 2013) and dialogue generation (He et al., 2017).",1 Introduction,[0],[0]
"Whether curated automatically or manually, practical KGs often fail to include many relevant facts.",1 Introduction,[0],[0]
"A popular approach for modeling incomplete KGs is knowledge graph embeddings, which map both entities and relations in the KG to a vector space and learn a truth value function for any potential KG triple parameterized by the entity and relation vectors (Yang et al., 2014; Dettmers et al., 2018).
",1 Introduction,[0],[0]
"Embedding based approaches ignore the symbolic compositionality of KG relations, which limit their application in more complex reasoning tasks.",1 Introduction,[0],[0]
"An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒",1 Introduction,[0],[0]
"bornIn(Obama, US), as shown in Figure 1.",1 Introduction,[0],[0]
Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable.,1 Introduction,[0],[0]
"Early work treats it as a link prediction problem and perform maximum-likelihood classification over either discrete path features (Lao et al., 2011, 2012; Gardner et al., 2013) or their hidden representations in a vector space (Guu et al., 2015; Toutanova et al., 2016; McCallum et al., 2017).
",1 Introduction,[0],[0]
"More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018).",1 Introduction,[0],[0]
"In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths.
",1 Introduction,[0],[0]
We refer to the RL formulation adopted by MINERVA as “learning to walk towards the answer” or “walk-based query-answering (QA)”.,1 Introduction,[0],[0]
"Walk-based QA eliminates the need to precompute path features, yet this setup poses several challenges for training.",1 Introduction,[0],[0]
"First, because practical KGs are intrinsically incomplete, the agent may arrive at a correct answer whose link to the source entity is missing from the training graph without receiving any reward (false negative targets, Figure 2).",1 Introduction,[0],[0]
"Second, since no ground truth path is available for training, the agent may traverse spurious paths that lead to a correct answer only incidentally (false positive paths).",1 Introduction,[0],[0]
"Because REINFORCE (Williams, 1992) is an on-policy RL algorithm (Sutton and Barto, 1998) which encourages past actions with high reward, it can bias the policy toward spurious paths found early in training (Guu et al., 2017).
",1 Introduction,[0],[0]
We propose two modeling advances for RL approaches in the walk-based QA framework to address the aforementioned problems.,1 Introduction,[0],[0]
"First, instead of using a binary reward based on whether the agent has reached a correct answer or not, we adopt pre-trained state-of-the-art embeddingbased models (Dettmers et al., 2018; Trouillon et al., 2016) to estimate a soft reward for target entities whose correctness cannot be determined.",1 Introduction,[0],[0]
"As embedding-based models capture link semantics well, unobserved but correct answers would receive a higher reward score compared to a true negative entity using a well-trained model.",1 Introduction,[0],[0]
"Second, we perform action dropout which randomly blocks some outgoing edges of the agent at each training step so as to enforce effective exploration of a diverse set of paths and dilute the negative impact of the spurious ones.",1 Introduction,[0],[0]
"Empirically, our overall model significantly improves over state-of-the-
art multi-hop reasoning approaches on four out of five benchmark KG datasets (UMLS, Kinship, FB15k-237, WN18RR).",1 Introduction,[0],[0]
It is also the first pathbased model that achieves consistently comparable or better performance than embedding-based models.,1 Introduction,[0],[0]
"We perform a thorough ablation study and result analysis, demonstrating the effect of each modeling innovation.",1 Introduction,[0],[0]
"In this section, we first review the walk-based QA framework (§2.2) and the on-policy reinforcement learning approach proposed by Das et al. (2018) (§2.3,§2.4).",2 Approach,[0],[0]
Then we describe our proposed solutions to the false negative reward and spurious path problems: knowledge-based reward shaping (§2.5) and action dropout (§2.6).,2 Approach,[0],[0]
"We formally represent a knowledge graph as G = (E ,R), where E is the set of entities and R is the set of relations.",2.1 Formal Problem Definition,[0],[0]
"Each directed link in the knowledge graph l = (es, r, eo) ∈ G represents a fact (also called a triple).
",2.1 Formal Problem Definition,[0],[0]
"Given a query (es, rq, ?), where es is the source entity and rq is the relation of interest, the goal is to perform an efficient search over G and collect the set of possible answers",2.1 Formal Problem Definition,[0],[0]
"Eo = {eo} where (es, rq, eo) /∈",2.1 Formal Problem Definition,[0],[0]
G due to incompleteness.,2.1 Formal Problem Definition,[0],[0]
"The search can be formulated as a Markov Decision Process (MDP) (Sutton and Barto, 1998): starting from es, the agent sequentially selects an outgoing edge l and traverses to a new entity until it arrives at a target.",2.2 Reinforcement Learning Formulation,[0],[0]
"Specifically, the MDP consists of the following components (Das et al., 2018).
",2.2 Reinforcement Learning Formulation,[0],[0]
States,2.2 Reinforcement Learning Formulation,[0],[0]
"Each state st = (et, (es, rq)) ∈ S is a tuple where et is the entity visited at step t and (es, rq) are the source entity and query relation.",2.2 Reinforcement Learning Formulation,[0],[0]
"et can be viewed as the state-dependent information while (es, rq) are the global context shared by all states.
",2.2 Reinforcement Learning Formulation,[0],[0]
Actions The set of possible actions At ∈,2.2 Reinforcement Learning Formulation,[0],[0]
"A at step t consists of the outgoing edges of et in G, i.e., At = {(r′, e′)|(et, r′, e′) ∈ G}.",2.2 Reinforcement Learning Formulation,[0],[0]
"To give the agent the option to terminat a search, a self-loop edge is added to every At.",2.2 Reinforcement Learning Formulation,[0],[0]
"When search is unrolled for a fixed number of steps T , the self-loop acts similarly to a “stop” action.
",2.2 Reinforcement Learning Formulation,[0],[0]
Transition A transition function δ,2.2 Reinforcement Learning Formulation,[0],[0]
": S×A→ S is defined by δ(st, At) = δ(et, (es, rq), At).",2.2 Reinforcement Learning Formulation,[0],[0]
"In walk-based QA, the transition is determined by G.
Rewards In the default formulation, the agent receives a terminal reward of 1 if it arrives at a correct target entity when search ends and 0 otherwise.
Rb(sT )",2.2 Reinforcement Learning Formulation,[0],[0]
"= {(es, rq, eT ) ∈ G}.",2.2 Reinforcement Learning Formulation,[0],[0]
(1),2.2 Reinforcement Learning Formulation,[0],[0]
"The search policy is parameterized using state information and global context, plus the search history (Das et al., 2018).
",2.3 Policy Network,[0],[0]
"Specifically, every entity and relation in G is assigned a dense vector embedding e ∈ d and r ∈",2.3 Policy Network,[0],[0]
d.,2.3 Policy Network,[0],[0]
"A particular action at = (rt+1, et+1) ∈",2.3 Policy Network,[0],[0]
At is represented as the concatenation of the relation embedding and the end node embedding at =,2.3 Policy Network,[0],[0]
"[r; e′t].
",2.3 Policy Network,[0],[0]
"The search history ht = (es, r1, e1, . . .",2.3 Policy Network,[0],[0]
", rt, et) ∈ H consists of the sequence of actions taken up to step t, and can be encoded using an LSTM:
h0 = LSTM(0, [r0; es]) (2) ht = LSTM(ht−1,at−1), t > 0, (3)
where r0 is a special start relation introduced to form a start action with es.
",2.3 Policy Network,[0],[0]
The action space At is encoded by stacking the embeddings of all actions in it:,2.3 Policy Network,[0],[0]
At ∈ |At|×2d.,2.3 Policy Network,[0],[0]
"And the policy network π is defined as:
πθ(at|st) = σ(At ×W2 ReLU(W1[et;ht; rq])), (4)
where σ is the softmax operator.",2.3 Policy Network,[0],[0]
"The policy network is trained by maximizing the expected reward over all queries in G:
J(θ) =",2.4 Optimization,[0],[0]
"(es,r,eo)∈G",2.4 Optimization,[0],[0]
"[ a1,...,aT∼πθ [R(sT |es, r)]",2.4 Optimization,[0],[0]
].,2.4 Optimization,[0],[0]
"(5) The optimization is done using the REINFORCE (Williams, 1992) algorithm, which iterates through all (es, r, eo) triples in G1 and updates
1This training strategy treats a query with n > 1 answers as n single-answer queries.",2.4 Optimization,[0],[0]
"In particular, given a query (es, rq, ?) with multiple answers {et1 , . . .",2.4 Optimization,[0],[0]
"etn}, when training w.r.t.",2.4 Optimization,[0],[0]
"the example (es, rq, eti), MINERVA removes all {etj |j ̸= i} observed in the training data from the possible set of target entities in the last search step so as to force the agent to walk towards eti .",2.4 Optimization,[0],[0]
"We adopt the same technique in our training.
",2.4 Optimization,[0],[0]
"θ with the following stochastic gradient:
∇θJ(θ)",2.4 Optimization,[0],[0]
"≈ ∇θ T∑
t=1
R(sT |es, r) log πθ(at|st).
",2.4 Optimization,[0],[0]
(6),2.4 Optimization,[0],[0]
"According to Equation 1, the agent receives a binary reward based solely on the observed answers in G. However, G is intrinsically incomplete and this approach penalizes the false negative search attempts identically to true negatives.",2.5 Knowledge-Based Reward Shaping,[0],[0]
"To alleviate this problem, we adopt existing KG embedding models designed for the purpose of KG completion (Trouillon et al., 2016; Dettmers et al., 2018) to estimate a soft reward for target entities whose correctness is unknown.
",2.5 Knowledge-Based Reward Shaping,[0],[0]
"Formally, the embedding models map E and R to a vector space, and estimate the likelihood of each fact l = (es, r, et) ∈ G using f(es, r, et), a composition function of the entity and relation embeddings.",2.5 Knowledge-Based Reward Shaping,[0],[0]
"f is trained by maximizing the likelihood of all facts in G. We propose the following reward shaping strategy (Ng et al., 1999):
R(sT ) = Rb(sT ) + (1−Rb(sT ))",2.5 Knowledge-Based Reward Shaping,[0],[0]
"f(es, rq, eT ).",2.5 Knowledge-Based Reward Shaping,[0],[0]
"(7) Namely, if the destination eT is a correct answer according to G, the agent receives reward 1.",2.5 Knowledge-Based Reward Shaping,[0],[0]
"Otherwise the agent receives a fact score estimated by f(es, rq, eT ), which is pre-trained.",2.5 Knowledge-Based Reward Shaping,[0],[0]
"Here we keep f in its general form and it can be replaced by any state-of-the-art model (Trouillon et al., 2016; Dettmers et al., 2018) or ensemble thereof.",2.5 Knowledge-Based Reward Shaping,[0],[0]
"The REINFORCE training algorithm performs onpolicy sampling according to πθ(at|st), and updates θ stochastically using Equation 6.",2.6 Action Dropout,[0],[0]
"Because the agent does not have access to any oracle path, it is possible for it to arrive at a correct answer eo via a path irrelevant to the query relation.",2.6 Action Dropout,[0],[0]
"As shown in Figure 1, the path Obama −endorsedBy→ McCain −liveIn→ U.S. ←locatedIn− Hawaii does not infer the fact bornIn(Obama,Hawaii).
",2.6 Action Dropout,[0],[0]
"Discriminating paths of different qualities is non-trivial, and existing RL approaches for walkbased KGQA largely rely on the terminal reward to bias the search.",2.6 Action Dropout,[0],[0]
"Since there are usually more spurious paths than correct ones, spurious paths are often found first, and following exploration can be increasingly biased towards them (Equation 6).
",2.6 Action Dropout,[0],[0]
"Entities with larger fan-in (in-degree) and fan-out (out-degree) often exacerbate this problem.
",2.6 Action Dropout,[0],[0]
"Guu et al. (2017) identified a similar issue in RL-based semantic parsing with weak supervision, where programs that do not semantically match the user utterance frequently pass the tests.",2.6 Action Dropout,[0],[0]
"To solve this problem, Guu et al. (2017) proposed randomized beam search combined with a meritocratic update rule to ensure all trajectories that obtain rewards are up-weighted roughly equally.
",2.6 Action Dropout,[0],[0]
Here we propose the action dropout technique which achieves similar effect as randomized search and is simpler to implement over graphs.,2.6 Action Dropout,[0],[0]
Action dropout randomly masks some outgoing edges for the agent in the sampling step of REINFORCE.,2.6 Action Dropout,[0],[0]
"The agent then performs sampling2 according to the adjusted action distribution
π̃θ(at|st) ∝",2.6 Action Dropout,[0],[0]
"(πθ(at|st) ·m+ ϵ) (8) mi ∼ Bernoulli(1− α), i = 1, . . .",2.6 Action Dropout,[0],[0]
"|At|, (9)
where each entry of m ∈ {0, 1}|At| is a binary variable sampled from the Bernoulli distribution with parameter 1 − α.",2.6 Action Dropout,[0],[0]
"A small value ϵ is used to smooth the distribution in case m = 0, where π̃θ(at|st) becomes uniform.
",2.6 Action Dropout,[0],[0]
Our overall approach is illustrated in Figure 3.,2.6 Action Dropout,[0],[0]
"In this section, we summarize the related work and discuss their connections to our approach.
",3 Related Work,[0],[0]
2We only modify the sampling distribution and still use πθ(at|st) to compute the gradient update in equation 6.,3 Related Work,[0],[0]
"KG embeddings (Bordes et al., 2013; Socher et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) are one-hop KG modeling approaches which learn a scoring function f(es, r, eo) to define a fuzzy truth value of a triple in the embedding space.",3.1 Knowledge Graph Embeddings,[0],[0]
"These models can be adapted for query answering by simply return the eo’s with the highest f(es, r, eo) scores.",3.1 Knowledge Graph Embeddings,[0],[0]
"Despite their simplicity, embedding-based models achieved state-of-the-art performance on KGQA (Das et al., 2018).",3.1 Knowledge Graph Embeddings,[0],[0]
"However, such models ignore the symbolic compositionality of KG relations, which limits their usage in more complex reasoning tasks.",3.1 Knowledge Graph Embeddings,[0],[0]
The reward shaping (RS) strategy we proposed is a step to combine their capability in modeling triple semantics with the symbolic reasoning capability of the path-based approach.,3.1 Knowledge Graph Embeddings,[0],[0]
"Multi-hop reasoning focus on learning symbolic inference rules from relational paths in the KG and has been formulated as sequential decision problems in recent works (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018).",3.2 Multi-Hop Reasoning,[0],[0]
"In particular, DeepPath (Xiong et al., 2017) first adopted REINFORCE to search for generic representative paths between pairs of entities.",3.2 Multi-Hop Reasoning,[0],[0]
"DIVA (Chen et al., 2018) also performs generic path search between entities using RL and its variational objective can be interpreted as model-based reward assignment.",3.2 Multi-Hop Reasoning,[0],[0]
"MINERVA (Das et al., 2018) first introduced RL
to search for answer entities of a particular KG query end-to-end.",3.2 Multi-Hop Reasoning,[0],[0]
"MINERVA uses entropy regularization to softly encourage the policy to sample diverse paths, and we show that hard action dropout is more effective in this setup.",3.2 Multi-Hop Reasoning,[0],[0]
"ReinforceWalk (Shen et al., 2018) further proposed to solve the reward sparsity problem in walk-based QA using off-policy learning.",3.2 Multi-Hop Reasoning,[0],[0]
ReinforceWalk scores the search targets with a value function which is updated based on the search history cached through epochs.,3.2 Multi-Hop Reasoning,[0],[0]
"In comparison, we leveraged existing embedding-based models for reward shaping, which is much more efficient during training.",3.2 Multi-Hop Reasoning,[0],[0]
"Recently, RL has seen a variety of applications in NLP including machine translation (Ranzato et al., 2015), summarization (Paulus et al., 2017), and semantic parsing (Guu et al., 2017).",3.3 Reinforcement Learning,[0],[0]
"Compared to the domain of gaming (Mnih et al., 2013) where RL is mostly applied for, RL formulations in NLP often have a large discrete action space.",3.3 Reinforcement Learning,[0],[0]
"For example, in machine translation, the space of possible actions is the entire vocabulary of a language.",3.3 Reinforcement Learning,[0],[0]
"Walk-based QA also suffers from this problem, as some entities may have thousands of neighbors (e.g. U.S.).",3.3 Reinforcement Learning,[0],[0]
"Since often there is no golden path available for a KG reasoning problem, we cannot leverage supervised pre-training to initialize the path search following the common practice in RL-based natural language generation (Ranzato et al., 2015).",3.3 Reinforcement Learning,[0],[0]
"On the other hand, the inference paths being studied in a KG are often much shorter (usually containing 2-5 steps) compared to the target sentences in the NL generation problems (often containing 20-30 words), which simplifies the training to some extent.",3.3 Reinforcement Learning,[0],[0]
We evaluate our modeling contributions on five KGs from different domains and exhibiting different graph properties (§ 4.1).,4 Experiment Setup,[0],[0]
We compare with two classes of state-of-the-art KG models: multi-hop neural symbolic approaches and KG embeddings (§4.2).,4 Experiment Setup,[0],[0]
"In this section, we describe the datasets and our experiment setup in detail.",4 Experiment Setup,[0],[0]
"We adopt five benchmark KG datasets for query answering: (1) Alyawarra Kinship, (2) Unified Medical Language Systems (Kok and Domingos,
2007), (3) FB15k-237 (Toutanova et al., 2015), (4) WN18RR (Dettmers et al., 2018), and (5) NELL995 (Xiong et al., 2017).",4.1 Dataset,[0],[0]
The statistics of the datasets are shown in Table 1.,4.1 Dataset,[0],[0]
We compare with three embedding based models:,4.2 Baselines and Model Variations,[0],[0]
"DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016) and ConvE (Dettmers et al., 2018).",4.2 Baselines and Model Variations,[0],[0]
"We also compare with three multi-hop neural symbolic models: (a) NTP-λ, an improved version of Neural Theorem Prover (Rocktäschel and Riedel, 2017), (b) Neural Logical Programming (NeuralLP) (Yang et al., 2017) and (c) MINERVA.",4.2 Baselines and Model Variations,[0],[0]
"For our own approach, we include two model variations that use ComplEx and ConvE as the reward shaping modules respectively, denoted as Ours(ComplEx) and Ours(ConvE).",4.2 Baselines and Model Variations,[0],[0]
"We quote the results of NeuralLP, NTP-λ and MINERVA reported in Das et al. (2018), and replicated the embedding based systems.3",4.2 Baselines and Model Variations,[0],[0]
Beam Search Decoding We perform beam search decoding to obtain a list of unique entity predictions.,4.3 Implementation Details,[0],[0]
"Because multiple paths may lead to the same target entity, we compute the list of unique entities reached in the final search step and assign each of them the maximum score of all paths that led to it.",4.3 Implementation Details,[0],[0]
We then output the top-ranked unique entities.,4.3 Implementation Details,[0],[0]
"We find this approach to improve over directly taking the entities ranked at the beam top, as many of them are repetitions.
",4.3 Implementation Details,[0],[0]
"KG Setup Following previous work, we treat every KG link as bidirectional and augment the graph with the reversed (eo, r−1, es) links.",4.3 Implementation Details,[0],[0]
"We use the same train, dev, and test set splits as Das et al. (2018).",4.3 Implementation Details,[0],[0]
"We exclude any link from the dev and
3 Das et al. (2018) reported MINERVA results with the entity embedding usage as an extra hyperparameter – the quoted performance of MINERVA in Table 2 on UMLS and Kinship were obtained with entity embeddings setting to zero.",4.3 Implementation Details,[0],[0]
"In contrast, our system always uses trained entity embeddings.
test set (and its reversed link) from the train set.",4.3 Implementation Details,[0],[0]
"Following Das et al. (2018), we cut the maximum number of outgoing edges of an entity by threshold η to prevent GPU memory overflow: for each entity we keep its top-η neighbors with the highest PageRank scores (Page et al., 1999) in the graph.
",4.3 Implementation Details,[0],[0]
Hyperparameters We set the entity and relation embedding size to 200 for all models.,4.3 Implementation Details,[0],[0]
"We use Xavier initialization (Glorot and Bengio, 2010) for the embeddings and the NN layers.",4.3 Implementation Details,[0],[0]
"For ConvE, we use the same convolution layer and label smoothing hyperparameters as Dettmers et al. (2018).",4.3 Implementation Details,[0],[0]
"For path-based models, we use a three-layer LSTM as the path encoder and set its hidden dimension to 200.",4.3 Implementation Details,[0],[0]
"We perform grid search on the reasoning path length (2, 3), the node fan-out threshold η (256- 512) and the action dropout rate α (0.1-0.9).",4.3 Implementation Details,[0],[0]
"Following Das et al. (2018), we add an entropy regularization term in the objective and tune the weight parameter β within 0-0.1.",4.3 Implementation Details,[0],[0]
"We use Adam optimization (Kingma and Ba, 2014) and search the learning rate (0.001-0.003) and mini-batch size (128- 512).4",4.3 Implementation Details,[0],[0]
"For all models we apply dropout to the entity and relation embeddings and all feed-forward layers, and search the dropout rates within 0-0.5.",4.3 Implementation Details,[0],[0]
"We use a decoding beam size of 512 for NELL995 and 128 for the other datasets.
",4.3 Implementation Details,[0],[0]
"Evaluation Protocol We convert each triple (es, r, eo) in the test set into a query and compute ranking-based evaluation metrics.",4.3 Implementation Details,[0],[0]
"The models take es, r as the input and output a list of candidate answers Eo =",4.3 Implementation Details,[0],[0]
"[e1, . .",4.3 Implementation Details,[0],[0]
.,4.3 Implementation Details,[0],[0]
", eL] ranked in decreasing order of confidence score.",4.3 Implementation Details,[0],[0]
"We compute
4On some datasets, we found larger batch size to continue improving the performance but had to stop at 512 due to memory constraints.
",4.3 Implementation Details,[0],[0]
"reo , the rank of eo among Eo, after removing the other correct answers from Eo and use it to compute two types of metrics: (1) Hits@k which is the percentage of examples where reo ≤ k and (2) mean reciprocal rank (MRR) which is the mean of 1/reo for all examples in the test set.",4.3 Implementation Details,[0],[0]
"We use the entire test set for evaluation, with the exception of NELL-995, where test triples with unseen entities are removed following Das et al. (2018).
",4.3 Implementation Details,[0],[0]
Our Pytorch implementation of all experiments is released at https://github.com/ salesforce/MultiHopKG.,4.3 Implementation Details,[0],[0]
Table 2 shows the evaluation results of our proposed approach and the baselines.,5.1 Model Comparison,[0],[0]
The top part presents embedding based approaches and the bottom part presents multi-hop reasoning,5.1 Model Comparison,[0],[0]
"approaches.5
We find embedding based models perform strongly on several datasets, achieving overall best evaluation metrics on UMLS, Kinship, FB15K237 and NELL-995 despite their simplicity.",5.1 Model Comparison,[0],[0]
"While previous path based approaches achieve comparable performance on some of the datasets (WN18RR, NELL-995, and UMLS), they perform significantly worse than the embedding based models on the other datasets (9.1 and 14.2 absolute points lower on Kinship and FB15k-237 respectively).",5.1 Model Comparison,[0],[0]
"A possible reason for this is that embedding based methods map every link in the KG into the same embedding space, which implicitly encodes the connectivity of the whole graph.",5.1 Model Comparison,[0],[0]
"In contrast, path based models use the discrete represen-
5We report the model robustness measurements in § A.1.
tation of a KG as input, and therefore have to leave out a significant proportion of the combinatorial path space by selection.",5.1 Model Comparison,[0],[0]
"For some path based approaches, computation cost is a bottleneck.",5.1 Model Comparison,[0],[0]
"In particular, NeuralLP and NTP-λ failed to scale to the larger datasets and their results are omitted from the table, as Das et al. (2018) reported.
",5.1 Model Comparison,[0],[0]
Ours is the first multi-hop reasoning approach which is consistently comparable or better than embedding based approaches on all five datasets.,5.1 Model Comparison,[0],[0]
"The best single model, Ours(ConvE), improves the SOTA performance of path-based models on three datasets (UMLS, Kinship, and FB15k-237) by 4%, 9%, and 39% respectively.",5.1 Model Comparison,[0],[0]
"On NELL-995, our approach did not significantly improve over existing SOTA.",5.1 Model Comparison,[0],[0]
"The NELL-995 dataset consists of only 12 relations in the test set and, as we further detail in the analysis (§ 5.3.3), our approach is less effective for those relation types.
",5.1 Model Comparison,[0],[0]
The model variations using different reward shaping modules perform similarly.,5.1 Model Comparison,[0],[0]
"While a better reward shaping module typically results in a better overall model, an exception is WN18RR, where ComplEx performs slightly worse on its own but is more helpful for reward shaping.",5.1 Model Comparison,[0],[0]
We left the study of the relationship between the reward shaping module accuracy and the overall model performance as future work.,5.1 Model Comparison,[0],[0]
We perform an ablation study where we remove reward shaping (−RS) and action dropout (−AD) from Ours(ConvE) and compare their MRRs to the whole model on the dev sets.6,5.2 Ablation Study,[0],[0]
"As shown in Table 3, on most datasets, removing each component results in a significant performance drop.",5.2 Ablation Study,[0],[0]
"The exception is WN18RR, where removing the ConvE reward shaping module improves the performance.7 Removing reward shaping on NELL-
6According to Table 3 and Table 2, the dev and test set evaluation metrics differ significantly on several datasets.",5.2 Ablation Study,[0],[0]
"We discuss the cause of this in § A.2.
",5.2 Ablation Study,[0],[0]
"7A possible explanation for this is that as path-based models tend to outperform the embedding based approaches on WN18RR, ConvE may be supplying more noise than useful
995 does not change the results significantly.",5.2 Ablation Study,[0],[0]
"In general, removing action dropout has a greater impact, suggesting that thorough exploration of the path space is important across datasets.",5.2 Ablation Study,[0],[0]
We are interested in studying the impact of each proposed enhancement on the training convergence rate.,5.3.1 Convergence Rate,[0],[0]
"In particular, we expect reward shaping to accelerate the convergence of RL (to a better performance level) as it propagates prior knowledge about the underlying KG to the agent.",5.3.1 Convergence Rate,[0],[0]
"On the other hand, a fair concern for action dropout is that it can be slower to train, as the agent is forced to explore a more diverse set of paths.",5.3.1 Convergence Rate,[0],[0]
"Figure 4 eliminates this concern.
",5.3.1 Convergence Rate,[0],[0]
The first row of Figure 4 shows the changes in dev set MRR of Ours(ConvE) (green ∗) and the two ablated models w.r.t.,5.3.1 Convergence Rate,[0],[0]
# epochs.,5.3.1 Convergence Rate,[0],[0]
"In general, the proposed approach is able to converge to a higher accuracy level much faster than either of the ablated models and the performance gap often persists until the end of training (on UMLS, Kinship, and FB15k-237).",5.3.1 Convergence Rate,[0],[0]
"Particularly, on FB15k-237, our approach still shows improvement even after the two ablated models start to overfit, with −AD beginning to overfit sooner.",5.3.1 Convergence Rate,[0],[0]
"On WN18RR, introducing reward shaping hurt dev set performance from the beginning, as discussed in § 5.2.",5.3.1 Convergence Rate,[0],[0]
"On NELL995, Ours(ConvE) performs significantly better in the beginning, but −RS gradually reaches a comparable performance level.
",5.3.1 Convergence Rate,[0],[0]
It is especially interesting that introducing action dropout immediately improves the model performance on all datasets.,5.3.1 Convergence Rate,[0],[0]
A possible explanation for this is that by exploring a more diverse set of paths the agent learns search policies that generalize better.,5.3.1 Convergence Rate,[0],[0]
We also compute the total number of unique paths the agent explores during training and visualize its change w.r.t.,5.3.2 Path Diversity,[0],[0]
# training epochs in the second row of Figure 4.,5.3.2 Path Diversity,[0],[0]
"When counting a unique path, we include both the edge label and intermediate entity.
information about the KG.",5.3.2 Path Diversity,[0],[0]
"Yet counter-intuitively, we found that adding the ComplEx reward shaping module helps, despite the fact that ComplEx performs slightly worse than ConvE on this dataset.",5.3.2 Path Diversity,[0],[0]
"This indicates that dev set accuracy is not the only factor which determines the effectiveness of reward shaping.
",5.3.2 Path Diversity,[0],[0]
"First we observe that, on all datasets, the agent explores a large number of paths before reaching a good performance level.",5.3.2 Path Diversity,[0],[0]
The speed of path discovery slowly decreases as training progresses.,5.3.2 Path Diversity,[0],[0]
"On smaller KGs (UMLS and Kinship), the rate of encountering new paths is significantly lower after a certain number of epochs, and the dev set accuracy plateaus correspondingly.",5.3.2 Path Diversity,[0],[0]
"On much larger KGs (FB15k-237, WN18RR, and NELL-995), we did not observe a significant slowdown before severe overfitting occurs and the dev set performance starts to drop.",5.3.2 Path Diversity,[0],[0]
"A possible reason for this is that the larger KGs are more sparsely connected compared to the smaller KGs (Table 1), therefore it is less efficient to gain generalizable knowledge from the KG by exploring a limited proportion of the path space through sampling.
",5.3.2 Path Diversity,[0],[0]
"Second, while removing action dropout significantly lowers the effectiveness of path exploration (orange !",5.3.2 Path Diversity,[0],[0]
"vs. green ∗), we observe that removing reward shaping (blue △) slightly increases the # paths visited if the action dropout rate is kept the same.",5.3.2 Path Diversity,[0],[0]
"This indicates that the correlation between
# paths explored and dev set performance is not strictly positive.",5.3.2 Path Diversity,[0],[0]
The best performing model is not always the model that explored the largest # paths.,5.3.2 Path Diversity,[0],[0]
It also demonstrates the role of reward shaping as a regularizer which guides the agent to avoid noisy paths with its prior knowledge.,5.3.2 Path Diversity,[0],[0]
We investigate the behaviors of our proposed approach w.r.t different relation types.,5.3.3 Performance w.r.t. Relation Types,[0],[0]
"For each KG, we classify its set of relations into two categories based on the answer set cardinality.",5.3.3 Performance w.r.t. Relation Types,[0],[0]
"Specifically, we define the metric ξr as the average answer set cardinality of all queries with topic relation r.",5.3.3 Performance w.r.t. Relation Types,[0],[0]
"We count r as a “to-many” relation if ξr > 1.5, which indicates that most queries in relation r has more than 1 correct answer; we count r as a “to-one” relation otherwise, meaning most queries of this relation have only 1 correct answer.
",5.3.3 Performance w.r.t. Relation Types,[0],[0]
"Table 4 shows the percentage of examples of tomany and to-one relations on each dev dataset and the MRR evaluation metrics of previously studied models computed on the examples of each relation
type.",5.3.3 Performance w.r.t. Relation Types,[0],[0]
"Since UMLS and Kinship are densely connected, they almost exclusively contain to-many relations.",5.3.3 Performance w.r.t. Relation Types,[0],[0]
FB15k-237 mostly contains to-many relations.,5.3.3 Performance w.r.t. Relation Types,[0],[0]
"In Figure 4, we observe the biggest relative gains from the ablated models on these three datasets.",5.3.3 Performance w.r.t. Relation Types,[0],[0]
WN18RR is more balanced and consists of slightly more to-many relations than toone relations.,5.3.3 Performance w.r.t. Relation Types,[0],[0]
The NELL-995 dev set is a unique one which almost exclusively consists of to-one relations.,5.3.3 Performance w.r.t. Relation Types,[0],[0]
"There is no common performance pattern over the two relation types across datasets: on some datasets all models perform better on tomany relations (UMLS, WN18RR) while others show the opposite trend (FB15k-237, NELL-995).",5.3.3 Performance w.r.t. Relation Types,[0],[0]
"We leave the study of these discrepancies to future work.
",5.3.3 Performance w.r.t. Relation Types,[0],[0]
We show the relative performance change of the ablated models−RS and−AD w.r.t. Ours(ConvE),5.3.3 Performance w.r.t. Relation Types,[0],[0]
in parentheses.,5.3.3 Performance w.r.t. Relation Types,[0],[0]
We observe that in general our proposed enhancements are effective in improving query-answering over both relation types (more effective for to-many relations).,5.3.3 Performance w.r.t. Relation Types,[0],[0]
"However, adding the ConvE reward shaping module on WN18RR hurts the performance over both to-many and toone relations (more for to-one relations).",5.3.3 Performance w.r.t. Relation Types,[0],[0]
"On NELL-995, both techniques hurt the performance over to-many relations.",5.3.3 Performance w.r.t. Relation Types,[0],[0]
"Since most benchmark datasets randomly split the KG triples into train, dev and test sets, the queries that have multiple answers may fall into multiple splits.",5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
"As a result, some of the test queries (es, rq, ?) are seen in the training set (with a different set of answers) while the others are not.",5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
We investigate the behaviors of our proposed approach w.r.t.,5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
"seen and unseen queries.
",5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
Table 5 shows the percentage of examples associated with seen and unseen queries on each dev dataset and the corresponding MRR evaluation metrics of previously studied models.,5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
"On
most datasets, the ratio of seen vs. unseen queries is similar to that of to-many vs. to-one relations (Table 4) as a result of random data split, with the exception of WN18RR.",5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
"On some datasets, all models perform better on seen queries (UMLS, Kinship, WN18RR) while others reveal the opposite trend.",5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
We leave the study of these model behaviors to future work.,5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
On NELL-995 both of our proposed enhancements are not effective over the seen queries.,5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
"In most cases, our proposed enhancements improve the performance over unseen queries, with AD being more effective.",5.3.4 Performance w.r.t. Seen Queries vs. Unseen Queries,[0],[0]
We propose two modeling advances for end-toend RL-based knowledge graph query answering: (1) reward shaping via graph completion and (2) action dropout.,6 Conclusions,[0],[0]
Our approach improves over state-of-the-art multi-hop reasoning models consistently on several benchmark KGs.,6 Conclusions,[0],[0]
"A detailed analysis indicates that the access to a more accurate environment representation (reward shaping) and a more thorough exploration of the search space (action dropout) are important to the performance boost.
",6 Conclusions,[0],[0]
"On the other hand, the performance gap between RL-based approaches and the embeddingbased approaches for KGQA remains.",6 Conclusions,[0],[0]
"In future work, we would like to investigate learnable reward shaping and action dropout schemes and apply model-based RL to this domain.",6 Conclusions,[0],[0]
"We thank Mark O. Riedl, Yingbo Zhou, James Bradbury and Vena Jia Li for their feedback on early draft of the paper, and Mark O. Riedl for helpful conversations on reward shaping.",Acknowledgements,[0],[0]
We thank the anonymous reviewers and the Salesforce research team members for their thoughtful comments and discussions.,Acknowledgements,[0],[0]
We thank Fréderic,Acknowledgements,[0],[0]
Godin for pointing out an error in Equation 8 in an early version of the paper.,Acknowledgements,[0],[0]
Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs).,abstractText,[0],[0]
"The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target.",abstractText,[0],[0]
"However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time.",abstractText,[0],[0]
"Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer.",abstractText,[0],[0]
We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained onehop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks.,abstractText,[0],[0]
Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.,abstractText,[0],[0]
Multi-Hop Knowledge Graph Reasoning with Reward Shaping,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 162–169 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Natural language generation (NLG) has a broad range of applications, from question answering systems to story generation, summarization etc.",1 Introduction,[0],[0]
"In this paper, we target a particular use case that is important for e-Commerce websites, which group multiple items on common pages called browse pages (BP).",1 Introduction,[0],[0]
"Each browse page contains an overview of various items which share some characteristics expressed as slot/value pairs.
",1 Introduction,[0],[0]
"For example, we can have a browse page for Halloween decoration, which will display different types like lights, figurines, and candy bowls.",1 Introduction,[0],[0]
"These different items of decoration have their own browse pages, which are linked from the BP for Halloween decoration.",1 Introduction,[0],[0]
"A ceramic candy bowl for Halloween can appear on various browse pages, e.g. on the BP for Halloween decoration, BP for Halloween candy bowls, as well as the (non Halloween-specific) BP for ceramic candy bowls.
",1 Introduction,[0],[0]
"To show customers which items are grouped on a browse page, we need a human-readable title of the content of that particular page.",1 Introduction,[0],[0]
"Different combinations of characteristics bijectively correspond to different browse pages, and consequently to different browse page titles.
",1 Introduction,[0],[0]
"Note that here, different from other natural language generation tasks described in the literature, slot names are already given; the task is to generate a title for a set of slots.",1 Introduction,[0],[0]
"Moreover, we do not perform any selection of the slots that the title should realize; but all slots need to be realized in order to have a unique title.",1 Introduction,[0],[0]
E-Commerce sites may have tens of millions of such browse pages in many different languages.,1 Introduction,[0],[0]
The number of unique slot-value pairs are in the order of hundreds of thousands.,1 Introduction,[0],[0]
"All these factors render the task of human creation of BP titles infeasible.
",1 Introduction,[0],[0]
"Mathur, Ueffing, and Leusch (2017) developed several different systems which generated titles for these pages automatically.",1 Introduction,[0],[0]
"These systems include rule-based approaches, statistical models, and combinations of the two.",1 Introduction,[0],[0]
"In this work, we investigate the use of neural sequence-to-sequence models for browse page title generation.",1 Introduction,[0],[0]
"These models have recently received much attention in the research community, and are becoming the new state of the art in machine translation (refer Section 4).
",1 Introduction,[0],[0]
"We will compare our neural generation models
162
against two state-of-the-art systems.
1.",1 Introduction,[0],[0]
"The baseline system for English and French implements a hybrid generation approach, which combines a rule-based approach (with a manually created grammar) and statistical machine translation (SMT) techniques.",1 Introduction,[0],[0]
"For French, we have monolingual data for training language model, which can be used in the SMT system.",1 Introduction,[0],[0]
"For English, we also have human-curated titles and can use those for training additional “translation” components for this hybrid system.
2.",1 Introduction,[0],[0]
"The system for German is an Automatic Post-Editing (APE) system – first introduced by Simard et al. (2007) – which generates titles with the rule-based approach, and then uses statistical machine translation techniques for automatically correcting the errors made by the rule-based approach.
",1 Introduction,[0],[0]
"In the following section, we describe a few of the previous works in the field of language generation from a knowledge base or linked data.",1 Introduction,[0],[0]
Section 3 addresses the idea of lexicalization of a browse node in linear form along with the normalization step to replace the slot values with placeholders.,1 Introduction,[0],[0]
"Sequence-to-sequence models for generation of titles are described in Section 4, followed by a description of joint learning over multiple languages in Section 5.",1 Introduction,[0],[0]
Experiments and results are described in Sections 6 and 7.,1 Introduction,[0],[0]
"The first works on NLG were mostly focused on rule-based language generation (Dale et al., 1998; Reiter et al., 2005; Green, 2006).",2 Related work,[0],[0]
"NLG systems typically perform three different steps: content selection, where a subset of relevant slot/value pairs are selected, followed by sentence planning, where these selected pairs are realized into their respective linguistic variations, and finally surface realization, where these linguistic structures are combined to generate text.",2 Related work,[0],[0]
"Our use case differs from the above in that there is no selection done on the slot/value pairs, but all of them undergo the sentence planning step.",2 Related work,[0],[0]
"In rule-based systems, all of the above steps rely on hand-crafted rules.
",2 Related work,[0],[0]
"Data driven approaches, on the other hand, either try to learn each of the steps automatically from the data Barzilay and Lapata (2005)
",2 Related work,[0],[0]
Dale et al. (1998) described the problem of generating natural language titles and short descriptions of structured nodes which consist of slot/value pairs.,2 Related work,[0],[0]
There are many research which deal with learning a generation model from parallel data.,2 Related work,[0],[0]
"These parallel data consist of the structured data and natural-language text, so that the model can learn to transform the structured data into text.",2 Related work,[0],[0]
"Duma and Klein (2013) generate short natural-language descriptions, taking structured DBPedia data as input.",2 Related work,[0],[0]
"Their approach learns text templates which are filled with the information from the structured data.
",2 Related work,[0],[0]
"Mei et al. (September, 2015) use recurrent neural network (LSTM) models to generate text from facts given in a knowledge base.",2 Related work,[0],[0]
Chisholm et al. (2017) solve the same problem by applying a machine translation system to a linearized version of the pairs.,2 Related work,[0],[0]
Several recent papers tackle the problem of generating a one-sentence introduction for a biography given structured biographical slot/value pairs.,2 Related work,[0],[0]
"One difference between our work and the papers above, (Mei et al., September, 2015), and (Chisholm et al., 2017), is that they perform selective generation, i.e. they run a selection step that determines the slot/value pairs which will be included in the realization.",2 Related work,[0],[0]
"In our use case however, all slot/value pairs are relevant and need to be realized.
",2 Related work,[0],[0]
Serban et al. (2016) generate questions from facts (structured input) by leveraging fact embeddings and then employing placeholders for handling rare words.,2 Related work,[0],[0]
"In their work, the placeholders are heuristically mapped to the facts, however, we map our placeholders depending on the neural attention (for details, see Section 4).",2 Related work,[0],[0]
Our first step towards title generation is verbalization of all slot/value pairs.,3 Lexicalization,[0],[0]
"This can be achieved by a rule-based approach as described in (Mathur et al., 2017).",3 Lexicalization,[0],[0]
"However, in the work presented here, we do not directly lexicalize the slot/value pairs, but realize them in a pseudo language first.",3 Lexicalization,[0],[0]
"For example, the pseudo-language sequence for the slot/value pairs in Table 1 is “ brand ACME cat Cell Phones & Smart Phones color white capacity 32GB”.1
1 cat refers to an e-Commerce category in the browse page.",3 Lexicalization,[0],[0]
Pseudo-language browse pages can still contain a large number of unique slot values.,3.1 Normalization,[0],[0]
"For example, there exist many different brands for smart phones (Samsung, Apple, Huawei, etc.).",3.1 Normalization,[0],[0]
"Large vocabulary is a known problem for neural systems, because rare or less frequent words tend to translate incorrectly due to data sparseness (Luong et al., 2015).",3.1 Normalization,[0],[0]
"At the same time, the softmax computation over the large vocabulary becomes intractable in current hardware.",3.1 Normalization,[0],[0]
"To avoid this issue, we normalize the pseudo-language sequences and thereby reduce the vocabulary size.",3.1 Normalization,[0],[0]
"For each language, we computed the 30 most frequent slot names and normalized their values via placeholders (Luong et al., August, 2015).",3.1 Normalization,[0],[0]
"For example, the lexicalization of “Brand: ACME” is “ brand ACME”, but after normalization, this becomes brand $brand|ACME.",3.1 Normalization,[0],[0]
This representation means that the slot name brand has the value of a placeholder brand which contains the entity called “ACME”.,3.1 Normalization,[0],[0]
"During training, we remove the entity from the normalized sequence, while keeping them during translation of development or evaluation set.",3.1 Normalization,[0],[0]
"The mapping of placeholders in the target text back to entity names is described in Section 4.
",3.1 Normalization,[0],[0]
The largest reduction in vocabulary size would be achieved by normalizing all slots.,3.1 Normalization,[0],[0]
"However, this would create several issues in generation.",3.1 Normalization,[0],[0]
Consider the pseudo-language sequence “ bike Road bike type Racing”.,3.1 Normalization,[0],[0]
"If we replace all slot values with placeholders, i.e. “ bike $bike type $type”, then the system will not have enough information for generating the title “Road racing bike”.",3.1 Normalization,[0],[0]
"Moreover, the boolean slots, such as “ comic Marvel comics signed No” would be normalized to placeholders as “ comic $comic signed $signed”, and we would loose the information (“No”) necessary to realize this title as “Unsigned Marvel comics”.",3.1 Normalization,[0],[0]
"We applied another way of reducing the vocabulary, called byte pair encoding (BPE) (Sennrich
et al., 2016), a technique often used in NMT systems (Bojar et al., 2017).",3.2 Sub-word units,[0],[0]
BPE is essentially a data compression technique which splits each word into sub-word units and allows the NMT system to train on a smaller vocabulary.,3.2 Sub-word units,[0],[0]
One of the advantages of BPE is that it propagates generation of unseen words (even with different morphological variations).,3.2 Sub-word units,[0],[0]
"However, in our use case, this can create issues, because if BPE splits a brand and generates an incorrect brand name in the target, an e-Commerce company could be legally liable for the mistake.",3.2 Sub-word units,[0],[0]
"In such case, one can first run the normalization with placeholders followed by BPE, but due to time constraints, we do not report experiments on the same.",3.2 Sub-word units,[0],[0]
"Sequence-to-sequence models in this work are based on an encoder-decoder model and an attention mechanism as described by Bahdanau et al. (May, 2016).",4 Sequence-to-Sequence Models,[0],[0]
"In this network, the encoder is a bidirectional RNN which encodes the information of a sentence X = (x1, x2, . . .",4 Sequence-to-Sequence Models,[0],[0]
"xm) of length m into a fixed length vector of size |hi|, where hi is the hidden state produced by the encoder for token xi.",4 Sequence-to-Sequence Models,[0],[0]
"Since our encoder is a bi-directional model, the encoded hidden state is hi = hi,fwd + hi,bwd, where hfwd and hbwd are unidirectional encoders, running from left to right and right to left, respectively.",4 Sequence-to-Sequence Models,[0],[0]
"That is, they are encoding the context to the left and to the right of the current token.
",4 Sequence-to-Sequence Models,[0],[0]
"Our decoder is a simple recurrent neural network (RNN) consisting of gated recurrent units (GRU) (Cho et al., 2014) because of their computationally efficiency.",4 Sequence-to-Sequence Models,[0],[0]
"The RNN predicts the target sequence Y = (y1, y2, . . .",4 Sequence-to-Sequence Models,[0],[0]
", yj , . . .",4 Sequence-to-Sequence Models,[0],[0]
", yl) based on the final encoded state h. Basically, the RNN predicts the target token yj ∈ V (with target vocabulary V) and emits a hidden state sj based on the previous recurrent state sj−1, the previous sequence of words Yj−1 = (y1, y2, . . .",4 Sequence-to-Sequence Models,[0],[0]
", yj−1) and Cj , a weighted attention vector.",4 Sequence-to-Sequence Models,[0],[0]
The attention vector is a weighted average of all the hidden source states,4 Sequence-to-Sequence Models,[0],[0]
"hi, where i = 1, . .",4 Sequence-to-Sequence Models,[0],[0]
.,4 Sequence-to-Sequence Models,[0],[0]
",m. Attention weight (aij) is computed between the hidden states hi and sj and is leveraged as a weight of that source state hi.",4 Sequence-to-Sequence Models,[0],[0]
"In generation, we make use of these alignment scores to align our placeholders.2 The target placeholders are bijectively mapped to those
2These placeholders are not to be confused with the placeholder for a tensor.
source placeholders whose alignment score (aij) is the highest at the time of generation.
",4 Sequence-to-Sequence Models,[0],[0]
"The decoder predicts a score for all the tokens in the target vocabulary, which is then normalized by a softmax function, and the token with the highest probability is predicted.",4 Sequence-to-Sequence Models,[0],[0]
"In this section, we present the extension of our work from a single-language setting to multilanguage settings.",5 Multilingual Generation,[0],[0]
"There have been various studies in the past that target neural machine translation from multiple source languages into a single target language (Zoph and Knight, Jan, 2016), from single source to multiple target languages (Dong et al., 2015) and multiple source to multiple target languages (Johnson et al., June, 2016).",5 Multilingual Generation,[0],[0]
One of the main motivation of joint learning in above works is to improve the translation quality on a low-resource language pair via transfer learning between related languages.,5 Multilingual Generation,[0],[0]
"For example, Johnson et al. (June, 2016) had no parallel data available to train a Japanese-to-Korean MT system, but training Japanese-English and English-Korean language pairs allowed their model to learn translations from Japanese to Korean without seeing any parallel data.",5 Multilingual Generation,[0],[0]
"In our case, the amount of training data for French is small compared to English and German (cf. Section 6.1).",5 Multilingual Generation,[0],[0]
"We propose joint learning of English, French and German, because we expect that transfer learning will improve generation for French.",5 Multilingual Generation,[0],[0]
"We investigate the joint training of pairs of these languages as well the combination of all three.
",5 Multilingual Generation,[0],[0]
"On top of the multi-lingual approach, we follow the work of Currey et al. (2017) who proposed copying monolingual data on both sides (source and target) as a way to improve the performance of NMT systems on low-resource languages.",5 Multilingual Generation,[0],[0]
"In machine translation, there are often named entities and nouns which need to be translated verbatim, and this copying mechanism helps in identifying them.",5 Multilingual Generation,[0],[0]
"Since our use case is monolingual generation, we expect a large gain from this copying approach because we have many brands and other slot values which need to occur verbatim in the generated titles.",5 Multilingual Generation,[0],[0]
"We have access to a large number of humancreated titles (curated titles) for English and German, and a small number of curated titles for French.",6.1 Data,[0],[0]
"When generating these titles, human annotators were specifically asked to realize all slots in the title.
",6.1 Data,[0],[0]
"We make use of a large monolingual out-ofdomain corpus for French, as it is a low-resource language.",6.1 Data,[0],[0]
"We collect item description data from an e-Commerce website and clean the data in the following way: 1) we train a language model (LM) on the small amount of French curated titles, 2) we tokenize the out-of-domain data, 3) we remove all sentences with length less than 5, 4) we compute the LM perplexity for each sentence in the out-ofdomain data, 5) we sort the sentences in increasing order of their perplexities and 6) select the top 500K sentences.",6.1 Data,[0],[0]
Statistics of the data sets are reported in Table 2.,6.1 Data,[0],[0]
"We compared the NLG systems in the single-, dual-, and multi-lingual settings.
",6.2 Systems,[0],[0]
Single-language setting:,6.2 Systems,[0],[0]
"This is the baseline NLG system, a straightforward sequence-tosequence model with attention as described in Luong et al. (August, 2015), trained separately for each language.",6.2 Systems,[0],[0]
"The vocabulary is computed on the concatenation of both source and target data, and the same vocabulary is used for both source and target languages in the experiments.
",6.2 Systems,[0],[0]
"We use Adam (Kingma and Ba, December, 2014) as a gradient descent approach for faster convergence.",6.2 Systems,[0],[0]
Initial learning rate is set to 0.0002 with a decay rate of 0.9.,6.2 Systems,[0],[0]
"The dimension of word embeddings is set to 620 and hidden layer size to
1000.",6.2 Systems,[0],[0]
"Dropout is set to 0.2 and is activated for all layers except the initial word embedding layer, because we want to realize all aspects, we cannot afford to zero out any token in the source.",6.2 Systems,[0],[0]
"We continue training of the model and evaluate on the development set after each epoch, stopping the training if the BLEU score on the development set does not increase for 10 iterations.
",6.2 Systems,[0],[0]
Baselines:,6.2 Systems,[0],[0]
"We compare our neural system with a fair baseline system (Baseline 1), which is a statistical MT system trained on the same parallel data as the neural system: the source side is the linearized pseudo-language sequence, and the target side is the curated title in natural language.",6.2 Systems,[0],[0]
"Baseline 2 is the either the hybrid system (for French and English) or the APE system (for German), both described in Section 1.",6.2 Systems,[0],[0]
"These are unfair baselines, because (1) the hybrid system employs a large number of hand-made rules in combination with statistical models (Mathur, Ueffing, and Leusch, 2017), while the neural systems are unaware of the knowledge encoded in those rules, (2) the APE system and neural systems learn from same amount of parallel data, but the APE system aims at correcting rule-based generated titles, whereas the neural system aims at generating titles directly from a linearized form, which is a harder task.",6.2 Systems,[0],[0]
"We compare our systems with the best performing systems of (Mathur et al., 2017), i.e. hybrid system for English and French, and APE system for German.
",6.2 Systems,[0],[0]
Multi-lingual setting: We train the neural model jointly on multiple languages to leverage transfer learning from a high-resource language to a low-resource one.,6.2 Systems,[0],[0]
"In our multi-lingual setting, we experiment with three different combinations to improve models for French: 1) English+French",6.2 Systems,[0],[0]
(en-fr) 2) German+French (de-fr) 3) English+French+German (en-fr-de).,6.2 Systems,[0],[0]
"English and French being close languages, we expect the enfr system to benefit more from transfer learning across languages than any other combination.",6.2 Systems,[0],[0]
"Although, as evident in Zoph and Knight (Jan, 2016), joint learning between the distant languages works better as they tend to disambiguate each other better than two languages which are close.",6.2 Systems,[0],[0]
"For comparison, we also run a combination of two highresource languages, i.e. English and German (ende), to see if transfer learning works for them.",6.2 Systems,[0],[0]
"It is important to note that in all multi-lingual sys-
tems the low-resourced language is over-sampled to balance the data.
",6.2 Systems,[0],[0]
"We used the same design parameters on the neural network in both the single-language and the multi-lingual setting.
",6.2 Systems,[0],[0]
"Normalized setting: On top of the systems above, we also experimented with the normalization scheme presented in Section 3.1.",6.2 Systems,[0],[0]
Normalization is useful in two ways: 1) It reduces the vocabulary size and 2) it avoids spurious generation of important aspect values (slot values).,6.2 Systems,[0],[0]
The second point is especially important in our case because this avoids highly sensitive issues such as brand violations.,6.2 Systems,[0],[0]
"MT researches have observed that NMT systems often generate very fluent output, but have a tendency to generate inadequate output, i.e. sentences or words which are not related to the given input (Koehn and Knowles, June, 2017).",6.2 Systems,[0],[0]
We alleviate this problem through the normalization described above.,6.2 Systems,[0],[0]
"After normalization, we see vocabulary reductions of 15% for French, 20% for German and as high as 35% for English.
",6.2 Systems,[0],[0]
"As described in Section 5, we also use byte pair encoding, with a BPE code size of 30,000 for all systems (with BPE).",6.2 Systems,[0],[0]
We train the codes on the concatenation of source and target since (in this monolingual generation task),6.2 Systems,[0],[0]
the vocabularies are very similar; the vocabulary size is around 30k for systems using BPE for both source and target.,6.2 Systems,[0],[0]
"We evaluate our systems with three different automatic metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and character FScore (Popović, 2016).",7 Results,[0],[0]
"Note that BLEU and character F-score are quality metrics, i.e. higher scores mean higher quality, while TER is an error metric, where lower scores indicate higher quality.",7 Results,[0],[0]
"All metrics compare the automatically generated title against a human-curated title and determine sequence matches on the word or character level.
",7 Results,[0],[0]
Table 3 summarizes results from all systems on the English test set.,7 Results,[0],[0]
"All neural systems are better than the fair Baseline 1 system.
",7 Results,[0],[0]
Normalization with tags (i.e. using placeholders) has a negative effect on English title quality both in the single-language setting en (67.1 vs. 68.4 BLEU) and in the dual-language setting en-fr (67.1 vs. 70.7 BLEU).,7 Results,[0],[0]
"However, title quality increases when using BPE instead (71.9 vs. 70.7 BLEU).",7 Results,[0],[0]
"On en-de, we observe gains
both from normalization with tags and from BPE.",7 Results,[0],[0]
"Again, BPE normalization works best.",7 Results,[0],[0]
"Both duallanguage systems with BPE achieve better performance that the best monolingual English system (71.9 and 72.7 vs. 68.4 BLEU).
",7 Results,[0],[0]
"The system en-frbig contains monolingual French data added via the copying mechanism, which improves title quality.",7 Results,[0],[0]
"It outperforms any other neural system and is on par with Baseline 2 (unfair baseline), even outperforming it in terms of TER.",7 Results,[0],[0]
"The multi-lingual system en-fr-de is very close to en-frbig according to all three metrics.
",7 Results,[0],[0]
Table 4 collects the results for all systems on the German test set.,7 Results,[0],[0]
"For the single-language setting, we see a loss of 7 BLEU points when normalizing the input sequence, which is caused by incorrect morphology in the titles.",7 Results,[0],[0]
"When using placeholders, the system generates entities in the title in the exact form in which they occur in the input.",7 Results,[0],[0]
"In German, however, the words often need to be inflected.",7 Results,[0],[0]
"For example, the slot “ brand Markenlos” should be realized as “Markenlose” (Unbranded) in the title, but the placeholder generates the input form “Markenlos” (without suffix ‘e’).",7 Results,[0],[0]
"This causes a huge deterioration in the word-level met-
rics BLEU and TER, but not as drastic in chrF1, which evaluates on the character level.
",7 Results,[0],[0]
"For German, there is a positive effect of transfer learning for both dual-language systems ende and de-frbig with BPE (79.6 and 80.0 vs. 78.2 BLEU).",7 Results,[0],[0]
"However, the combination of languages hurts when we combine languages at token level, i.e. without normalization or with tags.",7 Results,[0],[0]
"The performance of systems with BPE is even on par with or better than the strong baseline of 79.4 BLEU, both for combinations of two and of three languages.
",7 Results,[0],[0]
Table 5 summarizes the results from all systems on the French test set.,7 Results,[0],[0]
The single-language fr NMT system achieves a low BLEU score compared to the SMT system Baseline 1 (23.0 vs. 44.6).,7 Results,[0],[0]
"This is due to the very small amount of parallel data, which is a setting where SMT typically outperforms NMT as evidenced in Zoph et al. (April, 2016).",7 Results,[0],[0]
"Normalization has a big positive impact on all French systems (e.g. 27.4
vs. 23.0 BLEU for fr).",7 Results,[0],[0]
"The de-fr systems show a much larger gain from transfer learning than the en-fr systems, which validates Zoph and Knight (Jan, 2016)’s results, who show that transfer learning is better for distant languages than for similar languages.
",7 Results,[0],[0]
"For all three languages, copying monolingual data improves the NMT system by a large margin.
",7 Results,[0],[0]
The multi-lingual en-fr-de (BPE) system (with copied monolingual data) is the best system for all three languages.,7 Results,[0],[0]
"It has the additional advantage of being one single model that can cater to all three languages at once.
",7 Results,[0],[0]
Table 6 presents the example titles comparing different phenomena.,7 Results,[0],[0]
"The first block shows the usefulness of placeholders in system fr small ,tags (i.e. fr small , normalized with tags) where in comparison to fr small the brand is generated verbatim.",7 Results,[0],[0]
The second block shows the effectiveness of copying the data where “Cylindres” is generated correctly in the frbig (with BPE) system in comparison to fr small .,7 Results,[0],[0]
The last block shows that reordering and adequacy in generation can be improved with the helpful signals from high-resourced English and German languages.,7 Results,[0],[0]
"We developed neural language generation systems for an e-Commerce use case for three languages with very different amounts of training data and came to the following conclusions:
(1) The lack of resources in French leads to generation of low quality titles, but this can be drastically improved upon with transfer learning between French and English and/or German.
(2)",8 Conclusion,[0],[0]
"In case of low-resource languages, copying monolingual data (even if out-of-domain) improves the performance of the system.
",8 Conclusion,[0],[0]
"(3) Normalization with placeholders usually helps for languages with relatively easy morphology.
",8 Conclusion,[0],[0]
"(4) It is important to over-sample the lowresourced languages in order to balance the high& low-resourced data, thereby, creating a stable NLG system.
",8 Conclusion,[0],[0]
"(5) For French, a low-resource language in our use case, the hybrid system which combines manual rules and SMT technology is still far better than the best neural system.
",8 Conclusion,[0],[0]
"(6) The multi-lingual model has the best tradeoff, as it achieves the best results among the neural
systems in all three languages and it is one single model which can be deployed easily on a single GPU machine.",8 Conclusion,[0],[0]
Thanks to our colleague Pavel Petrushkov for all the help with the neural MT toolkit.,Acknowledgments,[0],[0]
"To provide better access of the inventory to buyers and better search engine optimization, e-Commerce websites are automatically generating millions of easily searchable browse pages.",abstractText,[0],[0]
A browse page groups multiple items with shared characteristics together.,abstractText,[0],[0]
It consists of a set of slot name/value pairs within a given category that are linked among each other and can be organized in a hierarchy.,abstractText,[0],[0]
This structure allows users to navigate laterally between different browse pages (i.e. browse between related items) or to dive deeper and refine their search.,abstractText,[0],[0]
These browse pages require a title describing the content of the page.,abstractText,[0],[0]
"Since the number of browse pages is huge, manual creation of these titles is infeasible.",abstractText,[0],[0]
Previous statistical and neural generation approaches depend heavily on the availability of large amounts of data in a language.,abstractText,[0],[0]
"In this research, we apply sequence-tosequence models to generate titles for high& low-resourced languages by leveraging transfer learning.",abstractText,[0],[0]
"We train these models on multilingual data, thereby creating one joint model which can generate titles in various different languages.",abstractText,[0],[0]
"Performance of the title generation system is evaluated on three different languages; English, German, and French, with a particular focus on low-resourced French language.",abstractText,[0],[0]
Multi-lingual neural title generation for e-Commerce browse pages,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 947–957, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric.,1 Introduction,[0],[0]
"The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice.",1 Introduction,[0],[0]
"Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric.",1 Introduction,[0],[0]
"Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others.
",1 Introduction,[0],[0]
However these approaches optimize towards the best score as reported by a single evaluation metric.,1 Introduction,[0],[0]
"MT system developers typically use BLEU and
ignore all the other metrics.",1 Introduction,[0],[0]
"This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic similarity through textual entailment (Padó et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010).",1 Introduction,[0],[0]
"While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011).
",1 Introduction,[0],[0]
"The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality.",1 Introduction,[0],[0]
"In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics.
",1 Introduction,[0],[0]
"Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012).",1 Introduction,[0],[0]
PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics.,1 Introduction,[0],[0]
"In (Duh et al., 2012) the choice of which option to use rests with the MT system developer and in that sense their approach is an a posteriori method to specify the preference (Marler and Arora, 2004).
",1 Introduction,[0],[0]
"In contrast to this, our tuning framework provides a principled way of using the Pareto optimal options using ensemble decoding (Razmara et al., 2012).",1 Introduction,[0],[0]
"We also introduce a novel method of ensemble tuning for jointly tuning multiple MT evaluation metrics and further combine this with the PMO ap-
947
proach (Duh et al., 2012).",1 Introduction,[0],[0]
We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning.,1 Introduction,[0],[0]
"Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now.
",1 Introduction,[0],[0]
Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task.,1 Introduction,[0],[0]
"HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output.",1 Introduction,[0],[0]
"In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations.
",2 Related Work,[0],[0]
"Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012).",2 Related Work,[0],[0]
"However, few works have investigated the multi-metric tuning problem in depth.",2 Related Work,[0],[0]
"Linear combination of BLEU and TER is reported in (Zaidan, 2009; Dyer et al., 2009; Servan and Schwenk, 2011); an alternative is to optimize on BLEU with MERT while enforcing that TER does not degrade per iteration (He and Way, 2009).",2 Related Work,[0],[0]
"Studies on metric tunability (Liu et al., 2011; CallisonBurch et al., 2011; Chen et al., 2012) have found that the metric used for evaluation may not be the best metric used for tuning.",2 Related Work,[0],[0]
"For instance, (Mauser et al., 2008; Cer et al., 2010) report that tuning on linear combinations of BLEU-TER is more robust than a single metric like WER.
",2 Related Work,[0],[0]
"The approach in (Devlin and Matsoukas, 2012) modifies the optimization function to include traits such as output length so that the hypotheses produced by the decoder have maximal score according to one metric (BLEU) but are subject to an output length constraint, e.g. that the output is 5% shorter.",2 Related Work,[0],[0]
This is done by rescoring an N-best list (forest) for the metric combined with each trait condition and then the different trait hypothesis are combined using a system combination step.,2 Related Work,[0],[0]
"The traits are in-
dependent of the reference (while tuning).",2 Related Work,[0],[0]
"In contrast, our method is able to combine multiple metrics (each of which compares to the reference) during the tuning step and we do not depend on N-best list (or forest) rescoring or system combination.
",2 Related Work,[0],[0]
Duh et.,2 Related Work,[0],[0]
"al. (2012) proposed a Pareto-based approach to SMT multi-metric tuning, where the linear combination weights do not need to be known in advance.",2 Related Work,[0],[0]
This is advantageous because the optimal weighting may not be known in advance.,2 Related Work,[0],[0]
"However, the notion of Pareto optimality implies that multiple ”best” solutions may exist, so the MT system developer may be forced to make a choice after tuning.
",2 Related Work,[0],[0]
These approaches require the MT system developer to make a choice either before tuning (e.g. in terms of linear combination weights) or afterwards (e.g. the Pareto approach).,2 Related Work,[0],[0]
Our method here is different in that we do not require any choice.,2 Related Work,[0],[0]
"We use ensemble decoding (Razmara et al., 2012) (see sec 3) to combine the different solutions resulting from the multi-metric optimization, providing an elegant solution for deployment.",2 Related Work,[0],[0]
"We extend this idea further and introduce ensemble tuning, where the metrics have separate set of weights.",2 Related Work,[0],[0]
The tuning process alternates between ensemble decoding and the update step where the weights for each metric are optimized separately followed by joint update of metric (meta) weights.,2 Related Work,[0],[0]
"We now briefly review ensemble decoding (Razmara et al., 2012) which is used as a component in the algorithms we present.",3 Ensemble Decoding,[0],[0]
"The prevalent model of statistical MT is a log-linear framework using a vector of feature functions φ:
p(e|f) ∝",3 Ensemble Decoding,[0],[0]
"exp ( w · φ ) (1)
",3 Ensemble Decoding,[0],[0]
The idea of ensemble decoding is to combine several models dynamically at decode time.,3 Ensemble Decoding,[0],[0]
"Given multiple models, the scores are combined for each partial hypothesis across the different models during decoding using a user-defined mixture operation ⊗.
p(e|f) ∝",3 Ensemble Decoding,[0],[0]
exp ( w1 · φ1 ⊗ w2 · φ2 ⊗ . . . ),3 Ensemble Decoding,[0],[0]
"(2)
(Razmara et al., 2012) propose several mixture operations, such as log-wsum (simple linear mixture), wsum (log-linear mixture) and max (choose lo-
cally best model) among others.",3 Ensemble Decoding,[0],[0]
The different mixture operations allows the user to encode the beliefs about the relative strengths of the models.,3 Ensemble Decoding,[0],[0]
It has been applied successfully for domain adaptation setting and shown to perform better approaches that pre-compute linear mixtures of different models.,3 Ensemble Decoding,[0],[0]
"In statistical MT, the multi-metric optimization problem can be expressed as:
w∗ = arg max w
g ( [M1(H), . . .",4 Multi-Metric Optimization,[0],[0]
",Mk(H)] )",4 Multi-Metric Optimization,[0],[0]
"(3)
where H = N",4 Multi-Metric Optimization,[0],[0]
"(f ;w)
where N (f ;w) is the decoding function generating a set of candidate hypotheses H based on the model parameters w, for the source sentences f .",4 Multi-Metric Optimization,[0],[0]
For each source sentence fi ∈ f there is a set of candidate hypotheses {hi} ∈ H .,4 Multi-Metric Optimization,[0],[0]
The goal of the optimization is to find the weights that maximize the function g(.),4 Multi-Metric Optimization,[0],[0]
"parameterized by different evaluation metrics M1, . . .",4 Multi-Metric Optimization,[0],[0]
",Mk.
",4 Multi-Metric Optimization,[0],[0]
"For the Pareto-optimal based approach such as PMO (Duh et al., 2012), we can replace g(·) above with gPMO(·) which returns the points in the Pareto frontier.",4 Multi-Metric Optimization,[0],[0]
"Alternately a weighted averaging function gwavg(·) would result in a linear combination of the metrics being considered, where the tuning method would maximize the joint metric.",4 Multi-Metric Optimization,[0],[0]
"This is similar to the (TER-BLEU)/2 optimization (Cer et al., 2010; Servan and Schwenk, 2011).
",4 Multi-Metric Optimization,[0],[0]
We introduce four methods based on the above formulation and each method uses a different type of g(·) function for combining different metrics and we compare experimentally with existing methods.,4 Multi-Metric Optimization,[0],[0]
"PMO (Duh et al., 2012) seeks to maximize the number of points in the Pareto frontier of the metrics considered.",4.1 PMO Ensemble,[0],[0]
The inner routine of the PMO-PRO tuning is described in Algorithm 1.,4.1 PMO Ensemble,[0],[0]
"This routine is contained within an outer loop that iterates for a fixed number iterations of decoding the tuning set and optimizing the weights.
",4.1 PMO Ensemble,[0],[0]
"The tuning process with PMO-PRO is independently repeated with different set of weights for metrics1 yielding a set of equivalent solutions
1For example",4.1 PMO Ensemble,[0],[0]
"Duh et al. (2012) use five different weight
Algorithm 1 PMO-PRO (Inner routine for tuning) 1: Input: Hypotheses H = N",4.1 PMO Ensemble,[0],[0]
"(f ;w); Weights w 2: Initialize T = {} 3: for each f in tuning set f do 4: {h} = H(f) 5: {M({h})} = ComputeMetricScore({h}, ê) 6: {F} = FindParetoFrontier({M({h})}) 7: for each h in {h} do 8: if h ∈ F then add (1, h) to T 9: else add (`, h) to T (see footnote 1) 10: wp ← PRO(T ) (optimize using PRO) 11:",4.1 PMO Ensemble,[0],[0]
"Output: Pareto-optimal weights wp
{ps1 , . . .",4.1 PMO Ensemble,[0],[0]
", psn} which are points on the Pareto frontier.",4.1 PMO Ensemble,[0],[0]
The user then chooses one solution by making a trade-off between the performance gains across different metrics.,4.1 PMO Ensemble,[0],[0]
"However, as noted earlier this a posteriori choice ignores other solutions that are indistinguishable from the chosen one.
",4.1 PMO Ensemble,[0],[0]
"We alleviate this by complementing PMO with ensemble decoding, which we call PMO ensemble, in which each point in the Pareto solution is a distinct component in the ensemble decoder.",4.1 PMO Ensemble,[0],[0]
This idea can also be used in other MMO approaches such as linear combination of metrics (gwavg(.)) mentioned above.,4.1 PMO Ensemble,[0],[0]
"In this view, PMO ensemble is a special case of ensemble combination, where the decoding is performed by an ensemble of optimal solutions.
",4.1 PMO Ensemble,[0],[0]
The ensemble combination model introduces new hyperparameters β that are the weights of the ensemble components (meta weights).,4.1 PMO Ensemble,[0],[0]
These ensemble weights could set to be uniform in a naı̈ve implementation.,4.1 PMO Ensemble,[0],[0]
"Or the user can encode her beliefs or expectations about the individual solutions {ps1 , . . .",4.1 PMO Ensemble,[0],[0]
", psn} to set the ensemble weights (based on the relative importance of the components).",4.1 PMO Ensemble,[0],[0]
"Finally, one could also include a meta-level tuning step to set the weights β.
",4.1 PMO Ensemble,[0],[0]
"The PMO ensemble approach is graphically illustrated in Figure 1; we will also refer to this figure while discussing other methods.2 The orig-
settings for metrics (M1, M2), viz.",4.1 PMO Ensemble,[0],[0]
"(0.0, 1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and (1.0, 0.0).",4.1 PMO Ensemble,[0],[0]
They combine the metric weights qi with the sentence-level metric scores Mi as ` = (∑ k qkMk ) /k,4.1 PMO Ensemble,[0],[0]
where ` is the target value for negative examples (the else line in Alg 1) in the optimization step.,4.1 PMO Ensemble,[0],[0]
"2The illustration is based on two metrics, metric-1 and metric-2, but could be applied to any number of metrics.",4.1 PMO Ensemble,[0],[0]
"Without loss of generality we assume accuracy metrics, i.e. higher
inal PMO-PRO seeks to maximize the points on the Pareto frontier (blue curve in the figure) leading to Pareto-optimal solutions.",4.1 PMO Ensemble,[0],[0]
"On the other hand, the PMO ensemble combines the different Paretooptimal solutions and potentially moving in the direction of dashed (green) arrows to some point that has higher score in either or both dimensions.",4.1 PMO Ensemble,[0],[0]
"Lateen EM has been proposed as a way of jointly optimizing multiple objectives in the context of dependency parsing (Spitkovsky et al., 2011).",4.2 Lateen MMO,[0],[0]
"It uses a secondary hard EM objective to move away, when the primary soft EM objective gets stuck in a local optima.",4.2 Lateen MMO,[0],[0]
"The course correction could be performed under different conditions leading to variations that are based on when and how often to shift from one objective function to another during optimization.
",4.2 Lateen MMO,[0],[0]
The lateen technique can be applied to the multimetric optimization in SMT by treating the different metrics as different objective functions.,4.2 Lateen MMO,[0],[0]
"While the several lateen variants are also applicable for our task, our objective here is to improve performance across the different metrics (being optimized).",4.2 Lateen MMO,[0],[0]
"Thus, we restrict ourselves to the style where the search alternates between the metrics (in round-robin fashion) at each iteration.",4.2 Lateen MMO,[0],[0]
"Since the notion of convergence is unclear in lateen setting, we stop after a fixed number of iterations optimizing the tuning set.",4.2 Lateen MMO,[0],[0]
"In terms of Figure 1, lateen MMO corresponds to alternately maximizing the metrics along two dimensions as depicted by the solid arrows.
",4.2 Lateen MMO,[0],[0]
"By the very nature of lateen-alternation, the
metric score is better.
weights obtained at each iteration are likely to be best for the metric that was optimized in that iteration.",4.2 Lateen MMO,[0],[0]
"Thus, one could use weights from the last k iterations (for lateen-tuning with as many metrics) and then decode the test set with an ensemble of these weights as in PMO ensemble.",4.2 Lateen MMO,[0],[0]
However in practice we find the weights to converge and we simply use the weights from the final iteration to decode the test set in our lateen experiments.,4.2 Lateen MMO,[0],[0]
At each iteration lateen MMO excludes all but one metric for optimization.,4.3 Union of Metrics,[0],[0]
An alternative would be to consider all the metrics at each iteration so that the optimizer could try to optimize them jointly.,4.3 Union of Metrics,[0],[0]
"This has been the general motivation for considering the linear combination of metrics (Cer et al., 2010; Servan and Schwenk, 2011) resulting in a joint metric, which is then optimized.
",4.3 Union of Metrics,[0],[0]
"However due to the scaling differences between the scores of different metrics, the linear combination might completely suppress the metric having scores in the lower-range.",4.3 Union of Metrics,[0],[0]
"As an example, the RIBES scores that are typically in the high 0.7-0.8 range, dominate the BLEU scores that is typically around 0.3.",4.3 Union of Metrics,[0],[0]
"While the weighted linear combination tries to address this imbalance, they introduce additional parameters that are manually fixed and not separately tuned.
",4.3 Union of Metrics,[0],[0]
We avoid this linear combination pitfall by taking the union of the metrics under which we consider the union of training examples from all metrics and optimize them jointly.,4.3 Union of Metrics,[0],[0]
"Mathematically,
w∗ = arg max w g(M1(H)) ∪ . . .",4.3 Union of Metrics,[0],[0]
∪ g(Mk(H)),4.3 Union of Metrics,[0],[0]
"(4)
Most of the optimization approaches involve two phases: i) select positive and negative examples and ii) optimize parameters to favour positive examples while penalizing negative ones.",4.3 Union of Metrics,[0],[0]
"In the union approach, we independently generate positive and negative sets of examples for all the metrics and take their union.",4.3 Union of Metrics,[0],[0]
"The optimizer now seeks to move towards positive examples from all metrics, while penalizing others.
",4.3 Union of Metrics,[0],[0]
"This is similar to the PMO-PRO approach except that here the optimizer tries to simultaneously maximize the number of high scoring points across all
metrics.",4.3 Union of Metrics,[0],[0]
"Thus, instead of the entire Pareto frontier curve in Figure 1, the union approach optimizes the two dimensions simultaneously in each iteration.",4.3 Union of Metrics,[0],[0]
"These methods, even though novel, under utilize the power of ensembles as they combine the solution only at the end of the tuning process.",5 Ensemble Tuning,[0],[0]
We would prefer to tightly integrate the idea of ensembles into the tuning.,5 Ensemble Tuning,[0],[0]
We thus extend the ensemble decoding to ensemble tuning.,5 Ensemble Tuning,[0],[0]
"The feature weights are replicated separately for each evaluation metric, which are treated as components in the ensemble decoding and tuned independently in the optimization step.",5 Ensemble Tuning,[0],[0]
Initially the ensemble decoder decodes a devset using a weighted ensemble to produce a single N-best list.,5 Ensemble Tuning,[0],[0]
"For the optimization, we employ a two-step approach of optimizing the feature weights (of each ensemble component) followed by a step for tuning the meta (component) weights.",5 Ensemble Tuning,[0],[0]
"The optimized weights are then used for decoding the devset in the next iteration and the process is repeated for a fixed number of iterations.
",5 Ensemble Tuning,[0],[0]
"Modifying the MMO representation in Equation 3, we formulate ensemble tuning as:
Hens = Nens ( f ; {wM};⊗; λ ) (5)
w∗ = {
arg max wMi
Hens | 1≤i≤k }
(6)
λ = arg max λ
g ({Mi(Hens)|1≤i≤k} ;w∗) (7)
Here the ensemble decoder function Nens(.) is parameterized by an ensemble of weights wM1 , . . .",5 Ensemble Tuning,[0],[0]
", wMk (denoted as {wM} in Eq 5) for each metric and a mixture operation (⊗).",5 Ensemble Tuning,[0],[0]
"λ represents the weights of the ensemble components.
",5 Ensemble Tuning,[0],[0]
Pseudo-code for ensemble tuning is shown in Algorithm 2.,5 Ensemble Tuning,[0],[0]
"In the beginning of each iteration (line 2), the tuning process ensemble decodes (line 4) the tuning set using the weights obtained from the previous iteration.",5 Ensemble Tuning,[0],[0]
"Equation 5 gives the detailed expression for the ensemble decoding, whereHens denotes the N-best list generated by the ensemble decoder.
",5 Ensemble Tuning,[0],[0]
The method now uses a dual tuning strategy involving two phases to optimize the weights.,5 Ensemble Tuning,[0],[0]
"In the first step it optimizes each of the k metrics independently (lines 6-7) along its respective dimension in
Algorithm 2 Ensemble Tuning Algorithm 1: Input: Tuning set f ,
Metrics M1, . . .",5 Ensemble Tuning,[0],[0]
",Mk (ensemble components)",5 Ensemble Tuning,[0],[0]
"Initial weights {wM} ← wM1 , . . .",5 Ensemble Tuning,[0],[0]
"wMk and Component (meta) weights λ
2: for j = 1, . . .",5 Ensemble Tuning,[0],[0]
do 3: {w(j)M },5 Ensemble Tuning,[0],[0]
"← {wM} 4: Ensemble decode the tuning set Hens = Nens(f ; {w(j)M };⊗; λ) 5: {wM} = {} 6: for each metric Mi ∈ {M} do 7: w∗Mi ← PRO(Hens, wMi) (use PRO) 8: Add w∗Mi to {wM} 9: λ← PMO-PRO(Hens, {wM}) (Alg 1)
10: Output: Optimal weights {wM} and λ
the multi-metric space (as shown by the solid arrows along the two axes in Figure 1).",5 Ensemble Tuning,[0],[0]
"This yields a new set of weights w∗ for the features in each metric.
",5 Ensemble Tuning,[0],[0]
The second tuning step (line 9) then optimizes the meta weights (λ) so as to maximize the multimetric objective along the joint k-dimensional space as shown in Equation 7.,5 Ensemble Tuning,[0],[0]
This is illustrated by the dashed arrows in the Figure 1.,5 Ensemble Tuning,[0],[0]
"While g(.) could be any function that combines multiple metrics, we use the PMO-PRO algorithm (Alg. 1) for this step.
",5 Ensemble Tuning,[0],[0]
The main difference between ensemble tuning and PMO ensemble is that the former is an ensemble model over metrics and the latter is an ensemble model over Pareto solutions.,5 Ensemble Tuning,[0],[0]
"Additionally, PMO ensemble uses the notion of ensembles only for the final decoding after tuning has completed.",5 Ensemble Tuning,[0],[0]
All the proposed methods fit naturally within the usual SMT tuning framework.,5.1 Implementation Notes,[0],[0]
"However, some changes are required in the decoder to support ensemble decoding and in the tuning scripts for optimizing with multiple metrics.",5.1 Implementation Notes,[0],[0]
"For ensemble decoding, the decoder should be able to use multiple weight vectors and dynamically combine them according to some desired mixture operation.",5.1 Implementation Notes,[0],[0]
"Note that, unlike Razmara et al. (2012), our approach uses just one model but has different weight vectors for each metric and the required decoder modifications are simpler than full ensemble decoding.
",5.1 Implementation Notes,[0],[0]
"While any of the mixture operations proposed by Razmara et al. (2012) could be used, in this pa-
per we use log-wsum – the linear combination of the ensemble components and log-wmax – the combination that prefers the locally best component.",5.1 Implementation Notes,[0],[0]
These are simpler to implement and also performed competitively in their domain adaptation experiments.,5.1 Implementation Notes,[0],[0]
"Unless explicitly noted otherwise, the results presented in Section 6 are based on linear mixture operation log-wsum, which empirically performed better than the log-wmax for ensemble tuning.",5.1 Implementation Notes,[0],[0]
We evaluate the different methods on ArabicEnglish translation in single as well as multiple references scenario.,6 Experiments,[0],[0]
Corpus statistics are shown in Table 1.,6 Experiments,[0],[0]
"For all the experiments in this paper, we use Kriya, our in-house Hierarchical phrasebased (Chiang, 2007) (Hiero) system, and integrated the required changes for ensemble decoding.",6 Experiments,[0],[0]
"Kriya performs comparably to the state of the art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets (Sankaran et al., 2012).
",6 Experiments,[0],[0]
"We use PRO (Hopkins and May, 2011) for optimizing the feature weights and PMO-PRO (Duh et al., 2012) for optimizing meta weights, wherever applicable.",6 Experiments,[0],[0]
"In both cases, we use SVMRank (Joachims, 2006) as the optimizer.
",6 Experiments,[0],[0]
We used the default parameter settings for different MT tuning metrics.,6 Experiments,[0],[0]
"For METEOR, we tried both METEOR-tune and METEOR-hter settings and found the latter to perform better in BLEU and TER scores, even though the former was marginally better in METEOR3 and RIBES scores.",6 Experiments,[0],[0]
We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-hter setting for both optimization and evaluation of all our experiments.,6 Experiments,[0],[0]
"Unlike conventional tuning methods, PMO (Duh et al., 2012) was originally evaluated on the tuning set to avoid potential mismatch with the test set.",6.1 Evaluation on Tuning Set,[0],[0]
"In order to ensure robustness of evaluation, they redecode the devset using the optimal weights from the last tuning iteration and report the scores on 1-
3This behaviour was also noted by Denkowski and Lavie (2011) in their analysis of Urdu-English system for tunable metrics task in WMT11.
best candidates.
",6.1 Evaluation on Tuning Set,[0],[0]
We follow the same strategy and compare our PMO-ensemble approach with PMO-PRO (denoted P) and a linear combination4 (denoted L) baseline.,6.1 Evaluation on Tuning Set,[0],[0]
"Similar to Duh et al. (2012), we use five different BLEU:",6.1 Evaluation on Tuning Set,[0],[0]
"RIBES weight settings, viz.",6.1 Evaluation on Tuning Set,[0],[0]
"(0.0, 1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and (1.0, 0.0), marked L1 through L5 or P1 through P5.",6.1 Evaluation on Tuning Set,[0],[0]
"The Pareto frontier is then computed from 80 points (5 runs and 15 iterations per run) on the devset.
",6.1 Evaluation on Tuning Set,[0],[0]
Figure 2(a) shows the Pareto frontier of L and P baselines using BLEU and RIBES as two metrics.,6.1 Evaluation on Tuning Set,[0],[0]
"The frontier of the P dominates that of L for most part showing that the PMO approach benefits from picking Pareto points during the optimization.
",6.1 Evaluation on Tuning Set,[0],[0]
We use the PMO-ensemble approach to combine the optimized weights from the 5 tuning runs and re-decode the devset employing ensemble decoding.,6.1 Evaluation on Tuning Set,[0],[0]
This yields the points LEns,6.1 Evaluation on Tuning Set,[0],[0]
"and PEns in the plot, which obtain better scores than most of the individual runs of L and P. This ensemble approach of combining the final weights also generalizes to the unseen test set as we show later.
",6.1 Evaluation on Tuning Set,[0],[0]
Figure 2(b) plots the change in BLEU during tuning in the multiple references and the single reference scenarios.,6.1 Evaluation on Tuning Set,[0],[0]
"We show for each baseline method L and P, plots for two different weight settings that obtain high BLEU and RIBES scores.",6.1 Evaluation on Tuning Set,[0],[0]
"In both datasets, our ensemble tuning approach dominates the curves of the (L and P) baselines.",6.1 Evaluation on Tuning Set,[0],[0]
"In summary, these results confirm that the ensemble approach achieves results that are competitive with previous MMO methods on the devset Pareto curve.",6.1 Evaluation on Tuning Set,[0],[0]
We now provide a more comprehensive evaluation on the test set.,6.1 Evaluation on Tuning Set,[0],[0]
"This section contains multi-metric optimization results on the unseen test sets, one test set has multiple references and the other has a single-reference.
",6.2 Evaluation on Test Set,[0],[0]
"4Linear combination is a generalized version of the combined (TER-BLEU)/2 metric and its variants.
",6.2 Evaluation on Test Set,[0],[0]
"We plot BLEU scores against other metrics (RIBES, METEOR and TER) and this allows us to compare the performance of each metric relative to the defacto standard BLEU metric.
",6.2 Evaluation on Test Set,[0],[0]
"Baseline points are identified by single letters B for BLEU, T for TER, etc. and the baseline (singlemetric optimized) score for each metric is indicated by a dashed line on the corresponding axis.",6.2 Evaluation on Test Set,[0],[0]
"MMO points use a series of single letters referring to the metrics used, e.g. BT for BLEU-TER.",6.2 Evaluation on Test Set,[0],[0]
The union of metrics method is identified with the suffix ’J’ and lateen method with suffix ’L’ (thus BT-L refers to the lateen tuning with BLEU-TER).,6.2 Evaluation on Test Set,[0],[0]
"MMO points without any suffix use the ensemble tuning approach.
",6.2 Evaluation on Test Set,[0],[0]
Figures 3 and 4(a) plot the scores for the MTA test set with 4-references.,6.2 Evaluation on Test Set,[0],[0]
We see noticeable and some statistically significant improvements in BLEU and RIBES (see Table 2 for BLEU improvements).,6.2 Evaluation on Test Set,[0],[0]
"All our MMO approaches, except for the union method, show gains on both BLEU and RIBES axes.",6.2 Evaluation on Test Set,[0],[0]
Figures 3(b) and 4(a) show that none of the proposed methods managed to improve the baseline scores for METEOR and TER.,6.2 Evaluation on Test Set,[0],[0]
"However, several of our ensemble tuning combinations work well for both METEOR (BR, BMRTB3, etc.) and TER (BMRT and BRT) in that they improved or were close to the baseline scores in either dimension.",6.2 Evaluation on Test Set,[0],[0]
"We again see in these figures that the MMO approaches can improve the BLEU-only tuning by 0.3 BLEU points, without much drop in other metrics.",6.2 Evaluation on Test Set,[0],[0]
"This is in tune with the finding that BLEU could be tuned easily (CallisonBurch et al., 2011) and also explains why it remains
a popular choice for optimizing SMT systems.",6.2 Evaluation on Test Set,[0],[0]
Among the different MMO methods the ensemble tuning performs better than lateen or union approaches.,6.2 Evaluation on Test Set,[0],[0]
"In terms of the number of metrics being optimized jointly, we see substantial gains when using a small number (typically 2 or 3) of metrics.",6.2 Evaluation on Test Set,[0],[0]
"Results seem to suffer beyond this number; probably because there might not be a space that contain solution(s) optimal for all the metrics that are jointly optimized.
",6.2 Evaluation on Test Set,[0],[0]
"We hypothesize that each metric correlates well
(in a looser sense) with few others, but not all.",6.2 Evaluation on Test Set,[0],[0]
"For example, union optimizations BR-J and BMT-J perform close to or better than RIBES and TER baselines, but get very poor score in METEOR.",6.2 Evaluation on Test Set,[0],[0]
"On the other hand BM-J is close to the METEOR baseline, while doing poorly on the RIBES and TER.",6.2 Evaluation on Test Set,[0],[0]
"This behaviour is also evident from the single-metric baselines, where R and T-only settings are clearly distinguished from the M-only system.",6.2 Evaluation on Test Set,[0],[0]
"It is not clear if such distinct classes of metrics could be bridged by some optimal solution and the metric dichotomy requires further study as this is key to practical multimetric tuning in SMT.
",6.2 Evaluation on Test Set,[0],[0]
The lateen and union approaches appear to be very sensitive to the number of metrics and they generally perform well for two metrics case and show degradation for more metrics.,6.2 Evaluation on Test Set,[0],[0]
"Unlike other
approaches, the union approach failed to improve over the baseline BLEU and this could be attributed to the conflict of interest among the metrics, while choosing example points for the optimization step.",6.2 Evaluation on Test Set,[0],[0]
The positive example preferred by a particular metric could be a negative example for the other metric.,6.2 Evaluation on Test Set,[0],[0]
This would only confuse the optimizer resulting in poor solutions.,6.2 Evaluation on Test Set,[0],[0]
"Our future line of work would be to study the effect of avoiding such of conflicting examples in the union approach.
",6.2 Evaluation on Test Set,[0],[0]
"For the single-reference (ISI) dataset, we only plot the BLEU-TER case in Figure 4(b) due to lack of space.",6.2 Evaluation on Test Set,[0],[0]
The results are similar to the multiple references set indicating that MMO approaches are equally effective for single references5.,6.2 Evaluation on Test Set,[0],[0]
"Table 2
5One could argue that MMO methods require multiple references since each metric might be picking out a different ref-
shows the BLEU scores for our ensemble tuning method (for various combinations) and we again see improvements over the baseline BLEU-only tuning.",6.2 Evaluation on Test Set,[0],[0]
So far we have shown that multi-metric optimization can improve over single-metric tuning on a single metric like BLEU and we have shown that our methods find a tuned model that performs well with respect to multiple metrics.,6.3 Human Evaluation,[0],[0]
Is the output that scores higher on multiple metrics actually a better translation?,6.3 Human Evaluation,[0],[0]
"To verify this, we conducted a post-editing human evaluation experiment.",6.3 Human Evaluation,[0],[0]
"We compared our ensemble tuning approach involving BLEU, METEOR and RIBES (B-M-R) with systems optimized for BLEU (B-only) and METEOR (M-only).
",6.3 Human Evaluation,[0],[0]
We selected 100 random sentences (that are at least 15 words long) from the Arabic-English MTA (4 references) test set and translated them using the three systems (two single metric systems and BMR ensemble tuning).,6.3 Human Evaluation,[0],[0]
We shuffled the resulting translations and split them into 3 sets such that each set has equal number of the translations from three systems.,6.3 Human Evaluation,[0],[0]
"The translations were edited by three human annotators in a post-editing setup, where the goal was to edit the translations to make them as close to the references as possible, using the Post-Editing Tool: PET (Aziz et al., 2012).",6.3 Human Evaluation,[0],[0]
The annotators were not Arabic-literate and relied only on the reference translations during post-editing.,6.3 Human Evaluation,[0],[0]
"The identifiers that link each translation to the system that generated it are removed to avoid annotator bias.
",6.3 Human Evaluation,[0],[0]
"In the end we collated post-edited translations for each system and then computed the system-level
erence sentence.",6.3 Human Evaluation,[0],[0]
"Our experiment shows that even with a single reference MMO methods can work.
",6.3 Human Evaluation,[0],[0]
"human-targeted (HBLEU, HMETEOR, HTER) scores, by using respective post-edited translations as the reference.",6.3 Human Evaluation,[0],[0]
"First comparing the HTER (Snover et al., 2006) scores shown in Table 3, we see that the single-metric system optimized for METEOR performs slightly worse than the one optimized for BLEU, despite using METEOR-hter version (Denkowski and Lavie, 2011).",6.3 Human Evaluation,[0],[0]
"Ensemble tuning-based system optimized for three metrics (BM-R) improves HTER by 4% and 6.3% over BLEU and METEOR optimized systems respectively.
",6.3 Human Evaluation,[0],[0]
"The single-metric system tuned with M-only setting scores high on HBLEU, closely followed by the ensemble system.",6.3 Human Evaluation,[0],[0]
We believe this to be caused by chance rather than any systematic gains by the Monly tuning; the ensemble system scores high on HMETEOR compared to the M-only system.,6.3 Human Evaluation,[0],[0]
"While HTER captures the edit distance to the targeted reference, HMETEOR and HBLEU metrics capture missing content words or synonyms by exploiting n-grams and paraphrase matching.
",6.3 Human Evaluation,[0],[0]
"We also computed the regular variants (BLEU, METEOR and TER), which are scored against original references.",6.3 Human Evaluation,[0],[0]
The ensemble system outperformed the single-metric systems in all the three metrics.,6.3 Human Evaluation,[0],[0]
The improvements were also statistically significant at p-value of 0.05 for BLEU and TER.,6.3 Human Evaluation,[0],[0]
We propose and present a comprehensive study of several multi-metric optimization (MMO) methods in SMT.,7 Conclusion,[0],[0]
"First, by exploiting the idea of ensemble decoding (Razmara et al., 2012), we propose an effective way to combine multiple Pareto-optimal model weights from previous MMO methods (e.g. Duh et al. (2012)), obviating the need for manually trading off among metrics.",7 Conclusion,[0],[0]
"We also proposed two new variants: lateen-style MMO and union of metrics.
",7 Conclusion,[0],[0]
We also extended ensemble decoding to a new tuning algorithm called ensemble tuning.,7 Conclusion,[0],[0]
This method demonstrates statistically significant gains for BLEU and RIBES with modest reduction in METEOR and TER.,7 Conclusion,[0],[0]
"Further, in our human evaluation, ensemble tuning obtains the best HTER among competing baselines, confirming that optimizing on multiple metrics produces human-preferred translations compared to the conventional optimization approach involving a single metric.",7 Conclusion,[0],[0]
This paper examines tuning for statistical machine translation (SMT) with respect to multiple evaluation metrics.,abstractText,[0],[0]
"We propose several novel methods for tuning towards multiple objectives, including some based on ensemble decoding methods.",abstractText,[0],[0]
"Pareto-optimality is a natural way to think about multi-metric optimization (MMO) and our methods can effectively combine several Pareto-optimal solutions, obviating the need to choose one.",abstractText,[0],[0]
Our best performing ensemble tuning method is a new algorithm for multi-metric optimization that searches for Pareto-optimal ensemble models.,abstractText,[0],[0]
We study the effectiveness of our methods through experiments on multiple as well as single reference(s) datasets.,abstractText,[0],[0]
"Our experiments show simultaneous gains across several metrics (BLEU, RIBES), without any significant reduction in other metrics.",abstractText,[0],[0]
This contrasts the traditional tuning where gains are usually limited to a single metric.,abstractText,[0],[0]
"Our human evaluation results confirm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one.",abstractText,[0],[0]
Multi-Metric Optimization Using Ensemble Tuning,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1092–1102 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Multimedia data (including text, image, audio and video) have increased dramatically recently, which makes it difficult for users to obtain important information efficiently.",1 Introduction,[0],[0]
"Multi-modal summarization (MMS) can provide users with textual summaries that can help acquire the gist of multimedia data in a short time, without reading documents or watching videos from beginning to end.
",1 Introduction,[0],[0]
"1http://www.nlpr.ia.ac.cn/cip/jjzhang.htm
The existing applications related to MMS include meeting record summarization (Erol et al., 2003; Gross et al., 2000), sport video summarization (Tjondronegoro et al., 2011; Hasan et al., 2013), movie summarization (Evangelopoulos et al., 2013; Mademlis et al., 2016), pictorial storyline summarization (Wang et al., 2012), timeline summarization (Wang et al., 2016b) and social multimedia summarization (Del Fabro et al., 2012; Bian et al., 2013; Schinas et al., 2015; Bian et al., 2015; Shah et al., 2015, 2016).",1 Introduction,[0],[0]
"When summarizing meeting recordings, sport videos and movies, such videos consist of synchronized voice, visual and captions.",1 Introduction,[0],[0]
"For the summarization of pictorial storylines, the input is a set of images with text descriptions.",1 Introduction,[0],[0]
"None of these applications focus on summarizing multimedia data that contain asynchronous information about general topics.
",1 Introduction,[0],[0]
"In this paper, as shown in Figure 1, we propose an approach to a generate textual summary from a set of asynchronous documents, images, audios and videos on the same topic.
",1 Introduction,[0],[0]
"Since multimedia data are heterogeneous and contain more complex information than pure text does, MMS faces a great challenge in addressing the semantic gap between different modalities.",1 Introduction,[0],[0]
The framework of our method is shown in Figure 1.,1 Introduction,[0],[0]
"For the audio information contained in videos, we obtain speech transcriptions through Automatic Speech Recognition (ASR) and design a method to use these transcriptions selectively.",1 Introduction,[0],[0]
"For visual information, including the key-frames extracted from videos and the images that appear in documents, we learn the joint representations of texts and images by using a neural network; we then can identify the text that is relevant to the image.",1 Introduction,[0],[0]
"In this way, audio and visual information can be integrated into a textual summary.
",1 Introduction,[0],[0]
"Traditional document summarization involves two essential aspects: (1) Salience: the summa-
1092
ry should retain significant content of the input documents.",1 Introduction,[0],[0]
(2) Non-redundancy: the summary should contain as little redundant content as possible.,1 Introduction,[0],[0]
"For MMS, we consider two additional aspects: (3) Readability: because speech transcriptions are occasionally ill-formed, we should try to get rid of the errors introduced by ASR.",1 Introduction,[0],[0]
"For example, when a transcription provides similar information to a sentence in documents, we should prefer the sentence to the transcription presented in the summary.",1 Introduction,[0],[0]
(4) Coverage for the visual information: images that appear in documents and videos often capture event highlights that are usually very important.,1 Introduction,[0],[0]
"Thus, the summary should cover as much of the important visual information as possible.",1 Introduction,[0],[0]
"All of the aspects can be jointly optimized by the budgeted maximization of submodular functions (Khuller et al., 1999).
",1 Introduction,[0],[0]
"Our main contributions are as follows:
• We design an MMS method that can automatically generate a textual summary from a set of asynchronous documents, images, audios and videos related to a specific topic.
",1 Introduction,[0],[0]
"• To select the representative sentences, we consider four criteria that are jointly optimized by the budgeted maximization of submodular functions.
",1 Introduction,[0],[0]
• We introduce an MMS corpus in English and Chinese.,1 Introduction,[0],[0]
The experimental results on this dataset demonstrate that our system can take advantage of multi-modal information and outperforms other baseline methods.,1 Introduction,[0],[0]
"Multi-document summarization (MDS) attempts to extract important information for a set of documents related to a topic to generate a short sum-
mary.",2.1 Multi-document Summarization,[0],[0]
"Graph based methods (Mihalcea and Tarau, 2004; Wan and Yang, 2006; Zhang et al., 2016) are commonly used.",2.1 Multi-document Summarization,[0],[0]
"LexRank (Erkan and Radev, 2011) first builds a graph of the documents, in which each node represents a sentence and the edges represent the relationship between sentences.",2.1 Multi-document Summarization,[0],[0]
"Then, the importance of each sentence is computed through an iterative random walk.",2.1 Multi-document Summarization,[0],[0]
"In recent years, much work has been done to summarize meeting recordings, sport videos, movies, pictorial storylines and social multimedia.
",2.2 Multi-modal Summarization,[0],[0]
"Erol et al. (2003) aim to create important segments of a meeting recording based on audio, text and visual activity analysis.",2.2 Multi-modal Summarization,[0],[0]
Tjondronegoro et al. (2011) propose a way to summarize a sporting event by analyzing the textual information extracted from multiple resources and identifying the important content in a sport video.,2.2 Multi-modal Summarization,[0],[0]
Evangelopoulos et al. (2013) use an attention mechanism to detect salient events in a movie.,2.2 Multi-modal Summarization,[0],[0]
Wang et al. (2012) and Wang et al. (2016b) use image-text pairs to generate a pictorial storyline and timeline summarization.,2.2 Multi-modal Summarization,[0],[0]
"Li et al. (2016) develop an approach for multimedia news summarization for searching results on the Internet, in which the hLDA model is introduced to discover the topic structure of the news documents.",2.2 Multi-modal Summarization,[0],[0]
"Then, a news article and an image are chosen to represent each topic.",2.2 Multi-modal Summarization,[0],[0]
"For social media summarization, Fabro et al. (2012) and Schinas et al. (2015) propose to summarize the real-life events based on multimedia content such as photos from Flickr and videos from YouTube.",2.2 Multi-modal Summarization,[0],[0]
Bian et al. (2013; 2015) propose a multimodal LDA to detect topics by capturing the correlations between textual and visual features of microblogs with embedded images.,2.2 Multi-modal Summarization,[0],[0]
The output of their method is a set of representative images that describe the events.,2.2 Multi-modal Summarization,[0],[0]
"Shah et al. (2015; 2016) introduce EventBuilder
which produces text summaries for a social event leveraging Wikipedia and visualizes the event with social media activities.
",2.2 Multi-modal Summarization,[0],[0]
"Most of the above studies focus on synchronous multi-modal content, i.e., in which images are paired with text descriptions and videos are paired with subtitles.",2.2 Multi-modal Summarization,[0],[0]
"In contrast, we perform summarization from asynchronous (i.e., there is no given description for images and no subtitle for videos) multi-modal information about news topics, including multiple documents, images and videos, to generate a fixed length textual summary.",2.2 Multi-modal Summarization,[0],[0]
This task is both more general and more challenging.,2.2 Multi-modal Summarization,[0],[0]
"The input is a collection of multi-modal dataM = {D1, ..., D|D|, V1, ..., V|V |} related to a news topic T , where each document Di = {Ti, Ii} consists of text Ti and image Ii (there may be no image for some documents).",3.1 Problem Formulation,[0],[0]
Vi denotes video.,3.1 Problem Formulation,[0],[0]
| · | denotes the cardinality of a set.,3.1 Problem Formulation,[0],[0]
The objective of our work is to automatically generate textual summary to represent the principle content ofM.,3.1 Problem Formulation,[0],[0]
There are many essential aspects in generating a good textual summary for multi-modal data.,3.2 Model Overview,[0],[0]
"The salient content in documents should be retained, and the key facts in videos and images should be covered.",3.2 Model Overview,[0],[0]
"Further, the summary should be readable and non-redundant and should follow the fixed length constraint.",3.2 Model Overview,[0],[0]
"We propose an extraction-based method in which all these aspects can be jointly optimized by the budgeted maximization of submodular functions defined as follows:
max S⊆T {F(S) : ∑ s∈S ls ≤ L} (1)
where T is the set of sentences, S is the summary, ls is length (number of words) of sentence s, L is budget, i.e., length constraint for the summary, and submodular function F(S) is the summary score related to the above-mentioned aspects.
",3.2 Model Overview,[0],[0]
"Text is the main modality of documents, and in some cases, images are embedded in documents.",3.2 Model Overview,[0],[0]
Videos consist of at least two types of modalities: audio and visual.,3.2 Model Overview,[0],[0]
"Next, we give overall processing methods for different modalities.
",3.2 Model Overview,[0],[0]
"Audio, i.e., speech, can be automatically transcribed into text by using an ASR system2.",3.2 Model Overview,[0],[0]
"Then, we can leverage a graph-based method to calculate the salience score for all of the speech transcriptions and for the original sentences in documents.",3.2 Model Overview,[0],[0]
"Note that speech transcriptions are often ill-formed; thus, to improve the readability, we should try to avoid the errors introduced by ASR.",3.2 Model Overview,[0],[0]
"In addition, audio features including acoustic confidence (Valenza et al., 1999), audio power (Christel et al., 1998) and audio magnitude (Dagtas and Abdel-Mottaleb, 2001) have proved to be helpful for speech and video summarization which will benefit our method.
",3.2 Model Overview,[0],[0]
"For visual, which is actually a sequence of images (frames), because most of the neighboring frames contain redundant information, we first extract the most meaningful frames, i.e., the keyframes, which can provide the key facts for the whole video.",3.2 Model Overview,[0],[0]
"Then, it is necessary to perform semantic analysis between text and visual.",3.2 Model Overview,[0],[0]
"To this end, we learn the joint representations for textual and visual modalities and can then identify the sentence that is relevant to the image.",3.2 Model Overview,[0],[0]
"In this way, we can guarantee the coverage of generated summary for the visual information.",3.2 Model Overview,[0],[0]
"We apply a graph-based LexRank algorithm (Erkan and Radev, 2011) to calculate salience score of the text unit, including the sentences in documents and the speech transcriptions from videos.",3.3 Salience for Text,[0],[0]
"LexRank first constructs a graph based on the text units and their relationship and then conducts an iteratively random walk to calculate the salience score of the text unit, sa(ti), until convergence using the following equation:
Sa(ti) = µ ∑
j
Sa(tj) ·Mji + 1− µ N (2)
where µ is the damping factor that is set to 0.85.",3.3 Salience for Text,[0],[0]
N is the total number of the text units.,3.3 Salience for Text,[0],[0]
"Mji is the relationship between text unit ti and tj , which is computed as follows:
",3.3 Salience for Text,[0],[0]
"Mji = sim(tj , ti) (3)
",3.3 Salience for Text,[0],[0]
"The text unit ti is represented by averaging the embeddings of the words (except stop-words) in ti. sim(·) denotes cosine similarity between two texts (negative similarities are replaced with 0).
",3.3 Salience for Text,[0],[0]
"2We use IBM Watson Speech to Text service: www.ibm.com/watson/developercloud/speech-to-text.html
For MMS task, we propose two guidance strategies to amend the affinity matrix M and calculate salience score of the text as shown in Figure 2.",3.3 Salience for Text,[0],[0]
The random walk process can be understood as a recommendation: Mji in Equation 2 denotes that tj will recommend ti to the degree of Mji.,3.3.1 Readability Guidance Strategies,[0],[0]
"The affinity matrix M in the LexRank model is symmetric, which means Mij = Mji.",3.3.1 Readability Guidance Strategies,[0],[0]
"In contrast, for MMS, considering the unsatisfactory quality of speech recognition, symmetric affinity matrices are inappropriate.",3.3.1 Readability Guidance Strategies,[0],[0]
"Specifically, to improve the readability, for a speech transcription, if there is a sentence in document that is related to this transcription, we would prefer to assign the text sentence a higher salience score than that assigned to the transcribed one.",3.3.1 Readability Guidance Strategies,[0],[0]
"To this end, the process of a random walk should be guided to control the recommendation direction: when a document sentence is related to a speech transcription, the symmetric weighted edge between them should be transformed into a unidirectional edge, in which we invalidate the direction from document sentence to the transcribed one.",3.3.1 Readability Guidance Strategies,[0],[0]
"In this way, speech transcriptions will not be recommended by the corresponding document sentences.",3.3.1 Readability Guidance Strategies,[0],[0]
Important speech transcriptions that cannot be covered by documents still have the chance to obtain high salience scores.,3.3.1 Readability Guidance Strategies,[0],[0]
"For the pair of a sentence ti and a speech transcription tj , Mij is computed as follows:
Mij = {
0, if sim(ti, tj) >",3.3.1 Readability Guidance Strategies,[0],[0]
"Ttext sim(ti, tj), otherwise
(4) where threshold Ttext is used to determine whether a sentence is related to others.",3.3.1 Readability Guidance Strategies,[0],[0]
"We obtain the proper semantic similarity threshold by testing on Microsoft Research Paraphrase (MSRParaphrase) dataset (Quirk et al., 2004).",3.3.1 Readability Guidance Strategies,[0],[0]
"It is a publicly avail-
able paraphrase corpus that consists of 5801 pairs of sentences, of which 3900 pairs are semantically equivalent.",3.3.1 Readability Guidance Strategies,[0],[0]
Some audio features can guide the summarization system to select more important and readable speech transcriptions.,3.3.2 Audio Guidance Strategies,[0],[0]
Valenza et al. (1999) use acoustic confidence to obtain accurate and readable summaries of broadcast news programs.,3.3.2 Audio Guidance Strategies,[0],[0]
Christel et al. (1998) and Dagtas and AbdelMottaleb (2001) apply audio power and audio magnitude to find significant audio events.,3.3.2 Audio Guidance Strategies,[0],[0]
"In our work, we first balance these three feature scores for each speech transcription by dividing their respective maximum values among the whole amount of audio, and we then average these scores to obtain the final audio score for speech transcription.",3.3.2 Audio Guidance Strategies,[0],[0]
"For each adjacent speech transcription pair (tk, tk′ ), if the audio score a(tk) for tk is smaller than a certain threshold while a(tk′ ) is greater, which means that tk′ is more important and readable than tk, then tk should recommend tk′ , but tk′ should not recommend tk.",3.3.2 Audio Guidance Strategies,[0],[0]
"We formulate it as follows: {
Mkk′ = sim(tk, tk′ )",3.3.2 Audio Guidance Strategies,[0],[0]
"Mk′k = 0
if a(tk)",3.3.2 Audio Guidance Strategies,[0],[0]
< Taudio and a(tk′ ),3.3.2 Audio Guidance Strategies,[0],[0]
>,3.3.2 Audio Guidance Strategies,[0],[0]
"Taudio (5)
where the threshold Taudio is the average audio score for all the transcriptions in the audio.
",3.3.2 Audio Guidance Strategies,[0],[0]
"Finally, affinity matrices are normalized so that each row adds up to 1.",3.3.2 Audio Guidance Strategies,[0],[0]
The key-frames contained in videos and the images embedded in documents often captures news highlights in which the important ones should be covered by the textual summary.,3.4 Text-Image Matching,[0],[0]
"Before measuring the coverage for images, we should train the model to bridge the gap between text and image, i.e., to match the text and image.
",3.4 Text-Image Matching,[0],[0]
We start by extracting key-frames of videos based on shot boundary detection.,3.4 Text-Image Matching,[0],[0]
A shot is defined as an unbroken sequence of frames.,3.4 Text-Image Matching,[0],[0]
"The abrupt transition of RGB histogram features often indicates shot boundaries (Zhuang et al., 1998).",3.4 Text-Image Matching,[0],[0]
"Specifically, when the transition of the RGB histogram feature for adjacent frames is greater than a certain ratio3 of the average transition for the whole video, we segment the shot.",3.4 Text-Image Matching,[0],[0]
"Then, the frames
3The ratio is determined by testing on the
in the middle of each shot are extracted as keyframes.",3.4 Text-Image Matching,[0],[0]
"These key-frames and images in documents make up the image set that the summary should cover.
",3.4 Text-Image Matching,[0],[0]
"Next, it is necessary to perform a semantic analysis between the text and the image.",3.4 Text-Image Matching,[0],[0]
"To this end, we learn the joint representations for textual and visual modalities by using a model trained on the Flickr30K dataset (Young et al., 2014), which contains 31,783 photographs of everyday activities, events and scenes harvested from Flickr.",3.4 Text-Image Matching,[0],[0]
Each photograph is manually labeled with 5 textual descriptions.,3.4 Text-Image Matching,[0],[0]
"We apply the framework of Wang et al. (2016a), which achieves state-of-the-art performance for text-image matching task on the Flickr30K dataset.",3.4 Text-Image Matching,[0],[0]
"The image is encoded by the VGG model (Simonyan and Zisserman, 2014) that has been trained on the ImageNet classification task following the standard procedure (Wang et al., 2016a).",3.4 Text-Image Matching,[0],[0]
The 4096-dimensional feature from the pre-softmax layer is used to represent the image.,3.4 Text-Image Matching,[0],[0]
The text is first encoded by the Hybrid GaussianLaplacian mixture model (HGLMM) using the method of Klein et al. (2014).,3.4 Text-Image Matching,[0],[0]
"Then, the HGLMM vectors are reduced to 6000 dimensions through PCA.",3.4 Text-Image Matching,[0],[0]
"Next, the sentence vector vs and image vector vi are mapped to a joint space by a two-branch neural network as follows:{
x = W2 · f(W1 · vs + bs) y = V2 · f(V1 · vi + bi) (6)
where W1 ∈",3.4 Text-Image Matching,[0],[0]
"R2048×6000, bs ∈ R2048, W2 ∈",3.4 Text-Image Matching,[0],[0]
"R512×2048, V1 ∈ R2048×4096, bi ∈ R2048,",3.4 Text-Image Matching,[0],[0]
"V2 ∈ R512×2048, f is Rectified Linear Unit (ReLU).
",3.4 Text-Image Matching,[0],[0]
"The max-margin learning framework is applied to optimize the neural network as follows:
L = ∑ i,k max[0,m+ s(xi, yi)− s(xi, yk)]
+ λ1 ∑ i,k max[0,m+ s(xi, yi)− s(xk, yi)] (7)
where for positive text-image pair (xi, yi), the top K most violated negative pairs (xi, yk) and (xk, yi) in each mini-batch are sampled.",3.4 Text-Image Matching,[0],[0]
"The objective function L favors higher matching score s(xi, yi) (cosine similarity) for positive text-image pairs than for negative pairs4.
shot detection dataset of TRECVID.",3.4 Text-Image Matching,[0],[0]
"http://wwwnlpir.nist.gov/projects/trecvid/
4In the experiments, K = 50, m = 0.1 and λ1 = 2.",3.4 Text-Image Matching,[0],[0]
"Wang et al. (2016a) also proved that structure-preserving constraints can make 1% Recall@1 improvement.
",3.4 Text-Image Matching,[0],[0]
Note that the images in Flickr30K are similar to our task.,3.4 Text-Image Matching,[0],[0]
"However, the image descriptions are much simpler than the text in news, so the model trained on Flickr30K cannot be directly used for our task.",3.4 Text-Image Matching,[0],[0]
"For example, some of the information contained in the news, such as the time and location of events, cannot be directly reflected by images.",3.4 Text-Image Matching,[0],[0]
"To solve this problem, we simplify each sentence and speech transcription based on semantic role labelling (Gildea and Jurafsky, 2002), in which each predicate indicates an event and the arguments express the relevant information of this event.",3.4 Text-Image Matching,[0],[0]
"ARG0 denotes the agent of the event, and ARG1 denotes the action.",3.4 Text-Image Matching,[0],[0]
"The assumption is that the concepts including agent, predicate and action compose the body of the event, so we extract “ARG0+predicate+ARG1” as the simplified sentence that is used to match the images.",3.4 Text-Image Matching,[0],[0]
"It is worth noting that there may be multiple predicateargument structures for one sentence and we extract all of them.
",3.4 Text-Image Matching,[0],[0]
"After the text-image matching model is trained and the sentences are simplified, for each textimage pair (Ti, Ij) in our task, we can identify the matched pairs if the score s(Ti, Ij) is greater than a threshold Tmatch.",3.4 Text-Image Matching,[0],[0]
"We set the threshold as the average matching score for the positive text-image pair in Flickr30K, although the matching performance for our task could in principle be improved by adjusting this parameter.",3.4 Text-Image Matching,[0],[0]
"We model the salience of a summary S as the sum of salience scores Sa(ti)5 of the sentence ti in the summary, combining a λ-weighted redundancy penalty term:
Fs(S) = ∑ ti∈S Sa(ti)− λs|S| ∑ ti,tj∈S sim(ti, tj) (8)
We model the summary S coverage for the image set I as the weighted sum of image covered by the summary:
Fc(S) = ∑ pi∈I Im(pi)bi (9)
where the weight Im(pi) for the image pi is the length ratio between the shot pi and the whole videos.",3.5 Multi-modal Summarization,[0],[0]
"bi is a binary variable to indicate
5Normalized by the maximum value among all the sentences.
",3.5 Multi-modal Summarization,[0],[0]
"whether an image pi is covered by the summary, i.e., whether there is at least one sentence in the summary matching the image.
",3.5 Multi-modal Summarization,[0],[0]
"Finally, considering all the modalities, the objective function is defined as follows:
Fm(S)",3.5 Multi-modal Summarization,[0],[0]
= 1 Ms ∑ ti∈S Sa(ti) + 1,3.5 Multi-modal Summarization,[0],[0]
"Mc ∑ pi∈I Im(pi)bi
− λm|S| ∑ i,j∈S sim(ti, tj)
(10) where Ms is the summary score obtained by Equation 8 and Mc is the summary score obtained by Equation 9.",3.5 Multi-modal Summarization,[0],[0]
The aim of Ms and Mc is to balance the aspects of salience and coverage for images.,3.5 Multi-modal Summarization,[0],[0]
"λs, and λm are determined by testing on development set.",3.5 Multi-modal Summarization,[0],[0]
"Note that to guaranteed monotone of F , λs, and λm should be lower than the minimum salience score of sentences.",3.5 Multi-modal Summarization,[0],[0]
"To further improve non-redundancy, we make sure that similarity between any pair of sentences in the summary is lower than Ttext.
",3.5 Multi-modal Summarization,[0],[0]
"Equations 8,9 and 10 are all monotone submodular functions under the budget constraint.",3.5 Multi-modal Summarization,[0],[0]
"Thus, we apply the greedy algorithm (Lin and Bilmes, 2010) guaranteeing near-optimization to solve the problem.",3.5 Multi-modal Summarization,[0],[0]
There is no benchmark dataset for MMS.,4.1 Dataset,[0],[0]
We construct a dataset as follows.,4.1 Dataset,[0],[0]
"We select 50 news topics in the most recent five years, 25 in English and 25 in Chinese.",4.1 Dataset,[0],[0]
We set 5 topics for each language as a development set.,4.1 Dataset,[0],[0]
"For each topic, we collect 20 documents within the same period using Google News search6 and 5-10 videos in CCTV.com7 and Youtube8.",4.1 Dataset,[0],[0]
More details of the corpus are illustrated in Table 1.,4.1 Dataset,[0],[0]
"Some examples of news topics are provided Table 2.
",4.1 Dataset,[0],[0]
We employ 10 graduate students to write reference summaries after reading documents and watching videos on the same topic.,4.1 Dataset,[0],[0]
We keep 3 reference summaries for each topic.,4.1 Dataset,[0],[0]
"The criteria for summarizing documents lie in: (1) retaining important content of the input documents and videos; (2) avoiding redundant information; (3) having a
6http://news.google.com/ 7http://www.cctv.com/ 8https://www.youtube.com/
good readability; (4) following the length limit.",4.1 Dataset,[0],[0]
"We set the length constraint for each English and Chinese summary to 300 words and 500 characters, respectively.",4.1 Dataset,[0],[0]
"Several models are compared in our experiments, including generating summaries with different modalities and different approaches to leverage images.
",4.2 Comparative Methods,[0],[0]
Text only.,4.2 Comparative Methods,[0],[0]
"This model generates summaries only using the text in documents.
",4.2 Comparative Methods,[0],[0]
Text + audio.,4.2 Comparative Methods,[0],[0]
"This model generates summaries using the text in documents and the speech transcriptions but without guidance strategies.
",4.2 Comparative Methods,[0],[0]
Text + audio + guide.,4.2 Comparative Methods,[0],[0]
"This model generates summaries using the text in documents and the speech transcriptions with guidance strategies.
",4.2 Comparative Methods,[0],[0]
The following models generate summaries using both documents and videos but take advantage of images in different ways.,4.2 Comparative Methods,[0],[0]
"The salience scores for text are obtained with guidance strategies.
",4.2 Comparative Methods,[0],[0]
Image caption.,4.2 Comparative Methods,[0],[0]
The image is first captioned using the model of Vinyals et al. (2016) which achieved first place in the 2015 MSCOCO Image Captioning Challenge.,4.2 Comparative Methods,[0],[0]
"This model generates summaries using text in documents, speech transcription and image captions.
",4.2 Comparative Methods,[0],[0]
"Note that the above-mentioned methods generate summaries by using Equation 8 and the follow-
ing methods using Equation 8 ,9 and 10.",4.2 Comparative Methods,[0],[0]
Image caption match.,4.2 Comparative Methods,[0],[0]
"This model uses generated image captions to match the text; i.e., if the similarity between a generated image caption and a sentence exceeds the threshold Ttext, the image and the sentence match.
",4.2 Comparative Methods,[0],[0]
Image alignment.,4.2 Comparative Methods,[0],[0]
"The images are aligned to the text in the following ways: The images in a document are aligned to all the sentences in this document and the key-frames in a shot are aligned to all the speech transcriptions in this shot.
",4.2 Comparative Methods,[0],[0]
Image match.,4.2 Comparative Methods,[0],[0]
The texts are matched with images using the approach introduced in Section 3.4.,4.2 Comparative Methods,[0],[0]
"We perform sentence9 and word tokenization, and all the Chinese sentences are segmented by Stanford Chinese Word Segmenter (Tseng et al., 2005).",4.3 Implementation Details,[0],[0]
"We apply Stanford CoreNLP toolkit (Levy and D. Manning, 2003; Klein and D. Manning, 2003) to perform lexical parsing and use semantic role labelling approach proposed by Yang and Zong (2014).",4.3 Implementation Details,[0],[0]
We use 300-dimension skipgram English word embeddings which are publicly available10.,4.3 Implementation Details,[0],[0]
"Given that text-image matching model and image caption generation model are trained in English, to create summaries in Chinese, we first translate the Chinese text into English via Google Translation11 and then conduct text and image matching.",4.3 Implementation Details,[0],[0]
"We use the ROUGE-1.5.5 toolkit (Lin and Hovy, 2003) to evaluate the output summaries.",4.4 Multi-modal Summarization Evaluation,[0],[0]
This evaluation metric measures the summary quality by matching n-grams between generated summary and reference summary.,4.4 Multi-modal Summarization Evaluation,[0],[0]
"Table 3 and Table 4 show the averaged ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4) F-scores regarding to the three reference summaries for each topic in English and Chinese.
",4.4 Multi-modal Summarization Evaluation,[0],[0]
"For the results of the English MMS, from the first three lines in Table 3 we can see that when summarizing without visual information, the method with guidance strategies performs slightly better than do the first two methods.",4.4 Multi-modal Summarization Evaluation,[0],[0]
"Because Rouge mainly measures word overlaps, manual evaluation is needed to confirm the impact of guidance strategies on improving readability.",4.4 Multi-modal Summarization Evaluation,[0],[0]
"It is in-
9We exclude sentences containing less than 5 words.",4.4 Multi-modal Summarization Evaluation,[0],[0]
"10https://code.google.com/archive/p/word2vec/ 11https://translate.google.com
troduced in Section 4.5.",4.4 Multi-modal Summarization Evaluation,[0],[0]
The rating ranges from 1 (the poorest) to 5 (the best).,4.4 Multi-modal Summarization Evaluation,[0],[0]
"When summarizing with textual and visual modalities, performances are not always improved, which indicates that the models of image caption, image caption match and image alignment are not suitable to MMS.",4.4 Multi-modal Summarization Evaluation,[0],[0]
"The image match model has a significant advantage over other comparative methods, which illustrates that it can make use of multi-modal information.
",4.4 Multi-modal Summarization Evaluation,[0],[0]
"Table 4 shows the Chinese MMS results, which are similar to the English results that the image match model achieves the best performance.",4.4 Multi-modal Summarization Evaluation,[0],[0]
"We find that the performance enhancement for the image match model is smaller in Chinese than it is in English, which may be due to the errors introduced by machine translation.
",4.4 Multi-modal Summarization Evaluation,[0],[0]
"We provides a generated summary in English using the image match model, which is shown in Figure 3.",4.4 Multi-modal Summarization Evaluation,[0],[0]
The readability and informativeness for summaries are difficult to evaluate formally.,4.5 Manual Summary Quality Evaluation,[0],[0]
We ask five graduate students to measure the quality of summaries generated by different methods.,4.5 Manual Summary Quality Evaluation,[0],[0]
"We calculate the average score for all of the topics, and the results are displayed in Table 5.",4.5 Manual Summary Quality Evaluation,[0],[0]
"Overall, our method with guidance strategies achieves higher scores than do the other methods, but it is still obviously poorer than the reference sum-
maries.",4.5 Manual Summary Quality Evaluation,[0],[0]
"Specifically, when speech transcriptions are not considered, the informativeness of the summary is the worst.",4.5 Manual Summary Quality Evaluation,[0],[0]
"However, adding speech transcriptions without guidance strategies decreases readability to a large extent, which indicates that guidance strategies are necessary for MMS.",4.5 Manual Summary Quality Evaluation,[0],[0]
"The image match model achieves higher informativeness scores than do the other methods without using images.
",4.5 Manual Summary Quality Evaluation,[0],[0]
We give two instances of readability guidance that arise between document text (DT) and speech transcriptions (ST) in Table 6.,4.5 Manual Summary Quality Evaluation,[0],[0]
The errors introduced by ASR include segmentation (instance A) and recognition (instance B) mistakes.,4.5 Manual Summary Quality Evaluation,[0],[0]
Text-image matching is the toughest module for our framework.,4.6 How Much is the Image Worth,[0],[0]
"Although we use a state-of-the-art approach to match the text and images, the performance is far from satisfactory.",4.6 How Much is the Image Worth,[0],[0]
"To find a somewhat strong upper-bound of the task, we choose five topics for each language to manually label the text-image matching pairs.",4.6 How Much is the Image Worth,[0],[0]
The MMS results on these topics are shown in Table 7 and Table 8.,4.6 How Much is the Image Worth,[0],[0]
"The experiments show that with the ground truth textimage matching result, the summary quality can be promoted to a considerable extent, which indicates visual information is crucial for MMS.
",4.6 How Much is the Image Worth,[0],[0]
An image and the corresponding texts obtained using different methods are given in Figure 4 an d Figure 5.,4.6 How Much is the Image Worth,[0],[0]
"We can conclude that the image caption
and the image caption match contain little of the image’s intrinsically intended information.",4.6 How Much is the Image Worth,[0],[0]
"The image alignment introduces more noise because it is possible that the whole text in documents or the speech transcriptions in shot are aligned to the document images or the key-frames, respectively.",4.6 How Much is the Image Worth,[0],[0]
"The image match can obtain similar results to the image manually match, which illustrates that the image match can make use of visual information to generate summaries.",4.6 How Much is the Image Worth,[0],[0]
"This paper addresses an asynchronous MMS task, namely, how to use related text, audio and video information to generate a textual summary.",5 Conclusion,[0],[0]
We formulate the MMS task as an optimization problem with a budgeted maximization of submodular functions.,5 Conclusion,[0],[0]
"To selectively use the transcription of audio, guidance strategies are designed using the graph model to effectively calculate the salience score for each text unit, leading to more readable and informative summaries.",5 Conclusion,[0],[0]
"We investigate various approaches to identify the relevance between the image and texts, and find that the image match model performs best.",5 Conclusion,[0],[0]
"The final experimental results obtained using our MMS corpus in both English and Chinese demonstrate that our system can benefit from multi-modal information.
",5 Conclusion,[0],[0]
"Adding audio and video does not seem to improve dramatically over text only model, which indicates that better models are needed to capture the interactions between text and other modalities, especially for visual.",5 Conclusion,[0],[0]
"We also plan to enlarge our MMS dataset, specifically to collect more videos.",5 Conclusion,[0],[0]
The research work has been supported by the Natural Science Foundation of China under Grant No. 61333018 and No. 61403379.,Acknowledgments,[0],[0]
"The rapid increase in multimedia data transmission over the Internet necessitates the multi-modal summarization (MMS) from collections of text, image, audio and video.",abstractText,[0],[0]
"In this work, we propose an extractive multi-modal summarization method that can automatically generate a textual summary given a set of documents, images, audios and videos related to a specific topic.",abstractText,[0],[0]
The key idea is to bridge the semantic gaps between multi-modal content.,abstractText,[0],[0]
"For audio information, we design an approach to selectively use its transcription.",abstractText,[0],[0]
"For visual information, we learn the joint representations of text and images using a neural network.",abstractText,[0],[0]
"Finally, all of the multimodal aspects are considered to generate the textual summary by maximizing the salience, non-redundancy, readability and coverage through the budgeted optimization of submodular functions.",abstractText,[0],[0]
"We further introduce an MMS corpus in English and Chinese, which is released to the public1.",abstractText,[0],[0]
The experimental results obtained on this dataset demonstrate that our method outperforms other competitive baseline methods.,abstractText,[0],[0]
"Multi-modal Summarization for Asynchronous Collection of Text, Image, Audio and Video",title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1918–1927 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1918",text,[0],[0]
"Machine reading comprehension (MRC), empowering computers with the ability to acquire knowledge and answer questions from textual data, is believed to be a crucial step in building a general intelligent agent (Chen et al., 2016).",1 Introduction,[0],[0]
Recent years have seen rapid growth in the MRC community.,1 Introduction,[0],[0]
"With the release of various datasets, the MRC task has evolved from the early cloze-style test (Hermann et al., 2015; Hill et al., 2015) to answer extraction from a single passage (Rajpurkar et al.,
*This work was done while the first author was doing internship at Baidu Inc.
2016) and to the latest more complex question answering on web data (Nguyen et al., 2016; Dunn et al., 2017; He et al., 2017).
",1 Introduction,[0],[0]
"Great efforts have also been made to develop models for these MRC tasks , especially for the answer extraction on single passage (Wang and Jiang, 2016; Seo et al., 2016; Pan et al., 2017).",1 Introduction,[0],[0]
"A significant milestone is that several MRC models have exceeded the performance of human annotators on the SQuAD dataset1 (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"However, this success on single Wikipedia passage is still not adequate, considering the ultimate goal of reading the whole web.",1 Introduction,[0],[0]
"Therefore, several latest datasets (Nguyen et al., 2016; He et al., 2017; Dunn et al., 2017) attempt to design the MRC tasks in more realistic settings by involving search engines.",1 Introduction,[0],[0]
"For each question, they use the search engine to retrieve multiple passages and the MRC models are required to read these passages in order to give the final answer.
",1 Introduction,[0],[0]
"One of the intrinsic challenges for such multipassage MRC is that since all the passages are question-related but usually independently written, it’s probable that multiple confusing answer candidates (correct or incorrect) exist.",1 Introduction,[0],[0]
Table 1 shows an example from MS-MARCO.,1 Introduction,[0],[0]
We can see that all the answer candidates have semantic matching with the question while they are literally different and some of them are even incorrect.,1 Introduction,[0],[0]
"As is shown by Jia and Liang (2017), these confusing answer candidates could be quite difficult for MRC models to distinguish.",1 Introduction,[0],[0]
"Therefore, special consideration is required for such multi-passage MRC problem.
",1 Introduction,[0],[0]
"In this paper, we propose to leverage the answer candidates from different passages to verify the final correct answer and rule out the noisy incorrect answers.",1 Introduction,[0],[0]
"Our hypothesis is that the cor-
1https://rajpurkar.github.io/SQuAD-explorer/
rect answers could occur more frequently in those passages and usually share some commonalities, while incorrect answers are usually different from one another.",1 Introduction,[0],[0]
The example in Table 1 demonstrates this phenomenon.,1 Introduction,[0],[0]
"We can see that the answer candidates extracted from the last four passages are all valid answers to the question and they are semantically similar to each other, while the answer candidates from the other two passages are incorrect and there is no supportive information from other passages.",1 Introduction,[0],[0]
"As human beings usually compare the answer candidates from different sources to deduce the final answer, we hope that MRC model can also benefit from the cross-passage answer verification process.
",1 Introduction,[0],[0]
"The overall framework of our model is demonstrated in Figure 1 , which consists of three modules.",1 Introduction,[0],[0]
"First, we follow the boundary-based MRC models (Seo et al., 2016; Wang and Jiang, 2016) to find an answer candidate for each passage by identifying the start and end position of the answer (Figure 2).",1 Introduction,[0],[0]
"Second, we model the meanings of the answer candidates extracted from those passages and use the content scores to measure the quality of the candidates from a second perspective.",1 Introduction,[0],[0]
"Third, we conduct the answer verification by enabling each answer candidate to attend to the other candidates based on their representations.",1 Introduction,[0],[0]
"We hope that the answer candidates can collect supportive information from each other according to their semantic similarities and further decide whether each candidate is correct or not.
",1 Introduction,[0],[0]
"Therefore, the final answer is determined by three factors: the boundary, the content and the crosspassage answer verification.",1 Introduction,[0],[0]
"The three steps are modeled using different modules, which can be jointly trained in our end-to-end framework.
",1 Introduction,[0],[0]
"We conduct extensive experiments on the MSMARCO (Nguyen et al., 2016) and DuReader (He et al., 2017) datasets.",1 Introduction,[0],[0]
The results show that our answer verification MRC model outperforms the baseline models by a large margin and achieves the state-of-the-art performance on both datasets.,1 Introduction,[0],[0]
"Figure 1 gives an overview of our multi-passage MRC model which is mainly composed of three modules including answer boundary prediction, answer content modeling and answer verification.",2 Our Approach,[0],[0]
"First of all, we need to model the question and passages.",2 Our Approach,[0],[0]
"Following Seo et al. (2016), we compute the question-aware representation for each passage (Section 2.1).",2 Our Approach,[0],[0]
"Based on this representation, we employ a Pointer Network (Vinyals et al., 2015) to predict the start and end position of the answer in the module of answer boundary prediction (Section 2.2).",2 Our Approach,[0],[0]
"At the same time, with the answer content model (Section 2.3), we estimate whether each word should be included in the answer and thus obtain the answer representations.",2 Our Approach,[0],[0]
"Next, in the answer verification module (Section 2.4), each answer candidate can attend to the other answer candidates to collect supportive information and we compute one score for each candidate
to indicate whether it is correct or not according to the verification.",2 Our Approach,[0],[0]
The final answer is determined by not only the boundary but also the answer content and its verification score (Section 2.5).,2 Our Approach,[0],[0]
"Given a question Q and a set of passages {Pi} retrieved by search engines, our task is to find the best concise answer to the question.",2.1 Question and Passage Modeling,[0],[0]
"First, we formally present the details of modeling the question and passages.
",2.1 Question and Passage Modeling,[0],[0]
Encoding We first map each word into the vector space by concatenating its word embedding and sum of its character embeddings.,2.1 Question and Passage Modeling,[0],[0]
"Then we employ bi-directional LSTMs (BiLSTM) to encode the question Q and passages {Pi} as follows:
uQt = BiLSTMQ(u Q t−1, [e Q t , c Q t ]) (1) uPit = BiLSTMP",2.1 Question and Passage Modeling,[0],[0]
"(u Pi t−1, [e Pi t , c Pi t ]) (2)
where eQt , c Q t , e Pi t , c Pi t are the word-level and character-level embeddings of the tth word.",2.1 Question and Passage Modeling,[0],[0]
"uQt and uPit are the encoding vectors of the t
th words in Q and Pi respectively.",2.1 Question and Passage Modeling,[0],[0]
"Unlike previous work (Wang et al., 2017c) that simply concatenates all the passages, we process the passages independently at the encoding and matching steps.
",2.1 Question and Passage Modeling,[0],[0]
Q-P Matching One essential step in MRC is to match the question with passages so that important information can be highlighted.,2.1 Question and Passage Modeling,[0],[0]
"We use the
Attention Flow Layer (Seo et al., 2016) to conduct the Q-P matching in two directions.",2.1 Question and Passage Modeling,[0],[0]
"The similarity matrix S ∈ R|Q|×|Pi| between the question and passage i is changed to a simpler version, where the similarity between the tth word in the question and the kth word in passage i is computed as:
St,k = u Q t ᵀ · uPik (3)
",2.1 Question and Passage Modeling,[0],[0]
Then the context-to-question attention and question-to-context attention is applied strictly following Seo et al. (2016) to obtain the questionaware passage representation {ũPit }.,2.1 Question and Passage Modeling,[0],[0]
We do not give the details here due to space limitation.,2.1 Question and Passage Modeling,[0],[0]
"Next, another BiLSTM is applied in order to fuse the contextual information and get the new representation for each word in the passage, which is regarded as the match output:
vPit = BiLSTMM (v Pi t−1, ũ Pi t ) (4)
",2.1 Question and Passage Modeling,[0],[0]
"Based on the passage representations, we introduce the three main modules of our model.",2.1 Question and Passage Modeling,[0],[0]
"To extract the answer span from passages, mainstream studies try to locate the boundary of the answer, which is called boundary model.",2.2 Answer Boundary Prediction,[0],[0]
"Following (Wang and Jiang, 2016), we employ Pointer Network (Vinyals et al., 2015) to compute the probability of each word to be the start or end position
of the span:
gtk = w",2.2 Answer Boundary Prediction,[0],[0]
a 1 ᵀ tanh(Wa2,2.2 Answer Boundary Prediction,[0],[0]
"[v P k ,h a t−1]) (5)
αtk = exp(g t k)/ ∑|P| j=1 exp(gtj) (6)
ct = ∑|P|
k=1 αtkv P k (7)
hat = LSTM(h a t−1, ct) (8)
By utilizing the attention weights, the probability of the kth word in the passage to be the start and end position of the answer is obtained as α1k and α2k.",2.2 Answer Boundary Prediction,[0],[0]
"It should be noted that the pointer network is applied to the concatenation of all passages, which is denoted as P so that the probabilities are comparable across passages.",2.2 Answer Boundary Prediction,[0],[0]
"This boundary model can be trained by minimizing the negative log probabilities of the true start and end indices:
Lboundary = − 1
N N∑ i=1",2.2 Answer Boundary Prediction,[0],[0]
"(logα1y1i + logα2y2i ) (9)
where N is the number of samples in the dataset and y1i , y 2 i are the gold start and end positions.",2.2 Answer Boundary Prediction,[0],[0]
Previous work employs the boundary model to find the text span with the maximum boundary score as the final answer.,2.3 Answer Content Modeling,[0],[0]
"However, in our context, besides locating the answer candidates, we also need to model their meanings in order to conduct the verification.",2.3 Answer Content Modeling,[0],[0]
"An intuitive method is to compute the representation of the answer candidates separately after extracting them, but it could be hard to train such model end-to-end.",2.3 Answer Content Modeling,[0],[0]
"Here, we propose a novel method that can obtain the representation of the answer candidates based on probabilities.
",2.3 Answer Content Modeling,[0],[0]
"Specifically, we change the output layer of the classic MRC model.",2.3 Answer Content Modeling,[0],[0]
"Besides predicting the boundary probabilities for the words in the passages, we also predict whether each word should be included in the content of the answer.",2.3 Answer Content Modeling,[0],[0]
"The content probability of the kth word is computed as:
pck = sigmoid(w c 1 ᵀReLU(Wc2v Pi k ))",2.3 Answer Content Modeling,[0],[0]
"(10)
Training this content model is also quite intuitive.",2.3 Answer Content Modeling,[0],[0]
"We transform the boundary labels into a continuous segment, which means the words within the answer span will be labeled as 1 and other words will be labeled as 0.",2.3 Answer Content Modeling,[0],[0]
"In this way, we define
the loss function as the averaged cross entropy:
Lcontent =− 1
N
1
|P| N∑ i=1 |P",2.3 Answer Content Modeling,[0],[0]
|∑ j=1,2.3 Answer Content Modeling,[0],[0]
"[yck log p c k
+ (1− yck) log(1− pck)]
(11)
",2.3 Answer Content Modeling,[0],[0]
The content probabilities provide another view to measure the quality of the answer in addition to the boundary.,2.3 Answer Content Modeling,[0],[0]
"Moreover, with these probabilities, we can represent the answer from passage i as a weighted sum of all the word embeddings in this passage:
rAi = 1 |Pi| ∑|Pi|",2.3 Answer Content Modeling,[0],[0]
k=1 pck[e,2.3 Answer Content Modeling,[0],[0]
"Pi k , c Pi k ] (12)",2.3 Answer Content Modeling,[0],[0]
"The boundary model and the content model focus on extracting and modeling the answer within a single passage respectively, with little consideration of the cross-passage information.",2.4 Cross-Passage Answer Verification,[0],[0]
"However, as is discussed in Section 1, there could be multiple answer candidates from different passages and some of them may mislead the MRC model to make an incorrect prediction.",2.4 Cross-Passage Answer Verification,[0],[0]
It’s necessary to aggregate the information from different passages and choose the best one from those candidates.,2.4 Cross-Passage Answer Verification,[0],[0]
"Therefore, we propose a method to enable the answer candidates to exchange information and verify each other through the cross-passage answer verification process.
",2.4 Cross-Passage Answer Verification,[0],[0]
"Given the representation of the answer candidates from all passages {rAi}, each answer candidate then attends to other candidates to collect supportive information via attention mechanism:
si,j = { 0, if i = j, rAi ᵀ · rAj , otherwise (13)
",2.4 Cross-Passage Answer Verification,[0],[0]
"αi,j = exp(si,j)/",2.4 Cross-Passage Answer Verification,[0],[0]
"∑n
k=1 exp(si,k) (14) r̃Ai = ∑n
j=1 αi,jr
Aj (15)
",2.4 Cross-Passage Answer Verification,[0],[0]
Here r̃Ai is the collected verification information from other passages based on the attention weights.,2.4 Cross-Passage Answer Verification,[0],[0]
"Then we pass it together with the original representation rAi to a fully connected layer:
gvi = w vᵀ[rAi , r̃Ai , rAi r̃Ai ] (16)
We further normalize these scores over all passages to get the verification score for answer candidate Ai:
pvi = exp(g v i )/",2.4 Cross-Passage Answer Verification,[0],[0]
"∑n j=1 exp(gvj ) (17)
",2.4 Cross-Passage Answer Verification,[0],[0]
"In order to train this verification model, we take the answer from the gold passage as the gold answer.",2.4 Cross-Passage Answer Verification,[0],[0]
"And the loss function can be formulated as the negative log probability of the correct answer:
Lverify =",2.4 Cross-Passage Answer Verification,[0],[0]
"− 1
N N∑ i=1",2.4 Cross-Passage Answer Verification,[0],[0]
"log pvyvi (18)
where yvi is the index of the correct answer in all the answer candidates of the ith instance .",2.4 Cross-Passage Answer Verification,[0],[0]
"As is described above, we define three objectives for the reading comprehension model over multiple passages: 1. finding the boundary of the answer; 2. predicting whether each word should be included in the content; 3. selecting the best answer via cross-passage answer verification.",2.5 Joint Training and Prediction,[0],[0]
"According to our design, these three tasks can share the same embedding, encoding and matching layers.",2.5 Joint Training and Prediction,[0],[0]
"Therefore, we propose to train them together as multi-task learning (Ruder, 2017).",2.5 Joint Training and Prediction,[0],[0]
"The joint objective function is formulated as follows:
L = Lboundary + β1Lcontent + β2Lverify (19)
where β1 and β2 are two hyper-parameters that control the weights of those tasks.
",2.5 Joint Training and Prediction,[0],[0]
"When predicting the final answer, we take the boundary score, content score and verification score into consideration.",2.5 Joint Training and Prediction,[0],[0]
We first extract the answer candidateAi that has the maximum boundary score from each passage i. This boundary score is computed as the product of the start and end probability of the answer span.,2.5 Joint Training and Prediction,[0],[0]
"Then for each answer candidate Ai, we average the content probabilities of all its words as the content score of Ai.",2.5 Joint Training and Prediction,[0],[0]
And we can also predict the verification score for Ai using the verification model.,2.5 Joint Training and Prediction,[0],[0]
"Therefore, the final answer can be selected from all the answer candidates according to the product of these three scores.",2.5 Joint Training and Prediction,[0],[0]
"To verify the effectiveness of our model on multipassage machine reading comprehension, we conduct experiments on the MS-MARCO (Nguyen et al., 2016) and DuReader (He et al., 2017) datasets.",3 Experiments,[0],[0]
Our method achieves the state-of-the-art performance on both datasets.,3 Experiments,[0],[0]
"We choose the MS-MARCO and DuReader datasets to test our method, since both of them are
designed from real-world search engines and involve a large number of passages retrieved from the web.",3.1 Datasets,[0],[0]
"One difference of these two datasets is that MS-MARCO mainly focuses on the English web data, while DuReader is designed for Chinese MRC.",3.1 Datasets,[0],[0]
This diversity is expected to reflect the generality of our method.,3.1 Datasets,[0],[0]
"In terms of the data size, MS-MARCO contains 102023 questions, each of which is paired up with approximately 10 passages for reading comprehension.",3.1 Datasets,[0],[0]
"As for DuReader, it keeps the top-5 search results for each question and there are totally 201574 questions.
",3.1 Datasets,[0],[0]
One prerequisite for answer verification is that there should be multiple correct answers so that they can verify each other.,3.1 Datasets,[0],[0]
Both the MS-MARCO and DuReader datasets require the human annotators to generate multiple answers if possible.,3.1 Datasets,[0],[0]
Table 2 shows the proportion of questions that have multiple answers.,3.1 Datasets,[0],[0]
"However, the same answer that occurs many times is treated as one single answer here.",3.1 Datasets,[0],[0]
"Therefore, we also report the proportion of questions that have multiple answer spans to match with the human-generated answers.",3.1 Datasets,[0],[0]
A span is taken as valid if it can achieve F1 score larger than 0.7 compared with any reference answer.,3.1 Datasets,[0],[0]
"From these statistics, we can see that the phenomenon of multiple answers is quite common for both MS-MARCO and DuReader.",3.1 Datasets,[0],[0]
These answers will provide strong signals for answer verification if we can leverage them properly.,3.1 Datasets,[0],[0]
"For MS-MARCO, we preprocess the corpus with the reversible tokenizer from Stanford CoreNLP",3.2 Implementation Details,[0],[0]
"(Manning et al., 2014) and we choose the span that achieves the highest ROUGE-L score with the reference answers as the gold span for training.",3.2 Implementation Details,[0],[0]
"We employ the 300-D pre-trained Glove embeddings (Pennington et al., 2014) and keep it fixed during training.",3.2 Implementation Details,[0],[0]
The character embeddings are randomly initialized with its dimension as 30.,3.2 Implementation Details,[0],[0]
"For DuReader, we follow the preprocessing described in He et al. (2017).
",3.2 Implementation Details,[0],[0]
"We tune the hyper-parameters according to the
validation performance on the MS-MARCO development set.",3.2 Implementation Details,[0],[0]
The hidden size is set to be 150 and we apply L2 regularization with its weight as 0.0003.,3.2 Implementation Details,[0],[0]
"The task weights β1, β2 are both set to be 0.5.",3.2 Implementation Details,[0],[0]
"To train our model, we employ the Adam algorithm (Kingma and Ba, 2014) with the initial learning rate as 0.0004 and the mini-batch size as 32.",3.2 Implementation Details,[0],[0]
"Exponential moving average is applied on all trainable variables with a decay rate 0.9999.
",3.2 Implementation Details,[0],[0]
Two simple yet effective technologies are employed to improve the final performance on these two datasets respectively.,3.2 Implementation Details,[0],[0]
"For MS-MARCO, approximately 8% questions have the answers as Yes or No, which usually cannot be solved by extractive approach (Tan et al., 2017).",3.2 Implementation Details,[0],[0]
"We address this problem by training a simple Yes/No classifier for those questions with certain patterns (e.g., starting with “is”).",3.2 Implementation Details,[0],[0]
"Concretely, we simply change the output layer of the basic boundary model so that it can predict whether the answer is “Yes” or “No”.",3.2 Implementation Details,[0],[0]
"For DuReader, the retrieved document usually contains a large number of paragraphs that cannot be fed into MRC models directly (He et al., 2017).",3.2 Implementation Details,[0],[0]
"The original paper employs a simple a simple heuristic strategy to select a representative paragraph for each document, while we train a paragraph ranking model for this.",3.2 Implementation Details,[0],[0]
We will demonstrate the effects of these two technologies later.,3.2 Implementation Details,[0],[0]
Table 3 shows the results of our system and other state-of-the-art models on the MS-MARCO test set.,3.3 Results on MS-MARCO,[0],[0]
"We adopt the official evaluation metrics, including ROUGE-L (Lin, 2004) and BLEU-1 (Papineni et al., 2002).",3.3 Results on MS-MARCO,[0],[0]
"As we can see, for both metrics, our single model outperforms all the other competing models with an evident margin, which is extremely hard considering the near-human per-
formance.",3.3 Results on MS-MARCO,[0],[0]
"If we ensemble the models trained with different random seeds and hyper-parameters, the results can be further improved and outperform the ensemble model in Tan et al. (2017), especially in terms of the BLEU-1.",3.3 Results on MS-MARCO,[0],[0]
The results of our model and several baseline systems on the test set of DuReader are shown in Table 4.,3.4 Results on DuReader,[0],[0]
"The BiDAF and Match-LSTM models are provided as two baseline systems (He et al., 2017).",3.4 Results on DuReader,[0],[0]
"Based on BiDAF, as is described in Section 3.2, we tried a new paragraph selection strategy by employing a paragraph ranking (PR) model.",3.4 Results on DuReader,[0],[0]
We can see that this paragraph ranking can boost the BiDAF baseline significantly.,3.4 Results on DuReader,[0],[0]
"Finally, we implement our system based on this new strategy, and our system (single model) achieves further improvement by a large margin.",3.4 Results on DuReader,[0],[0]
"To get better insight into our system, we conduct in-depth ablation study on the development set of MS-MARCO, which is shown in Table 5.",4.1 Ablation Study,[0],[0]
"Following Tan et al. (2017), we mainly focus on the ROUGE-L score that is averaged case by case.
",4.1 Ablation Study,[0],[0]
We first evaluate the answer verification by ablating the cross-passage verification model so that the verification loss and verification score will not be used during training and testing.,4.1 Ablation Study,[0],[0]
Then we remove the content model in order to test the necessity of modeling the content of the answer.,4.1 Ablation Study,[0],[0]
"Since we don’t have the content scores, we use the boundary probabilities instead to compute the answer representation for verification.",4.1 Ablation Study,[0],[0]
"Next, to show the benefits of joint training, we train the boundary model separately from the other two models.",4.1 Ablation Study,[0],[0]
"Finally, we remove the yes/no classification in order to show the real improvement of our end-toend model compared with the baseline method that predicts the answer with only the boundary model.
",4.1 Ablation Study,[0],[0]
"From Table 5, we can see that the answer verification makes a great contribution to the overall improvement, which confirms our hypothesis that cross-passage answer verification is useful for the multi-passage MRC.",4.1 Ablation Study,[0],[0]
"For the ablation of the content model, we analyze that it will not only affect the content score itself, but also violate the verification model since the content probabilities are necessary for the answer representation, which will be further analyzed in Section 4.3.",4.1 Ablation Study,[0],[0]
"Another discovery is that jointly training the three models can provide great benefits, which shows that the three tasks are actually closely related and can boost each other with shared representations at bottom layers.",4.1 Ablation Study,[0],[0]
"At last, comparing our method with the baseline, we achieve an improvement of nearly
3 points without the yes/no classification.",4.1 Ablation Study,[0],[0]
This significant improvement proves the effectiveness of our approach.,4.1 Ablation Study,[0],[0]
"To demonstrate how each module of our model takes effect when predicting the final answer, we conduct a case study in Table 6 with the same example that we discussed in Section 1.",4.2 Case Study,[0],[0]
"For each answer candidate, we list three scores predicted by the boundary model, content model and verification model respectively.
",4.2 Case Study,[0],[0]
"On the one hand, we can see that these three scores generally have some relevance.",4.2 Case Study,[0],[0]
"For example, the second candidate is given lowest scores by all the three models.",4.2 Case Study,[0],[0]
We analyze that this is because the models share the same encoding and matching layers at bottom level and this relevance guarantees that the content and verification models will not violate the boundary model too much.,4.2 Case Study,[0],[0]
"On the other hand, we also see that the verification score can really make a difference here when the boundary model makes an incorrect decision among the confusing answer candidates ([1], [3], [4], [6]).",4.2 Case Study,[0],[0]
"Besides, as we expected, the verification model tends to give higher scores for those answers that have semantic commonality with each other ([3], [4], [6]), which are all valid answers in this case.",4.2 Case Study,[0],[0]
"By multiplying the three scores, our model finally predicts the answer correctly.",4.2 Case Study,[0],[0]
"In our model, we compute the answer representation based on the content probabilities predicted by a separate content model instead of directly using the boundary probabilities.",4.3 Necessity of the Content Model,[0],[0]
We argue that this content model is necessary for our answer verification process.,4.3 Necessity of the Content Model,[0],[0]
"Figure 2 plots the predicted content probabilities as well as the boundary probabilities
for a passage.",4.3 Necessity of the Content Model,[0],[0]
We can see that the boundary and content probabilities capture different aspects of the answer.,4.3 Necessity of the Content Model,[0],[0]
"Since answer candidates usually have similar boundary words, if we compute the answer representation based on the boundary probabilities, it’s difficult to model the real difference among different answer candidates.",4.3 Necessity of the Content Model,[0],[0]
"On the contrary, with the content probabilities, we pay more attention to the content part of the answer, which can provide more distinguishable information for verifying the correct answer.",4.3 Necessity of the Content Model,[0],[0]
"Furthermore, the content probabilities can also adjust the weights of the words within the answer span so that unimportant words (e.g. “and” and “.”) get lower weights in the final answer representation.",4.3 Necessity of the Content Model,[0],[0]
We believe that this refined representation is also good for the answer verification process.,4.3 Necessity of the Content Model,[0],[0]
"Machine reading comprehension made rapid progress in recent years, especially for singlepassage MRC task, such as SQuAD (Rajpurkar et al., 2016).",5 Related Work,[0],[0]
"Mainstream studies (Seo et al., 2016; Wang and Jiang, 2016; Xiong et al., 2016) treat reading comprehension as extracting answer span from the given passage, which is usually achieved by predicting the start and end position of the answer.",5 Related Work,[0],[0]
"We implement our boundary model similarly by employing the boundary-based pointer network (Wang and Jiang, 2016).",5 Related Work,[0],[0]
"Another inspiring work is from Wang et al. (2017c), where the authors propose to match the passage against itself so that the representation can aggregate evidence from the whole passage.",5 Related Work,[0],[0]
Our verification model adopts a similar idea.,5 Related Work,[0],[0]
"However, we collect information across passages and our attention is based on the answer representation, which is much more efficient than attention over all passages.",5 Related Work,[0],[0]
"For the model training, Xiong et al. (2017) argues that the boundary loss encourages exact answers at the
cost of penalizing overlapping answers.",5 Related Work,[0],[0]
Therefore they propose a mixed objective that incorporates rewards derived from word overlap.,5 Related Work,[0],[0]
Our joint training approach has a similar function.,5 Related Work,[0],[0]
"By taking the content and verification loss into consideration, our model will give less loss for overlapping answers than those unmatched answers, and our loss function is totally differentiable.
",5 Related Work,[0],[0]
"Recently, we also see emerging interests in multi-passage MRC from both the academic (Dunn et al., 2017; Joshi et al., 2017) and industrial community (Nguyen et al., 2016; He et al., 2017).",5 Related Work,[0],[0]
"Early studies (Shen et al., 2017; Wang et al., 2017c) usually concat those passages and employ the same models designed for singlepassage MRC.",5 Related Work,[0],[0]
"However, more and more latest studies start to design specific methods that can read multiple passages more effectively.",5 Related Work,[0],[0]
"In the aspect of passage selection, Wang et al. (2017a) introduced a pipelined approach that rank the passages first and then read the selected passages for answering questions.",5 Related Work,[0],[0]
Tan et al. (2017) treats the passage ranking as an auxiliary task that can be trained jointly with the reading comprehension model.,5 Related Work,[0],[0]
"Actually, the target of our answer verification is very similar to that of the passage selection, while we pay more attention to the answer content and the answer verification process.",5 Related Work,[0],[0]
"Speaking of the answer verification, Wang et al. (2017b) has a similar motivation to ours.",5 Related Work,[0],[0]
They attempt to aggregate the evidence from different passages and choose the final answer from n-best candidates.,5 Related Work,[0],[0]
"However, they implement their idea as a separate reranking step after reading comprehension, while our answer verification is a component of the whole model that can be trained end-to-end.",5 Related Work,[0],[0]
"In this paper, we propose an end-to-end framework to tackle the multi-passage MRC task .",6 Conclusion,[0],[0]
"We
creatively design three different modules in our model, which can find the answer boundary, model the answer content and conduct cross-passage answer verification respectively.",6 Conclusion,[0],[0]
All these three modules can be trained with different forms of the answer labels and training them jointly can provide further improvement.,6 Conclusion,[0],[0]
"The experimental results demonstrate that our model outperforms the baseline models by a large margin and achieves the state-of-the-art performance on two challenging datasets, both of which are designed for MRC on real web data.",6 Conclusion,[0],[0]
"This work is supported by the National Basic Research Program of China (973 program, No. 2014CB340505) and Baidu-Peking University Joint Project.",Acknowledgments,[0],[0]
We thank the Microsoft MSMARCO team for evaluating our results on the anonymous test set.,Acknowledgments,[0],[0]
"We also thank Ying Chen, Xuan Liu and the anonymous reviewers for their constructive criticism of the manuscript.",Acknowledgments,[0],[0]
Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine.,abstractText,[0],[0]
"Compared with MRC on a single passage, multi-passage MRC is more challenging, since we are likely to get multiple confusing answer candidates from different passages.",abstractText,[0],[0]
"To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations.",abstractText,[0],[0]
"Specifically, we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification.",abstractText,[0],[0]
"The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings.",abstractText,[0],[0]
Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3188–3197 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3188",text,[0],[0]
Neural text generation has attracted much attention in recent years thanks to its impressive generation accuracy and wide applicability.,1 Introduction,[0],[0]
"In addition to demonstrating compelling results for machine translation (MT) (Sutskever et al., 2014; Bahdanau et al., 2014), by simple adaptation, practically very same or similar models have also proven to be successful for summarization (Rush et al., 2015; Nallapati et al., 2016) and image or video captioning (Venugopalan et al., 2015; Xu et al., 2015a).
",1 Introduction,[0],[0]
"The most common neural text generation model is based on the encoder-decoder framework (Sutskever et al., 2014) which generates a variable-length output sequence using an RNNbased decoder with attention mechanisms (Bahdanau et al., 2014; Xu et al., 2015b).",1 Introduction,[0],[0]
"There are many recent efforts in improving the generation accuracy, e.g., ConvS2S (Gehring et al., 2017) and
Transformer (Vaswani et al., 2017).",1 Introduction,[0],[0]
"However, all these efforts are limited to training with a single reference even when multiple references are available.
",1 Introduction,[0],[0]
Multiple references are essential for evaluation due to the non-uniqueness of translation and generation unlike classification tasks.,1 Introduction,[0],[0]
"In MT, even though the training sets are usually with single reference (bitext), the evaluation sets often come with multiple references.",1 Introduction,[0],[0]
"For example, the NIST Chinese-to-English and Arabic-to-English MT evaluation datasets (2003–2008) have in total around 10,000 Chinese sentences and 10,000 Arabic sentences each with 4 different English translations.",1 Introduction,[0],[0]
"On the other hand, for image captioning datasets, multiple references are more common not only for evaluation, but also for training, e.g., the MSCOCO (Lin et al., 2014) dataset provides 5 references per image and PASCAL50S and ABSTRACT-50S (Vedantam et al., 2015) even provide 50 references per image.",1 Introduction,[0],[0]
Can we use the extra references during training?,1 Introduction,[0],[0]
"How much can we benefit from training with multiple references?
",1 Introduction,[0],[0]
"We therefore first investigate several different ways of utilizing existing human-annotated references, which include Sample One (Karpathy and Fei-Fei, 2015), Uniform, and Shuffle methods (explained in Sec. 2).",1 Introduction,[0],[0]
"Although Sample One has been explored in image captioning, to the best of our knowledge, this is the first time that an MT system is trained with multiple references.
",1 Introduction,[0],[0]
"Actually, four or five references still cover only a tiny fraction of the exponentially large space of potential references (Dreyer and Marcu, 2012).",1 Introduction,[0],[0]
"More importantly, encouraged by the success of training with multiple human references, we further propose a framework to generate many more pseudo-references automatically.",1 Introduction,[0],[0]
"In particular, we design a neural multiple-sequence alignment algo-
rithm to compress all existing human references into a lattice by merging similar words across different references (see examples in Fig. 1); this can be viewed as a modern, neural version of paraphrasing with multiple-sequence alignment (Barzilay and Lee, 2003, 2002).",1 Introduction,[0],[0]
"We can then generate theoretically exponentially more references from the lattice.
",1 Introduction,[0],[0]
"We make the following main contributions:
•",1 Introduction,[0],[0]
"Firstly, we investigate three different methods for multi-reference training on both MT and image captioning tasks (Section 2).
•",1 Introduction,[0],[0]
"Secondly, we propose a novel neural network-based multiple sequence alignment model to compress the existing references into lattices.",1 Introduction,[0],[0]
"By traversing these lattices, we generate exponentially many new pseudoreferences (Section 3).
",1 Introduction,[0],[0]
• We report substantial improvements over strong baselines in both MT (+1.5 BLEU) and image captioning (+3.1 BLEU / +11.7 CIDEr) by training on the newly generated pseudo-references (Section 4).,1 Introduction,[0],[0]
"In order to make the multiple reference training easy to adapt to any frameworks, we do not change anything from the existing models itself.",2 Using Multiple References,[0],[0]
"Our multiple reference training is achieved by converting a multiple reference dataset to a single reference dataset without losing any information.
",2 Using Multiple References,[0],[0]
"Considering a multiple reference dataset D, where the ith training example, (xi, Yi), includes one source input xi, which is a source sentence in MT or image vector in image captioning, and a reference set Yi = {y1i ,y2i , ...yKi } of K references.",2 Using Multiple References,[0],[0]
"We have the following methods to convert the multiple reference dataset to a single reference dataset D′ (note that the following D′sample one, D ′ uniform and D′shuffle are ordered sets):",2 Using Multiple References,[0],[0]
Sample One,2 Using Multiple References,[0],[0]
:,2 Using Multiple References,[0],[0]
The most straightforward way is to use a different reference in different epochs during training to explore the variances between references.,2 Using Multiple References,[0],[0]
"For each example, we randomly pick one of the K references in each training epoch (note that the random function will be used in each epoch).",2 Using Multiple References,[0],[0]
"This method is commonly used in existing image captioning literatures, such as (Karpathy and FeiFei, 2015), but never used in MT.",2 Using Multiple References,[0],[0]
"This approach
can be formalized as:
D′sample one = |D|⋃ i=1",2 Using Multiple References,[0],[0]
"{(xi,ykii )}, ki = rand(1, ...,K)
Uniform:",2 Using Multiple References,[0],[0]
"Although all references are accessible by using Sample One, it is not guaranteed that all references are used during training.",2 Using Multiple References,[0],[0]
So we introduce Uniform which basically copies xi training example K times and each time with a different reference.,2 Using Multiple References,[0],[0]
"This approach can be formalized as:
D′uniform = |D|⋃ i=1",2 Using Multiple References,[0],[0]
K⋃,2 Using Multiple References,[0],[0]
"k=1 {(xi,yki )}
Shuffle is based on Uniform, but shuffles all the source and reference pairs in random order before each epoch.",2 Using Multiple References,[0],[0]
"So, formally it is:
D′shuffle =",2 Using Multiple References,[0],[0]
"Shuffle(D ′ uniform)
",2 Using Multiple References,[0],[0]
Sample One is supervised by different training signals in different epochs while both Uniform and Shuffle include all the references at one time.,2 Using Multiple References,[0],[0]
Note that we use mini-batch during training.,2 Using Multiple References,[0],[0]
"When we set the batch size equal to the entire training set size in both Uniform and Shuffle, they become equivalent.",2 Using Multiple References,[0],[0]
"In text generation tasks, the given multiple references are only a small portion in the whole space of potential references.",3 Pseudo-References Generation,[0],[0]
"To cover a larger number of references during training, we want to generate more pseudo-references which is similar to existing ones.
",3 Pseudo-References Generation,[0],[0]
"Our basic idea is to compress different references y0,y1, ...,yK into a lattice.",3 Pseudo-References Generation,[0],[0]
We achieve this by merging similar words in the references.,3 Pseudo-References Generation,[0],[0]
"Finally, we generate more pseudo-references by simply traversing the compressed lattice and select those with high quality according to its BLEU score.
",3 Pseudo-References Generation,[0],[0]
"Take the following three references from the NIST Chinese-to-English machine translation dataset as an example:
1.",3 Pseudo-References Generation,[0],[0]
"Indonesia reiterated its opposition to foreign military presence
2.",3 Pseudo-References Generation,[0],[0]
"Indonesia repeats its opposition against station of foreign troops in Indonesia
3.",3 Pseudo-References Generation,[0],[0]
"Indonesia reiterates opposition to
garrisoning foreign armies",3 Pseudo-References Generation,[0],[0]
The simplest way to compress different references into a lattice is to do pairwise reference compression iteratively.,3.1 Naive Idea: Hard Word Alignment,[0],[0]
"At each time, we select two references and merge the same words in them.
",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Considering the previous example, we can derive an initial lattice from the three references as shown in Fig. 1(a).",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Assume that we first do a pairwise reference compression on first two references, we can merge at four sharing words: Indonesia, its, opposition and foreign, and the lattice will turn to Fig. 1(b).",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"If we further compress the first and third references, we can merge at Indonesia, opposition, to and foreign, which gives the lattice Fig. 1(c).",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"By simply traversing the final lattice, 33 new pseudo-references can be generated.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"For example:
1.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Indonesia reiterated its opposition
to garrisoning foreign armies
2.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Indonesia repeats its opposition to foreign military presence
3.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Indonesia reiterates opposition to foreign troops in Indonesia
4. ...
",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"However, this simple hard alignment method (only identical words can be aligned) suffers from two problems:
1.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
Different words may have similar meanings and need to be merged together.,3.1 Naive Idea: Hard Word Alignment,[0],[0]
"For example, in the previous example, reiterated, repeats and reiterates should be merged together.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Similarly, military, troops and armies also have similar meanings.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"If the
lattice can align these words, we can generate the lattice shown in Fig. 1(e) which can generate 213 pseudo-references.
2.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
Identical words may have different meaning in different contexts and should not be merged.,3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Considering the following two references from the COCO image captioning dataset (corresponding picture is shown in Fig. 2):
1.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Two elephants in an enclosure next to a brick building
2.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Two elephants try to fit through a
small entry
Following the previously described algorithm, we can merge the two references at “two elephants”, at “to” and at “a”.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"However, “to” in the two references are very different (it is a preposition in the first reference and an infinitive in the second) and should not be merged.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Thus, the lattice in Fig. 2(b) will generate the following wrong pseudo-references:
1.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Two elephants try to a small entry
2.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"Two elephants in an enclosure next
to fit through a brick building
Therefore, we need to investigate a better method to compress the lattice.",3.1 Naive Idea: Hard Word Alignment,[0],[0]
"To tackle the above listed two problems of hard alignment, we need to identify synonyms and words with similar meanings.",3.2 Measuring Word Similarity in Context,[0],[0]
Barzilay and Lee (2002) utilize an external synonyms dictionary to get the similarity score between words.,3.2 Measuring Word Similarity in Context,[0],[0]
"However, this method ignores the given context of each word.",3.2 Measuring Word Similarity in Context,[0],[0]
"For example, in Fig. 1(a), there are two Indonesia’s in the second path of reference.",3.2 Measuring Word Similarity in Context,[0],[0]
"If we use a synonyms dictionary, both Indonesia tokens will be aligned to the Indonesia in the first
or third sentence with the same score.",3.2 Measuring Word Similarity in Context,[0],[0]
"This incorrect alignment would lead to meaningless lattice.
",3.2 Measuring Word Similarity in Context,[0],[0]
"Thus, we introduce the semantic substitution matrix which measures the semantic similarity of each word pairs in context.",3.2 Measuring Word Similarity in Context,[0],[0]
"Formally, given a sentence pair yi and yj , we build a semantic substitution matrix M = R|yi|×|yj |, whose cell Mu,v represents the similarity score between word yi,u and word yj,v.
We propose a new neural network-based multiple sequence alignment algorithm to take context into consideration.",3.2 Measuring Word Similarity in Context,[0],[0]
"We first build a language model (LM) to obtain the semantic representation of each word, then these word representations are used to construct the semantic substitution matrix between sentences.
",3.2 Measuring Word Similarity in Context,[0],[0]
"Fig. 3 shows the architecture of the bidirectional LM (Mousa and Schuller, 2017).",3.2 Measuring Word Similarity in Context,[0],[0]
"The optimization goal of our LM is to minimize the ith word’s prediction error given the surrounding word’s hidden state:
p(wi | −−→ hi−1 ⊕ ←−− hi+1) (1)
For any new given sentences, we concatenate both forward and backward hidden states to represent each word yi,u in a sentence yi.",3.2 Measuring Word Similarity in Context,[0],[0]
"We then calculate the normalized cosine similarity score of word yi,u and yj,v as:
Mu,v = cosine",3.2 Measuring Word Similarity in Context,[0],[0]
( −→ hu ⊕ ←−,3.2 Measuring Word Similarity in Context,[0],[0]
"hu, −→ hv ⊕ ←− hv) (2)
",3.2 Measuring Word Similarity in Context,[0],[0]
Fig. 4 shows an example of the semantic substitution matrix of first two sentences in example references of Fig. 1(a).,3.2 Measuring Word Similarity in Context,[0],[0]
"With the help of semantic substitution matrix Mu,v which measures pairwise word similarity, we need to find the optimal word alignment to compress references into a lattice.
",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"Unfortunately, this computation is exponential in the number of sequences.",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"Thus, we use iterative pairwise alignment which greedily merges sentence pairs (Durbin et al., 1998).
",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"Based on pairwise substitution matrix we can define an optimal pairwise sequence alignment as an optimal path from M0,0 to M|yi|,|yj |.",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
This is a dynamic programming problem with the state transition function described in Equation (3).,3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
Fig. 5 shows the optimal path according to the semantic substitution matrix in Fig. 4.,3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"There is a gap if the continuous step goes vertical or horizontal, and an alignment if it goes diagonal.
",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"opt(u, v)=  opt(u−1, v−1)+Mu,v opt(u−1, v) opt(u, v−1)
(3)
",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
What order should we follow to do the iterative pairwise word alignment?,3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"Intuitively, we need to compress the most similar reference pair first, since this compression will lead to more aligned words.",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"Following this intuition, we order reference pairs by the maximum alignment score opt(|yi|, |yj |) (i.e. the score of bottom-right cell in Fig. 5) which is the sum of all aligned words.
",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"Using this order, we can iteratively merge each sentence pair in descending order, unless both the sentences have already been merged (this will prevent generating a cyclic lattice).
",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"Since the semantic substitution matrix Mu,v, defined as a normalized cosine similarity, scales in (0, 1), it’s very likely for the DP algorithm to align unrelated words.",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"To tackle this problem, we deduct a global penalty p from each cell of Mu,v. With the global penalty p, the DP algorithm will not align a word pair (yi,u,yi,v) unless Mu,v ≥ p.
After the pairwise references alignment, we merge those aligned words.",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"For example, in Fig. 1, after we generate an initial lattice as shown in Fig. 1(a), we then calculate the maximum alignment score of all sentence pairs.",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"After that, the lattice turns into Fig. 1(d) by merging the first two references (assuming they have the highest score) according to pairwise alignment shown in Fig. 5.",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
Then we pick the sentence pair with next highest alignment score (assuming it’s the last two sentences).,3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
"Similar to the previous step, we find alignments according to the dynamic programming and merge to the final lattice (see Fig. 1(e)).",3.3 Iterative Pairwise Word Alignment using Dynamic Programming,[0],[0]
We generate pseudo-references by simply traversing the generated lattice.,3.4 Traverse Lattice and Pseudo-References Selection by BLEU,[0],[0]
"For example, if we traverse the final lattice shown in Fig. 1(e), we can generate 213 pseudo-refrences in total.
",3.4 Traverse Lattice and Pseudo-References Selection by BLEU,[0],[0]
"Then, we can put those generated pseudoreferences to expand the training dataset.",3.4 Traverse Lattice and Pseudo-References Selection by BLEU,[0],[0]
"To balance the number of generated pseudo-references for each example, we force the total number of pseudo-references from each example to be
K ′. For those examples generating k pseudoreferences and k > K ′, we calculate all pseudoreferences’ BLEU scores based on gold references, and only keep top K ′−k pseudo-references with highest BLEU score.",3.4 Traverse Lattice and Pseudo-References Selection by BLEU,[0],[0]
"To investigate the empirical performances of our proposed algorithm, we conduct experiments on machine translation and image captioning.",4 Experiments,[0],[0]
"We evaluate our approach on NIST Chinese-toEnglish translation dataset which consists of 1M pairs of single reference data and 5974 pairs of 4 reference data (NIST 2002, 2003, 2004, 2005, 2006, 2008).",4.1 Machine Translation,[0],[0]
Table 1 shows the statistics of this dataset.,4.1 Machine Translation,[0],[0]
"We first pre-train our model on a 1M pairs single reference dataset and then train on the NIST 2002, 2003, 2004, 2005.",4.1 Machine Translation,[0],[0]
"We use the NIST 2006
dataset as validation set and NIST 2008 as test sets.
",4.1 Machine Translation,[0],[0]
Fig. 6(a) analyzes the number and quality of generated references using our proposed approach.,4.1 Machine Translation,[0],[0]
We set the global penalty as 0.9 and only calculate the top 50 generated references for the average BLEU analysis.,4.1 Machine Translation,[0],[0]
"From the figure, we can see that when the sentence length grows, the number of generated references grows exponentially.",4.1 Machine Translation,[0],[0]
"To generate enough references for the following experiments, we set an initial global penalty as 0.9 and gradually decrease it by 0.05 until we collect no less than 100 references.",4.1 Machine Translation,[0],[0]
"We train a bidirectional language model on the pre-training dataset and training dataset with Glove (Pennington et al., 2014) word embedding size of 300 dimension, for 20 epochs to minimize the perplexity
We employ byte-pair encoding (BPE) (Sennrich et al., 2015) which reduces the source and target language vocabulary sizes to 18k and 10k.",4.1 Machine Translation,[0],[0]
"We adopt length reward (Huang et al., 2017) to find optimal sentence length.",4.1 Machine Translation,[0],[0]
We use a two layer bidirectional LSTM as the encoder and a two layer LSTM as the decoder.,4.1 Machine Translation,[0],[0]
"We perform pre-training for 20 epochs to minimize perplexity on the 1M dataset, with a batch size of 64, word embedding size of 500, beam size of 15, learning rate of 0.1, learning rate decay of 0.5 and dropout rate of 0.3.",4.1 Machine Translation,[0],[0]
"We then train the model in 30 epochs and use the best batch size among 100, 200, 400 for each update method.",4.1 Machine Translation,[0],[0]
"These batch sizes are multiple of the number of references used in experiments, so it is guaranteed that all the references of one single example are in one batch for the Uniform method.",4.1 Machine Translation,[0],[0]
The learning rate is set as 0.01 and learning rate decay as 0.75.,4.1 Machine Translation,[0],[0]
"We do each experiment three times and report the average result.
",4.1 Machine Translation,[0],[0]
Table 2 shows the translation quality on the devset of machine translation task.,4.1 Machine Translation,[0],[0]
"Besides the original 4 references in the training set, we generate another four dataset with 10, 20, 50 and 100 references including pseudo-references using hard word alignment and soft word alignment.",4.1 Machine Translation,[0],[0]
"We compare the three update methods (Sample One, Uniform, Shuffle) with always using the first reference (First).",4.1 Machine Translation,[0],[0]
All results of soft word alignment are better than corresponding hard word alignment results and the best result is achieved with 50 references using Uniform and soft word alignment.,4.1 Machine Translation,[0],[0]
"According to Table 3, Shuffle with original 4 references has +0.7 BLEU improvement and Uniform
with 50 references has +1.5 BLEU improvement.",4.1 Machine Translation,[0],[0]
"From Fig. 7(b), we can see that using the Sample One method, the translation quality drops dramatically with more than 10 references.",4.1 Machine Translation,[0],[0]
This may be due to the higher variance of used reference in each epoch.,4.1 Machine Translation,[0],[0]
"For the image captioning task, we use the widelyused MSCOCO image captioning dataset.",4.2 Image Captioning,[0],[0]
"Following prior work, we use the Kapathy split (Karpathy and Fei-Fei, 2015).",4.2 Image Captioning,[0],[0]
Table 1 shows the statistics of this dataset.,4.2 Image Captioning,[0],[0]
"We use Resnet (He et al., 2016) to extract image feature of 2048 feature size and simple fully connected layer of size 512 to an LSTM de-
coder.",4.2 Image Captioning,[0],[0]
We train every model for 100 epochs and calculate the BLEU score on validation set and select the best model.,4.2 Image Captioning,[0],[0]
"For every update method, we find the optimal batch size among 50, 250, 500, 1000",4.2 Image Captioning,[0],[0]
"and we use a beam size of 5.
Fig. 6(b) analyzes the correlation between average references length with the number and quality of generated references.",4.2 Image Captioning,[0],[0]
We set global penalty as 0.6 (which is also adopted for the generated references in the following experiments) and calculate the top 50 generated references for the average BLEU analysis.,4.2 Image Captioning,[0],[0]
"Since the length of original references is much shorter than the previous machine translation dataset, it has worse quality and fewer generated references.
",4.2 Image Captioning,[0],[0]
Table 4 shows that the best result is achieved with 20 references using Shuffle.,4.2 Image Captioning,[0],[0]
"This result is
different from the result of machine translation task where Uniform method is the best.",4.2 Image Captioning,[0],[0]
This may be because the references in image captioning dataset are much more diverse than those in machine translation dataset.,4.2 Image Captioning,[0],[0]
Different captions of one image could even talk about different aspects.,4.2 Image Captioning,[0],[0]
"When using the Uniform method, the high variance of references in one batch may harm the model and lead to worse text generation quality.",4.2 Image Captioning,[0],[0]
"Table 5 shows that it outperforms Sample One with 4 original references, which is adopted in previous work (Karpathy and Fei-Fei, 2015), +3.1 BLEU score and +11.7 CIDEr.",4.2 Image Captioning,[0],[0]
Fig. 6 shows a training example in the COCO dataset and its corresponding generated lattice and pseudo-references which is sorted according to its BLEU score.,4.3 Case Study,[0],[0]
Our proposed algorithm generates 73724 pseudo-references in total.,4.3 Case Study,[0],[0]
All the top 50 pseudo-references’ BLEU scores are above 97.1 and the top three even achieve 100.0 BLEU score though they are not identical to any original references.,4.3 Case Study,[0],[0]
"Although the BLEU of last two sentences is 0.0, they are still valid to describe this picture.",4.3 Case Study,[0],[0]
"We introduce several multiple-reference training methods and a neural-based lattice compression framework, which can generate more training references based on existing ones.",5 Conclusions,[0],[0]
Our proposed framework outperforms the baseline models on both MT and image captioning tasks.,5 Conclusions,[0],[0]
"This work was supported in part by DARPA grant N66001-17-2-4030, and NSF grants IIS1817231 and IIS-1656051.",Acknowledgments,[0],[0]
We thank the anonymous reviewers for suggestions and Juneki Hong for proofreading.,Acknowledgments,[0],[0]
"Neural text generation, including neural machine translation, image captioning, and summarization, has been quite successful recently.",abstractText,[0],[0]
"However, during training time, typically only one reference is considered for each example, even though there are often multiple references available, e.g., 4 references in NIST MT evaluations, and 5 references in image captioning data.",abstractText,[0],[0]
We first investigate several different ways of utilizing multiple human references during training.,abstractText,[0],[0]
"But more importantly, we then propose an algorithm to generate exponentially many pseudo-references by first compressing existing human references into lattices and then traversing them to generate new pseudo-references.",abstractText,[0],[0]
These approaches lead to substantial improvements over strong baselines in both machine translation (+1.5 BLEU) and image captioning (+3.1 BLEU / +11.7 CIDEr).,abstractText,[0],[0]
Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 833–844 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
833",text,[0],[0]
"Personal devices that interact with users via natural language conversation are becoming ubiquitous (e.g., Siri, Alexa), however, very little of that conversation today allows the user to teach, and then query, new knowledge.",1 Introduction,[0],[0]
"Most of the focus in
these personal devices has been on Question Answering (QA) over general world-knowledge (e.g., “who was the president in 1980” or “how many ounces are in a cup”).",1 Introduction,[0],[0]
"These devices open a new and exciting possibility of enabling end-users to teach machines in natural language, e.g., by expressing the state of their personal world to its virtual assistant (e.g., via narrative about people and events in that user’s life) and enabling the user to ask questions over that personal knowledge (e.g., “which engineers in the QC team were involved in the last meeting with the director?”).
",1 Introduction,[0],[0]
"This type of questions highlight a unique blend of two conventional streams of research in Question Answering (QA) – QA over structured sources such as knowledge bases (KBs), and QA over unstructured sources such as free text.",1 Introduction,[0],[0]
"This blend is a natural consequence of our problem setting: (i) users may choose to express rich relational knowledge about their world, in turn enabling them to pose complex composi-
1.",1 Introduction,[0],[0]
There is an associate professor named Andy 2.,1 Introduction,[0],[0]
He returned from a sabbatical 3.,1 Introduction,[0],[0]
This professor currently has funding 4.,1 Introduction,[0],[0]
There is a masters level course called G301 5.,1 Introduction,[0],[0]
That course is taught by him 6.,1 Introduction,[0],[0]
"That class is part of the mechanical
engineering department 7.",1 Introduction,[0],[0]
Roslyn is a student in this course 8.,1 Introduction,[0],[0]
U203 is a undergraduate level course 9.,1 Introduction,[0],[0]
"Peggy and that student are TAs for this
course
…",1 Introduction,[0],[0]
What students are advised by a professor with funding?,1 Introduction,[0],[0]
"[Albertha, Roslyn, Peggy, Lucy, Racquel]
What assistant professors advise students who passed their thesis proposal?",1 Introduction,[0],[0]
"[Andy]
Which courses have masters student TAs?",1 Introduction,[0],[0]
"[G301, U101 ]
Who are the professors working on unsupervised machine learning?",1 Introduction,[0],[0]
"[Andy, Hanna]",1 Introduction,[0],[0]
7.,6. Andrew is no longer assigned to that project,[0],[0]
"That developer resolved the changelog needs
to be added issue
…
Are there any developers assigned to projects in the evaluation stage?",6. Andrew is no longer assigned to that project,[0],[0]
"[Tawnya, Charlott, Hiram]
Who is the null pointer exception during parsing issue assigned to?",6. Andrew is no longer assigned to that project,[0],[0]
"Hiram
Are there any issues that are resolved for experimental projects?",6. Andrew is no longer assigned to that project,[0],[0]
"[saving data throws exception, wrong pos tag on consecutive words]
Academic Department World Software Engineering World
Figure 2:",6. Andrew is no longer assigned to that project,[0],[0]
Illustrative snippets from two sample worlds.,6. Andrew is no longer assigned to that project,[0],[0]
"We aim to generate natural-sounding first-person narratives from five diverse worlds, covering a range of different events, entities and relations.
tional queries (e.g., “all CS undergrads who took my class last semester”), while at the same time (ii) personal knowledge generally evolves through time and has an open and growing set of relations, making natural language the only practical interface for creating and maintaining that knowledge by non-expert users.",6. Andrew is no longer assigned to that project,[0],[0]
"In short, the task that we address in this work is: multi-relational question answering from dynamic knowledge expressed via narrative.
",6. Andrew is no longer assigned to that project,[0],[0]
"Although we hypothesize that questionanswering over personal knowledge of this sort is ubiquitous (e.g., between a professor and their administrative assistant, or even if just in the user’s head), such interactions are rarely recorded, presenting a significant practical challenge to collecting a sufficiently large real-world dataset of this type.",6. Andrew is no longer assigned to that project,[0],[0]
"At the same time, we hypothesize that the technical challenges involved in developing models for relational question answering from narrative would not be fundamentally impacted if addressed via sufficiently rich, but controlled simulated narratives.",6. Andrew is no longer assigned to that project,[0],[0]
"Such simulations also offer the advantage of enabling us to directly experiment with stories and queries of different complexity, potentially offering additional insight into the fundamental challenges of this task.
",6. Andrew is no longer assigned to that project,[0],[0]
"While our problem setting blends the problems
of relational question answering over knowledge bases and question answering over text, our hypothesis is that end-to-end QA models may learn to answer such multisentential relational queries, without relying on an intermediate knowledge base representation.",6. Andrew is no longer assigned to that project,[0],[0]
"In this work, we conduct an extensive evaluation of a set of state-of-the-art end-to-end QA models on our task and analyze their results.",6. Andrew is no longer assigned to that project,[0],[0]
Question answering has been mainly studied in two different settings: KB-based and text-based.,2 Related Work,[0],[0]
"KB-based QA mostly focuses on parsing questions to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2012; Berant et al., 2013; Kwiatkowski et al., 2013; Yih et al., 2015) in order to better retrieve answer candidates from a knowledge base.",2 Related Work,[0],[0]
Text-based QA aims to directly answer questions from the input text.,2 Related Work,[0],[0]
"This includes works on early information retrieval-based methods (Banko et al., 2002; Ahn et al., 2004) and methods that build on extracted structured representations from both the question and the input text (Sachan et al., 2015; Sachan and Xing, 2016; Khot et al., 2017; Khashabi et al., 2018b).",2 Related Work,[0],[0]
"Although these structured presentations make reasoning more effective, they rely on sophisticated
NLP pipelines and suffer from error propagation.",2 Related Work,[0],[0]
"More recently, end-to-end neural architectures have been successfully applied to textbased QA, including Memory-augmented neural networks (Sukhbaatar et al., 2015; Miller et al., 2016; Kumar et al., 2016) and attention-based neural networks (Hermann et al., 2015; Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2017; Xiong et al., 2017; Seo et al., 2017; Chen et al., 2017).",2 Related Work,[0],[0]
"In this work, we focus on QA over text (where the text is generated from a supporting KB) and evaluate several state-of-the-art memoryaugmented and attention-based neural architectures on our QA task.",2 Related Work,[0],[0]
"In addition, we consider a sequence-to-sequence model baseline (Bahdanau et al., 2015), which has been widely used in dialog (Vinyals and Le, 2015; Ghazvininejad et al., 2017) and recently been applied to generating answer values from Wikidata (Hewlett et al., 2016).
",2 Related Work,[0],[0]
There are numerous datasets available for evaluating the capabilities of QA systems.,2 Related Work,[0],[0]
"For example, MCTest (Richardson et al., 2013) contains comprehension questions for fictional stories.",2 Related Work,[0],[0]
"Allen AI Science Challenge (Clark, 2015) contains science questions that can be answered with knowledge from text books.",2 Related Work,[0],[0]
"RACE (Lai et al., 2017) is an English exam dataset for middle and high school Chinese students.",2 Related Work,[0],[0]
"MULTIRC (Khashabi et al., 2018a) is a dataset that focuses on evaluating multi-sentence reasoning skills.",2 Related Work,[0],[0]
"These datasets all require humans to carefully design multiplechoice questions and answers, so that certain aspects of the comprehension and reasoning capabilities are properly evaluated.",2 Related Work,[0],[0]
"As a result, it is difficult to collect them at scale.",2 Related Work,[0],[0]
"Furthermore, as the knowledge required for answering each question is not clearly specified in these datasets, it can be hard to identify the limitations of QA systems and propose improvements.
",2 Related Work,[0],[0]
Weston et al. (2015) proposes to use synthetic QA tasks (the BABI dataset) to better understand the limitations of QA systems.,2 Related Work,[0],[0]
"BABI builds on a simulated physical world similar to interactive fiction (Montfort, 2005) with simple objects and relations and includes 20 different reasoning tasks.",2 Related Work,[0],[0]
"Various types of end-to-end neural networks (Sukhbaatar et al., 2015; Lee et al., 2015; Peng et al., 2015) have demonstrated promising accuracies on this dataset.",2 Related Work,[0],[0]
"However, the performance can hardly translate to real-world QA datasets, as BABI uses a small vocabulary (150
words) and short sentences with limited language variations (e.g., nesting sentences, coreference).",2 Related Work,[0],[0]
"A more sophisticated QA dataset with a supporting KB is WIKIMOVIES (Miller et al., 2016), which contains 100k questions about movies, each of them is answerable by using either a KB or a Wikipedia article.",2 Related Work,[0],[0]
"However, WIKIMOVIES is highly domain-specific, and similar to BABI, the questions are designed to be in simple forms with little compositionality and hence limit the difficulty level of the tasks.
",2 Related Work,[0],[0]
"Our dataset differs in the above datasets in that (i) it contains five different realistic domains permitting cross-domain evaluation to test the ability of models to generalize beyond a fixed set of KB relations, (ii) it exhibits rich referring expressions and linguistic variations (vocabulary much larger than the BABI dataset), (iii) questions in our dataset are designed to be deeply compositional and can cover multiple relations mentioned across multiple sentences.
",2 Related Work,[0],[0]
"Other large-scale QA datasets include Clozestyle datasets such as CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2015), and Who Did What (Onishi et al., 2016); datasets with answers being spans in the document, such as SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and TriviaQA (Joshi et al., 2017); and datasets with human generated answers, for instance, MS MARCO (Nguyen et al., 2016) and SearchQA",2 Related Work,[0],[0]
"(Dunn et al., 2017).",2 Related Work,[0],[0]
One common drawback of these datasets is the difficulty in accessing a system’s capability of integrating information across a document context.,2 Related Work,[0],[0]
"Kočiskỳ et al. (2017) recently emphasized this issue and proposed NarrativeQA, a dataset of fictional stories with questions that reflect the complexity of narratives: characters, events, and evolving relations.",2 Related Work,[0],[0]
"Our dataset contains similar narrative elements, but it is created with a supporting KB and hence it is easier to analyze and interpret results in a controlled setting.",2 Related Work,[0],[0]
"In this work, we synthesize narratives in five diverse worlds, each containing a thousand narratives and where each narrative describes the evolution of a simulated user’s world from a firstperson perspective.",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"In each narrative, the simu-
lated user may introduce new knowledge, update existing knowledge or express a state change (e.g., “Homework 3 is now due on Friday” or “Samantha passed her thesis defense”).",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"Each narrative is interleaved with questions about the current state of the world, and questions range in complexity depending on the amount of knowledge that needs to be integrated to answer them.",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"This allows us to benchmark a range of QA models at their ability to answer questions that require different extents of relational reasoning to be answered.
",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"The set of worlds that we simulate as part of this work are as follows:
1.",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
MEETING WORLD:,3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"This world describes situations related to professional meetings, e.g., meetings being set/cancelled, people attending meetings, topics of meetings.
2.",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
HOMEWORK WORLD:,3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"This world describes situations from the first-person perspective of a student, e.g., courses taken, assignments in different courses, deadlines of assignments.
3.",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
SOFTWARE ENGINEERING WORLD:,3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"This world describes situations from the first-person perspective of a software development manager, e.g., task assignment to different project team members, stages of software development, bug tickets.
4.",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
ACADEMIC DEPARTMENT WORLD:,3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"This world describes situations from the first-person perspective of a professor, e.g., teaching assignments, faculty going/returning from sabbaticals, students from different departments taking/dropping courses.
5.",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
SHOPPING WORLD:,3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"This world describes situations about a person shopping for various occasions, e.g., adding items to a shopping list, purchasing items at different stores, noting where items are on sale.",3 TEXTWORLDS: Simulated Worlds for Multi-Relational QA from Narratives,[0],[0]
"Each world is represented by a set of entities E and a set of unary, binary or ternary relations R. Formally, a single step in one simulation of a world involves a combination of instantiating new entities and defining new (or mutating existing) relations between entities.",3.1 Narrative,[0],[0]
"Practically, we implement each world as a collection of classes and
methods, with each step of the simulation creating or mutating class instances by sampling entities and methods on those entities.",3.1 Narrative,[0],[0]
"By design, these classes and methods are easy to extend, to either enrich existing worlds or create new ones.",3.1 Narrative,[0],[0]
"Each simulation step is then expressed as a natural language statement, which is added to the narrative.",3.1 Narrative,[0],[0]
"In the process of generating a natural language expression, we employ a rich mechanism for generating anaphora, such as “meeting with John about the performance review” and “meeting that I last added”, in addition to simple pronoun references.",3.1 Narrative,[0],[0]
This allows us to generate more natural and flowing narratives.,3.1 Narrative,[0],[0]
"These references are generated and composed automatically by the underlying TEXTWORLDS framework, significantly reducing the effort needed to build new worlds.",3.1 Narrative,[0],[0]
"Furthermore, all generated stories also provide additional annotation that maps all entities to underlying gold-standard KB ids, allowing to perform experiments that provide models with different degrees of access to the “simulation oracle”.
",3.1 Narrative,[0],[0]
"We generate 1,000 narratives within each world, where each narrative consists of 100 sentences, plus up to 300 questions interleaved randomly within the narrative.",3.1 Narrative,[0],[0]
See Figure 1 for two example narratives.,3.1 Narrative,[0],[0]
"Each story in a given world samples its entities from a large general pool of entity names collected from the web (e.g., people names, university names).",3.1 Narrative,[0],[0]
"Although some entities do overlap between stories, each story in a given world contains a unique flow of events and entities involved in those events.",3.1 Narrative,[0],[0]
See Table 1 for the data statistics.,3.1 Narrative,[0],[0]
"Formally, questions are queries over the knowledge-base in the state defined up to the point when the question is asked in the narrative.",3.2 Questions,[0],[0]
"In the narrative, the questions are expressed
in natural language, employing the same anaphora mechanism used in generating the narrative (e.g., “who is attending the last meeting I added?”).
",3.2 Questions,[0],[0]
"We categorize generated questions into four types, reflecting the number and types of facts required to answer them; questions that require more facts to answer are typically more compositional in nature.",3.2 Questions,[0],[0]
"We categorize each question in our dataset into one of the following four categories:
Single Entity/Single Relation Answers to these questions are a single entity, e.g. “what is John’s email address?”, or expressed in lambda-calculus notation:
λx.EmailAddress(John, x)
",3.2 Questions,[0],[0]
"The answers to these questions are found in a single sentence in the narrative, although it is possible that the answer may change through the course of the narrative (e.g., “John’s new office is GHC122”).
",3.2 Questions,[0],[0]
"Multi-Entity/Single Relation Answers to these questions can be multiple entities but involve a single relation, e.g., “Who is enrolled in the Math class?”, or expressed in lambda calculus notation:
λx.TakingClass(x, Math)
",3.2 Questions,[0],[0]
"Unlike the previous category, answers to these questions can be sets of entities.
",3.2 Questions,[0],[0]
Multi-Entity/Two Relations,3.2 Questions,[0],[0]
"Answers to these questions can be multiple entities and involve two relations, e.g., “Who is enrolled in courses that I am teaching?”, or expressed in lambda calculus:
λx.∃y.",3.2 Questions,[0],[0]
"EnrolledInClass(x, y) ∧ CourseTaughtByMe(y)
",3.2 Questions,[0],[0]
"Multi-Entity/Three Relations Answers to these questions can be multiple entities and involve three relations, e.g., “Which undergraduates are
enrolled in courses that I am teaching?”, or expressed in lambda calculus notation:
λx.∃y.",3.2 Questions,[0],[0]
"EnrolledInClass(x, y) ∧ CourseTaughtByMe(y) ∧ Undergrad(x)
",3.2 Questions,[0],[0]
"In the data that we generate, answers to questions are always sets of spans in the narrative (the reason for this constraint is for easier evaluation of several existing machine-reading models; this assumption can easily be relaxed in the simulation).",3.2 Questions,[0],[0]
"In all of our evaluations, we will partition our results by one of the four question categories listed above, which we hypothesize correlates with the difficulty of a question.",3.2 Questions,[0],[0]
"We develop several baselines for our QA task, including a logistic regression model and four different neural network models: Seq2Seq (Bahdanau et al., 2015), MemN2N (Sukhbaatar et al., 2015), BiDAF (Seo et al., 2017), and DrQA (Chen et al., 2017).",4 Methods,[0],[0]
"These models generate answers in different ways, e.g., predicting a single entity, predicting spans of text, or generating answer sequences.",4 Methods,[0],[0]
"Therefore, we implement two experimental settings: ENTITY and RAW.",4 Methods,[0],[0]
"In the ENTITY setting, given a question and a story, we treat all the entity spans in the story as candidate answers, and the prediction task becomes a classification problem.",4 Methods,[0],[0]
"In the RAW setting, a model needs to predict the answer spans.",4 Methods,[0],[0]
"For logistic regression and MemN2N, we adopt the ENTITY setting as they are naturally classification models.",4 Methods,[0],[0]
This ideally provides an upper bound on the performance when considering answer candidate generation.,4 Methods,[0],[0]
"For all the other models, we can apply the RAW setting.",4 Methods,[0],[0]
"The logistic regression baseline predicts the likelihood of an answer candidate being a true answer.
",4.1 Logistic Regression,[0],[0]
"For each answer candidate e and a given question, we extract the following features: (1) The frequency of e in the story; (2) The number of words within e; (3) Unigrams and bigrams within e; (4) Each non-stop question word combined with each non-stop word within e; (5) The average minimum distance between each non-stop question word and e in the story; (6) The common words (excluding stop words) between the question and the text surrounding of e (within a window of 10 words); (7) Sum of the frequencies of the common words to the left of e, to the right e, and both.",4.1 Logistic Regression,[0],[0]
These features are designed to help the model pick the correct answer spans.,4.1 Logistic Regression,[0],[0]
"They have shown to be effective for answer prediction in previous work (Chen et al., 2016; Rajpurkar et al., 2016).
",4.1 Logistic Regression,[0],[0]
We associate each answer candidate with a binary label indicating whether it is a true answer.,4.1 Logistic Regression,[0],[0]
We train a logistic regression classifier to produce a probability score for each answer candidate.,4.1 Logistic Regression,[0],[0]
"During test, we search for an optimal threshold that maximizes the F1 performance on the validation data.",4.1 Logistic Regression,[0],[0]
"During training, we optimize the cross-entropy loss using Adam (Kingma and Ba, 2014) with an initial learning rate of 0.01.",4.1 Logistic Regression,[0],[0]
"We use a batch size of 10, 000 and train with 5 epochs.",4.1 Logistic Regression,[0],[0]
Training takes roughly 10 minutes for each domain on a Titan X GPU.,4.1 Logistic Regression,[0],[0]
"The seq2seq model is based on the sequence to sequence model presented in (Bahdanau et al., 2015), which includes an attention model.",4.2 Seq2Seq,[0],[0]
"Bahdanau et al. (Bahdanau et al., 2015) have used this model to build a neural based machine translation performing at the state-of-the-art.",4.2 Seq2Seq,[0],[0]
"We adopt this model to fit our own domain by including a preprocessing step in which all statements are concatenated with a dedicated token, while eliminating all previously asked questions, and the current question is added at the end of the list of statements.",4.2 Seq2Seq,[0],[0]
The answers are treated as a sequence of words.,4.2 Seq2Seq,[0],[0]
"We use word embeddings (Zou et al., 2013), as it was shown to improve accuracy.",4.2 Seq2Seq,[0],[0]
"We use 3 GRU (Cho et al., 2014) connected layers, each with a capacity of 256.",4.2 Seq2Seq,[0],[0]
Our batch size was set to 16.,4.2 Seq2Seq,[0],[0]
"We use gradient descent with an initial learning rate of 0.5 and a decay factor of 0.99, iterating on the data for 50, 000 steps (5 epochs).",4.2 Seq2Seq,[0],[0]
The training process for each domain took approximately 48 hours on a Titan X GPU.,4.2 Seq2Seq,[0],[0]
"End-To-End Memory Network (MemN2N) is a neural architecture that encodes both long-term and short-term context into a memory and iteratively reads from the memory (i.e., multiple hops) relevant information to answer a question (Sukhbaatar et al., 2015).",4.3 MemN2N,[0],[0]
"It has been shown to be effective for a variety of question answering tasks (Weston et al., 2015; Sukhbaatar et al., 2015; Hill et al., 2015).
",4.3 MemN2N,[0],[0]
"In this work, we directly apply MemN2N to our task with a small modification.",4.3 MemN2N,[0],[0]
"Originally, MemN2N was designed to produce a single answer for a question, so at the prediction layer, it uses softmax to select the best answer from the answer candidates.",4.3 MemN2N,[0],[0]
"In order to account for multiple answers for a given question, we modify the prediction layer to apply the logistic function and optimize the cross entropy loss instead.",4.3 MemN2N,[0],[0]
"For training, we use the parameter setting as in a publicly available MemN2N 1 except that we set the embedding size to 300 instead of 20.",4.3 MemN2N,[0],[0]
We train the model for 100 epochs and it takes about 2 hours for each domain on a Titan X GPU.,4.3 MemN2N,[0],[0]
"BiDAF (Bidirectional Attention Flow Networks) (Seo et al., 2017) is one of the topperforming models on the span-based question answering dataset SQuAD",4.4 BiDAF-M,[0],[0]
"(Rajpurkar et al., 2016).",4.4 BiDAF-M,[0],[0]
"We reimplement BiDAF with simplified parameterizations and change the prediction layer so that it can predict multiple answer spans.
",4.4 BiDAF-M,[0],[0]
"Specifically, we encode the input story {x1, ..., xT } and a given question {q1, ..., qJ} at the character level and the word level, where the character level uses CNNs and the word level uses pre-trained word vectors.",4.4 BiDAF-M,[0],[0]
The concatenation of the character and word embeddings are passed to a bidirectional LSTM to produce a contextual embedding for each word in the story context and in the question.,4.4 BiDAF-M,[0],[0]
"Then, we apply the same bidirectional attention flow layer to model the interactions between the context and question embeddings, producing question-aware feature vectors for each word in the context, denoted as G ∈ Rdg×T .",4.4 BiDAF-M,[0],[0]
"G is then fed into a bidirectional LSTM layer to obtain a feature matrix M1 ∈ Rd1×T for predicting the start offset of the answer span, and M1 is then passed into
1https://github.com/domluna/memn2n
another bidirectional LSTM layer to obtain a feature matrix M2 ∈ Rd2×T for predicting the end offset of the answer span.",4.4 BiDAF-M,[0],[0]
We then compute two probability scores for each word i in the narrative: pstart = sigmoid(wT1,4.4 BiDAF-M,[0],[0]
[G;M1]) and pend = sigmoid(wT2,4.4 BiDAF-M,[0],[0]
"[G;M1;M2]), where w1 and w2 are trainable weights.",4.4 BiDAF-M,[0],[0]
"The training objective is simply the sum of cross-entropy losses for predicting the start and end indices.
",4.4 BiDAF-M,[0],[0]
"We use 50 1D filters for CNN character embedding, each with a width of 5.",4.4 BiDAF-M,[0],[0]
The word embedding size is 300 and the hidden dimension for LSTMs is 128.,4.4 BiDAF-M,[0],[0]
"For optimization, we use Adam (Kingma and Ba, 2014) with an initial learning rate of 0.001, and use a minibatch size of 32 for 15 epochs.",4.4 BiDAF-M,[0],[0]
The training process takes roughly 20 hours for each domain on a Titan X GPU.,4.4 BiDAF-M,[0],[0]
"DrQA (Chen et al., 2017) is an open-domain QA system that has demonstrated strong performance on multiple QA datasets.",4.5 DrQA-M,[0],[0]
We modify the Document Reader component of DrQA and implement it in a similar framework as BiDAF-M for fair comparisons.,4.5 DrQA-M,[0],[0]
"First, we employ the same character-level and word-level encoding layers to both the input story and a given question.",4.5 DrQA-M,[0],[0]
We then use the concatenation of the character and word embeddings as the final embeddings for words in the story and in the question.,4.5 DrQA-M,[0],[0]
"We compute the aligned question embedding (Chen et al., 2017) as a feature vector for each word in the story and concatenate it with the story word embedding and pass it into a bidirectional LSTM to obtain the contextual embeddings E ∈ Rd×T for words in the story.",4.5 DrQA-M,[0],[0]
"Another bidirectional LSTM is used to obtain the contextual embeddings for the question, and self-
attention is used to compress them into one single vector q ∈ Rd.",4.5 DrQA-M,[0],[0]
"The final prediction layer uses a bilinear term to compute scores for predicting the start offset: pstart = sigmoid(qTW1E) and another bilinear term for predicting the end offset: pend = sigmoid(qTW2E), where W1 and W2 are trainable weights.",4.5 DrQA-M,[0],[0]
"The training loss is the same as in BiDAF-M, and we use the same parameter setting.",4.5 DrQA-M,[0],[0]
Training takes roughly 10 hours for each domain on a Titan X GPU.,4.5 DrQA-M,[0],[0]
We use two evaluation settings for measuring performance at this task: within-world and acrossworld.,5 Experiments,[0],[0]
"In the within-world evaluation setting, we test on the same world that the model was trained on.",5 Experiments,[0],[0]
"We then compute the precision, recall and F1 for each question and report the macro-average F1 score for questions in each world.",5 Experiments,[0],[0]
"In the acrossworld evaluation setting, the model is trained on four out of the five worlds, and tested on the remaining world.",5 Experiments,[0],[0]
"The across-world regime is obviously more challenging, as it requires the model to be able to learn to generalize to unseen relations and vocabulary.",5 Experiments,[0],[0]
"We consider the across-world evaluation setting to be the main evaluation criteria for any future models used on this dataset, as it mimics the practical requirement of any QA system used in personal assistants: it has to be able to answer questions on any new domain the user introduces to the system.",5 Experiments,[0],[0]
We draw several important observations from our results.,5.1 Results,[0],[0]
"First, we observe that more compositional questions (i.e., those that integrate multiple relations) are more challenging for most models - as
all models (except Seq2seq) decrease in performance with the number of relations composed in a question (Figure 5.1).",5.1 Results,[0],[0]
"This can be in part explained by the fact that more composition questions are typically longer, and also require the model to integrate more sources of information in the narrative in order to answer them.",5.1 Results,[0],[0]
One surprising observation from our results is that the performance on questions that ask about a single relation and have only a single answer is lower than questions that ask about a single relation but that can have multiple answers (see detailed results in the Appendix).,5.1 Results,[0],[0]
"This is in part because questions that can have multiple answers typically have canonical entities as answers (e.g., person’s name), and these entities generally repeat in the text, making it easier for the model to find the correct answer.
",5.1 Results,[0],[0]
Table 3 reports the overall (macro-average) F1 scores for different baselines.,5.1 Results,[0],[0]
We can see that BiDAF-M and DrQA-M perform surprisingly well in the within-world evaluation even though they do not use any entity span information.,5.1 Results,[0],[0]
"In particular, DrQA-M outperforms BiDAF-M which suggests that modeling question-context interactions using simple bilinear terms have advantages over using more complex bidirectional attention flows.",5.1 Results,[0],[0]
The lower performance of MemN2N suggests that its effectiveness on the BABI dataset does not directly transfer to our dataset.,5.1 Results,[0],[0]
Note that the original MemN2N architecture uses simple bag-of-words and position encoding for sentences.,5.1 Results,[0],[0]
"This may work well on dataset with a simple vocabulary, for example, MemN2N performs the best in the SOFTWARE world as the SOFTWARE world has
a smaller vocabulary compared to other worlds.",5.1 Results,[0],[0]
"In general, we believe that better text representations for questions and narratives can lead to improved performance.",5.1 Results,[0],[0]
Seq2Seq model also did not perform as well.,5.1 Results,[0],[0]
This is due to the inherent difficulty of generation and encoding long sequences.,5.1 Results,[0],[0]
We found that it performs better when training and testing on shorter stories (limited to 30 statements).,5.1 Results,[0],[0]
"Interestingly, the logistic regression baseline performs on a par with MemN2N, but there is still a large performance gap to BiDAF-M and DrQA-M, and the gap is greater for questions that compose multiple relations.
",5.1 Results,[0],[0]
"In the across-world setting, the performance of all methods dramatically decreases.2 This suggests the limitations of these methods in generalizing to unseen relations and vocabulary.",5.1 Results,[0],[0]
The span-based models BiDAF-M and DrQA-M have an advantage in this setting as they can learn to answer questions based on the alignment between the question and the narrative.,5.1 Results,[0],[0]
"However, the low performance still suggests their limitations in transferring question answering capabilities.",5.1 Results,[0],[0]
"In this work, we have taken the first steps towards the task of multi-relational question answering expressed through personal narrative.",6 Conclusion,[0],[0]
Our hypothesis is that this task will become increasingly important as users begin to teach personal knowledge about their world to the personal assistants embedded in their devices.,6 Conclusion,[0],[0]
This task naturally synthesizes two main branches of question answering research: QA over KBs and QA over free text.,6 Conclusion,[0],[0]
One of our main contributions is a collection of diverse datasets that feature rich compositional questions over a dynamic knowledge graph expressed through simulated narrative.,6 Conclusion,[0],[0]
Another contribution of our work is a thorough set of experiments and analysis of different types of endto-end architectures for QA at their ability to answer multi-relational questions of varying degrees of compositionality.,6 Conclusion,[0],[0]
"Our long-term goal is that both the data and the simulation code we release will inspire and motivate the community to look towards the vision of letting end-users teach our personal assistants about the world around us.
",6 Conclusion,[0],[0]
"2In order to allow generalization across different domains for the Seq2Seq model, we replace entities appearing in each story with an id that correlates to their appearance order.",6 Conclusion,[0],[0]
"After the model outputs its prediction, the entity ids are converted back to the entity phrase.
",6 Conclusion,[0],[0]
The TEXTWORDSQA dataset and the code can be downloaded at https://igorlabutov.,6 Conclusion,[0],[0]
github.io/textworldsqa.github.io/,6 Conclusion,[0],[0]
"This paper was supported in part by Verizon InMind (Azaria and Hong, 2016).",7 Acknowledgments,[0],[0]
One of the GPUs used in this work was donated by Nvidia.,7 Acknowledgments,[0],[0]
"Question Answering (QA), as a research field, has primarily focused on either knowledge bases (KBs) or free text as a source of knowledge.",abstractText,[0],[0]
"These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them.",abstractText,[0],[0]
"In this work, we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multirelational QA over personal narrative.",abstractText,[0],[0]
"As a first step towards this goal, we make three key contributions: (i) we generate and release TEXTWORLDSQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, (ii) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and (iii) we release a lightweight Python-based framework we call TEXTWORLDS for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.",abstractText,[0],[0]
Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219–3232 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3219",text,[0],[0]
"As scientific communities grow and evolve, new tasks, methods, and datasets are introduced and different methods are compared with each other.",1 Introduction,[0],[0]
"Despite advances in search engines, it is still hard to identify new technologies and their relationships with what existed before.",1 Introduction,[0],[0]
"To help researchers more quickly identify opportunities for new combinations of tasks, methods and data, it is important to design intelligent algorithms that can extract and organize scientific information from a large collection of documents.
",1 Introduction,[0],[0]
Organizing scientific information into structured knowledge bases requires information extraction (IE) about scientific entities and their relationships.,1 Introduction,[0],[0]
"However, the challenges associated with scientific IE are greater than for a general domain.",1 Introduction,[0],[0]
"First, annotation of scientific text requires domain expertise which makes annotation costly and limits resources.
",1 Introduction,[0],[0]
1Data and code are publicly available at: http://nlp.,1 Introduction,[0],[0]
"cs.washington.edu/sciIE/
",1 Introduction,[0],[0]
"In addition, most relation extraction systems are designed for within-sentence relations.",1 Introduction,[0],[0]
"However, extracting information from scientific articles requires extracting relations across sentences.",1 Introduction,[0],[0]
Figure 1 illustrates this problem.,1 Introduction,[0],[0]
"The cross-sentence relations between some entities can only be connected by entities that refer to the same scientific concept, including generic terms (such as the pronoun it, or phrases like our method) that are not informative by themselves.",1 Introduction,[0],[0]
"With co-reference, context-free grammar can be connected to MORPA through the intermediate co-referred pronoun it.",1 Introduction,[0],[0]
"Applying existing IE systems to this data, without co-reference, will result in much lower relation coverage (and a sparse knowledge base).
",1 Introduction,[0],[0]
"In this paper, we develop a unified learning model for extracting scientific entities, relations, and coreference resolution.",1 Introduction,[0],[0]
"This is different from previous work (Luan et al., 2017b; Gupta and Manning, 2011; Tsai et al., 2013; Gábor et al., 2018) which often addresses these tasks as independent
components of a pipeline.",1 Introduction,[0],[0]
"Our unified model is a multi-task setup that shares parameters across low-level tasks, making predictions by leveraging context across the document through coreference links.",1 Introduction,[0],[0]
"Specifically, we extend prior work for learning span representations and coreference resolution (Lee et al., 2017; He et al., 2018).",1 Introduction,[0],[0]
"Different from a standard tagging system, our system enumerates all possible spans during decoding and can effectively detect overlapped spans.",1 Introduction,[0],[0]
"It avoids cascading errors between tasks by jointly modeling all spans and span-span relations.
",1 Introduction,[0],[0]
"To explore this problem, we create a dataset SCIERC for scientific information extraction, which includes annotations of scientific terms, relation categories and co-reference links.",1 Introduction,[0],[0]
"Our experiments show that the unified model is better at predicting span boundaries, and it outperforms previous state-of-the-art scientific IE systems on entity and relation extraction (Luan et al., 2017b; Augenstein et al., 2017).",1 Introduction,[0],[0]
"In addition, we build a scientific knowledge graph integrating terms and relations extracted from each article.",1 Introduction,[0],[0]
"Human evaluation shows that propagating coreference can significantly improve the quality of the automatic constructed knowledge graph.
",1 Introduction,[0],[0]
In summary we make the following contributions.,1 Introduction,[0],[0]
"We create a dataset for scientific information extraction by jointly annotating scientific entities, relations, and coreference links.",1 Introduction,[0],[0]
"Extending a previous end-to-end coreference resolution system, we develop a multi-task learning framework that can detect scientific entities, relations, and coreference clusters without hand-engineered features.",1 Introduction,[0],[0]
We use our unified framework to build a scientific knowledge graph from a large collection of documents and analyze information in scientific literature.,1 Introduction,[0],[0]
There has been growing interest in research on automatic methods for information extraction from scientific articles.,2 Related Work,[0],[0]
"Past research in scientific IE addressed analyzing citations (Athar and Teufel, 2012b,a; Kas, 2011; Gabor et al., 2016; Sim et al., 2012; Do et al., 2013; Jaidka et al., 2014; AbuJbara and Radev, 2011), analyzing research community (Vogel and Jurafsky, 2012; Anderson et al., 2012), and unsupervised methods for extracting scientific entities and relations (Gupta and Manning, 2011; Tsai et al., 2013; Gábor et al., 2016).
",2 Related Work,[0],[0]
"More recently, two datasets in SemEval 2017
and 2018 have been introduced, which facilitate research on supervised and semi-supervised learning for scientific information extraction.",2 Related Work,[0],[0]
"SemEval 17 (Augenstein et al., 2017) includes 500 paragraphs from articles in the domains of computer science, physics, and material science.",2 Related Work,[0],[0]
"It includes three types of entities (called keyphrases): Tasks, Methods, and Materials and two relation types: hyponym-of and synonym-of.",2 Related Work,[0],[0]
"SemEval 18 (Gábor et al., 2018) is focused on predicting relations between entities within a sentence.",2 Related Work,[0],[0]
It consists of six relation types.,2 Related Work,[0],[0]
"Using these datasets, neural models (Ammar et al., 2017, 2018; Luan et al., 2017b; Augenstein and Søgaard, 2017) are introduced for extracting scientific information.",2 Related Work,[0],[0]
"We extend these datasets by increasing relation coverage, adding cross-sentence coreference linking, and removing some annotation constraints.",2 Related Work,[0],[0]
"Different from most previous IE systems for scientific literature and general domains (Miwa and Bansal, 2016; Xu et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Luan et al., 2018; Adel and Schütze, 2017), which use preprocessed syntactic, discourse or coreference features as input, our unified framework does not rely on any pipeline processing and is able to model overlapping spans.
",2 Related Work,[0],[0]
"While Singh et al. (2013) show improvements by jointly modeling entities, relations, and coreference links, most recent neural models for these tasks focus on single tasks (Clark and Manning, 2016; Wiseman et al., 2016; Lee et al., 2017; Lample et al., 2016; Peng et al., 2017) or joint entity and relation extraction (Katiyar and Cardie, 2017; Zhang et al., 2017; Adel and Schütze, 2017; Zheng et al., 2017).",2 Related Work,[0],[0]
"Among those studies, many papers assume the entity boundaries are given, such as (Clark and Manning, 2016), Adel and Schütze (2017) and Peng et al. (2017).",2 Related Work,[0],[0]
Our work relaxes this constraint and predicts entity boundaries by optimizing over all possible spans.,2 Related Work,[0],[0]
"Our model draws from recent end-to-end span-based models for coreference resolution (Lee et al., 2017, 2018) and semantic role labeling (He et al., 2018) and extends them for the multi-task framework involving the three tasks of identification of entity, relation and coreference.
",2 Related Work,[0],[0]
Neural multi-task learning has been applied to a range of NLP tasks.,2 Related Work,[0],[0]
"Most of these models share word-level representations (Collobert and Weston, 2008; Klerke et al., 2016; Luan et al., 2016, 2017a; Rei, 2017), while Peng et al. (2017) uses high-order cross-task factors.",2 Related Work,[0],[0]
"Our model instead propagates
cross-task information via span representations, which is related to Swayamdipta et al. (2017).",2 Related Work,[0],[0]
"Our dataset (called SCIERC) includes annotations for scientific entities, their relations, and coreference clusters for 500 scientific abstracts.",3 Dataset,[0],[0]
These abstracts are taken from 12 AI conference/workshop proceedings in four AI communities from the Semantic Scholar Corpus2.,3 Dataset,[0],[0]
"SCIERC extends previous datasets in scientific articles SemEval 2017 Task 10 (SemEval 17) (Augenstein et al., 2017) and SemEval 2018",3 Dataset,[0],[0]
"Task 7 (SemEval 18) (Gábor et al., 2018) by extending entity types, relation types, relation coverage, and adding cross-sentence relations using coreference links.",3 Dataset,[0],[0]
"Our dataset is publicly available at: http://nlp.cs.washington. edu/sciIE/. Table 1 shows the statistics of SCIERC.
",3 Dataset,[0],[0]
Annotation Scheme,3 Dataset,[0],[0]
"We define six types for annotating scientific entities (Task, Method, Metric, Material, Other-ScientificTerm and Generic) and seven relation types (Compare, Part-of, Conjunction, Evaluate-for, Feature-of, Used-for, HyponymOf).",3 Dataset,[0],[0]
Directionality is taken into account except for the two symmetric relation types (Conjunction and Compare).,3 Dataset,[0],[0]
Coreference links are annotated between identical scientific entities.,3 Dataset,[0],[0]
A Generic entity is annotated only when the entity is involved in a relation or is coreferred with another entity.,3 Dataset,[0],[0]
"Annotation guidelines can be found in Appendix A. Figure 1 shows an annotated example.
",3 Dataset,[0],[0]
"Following annotation guidelines from QasemiZadeh and Schumann (2016) and using the BRAT interface (Stenetorp et al., 2012), our annotators perform a greedy annotation for spans and always prefer the longer span whenever ambiguity occurs.",3 Dataset,[0],[0]
"Nested spans are allowed when a subspan has a relation/coreference link with another term outside the span.
",3 Dataset,[0],[0]
Human Agreements One domain expert annotated all the documents in the dataset; 12% of the data is dually annotated by 4 other domain experts to evaluate the user agreements.,3 Dataset,[0],[0]
"The kappa score for annotating entities is 76.9%, relation extraction is 67.8% and coreference is 63.8%.
",3 Dataset,[0],[0]
"2These conferences include general AI (AAAI, IJCAI), NLP (ACL, EMNLP, IJCNLP), speech (ICASSP, Interspeech), machine learning (NIPS, ICML), and computer vision (CVPR, ICCV, ECCV) at http://labs.semanticscholar.",3 Dataset,[0],[0]
"org/corpus/
Comparison with previous datasets SCIERC is focused on annotating cross-sentence relations and has more relation coverage than SemEval 17 and SemEval 18, as shown in Table 1.",3 Dataset,[0],[0]
SemEval 17 is mostly designed for entity recognition and only covers two relation types.,3 Dataset,[0],[0]
"The task in SemEval 18 is to classify a relation between a pair of entities given entity boundaries, but only intra-sentence relations are annotated and each entity only appears in one relation, resulting in sparser relation coverage than our dataset (3.2 vs. 9.4 relations per abstract).",3 Dataset,[0],[0]
"SCIERC extends these datasets by adding more relation types and coreference clusters, which allows representing cross-sentence relations, and removing annotation constraints.",3 Dataset,[0],[0]
Table 1 gives a comparison of statistics among the three datasets.,3 Dataset,[0],[0]
"In addition, SCIERC aims at including broader coverage of general AI communities.",3 Dataset,[0],[0]
"We develop a unified framework (called SCIIE) to identify and classify scientific entities, relations, and coreference resolution across sentences.",4 Model,[0],[0]
"SCIIE is a multi-task learning setup that extends previous span-based models for coreference resolution (Lee et al., 2017) and semantic role labeling (He et al., 2018).",4 Model,[0],[0]
"All three tasks of entity recognition, relation extraction, and coreference resolution are treated as multinomial classification problems with shared span representations.",4 Model,[0],[0]
SCIIE benefits from expressive contextualized span representations as classifier features.,4 Model,[0],[0]
"By sharing span representations, sentence-level tasks can benefit from information propagated from coreference resolution across sentences, without increasing the complexity of inference.",4 Model,[0],[0]
Figure 2 shows a high-level overview of the SCIIE multi-task framework.,4 Model,[0],[0]
"The input is a document represented as a sequence of words D = {w1, . . .",4.1 Problem Definition,[0],[0]
", wn}, from which we derive S = {s1, . . .",4.1 Problem Definition,[0],[0]
", sN}, the set of all possible
within-sentence word sequence spans (up to a reasonable length) in the document.",4.1 Problem Definition,[0],[0]
"The output contains three structures: the entity types E for all spans S, the relations R for all pair of spans S×S, and the coreference links C for all spans in S. The output structures are represented with a set of discrete random variables indexed by spans or pairs of spans.",4.1 Problem Definition,[0],[0]
"Specifically, the output structures are defined as follows.",4.1 Problem Definition,[0],[0]
Entity recognition is to predict the best entity type for every candidate span.,4.1 Problem Definition,[0],[0]
Let LE represent the set of all possible entity types including the null-type .,4.1 Problem Definition,[0],[0]
"The output structure E is a set of random variables indexed by spans: ei ∈ LE for i = 1, . . .",4.1 Problem Definition,[0],[0]
", N .",4.1 Problem Definition,[0],[0]
"Relation extraction is to predict the best relation type given an ordered pair of spans (si, sj).",4.1 Problem Definition,[0],[0]
Let LR be the set of all possible relation types including the null-type .,4.1 Problem Definition,[0],[0]
"The output structure R is a set of random variables indexed over pairs of spans (i, j) that belong to the same sentence: rij ∈ LR for i, j = 1, . . .",4.1 Problem Definition,[0],[0]
", N .",4.1 Problem Definition,[0],[0]
"Coreference resolution is to predict the best antecedent (including a special null antecedent) given a span, which is the same mention-ranking model used in Lee et al. (2017).",4.1 Problem Definition,[0],[0]
"The output structure C is a set of random variables defined as: ci ∈ {1, . .",4.1 Problem Definition,[0],[0]
.,4.1 Problem Definition,[0],[0]
", i− 1, } for i = 1, . . .",4.1 Problem Definition,[0],[0]
", N .",4.1 Problem Definition,[0],[0]
"We formulate the multi-task learning setup as learning the conditional probability distribution P (E,R,C|D).",4.2 Model Definition,[0],[0]
"For efficient training and inference, we decompose P (E,R,C|D)",4.2 Model Definition,[0],[0]
"assuming spans are
conditionally independent given D:
P (E,R,C | D) = P (E,R,C, S | D) (1)
",4.2 Model Definition,[0],[0]
= N∏ i=1,4.2 Model Definition,[0],[0]
"P (ei | D)P (ci | D) N∏ j=1 P (rij | D),
where the conditional probabilities of each random variable are independently normalized: P (ei = e | D) = exp(ΦE(e, si))∑
e′∈LE exp(ΦE(e ′, si))
(2)
P (rij = r | D) = exp(ΦR(r, si, sj))∑
r′∈LR",4.2 Model Definition,[0],[0]
"exp(ΦR(r ′, si, sj))
P (ci = j | D) = exp(ΦC(si, sj))∑
j′∈{1,...,i−1, } exp(ΦC(si, sj′)) ,
where ΦE denotes the unnormalized model score for an entity type e and a span si, ΦR denotes the score for a relation type r and span pairs si, sj , and ΦC denotes the score for a binary coreference link between si and sj .",4.2 Model Definition,[0],[0]
"These Φ scores are further decomposed into span and pairwise span scores computed from feed-forward networks, as will be explained in Section 4.3.
",4.2 Model Definition,[0],[0]
"For simplicity, we omit D from the Φ functions and S from the observation.
",4.2 Model Definition,[0],[0]
"Objective Given a set of all documents D, the model loss function is defined as a weighted sum of the negative log-likelihood loss of all three tasks:
− ∑
(D,R∗,E∗,C∗)∈D
{ λE logP (E ∗ | D) (3)
+ λR",4.2 Model Definition,[0],[0]
"logP (R ∗ | D) + λC logP (C∗ | D)
}
where E∗, R∗, and C∗ are gold structures of the entity types, relations, and coreference, respectively.",4.2 Model Definition,[0],[0]
"The task weights λE, λR, and λC are introduced as hyper-parameters to control the importance of each task.
",4.2 Model Definition,[0],[0]
"For entity recognition and relation extraction, P (E∗ | D) and P (R∗ | D) are computed with the definition in Equation (2).",4.2 Model Definition,[0],[0]
"For coreference resolution, we use the marginalized loss following Lee et al. (2017) since each mention can have multiple correct antecedents.",4.2 Model Definition,[0],[0]
"Let C∗i be the set of all correct antecedents for span i, we have: logP (C∗ | D) = ∑ i=1..",4.2 Model Definition,[0],[0]
N log ∑ c∈C∗i P (c | D).,4.2 Model Definition,[0],[0]
We use feedforward neural networks (FFNNs) over shared span representations g to compute a set of span and pairwise span scores.,4.3 Scoring Architecture,[0],[0]
"For the span scores, φe(si) measures how likely a span si has an entity type e, and φmr(si) and φmc(si) measure how likely a span si is a mention in a relation or a coreference link, respectively.",4.3 Scoring Architecture,[0],[0]
"The pairwise scores φr(si, sj) and φc(si, sj) measure how likely two spans are associated in a relation r or a coreference link, respectively.",4.3 Scoring Architecture,[0],[0]
Let gi be the fixed-length vector representation for span si.,4.3 Scoring Architecture,[0],[0]
"For different tasks, the span scores φx(si) for x ∈ {e,mc,mr} and pairwise span scores φy(si, sj) for y ∈ {r, c} are computed as follows:
φx(si)",4.3 Scoring Architecture,[0],[0]
"=wx · FFNNx(gi) φy(si, sj) =wy · FFNNy([gi,gj ,gi gj ]),
where is element-wise multiplication, and {wx,wy} are neural network parameters to be learned.
",4.3 Scoring Architecture,[0],[0]
"We use these scores to compute the different Φ:
ΦE(e, si) = φe(si) (4)
ΦR(r, si, sj) = φmr(si) + φmr(sj) + φr(si, sj)
ΦC(si, sj) = φmc(si) + φmc(sj) + φc(si, sj)
",4.3 Scoring Architecture,[0],[0]
"The scores in Equation (4) are defined for entity types, relations, and antecedents that are not the null-type .",4.3 Scoring Architecture,[0],[0]
"Scores involving the null label are set to a constant 0: ΦE( , si) = ΦR( , si, sj) = ΦC(si, ) = 0.
",4.3 Scoring Architecture,[0],[0]
"We use the same span representations g from (Lee et al., 2017) and share them across the three tasks.",4.3 Scoring Architecture,[0],[0]
"We start by building bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) from word, character and ELMo (Peters et al., 2018) embeddings.
",4.3 Scoring Architecture,[0],[0]
"For a span si, its vector representation gi is constructed by concatenating si’s left and right end points from the BiLSTM outputs, an attentionbased soft “headword,” and embedded span width features.",4.3 Scoring Architecture,[0],[0]
Hyperparameters and other implementation details will be described in Section 6.,4.3 Scoring Architecture,[0],[0]
"Following previous work, we use beam pruning to reduce the number of pairwise span factors from O(n4) to O(n2) at both training and test time, where n is the number of words in the document.",4.4 Inference and Pruning,[0],[0]
"We define two separate beams: BC to prune spans for the coreference resolution task, and BR for relation extraction.",4.4 Inference and Pruning,[0],[0]
"The spans in the beams are sorted by their span scores φmc and φmr respectively, and the sizes of the beams are limited by λCn and λRn.",4.4 Inference and Pruning,[0],[0]
"We also limit the maximum width of spans to a fixed number W , which further reduces the number of span factors to O(n).",4.4 Inference and Pruning,[0],[0]
We construct a scientific knowledge graph from a large corpus of scientific articles.,5 Knowledge Graph Construction,[0],[0]
The corpus includes all abstracts (110k in total) from 12 AI conference proceedings from the Semantic Scholar Corpus.,5 Knowledge Graph Construction,[0],[0]
Nodes in the knowledge graph correspond to scientific entities.,5 Knowledge Graph Construction,[0],[0]
Edges correspond to scientific relations between pairs of entities.,5 Knowledge Graph Construction,[0],[0]
The edges are typed according to the relation types defined in Section 3.,5 Knowledge Graph Construction,[0],[0]
Figure 4 shows a part of a knowledge graph created by our method.,5 Knowledge Graph Construction,[0],[0]
"For example, Statistical Machine Translation (SMT) and grammatical error correction are nodes in the graph, and they are connected through a Used-for relation type.",5 Knowledge Graph Construction,[0],[0]
"In order to construct the knowledge graph for the whole corpus, we first apply the SCIIE model over single documents and then integrate the entities and relations across multiple documents (Figure 3).
",5 Knowledge Graph Construction,[0],[0]
Extracting nodes (entities),5 Knowledge Graph Construction,[0],[0]
"The SCIIE model extracts entities, their relations, and coreference
clusters within one document.",5 Knowledge Graph Construction,[0],[0]
Phrases are heuristically normalized (described in Section 6) using entities and coreference links.,5 Knowledge Graph Construction,[0],[0]
"In particular, we link all entities that belong to the same coreference cluster to replace generic terms with any other nongeneric term in the cluster.",5 Knowledge Graph Construction,[0],[0]
"Moreover, we replace all the entities in the cluster with the entity that has the longest string.",5 Knowledge Graph Construction,[0],[0]
Our qualitative analysis shows that there are fewer ambiguous phrases using coreference links (Figure 5).,5 Knowledge Graph Construction,[0],[0]
We calculate the frequency counts of all entities that appear in the whole corpus.,5 Knowledge Graph Construction,[0],[0]
"We assign nodes in the knowledge graph by selecting the most frequent entities (with counts > k) in the corpus, and merge in any remaining entities for which a frequent entity is a substring.
",5 Knowledge Graph Construction,[0],[0]
"Assigning edges (relations) A pair of entities may appear in different contexts, resulting in different relation types between those entities (Figure 6).",5 Knowledge Graph Construction,[0],[0]
"For every pair of entities in the graph, we calculate the frequency of different relation types across the whole corpus.",5 Knowledge Graph Construction,[0],[0]
We assign edges between entities by selecting the most frequent relation type.,5 Knowledge Graph Construction,[0],[0]
We evaluate our unified framework SCIIE on SCIERC and SemEval 17.,6 Experimental Setup,[0],[0]
"The knowledge graph for
scientific community analysis is built using the Semantic Scholar Corpus (110k abstracts in total).",6 Experimental Setup,[0],[0]
"We compare our model with the following baselines on SCIERCdataset:
• LSTM+CRF",6.1 Baselines,[0],[0]
"The state-of-the-art NER system (Lample et al., 2016), which applies CRF on top of LSTM for named entity tagging, the approach has also been used in scientific term extraction (Luan et al., 2017b).
",6.1 Baselines,[0],[0]
"• LSTM+CRF+ELMo LSTM+CRF with ELMO as an additional input feature.
• E2E Rel State-of-the-art joint entity and relation extraction system (Miwa and Bansal, 2016) that has also been used in scientific literature (Peters et al., 2017; Augenstein et al., 2017).",6.1 Baselines,[0],[0]
"This system uses syntactic features such as part-of-speech tagging and dependency parsing.
",6.1 Baselines,[0],[0]
•,6.1 Baselines,[0],[0]
E2E Rel(Pipeline) Pipeline setting of E2E Rel.,6.1 Baselines,[0],[0]
"Extract entities first and use entity results as input to relation extraction task.
",6.1 Baselines,[0],[0]
•,6.1 Baselines,[0],[0]
"E2E Rel+ELMo E2E Rel with ELMO as an additional input feature.
•",6.1 Baselines,[0],[0]
E2E Coref State-of-the-art coreference system Lee et al. (2017) combined with ELMO.,6.1 Baselines,[0],[0]
"Our system SCIIE extends E2E Coref with multi-task learning.
",6.1 Baselines,[0],[0]
"In the SemEval task, we compare our model SCIIE with the best reported system in the SemEval leaderboard (Peters et al., 2017), which extends E2E Rel with several in-domain features such as gazetteers extracted from existing knowledge bases and model ensembles.",6.1 Baselines,[0],[0]
"We also compare with the state of the art on keyphrase extraction (Luan et al., 2017b), which applies semi-supervised methods to a neural tagging model.3",6.1 Baselines,[0],[0]
Our system extends the implementation and hyperparameters from Lee et al. (2017) with the following adjustments.,6.2 Implementation details,[0],[0]
We use a 1 layer BiLSTM with 200-dimensional hidden layers.,6.2 Implementation details,[0],[0]
All the FFNNs have 2 hidden layers of 150 dimensions each.,6.2 Implementation details,[0],[0]
"We use 0.4 variational dropout (Gal and Ghahramani, 2016) for the LSTMs, 0.4 dropout for the FFNNs, and 0.5 dropout for the input embeddings.",6.2 Implementation details,[0],[0]
We model spans up to 8 words.,6.2 Implementation details,[0],[0]
"For beam pruning, we use λC = 0.3 for coreference resolution and λR = 0.4 for relation extraction.",6.2 Implementation details,[0],[0]
"For constructing the knowledge graph, we use the following heuristics to normalize the entity phrases.",6.2 Implementation details,[0],[0]
We replace all acronyms with their corresponding full name and normalize all the plural terms with their singular counterparts.,6.2 Implementation details,[0],[0]
We evaluate SCIIE on SCIERC and SemEval 17 datasets.,7 Experimental Results,[0],[0]
We provide qualitative results and human evaluation of the constructed knowledge graph.,7 Experimental Results,[0],[0]
"Results on SciERC Table 2 compares the result of our model with baselines on the three tasks: entity recognition (Table 2a), relation extraction (Table 2b), and coreference resolution (Table 2c).",7.1 IE Results,[0],[0]
"As evidenced by the table, our unified multi-task setup
3We compare with the inductive setting results.
",7.1 IE Results,[0],[0]
SCIIE outperforms all the baselines.,7.1 IE Results,[0],[0]
"For entity recognition, our model achieves 1.3% and 2.4% relative improvement over LSTM+CRF with and without ELMO, respectively.",7.1 IE Results,[0],[0]
"Moreover, it achieves 1.8% and 2.7% relative improvement over E2E Rel with and without ELMO, respectively.",7.1 IE Results,[0],[0]
"For relation extraction, we observe more significant improvement with 13.1% relative improvement over E2E Rel and 7.4% improvement over E2E Rel with ELMO.",7.1 IE Results,[0],[0]
"For coreference resolution, SCIIE outperforms E2E Coref with 4.5% relative improvement.",7.1 IE Results,[0],[0]
We still observe a large gap between human-level performance and a machine learning system.,7.1 IE Results,[0],[0]
"We invite the community to address this challenging task.
",7.1 IE Results,[0],[0]
Ablations We evaluate the effect of multi-task learning in each of the three tasks defined in our dataset.,7.1 IE Results,[0],[0]
Table 3 reports the results for individual tasks when additional tasks are included in the learning objective function.,7.1 IE Results,[0],[0]
We observe that performance improves with each added task in the objective.,7.1 IE Results,[0],[0]
"For example, Entity recognition (65.7) benefits from both coreference resolution (67.5) and relation extraction (66.8).",7.1 IE Results,[0],[0]
"Relation extrac-
tion (37.9) significantly benefits when multi-tasked with coreference resolution (7.1% relative improvement).",7.1 IE Results,[0],[0]
"Coreference resolution benefits when multitasked with relation extraction, with 4.9% relative improvement.
Results on SemEval 17 Table 4 compares the results of our model with the state of the art on the SemEval 17 dataset for tasks of span identification, keyphrase extraction and relation extraction as well as the overall score.",7.1 IE Results,[0],[0]
Span identification aims at identifying spans of entities.,7.1 IE Results,[0],[0]
Keyphrase classification and relation extraction has the same setting with the entity and relation extraction in SCIERC.,7.1 IE Results,[0],[0]
Our model outperforms all the previous models that use hand-designed features.,7.1 IE Results,[0],[0]
We observe more significant improvement in span identification than keyphrase classification.,7.1 IE Results,[0],[0]
This confirms the benefit of our model in enumerating spans (rather than BIO tagging in state-of-the-art systems).,7.1 IE Results,[0],[0]
"Moreover, we have competitive results compared to the previous state of the art in relation extraction.",7.1 IE Results,[0],[0]
"We observe less gain compared to the SCIERC dataset mainly because there are no coference links, and the relation types are not comprehensive.",7.1 IE Results,[0],[0]
"We provide qualitative analysis and human evaluations on the constructed knowledge graph.
",7.2 Knowledge Graph Analysis,[0],[0]
"Scientific trend analysis Figure 7 shows the historical trend analysis (from 1996 to 2016) of the most popular applications of the phrase neural network, selected according to the statistics of the extracted relation triples with the ‘Used-for’ relation type from speech, computer vision, and NLP conference papers.",7.2 Knowledge Graph Analysis,[0],[0]
"We observe that, before 2000, neural network has been applied to a greater percentage of speech applications compared to the NLP and computer vision papers.",7.2 Knowledge Graph Analysis,[0],[0]
"In NLP, neural networks first gain popularity in language modeling
and then extend to other tasks such as POS Tagging and Machine Translation.",7.2 Knowledge Graph Analysis,[0],[0]
"In computer vision, the application of neural networks gains popularity in object recognition earlier (around 2010) than the other two more complex tasks of object detection and image segmentation (hardest and also the latest).
",7.2 Knowledge Graph Analysis,[0],[0]
"Knowledge Graph Evaluation Figure 8 shows the human evaluation of the constructed knowledge graph, comparing the quality of automatically generated knowledge graphs with and without the coreference links.",7.2 Knowledge Graph Analysis,[0],[0]
We randomly select 10 frequent scientific entities and extract all the relation triples that include one of the selected entities leading to 1.5k relation triples from both systems.,7.2 Knowledge Graph Analysis,[0],[0]
"We ask four domain experts to annotate each of these ex-
tracted relations to define ground truth labels.",7.2 Knowledge Graph Analysis,[0],[0]
Each domain expert is assigned 2 or 3 entities and all of the corresponding relations.,7.2 Knowledge Graph Analysis,[0],[0]
Figure 8 shows precision/recall curves for both systems.,7.2 Knowledge Graph Analysis,[0],[0]
"Since it is not feasible to compute the actual recall of the systems, we compute the pseudo-recall (Zhang et al., 2015) based on the output of both systems.",7.2 Knowledge Graph Analysis,[0],[0]
We observe that the knowledge graph curve with coreference linking is mostly above the curve without coreference linking.,7.2 Knowledge Graph Analysis,[0],[0]
"The precision of both systems is high (above 84% for both systems), but the system with coreference links has significantly higher recall.",7.2 Knowledge Graph Analysis,[0],[0]
"In this paper, we create a new dataset and develop a multi-task model for identifying entities, relations, and coreference clusters in scientific articles.",8 Conclusion,[0],[0]
"By sharing span representations and leveraging crosssentence information, our multi-task setup effectively improves performance across all tasks.",8 Conclusion,[0],[0]
"Moreover, we show that our multi-task model is better at predicting span boundaries and outperforms previous state-of-the-art scientific IE systems on entity and relation extraction, without using any handengineered features or pipeline processing.",8 Conclusion,[0],[0]
"Using our model, we are able to automatically organize the extracted information from a large collection of scientific articles into a knowledge graph.",8 Conclusion,[0],[0]
"Our analysis shows the importance of coreference links in making a dense, useful graph.
",8 Conclusion,[0],[0]
"We still observe a large gap between the performance of our model and human performance, confirming the challenges of scientific IE.",8 Conclusion,[0],[0]
Future work includes improving the performance using semisupervised techniques and providing in-domain features.,8 Conclusion,[0],[0]
We also plan to extend our multi-task framework to information extraction tasks in other domains.,8 Conclusion,[0],[0]
"This research was supported by the Office of Naval Research under the MURI grant N00014-18-1-
2670, NSF (IIS 1616112, III 1703166), Allen Distinguished Investigator Award, and gifts from Allen Institute for AI, Google, Amazon, and Bloomberg.",Acknowledgments,[0],[0]
We are grateful to Waleed Ammar and AI2 for sharing the Semantic Scholar Corpus.,Acknowledgments,[0],[0]
"We also thank the anonymous reviewers, UW-NLP group and Shoou-I Yu for their helpful comments.",Acknowledgments,[0],[0]
A.1 Entity Category •,A Annotation Guideline,[0],[0]
"Task: Applications, problems to solve, sys-
tems to construct.
",A Annotation Guideline,[0],[0]
"E.g. information extraction, machine reading system, image segmentation, etc.
•",A Annotation Guideline,[0],[0]
"Method: Methods , models, systems to use, or tools, components of a system, frameworks.
",A Annotation Guideline,[0],[0]
"E.g. language model, CORENLP, POS parser, kernel method, etc.
•",A Annotation Guideline,[0],[0]
"Evaluation Metric: Metrics, measures, or entities that can express quality of a system/method.
",A Annotation Guideline,[0],[0]
"E.g. F1, BLEU, Precision, Recall, ROC curve, mean reciprocal rank, mean-squared error, robustness, time complexity, etc.
•",A Annotation Guideline,[0],[0]
"Material: Data, datasets, resources, Corpus, Knowledge base.
",A Annotation Guideline,[0],[0]
"E.g. image data, speech data, stereo images, bilingual dictionary, paraphrased questions, CoNLL, Panntreebank, WordNet, Wikipedia, etc.
",A Annotation Guideline,[0],[0]
"• Evaluation Metric: Metric measure or term that can express quality of a system/method.
",A Annotation Guideline,[0],[0]
"E.g. F1, BLEU, Precision, Recall, ROC curve, mean reciprocal rank, mean-squared error,robustness, compile time, time complexity...
",A Annotation Guideline,[0],[0]
•,A Annotation Guideline,[0],[0]
"Generic: General terms or pronouns that may refer to a entity but are not themselves informative, often used as connection words.
",A Annotation Guideline,[0],[0]
"E.g model, approach, prior knowledge, them, it...
",A Annotation Guideline,[0],[0]
A.2 Relation Category Relation link can not go beyond sentence boundary.,A Annotation Guideline,[0],[0]
"We define 4 asymmetric relation types (Used-for, Feature-of, Hyponym-of, Part-of ), together with 2 symmetric relation types (Compare, Conjunction).",A Annotation Guideline,[0],[0]
"B always points to A for asymmetric relations
• Used-for: B is used for A, B models A, A is trained on B, B exploits A, A is based on B. E.g.
The TISPER system has been designed to enable many text applications.
",A Annotation Guideline,[0],[0]
Our method models user proficiency.,A Annotation Guideline,[0],[0]
"Our algorithms exploits local soothness.
",A Annotation Guideline,[0],[0]
"• Feature-of: B belongs to A, B is a feature of A, B is under A domain.",A Annotation Guideline,[0],[0]
"E.g.
prior knowledge of the model genre-specific regularities of discourse structure English text in science domain
• Hyponym-of: B is a hyponym of A, B is a type of A. E.g.
TUIT is a software library NLP applications such as machine translation and language generation
• Part-of: B is a part of A...",A Annotation Guideline,[0],[0]
"E.g.
The system includes two models: speech recognition and natural language understanding We incorporate NLU module to the system.
",A Annotation Guideline,[0],[0]
• Compare: Symmetric relation (use blue to denote entity).,A Annotation Guideline,[0],[0]
"Opposite of conjunction, compare two models/methods, or listing two opposing entities.",A Annotation Guideline,[0],[0]
"E.g.
Unlike the quantitative prior, the qualitative prior is often ignored...",A Annotation Guideline,[0],[0]
"We compare our system with previous sequential tagging systems...
•",A Annotation Guideline,[0],[0]
Conjunction: Symmetric relation (use blue to denote entity).,A Annotation Guideline,[0],[0]
Function as similar role or use/incorporate with.,A Annotation Guideline,[0],[0]
"E.g.
obtained from human expert or knowledge base NLP applications such as machine translation and language generation
A.3 Coreference Two Entities that points to the same concept.
",A Annotation Guideline,[0],[0]
•,A Annotation Guideline,[0],[0]
"Anaphora and Cataphora:
We introduce a machine reading system...",A Annotation Guideline,[0],[0]
The system...,A Annotation Guideline,[0],[0]
The prior knowledge include...,A Annotation Guideline,[0],[0]
"Such knowledge can be applied to...
•",A Annotation Guideline,[0],[0]
"Coreferring noun phrase:
We develop a part-of-speech tagging system...",A Annotation Guideline,[0],[0]
"The POS tagger...
A.4 Notes 1.",A Annotation Guideline,[0],[0]
"Entity boundary annotation follows the
ACL RD-TEC Annotation Guideline (QasemiZadeh and Schumann, 2016), with the extention that spans can be embedded in longer spans, only if the shorter span is involved in a relation.
2.",A Annotation Guideline,[0],[0]
"Do not include determinators (such as the, a), or adjective pronouns (such as this,its, these, such) to the span.",A Annotation Guideline,[0],[0]
"If generic phrases are not involved in a relation, do not tag them.
",A Annotation Guideline,[0],[0]
3.,A Annotation Guideline,[0],[0]
"Do not tag relation if one entity is:
• Variable bound: We introduce a neural based approach..",A Annotation Guideline,[0],[0]
"Its benefit is... • The word which:
We introduce a neural based approach, which is a...
4.",A Annotation Guideline,[0],[0]
"Do not tag coreference if the entity is
• Generically-used Other-ScientificTerm: ...advantage gained from local smoothness which...",A Annotation Guideline,[0],[0]
We present algorithms exploiting local smoothness in more aggressive ways...,A Annotation Guideline,[0],[0]
"• Same scientific term but refer to different
examples: We use a data structure, we also use another data structure...
5.",A Annotation Guideline,[0],[0]
"Do not label negative relations:
X is not used in Y or X is hard to be applied in Y",A Annotation Guideline,[0],[0]
Here we take a screen shot of the BRAT interface for an ACL paper in Figure 9.,B Annotation and Knowledge Graph Examples,[0],[0]
We also attach the original figure of Figure 3 in Figure 10.,B Annotation and Knowledge Graph Examples,[0],[0]
"More examples can be found in the project website4.
",B Annotation and Knowledge Graph Examples,[0],[0]
4http://nlp.cs.washington.edu/sciIE/,B Annotation and Knowledge Graph Examples,[0],[0]
"We introduce a multi-task setup of identifying and classifying entities, relations, and coreference clusters in scientific articles.",abstractText,[0],[0]
"We create SCIERC, a dataset that includes annotations for all three tasks and develop a unified framework called Scientific Information Extractor (SCIIE) for with shared span representations.",abstractText,[0],[0]
The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links.,abstractText,[0],[0]
Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features.,abstractText,[0],[0]
"We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.1",abstractText,[0],[0]
"Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1896–1906 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
Multi-task learning (MTL) and semi-supervised learning are both successful paradigms for learning in scenarios with limited labelled data and have in recent years been applied to almost all areas of NLP.,1 Introduction,[0],[0]
"Applications of MTL in NLP, for example, include partial parsing (Søgaard and Goldberg, 2016), text normalisation (Bollman et al., 2017), neural machine translation (Luong et al., 2016), and keyphrase boundary classification (Augenstein and Søgaard, 2017).
",1 Introduction,[0],[0]
"Contemporary work in MTL for NLP typically focuses on learning representations that are useful across tasks, often through hard parameter sharing of hidden layers of neural networks (Collobert et al., 2011; Søgaard and Goldberg, 2016).",1 Introduction,[0],[0]
"If tasks share optimal hypothesis classes at the level of these representations, MTL leads to improvements (Baxter, 2000).",1 Introduction,[0],[0]
"However, while sharing hidden layers of neural networks is an effective regulariser (Søgaard and Goldberg, 2016), we potentially loose synergies between the classification functions trained to associate these representations with class labels.",1 Introduction,[0],[0]
"This paper sets out to build an architecture in which such synergies are exploited,
?",1 Introduction,[0],[0]
"The first two authors contributed equally.
with an application to pairwise sequence classification tasks.",1 Introduction,[0],[0]
"Doing so, we achieve a new state of the art on topic-based sentiment analysis.
",1 Introduction,[0],[0]
"For many NLP tasks, disparate label sets are weakly correlated, e.g. part-of-speech tags correlate with dependencies (Hashimoto et al., 2017), sentiment correlates with emotion (Felbo et al., 2017; Eisner et al., 2016), etc.",1 Introduction,[0],[0]
"We thus propose to induce a joint label embedding space (visualised in Figure 2) using a Label Embedding Layer that allows us to model these relationships, which we show helps with learning.
",1 Introduction,[0],[0]
"In addition, for tasks where labels are closely related, we should be able to not only model their relationship, but also to directly estimate the corresponding label of the target task based on auxiliary predictions.",1 Introduction,[0],[0]
"To this end, we propose to train a Label Transfer Network (LTN) jointly with the model to produce pseudo-labels across tasks.
",1 Introduction,[0],[0]
"The LTN can be used to label unlabelled and auxiliary task data by utilising the ‘dark knowledge’ (Hinton et al., 2015) contained in auxiliary model predictions.",1 Introduction,[0],[0]
"This pseudo-labelled data is then incorporated into the model via semisupervised learning, leading to a natural combination of multi-task learning and semi-supervised learning.",1 Introduction,[0],[0]
"We additionally augment the LTN with data-specific diversity features (Ruder and Plank, 2017) that aid in learning.
",1 Introduction,[0],[0]
Contributions,1 Introduction,[0],[0]
Our contributions are: a) We model the relationships between labels by inducing a joint label space for multi-task learning.,1 Introduction,[0],[0]
b),1 Introduction,[0],[0]
We propose a Label Transfer Network that learns to transfer labels between tasks and propose to use semi-supervised learning to leverage them for training.,1 Introduction,[0],[0]
c),1 Introduction,[0],[0]
We evaluate MTL approaches on a variety of classification tasks and shed new light on settings where multi-task learning works.,1 Introduction,[0],[0]
d),1 Introduction,[0],[0]
"We perform an extensive ablation study of our model.
1896
e)",1 Introduction,[0],[0]
We report state-of-the-art performance on topicbased sentiment analysis.,1 Introduction,[0],[0]
"Learning task similarities Existing approaches for learning similarities between tasks enforce a clustering of tasks (Evgeniou et al., 2005; Jacob et al., 2009), induce a shared prior (Yu et al., 2005; Xue et al., 2007; Daumé III, 2009), or learn a grouping (Kang et al., 2011; Kumar and Daumé III, 2012).",2 Related work,[0],[0]
These approaches focus on homogeneous tasks and employ linear or Bayesian models.,2 Related work,[0],[0]
"They can thus not be directly applied to our setting with tasks using disparate label sets.
",2 Related work,[0],[0]
"Multi-task learning with neural networks Recent work in multi-task learning goes beyond hard parameter sharing (Caruana, 1993) and considers different sharing structures, e.g. only sharing at lower layers (Søgaard and Goldberg, 2016) and induces private and shared subspaces (Liu et al., 2017; Ruder et al., 2017).",2 Related work,[0],[0]
"These approaches, however, are not able to take into account relationships between labels that may aid in learning.",2 Related work,[0],[0]
"Another related direction is to train on disparate annotations of the same task (Chen et al., 2016; Peng et al., 2017).",2 Related work,[0],[0]
"In contrast, the different nature of our tasks requires a modelling of their label spaces.
",2 Related work,[0],[0]
"Semi-supervised learning There exists a wide range of semi-supervised learning algorithms, e.g., self-training, co-training, tri-training, EM, and combinations thereof, several of which have also been used in NLP.",2 Related work,[0],[0]
"Our approach is probably most closely related to an algorithm called coforest (Li and Zhou, 2007).",2 Related work,[0],[0]
"In co-forest, like here, each learner is improved with unlabeled instances labeled by the ensemble consisting of all the other learners.",2 Related work,[0],[0]
"Note also that several researchers have proposed using auxiliary tasks that are unsupervised (Plank et al., 2016; Rei, 2017), which also leads to a form of semi-supervised models.
",2 Related work,[0],[0]
Label transformations The idea of manually mapping between label sets or learning such a mapping to facilitate transfer is not new.,2 Related work,[0],[0]
"Zhang et al. (2012) use distributional information to map from a language-specific tagset to a tagset used for other languages, in order to facilitate crosslingual transfer.",2 Related work,[0],[0]
"More related to this work, Kim et al. (2015) use canonical correlation analysis to transfer between tasks with disparate label spaces.",2 Related work,[0],[0]
"There has also been work on label transformations
in the context of multi-label classification problems (Yeh et al., 2017).",2 Related work,[0],[0]
"In our multi-task learning scenario, we have access to labelled datasets for T tasks T1, . . .",3.1 Problem definition,[0],[0]
", TT at training time with a target task TT that we particularly care about.",3.1 Problem definition,[0],[0]
The training dataset for task Ti consists of Nk examples XTi =,3.1 Problem definition,[0],[0]
"{xTi1 , . . .",3.1 Problem definition,[0],[0]
", xTiNk}",3.1 Problem definition,[0],[0]
"and their labels YTi = {yTi1 , . . .",3.1 Problem definition,[0],[0]
",yTiNk}.",3.1 Problem definition,[0],[0]
"Our base model is a deep neural network that performs classic hard parameter sharing (Caruana, 1993):",3.1 Problem definition,[0],[0]
"It shares its parameters across tasks and has task-specific softmax output layers, which output a probability distribution pTi for task Ti according to the following equation:
pTi = softmax(WTih+ bTi)",3.1 Problem definition,[0],[0]
"(1)
where softmax(x) = ex/ ∑‖x‖
i=1",3.1 Problem definition,[0],[0]
"e xi , WTi ∈
RLi×h, bTi ∈ RLi is the weight matrix and bias term of the output layer of task Ti respectively, h ∈",3.1 Problem definition,[0],[0]
"Rh is the jointly learned hidden representation, Li is the number of labels for task Ti, and h is the dimensionality of h.
The MTL model is then trained to minimise the sum of the individual task losses:
L = λ1L1",3.1 Problem definition,[0],[0]
+ .,3.1 Problem definition,[0],[0]
.,3.1 Problem definition,[0],[0]
.+,3.1 Problem definition,[0],[0]
"λTLT (2) where Li is the negative log-likelihood objec-
tive Li = H(pTi ,yTi)",3.1 Problem definition,[0],[0]
=,3.1 Problem definition,[0],[0]
− 1N ∑ n ∑ j logp Ti j y Ti j,3.1 Problem definition,[0],[0]
and λi is a parameter that determines the weight of task Ti.,3.1 Problem definition,[0],[0]
"In practice, we apply the same weight to all tasks.",3.1 Problem definition,[0],[0]
We show the full set-up in Figure 1a.,3.1 Problem definition,[0],[0]
"In order to learn the relationships between labels, we propose a Label Embedding Layer (LEL) that embeds the labels of all tasks in a joint space.",3.2 Label Embedding Layer,[0],[0]
"Instead of training separate softmax output layers as above, we introduce a label compatibility function c(·, ·) that measures how similar a label with embedding l is to the hidden representation h:
c(l,h) =",3.2 Label Embedding Layer,[0],[0]
l · h (3) where · is the dot product.,3.2 Label Embedding Layer,[0],[0]
This is similar to the Universal Schema Latent Feature Model introduced by Riedel et al. (2013).,3.2 Label Embedding Layer,[0],[0]
"In contrast to
12/6/2017 multi-task_learning.html
1/2
12/6/2017 label_embedding_layer.html
1/2
12/6/2017 label_transfer_network.html
2/3
other models that use the dot product in the objective function, we do not have to rely on negative sampling and a hinge loss (Collobert and Weston, 2008) as negative instances (labels) are known.",3.2 Label Embedding Layer,[0],[0]
"For efficiency purposes, we use matrix multiplication instead of a single dot product and softmax instead of sigmoid activations:
p = softmax(Lh) (4)
where L ∈ R( ∑
i Li)×l is the label embedding matrix for all tasks and l is the dimensionality of the label embeddings.",3.2 Label Embedding Layer,[0],[0]
"In practice, we set l to the hidden dimensionality h.",3.2 Label Embedding Layer,[0],[0]
We use padding if l < h.,3.2 Label Embedding Layer,[0],[0]
We apply a task-specific mask to L in order to obtain a task-specific probability distribution pTi .,3.2 Label Embedding Layer,[0],[0]
"The LEL is shared across all tasks, which allows us to learn the relationships between the labels in the joint embedding space.",3.2 Label Embedding Layer,[0],[0]
We show MTL with the LEL in Figure 1b.,3.2 Label Embedding Layer,[0],[0]
The LEL allows us to learn the relationships between labels.,3.3 Label Transfer Network,[0],[0]
"In order to make use of these relationships, we would like to leverage the predictions of our auxiliary tasks to estimate a label for the target task.",3.3 Label Transfer Network,[0],[0]
"To this end, we introduce the Label Transfer Network (LTN).",3.3 Label Transfer Network,[0],[0]
This network takes the auxiliary task outputs as input.,3.3 Label Transfer Network,[0],[0]
"In particular, we define the output label embedding oi of task Ti as
the sum of the task’s label embeddings lj weighted with their probability",3.3 Label Transfer Network,[0],[0]
"pTij :
oi =
Li∑
j=1
pTij lj (5)
",3.3 Label Transfer Network,[0],[0]
"The label embeddings l encode general relationship between labels, while the model’s probability distribution pTi over its predictions encodes finegrained information useful for learning (Hinton et al., 2015).",3.3 Label Transfer Network,[0],[0]
The LTN is trained on labelled target task data.,3.3 Label Transfer Network,[0],[0]
"For each example, the corresponding label output embeddings of the auxiliary tasks are fed into a multi-layer perceptron (MLP), which is trained with a negative log-likelihood objective LLTN to produce a pseudo-label zTT for the target task TT :
LTNT = MLP([o1, . . .",3.3 Label Transfer Network,[0],[0]
",oT−1]) (6)
where [·, ·] designates concatenation.",3.3 Label Transfer Network,[0],[0]
The mapping of the tasks in the LTN yields another signal that can be useful for optimisation and act as a regulariser.,3.3 Label Transfer Network,[0],[0]
"The LTN can also be seen as a mixtureof-experts layer (Jacobs et al., 1991) where the experts are the auxiliary task models.",3.3 Label Transfer Network,[0],[0]
"As the label embeddings are learned jointly with the main model, the LTN is more sensitive to the relationships between labels than a separately learned mixture-of-experts model that only relies on the experts’ output distributions.",3.3 Label Transfer Network,[0],[0]
"As such, the LTN
can be directly used to produce predictions on unseen data.",3.3 Label Transfer Network,[0],[0]
"The downside of the LTN is that it requires additional parameters and relies on the predictions of the auxiliary models, which impacts the runtime during testing.",3.4 Semi-supervised MTL,[0],[0]
"Instead, of using the LTN for prediction directly, we can use it to provide pseudolabels for unlabelled or auxiliary task data by utilising auxiliary predictions for semi-supervised learning.
",3.4 Semi-supervised MTL,[0],[0]
"We train the target task model on the pseudolabelled data to minimise the squared error between the model predictions pTi and the pseudo labels zTi produced by the LTN:
Lpseudo =MSE(pTT , zTT ) = ||pTT",3.4 Semi-supervised MTL,[0],[0]
"− zTT ||2 (7)
We add this loss term to the MTL loss in Equation 2.",3.4 Semi-supervised MTL,[0],[0]
"As the LTN is learned together with the MTL model, pseudo-labels produced early during training will likely not be helpful as they are based on unreliable auxiliary predictions.",3.4 Semi-supervised MTL,[0],[0]
"For this reason, we first train the base MTL model until convergence and then augment it with the LTN.",3.4 Semi-supervised MTL,[0],[0]
We show the full semi-supervised learning procedure in Figure 1c.,3.4 Semi-supervised MTL,[0],[0]
"When there is a domain shift between the datasets of different tasks as is common for instance when learning NER models with different label sets, the output label embeddings might not contain sufficient information to bridge the domain gap.
",3.5 Data-specific features,[0],[0]
"To mitigate this discrepancy, we augment the LTN’s input with features that have been found useful for transfer learning (Ruder and Plank, 2017).",3.5 Data-specific features,[0],[0]
"In particular, we use the number of word types, type-token ratio, entropy, Simpson’s index, and Rényi entropy as diversity features.",3.5 Data-specific features,[0],[0]
We calculate each feature for each example.1,3.5 Data-specific features,[0],[0]
The features are then concatenated with the input of the LTN.,3.5 Data-specific features,[0],[0]
Hard parameter sharing can be overly restrictive and provide a regularisation that is too heavy when jointly learning many tasks.,3.6 Other multi-task improvements,[0],[0]
"For this reason, we propose several additional improvements that seek
1For more information regarding the feature calculation, refer to Ruder and Plank (2017).
to alleviate this burden: We use skip-connections, which have been shown to be useful for multitask learning in recent work (Ruder et al., 2017).",3.6 Other multi-task improvements,[0],[0]
"Furthermore, we add a task-specific layer before the output layer, which is useful for learning taskspecific transformations of the shared representations (Søgaard and Goldberg, 2016; Ruder et al., 2017).",3.6 Other multi-task improvements,[0],[0]
"For our experiments, we evaluate on a wide range of text classification tasks.",4 Experiments,[0],[0]
"In particular, we choose pairwise classification tasks—i.e. those that condition the reading of one sequence on another sequence—as we are interested in understanding if knowledge can be transferred even for these more complex interactions.",4 Experiments,[0],[0]
"To the best of our knowledge, this is the first work on transfer learning between such pairwise sequence classification tasks.",4 Experiments,[0],[0]
"We implement all our models in Tensorflow (Abadi et al., 2016) and release the code at https://github.com/ coastalcph/mtl-disparate.",4 Experiments,[0],[0]
"We use the following tasks and datasets for our experiments, show task statistics in Table 1, and summarise examples in Table 2:
Topic-based sentiment analysis Topic-based sentiment analysis aims to estimate the sentiment of a tweet known to be about a given topic.",4.1 Tasks and datasets,[0],[0]
"We use the data from SemEval-2016 Task 4 Subtask B and C (Nakov et al., 2016) for predicting on a twopoint scale of positive and negative (Topic-2) and five-point scale ranging from highly negative to highly positive (Topic-5) respectively.",4.1 Tasks and datasets,[0],[0]
"An example from this dataset would be to classify the
tweet “No power at home, sat in the dark listening to AC/DC in the hope it’ll make the electricity come back again” known to be about the topic “AC/DC”, which is labelled as a positive sentiment.",4.1 Tasks and datasets,[0],[0]
"The evaluation metrics for Topic-2 and Topic-5 are macro-averaged recall (ρPN ) and macro-averaged mean absolute error (MAEM ) respectively, which are both averaged across topics.
",4.1 Tasks and datasets,[0],[0]
"Target-dependent sentiment analysis Targetdependent sentiment analysis (Target) seeks to classify the sentiment of a text’s author towards an entity that occurs in the text as positive, negative, or neutral.",4.1 Tasks and datasets,[0],[0]
We use the data from Dong et al. (2014).,4.1 Tasks and datasets,[0],[0]
An example instance is the expression “how do you like settlers of catan for the wii?” which is labelled as neutral towards the target “wii’.’,4.1 Tasks and datasets,[0],[0]
"The evaluation metric is macroaveraged F1 (FM1 ).
",4.1 Tasks and datasets,[0],[0]
"Aspect-based sentiment analysis Aspect-based sentiment analysis is the task of identifying
whether an aspect, i.e. a particular property of an item is associated with a positive, negative, or neutral sentiment (Ruder et al., 2016).",4.1 Tasks and datasets,[0],[0]
"We use the data of SemEval-2016 Task 5 Subtask 1 Slot 3 (Pontiki et al., 2016) for the laptops (ABSA-L) and restaurants (ABSA-R) domains.",4.1 Tasks and datasets,[0],[0]
"An example is the sentence “For the price, you cannot eat this well in Manhattan”, labelled as positive towards both the aspects “restaurant prices” and “food quality”.",4.1 Tasks and datasets,[0],[0]
"The evaluation metric for both domains is accuracy (Acc).
",4.1 Tasks and datasets,[0],[0]
"Stance detection Stance detection (Stance) requires a model, given a text and a target entity, which might not appear in the text, to predict whether the author of the text is in favour or against the target or whether neither inference is likely (Augenstein et al., 2016).",4.1 Tasks and datasets,[0],[0]
"We use the data of SemEval-2016 Task 6 Subtask B (Mohammad et al., 2016).",4.1 Tasks and datasets,[0],[0]
"An example from this dataset would be to predict the stance of the tweet “Be prepared - if we continue the policies of the liberal left, we will be #Greece” towards the topic “Donald Trump”, labelled as “favor”.",4.1 Tasks and datasets,[0],[0]
"The evaluation metric is the macro-averaged F1 score of the “favour” and “against” classes (FFA1 ).
",4.1 Tasks and datasets,[0],[0]
"Fake news detection The goal of fake news detection in the context of the Fake News Challenge2 is to estimate whether the body of a news article agrees, disagrees, discusses, or is unrelated towards a headline.",4.1 Tasks and datasets,[0],[0]
We use the data from the first stage of the Fake News Challenge (FNC-1).,4.1 Tasks and datasets,[0],[0]
"An example for this dataset is the document “Dino Ferrari hooked the whopper wels catfish, (...), which could be the biggest in the world.”",4.1 Tasks and datasets,[0],[0]
with the headline “Fisherman lands 19 STONE catfish which could be the biggest in the world to be hooked” labelled as “agree”.,4.1 Tasks and datasets,[0],[0]
"The evaluation metric is accuracy (Acc)3.
",4.1 Tasks and datasets,[0],[0]
"Natural language inference Natural language inference is the task of predicting whether one sentences entails, contradicts, or is neutral towards another one.",4.1 Tasks and datasets,[0],[0]
"We use the Multi-Genre NLI corpus (MultiNLI) from the RepEval 2017 shared task (Nangia et al., 2017).",4.1 Tasks and datasets,[0],[0]
"An example for an instance would be the sentence pair “Fun for only children”, “Fun for adults and children”, which are in a “contradiction” relationship.",4.1 Tasks and datasets,[0],[0]
"The evaluation metric is accuracy (Acc).
",4.1 Tasks and datasets,[0],[0]
2http://www.fakenewschallenge.org/ 3We use the same metric as Riedel et al. (2017).,4.1 Tasks and datasets,[0],[0]
"Our base model is the Bidirectional Encoding model (Augenstein et al., 2016), a state-of-theart model for stance detection that conditions a bidirectional LSTM (BiLSTM) encoding of a text on the BiLSTM encoding of the target.",4.2 Base model,[0],[0]
"Unlike Augenstein et al. (2016), we do not pre-train word embeddings on a larger set of unlabelled indomain text for each task as we are mainly interested in exploring the benefit of multi-task learning for generalisation.",4.2 Base model,[0],[0]
"We use BiLSTMs with one hidden layer of 100 dimensions, 100-dimensional randomly initialised word embeddings, a label embedding size of 100.",4.3 Training settings,[0],[0]
"We train our models with RMSProp, a learning rate of 0.001, a batch size of 128, and early stopping on the validation set of the main task with a patience of 3.",4.3 Training settings,[0],[0]
"Our main results are shown in Table 3, with a comparison against the state of the art.",5 Results,[0],[0]
"We present the results of our multi-task learning network with label embeddings (MTL + LEL), multi-task learning with label transfer (MTL + LEL + LTN), and the semi-supervised extension of this model.",5 Results,[0],[0]
"On 7/8 tasks, at least one of our architectures is better than single-task learning; and in 4/8, all our architectures are much better than single-task learning.
",5 Results,[0],[0]
"The state-of-the-art systems we compare against are often highly specialised, taskdependent architectures.",5 Results,[0],[0]
"Our architectures, in contrast, have not been optimised to compare
favourably against the state of the art, as our main objective is to develop a novel approach to multi-task learning leveraging synergies between label sets and knowledge of marginal distributions from unlabeled data.",5 Results,[0],[0]
"For example, we do not use pre-trained word embeddings (Augenstein et al., 2016; Palogiannidi et al., 2016; Vo and Zhang, 2015), class weighting to deal with label imbalance (Balikas and Amini, 2016), or domainspecific sentiment lexicons (Brun et al., 2016; Kumar et al., 2016).",5 Results,[0],[0]
"Nevertheless, our approach outperforms the state-of-the-art on two-way topic-based sentiment analysis (Topic-2).
",5 Results,[0],[0]
"The poor performance compared to the stateof-the-art on FNC and MultiNLI is expected; as we alternate among the tasks during training, our model only sees a comparatively small number of examples of both corpora, which are one and two orders of magnitude larger than the other datasets.",5 Results,[0],[0]
"For this reason, we do not achieve good performance on these tasks as main tasks, but they are still useful as auxiliary tasks as seen in Table 4.",5 Results,[0],[0]
"Our results above show that, indeed, modelling the similarity between tasks using label embeddings sometimes leads to much better performance.",6.1 Label Embeddings,[0],[0]
Figure 2 shows why.,6.1 Label Embeddings,[0],[0]
"In Figure 2, we visualise the label embeddings of an MTL+LEL model trained on all tasks, using PCA.",6.1 Label Embeddings,[0],[0]
"As we can see, similar labels are clustered together across tasks, e.g. there are two positive clusters (middle-right and top-right), two negative clusters (middle-left and bottom-left), and two neutral clusters (middle-top
and middle-bottom).",6.1 Label Embeddings,[0],[0]
"Our visualisation also provides us with a picture of what auxilary tasks are beneficial, and to what extent we can expect synergies from multitask learning.",6.1 Label Embeddings,[0],[0]
"For instance, the notion of positive sentiment appears to be very similar across the topic-based and aspect-based tasks, while the conceptions of negative and neutral sentiment differ.",6.1 Label Embeddings,[0],[0]
"In addition, we can see that the model has failed to learn a relationship between MultiNLI labels and those of other tasks, possibly accounting for its poor performance on the inference task.",6.1 Label Embeddings,[0],[0]
"We did not evaluate the correlation between label embeddings and task performance, but Bjerva (2017) recently suggested that mutual information of target and auxiliary task label sets is a good predictor of gains from multi-task learning.",6.1 Label Embeddings,[0],[0]
"For each task, we show the auxiliary tasks that achieved the best performance on the development data in Table 4.",6.2 Auxilary Tasks,[0],[0]
"In contrast to most existing work, we did not restrict ourselves to performing multitask learning with only one auxiliary task (Søgaard and Goldberg, 2016; Bingel and Søgaard, 2017).",6.2 Auxilary Tasks,[0],[0]
Indeed we find that most often a combination of auxiliary tasks achieves the best performance.,6.2 Auxilary Tasks,[0],[0]
Indomain tasks are less used than we assumed; only Target is consistently used by all Twitter main tasks.,6.2 Auxilary Tasks,[0],[0]
"In addition, tasks with a higher number of labels, e.g. Topic-5 are used more often.",6.2 Auxilary Tasks,[0],[0]
"Such tasks provide a more fine-grained reward signal, which may help in learning representations that generalise better.",6.2 Auxilary Tasks,[0],[0]
"Finally, tasks with large amounts
of training data such as FNC-1 and MultiNLI are also used more often.",6.2 Auxilary Tasks,[0],[0]
"Even if not directly related, the larger amount of training data that can be indirectly leveraged via multi-task learning may help the model focus on relevant parts of the representation space (Caruana, 1993).",6.2 Auxilary Tasks,[0],[0]
"These observations shed additional light on when multi-task learning may be useful that go beyond existing studies (Bingel and Søgaard, 2017).",6.2 Auxilary Tasks,[0],[0]
"We now perform a detailed ablation analysis of our model, the results of which are shown in Table 5.",6.3 Ablation analysis,[0],[0]
"We ablate whether to use the LEL (+ LEL), whether to use the LTN (+ LTN), whether to use the LEL output or the main model output for prediction (main model output is indicated by , main model), and whether to use the LTN as a regulariser or for semi-supervised learning (semisupervised learning is indicated by + semi).",6.3 Ablation analysis,[0],[0]
"We further test whether to use diversity features (– diversity feats) and whether to use main model predictions for the LTN (+ main model feats).
",6.3 Ablation analysis,[0],[0]
"Overall, the addition of the Label Embedding Layer improves the performance over regular MTL in almost all cases.",6.3 Ablation analysis,[0],[0]
"To understand the performance of the LTN, we analyse learning curves of the relabelling function vs. the main model.",6.4 Label transfer network,[0],[0]
Examples for all tasks without semi-supervised learning are shown in Figure 3.,6.4 Label transfer network,[0],[0]
One can observe that the relabelling model does not take long to converge as it has fewer parameters than the main model.,6.4 Label transfer network,[0],[0]
"Once the relabelling model is learned alongside the main
model, the main model performance first stagnates, then starts to increase again.",6.4 Label transfer network,[0],[0]
"For some of the tasks, the main model ends up with a higher task score than the relabelling model.",6.4 Label transfer network,[0],[0]
"We hypothesise that the softmax predictions of other, even highly related tasks are less helpful for predicting main labels than the output layer of the main task model.",6.4 Label transfer network,[0],[0]
"At best, learning the relabelling model alongside the main model might act as a regulariser to the main model and thus improve the main model’s performance over a baseline MTL model, as it is the case for TOPIC-5 (see Table 5).
",6.4 Label transfer network,[0],[0]
"To further analyse the performance of the LTN, we look into to what degree predictions of the main model and the relabelling model for individual instances are complementary to one another.",6.4 Label transfer network,[0],[0]
"Or, said differently, we measure the percentage of correct predictions made only by the relabelling
model or made only by the main model, relative to the number of correct predictions overall.",6.4 Label transfer network,[0],[0]
Results of this for each task are shown in Table 6 for the LTN with and without semi-supervised learning.,6.4 Label transfer network,[0],[0]
"One can observe that, even though the relabelling function overall contributes to the score to a lesser degree than the main model, a substantial number of correct predictions are made by the relabelling function that are missed by the main model.",6.4 Label transfer network,[0],[0]
"This is most prominently pronounced for ABSA-R, where the proportion is 14.6.",6.4 Label transfer network,[0],[0]
We have presented a multi-task learning architecture that (i) leverages potential synergies between classifier functions relating shared representations with disparate label spaces and (ii) enables learning from mixtures of labeled and unlabeled data.,7 Conclusion,[0],[0]
We have presented experiments with combinations of eight pairwise sequence classification tasks.,7 Conclusion,[0],[0]
"Our results show that leveraging synergies between label spaces sometimes leads to big improvements, and we have presented a new state of the art for topic-based sentiment analysis.",7 Conclusion,[0],[0]
"Our analysis further showed that (a) the learned label embeddings were indicative of gains from multitask learning, (b) auxiliary tasks were often beneficial across domains, and (c) label embeddings almost always led to better performance.",7 Conclusion,[0],[0]
We also investigated the dynamics of the label transfer network we use for exploiting the synergies between disparate label spaces.,7 Conclusion,[0],[0]
Sebastian Ruder is supported by the Irish Research Council Grant Number EBPPG/2014/30 and Science Foundation Ireland Grant Number SFI/12/RC/2289.,Acknowledgments,[0],[0]
Anders Søgaard is supported by the ERC Starting Grant Number 313695.,Acknowledgments,[0],[0]
Isabelle Augenstein is supported by Eurostars grant Number E10138.,Acknowledgments,[0],[0]
We further gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.,Acknowledgments,[0],[0]
"We combine multi-task learning and semisupervised learning by inducing a joint embedding space between disparate label spaces and learning transfer functions between label embeddings, enabling us to jointly leverage unlabelled data and auxiliary, annotated datasets.",abstractText,[0],[0]
We evaluate our approach on a variety of sequence classification tasks with disparate label spaces.,abstractText,[0],[0]
We outperform strong single and multi-task baselines and achieve a new stateof-the-art for topic-based sentiment analysis.,abstractText,[0],[0]
Multi-task Learning of Pairwise Sequence Classification Tasks Over Disparate Label Spaces,title,[0],[0]
"In the multi-task learning setting (Caruana, 1997)",1. Introduction,[0],[0]
a learner is given a collection of prediction tasks that all need to be solved.,1. Introduction,[0],[0]
The hope is that the overall prediction quality can be improved by processing the tasks jointly and sharing information between them.,1. Introduction,[0],[0]
"Indeed, theoretical as well as experimental studies have shown that information transfer can reduce the amount of annotated examples per task needed to achieve good performance under various assumptions on how the learning tasks are related.
",1. Introduction,[0],[0]
"All existing multi-task learning approaches have in common, however, that they need at least some labeled training data for every task of interest.",1. Introduction,[0],[0]
"In this paper, we study a new and more challenging setting, in which for a subset of the tasks (typically the large majority) only unlabeled data
1IST Austria.",1. Introduction,[0],[0]
"Correspondence to: Anastasia Pentina <apentina@ist.ac.at>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
is available.",1. Introduction,[0],[0]
"In practice, it is highly desirable to be able to handle this situation for problems with a very large number of tasks, such as sentiment analysis for market studies: for different products different attributes",1. Introduction,[0],[0]
matter,1. Introduction,[0],[0]
"and, thus, each product should be have its own predictor and forms its own learning task.",1. Introduction,[0],[0]
"At the same time annotating data for each such task is prohibitive, especially when new products are constantly added to the market.",1. Introduction,[0],[0]
"Another example are prediction problems, for which the fixed cost of obtaining any labels for a task can be high, even when the variable cost per label are reasonable.",1. Introduction,[0],[0]
"This is a well-known issue when using crowd sourcing for data annotation: recruiting and training annotators first imposes a large overhead, and only afterwards many labels can be obtained within a short time and at a low cost.
",1. Introduction,[0],[0]
A distinctive feature of the setting we study is that it requires two types of information transfer: between the labeled tasks and from labeled to unlabeled ones.,1. Introduction,[0],[0]
"While the first type is common in multi-task learning, none of the existing multi-task methods is able to handle the second type.",1. Introduction,[0],[0]
"In contrast, information transfer from labeled to unlabeled tasks is commonly studied in domain adaptation research, where, however, transfer of the first type is typically not considered.",1. Introduction,[0],[0]
"Thus, the setting of multi-task learning with labeled and unlabeled tasks can be seen as a blend of traditional multi-task learning and domain adaptation.
",1. Introduction,[0],[0]
"In this work we focus on a transfer method that learns a predictor for every task of interest by minimizing a taskspecific convex combination of training errors on the labeled tasks (Ben-David et al., 2007; Mansour et al., 2009).",1. Introduction,[0],[0]
We choose this method because it allows us to capture both types of information transfer – between the labeled tasks and from labeled to unlabeled ones – in a unified fashion.,1. Introduction,[0],[0]
"Clearly, the success of this approach depends on the choice of the weights in the convex combinations.",1. Introduction,[0],[0]
"Moreover, one can expect it also to depend on the subset of labeled tasks as well, because some subsets of tasks might be more informative and representative than the others.",1. Introduction,[0],[0]
This suggests that it will be beneficial if the labeled subset is not arbitrary but if it can be chosen in a data-dependent way.,1. Introduction,[0],[0]
"We refer to this learning scenario, where initially every task is represented only by a set of unlabeled examples and the learner can choose for which tasks to request some labels, as active task selection.
",1. Introduction,[0],[0]
Our main result is a generalization bound that quantifies both of the aforementioned effects: it relates the total multitask error to quantities that depend on the subset of labeled tasks and on the task-specific weights used for information transfer.,1. Introduction,[0],[0]
"Using the computable quantities in the bound as an objective function and minimizing it numerically, we obtain a principled algorithm for selecting which tasks to have labeled (in the active task selection scenario) and for choosing task-specific weights and predictors for all tasks, labeled as well as unlabeled.",1. Introduction,[0],[0]
"We highlight the practical usefulness of the derived method by experiments on synthetic and real data.
",1. Introduction,[0],[0]
"The success of any information transfer approach, regardless whether it is applied in the multi-task or the domain adaptation scenario, depends on the relatedness between tasks of interest.",1. Introduction,[0],[0]
"Indeed, one cannot expect to benefit from information transfer between the labeled tasks or to be able to obtain solutions of reasonable quality for the unlabeled ones if the given tasks are completely unrelated.",1. Introduction,[0],[0]
An advantage of the method we propose is that from the associated generalization bound we can read off explicitly under which conditions the algorithm can be expected to succeed.,1. Introduction,[0],[0]
"In particular, it suggests that the proposed method is likely to succeed if the given set of tasks satisfies the following assumption of task smoothness: if two tasks are similar in their marginal distributions, then their optimal prediction functions are also likely to be similar.",1. Introduction,[0],[0]
A more formal definition will be given in Section 3.,1. Introduction,[0],[0]
"The task smoothness assumption resembles the classical smoothness assumption of semi-supervised learning (Chapelle et al., 2006).",1. Introduction,[0],[0]
"It can be expected to hold in many real-world settings with a large number of tasks, for example in the aforementioned case of sentiment analysis: if two products are described using similar words, these words would likely have similar connotation for both products.",1. Introduction,[0],[0]
"Note, also, that a similar assumption appears implicitly in (Blanchard et al., 2011).",1. Introduction,[0],[0]
Most existing multi-task learning methods work in the fully supervised setting and aim at improving the overall prediction quality by sharing information between the tasks.,1.1. Related Work,[0],[0]
"For this they employ different types of transfer: instance-transfer methods re-use training samples from different tasks (Crammer & Mansour, 2012), parametertransfer methods assume that the predictors for all tasks are similar to each other in some norm and exploit this fact through specific regularizers (Evgeniou & Pontil, 2004), representation-transfer approaches assume that the predictors for all tasks share a common (low-dimensional) representation that can be learned from the data (Argyriou et al., 2007; 2008).",1.1. Related Work,[0],[0]
"Follow-up works extended and generalized these concepts, e.g. by learning the relatedness of tasks (Saha et al., 2011; Kang et al., 2011) or sharing only
between subgroups of tasks (Xue et al., 2007; Kumar & Daumé III, 2012; Barzilai & Crammer, 2015).",1.1. Related Work,[0],[0]
"However, all of the above methods require at least some labeled data for each task.
",1.1. Related Work,[0],[0]
"To our knowledge, the only existing multi-task method that can be applied in the considered setting where for some tasks only unlabeled data is available is (Khosla et al., 2012).",1.1. Related Work,[0],[0]
"Motivated by the problem of dataset bias, this method relies on the assumption that different tasks are minor modifications (i.e. biased versions) of the same, true prediction problem.",1.1. Related Work,[0],[0]
"Similarly to (Evgeniou & Pontil, 2004), it uses specific regularizers and trains predictors for all tasks jointly as small perturbations of a common predictor, which corresponds to the hypothetical unbiased task and can potentially be applied to unseen problems.",1.1. Related Work,[0],[0]
"Thus, applied in the considered setting, this method provides one predictor for all unlabeled tasks and treats the labeled ones as slight variations of them.
",1.1. Related Work,[0],[0]
Information transfer from labeled to unlabeled tasks is the question typically studied in domain adaptation research.,1.1. Related Work,[0],[0]
"In fact, if the set of labeled tasks is fixed, any domain adaptation technique might be used to obtain solutions for unlabeled tasks, in particular those based on source reweighting (Shimodaira, 2000), representation learning (Pan et al., 2011; Glorot et al., 2011), or semisupervised transfer (Xing et al., 2007).",1.1. Related Work,[0],[0]
"However, by design all domain adaptation methods aim at finding the best predictor on a single target task given a fixed set of source tasks.",1.1. Related Work,[0],[0]
"Therefore none of them can readily be applied in the active task selection setting, where the learner needs to select the labeled tasks that would lead to good performance across all tasks.
",1.1. Related Work,[0],[0]
"A second related setting is zero-shot learning (Larochelle et al., 2008; Lampert et al., 2013; Palatucci et al., 2009), where contextual, usually semantic, information is used to solve a learning task for which no training data is available.",1.1. Related Work,[0],[0]
"The situation we are interested in is more specific than this, though, as we assume that unlabeled data of the tasks is available, not context in an arbitrary form.",1.1. Related Work,[0],[0]
"As we will show, this allows us to derive formal performance guarantees that zero-shot learning methods typically lack.
",1.1. Related Work,[0],[0]
"The active task selection scenario is directly related to the question of identifying a representative set of source tasks in domain adaptation, a question that has previously been raised in the context of sentiment analysis (Blitzer et al., 2007).",1.1. Related Work,[0],[0]
"It also shares some features with active learning, where the learner is given a set of unlabeled samples and can choose a subset to obtain labels for.",1.1. Related Work,[0],[0]
"A fundamental difference is, however, that in active learning the learner needs to find a single prediction function for all labeled and unlabeled data while in the multi-task setting each task, including unlabeled ones, potentially requires its own predictor.
",1.1. Related Work,[0],[0]
"In the multi-task or zero-shot setting, active learning has so far not found widespread use.",1.1. Related Work,[0],[0]
"Exemplary works in this direction are (Reichart et al., 2008; Saha et al., 2010; Gavves et al., 2015), which, however, use active learning on the level of training examples, not tasks.",1.1. Related Work,[0],[0]
"The idea of choosing tasks was used in active curriculum selection (Ruvolo & Eaton, 2013; Pentina et al., 2015), where the learner can influence the order in which tasks are processed.",1.1. Related Work,[0],[0]
However these methods nevertheless require annotated examples for all tasks of interest.,1.1. Related Work,[0],[0]
In the multi-task setting the learner observes a collection of prediction tasks and its goal is to learn all of them.,2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"Formally, we assume that there is a set of T tasks {〈D1, f1〉, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", 〈DT , fT 〉}, where each task t is defined by a marginal distributionDt over the input space X and a deterministic labeling function ft : X → Y .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"The goal of the learner is to find T predictors h1, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", hT in a hypothesis set H ⊂ {h : X → Y} that would minimize the average expected risk:
er(h1, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", hT )",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"= 1
T T∑ t=1 ert(ht)",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", (1)
where ert(ht)",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
= E x∼Dt,2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"`(ht(x), ft(x))",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
".
",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"In this work we concentrate on the case of binary classification tasks, Y = {−1, 1}, and 0/1-loss, `(y1, y2)",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
= 0,2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"if y1 = y2, and `(y1, y2) = 1 otherwise.
",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
In the fully-supervised setting the learner is given a training set of annotated examples for every task of interest.,2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"In contrast, we consider the scenario where every task t is represented by a set St = {xt1, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", xtn} of n unlabeled examples sampled i.i.d.",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
according to the marginal distribution Dt.,2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"For a subset of k tasks {i1, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", ik}, which are either predefined or, in the active scenario, can be selected based on the unlabeled data, the learner is given labels for a random subset Sij ⊂ Sij of m points.
",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"To obtain a predictor for any task, labeled or unlabeled, we consider a method that minimizes a convex combination of training errors of the labeled tasks.",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"This choice allows us to capture, in a unified fashion, both types of information transfer – between the labeled tasks and from labeled to unlabeled ones.",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"Formally, for a set of tasks I = {i1, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", ik} ⊂ {1, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", T} we define:
ΛI = { α ∈",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"[0, 1]T :
T∑ i=1",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
αi,2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"= 1; suppα ⊆ I
} (2)
for suppα = {i ∈ {1, . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
.,2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", T} : αi 6= 0}.",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"Given a weight vector α ∈ ΛI , the α-weighted empirical error of a hypoth-
esis h ∈ H is defined as follows: êrα(h) = ∑ i∈I αiêri(h), (3)
where êri(h) = 1
m ∑ (x,y)∈Si `(h(x), y).",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"(4)
In order to obtain a solution for any task t the learner minimizes êrαt(h) for some αt ∈ ΛI , where I is the set of labeled tasks, potentially in combination with some regularization.
",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"The success of this approach depends on the subset I of tasks that are labeled and on the weights α1, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", αT .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"The following theorem quantifies both of these effects and will later be used to chose α1, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", αT and potentially I in a principled way.",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
Theorem 1.,2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"Let d be the VC dimension of the hypothesis set H, k be the number of labeled tasks, S1, . . .",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
", ST be",2. MTL with Labeled and Unlabeled Tasks,[0],[0]
"i.i.d.∼ Di, and S1, . . .","T sets of size n each, where Si",[0],[0]
", ST be their random subsets of size m each, for which labels would be provided if the corresponding task is selected as labeled.","T sets of size n each, where Si",[0],[0]
Then for any δ > 0,"T sets of size n each, where Si",[0],[0]
"with probability at least 1 − δ over S1, . . .","T sets of size n each, where Si",[0],[0]
", ST and S1, . . .","T sets of size n each, where Si",[0],[0]
", ST uniformly for all choices of labeled tasks I = {i1, . . .","T sets of size n each, where Si",[0],[0]
", ik} and weights α1, . . .","T sets of size n each, where Si",[0],[0]
", αT ∈ ΛI , provided that they are fully determined by the unlabeled data only, and for all possible choices of h1, . . .","T sets of size n each, where Si",[0],[0]
", hT ∈ H the following inequality holds:
1
T T∑ t=1","T sets of size n each, where Si",[0],[0]
ert(ht)≤ 1 T T∑ t=1,"T sets of size n each, where Si",[0],[0]
êrαt(ht)+ 1 T T∑ t=1 ∑ i∈I αti,"T sets of size n each, where Si",[0],[0]
"disc(St, Si)
+ A
T ‖α‖2,1","T sets of size n each, where Si",[0],[0]
"+
B T ‖α‖1,2 +","T sets of size n each, where Si",[0],[0]
"C+D+ 1 T T∑ t=1 ∑ i∈I αtiλti, (5)
where
disc(St, Si) = max h,h′∈H
|êrSt(h, h′)− êrSi(h, h′)|
with êrSi(h, h ′) =","T sets of size n each, where Si",[0],[0]
1n,"T sets of size n each, where Si",[0],[0]
"∑n j=1 `(h(x i j), h
′(xij)) is the empirical discrepancy between unlabeled samples St and Si, and
λij = min h∈H (eri(h) + erj(h))
‖α‖2,1 = T∑ t=1 √∑ i∈I (αti) 2, ‖α‖1,2 = √√√√∑ i∈I ( T∑ t=1 αti )2 ,
A =
√ 2d log(ekm/d)
m , B =
√ log(4/δ)
2m
C =
√ 8(log T + d log(enT/d))
","T sets of size n each, where Si",[0],[0]
n,"T sets of size n each, where Si",[0],[0]
"+
√ 2
n log
4 δ ,
D = 2
√ 2d log(2n)","T sets of size n each, where Si",[0],[0]
"+ 2 log(T ) + log(4/δ)
","T sets of size n each, where Si",[0],[0]
"n .
","T sets of size n each, where Si",[0],[0]
Proof Sketch (the full proof can be found in the supplemental material).,"T sets of size n each, where Si",[0],[0]
"By Theorem 2 in (Ben-David et al., 2010), for any two tasks t","T sets of size n each, where Si",[0],[0]
"and i the following inequality holds for every h ∈ H:
ert(h) ≤","T sets of size n each, where Si",[0],[0]
eri(h),"T sets of size n each, where Si",[0],[0]
"+ disc(Dt, Di) + λti.","T sets of size n each, where Si",[0],[0]
"(6)
Thus, we obtain the following bound on the average expected error over all tasks in terms of the error on the labeled tasks:
1
T T∑ t=1","T sets of size n each, where Si",[0],[0]
ert(ht)≤ 1 T T∑ t=1,"T sets of size n each, where Si",[0],[0]
"erαt(ht) (7)
+ 1
T T∑ t=1 ∑ i∈I αti disc(Dt, Di) + 1 T T∑ t=1 ∑ i∈I αtiλti,
with erαt(ht) = ∑ i∈I αti E x∼Di","T sets of size n each, where Si",[0],[0]
"`(ht(x), fi(x)), and (8)
erDi(h, h ′) =","T sets of size n each, where Si",[0],[0]
"E x∼Di `(h(x), h′(x)), and (9)
disc(Dt, Di)= max h,h′∈H
| erDt(h, h′)−erDi(h, h′)| (10)
is the discrepancy between two distributions (Kifer et al., 2004; Mansour et al., 2009; Ben-David et al., 2010).","T sets of size n each, where Si",[0],[0]
"In order to prove the statement of the theorem we need to relate the α-weighted expected errors and discrepancies between the marginal distributions in (7) to their empirical estimates.
","T sets of size n each, where Si",[0],[0]
The proof consists of three steps.,"T sets of size n each, where Si",[0],[0]
"First, we show that, conditioned on the unlabeled data, 1T ∑T t=1 ẽrαt can be upper
bounded in terms of 1T ∑T t=1 êrαt , where:
ẽrα(h) = ∑ i∈I αiẽri(h) =","T sets of size n each, where Si",[0],[0]
∑,"T sets of size n each, where Si",[0],[0]
"i∈I αi n n∑ j=1 `(h(xij), fi(x i j)).
","T sets of size n each, where Si",[0],[0]
This quantity can be interpreted as a training error if the learner would receive the labels for all the samples for the chosen tasks I .,"T sets of size n each, where Si",[0],[0]
Note that in case of m = n this step is not needed and we can avoid the corresponding complexity terms.,"T sets of size n each, where Si",[0],[0]
In the second step we relate the average α-weighted expected errors to 1T ∑T t=1 ẽrαt .,"T sets of size n each, where Si",[0],[0]
"In the third step we conclude the proof by bounding the pairwise discrepancies in terms of their empirical estimates.
","T sets of size n each, where Si",[0],[0]
Step 1.,"T sets of size n each, where Si",[0],[0]
"Fix the unlabeled sets S1, . . .","T sets of size n each, where Si",[0],[0]
", ST .","T sets of size n each, where Si",[0],[0]
"They fully determine the choice of labeled tasks I and the weights α1, . . .","T sets of size n each, where Si",[0],[0]
", αT .","T sets of size n each, where Si",[0],[0]
"Therefore, conditioned on the unlabeled data, these quantities can be considered constant and the bound has to hold uniformly only with respect to h1, . . .","T sets of size n each, where Si",[0],[0]
", hT .
","T sets of size n each, where Si",[0],[0]
"In order to simplify the notation we assume that I = {1, . . .","T sets of size n each, where Si",[0],[0]
", k} and define:
Φ(S1, . . .","T sets of size n each, where Si",[0],[0]
", Sk) = sup h1,...,hT
1
T T∑ t=1 ẽrαt(ht)−","T sets of size n each, where Si",[0],[0]
"êrαt(ht).
(11)
Note that one could analyze this quantity using standard techniques from Rademacher analysis, if the labeled examples were sampled from the unlabeled sets i.i.d., i.e. with replacement.","T sets of size n each, where Si",[0],[0]
"However, since we assume that for every i Si is a subset of Si, i.e. the labeled examples are sampled randomly without replacement, there are dependencies between the labeled examples.","T sets of size n each, where Si",[0],[0]
"Therefore we utilize techniques from the literature on transductive learning (ElYaniv & Pechyony, 2007) instead.","T sets of size n each, where Si",[0],[0]
"We first apply Doob’s construction to Φ in order to obtain a martingale sequence and then use McDiarmid’s inequality for martingales (McDiarmid, 1989).","T sets of size n each, where Si",[0],[0]
"As a result we obtain that with probability at least 1− δ/4 over sampling labeled examples:
Φ ≤ E S1,...,Sk
Φ + 1
T √√√√ k∑ i=1","T sets of size n each, where Si",[0],[0]
( T∑ t=1 αti )2√ log(4/δ) 2m .,"T sets of size n each, where Si",[0],[0]
"(12)
Now we need to upper bound EΦ.","T sets of size n each, where Si",[0],[0]
"Using results from (Tolstikhin et al., 2014) and (Hoeffding, 1963)","T sets of size n each, where Si",[0],[0]
"we observe that:
E S1,...,Sk Φ(S1, . . .","T sets of size n each, where Si",[0],[0]
", Sk) ≤ E S̃1,...,S̃k Φ(S̃1, . . .","T sets of size n each, where Si",[0],[0]
", S̃k), (13)
where S̃i is a set of m points sampled from Si i.i.d.","T sets of size n each, where Si",[0],[0]
with replacement (in contrast to sampling without replacement corresponding to Si).,"T sets of size n each, where Si",[0],[0]
This means that we can upper bound the expectation of Φ over samples with dependencies by the expectation over independent samples.,"T sets of size n each, where Si",[0],[0]
"By doing so, applying the symmetrization trick, and introducing Rademacher random variables, we obtain that:
E S1,...,Sk Φ ≤ 1 T T∑ t=1 √√√√ k∑ i=1","T sets of size n each, where Si",[0],[0]
(αti) 2 · √ 2d log(ekm/d),"T sets of size n each, where Si",[0],[0]
m .,"T sets of size n each, where Si",[0],[0]
"(14)
A combination of (12) and (14) shows that (conditioned on the unlabeled data) with probability at least 1 − δ/4 over sampling labeled examples uniformly for all choices of h1, . . .","T sets of size n each, where Si",[0],[0]
", hT the following holds:
1
T T∑ t=1 ẽrαt(ht) ≤ 1 T T∑ t=1 êrαt(ht)+1 A T ‖α‖2,1 + B T ‖α‖1,2.
(15) Step 2.","T sets of size n each, where Si",[0],[0]
"Now we relate 1T ∑T t=1 ẽrαt to 1 T ∑T t=1 erαt .
","T sets of size n each, where Si",[0],[0]
"The choice of the tasks to label, I , the corresponding weights, α, and the predictors, h, all depend on the unlabeled data.","T sets of size n each, where Si",[0],[0]
"Therefore, we aim for a bound that is uniform in all three parameters.","T sets of size n each, where Si",[0],[0]
"We define:
Ψ(S1, . . .","T sets of size n each, where Si",[0],[0]
", ST ) =
sup I sup α1,...,αT∈ΛI sup h1,...,hT
1
T T∑ t=1","T sets of size n each, where Si",[0],[0]
T∑ i=1,"T sets of size n each, where Si",[0],[0]
"αti(eri(ht)− ẽri(ht)).
","T sets of size n each, where Si",[0],[0]
"The main instrument that we use here is a refined version of McDiarmid’s inequality, which is due to (Maurer, 2006).","T sets of size n each, where Si",[0],[0]
"It allows us to use the standard Rademacher analysis, while taking into account the internal structure of the weights α1, . . .","T sets of size n each, where Si",[0],[0]
", αT .","T sets of size n each, where Si",[0],[0]
"As a result we obtain that with probability at least 1 − δ/4 simultaneously for all choices of tasks to be labeled, I , weights α1, . . .","T sets of size n each, where Si",[0],[0]
", αT ∈ ΛI and hypotheses h1, . . .","T sets of size n each, where Si",[0],[0]
", hT :
1
T T∑ t=1","T sets of size n each, where Si",[0],[0]
"erαt(ht) ≤ 1 T T∑ t=1 ẽrαt(ht) + C. (16)
","T sets of size n each, where Si",[0],[0]
Step 3.,"T sets of size n each, where Si",[0],[0]
To conclude the proof we bound the pairwise discrepancies in terms of their finite sample estimates.,"T sets of size n each, where Si",[0],[0]
"According to Lemma 1 in (Ben-David et al., 2010) for any pair of tasks i, j and any δ > 0","T sets of size n each, where Si",[0],[0]
"with probability at least 1− δ:
disc(Di, Dj) ≤ disc(Si, Sj)+2 √ 2d log(2n)","T sets of size n each, where Si",[0],[0]
"+ log(2/δ)
","T sets of size n each, where Si",[0],[0]
"n .
","T sets of size n each, where Si",[0],[0]
We apply this result to every pair of tasks and combine the results using the uniform bound argument.,"T sets of size n each, where Si",[0],[0]
This yields the remaining two terms on the right hand side: the weighted average of the sample-based discrepancies and the constant D. By combining the result with (15) and (16) we obtain the statement of the theorem.,"T sets of size n each, where Si",[0],[0]
"The left-hand side of inequality (5) is the average expected error over all T tasks, the quantity of interest that the learner would like to minimize but cannot directly compute.",3. Explanation and Interpretation,[0],[0]
"It is upper-bounded by the sum of two complexity terms and five task-dependent terms: weighted training errors on the labeled tasks, weighted averages of the distances to the labeled tasks in terms of the empirical discrepancies, two mixed norms of the weights α and a weighted average of λ-s. The complexity terms C and D behave as O( √ d log(nT )/n) and converge to zero when the number of unlabeled examples per task, n, tends to infinity.",3. Explanation and Interpretation,[0],[0]
"In contrast, AT ‖α‖2,1 + B T ‖α‖1,2 in the worst case of
‖α‖2,1 = ‖α‖1,2 = T behaves as O( √ d log(km)/m) and converges to zero when the number of labeled examples per labeled task, m, tends to infinity.",3. Explanation and Interpretation,[0],[0]
"In order for these terms to be balanced, i.e. for the uncertainty coming from the estimation of discrepancy to not dominate the uncertainty from the estimation of the α-weighted risks, the number of unlabeled examples per task n should be significantly (for k T ) larger than m. However, this is not a strong limitation under the common assumption that obtaining enough unlabeled examples is significantly cheaper than annotated ones.
",3. Explanation and Interpretation,[0],[0]
"The remaining terms on the right-hand side of (5) depend on the set of labeled tasks I , the tasks-specific weights α-s and hypotheses h-s. Thus, by minimizing them with respect to these quantities one can expect to obtain values for them that are beneficial for solving all tasks of interest based on the given data.",3. Explanation and Interpretation,[0],[0]
"For the theorem to hold, the set of labeled tasks and the weights may not depend on the labels.",3. Explanation and Interpretation,[0],[0]
"The part of the bound that can be estimated based on the unlabeled data only, and therefore to select I (in the active scenario) and α1, . . .",3. Explanation and Interpretation,[0],[0]
", αT is:
1
T T∑ t=1 ∑ i∈I αti",3. Explanation and Interpretation,[0],[0]
"disc(St, Si) +",3. Explanation and Interpretation,[0],[0]
"A T ‖α‖2,1",3. Explanation and Interpretation,[0],[0]
+,3. Explanation and Interpretation,[0],[0]
"B T ‖α‖1,2.",3. Explanation and Interpretation,[0],[0]
"(17)
The first term in (17) is the average weighted distance from every task to the labeled ones, as measured by the discrepancy between the corresponding unlabeled training samples.",3. Explanation and Interpretation,[0],[0]
"This term suggests that for every task t the largest weight, i.e. the highest impact in terms of information transfer, should be put on a labeled task i that has a similar marginal distribution.",3. Explanation and Interpretation,[0],[0]
"Note that the employed ”similarity”, which is captured by the discrepancy, directly depends on the considered hypothesis class and loss function and, thus, is tailored to a particular setting of interest.",3. Explanation and Interpretation,[0],[0]
"At the same time, the mixed-norm terms ‖α‖1,2 and ‖α‖2,1 prevent the learner from putting all weight on the single closest labeled task and can be seen as some form of regularization.",3. Explanation and Interpretation,[0],[0]
"In particular, they encourage information transfer also between the labeled tasks, since minimizing just the first term in (17) for every labeled tasks i ∈",3. Explanation and Interpretation,[0],[0]
"I would result in all weight to be put on task i itself and nothing on other tasks, because by definition disc(Si, Si) = 0.
",3. Explanation and Interpretation,[0],[0]
"The first mixed-norm term, ‖α‖2,1 influences every αt independently and encourages the learner to use data from multiple labeled tasks for adaptation.",3. Explanation and Interpretation,[0],[0]
"Thus, it captures the intuition that sharing from multiple labeled tasks should improve the performance.",3. Explanation and Interpretation,[0],[0]
"In contrast, ‖α‖1,2 connects the weights for all tasks.",3. Explanation and Interpretation,[0],[0]
"This term suggests to label tasks that all would be equally useful, thus preventing spending resources on tasks that would be informative for only a few of the remaining ones.",3. Explanation and Interpretation,[0],[0]
"Also, it prevents the learner from having super-influential labeled tasks that share with too many others.",3. Explanation and Interpretation,[0],[0]
"Such cases would be very unstable in the worst case scenario: mistakes on such tasks would propagate and have a major effect on the overall performance.
",3. Explanation and Interpretation,[0],[0]
The effect of the mixed-norm terms can also be seen through the lens of the convergence rates.,3. Explanation and Interpretation,[0],[0]
"Indeed, as already mentioned above, in the case of every αt having only one non-zero component, ‖α‖2,1 and ‖α‖1,2 are equal to T and thus the convergence rate1 is Õ( √ 1/m).",3. Explanation and Interpretation,[0],[0]
"However, in the opposite extreme, if every αt weights all the labeled tasks equally, i.e. αti = 1/k for all t ∈ {1, . . .",3. Explanation and Interpretation,[0],[0]
", T} and
1Õ(·) is an analog of O(·) that hides logarithmic factors
",3. Explanation and Interpretation,[0],[0]
i ∈,3. Explanation and Interpretation,[0],[0]
"I , then ‖α‖2,1 = ‖α‖1,2 = T√k and the convergence rate improves to Õ( √ 1/km), which is the best one can expect from having a total of km labeled examples.
",3. Explanation and Interpretation,[0],[0]
"The only term on the right-hand side of (5) that depends on the hypotheses h1, . . .",3. Explanation and Interpretation,[0],[0]
", hT and can be used to make a favorable choice is the weighted training error on the labeled tasks.",3. Explanation and Interpretation,[0],[0]
"Thus, the generalization bound of Theorem 1 suggest the following algorithm (Figure 1):
Algorithm 1. 1. estimate pairwise discrepancies between the tasks
based on the unlabeled data 2.",3. Explanation and Interpretation,[0],[0]
"choose the tasks I to be labeled (in the active case)
and the weights α1, . . .",3. Explanation and Interpretation,[0],[0]
", αT by minimizing (17) 3. receive labels for the labeled tasks I 4.",3. Explanation and Interpretation,[0],[0]
"for every task t train a classifier by minimizing (3)
using the obtained weights",3. Explanation and Interpretation,[0],[0]
"αt.
Note, that this procedure is justified by Theorem 1: all choices are done in agreement with the conditions of the theorem and, because the inequality (5) holds uniformly for all eligible choices of labeled tasks, weights and predictors, the guarantees also hold for the resulting solution.
",3. Explanation and Interpretation,[0],[0]
"Algorithm 1 is guaranteed to perform well, if the solution it finds leads to a low value of the right-hand side of (5).",3. Explanation and Interpretation,[0],[0]
"By construction, it minimizes all data-dependent terms in (5), except for one quantity that cannot be estimated from the available data:
1
T T∑ t=1 ∑ i∈I αtiλti.",3. Explanation and Interpretation,[0],[0]
"(18)
While discrepancy captures the similarity between marginal distributions, the λ-terms reflect the similarity between labeling functions: for every pair of task, t, and labeled task, i ∈ I , the corresponding value λti is small if there exists a hypothesis that performs well on both tasks.",3. Explanation and Interpretation,[0],[0]
"Thus, Algorithm 1 can be expected to perform well, if for any two given tasks t and i that are close to each other in terms of discrepancy (and thus in the minimization of (17) the corresponding αti is large), there exists a hypothesis
that performs well on both of them (i.e. the corresponding λti is small).",3. Explanation and Interpretation,[0],[0]
"We refer to this property of the set of learning tasks as task smoothness.
",3. Explanation and Interpretation,[0],[0]
Training predictors for every task of interest using data from all labeled tasks improves the statistical guarantees of the learner.,3. Explanation and Interpretation,[0],[0]
"However, it results in empirical risk minimization on up to km samples for T different weighted combinations.",3. Explanation and Interpretation,[0],[0]
"Since we are most interested in the situation when T is large, one might be interested in way to reduce the amount of necessary computation.",3. Explanation and Interpretation,[0],[0]
"One way to do so is to drop the mixed-norm terms from the objective function (17), in which case it reduces to
1
T T∑ t=1 ∑ i∈I αti",3. Explanation and Interpretation,[0],[0]
"disc(St, Si).",3. Explanation and Interpretation,[0],[0]
"(19)
",3. Explanation and Interpretation,[0],[0]
This expression is linear in α and thus minimizing it for a fixed set I will lead to assigning each task to a single labeled task that is closest to it in terms of empirical discrepancy.,3. Explanation and Interpretation,[0],[0]
Each labeled task will be assigned to itself.,3. Explanation and Interpretation,[0],[0]
"Consequently, the learner must train only k predictors, one for each labeled task, using only its m samples.",3. Explanation and Interpretation,[0],[0]
The expression (19) can be seen as the k-medoids clustering objective with tasks corresponding to points in the space with (semi)metric defined by empirical discrepancy and labeled tasks correspond to the centers of the clusters.,3. Explanation and Interpretation,[0],[0]
"Thus, this method reduces to k-medoids clustering, resembling the suggestion of Blitzer et al. (2007).",3. Explanation and Interpretation,[0],[0]
"Note that, nevertheless, the conditions of Theorem 1 are fulfilled, and thus its guarantees will hold for the obtained solution.",3. Explanation and Interpretation,[0],[0]
"The guarantees will be more pessimistic, however, than those from Algorithm 1, as the minimization ignores parts of the bound (5) and will not use the potentially beneficial transfer between labeled tasks.",3. Explanation and Interpretation,[0],[0]
"To illustrate that the proposed algorithm can also be practically useful, we performed experiments on synthetic and real data.",4. Experiments,[0],[0]
"In both cases we choose H to be the set of all linear predictors with a bias term on X = Rd.
Synthetic data.",4. Experiments,[0],[0]
"We generate T = 1000 binary classifica-
tion tasks in R2.",4. Experiments,[0],[0]
"For each task t its marginal distribution Dt is a unit-variance Gaussian with mean µt chosen uniformly at random from the set [−5, 5]×",4. Experiments,[0],[0]
"[−5, 5].",4. Experiments,[0],[0]
"The label +1 is assigned to all points that have angle between 0 and π with µt (computed counter-clockwise), the other points are labeled −1.",4. Experiments,[0],[0]
"We use n = 1000 unlabeled and m = 100 labeled examples per task.
",4. Experiments,[0],[0]
Real Data.,4. Experiments,[0],[0]
"We curate a Multitask dataset of product reviews2 from the corpus of Amazon product data3 (McAuley et al., 2015a;b).",4. Experiments,[0],[0]
We select the products for which there are at least 300 positive reviews (with scores 4 or 5) and at least 300 negative reviews (with scores 1 or 2).,4. Experiments,[0],[0]
Each of the resulting 957 products we treat as a binary classification task of predicting whether a review is positive or negative.,4. Experiments,[0],[0]
"For every review we extract features by first pre-processing (removing all non-alphabetical characters, transforming the rest into lower case and removing stop words) and then applying the sentence embedding procedure of (Arora et al., 2017) using 25-dimensional GloVe word embedding (Pennington et al., 2014).",4. Experiments,[0],[0]
We use n = 500 unlabeled samples per task and label a subset of m = 400 examples for each of the selected tasks.,4. Experiments,[0],[0]
"The remaining data is used for testing.
",4. Experiments,[0],[0]
Methods.,4. Experiments,[0],[0]
"We evaluate the proposed method in the case when the set of labeled tasks is predefined (referred to as DA) by setting the set I to be a random subset of tasks and minimizing (17) only with respect to α-s and in the active task selection scenario where (17) is minimized
2 http://cvml.ist.ac.at/productreviews/ 3 http://jmcauley.ucsd.edu/data/amazon/
with respect to both I and α-s (referred to as Active DA).",4. Experiments,[0],[0]
"We compare these methods to a multi-task method based on (Khosla et al., 2012), also with random labeled tasks (the same ones as for DA).",4. Experiments,[0],[0]
"Specifically, we solve:
min w,v,b
C ( ‖w‖2+ 1
k ∑ t∈I ‖vt‖2 ) + 1−γ km ∑ t∈I,(x,y)∈St (wTx+b−y)2
+ γ
km ∑ t∈I ∑ (x,y)∈St ((wT + vTt )x+",4. Experiments,[0],[0]
"(b+ bt)− y)2 (20)
for γ ∈ {0, 0.1, . . .",4. Experiments,[0],[0]
", 1} and use (w, b) for making predictions on all unlabeled tasks and (w + vt, b + bt) for each labeled task t ∈ I .",4. Experiments,[0],[0]
"For every number of labeled tasks we report the result for γ that has the best test performance averaged over 10 repeats (denoted by Multi-task), as an upper performance bound on what could be achieved by model selection.
",4. Experiments,[0],[0]
We also evaluate the discussed simplification of the proposed methods that consists of minimizing (19).,4. Experiments,[0],[0]
We refer to these as DA-SS (for random predefined labeled tasks) and as Active DA-SS (in the active task selection scenario).,4. Experiments,[0],[0]
"The SS stands for single source, as in this setting, each task is solved based on information from only one labeled tasks.
",4. Experiments,[0],[0]
To provide further context for the results we also report the results of learning independent ridge regressions with access to labels for all tasks (denoted by Fully Labeled).,4. Experiments,[0],[0]
"However, this baseline has access to many more annotated examples in total than all other methods.",4. Experiments,[0],[0]
"In order to quantify this effect we also consider the setting when the learner
has access to labels for all tasks, but fewer of them: namely, when the number of labeled tasks is k, the number of labels per task is mk/T , i.e. the total amount of labeled examples is mk, the same as for all other methods.",4. Experiments,[0],[0]
In this case we evaluate two methods.,4. Experiments,[0],[0]
"The first one learns ridge regressions for every task independently and thus can be seen as a reference point for the methods that do not involve information transfer between the labeled tasks, i.e. DA-SS and Active DA-SS.",4. Experiments,[0],[0]
"The second reference method is based on (Evgeniou & Pontil, 2004) and consists of minimizing (20) with γ set to 1 and processing all tasks as labeled.",4. Experiments,[0],[0]
"This approach transfers information between all the tasks and therefore we refer to it when evaluating the methods that involve information transfer between the labeled tasks, i.e. DA, Active DA and Multi-task.
",4. Experiments,[0],[0]
Implementation.,4. Experiments,[0],[0]
"We estimate the empirical discrepancies between pairs of tasks by finding a hypothesis in H that minimizes the squared loss for the binary classification problem of separating the two sets of instances, as in (BenDavid et al., 2010).",4. Experiments,[0],[0]
To minimize (17) for a given set of labeled tasks we use gradient descent.,4. Experiments,[0],[0]
"It is also used as a subroutine when minimizing (17) with respect to both I and α-s, for which we employ the GraSP algorithm (Bahmani et al., 2013).",4. Experiments,[0],[0]
"Active DA-SS involves the minimization of the k-medoid risk (19), which we perform using a local search as in (Park & Jun, 2009).",4. Experiments,[0],[0]
"For both methods for the active task selection scenario we used the heuristic from k-means++ (Arthur & Vassilvitskii, 2007) for initialization.",4. Experiments,[0],[0]
To obtain classifiers for the individual tasks in all scenarios we use least-squares ridge regression.,4. Experiments,[0],[0]
"Regularization constants for all methods we selected from the set {0}∪{10−17, 10−16 . . .",4. Experiments,[0],[0]
"108} by 5×5-fold cross validation.
Results.",4. Experiments,[0],[0]
The results are shown in Figure 4.,4. Experiments,[0],[0]
"First, one can see that the proposed domain adaptation-inspired method DA outperforms the multi-task method (20).",4. Experiments,[0],[0]
This could be due to higher flexibility of DA compared to Multi-task as the latter provides only one predictor for all unlabeled tasks.,4. Experiments,[0],[0]
"Indeed, the difference is most apparent in the experiment with synthetic data, where by design there is no single predictor that could perform well on a large fraction of tasks.",4. Experiments,[0],[0]
"Results on the product reviews indicate that DA’s flexibility of learning a specific predictor for every task can be advantageous in more realistic scenarios as well.
",4. Experiments,[0],[0]
"Second, on both datasets both methods for active task selection, i.e. Active DA and Active DA-SS, outperform the corresponding passive methods, i.e. DA and DA-SS, systematically across various fractions of the labeled tasks.",4. Experiments,[0],[0]
"In particular, both active task selection methods require substantially fewer tasks labeled to achieve the same accuracy as their analogs with randomly chosen tasks.",4. Experiments,[0],[0]
"This confirms the intuition that selecting which tasks to label in a datadependent way is beneficial and demonstrates that Theo-
rem 1 is capable of capturing this effect.
",4. Experiments,[0],[0]
"Another interesting observation that can be made from the results in Figure 4 is that both active and passive domain adaptation-inspired methods clearly outperform the corresponding partially labeled baselines, especially for small fractions of labeled tasks.",4. Experiments,[0],[0]
"This indicates that having more labels for fewer tasks rather than only few labels for all tasks could be beneficial not only in terms of annotation costs, but also in terms of prediction accuracy.
",4. Experiments,[0],[0]
"As the number of labeled tasks gets larger, e.g. half of all tasks, the performance of the active task selection learner becomes almost identical to the performance of the Fully Labeled method, even improving over it in the case of multi-source transfer on synthetic data.",4. Experiments,[0],[0]
This confirms the intuition that in the case of many related tasks even a fraction of the tasks can contain enough information for solving all tasks.,4. Experiments,[0],[0]
In this work we introduced and studied a variant of multitask learning in which annotated data is available only for some of the tasks.,5. Conclusion,[0],[0]
"This setting combines aspects of traditional multi-task learning, namely the transfer of information between labeled tasks, with aspects typical for domain adaptation problems, namely transferring information from labeled tasks to solve tasks for which only unlabeled data is available.",5. Conclusion,[0],[0]
The success of the learner in this setting depends on the effectiveness of information transfer and informativeness of the set of labeled tasks.,5. Conclusion,[0],[0]
"We analyzed two scenarios: a passive one, in which the set of labeled tasks is predefined, and the active task selection scenario, in which the learner decides for which tasks to query labels.
",5. Conclusion,[0],[0]
Our main technical contribution is a generalization bound that quantifies the informativeness of the set of labeled tasks and the effectiveness of information transfer.,5. Conclusion,[0],[0]
We demonstrated how the bound can be used to make the choice of labeled tasks (in the active scenario) and to transfer information between the tasks in a principled way.,5. Conclusion,[0],[0]
We also showed how the terms in the bound have intuitive interpretations and provide guidance under which assumption of tasks relatedness the induced algorithm is expected to work well.,5. Conclusion,[0],[0]
"Our empirical evaluation demonstrated that the proposed methods work also well in practice.
",5. Conclusion,[0],[0]
For future work we plan to further exploit the idea of active learning in the multi-task setting.,5. Conclusion,[0],[0]
"In particular, we are interested in identifying whether by allowing the learner to make its decision on which tasks to label in an iterative way, rather than forcing it to choose all the tasks at the same time, one could obtain better learning guarantees as well as more effective learning methods.",5. Conclusion,[0],[0]
We thank Alexander Zimin and Marius Kloft for useful discussions.,Acknowledgments,[0],[0]
This work was in parts funded by the European Research Council under the European Union’s Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 308036.,Acknowledgments,[0],[0]
"In multi-task learning, a learner is given a collection of prediction tasks and needs to solve all of them.",abstractText,[0],[0]
"In contrast to previous work, which required that annotated training data is available for all tasks, we consider a new setting, in which for some tasks, potentially most of them, only unlabeled training data is provided.",abstractText,[0],[0]
"Consequently, to solve all tasks, information must be transferred between tasks with labels and tasks without labels.",abstractText,[0],[0]
"Focusing on an instance-based transfer method we analyze two variants of this setting: when the set of labeled tasks is fixed, and when it can be actively selected by the learner.",abstractText,[0],[0]
We state and prove a generalization bound that covers both scenarios and derive from it an algorithm for making the choice of labeled tasks (in the active case) and for transferring information between the tasks in a principled way.,abstractText,[0],[0]
We also illustrate the effectiveness of the algorithm by experiments on synthetic and real data.,abstractText,[0],[0]
Multi-task Learning with Labeled and Unlabeled Tasks,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1273–1283 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1117",text,[0],[0]
"Video captioning is the task of automatically generating a natural language description of the content of a video, as shown in Fig. 1.",1 Introduction,[0],[0]
It has various applications such as assistance to a visually impaired person and improving the quality of online video search or retrieval.,1 Introduction,[0],[0]
"This task has gained recent momentum in the natural language processing and computer vision communities, esp.",1 Introduction,[0],[0]
with the advent of powerful image processing features as well as sequence-to-sequence LSTM models.,1 Introduction,[0],[0]
"It
is also a step forward from static image captioning, because in addition to modeling the spatial visual features, the model also needs to learn the temporal across-frame action dynamics and the logical storyline language dynamics.
",1 Introduction,[0],[0]
"Previous work in video captioning (Venugopalan et al., 2015a; Pan et al., 2016b) has shown that recurrent neural networks (RNNs) are a good choice for modeling the temporal information in the video.",1 Introduction,[0],[0]
A sequence-to-sequence model is then used to ‘translate’ the video to a caption.,1 Introduction,[0],[0]
Venugopalan et al. (2016) showed linguistic improvements over this by fusing the decoder with external language models.,1 Introduction,[0],[0]
"Furthermore, an attention mechanism between the video frames and the caption words captures some of the temporal matching relations better (Yao et al., 2015; Pan et al., 2016a).",1 Introduction,[0],[0]
"More recently, hierarchical two-level RNNs were proposed to allow for longer inputs and to model the full paragraph caption dynamics of long video clips (Pan et al., 2016a; Yu et al., 2016).
",1 Introduction,[0],[0]
"Despite these recent improvements, video captioning models still suffer from the lack of sufficient temporal and logical supervision to be able to correctly capture the action sequence and storydynamic language in videos, esp.",1 Introduction,[0],[0]
in the case of short clips.,1 Introduction,[0],[0]
"Hence, they would benefit from incorporating such complementary directed knowledge, both visual and textual.",1 Introduction,[0],[0]
"We address this by jointly training the task of video captioning with two related directed-generation tasks: a temporally-
1273
directed unsupervised video prediction task and a logically-directed language entailment generation task.",1 Introduction,[0],[0]
"We model this via many-to-many multi-task learning based sequence-to-sequence models (Luong et al., 2016) that allow the sharing of parameters among the encoders and decoders across the three different tasks, with additional shareable attention mechanisms.
",1 Introduction,[0],[0]
"The unsupervised video prediction task, i.e., video-to-video generation (adapted from Srivastava et al. (2015)), shares its encoder with the video captioning task’s encoder, and helps it learn richer video representations that can predict their temporal context and action sequence.",1 Introduction,[0],[0]
"The entailment generation task, i.e., premise-to-entailment generation (based on the image caption domain SNLI corpus (Bowman et al., 2015)), shares its decoder with the video captioning decoder, and helps it learn better video-entailing caption representations, since the caption is essentially an entailment of the video, i.e., it describes subsets of objects and events that are logically implied by or follow from the full video content).",1 Introduction,[0],[0]
"The overall many-tomany multi-task model combines all three tasks.
",1 Introduction,[0],[0]
"Our three novel multi-task models show statistically significant improvements over the state-ofthe-art, and achieve the best-reported results (and rank) on multiple datasets, based on several automatic and human evaluations.",1 Introduction,[0],[0]
"We also demonstrate that video captioning, in turn, gives mutual improvements on the new multi-reference entailment generation task.",1 Introduction,[0],[0]
"Early video captioning work (Guadarrama et al., 2013; Thomason et al., 2014; Huang et al., 2013) used a two-stage pipeline to first extract a subject, verb, and object (S,V,O) triple and then generate a sentence based on it.",2 Related Work,[0],[0]
Venugopalan et al. (2015b) fed mean-pooled static frame-level visual features (from convolution neural networks pre-trained on image recognition) of the video as input to the language decoder.,2 Related Work,[0],[0]
"To harness the important frame sequence temporal ordering, Venugopalan et al. (2015a) proposed a sequence-to-sequence model with video encoder and language decoder RNNs.
",2 Related Work,[0],[0]
"More recently, Venugopalan et al. (2016) explored linguistic improvements to the caption decoder by fusing it with external language models.",2 Related Work,[0],[0]
"Moreover, an attention or alignment mechanism was added between the encoder and the decoder
to learn the temporal relations (matching) between the video frames and the caption words (Yao et al., 2015; Pan et al., 2016a).",2 Related Work,[0],[0]
"In contrast to static visual features, Yao et al. (2015) also considered temporal video features from a 3D-CNN model pretrained on an action recognition task.
",2 Related Work,[0],[0]
"To explore long range temporal relations, Pan et al. (2016a) proposed a two-level hierarchical RNN encoder which limits the length of input information and allows temporal transitions between segments.",2 Related Work,[0],[0]
Yu et al. (2016)’s hierarchical RNN generates sentences at the first level and the second level captures inter-sentence dependencies in a paragraph.,2 Related Work,[0],[0]
Pan et al. (2016b) proposed to simultaneously learn the RNN word probabilities and a visual-semantic joint embedding space that enforces the relationship between the semantics of the entire sentence and the visual content.,2 Related Work,[0],[0]
"Despite these useful recent improvements, video captioning still suffers from limited supervision and generalization capabilities, esp.",2 Related Work,[0],[0]
given the complex action-based temporal and story-based logical dynamics that need to be captured from short video clips.,2 Related Work,[0],[0]
"Our work addresses this issue by bringing in complementary temporal and logical knowledge from video prediction and textual entailment generation tasks (respectively), and training them together via many-to-many multi-task learning.
",2 Related Work,[0],[0]
"Multi-task learning is a useful learning paradigm to improve the supervision and the generalization performance of a task by jointly training it with related tasks (Caruana, 1998; Argyriou et al., 2007; Kumar and Daumé III, 2012).",2 Related Work,[0],[0]
"Recently, Luong et al. (2016) combined multi-task learning with sequence-to-sequence models, sharing parameters across the tasks’ encoders and decoders.",2 Related Work,[0],[0]
They showed improvements on machine translation using parsing and image captioning.,2 Related Work,[0],[0]
"We additionally incorporate an attention mechanism to this many-to-many multi-task learning approach and improve the multimodal, temporal-logical video captioning task by sharing its video encoder with the encoder of a video-to-video prediction task and by sharing its caption decoder with the decoder of a linguistic premise-to-entailment generation task.
",2 Related Work,[0],[0]
Image representation learning has been successful via supervision from very large object-labeled datasets.,2 Related Work,[0],[0]
"However, similar amounts of supervision are lacking for video representation learning.",2 Related Work,[0],[0]
"Srivastava et al. (2015) address this by propos-
ing unsupervised video representation learning via sequence-to-sequence RNN models, where they reconstruct the input video sequence or predict the future sequence.",2 Related Work,[0],[0]
"We model video generation with an attention-enhanced encoder-decoder and harness it to improve video captioning.
",2 Related Work,[0],[0]
"The task of recognizing textual entailment (RTE) is to classify whether the relationship between a premise and hypothesis sentence is that of entailment (i.e., logically follows), contradiction, or independence (neutral), which is helpful for several downstream NLP tasks.",2 Related Work,[0],[0]
"The recent Stanford Natural Language Inference (SNLI) corpus by Bowman et al. (2015) allowed training end-to-end neural networks that outperform earlier feature-based RTE models (Lai and Hockenmaier, 2014; Jimenez et al., 2014).",2 Related Work,[0],[0]
"However, directly generating the entailed hypothesis sentences given a premise sentence would be even more beneficial than retrieving or reranking sentence pairs, because most downstream generation tasks only come with the source sentence and not pairs.",2 Related Work,[0],[0]
"Recently, Kolesnyk et al. (2016) tried a sequenceto-sequence model for this on the original SNLI dataset, which is a single-reference setting and hence restricts automatic evaluation.",2 Related Work,[0],[0]
"We modify the SNLI corpus to a new multi-reference (and a more challenging zero train-test premise overlap) setting, and present a novel multi-task training setup with the related video captioning task (where the caption also entails a video), showing mutual improvements on both the tasks.",2 Related Work,[0],[0]
We first discuss a simple encoder-decoder model as a baseline reference for video captioning.,3 Models,[0],[0]
"Next, we improve this via an attention mechanism.",3 Models,[0],[0]
"Finally, we present similar models for the unsupervised video prediction and entailment generation tasks, and then combine them with video captioning via the many-to-many multi-task approach.",3 Models,[0],[0]
"Our baseline model is similar to the standard machine translation encoder-decoder RNN
model (Sutskever et al., 2014) where the final state of the encoder RNN is input as an initial state to the decoder RNN, as shown in Fig. 2.",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"The RNN is based on Long Short Term Memory (LSTM) units, which are good at memorizing long sequences due to forget-style gates (Hochreiter and Schmidhuber, 1997).",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"For video captioning, our input to the encoder is the video frame features1 {f1, f2, ..., fn} of length n, and the caption word sequence {w1, w2, ..., wm} of length m is generated during the decoding phase.",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
The distribution of the output sequence w.r.t.,3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"the input sequence is:
p(w1, ..., wm|f1, ..., fn) = m∏
t=1
p(wt|hdt )",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"(1)
where hdt is the hidden state at the t th time step of the decoder RNN, obtained from hdt−1 and wt−1 via the standard LSTM-RNN equations.",3.1 Baseline Sequence-to-Sequence Model,[0],[0]
The distribution p(wt|hdt ) is given by softmax over all the words in the vocabulary.,3.1 Baseline Sequence-to-Sequence Model,[0],[0]
"Our attention model architecture is similar to Bahdanau et al. (2015), with a bidirectional LSTMRNN as the encoder and a unidirectional LSTMRNN as the decoder, see Fig. 3.",3.2 Attention-based Model,[0],[0]
"At each time step t, the decoder LSTM hidden state hdt is a nonlinear recurrent function of the previous decoder hidden state hdt−1, the previous time-step’s generated word wt−1, and the context vector ct:
hdt = S(h d t−1, wt−1, ct) (2)
",3.2 Attention-based Model,[0],[0]
"1We use several popular image features such as VGGNet, GoogLeNet and Inception-v4.",3.2 Attention-based Model,[0],[0]
"Details in Sec. 4.1.
where ct is a weighted sum of encoder hidden states {hei}:
ct =
n∑
i=1
αt,ih e i (3)
These attention weights {αt,i} act as an alignment mechanism by giving higher weights to certain encoder hidden states which match that decoder time step better, and are computed as:
αt,i = exp(et,i)∑n k=1 exp(et,k)
(4)
where the attention function et,i is defined as:
et,i = w T",3.2 Attention-based Model,[0],[0]
tanh(W,3.2 Attention-based Model,[0],[0]
eah e,3.2 Attention-based Model,[0],[0]
"i +W d ah d t−1 + ba) (5)
where w, W ea , W d a , and ba are learned parameters.",3.2 Attention-based Model,[0],[0]
This attention-based sequence-to-sequence model (Fig. 3) is our enhanced baseline for video captioning.,3.2 Attention-based Model,[0],[0]
We next discuss similar models for the new tasks of unsupervised video prediction and entailment generation and then finally share them via multi-task learning.,3.2 Attention-based Model,[0],[0]
We model unsupervised video representation by predicting the sequence of future video frames given the current frame sequence.,3.3 Unsupervised Video Prediction,[0],[0]
"Similar to Sec. 3.2, a bidirectional LSTM-RNN encoder and an LSTM-RNN decoder is used, along with attention.",3.3 Unsupervised Video Prediction,[0],[0]
"If the frame level features of a video of length n are {f1, f2, ..., fn}, these are divided into two sets such that given the current frames {f1, f2, .., fk} (in its encoder), the model has to predict (decode) the rest of the frames {fk+1, fk+2, .., fn}.",3.3 Unsupervised Video Prediction,[0],[0]
"The motivation is that this
helps the video encoder learn rich temporal representations that are aware of their action-based context and are also robust to missing frames and varying frame lengths or motion speeds.",3.3 Unsupervised Video Prediction,[0],[0]
"The optimization function is defined as:
minimize φ
n−k∑
t=1
||fdt − ft+k||22 (6)
where φ are the model parameters, ft+k is the true future frame feature at decoder time step t and fdt is the decoder’s predicted future frame feature at decoder time step t, defined as:
fdt = S(h d t−1, f d t−1, ct) (7)
similar to Eqn. 2, with hdt−1 and f d t−1 as the previous time step’s hidden state and predicted frame feature respectively, and ct as the attentionweighted context vector.",3.3 Unsupervised Video Prediction,[0],[0]
"Given a sentence (premise), the task of entailment generation is to generate a sentence (hypothesis) which is a logical deduction or implication of the premise.",3.4 Entailment Generation,[0],[0]
Our entailment generation model again uses a bidirectional LSTM-RNN encoder and LSTM-RNN decoder with an attention mechanism (similar to Sec. 3.2).,3.4 Entailment Generation,[0],[0]
"If the premise sp is a sequence of words {wp1, wp2, ..., wpn} and the hypothesis sh is {wh1 , wh2 , ..., whm}, the distribution of the entailed hypothesis w.r.t.",3.4 Entailment Generation,[0],[0]
"the premise is:
p(wh1 , ..., w h m|wp1, ..., wpn) =
m∏
t=1
p(wht |hdt ) (8)
where the distribution p(wht |hdt ) is again obtained via softmax over all the words in the vocabulary and the decoder state hdt is similar to Eqn. 2.",3.4 Entailment Generation,[0],[0]
Multi-task learning helps in sharing information between different tasks and across domains.,3.5 Multi-Task Learning,[0],[0]
"Our primary aim is to improve the video captioning model, where visual content translates to a textual form in a directed (entailed) generation way.",3.5 Multi-Task Learning,[0],[0]
"Hence, this presents an interesting opportunity to share temporally and logically directed knowledge with both visual and linguistic generation tasks.",3.5 Multi-Task Learning,[0],[0]
"Fig. 4 shows our overall many-to-many multi-task model for jointly learning video captioning, unsupervised video prediction, and textual entailment generation.",3.5 Multi-Task Learning,[0],[0]
"Here, the video captioning task shares its video encoder (parameters) with the encoder of the video prediction task (one-to-many setting) so as to learn context-aware and temporally-directed visual representations (see Sec. 3.3).
",3.5 Multi-Task Learning,[0],[0]
"Moreover, the decoder of the video captioning task is shared with the decoder of the textual entailment generation task (many-to-one setting), thus helping generate captions that can ‘entail’, i.e., are logically implied by or follow from the video content (see Sec. 3.4).2",3.5 Multi-Task Learning,[0],[0]
"In both the one-tomany and the many-to-one settings, we also allow the attention parameters to be shared or separated.",3.5 Multi-Task Learning,[0],[0]
"The overall many-to-many setting thus improves both the visual and language representations of the video captioning model.
",3.5 Multi-Task Learning,[0],[0]
We train the multi-task model by alternately optimizing each task in mini-batches based on a mixing ratio.,3.5 Multi-Task Learning,[0],[0]
"Let αv, αf , and αe be the number of mini-batches optimized alternately from each of these three tasks – video captioning, unsupervised video future frames prediction, and entailment generation, resp.",3.5 Multi-Task Learning,[0],[0]
Then the mixing ratio is defined as αv(αv+αf+αe) :,3.5 Multi-Task Learning,[0],[0]
αf (αv+αf+αe) : αe(αv+αf+αe) .,3.5 Multi-Task Learning,[0],[0]
Video Captioning Datasets We report results on three popular video captioning datasets.,4.1 Datasets,[0],[0]
"First, we use the YouTube2Text or MSVD (Chen and Dolan, 2011) for our primary results, which con-
2Empirically, logical entailment helped captioning more than simple fusion with language modeling (i.e., partial sentence completion with no logical implication), because a caption also entails a video in a logically-directed sense and hence the entailment generation task matches the video captioning task better than language modeling.",4.1 Datasets,[0],[0]
"Moreover, a multi-task setup is more suitable to add directed information such as entailment (as opposed to pretraining or fusion with only the decoder).",4.1 Datasets,[0],[0]
"Details in Sec. 5.1.
tains 1970 YouTube videos in the wild with several different reference captions per video (40 on average).",4.1 Datasets,[0],[0]
"We also use MSR-VTT (Xu et al., 2016) with 10, 000 diverse video clips (from a video search engine) – it has 200, 000 video clipsentence pairs and around 20 captions per video; and M-VAD (Torabi et al., 2015) with 49, 000 movie-based video clips but only 1 or 2 captions per video, making most evaluation metrics (except paraphrase-based METEOR) infeasible.",4.1 Datasets,[0],[0]
We use the standard splits for all three datasets.,4.1 Datasets,[0],[0]
"Further details about all these datasets are provided in the supplementary.
",4.1 Datasets,[0],[0]
"Video Prediction Dataset For our unsupervised video representation learning task, we use the UCF-101 action videos dataset (Soomro et al., 2012), which contains 13, 320 video clips of 101 action categories, and suits our video captioning task well because it also contains short video clips of a single action or few actions.",4.1 Datasets,[0],[0]
"We use the standard splits – further details in supplementary.
",4.1 Datasets,[0],[0]
"Entailment Generation Dataset For the entailment generation encoder-decoder model, we use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), which contains human-annotated English sentence pairs with classification labels of entailment, contradiction and neutral.",4.1 Datasets,[0],[0]
"It has a total of 570, 152 sentence pairs out of which 190, 113 correspond to true entailment pairs, and we use this subset in our multi-task video captioning model.",4.1 Datasets,[0],[0]
"For improving video captioning, we use the same training/validation/test splits as provided by Bowman et al. (2015), which is 183, 416 training, 3, 329 validation, and 3, 368 testing pairs (for the entailment subset).
",4.1 Datasets,[0],[0]
"However, for the entailment generation multitask results (see results in Sec. 5.3), we modify the splits so as to create a multi-reference setup which can afford evaluation with automatic metrics.",4.1 Datasets,[0],[0]
A given premise usually has multiple entailed hypotheses but the original SNLI corpus is set up as single-reference (for classification).,4.1 Datasets,[0],[0]
"Due to this, the different entailed hypotheses of the same premise land up in different splits of the dataset (e.g., one in train and one in test/validation) in many cases.",4.1 Datasets,[0],[0]
"Therefore, we regroup the premiseentailment pairs and modify the split as follows: among the 190, 113 premise-entailment pairs subset of the SNLI corpus, there are 155, 898 unique premises; out of which 145, 822 have only one hy-
pothesis and we make this the training set, and the rest of them (10, 076) have more than one hypothesis, which we randomly shuffle and divide equally into test and validation sets, so that each of these two sets has approximately the same distribution of the number of reference hypotheses per premise.
",4.1 Datasets,[0],[0]
"These new validation and test sets hence contain premises with multiple entailed hypotheses as ground truth references, thus allowing for automatic metric evaluation, where differing generations still get positive scores by matching one of the multiple references.",4.1 Datasets,[0],[0]
"Also, this creates a more challenging dataset for entailment generation because of zero premise overlap between the training and val/test sets.",4.1 Datasets,[0],[0]
"We will make these split details publicly available.
",4.1 Datasets,[0],[0]
"Pre-trained Visual Frame Features For the three video captioning and UCF-101 datasets, we fix our sampling rate to 3fps to bring uniformity in the temporal representation of actions across all videos.",4.1 Datasets,[0],[0]
"These sampled frames are then converted into features using several stateof-the-art pre-trained models on ImageNet (Deng et al., 2009) – VGGNet (Simonyan and Zisserman, 2015), GoogLeNet (Szegedy et al., 2015; Ioffe and Szegedy, 2015), and Inception-v4 (Szegedy et al., 2016).",4.1 Datasets,[0],[0]
Details of these feature dimensions and layer positions are in the supplementary.,4.1 Datasets,[0],[0]
"For our video captioning as well as entailment generation results, we use four diverse automatic evaluation metrics that are popular for image/video captioning and language generation in general: METEOR (Denkowski and Lavie, 2014), BLEU-4 (Papineni et al., 2002), CIDEr-D (Vedantam et al., 2015), and ROUGE-L (Lin, 2004).",4.2 Evaluation (Automatic and Human),[0],[0]
"Particularly, METEOR and CIDEr-D have been justified to be better for generation tasks, because CIDEr-D uses consensus among the (large) number of references and METEOR uses soft matching based on stemming, paraphrasing, and WordNet synonyms.",4.2 Evaluation (Automatic and Human),[0],[0]
"We use the standard evaluation code from the Microsoft COCO server (Chen et al., 2015) to obtain these results and also to compare the results with previous papers.3
We also present human evaluation results based
3We use avg.",4.2 Evaluation (Automatic and Human),[0],[0]
"of these four metrics on validation set to choose the best model, except for single-reference M-VAD dataset where we only report and choose based on METEOR.
",4.2 Evaluation (Automatic and Human),[0],[0]
"on relevance (i.e., how related is the generated caption w.r.t.",4.2 Evaluation (Automatic and Human),[0],[0]
"the video contents such as actions, objects, and events; or is the generated hypothesis entailed or implied by the premise) and coherence (i.e., a score on the logic, readability, and fluency of the generated sentence).",4.2 Evaluation (Automatic and Human),[0],[0]
"We tune all hyperparameters on the dev splits: LSTM-RNN hidden state size, learning rate, weight initializations, and mini-batch mixing ratios (tuning ranges in supplementary).",4.3 Training Details,[0],[0]
We use the following settings in all of our models (unless otherwise specified): we unroll video encoder/decoder RNNs to 50 time steps and language encoder/decoder RNNs to 30 time steps.,4.3 Training Details,[0],[0]
We use a 1024-dimension RNN hidden state size and 512-dim vectors to embed visual features and word vectors.,4.3 Training Details,[0],[0]
"We use Adam optimizer (Kingma and Ba, 2015).",4.3 Training Details,[0],[0]
We apply a dropout of 0.5.,4.3 Training Details,[0],[0]
See subsections below and supp for full details.,4.3 Training Details,[0],[0]
"Table 1 presents our primary results on the YouTube2Text (MSVD) dataset, reporting several previous works, all our baselines and attention model ablations, and our three multi-task models, using the four automated evaluation metrics.",5.1 Video Captioning on YouTube2Text,[0],[0]
"For each subsection below, we have reported the important training details inline, and refer to the supplementary for full details (e.g., learning rates and initialization).
",5.1 Video Captioning on YouTube2Text,[0],[0]
Baseline Performance We first present all our baseline model choices (ablations) in Table 1.,5.1 Video Captioning on YouTube2Text,[0],[0]
Our baselines represent the standard sequence-tosequence model with three different visual feature types as well as those with attention mechanisms.,5.1 Video Captioning on YouTube2Text,[0],[0]
Each baseline model is trained with three random seed initializations and the average is reported (for stable results).,5.1 Video Captioning on YouTube2Text,[0],[0]
"The final baseline model ⊗ instead uses an ensemble (E), which is a standard denoising method (Sutskever et al., 2014) that performs inference over ten randomly initialized models, i.e., at each time step t of the decoder, we generate a word based on the avg.",5.1 Video Captioning on YouTube2Text,[0],[0]
of the likelihood probabilities from the ten models.,5.1 Video Captioning on YouTube2Text,[0],[0]
"Moreover, we use beam search with size 5 for all baseline models.",5.1 Video Captioning on YouTube2Text,[0],[0]
"Overall, the final baseline model with Inceptionv4 features, attention, and 10-ensemble performs
well (and is better than all previous state-of-theart), and so we next add all our novel multi-task models on top of this final baseline.
",5.1 Video Captioning on YouTube2Text,[0],[0]
"Multi-Task with Video Prediction (1-to-M) Here, the video captioning and unsupervised video prediction tasks share their encoder LSTM-RNN weights and image embeddings in a one-to-many multi-task setting.",5.1 Video Captioning on YouTube2Text,[0],[0]
Two important hyperparameters tuned (on the validation set of captioning datasets) are the ratio of encoder vs decoder frames for video prediction on UCF-101 (where we found that 80% of frames as input and 20% for prediction performs best); and the mini-batch mixing ratio between the captioning and video prediction tasks (where we found 100 : 200 works well).,5.1 Video Captioning on YouTube2Text,[0],[0]
Table 1 shows a statistically significant improvement4 in all metrics in comparison to the best baseline (non-multitask) model as well as w.r.t.,5.1 Video Captioning on YouTube2Text,[0],[0]
"all previous works, demonstrating the effectiveness of multi-task learning for video captioning with video prediction, even with unsupervised signals.
",5.1 Video Captioning on YouTube2Text,[0],[0]
"Multi-Task with Entailment Generation (Mto-1) Here, the video captioning and entailment generation tasks share their language decoder LSTM-RNN weights and word embeddings in a many-to-one multi-task setting.",5.1 Video Captioning on YouTube2Text,[0],[0]
"We observe
4Statistical significance of p < 0.01 for CIDEr-D and ROUGE-L, p < 0.02 for BLEU-4, p < 0.03 for METEOR, based on the bootstrap test (Noreen, 1989; Efron and Tibshirani, 1994) with 100K samples.
that a mixing ratio of 100 : 50 alternating minibatches (between the captioning and entailment tasks) works well here.",5.1 Video Captioning on YouTube2Text,[0],[0]
"Again, Table 1 shows statistically significant improvements5 in all the metrics in comparison to the best baseline model (and all previous works) under this multi-task setting.",5.1 Video Captioning on YouTube2Text,[0],[0]
"Note that in our initial experiments, our entailment generation model helped the video captioning task significantly more than the alternative approach of simply improving fluency by adding (or deep-fusing) an external language model (or pre-trained word embeddings) to the decoder (using both in-domain and out-of-domain language models), again because a caption also ‘entails’ a video in a logically-directed sense and hence this matches our captioning task better (also see results of Venugopalan et al. (2016) in Table 1).
",5.1 Video Captioning on YouTube2Text,[0],[0]
"Multi-Task with Video and Entailment Generation (M-to-M) Combining the above one-tomany and many-to-one multi-task learning models, our full model is the 3-task, many-to-many model (Fig. 4) where both the video encoder and the language decoder of the video captioning model are shared (and hence improved) with that of the unsupervised video prediction and entailment generation models, respectively.6",5.1 Video Captioning on YouTube2Text,[0],[0]
"A mixing ratio of 100 : 100 : 50 alternate mini-batches
5Statistical significance of p < 0.01 for all four metrics.",5.1 Video Captioning on YouTube2Text,[0],[0]
"6We found the setting with unshared attention parameters to work best, likely because video captioning and video prediction prefer very different alignment distributions.
of video captioning, unsupervised video prediction, and entailment generation, resp.",5.1 Video Captioning on YouTube2Text,[0],[0]
works well.,5.1 Video Captioning on YouTube2Text,[0],[0]
"Table 1 shows that our many-to-many multi-task model again outperforms our strongest baseline (with statistical significance of p < 0.01 on all metrics), as well as all the previous state-of-theart results by large absolute margins on all metrics.",5.1 Video Captioning on YouTube2Text,[0],[0]
"It also achieves significant improvements on some metrics over the one-to-many and many-toone models.7 Overall, we achieve the best results to date on YouTube2Text (MSVD) on all metrics.",5.1 Video Captioning on YouTube2Text,[0],[0]
"In Table 2, we also train and evaluate our final many-to-many multi-task model on two other video captioning datasets (using their standard splits; details in supplementary).","5.2 Video Captioning on MSR-VTT, M-VAD",[0],[0]
"First, we evaluate on the new MSR-VTT dataset (Xu et al., 2016).","5.2 Video Captioning on MSR-VTT, M-VAD",[0],[0]
"Since this is a recent dataset, we list previous works’ results as reported by the MSR-VTT dataset paper itself.8 We improve over all of these significantly.","5.2 Video Captioning on MSR-VTT, M-VAD",[0],[0]
"Moreover, they maintain a leaderboard9 on this dataset and we also report the top 3 systems from it.","5.2 Video Captioning on MSR-VTT, M-VAD",[0],[0]
"Based on their ranking method, our multi-task model achieves the new rank 1 on this leaderboard.","5.2 Video Captioning on MSR-VTT, M-VAD",[0],[0]
"In Table 3, we further evaluate our model on the challenging movie-based M-VAD dataset, and again achieve improvements over all previous work (Venugopalan et al., 2015a;
7Many-to-many model’s improvements have a statistical significance of p < 0.01 on all metrics w.r.t. baseline, and p < 0.01 on CIDEr-D w.r.t.","5.2 Video Captioning on MSR-VTT, M-VAD",[0],[0]
"both one-to-many and many-toone models, and p < 0.04 on METEOR w.r.t.","5.2 Video Captioning on MSR-VTT, M-VAD",[0],[0]
"one-to-many.
","5.2 Video Captioning on MSR-VTT, M-VAD",[0],[0]
"8In their updated supplementary at https: //www.microsoft.com/en-us/research/wp-content/ uploads/2016/10/cvpr16.supplementary.pdf
9 http://ms-multimedia-challenge.com/leaderboard
Pan et al., 2016a; Yao et al., 2015).10","5.2 Video Captioning on MSR-VTT, M-VAD",[0],[0]
"Above, we showed that the new entailment generation task helps improve video captioning.",5.3 Entailment Generation Results,[0],[0]
"Next, we show that the video captioning task also inversely helps the entailment generation task.",5.3 Entailment Generation Results,[0],[0]
"Given a premise, the task of entailment generation is to generate an entailed hypothesis.",5.3 Entailment Generation Results,[0],[0]
"We use only the entailment pairs subset of the SNLI corpus for this, but with a multi-reference split setup to allow automatic metric evaluation and a zero traintest premise overlap (see Sec. 4.1).",5.3 Entailment Generation Results,[0],[0]
All the hyperparameter details (again tuned on the validation set) are presented in the supplementary.,5.3 Entailment Generation Results,[0],[0]
"Table 4 presents the entailment generation results for the baseline (sequence-to-sequence with attention, 3- ensemble, beam search) and the multi-task model which uses video captioning (shared decoder) on top of the baseline.",5.3 Entailment Generation Results,[0],[0]
A mixing ratio of 100 : 20 alternate mini-batches of entailment generation and video captioning (resp.) works well.11,5.3 Entailment Generation Results,[0],[0]
The multitask model achieves stat.,5.3 Entailment Generation Results,[0],[0]
"significant (p < 0.01) improvements over the baseline on all metrics, thus demonstrating that video captioning and entailment generation both mutually help each other.",5.3 Entailment Generation Results,[0],[0]
"In addition to the automated evaluation metrics, we present pilot-scale human evaluations on the YouTube2Text (Table 1) and entailment generation (Table 4) results.",5.4 Human Evaluation,[0],[0]
"In each case, we compare our strongest baseline with our final multi-task model by taking a random sample of 200 generated captions (or entailed hypotheses) from the test set and removing the model identity to anonymize the two models, and ask the human evaluator to choose the better model based on relevance and coherence (described in Sec. 4.2).",5.4 Human Evaluation,[0],[0]
"As shown in Table 5, the multi-task models are always better than the strongest baseline for both video captioning and entailment generation, on both relevance
10Following previous work, we only use METEOR because M-VAD only has a single reference caption per video.
11Note that this many-to-one model prefers a different mixing ratio and learning rate than the many-to-one model for improving video captioning (Sec. 5.1), because these hyperparameters depend on the primary task being improved, as also discussed in previous work (Luong et al., 2016).
and coherence, and with similar improvements (2- 7%) as the automatic metrics (shown in Table 1).",5.4 Human Evaluation,[0],[0]
"Fig. 5 shows video captioning generation results on the YouTube2Text dataset where our final M-to-M multi-task model is compared with our strongest attention-based baseline model for three categories of videos: (a) complex examples where the multi-task model performs better than the baseline; (b) ambiguous examples (i.e., ground truth itself confusing) where multi-task model still correctly predicts one of the possible categories (c) complex examples where both models perform poorly.",5.5 Analysis,[0],[0]
"Overall, we find that the multi-task model generates captions that are better at both temporal action prediction and logical entailment (i.e., correct subset of full video premise",5.5 Analysis,[0],[0]
) w.r.t.,5.5 Analysis,[0],[0]
the ground truth captions.,5.5 Analysis,[0],[0]
"The supplementary also provides
ablation examples of improvements by the 1-to-M video prediction based multi-task model alone, as well as by the M-to-1 entailment based multi-task model alone (over the baseline).
",5.5 Analysis,[0],[0]
"On analyzing the cases where the baseline is better than the final M-to-M multi-task model, we find that these are often scenarios where the multitask model’s caption is also correct but the baseline caption is a bit more specific, e.g., “a man is holding a gun” vs “a man is shooting a gun”.
",5.5 Analysis,[0],[0]
"Finally, Table 6 presents output examples of our entailment generation multi-task model (Sec. 5.3), showing how the model accurately learns to produce logically implied subsets of the premise.",5.5 Analysis,[0],[0]
"We presented a multimodal, multi-task learning approach to improve video captioning by incorporating temporally and logically directed knowledge via video prediction and entailment generation tasks.",6 Conclusion,[0],[0]
"We achieve the best reported results (and rank) on three datasets, based on multiple automatic and human evaluations.",6 Conclusion,[0],[0]
We also show mutual multi-task improvements on the new entailment generation task.,6 Conclusion,[0],[0]
"In future work, we are applying our entailment-based multi-task paradigm to other directed language generation tasks such as image captioning and document summarization.",6 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This work was partially supported by a Google Faculty Research Award, an IBM Faculty Award, a Bloomberg Data Science Research Grant, and NVidia GPU awards.",Acknowledgments,[0],[0]
"Video captioning, the task of describing the content of a video, has seen some promising improvements in recent years with sequence-to-sequence models, but accurately learning the temporal and logical dynamics involved in the task still remains a challenge, especially given the lack of sufficient annotated data.",abstractText,[0],[0]
"We improve video captioning by sharing knowledge with two related directed-generation tasks: a temporally-directed unsupervised video prediction task to learn richer context-aware video encoder representations, and a logically-directed language entailment generation task to learn better video-entailing caption decoder representations.",abstractText,[0],[0]
"For this, we present a many-to-many multi-task learning model that shares parameters across the encoders and decoders of the three tasks.",abstractText,[0],[0]
We achieve significant improvements and the new state-of-the-art on several standard video captioning datasets using diverse automatic and human evaluations.,abstractText,[0],[0]
We also show mutual multi-task improvements on the entailment generation task.,abstractText,[0],[0]
Multi-Task Video Captioning with Video and Entailment Generation,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2326–2335, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003).",1 Introduction,[0],[0]
"Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014).",1 Introduction,[0],[0]
The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector.,1 Introduction,[0],[0]
"A good representation of the variable-length text should fully capture the semantics of natural language.
",1 Introduction,[0],[0]
"Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al.,
∗Corresponding author
2015) and machine translation (Sutskever et al., 2014).",1 Introduction,[0],[0]
"LSTM is an extension of the recurrent neural network (RNN) (Elman, 1990), which can capture the long-term and short-term dependencies and is very suitable to model the variable-length texts.",1 Introduction,[0],[0]
"Besides, LSTM is also sensitive to word order and does not rely on the external syntactic structure as recursive neural network (Socher et al., 2013).",1 Introduction,[0],[0]
"However, when modeling long texts, such as documents, LSTM need to keep the useful features for a quite long period of time.",1 Introduction,[0],[0]
The longterm dependencies need to be transmitted one-byone along the sequence.,1 Introduction,[0],[0]
Some important features could be lost in transmission process.,1 Introduction,[0],[0]
"Besides, the error signal is also back-propagated one-byone through multiple time steps in the training phase with back-propagation through time (BPTT) (Werbos, 1990) algorithm.",1 Introduction,[0],[0]
The learning efficiency could also be decreased for the long texts.,1 Introduction,[0],[0]
"For example, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document.
",1 Introduction,[0],[0]
"In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) to capture the valuable information with different timescales.",1 Introduction,[0],[0]
"Inspired by the works of (El Hihi and Bengio, 1995) and (Koutnik et al., 2014), we partition the hidden states of the standard LSTM into several groups.",1 Introduction,[0],[0]
Each group is activated and updated at different time periods.,1 Introduction,[0],[0]
"The fast-speed groups keep the short-term memories, while the slow-speed groups keep the long-term memories.",1 Introduction,[0],[0]
We evaluate our model on four benchmark datasets of text classification.,1 Introduction,[0],[0]
"Experimental results show that our model can not only handle short texts, but can model long texts.
",1 Introduction,[0],[0]
"Our contributions can be summarized as follows.
",1 Introduction,[0],[0]
"• With the multiple different timescale memories, MT-LSTM easily carries the crucial information over a long distance.",1 Introduction,[0],[0]
"MT-LSTM
2326
can well model both short and long texts.
",1 Introduction,[0],[0]
• MT-LSTM has faster convergence speed than the standard LSTM since the error signal can be back-propagated through multiple timescales in the training phase.,1 Introduction,[0],[0]
The primary role of the neural models is to represent the variable-length sentence or document as a fixed-length vector.,2 Neural Models for Sentences and Documents,[0],[0]
"These models generally consist of a projection layer that maps words, subword units or n-grams to vector representations (often trained beforehand with unsupervised methods), and then combine them with the different architectures of neural networks.",2 Neural Models for Sentences and Documents,[0],[0]
"Most of these models for distributed representations of sentences or documents can be classified into four categories.
",2 Neural Models for Sentences and Documents,[0],[0]
"Bag-of-words models A simple and intuitive method is the Neural Bag-of-Words (NBOW) model, in which the representation of sentences or documents can be generated by averaging constituent word representations.",2 Neural Models for Sentences and Documents,[0],[0]
"However, the main drawback of NBOW is that the word order is lost.",2 Neural Models for Sentences and Documents,[0],[0]
"Although NBOW is effective for general document classification, it is not suitable for short sentences.
",2 Neural Models for Sentences and Documents,[0],[0]
"Sequence models Sequence models construct the representation of sentences or documents based on the recurrent neural network (RNN) (Mikolov et al., 2010) or the gated versions of RNN (Sutskever et al., 2014; Chung et al., 2014).",2 Neural Models for Sentences and Documents,[0],[0]
"Sequence models are sensitive to word order, but they have a bias towards the latest input words.",2 Neural Models for Sentences and Documents,[0],[0]
"This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts.",2 Neural Models for Sentences and Documents,[0],[0]
"Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN.
",2 Neural Models for Sentences and Documents,[0],[0]
"Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013).",2 Neural Models for Sentences and Documents,[0],[0]
"Recursive neural network (RecNN) adopts
a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013).",2 Neural Models for Sentences and Documents,[0],[0]
At every node in the tree the contexts at the left and right children of the node are combined by a classical layer.,2 Neural Models for Sentences and Documents,[0],[0]
The weights of the layer are shared across all nodes in the tree.,2 Neural Models for Sentences and Documents,[0],[0]
The layer computed at the top node gives a representation for the sentence.,2 Neural Models for Sentences and Documents,[0],[0]
"However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree.
",2 Neural Models for Sentences and Documents,[0],[0]
"Convolutional models Convolutional neural network (CNN) is also used to model sentences (Collobert et al., 2011; Kalchbrenner et al., 2014; Hu et al., 2014).",2 Neural Models for Sentences and Documents,[0],[0]
"It takes as input the embeddings of words in the sentence aligned sequentially, and summarizes the meaning of a sentence through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer.",2 Neural Models for Sentences and Documents,[0],[0]
CNN can maintain the word order information and learn more abstract characteristics.,2 Neural Models for Sentences and Documents,[0],[0]
"A recurrent neural network (RNN) (Elman, 1990) is able to process a sequence of arbitrary length by recursively applying a transition function to its internal hidden state vector ht of the input sequence.",3 Long Short-Term Memory Networks,[0],[0]
"The activation of the hidden state ht at time-step t is computed as a function f of the current input symbol xt and the previous hidden state ht−1
ht = { 0 t = 0 f(ht−1,xt) otherwise
(1)
",3 Long Short-Term Memory Networks,[0],[0]
"It is common to use the state-to-state transition function f as the composition of an element-wise nonlinearity with an affine transformation of both xt and ht−1.
",3 Long Short-Term Memory Networks,[0],[0]
"Traditionally, a simple strategy for modeling sequence is to map the input sequence to a fixedsized vector using one RNN, and then to feed the vector to a softmax layer for classification or other tasks (Sutskever et al., 2014; Cho et al., 2014).
",3 Long Short-Term Memory Networks,[0],[0]
"Unfortunately, a problem with RNNs with transition functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences (Bengio et al., 1994; Hochreiter et al., 2001; Hochreiter and Schmidhuber, 1997).",3 Long Short-Term Memory Networks,[0],[0]
"This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence.
",3 Long Short-Term Memory Networks,[0],[0]
"Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning longterm dependencies.",3 Long Short-Term Memory Networks,[0],[0]
The LSTM maintains a separate memory cell inside it that updates and exposes its content only when deemed necessary.,3 Long Short-Term Memory Networks,[0],[0]
A number of minor modifications to the standard LSTM unit have been made.,3 Long Short-Term Memory Networks,[0],[0]
"While there are numerous LSTM variants, here we describe the implementation used by Graves (2013).
",3 Long Short-Term Memory Networks,[0],[0]
"We define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units.",3 Long Short-Term Memory Networks,[0],[0]
"The entries of the gating vectors it, ft and ot are in [0, 1].",3 Long Short-Term Memory Networks,[0],[0]
"The LSTM transition equations are the following:
it = σ(Wixt + Uiht−1 + Vict−1) (2) ft = σ(Wfxt",3 Long Short-Term Memory Networks,[0],[0]
"+ Ufht−1 + Vfct−1), (3) ot = σ(Woxt + Uoht−1 + Voct), (4) c̃t = tanh(Wcxt + Ucht−1), (5) ct",3 Long Short-Term Memory Networks,[0],[0]
= f it ⊙,3 Long Short-Term Memory Networks,[0],[0]
ct−1,3 Long Short-Term Memory Networks,[0],[0]
+,3 Long Short-Term Memory Networks,[0],[0]
"it ⊙ c̃t, (6) ht = ot ⊙ tanh(ct), (7)
where xt is the input at the current time step, σ denotes the logistic sigmoid function and ⊙ denotes elementwise multiplication.",3 Long Short-Term Memory Networks,[0],[0]
"Intuitively, the forget gate controls the amount of which each unit of the memory cell is erased, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state.
",3 Long Short-Term Memory Networks,[0],[0]
Figure 1 shows the structure of a LSTM unit.,3 Long Short-Term Memory Networks,[0],[0]
"In
particular, these gates and the memory cell allow a LSTM unit to adaptively forget, memorize and expose the memory content.",3 Long Short-Term Memory Networks,[0],[0]
"If the detected feature, i.e., the memory content, is deemed important, the forget gate will be closed and carry the memory content across many time-steps, which is equivalent to capturing a long-term dependency.",3 Long Short-Term Memory Networks,[0],[0]
"On the other hand, the unit may decide to reset the memory content by opening the forget gate.",3 Long Short-Term Memory Networks,[0],[0]
"h1 h2 h3 h4 · · · hT softmax
x1 x2 x3 x4",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
xT,4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"y
(a) Unfolded LSTM
LSTM can capture the long-term and short-term dependencies in a sequence.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
But the long-term dependencies need to be transmitted one-by-one along the sequence.,4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"Some important information could be lost in transmission process for long texts, such as documents.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"Besides, the error signal is back-propagated through multiple time steps when we use the back-propagation through time (BPTT) (Werbos, 1990) algorithm.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
The training efficiency could also be low for the long texts.,4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"For example, if a valuable feature occurs at the begin of a long document, we need to back-propagate the error through the whole document.
",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"Inspired by the works of (El Hihi and Bengio, 1995) and (Koutnik et al., 2014), which use de-
layed connections and units operating at different timescales to improve the simple RNN, we separate the LSTM units into several groups.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"Different groups capture different timescales dependencies.
",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"More formally, the LSTM units are partitioned into g groups {G1, · · · , Gg}.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"Each group Gk, (1 ≤ k ≤ g) is activated at different time periods Tk.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"Accordingly, the gates and weight matrices are also partitioned to maintain the corresponding LSTM groups.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"The MT-LSTM with just one group is the same to the standard LSTM.
",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"At each time step t, only the groups Gk that satisfy (tMOD Tk) = 0 are executed.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"The choice of the set of periods Tk ∈ {T1, · · · , Tg} is arbitrary.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"Here, we use the exponential series of periods: group Gk has the period of Tk = 2k−1.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"The group G1 is the fastest one and can be executed at every time step, which works like the standard LSTM.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"The group Gk is the slowest one.
",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"At time step t, the memory cell vector and hidden state vector of group Gk are calculate in two cases:
(1) When group Gk is activated at time step t, the LSMT units of this group are calculated by the following equations:
ikt = σ(W k i xt + g∑ j=1",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"Uj→ki h j t−1 + g∑ j=1 Vj→ki c j t−1), (8)
fkt = σ(W k fxt + g∑ j=1 Uj→kf h j t−1 + g∑",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
j=1 Vj→kf,4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"c j t−1), (9)
okt = σ(W k oxt + g∑ j=1 Uj→ko h j t−1 + g∑ j=1 Vj→ko",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"c j t), (10)
",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"c̃kt = tanh(W k cxt + g∑ j=1 Uj→kc h j t−1), (11) ckt",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
= f k,4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
t ⊙,4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
ckt−1 + ikt,4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"⊙ c̃kt , (12) hkt = o k",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"t ⊙ tanh(ckt ), (13)
where ikt , f k t and o",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
k,4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"t are the vectors of input gates, forget gates, and output gates of group Gk at time step t respectively; ckt and h k t are the memory cell vector and hidden state vector of group",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"Gk at time step t respectively.
",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"(2) When group Gk is non-activated at time step t, its LSMT units keep unchanged.
",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"ckt = c k t−1, (14) hkt = h k t−1.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"(15)
Figure 3 shows the different between the standard LSTM and MT-LSTM.",4 Multi-Timescale Long Short-Term Memory Neural Network,[0],[0]
"The feedback mechanism of LSTM is implemented by the recurrent connections from time step t − 1 to t. Since the MT-LSTM groups are updated with the different frequencies, we can regard the different group as the human memory.",4.1 Two Feedback Strategies,[0],[0]
"The fast-speed groups are short-term memories, while the slow-speed groups are long-term memories.",4.1 Two Feedback Strategies,[0],[0]
"Therefore, an important consideration is what feedback mechanism is between the shortterm and long-term memories.
",4.1 Two Feedback Strategies,[0],[0]
"For the proposed MT-LSTM, we consider two feedback strategies to define the connectivity patterns among the different groups.
",4.1 Two Feedback Strategies,[0],[0]
"Fast-to-Slow (F2S) Strategy Intuitively, when we accumulate the short-term memory to a certain degree, we store some valuable information from the short-term memory into the long-term memory.",4.1 Two Feedback Strategies,[0],[0]
"Therefore, we firstly define a fast to slow strategy, which updates the slower group using the faster group.",4.1 Two Feedback Strategies,[0],[0]
The connections from group j to group k exist if and only if Tj ≤ Tk.,4.1 Two Feedback Strategies,[0],[0]
"The weight matrices Uj→ki , U j→k f , U j→k o , U j→k c , V j→k i , Vj→kf , V j→k o are set to zero when Tj > Tk.
",4.1 Two Feedback Strategies,[0],[0]
"The F2S updating strategy is shown in Figure 3a.
",4.1 Two Feedback Strategies,[0],[0]
"Slow-to-Fast (S2F) Strategy Following the work of (Koutnik et al., 2014), we also investigate another update scheme from slow-speed group to fast-speed group.",4.1 Two Feedback Strategies,[0],[0]
The motivation is that a long term memory can be “distilled” into a short-term memory.,4.1 Two Feedback Strategies,[0],[0]
The connections from group j to group i exist only if Tj ≥ Ti.,4.1 Two Feedback Strategies,[0],[0]
"The weight matrices Uj→ki , Uj→kf , U j→k o , U j→k c , V j→k i , V j→k f , V j→k o are set to zero when Tj < Tk.",4.1 Two Feedback Strategies,[0],[0]
The S2F update strategy is shown in Figure 3b.,4.1 Two Feedback Strategies,[0],[0]
Another consideration is how many groups need to be used.,4.2 Dynamic Selection of the Number of the MT-LSTM Unit Groups,[0],[0]
An intuitive way is that we need more groups for long texts than short texts.,4.2 Dynamic Selection of the Number of the MT-LSTM Unit Groups,[0],[0]
"The number of the group depends the length of the texts.
",4.2 Dynamic Selection of the Number of the MT-LSTM Unit Groups,[0],[0]
"Here, we use a simple dynamic strategy to choose the maximum number of groups, and then the best g is chosen as a hyperparameter according to different tasks.",4.2 Dynamic Selection of the Number of the MT-LSTM Unit Groups,[0],[0]
"The upper bound of the number of groups is calculated by
g = log2 L− 1, (16) where L is the average length of the corpus.",4.2 Dynamic Selection of the Number of the MT-LSTM Unit Groups,[0],[0]
"Thus, the slowest group is activated at least twice.",4.2 Dynamic Selection of the Number of the MT-LSTM Unit Groups,[0],[0]
"In each of the experiments, the hidden layer at the last moment has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence.",5 Training,[0],[0]
The network is trained to minimise the cross-entropy of the predicted and true distributions; the objective includes an L2 regularization term over the parameters.,5 Training,[0],[0]
"The network is trained with backpropagation and the gradientbased optimization is performed using the Adagrad update rule (Duchi et al., 2011).
",5 Training,[0],[0]
The back propagation of the error propagation is similar to LSTM as well.,5 Training,[0],[0]
"The only difference is that the error propagates only from groups that were executed at time step t. The error of nonactivated groups gets copied back in time (similarly to copying the activations of nodes not activated at the time step t during the corresponding forward pass), where it is added to the backpropagated error.",5 Training,[0.9531990261127293],['The error is propagated through multiple time steps in the recurrent structure until it reaches the encoder.']
"In this section, we investigate the empirical performances of our proposed MT-LSTM model on four benchmark datasets for sentence and document classification and then compare it to other competitor models.",6 Experiments,[0],[0]
We evaluate our model on four different datasets.,6.1 Datasets,[0],[0]
"The first three datasets are sentence-level, and the last dataset is document-level.",6.1 Datasets,[0],[0]
The detailed statistics about the four datasets are listed in Table 1.,6.1 Datasets,[0],[0]
"Each dataset is briefly described as follows.
",6.1 Datasets,[0],[0]
• SST-1,6.1 Datasets,[0],[0]
"The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013).",6.1 Datasets,[0],[0]
• SST-2,6.1 Datasets,[0],[0]
The movie reviews with binary classes.,6.1 Datasets,[0],[0]
It is also from the Stanford Sentiment Treebank. •,6.1 Datasets,[0],[0]
QC,6.1 Datasets,[0],[0]
"The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002).",6.1 Datasets,[0],[0]
• IMDB,6.1 Datasets,[0],[0]
"The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011).",6.1 Datasets,[0],[0]
One key aspect of this dataset is that each movie review has several sentences.,6.1 Datasets,[0],[0]
"We compare our model with the following models:
• NB-SVM and MNB.",6.2 Competitor Models,[0],[0]
"Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012).",6.2 Competitor Models,[0],[0]
•,6.2 Competitor Models,[0],[0]
NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer.,6.2 Competitor Models,[0],[0]
"• RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b).",6.2 Competitor Models,[0],[0]
"• MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012).
",6.2 Competitor Models,[0],[0]
1http://nlp.stanford.edu/sentiment.,6.2 Competitor Models,[0],[0]
"2http://cogcomp.cs.illinois.edu/Data/
QA/QC/. 3http://ai.stanford.edu/˜amaas/data/ sentiment/
• RNTN Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013).",6.2 Competitor Models,[0],[0]
"• AdaSent Self-adaptive hierarchical sentence model with gated mechanism (Zhao et al., 2015).",6.2 Competitor Models,[0],[0]
• DCNN,6.2 Competitor Models,[0],[0]
"Dynamic Convolutional Neural Network with dynamic k-max pooling (Kalchbrenner et al., 2014).",6.2 Competitor Models,[0],[0]
"• CNN-non-static and CNN-multichannel Convolutional Neural Network (Kim, 2014).",6.2 Competitor Models,[0],[0]
"• PV Logistic regression on top of paragraph vectors (Le and Mikolov, 2014).",6.2 Competitor Models,[0],[0]
"Here, we use the popular open source implementation of PV in Gensim4.",6.2 Competitor Models,[0],[0]
•,6.2 Competitor Models,[0],[0]
LSTM,6.2 Competitor Models,[0],[0]
The standard LSTM for text classification.,6.2 Competitor Models,[0],[0]
We use the implementation of Graves (2013).,6.2 Competitor Models,[0],[0]
The unfolded illustration is shown in Figure 2a.,6.2 Competitor Models,[0],[0]
"In all of our experiments, the word embeddings are trained using word2vec (Mikolov et al., 2013a) on the Wikipedia corpus (1B words).",6.3 Hyperparameters and Training,[0],[0]
"The vocabulary size is about 500,000.",6.3 Hyperparameters and Training,[0],[0]
"The the word embeddings are fine-tuned during training to improve the performance (Collobert et al., 2011).",6.3 Hyperparameters and Training,[0],[0]
"The other parameters are initialized by randomly sampling from uniform distribution in [-0.1, 0.1].",6.3 Hyperparameters and Training,[0],[0]
The hyperparameters which achieve the best performance on the development set will be chosen for the final evaluation.,6.3 Hyperparameters and Training,[0],[0]
"For datasets without development set, we use 10-fold cross-validation (CV) instead.",6.3 Hyperparameters and Training,[0],[0]
The final hyper-parameters for the LSTM and MTLSTM are set as Figure 2.,6.3 Hyperparameters and Training,[0],[0]
"Table 3 shows the classification accuracies of the standard LSTM, MT-LSTM compared with the competitor models.
",6.4 Results,[0],[0]
"Firstly, we compare two feedback strategies of MT-LSTM.",6.4 Results,[0],[0]
"The fast-to-slow feedback strat-
4https://github.com/piskvorky/gensim/
egy (MT-LSTM (F2S)) is better than the slow-tofast strategy (MT-LSTM (S2F)), which indicates that MT-LSTM benefits from periodically storing some valuable information “purified” from the short-term memory into the long-term memory.",6.4 Results,[0],[0]
"In the following discussion, we use fast-to-slow feedback strategy as the default setting of MT-LSTM.
",6.4 Results,[0],[0]
"Compared with the standard LSTM, MT-LSTM results in significantly improvements with the same size of hidden layers.
",6.4 Results,[0],[0]
"MT-LSTM outperforms the competitor models on the SST-1, QC and IMDB datasets, and is close to the two best CNN based models on the SST-2 dataset.",6.4 Results,[0],[0]
But MT-LSTM uses much fewer parameters than the CNN based models.,6.4 Results,[0],[0]
"The number of parameters of LSTM range from 10K to 40K while the number of parameters is about 400K in CNN.
",6.4 Results,[0],[0]
"Moreover, MT-LSTM can not only handle short texts, but can model long texts in classification task.
",6.4 Results,[0],[0]
"Documents Modeling Most of the competitor models cannot deal with the texts of with several sentences (paragraphs, documents).",6.4 Results,[0],[0]
"For instance, MV-RNN and RNTN (Socher et al., 2013) are based on the parsing over each sentence and it is unclear how to combine the representations over many sentences.",6.4 Results,[0],[0]
"The convolutional models, such as CNN (Kim, 2014) and AdaSent (Zhao et al., 2015), need more hidden layers or nodes for long texts and result in a very complicated model.",6.4 Results,[0],[0]
These models therefore are restricted to working on sentences instead of paragraphs or documents.,6.4 Results,[0],[0]
"Denil et al. (2014) used two-level version of DCNN (Kalchbrenner et al., 2014) to model documents.",6.4 Results,[0],[0]
"The first level uses a DCNN to trans-
form embeddings for the words in each sentence into an embedding for the entire sentence.",6.4 Results,[0],[0]
The second level uses another DCNN to transform sentence embeddings from the first level into a single embedding vector that represents the entire document.,6.4 Results,[0],[0]
"However, their result is unsatisfactory and they reported that the IMDB dataset is too small to train a CNN model.
",6.4 Results,[0],[0]
The standard LSTM has an advantage to model documents due to its simplification.,6.4 Results,[0],[0]
"However, it is also difficult to train LSTM since the error signals need to be back-propagated over a long distance
with the BPTT algorithm.",6.4 Results,[0],[0]
Our MT-LSTM can alleviate this problem with multiple timescale memories.,6.4 Results,[0],[0]
The experiment on IMDB dataset demonstrates this advantage.,6.4 Results,[0],[0]
"MTLSTM achieves the accuracy of 92.1% , which are better than the other models.
",6.4 Results,[0],[0]
"Moreover, MT-LSTM converges at a faster rate than the standard LSTM.",6.4 Results,[0],[0]
Figure 4 plots the convergence on the IMDB dataset.,6.4 Results,[0],[0]
"In practice, MTLSTM is approximately three times faster than the standard LSTM since the hidden states of lowspeed group often keep unchanged and need not to be re-calculated at each time step.
",6.4 Results,[0],[0]
"Impact of the Different Number of Memory Groups In our model, the number of memory groups is a hyperparameter.",6.4 Results,[0],[0]
"Here we plotted the accuracy curves of our model with the different numbers of memory groups in Figure 5 to show its impacts on the four datasets.
",6.4 Results,[0],[0]
"When the length of text (SST-1, SST-2 and QC) is small, not all memory groups can be activated if we set too many groups, which may harm the performance.",6.4 Results,[0],[0]
"When dealing with the long texts (IMBD), more groups lead to a better performance.",6.4 Results,[0],[0]
"The performance can be improved with the increase of the number of memory groups.
",6.4 Results,[0],[0]
"According to our dynamic strategy, the maximum numbers of groups is 3, 3, 2, 7 for the four datasets.",6.4 Results,[0],[0]
"The best numbers of groups from experiments are 3, 3, 3, 5 respectively.",6.4 Results,[0],[0]
"Therefor, our dynamic strategy is reasonable.",6.4 Results,[0],[0]
"All the datasets except QC, the best number of groups is equal to or smaller than our calculated upper bound.",6.4 Results,[0],[0]
MTLSMT suffers underfitting when the number of groups is larger than the upper bound.,6.4 Results,[0],[0]
"To get an intuitive understanding of what is happening when we use LSTM or MT-LSTM to predict the class of text, we design an experiment to analyze the output of LSTM and MT-LSTM at each time step.
",6.5 Case Study,[0],[0]
"We sample three sentences from the SST-2 test dataset, and the dynamical changes of the predicted sentiment score over time are shown in Figure 6.",6.5 Case Study,[0],[0]
"It is intriguing to notice that our model can handle the rhetorical question well.
",6.5 Case Study,[0],[0]
The first sentence “Is this progress ?” has a negative sentiment.,6.5 Case Study,[0],[0]
"Although the word “progress” is positive, our model can adjust the sentiment correctly after seeing the question mark “?”, and finally gets a correct prediction.
",6.5 Case Study,[0],[0]
The second sentence “He ’d create a movie better than this .”,6.5 Case Study,[0],[0]
also has a negative sentiment.,6.5 Case Study,[0],[0]
The word “better” is positive.,6.5 Case Study,[0],[0]
"Our model finally gets a correct negative prediction after seeing “than this”, while LSTM gets a wrong prediction.
",6.5 Case Study,[0],[0]
"The third sentence “ It ’s not exactly a gourmet meal but fare is fair , even coming from the drive .” is positive and has more complicated semantic composition.",6.5 Case Study,[0],[0]
"Our model can still capture the useful long-term features and gets the correct prediction, while LSTM does not work well.",6.5 Case Study,[0],[0]
There are many previous works to model the variable-length text as a fixed-length vector.,7 Related Work,[0],[0]
"Specific to text classification task, most of the models cannot deal with the texts of several sentences (paragraphs, documents), such as MV-RNN (Socher et al., 2012), RNTN (Socher et al., 2013), CNN (Kim, 2014), AdaSent (Zhao et al., 2015), and so on.",7 Related Work,[0],[0]
"The simple neural bag-of-words model can deal with long texts, but it loses the word order information.",7 Related Work,[0],[0]
"PV (Le and Mikolov, 2014) works in unsupervised way, and the learned vector cannot be fine-tuned on the specific task.
",7 Related Work,[0],[0]
Our proposed MT-LSTM can handle short texts as well as long texts in classification task.,7 Related Work,[0],[0]
"In this paper, we introduce the MT-LSTM, a generalization of LSTMs to capture the information with different timescales.",8 Conclusion,[0],[0]
MT-LSTM can well model both short and long texts.,8 Conclusion,[0],[0]
With the multiple different timescale memories.,8 Conclusion,[0],[0]
"Intuitively, MTLSTM easily carries the crucial information over a long distance.",8 Conclusion,[0],[0]
"Another advantage of MT-LSTM is that the training speed is faster than the standard LSTM (approximately three times faster in practice).
",8 Conclusion,[0],[0]
"In future work, we would like to investigate the other feedback mechanism between the short-term and long-term memories.",8 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
"This work was partially funded by the National Natural Science Foundation of China (61472088, 61473092), National High Technology Research and Development Program of China (2015AA015408), Shanghai Science and Technology Development Funds (14ZR1403200).",Acknowledgments,[0],[0]
Neural network based methods have obtained great progress on a variety of natural language processing tasks.,abstractText,[0],[0]
"However, it is still a challenge task to model long texts, such as sentences and documents.",abstractText,[0],[0]
"In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts.",abstractText,[0],[0]
MTLSTM partitions the hidden states of the standard LSTM into several groups.,abstractText,[0],[0]
Each group is activated at different time periods.,abstractText,[0],[0]
"Thus, MT-LSTM can model very long documents as well as short sentences.",abstractText,[0],[0]
Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task.,abstractText,[0],[0]
Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1118–1127 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1118",text,[0],[0]
Building a chatbot that can naturally and consistently converse with human-beings on opendomain topics draws increasing research interests in past years.,1 Introduction,[0],[0]
"One important task in chatbots is response selection, which aims to select the bestmatched response from a set of candidates given the context of a conversation.",1 Introduction,[0],[0]
"Besides playing a critical role in retrieval-based chatbots (Ji et al., 2014), response selection models have been used in automatic evaluation of dialogue generation
∗Equally contributed.",1 Introduction,[0],[0]
†Work done as a visiting scholar at Baidu.,1 Introduction,[0],[0]
"Wayne Xin Zhao is an associate professor of Renmin University of China and can be reached at batmanfly@ruc.edu.cn.
",1 Introduction,[0],[0]
"(Lowe et al., 2017) and the discriminator of GANbased (Generative Adversarial Networks) neural dialogue generation (Li et al., 2017).
",1 Introduction,[0],[0]
"Early studies on response selection only use the last utterance in context for matching a reply, which is referred to as single-turn response selection (Wang et al., 2013).",1 Introduction,[0],[0]
"Recent works show that the consideration of a multi-turn context can facilitate selecting the next utterance (Zhou et al., 2016; Wu et al., 2017).",1 Introduction,[0],[0]
"The reason why richer contextual information works is that human generated responses are heavily dependent on the previous dialogue segments at different granularities (words, phrases, sentences, etc), both semantically and functionally, over multiple turns rather than one turn (Lee et al., 2006; Traum and Heeman, 1996).",1 Introduction,[0],[0]
Figure 1 illustrates semantic connectivities between segment pairs across context and response.,1 Introduction,[0],[0]
"As demonstrated, generally there are two kinds of matched segment pairs at different granularities across context and response: (1) surface text relevance, for example the lexical overlap of words “packages”-“package” and phrases “debian package manager”-“debian pack-
age manager”.",1 Introduction,[0],[0]
(2) latent dependencies upon which segments are semantically/functionally related to each other.,1 Introduction,[0],[0]
"Such as the word “it” in the response, which refers to “dpkg” in the context, as well as the phrase “its just reassurance” in the response, which latently points to “what packages are installed on my system”, the question that speaker A wants to double-check.
",1 Introduction,[0],[0]
"Previous studies show that capturing those matched segment pairs at different granularities across context and response is the key to multiturn response selection (Wu et al., 2017).",1 Introduction,[0],[0]
"However, existing models only consider the textual relevance, which suffers from matching response that latently depends on previous turns.",1 Introduction,[0],[0]
"Moreover, Recurrent Neural Networks (RNN) are conveniently used for encoding texts, which is too costly to use for capturing multi-grained semantic representations (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017).",1 Introduction,[0],[0]
"As an alternative, we propose to match a response with multi-turn context using dependency information based entirely on attention mechanism.",1 Introduction,[0],[0]
"Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017), which addresses the issue of sequence-to-sequence generation only using attention, and we extend the key attention mechanism of Transformer in two ways:
self-attention By making a sentence attend to itself, we can capture its intra word-level dependencies.",1 Introduction,[0],[0]
"Phrases, such as “debian package manager”, can be modeled with wordlevel self-attention over word-embeddings, and sentence-level representations can be constructed in a similar way with phraselevel self-attention.",1 Introduction,[0],[0]
"By hierarchically stacking self-attention from word embeddings, we can gradually construct semantic representations at different granularities.
cross-attention By making context and response attend to each other, we can generally capture dependencies between those latently matched segment pairs, which is able to provide complementary information to textual relevance for matching response with multi-turn context.
",1 Introduction,[0],[0]
"We jointly introduce self-attention and crossattention in one uniform neural matching network, namely the Deep Attention Matching Network
(DAM), for multi-turn response selection.",1 Introduction,[0],[0]
"In practice, DAM takes each single word of an utterance in context or response as the centric-meaning of an abstractive semantic segment, and hierarchically enriches its representation with stacked self-attention, gradually producing more and more sophisticated segment representations surrounding the centric-word.",1 Introduction,[0],[0]
"Each utterance in context and response are matched based on segment pairs at different granularities, considering both textual relevance and dependency information.",1 Introduction,[0],[0]
"In this way, DAM generally captures matching information between the context and the response from word-level to sentence-level, important matching features are then distilled with convolution & maxpooling operations, and finally fused into one single matching score via a single-layer perceptron.
",1 Introduction,[0],[0]
"We test DAM on two large-scale public multiturn response selection datasets, the Ubuntu Corpus v1 and Douban Conversation Corpus.",1 Introduction,[0],[0]
"Experimental results show that our model significantly outperforms the state-of-the-art models, and the improvement to the best baseline model on R10@1 is over 4%.",1 Introduction,[0],[0]
"What is more, DAM is expected to be convenient to deploy in practice because most attention computation can be fully parallelized (Vaswani et al., 2017).",1 Introduction,[0],[0]
Our contributions are two-folds: (1) we propose a new matching model for multi-turn response selection with selfattention and cross-attention.,1 Introduction,[0],[0]
"(2) empirical results show that our proposed model significantly outperforms the state-of-the-art baselines on public datasets, demonstrating the effectiveness of selfattention and cross-attention.",1 Introduction,[0],[0]
"To build an automatic conversational agent is a long cherished goal in Artificial Intelligence (AI) (Turing, 1950).",2.1 Conversational System,[0],[0]
"Previous researches include taskoriented dialogue system, which focuses on completing tasks in vertical domain, and chatbots, which aims to consistently and naturally converse with human-beings on open-domain topics.",2.1 Conversational System,[0],[0]
"Most modern chatbots are data-driven, either in a fashion of information-retrieval (Ji et al., 2014; Banchs and Li, 2012; Nio et al., 2014; Ameixa et al., 2014) or sequence-generation (Ritter et al., 2011).",2.1 Conversational System,[0],[0]
"The retrieval-based systems enjoy the advantage of informative and fluent responses because it searches a large dialogue repository and selects
candidate that best matches the current context.",2.1 Conversational System,[0],[0]
"The generation-based models, on the other hand, learn patterns of responding from dialogues and can directly generalize new responses.",2.1 Conversational System,[0],[0]
Researches on response selection can be generally categorized into single-turn and multi-turn.,2.2 Response Selection,[0],[0]
"Most early studies are single-turn that only consider the last utterance for matching response (Wang et al., 2013, 2015).",2.2 Response Selection,[0],[0]
"Recent works extend it to multiturn conversation scenario, Lowe et al.,(2015) and Zhou et al.,(2016) use RNN to read context and response, use the last hidden states to represent context and response as two semantic vectors, and measure their relevance.",2.2 Response Selection,[0],[0]
"Instead of only considering the last states of RNN, Wu et al.,(2017) take hidden state at each time step as a text segment representation, and measure the distance between context and response via segment-segment matching matrixes.",2.2 Response Selection,[0],[0]
"Nevertheless, matching with dependency information is generally ignored in previous works.",2.2 Response Selection,[0],[0]
"Attention has been proven to be very effective in Natural Language Processing (NLP) (Bahdanau et al., 2015; Yin et al., 2016; Lin et al., 2017) and other research areas (Xu et al., 2015).",2.3 Attention,[0],[0]
"Recently, Vaswani et al.,(2017) propose a novel sequenceto-sequence generation network, the Transformer,
which is entirely based on attention.",2.3 Attention,[0],[0]
"Not only Transformer can achieve better translation results than convenient RNN-based models, but also it is very fast in training/predicting as the computation of attention can be fully parallelized.",2.3 Attention,[0],[0]
"Previous works on attention mechanism show the superior ability of attention to capture semantic dependencies, which inspires us to improve multi-turn response selection with attention mechanism.",2.3 Attention,[0],[0]
"Given a dialogue data set D = {(c, r, y)Z}NZ=1, where c = {u0, ..., un−1} represents a conversation context with {ui}n−1i=0 as utterances and r as a response candidate.",3.1 Problem Formalization,[0],[0]
"y ∈ {0, 1} is a binary label, indicating whether r is a proper response for c. Our goal is to learn a matching model g(c, r) with D, which can measure the relevance between any context c and candidate response r.",3.1 Problem Formalization,[0],[0]
"Figure 2 gives an overview of DAM, which generally follows the representation-matchingaggregation framework to match response with multi-turn context.",3.2 Model Overview,[0],[0]
For each utterance ui =,3.2 Model Overview,[0],[0]
"[wui,k] nui−1 k=0 in a context and its response candidate r =",3.2 Model Overview,[0],[0]
"[wr,t]nr−1t=0 , where nui and nr stand for the numbers of words, DAM first looks up a shared word embedding table and represents ui and r as sequences of word embeddings, namely U0i =
[e0ui,0, ..., e 0 ui,nui−1 ] and R0 =",3.2 Model Overview,[0],[0]
"[e0r,0, ..., e0r,nr−1] respectively, where e ∈ Rd denotes a d-dimension word embedding.
",3.2 Model Overview,[0],[0]
"A representation module then starts to construct semantic representations at different granularities for ui and r. Practically, L identical layers of self-attention are hierarchically stacked, each lth self-attention layer takes the output of the l − 1th layer as its input, and composites the input semantic vectors into more sophisticated representations based on self-attention.",3.2 Model Overview,[0],[0]
"In this way, multigrained representations of ui and r are gradually constructed, denoted as [Uli]Ll=0 and [R
l]Ll=0 respectively.
",3.2 Model Overview,[0],[0]
"Given [U0i , ...,ULi ] and [R0, ...,RL], utterance ui and response r are then matched with each other in a manner of segment-segment similarity matrix.",3.2 Model Overview,[0],[0]
"Practically, for each granularity l ∈",3.2 Model Overview,[0],[0]
"[0...L], two kinds of matching matrixes are constructed, i.e., the self-attention-match Mui,r,lself and cross-attention-match Mui,r,lcross , measuring the relevance between utterance and response with textual information and dependency information respectively.
",3.2 Model Overview,[0],[0]
Those matching scores are finally merged into a 3D matching image Q1.,3.2 Model Overview,[0],[0]
"Each dimension of Q represents each utterance in context, each word in utterance and each word in response respectively.",3.2 Model Overview,[0],[0]
"Important matching information between segment pairs across multi-turn context and candidate response is then extracted via convolution with max-pooling operations, and further fused into one matching score via a single-layer perceptron, representing the matching degree between the response candidate and the whole context.
",3.2 Model Overview,[0],[0]
"Specifically, we use a shared component, the Attentive Module, to implement both selfattention in representation and cross-attention in matching.",3.2 Model Overview,[0],[0]
We will discuss in detail the implementation of Attentive Module and how we used it to implement both self-attention and cross-attention in following sections.,3.2 Model Overview,[0],[0]
"Figure 3 shows the structure of Attentive Module, which is similar to that used in Transformer (Vaswani et al., 2017).",3.3 Attentive Module,[0],[0]
"Attentive Module has three input sentences: the query sentence, the key sentence and the value sentence, namely Q =",3.3 Attentive Module,[0],[0]
"[ei] nQ−1 i=0 ,K =",3.3 Attentive Module,[0],[0]
"[ei] nK−1 i=0 ,V =",3.3 Attentive Module,[0],[0]
"[ei] nV−1 i=0 respec-
1We refer to it as Q because it is like a cube.
",3.3 Attentive Module,[0],[0]
"tively, where nQ, nK and nV denote the number of words in each sentence and ei stands for a ddimension embedding, nK is equal to nV .",3.3 Attentive Module,[0],[0]
"The Attentive Module first takes each word in the query sentence to attend to words in the key sentence via Scaled Dot-Product Attention (Vaswani et al., 2017), then applies those attention results upon the value sentence, which is defined as:
Att(Q,K) =",3.3 Attentive Module,[0],[0]
"[ softmax(
Q[i] · KT√ d ) ]nQ−1 i=0 (1)
Vatt = Att(Q,K) · V ∈ RnQ×d (2)
where Q[i] is the ith embedding in the query sentence Q.",3.3 Attentive Module,[0],[0]
"Each row of Vatt, denoted as Vatt[i], stores the fused semantic information of words in the value sentence that possibly have dependencies to the ith word in query sentence.",3.3 Attentive Module,[0],[0]
"For each i, Vatt[i] and Q[i] are then added up together, compositing them into a new representation that contains their joint meanings.",3.3 Attentive Module,[0],[0]
"A layer normalization operation (Ba et al., 2016) is then applied, which prevents vanishing or exploding of gradients.",3.3 Attentive Module,[0],[0]
"A feed-forward network FFN with RELU (LeCun et al., 2015) activation is then applied upon the normalization result, in order to further process the fused embeddings, defined as:
FFN(x) = max(0, xW1 + b1)W2 + b2 (3)
where, x is a 2D-tensor in the same shape of query sentence Q and W1, b1,W2, b2 are learnt parameters.",3.3 Attentive Module,[0],[0]
"This kind of activation is empirically useful in other works, and we also adapt it in our model.",3.3 Attentive Module,[0],[0]
"The result FFN(x) is a 2D-tensor that has the same shape as x, FFN(x) is then residually added (He et al., 2016) to x, and the fusion result is then normalized as the final outputs.",3.3 Attentive Module,[0],[0]
"We refer to the whole Attentive Module as:
AttentiveModule(Q,K,V) (4)
",3.3 Attentive Module,[0],[0]
"As described, Attentive Module can capture dependencies across query sentence and key sentence, and further use the dependency information to composite elements in the query sentence and the value sentence into compositional representations.",3.3 Attentive Module,[0],[0]
We exploit this property of the Attentive Module to construct multi-grained semantic representations as well as match with dependency information.,3.3 Attentive Module,[0],[0]
"Given U0i or R0, the word-level embedding representations for utterance ui or response r, DAM takes U0i ro R0 as inputs and hierarchically stacks the Attentive Module to construct multi-grained representations of ui and r, which is formulated as:
Ul+1i =",3.4 Representation,[0],[0]
AttentiveModule(U,3.4 Representation,[0],[0]
l,3.4 Representation,[0],[0]
"i,U l",3.4 Representation,[0],[0]
"i,U l i) (5) Rl+1",3.4 Representation,[0],[0]
"= AttentiveModule(Rl,Rl,Rl) (6)
where l ranges from 0 to L − 1, denoting the different levels of granularity.",3.4 Representation,[0],[0]
"By this means, words in each utterance or response repeatedly function together to composite more and more holistic representations, we refer to those multi-grained representations as [U0i , ...,ULi ] and [R0, ...,RL] hereafter.",3.4 Representation,[0],[0]
"Given [Uli]Ll=0 and [R
l]Ll=0, two kinds of segmentsegment matching matrixes are constructed at each level of granularity l, i.e., the self-attention-match Mui,r,lself and cross-attention-match M ui,r,l cross .",3.5 Utterance-Response Matching,[0],[0]
"M ui,r,l self is defined as:
Mui,r,lself = {U l i[k] T · Rl[t]}nui×nr (7)
in which, each element in the matrix is the dotproduct of Uli[k] and Rl[t], the kth embedding in Uli and the tth embedding in Rl, reflecting the textual relevance between the kth segment in ui and tth segment in r at the lth granularity.",3.5 Utterance-Response Matching,[0],[0]
"The crossattention-match matrix is based on cross-attention, which is defined as:
Ũ l
i =",3.5 Utterance-Response Matching,[0],[0]
AttentiveModule(U,3.5 Utterance-Response Matching,[0],[0]
"l i,R l,Rl) (8)
R̃ l = AttentiveModule(Rl,Uli,U l i) (9)
",3.5 Utterance-Response Matching,[0],[0]
"Mui,r,lcross = {Ũ l i[k] T · R̃l[t]}nui×nr (10)
where we use Attentive Module to make Uli and Rl crossly attend to each other, constructing two
new representations for both of them, written as Ũ",3.5 Utterance-Response Matching,[0],[0]
l i and R̃ l respectively.,3.5 Utterance-Response Matching,[0],[0]
"Both Ũ l i and R̃ l
implicitly capture semantic structures that cross the utterance and response.",3.5 Utterance-Response Matching,[0],[0]
"In this way, those inter-dependent segment pairs are close to each other in representations, and dot-products between those latently inter-dependent pairs could get increased, providing dependency-aware matching information.",3.5 Utterance-Response Matching,[0],[0]
"DAM finally aggregates all the segmental matching degrees across each utterance and response into a 3D matching image Q, which is defined as:
Q = {Qi,k,t}n×nui×nr (11)
where each pixel Qi,k,t is formulated as:
Qi,k,t =
[Mui,r,lself",3.6 Aggregation,[0],[0]
"[k, t]] L l=0 ⊕",3.6 Aggregation,[0],[0]
"[Mui,r,lcross [k, t]]Ll=0
(12)
⊕ is concatenation operation, and each pixel has 2(L + 1) channels, storing the matching degrees between one certain segment pair at different levels of granularity.",3.6 Aggregation,[0],[0]
DAM then leverages twolayered 3D convolution with max-pooling operations to distill important matching features from the whole image.,3.6 Aggregation,[0],[0]
"The operation of 3D convolution with max-pooling is the extension of typical 2D convolution, whose filters and strides are 3D cubes2.",3.6 Aggregation,[0],[0]
"We finally compute matching score g(c, r) based on the extracted matching features fmatch(c, r) via a single-layer perceptron, which is formulated as:
g(c, r) = σ(W3fmatch(c, r) + b3) (13)
where W3 and b3 are learnt parameters, and σ is sigmoid function that gives the probability if r is a proper candidate to c.",3.6 Aggregation,[0],[0]
"The loss function of DAM is the negative log likelihood, defined as:
p(y|c, r) =",3.6 Aggregation,[0],[0]
"g(c, r)y + (1− g(c, r))(1− y) (14) L(·) =",3.6 Aggregation,[0],[0]
"− ∑ (c,r,y)∈D log(p(y|c, r))",3.6 Aggregation,[0],[0]
(15),3.6 Aggregation,[0],[0]
2https://www.tensorflow.org/api docs/python/tf/nn/conv3d,4 Experiment,[0],[0]
"We test DAM on two public multi-turn response selection datasets, the Ubuntu Corpus V1 (Lowe et al., 2015) and the Douban Conversation Corpus (Wu et al., 2017).",4.1 Dataset,[0],[0]
The former one contains multiturn dialogues about Ubuntu system troubleshooting in English and the later one is crawled from a Chinese social networking on open-domain topics.,4.1 Dataset,[0],[0]
"The Ubuntu training set contains 0.5 million multiturn contexts, and each context has one positive response that generated by human and one negative response which is randomly sampled.",4.1 Dataset,[0],[0]
"Both validation and testing sets of Ubuntu Corpus have 50k contexts, where each context is provided with one positive response and nine negative replies.",4.1 Dataset,[0],[0]
"The Douban corpus is constructed in a similar way to the Ubuntu Corpus, except that its validation set contains 50k instances with 1:1 positive-negative ratios and the testing set of Douban corpus is consisted of 10k instances, where each context has 10 candidate responses, collected via a tiny invertedindex system (Lucene3), and labels are manually annotated.",4.1 Dataset,[0],[0]
"We use the same evaluation metrics as in previous works (Wu et al., 2017).",4.2 Evaluation Metric,[0],[0]
"Each comparison model is asked to select k best-matched response from n available candidates for the given conversation context c, and we calculate the recall of the true positive replies among the k selected ones as the main evaluation metric, denoted as Rn@k = ∑k i=1",4.2 Evaluation Metric,[0],[0]
yi∑n i=1,4.2 Evaluation Metric,[0],[0]
"yi
, where yi is the binary label for each candidate.",4.2 Evaluation Metric,[0],[0]
"In addition to Rn@k, we use MAP (Mean Average Precision) (Baeza-
3https://lucenent.apache.org/
Yates et al., 1999), MRR (Mean Reciprocal Rank) (Voorhees et al., 1999), and Precision-at-one P@1 especially for Douban corpus, following the setting of previous works (Wu et al., 2017).",4.2 Evaluation Metric,[0],[0]
"RNN-based models : Previous best performing
models are based on RNNs",4.3 Comparison Methods,[0],[0]
", we choose representative models as baselines, including SMNdynamic(Wu et al., 2017), Multiview(Zhou et al., 2016), DualEncoderlstm and DualEncoderbilstm (Lowe et al., 2015), DL2R (Yan et al., 2016), Match-LSTM (Wang and Jiang, 2017) and MV-LSTM (Pang et al., 2016), where SMNdynamic achieves the best scores against all the other published works, and we take it as our stateof-the-art baseline.
",4.3 Comparison Methods,[0],[0]
"Ablation : To verify the effects of multi-grained representation, we setup two comparison models, i.e., DAMfirst and DAMlast, which dispense with the multi-grained representations in DAM, and use representation results from the 0th layer and Lth layer of self-attention instead.",4.3 Comparison Methods,[0],[0]
"Moreover, we setup DAMself and DAMcross, which only use self-attention-match or cross-attention-match respectively, in order to examine the effectiveness of both self-attention-match and cross-attention-match.",4.3 Comparison Methods,[0],[0]
We copy the reported evaluation results of all baselines for comparison.,4.4 Model Training,[0],[0]
"DAM is implemented in tensorflow4, and the used vocabularies, word em-
4https://www.tensorflow.org.",4.4 Model Training,[0],[0]
"Our code and data will be available at https://github.com/baidu/Dialogue/DAM
bedding sizes for Ubuntu corpus and Douban corpus are all set as same as the SMN (Wu et al., 2017).",4.4 Model Training,[0],[0]
"We consider at most 9 turns and 50 words for each utterance (response) in our experiments, word embeddings are pre-trained using training sets via word2vec (Mikolov et al., 2013), similar to previous works.",4.4 Model Training,[0],[0]
"We use zero-pad to handle the variable-sized input and parameters in FFN are set to 200, same as word-embedding size.",4.4 Model Training,[0],[0]
"We test stacking 1-7 self-attention layers, and reported our results with 5 stacks of self-attention because it gains the best scores on validation set.",4.4 Model Training,[0],[0]
"The 1st convolution layer has 32 [3,3,3] filters with [1,1,1] stride, and its max-pooling size is [3,3,3] with [3,3,3] stride.",4.4 Model Training,[0],[0]
"The 2nd convolution layer has 16 [3,3,3] filters with [1,1,1] stride, and its maxpooling size is also [3,3,3] with [3,3,3] stride.",4.4 Model Training,[0],[0]
"We tune DAM and the other ablation models with adam optimizer (Le et al., 2011) to minimize loss function defined in Eq 15.",4.4 Model Training,[0],[0]
"Learning rate is initialized as 1e-3 and gradually decreased during training, and the batch-size is 256.",4.4 Model Training,[0],[0]
We use validation sets to select the best models and report their performances on test sets.,4.4 Model Training,[0],[0]
Table 1 shows the evaluation results of DAM as well as all comparison models.,4.5 Experiment Result,[0],[0]
"As demonstrated, DAM significantly outperforms other competitors on both Ubuntu Corpus and Douban Conversation Corpus, including SMNdynamic, which is the state-of-the-art baseline, demonstrating the superior power of attention mechanism in matching response with multi-turn context.",4.5 Experiment Result,[0],[0]
"Besides, both the performances of DAMfirst and DAMself decrease a lot compared with DAM, which shows the effectiveness of self-attention and cross-attention.",4.5 Experiment Result,[0],[0]
"Both DAMfirst and DAMlast underperform DAM, which demonstrates the benefits of using multigrained representations.",4.5 Experiment Result,[0],[0]
"Also the absence of self-attention-match brings down the precision, as shown in DAMcross, exhibiting the necessity of jointly considering textual relevance and dependency information in response selection.
",4.5 Experiment Result,[0],[0]
"One notable point is that, while DAMfirst is able to achieve close performance to SMNdynamic, it is about 2.3 times faster than SMNdynamic in our implementation as it is very simple in computation.",4.5 Experiment Result,[0],[0]
"We believe that DAMfirst is more suitable to the scenario that has limitations in computation time or memories but requires high precise, such
as industry application or working as an component in other neural networks like GANs.",4.5 Experiment Result,[0],[0]
We use the Ubuntu Corpus for analyzing how selfattention and cross-attention work in DAM from both quantity analysis as well as visualization.,5 Analysis,[0],[0]
We first study how DAM performs in different utterance number of context.,5.1 Quantity Analysis,[0],[0]
The left part in Figure 4 shows the changes of R10@1 on Ubuntu Corpus across contexts with different number of utterance.,5.1 Quantity Analysis,[0],[0]
"As demonstrated, while being good at matching response with long context that has more than 4 utterances, DAM can still stably deal with short context that only has 2 turns.
",5.1 Quantity Analysis,[0],[0]
"Moreover, the right part of Figure 4 gives the comparison of performance across different contexts with different average utterance text length and self-attention stack depth.",5.1 Quantity Analysis,[0],[0]
"As demonstrated, stacking self-attention can consistently improve matching performance for contexts having different average utterance text length, implying the stability advantage of using multi-grained semantic representations.",5.1 Quantity Analysis,[0],[0]
"The performance of matching short utterances, that have less than 10 words, is obviously lower than the other longer ones.",5.1 Quantity Analysis,[0],[0]
"This is because the shorter the utterance text is, the fewer information it contains, and the more difficult for selecting the next utterance, while stacking self-attention can still help in this case.",5.1 Quantity Analysis,[0],[0]
"However for long utterances like containing more than 30 words, stacking self-attention can significantly improve the matching performance, which means that the more information an utterance contains, the more stacked self-attention it needs to capture its intra semantic structures.
",5.1 Quantity Analysis,[0],[0]
"no clue what
do you
need it
for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1",5.1 Quantity Analysis,[0],[0]
do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew he re el se
turn 0
re sp
on se
self−attention−match in stack 0
no clue what
do you
need it
for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1",5.1 Quantity Analysis,[0],[0]
do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew he re el se
turn 0
re sp
on se
self−attention−match in stack 2
no clue what
do you
need it
for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1",5.1 Quantity Analysis,[0],[0]
do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew he re el se
turn 0
re sp
on se
self−attention−match in stack 4
no clue what
do you
need it
for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1",5.1 Quantity Analysis,[0],[0]
do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew he re el se
turn 0
re sp
on se
cross−attention−match in stack 4
no clue what
do you
need it
for its
just reassurance
as i
dont know
the debain
package manager
no clu e wh",5.1 Quantity Analysis,[0],[0]
"at do yo u ne ed it fo r. its ju st
re as
su ra
nc e
as i do nt kn ow th e de ba in pa ck ag e m an ag
er
response
re sp
on se
self−attention of response in stack 3
hi i
am looking
to see
what packages
are installed
on my
system i.1
dont see.1
a path
is the list
being held
somewhere else
",5.1 Quantity Analysis,[0],[0]
"hi i am lo ok in
g to se e wh",5.1 Quantity Analysis,[0],[0]
at pa ck ag es ar e in st al le d on m y sy st em i.1 do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew he re el se
.",5.1 Quantity Analysis,[0],[0]
"turn 0
tu rn
0 self−attention of turn 0 in stack 3
no clue what
do you
need it
for its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e wh at pa ck ag es ar e in st al le d on m y sy st em i.1",5.1 Quantity Analysis,[0],[0]
do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
so m ew he re el se .,5.1 Quantity Analysis,[0],[0]
"turn 0 re
sp on
se
attention of response over turn 0 in stack 4
hi i
am looking
to see
what packages
are installed
on my
system i.1
",5.1 Quantity Analysis,[0],[0]
"dont see.1
a path
is the list
being held
somewhere else
no clu e wh at do yo u ne ed it fo r.",5.1 Quantity Analysis,[0],[0]
"its ju st
re as
su ra
nc e
as i do nt kn ow th e de ba in pa ck ag e m an ag
er
response
tu rn
0
attention of turn 0 over response in stack 4
self-attention cross-attention
no clue what
do you
need it
for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st",5.1 Quantity Analysis,[0],[0]
em do nt a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
turn 0
re sp
on se
prior−match in stack 0
no clue what
do you
need it
for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st",5.1 Quantity Analysis,[0],[0]
em do nt a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
turn 0
re sp
on se
prior−match in stack 2
no clue what
do you
need it
for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st",5.1 Quantity Analysis,[0],[0]
em do nt a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
turn 0
re sp
on se
prior−match in stac 4
no clue what
do you
need it
for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st",5.1 Quantity Analysis,[0],[0]
em do nt a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
turn 0
re sp
on se
posterior−match in stack 4
no clue what
do you
need it
for its
just reassurance
as i
dont know
the debain
package manager
no cl",5.1 Quantity Analysis,[0],[0]
ue w ha t,5.1 Quantity Analysis,[0],[0]
do yo u ne ed,5.1 Quantity Analysis,[0],[0]
"it fo r. its ju st
re as
su ra
nc e
as i do nt kn ow th e de ba in pa ck ag e m an ag
er
response
re sp
on se
self−attention of response in stack 3
hi i
am looking
to see
what packages
are installed
on my
system dont
a path
is the list
being held
somewhere else
",5.1 Quantity Analysis,[0],[0]
"hi i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st em i.1",5.1 Quantity Analysis,[0],[0]
do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
.
",5.1 Quantity Analysis,[0],[0]
"turn 0
tu rn
0
self−attention of turn 0 in stack 3
no clue what
do you
need it
for its
just reassurance
as i
dont know
the debain
package manager
",5.1 Quantity Analysis,[0],[0]
"hi i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st em i.1",5.1 Quantity Analysis,[0],[0]
do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
.
",5.1 Quantity Analysis,[0],[0]
"turn 0
re
sp
on se
attention of response over turn 0 in stack 4
hi i
am looking
to see
what packages
are installed
on my
system",5.1 Quantity Analysis,[0],[0]
"dont
a path
is the list
being held
somewhere else
no cl",5.1 Quantity Analysis,[0],[0]
ue w ha t,5.1 Quantity Analysis,[0],[0]
do yo u ne ed,5.1 Quantity Analysis,[0],[0]
"it fo r. its ju st
re as
su ra
nc e
as i do nt kn ow th e de ba in pa ck ag e m an ag
er
response
tu rn
0
attention of turn 0 over response in stack 4
prior-match posterior-match
self-attention cross-attention
no clue hat do
you need
it for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont kno
the debain
package anager
hi i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st em do nt a pa th is th e lis t be in g",5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
turn 0
re sp
on se
prior atch in stack 0
no clue hat do
you need
it for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont kno
the debain
package anager
hi i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st em do nt a pa th is th e lis t be in g",5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
turn 0
re sp
on se
prior atch in stack 2
no clue hat do
you need
it for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont kno
the debain
package anager
hi i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st em do nt a pa th is th e lis t be in g",5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
turn 0
re sp
on se
prior−match in stac 4
no clue what
do you
need it
for.",5.1 Quantity Analysis,[0],[0]
"its
just reassurance
as i
dont know
the debain
package manager
hi",5.1 Quantity Analysis,[0],[0]
"i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st",5.1 Quantity Analysis,[0],[0]
em do nt a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
turn 0
re sp
on se
posterior−match in stack 4
no clue what
do you
need it
for its
just reassurance
as i
dont know
the debain
package manager
no cl",5.1 Quantity Analysis,[0],[0]
ue w ha t,5.1 Quantity Analysis,[0],[0]
do yo u ne ed,5.1 Quantity Analysis,[0],[0]
"it fo r. its ju st
re as
su ra
nc e
as i do nt kn ow th e de ba in pa ck ag e m an ag
er
response
re sp
on se
self−attention of espo e in stack 3
hi i
am looking
to see
what packages
are installed
on my
ystem dont
a path
is the list
being held
somewhere else
",5.1 Quantity Analysis,[0],[0]
"hi i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st em i.1",5.1 Quantity Analysis,[0],[0]
do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
.
",5.1 Quantity Analysis,[0],[0]
"t
tu rn
0
self−atte tion of turn 0 in stack 3
no clue what
do you
need it
for its
just rea surance
as i
dont know
the debain
package manager
",5.1 Quantity Analysis,[0],[0]
"hi i am lo ok in
g to se e w ha t
pa ck
ag es
ar e
in st
al le d on m
y sy
st em i.1",5.1 Quantity Analysis,[0],[0]
do nt se e. 1 a pa th is th e lis t be in g,5.1 Quantity Analysis,[0],[0]
he ld,5.1 Quantity Analysis,[0],[0]
"so m ew
he re
el se
.
",5.1 Quantity Analysis,[0],[0]
"turn 0
re
sp
on se
attention of esponse over turn 0 in stack 4
hi i
am looking
to see
what packages
are installed
on my
system",5.1 Quantity Analysis,[0],[0]
"dont
a path
is the list
being held
somewhere else
no cl",5.1 Quantity Analysis,[0],[0]
ue w ha t,5.1 Quantity Analysis,[0],[0]
do yo u ne ed,5.1 Quantity Analysis,[0],[0]
"it fo r. its ju st
re as
su ra
nc e
as i do nt kn ow th e de ba in pa ck ag e m an ag
er
response
tu rn
0
attention of turn 0 over response in stack 4
prior-match posterior-match
self-attention cross-attention
s l - tention-match in stack 0 lf-a tention-match in stack 2 self-attention-match in stack 4 cross-attention-match in stack 4 self-attention-match cross-attention-match
Figure 5: Visualization of self-attention-match, cross-attention-match as well as the distribution of self-attention and crossattention in matching response with the first utterance in Figure 1.",5.1 Quantity Analysis,[0],[0]
Each c lored grid represents the matching degree or attention score between two words.,5.1 Quantity Analysis,[0],[0]
"The deeper the color is, the more important this grid is.",5.1 Quantity Analysis,[0],[0]
We study the case in Figure 1 for analyzing in detail how self-attention and cross-attention work.,5.2 Visualization,[0],[0]
"Practically, we apply a softmax operation over self-attention-match and cross-attention-match, to examine the variance of dominating matching pairs during stacking self-attention or applying cross-attention.",5.2 Visualization,[0],[0]
"Figure 5 gives the visualization results of the 0th, 2nd and 4th self-attention-match matrixes, the 4th cross-attention-match matrix, as well as the distribution of self-attention and crossattention in the 4th layer in matching response with the first utterance (turn 0) due to space limitation.",5.2 Visualization,[0],[0]
"As demonstrated, important matching pairs in selfattention-match in stack 0 are nouns, verbs, like “package” and “packages”, those are similar in topics.",5.2 Visualization,[0],[0]
"However matching scores between prepositions or pronouns pairs, such as “do” and “what”, become more important in self-attention-match in stack 4.",5.2 Visualization,[0],[0]
"The visualization results of self-attention show the reason why matching between prepositions or pronouns matters, as demonstrated, selfattention generally capture the semantic structure of “no clue what do you need package manager” for “do” in response and “what packages are installed” for “what” in utterance, making segments surrounding “do” and “what” close to each other in representations, thus increases their dot-product results.
",5.2 Visualization,[0],[0]
"Also as shown in Figure 5, self-attentionmatch and cros -attention-match capture complementary information in matching utterance with response.",5.2 Visualization,[0],[0]
Words like “reassurance” and “its” in response significantly get larger matching scores in cross-attention-match compared with self-attention-match.,5.2 Visualization,[0],[0]
"According to the visualization of cross-attention, “reassurance” generally depends on “system” “don’t” and “held” in utterance, which makes it close to words like “list”, “installed” or “held” of utterance.",5.2 Visualization,[0],[0]
"Scores of crossattention-match trend to centralize on several segments, which probably means that those segments in response generally capture structure-semantic information across utterance and response, amplifying their matching scores against the others.",5.2 Visualization,[0],[0]
"To understand the limitations of DAM and where the future improvements might lie, we analyze 100 strong bad cases from test-set that fail in R10@5.",5.3 Error Analysis,[0],[0]
"We find two major kinds of bad cases: (1) fuzzycandidate, where response candidates are basically proper for the conversation context, except for a few improper details.",5.3 Error Analysis,[0],[0]
"(2) logical-error, where response candidates are wrong due to logical mismatch, for example, given a conversation context A: “I just want to stay at home tomorrow.”",5.3 Error Analysis,[0],[0]
", B: “Why not go hiking?",5.3 Error Analysis,[0],[0]
"I can go with
you.”",5.3 Error Analysis,[0],[0]
", response candidate like “Sure, I was planning to go out tomorrow.” is logically wrong because it is contradictory to the first utterance of speaker A.",5.3 Error Analysis,[0],[0]
"We believe generating adversarial examples, rather than randomly sampling, during training procedure may be a good idea for addressing both fuzzy-candidate and logical-error, and to capture logic-level information hidden behind conversation text is also worthy to be studied in the future.",5.3 Error Analysis,[0],[0]
"In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention.",6 Conclusion,[0],[0]
Our solution extends the attention mechanism of Transformer in two ways: (1) using stacked selfattention to harvest multi-grained semantic representations.,6 Conclusion,[0],[0]
(2) utilizing cross-attention to match with dependency information.,6 Conclusion,[0],[0]
Empirical results on two large-scale datasets demonstrate the effectiveness of self-attention and cross-attention in multi-turn response selection.,6 Conclusion,[0],[0]
"We believe that both self-attention and cross-attention could benefit other research area, including spoken language understanding, dialogue state tracking or seq2seq dialogue generation.",6 Conclusion,[0],[0]
We would like to explore in depth how attention can help improve neural dialogue modeling for both chatbots and taskoriented dialogue systems in our future work.,6 Conclusion,[0],[0]
We gratefully thank the anonymous reviewers for their insightful comments.,Acknowledgement,[0],[0]
"This work is supported by the National Basic Research Program of China (973 program, No. 2014CB340505).",Acknowledgement,[0],[0]
"Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context.",abstractText,[0],[0]
"In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention.",abstractText,[0],[0]
"Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) and we extend the attention mechanism in two ways.",abstractText,[0],[0]
"First, we construct representations of text segments at different granularities solely with stacked self-attention.",abstractText,[0],[0]
"Second, we try to extract the truly matched segment pairs with attention across the context and response.",abstractText,[0],[0]
We jointly introduce those two kinds of attention in one uniform neural network.,abstractText,[0],[0]
Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models.,abstractText,[0],[0]
Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network,title,[0],[0]
"Machine-learned predictors are informing decisions that affect all aspects of life; from news article recommendations to criminal sentencing decisions to healthcare diagnostics, increasingly algorithms are used to make predictions about individuals.",1. Introduction,[0],[0]
A potential risk is that these predictors might discriminate against groups of individuals that are protected by law or by ethics.,1. Introduction,[0],[0]
"Indeed, examples of such unintended but harmful discrimination have been well-documented across many learning tasks including image classification (Buolamwini & Gebru, 2018) and natural language tasks (Bolukbasi et al., 2016).",1. Introduction,[0],[0]
"This work aims to mitigate such risks of algorithmic discrimination in the context of prediction tasks.
",1. Introduction,[0],[0]
"The output of a learning algorithm can be discriminatory for
1Computer Science Department, Stanford University, Stanford, CA 2Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot, Israel.",1. Introduction,[0],[0]
"Correspondence to: Michael P. Kim <mpk@cs.stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
a number of reasons.,1. Introduction,[0],[0]
"First, the training data may contain biases that should be corrected.",1. Introduction,[0],[0]
"Second, the analysis of the training data may inadvertently introduce biases that are not borne out in the data.",1. Introduction,[0],[0]
"In this work, we focus on the latter concern.
",1. Introduction,[0],[0]
"Indeed, even given accurate ground-truth training data, the typical approach to supervised learning – choosing a model that minimizes the expected loss on the training data – runs the risk of choosing a prediction model that is good for the majority population, but overlooks the minority populations.",1. Introduction,[0],[0]
Consider the case where a financial institution trains a model to predict the probability that applicants will default on their loans.,1. Introduction,[0],[0]
"If on average, the individuals from S are financially disadvantaged compared to the majority population, the model may assign a fixed, low probability to all i ∈ S, while still achieving good empirical loss by predicting very accurately in the majority population.",1. Introduction,[0],[0]
"Such a model discriminates against the qualified members of S. Worse yet, this form of discrimination has the potential to amplify S’s underrepresentation by refusing to approve members that are capable of repaying the loan.
",1. Introduction,[0],[0]
"Focusing on such concerns, we develop a theoretical framework that aims to mitigate such risks of algorithmic discrimination, in the context of prediction tasks.",1. Introduction,[0],[0]
"Specifically, we focus on a setting where a learner has access to a small sample of ground truth data D from some domain of individuals X .",1. Introduction,[0],[0]
Each individual i ∈ D has a boolean label oi,1. Introduction,[0],[0]
∈,1. Introduction,[0],[0]
"{0, 1} representing the outcome of a certain stochastic event (ad click, loan repayment, cancer diagnosis, etc.) that the learner wishes to predict.",1. Introduction,[0],[0]
"We suppose that for each i ∈ X , there is an underlying probability p∗i which governs the distribution of the resulting outcome oi.",1. Introduction,[0],[0]
We say a predictor f : X,1. Introduction,[0],[0]
"→ [0, 1] is a map from individuals i ∈ X to an estimate of the true parameters.",1. Introduction,[0],[0]
"Next, we discuss desirable properties of predictors that motivate our new perspective on fairness.
",1. Introduction,[0],[0]
Calibration and Multicalibration.,1. Introduction,[0],[0]
"If we do not want a predictor f to downplay the fitness of a group S ⊆ X , we can require that it be (approximately) unbiased over S; namely, that
∣∣Ei∼S",1. Introduction,[0],[0]
"[fi − p∗i ]∣∣ ≤ α, for some small α ≥ 0.",1. Introduction,[0],[0]
This means that the expectation of f and p∗ over S are almost identical.,1. Introduction,[0],[0]
"Calibration strengthens this requirement by essentially asking that for any particular
value v, if we let Sv = {i ∈ S : fi = v} be the subset of S of individuals with predicted probability v, then∣∣Ei∼Sv [fi − p∗i ]∣∣ = |v",1. Introduction,[0],[0]
− Ei∼Sv [p∗i ]| ≤ α.,1. Introduction,[0],[0]
"While this notion already precludes some forms of discrimination, a principle weakness of calibration as a fairness concept is that the guarantees are too coarse.",1. Introduction,[0],[0]
"Indeed, weaknesses of group fairness notions were discussed in (Dwork et al., 2012), as a motivation for introducing an individual fairness notion.",1. Introduction,[0],[0]
A specific way to discriminate while satisfying calibration is to assign every member of S the value Ei∼S [p∗i ].,1. Introduction,[0],[0]
"While being perfectly calibrated over S, the qualified members of S with large values p∗i will be hurt.
",1. Introduction,[0],[0]
"Calibration is typically applied to large, often disjoint, sets of protected groups; that is, the guarantees are only required to hold on average over a population defined by a small number of sensitive attributes, like race or gender.",1. Introduction,[0],[0]
"A stronger definition of fairness would ensure that the predictions on every subpopulation would be calibrated, including, for instance, the qualified members of S from the example above.",1. Introduction,[0],[0]
"The problem with such a notion is that it is informationtheoretically unattainable from a small sample of labeled examples, as it essentially requires perfect predictions.",1. Introduction,[0],[0]
"As such, we need an intermediary definition that balances the desire to protect important subgroups and the information bottleneck that arises when learning from a small sample.
",1. Introduction,[0],[0]
"To motivate our notion, suppose a learning algorithm produces a predictor f .",1. Introduction,[0],[0]
"Then, more outcomes are determined, and an auditor finds a subpopulation S whose outcomes outperform the predictions made by f .",1. Introduction,[0],[0]
Perhaps the learning algorithm was lazy and neglected to identify the higher potential in S?,1. Introduction,[0],[0]
Perhaps the individuals of S were simply lucky?,1. Introduction,[0],[0]
How can we tell?,1. Introduction,[0],[0]
"To answer these questions, we take the following perspective: on the one hand, we can only expect a learner to produce a predictor that is calibrated on sets that could have been identified efficiently from the data at hand; on the other hand, we expect the learner to produce a predictor that is calibrated on every efficiently-identifiable subset.",1. Introduction,[0],[0]
"This motivates our definition of multicalibration, which loosely says: “A predictor f is multicalibrated with respect to a family of subpopulations C if it is calibrated with respect to every S ∈ C.”
In a nutshell, multicalibration guarantees highly-accurate predictions for every subpopulation of individuals identified by a specified collection C of subpopulations of individuals.",1. Introduction,[0],[0]
"While our results can be applied to any set system C, typically, we will think of C as a collection of subsets where set membership can be determined efficiently – for instance, subpopulations defined by the conjunctions of a small number of boolean features or by small decision trees.",1. Introduction,[0],[0]
"In this sense, we can take C to be sets identified by a class of bounded computations.",1. Introduction,[0],[0]
"As we increase the expressiveness of C, the fairness guarantee becomes stronger; no subpopulation that
can be identified within the class will be overlooked.
",1. Introduction,[0],[0]
"In the mortgage repayment example above, if the qualified members of S can be identified by some computation c ∈ C, then the resulting predictor cannot ignore the variance within S.",1. Introduction,[0],[0]
"We emphasize that the class C can be quite rich and, in particular, can contain many overlapping subgroups of a protected group S.",1. Introduction,[0],[0]
"In this sense, multicalibration goes far beyond calibration for a handful of sensitive groups, providing calibration for all computationally-identifiable subsets, where the notion of computational-identifiability is parameterized by the expressiveness of C.",1. Introduction,[0],[0]
We investigate the new notion of multicalibration from an algorithmic and complexity theoretic perspective.,1.1. Our Contributions,[0],[0]
"We present a simple, general-purpose algorithm for learning a predictor from a small set of labeled examples that is multicalibrated with respect to any given class C. The algorithm is an iterative method, similar to boosting, that can be viewed as a variant of functional gradient descent.",1.1. Our Contributions,[0],[0]
A number of subtleties arise when learning a multicalibrated predictor due to the fact that the calibration constraints change based on the current set of predictions made by the predictor.,1.1. Our Contributions,[0],[0]
"To guarantee generalization from a small sample of training examples, we leverage results from a new line of work connecting differential privacy to robust adaptive data analysis (Dwork et al., 2015a;b;c; Bassily et al., 2016).
",1.1. Our Contributions,[0],[0]
"We place no explicit restrictions on the hypothesis class of the learned predictor; instead, we show that implicitly our algorithm learns a model that provably generalizes well to unseen data, which may be of independent interest.",1.1. Our Contributions,[0],[0]
"We demonstrate this implicit generalization by showing the predictions we learn are compressible, in a sense similar to decomposition lemmas from pseudorandomness (Trevisan et al., 2009).",1.1. Our Contributions,[0],[0]
"In the language of circuit complexity, we show that we can build a circuit, only slightly larger than the circuits from C, that implements the learned predictor.",1.1. Our Contributions,[0],[0]
"As a corollary, the learned predictor is efficient in both space to represent and time to evaluate.
",1.1. Our Contributions,[0],[0]
"We also study the computational complexity of learning multicalibrated predictors for structured classes C. We show a strong connection between the complexity of learning a multicalibrated predictor and agnostic learning (Haussler, 1992; Kearns et al., 1994).",1.1. Our Contributions,[0],[0]
"In the positive direction, if there is an efficient (weak) agnostic learner (Kalai et al., 2008; Feldman, 2010) for a class C, then we can achieve similarly efficient multicalibration over C. In the other direction, we show that learning a multicalibrated predictor on all sets defined by C is as hard as weak agnostic learning C.",1.1. Our Contributions,[0],[0]
"In this sense, the complexity of learning a multicalibrated predictor with respect to a class C is equivalent to the complexity of weak agnostic learning C.
Finally, we demonstrate that the goal of multicalbration is aligned with the goal of achieving high-utility predictions.",1.1. Our Contributions,[0],[0]
"In particular, given any predictor h, we can post-process h to obtain a multicalibrated predictor f whose squared error is no worse than that of h.",1.1. Our Contributions,[0],[0]
The complexity of evaluating the predictor f is only slightly larger than that of h.,1.1. Our Contributions,[0],[0]
"In this sense, unlike many fairness notions, multicalibration is not at odds with predictive power and can be paired with any predictive model at essentially no cost to its accuracy.",1.1. Our Contributions,[0],[0]
Let X denote the domain of (feature vectors of) individuals; we wish to predict whether some event will occur for each individual.,2. Multicalibration Preliminaries,[0],[0]
"For each i ∈ X , we assume there is some unknown probability p∗i ∈",2. Multicalibration Preliminaries,[0],[0]
"[0, 1]; we make no assumptions on the structure of p∗ : X",2. Multicalibration Preliminaries,[0],[0]
"→ [0, 1].",2. Multicalibration Preliminaries,[0],[0]
"In particular, we assume that there is enough uncertainty in the outcomes that it may be hard to learn p∗ directly.",2. Multicalibration Preliminaries,[0],[0]
"Let D denote the distribution over individuals, supported on X ; for S ⊆ X , let i ∼ S denote a sample drawn from D conditioned on membership in S.1 In our learning setting, the algorithm has access to a small number of labeled individuals D ⊆ X , where for each i ∈ D, the label is the outcome oi ∼ Ber(p∗i ) of an independent Bernoulli trial.",2. Multicalibration Preliminaries,[0],[0]
"Given these samples, the learner aims to produce a predictor f :",2. Multicalibration Preliminaries,[0],[0]
X,2. Multicalibration Preliminaries,[0],[0]
"→ [0, 1] that achieves multicalibration, described formally next.
",2. Multicalibration Preliminaries,[0],[0]
Multicalibration.,2. Multicalibration Preliminaries,[0],[0]
"The most basic property we might hope for from a predictor is unbiasedness, i.e. that the predictions are accurate in expectation.
",2. Multicalibration Preliminaries,[0],[0]
Definition (Accuracy in expectation).,2. Multicalibration Preliminaries,[0],[0]
"For any α > 0 and S ⊆ X , a predictor f is α-accurate-in-expectation (AE) with respect to S if ∣∣∣ E
i∼S",2. Multicalibration Preliminaries,[0],[0]
[fi − p∗i ] ∣∣∣ ≤ α.,2. Multicalibration Preliminaries,[0],[0]
"(1) While this condition is necessary to achieve unbiased predictions, it is not sufficient to prevent all forms of discrimination; in particular, a predictor can be unbiased on a set S while introducing variance that is not borne out in the data, artificially treating similar individuals differently.",2. Multicalibration Preliminaries,[0],[0]
Calibration mitigates this form of discrimination by considering the expected values over categories Sv = {i : fi = v} defined by the predictor f .,2. Multicalibration Preliminaries,[0],[0]
"Specifically, α-calibration with respect to S requires that for all but an α-fraction of a set S, the average of the true probabilities of the individuals receiving prediction v is α-close to v.
1We remark that in order to guarantee a meaningful notion of fairness, we assume that the subpopulations we wish to protect are sufficiently represented in the distribution D, in order to see these populations in a random sample.",2. Multicalibration Preliminaries,[0],[0]
"Understanding how much representation is necessary in practice remains an interesting question for future empirical investigations.
",2. Multicalibration Preliminaries,[0],[0]
Definition (Calibration).,2. Multicalibration Preliminaries,[0],[0]
For any v ∈,2. Multicalibration Preliminaries,[0],[0]
"[0, 1], S ⊆ X , and predictor f , let Sv = {i : fi = v}.",2. Multicalibration Preliminaries,[0],[0]
For α ∈,2. Multicalibration Preliminaries,[0],[0]
"[0, 1], f is α-calibrated with respect to S if there exists some S′ ⊆ S with Pri∼D[i ∈ S′] ≥ (1− α) · Pri∼D[i ∈ S] such that for all v ∈",2. Multicalibration Preliminaries,[0],[0]
"[0, 1], ∣∣∣∣ Ei∼Sv∩S′[fi − p∗i ]
∣∣∣∣ ≤ α.",2. Multicalibration Preliminaries,[0],[0]
(2) Note that α-calibration with respect to S implies 2α-AE with respect to S. Our definition only requires the notion of calibration to hold on a (1−α)-fraction of each S; this is for technical reasons due to learning from a small sample and needing to discretize the range,2. Multicalibration Preliminaries,[0],[0]
"[0, 1] of the learned predictor.
",2. Multicalibration Preliminaries,[0],[0]
"For a collection of subsets C, we say that a predictor is (C, α)-multicalibrated if it is α-calibrated simultaneously on all S ∈ C. Definition (Multicalibration).",2. Multicalibration Preliminaries,[0],[0]
Let C ⊆ 2X be a collection of subsets of X and α ∈,2. Multicalibration Preliminaries,[0],[0]
"[0, 1].",2. Multicalibration Preliminaries,[0],[0]
"A predictor f is (C, α)multicalibrated if for all S ∈ C, f is α-calibrated with respect to S.
Discretization.",2. Multicalibration Preliminaries,[0],[0]
Even though α-calibration is a meaningful definition if we allow for arbitrary predictions fi ∈,2. Multicalibration Preliminaries,[0],[0]
"[0, 1], computationally, we need to maintain some discretization on the values v ∈",2. Multicalibration Preliminaries,[0],[0]
"[0, 1].",2. Multicalibration Preliminaries,[0],[0]
"Formally, we will use the following technical definition.
",2. Multicalibration Preliminaries,[0],[0]
Definition (λ-discretization).,2. Multicalibration Preliminaries,[0],[0]
Let λ > 0.,2. Multicalibration Preliminaries,[0],[0]
"The λ-discretization of [0, 1], denoted by Λ[0, 1] ={ λ 2 , 3λ 2 , . . .",2. Multicalibration Preliminaries,[0],[0]
", 1− λ 2 } , is the set of 1/λ evenly spaced real values over [0, 1].",2. Multicalibration Preliminaries,[0],[0]
"For v ∈ Λ[0, 1], let
λ(v) =",2. Multicalibration Preliminaries,[0],[0]
"[v − λ/2, v + λ/2)
be the λ-interval centered around v (except for the final interval, which will be [1− λ, 1]).
",2. Multicalibration Preliminaries,[0],[0]
"If we take λ = α, then the λ-discretization of a (C, α)multicalibrated predictor will be (C, 2α)-multicalibrated.
",2. Multicalibration Preliminaries,[0],[0]
"In what follows, we give an overview of our results and a flavor of the proof techniques.",2. Multicalibration Preliminaries,[0],[0]
"We defer complete coverage of the results and formal proofs to the Supplementary Materials (see also, the archival version (Hébert-Johnson et al., 2017)).",2. Multicalibration Preliminaries,[0],[0]
The first question to address is whether multicalibration is feasible.,3. Learning Multicalibrated Predictors,[0],[0]
"For instance, it could be the case that the requirements of multicalibration are so strong that they would require learning and representing an arbitrarily complex function p∗ very precisely, which can be infeasible in our setting.",3. Learning Multicalibrated Predictors,[0],[0]
Our first result characterizes the complexity of representing a multicalbrated predictor.,3. Learning Multicalibrated Predictors,[0],[0]
"We demonstrate that
multicalibration, indeed, can be achieved efficiently: for any p∗ and any collection of large subsets C, there exists a predictor that is α-multicalibrated on C, whose complexity is only slightly larger than the complexity required to describe the sets of C. For concreteness, we use circuit size as our measure of complexity in the following theorem.",3. Learning Multicalibrated Predictors,[0],[0]
Theorem 1.,3. Learning Multicalibrated Predictors,[0],[0]
"Suppose C ⊆ 2X is collection of sets where for S ∈ C, there is a circuit of size s that computes membership in S and Pri∼D[i ∈ S] ≥ γ.",3. Learning Multicalibrated Predictors,[0],[0]
"For any p∗ : X → [0, 1], there is a predictor that is (C, α)-multicalibrated implemented by a circuit of size O(s/α4γ).",3. Learning Multicalibrated Predictors,[0],[0]
"In fact, we prove Theorem 1 algorithmically by learning (C, α)-multicalibrated predictors from labeled samples.",3.1. The Algorithm,[0],[0]
Our algorithm is an iterative procedure.,3.1. The Algorithm,[0],[0]
"At a high level, the algorithm maintains a candidate predictor f , and at each iteration, corrects the candidate values of some subset that violates calibration until the candidate predictor is α-calibrated on every S ∈ C.",3.1. The Algorithm,[0],[0]
"We show that even if C is very large (e.g. exponential in the other relevant parameters), the number of updates we make and thus, the complexity of the learned model is bounded (polynomially in 1/α, 1/γ).
",3.1. The Algorithm,[0],[0]
"Recall that calibration over a set S requires that on the subsets Sv = {i ∈ S : fi = v} (which we will refer to throughout as categories), the expected value of the true probabilities Ei∼Sv [p∗i ] on this set is close to v. As such, the algorithm is easiest to describe in the statistical query model, where we query for estimates of the true statistics on subsets of the population and update the predictor based on these estimates.",3.1. The Algorithm,[0],[0]
"In particular, given a statistical query oracle that guarantees tolerance ω = O(αγ), the estimates will be accurate enough to guarantee α-calibration on sets S with such that Pri∼D[i ∈ S] ≥ γ.
",3.1. The Algorithm,[0],[0]
Adaptive Generalization.,3.1. The Algorithm,[0],[0]
"When we turn to adapting the algorithm to learn from random samples, the algorithm answers these statistical queries using the empirical estimates on some random sample from the population.",3.1. The Algorithm,[0],[0]
"Standard uniform convergence arguments (Kearns & Vazirani, 1994) show that if the set of queries we might ask is fixed in advance, then we could bound the sample complexity needed to answer these non-adaptive queries as Õ(log |C|/ω2).",3.1. The Algorithm,[0],[0]
"Note, however, that the categories Sv whose expectations we query are selected adaptively (i.e. with dependence on the results of prior queries).",3.1. The Algorithm,[0],[0]
"In particular, the definition of the categories Sv depends on the current values of the predictor f ; thus, when we update f based on the result of a statistical query, the set of categories on which we might ask a statistical query changes.",3.1. The Algorithm,[0],[0]
"In this case, we cannot simply apply concentration inequalities and take a union bound to guarantee good generalization without resampling every time we update the predictor.
",3.1. The Algorithm,[0],[0]
"To avoid this blow-up in sample complexity, we appeal to recently-uncovered connections between differential privacy and adaptive data analysis developed in (Dwork et al., 2015a;b;c; Bassily et al., 2016).",3.1. The Algorithm,[0],[0]
"To answer the statistical queries, our algorithm deliberately interacts with the data through a so-called guess-and-check oracle.",3.1. The Algorithm,[0],[0]
"In particular, each time the algorithm needs to know the value of a statistical query on a set S, rather than asking the query directly, we require that the algorithm submit its current guess fS = Ei∼S",3.1. The Algorithm,[0],[0]
"[fi] to the oracle, as well as an acceptable relative error window ω ∈",3.1. The Algorithm,[0],[0]
"[0, 1].",3.1. The Algorithm,[0],[0]
"Intuitively, if the algorithm’s guess is far from the window centered around the true expectation, then the oracle will respond with the answer to a statistical query with tolerance α ·Pri∼D[i ∈ S].",3.1. The Algorithm,[0],[0]
"If, however, the guess is sufficiently close to the true value, then the oracle responds with X to indicate that the current guess is close to the expectation, without revealing another answer.",3.1. The Algorithm,[0],[0]
Definition (Guess-and-check oracle).,3.1. The Algorithm,[0],[0]
Let q̃ : 2X ×,3.1. The Algorithm,[0],[0]
"[0, 1]× [0, 1] → [0, 1] ∪ {X}.",3.1. The Algorithm,[0],[0]
"q̃ is a guess-and-check oracle if for S ⊆ X with pS = Ei∼S [p∗i ], v ∈",3.1. The Algorithm,[0],[0]
"[0, 1], and any α > 0, the response to q̃(S, v, ω) satisfies the following conditions:
• if |pS",3.1. The Algorithm,[0],[0]
"− v| < 2ω, then q̃(S, v, ω) = X
• if |pS",3.1. The Algorithm,[0],[0]
"− v| > 4ω, then q̃(S, v, ω) ∈",3.1. The Algorithm,[0],[0]
"[0, 1]
• if q̃(S, v, ω) 6=",3.1. The Algorithm,[0],[0]
"X, then
pS − ω ≤ q̃(S, v, ω) ≤ pS + ω.
",3.1. The Algorithm,[0],[0]
Note that if the guess is such that |pS,3.1. The Algorithm,[0],[0]
− v| ∈,3.1. The Algorithm,[0],[0]
"[2ω, 4ω], the the oracle may respond with some ω-accurate r ∈",3.1. The Algorithm,[0],[0]
"[0, 1] or with X.",3.1. The Algorithm,[0],[0]
"If we have a lower bound ω0 = Pri∼D[i ∈ S] ·ω on a sequence of guess-and-check queries, we can implement the queries using a statistical query oracle with tolerance τ ≤ ω0; the advantage of using this guess-and-check framework is that it can be implemented using tools developed for differential privacy (Hardt & Rothblum, 2010).",3.1. The Algorithm,[0],[0]
"This will in turn allow us to give an algorithm for learning (C, α)multicalibrated predictors from a small number of samples that generalizes well.
",3.1. The Algorithm,[0],[0]
"With the definition of this mechanism in place, we give a description of the procedure in Algorithm 1.
",3.1. The Algorithm,[0],[0]
Implicit Representation of C. While this procedure will work for any collection C for efficiency’s sake (in the algorithm and the learned predictor),3.1. The Algorithm,[0],[0]
", it is important that we have some implicit representation of S ∈ C – i.e. membership tests can be evaluated by a simple model like a decision tree, neural network, etc.",3.1. The Algorithm,[0],[0]
"In particular, even though the algorithm updates the predictions for all i ∈ X , this update can be done implicitly by stringing together a “circuit” that tests membership, followed by the appropriate addition if the individual passes the test.
",3.1. The Algorithm,[0],[0]
"Algorithm 1 – Learning a (C, α)-multicalibrated predictor Let α, λ > 0 and let C ⊆ 2X .",3.1. The Algorithm,[0],[0]
"Let q̃(·, ·, ·) be a guess-and-check oracle.
",3.1. The Algorithm,[0],[0]
"• Initialize: f = (1/2, . . .",3.1. The Algorithm,[0],[0]
", 1/2) ∈",3.1. The Algorithm,[0],[0]
"[0, 1]X
• Repeat: ◦",3.1. The Algorithm,[0],[0]
"For each S ∈ C and v ∈ Λ[0, 1]:
– Let Sv = S ∩ {i : fi ∈ λ(v)} – if Pri∼D[i ∈",3.1. The Algorithm,[0],[0]
Sv] < αλ · Pri∼D[i ∈,3.1. The Algorithm,[0],[0]
"S]:
continue – Let v̄ = Ei∼Sv [fi] – Let r = q̃(Sv, v̄, α/4) –",3.1. The Algorithm,[0],[0]
"If r 6= X:
update fi ← fi + (r − v̄) for all i ∈",3.1. The Algorithm,[0],[0]
"Sv (project onto [0, 1] if necessary)
◦",3.1. The Algorithm,[0],[0]
"If no Sv updated: exit
• For v ∈ Λ[0, 1]: ◦ Let v̄ = Ei∼λ(v)[fi] ◦",3.1. The Algorithm,[0],[0]
"For i ∈ λ(v): fi ← v̄
• Output f
Formally, we prove the following theorem.
",3.1. The Algorithm,[0],[0]
Theorem 2.,3.1. The Algorithm,[0],[0]
"Suppose C ⊆ 2X is collection of sets such that for all S ∈ C, Pri∼D[i ∈",3.1. The Algorithm,[0],[0]
"S] ≥ γ, and suppose set membership can be evaluated in time t. Then Algorithm 1 run with λ = α learns a predictor of f :",3.1. The Algorithm,[0],[0]
X,3.1. The Algorithm,[0],[0]
"→ [0, 1] that is (C, 2α)-multicalibrated for p∗ from O(log(|C|)/α11/2γ3/2) samples in time O(|C| · t · poly(1/α, 1/γ)).",3.1. The Algorithm,[0],[0]
"Observing the linear dependence in the running time on |C|, it is natural to try to develop a learning procedure with subpolynomial, or even polylogarithmic, dependence on |C|.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
Our next results aim to characterize when this optimistic goal is possible – and when it is not.,4. Multicalibration and Weak Agnostic Learning,[0],[0]
We emphasize that the algorithm of Theorem 2 learns a multicalibrated predictor for arbitrary p∗ : X,4. Multicalibration and Weak Agnostic Learning,[0],[0]
"→ [0, 1] and C. In the setting where we cannot exploit structure in p∗ to learn efficiently, we might hope to exploit structure, if it exists, in the collection of subsets C.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Indeed, we demonstrate a connection between our goal of learning a multicalibrated predictor and weak agnostic learning, introduced in the literature on agnostic boosting (Ben-David et al., 2001; Kalai et al., 2008; Kanade & Kalai, 2009; Feldman, 2010).",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"More formally, we require a (ρ, τ)-weak agnostic learner as in (Kalai et al., 2008; Feldman, 2010).",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"We describe the distribution-specific learner of
(Feldman, 2010), where the samples and inner product in the definition are taken over the fixed data distribution D. Definition (Weak agnostic learner).",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Let ρ ≥ τ > 0, C ⊆ 2X , and H ⊆",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"[−1, 1]X .",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"A (ρ, τ)-weak agnostic learner L for a concept class C with hypothesis class H solves the following promise problem: given a collection of labeled samples {(i, yi)} where i ∼ D and yi ∈",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"[−1, 1], if there is some c ∈ C such that Ei∼D[ci · yi] > ρ, then L returns some h ∈ H such that Ei∼D[hi · yi] > τ .
",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Intuitively, if there is a concept c ∈ C that correlates nontrivially with the observed labels, then the weak agnostic learner returns a hypothesis h (not necessarily from C), that is also nontrivially correlated with the observed labels.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"In particular, ρ and τ are typically taken to be ρ = 1/p(d) and τ = 1/q(d) for polynomials p(d) ≤ q(d), where d = log(|C|).
",4. Multicalibration and Weak Agnostic Learning,[0],[0]
Efficient Multicalibration from Agnostic Learning.,4. Multicalibration and Weak Agnostic Learning,[0],[0]
Our next result shows that efficient weak agnostic learning over C implies efficient learning of α-multicalibrated predictors on C. Theorem 3.,4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Let ρ, τ > 0 and C ⊆ 2X be some concept class.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"If C admits a (ρ, τ)-weak agnostic learner that runs in time T (|C| , ρ, τ), then there is an algorithm that learns a predictor that is (C, α)-multicalibrated on C′ = {S ∈ C : Pri∼D[i ∈",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"S] ≥ γ} in time O(T (|C| , ρ, τ) · poly(1/α, 1/λ, 1/γ)) as long as ρ ≤ α2λγ/2 and τ = poly(α, λ, γ).
",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Recall, in our algorithm for learning multicalibrated predictors, we maintain a candidate predictor f , and iteratively search for some set S ∈ C on which f is not calibrated.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"To solve this search problem more quickly, we frame the search as weak agnostic learning over a concept class derived from C and over the hypothesis class ofH = {h : X",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"→ [−1, 1]}.
Specifically, consider the concept class defined by the collection of subsets C, where for each S ∈ C, we include the concept cS :",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"X → {−1, 1} where cS(i) = 1 if and only if i ∈ S.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
We show how to design a “labeling” ` : X,4. Multicalibration and Weak Agnostic Learning,[0],[0]
"→ [−1, 1] for individuals such that if f violates the calibration constraint on any S ∈ C, then the concept cS correlates nontrivially with the labels over the distribution of individuals, i.e. 〈cS , `〉 ≥ ρ for some ρ > 0.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Specifically, we will consider for each v ∈ Λ[0, 1], the following learning problem.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"For i ∈ Xv, let `i = fi−oi2 .",4. Multicalibration and Weak Agnostic Learning,[0],[0]
For i ∈ X \,4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Xv, let `i = 0.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"We claim that if there is some Sv currently in violation of multicalibration, then for i ∼ D, the labeled samples of either (i, `i) or (i,−`i) satisfy the weak learning promise for ρ = αβ/2.
",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Thus, if f is not yet multicalibrated on C, then we are promised that there is some concept cS with nontrivial correlation with the labels; we observe that this promise is
exactly the requirement for a weak agnostic learner, as defined in (Kalai et al., 2008; Feldman, 2010).",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"In particular, given labeled samples (i, `(i)) sampled according to D, if there is a concept cS with correlation at least ρ with `, then the weak agnostic learner returns a hypothesis h that is τ correlated with ` for some τ",4. Multicalibration and Weak Agnostic Learning,[0],[0]
< ρ.,4. Multicalibration and Weak Agnostic Learning,[0],[0]
"The catch is that this hypothesis may not be in our concept class C, so we cannot directly “correct” any S ∈ C.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Nevertheless, the labeling on individuals ` is designed such that given the hypothesis h, we can still extract an update to f that will make global progress towards the goal of attaining calibration.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"As long as τ is nontrvially lower bounded, we can upper bound the number of calls we need to make to the weak learner.
",4. Multicalibration and Weak Agnostic Learning,[0],[0]
Efficient Agnostic Learning from Multicalibration.,4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Our results so far show that under the right structural assumptions on p∗ or on C, a multicalibrated predictor may be learned more efficiently than our upper bound for the general case.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Returning to the general case, we may wonder if these structural assumptions are necessary; we answer this question in the positive.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
We show that for worst-case p∗ learning a multicalibrated predictor on C is as hard as weak agnostic learning for the class C. Theorem 4.,4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Let α, γ > 0",4. Multicalibration and Weak Agnostic Learning,[0],[0]
and suppose C ⊆ 2X is a concept class.,4. Multicalibration and Weak Agnostic Learning,[0],[0]
"If there is an algorithm for learning a (C′, α)-multicalibrated predictor on C′ = {S ∈ C : Pri∼D[i ∈ S] ≥ γ} in time T (|C| , α, γ) then we can implement a (ρ, τ)-weak agnostic learner for C in time O(T (|C| , α, γ) · poly(1/τ)) for any ρ, τ > 0",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"such that τ ≤ min {ρ− 2γ, ρ/4− 4α}.
",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Specifically, we show how to implement a weak agnostic learner for C, given an algorithm to learn an αmulticalibrated predictor f with respect to C (in fact, we only need the predictor to be multicalibrated on C′ = {S ∈ C : Pri∼D[i ∈",4. Multicalibration and Weak Agnostic Learning,[0],[0]
S] ≥ γ}).,4. Multicalibration and Weak Agnostic Learning,[0],[0]
"The key lemma for this reduction says that if there is some c ∈ C that is nontrivially correlated with the labels, then f is also nontrivially correlated with c. In general, agnostic learning is considered a notoriously hard computational problem.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"In particular, under cryptographic assumptions (Valiant, 1984; Goldreich et al., 1984; Bogdanov & Rosen, 2017), this result implies that there is some constant t > 0, such that any algorithm that learns a (C, α)-multicalibrated predictor requires Ω(|C|t) time for arbitrary C.
In combination, these results show that the complexity of learning a multicalibrated predictor with respect to a class C is equivalent to the complexity of weak agnostic learning C.",4. Multicalibration and Weak Agnostic Learning,[0],[0]
"Finally, we return our attention to investigating the utility of multicalibrated predictors.",5. Best-in-class Predictions,[0],[0]
"Above, we have argued that multicalibration provides a strong protection of groups against
discrimination.",5. Best-in-class Predictions,[0],[0]
We show that this protection comes at (next to) no cost in the utility of the predictor.,5. Best-in-class Predictions,[0],[0]
"This result adds to the growing literature on fairness-accuracy trade-offs (Fish et al., 2016; Berk et al., 2017; Chouldechova & G’Sell, 2017).
",5. Best-in-class Predictions,[0],[0]
Theorem 5.,5. Best-in-class Predictions,[0],[0]
Suppose C ⊆ 2X is a collection of subsets of X andH is a set of predictors.,5. Best-in-class Predictions,[0],[0]
"There is a predictor f that is α-multicalibrated on C such that
E i∼X",5. Best-in-class Predictions,[0],[0]
"[(fi − p∗i )2]− E i∼X [(h∗i − p∗i )2] < 6α,
where h∗ = argminh∈H Ei∼X",5. Best-in-class Predictions,[0],[0]
[(h−p∗)2].,5. Best-in-class Predictions,[0],[0]
"Further, suppose that for all S ∈ C, Pri∼D[i ∈ S] ≥ γ, and suppose that set membership for S ∈ C and h ∈ H are computable by circuits of size at most s; then f is computable by a circuit of size at most O(s/α4γ).
",5. Best-in-class Predictions,[0],[0]
"We can interpret Theorem 5 in different ways based on the choice ofH. Suppose there is some sophisticated learning algorithm that produces some predictor h that obtains exceptional performance, but may violate calibration arbitrarily.",5. Best-in-class Predictions,[0],[0]
"If we take H = {h}, then this result says: enforcing calibration on h after learning does not hurt the accuracy by much.
",5. Best-in-class Predictions,[0],[0]
"Taking a different perspective, we can also think ofH as a set of predictors that, say, are implemented by a circuit class of bounded complexity (e.g. conjunctions of k variables, halfspaces, circuits of size s).",5. Best-in-class Predictions,[0],[0]
"Leveraging Theorem 1 and Theorem 2, this theorem shows that for any such class of predictorsH of bounded complexity, there exists a multicalibrated predictor with similar complexity that performs as well as",5. Best-in-class Predictions,[0],[0]
the best h∗ ∈ H.,5. Best-in-class Predictions,[0],[0]
"In this sense, with just a slight overhead in complexity, multicalibrated predictors can achieve “best-in-class” predictions.
",5. Best-in-class Predictions,[0],[0]
"In contrast to many other notions of fairness, multicalibration does not limit the utility of a predictor.",5. Best-in-class Predictions,[0],[0]
"Further, to prove that multicalibration does not negatively impact the utility, we in fact, show a much stronger statement: if applying multicalibration to some h ∈ H changes the predictions of h significantly (i.e. if Ei∼D[(fi",5. Best-in-class Predictions,[0],[0]
"− hi)2] is large), then this change represents an improvement in squared error.",5. Best-in-class Predictions,[0],[0]
"In this sense, requiring multicalibration is aligned with the goals of learning a high-utility predictor.
",5. Best-in-class Predictions,[0],[0]
We give a flavor of our approach to proving Theorem 5.,5. Best-in-class Predictions,[0],[0]
"Consider some h ∈ H and consider the partition of X into sets according to the predictions of h – in particular, we will first apply a λ-discretization to the range of each h to partition X into categories.",5. Best-in-class Predictions,[0],[0]
"That is, let Sv(h) =",5. Best-in-class Predictions,[0],[0]
"{i : hi ∈ λ(v)}, and note that Sv(h) is disjoint from Sv′(h) for v 6= v′, and ⋃ v∈Λ[0,1] Sv(h) = X .",5. Best-in-class Predictions,[0],[0]
"In addition to calibrating with respect to S ∈ C, we can also ask for calibration on Sv(h) for all h ∈ H and v ∈ Λ[0, 1].",5. Best-in-class Predictions,[0],[0]
"Specifically, let S(H) = {Sv(h)}h∈H,v∈Λ[0,1]; we consider imposing cali-
bration on C∪S(H).",5. Best-in-class Predictions,[0],[0]
"Calibrating in this manner protects the groups defined by C but additionally gives a strong utility guarantee, captured by the following lemma.
",5. Best-in-class Predictions,[0],[0]
Lemma.,5. Best-in-class Predictions,[0],[0]
"Suppose g is a λ-discretized predictor and let S(g) = {Sv(g)}v∈Λ[0,1].",5. Best-in-class Predictions,[0],[0]
"Suppose f is an arbitrary (S(g), α)-multicalibrated predictor.",5. Best-in-class Predictions,[0],[0]
"Then for v ∈ Λ[0, 1],
E i∼Sv(g)
",5. Best-in-class Predictions,[0],[0]
"[ (gi − fi)2 ] − (4α+ λ)
≤ E i∼Sv(g)
[ (gi − p∗i )2 ]",5. Best-in-class Predictions,[0],[0]
"− E i∼Sv(g) [ (fi − p∗i )2 ] .
",5. Best-in-class Predictions,[0],[0]
"This lemma shows that calibrating on the categories of a predictor not only prevents the squared prediction error from degrading beyond a small additive approximation, but it also guarantees that if calibrating changes the predictor significantly on any category, this change represents significant progress towards the true underlying probabilities on this category.",5. Best-in-class Predictions,[0],[0]
"Assuming Lemma 5, Theorem 5 follows.
",5. Best-in-class Predictions,[0],[0]
"Note that Lemma 5 shows that this best-in-class property holds not just over the entire domain X , but on every sufficiently large category Sv(h) identified by some h ∈ H. That is, if f is calibrated on S(H), then for every category Sv(h), the average squared prediction error Ei∼Sv(h)",5. Best-in-class Predictions,[0],[0]
[ (fi − p∗i )2 ] will be at most 6α worse than prediction given by h on this set.,5. Best-in-class Predictions,[0],[0]
"If we view H as defining a set S(H) of “computationally-identifiable” categories, then we can view any predictor that is calibrated on S(H) as at least as fair and at least as accurate on this set of computationallyidentifiable categories as the predictor that identified the group (up to some small additive approximation).",5. Best-in-class Predictions,[0],[0]
Calibration.,6. Related Works and Discussion,[0],[0]
"Calibration is a well-studied concept in the literature on statistics and econometrics, particularly forecasting.",6. Related Works and Discussion,[0],[0]
"For a background on calibration in this context, see (Sandroni et al., 2003; Foster & Hart, 2015) and the references therein.",6. Related Works and Discussion,[0],[0]
"Calibration has also been studied in the context of structured predictions where the supported set of predictions is large (Kuleshov & Liang, 2015).",6. Related Works and Discussion,[0],[0]
"Our algorithmic result for multicalibration bears similarity to works from the online learning literature (Blum & Mansour, 2007; Khot & Ponnuswami, 2008; Trevisan et al., 2009).",6. Related Works and Discussion,[0],[0]
"While these works are similar in spirit, none of the algorithmic results apply directly to our setting of multicalibration.",6. Related Works and Discussion,[0],[0]
"We are unaware of prior works drawing connections between calibration and differential privacy / adaptive data analysis.
",6. Related Works and Discussion,[0],[0]
Parity and Balance.,6. Related Works and Discussion,[0],[0]
Other works on fairness in classification tend to look at parity-based notions of fairness.,6. Related Works and Discussion,[0],[0]
"Specifically, the notion of statistical parity (Dwork et al., 2012) and balanced error rates (Hardt et al., 2016) aim to enforce some notion of equal treatment across groups of
individuals defined by sensitive features, like race, gender, etc.",6. Related Works and Discussion,[0],[0]
"In (Hardt et al., 2016)",6. Related Works and Discussion,[0],[0]
"it is shown how to obtain equalized odds, a definition related to error-rate balance, as a post-processing step of “correcting” any predictor.
",6. Related Works and Discussion,[0],[0]
"While both calibration and balance (as well as other related variants) intuitively seem like good properties to expect in a fair predictor (even if they are a bit weak), it is impossible to satisfy both notions simultaneously (in non-degenerate cases) (Kleinberg et al., 2017; Chouldechova, 2017; Pleiss et al., 2017), and there is much debate about how to proceed given this incompatibility (Corbett-Davies et al., 2017).",6. Related Works and Discussion,[0],[0]
"The inherent conflict between balance and calibration, combined with our observation that calibration is always aligned with the goal of accurate high-utility predictions, implies that at times, balance must be at odds with obtaining predictive utility.",6. Related Works and Discussion,[0],[0]
"In this work, we strengthen the protections implied by calibration, rather than enforcing error-rate balance.",6. Related Works and Discussion,[0],[0]
"While there are certainly contexts in which “equalizing the odds” across groups is a good idea, there are also contexts where calibration is a more appropriate notion of fairness.
",6. Related Works and Discussion,[0],[0]
"One particular critique of balanced error rates as a fairness notion is that given two populations S, T ⊆ X with different base rates (i.e. p∗i > p ∗ j for i ∈ S, j ∈ T ), the Bayes Optimal predictor p∗ will not be balanced.",6. Related Works and Discussion,[0],[0]
"That is, even given access to perfect information about the underlying probabilities, the stochasticity in the outcomes will lead to different false positive and false negative rates.",6. Related Works and Discussion,[0],[0]
"In this sense, balance can be viewed as an a posteriori notion of fairness (fairness with respect to outcomes), while our notion of multicalibration is an a priori notion of fairness (fairness with respect to given data).",6. Related Works and Discussion,[0],[0]
"In a prediction setting where, given the data, there is still significant uncertainty in the outcome, we feel that multicalibration should be considered as an alternative to balanced error rates.",6. Related Works and Discussion,[0],[0]
"That said, a serious form of discrimination could arise if the uncertainty in outcomes is very different across different subpopulations; this would be a form of information-theoretic discrimination that multicalibration could help to identify, but could not remedy directly.
Between Populations and Individuals.",6. Related Works and Discussion,[0],[0]
"Most fairness notions are statistical in nature; roughly, these definitions – including statistical parity (Dwork et al., 2012), balanced error-rates (Hardt et al., 2016), and calibration – say that treatment across groups should be equitable on-average (for different notions equitable).",6. Related Works and Discussion,[0],[0]
"In a notable work, (Dwork et al., 2012) critique these broad-strokes statistical definitions and propose an individual notion of fairness, which aims to “treat similar individuals similarly”.",6. Related Works and Discussion,[0],[0]
A key challenge to this approach is that it assumes access to a taskspecific metric for every pair of individuals.,6. Related Works and Discussion,[0],[0]
"In the practical setting, where we want to learn from a small sample, we cannot hope to achieve such an information-theoretic notion
of fairness.",6. Related Works and Discussion,[0],[0]
One can view multicalibration as a meaningful compromise between group fairness (satisfying calibration) and individual-calibration (closely matching p∗i ).,6. Related Works and Discussion,[0],[0]
"The multicalibration framework presented in this work inspired subsequent work investigating how to interpolate between statistical and individual notions of “metric fairness” for general similarity metrics (Kim et al., 2018b), as well as further theoretical and empirical investigations of multi-accuracyin-expectation in the context of binary classification (Kim et al., 2018a).
",6. Related Works and Discussion,[0],[0]
"Contemporary independent work of (Kearns et al., 2017) also investigates strengthening the guarantees of notions of group fairness by requiring that these properties hold for a much richer collection of sets.",6. Related Works and Discussion,[0],[0]
"Unlike our work, their definitions require balance or statistical parity on these collection of sets.",6. Related Works and Discussion,[0],[0]
"Despite similar motivations, the two approaches to subgroup fairness differ in substantial ways.",6. Related Works and Discussion,[0],[0]
"As a concrete example, multicalibration is aligned with the incentives of achieving high-utility predictors; this is not necessarily the case with balance-based notions of fairness.",6. Related Works and Discussion,[0],[0]
"Indeed, in the setting considered in this work, one of the motivations for multicalibration is the earlier critique of balance that may only be heightened when considering “multi-balance”.
",6. Related Works and Discussion,[0],[0]
"Consider the example from (Dwork et al., 2012) where we wish to predict future success in school.",6. Related Works and Discussion,[0],[0]
"In a population S, the strongest students apply to Engineering whereas in the general population T , they apply to Business.",6. Related Works and Discussion,[0],[0]
Enforcing balance between the Business applicants and Engineering applicants within both groups would be unfair to qualified applicants in both groups (i.e. the Engineering students of S and the Business students of T ).,6. Related Works and Discussion,[0],[0]
"Essentially, carving up the space of individuals into subgroups exaggerates the differences in the base rates, which leads to mistreatment.",6. Related Works and Discussion,[0],[0]
"Preventing discrimination by algorithms is subtle, and different scenarios will call for different notions of protection.",6. Related Works and Discussion,[0],[0]
"Still, these works collectively validate the need to investigate attainable approaches to mitigating discrimination beyond large protected groups.
",6. Related Works and Discussion,[0],[0]
"Corrective Discrimination Multicalibration represents a powerful tool to address a certain form of discrimination, but it is not universally-applicable.",6. Related Works and Discussion,[0],[0]
"Consider the mortgage example again: perhaps the number of members of S that received loans in the past is small (and thus there are too few examples for fine-grained learning within S); perhaps the attributes are too limited to identify the qualified members of S (taking this point to the extreme, perhaps the only available attribute is membership in S).",6. Related Works and Discussion,[0],[0]
"In these cases, the data may be insufficient for multicalibration to provide meaningful guarantees.",6. Related Works and Discussion,[0],[0]
"Further, even if the algorithm was given access to unlimited rich data such that refined values of p∗ could be recovered, there are situations where preferential treatment may be in order: after all, the salaries of
members of S may be lower due to historical discrimination.",6. Related Works and Discussion,[0],[0]
"For these reasons, the concern that balance is inconsistent with p∗ could be answered with: “yes, and purposely so!”",6. Related Works and Discussion,[0],[0]
"Indeed, (Hardt et al., 2016) promotes enforcing a equalized odds as a form of “corrective discrimination.”",6. Related Works and Discussion,[0],[0]
"While this type of advocacy is important in many settings, multicalibration represents a different addition to the quiver of anti-discrimination measures, which we also believe is natural and desirable in many settings.
",6. Related Works and Discussion,[0],[0]
"Consider another example where multicalibration is appropriate, but equalizing error rates might not be: suppose a genomics company offers individuals a prediction of their likelihood of developing certain genetic disorders.",6. Related Works and Discussion,[0],[0]
"These disorders have different rates across different populations; e.g., Tay-Sachs disease is rare in the general population, but occurs much more frequently in the Ashkenazi population.",6. Related Works and Discussion,[0],[0]
We certainly do not want to enforce balance on the Ashkenazi population by down-weighting the prediction that individuals would have Tay-Sachs (as they are endogenously more likely to have the disease).,6. Related Works and Discussion,[0],[0]
"However, we also don’t want the company to base its prediction solely on the Ashkenazi feature.",6. Related Works and Discussion,[0],[0]
"Instead, enforcing multicalibration would require that the learning algorithm investigate both the Ashkenazi and non-Ashkenazi population to predict accurately in each group (even if this means a higher false positive rate in the Ashkenazi population).",6. Related Works and Discussion,[0],[0]
"In this case, relying on p∗ seems to be well-aligned with promoting fairness.
",6. Related Works and Discussion,[0],[0]
Conclusion.,6. Related Works and Discussion,[0],[0]
Multicalibration addresses a specific form of discrimination that can occur in prediction systems learned from data.,6. Related Works and Discussion,[0],[0]
"In particular, multicalibration requires that the learned predictor accurately reflects the “computationallyidentifiable” variance present in the data, without introducing spurious variance.",6. Related Works and Discussion,[0],[0]
"Multicalibration is most appropriate in settings where perfect predictions at an individual level are considered the fairest predictions, but where we do not have rich enough training data to make perfect predictions.",6. Related Works and Discussion,[0],[0]
"Importantly, in this context, there is no fairness-utility tradeoff!",6. Related Works and Discussion,[0],[0]
Enforcing multicalibration only improves the predictive power of the resulting model.,6. Related Works and Discussion,[0],[0]
"Instead, this work identifies and aims to address a “fairness-information” tradeoff; while we cannot achieve the information-theoretic ideal predictions from a small sample of training data, we show that attaining a meaningful complexity-theoretic relaxation of this goal is feasible through multicalibration.",6. Related Works and Discussion,[0],[0]
"Finally, we consider the interplay between multicalibration and “corrective discrimination,” such as the transformation of (Hardt et al., 2016), to be an important direction for further research.",6. Related Works and Discussion,[0],[0]
"The authors thank Cynthia Dwork, Roy Frostig, Parikshit Gopalan, Moritz Hardt, Aditi Raghunathan, Jacob Steinhardt, and Greg Valiant for helpful discussions related to this work.",Acknowledgments,[0],[0]
We thank the anonymous reviewers for their detailed feedback.,Acknowledgments,[0],[0]
MPK was supported by NSF grant CNS122864.,Acknowledgments,[0],[0]
OR was supported by NSF grant CCF-1749750.,Acknowledgments,[0],[0]
GNR was supported by ISF grant No. 5219/17.,Acknowledgments,[0],[0]
We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data).,abstractText,[0],[0]
Multicalibration guarantees meaningful (calibrated) predictions for every subpopulation that can be identified within a specified class of computations.,abstractText,[0],[0]
"The specified class can be quite rich; in particular, it can contain many overlapping subgroups of a protected group.",abstractText,[0],[0]
We demonstrate that in many settings this strong notion of protection from discrimination is provably attainable and aligned with the goal of accurate predictions.,abstractText,[0],[0]
"Along the way, we present algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and illustrate tight connections to the agnostic learning model.",abstractText,[0],[0]
Multicalibration: Calibration for the (Computationally-Identifiable) Masses,title,[0],[0]
"Existing automatic speech recognition (ASR) systems are based on a complicated hybrid of separate components, including acoustic, phonetic, and language models (Jelinek, 1976).",1. Introduction,[0],[0]
"Such systems are typically based on deep neural network acoustic models combined with hidden Markov models to represent the language and phonetic contextdependent state and their temporal alignment with the acoustic signal (DNN-HMM) (Bourlard & Morgan, 1994; Hinton et al., 2012).",1. Introduction,[0],[0]
"As a simpler alternative, end-to-end speech recognition paradigm has attracted great research
interest (Chorowski et al., 2014; 2015; Chan et al., 2016; Graves & Jaitly, 2014; Miao et al., 2015).",1. Introduction,[0],[0]
This paradigm simplifies the above hybrid architecture by subsuming it into a single neural network.,1. Introduction,[0],[0]
"Specifically, an attentionbased encoder-decoder framework (Chorowski et al., 2014) integrates all of those components using a set of recurrent neural networks (RNN), which map from acoustic feature sequences to character label sequences.
",1. Introduction,[0],[0]
"However, existing end-to-end frameworks have focused on clean speech, and do not include speech enhancement, which is essential to good performance in noisy environments.",1. Introduction,[0],[0]
"For example, recent industrial applications (e.g., Amazon echo) and benchmark studies (Barker et al., 2016; Kinoshita et al., 2016) show that multichannel speech enhancement techniques, using beamforming methods, produce substantial improvements as a pre-processor for conventional hybrid systems, in the presence of strong background noise.",1. Introduction,[0],[0]
"In light of the above trends, this paper extends the existing attention-based encoder-decoder framework by integrating multichannel speech enhancement.",1. Introduction,[0],[0]
"Our proposed multichannel end-to-end speech recognition framework is trained to directly translate from multichannel acoustic signals to text.
",1. Introduction,[0],[0]
"A key concept of the multichannel end-to-end framework is to optimize the entire inference procedure, including the beamforming, based on the final ASR objectives, such as word/character error rate (WER/CER).",1. Introduction,[0],[0]
"Traditionally, beamforming techniques such as delay-and-sum and filterand-sum are optimized based on a signal-level loss function, independently of speech recognition task (Benesty et al., 2008; Van Veen & Buckley, 1988).",1. Introduction,[0],[0]
"Their use in ASR requires ad-hoc modifications such as Wiener post-filtering or distortionless constraints, as well as steering mechanisms determine a look direction to focus the beamformer on the target speech (Wölfel & McDonough, 2009).",1. Introduction,[0],[0]
"In contrast, our framework incorporates recently proposed neural beamforming mechanisms as a differentiable component to allow joint optimization of the multichannel speech
ar X
iv :1
70 3.
04 78
3v 1
[ cs
.S",1. Introduction,[0],[0]
"D
] 1
4 M
ar 2
enhancement within the end-to-end system to improve the ASR objective.
",1. Introduction,[0],[0]
"Recent studies on neural beamformers can be categorized into two types: (1) beamformers with a filter estimation network (Xiao et al., 2016a; Li et al., 2016) and (2) beamformers with a mask estimation network (Heymann et al., 2016; Erdogan et al., 2016).",1. Introduction,[0],[0]
Both methods obtain an enhanced signal based on the formalization of the conventional filter-and-sum beamformer in the time-frequency domain.,1. Introduction,[0],[0]
The main difference between them is how the multichannel filters are produced by the neural network.,1. Introduction,[0],[0]
"In the former approach, the multichannel filter coefficients are direct outputs of the network.",1. Introduction,[0],[0]
"In the latter approach, a network first estimates time-frequency masks, which are used to compute expected speech and noise statistics.",1. Introduction,[0],[0]
"Then, using these statistics, the filter coefficients are computed based on the well-known MVDR (minimum variance distortionless response) formalization (Capon, 1969).",1. Introduction,[0],[0]
"In both approaches, the estimated filter coefficients are then applied to the multichannel noisy signal to enhance the speech signal.",1. Introduction,[0],[0]
"Note that the mask estimation approach has the advantage of leveraging well-known techniques, but it requires parallel data composed of aligned clean and noisy speech, which are usually difficult to obtain without data simulation.
",1. Introduction,[0],[0]
"Recently, it has been reported that the mask estimationbased approaches (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016) achieve great performance in noisy speech recognition benchmarks (e.g., CHiME 3 and 4 challenges)1.",1. Introduction,[0],[0]
"Although this paper proposes to incorporate both mask and filter estimation approaches in an endto-end framework, motivated by those successes, we focus more on the mask estimation, implementing it along with the MVDR estimation as a differentiable network.",1. Introduction,[0],[0]
Our MVDR formulation estimates the speech image at the reference microphone and includes selection of the reference microphone using an attention mechanism.,1. Introduction,[0],[0]
"By using channel-independent mask estimation along with this reference selection, the model can generalize to different microphone array geometries (number of channels, microphone locations, and ordering), unlike the filter estimation approach.",1. Introduction,[0],[0]
"Finally, because the masks are latent variables in the end-to-end training, we no longer need parallel clean and noisy speech.
",1. Introduction,[0],[0]
"The main advantages of our proposed multichannel end-toend speech recognition system are:
1.",1. Introduction,[0],[0]
"Overall inference from speech enhancement to recognition is jointly optimized for the ASR objective.
",1. Introduction,[0],[0]
"1Yoshioka et al. 2015 uses a clustering technique to perform mask estimation rather than the neural network-based techniques, but it uses the same MVDR formulation for filter estimation.
2.",1. Introduction,[0],[0]
"The trained system can be used for input signals with arbitrary number and order of channels.
3.",1. Introduction,[0],[0]
Parallel clean and noisy data are not required.,1. Introduction,[0],[0]
We can optimize the speech enhancement component with noisy signals and their transcripts.,1. Introduction,[0],[0]
"This section explains a conventional attention-based encoder-decoder framework, which is used to directly deal with variable length input and output sequences.",2. Overview of attention-based encoder-decoder networks,[0],[0]
"The framework consists of two RNNs, called encoder and decoder respectively, and an attention mechanism, which connects the encoder and decoder, as shown in Figure 1.",2. Overview of attention-based encoder-decoder networks,[0],[0]
"Given a T -length sequence of input features O = {ot ∈ RDO |t = 1, · · · , T}, the network generates an N -length sequence of output labels Y = {yn ∈ V|n = 1, · · · , N}, where ot is a DO-dimensional feature vector (e.g., log Mel filterbank) at input time step t, and yn is a label symbol (e.g., character) at output time step n in label set V .
",2. Overview of attention-based encoder-decoder networks,[0],[0]
"First, given an input sequence O, the encoder network transforms it to an L-length high-level feature sequence H = {hl ∈ RDH |l = 1, · · · , L}, where hl is a DHdimensional state vector at a time step l of encoder’s top layer.",2. Overview of attention-based encoder-decoder networks,[0],[0]
"In this work, the encoder network is composed of a bidirectional long short-term memory (BLSTM) recurrent network.",2. Overview of attention-based encoder-decoder networks,[0],[0]
"To reduce the input sequence length, we apply a subsampling technique (Bahdanau et al., 2016) to some layers.",2. Overview of attention-based encoder-decoder networks,[0],[0]
"Therefore, l represents the frame index subsampled from t and L is less than T .
",2. Overview of attention-based encoder-decoder networks,[0],[0]
"Next, the attention mechanism integrates all encoder outputs H into a DH-dimensional context vector cn ∈ RDH
based on an L-dimensional attention weight vector an ∈",2. Overview of attention-based encoder-decoder networks,[0],[0]
"[0, 1]L, which represents a soft alignment of encoder outputs at an output time step n. In this work, we adopt a location-based attention mechanism (Chorowski et al., 2015), and an and cn are formalized as follows:
",2. Overview of attention-based encoder-decoder networks,[0],[0]
"fn = F ∗ an−1, (1) kn,l = w Ttanh(VSsn + V Hhl + V Ffn,l + b), (2)
",2. Overview of attention-based encoder-decoder networks,[0],[0]
"an,l = exp(αkn,l)∑L l=1 exp(αkn,l) , cn = L∑ l=1 an,lhl, (3)
where w ∈ R1×DW , VH ∈ RDW×DH , VS ∈ RDW×DS , VF ∈ RDW×DF are trainable weight matrices, b ∈ RDW is a trainable bias vector, F ∈ RDF×1×Df is a trainable convolution filter.",2. Overview of attention-based encoder-decoder networks,[0],[0]
"sn ∈ RDS is a DS-dimensional hidden state vector obtained from an upper decoder network at n, and α is a sharpening factor (Chorowski et al., 2015).",2. Overview of attention-based encoder-decoder networks,[0],[0]
"∗ denotes the convolution operation.
",2. Overview of attention-based encoder-decoder networks,[0],[0]
"Then, the decoder network incrementally updates a hidden state sn and generates an output label yn as follows:
sn = Update(sn−1, cn−1, yn−1), (4) yn = Generate(sn, cn), (5)
where the Generate(·) and Update(·) functions are composed of a feed forward network and an LSTM-based recurrent network, respectively.
",2. Overview of attention-based encoder-decoder networks,[0],[0]
"Now, we can summarize these procedures as follows: P (Y |O) = ∏ n P (yn|O, y1:n−1), (6)
H = Encoder(O), (7) cn = Attention(an−1, sn, H), (8) yn = Decoder(cn, y1:n−1), (9)
where Encoder(·) = BLSTM(·), Attention(·) corresponds to Eqs.",2. Overview of attention-based encoder-decoder networks,[0],[0]
"(1)-(3), and Decoder(·) corresponds to Eqs.",2. Overview of attention-based encoder-decoder networks,[0],[0]
(4) and (5).,2. Overview of attention-based encoder-decoder networks,[0],[0]
"Here, special tokens for start-of-sentence (sos) and end-of-sentence (eos) are added to the label set V .",2. Overview of attention-based encoder-decoder networks,[0],[0]
The decoder starts the recurrent computation with the (sos) label and continues to generate output labels until the (eos) label is emitted.,2. Overview of attention-based encoder-decoder networks,[0],[0]
"Figure 1 illustrates such procedures.
",2. Overview of attention-based encoder-decoder networks,[0],[0]
"Based on the cross-entropy criterion, the loss function is defined using Eq.",2. Overview of attention-based encoder-decoder networks,[0],[0]
"(6) as follows:
L = − lnP (Y ∗|O) =",2. Overview of attention-based encoder-decoder networks,[0],[0]
"− ∑ n lnP (y∗n|O, y∗1:n−1), (10)
where Y ∗ is the ground truth of a whole sequence of output labels and y∗1:n−1 is the ground truth of its subsequence until an output time step n− 1.
",2. Overview of attention-based encoder-decoder networks,[0],[0]
"In this framework, the whole networks including the encoder, attention, and decoder can be optimized to generate
the correct label sequence.",2. Overview of attention-based encoder-decoder networks,[0],[0]
This consistent optimization of all relevant procedures is the main motivation of the endto-end framework.,2. Overview of attention-based encoder-decoder networks,[0],[0]
"This section explains neural beamformer techniques, which are integrated with the encoder-decoder network in the following section.",3. Neural beamformers,[0],[0]
"This paper uses frequency-domain beamformers rather than time-domain ones, which achieve significant computational complexity reduction in multichannel neural processing (Li et al., 2016; Sainath et al., 2016).",3. Neural beamformers,[0],[0]
"In the frequency domain representation, a filter-and-sum beamformer obtains an enhanced signal as follows:
x̂t,f = C∑ c=1 gt,f,cxt,f,c, (11)
where xt,f,c ∈ C is an STFT coefficient of c-th channel noisy signal at a time-frequency bin (t, f).",3. Neural beamformers,[0],[0]
"gt,f,c ∈ C is a corresponding beamforming filter coefficient.",3. Neural beamformers,[0],[0]
"x̂t,f ∈ C is an enhanced STFT coefficient, and C is the numbers of channels.
",3. Neural beamformers,[0],[0]
"In this paper, we adopt two types of neural beamformers, which basically follow Eq.",3. Neural beamformers,[0],[0]
(11); 1) filter estimation network and 2) mask estimation network.,3. Neural beamformers,[0],[0]
Figure 2 illustrates the schematic structure of each approach.,3. Neural beamformers,[0],[0]
"The main difference between them is how to compute the filter coefficient gt,f,c.",3. Neural beamformers,[0],[0]
The following subsections describe each approach.,3. Neural beamformers,[0],[0]
"The filter estimation network directly estimates a timevariant filter coefficients {gt,f,c}T,F,Ct=1,f=1,c=1 as the outputs of the network, which was originally proposed in (Li et al., 2016).",3.1. Filter estimation network approach,[0],[0]
"F is the dimension of STFT features.
",3.1. Filter estimation network approach,[0],[0]
This approach uses a single real-valued BLSTM network to predict the real and imaginary parts of the complex-valued filter coefficients at an every time step.,3.1. Filter estimation network approach,[0],[0]
"Therefore, we introduce multiple (2 × C) output layers to separately compute the real and imaginary parts of the filter coefficients for each channel.",3.1. Filter estimation network approach,[0],[0]
"Then, the network outputs time-variant filter coefficients gt,c = {gt,f,c}Ff=1 ∈ CF at a time step t for c-th channel as follows;
Z = BLSTM({x̄t}Tt=1), (12) <(gt,c) =",3.1. Filter estimation network approach,[0],[0]
"tanh(W<c zt + b<c ), (13) =(gt,c) =",3.1. Filter estimation network approach,[0],[0]
"tanh(W=c zt + b=c ), (14)
",3.1. Filter estimation network approach,[0],[0]
"where Z = {zt ∈ RDZ |t = 1, · · · , T}is a sequence of DZdimensional output vectors of the BLSTM network.",3.1. Filter estimation network approach,[0],[0]
x̄t,3.1. Filter estimation network approach,[0],[0]
"= {<(xt,f,c),=(xt,f,c)}F,Cf=1,c=1 ∈ R2FC is an input feature of a 2FC-dimensional real-value vector for the BLSTM network.",3.1. Filter estimation network approach,[0],[0]
This is obtained by concatenating the real and imaginary parts of all STFT coefficients in all channels.,3.1. Filter estimation network approach,[0],[0]
"<(gt,c) and =(gt,c) is the real and imaginary part of filter coefficients, W<c ∈ RF×DZ and W=c ∈ RF×DZ are the weight matrices of the output layer for c-th channel, and b<c ∈ RF and b=c ∈",3.1. Filter estimation network approach,[0],[0]
RF are their corresponding bias vectors.,3.1. Filter estimation network approach,[0],[0]
"Using the estimated filters gt,c, the enhanced STFT coefficients x̂t,f are obtained based on Eq.",3.1. Filter estimation network approach,[0],[0]
"(11).
",3.1. Filter estimation network approach,[0],[0]
This approach has several possible problems due to its formalization.,3.1. Filter estimation network approach,[0],[0]
"The first issue is the high flexibility of the estimated filters {gt,f,c}T,F,Ct=1,f=1,c=1, which are composed of a large number of unconstrained variables (2TFC) estimated from few observations.",3.1. Filter estimation network approach,[0],[0]
This causes problems such as training difficulties and over-fitting.,3.1. Filter estimation network approach,[0],[0]
The second issue is that the network structure depends on the number and order of channels.,3.1. Filter estimation network approach,[0],[0]
"Therefore, a new filter estimation network has to be trained when we change microphone configurations.",3.1. Filter estimation network approach,[0],[0]
The key point of the mask estimation network approach is that it constrains the estimated filters based on wellfounded array signal processing principles.,3.2. Mask estimation network approach,[0],[0]
"Here, the network estimates the time-frequency masks, which are used to compute the time-invariant filter coefficients {gf,c}F,Cf=1,c=1 based on the MVDR formalizations.",3.2. Mask estimation network approach,[0],[0]
This is the main difference between this approach and the filter estimation network approach described in Section 3.1.,3.2. Mask estimation network approach,[0],[0]
"Also, mask-based beamforming approaches have achieved great performance in noisy speech recognition benchmarks (Yoshioka et al., 2015; Heymann et al., 2016; Erdogan et al., 2016).",3.2. Mask estimation network approach,[0],[0]
"Therefore, this paper proposes to use a maskbased MVDR beamformer, where overall procedures are formalized as a differentiable network for the subsequent end-to-end speech recognition system.",3.2. Mask estimation network approach,[0],[0]
"Figure 3 summarizes the overall procedures to compute the filter coefficients, which is a detailed flow of Figure 2 (b).",3.2. Mask estimation network approach,[0],[0]
"One of the MVDR formalizations computes the timeinvariant filter coefficients g(f) = {gf,c}Cc=1 ∈ CC in Eq.",3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
"(11) as follows (Souden et al., 2010):
g(f) = ΦN(f)−1ΦS(f)
Tr(ΦN(f)−1ΦS(f)) u, (15)
where ΦS(f) ∈ CC×C and ΦN(f) ∈ CC×C are the crosschannel power spectral density (PSD) matrices (also known as spatial covariance matrices) for speech and noise signals, respectively.",3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
"u ∈ RC is the one-hot vector representing a reference microphone, and Tr(·) is the matrix trace operation.",3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
"Note that although the formula contains a matrix inverse, the number of channels is relatively small, and so the forward pass and derivatives can be efficiently computed.
",3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
"Based on (Yoshioka et al., 2015; Heymann et al., 2016), the PSD matrices are robustly estimated using the expectation with respect to time-frequency masks as follows:
ΦS(f) =",3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
"1∑T
t=1m S t,f T∑ t=1",3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
"mSt,fxt,fx † t,f , (16)
ΦN(f) = 1∑T
t=1m N t,f T∑ t=1 mNt,fxt,fx † t,f , (17)
where xt,f = {xt,f,c}Cc=1 ∈ CC is the spatial vector of an observed signal for each time-frequency bin, mSt,f ∈",3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
"[0, 1] and mNt,f ∈",3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
"[0, 1] are the time-frequency masks for speech and noise, respectively.",3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
† represents the conjugate transpose.,3.2.1. MASK-BASED MVDR FORMALIZATION,[0],[0]
"In the mask estimation network approach, we use two realvalued BLSTM networks; one for a speech mask and the other for a noise mask.",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"Each network outputs the timefrequency mask as follows:
ZSc = BLSTM S({x̄t,c}Tt=1), (18)
mSt,c = sigmoid(W SzSt,c + b S), (19)
",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"ZNc = BLSTM N({x̄t,c}Tt=1), (20)
mNt,c = sigmoid(W NzNt,c + b N), (21)
where ZSc = {zSt,c ∈ RDZ |t = 1, · · · , T} is the output sequence of DZ-dimensional vectors of the BLSTM network to obtain a speech mask over c-th channel’s input STFTs.",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
ZNc is the BLSTM output sequence for a noise mask.,3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"x̄t,c = {<(xt,f,c),=(xt,f,c)}Ff=1 ∈",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
R2F is an input feature of a 2F -dimensional real-value vector.,3.2.2. MASK ESTIMATION NETWORK,[0],[0]
This is obtained by concatenating the real and imaginary parts of all STFT features at c-th channel.,3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"mSt,c = {mSt,f,c}Ff=1 ∈",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"[0, 1]F and mNt,c are the estimated speech and noise masks for every c-th channel at a time step t, respectively.",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"WS,WN ∈ RF×DZ are the weight matrices of the output layers to finally output speech and noise masks, respectively, and bS,bN ∈ RF are their corresponding bias vectors.
",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"After computing the speech and noise masks for each channel, the averaged masks are obtained as follows:
mSt = 1
C C∑ c=1",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"mSt,c, m N t = 1 C C∑ c=1",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"mNt,c. (22)
",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
We use these averaged masks to estimate the PSD matrices as described in Eqs.,3.2.2. MASK ESTIMATION NETWORK,[0],[0]
(16) and (17).,3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"The MVDR beamformer through this BLSTM mask estimation is originally proposed in (Heymann et al., 2016), but our neural beamformer further extends it with attention-based reference selection, which is described in the next subsection.",3.2.2. MASK ESTIMATION NETWORK,[0],[0]
"To incorporate the reference microphone selection in a neural beamformer framework, we use a soft-max for the vector u in Eq.",3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
(15) derived from an attention mechanism.,3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
"In this approach, the reference microphone vector u is estimated from time-invariant feature vectors qc and rc as follows:
k̃c = v Ttanh(VQqc + V Rrc + b̃), (23) uc = exp(βk̃c)∑C c=1 exp(βk̃c) , (24)
where v ∈ R1×DV ,VZ ∈ RDV×2DZ ,VR ∈",3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
"RDV×2F are trainable weight parameters, b̃ ∈ RDV is a trainable bias vector.",3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
β is the sharpening factor.,3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
"We use two types of
features; 1) the time-averaged state vector qc ∈ R2DZ extracted from the BLSTM networks for speech and noise masks in Eqs.",3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
"(18) and (20), i.e.,
qc = 1
T T∑ t=1 {zSt,c, zNt,c}, (25)
and 2) the PSD feature rc ∈ R2F , which incorporates the spatial information into the attention mechanism.",3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
"The following equation represents how to compute rc:
rc = 1
C − 1 C∑ c′=1,c′ 6=c {<(φSf,c,c′),=(φSf,c,c′)}Ff=1, (26)
where φSf,c,c′ ∈ C is the entry in c-th row and c′-th column of the speech PSD matrix ΦS(f) in Eq.",3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
(16).,3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
The PSD matrix represents correlation information between channels.,3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
"To select a reference microphone, the spatial correlation related to speech signals is more informative, and therefore, we only use the speech PSD matrix ΦS(f) as a feature.
",3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
"Note that, in this mask estimation based MVDR beamformer, masks for each channel are computed separately using the same BLSTM network unlike Eq. (12), and the mask estimation network is independent of channels.",3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
"Similarly, the reference selection network is also independent of channels, and the beamformer deals with input signals with arbitrary number and order of channels without re-training or re-configuration of the network.",3.2.3. ATTENTION-BASED REFERENCE SELECTION,[0],[0]
"In this work, we propose a multichannel end-to-end speech recognition, which integrates all components with a single neural architecture.",4. Multichannel end-to-end ASR,[0],[0]
"We adopt neural beamformers (Section 3) as a speech enhancement part, and the attention-based encoder-decoder (Section 2) as a speech recognition part.
",4. Multichannel end-to-end ASR,[0],[0]
"The entire procedure to generate the sequence of output labels Ŷ from the multichannel inputs {Xc}Cc=1 is formalized as follows:
X̂ = Enhance({Xc}Cc=1), (27) Ô = Feature(X̂), (28)
Ĥ = Encoder(Ô), (29)
ĉn = Attention(ân−1, ŝn, Ĥ), (30) ŷn = Decoder(ĉn, ŷ1:n−1).",4. Multichannel end-to-end ASR,[0],[0]
"(31)
Enhance(·) is a speech enhancement function realized by the neural beamformer based on Eq.",4. Multichannel end-to-end ASR,[0],[0]
"(11) with the filter or mask estimation network (Section 3.1 or 3.2).
",4. Multichannel end-to-end ASR,[0],[0]
Feature(·) is a feature extraction function.,4. Multichannel end-to-end ASR,[0],[0]
"In this work, we use a normalized log Mel filterbank transform to obtain
ôt ∈ RDO computed from the enhanced STFT coefficients x̂t ∈ CF as an input of attention-based encoder-decoder:
pt = {<(x̂t,f )2 + =(x̂t,f )2}Ff=1, (32) ôt",4. Multichannel end-to-end ASR,[0],[0]
"= Norm(log(Mel(pt))), (33)
where pt ∈ RF is a real-valued vector of the power spectrum of the enhanced signal at a time step t, Mel(·) is the operation of DO × F Mel matrix multiplication, and Norm(·) is the operation of global mean and variance normalization so that its mean and variance become 0 and 1.
",4. Multichannel end-to-end ASR,[0],[0]
"Encoder(·), Attention(·), and Decoder(·) are defined in Eqs.",4. Multichannel end-to-end ASR,[0],[0]
"(7), (8), and (9), respectively, with the sequence of the enhanced log Mel filterbank like features Ô as an input.
",4. Multichannel end-to-end ASR,[0],[0]
"Thus, we can build a multichannel end-to-end speech recognition system, which converts multichannel speech signals to texts with a single network.",4. Multichannel end-to-end ASR,[0],[0]
"Note that because all procedures, such as enhancement, feature extraction, encoder, attention, and decoder, are connected with differentiable graphs, we can optimize the overall inference to generate a correct label sequence.
",4. Multichannel end-to-end ASR,[0],[0]
"Relation to prior works
There have been several related studies of neural beamformers based on the filter estimation (Li et al., 2016; Xiao et al., 2016a) and the mask estimation (Heymann et al., 2016; Erdogan et al., 2016; Xiao et al., 2016b).",4. Multichannel end-to-end ASR,[0],[0]
"The main difference is that such preceding studies use a component-level training objective within the conventional hybrid frameworks, while our work focuses on the entire end-to-end objective.",4. Multichannel end-to-end ASR,[0],[0]
"For example, Heymann et al., 2016; Erdogan et al., 2016 use a signal-level objective (binary mask classification or regression) to train a network given parallel clean and noisy speech data.",4. Multichannel end-to-end ASR,[0],[0]
"Li et al., 2016; Xiao et al., 2016a;b use ASR objectives (HMM state classification or sequence discriminative training), but they are still based on the hybrid approach.",4. Multichannel end-to-end ASR,[0],[0]
"Speech recognition with raw multichannel waveforms (Hoshen et al., 2015; Sainath et al., 2016) can also be seen as using a neural beamformer, where the filter coefficients are represented as network parameters, but again these methods are still based on the hybrid approach.
",4. Multichannel end-to-end ASR,[0],[0]
"As regards end-to-end speech recognition, all existing studies are based on a single channel setup.",4. Multichannel end-to-end ASR,[0],[0]
"For example, most studies focus on a standard clean speech recognition setup without speech enhancement.",4. Multichannel end-to-end ASR,[0],[0]
"(Chorowski et al., 2014; Graves & Jaitly, 2014; Chorowski et al., 2015; Chan et al., 2016; Miao et al., 2015; Zhang et al., 2016; Kim et al., 2016; Lu et al., 2016).",4. Multichannel end-to-end ASR,[0],[0]
"Amodei et al., 2016 discusses endto-end speech recognition in a noisy environment, but this method deals with the noise robustness by preparing various types of simulated noisy speech for training data, and
does not incorporate multichannel speech enhancement in their networks.",4. Multichannel end-to-end ASR,[0],[0]
We study the effectiveness of our multichannel end-toend system compared to a baseline end-to-end system with noisy speech or beamformed inputs.,5. Experiments,[0],[0]
"We use the two multichannel speech recognition benchmarks, CHiME-4 (Vincent et al., 2016) and AMI (Hain et al., 2007).
",5. Experiments,[0],[0]
"CHiME-4 is a speech recognition task in public noisy environments, consisting of speech recorded using a tablet device with 6-channel microphones.",5. Experiments,[0],[0]
It consists of real and simulated data.,5. Experiments,[0],[0]
The training set consists of 3 hours of real speech data uttered by 4 speakers and 15 hours of simulation speech data uttered by 83 speakers.,5. Experiments,[0],[0]
"The development set consists of 2.9 hours of real and simulation speech data uttered by 4 speakers, respectively.",5. Experiments,[0],[0]
"The evaluation set consists of 2.2 hours of real and simulation speech data uttered by 4 speakers, respectively.",5. Experiments,[0],[0]
"We excluded the 2nd channel signals, which is captured at the microphone located on the backside of the tablet, and used 5 channels for the following multichannel experiments (C = 5).
",5. Experiments,[0],[0]
"AMI is a speech recognition task in meetings, consisting of speech recorded using 8-channel circular microphones (C = 8).",5. Experiments,[0],[0]
It consists of only real data.,5. Experiments,[0],[0]
The training set consists of about 78 hours of speech data uttered by 135 speakers.,5. Experiments,[0],[0]
"the development and evaluation sets consist of about 9 hours of speech data uttered by 18 and 16 speakers, respectively.",5. Experiments,[0],[0]
"The amount of training data (i.e., 78 hours) is larger than one for CHiME-4 (i.e., 18 hours), and we mainly used CHiME-4 data to demonstrate our experiments.",5. Experiments,[0],[0]
We used 40-dimensional log Mel filterbank coefficients as an input feature vector for both noisy and enhanced speech signals (DO = 40).,5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"In this experiment, we used 4-layer BLSTM with 320 cells in the encoder (DH = 320), and 1-layer LSTM with 320 cells in the decoder (DS = 320).",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"In the encoder, we subsampled the hidden states of the first and second layers and used every second of hidden states for the subsequent layer’s inputs.",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"Therefore, the number of hidden states at the encoder’s output layer is reduced to L = T/4.",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"After every BLSTM layer, we used a linear projection layer with 320 units to combine the forward and backward LSTM outputs.",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"For the attention mechanism, 10 centered convolution filters (DF = 10) of width 100 (Df = 100) were used to extract the convolutional features.",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"We set the attention inner product dimension as 320 (DW = 320), and used the sharpening factor α = 2.",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"To boost the optimization in a noisy environment, we adopted a joint
CTC-attention multi-task loss function (Kim et al., 2016), and set the CTC loss weight as 0.1.
",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"For decoding, we used a beam search algorithm similar to (Sutskever et al., 2014) with the beam size 20 at each output step to reduce the computation cost.",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
CTC scores were also used to re-score the hypotheses with 0.1 weight.,5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"We adopted a length penalty term (Chorowski et al., 2015) to the decoding objective and set the penalty weight as 0.3.",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"In the CHiME-4 experiments, we only allowed the hypotheses whose length were within 0.3×L and 0.75×L during decoding, while the hypothesis lengths in the AMI experiments were automatically determined based on the above scores.",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
"Note that we pursued a pure end-to-end setup without using any external lexicon or language models, and used CER as an evaluation metric.",5.1.1. ENCODER-DECODER NETWORKS,[0],[0]
256 STFT coefficients and the offset were computed from 25ms-width hamming window with 10ms shift (F = 257).,5.1.2. NEURAL BEAMFORMERS,[0],[0]
Both filter and mask estimation network approaches used similar a 3-layer BLSTM with 320 cells (DZ = 320) without the subsampling technique.,5.1.2. NEURAL BEAMFORMERS,[0],[0]
"For the reference selection attention mechanism, we used the same attention inner product dimension (DV = 320) and sharpening factor β = 2 as those of the encoder-decoder network.",5.1.2. NEURAL BEAMFORMERS,[0],[0]
"All the parameters are initialized with the range [-0.1, 0.1] of a uniform distribution.",5.1.3. SHARED CONFIGURATIONS,[0],[0]
"We used the AdaDelta algorithm (Zeiler, 2012) with gradient clipping (Pascanu et al., 2013) for optimization.",5.1.3. SHARED CONFIGURATIONS,[0],[0]
We initialized the AdaDelta hyperparameters ρ = 0.95 and = 1−8.,5.1.3. SHARED CONFIGURATIONS,[0],[0]
"Once the loss over the validation set was degraded, we decreased the AdaDelta hyperparameter by multiplying it by 0.01 at each subsequent epoch.",5.1.3. SHARED CONFIGURATIONS,[0],[0]
The training procedure was stopped after 15 epochs.,5.1.3. SHARED CONFIGURATIONS,[0],[0]
"During the training, we adopted multi-condition training strategy, i.e., in addition to the optimization with the enhanced features through the neural beamformers, we also used the noisy multichannel speech data as an input of encoder-decoder networks without through the neural beamformers to improve the robustness of the encoderdecoder networks.",5.1.3. SHARED CONFIGURATIONS,[0],[0]
"All the above networks are implemented by using Chainer (Tokui et al., 2015).",5.1.3. SHARED CONFIGURATIONS,[0],[0]
"Table 1 shows the recognition performances of CHiME4 with the five systems: NOISY, BEAMFORMIT, FILTER NET, MASK NET (REF), and MASK NET (ATT).",5.2. Results,[0],[0]
"NOISY and BEAMFORMIT were the baseline singlechannel end-to-end systems, which did not include the speech enhancement part in their frameworks.",5.2. Results,[0],[0]
"Their endto-end networks were trained only with noisy speech data
by following a conventional multi-condition training strategy (Vincent et al., 2016).",5.2. Results,[0],[0]
"During decoding, NOISY used single-channel noisy speech data from ’isolated 1ch track’ in CHiME-4 as an input, while BEAMFORMIT used the enhanced speech data obtained from 5-channel signals with BeamformIt (Anguera et al., 2007), which is well-known delay-and-sum beamformer, as an input.
FILTER NET, MASK NET (REF), and MASK NET (ATT) were the multichannel end-to-end systems described in Section 4.",5.2. Results,[0],[0]
"To evaluate the validity of the reference selection, we prepared MASK NET (ATT) based on the maskbased beamformer with attention-based reference selection described in Section 3.2.3, and MASK NET (REF) with 5-th channel as a fixed reference microphone, which is located on the center front of the tablet device.
",5.2. Results,[0],[0]
"Table 1 shows that BEAMFORMIT, FILTER NET, MASK NET (REF), and MASK NET (ATT) outperformed NOISY, which confirms the effectiveness of combining speech enhancement with the attention-based encoderdecoder framework.",5.2. Results,[0],[0]
The comparison of MASK NET (REF) and MASK NET (ATT) validates the use of the attention-based mechanism for reference selection.,5.2. Results,[0],[0]
"FILTER NET, which is based on the filter estimation network described in Section 3.1, also improved the performance compared to NOISY, but worse than MASK NET (ATT).",5.2. Results,[0],[0]
"This is because it is difficult to optimize the filter estimation network due to a lack of restriction to estimate filter coefficients, and it needs some careful optimization, as suggested by (Xiao et al., 2016a).",5.2. Results,[0],[0]
"Finally, MASK NET (ATT) achieved better recognition performance than BEAMFORMIT, which proves the effectiveness of our joint integration rather than a pipe-line combination of speech enhancement and (end-to-end) speech recognition.
",5.2. Results,[0],[0]
"To further investigate the effectiveness of our proposed multichannel end-to-end framework, we also conducted the experiment on the AMI corpus.",5.2. Results,[0],[0]
"Table 2 compares the recognition performance of the three systems: NOISY, BEAMFORMIT, and MASK NET (ATT).",5.2. Results,[0],[0]
"In NOISY, we used noisy speech data from the 1st channel in AMI as an input to the system.",5.2. Results,[0],[0]
"Table 2 shows that, even in the AMI, our proposed MASK NET (ATT) achieved bet-
ter recognition performance than the attention-based baselines (NOISY and BEAMFORMIT), which also confirms the effectiveness of our proposed multichannel end-to-end framework.",5.2. Results,[0],[0]
Note that BEAMFORMIT was worse than NOISY even with the enhanced signals.,5.2. Results,[0],[0]
This phenomenon is sometimes observed in noisy speech recognition that the distortion caused by sole speech enhancement degrades the performance without re-training.,5.2. Results,[0],[0]
"Our end-to-end system jointly optimizes the speech enhancement part with the ASR objective, and can avoid such degradations.",5.2. Results,[0],[0]
"As we discussed in Section 3.2, one unique characteristic of our proposed MASK NET (ATT) is the robustness/invariance against the number and order of channels without re-training.",5.3. Influence on the number and order of channels,[0],[0]
Table 3 shows an influence of the CHiME-4 validation accuracies on the number and order of channels.,5.3. Influence on the number and order of channels,[0],[0]
The validation accuracy was computed conditioned on the ground truth labels y∗1,5.3. Influence on the number and order of channels,[0],[0]
:n−1 in Eq,5.3. Influence on the number and order of channels,[0],[0]
.,5.3. Influence on the number and order of channels,[0],[0]
"(10) during decoder’s recursive character generation, which has a strong correlation with CER.",5.3. Influence on the number and order of channels,[0],[0]
"The second column of the table represents the channel indices, which were used as an input of the same MASK NET (ATT) network.
",5.3. Influence on the number and order of channels,[0],[0]
"Comparison of 5 6 4 3 1 and 3 4 1 5 6 shows that the order of channels did not affect the recognition performance of MASK NET (ATT) at all, as we expected.",5.3. Influence on the number and order of channels,[0],[0]
"In addition, even when we used fewer three or four channels as an input, MASK NET (ATT) still outperformed NOISY (single channel).",5.3. Influence on the number and order of channels,[0],[0]
"These results confirm that our proposed multichannel end-to-end system can deal with input signals with arbitrary number and order of channels, without any reconfiguration and re-training.",5.3. Influence on the number and order of channels,[0],[0]
"To analyze the behavior of our developed speech enhancement component with a neural beamformer, Figure 4 visualizes the spectrograms of the same CHiME-4 utterance with the 5-th channel noisy signal, enhanced signal with BeamformIt, and enhanced signal with our proposed MASK NET (ATT).",5.4. Visualization of beamformed features,[0],[0]
"We could confirm that the BeamformIt and MASK NET (ATT) successfully suppressed the noises comparing to the 5-th channel signal by eliminat-
ing blurred red areas overall.",5.4. Visualization of beamformed features,[0],[0]
"In addition, by focusing on the insides of black boxes, the harmonic structure, which was corrupted in the 5-th channel signal, was recovered in BeamformIt and MASK NET (ATT).
",5.4. Visualization of beamformed features,[0],[0]
"This result suggests that our proposed MASK NET (ATT) successfully learned a noise suppression function similar to the conventional beamformer, although it is optimized based on the end-to-end ASR objective, without explicitly using clean data as a target.",5.4. Visualization of beamformed features,[0],[0]
"In this paper, we extended an existing attention-based encoder-decoder framework by integrating a neural beamformer and proposed a multichannel end-to-end speech recognition framework.",6. Conclusions,[0],[0]
"It can jointly optimize the overall inference in multichannel speech recognition (i.e., from speech enhancement to speech recognition) based on the end-to-end ASR objective, and it can generalize to dif-
ferent numbers and configurations of microphones.",6. Conclusions,[0],[0]
"The experimental results on challenging noisy speech recognition benchmarks, CHiME-4 and AMI, show that the proposed framework outperformed the end-to-end baseline with noisy and delay-and-sum beamformed inputs.
",6. Conclusions,[0],[0]
"The current system still has data sparseness issues due to the lack of lexicon and language models, unlike the conventional hybrid approach.",6. Conclusions,[0],[0]
"Therefore, the results reported in the paper did not reach the state-of-the-art performance in these benchmarks, but they are still convincing to show the effectiveness of the proposed framework.",6. Conclusions,[0],[0]
Our most important future work is to overcome these data sparseness issues by developing adaptation techniques of an end-to-end framework with the incorporation of linguistic resources.,6. Conclusions,[0],[0]
The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology.,abstractText,[0],[0]
"Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components.",abstractText,[0],[0]
In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network.,abstractText,[0],[0]
This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective.,abstractText,[0],[0]
Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.,abstractText,[0],[0]
Multichannel End-to-end Speech Recognition ,title,[0],[0]
"In the multilabel classification problem, we are given a set of labeled training data {(xi, yi)}ni=1, where xi ∈",1. Introduction,[0],[0]
"Rp are the input features for each data instances and yi ∈ {0, 1}d
1Department of Computer Science and Engineering, University of Minnesota at Twin Cities, MN USA.",1. Introduction,[0],[0]
"2College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, MA, USA.. Correspondence to:",1. Introduction,[0],[0]
"Shashanka Ubaru <ubaru001@umn.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
are vectors indicating the corresponding labels (classes) the data instances belong to.",1. Introduction,[0],[0]
The vector yi has a one in the jth coordinate if the ith data point belongs to jth class.,1. Introduction,[0],[0]
"We wish to learn a mapping (prediction rule) between the features and the labels, such that, we can predict the class label vector y of a new data point x correctly.",1. Introduction,[0],[0]
"Such multilabel classification problems occur in many domains such as text mining, computer vision, music, and bioinformatics (Barutcuoglu et al., 2006; Trohidis, 2008; Tai & Lin, 2012), and modern applications involve large number of labels.",1. Introduction,[0],[0]
"Popular applications with many labels include image and video annotation (Wang et al., 2009), web page categorization (Agrawal et al., 2013), text and document categorization (Tsoumakas et al., 2008), and others (Bhatia et al., 2015).",1. Introduction,[0],[0]
"In most of these applications, the label vectors yi are sparse (with average sparsity of k d), i.e., each data point belongs to a few (average k out of d) classes.",1. Introduction,[0],[0]
"The multiclass classification is an instance of the multilabel classification, where all data points belong to only one of the d classes (k = 1).
",1. Introduction,[0],[0]
"The simple binary classification problem, where d = 2 and k = 1 is well-studied, and several efficient algorithms have been proposed in the literature.",1. Introduction,[0],[0]
"A natural approach used to solve the multiclass (d > 2, k = 1) classification problem is to reduce the problem into a set of binary classification problem, and then employ the efficient binary classifiers to solve the individual problems.",1. Introduction,[0],[0]
"Popular methods based on this approach are: one-vs-all, all-pairs, and the error-correcting output code (ECOC) (Dietterich & Bakiri, 1995) methods.",1. Introduction,[0],[0]
"In ECOC method, m-dimensional binary vectors (typically codewords from an error correcting code with m ≤ d) are assigned to each class, and m binary classifiers are learned.",1. Introduction,[0],[0]
"For the jth classification, the jth coordinate of the corresponding codeword is used as the binary label for each class.",1. Introduction,[0],[0]
"In the modern applications, where d is typically very large, this approach is found to be very efficient due to the reduction of the class dimension.
",1. Introduction,[0],[0]
The idea of ECOC approach has been extended to the multilabel classification (MLC) problem.,1. Introduction,[0],[0]
"In the multiclass classification, using codewords for each class in ECOC is equivalent to multiplying the code matrix to the label vectors (since the label vectors are basis vectors).",1. Introduction,[0],[0]
"Hence, in the multilabel setting, the reduction from d dimensional label vectors to m dimensional can be achieved by multiply-
ing a code matrix A ∈ Rm×d to the label vector y. This reduction method was analyzed from the compressed sensing point of view in (Hsu et al., 2009), with the assumption of output sparsity, i.e., y is sparse (with average sparsity k).",1. Introduction,[0],[0]
"Using compressed sensing (CS) theory, the results in (Hsu et al., 2009) show that for a linear hypothesis class and under the squared loss, a random embedding (random code matrix) of the classes to m = O(k log d) dimensions does not increase the L2 risk of the classifier.",1. Introduction,[0],[0]
"Similarly, (Kapoor et al., 2012) discusses MLC using compressed sensing in the Bayesian framework.",1. Introduction,[0],[0]
"However, the CS approach requires solving an optimization problem to recover the label vector.",1. Introduction,[0],[0]
"Constructions with faster recovery algorithms exist (see, e.g., (Jafarpour et al., 2009))",1. Introduction,[0],[0]
"but we cannot obtain L2 norm results with them.
",1. Introduction,[0],[0]
"Alternatively, embedding based approaches have been proposed to reduce the effective number of labels.",1. Introduction,[0],[0]
"These methods reduce the label dimension by projecting label vectors onto a low dimensional space, based on the assumption that the label matrix Y =",1. Introduction,[0],[0]
"[y1, . . .",1. Introduction,[0],[0]
", yn] is low-rank.",1. Introduction,[0],[0]
The various embedding methods proposed in the literature mainly differ in the way this reduction is achieved.,1. Introduction,[0],[0]
"The reduction is achieved using SVD in (Tai & Lin, 2012), while column subset selection is used in (Bi & Kwok, 2013).",1. Introduction,[0],[0]
"(Zhang & Schneider, 2011) used canonical correlation analysis and (Chen & Lin, 2012) used an SVD approach that leverages the feature space information.",1. Introduction,[0],[0]
"(Yu et al., 2014) discussed multilabel classification with missing entries and used an embedding method with a regularized least squares objective.",1. Introduction,[0],[0]
"These embedding methods capture the label correlation, and Euclidean distance error guarantees are established.",1. Introduction,[0],[0]
"However, the low rank assumption breaks down in many situations (Bhatia et al., 2015; Xu et al., 2016), e.g., data is power law distributed (Babbar & Schölkopf, 2017).
",1. Introduction,[0],[0]
"The state of the art embedding method called SLEEC (Sparse Local Embedding for Extreme Classification) (Bhatia et al., 2015) overcomes the limitations of previous embedding methods by first clustering the data into smaller regions, and then performs local embeddings of label vectors by preserving distances to nearest label vectors.",1. Introduction,[0],[0]
"However, this method also has many shortcomings, see (Babbar & Schölkopf, 2017).",1. Introduction,[0],[0]
"Moreover, most of these embedding based methods are very expensive.",1. Introduction,[0],[0]
"They involve eigenvalue or singular value decompositions and matrix inversions, and may require solving convex optimization problems, all of which become impractical for very large d.",1. Introduction,[0],[0]
"In all the embedding methods and the CS method, the reduced label space is a real space (no longer binary).",1. Introduction,[0],[0]
Hence we need to use regressors for training and cannot leverage the efficient binary classifiers for effective training for the model.,1. Introduction,[0],[0]
Prediction will also involve rounding/thresholding of real values.,1. Introduction,[0],[0]
"This is additional work, and choosing a right threshold is sometimes problematic.
",1. Introduction,[0],[0]
Proposed Approach.,1. Introduction,[0],[0]
"In this paper, we present a novel reduction approach to solve the MLC problem.",1. Introduction,[0],[0]
"Our approach assumes output sparsity (sparse label vectors with k d) similar to the CS approach, but reduces a large binary label vector to a binary vector of smaller size.",1. Introduction,[0],[0]
"Since the reduced label vectors are binary, we can use the efficient binary classifiers for effective training for the model.",1. Introduction,[0],[0]
Our prediction algorithm is extremely simple and does not involve any matrix inversion or solving optimization algorithm.,1. Introduction,[0],[0]
The prediction algorithm can also detect and correct errors.,1. Introduction,[0],[0]
"Hence, even if a constant fraction of the binary classifiers mis-classify, our prediction error will be zero.
",1. Introduction,[0],[0]
"Our approach is based on the popular group testing problem (Dorfman, 1943; Du & Hwang, 2000).",1. Introduction,[0],[0]
"In the group testing problem, we wish to efficiently identify a small number k of defective elements in a population of large size d.",1. Introduction,[0],[0]
"The idea is to test the items in groups with the premise that most tests will return negative results, clearing the entire group.",1. Introduction,[0],[0]
Only fewm d tests are needed to detect the k defectives.,1. Introduction,[0],[0]
The items can be grouped in either an adaptive or nonadaptive fashion.,1. Introduction,[0],[0]
"In the nonadaptive group testing scheme, the grouping for each test can be described using an m× d binary (0/1 entries) matrix A.
We make the crucial observation that, the MLC problem can be solved using the group testing (GT) premise.",1. Introduction,[0],[0]
"That is, the problem of estimating the (few) classes of a data instance from a large set of classes, is similar to identifying a small set of items from a large set.",1. Introduction,[0],[0]
We consider a group testing binary matrix A and reduce the label vectors yi’s to smaller binary vectors zi using the boolean OR operation zi =,1. Introduction,[0],[0]
A ∨ yi (described later).,1. Introduction,[0],[0]
We can now use binary classifiers on zi for training.,1. Introduction,[0],[0]
The m classifiers learn to test whether the data belongs to a group (of labels) or not.,1. Introduction,[0],[0]
"During prediction, the label vector can be recovered from the predictions of the classifiers using a simple inexpensive algorithm (requiring no matrix inversion or solving optimization algorithms).",1. Introduction,[0],[0]
A low prediction cost is extremely desirable in real time applications.,1. Introduction,[0],[0]
"Depending on a certain property called (k, e)-disjunct property of the group testing matrix A, the recovery algorithm can correct up to be/2c errors in the prediction.",1. Introduction,[0],[0]
"We discuss various constructions for the group testing matrix A which have the desired (k, e)-disjunct property.",1. Introduction,[0],[0]
"We advocate the use of concatenated Reed Solomon codes (Kautz & Singleton, 1964), and unbalanced bipartite expander graphs (Vadhan, 2012) as the group testing matrix A. The optimal number of binary classifiers required for exact recovery (to form a (k, e)-disjunct matrix) will be m = Θ(k2 logk d).",1. Introduction,[0],[0]
"However, we show how this can be reduced to m = O(k log d) if we tolerate a small ε error in the labels recovery.
",1. Introduction,[0],[0]
"The idea of grouping the labels helps overcome the issues most existing methods encounter; e.g., when the data has
power law distribution (Babbar & Schölkopf, 2017), that is many labels have very few training instances (which is the case in most popular datasets), and tail labels (Xu et al., 2016).",1. Introduction,[0],[0]
"Since the classifiers in our approach learn to test for groups of labels, we will have more training instances per group yielding effective classifiers.",1. Introduction,[0],[0]
"It is well known that the one-vs-rest is a highly effective method (expensive), and recently a (doubly) parallelized version of this method called DiSMEC (Babbar & Schölkopf, 2017) was shown to be very effective.",1. Introduction,[0],[0]
"Our approach is similar to one-vs-rest, but the classifiers test for a group of labels, and we require very few classifiers (O(log d) instead of d).",1. Introduction,[0],[0]
"Our approach also resembles the Bloom filter method (Cisse et al., 2013), which is based on using hash functions to reduce the label size.",1. Introduction,[0],[0]
"However, for Bloom filters the lower dimension m can be larger than O(k log d) (no bounds are established) and they may yield many false positives.",1. Introduction,[0],[0]
"For proper encoding this method requires clustering of the labels.
",1. Introduction,[0],[0]
We establish Hamming loss error bounds for the proposed approach.,1. Introduction,[0],[0]
"Due to the error correcting capabilities of the algorithm, even if a fraction of classifiers mis-classify, we can achieve zero prediction error.",1. Introduction,[0],[0]
Numerical experiments with various datasets illustrate the superior performance of our group testing approach with different GT matrices.,1. Introduction,[0],[0]
"Our method is extremely inexpensive compared to the CS approach and especially compared to the embedding based methods, making it very desirable for real time applications too.",1. Introduction,[0],[0]
The results we obtain using the GT approach are more accurate compared to the other popular methods (in terms of Hamming distance).,1. Introduction,[0],[0]
"For many examples, the training errors we obtained were almost zero and the test errors were also quite low.",1. Introduction,[0],[0]
Group testing.,2. Preliminaries,[0],[0]
"Formally, the group testing problem involves identifying an unknown k-sparse binary vector y ∈ {0, 1}d, such that |supp(y)| ≤ k, where supp(y) := {i : yi 6= 0} is called the support of the vector y, by performing a small number of tests (measurements).",2. Preliminaries,[0],[0]
"In the MLC problem, we can view this vector as the sparse label vector y of the data (indicating the k labels).
",2. Preliminaries,[0],[0]
"A nonadaptive group testing scheme with m tests is described by an m × d binary matrix A, where each row corresponds to a test, and Aij = 1 if and only if the ith test includes the jth element.",2. Preliminaries,[0],[0]
The measured vector z is the boolean OR operation between the matrix A and the label vector y.,2. Preliminaries,[0],[0]
The boolean OR operation z,2. Preliminaries,[0],[0]
= A ∨ y can simply be obtained by setting every nonzero entry of the usual matrix-vector product Ay to 1 (and leaving the zero entries as they are).,2. Preliminaries,[0],[0]
"It can also be thought of as coordinate-wise Boolean OR of the columns of A that correspond to the nonzero entries of y.
Definition 1 (Disjunctness).",2. Preliminaries,[0],[0]
An m × d binary matrix,2. Preliminaries,[0],[0]
"A is called k-disjunct if the support of any of its columns is not contained in the union of the supports of any other k columns.
",2. Preliminaries,[0],[0]
"A k-disjunct matrix gives a group testing scheme that identifies any defective set up to size k exactly.
",2. Preliminaries,[0],[0]
Definition 2 (Error Correction).,2. Preliminaries,[0],[0]
An m × d binary matrix,2. Preliminaries,[0],[0]
"A is called (k, e)-disjunct, e ≥ 1, (k-disjunct and e-error detecting) if for every set S of columns of A with |S| ≤ k, and i /∈",2. Preliminaries,[0],[0]
"S, we have |supp(A(i))\ ∪j∈S supp(A(j))|",2. Preliminaries,[0],[0]
>,2. Preliminaries,[0],[0]
"e, where A(i) denote the ith column of A.
A (k, e)-disjunct matrix can detect up to e errors in the measurements and can correct up to be/2c errors.
",2. Preliminaries,[0],[0]
"Several random and deterministic construction of kdisjunct matrices have been proposed in the literature (Kautz & Singleton, 1964; Du & Hwang, 2000).",2. Preliminaries,[0],[0]
"Matrices from error correcting codes and expander graphs have also been designed (Dyachkov et al., 2000; Ubaru et al., 2016; Cheraghchi, 2010; Mazumdar, 2016).",2. Preliminaries,[0],[0]
"In this section, we present our main idea of adapting the group testing scheme to the multilabel classification problem (MLGT).
Training.",3. MLC via Group testing,[0],[0]
"Suppose we are given n training instances {(xi, yi)}ni=1, where xi ∈",3. MLC via Group testing,[0],[0]
"Rp are the input features for each instances and yi ∈ {0, 1}d are corresponding label vectors.",3. MLC via Group testing,[0],[0]
We begin by assuming that each data instance belongs to at most k classes (the label vector y is k sparse).,3. MLC via Group testing,[0],[0]
"We consider a (k, e)-disjunct matrix A ∈ Rm×d.",3. MLC via Group testing,[0],[0]
"We then compute the reduced measured (label) vectors zi for each label vectors yi, i = 1, . . .",3. MLC via Group testing,[0],[0]
", n using the boolean OR operation zi =",3. MLC via Group testing,[0],[0]
A ∨ yi.,3. MLC via Group testing,[0],[0]
"We can now train m binary classifiers {wj}mj=1 based on {xi, zi}ni=1 with jth entry of zi indicating which class (1/0) the ith instance belongs to for the jth classifier.",3. MLC via Group testing,[0],[0]
"Algorithm 1 summarizes our training algorithm.
",3. MLC via Group testing,[0],[0]
Algorithm 1 MLGT:,3. MLC via Group testing,[0],[0]
"Training Algorithm Input: Training data {(xi, yi)}ni=1, group testing matrix A ∈ Rm×d, a binary classifier algorithm C. Output: m classifiers {wj}mj=1. for i = 1, . . .",3. MLC via Group testing,[0],[0]
", n. do zi =",3. MLC via Group testing,[0],[0]
"A ∨ yi.
end for for j = 1, . . .",3. MLC via Group testing,[0],[0]
",m. do wj = C({(xi, zij)}ni=1).",3. MLC via Group testing,[0],[0]
"end for
Prediction.",3. MLC via Group testing,[0],[0]
"In the prediction stage, given a test data x ∈",3. MLC via Group testing,[0],[0]
"Rp, we use the m classifiers {wj}mj=1 to obtain a predicted
reduced label vector ẑ. We know that a k sparse label vector can be recovered exactly, if the group testing matrix A is a k-disjunct matrix.",3. MLC via Group testing,[0],[0]
"With a (k, e)-disjunct matrix, e ≥ 1, we can recover the k sparse label vector exactly, even if be/2c binary classifiers mis-classify, using the following decoder.
",3. MLC via Group testing,[0],[0]
"Decoder : Given a predicted reduced label vector ẑ, and a group testing matrix A, set the coordinate position of ŷ corresponding to l ∈",3. MLC via Group testing,[0],[0]
"[1, . . .",3. MLC via Group testing,[0],[0]
", d] to 1 if and only if |supp(A(l))\supp(ẑ)| < e/2.
",3. MLC via Group testing,[0],[0]
"That is, we set the lth coordinate of ŷ to 1, if the number of coordinates that are in the support of the corresponding column A(l) but are not in the predicted reduced vector ẑ, is less than e/2.",3. MLC via Group testing,[0],[0]
The decoder returns the exact label vector even if up to e/2 binary classifiers make errors.,3. MLC via Group testing,[0],[0]
"Algorithm 2 summarizes our prediction algorithm.
",3. MLC via Group testing,[0],[0]
Algorithm 2 MLGT:,3. MLC via Group testing,[0],[0]
Prediction Algorithm Input: Test data x ∈,3. MLC via Group testing,[0],[0]
"Rp, the GT matrix A ∈ Rm×d which is (k, e)-disjunct (e ≥ 1), m classifiers {wj}mj=1.",3. MLC via Group testing,[0],[0]
Output: predicted label ŷ. Compute,3. MLC via Group testing,[0],[0]
ẑ =,3. MLC via Group testing,[0],[0]
"[w1(x), . .",3. MLC via Group testing,[0],[0]
.,3. MLC via Group testing,[0],[0]
", wm(x)].",3. MLC via Group testing,[0],[0]
Set ŷ,3. MLC via Group testing,[0],[0]
← 0.,3. MLC via Group testing,[0],[0]
"for l = 1, . . .",3. MLC via Group testing,[0],[0]
", d do
if |supp(A(l))\supp(ẑ)| < e/2 then ŷl = 1.
end if end for
Note that the prediction algorithm is very inexpensive (requires no matrix inversion or solving optimization).",3. MLC via Group testing,[0],[0]
"It is equivalent to an AND operation between a binary sparse matrix and a binary (likely sparse) vector, which should cost less than a sparse matrix vector productO(nnz(A))",3. MLC via Group testing,[0],[0]
"≈ O(kd), where nnz(A) is the number of nonzero entries of A.",3. MLC via Group testing,[0],[0]
It is an interesting future work to design an even faster prediction algorithm.,3. MLC via Group testing,[0],[0]
"In order to recover a k sparse label vector exactly, we know that the group testing matrixAmust be a k-disjunct matrix.",4. Constructions,[0],[0]
"With a (k, e)-disjunct matrix, our algorithm can extract the sparse label vector exactly even if e/2 binary classifiers make errors (mis-classify).",4. Constructions,[0],[0]
"Here, we present the results that will help us construct specific GT matrices with the desired properties.",4. Constructions,[0],[0]
Proposition 1 (Random Construction).,4.1. Random Constructions,[0],[0]
"An m× d random binary {0, 1} matrix A where each entry is 1 with probability ρ = 1k+1 , is (k, 3k log d)-disjunct with very high probability, if m = O(k2 log d).
",4.1. Random Constructions,[0],[0]
"If we tolerate a small ε fraction of sparsity label misclassifications (i.e., εk errors in the recovered label vector), which we call ε-tolerance group testing, then we can follow the analysis of Theorem 8.1.1 in (Du & Hwang, 2000), to show that it is sufficient to have m = O(k log d) number of classifiers.",4.1. Random Constructions,[0],[0]
"Further, we can derive the following result.
",4.1. Random Constructions,[0],[0]
Theorem 1.,4.1. Random Constructions,[0],[0]
Suppose we wish to recover a k sparse binary vector y ∈ Rd.,4.1. Random Constructions,[0],[0]
"A random binary {0, 1} matrix A where each entry is 1 with probability ρ = 1/k recovers 1 − ε proportion of the support of y correctly with high probability, for any ε > 0, with m = O(k log d).",4.1. Random Constructions,[0],[0]
"This matrix will also detect e = Ω(m) errors.
",4.1. Random Constructions,[0],[0]
The proofs of the proposition and the theorem can be found in the supplementary.,4.1. Random Constructions,[0],[0]
"Kautz and Singleton (Kautz & Singleton, 1964) introduced a two-level construction in which a q-ary ( q > 2) ReedSolomon (RS) code is concatenated with a unit-weight binary code.",4.2. Concatenated code based constructions,[0],[0]
"The construction starts with a q-ary ( q > 2) RS code of length q − 1, and replaces the q-ary symbols in the codewords by unit weight binary vectors of length q. That is, the q-ary symbols are replaced as 0 → 100 . . .",4.2. Concatenated code based constructions,[0],[0]
0,4.2. Concatenated code based constructions,[0],[0]
; 1 → 010 . .,4.2. Concatenated code based constructions,[0],[0]
.,4.2. Concatenated code based constructions,[0],[0]
0;,4.2. Concatenated code based constructions,[0],[0]
q − 1 → 0 . . .,4.2. Concatenated code based constructions,[0],[0]
01.,4.2. Concatenated code based constructions,[0],[0]
This gives us a binary matrix with m = q(q − 1) rows.,4.2. Concatenated code based constructions,[0],[0]
This matrix belongs to a broad class of error correcting codes called the constant weight codes (each codeword/column has a constant number of ones w).,4.2. Concatenated code based constructions,[0],[0]
"For this Kautz-Singleton construction, w = q−1.",4.2. Concatenated code based constructions,[0],[0]
Proposition 2 (Kautz-Singleton construction).,4.2. Concatenated code based constructions,[0],[0]
"A KautzSingleton construction with (k logk d)-ary Reed-Solomon (RS) code is a (k, (k − 1) logk d)-disjunct matrix with m = Θ(k2 log2k d).
",4.2. Concatenated code based constructions,[0],[0]
Proof.,4.2. Concatenated code based constructions,[0],[0]
A constant weight code matrix is k disjunct matrix with k = b,4.2. Concatenated code based constructions,[0],[0]
"w−1w−h/2c, where w is the weight and h is the distance of the code, (see, Theorem 7.3.3 in (Du & Hwang, 2000)).",4.2. Concatenated code based constructions,[0],[0]
The distance of the q-ary RS code is h = 2(q − logq(d)).,4.2. Concatenated code based constructions,[0],[0]
"Hence, we get k = q−2 logq d−1
.",4.2. Concatenated code based constructions,[0],[0]
"So, for a kdisjunct matrix, we choose q = k logk d. A code with distance h will have e = h/2 (by using Corollary 8.3.2 in (Du & Hwang, 2000)).",4.2. Concatenated code based constructions,[0],[0]
"Thus, e = q −",4.2. Concatenated code based constructions,[0],[0]
logq d ≈ (k − 1) logk d. m = q(q,4.2. Concatenated code based constructions,[0],[0]
− 1) = Θ(k2 log2k,4.2. Concatenated code based constructions,[0],[0]
"d).
",4.2. Concatenated code based constructions,[0],[0]
Other code based constructions are given in supplementary.,4.2. Concatenated code based constructions,[0],[0]
"Expander graphs have popularly been used in many applications, for example, in coding theory (Sipser & Spielman, 1996), in compressed sensing (Jafarpour et al., 2009), etc.",4.3. Expander graphs,[0],[0]
"In an expander graph, every small set of vertices “expands”: the are “sparse” yet very “well-connected” (see
formal definition below).",4.3. Expander graphs,[0],[0]
With high probability a random graph is a good expander.,4.3. Expander graphs,[0],[0]
"Construction of “lossless” expanders have been notoriously difficult.
",4.3. Expander graphs,[0],[0]
Definition 3 (Unbalanced Lossless Expander Graphs).,4.3. Expander graphs,[0],[0]
"A (k, )-unbalanced bipartite expander graph is a bipartite graph G(L,R,E), |L| = d, |R| = m, where L is the set of left nodes and R is the set of right nodes, with regular left degree ` such that for any S ⊂ L, if |S| ≤ k then the set of neighbors N(S) of S has the size N(S) >",4.3. Expander graphs,[0],[0]
"`|S|.
",4.3. Expander graphs,[0],[0]
"The following proposition describe the expander property of random graphs.
",4.3. Expander graphs,[0],[0]
Proposition 3.,4.3. Expander graphs,[0],[0]
"A random construction of bipartite graphs G(L,R,E) with |L| = d with overwhelming probability, is (k, )-lossless",4.3. Expander graphs,[0],[0]
"`-regular expander where ` = O(log d/ ) with |R| = m = O(k`/ ).
",4.3. Expander graphs,[0],[0]
The trade-off of this proposition is close to the best we can hope for.,4.3. Expander graphs,[0],[0]
"The proof can be shown by simple random choice and can be found in (Vadhan, 2012) or in (Cheraghchi, 2010).
",4.3. Expander graphs,[0],[0]
"The next definition and the subsequent two claims are from (Cheraghchi, 2010).",4.3. Expander graphs,[0],[0]
"First, let us now connect a lossless expander with disjunct matrix.
",4.3. Expander graphs,[0],[0]
Definition 4.,4.3. Expander graphs,[0],[0]
"A bipartite graph G(L,R,E) is called (k, e)-disjunct if, for every left vertex i ∈ L and every set S ⊆ L such that |S| ≤ k",4.3. Expander graphs,[0],[0]
and i /∈,4.3. Expander graphs,[0],[0]
"S, we have |N(i)\N(S)| > e.
It can be seen that the bipartite adjacency matrix A of a disjunct graph G is a disjunct matrix.
",4.3. Expander graphs,[0],[0]
Proposition 4.,4.3. Expander graphs,[0],[0]
"Let G be a (k, e)-disjunct graph with adjacency matrix A. Then for every pair of y, y′ ∈ {0, 1}d of k -sparse vectors, we have ∆(A∨y,A∨y′) >",4.3. Expander graphs,[0],[0]
"e, where ∆(·) denotes the Hamming distance between vectors.
",4.3. Expander graphs,[0],[0]
"The following proposition relates expander graphs with disjunct graphs.
",4.3. Expander graphs,[0],[0]
Proposition 5.,4.3. Expander graphs,[0],[0]
"Let G be a `-regular (k, )-lossless expander.",4.3. Expander graphs,[0],[0]
"Then, for every α ∈",4.3. Expander graphs,[0],[0]
"[0, 1),G is (k−1, α`)-disjunct provided that < 1−α` .
",4.3. Expander graphs,[0],[0]
"Combining these comments, we get the following:
Proposition 6 (Random Graphs).",4.3. Expander graphs,[0],[0]
"The adjacency matrix of a randomly constructed bipartite graph is, with overwhelming probability, k-disjunct with m = O(k2 log(d/k)).",4.3. Expander graphs,[0],[0]
"More generally, for every α ∈",4.3. Expander graphs,[0],[0]
"[0, 1), random graphs are (k, e)-disjunct, with e = Ω(αk log d/(1− α2))",4.3. Expander graphs,[0],[0]
"with m = Ω(αk2 log(d/k)/(1− α2)).
",4.3. Expander graphs,[0],[0]
"There is an explicit construction of unbalanced (k, )- lossless expanders for any setting of d and m and is, to our knowledge, the best possible, in (Capalbo et al., 2002).
",4.3. Expander graphs,[0],[0]
These constructions yield explicit k-disjunct graphs with m = O(k2quasipoly(log d)).,4.3. Expander graphs,[0],[0]
"Other random constructions are discussed in the supplementary.
",4.3. Expander graphs,[0],[0]
"With all the above constructions, we can correct a reasonably large number of e errors by the binary classifiers.",4.3. Expander graphs,[0],[0]
The number of classifiers required for MLGT will be m = O(k2 log d) which is more than the CS approach where m = O(k log d).,4.3. Expander graphs,[0],[0]
"However, our analysis is for the worst case: as we saw in Theorem 1, if we tolerate a small ε fraction of error in recovery, we can achieve m = O(k log d) for MLGT as well.",4.3. Expander graphs,[0],[0]
"Moreover, MLGT yields zero prediction error for a k sparse label vector even if up to e/2 classifiers mis-classify.",4.3. Expander graphs,[0],[0]
"With MLCS, we only get an error guarantees and with respect to 2-norm (not Hamming distance which is more natural for classification).",4.3. Expander graphs,[0],[0]
"Here we summarize the theoretical error guarantees for multilabel classification using group testing (MLGT).
",5. Error Analysis,[0],[0]
Theorem 2.,5. Error Analysis,[0],[0]
"Consider MLGT with an m× d binary matrix A, and a label vector y with sparsity at most k.",5. Error Analysis,[0],[0]
Suppose,5. Error Analysis,[0],[0]
"A is (k, e)-disjunct, and we use Algorithm 2 during prediction.",5. Error Analysis,[0],[0]
Let ŷ be the predicted label vector and ∆(·) denote the Hamming distance between vectors.,5. Error Analysis,[0],[0]
"If t number of binary classifiers that make errors in prediction, then we have
• If t ≤ be/2c, then the prediction error ∆(y, ŷ) = 0.",5. Error Analysis,[0],[0]
•,5. Error Analysis,[0],[0]
"If t > be/2c, ∆(y, ŷ) ≤ w(t−e/2) (Hamming error),
where w is the maximum weight of rows in A.",5. Error Analysis,[0],[0]
"In particular, the error rate (average error per class) will be w d (t− e/2).
",5. Error Analysis,[0],[0]
"If A is a k-disjunct with ε error tolerance, then the prediction error will be at most (w(t− e/2) + εk).
",5. Error Analysis,[0],[0]
Proof.,5. Error Analysis,[0],[0]
"When, t ≤ e/2, we know that the decoding algorithm will still recover the exact label vector due to the error correcting property of the (k, e)-disjunct matrix.",5. Error Analysis,[0],[0]
"When, t > e/2, e/2 of the errors are corrected.",5. Error Analysis,[0],[0]
"For every remaining t − e/2 errors, if w is the maximum weight of rows in A, a maximum of w errors can occur in the predicted label.",5. Error Analysis,[0],[0]
"This is because, the support different |A(l)\ẑ| can change for a maximum of w columns.",5. Error Analysis,[0],[0]
"Hence, the error can be at most w(t − e/2), and the error rate will be wd (t − e/2).",5. Error Analysis,[0],[0]
"For the k-disjunct matrix with ε error tolerance, the decoding algorithm can make up to εk errors in addition to w(t− e/2).
",5. Error Analysis,[0],[0]
Let us see how the error-rate of various group testing constructions translate to MLGT.,5. Error Analysis,[0],[0]
"In the case of a random matrix construction, we have w ≈ d/k.",5. Error Analysis,[0],[0]
"So, the error rate for this matrix will be (t−e/2)/k.",5. Error Analysis,[0],[0]
"From proposition 1, we can take m = k2 log d, and e = 3k log d. Hence, the error rate
for a random (k, e) disjunct matrix will be t/k− 3/2 log d, for any t > 3/2k log d. For any t less than this the error rate will be zero.",5. Error Analysis,[0],[0]
"Similarly, we can see that the randomized construction of Thm. 1 with m = O(k log d) rows, gives the average error rate is (t/k−O(log d) + εk/d) for t > k log d.",5. Error Analysis,[0],[0]
"The error rates of other constructions can be calculated in the same way, see supplementary.
",5. Error Analysis,[0],[0]
The above theorem also shows that a Hamming error regret R (R ≡ |error in the method - least error possible|) in the binary classifiers will transform linearly to the overall regret of at most w(R − e/2) for the MLGT.,5. Error Analysis,[0],[0]
"For the CS approach, the results in (Hsu et al., 2009) show that a L2 regret R2 (an L2-norm error regret) in the regressor will translate as √ R2 to the overall regret (which is worse).",5. Error Analysis,[0],[0]
"This is because, a CS based analysis involves, L2-norm errors and Restricted Isometric Property (RIP) of the compression matrix with respect to L2 norm.",5. Error Analysis,[0],[0]
"However, L2 error metric is never used for evaluation in practice.
",5. Error Analysis,[0],[0]
"Hamming Loss: Since we operate in the binary field, we derive error (regret) bounds, as well as present experimental results with respect to Hamming loss.",5. Error Analysis,[0],[0]
"In certain applications we may be interested in only predicting the top k1 < k labels correctly, e.g., tagging and recommendation.",5. Error Analysis,[0],[0]
"In such situations, it has been argued that the Hamming loss is not a perfect measure (Jain et al., 2016), and alternate measures such as Precison@k (defined later) for k = 1, 3, 5 have been used (Agrawal et al., 2013).",5. Error Analysis,[0],[0]
"However, these measures assume there is a ranking amongst the labels, which can be obtained only when operating in the real space.",5. Error Analysis,[0],[0]
"Also, these measures ignore the false labels.",5. Error Analysis,[0],[0]
Our approach considers labels as just binary vectors (cannot rank) and attempts to predict all labels correctly (hence Hamming loss).,5. Error Analysis,[0],[0]
Almost all available mutlilabel datasets have binary label vectors and do not come with the priority information of labels within the large output classes.,5. Error Analysis,[0],[0]
"There are recent works which try to rank the labels first and then classify, e.g. (Jain et al., 2016; Chzhen et al., 2017).",5. Error Analysis,[0],[0]
"Hamming loss is nonetheless an interesting error metric for applications where we need to predict all labels correctly and require few/no false labels, hence is worth analyzing.",5. Error Analysis,[0],[0]
"Most of the recent popular (embedding) methods tend to give good results with respect to Precison@k, but give poor Hamming errors due to large number of false labels.",5. Error Analysis,[0],[0]
Our approach gives very low Hamming loss both theoretically and practically.,5. Error Analysis,[0],[0]
"In this section, we illustrate the performance of the proposed group testing approach in the multilabel classification problems (MLGT) via several numerical experiments on various datasets.
",6. Numerical Experiments,[0],[0]
Datasets: We use some popular publicly available multilabel datasets in our experiments.,6. Numerical Experiments,[0],[0]
"All datasets were obtained from The Extreme Classification Repository1 (Bhatia et al., 2015).",6. Numerical Experiments,[0],[0]
Details about the datasets and the references for their original sources can be found in the repository.,6. Numerical Experiments,[0],[0]
"Table 1 in the supplementary gives data details.
",6. Numerical Experiments,[0],[0]
"Constructions: For MLGT, we consider three different group testing constructions.",6. Numerical Experiments,[0],[0]
"The Kautz-Singleton construction with q-ary Reed-Solomon (RS) codes, where we use RS codes (MacWilliams & Sloane, 1977) with q = 16 and m = 240; and q = 8 and m = 56.",6. Numerical Experiments,[0],[0]
"To get desired number of codewords (equal to number of labels), we use appropriate message length.",6. Numerical Experiments,[0],[0]
"For example, if d ≤ 4096, q = 16, then we use message length of 3, and if d ≤ 65536, we use message length of 5.",6. Numerical Experiments,[0],[0]
"We also use two random GT constructions, namely, the random expander graphs and the sparse random constructions discussed in sec. 4.",6. Numerical Experiments,[0],[0]
"For MLCS (compressed sensing approach), we again consider three different types compression matrices, namely, random Gaussian matrices, compressed Hadamard matrices and random expander graphs (expander graphs have been used for CS too (Jafarpour et al., 2009)).
",6. Numerical Experiments,[0],[0]
Evaluation metrics: Two evaluation metrics are used to analyze the performances of the different methods.,6. Numerical Experiments,[0],[0]
"First is the Hamming loss error, the Hamming distance between the predicted vector ŷ and the actual label vector y, ∆(y, ŷ).",6. Numerical Experiments,[0],[0]
"This metric tells us how close is the recovered vector ŷ is from the exact label vector y, and is more suitable for binary vectors.",6. Numerical Experiments,[0],[0]
Hamming loss captures the information of both correct predictions and false labels.,6. Numerical Experiments,[0],[0]
All prediction errors reported (training and test) are Hamming loss errors.,6. Numerical Experiments,[0],[0]
"The second metric used is Precison@k (P@k), which is a popular metric used in MLC literature (Agrawal et al., 2013).",6. Numerical Experiments,[0],[0]
This measures the precision of predicting the first k coordinates |supp(ŷ1:k) ∩ supp(y)|/k.,6. Numerical Experiments,[0],[0]
"Since we cannot score the labels, we use k = nnz(y) the output sparsity of the true label for this measure.",6. Numerical Experiments,[0],[0]
This is equivalent to checking whether the method predicted all the labels the data belongs to correctly or not (ignoring misclassification).,6. Numerical Experiments,[0],[0]
"When Precision@k for k = 1, 3, 5 are used, one is checking whether the top 1, 3 or 5 labels are predicted correctly (ignoring other and false labels).
",6. Numerical Experiments,[0],[0]
MLGT vs MLCS:,6. Numerical Experiments,[0],[0]
"In the first set of experiments, we compare the performances of the group testing approach (MLGT) and the compressed sensing approach (MLCS) using different group testing constructions and different compression matrices.",6. Numerical Experiments,[0],[0]
A least squares binary classifier was used {wj}mj=1 for MLGT.,6. Numerical Experiments,[0],[0]
Least squares regression with `2 regularization (ridge regression) is used as the regressors for MLCS and other embedding based methods.,6. Numerical Experiments,[0],[0]
"Orthogo-
1https://manikvarma.github.io/downloads/ XC/XMLRepository.html
nal Matching Pursuit (OMP) (Tropp & Gilbert, 2007) was used for sparse recovery in MLCS.",6. Numerical Experiments,[0],[0]
"Additional details are given in supplementary.
",6. Numerical Experiments,[0],[0]
Table 1 compares the performances of MLCS and MLGT for different CS and GT matrices on different datasets.,6. Numerical Experiments,[0],[0]
The average training and the test errors (Hamming losses) are reported along with the average Precison@k obtained for training and test data.,6. Numerical Experiments,[0],[0]
The methods and the number of classifiers/regressors m used are also listed.,6. Numerical Experiments,[0],[0]
"For example, GT:RS code q=16, implies MLGT was the method with the RS code construction with q=16.",6. Numerical Experiments,[0],[0]
"The number of training points n, test points nt and the number of features p used in the experiments are reported next to the datasets.",6. Numerical Experiments,[0],[0]
"kmax means the maximum sparsity in the data (only those data points below this sparsity were used), anf bark is the average sparsity.",6. Numerical Experiments,[0],[0]
"For the latter three datasets, the feature space was reduced to select only dominant features (data are very sparse and only few features are prominent).
",6. Numerical Experiments,[0],[0]
"We observe that, the MLGT method with all the three GT constructions outperforms the MLCS method.",6. Numerical Experiments,[0],[0]
"In most cases, the training errors (almost zero) and Precison@k (almost one) for MLGT methods are extremely good.",6. Numerical Experiments,[0],[0]
"This is because, the binary classifiers are optimally trained on the reduced binary vectors and since the matrices used
were k-disjunct, we had zero recovery error in most cases.",6. Numerical Experiments,[0],[0]
"Hence, the predicted labels for training data were extremely accurate.",6. Numerical Experiments,[0],[0]
The results on test data are also better for MLGT in almost all cases.,6. Numerical Experiments,[0],[0]
"The results we obtained for the dataset Delicious were consistently poor, see supplementary.",6. Numerical Experiments,[0],[0]
"We also observed that MLGT is significantly faster than MLCS (as expected) because, MLCS uses an optimization algorithm (OMP) for recovery of labels, see Table 3 for runtimes.
",6. Numerical Experiments,[0],[0]
"In the prediction algorithm of MLGT, we have a parameter e, the number of errors the algorithm should try to correct.",6. Numerical Experiments,[0],[0]
"The ideal value for e will depend on the GT matrix used, the values ofm, k and d. However, note that we can test for different values of e at no additional cost.",6. Numerical Experiments,[0],[0]
"That is, once we compute the Boolean AND between the predicted reduced vector and the GT matrix (the dominant operation), we can get different prediction vectors for a range of e and choose an e that gives the highest training P@k.
",6. Numerical Experiments,[0],[0]
"Figure 1 plots the average training and test errors and average Precison@k against the sparsity k of the label vectors (data with label sparsity k used) obtained for MLGT and MLCS methods with the three different matrices respectively, seen in Table 1.",6. Numerical Experiments,[0],[0]
The dataset used was RCV1-2K. This dataset has at least 2000 training points and 500 testing points for each label sparsity ranging from 1 to 10.,6. Numerical Experiments,[0],[0]
We observe that the training error for MLGT methods are almost zero and training Precison@k almost one.,6. Numerical Experiments,[0],[0]
(This behavior was seen in Table 1 as well).,6. Numerical Experiments,[0],[0]
"Results with test data
for MLGT are also impressive, achieving Precison@k of almost 0.8 for small k.
One vs all: We next compare MLGT against the one versus all (OvsA) method on two small datasets.",6. Numerical Experiments,[0],[0]
"Note that OvsA required d classifiers to be trained, hence is impractical for larger datasets, and we will need a distributed implementation such as DiSMEC (Babbar & Schölkopf, 2017).",6. Numerical Experiments,[0],[0]
Table 2 gives the results for MLGT and OvsA methods for two small datasets. n,6. Numerical Experiments,[0],[0]
"= 5000,nt = 1000, and for MLGT m = 50.",6. Numerical Experiments,[0],[0]
The table lists the Hamming test errors and P@k for the two methods.,6. Numerical Experiments,[0],[0]
The table also gives the overall runtimes for the two methods.,6. Numerical Experiments,[0],[0]
"We note that wrt. to both metrics, MLGT performs better than OvsA. This is due to two reasons.",6. Numerical Experiments,[0],[0]
"First, MLGT groups the labels hence has more training samples per group, yielding better classifiers.",6. Numerical Experiments,[0],[0]
"Second, the error correction by the prediction algorithm corrects few classification errors.",6. Numerical Experiments,[0],[0]
"Clearly, MLGT is faster than OvsA. However, OvsA gives better training errors.
",6. Numerical Experiments,[0],[0]
"Embedding methods: In the next set of experiments, we compare the performance of MLGT against the popular embedding based methods.",6. Numerical Experiments,[0],[0]
We compare with the following methods.,6. Numerical Experiments,[0],[0]
"ML-CSSP, is an embedding method based on column subset selection (Bi & Kwok, 2013).",6. Numerical Experiments,[0],[0]
"PLST, is Principal Label Space Transformation (Tai & Lin, 2012), an embedding method based on SVD (code is made available online by the authors).",6. Numerical Experiments,[0],[0]
"SLEEC, Sparse Local Embeddings for Extreme Classification (Bhatia et al., 2015), is the state of the art embedding method based on clustering using nearest neighbors and then embedding in the cluster space (code is made available online by the authors).",6. Numerical Experiments,[0],[0]
"For MLGT, we use the random expander graph constructions.",6. Numerical Experiments,[0],[0]
"For MLCS, we use random Gaussian matrices.",6. Numerical Experiments,[0],[0]
"Same least squares regressor was used in all the latter four methods.
",6. Numerical Experiments,[0],[0]
Table 3 lists the test (Hamming) errors obtained for the different methods on various datasets.,6. Numerical Experiments,[0],[0]
We use smaller datasets since the embedding based methods do not scale well for large datasets.,6. Numerical Experiments,[0],[0]
We also used only 2000 training points and 500 test points in each cases.,6. Numerical Experiments,[0],[0]
We observe that MLGT outperforms the other methods in most cases.,6. Numerical Experiments,[0],[0]
"The datasets have very sparse label (avg. sparsity of around k̄ ≈ 4), but the outputs of MLCSSP and PLST are not very sparse.",6. Numerical Experiments,[0],[0]
"Hence, we see high Hamming error for these two methods, since they yield a lot of false labels.",6. Numerical Experiments,[0],[0]
"Moreover, these embedding methods are significantly more expensive than MLGT for larger datasets.",6. Numerical Experiments,[0],[0]
"The runtimes for each method are also listed in the table.
",6. Numerical Experiments,[0],[0]
"The runtimes reported (using cputime in Matlab) includes generation of compression matrices, multiplying the matrix to the label vectors (boolean OR/SVD computation), training the m classifiers, and prediction of n training and nt test points.",6. Numerical Experiments,[0],[0]
"SLEEC performs reasonably well on all datasets (the ideal parameters to be set in this algorithm for each of these datasets were provided by the authors online), and gives better P@k than MLGT for some datasets.",6. Numerical Experiments,[0],[0]
"For Delicious dataset, the value of k is high and SLEEC beats MLGT.",6. Numerical Experiments,[0],[0]
"However, SLEEC algorithm has many parameters to set, and for larger datasets, the algorithm is very expensive compared to all other methods.
",6. Numerical Experiments,[0],[0]
These experiments illustrate that MLGT performs exceptionally well in practice.,6. Numerical Experiments,[0],[0]
The concatenated RS codes and the bipartite expander graphs constructions proposed are simple to generate and they exist for large sizes.,6. Numerical Experiments,[0],[0]
"Hence, these constructions can be easy applied to extreme classification problems.",6. Numerical Experiments,[0],[0]
Authors would like to thank Dr. Manik Varma and his team for making many MLC datasets and codes available online.,Acknowledgements,[0],[0]
"This work was supported by NSF under grant NSF/CCF1318597, NSF/CCF-1318093, NSF/CCF 1642550.",Acknowledgements,[0],[0]
"In recent years, the multiclass and mutlilabel classification problems we encounter in many applications have very large (10 − 10) number of classes.",abstractText,[0],[0]
"However, each instance belongs to only one or few classes, i.e., the label vectors are sparse.",abstractText,[0],[0]
"In this work, we propose a novel approach based on group testing to solve such large multilabel classification problems with sparse label vectors.",abstractText,[0],[0]
"We describe various group testing constructions, and advocate the use of concatenated Reed Solomon codes and unbalanced bipartite expander graphs for extreme classification problems.",abstractText,[0],[0]
The proposed approach has several advantages theoretically and practically over existing popular methods.,abstractText,[0],[0]
Our method operates on the binary alphabet and can utilize the well-established binary classifiers for learning.,abstractText,[0],[0]
The error correction capabilities of the codes are leveraged for the first time in the learning problem to correct prediction errors.,abstractText,[0],[0]
"Even if a linearly growing number of classifiers mis-classify, these errors are fully corrected.",abstractText,[0],[0]
We establish Hamming loss error bounds for the approach.,abstractText,[0],[0]
"More importantly, our method utilizes a simple prediction algorithm and does not require matrix inversion or solving optimization problems making the algorithm very inexpensive.",abstractText,[0],[0]
Numerical experiments with various datasets illustrate the superior performance of our method.,abstractText,[0],[0]
Multilabel Classification with Group Testing and Codes,title,[0],[0]
"In numerous applications in engineering and sciences, data are often organized in a multilevel structure.",1. Introduction,[0],[0]
"For instance, a typical structural view of text data in machine learning is to have words grouped into documents, documents are grouped into corpora.",1. Introduction,[0],[0]
A prominent strand of modeling and algorithmic works in the past couple decades has been to discover latent multilevel structures from these hierarchically structured data.,1. Introduction,[0],[0]
"For specific clustering tasks, one may be interested in simultaneously partitioning the data in each group (to obtain local clusters) and partitioning a collection of data groups (to obtain global clusters).",1. Introduction,[0],[0]
"Another concrete example is the problem of clustering images (i.e., global clusters) where each image contains partions of multiple annotated regions (i.e., local clusters) (Oliva and Torralba,
1Department of Statistics, University of Michigan, USA.",1. Introduction,[0],[0]
2Adobe Research.,1. Introduction,[0],[0]
"3Center for Pattern Recognition and Data Analytics (PRaDA), Deakin University, Australia.",1. Introduction,[0],[0]
"Correspondence to: Nhat Ho <minhnhat@umich.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"1Code is available at https://github.com/ moonfolk/Multilevel-Wasserstein-Means
2001).",1. Introduction,[0],[0]
"While hierachical clustering techniques may be employed to find a tree-structed clustering given a collection of data points, they are not applicable to discovering the nested structure of multilevel data.",1. Introduction,[0],[0]
"Bayesian hierarchical models provide a powerful approach, exemplified by influential works such as (Blei et al., 2003; Pritchard et al., 2000; Teh et al., 2006).",1. Introduction,[0],[0]
"More specific to the simultaneous and multilevel clustering problem, we mention the paper of (Rodriguez et al., 2008).",1. Introduction,[0],[0]
"In this interesting work, a Bayesian nonparametric model, namely the nested Dirichlet process (NDP) model, was introduced that enables the inference of clustering of a collection of probability distributions from which different groups of data are drawn.",1. Introduction,[0],[0]
"With suitable extensions, this modeling framework has been further developed for simultaneous multilevel clustering, see for instance, (Wulsin et al., 2016; Nguyen et al., 2014; Huynh et al., 2016).
",1. Introduction,[0],[0]
"The focus of this paper is on the multilevel clustering problem motivated in the aforementioned modeling works, but we shall take a purely optimization approach.",1. Introduction,[0],[0]
We aim to formulate optimization problems that enable the discovery of multilevel clustering structures hidden in grouped data.,1. Introduction,[0],[0]
Our technical approach is inspired by the role of optimal transport distances in hierarchical modeling and clustering problems.,1. Introduction,[0],[0]
"The optimal transport distances, also known as Wasserstein distances (Villani, 2003), have been shown to be the natural distance metric for the convergence theory of latent mixing measures arising in both mixture models (Nguyen, 2013) and hierarchical models (Nguyen, 2016).",1. Introduction,[0],[0]
"They are also intimately connected to the problem of clustering — this relationship goes back at least to the work of (Pollard, 1982), where it is pointed out that the well-known K-means clustering algorithm can be directly linked to the quantization problem — the problem of determining an optimal finite discrete probability measure that minimizes its second-order Wasserstein distance from the empirical distribution of given data (Graf and Luschgy, 2000).
",1. Introduction,[0],[0]
"If one is to perform simultaneous K-means clustering for hierarchically grouped data, both at the global level (among groups), and local level (within each group), then this can be achieved by a joint optimization problem defined with suitable notions of Wasserstein distances inserted into the objective function.",1. Introduction,[0],[0]
"In particular, multilevel clustering requires the optimization in the space of probability mea-
sures defined in different levels of abstraction, including the space of measures of measures on the space of grouped data.",1. Introduction,[0],[0]
"Our goal, therefore, is to formulate this optimization precisely, to develop algorithms for solving the optimization problem efficiently, and to make sense of the obtained solutions in terms of statistical consistency.
",1. Introduction,[0],[0]
"The algorithms that we propose address directly a multilevel clustering problem formulated from a purely optimization viewpoint, but they may also be taken as a fast approximation to the inference of latent mixing measures that arise in the nested Dirichlet process of (Rodriguez et al., 2008).",1. Introduction,[0],[0]
"From a statistical viewpoint, we shall establish a consistency theory for our multilevel clustering problem in the manner achieved for K-means clustering (Pollard, 1982).",1. Introduction,[0],[0]
"From a computational viewpoint, quite interestingly, we will be able to explicate and exploit the connection betwen our optimization and that of finding the Wasserstein barycenter (Agueh and Carlier, 2011), an interesting computational problem that have also attracted much recent interests, e.g., (Cuturi and Doucet, 2014).
",1. Introduction,[0],[0]
"In summary, the main contributions offered in this work include (i) a new optimization formulation to the multilevel clustering problem using Wasserstein distances defined on different levels of the hierarchical data structure; (ii) fast algorithms by exploiting the connection of our formulation to the Wasserstein barycenter problem; (iii) consistency theorems established for proposed estimates under very mild condition of data’s distributions; (iv) several flexibile alternatives by introducing constraints that encourage the borrowing of strength among local and global clusters, and (v) finally, demonstration of efficiency and flexibility of our approach in a number of simulated and real data sets.
",1. Introduction,[0],[0]
The paper is organized as follows.,1. Introduction,[0],[0]
"Section 2 provides preliminary background on Wasserstein distance, Wasserstein barycenter, and the connection between K-means clustering and the quantization problem.",1. Introduction,[0],[0]
"Section 3 presents several optimization formulations of the multilevel clusering problem, and the algorithms for solving them.",1. Introduction,[0],[0]
Section 4 establishes consistency results of the estimators introduced in Section 4.,1. Introduction,[0],[0]
Section 5 presents careful simulation studies with both synthetic and real data.,1. Introduction,[0],[0]
"Finally, we conclude the paper with a discussion in Section 6.",1. Introduction,[0],[0]
"Additional technical details, including all proofs, are given in the Supplement.",1. Introduction,[0],[0]
"For any given subset Θ ⊂ Rd, let P(Θ) denote the space of Borel probability measures on Θ. The Wasserstein space of order r ∈",2. Background,[0],[0]
"[1,∞) of probability measures on Θ is de-
fined as Pr(Θ) = { G ∈ P(Θ) : ´ ‖x‖rdG(x) < ∞ } ,
where ‖.‖ denotes Euclidean metric in Rd.",2. Background,[0],[0]
"Addition-
",2. Background,[0],[0]
"ally, for any k ≥ 1 the probability simplex is denoted by ∆k = { u ∈",2. Background,[0],[0]
"Rk : ui ≥ 0, k∑ i=1",2. Background,[0],[0]
ui,2. Background,[0],[0]
= 1 },2. Background,[0],[0]
.,2. Background,[0],[0]
"Finally, let Ok(Θ) (resp., Ek(Θ)) be the set of probability measures with at most (resp., exactly) k support points in Θ.
Wasserstein distances For any elements G and G′ in Pr(Θ) where r ≥ 1, the Wasserstein distance of order r between G and G′ is defined as (cf. (Villani, 2003)):
Wr(G,G ′) =",2. Background,[0],[0]
"( inf
π∈Π(G,G′)
ˆ
Θ2
‖x−",2. Background,[0],[0]
"y‖rdπ(x, y) )",2. Background,[0],[0]
"1/r
where Π(G,G′) is the set of all probability measures on Θ×Θ that have marginalsG andG′.",2. Background,[0],[0]
"In words,W rr (G,G′) is the optimal cost of moving mass from G to G′, where the cost of moving unit mass is proportional to r-power of Euclidean distance in Θ. When G and G′ are two discrete measures with finite number of atoms, fast computation of Wr(G,G
′) can be achieved (see, e.g., (Cuturi, 2013)).",2. Background,[0],[0]
"The details of this are deferred to the Supplement.
",2. Background,[0],[0]
"By a recursion of concepts, we can speak of measures of measures, and define a suitable distance metric on this abstract space: the space of Borel measures on Pr(Θ), to be denoted by Pr(Pr(Θ)).",2. Background,[0],[0]
"This is also a Polish space (that is, complete and separable metric space) as Pr(Θ) is a Polish space.",2. Background,[0],[0]
"It will be endowed with a Wasserstein metric of order r that is induced by a metric Wr on Pr(Θ) as follows (cf. Section 3 of (Nguyen, 2016)): for any D,D′ ∈ Pr(Pr(Θ))
",2. Background,[0],[0]
"Wr(D,D′) := ( inf ˆ
Pr(Θ)2
W rr (G,G ′)dπ(G,G′)
)1/r
where the infimum in the above ranges over all π ∈ Π(D,D′) such that Π(D,D′) is the set of all probability measures on Pr(Θ)×Pr(Θ) that has marginals D and D′.",2. Background,[0],[0]
"In words, Wr(D,D′) corresponds to the optimal cost of moving mass from D to D′, where the cost of moving unit mass in its space of support Pr(Θ) is proportional to the r-power of the Wr distance in Pr(Θ).",2. Background,[0],[0]
"Note a slight notational abuse —Wr is used for both Pr(Θ) and Pr(Pr(Θ)), but it should be clear which one is being used from context.
",2. Background,[0],[0]
Wasserstein barycenter,2. Background,[0],[0]
"Next, we present a brief overview of Wasserstein barycenter problem, first studied by (Agueh and Carlier, 2011) and subsequentially many others (e.g., (Benamou et al., 2015; Solomon et al., 2015; Álvarez Estebana et al., 2016)).",2. Background,[0],[0]
"Given probability measures P1, P2, . . .",2. Background,[0],[0]
", PN ∈ P2(Θ) for N ≥ 1, their Wasserstein barycenter PN,λ is such that
PN,λ = arg min P∈P2(Θ) N∑ i=1",2. Background,[0],[0]
"λiW 2 2 (P, Pi) (1)
where λ ∈ ∆N denote weights associated with P1, . . .",2. Background,[0],[0]
", PN .",2. Background,[0],[0]
"When P1, . . .",2. Background,[0],[0]
", PN are discrete measures with finite number of atoms and the weights λ are uniform, it was shown by (Anderes et al., 2015) that the problem of finding Wasserstein barycenter PN,λ over the space P2(Θ) in (1) is reduced to search only over a much simpler space
Ol(Θ) where l = N∑ i=1 si −N + 1",2. Background,[0],[0]
and si is the number of components of Pi for all 1 ≤ i ≤ N .,2. Background,[0],[0]
"Efficient algorithms for finding local solutions of the Wasserstein barycenter problem over Ok(Θ) for some k ≥ 1 have been studied recently in (Cuturi and Doucet, 2014).",2. Background,[0],[0]
These algorithms will prove to be a useful building block for our method as we shall describe in the sequel.,2. Background,[0],[0]
"The notion of Wasserstein barycenter has been utilized for approximate Bayesian inference (Srivastava et al., 2015).
",2. Background,[0],[0]
"K-means as quantization problem The well-known Kmeans clustering algorithm can be viewed as solving an optimization problem that arises in the problem of quantization, a simple but very useful connection (Pollard, 1982; Graf and Luschgy, 2000).",2. Background,[0],[0]
The connection is the following.,2. Background,[0],[0]
"Given n unlabelled samples Y1, . . .",2. Background,[0],[0]
", Yn ∈ Θ. Assume that these data are associated with at most k clusters where k ≥ 1 is some given number.",2. Background,[0],[0]
"The K-means problem finds the set S containing at most k elements θ1, . . .",2. Background,[0],[0]
", θk ∈ Θ that minimizes the following objective
inf S:|S|≤k
1
n n∑ i=1",2. Background,[0],[0]
"d2(Yi, S).",2. Background,[0],[0]
"(2)
Let Pn = 1
n n∑ i=1 δYi be the empirical measure of data
Y1, . . .",2. Background,[0],[0]
", Yn.",2. Background,[0],[0]
"Then, problem (2) is equivalent to finding a discrete probability measure G which has finite number of support points and solves:
inf G∈Ok(Θ)
",2. Background,[0],[0]
"W 22 (G,Pn).",2. Background,[0],[0]
"(3)
Due to the inclusion of Wasserstein metric in its formulation, we call this a Wasserstein means problem.",2. Background,[0],[0]
This problem can be further thought of as a Wasserstein barycenter problem where N = 1.,2. Background,[0],[0]
"In light of this observation, as noted by (Cuturi and Doucet, 2014), the algorithm for finding the Wasserstein barycenter offers an alternative for the popular Loyd’s algorithm for determing local minimum of the K-means objective.",2. Background,[0],[0]
"Givenm groups of nj exchangeable data pointsXj,i where 1 ≤ j ≤ m, 1 ≤ i ≤ nj , i.e., data are presented in a two-level grouping structure, our goal is to learn about the two-level clustering structure of the data.",3. Clustering with multilevel structure data,[0],[0]
"We want to obtain simultaneously local clusters for each data group, and global clusters among all groups.",3. Clustering with multilevel structure data,[0],[0]
"For any j = 1, . . .",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
",m, we denote the empirical measure for group j by P jnj :",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"= 1
nj nj∑ i=1",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"δXj,i .",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"Throughout this sec-
tion, for simplicity of exposition we assume that the number of both local and global clusters are either known or bounded above by a given number.",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"In particular, for local clustering we allow group j to have at most kj clusters for j = 1, . . .",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
",m. For global clustering, we assume to have M group (Wasserstein) means among the m given groups.
",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"High level idea For local clustering, for each j = 1, . . .",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
",m, performing a K-means clustering for group j, as expressed by (3), can be viewed as finding a finite discrete measure Gj ∈ Okj (Θ) that minimizes squared Wasserstein distance W 22",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"(Gj , P j nj ).",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"For global clustering, we are interested in obtaining clusters out of m groups, each of which is now represented by the discrete measure Gj , for j = 1, . . .",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
",m. Adopting again the viewpoint of Eq.",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"(3), provided that all of Gjs are given, we can apply K-means quantization method to find their distributional clusters.",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"The global clustering in the space of measures of measures on Θ can be succintly expressed by
inf H∈EM (P2(Θ))
",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"W 22
( H, 1
m m∑ j=1 δGj ) .
",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"However, Gj are not known — they have to be optimized through local clustering in each data group.
",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"MWM problem formulation We have arrived at an objective function for jointly optimizing over both local and global clusters
inf Gj∈Okj (Θ), H∈EM (P2(Θ))
",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"m∑ j=1 W 22 (Gj , P j nj )",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
+,3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"W 2 2 (H, 1 m m∑ j=1 δGj ).",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"(4)
We call the above optimization the problem of Multilevel Wasserstein Means (MWM).",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"The notable feature of MWM is that its loss function consists of two types of distances associated with the hierarchical data structure: one is distance in the space of measures, e.g., W 22 (Gj , P j nj ), and the other in space of measures of measures, e.g., W 22 (H, 1
m m∑ j=1 δGj ).",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"By adopting K-means optimization to
both local and global clustering, the multilevel Wasserstein means problem might look formidable at the first sight.",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"Fortunately, it is possible to simplify this original formulation substantially, by exploiting the structure ofH.
Indeed, we can show that formulation (4) is equivalent to the following optimization problem, which looks much simpler as it involves only measures on Θ:
inf Gj∈Okj (Θ),H m∑ j=1 W 22 (Gj , P j nj ) + d2W2(Gj ,H) m (5)
",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"where d2W2(G,H) := min1≤i≤M W 22 (G,Hi) and H = (H1, . .",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
.,3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
",HM ), with each Hi ∈ P2(Θ).",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
The proof of this equivalence is deferred to Proposition B.4 in the Supplement.,3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
"Before going into to the details of the algorithm for solving (5) in Section 3.1.2, we shall present some simpler cases, which help to illustrate some properties of the optimal solutions of (5), while providing insights of subsequent developments of the MWM formulation.",3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
Readers may proceed directly to Section 3.1.2 for the description of the algorithm in the first reading.,3.1. Multilevel Wasserstein Means (MWM) Algorithm,[0],[0]
Example 1.,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Suppose kj = 1 and nj = n for all 1 ≤ j ≤ m, and M = 1.",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
Write H = H ∈ P2(Θ).,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Under this setting, the objective function (5) can be rewritten as
inf θj∈Θ,
H∈P2(Θ)
m∑ j=1 n∑ i=1",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"‖θj −Xj,i‖2 +",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"W 22 (δθj , H)/m, (6)
where Gj = δθj for any 1 ≤ j ≤ m.",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"From the result of Theorem A.1 in the Supplement,
inf θj∈Θ m∑ j=1 W 22 (δθj , H) ≥ inf H∈E1(Θ) m∑ j=1 W 22 (Gj , H)
",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"= m∑ j=1 ‖θj − ( m∑ i=1 θi)/m‖2,
where second infimum is achieved when H",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
= δ,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
( m∑ j=1 θj)/m .,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Thus, objective function (6) may be rewritten as
inf θj∈Θ m∑ j=1 n∑ i=1",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
‖θj,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"−Xj,i‖2 +",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
‖mθj,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
− ( m∑ l=1 θl)‖2/m3.,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
Write Xj = ( n∑ i=1,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Xj,i)/n for all 1 ≤ j ≤ m. As m ≥ 2, we can check that the unique optimal solutions for the
above optimization problem are θj = (
(m2n + 1)Xj",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
+∑ i 6=j Xi ) /(m2n,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
+ m) for any 1 ≤ j ≤ m.,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"If we further assume that our dataXj,i are i.i.d samples from probability measure P j having mean µj = EX∼P j (X) for any 1 ≤ j ≤ m, the previous result implies that θi",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
6→ θj for almost surely as long as µi 6= µj .,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"As a consequence, if µj are pairwise different, the multi-level Wasserstein means under that simple scenario of (5) will not have identical centers among local groups.",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"On the other hand, we have W 22 (Gi, Gj) = ‖θi",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"− θj‖2 =( mn
mn+ 1
)2 ‖Xi − Xj‖2.",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Now, from the definition of
Wasserstein distance
W 22 (P i n, P j n) =",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"min
σ
1
n n∑ l=1 ‖Xi,l −Xj,σ(l)‖2
≥ ‖Xi −Xj‖2,
where σ in the above sum varies over all the permutation of {1, 2, . . .",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
", n} and the second inequality is due to Cauchy-Schwarz’s inequality.",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"It implies that as long as W 22 (P i n, P j n) is small, the optimal solutionGi andGj of (6) will be sufficiently close to each other.",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"By letting n→∞, we also achieve the same conclusion regarding the asymptotic behavior of Gi and Gj with respect to W2(P i, P j).
",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
Example 2.,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
kj = 1 and nj = n for all 1 ≤ j ≤ m,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
and M = 2.,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Write H = (H1, H2).",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Moreover, assume that there is a strict subset A of {1, 2, . . .",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
",m} such that
max { max i,j∈A W2(P i n, P j n),
max i,j∈Ac
W2(P i n, P j n)
} min
i∈A,j∈Ac W2(P
i n, P j n),
i.e., the distances of empirical measures P in and P j n",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
when i and j belong to the same set A or Ac are much less than those when i and j do not belong to the same set.,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Under this condition, by using the argument from part (i) we can write the objective function (5) as
inf θj∈Θ,
H1∈P2(Θ)
∑ j∈A n∑ i=1",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"‖θj −Xj,i‖2 +",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"W 22 (δθj , H1)",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"|A| +
inf θj∈Θ,
H2∈P2(Θ)
∑ j∈Ac n∑ i=1",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"‖θj −Xj,i‖2 +",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"W 22 (δθj , H2) |Ac| .
",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"The above objective function suggests that the optimal solutions θi, θj (equivalently, Gi and Gj) will not be close to each other as long as i and j do not belong to the same set A or Ac, i.e., P in and P j n are very far.",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Therefore, the two groups of “local” measures Gj do not share atoms under that setting of empirical measures.
",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
The examples examined above indicate that the MWM problem in general do not “encourage” the local measures Gj to share atoms among each other in its solution.,3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
"Additionally, when the empirical measures of local groups are very close, it may also suggest that they belong to the same cluster and the distances among optimal local measures Gj can be very small.",3.1.1. PROPERTIES OF MWM IN SPECIAL CASES,[0],[0]
Now we are ready to describe our algorithm in the general case.,3.1.2. ALGORITHM DESCRIPTION,[0],[0]
This is a procedure for finding a local minimum of Problem (5) and is summarized in Algorithm 1.,3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"We prepare the following details regarding the initialization and updating steps required by the algorithm:
Algorithm 1 Multilevel Wasserstein Means (MWM) Input: Data Xj,i, Parameters kj , M .",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
Output: prob.,3.1.2. ALGORITHM DESCRIPTION,[0],[0]
measures,3.1.2. ALGORITHM DESCRIPTION,[0],[0]
Gj and elements Hi of H .,3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"Initialize measures G(0)j , elements H (0) i of H (0), t = 0.
while Y (t)j , b (t) j , H (t)",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"i have not converged do
1.",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
Update Y (t)j and b (t) j for 1 ≤ j ≤ m: for j = 1 to m do ij ← arg,3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"min
1≤u≤M W 22 (G (t) j , H (t) u ).
",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"G (t+1) j ← arg min Gj∈Okj (Θ) W 22 (Gj , P j nj )+",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"+W 22 (Gj , H (t) ij
)/m.",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
end for 2.,3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"Update H(t)i for 1 ≤ i ≤M : for j = 1 to m do ij ← arg min
1≤u≤M W 22 (G (t+1) j , H (t) u ).
end for for i = 1 to M do Ci ← {l : il = i} for 1 ≤ i ≤M .",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"H
(t+1)",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"i ← arg min
Hi∈P2(Θ) ∑ l∈Ci W 22 (Hi, G (t+1) l ).
end for 3. t←",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"t+ 1.
end while
•",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"The initialization of local measures G(0)j (i.e., the initialization of their atoms and weights) can be obtained by performing K-means clustering on local data",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"Xj,i for 1 ≤ j ≤ m.",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
The initialization of elements H(0)i of H(0) is based on a simple extension of the K-means algorithm.,3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"Details are given in Algorithm 3 in the Supplement;
•",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"The updates G(t+1)j can be computed efficiently by simply using algorithms from (Cuturi and Doucet, 2014) to search for local solutions of these barycenter problems within the space Okj (Θ) from the atoms and weights of G(t)j ;
• Since all G(t+1)j are finite discrete measures, finding the updates for H(t+1)i over the whole space P2(Θ) can be reduced to searching for a local solution within space Ol(t) where l(t) =
∑ j∈Ci |supp(G(t+1)j )| − |Ci|
from the global atoms H(t)i of H (t) (Justification of this reduction is derived from Theorem A.1 in the Supplement).",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"This again can be done by utilizing algorithms from (Cuturi and Doucet, 2014).",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"Note that, as l(t) becomes very large when m is large, to speed up the computation of Algorithm 1 we impose a threshold L, e.g., L = 10, for l(t) in its implementation.
",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"The following guarantee for Algorithm 1 can be established:
Theorem 3.1.",3.1.2. ALGORITHM DESCRIPTION,[0],[0]
Algorithm 1 monotonically decreases the objective function (4) of the MWM formulation.,3.1.2. ALGORITHM DESCRIPTION,[0],[0]
"As we have observed from the analysis of several specific cases, the multilevel Waserstein means formulation may not encourage the sharing components locally among m groups in its solution.",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"However, enforced sharing has been demonstrated to be a very useful technique, which leads to the “borrowing of strength” among different parts of the model, consequentially improving the inferential efficiency (Teh et al., 2006; Nguyen, 2016).",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"In this section, we seek to encourage the borrowing of strength among groups by imposing additional constraints on the atoms of G1, . . .",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
", Gm in the original MWM formulation (4).",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"Denote
AM,SK = { Gj ∈ OK(Θ), H ∈ EM (P(Θ)) : supp(Gj) ⊆
SK ∀1 ≤ j ≤ m } for any given K,M ≥ 1 where the
constraint set SK has exactly K elements.",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"To simplify the exposition, let us assume that kj = K for all 1 ≤ j ≤ m. Consider the following locally constrained version of the multilevel Wasserstein means problem
inf m∑ j=1 W 22",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"(Gj , P j nj )",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
+,3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"W 2 2 (H, 1 m m∑ j=1 δGj ).",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"(7)
where SK , Gj ,H ∈ AM,SK in the above infimum.",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
We call the above optimization the problem of Multilevel Wasserstein Means with Sharing (MWMS).,3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"The local constraint assumption supp(Gj) ⊆ SK had been utilized previously in the literature — see for example the work of (Kulis and Jordan, 2012), who developed an optimization-based approach to the inference of the HDP (Teh et al., 2006), which also encourages explicitly the sharing of local group means among local clusters.",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"Now, we can rewrite objective function (7) as follows
inf SK ,Gj ,H∈BM,SK m∑ j=1 W 22 (Gj , P j nj ) + d2W2(Gj ,H) m (8)
where BM,SK = { Gj ∈ OK(Θ), H = (H1, . . .",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
",HM ) :
supp(Gj) ⊆ SK ∀1 ≤ j ≤ m } .",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"The high level idea
of finding local minimums of objective function (8) is to first, update the elements of constraint set SK to provide the supports for local measuresGj and then, obtain the weights of these measures as well as the elements of global set H by computing appropriate Wasserstein barycenters.",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
"Due to space constraint, the details of these steps of the MWMS Algorithm (Algorithm 2) are deferred to the Supplement.",3.2. Multilevel Wasserstein Means with Sharing,[0],[0]
We proceed to establish consistency for the estimators introduced in the previous section.,4. Consistency results,[0],[0]
"For the brevity of the presentation, we only focus on the MWM method; consistency for MWMS can be obtained in a similar fashion.",4. Consistency results,[0],[0]
"Fix m, and assume that P j is the true distribution of data Xj,i for j = 1, . . .",4. Consistency results,[0],[0]
",m. Write G = (G1, . . .",4. Consistency results,[0],[0]
", Gm) and n = (n1, . . .",4. Consistency results,[0],[0]
", nm).",4. Consistency results,[0],[0]
"We say n → ∞ if nj → ∞ for j = 1, . . .",4. Consistency results,[0],[0]
",m. Define the following functions fn(G,H) =",4. Consistency results,[0],[0]
"m∑ j=1 W 22 (Gj , P j nj )",4. Consistency results,[0],[0]
+,4. Consistency results,[0],[0]
"W 2 2 (H, 1 m m∑ j=1 δGj ),
f(G,H) =",4. Consistency results,[0],[0]
"m∑ j=1 W 22 (Gj , P j) +W 22 (H, 1 m m∑ j=1 δGj ),
where Gj ∈ Okj (Θ),",4. Consistency results,[0],[0]
H ∈ EM (P(Θ)),4. Consistency results,[0],[0]
as 1 ≤ j ≤ m.,4. Consistency results,[0],[0]
"The first consistency property of the WMW formulation:
Theorem 4.1.",4. Consistency results,[0],[0]
"Given that P j ∈ P2(Θ) for 1 ≤ j ≤ m. Then, there holds almost surely, as n→∞
inf Gj∈Okj (Θ), H∈EM (P2(Θ))",4. Consistency results,[0],[0]
"fn(G,H)− inf Gj∈Okj (Θ), H∈EM (P2(Θ))",4. Consistency results,[0],[0]
"f(G,H)→ 0.
",4. Consistency results,[0],[0]
The next theorem establishes that the “true” global and local clusters can be recovered.,4. Consistency results,[0],[0]
"To this end, assume that for each n there is an optimal solution (Ĝn11 , . . .",4. Consistency results,[0],[0]
", Ĝ nm m , Ĥn) or in short (Ĝ n ,Hn) of the objective function (4).",4. Consistency results,[0],[0]
"Moreover, there exist a (not necessarily unique) optimal solution minimizing f(G,H) overGj ∈",4. Consistency results,[0],[0]
Okj (Θ) andH ∈ EM (P2(Θ)).,4. Consistency results,[0],[0]
Let F be the collection of such optimal solutions.,4. Consistency results,[0],[0]
"For any Gj ∈ Okj (Θ) andH ∈ EM (P2(Θ)), define
d(G,H,F) = inf (G0,H0)∈F m∑ j=1 W 22 (Gj , G 0 j )",4. Consistency results,[0],[0]
+,4. Consistency results,[0],[0]
"W 2 2 (H,H0).
",4. Consistency results,[0],[0]
"Given the above assumptions, we have the following result regarding the convergence of (Ĝ n ,Hn):
Theorem 4.2.",4. Consistency results,[0],[0]
Assume that Θ is bounded and P j ∈ P2(Θ) for all 1 ≤ j ≤ m.,4. Consistency results,[0],[0]
"Then, we have d(Ĝ n , Ĥn,F) → 0 as n→∞ almost surely.
",4. Consistency results,[0],[0]
Remark: (i),4. Consistency results,[0],[0]
The assumption Θ is bounded is just for the convenience of proof argument.,4. Consistency results,[0],[0]
We believe that the conclusion of this theorem may still hold when Θ = Rd.,4. Consistency results,[0],[0]
(ii),4. Consistency results,[0],[0]
"If |F| = 1, i.e., there exists an unique optimal solution G0,H0 minimizing f(G,H) over Gj ∈ Okj (Θ) and H ∈ EM (P2(Θ)), the result of Theorem 4.2 implies that W2(Ĝ nj j , G 0 j )",4. Consistency results,[0],[0]
"→ 0 for 1 ≤ j ≤ m and W2(Ĥn,H0) → 0 as n→∞.",4. Consistency results,[0],[0]
"In this section, we are interested in evaluating the effectiveness of both MWM and MWMS clustering algorithms by considering different synthetic data generating processes.",5.1. Synthetic data,[0],[0]
"Unless otherwise specified, we set the number of groups m = 50, number of observations per group nj = 50 in d = 10 dimensions, number of global clusters M = 5 with 6 atoms.",5.1. Synthetic data,[0],[0]
For Algorithm 1 (MWM) local measures Gj have 5 atoms each; for Algorithm 2 (MWMS) number of atoms in constraint set SK is 50.,5.1. Synthetic data,[0],[0]
As a benchmark for the comparison we will use a basic 3-stage K-means approach (the details of which can be found in the Supplement).,5.1. Synthetic data,[0],[0]
"The Wasserstein distance between the estimated distributions (i.e. Ĝ1, . . .",5.1. Synthetic data,[0],[0]
", Ĝm; Ĥ1, . . .",5.1. Synthetic data,[0],[0]
", ĤM ) and the data generating ones will be used as the comparison metric.
",5.1. Synthetic data,[0],[0]
"Recall that the MWM formulation does not impose constraints on the atoms of Gi, while the MWMS formulation explicitly enforces the sharing of atoms across these measures.",5.1. Synthetic data,[0],[0]
We used multiple layers of mixtures while adding Gaussian noise at each layer to generate global and local clusters and the no-constraint (NC) data.,5.1. Synthetic data,[0],[0]
We varied number of groups m from 500 to 10000.,5.1. Synthetic data,[0],[0]
"We notice that the 3-stage K-means algorithm performs the best when there is no constraint structure and variance is constant across clusters (Fig. 1(a) and 2(a)) — this is, not surprisingly, a favorable setting for the basic K-means method.",5.1. Synthetic data,[0],[0]
"As soon as we depart from the (unrealistic) constant-variance, no-sharing assumption, both of our algorithms start to outperform the basic three-stage K-means.",5.1. Synthetic data,[0],[0]
The superior performance is most pronounced with local-constraint (LC) data (with or without constant variance conditions).,5.1. Synthetic data,[0],[0]
"See Fig. 1(c,d).",5.1. Synthetic data,[0],[0]
"It is worth noting that even when group variances are constant, the 3-stage K-means is no longer longer effective because now fails to account for the shared structure.",5.1. Synthetic data,[0],[0]
"Whenm = 50 and group sizes are larger, we set SK = 15.",5.1. Synthetic data,[0],[0]
"Results are reported in Fig. 2 (c), (d).",5.1. Synthetic data,[0],[0]
These results demonstrate the effectiveness and flexibility of our both algorithms.,5.1. Synthetic data,[0],[0]
"We applied our multilevel clustering algorithms to two realworld datasets: LabelMe and StudentLife.
LabelMe dataset consists of 2, 688 annotated images which are classified into 8 scene categories including tall buildings, inside city, street, highway, coast, open country, mountain, and forest (Oliva and Torralba, 2001) .",5.2. Real data analysis,[0],[0]
Each image contains multiple annotated regions.,5.2. Real data analysis,[0],[0]
"Each region, which is annotated by users, represents an object in the image.",5.2. Real data analysis,[0],[0]
"As shown in Figure 4, the left image is an image from open country category and contains 4 regions while the right panel denotes an image of tall buildings category
including 16 regions.",5.2. Real data analysis,[0],[0]
Note that the regions in each image can be overlapped.,5.2. Real data analysis,[0],[0]
"We remove the images containing less then 4 regions and obtain 1, 800 images.
",5.2. Real data analysis,[0],[0]
"We then extract GIST feature (Oliva and Torralba, 2001) for each region in a image.",5.2. Real data analysis,[0],[0]
GIST is a visual descriptor to represent perceptual dimensions and oriented spatial structures of a scene.,5.2. Real data analysis,[0],[0]
Each GIST descriptor is a 512- dimensional vector.,5.2. Real data analysis,[0],[0]
We further use PCA to project GIST features into 30 dimensions.,5.2. Real data analysis,[0],[0]
"Finally, we obtain 1, 800 “documents”, each of which contains regions as observations.",5.2. Real data analysis,[0],[0]
Each region now is represented by a 30-dimensional vector.,5.2. Real data analysis,[0],[0]
We now can perform clustering regions in every image since they are visually correlated.,5.2. Real data analysis,[0],[0]
"In the next level of clustering, we can cluster images into scene categories.
",5.2. Real data analysis,[0],[0]
StudentLife dataset is a large dataset frequently used in pervasive and ubiquitous computing research.,5.2. Real data analysis,[0],[0]
"Data signals
consist of multiple channels (e.g., WiFi signals, Bluetooth scan, etc.), which are collected from smartphones of 49 students at Dartmouth College over a 10-week spring term in 2013.",5.2. Real data analysis,[0],[0]
"However, in our experiments, we use only WiFi signal strengths.",5.2. Real data analysis,[0],[0]
"We applied a similar procedure described in (Nguyen et al., 2016) to pre-process the data.",5.2. Real data analysis,[0],[0]
We aggregate the number of scans by each Wifi access point and select 500 Wifi Ids with the highest frequencies.,5.2. Real data analysis,[0],[0]
"Eventually, we obtain 49 “documents” with totally approximately 4.6 million 500-dimensional data points.
",5.2. Real data analysis,[0],[0]
Experimental results.,5.2. Real data analysis,[0],[0]
"To quantitatively evaluate our proposed methods, we compare our algorithms with several base-line methods: K-means, three-stage K-means (TSKmeans) as described in the Supplement, MC2-SVI without context (Huynh et al., 2016).",5.2. Real data analysis,[0],[0]
Clustering performance in Table 1 is evaluated with the image clustering problem for LabelMe dataset.,5.2. Real data analysis,[0],[0]
"With K-means, we average all data points
to obtain a single vector for each images.",5.2. Real data analysis,[0],[0]
"K-means needs much less time to run since the number of data points is now reduced to 1, 800.",5.2. Real data analysis,[0],[0]
"For MC2-SVI, we used stochastic varitational and a parallelized Spark-based implementation in (Huynh et al., 2016) to carry out experiments.",5.2. Real data analysis,[0],[0]
This implementation has the advantage of making use of all of 16 cores on the test machine.,5.2. Real data analysis,[0],[0]
The running time for MC2-SVI is reported after scanning one epoch.,5.2. Real data analysis,[0],[0]
"In terms of clustering accuracy, MWM and MWMS algorithms perform the best.
",5.2. Real data analysis,[0],[0]
Fig.,5.2. Real data analysis,[0],[0]
3a demonstrates five representative image clusters with six randomly chosen images in each (on the right) which are discovered by our MWMS algorithm.,5.2. Real data analysis,[0],[0]
We also accumulate labeled tags from all images in each cluster to produce the tag-cloud on the left.,5.2. Real data analysis,[0],[0]
These tag-clouds can be considered as visual ground truth of clusters.,5.2. Real data analysis,[0],[0]
"Our algorithm can group images into clusters which are consistent with their tag-clouds.
",5.2. Real data analysis,[0],[0]
We use StudentLife dataset to demonstrate the capability of multilevel clustering with large-scale datasets.,5.2. Real data analysis,[0],[0]
This dataset not only contains a large number of data points but presents in high dimension.,5.2. Real data analysis,[0],[0]
Our algorithms need approximately 1 hour to perform multilevel clustering on this dataset.,5.2. Real data analysis,[0],[0]
Fig.,5.2. Real data analysis,[0],[0]
3b presents two levels of clusters discovered by our algorithms.,5.2. Real data analysis,[0],[0]
The innermost (blue) and outermost (green) rings depict local and global clusters respectively.,5.2. Real data analysis,[0],[0]
"Global clusters represent groups of students while local clusters shared between students (“documents”) may be used to infer loca-
tions of students’ activities.",5.2. Real data analysis,[0],[0]
"From these clusteing we can dissect students’ shared location (activities), e.g. Student 49 (U49) mainly takes part in activity location 4 (L4).",5.2. Real data analysis,[0],[0]
We have proposed an optimization based approach to multilevel clustering using Wasserstein metrics.,6. Discussion,[0],[0]
There are several possible directions for extensions.,6. Discussion,[0],[0]
"Firstly, we have only considered continuous data; it is of interest to extend our formulation to discrete data.",6. Discussion,[0],[0]
"Secondly, our method requires knowledge of the numbers of clusters both in local and global clustering.",6. Discussion,[0],[0]
"When these numbers are unknown, it seems reasonable to incorporate penalty on the model complexity.",6. Discussion,[0],[0]
"Thirdly, our formulation does not directly account for the “noise” distribution away from the (Wasserstein) means.",6. Discussion,[0],[0]
"To improve the robustness, it may be desirable to make use of the first-order Wasserstein metric instead of the second-order one.",6. Discussion,[0],[0]
"Finally, we are interested in extending our approach to richer settings of hierarchical data, such as one when group level-context is available.
",6. Discussion,[0],[0]
Acknowledgement.,6. Discussion,[0],[0]
"This research is supported in part by grants NSF CAREER DMS-1351362, NSF CNS-1409303, the Margaret and Herman Sokol Faculty Award and research gift from Adobe Research (XN).",6. Discussion,[0],[0]
DP gratefully acknowledges the partial support from the Australian Research Council (ARC) and AOARD (FA2386-16-1-4138).,6. Discussion,[0],[0]
"We propose a novel approach to the problem of multilevel clustering, which aims to simultaneously partition data in each group and discover grouping patterns among groups in a potentially large hierarchically structured corpus of data.",abstractText,[0],[0]
"Our method involves a joint optimization formulation over several spaces of discrete probability measures, which are endowed with Wasserstein distance metrics.",abstractText,[0],[0]
"We propose a number of variants of this problem, which admit fast optimization algorithms, by exploiting the connection to the problem of finding Wasserstein barycenters.",abstractText,[0],[0]
Consistency properties are established for the estimates of both local and global clusters.,abstractText,[0],[0]
"Finally, experiment results with both synthetic and real data are presented to demonstrate the flexibility and scalability of the proposed approach.",abstractText,[0],[0]
1,abstractText,[0],[0]
Multilevel Clustering via Wasserstein Means,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 227–231, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"Automated text summarization is an active field of research in various communities, including Information Retrieval, Natural Language Processing, and Text Mining.
",1 Introduction,[0],[0]
"Some authors reduce summarization to the maximum coverage problem (Takamura and Okumura, 2009; Gillick and Favre, 2009) which, despite positive results, is known as NPhard (Khuller et al., 1999).",1 Introduction,[0],[0]
"Because linear programming (LP) helps to find an accurate approximated solution to this problem it has recently become very popular in the summarization field (Gillick and Favre, 2009; Woodsend and Lapata, 2010; Hitoshi Nishikawa and Kikui, 2010; Makino et al., 2011).
",1 Introduction,[0],[0]
"Trying to solve a trade-off between summary quality and time complexity, we propose a summarization model solving the approximated maximum coverage problem by linear programming in
polynomial time.",1 Introduction,[0],[0]
We measure information coverage by an objective function and strive to obtain a summary that preserves its optimal value as much as possible.,1 Introduction,[0],[0]
Three objective functions considering different metrics of information are introduced and evaluated.,1 Introduction,[0],[0]
"The main achievement of our method is a text representation model expanding a classic vector space model (Salton et al., 1975) to hyperplane and half-spaces and making it possible to represent an exponential number of extracts without computing them explicitly.",1 Introduction,[0],[0]
"This model also enables us to find the optimal extract by simple optimizing an objective function in polynomial time, using linear programming over rationals.",1 Introduction,[0],[0]
"For the first time, the frequent sequence mining was integrated with the maximal coverage approach in order to obtain a summary that best describes the summarized document.",1 Introduction,[0.9543728582060989],"['Currently, the update for the encoder only happens when a translation error occurs in the decoder.']"
"One of the introduced objective functions implements this idea.
",1 Introduction,[0],[0]
"Our method ranks and extracts significant sentences into a summary, without any need in morphological text analysis.",1 Introduction,[0],[0]
"It was applied for both single-document (MSS) and multi-document (MMS) MultiLing 2015 summarization tasks, in three languages–English, Hebrew, and Arabic.",1 Introduction,[0],[0]
"In this paper we present experimental results in comparison with other systems that participated in the same tasks, using the same languages.",1 Introduction,[0],[0]
We are given a document or a set of related documents in UTF-8 encoding.,2 Preprocessing and definitions,[0],[0]
"Documents are split into sentences S1, ..., Sn.",2 Preprocessing and definitions,[0],[0]
"All sentences undergo tokenization, stop-word removal, and stemming.",2 Preprocessing and definitions,[0],[0]
"For some languages, stemming may be very basic or absent, and a list of stop-words may be unavailable.",2 Preprocessing and definitions,[0],[0]
"All these factors affect summarization quality.
",2 Preprocessing and definitions,[0],[0]
"Unique stemmed words are called terms and are denoted by T1, ..., Tm.",2 Preprocessing and definitions,[0],[0]
"Every sentence is modeled as a sequence of terms from T1, ..., Tm where each
227
term may appear zero or more times in a sentence.",2 Preprocessing and definitions,[0],[0]
"We are also given the desired number of words for a summary, denoted by MaxWords .
",2 Preprocessing and definitions,[0],[0]
"The goal of extractive summarization is to find a subset of sentences S1, ..., Sn that has no more than MaxWords words and conveys as much information as possible about the documents.",2 Preprocessing and definitions,[0],[0]
"Because it is difficult, or even impossible, to know what humans consider to be the best summary, we approximate the human decision process by optimizing certain objective functions over representation of input documents constructed according to our model.",2 Preprocessing and definitions,[0],[0]
"The number of words in a summary, sentences, and terms, are represented as constraints in our model.",2 Preprocessing and definitions,[0],[0]
"In the polytope model (Litvak and Vanetik, 2014) a document is viewed as an integer sentence-term matrix A = (aij), where aij denotes the number of appearances of term Tj in sentence Si.",3.1 Definitions,[0],[0]
"A row i of matrix A is used to define a linear constraint for sentence Si as follows:
m∑ j=1 aijxij ≤ m∑ j=1 aij (1)
",3.1 Definitions,[0],[0]
Equation (1) also defines the lower half-space in Rmn corresponding to sentence Si.,3.1 Definitions,[0],[0]
"Together with additional constraints, such as a bound MaxWords on the number of words in the summary, we obtain a system of linear inequalities that describes the intersection of corresponding lower half-spaces of Rmn, forming a closed convex polyhedron called a polytope: ∑m j=1 aijxij ≤ ∑m j=1 aij , ∀i = 1..n
0 ≤",3.1 Definitions,[0],[0]
"xij ≤ 1, ∀i = 1..n, j = 1..m∑n i=1",3.1 Definitions,[0],[0]
"∑m j=1 aijxij ≤ MaxWords
(2)
",3.1 Definitions,[0],[0]
"All possible extractive summaries are represented by vertices of the polytope defined in (2).
",3.1 Definitions,[0],[0]
It remains only to define an objective function which optimum on the polytope boundary will define the summary we seek.,3.1 Definitions,[0],[0]
"Because such an optimum may be achieved not on a polytope vertex but rather on one of polytope faces (because we use linear programming over rationals), we need only to locate the vertex of a polytope closest to the point of optimum.",3.1 Definitions,[0],[0]
"This task is done by finding distances from the optimum to every one of the sentence hyperplanes and selecting those with
minimal distance to the point of optimum.",3.1 Definitions,[0],[0]
"If there are too many candidate sentences, we give preference to those closest to the beginning of the document.
",3.1 Definitions,[0],[0]
"The main advantage of this model is the relatively low number of constraints (comparable with the number of terms and sentences in a document) and both the theoretical and practical polynomial running times of LP over rationals (Karmarkar, 1984).",3.1 Definitions,[0],[0]
"In this section, we describe the objective functions we used in our system.",3.2 Objective functions,[0],[0]
"Humans identify good summaries immediately, but specifying summary quality as a linear function of terms, sentences, and their parameters is highly nontrivial.",3.2 Objective functions,[0],[0]
"In most cases, additional parameters, variables, and constraints must be added to the model.",3.2 Objective functions,[0],[0]
"The first objective function maximizes relevance of sentences chosen for a summary, while minimizing pairwise redundancy between them.
",3.3 Maximal sentence relevance,[0],[0]
We define relevance cosrel,3.3 Maximal sentence relevance,[0],[0]
"i of a sentence Si as a cosine similarity between the sentence, viewed as a weighted vector of its terms, and the document.",3.3 Maximal sentence relevance,[0],[0]
Relevance values are completely determined by the text and are not affected by choice of a summary.,3.3 Maximal sentence relevance,[0],[0]
"Every sentence Si is represented by a sentence variable:
si = ∑m j=1 aijxij/ ∑m j=1 aij (3)
Formally, variable si represents the hyperplane bounding the lower half-space of Rmn related to sentence Si and bounding the polytope.",3.3 Maximal sentence relevance,[0],[0]
"Clearly, si assumes values in range [0, 1], where 0 means that the sentence is completely omitted from the summary and 1 means that the sentence is definitely chosen for the summary.",3.3 Maximal sentence relevance,[0],[0]
"Relevance of all sentences in the summary is described by the expression
n∑ i=1 cosrel isi (4)
Redundancy needs to be modeled and computed for every pair of sentences separately.",3.3 Maximal sentence relevance,[0],[0]
"We use additional redundancy variables red ij for every pair Si, Sj of sentences where i <",3.3 Maximal sentence relevance,[0],[0]
j.,3.3 Maximal sentence relevance,[0],[0]
"Every one of these variables is 0 − 1 bounded and achieves a value of 1 only if both sentences are chosen for
the summary with the help of these constraints: 0 ≤ red ij ≤ 1, 0 ≤",3.3 Maximal sentence relevance,[0],[0]
"i < j ≤ n red ij ≤ si, red ij ≤",3.3 Maximal sentence relevance,[0],[0]
sj si + sj,3.3 Maximal sentence relevance,[0],[0]
"− red ij ≤ 1 (5)
",3.3 Maximal sentence relevance,[0],[0]
"The numerical redundancy coefficient for sentences Si and Sj is their cosine similarity as term vectors, which we compute directly from the text and denote by cosred ij .",3.3 Maximal sentence relevance,[0],[0]
"The objective function we use to maximize relevance of the chosen sentences while minimizing redundancy is
max n∑
i=1
cosrel isi",3.3 Maximal sentence relevance,[0],[0]
"− n∑
i=1 n∑ j=1 cosred ijred ij (6)",3.3 Maximal sentence relevance,[0],[0]
"The second proposed objective function maximizes the weighted sum of bigrams (consecutive term pairs appearing in sentences), where the weight of a bigram denotes its importance.
",3.4 Sum of bigrams,[0],[0]
"The importance count ij of a bigram (Ti, Tj) is computed as the number of its appearances in the document.",3.4 Sum of bigrams,[0],[0]
"It is quite possible that this bigram appears twice in one sentence, and once in another, and i = j is possible as well.
",3.4 Sum of bigrams,[0],[0]
"In order to represent bigrams, we introduce new bigram variables bgij for i, j = 1..m, covering all possible term pairs.",3.4 Sum of bigrams,[0],[0]
An appearance of a bigram in sentence,3.4 Sum of bigrams,[0],[0]
"Sk is modeled by a 0 − 1 bounded variable bgkij , and c k ij denotes the number of times this bigram appears in sentence",3.4 Sum of bigrams,[0],[0]
Sk.,3.4 Sum of bigrams,[0],[0]
"A bigram is represented by a normalized sum of its appearances in various sentences as follows:{
0 ≤ bgkij ≤ 1, ∀i, j, k bgij = ∑n k=1 c k",3.4 Sum of bigrams,[0],[0]
ijbg k,3.4 Sum of bigrams,[0],[0]
ij/,3.4 Sum of bigrams,[0],[0]
∑n k=1,3.4 Sum of bigrams,[0],[0]
"c k ij
(7)
",3.4 Sum of bigrams,[0],[0]
"Additionally, the appearance bgkij of a bigram in sentence",3.4 Sum of bigrams,[0],[0]
"Sk is tied to terms Ti and Tj composing it, with the help of variables xki and xkj denoting appearances of these terms in Sk:
bgkij ≤",3.4 Sum of bigrams,[0],[0]
xki bgkij ≤,3.4 Sum of bigrams,[0],[0]
xkj xki +,3.4 Sum of bigrams,[0],[0]
xkj,3.4 Sum of bigrams,[0],[0]
"− bgkij ≤ 1
(8)
The constraints in (8) express the fact that a bigram cannot appear without the terms composing it, and appearance of both terms causes, in turn, the appearance of a bigram.",3.4 Sum of bigrams,[0],[0]
"Our objective function is:
max :",3.4 Sum of bigrams,[0],[0]
"m∑
i=1",3.4 Sum of bigrams,[0],[0]
m∑ j=1 count ijbgij (9),3.4 Sum of bigrams,[0],[0]
"The third proposed objective function modifies the model so that only the most important terms are taken into account.
",3.5 Maximal relevance with frequent itemsets,[0],[0]
"Let us view each sentence Si as a sequence (Ti1, . . .",3.5 Maximal relevance with frequent itemsets,[0],[0]
", Tin) of terms, and the order of terms preserves the original word order of a sentence.",3.5 Maximal relevance with frequent itemsets,[0],[0]
Source documents are viewed as a database of sentences.,3.5 Maximal relevance with frequent itemsets,[0],[0]
"Database size is n. Let s = (Ti1, . . .",3.5 Maximal relevance with frequent itemsets,[0],[0]
", Tik) be a sequence of terms of size k. Support of s in the database is the ratio of sentences containing this sequence, to the database size n.
",3.5 Maximal relevance with frequent itemsets,[0],[0]
Given a user-defined support bound S ∈,3.5 Maximal relevance with frequent itemsets,[0],[0]
"[0, 1], a term sequence s is frequent if support(s) ≥ S .",3.5 Maximal relevance with frequent itemsets,[0],[0]
"Frequent term sequences can be computed by a multitude of existing algorithms, such as Apriori (Agrawal et al., 1994), FreeSpan (Han et al., 2000), GSP (Zaki, 2001), etc.
",3.5 Maximal relevance with frequent itemsets,[0],[0]
"In order to modify the generic model described in (2), we first find all frequent sequences in the documents and store them in set F .",3.5 Maximal relevance with frequent itemsets,[0],[0]
"Then we sort F first by decreasing sequence size and then by decreasing support, and finally we keep only top B sequences for a user-defined boundary B.
We modify the general model (2) by representing sentences as sums of their frequent sequences from F .",3.5 Maximal relevance with frequent itemsets,[0],[0]
"Let F = {f1, . . .",3.5 Maximal relevance with frequent itemsets,[0],[0]
", fk}, sorted by decreasing size and then by decreasing support.",3.5 Maximal relevance with frequent itemsets,[0],[0]
"A sentence Si is said to contain fj if it contains it as a term sequence and no part of fj in Si is covered by sequences f1, . . .",3.5 Maximal relevance with frequent itemsets,[0],[0]
", fj−1.
Let count ij denote the number of times sentence Si contains frequent term sequence fj .",3.5 Maximal relevance with frequent itemsets,[0],[0]
Variables fij denote the appearance of sequence fj in sentence Si.,3.5 Maximal relevance with frequent itemsets,[0],[0]
"We replace the polytope (2) by:{ ∑k
j=1 count ijfij ≤",3.5 Maximal relevance with frequent itemsets,[0],[0]
"∑k
j=1 count ij , ∀i = 1..n 0 ≤ fij ≤ 1, ∀i = 1..n, j = 1..k
(10) We add variables describing the relevance of each sentence by introducing sentence variables:
si = ∑k j=1 countijfij/ ∑k j=1 countij (11)
Defining a boundary on the length of a summary now requires an additional constraint because frequent sequences do not contain all the terms in the sentences.",3.5 Maximal relevance with frequent itemsets,[0],[0]
"Summary size is bounded as follows:
n∑ i=1",3.5 Maximal relevance with frequent itemsets,[0],[0]
"lengthisi ≤ MaxWords (12)
",3.5 Maximal relevance with frequent itemsets,[0],[0]
"Here, lengthi is the exact word count of sentence Si.
",3.5 Maximal relevance with frequent itemsets,[0],[0]
Relevance freqrel,3.5 Maximal relevance with frequent itemsets,[0],[0]
"i of a sentence Si is defined as a cosine similarity between the vector of terms in Si covered by members of F , and the entire document.",3.5 Maximal relevance with frequent itemsets,[0],[0]
The difference between this approach and the one described in Section 3.3 is that only frequent terms are taken into account when computing sentence-document similarity.,3.5 Maximal relevance with frequent itemsets,[0],[0]
"The resulting objective function maximizes relevance of chosen sentences while minimizing redundancy defined in (5):
max n∑
i=1
freqrel isi",3.5 Maximal relevance with frequent itemsets,[0],[0]
"− n∑
i=1 n∑ j=1 cosred ijred ij (13)",3.5 Maximal relevance with frequent itemsets,[0],[0]
"Tables 4, 4, and 1 contain the summarized results of automated evaluations for MultiLing 2015, single-document summarization (MSS) task for English, Hebrew, and Arabic corpora, respectively.",4 Experiments,[0],[0]
"The quality of the summaries is measured by ROUGE-1 (Recall, Precision, and Fmeasure).(Lin, 2004)",4 Experiments,[0],[0]
"We also demonstrate the absolute ranks of each submission–P-Rank, R-Rank, and F-Rank–when their scores are sorted by Precision, Recall, and F-measure, respectively.",4 Experiments,[0],[0]
Only the best submissions (in terms of F-measure) for each participated system are presented and sorted in descending order of their F-measure scores.,4 Experiments,[0],[0]
"Two systems–Oracles and Lead–were used as topline and baseline summarizers, respectively.",4 Experiments,[0],[0]
"Oracles compute summaries for each article using the combinatorial covering algorithm in (Davis et al., 2012)–sentences were selected from a text to maximally cover the tokens in the human summary, using as few sentences as possible until its size exceeded the human summary, at which point it was truncated.",4 Experiments,[0],[0]
"Because Oracles can actually “see” the human summaries, it is considered as the optimal algorithm and its scores are the best scores that extractive approaches can achieve.",4 Experiments,[0],[0]
"Lead simply extracts the leading substring of the body text of the articles having the same length as the human summary of the article.
",4 Experiments,[0],[0]
"Below we summarize the comparative results for our summarizer (denoted in the following tables by Poly) in both tasks, in terms of Rouge-1, F-measure.",4 Experiments,[0],[0]
"For comparisons, we consider the best result out of 3 functions: coverage of frequent sequences for English and coverage of meaningful words for Hebrew and Arabic.",4 Experiments,[0],[0]
English:,4 Experiments,[0],[0]
4th places out of 9 participants in both MSS and MMS tasks.,4 Experiments,[0],[0]
Hebrew:,4 Experiments,[0],[0]
"3rd place out of 7 and out of 9 partici-
pants in MSS and MMS tasks, respectively; and the highest recall score in MMS task.",4 Experiments,[0],[0]
"Arabic: 5th place out of 7 systems in MSS task, and 4th place out of 9 participants and the highest recall score in MMS task.",4 Experiments,[0],[0]
"As can be seen, the best performance for our summarizer has been achieved on the dataset of Hebrew documents.",4 Experiments,[0],[0]
"For example, only the top-line Oracles and the supervised MUSE summarizers outperformed our system in MSS task.",4 Experiments,[0],[0]
Poly also outperformed Gillick (2009) model using ILP.,4 Experiments,[0],[0]
The average running time for Poly is 500 ms per document.,4 Experiments,[0],[0]
In this paper we present an extractive summarization system based on a linear programming model.,5 Conclusions and Future Work,[0],[0]
We represent the document as a set of intersecting hyperplanes.,5 Conclusions and Future Work,[0],[0]
Every possible summary of a document is represented as the intersection of two or more hyperlanes.,5 Conclusions and Future Work,[0],[0]
We consider the summary to be the best if the optimal value of the objective function is achieved during summarization.,5 Conclusions and Future Work,[0],[0]
We introduce multiple objective functions describing the relevance of a sentence in terms of information coverage.,5 Conclusions and Future Work,[0],[0]
The results obtained by automatic evaluation show that the introduced approach performs quite well for Hebrew and English.,5 Conclusions and Future Work,[0],[0]
Only top-line and supervised summarizers outperform Poly on the Hebrew corpus.,5 Conclusions and Future Work,[0],[0]
"It is worth noting that our system is unsupervised and does not require annotated data, and it has polynomial running time.",5 Conclusions and Future Work,[0],[0]
The problem of extractive text summarization for a collection of documents is defined as the problem of selecting a small subset of sentences so that the contents and meaning of the original document set are preserved in the best possible way.,abstractText,[0],[0]
In this paper we describe the linear programming-based global optimization model to rank and extract the most relevant sentences to a summary.,abstractText,[0],[0]
We introduce three different objective functions being optimized.,abstractText,[0],[0]
"These functions define a relevance of a sentence that is being maximized, in different manners, such as: coverage of meaningful words of a document, coverage of its bigrams, or coverage of frequent sequences of words.",abstractText,[0],[0]
We supply here an overview of our system’s participation in the MultiLing contest of SIGDial 2015.,abstractText,[0],[0]
Multilingual Summarization with Polytope Model,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2225–2235 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2225
Multimodal affective computing, learning to recognize and interpret human affect and subjective information from multiple data sources, is still challenging because: (i) it is hard to extract informative features to represent human affects from heterogeneous inputs; (ii) current fusion strategies only fuse different modalities at abstract levels, ignoring time-dependent interactions between modalities. Addressing such issues, we introduce a hierarchical multimodal architecture with attention and word-level fusion to classify utterancelevel sentiment and emotion from text and audio data. Our introduced model outperforms state-of-the-art approaches on published datasets, and we demonstrate that our model’s synchronized attention over modalities offers visual interpretability.",text,[0],[0]
"With the recent rapid advancements in social media technology, affective computing is now a popular task in human-computer interaction.",1 Introduction,[0],[0]
"Sentiment analysis and emotion recognition, both of which require applying subjective human concepts for detection, can be treated as two affective computing subtasks on different levels (Poria et al., 2017a).",1 Introduction,[0],[0]
"A variety of data sources, including voice, facial expression, gesture, and linguistic content have been employed in sentiment analysis and emotion recognition.",1 Introduction,[0],[0]
"In this paper, we focus on a multimodal structure to leverage the advantages of each data source.",1 Introduction,[0],[0]
"Specifically, given an utterance, we consider the linguistic content and acoustic characteristics together to recognize the opinion or emotion.",1 Introduction,[0],[0]
"Our work is important and useful
∗ Equally Contribution
because speech is the most basic and commonly used form of human expression.
",1 Introduction,[0],[0]
"A basic challenge in sentiment analysis and emotion recognition is filling the gap between extracted features and the actual affective states (Zhang et al., 2017).",1 Introduction,[0],[0]
"The lack of high-level feature associations is a limitation of traditional approaches using low-level handcrafted features as representations (Seppi et al., 2008; Rozgic et al., 2012).",1 Introduction,[0],[0]
"Recently, deep learning structures such as CNNs and LSTMs have been used to extract high-level features from text and audio (Eyben et al., 2010a; Poria et al., 2015).",1 Introduction,[0],[0]
"However, not all parts of the text and vocal signals contribute equally to the predictions.",1 Introduction,[0],[0]
A specific word may change the entire sentimental state of text; a different vocal delivery may indicate inverse emotions despite having the same linguistic content.,1 Introduction,[0],[0]
"Recent approaches introduce attention mechanisms to focus the models on informative words (Yang et al., 2016) and attentive audio frames (Mirsamadi et al., 2017) for each individual modality.",1 Introduction,[0],[0]
"However, to our knowledge, there is no common multimodal structure with attention for utterancelevel sentiment and emotion classification.",1 Introduction,[0],[0]
"To address such issue, we design a deep hierarchical multimodal architecture with an attention mechanism to classify utterance-level sentiments and emotions.",1 Introduction,[0],[0]
"It extracts high-level informative textual and acoustic features through individual bidirectional gated recurrent units (GRU) and uses a multi-level attention mechanism to select the informative features in both the text and audio module.
",1 Introduction,[0],[0]
Another challenge is the fusion of cues from heterogeneous data.,1 Introduction,[0],[0]
"Most previous works focused on combining multimodal information at a holistic level, such as integrating independent predictions of each modality via algebraic rules (Wöllmer et al., 2013) or fusing the extracted modality-specific features from entire utterances
(Poria et al., 2016).",1 Introduction,[0],[0]
"They extract word-level features in a text branch, but process audio at the frame-level or utterance-level.",1 Introduction,[0],[0]
"These methods fail to properly learn the time-dependent interactions across modalities and restrict feature integration at timestamps due to the different time scales and formats of features of diverse modalities (Poria et al., 2017a).",1 Introduction,[0],[0]
"However, to determine human meaning, it is critical to consider both the linguistic content of the word and how it is uttered.",1 Introduction,[0],[0]
"A loud pitch on different words may convey inverse emotions, such as the emphasis on “hell” for anger but indicating happy on “great”.",1 Introduction,[0],[0]
Synchronized attentive information across text and audio would then intuitively help recognize the sentiments and emotions.,1 Introduction,[0],[0]
"Therefore, we compute a forced alignment between text and audio for each word and propose three fusion approaches (horizontal, vertical, and fine-tuning attention fusion) to integrate both the feature representations and attention at the word-level.
",1 Introduction,[0],[0]
We evaluated our model on four published sentiment and emotion datasets.,1 Introduction,[0],[0]
Experimental results show that the proposed architecture outperforms state-of-the-art approaches.,1 Introduction,[0],[0]
"Our methods also allow for attention visualization, which can be used for interpreting the internal attention distribution for both single- and multi-modal systems.",1 Introduction,[0],[0]
"The contributions of this paper are: (i) a hierarchical multimodal structure with attention mechanism to learn informative features and high-level associations from both text and audio; (ii) three wordlevel fusion strategies to combine features and learn correlations in a common time scale across different modalities; (iii) word-level attention visualization to help human interpretation.
",1 Introduction,[0],[0]
The paper is organized as follows: We list related work in section 2.,1 Introduction,[0],[0]
Section 3 describes the proposed structure in detail.,1 Introduction,[0],[0]
We present the experiments in section 4 and provide the result analysis in section 5.,1 Introduction,[0],[0]
We discuss the limitations in section 6 and conclude with section 7.,1 Introduction,[0],[0]
"Despite the large body of research on audio-visual affective analysis, there is relatively little work on combining text data.",2 Related Work,[0],[0]
"Early work combined human transcribed lexical features and low-level handcrafted acoustic features using feature-level fusion (Forbes-Riley and Litman, 2004; Litman and Forbes-Riley, 2004).",2 Related Work,[0],[0]
"Others used SVMs fed bag
of words (BoW) and part of speech (POS) features in addition to low-level acoustic features (Seppi et al., 2008; Rozgic et al., 2012; Savran et al., 2012; Rosas et al., 2013; Jin et al., 2015).",2 Related Work,[0],[0]
All of the above extracted low-level features from each modality separately.,2 Related Work,[0],[0]
"More recently, deep learning was used to extract higher-level multimodal features.",2 Related Work,[0],[0]
"Bidirectional LSTMs were used to learn long-range dependencies from low-level acoustic descriptors and derivations (LLDs) and visual features (Eyben et al., 2010a; Wöllmer et al., 2013).",2 Related Work,[0],[0]
"CNNs can extract both textual (Poria et al., 2015) and visual features (Poria et al., 2016) for multiple kernel learning of feature-fusion.",2 Related Work,[0],[0]
"Later, hierarchical LSTMs were used (Poria et al., 2017b).",2 Related Work,[0],[0]
"A deep neural network was used for feature-level fusion in (Gu et al., 2018) and (Zadeh et al., 2017) introduced a tensor fusion network to further improve the performance.",2 Related Work,[0],[0]
"A very recent work using word-level fusion was provided by (Chen et al., 2017).",2 Related Work,[0],[0]
"The key differences between this work and the proposed architecture are: (i) we design a fine-tunable hierarchical attention structure to extract word-level features for each individual modality, rather than simply using the initialized textual embedding and extracted LLDs from COVAREP (Degottex et al., 2014); (ii) we propose diverse representation fusion strategies to combine both the word-level representations and attention weights, instead of using only word-level fusion; (iii) our model allows visualizing the attention distribution at both the individual modality and at fusion to help model interpretability.
",2 Related Work,[0],[0]
"Our architecture is inspired by the document classification hierarchical attention structure that works at both the sentence and word level (Yang et al., 2016).",2 Related Work,[0],[0]
"For audio, an attention-based BLSTM and CNN were applied to discovering emotion from frames (Huang and Narayanan, 2016; Neumann and Vu, 2017).",2 Related Work,[0],[0]
"Frame-level weighted-pooling with local attention was shown to outperform frame-wise, final-frame, and framelevel mean-pooling for speech emotion recognition (Mirsamadi et al., 2017).",2 Related Work,[0],[0]
We introduce a multimodal hierarchical attention structure with word-level alignment for sentiment analysis and emotion recognition (Figure 1).,3 Method,[0],[0]
"The model consists of three major parts: text attention module, audio attention module, and word-
level fusion module.",3 Method,[0],[0]
We first make a forced alignment between the text and audio during preprocessing.,3 Method,[0],[0]
"Then, the text attention module and audio attention module extract the features from the corresponding inputs (shown in Algorithm 1).",3 Method,[0],[0]
The word-level fusion module fuses the extracted feature vectors and makes the final prediction via a shared representation (shown in Algorithm 2).,3 Method,[0],[0]
The forced alignment between the audio and text on the word-level prepares the different data for feature extraction.,3.1 Forced Alignment and Preprocessing,[0],[0]
We align the data at the wordlevel because words are the basic unit in English for human speech comprehension.,3.1 Forced Alignment and Preprocessing,[0],[0]
"We used aeneas1 to determine the time interval for each word in the audio file based on the Sakoe-Chiba Band Dynamic Time Warping (DTW) algorithm (Sakoe and Chiba, 1978).
",3.1 Forced Alignment and Preprocessing,[0],[0]
"For the text input, we first embedded the words into 300-dimensional vectors by word2vec (Mikolov et al., 2013), which gives us the best result compared to GloVe and LexVec.",3.1 Forced Alignment and Preprocessing,[0],[0]
Unknown words were randomly initialized.,3.1 Forced Alignment and Preprocessing,[0],[0]
"Given a sentence S with N words, let wi represent the ith word.",3.1 Forced Alignment and Preprocessing,[0],[0]
"We embed the words through the word2vec embedding matrix We by:
Ti =Wewi, i ∈",3.1 Forced Alignment and Preprocessing,[0],[0]
"[1, N ] (1)
where Ti is the embedded word vector.",3.1 Forced Alignment and Preprocessing,[0],[0]
"For the audio input, we extracted Melfrequency spectral coefficients (MFSCs) from raw audio signals as acoustic inputs for two reasons.",3.1 Forced Alignment and Preprocessing,[0],[0]
"Firstly, MFSCs maintain the locality of the data by preventing new bases of spectral energies resulting from discrete cosine transform in MFCCs extraction (Abdel-Hamid et al., 2014).",3.1 Forced Alignment and Preprocessing,[0],[0]
"Secondly, it has more dimensions in the frequency domain that aid learning in deep models (Gu et al., 2017).",3.1 Forced Alignment and Preprocessing,[0],[0]
We used 64 filter banks to extract the MFSCs for each audio frame to form the MFSCs map.,3.1 Forced Alignment and Preprocessing,[0],[0]
"To facilitate training, we only used static coefficients.",3.1 Forced Alignment and Preprocessing,[0],[0]
"Each word’s MFSCs can be represented as a matrix with 64×n dimensions, where n is the interval for the given word in frames.",3.1 Forced Alignment and Preprocessing,[0],[0]
"We zero-pad all intervals to the same length L, the maximum frame numbers of the word in the dataset.",3.1 Forced Alignment and Preprocessing,[0],[0]
"We did extract LLD features using OpenSmile (Eyben et al., 2010b) software and combined them with the MFSCs during our training stage.",3.1 Forced Alignment and Preprocessing,[0],[0]
"However, we did not find an
1https://www.readbeyond.it/aeneas/
obvious performance improvement, especially for the sentiment analysis.",3.1 Forced Alignment and Preprocessing,[0],[0]
"Considering the training cost of the proposed hierarchical acoustic architecture, we decided the extra features were not worth the tradeoff.",3.1 Forced Alignment and Preprocessing,[0],[0]
"The output is a 3D MFSCs map with dimensions [N, 64, L].",3.1 Forced Alignment and Preprocessing,[0],[0]
"To extract features from embedded text input at the word level, we first used bidirectional GRUs, which are able to capture the contextual information between words.",3.2 Text Attention Module,[0],[0]
"It can be represented as:
t h→i , t h ← i = bi GRU(Ti), i ∈",3.2 Text Attention Module,[0],[0]
"[1, N ] (2)
where bi GRU is the bidirectional GRU, t h→i and t h←i denote respectively the forward and backward contextual state of the input text.",3.2 Text Attention Module,[0],[0]
We combined t h→i and t h ← i as t hi to represent the feature vector for the ith word.,3.2 Text Attention Module,[0],[0]
"We choose GRUs instead of LSTMs because our experiments show that LSTMs lead to similar performance (0.07% higher accuracy) with around 25% more trainable parameters.
",3.2 Text Attention Module,[0],[0]
"To create an informative word representation, we adopted a word-level attention strategy that generates a one-dimensional vector denoting the importance for each word in a sequence (Yang et al., 2016).",3.2 Text Attention Module,[0],[0]
"As defined by (Bahdanau et al.,
Algorithm 1 FEATURE EXTRACTION 1: procedure FORCED ALIGNMENT 2:",3.2 Text Attention Module,[0],[0]
Determine time interval of each word 3: find wi←→,3.2 Text Attention Module,[0],[0]
"[Aij], j ∈",3.2 Text Attention Module,[0],[0]
"[1, L], i ∈",3.2 Text Attention Module,[0],[0]
"[1, N ] 4: end procedure 5: procedure TEXT BRANCH 6:",3.2 Text Attention Module,[0],[0]
Text Attention Module 7: for i ∈,3.2 Text Attention Module,[0],[0]
"[1, N ] do 8: Ti ← getEmbedded(wi) 9: t hi ← bi GRU(Ti) 10: t ei ← getEnergies(t hi) 11: t αi ← getDistribution(t ei) 12: end for 13: return t hi, t αi 14: end procedure 15: procedure AUDIO BRANCH 16: for i ∈",3.2 Text Attention Module,[0],[0]
"[1, N ] do 17: Frame-Level Attention Module 18: for j ∈",3.2 Text Attention Module,[0],[0]
"[1, L] do 19: f hij ← bi GRU(Aij) 20: f eij ← getEnergies(f hij) 21:",3.2 Text Attention Module,[0],[0]
f αij ← getDistribution(f eij) 22: end for 23:,3.2 Text Attention Module,[0],[0]
"f Vi ← weightedSum(f αij , f hij) 24: Word-Level Attention Module 25: w hi ← bi GRU(f Vi) 26: w ei ← getEnergies(w hi) 27:",3.2 Text Attention Module,[0],[0]
"w αi ← getDistribution(w ei) 28: end for 29: return w hi, w αi 30: end procedure
2014), we compute the textual attentive energies t ei and textual attention distribution t αi by:
t ei = tanh(Wtt hi + bt), i ∈",3.2 Text Attention Module,[0],[0]
"[1, N ] (3)
t αi",3.2 Text Attention Module,[0],[0]
=,3.2 Text Attention Module,[0],[0]
"exp(t ei >vt)∑N k=1exp(t ek >vt) (4)
where Wt and bt are the trainable parameters and vt is a randomly-initialized word-level weight vector in the text branch.",3.2 Text Attention Module,[0],[0]
"To learn the word-level interactions across modalities, we directly use the textual attention distribution t αi and textual bidirectional contextual state t hi as the output to aid word-level fusion, which allows further computations between text and audio branch on both the contextual states and attention distributions.",3.2 Text Attention Module,[0],[0]
"We designed a hierarchical attention model with frame-level acoustic attention and word-level at-
tention for acoustic feature extraction.",3.3 Audio Attention Module,[0],[0]
Frame-level Attention captures the important MFSC frames from the given word to generate the word-level acoustic vector.,3.3 Audio Attention Module,[0],[0]
"Similar to the text attention module, we used a bidirectional GRU:
f h→ij , f h ← ij = bi GRU(Aij), j ∈",3.3 Audio Attention Module,[0],[0]
"[1, L] (5)
where f h→ij and f h ← ij denote the forward and backward contextual states of acoustic frames.",3.3 Audio Attention Module,[0],[0]
"Aij denotes the MFSCs of the jth frame from the ith word, i ∈",3.3 Audio Attention Module,[0],[0]
"[1, N ].",3.3 Audio Attention Module,[0],[0]
"f hij represents the hidden state of the jth frame of the ith word, which consists of f h→ij and f h ← ij .",3.3 Audio Attention Module,[0],[0]
We apply the same attention mechanism used for textual attention module to extract the informative frames using equation 3 and 4.,3.3 Audio Attention Module,[0],[0]
"As shown in Figure 1, the input of equation 3 is f hij and the output is the framelevel acoustic attentive energies f eij .",3.3 Audio Attention Module,[0],[0]
We calculate the frame-level attention distribution f αij by using f eij as the input for equation 4.,3.3 Audio Attention Module,[0],[0]
We form the word-level acoustic vector f Vi by taking a weighted sum of bidirectional contextual state f hij of the frame and the corresponding framelevel attention distribution f αij,3.3 Audio Attention Module,[0],[0]
"Specifically,
f Vi = ∑
j f αijf hij (6)
Word-level Attention aims to capture the word-level acoustic attention distribution w αi based on formed word vector f Vi.",3.3 Audio Attention Module,[0],[0]
"We first used equation 2 to generate the word-level acoustic contextual states w hi, where the input is f Vi and w hi = (w h→i , w h ← i ).",3.3 Audio Attention Module,[0],[0]
"Then, we compute the word-level acoustic attentive energies w ei via equation 3 as the input for equation 4.",3.3 Audio Attention Module,[0],[0]
The final output is an acoustic attention distribution w αi from equation 4 and acoustic bidirectional contextual state w hi.,3.3 Audio Attention Module,[0],[0]
Fusion is critical to leveraging multimodal features for decision-making.,3.4 Word-level Fusion Module,[0],[0]
Simple feature concatenation without considering the time scales ignores the associations across modalities.,3.4 Word-level Fusion Module,[0],[0]
We introduce word-level fusion capable of associating the text and audio at each word.,3.4 Word-level Fusion Module,[0],[0]
"We propose three fusion strategies (Figure 2 and Algorithm 2): horizontal fusion, vertical fusion, and fine-tuning attention fusion.",3.4 Word-level Fusion Module,[0],[0]
"These methods allow easy synchronization between modalities, taking advantage of the attentive associations across text and audio, creating a shared high-level representation.
",3.4 Word-level Fusion Module,[0],[0]
Algorithm 2 FUSION 1: procedure FUSION BRANCH 2: Horizontal Fusion (HF) 3: for i ∈,3.4 Word-level Fusion Module,[0],[0]
"[1, N ] do 4: t Vi ← weighted(t αi, t hi) 5: w Vi ← weighted(w αi, w hi) 6: Vi ← dense([t Vi, w Vi]) 7: end for 8: Vertical Fusion (VF) 9: for i ∈",3.4 Word-level Fusion Module,[0],[0]
"[1, N ] do 10: hi ← dense([t hi, w hi]) 11: s αi ← average([t αi, w αi]) 12: Vi ← weighted(hi, s αi) 13: end for 14: Fine-tuning Attention Fusion (FAF) 15: for i ∈",3.4 Word-level Fusion Module,[0],[0]
"[1, N ] do 16: u ei ← getEnergies(hi) 17: u αi ← getDistribution(u ei, s αi) 18: Vi ← weighted(hi, u αi) 19: end for 20: Decision Making 21: E ← convNet(V1, V2, ..., VN ) 22: return E 23: end procedure
Horizontal Fusion (HF) provides the shared representation that contains both the textual and acoustic information for a given word (Figure 2 (a)).",3.4 Word-level Fusion Module,[0],[0]
The HF has two steps: (i) combining the bidirectional contextual states (t hi and w hi in Figure 1) and attention distributions for each branch (t αi,3.4 Word-level Fusion Module,[0],[0]
and w αi in Figure 1),3.4 Word-level Fusion Module,[0],[0]
independently to form the word-level textual and acoustic representations.,3.4 Word-level Fusion Module,[0],[0]
"As shown in Figure 2, given the input (t αi,
t hi) and (w αi, w hi), we first weighed each input branch by:
t Vi = t αit hi (7)
",3.4 Word-level Fusion Module,[0],[0]
"w Vi = w αiw hi (8)
where t Vi and w Vi are word-level representations for text and audio branches, respectively; (ii) concatenating them into a single space and further applying a dense layer to create the shared context vector Vi, and Vi = (t Vi, w Vi).",3.4 Word-level Fusion Module,[0],[0]
The HF combines the unimodal contextual states and attention weights; there is no attention interaction between the text modality and audio modality.,3.4 Word-level Fusion Module,[0],[0]
"The shared vectors retain the most significant characteristics from respective branches and encourages the decision making to focus on local informative features.
",3.4 Word-level Fusion Module,[0],[0]
"Vertical Fusion (VF) combines textual attentions and acoustic attentions at the word-level, using a shared attention distribution over both modalities instead of focusing on local informative representations (Figure 2 (b)).",3.4 Word-level Fusion Module,[0],[0]
"The VF is computed in three steps: (i) using a dense layer after the concatenation of the word-level textual (t hi) and acoustic (w hi) bidirectional contextual states to form the shared contextual state hi; (ii) averaging the textual (t αi) and acoustic (w αi) attentions for each word as the shared attention distribution s αi; (iii) computing the weight of hi and s αi as final shared context vectors Vi, where Vi = his αi.",3.4 Word-level Fusion Module,[0],[0]
"Because the shared attention distribution (s αi) is based on averages of unimodal attentions, it is a joint attention of both textual and acoustic attentive information.
",3.4 Word-level Fusion Module,[0],[0]
"Fine-tuning Attention Fusion (FAF) preserves the original unimodal attentions and provides
a fine-tuning attention for the final prediction (Figure2 (c)).",3.4 Word-level Fusion Module,[0],[0]
The averaging of attention weights in vertical fusion potentially limits the representational power.,3.4 Word-level Fusion Module,[0],[0]
"Addressing such issue, we propose a trainable attention layer to tune the shared attention in three steps: (i) computing the shared attention distribution s αi and shared bidirectional contextual states hi separately using the same approach as in vertical fusion; (ii) applying attention fine-tuning:
u ei = tanh(Wuhi + bu) (9)
u αi = exp(u ei >vu)∑N k=1exp(u ek >vu) +",3.4 Word-level Fusion Module,[0],[0]
"s αi (10)
where Wu, bu, and vu are additional trainable parameters.",3.4 Word-level Fusion Module,[0],[0]
The u αi can be understood as the sum of the fine-tuning score and the original shared attention distribution s αi; (iii) calculating the weight of u αi and hi to form the final shared context vector Vi.,3.4 Word-level Fusion Module,[0],[0]
The output of the fusion layer Vi is the ith shared word-level vectors.,3.5 Decision Making,[0],[0]
"To further make use of the combined features for classification, we applied a CNN structure with one convolutional layer and one max-pooling layer to extract the final representation from shared word-level vectors (Poria et al., 2016; Wang et al., 2016).",3.5 Decision Making,[0],[0]
"We set up various widths for the convolutional filters (Kim, 2014) and generated a feature map ck by:
fi = tanh(WcVi:i+k−1 + bc) (11)
ck = max{f1, f2, ..., fN} (12)
where k is the width of the convolutional filters, fi represents the features from window i to i+k−1.",3.5 Decision Making,[0],[0]
Wc and bc are the trainable weights and biases.,3.5 Decision Making,[0],[0]
We get the final representation c by concatenating all the feature maps.,3.5 Decision Making,[0],[0]
A softmax function is used for the final classification.,3.5 Decision Making,[0],[0]
"We evaluated our model on four published datasets: two multimodal sentiment datasets (MOSI and YouTube) and two multimodal emotion recognition datasets (IEMOCAP and EmotiW).
",4.1 Datasets,[0],[0]
"MOSI dataset is a multimodal sentiment intensity and subjectivity dataset consisting of 93 reviews with 2199 utterance segments (Zadeh et al., 2016).",4.1 Datasets,[0],[0]
Each segment was labeled by five individual annotators between -3 (strong negative) to +3 (strong positive).,4.1 Datasets,[0],[0]
"We used binary labels based on the sign of the annotations’ average.
",4.1 Datasets,[0],[0]
"YouTube dataset is an English multimodal dataset that contains 262 positive, 212 negative, and 133 neutral utterance-level clips provided by (Morency et al., 2011).",4.1 Datasets,[0],[0]
"We only consider the positive and negative labels during our experiments.
",4.1 Datasets,[0],[0]
"IEMOCAP is a multimodal emotion dataset including visual, audio, and text data (Busso et al., 2008).",4.1 Datasets,[0],[0]
"For each sentence, we used the label agreed on by the majority (at least two of the three annotators).",4.1 Datasets,[0],[0]
"In this study, we evaluate both the 4- catgeory (happy+excited, sad, anger, and neutral) and 5-catgeory(happy+excited, sad, anger, neutral, and frustration) emotion classification problems.",4.1 Datasets,[0],[0]
"The final dataset consists of 586 happy, 1005 excited, 1054 sad, 1076 anger, 1677 neutral, and 1806 frustration.
",4.1 Datasets,[0],[0]
EmotiW2 is an audio-visual multimodal utterance-level emotion recognition dataset consist of video clips.,4.1 Datasets,[0],[0]
"To keep the consistency with the IEMOCAP dataset, we used four emotion categories as the final dataset including 150 happy, 117 sad, 133 anger, and 144 neutral.",4.1 Datasets,[0],[0]
We used IBM Watson3 speech to text software to transcribe the audio data into text.,4.1 Datasets,[0],[0]
We compared the proposed architecture to published models.,4.2 Baselines,[0],[0]
"Because our model focuses on extracting sentiment and emotions from human speech, we only considered the audio and text branch applied in the previous studies.",4.2 Baselines,[0],[0]
BL-SVM extracts a bag-of-words as textual features and low-level descriptors as acoustic features.,4.2.1 Sentiment Analysis Baselines,[0],[0]
"An SVM structure is used to classify the sentiments (Rosas et al., 2013).
",4.2.1 Sentiment Analysis Baselines,[0],[0]
LSTM-SVM uses LLDs as acoustic features and bag-of-n-grams (BoNGs) as textual features.,4.2.1 Sentiment Analysis Baselines,[0],[0]
"The final estimate is based on decision-level fusion of text and audio predictions (Wöllmer et al., 2013).
",4.2.1 Sentiment Analysis Baselines,[0],[0]
"2https://cs.anu.edu.au/few/ChallengeDetails.html 3https://www.ibm.com/watson/developercloud/speech-
to-text/api/v1/
C-MKL1 uses a CNN structure to capture the textual features and fuses them via multiple kernel learning for sentiment analysis (Poria et al., 2015).
",4.2.1 Sentiment Analysis Baselines,[0],[0]
"TFN uses a tensor fusion network to extract interactions between different modality-specific features (Zadeh et al., 2017).
LSTM(A) introduces a word-level LSTM with temporal attention structure to predict sentiments on MOSI dataset (Chen et al., 2017).",4.2.1 Sentiment Analysis Baselines,[0],[0]
SVM Trees extracts LLDs and handcrafted bagof-words as features.,4.2.2 Emotion Recognition Baselines,[0],[0]
"The model automatically generates an ensemble of SVM trees for emotion classification (Rozgic et al., 2012).
",4.2.2 Emotion Recognition Baselines,[0],[0]
GSV-eVector generates new acoustic representations from selected LLDs using Gaussian Supervectors and extracts a set of weighed handcrafted textual features as an eVector.,4.2.2 Emotion Recognition Baselines,[0],[0]
"A linear kernel SVM is used as the final classifier (Jin et al., 2015).
",4.2.2 Emotion Recognition Baselines,[0],[0]
C-MKL2 extracts textual features using a CNN and uses openSMILE to extract 6373 acoustic features.,4.2.2 Emotion Recognition Baselines,[0],[0]
"Multiple kernel learning is used as the final classifier (Poria et al., 2016).
",4.2.2 Emotion Recognition Baselines,[0],[0]
H-DMS uses a hybrid deep multimodal structure to extract both the text and audio emotional features.,4.2.2 Emotion Recognition Baselines,[0],[0]
"A deep neural network is used for feature-level fusion (Gu et al., 2018).",4.2.2 Emotion Recognition Baselines,[0],[0]
"Utterance-level Fusion (UL-Fusion) focuses on fusing text and audio features from an entire utterance (Gu et al., 2017).",4.2.3 Fusion Baselines,[0],[0]
We simply concatenate the textual and acoustic representations into a joint feature representation.,4.2.3 Fusion Baselines,[0],[0]
"A softmax function is used for sentiment and emotion classification.
",4.2.3 Fusion Baselines,[0],[0]
"Decision-level Fusion (DL-Fusion) Inspired by (Wöllmer et al., 2013), we extract textual and
acoustic sentence representations individually and infer the results via two softmax classifiers, respectively.",4.2.3 Fusion Baselines,[0],[0]
"As suggested by Wöllmer, we calculate a weighted sum of the text (1.2) result and audio (0.8) result as the final prediction.",4.2.3 Fusion Baselines,[0],[0]
We implemented the model in Keras with Tensorflow as the backend.,4.3 Model Training,[0],[0]
"We set 100 as the dimension for each GRU, meaning the bidirectional GRU dimension is 200.",4.3 Model Training,[0],[0]
"For the decision making, we selected 2, 3, 4, and 5 as the filter width and apply 300 filters for each width.",4.3 Model Training,[0],[0]
We used the rectified linear unit (ReLU) activation function and set 0.5 as the dropout rate.,4.3 Model Training,[0],[0]
"We also applied batch normalization functions between each layer to overcome internal covariate shift (Ioffe and Szegedy, 2015).",4.3 Model Training,[0],[0]
We first trained the text attention module and audio attention module individually.,4.3 Model Training,[0],[0]
"Then, we tuned the fusion network based on the word-level representation outputs from each fine-tuning module.",4.3 Model Training,[0],[0]
"For all training procedures, we set the learning rate to 0.001 and used Adam optimization and categorical cross-entropy loss.",4.3 Model Training,[0],[0]
"For all datasets, we considered the speakers independent and used an 80-20 training-testing split.",4.3 Model Training,[0],[0]
We further separated 20% from the training dataset for validation.,4.3 Model Training,[0],[0]
We trained the model with 5-fold cross validation and used 8 as the mini batch size.,4.3 Model Training,[0],[0]
We set the same amount of samples from each class to balance the training dataset during each iteration.,4.3 Model Training,[0],[0]
"The experimental results of different datasets show that our proposed architecture achieves state-of-the-art performance in both sentiment
analysis and emotion recognition (Table 1).",5.1 Comparison with Baselines,[0],[0]
"We re-implemented some published methods (Rosas et al., 2013; Wöllmer et al., 2013) on MOSI to get baselines.
",5.1 Comparison with Baselines,[0],[0]
"For sentiment analysis, the proposed architecture with FAF strategy achieves 76.4% weighted accuracy, which outperforms all the five baselines (Table 1).",5.1 Comparison with Baselines,[0],[0]
The result demonstrates that the proposed hierarchical attention architecture and word-level fusion strategies indeed help improve the performance.,5.1 Comparison with Baselines,[0],[0]
"There are several findings worth mentioning: (i) our model outperforms the baselines without using the low-level handcrafted acoustic features, indicating the sufficiency of MFSCs; (ii) the proposed approach achieves performance comparable to the model using text, audio, and visual data together (Zadeh et al., 2017).",5.1 Comparison with Baselines,[0],[0]
"This demonstrates that the visual features do not contribute as much during the fusion and prediction on MOSI; (iii) we notice that (Poria et al., 2017b) reports better accuracy (79.3%) on MOSI, but their model uses a set of utterances instead of a single utterance as input.
",5.1 Comparison with Baselines,[0],[0]
"For emotion recognition, our model with FAF achieves 72.7% accuracy, outperforming all the baselines.",5.1 Comparison with Baselines,[0],[0]
"The result shows the proposed model brings a significant accuracy gain to emotion recognition, demonstrating the pros of the finetuning attention structure.",5.1 Comparison with Baselines,[0],[0]
It also shows that wordlevel attention indeed helps extract emotional features.,5.1 Comparison with Baselines,[0],[0]
"Compared to C-MKL2 and SVM Trees that require feature selection before fusion and prediction, our model does not need an additional architecture to select features.",5.1 Comparison with Baselines,[0],[0]
"We further evaluated our models on 5 emotion categories, including frustration.",5.1 Comparison with Baselines,[0],[0]
Our model shows 4.2% performance improvement over H-DMS and achieves 0.644 weighted-F1.,5.1 Comparison with Baselines,[0],[0]
"As H-DMS only achieves 0.594 F1 and also uses low-level handcrafted features, our model is more robust and efficient.
",5.1 Comparison with Baselines,[0],[0]
"From Table 1, all the three proposed fusion strategies outperform UL-Fusion and DL-Fusion on both MOSI and IEMOCAP.",5.1 Comparison with Baselines,[0],[0]
"Unlike utterancelevel fusion that ignores the time-scale-sensitive associations across modalities, word-level fusion combines the modality-specific features for each word by aligning text and audio, allowing associative learning between the two modalities, similar to what humans do in natural conversation.",5.1 Comparison with Baselines,[0],[0]
"The result indicates that the proposed methods improve the model performance by around 6% accu-
racy.",5.1 Comparison with Baselines,[0],[0]
"We also notice that the structure with FAF outperforms the HF and VF on both MOSI and IEMOCAP dataset, which demonstrates the effectiveness and importance of the FAF strategy.",5.1 Comparison with Baselines,[0],[0]
"From Table 2, we see that textual information dominates the sentiment prediction on MOSI and there is an only 1.4% accuracy improvement from fusing text and audio.",5.2 Modality and Generalization Analysis,[0],[0]
"However, on IEMOCAP, audio-only outperforms text-only, but as expected, there is a significant performance improvement by combining textual and audio.",5.2 Modality and Generalization Analysis,[0],[0]
"The difference in modality performance might because of the more significant role vocal delivery plays in emotional expression than in sentimental expression.
",5.2 Modality and Generalization Analysis,[0],[0]
We further tested the generalizability of the proposed model.,5.2 Modality and Generalization Analysis,[0],[0]
"For sentiment generalization testing, we trained the model on MOSI and tested on the YouTube dataset (Table 3), which achieves 66.2% accuracy and 0.665 F1 scores.",5.2 Modality and Generalization Analysis,[0],[0]
"For emotion recognition generalization testing, we tested the model (trained on IEMOCAP) on EmotiW and achieves 61.4% accuracy.",5.2 Modality and Generalization Analysis,[0],[0]
"The potential reasons that may influence the generalization are: (i) the biased labeling for different datasets (five annotators of MOSI vs one annotator of Youtube); (ii) incomplete utterance in YouTube dataset (such as “about”, “he”, etc.); (iii) without enough speech information (EmotiW is a wild audiovisual dataset that focuses on facial expression).",5.2 Modality and Generalization Analysis,[0],[0]
"Our model allows us to easily visualize the attention weights of text, audio, and fusion to better understand how the attention mechanism works.",5.3 Visualize Attentions,[0],[0]
"We introduce the emotional distribution visualizations for word-level acoustic attention (w αi), word-level textual attention (t αi), shared attention (s αi), and fine-tuning attention based on the FAF structure (u αi) for two example sentences (Figure 3).",5.3 Visualize Attentions,[0],[0]
"The color gradation represents the importance of the corresponding source data at the word-level.
",5.3 Visualize Attentions,[0],[0]
"Based on our visualization, the textual attention distribution (t αi) denotes the words that carry the most emotional significance, such as “hell” for anger (Figure 3 a).",5.3 Visualize Attentions,[0],[0]
"The textual attention shows that “don’t”, “like”, and “west-sider” have similar weights in the happy example (Figure 3 b).",5.3 Visualize Attentions,[0],[0]
It is hard to assign this sentence happy given only the text attention.,5.3 Visualize Attentions,[0],[0]
"However, the acoustic attention focuses on “you’re” and “west-sider”, removing emphasis from “don’t” and “like”.",5.3 Visualize Attentions,[0],[0]
"The shared attention (s αi) and fine-tuning attention (u αi) successfully combine both textual and acoustic attentions and assign joint attention to the correct words, which demonstrates that the proposed method can capture emphasis from both modalities at the word-level.",5.3 Visualize Attentions,[0],[0]
There are several limitations and potential solutions worth mentioning: (i) the proposed architecture uses both the audio and text data to analyze the sentiments and emotions.,6 Discussion,[0],[0]
"However, not all the data sources contain or provide textual information.",6 Discussion,[0],[0]
Many audio-visual emotion clips only have acoustic and visual information.,6 Discussion,[0],[0]
The proposed architecture is more related to spoken language analysis than predicting the sentiments or emotions based on human speech.,6 Discussion,[0],[0]
Automatic speech recognition provides a potential solution for generating the textual information from vocal signals.,6 Discussion,[0],[0]
"(ii)
",6 Discussion,[0],[0]
The word alignment can be easily applied to human speech.,6 Discussion,[0],[0]
"However, it is difficult to align the visual information with text, especially if the text only describes the video or audio.",6 Discussion,[0],[0]
Incorporating visual information into an aligning model like ours would be an interesting research topic.,6 Discussion,[0],[0]
"(iii) The limited amount of multimodal sentiment analysis and emotion recognition data is a key issue for current research, especially for deep models that require a large number of samples.",6 Discussion,[0],[0]
"Compared large unimodal sentiment analysis and emotion recognition datasets, the MOSI dataset only consists of 2199 sentence-level samples.",6 Discussion,[0],[0]
"In our experiments, the EmotiW and MOUD datasets could only be used for generalization analysis due to their small size.",6 Discussion,[0],[0]
Larger and more general datasets are necessary for multimodal sentiment analysis and emotion recognition in the future.,6 Discussion,[0],[0]
"In this paper, we proposed a deep multimodal architecture with hierarchical attention for sentiment and emotion classification.",7 Conclusion,[0],[0]
"Our model aligned the text and audio at the word-level and applied attention distributions on textual word vectors, acoustic frame vectors, and acoustic word vectors.",7 Conclusion,[0],[0]
We introduced three fusion strategies with a CNN structure to combine word-level features to classify emotions.,7 Conclusion,[0],[0]
Our model outperforms the state-ofthe-art methods and provides effective visualization of modality-specific features and fusion feature interpretation.,7 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their valuable comments and feedback.,Acknowledgments,[0],[0]
We thank the useful suggestions from Kaixiang Huang.,Acknowledgments,[0],[0]
This research was funded by the National Institutes of Health under Award Number R01LM011834.,Acknowledgments,[0],[0]
"Multimodal affective computing, learning to recognize and interpret human affect and subjective information from multiple data sources, is still challenging because: (i) it is hard to extract informative features to represent human affects from heterogeneous inputs; (ii) current fusion strategies only fuse different modalities at abstract levels, ignoring time-dependent interactions between modalities.",abstractText,[0],[0]
"Addressing such issues, we introduce a hierarchical multimodal architecture with attention and word-level fusion to classify utterancelevel sentiment and emotion from text and audio data.",abstractText,[0],[0]
"Our introduced model outperforms state-of-the-art approaches on published datasets, and we demonstrate that our model’s synchronized attention over modalities offers visual interpretability.",abstractText,[0],[0]
Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 679–686 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics
Emojis are small images that are commonly included in social media text messages. The combination of visual and textual content in the same message builds up a modern way of communication, that automatic systems are not used to deal with. In this paper we extend recent advances in emoji prediction by putting forward a multimodal approach that is able to predict emojis in Instagram posts. Instagram posts are composed of pictures together with texts which sometimes include emojis. We show that these emojis can be predicted by using the text, but also using the picture. Our main finding is that incorporating the two synergistic modalities, in a combined model, improves accuracy in an emoji prediction task. This result demonstrates that these two modalities (text and images) encode different information on the use of emojis and therefore can complement each other.",text,[0],[0]
"In the past few years the use of emojis in social media has increased exponentially, changing the way we communicate.",1 Introduction,[0],[0]
The combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.,1 Introduction,[0],[0]
"Recent work (Barbieri et al., 2017) has shown that textual information can be used to predict emojis associated to text.",1 Introduction,[0],[0]
"In this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in order to obtain more accurate emojiprediction models.
",1 Introduction,[0],[0]
We explore the use of emojis in the social media platform Instagram.,1 Introduction,[0],[0]
"We put forward a multimodal approach to predict the emojis associated to an In-
stagram post, given its picture and text1.",1 Introduction,[0],[0]
"Our task and experimental framework are similar to (Barbieri et al., 2017), however, we use different data (Instagram instead of Twitter) and, in addition, we rely on images to improve the selection of the most likely emojis to associate to a post.",1 Introduction,[0],[0]
We show that a multimodal approach (textual and visual content of the posts) increases the emoji prediction accuracy compared to the one that only uses textual information.,1 Introduction,[0],[0]
"This suggests that textual and visual content embed different but complementary features of the use of emojis.
",1 Introduction,[0],[0]
"In general, an effective approach to predict the emoji to be associated to a piece of content may help to improve natural language processing tasks (Novak et al., 2015), such as information retrieval, generation of emoji-enriched social media content, suggestion of emojis when writing text messages or sharing pictures online.",1 Introduction,[0],[0]
"Given that emojis may also mislead humans (Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding.",1 Introduction,[0],[0]
"As a consequence, by modeling the semantics of emojis, we can improve highly-subjective tasks like sentiment analysis, emotion recognition and irony detection (Felbo et al., 2017).",1 Introduction,[0],[0]
Dataset:,2 Dataset and Task,[0],[0]
"We gathered Instagram posts published between July 2016 and October 2016, and geolocalized in the United States of America.",2 Dataset and Task,[0],[0]
"We considered only posts that contained a photo together with the related user description of at least 4 words and exactly one emoji.
",2 Dataset and Task,[0],[0]
"Moreover, as done by Barbieri et al. (2017), we considered only the posts which include one and only one of the 20 most frequent emojis (the
1In this paper we only utilize the first comment issued by the user who posted the picture.
679
most frequent emojis are shown in Table 3).",2 Dataset and Task,[0],[0]
"Our dataset is composed of 299,809 posts, each containing a picture, the text associated to it and only one emoji.",2 Dataset and Task,[0],[0]
"In the experiments we also considered the subsets of the 10 (238,646 posts) and 5 most frequent emojis (184,044 posts) (similarly to the approach followed by Barbieri et al. (2017)).
",2 Dataset and Task,[0],[0]
"Task: We extend the experimental scheme of Barbieri et al. (2017), by considering also visual information when modeling posts.",2 Dataset and Task,[0],[0]
We cast the emoji prediction problem as a classification task: given an image or a text (or both inputs in the multimodal scenario) we select the most likely emoji that could be added to (thus used to label) such contents.,2 Dataset and Task,[0],[0]
"The task for our machine learning models is, given the visual and textual content of a post, to predict the single emoji that appears in the input comment.",2 Dataset and Task,[0],[0]
We present and motivate the models that we use to predict an emoji given an Instagram post composed by a picture and the associated comment.,3 Models,[0],[0]
"Deep Residual Networks (ResNets) (He et al., 2016) are Convolutional Neural Networks which were competitive in several image classification tasks (Russakovsky et al., 2015; Lin et al., 2014) and showed to be one of the best CNN architectures for image recognition.",3.1 ResNets,[0],[0]
"ResNet is a feedforward CNN that exploits “residual learning”, by bypassing two or more convolution layers (like similar previous approaches (Sermanet and LeCun, 2011)).",3.1 ResNets,[0],[0]
"We use an implementation of the original ResNet where the scale and aspect ratio augmentation are from (Szegedy et al., 2015), the photometric distortions from (Howard, 2013) and weight decay is applied to all weights and biases (instead of only weights of the convolution layers).",3.1 ResNets,[0],[0]
"The network we used is composed of 101 layers (ResNet-101), initialized with pretrained parameters learned on ImageNet (Deng et al., 2009).",3.1 ResNets,[0],[0]
We use this model as a starting point to later finetune it on our emoji classification task.,3.1 ResNets,[0],[0]
Learning rate was set to 0.0001 and we early stopped the training when there was not improving in the validation set.,3.1 ResNets,[0],[0]
"Fastext (Joulin et al., 2017) is a linear model for text classification.",3.2 FastText,[0],[0]
"We decided to employ FastText as it has been shown that on specific classification tasks, it can achieve competitive results, comparable to complex neural classifiers (RNNs and CNNs), while being much faster.",3.2 FastText,[0],[0]
"FastText represents a valid approach when dealing with social media content classification, where huge amounts of data needs to be processed and new and relevant information is continuously generated.",3.2 FastText,[0],[0]
"The FastText algorithm is similar to the CBOW algorithm (Mikolov et al., 2013), where the middle word is replaced by the label, in our case the emoji.",3.2 FastText,[0],[0]
"Given a set of N documents, the loss that the model attempts to minimize is the negative log-likelihood over the labels (in our case, the emojis):
loss = − 1 N
n=1∑
N
en log(softmax (BAxn))
where en is the emoji included in the n-th Instagram post, represented as hot vector, and used as label.",3.2 FastText,[0],[0]
"A and B are affine transformations (weight matrices), and xn is the unit vector of the bag of features of the n-th document (comment).",3.2 FastText,[0],[0]
"The bag of features is the average of the input words, represented as vectors with a look-up table.",3.2 FastText,[0],[0]
Barbieri et al. (2017) propose a recurrent neural network approach for the emoji prediction task.,3.3 B-LSTM Baseline,[0],[0]
"We use this model as baseline, to verify whether FastText achieves comparable performance.",3.3 B-LSTM Baseline,[0],[0]
"They used a Bidirectional LSTM with character representation of the words (Ling et al., 2015; Ballesteros et al., 2015) to handle orthographic variants (or even spelling errors) of the same word that occur in social media (e.g. cooooool vs cool).",3.3 B-LSTM Baseline,[0],[0]
"In order to study the relation between Instagram posts and emojis, we performed two different experiments.",4 Experiments and Evaluation,[0],[0]
In the first experiment (Section 4.2) we compare the FastText model with the state of the art on emoji classification (B-LSTM) by Barbieri et al. (2017).,4 Experiments and Evaluation,[0],[0]
Our second experiment (Section 4.3) evaluates the visual (ResNet) and textual (FastText) models on the emoji prediction task.,4 Experiments and Evaluation,[0],[0]
"Moreover, we evaluate a multimodal combination of both models respectively based on visual and
textual inputs.",4 Experiments and Evaluation,[0],[0]
"Finally we discuss the contribution of each modality to the prediction task.
",4 Experiments and Evaluation,[0],[0]
"We use 80% of our dataset (introduced in Section 2) for training, 10% to tune our models, and 10% for testing (selecting the sets randomly).",4 Experiments and Evaluation,[0],[0]
"To model visual features we first finetune the ResNet (process described in Section 3.1) on the emoji prediction task, then extract the vectors from the input of the last fully connected layer (before the softmax).",4.1 Feature Extraction and Classifier,[0],[0]
"The textual embeddings are the bag of features shown in Section 3.2 (the xn vectors), extracted after training the FastText model on the emoji prediction task.
",4.1 Feature Extraction and Classifier,[0],[0]
"With respect to the combination of textual and visual modalities, we adopt a middle fusion approach (Kiela and Clark, 2015): we associate to each Instagram post a multimodal embedding obtained by concatenating the unimodal representations of the same post (i.e. the visual and textual embeddings), previously learned.",4.1 Feature Extraction and Classifier,[0],[0]
"Then, we feed a classifier2 with visual (ResNet), textual (FastText), or multimodal feature embeddings, and test the accuracy of the three systems.",4.1 Feature Extraction and Classifier,[0],[0]
"To compare the FastText model with the word and character based B-LSTMs presented by Barbieri et al. (2017), we consider the same three emoji prediction tasks they proposed: top-5, top-10 and top-20 emojis most frequently used in their Tweet datasets.",4.2 B-LSTM / FastText Comparison,[0],[0]
In this comparison we used the same Twitter datasets.,4.2 B-LSTM / FastText Comparison,[0],[0]
"As we can see in Table 1 FastText model is competitive, and it is also able to outperform the character based B-LSTM in one of the emoji prediction tasks (top-20 emojis).",4.2 B-LSTM / FastText Comparison,[0],[0]
"This result suggests that we can employ FastText to represent Social Media short text (such as Twitter or Instragram) with reasonable accuracy.
",4.2 B-LSTM / FastText Comparison,[0],[0]
2L2 regularized logistic regression,4.2 B-LSTM / FastText Comparison,[0],[0]
"We present the results of the three emoji classification tasks, using the visual, textual and multimodal features (see Table 2).
",4.3 Multimodal Emoji Prediction,[0],[0]
"The emoji prediction task seems difficult by just using the image of the Instagram post (Visual), even if it largely outperforms the majority baseline3 and weighted random4.",4.3 Multimodal Emoji Prediction,[0],[0]
We achieve better performances when we use feature embeddings extracted from the text.,4.3 Multimodal Emoji Prediction,[0],[0]
"The most interesting finding is that when we use a multimodal combination of visual and textual features, we get a nonnegligible improvement.",4.3 Multimodal Emoji Prediction,[0],[0]
"This suggests that these two modalities embed different representations of the posts, and when used in combination they are synergistic.",4.3 Multimodal Emoji Prediction,[0],[0]
"It is also interesting to note that the more emojis to predict, the higher improvement the multimodal system provides over the text only system (3.28% for top-5 emojis, 7.31% for top-10 emojis, and 13.42 for the top-20 emojis task).",4.3 Multimodal Emoji Prediction,[0],[0]
"In Table 3 we show the results for each class in the top-20 emojis task.
",4.4 Qualitative Analysis,[0],[0]
The emoji with highest F1 using the textual features is the most frequent one (0.62) and the US flag (0.52).,4.4 Qualitative Analysis,[0],[0]
"The latter seems easy to predict since it appears in specific contexts: when the word USA/America is used (or when American cities are referred, like #NYC).
",4.4 Qualitative Analysis,[0],[0]
The hardest emojis to predict by the text only system are the two gestures (0.12) and (0.13).,4.4 Qualitative Analysis,[0],[0]
"The first one is often selected when the gold stan-
3Always predict since it is the most frequent emoji.",4.4 Qualitative Analysis,[0],[0]
"4Random keeping labels distribution of the training set
dard emoji is the second one or is often mispredicted by wrongly selecting or .
",4.4 Qualitative Analysis,[0],[0]
Another relevant confusion scenario related to emoji prediction has been spotted by Barbieri et al. (2017): relying on Twitter textual data they showed that the emoji was hard to predict as it was used similarly to .,4.4 Qualitative Analysis,[0],[0]
"Instead when we consider Instagram data, the emoji is easier to predict (0.23), even if it is often confused with .
",4.4 Qualitative Analysis,[0],[0]
"When we rely on visual contents (Instagram picture), the emojis which are easily predicted are the ones in which the associated photos are similar.",4.4 Qualitative Analysis,[0],[0]
"For instance, most of the pictures associated to
are dog/pet pictures.",4.4 Qualitative Analysis,[0],[0]
"Similarly, is predicted along with very bright pictures taken outside.",4.4 Qualitative Analysis,[0],[0]
is correctly predicted along with pictures related to gym and fitness.,4.4 Qualitative Analysis,[0],[0]
"The accuracy of is also high since most posts including this emoji are related to fitness (and the pictures are simply either selfies at the gym, weight lifting images, or protein food).
",4.4 Qualitative Analysis,[0],[0]
Employing a multimodal approach improves performance.,4.4 Qualitative Analysis,[0],[0]
"This means that the two modalities are somehow complementary, and adding visual information helps to solve potential ambiguities that arise when relying only on textual content.",4.4 Qualitative Analysis,[0],[0]
In Figure 1 we report the confusion matrix of the multimodal model.,4.4 Qualitative Analysis,[0],[0]
"The emojis are plotted from the most frequent to the least, and we can see that the model tends to mispredict emojis selecting more frequent emojis (the left part of the matrix is brighter).",4.4 Qualitative Analysis,[0],[0]
"In order to show the parts of the image most relevant for each class we analyze the global average pooling (Lin et al., 2013) on the convolutional
feature maps (Zhou et al., 2016).",4.4.1 Saliency Maps,[0],[0]
By visually observing the image heatmaps of the set of Instagram post pictures we note that in most cases it is quite difficult to determine a clear association between the emoji used by the user and some particular portion of the image.,4.4.1 Saliency Maps,[0],[0]
"Detecting the correct emoji given an image is harder than a simple object recognition task, as the emoji choice depends on subjective emotions of the user who posted the image.",4.4.1 Saliency Maps,[0],[0]
"In Figure 2 we show the first four predictions of the CNN for three pictures, and where the network focuses (in red).",4.4.1 Saliency Maps,[0],[0]
"We can see that in the first example the network selects the smile with sunglasses because of the legs in the bottom of the image, the dog emoji is selected while focusing on the dog in the image, and the smiling emoji while focusing on the person in the back, who is lying on a hammock.",4.4.1 Saliency Maps,[0],[0]
"In the second example the network selects again the due to the water and part of the kayak, the heart emoji focusing on the city landscape, and the praying emoji
focusing on the sky.",4.4.1 Saliency Maps,[0],[0]
"The same “praying” emoji is also selected when focusing on the luxury car in the third example, probably because the same emoji is used to express desire, i.e. “please, I want this awesome car”.
",4.4.1 Saliency Maps,[0],[0]
"It is interesting to note that images can give context to textual messages like in the following Instagram posts: (1)“Love my new home ” (associated to a picture of a bright garden, outside) and (2) “I can’t believe it’s the first day of school!!!
",4.4.1 Saliency Maps,[0],[0]
I love being these boys’ mommy!!!!,4.4.1 Saliency Maps,[0],[0]
#myboys #mommy ” (associated to picture of two boys wearing two blue shirts).,4.4.1 Saliency Maps,[0],[0]
In both examples the textual system predicts .,4.4.1 Saliency Maps,[0],[0]
"While the multimodal system correctly predicts both of them: the blue color in the picture associated to (2) helps to change the color of the heart, and the sunny/bright picture of the garden in (1) helps to correctly predict .",4.4.1 Saliency Maps,[0],[0]
"Modeling the semantics of emojis, and their applications, is a relatively novel research problem with direct applications in any social media task.",5 Related Work,[0],[0]
"Since emojis do not have a clear grammar, it is not clear their role in text messages.",5 Related Work,[0],[0]
"Emojis are considered function words or even affective markers (Na’aman et al., 2017), that can potentially affect the overall semantics of a message (Donato and Paggio, 2017).
",5 Related Work,[0],[0]
"Emojis can encode different meanings, and they can be interpreted differently.",5 Related Work,[0],[0]
"Emoji interpretation has been explored user-wise (Miller et al., 2017), location-wise, specifically in countries (Barbieri et al., 2016b) and cities (Barbieri et al., 2016a), and gender-wise (Chen et al., 2017) and time-wise (Barbieri et al., 2018).
",5 Related Work,[0],[0]
"Emoji sematics and usage have been studied with distributional semantics, with models trained on Twitter data (Barbieri et al., 2016c), Twitter data together with the official unicode description (Eisner et al., 2016), or using text from a popular keyboard app Ai et al. (2017).",5 Related Work,[0],[0]
"In the same
context, Wijeratne et al. (2017a) propose a platform for exploring emoji semantics.",5 Related Work,[0],[0]
"In order to further study emoji semantics, two datasets with pairwise emoji similarity, with human annotations, have been proposed: EmoTwi50 (Barbieri et al., 2016c) and EmoSim508 (Wijeratne et al., 2017b).",5 Related Work,[0],[0]
"Emoji similarity has been also used for proposing efficient keyboard emoji organization (Pohl et al., 2017).",5 Related Work,[0],[0]
"Recently, Barbieri and Camacho-Collados (2018) show that emoji modifiers (skin tones and gender) can affect the semantics vector representation of emojis.
",5 Related Work,[0],[0]
Emoji play an important role in the emotional content of a message.,5 Related Work,[0],[0]
"Several sentiment lexicons for emojis have been proposed (Novak et al., 2015; Kimura and Katsurai, 2017; Rodrigues et al., 2018) and also studies in the context of emotion and emojis have been published recently (Wood and Ruder, 2016; Hu et al., 2017).
",5 Related Work,[0],[0]
"During the last decade several studies have shown how sentiment analysis improves when we jointly leverage information coming from different modalities (e.g. text, images, audio, video) (Morency et al., 2011; Poria et al., 2015; Tran and Cambria, 2018).",5 Related Work,[0],[0]
"In particular, when we deal with Social Media posts, the presence of both textual and visual content has promoted a number of investigations on sentiment or emotions (Baecchi et al., 2016; You et al., 2016b,a; Yu et al., 2016; Chen et al., 2015) or emojis (Cappallo et al., 2015, 2018).",5 Related Work,[0],[0]
In this work we explored the use of emojis in a multimodal context (Instagram posts).,6 Conclusions,[0],[0]
"We have shown that using a synergistic approach, thus relying on both textual and visual contents of social media posts, we can outperform state of the art unimodal approaches (based only on textual contents).",6 Conclusions,[0],[0]
"As future work, we plan to extend our models by considering the prediction of more than one emoji per Social Media post and also considering a bigger number of labels.",6 Conclusions,[0],[0]
We thank the anonymous reviewers for their important suggestions.,Acknowledgments,[0],[0]
"Francesco B. and Horacio S. acknowledge support from the TUNER project (TIN2015-65308-C5-5-R, MINECO/FEDER, UE) and the Maria de Maeztu Units of Excellence Programme (MDM-2015-0502).",Acknowledgments,[0],[0]
Emojis are small images that are commonly included in social media text messages.,abstractText,[0],[0]
"The combination of visual and textual content in the same message builds up a modern way of communication, that automatic systems are not used to deal with.",abstractText,[0],[0]
In this paper we extend recent advances in emoji prediction by putting forward a multimodal approach that is able to predict emojis in Instagram posts.,abstractText,[0],[0]
Instagram posts are composed of pictures together with texts which sometimes include emojis.,abstractText,[0],[0]
"We show that these emojis can be predicted by using the text, but also using the picture.",abstractText,[0],[0]
"Our main finding is that incorporating the two synergistic modalities, in a combined model, improves accuracy in an emoji prediction task.",abstractText,[0],[0]
This result demonstrates that these two modalities (text and images) encode different information on the use of emojis and therefore can complement each other.,abstractText,[0],[0]
Multimodal Emoji Prediction,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1481–1491 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"FrameNet Semantic Role Labeling analyzes sentences with respect to frame-semantic structures based on FrameNet (Fillmore et al., 2003).",1 Introduction,[0],[0]
"Typically, this involves two steps: First, Frame Identification (FrameId), capturing the context around a predicate (frame evoking element) and assigning a frame, basically a word sense label for a prototypical situation, to it.",1 Introduction,[0],[0]
"Second, Role Labeling, i.e. identifying the participants (fillers) of the predicate and connecting them with predefined frame-
∗named alphabetically 1https://github.com/UKPLab/
naacl18-multimodal-frame-identification
specific role labels.",1 Introduction,[0],[0]
"FrameId is crucial to the success of Semantic Role Labeling as FrameId errors account for most wrong predictions in current systems (Hartmann et al., 2017).",1 Introduction,[0],[0]
"Consequently, improving FrameId is of major interest.
",1 Introduction,[0],[0]
"The main challenge and source of prediction errors of FrameId systems are ambiguous predicates, which can evoke several frames, e.g., the verb sit evokes the frame Change posture in a context like ‘a person is sitting back on a bench’, while it evokes Being located when ‘a company is sitting in a city’.",1 Introduction,[0],[0]
"Understanding the predicate context, and thereby the context of the situation (here, ‘Who / what is sitting where?’), is crucial to identifying the correct frame for ambiguous cases.
",1 Introduction,[0],[0]
"State-of-the-art FrameId systems model the situational context using pretrained distributed word embeddings (see Hermann et al., 2014).",1 Introduction,[0],[0]
"Hence, it is assumed that the context of the situation is explicitly expressed in words.",1 Introduction,[0],[0]
"However, language understanding involves implicit knowledge, which is not mentioned but still seems obvious to humans, e.g., ‘people can sit back on a bench, but companies cannot’, ‘companies are in cities’.",1 Introduction,[0],[0]
"Such implicit common sense knowledge is obvious enough to be rarely expressed in sentences, but is more likely to be present in images.",1 Introduction,[0],[0]
"Figure 1 takes the ambiguous predicate sit to illustrate
1481
how images can provide access to implicit common sense knowledge crucial to FrameId.
",1 Introduction,[0],[0]
"When looking at the semantics of events, FrameId has commonalities with event prediction tasks.",1 Introduction,[0],[0]
These aim at linking events and their participants to script knowledge and at predicting events in narrative chains.,1 Introduction,[0],[0]
"Ahrendt and Demberg (2016) argue that knowing about the participants helps to identify the event, which suggests the need for implicit context knowledge also for FrameId.",1 Introduction,[0],[0]
"This specifically applies to images, which can reflect properties of the participants of a situation in a inherently different way, see Fig. 1.
",1 Introduction,[0],[0]
We analyze whether multimodal representations grounded in images can encode common sense knowledge to improve FrameId.,1 Introduction,[0],[0]
"To that end, we extend SimpleFrameId",1 Introduction,[0],[0]
"(Hartmann et al., 2017), a recent FrameId model based on distributed word embeddings, to the multimodal case and evaluate for English and German.",1 Introduction,[0],[0]
Note that there is a general lack of evaluation of FrameId systems for languages other than English.,1 Introduction,[0],[0]
"This is problematic as they yield different challenges; German, for example, due to long distance dependencies.",1 Introduction,[0],[0]
"Also, word embeddings trained on different languages have different strengths in ambiguous words.",1 Introduction,[0],[0]
"We elaborate on insights from using different datasets by language.
",1 Introduction,[0],[0]
Contributions.,1 Introduction,[0],[0]
"(1) We propose a pipeline and architecture of a FrameId system, extending stateof-the-art methods with the option of using implicit multimodal knowledge.",1 Introduction,[0],[0]
"It is flexible toward modality and language, reaches state-of-the-art accuracy on English FrameId data, clearly outperforming several baselines, and sets a new state of the art on German FrameId data.",1 Introduction,[0],[0]
"(2) We discuss properties of language and meaning with respect to implicit knowledge, as well as the potential of multimodal representations for FrameId.",1 Introduction,[0],[0]
(3) We perform a detailed analysis of FrameId systems.,1 Introduction,[0],[0]
"First, we develop a new strong baseline.",1 Introduction,[0],[0]
"Second, we suggest novel evaluation metrics that are essential for assessing ambiguous and rare frame instances.",1 Introduction,[0],[0]
We show our system’s advantage over the strong baseline in this regard and by this improve upon the main source of errors.,1 Introduction,[0],[0]
"Third, we analyze gold annotated datasets for English and German showing their different strengths.",1 Introduction,[0],[0]
"Finally, we release the implementation of our system, our evaluation splits for SALSA 2.0, and the embeddings for synsets and IMAGINED words.",1 Introduction,[0],[0]
"State-of-the-art FrameId systems rely on pretrained word embeddings as input (Hermann et al., 2014).",2.1 Frame identification,[0],[0]
"This proved to be helpful: those systems consistently outperform the previously leading FrameId system SEMAFOR (Das et al., 2014), which is based on a handcrafted set of features.",2.1 Frame identification,[0],[0]
The open source neural network-based FrameId system SimpleFrameId,2.1 Frame identification,[0],[0]
"(Hartmann et al., 2017) is conceptually simple, yet yields competitive accuracy.",2.1 Frame identification,[0],[0]
Its input representation is a concatenation of the predicate’s pretrained embedding and an embedding of the predicate context.,2.1 Frame identification,[0],[0]
The dimensionwise mean of the pretrained embeddings of all words in the sentence is taken as the context.,2.1 Frame identification,[0],[0]
"In this work, we first aim at improving the representation of the predicate context using multimodal embeddings, and second at assessing the applicability to another language, namely German.
",2.1 Frame identification,[0],[0]
Common sense knowledge for language understanding.,2.1 Frame identification,[0],[0]
"Situational background knowledge can be described in terms of frames (Fillmore, 1985) and scripts (Schank and Abelson, 2013).",2.1 Frame identification,[0],[0]
Ahrendt and Demberg (2016) report that knowing about a script’s participants aids in predicting events linked to script knowledge.,2.1 Frame identification,[0],[0]
"Transferring this insight to FrameId, we assume that a rich context representation helps to identify the sense of ambiguous predicates.",2.1 Frame identification,[0],[0]
"Addressing ambiguous predicates where participants have different properties depending on the context, Feizabadi and Padó (2012) give some examples where the location plays a discriminating role as participant: motion verbs that have both a concrete motion sense and a more abstract sense in the cognitive domain, e.g., struggle, lean, follow.
",2.1 Frame identification,[0],[0]
Frame identification in German.,2.1 Frame identification,[0],[0]
"Shalmaneser (Erk and Pado, 2006) is a toolbox for semantic role assignment on FrameNet schemata of English and German (integrated into the SALSA project for German).",2.1 Frame identification,[0],[0]
"Shalmaneser uses a Naive Bayes classifier to identify frames, together with features for a bag-of-word context with a window over sentences, bigrams, and trigrams of the target word and dependency annotations.",2.1 Frame identification,[0],[0]
They report an F1 of 75.1 % on FrameNet 1.2 and 60 % on SALSA 1.0.,2.1 Frame identification,[0],[0]
These scores are difficult to compare against more recent work as the evaluation uses older versions of datasets and custom splits.,2.1 Frame identification,[0],[0]
"Shalmaneser
requires software dependencies that are not available anymore, hindering application to new data.",2.1 Frame identification,[0],[0]
"To the best of our knowledge, there is no FrameId system evaluated on SALSA 2.0.
",2.1 Frame identification,[0],[0]
"Johannsen et al. (2015) present a simple, but weak translation baseline for cross-lingual FrameId.",2.1 Frame identification,[0],[0]
"A SEMAFOR-based system is trained on English FrameNet and tested on German Wikipedia sentences, translated word-by-word to English.",2.1 Frame identification,[0],[0]
This translation baseline reaches an F1 score of 8.5 % on the German sentences when translated to English.,2.1 Frame identification,[0],[0]
The performance of this weak translation baseline is worse than that of another simple baseline: a ‘most frequent sense baseline’ – computing majority votes for German (and many other languages) – reaches an F1 score of 53.0 % on the German sentences.,2.1 Frame identification,[0],[0]
"This shows that pure translation does not help with FrameId and, furthermore, indicates a large room for improvement for FrameId in languages other than English.",2.1 Frame identification,[0],[0]
"There is a growing interest in Natural Language Processing for enriching traditional approaches with knowledge from the visual domain, as images capture qualitatively different information compared to text.",2.2 Multimodal representation learning,[0],[0]
"Regarding FrameId, to the best of our knowledge, multimodal approaches have not yet been investigated.",2.2 Multimodal representation learning,[0],[0]
"For other tasks, multimodal approaches based on pretrained embeddings are reported to be superior to unimodal approaches.",2.2 Multimodal representation learning,[0],[0]
"Textual embeddings have been enriched with information from the visual domain, e.g., for Metaphor Identification (Shutova et al., 2016), Question Answering (Wu et al., 2017), and Word Pair Similarity (Collell et al., 2017).",2.2 Multimodal representation learning,[0],[0]
"The latter presents a simple, but effective way of extending textual embeddings with so-called multimodal IMAGINED embeddings by a learned mapping from language to vision.",2.2 Multimodal representation learning,[0],[0]
"We apply the IMAGINED method to our problem.
",2.2 Multimodal representation learning,[0],[0]
"In this work, we aim to uncover whether representations that are grounded in images can help to improve the accuracy of FrameId.",2.2 Multimodal representation learning,[0],[0]
Our application case of FrameId is more complex than a comparison on the word-pair level as it considers a whole sentence in order to identify the predicate’s frame.,2.2 Multimodal representation learning,[0],[0]
"However, we see a potential for multimodal IMAGINED embeddings to help: their mapping from text to multimodal representations is learned
from images for nouns.",2.2 Multimodal representation learning,[0],[0]
"Such nouns, in turn, are candidates for role fillers of predicates.",2.2 Multimodal representation learning,[0],[0]
"In order to identify the correct sense of an ambiguous predicate, it could help to enrich the representation of the context situation with multimodal embeddings for the entities that are linked by the predicate.",2.2 Multimodal representation learning,[0],[0]
"Our system builds upon the SimpleFrameId (Hartmann et al., 2017) system for English FrameId based on textual word embeddings.",3 Our Multimodal FrameId Model,[0],[0]
We extend it to multimodal and multilingual use cases; see Fig. 2 for a sketch of the system pipeline.,3 Our Multimodal FrameId Model,[0],[0]
"Same as SimpleFrameId, our system is based on pretrained embeddings to build the input representation out of the predicate context and the predicate itself.
",3 Our Multimodal FrameId Model,[0],[0]
"However, different to SimpleFrameId, our representation of the predicate context is multimodal: beyond textual embeddings we also use IMAGINED and visual embeddings.",3 Our Multimodal FrameId Model,[0],[0]
"More precisely, we concatenate all unimodal representations of the predicate context, which in turn are the unimodal mean embeddings of all words in the sentence.",3 Our Multimodal FrameId Model,[0],[0]
"We use concatenation for fusing the different embeddings as it is the simplest yet successful fusion approach (Bruni et al., 2014; Kiela and Bottou, 2014).",3 Our Multimodal FrameId Model,[0],[0]
"The input representation is processed by a two-layer Multilayer Perceptron (MLP, Rosenblatt, 1958), where we adapt the number of hidden nodes to the increased input size and apply dropout to all hidden layers to prevent overfitting (Srivastava et al., 2014).",3 Our Multimodal FrameId Model,[0],[0]
Each node in the output layer corresponds to one frame-label class.,3 Our Multimodal FrameId Model,[0],[0]
"We use rectified linear units (Nair and Hinton, 2010) as activation function for the hidden layers, and a soft-
max for the output layer yielding a multinomial distribution over frames.",3 Our Multimodal FrameId Model,[0],[0]
We take its argmax as the final prediction at test time.,3 Our Multimodal FrameId Model,[0],[0]
"Optionally, filtering based on the lexicon can be performed on the predicted probabilities for each frame label.",3 Our Multimodal FrameId Model,[0],[0]
"The development set was used to determine the architecture and hyperparameters, see Sec. 6.
Majority baselines.",3 Our Multimodal FrameId Model,[0],[0]
We propose a new strong baseline based on a combination of two existing ones.,3 Our Multimodal FrameId Model,[0],[0]
"These are: first, the most-frequent-sense baseline using the data majority (Data Baseline) to determine the most frequent frame for a predicate; second, the baseline introduced by Hartmann et al. (2017) using a lexicon (Lexicon Baseline) to consider the data counts of the Data Baseline only for those frames available for a predicate.",3 Our Multimodal FrameId Model,[0],[0]
"We propose to combine them into a Data-Lexicon Baseline, which uses the lexicon for unambiguous predicates and for ambiguous ones it uses the data majority.",3 Our Multimodal FrameId Model,[0],[0]
"This way, we trust the lexicon for unambiguous predicates but not for ambiguous ones, there we rather consider the data majority.",3 Our Multimodal FrameId Model,[0],[0]
"Comparing a system to these baselines helps to see whether it just memorizes the data majority or the lexicon, or actually captures more.
",3 Our Multimodal FrameId Model,[0],[0]
All majority baselines strongly outperform the weak translation baseline of Johannsen et al. (2015) when training the system on English data and evaluating it on German data.,3 Our Multimodal FrameId Model,[0],[0]
Textual embeddings for words.,4 Preparation of Input Embeddings,[0],[0]
"We use the 300-dimensional GloVe embeddings (Pennington et al., 2014) for English, and the 100-dimensional embeddings of Reimers et al. (2014) for German.",4 Preparation of Input Embeddings,[0],[0]
"GloVe and Reimers have been trained on the Wikipedia of their targeted language and on additional newswire text to cover more domains, resulting in similarly low out-of-vocabulary scores.
",4 Preparation of Input Embeddings,[0],[0]
Visual embeddings for synsets.,4 Preparation of Input Embeddings,[0],[0]
"We obtain visual embeddings for WordNet synsets (Fellbaum, 1998; , Ed.):",4 Preparation of Input Embeddings,[0],[0]
"we apply the pretrained VGG-m128 Convolutional Neural Network model (Chatfield et al., 2014) to images for synsets from ImageNet (Deng et al., 2009), we extract the 128- dimensional activation of the last layer (before the softmax) and then we L2-normalize it.",4 Preparation of Input Embeddings,[0],[0]
"We use the images of the WN9-IMG dataset (Xie et al., 2017), which links WordNet synsets to a collection of ten ImageNet images.",4 Preparation of Input Embeddings,[0],[0]
"We average the em-
beddings of all images corresponding to a synset, leading to a vocabulary size of 6555 synsets.",4 Preparation of Input Embeddings,[0],[0]
"All synsets in WN9-IMG are part of triples of the form entity-relation-entity, i.e. synset-relation-synset.",4 Preparation of Input Embeddings,[0],[0]
"Such synset entities that are participants of relations with other synset entities are candidates for incorporating the role fillers for predicates and, therefore, may help to find the correct frame for a predicate (see Sec. 5 for details about sensedisambiguation.)
",4 Preparation of Input Embeddings,[0],[0]
Linguistic embeddings for synsets.,4 Preparation of Input Embeddings,[0],[0]
"We obtain 300-dimensional linguistic synset embeddings: we apply the AutoExtend approach (Rothe and Schütze, 2015) to GloVe embeddings and produce synset embeddings for all synsets having at least one synset lemma in the GloVe embeddings.",4 Preparation of Input Embeddings,[0],[0]
This leads to a synset vocabulary size of 79 141.,4 Preparation of Input Embeddings,[0],[0]
"Linguistic synset embeddings are based on textual word embeddings and the synset information known by the knowledge base WordNet, thus they complement the visual synset embeddings.
",4 Preparation of Input Embeddings,[0],[0]
IMAGINED embeddings for words.,4 Preparation of Input Embeddings,[0],[0]
"We use the IMAGINED method (Collell et al., 2017) for learning a mapping function: it maps from the word embedding space to the visual embedding space given those words that occur in both pretrained embedding spaces (7220 for English and 7739 for German).",4 Preparation of Input Embeddings,[0],[0]
"To obtain the English synset lemmas, we extract all lemmas of a synset and keep those that are nouns.",4 Preparation of Input Embeddings,[0],[0]
We automatically translate English nouns to German nouns using the Google Translate API to obtain the corresponding German synset lemmas.,4 Preparation of Input Embeddings,[0],[0]
"The IMAGINED method is promising for cases where one embedding space (here, the textual one) has many instances without correspondence in the other embeddings space (here, the visual one), but the user still aims at obtaining instances of the first in the second space.",4 Preparation of Input Embeddings,[0],[0]
We aim to obtain visual correspondences for the textual embeddings in order to incorporate regularities from images into our system.,4 Preparation of Input Embeddings,[0],[0]
The mapping is a nonlinear transformation using a simple neural network.,4 Preparation of Input Embeddings,[0],[0]
The objective is to minimize the cosine distance between each mapped representation of a word and the corresponding visual representation.,4 Preparation of Input Embeddings,[0],[0]
"Finally, a multimodal representation for any word can be obtained by applying this mapping to the word embedding.",4 Preparation of Input Embeddings,[0],[0]
English FrameId:,5 Data and Preparation of Splits,[0],[0]
Berkeley FrameNet.,5 Data and Preparation of Splits,[0],[0]
"The Berkeley FrameNet (Baker et al., 1998; Ruppenhofer et al., 2016) is an ongoing project for building a large lexical resource for English with expert annotations based on frame semantics (Fillmore, 1976).",5 Data and Preparation of Splits,[0],[0]
"It consists of two parts, a manually created lexicon that maps predicates to the frames they can evoke, and fully annotated texts (fulltext).",5 Data and Preparation of Splits,[0],[0]
"The mapping can be used to facilitate the frame identification for a predicate in a sentence, e.g., a sentence in the fulltext corpus.",5 Data and Preparation of Splits,[0],[0]
"Table 1 contains the lexicon statistics, Table 2 (top left) the dataset statistics.",5 Data and Preparation of Splits,[0],[0]
"In this work, we use FrameNet 1.5 to ensure comparability with the previous state of the art, with the common evaluation split for FrameId systems introduced by Das and Smith (2011) (with the development split of Hermann et al., 2014).",5 Data and Preparation of Splits,[0],[0]
"Due to having a single annotation as consent of experts, it is hard to estimate a performance bound of a single human for the fulltext annotation.
",5 Data and Preparation of Splits,[0],[0]
German FrameId: SALSA.,5 Data and Preparation of Splits,[0],[0]
"The SALSA project (Burchardt et al., 2006; Rehbein et al., 2012) is a completed annotation project, which serves as the German counterpart to FrameNet.",5 Data and Preparation of Splits,[0],[0]
Its annotations are based on FrameNet up to version 1.2.,5 Data and Preparation of Splits,[0],[0]
SALSA adds proto-frames to properly annotate senses that are not covered by the English FrameNet.,5 Data and Preparation of Splits,[0],[0]
"For a more detailed description of differences between FrameNet and SALSA, see Ellsworth et al. (2004); Burchardt et al. (2009).",5 Data and Preparation of Splits,[0],[0]
SALSA also provides a lexicon (see Table 1 for statistics) and fully annotated texts.,5 Data and Preparation of Splits,[0],[0]
"There are two releases of SALSA: 1.0 (Burchardt et al., 2006) used for Shalmaneser (Erk and Pado, 2006) (cf. Sec. 2.1), and the final release 2.0 (Rehbein et al., 2012), which contains more annotations and adds nouns as predicates.",5 Data and Preparation of Splits,[0],[0]
"We use the final release.
",5 Data and Preparation of Splits,[0],[0]
"SALSA has no standard evaluation split; Erk and Pado (2006) used an undocumented random
split.",5 Data and Preparation of Splits,[0],[0]
"Also, it is not possible to follow the splitting method of Das and Smith (2011), as SALSA project distributions do not map to documents.",5 Data and Preparation of Splits,[0],[0]
"We suggest splitting based on sentences, i.e. all annotations of a sentence are in the same set to avoid mixing training and test sets.",5 Data and Preparation of Splits,[0],[0]
"We assign sentences to 100 buckets based on their IDs and create a 70/15/15 split for training, development, and test sets based on the bucket order.",5 Data and Preparation of Splits,[0],[0]
This procedure allows future work to be evaluated on the same data.,5 Data and Preparation of Splits,[0],[0]
"Table 2 (bottom left) shows the dataset statistics.
",5 Data and Preparation of Splits,[0],[0]
Synsets in FrameNet and SALSA.,5 Data and Preparation of Splits,[0],[0]
"To prepare the datasets for working with the synset embeddings, we sense-disambiguate all sentences using the API of BabelNet (Navigli and Ponzetto, 2010), which returns multilingual synsets.",5 Data and Preparation of Splits,[0],[0]
"We thus depend on the state-of-the-art accuracy of BabelNet (Navigli and Ponzetto, 2012) when using synset embeddings on sense-disambiguated sentences.",5 Data and Preparation of Splits,[0],[0]
"However, this dependence does not hold when applying IMAGINED embeddings to sentences, as the mapping from words to IMAGINED embeddings does not need any synsets labeled in the sentences.",5 Data and Preparation of Splits,[0],[0]
After sense-disambiguation some sentences do not contain any synset available in our synset embeddings.,5 Data and Preparation of Splits,[0],[0]
The statistics of those sentences that have at least one synset embedding (visual or linguistic AutoExtend) is given in Table 2 (right).,5 Data and Preparation of Splits,[0],[0]
We contrast our system’s performance for context representations based on unimodal (textual) versus multimodal (textual and visual) embeddings.,6 Experimental Setup,[0],[0]
"Also, we compare English against German data.",6 Experimental Setup,[0],[0]
"We run the prediction ten times to reduce noise in
the evaluation (cf. Reimers and Gurevych, 2017) and report the mean for each metric.
",6 Experimental Setup,[0],[0]
Use of lexicon.,6 Experimental Setup,[0],[0]
"We evaluate our system in two settings: with and without lexicon, as suggested by Hartmann et al. (2017).",6 Experimental Setup,[0],[0]
"In the with-lexicon setting, the lexicon is used to reduce the choice of frames for a predicate to only those listed in the lexicon.",6 Experimental Setup,[0],[0]
"If the predicate is not in the lexicon, it corresponds to the without-lexicon setting, where the choice has to be done amongst all frames.
",6 Experimental Setup,[0],[0]
Evaluation metrics.,6 Experimental Setup,[0],[0]
"FrameId systems are usually compared in terms of accuracy, which we adopt for comparability.",6 Experimental Setup,[0],[0]
"As a multiclass classification problem, FrameId has to cope with a strong variation in the annotation frequency of frame classes.",6 Experimental Setup,[0],[0]
Minority classes are frames that occur only rarely; majority classes occur frequently.,6 Experimental Setup,[0],[0]
"Note that the accuracy is biased toward majority classes, explaining the success of majority baselines on imbalanced datasets such as FrameNet.
",6 Experimental Setup,[0],[0]
"Alternatively, the F1 score is sometimes reported as it takes a complementary perspective.",6 Experimental Setup,[0],[0]
"The F-measure is the harmonic mean of precision and recall, measuring exactness and completeness of a model, respectively.",6 Experimental Setup,[0],[0]
"In previous work, microaveraging is used to compute F1 scores.",6 Experimental Setup,[0],[0]
"Yet, similar to the accuracy, micro-averaging introduces a bias toward majority classes.",6 Experimental Setup,[0],[0]
"We compute F1macro instead, for which precision and recall are computed for each class and averaged afterwards, giving equal weight to all classes.
",6 Experimental Setup,[0],[0]
"Taken together, this yields scores that underestimate (F1-macro) and overestimate (average accuracy) on imbalanced datasets.",6 Experimental Setup,[0],[0]
Previous work just used the overestimate such that a comparison is possible in terms of accuracy in the with-lexicon setting.,6 Experimental Setup,[0],[0]
"We suggest to use F1-macro additionally to analyze rare, but interesting classes.",6 Experimental Setup,[0],[0]
"Thus, a comparison within our work is possible for both aspects, giving a more detailed picture.",6 Experimental Setup,[0],[0]
"Note that previous work reports one score whilst we report the mean score of ten runs.
",6 Experimental Setup,[0],[0]
Hyperparameters.,6 Experimental Setup,[0],[0]
"We identified the best hyperparameters for the English and German data based on the respective development sets.2 The Multilayer Perceptron architecture performed con-
2Differences in hyperparameters to SimpleFrameId: ‘nadam’ as optimizer instead of ‘adagrad’, dropout on hidden layers and early stopping to regularize training.",6 Experimental Setup,[0],[0]
"Different number of hidden units, optimized by grid search.
sistently better than a more complex Gated Recurrent Unit model (Cho et al., 2014).",6 Experimental Setup,[0],[0]
We found that more than two hidden layers did not bring any improvement over two layers; using dropout on the hidden layers helped to increase the accuracy.,6 Experimental Setup,[0],[0]
"Among the various input representations, a concatenation of the representations of context and predicate was the best amongst others, including dependencies, lexicon indicators, and part-ofspeech tags.",6 Experimental Setup,[0],[0]
"Training is done using Nesterovaccelerated Adam (Nadam, Dozat, 2016) with default parameters.",6 Experimental Setup,[0],[0]
A batch size of 128 is used.,6 Experimental Setup,[0],[0]
"Learning stops if the development accuracy has not improved for four epochs, and the learning rate is reduced by factor of two if there has not been any improvement for two epochs.",6 Experimental Setup,[0],[0]
"First, we report our results on English data (see Table 3, top) and then, we compare against German data (see Table 3, bottom).",7 Results,[0],[0]
Baseline.,7.1 English FrameNet data,[0],[0]
"Our new strong Data-Lexicon Baseline reaches a considerable accuracy of 86.32 %, which is hard to beat by trained models.",7.1 English FrameNet data,[0],[0]
"Even the most recent state of the art only beats it by about two points: 88.41 % (Hermann et al., 2014).",7.1 English FrameNet data,[0],[0]
"However, the accuracy of the baseline drops for ambiguous predicates (69.73 %) and the F1-macro score reveals its weakness toward minority classes (drop from 64.54 % to 37.42 %).
",7.1 English FrameNet data,[0],[0]
Unimodal.,7.1 English FrameNet data,[0],[0]
"Our unimodal system trained and evaluated on English data slightly exceeds the accuracy of the previous state of the art (88.66 % on average versus 88.41 % for Hermann et al., 2014); our best run’s accuracy is 89.35 %.",7.1 English FrameNet data,[0],[0]
"Especially on ambiguous predicates, i.e. the difficult and therefore interesting cases, our average accuracy surpasses that of previous work by more than one point (the best run by almost three points).",7.1 English FrameNet data,[0],[0]
"Considering the proposed F1-macro score for an assessment of the performance on minority classes and ambiguous predicates reveals our main improvement: Our system substantially outperforms the strong Data-Lexicon Baseline, demonstrating that our system differs from memorizing majorities and actually improves minority cases.
",7.1 English FrameNet data,[0],[0]
Multimodal.,7.1 English FrameNet data,[0],[0]
"From a range of multimodal context representations as extensions to our system,
the most helpful one is the concatenation of IMAGINED embeddings and visual synset embeddings: it outperforms the unimodal approach slightly in all measurements.",7.1 English FrameNet data,[0],[0]
"We observe that the improvements are more pronounced for difficult cases, such as for rare and ambiguous cases (one point improvement in F1-macro), as well as in the absence of a lexicon (up to two points improvement).
",7.1 English FrameNet data,[0],[0]
Significance tests.,7.1 English FrameNet data,[0],[0]
"We conduct a single sample t-test to judge the difference between previous state-of-the-art accuracy (Hermann et al., 2014) and our unimodal approach.",7.1 English FrameNet data,[0],[0]
The null hypothesis (expected value of our sample of ten accuracy scores equals previous state-of-the-art accuracy) is rejected at a significance level of α = 0.05 (p = 0.0318).,7.1 English FrameNet data,[0],[0]
"In conclusion, even our unimodal approach outperforms prior state of the art in terms of accuracy.
",7.1 English FrameNet data,[0],[0]
"To judge the difference between our unimodal and our multimodal approach, we conduct a t-test for the means of the two independent samples.",7.1 English FrameNet data,[0],[0]
The null hypothesis states identical expected values for our two samples of ten accuracy scores.,7.1 English FrameNet data,[0],[0]
"Regarding the setting with lexicon, the null hypothesis cannot be rejected at a significance level of α = 0.05 (p = 0.2181).",7.1 English FrameNet data,[0],[0]
"However, concerning accuracy scores without using the lexicon, the null hypothesis is rejected at a significance level of α = 0.05 (p < 0.0001).",7.1 English FrameNet data,[0],[0]
"In conclusion, the multimodal approach has a slight overall advan-
tage and, interestingly, has a considerable advantage over the unimodal one when confronted with a more difficult setting of not using the lexicon.",7.1 English FrameNet data,[0],[0]
German results.,7.2 German SALSA versus English data,[0],[0]
"Our system evaluated on German data sets a new state of the art on this corpus with 80.76 % accuracy, outperforming the baselines (77.16 %; no other system evaluated on this dataset).",7.2 German SALSA versus English data,[0],[0]
The difference in F1-macro between the majority baselines and our system is smaller than for the English FrameNet.,7.2 German SALSA versus English data,[0],[0]
"This indicates that the majorities learned from data are more powerful in the German case with SALSA than in the English case, when comparing against our system.",7.2 German SALSA versus English data,[0],[0]
"Multimodal context representations cannot show an improvement for SALSA with this general dataset.
Lexicon.",7.2 German SALSA versus English data,[0],[0]
"We report results achieved without the lexicon to evaluate independently of its quality (Hartmann et al., 2017).",7.2 German SALSA versus English data,[0],[0]
"On English data, our systems outperforms Hartmann et al. (2017) by more than two points in accuracy and we achieve a large improvement over the Data Baseline.",7.2 German SALSA versus English data,[0],[0]
"Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data.",7.2 German SALSA versus English data,[0],[0]
"For German data, the increase of F1-macro with lexicon versus without is small (one point).",7.2 German SALSA versus English data,[0],[0]
Insights from the baseline.,8.1 English data,[0],[0]
Many indicators point to our approach not just learning the data majority: our trained models have better F1-macro and especially much higher ambiguous F1-macro scores with lexicon.,8.1 English data,[0],[0]
"This clearly suggests that our system is capable of acquiring more expressiveness than the baselines do by counting majorities.
",8.1 English data,[0],[0]
Impact of multimodal representations.,8.1 English data,[0],[0]
Multimodal context representations improve results compared to unimodal ones.,8.1 English data,[0],[0]
It helps to incorporate visual common sense knowledge about the situation’s participants.,8.1 English data,[0],[0]
"Referring back to our example of the ambiguous predicate sit, the multimodal approach is able to transfer the knowledge to the test sentence ‘Al-Anbar in general, and Ramadi in particular, are sat with the Americans in Jordan.’",8.1 English data,[0],[0]
by correctly identifying the frame Being located whilst the unimodal approach fails with predicting Change posture.,8.1 English data,[0],[0]
The increase in performance when adding information from visual synset embeddings is not simply due to higher dimensionality of the embedding space.,8.1 English data,[0],[0]
"To verify, we further investigate extending the unimodal system with random word embeddings.",8.1 English data,[0],[0]
"This leads to a drop in performance compared to using just the unimodal representations or using these in combination with the proposed multimodal embeddings, especially in the setting without lexicon.",8.1 English data,[0],[0]
"Interestingly, replacing visual synset embeddings with linguistic synset embeddings (AutoExtend by Rothe and Schütze (2015), see Sec. 4) in further investigations also showed that visual embeddings yield better performance.",8.1 English data,[0],[0]
This points out the potential for incorporating even more image evidence to extend our approach.,8.1 English data,[0],[0]
Difficulties for German data.,8.2 German versus English data,[0],[0]
"The impact of multimodal context representations is more dif-
ficult to interpret for the German dataset.",8.2 German versus English data,[0],[0]
The fact that they have not helped here may be due to mismatches when translating the English nouns of a synset to German in order to train the IMAGINED embeddings.,8.2 German versus English data,[0],[0]
"Here, we see room for future work to improve on simple translation by sensebased translations.",8.2 German versus English data,[0],[0]
"In SALSA, a smaller portion of sentences has at least one synset embedding, see Table 2.",8.2 German versus English data,[0],[0]
"For further investigations, we reduced the dataset to only sentences actually containing a synset embedding.",8.2 German versus English data,[0],[0]
"Then, minor improvements of the multimodal approach were visible for SALSA.",8.2 German versus English data,[0],[0]
"This points out that a dataset containing more words linking to implicit knowledge in images (visual synset embeddings) can profit more from visual and IMAGINED embeddings.
",8.2 German versus English data,[0],[0]
Impact of lexicon: English versus German.,8.2 German versus English data,[0],[0]
"Even if both lexica approximately define the same number of frames (see Table 1), the number of defined lexical units (distinct predicate-frame combinations) in SALSA is smaller.",8.2 German versus English data,[0],[0]
This leads to a lexicon that is a magnitude smaller than the FrameNet lexicon.,8.2 German versus English data,[0],[0]
"Thus, the initial situation for the German case is more difficult.",8.2 German versus English data,[0],[0]
"The impact of the lexicon for SALSA is smaller than for FrameNet (best visible in the increase of F1-macro with using the lexicon compared to without), which can be explained by the larger percentage of ambiguous predicates (especially evoking proto-frames) and the smaller size of the lexicon.",8.2 German versus English data,[0],[0]
"The evaluation on two different languages highlights the impact of an elaborate, manually created lexicon: it boosts the performance on frame classes that are less present in the training data.",8.2 German versus English data,[0],[0]
"English FrameId benefits from the large high-quality lexicon, whereas German FrameId currently lacks a high-quality lexicon that is large enough to benefit the FrameId task.
",8.2 German versus English data,[0],[0]
Dataset properties: English versus German.,8.2 German versus English data,[0],[0]
"To better understand the influence of the dataset on the prediction errors, we further analyze the errors of our approach (see Table 4) following Palmer
and Sporleder (2010).",8.2 German versus English data,[0],[0]
"A wrong prediction can either be a normal classification error, or it can be the result of an instance that was unseen at training time, which means that the error is due to the training set.",8.2 German versus English data,[0],[0]
The instance can either be completely unseen or unseen with the target label.,8.2 German versus English data,[0],[0]
"We observe that FrameNet has larger issues with unseen data compared to SALSA, especially data that was unseen with one specific label but seen with another label.",8.2 German versus English data,[0],[0]
"This is due to the uneven split of the documents in FrameNet, leading to data from different source documents and domains in the training and test split.",8.2 German versus English data,[0],[0]
SALSA does not suffer from this problem as much since the split was performed differently.,8.2 German versus English data,[0],[0]
It would be worth considering the same splitting method for FrameNet.,8.2 German versus English data,[0],[0]
"As stated previously, FrameId has commonalities with event prediction.",8.3 Future work,[0],[0]
"Since identifying frames is only one way of capturing events, our approach is transferable to other schemes of event prediction and visual knowledge about participants of situations should be beneficial there, too.",8.3 Future work,[0],[0]
"It would be interesting to evaluate the multimodal architecture on other predicate-argument frameworks, e.g., script knowledge or VerbNet style Semantic Role Labeling.",8.3 Future work,[0],[0]
"In particular the exploration our findings on visual contributions to FrameId in the context of further event prediction tasks forms an interesting next step.
",8.3 Future work,[0],[0]
"More precisely, future work should consider using implicit knowledge not only from images of the participants of the situation, but also from the entire scene in order to directly capture relations between the participants.",8.3 Future work,[0],[0]
This could provide access to a more holistic understanding of the scene.,8.3 Future work,[0],[0]
"The following visual tasks with accompanying datasets could serve as a starting point: (a) visual Verb Sense Disambiguation with the VerSe dataset (Gella et al., 2016) and (b) visual SRL with several datasets, e.g., imSitu (Yatskar et al., 2016) (linked to FrameNet), V-COCO (Gupta and Malik, 2015) (verbs linked to COCO), VVN (Ronchi and Perona, 2015) (visual VerbNet) or even SRL grounded in video clips for the cooking-domain (Yang et al., 2016) and visual Situation Recognition (Mallya and Lazebnik, 2017).",8.3 Future work,[0],[0]
"Such datasets could be used for extracting visual embeddings for verbs or even complex situations in order to improve the visual component in the embeddings
for our FrameId system.",8.3 Future work,[0],[0]
"Vice versa: visual tasks could profit from multimodal approaches (Baltrušaitis et al., 2017) in a similar sense as our textual task, FrameId, profits from additional information encoded in further modalities.",8.3 Future work,[0],[0]
"Moreover, visual SRL might profit from our multimodal FrameId system to a similar extend as any FrameNet SRL task profits from correctly identified frames (Hartmann et al., 2017).
",8.3 Future work,[0],[0]
"Regarding the combination of embeddings from different modalities, we suggest to experiment with different fusion strategies complementing the middle fusion (concatenation) and the mapping (IMAGINED method).",8.3 Future work,[0],[0]
This could be a late fusion at decision level operating like an ensemble.,8.3 Future work,[0],[0]
"In this work, we investigated multimodal representations for Frame Identification (FrameId) by incorporating implicit knowledge, which is better reflected in the visual domain.",9 Conclusion,[0],[0]
We presented a flexible FrameId system that is independent of modality and language in its architecture.,9 Conclusion,[0],[0]
With this flexibility it is possible to include textual and visual knowledge and to evaluate on gold data in different languages.,9 Conclusion,[0],[0]
"We created multimodal representations from textual and visual domains and showed that for English FrameNet data, enriching the textual representations with multimodal ones improves the accuracy toward a new state of the art.",9 Conclusion,[0],[0]
"For German SALSA data, we set a new state of the art with textual representations only and discuss why incorporating multimodal information is more difficult.",9 Conclusion,[0],[0]
"For both datasets, our system is particularly strong with respect to ambiguous and rare classes, considerably outperforming our new Data-Lexicon Baseline and thus addressing a key challenge in FrameId.",9 Conclusion,[0],[0]
"This work has been supported by the DFGfunded research training group “Adaptive Preparation of Information form Heterogeneous Sources” (AIPHES, GRK 1994/1).",Acknowledgments,[0],[0]
We also acknowledge the useful comments of the anonymous reviewers.,Acknowledgments,[0],[0]
"An essential step in FrameNet Semantic Role Labeling is the Frame Identification (FrameId) task, which aims at disambiguating a situation around a predicate.",abstractText,[0],[0]
"Whilst current FrameId methods rely on textual representations only, we hypothesize that FrameId can profit from a richer understanding of the situational context.",abstractText,[0],[0]
"Such contextual information can be obtained from common sense knowledge, which is more present in images than in text.",abstractText,[0],[0]
"In this paper, we extend a state-of-the-art FrameId system in order to effectively leverage multimodal representations.",abstractText,[0],[0]
We conduct a comprehensive evaluation on the English FrameNet and its German counterpart SALSA.,abstractText,[0],[0]
"Our analysis shows that for the German data, textual representations are still competitive with multimodal ones.",abstractText,[0],[0]
"However on the English data, our multimodal FrameId approach outperforms its unimodal counterpart, setting a new state of the art.",abstractText,[0],[0]
"Its benefits are particularly apparent in dealing with ambiguous and rare instances, the main source of errors of current systems.",abstractText,[0],[0]
"For research purposes, we release (a) the implementation of our system, (b) our evaluation splits for SALSA 2.0, and (c) the embeddings for synsets and IMAGINED",abstractText,[0],[0]
words.1,abstractText,[0],[0]
Multimodal Frame Identification with Multilingual Evaluation,title,[0],[0]
"Proceedings of the SIGDIAL 2018 Conference, pages 140–150, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics
140",text,[0],[0]
"The interplay between vision and language has created a range of interesting applications, including image captioning (Karpathy and FeiFei, 2015), visual question generation (VQG) (Mostafazadeh et al., 2016), visual question answering (VQA) (Antol et al., 2015), and reference expressions (Hu et al., 2016).",1 Introduction,[0],[0]
"Visual dialog (Das et al., 2017b) extends the VQA problem to multi-turn visual-grounded conversations without specific goals.",1 Introduction,[0],[0]
"In this paper, we study the task-oriented visual dialog setting that requires the agent to learn the multimodal representation and dialog policy for decision making.",1 Introduction,[0],[0]
"We argue that a task-oriented visual intelligent conversational sys-
tem should not only acquire vision and language understanding but also make appropriate decisions efficiently in a situated environment.",1 Introduction,[0],[0]
"Specifically, we designed a 20 images guessing game using the Visual Dialog dataset (Das et al., 2017a).",1 Introduction,[0],[0]
This game is the visual analog of the popular 20 question game.,1 Introduction,[0],[0]
"The agent aims to learn a dialog policy that can guess the correct image through question answering using the minimum number of turns.
",1 Introduction,[0],[0]
"Previous work on visual dialogs (Das et al., 2017a,b; Chattopadhyay et al., 2017) focused mainly on vision-to-language understanding and generation instead of dialog policy learning.",1 Introduction,[0],[0]
They let an agent ask a fixed number of questions to rank the images or let humans make guesses at the end of the conversations.,1 Introduction,[0],[0]
"However, such setting is not realistic in real-world task-oriented applications, because in task-oriented applications, not only completing the task successfully is important but also completing it efficiently.",1 Introduction,[0],[0]
"In addition, the agent should also be informed of the wrong guesses, so that it becomes more aware of the vision context.",1 Introduction,[0],[0]
"However, solving such real-world setting is a challenge.",1 Introduction,[0],[0]
"The system needs to handle the large dynamically updated multimodal stateaction space and also leverage the signals in the feedback loop coming from different sub-tasks.
",1 Introduction,[0],[0]
We propose a multimodal hierarchical reinforcement learning framework that allows learning visual dialog state tracking and dialog policy jointly to complete visual dialog tasks efficiently.,1 Introduction,[0],[0]
"The framework we propose takes inspiration from feudal reinforcement learning (FRL) (Dayan and Hinton, 1993), where levels of hierarchy within an agent communicate via explicit goals in a topdown fashion.",1 Introduction,[0],[0]
"In our case, it decomposes the decision into two steps: a first step where a master policy selects between verbal task (information query) and vision task (image retrieval), and a second step where a primitive action (question or im-
age) is chosen from the selected task.",1 Introduction,[0],[0]
"Hierarchical RL that relies on space abstraction, such as FRL, is useful to address the challenge of large discrete action space and has been shown to be effective in dialog systems, especially for large domain dialog management(Casanueva et al., 2018).",1 Introduction,[0],[0]
"Besides, we propose a new technique called state adaptation in order to make the multimodal dialog state more aware of the constantly changing visual context.",1 Introduction,[0],[0]
We demonstrate the efficacy of this technique through ablation analysis.,1 Introduction,[0],[0]
Visual dialog requires the agent to hold a multiturn conversation about visual content.,2.1 Visual Dialog,[0],[0]
"Several visual dialog tasks have been developed, including image grounded conversation generation (Mostafazadeh et al., 2017).",2.1 Visual Dialog,[0],[0]
Guess,2.1 Visual Dialog,[0],[0]
What?!,2.1 Visual Dialog,[0],[0]
"(De Vries et al., 2017) involves locating visual objects using dialogs.",2.1 Visual Dialog,[0],[0]
"VisDial (Das et al., 2017a) situates an answer-bot (A-Bot) to answer questions from a question-bot (Q-Bot) about an image.",2.1 Visual Dialog,[0],[0]
Das et al. (2017b) applied reinforcement learning (RL) to the VisDial task to learn the policies for the Q/A-Bots to collaboratively rank the correct image among a set of candidates.,2.1 Visual Dialog,[0],[0]
"However, their Q-Bot can only ask questions and cannot make guesses.",2.1 Visual Dialog,[0],[0]
Chattopadhyay et al. (2017) further evaluated the pre-trained A-bot in a similar setting to answer human generated questions.,2.1 Visual Dialog,[0],[0]
"Since humans are tasked to ask questions, the policy learning of QBot is not investigated.",2.1 Visual Dialog,[0],[0]
"Finally, (Manuvinakurike et al., 2017) proposed a incremental dialogue policy learning method for image guessing.",2.1 Visual Dialog,[0],[0]
"However, their dialog state only used language information and did not include visual information.",2.1 Visual Dialog,[0],[0]
We build upon prior works and propose a framework that learns an optimal dialog policy for the Q-Bot to perform both question selection and image guessing through exploiting multimodal information.,2.1 Visual Dialog,[0],[0]
"RL is a popular approach to learn an optimal dialog policy for task-oriented dialog systems (Singh et al., 2002; Williams and Young, 2007; Georgila and Traum, 2011; Lee and Eskenazi, 2012; Yu et al., 2017).",2.2 Reinforcement Learning,[0],[0]
The deep Q-Network (DQN) introduced by Mnih et al. (2015) achieved human-level performance in Atari games based on deep neural networks.,2.2 Reinforcement Learning,[0],[0]
"Deep RL was then used to jointly
learn the dialog state tracking and policy optimization in an end-to-end manner (Zhao and Eskenazi, 2016).",2.2 Reinforcement Learning,[0],[0]
"In our framework, we use a DQN to learn the higher level policy for question selection or image guessing.",2.2 Reinforcement Learning,[0],[0]
Van Hasselt et al. (2016) proposed a double DQN to overcome the overestimation problem in the Q-Learning and Schaul et al. (2015) suggested prioritized experience replay to improve the data sampling efficiency for training DQN.,2.2 Reinforcement Learning,[0],[0]
We apply both techniques in our implementation.,2.2 Reinforcement Learning,[0],[0]
"One limitation of DQNs is that they cannot handle unbounded action space, which is often the case for natural language interaction.",2.2 Reinforcement Learning,[0],[0]
He et al. (2015) proposed Deep Reinforcement Relevance Network (DRRN) that can handle inherently large discrete natural language action space.,2.2 Reinforcement Learning,[0],[0]
"Specifically, the DRRN takes both the state and natural language actions as inputs and computes a Q-value for each state action pair.",2.2 Reinforcement Learning,[0],[0]
"Thus, we use a DRRN as our question selection policy to approximate the value function for any question candidate.
",2.2 Reinforcement Learning,[0],[0]
"Our work is also related to hierarchical reinforcement learning (HRL) which often decomposes the problem into several sub-problems and achieves better learning convergence rate and generalization compared to flat RL (Sutton et al., 1999; Dietterich, 2000).",2.2 Reinforcement Learning,[0],[0]
"HRL has been applied to dialog management (Lemon et al., 2006; Cuayáhuitl et al., 2010; Budzianowski et al., 2017) which decomposes the dialog policy with respect to system goals or domains.",2.2 Reinforcement Learning,[0],[0]
"When the system enters a sub-task, the selected dialog policy will be used and continue to operate until the subproblem is solved, however the terminate condition for a subproblem has to be predefined.",2.2 Reinforcement Learning,[0],[0]
"Different from prior work, our proposed architecture uses hierarchical dialog policy to combine two RL architectures within a control flow, i.e., DQN and DRRN, in order to jointly learn multimodal dialog state representation and dialog policy.",2.2 Reinforcement Learning,[0],[0]
"Note that our HRL framework resembles the FRL hierarchy (Dayan and Hinton, 1993) that exploits space abstraction, state sharing and sequential execution.",2.2 Reinforcement Learning,[0],[0]
Figure 2 shows an overview of the multimodal hierarchical reinforcement learning framework and the simulated environment.,3 Proposed Framework,[0],[0]
There are four main modules in the framework.,3 Proposed Framework,[0],[0]
"The visual dialog semantic embedding module learns a multimodal dialog state representation to support the visual
dialog state tracking module with attention signals.",3 Proposed Framework,[0],[0]
Then the hierarchical policy learning module takes the visual dialog state as the input to optimize the high-level control policy between question selection and image retrieval.,3 Proposed Framework,[0],[0]
This module learns the multimodal representation for the downstream visual dialog state tracking.,3.1 Visual Dialog Semantic Embedding,[0],[0]
Figure 3 shows the network architecture for pretraining the visual dialog semantic embedding.,3.1 Visual Dialog Semantic Embedding,[0],[0]
"A VGG-19 CNN (Simonyan and Zisserman, 2014) and a multilayer perceptron (MLP) with L2 normalization are used to encode visual information (images) as a vector I ∈",3.1 Visual Dialog Semantic Embedding,[0],[0]
Rk.,3.1 Visual Dialog Semantic Embedding,[0],[0]
"We use a dialogconditioned attentive encoder (Lu et al., 2017) to encode textual information as a vector T ∈ Rk where k is the joint embedding size.",3.1 Visual Dialog Semantic Embedding,[0],[0]
"The image caption(c) is encoded with a LSTM to get a vector mc and each QA pair (H0, ...,Ht) is encoded separately with another LSTM as Mht ∈ Rd×t where t is the turn index and d is the LSTM embedding size.",3.1 Visual Dialog Semantic Embedding,[0],[0]
"Conditioned on the image caption embedding, the model attends to the dialog history:
zht = w T",3.1 Visual Dialog Semantic Embedding,[0],[0]
a tanh(WhM h t +,3.1 Visual Dialog Semantic Embedding,[0],[0]
"(Wcm c t)1 T ) (1) αht = softmax(z h t ) (2)
where 1 is a vector with all elements set to 1, Wh,Wc ∈ Rt×d and wa ∈",3.1 Visual Dialog Semantic Embedding,[0],[0]
Rk are parameters to be learned.,3.1 Visual Dialog Semantic Embedding,[0],[0]
α ∈,3.1 Visual Dialog Semantic Embedding,[0],[0]
Rk is the attention weight over history.,3.1 Visual Dialog Semantic Embedding,[0],[0]
The attended history feature m̂ht is the weighted sum of each column of Mht with α h t .,3.1 Visual Dialog Semantic Embedding,[0],[0]
Then m̂ht is concatenated withm c and encoded via MLP and l2 norm to get the final textual embedding (T ).,3.1 Visual Dialog Semantic Embedding,[0],[0]
"We train the network with pairwise ranking loss (Kiros et al., 2014) on cosine similarities between the textual and visual embedding.",3.1 Visual Dialog Semantic Embedding,[0],[0]
"The pretraining step allows the module to have better generalization and improve convergence performance in the RL training.
",3.1 Visual Dialog Semantic Embedding,[0],[0]
"Given the QA pairs from the simulated environ-
ment, the output of this module can also be used for the image retrieval sub-task.",3.1 Visual Dialog Semantic Embedding,[0],[0]
"To verify the quality of this module, we perform a sanity check on an image retrieval task, similar to (Das et al., 2017b).",3.1 Visual Dialog Semantic Embedding,[0],[0]
We used the output of the module to rank the 20 images in the game setting.,3.1 Visual Dialog Semantic Embedding,[0],[0]
"Among 1000 games, we achieved 96.8% accuracy for recall@1 (the target image ranked the highest), which means that this embedding module can provide reliable reward signal in an image retrieval task for the RL training if given the relevant dialog history.",3.1 Visual Dialog Semantic Embedding,[0],[0]
This module utilizes the output from the visual dialog semantic embedding to formulate the final dialog state representation.,3.2 Visual Dialog State Tracking,[0],[0]
"We track three types of state information, the dialog meta information (META), the vision belief (V B) and the vision context (V C).",3.2 Visual Dialog State Tracking,[0],[0]
"The dialog meta information includes the number of questions asked, the number of images guessed and the last action.",3.2 Visual Dialog State Tracking,[0],[0]
"The vision belief state is the output of the visual dialog semantic embedding module, which captures the internal multimodal information of the agent.",3.2 Visual Dialog State Tracking,[0],[0]
We initialize the VB with only the encoding of the image caption and update it with each new incoming QA pair.,3.2 Visual Dialog State Tracking,[0],[0]
The vision context state represents the visual information of the environment.,3.2 Visual Dialog State Tracking,[0],[0]
"In order to make the agent more aware of the dynamic visual context and which images to attend more, we introduce a new technique called state adaptation as it updates the vision context state with the attention scores.",3.2 Visual Dialog State Tracking,[0],[0]
"The V C is initialized as the average of image vectors and updated as follows:
αr,t,i = sigmoid(VBr,t · Ir,i) (3) VCr,t = ∑20
i=1",3.2 Visual Dialog State Tracking,[0],[0]
"αr,t,iIr,i∑20 i=1",3.2 Visual Dialog State Tracking,[0],[0]
"αi
(4)
",3.2 Visual Dialog State Tracking,[0],[0]
"where r, t and i refer to episode, dialog turn and image index.",3.2 Visual Dialog State Tracking,[0],[0]
The V C is then adjusted based on the attention scores (see equation 4).,3.2 Visual Dialog State Tracking,[0],[0]
The attention scores calculated by dot product in the equation 3 represent the affinity between the current vision belief state and each image vector.,3.2 Visual Dialog State Tracking,[0],[0]
"In the case of wrong guesses (informed by the simulator), we set the attention score for that wrong image to zero.",3.2 Visual Dialog State Tracking,[0],[0]
This method is inspired by Tian et al. (2017) who explicitly weights context vectors by context-query relevance for encoding dialog context.,3.2 Visual Dialog State Tracking,[0],[0]
"The question selection sub-task also takes the
vision context state as input and the vision belief state is used in the image retrieval sub-task.",3.2 Visual Dialog State Tracking,[0],[0]
"The goal is to learn a dialog policy that makes decisions based on the current visual dialog state, i.e, asking a question about the image or making a guess about the image that the user is thinking of.",3.3 Hierarchical Policy Learning,[0],[0]
"As the agent is situated in a dynamically changing vision context to update its internal decisionmaking model (approximated by the belief state) with new dialog exchange, we treat such environment as a Partially Observable Markov Decision Process (POMDP) and solve it using deep reinforcement learning.",3.3 Hierarchical Policy Learning,[0],[0]
We now describe the key components: Dialog State comes from the visual dialog state tracking module as mentioned in Section 3.2 Policy Learning:,3.3 Hierarchical Policy Learning,[0],[0]
"Given the above dialog state, we introduce a hierarchical dialog policy that contains a high-level control policy and a low-level question selection policy.",3.3 Hierarchical Policy Learning,[0],[0]
"We learn the control policy with a Double DQN that decides between “question” or “guess” at a game step.
",3.3 Hierarchical Policy Learning,[0],[0]
"If the high-level action is a “question”, then the control is passed over to the low-level policy, which needs to select a question.",3.3 Hierarchical Policy Learning,[0],[0]
"One challenge is that the list of candidate questions are different for every game, and the number of candidate questions for different images is also different as well.",3.3 Hierarchical Policy Learning,[0],[0]
This prohibits us using a standard DQN with fixed number of actions.,3.3 Hierarchical Policy Learning,[0],[0]
He et al. (2015) showed that modeling state embedding and action embedding separately in DRRN has superior performance than per-action DQN as well as other DQN variants for dealing with natural language action spaces.,3.3 Hierarchical Policy Learning,[0],[0]
"Therefore, we use the DRRN to solve this problem, which computes a matching score between the shared current vision context state and the embedding of each question candidate.",3.3 Hierarchical Policy Learning,[0],[0]
We use a softmax selection strategy as the exploration policy during the learning stage.,3.3 Hierarchical Policy Learning,[0],[0]
"The hierarchical policy learning algorithm is described in the Appendix Algorithm 1.
",3.3 Hierarchical Policy Learning,[0],[0]
"If the high-level action is “guess”, then an image is retrieved using cosine distance between each image vector and the vision belief vector.",3.3 Hierarchical Policy Learning,[0],[0]
"It is worth mentioning that although the action space of the image retrieval sub-task can be incorporated into a flat DRRN combined with text-based inputs,the training is unstable and does not converge
within this flat RL framework.",3.3 Hierarchical Policy Learning,[0],[0]
We suspect this is due to the sample efficiency problem with large multimodal action space for which the question action or guess action typically results in different reward signals.,3.3 Hierarchical Policy Learning,[0],[0]
"Therefore, we did not compare our proposed method against a flat RL model.",3.3 Hierarchical Policy Learning,[0],[0]
Rewards:,3.3 Hierarchical Policy Learning,[0],[0]
"The reward function is decomposed as R = RG + RQ + RI where RG means the final game reward(win/loss= ±10), RI refers to wrong guess penalty (-3).",3.3 Hierarchical Policy Learning,[0],[0]
"We define RQ as the pseudo reward for the sub-task of question selection as
RQ =",3.3 Hierarchical Policy Learning,[0],[0]
At −At−1 (5),3.3 Hierarchical Policy Learning,[0],[0]
"At = sigmoid(VBr,t · Itarget) (6)
where t refers to the dialog turn and affinity scores (At andAt−1) are the outputs of the sigmoid function that scales the similarity score (0-1) of the vision belief state and the target image vector.",3.3 Hierarchical Policy Learning,[0],[0]
The intuition is that different questions provide various information gains for the agent.,3.3 Hierarchical Policy Learning,[0],[0]
"The integration of RQ is a reward shaping (Ng et al., 1999) technique that aims to provide immediate rewards to make the RL training more efficient.",3.3 Hierarchical Policy Learning,[0],[0]
"At each turn, if the verbal task (question selection) is chosen, the RQ would serve as immediate reward for training the DQN and DRRN",3.3 Hierarchical Policy Learning,[0],[0]
"while if the vision task (image retrieval) is chosen, only the RI is available for training DQN.",3.3 Hierarchical Policy Learning,[0],[0]
"At the end of a game, the reward function varies based on the primitive action and the final game result.",3.3 Hierarchical Policy Learning,[0],[0]
The question selection module selects the best question in order to acquire relevant information to update the image belief state.,3.4 Question Selection,[0],[0]
"As discussed in Section 3.3, we used a discriminative approach to select the next question for the agent by learning the policy in a DRRN.",3.4 Question Selection,[0],[0]
It leverages the existing question candidate pool that is constructed differently with respect to different experiment settings in Section 4.4.,3.4 Question Selection,[0],[0]
"Ideally we would like to generate realistic questions online towards a specific goal (Zhang et al., 2017) and we leave this generative approach for future study.",3.4 Question Selection,[0],[0]
We first describe the simulation of the environment.,4 Experiments,[0],[0]
"Then, we talk about different dialog policy models and implementation details.",4 Experiments,[0],[0]
"Finally, we discuss three different experimental settings to evaluate the proposed framework.",4 Experiments,[0],[0]
We constructed a simulator for 20 images guessing game using the VisDial dataset.,4.1 Simulator Construction,[0],[0]
Each image corresponds to a dialog consisting of ten rounds of question answering generated by humans.,4.1 Simulator Construction,[0],[0]
"To make the task setting meaningful and the training time manageable, we pre",4.1 Simulator Construction,[0],[0]
-process and select 1000 sets of games consisting of 20 similar images.,4.1 Simulator Construction,[0],[0]
The simulator provides the reward signals and answers related to the target image.,4.1 Simulator Construction,[0],[0]
It also tracks the internal game state.,4.1 Simulator Construction,[0],[0]
"A game is terminated when one of the three conditions is fulfilled: 1) the agent guesses the correct answer, 2) the max number of guesses is reached (three guesses) or 3) the max number of dialog turns is reached.",4.1 Simulator Construction,[0],[0]
The agent wins the game when it guesses the correct image.,4.1 Simulator Construction,[0],[0]
"If the agent wins the game, it gets a reward of 10, and if the agent loses the game, it gets a reward of −10.",4.1 Simulator Construction,[0],[0]
The agent also receives a −3 penalty for each wrong guess.,4.1 Simulator Construction,[0],[0]
"To evaluate the contribution of each technique in the multimodal hierarchical framework: the hierarchical policy, the state adaptation, and the reward shaping, we evaluate five different policy models and perform ablation analysis.",4.2 Policy Models,[0],[0]
We describe each model as follows: - Random Policy (Rnd): The agent randomly selects a question or makes a guess at any step.,4.2 Policy Models,[0],[0]
"- Random Question+DQN (Rnd+DQN): The agent randomly selects a question but a DQN is used to optimize the hierarchical decision of making a guess or asking a question. - DRRN+DQN (HRL): Similar to Rnd+ DQN, except that a DRRN is used to optimize the question selection process - DRRN+DQN+State Apdation (HRL+SA): Similar to HRL, except incorporating the state adaptation, which is similar to the attention re-weighting concept in the vision context state.",4.2 Policy Models,[0],[0]
- DRRN+DQN+State Apdation+Reward,4.2 Policy Models,[0],[0]
"Shaping (HRL+SAR): Similar to HRL+SA, except that reward shaping is applied.",4.2 Policy Models,[0],[0]
The details about data pre-processing and training hyper-parameters are described in the Appendix.,4.3 Implementation Details,[0],[0]
"During the training, the DQN uses the -greedy policy and the DRRN uses the softmax policy for exploration, where is linearly decreased from 1
to 0.1.",4.3 Implementation Details,[0],[0]
"The resulting framework was trained up to 20,000 iterations for Experiment 1 and 95,000 iterations for Experiment 2 and 3, and evaluated at every 1000 iterations with greedy policy.",4.3 Implementation Details,[0],[0]
At each evaluation we record the performance of different models with a greedy policy for 100 independent games.,4.3 Implementation Details,[0],[0]
The evaluation metrics are the win rate and the average number of dialog turns.,4.3 Implementation Details,[0],[0]
We conduct three sets of experiments to explore the effectiveness of the proposed multimodal hierarchical reinforcement learning framework in a real-world scenario step by step.,4.4 Experimental Setting,[0],[0]
The first experiment constrains the agent to select among the 10 human generated question-answer pairs.,4.4 Experimental Setting,[0],[0]
This setting enables us to assess the effectiveness of the framework in a less error-prone setting.,4.4 Experimental Setting,[0],[0]
The second experiment does not require a human to generate the answer to emulate a more realistic environment.,4.4 Experimental Setting,[0],[0]
"Specifically, we enlarge the number of questions by including 200 human generated questions for the 20 images, and use a pre-trained visual question answer model to generate answers with respect to the target image.",4.4 Experimental Setting,[0],[0]
"In the last experiment, we further automate the process by generating questions given the 20 images using a pretrained visual question generation model.",4.4 Experimental Setting,[0],[0]
So the agent does not require any human input with respect to any image for training.,4.4 Experimental Setting,[0],[0]
We evaluate the models described in Section 4.2 under the settings described in Section 4.4 and report results as following.,5 Results,[0],[0]
The agent selects the next question among the 10 question-answer pairs human generated and want to identify the targeted image accurately and efficiently through natural language conversation.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
We terminate the dialog after ten turns.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
Each model’s performance is shown in Table 1. HRL+SAR achieves the best win rate with statistical significance.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
The HRL+SAR policy model performs much better than methods without hierarchical control structure and state adaptation.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
The learning curves in Figure 4 and 5 reveal that the HRL+SAR converges faster.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"We further perform bootstrap tests by resampling the game results
from each experiment with replacement 1,000 times.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
Then we calculate the probability of significance level for the difference of average win rates or average turn length to check whether the relative performance improvement from the last baseline is statistically significant.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
The result shows that the question selection (DRRN) and state adaptation bring the most significant performance improvements (p < 0.01) while reward shaping has less impact (p < 0.05).,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
We also observe that the average number of turns with hierarchical policy learning (HRL) is slightly longer than that of Rnd+DQN but with less statistically significant difference.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"This is probably because this setting provides the 10 predefined question-answer pairs with a smaller action space, the DQN model tends to encourage the agent to make guesses quicker, while policy models with hierarchical structures tends to optimize the overall task completion rate.
",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
We find that RL methods (DQN & DRRN) significantly improve the win rate as they learn to select the optimal list of questions to ask.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"We also observe that our proposed state adaptation method
for vision context state helps achieve the largest performance improvement.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"The hierarchical control architecture and the state abstraction sharing (Dietterich, 2000) also improve both learning speed and agent performance.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"This aligns with the observation in Budzianowski et al. (2017).
",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"Moreover, on average, we observe that after seven turns, the agent was able to select the target image with a sufficiently high success rate.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
We further explore if the proposed hierarchical framework enables efficient decision-making when compared to the agent that keeps asking questions and only makes the guess at the end of the dialog.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
We refer to such models as the oracle baselines.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"For example, the Oracle@7 makes the guess at the 7th turn based on the previous dialog history with the correct order of questionanswer pairs in the dataset.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"The oracle baselines are strong, since they represent the best performance the model can get given the optimal question order provided by human.
",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
Table 2 shows the performance of the oracle baselines with various fixed turns.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
We performed significance tests between each oracle baseline and the hierarchical framework.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"Since our hierarchical framework requires on average 7.22 turns to complete, so we compared it with Oracle@7 and Oracle@8.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"We found that the proposed method outperforms Oracle@7 with p − value < 0.01, and similar to Oracle@8 (significant difference
(p − value > 0.1).",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"The reason that the hierarchical framework can outperform Oracle@7 is that it learns to make a guess whenever the agent is confident enough, therefore achieving better win rate.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"Oracle@8 in general receives more information as the dialogs are longer, therefore has an advantage over the hierarchical method.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"However, it still performs similar to the proposed method, which demonstrates that by learning the hierarchical decision, it enables the agent to achieve the goal more efficiently.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
One thing we need to point out is that the proposed method also received extra information about whether the guess is correct or not from the environment.,5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"Oracle baselines do not have such information, as it can only make a guess at the end of the dialog.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"Oracle@9 and @10 are better than the hierarchical framework statistically, because they acquire much more information by having longer turns.",5.1 Experiment 1: Human Generated Question-Answer Pairs,[0],[0]
"To make the experimental setting more realistic, we select 200 questions generated by a human with respect to 20 images provided and create a user simulator that generates the answers related to the target image.",5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
"Here, as the questions space is larger, we terminate the dialog after 20 turns.",5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
"We follow the supervised training scheme discussed in (Das et al., 2017b) to train the visual question generation module offline.
",5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
Results in Table 3 indicate that HRL+SAR significantly outperforms Rnd and Rnd+DQN in both win rate and average number of dialog turns.,5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
"The setting in Experiment 2 is more challenging than that of Experiment 1, because the visual ques-
tion module introduces noise that can influence the policy learning.",5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
"However, the noise also simulates the real-world scenario that a user might have an implicit goal that may change within the task.",5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
A user can also accidentally make errors in answering the question.,5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
The proposed hierarchical framework (HRL+SAR) with state adaptation and reward shaping achieves the best win rate and the least number of dialog turns in this noisy experiment setting.,5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
"As compared to Experiment 1, the policy models with hierarchical structures can both optimize the overall task completion rate and the dialog turns.",5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
"We did not report oracle baselines results, since the oracle order of all the questions (ideally generated by humans) was not available.",5.2 Experiment 2: Questions Generated by Human and Answers Generated Automatically,[0],[0]
"In this setting, both questions and answers are generated automatically through pre-trained visual question and answer generation models (Das et al., 2017b).",5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
Such setting enables the agent to play the guessing game given any image as no human input of the image is needed.,5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
Notice that the answers should be generated with respect to a target image for our task setting.,5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
"In this setting, we also set the maximum number of dialog turns to be 20.
",5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
The results in Table 4 show that the performance of the three policies significantly dropped compared to Experiment 2.,5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
"Such observation is expected, as the noise coming from both the visual question and answer generation module increases the task difficulty.",5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
"However, the proposed HRL+SAR is still more resilient to the noise and achieves a higher win rate and less average number of turns compared to other baselines.",5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
"Figure 5 from the Appendix shows that in Experiment 2
the agent tends select relevant questions faster to ask although the answers can be misleading.",5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
"On the other hand, in Experiment 3, the agent reacts to the generated question and answers slower to complete the task.",5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
The model performance decreases when we increase the task difficulty in order to emulate the real-world scenarios.,5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
"It hints that there is a possible limitation of using the VisDial dataset, because the dialog is constructed by users who casually talk about MS COCO images (Chen et al., 2015) instead of exchanging with an explicit contextual goal in the dialog.",5.3 Experiment 3: Question-Answer Pairs Generated Automatically,[0],[0]
We develop a framework for task-oriented visual dialog systems and demonstrate the efficacy of integrating multimodal state representation with hierarchical decision learning in an image guessing game.,6 Discussion and Future Work,[0],[0]
We also introduce a new technique called state adaptation to further improve the task performance through integrating context awareness.,6 Discussion and Future Work,[0],[0]
"We also test the proposed framework in various noisy settings to simulate real-world scenarios and achieve robust results.
",6 Discussion and Future Work,[0],[0]
The proposed framework is practical and extensible for real-world applications.,6 Discussion and Future Work,[0],[0]
"For example, the designed system can act as a fashion shopping assistant to help customers pick clothes through strategically inquiring their preferences while leveraging vision intelligence.",6 Discussion and Future Work,[0],[0]
"In another application, such as criminology practice, the agent can communicate with witnesses to identify suspects from a large face database.
",6 Discussion and Future Work,[0],[0]
"Although games provide a rich domain for multimodal learning research, admittedly it is challenging to evaluate a multimodal dialog system due to the data scarcity problem.",6 Discussion and Future Work,[0],[0]
"In future work, we would like to extend and apply the proposed framework for human studies in a situated realworld application, such as a shopping scenario.",6 Discussion and Future Work,[0],[0]
"We also plan to incorporate domain knowledge and database interactions into the system framework design, which will make the dialog system more flexible and effective.",6 Discussion and Future Work,[0],[0]
Another possible extension of the framework is to update the off-line question and answer generation modules with an online generative version and retrain the module with reinforcement learning.,6 Discussion and Future Work,[0],[0]
"After data pre-processing, we had a vocabulary size of 8,957 and image vector dimension of
4,096.",A Data Pre-Processing and Training Details,[0],[0]
"To pre-train the visual dialog semantic embedding, we used the following parameters: the size of word embedding is 300; the size of LSTMs is 512; 0.2 dropout rate and the final embedding size 1024 with MLP and l2 norm.",A Data Pre-Processing and Training Details,[0],[0]
We fixed the visual dialog semantic embedding during the RL training.,A Data Pre-Processing and Training Details,[0],[0]
"The high-level policy learning module - Double DQN was trained with the following hyperparameters: three MLP layers of sizes 1000, 500 and 50 with tanh activation respectively.",A Data Pre-Processing and Training Details,[0],[0]
"For hyper-parameters of DQN, the behavior network was updated every 5 steps and the interval for updating the target network is",A Data Pre-Processing and Training Details,[0],[0]
500. -greedy,A Data Pre-Processing and Training Details,[0],[0]
"exploration was used for training, where is linearly decreased from 1 to 0.1.",A Data Pre-Processing and Training Details,[0],[0]
The question selection module - DRRN encodes the context vector and question vector separately with two MLP layers of sizes 256 and 128 and dot product was used as the interaction function.,A Data Pre-Processing and Training Details,[0],[0]
"The experience replay buffer sizes are 25,000 for DQN and 50,000 for DRRN.",A Data Pre-Processing and Training Details,[0],[0]
Both RL networks were trained through RMSProp with batch size 64.,A Data Pre-Processing and Training Details,[0],[0]
Bootstrapping and prioritized replay were also used to facilitate RL training.,A Data Pre-Processing and Training Details,[0],[0]
The reward discount factor was set to be 0.99.,A Data Pre-Processing and Training Details,[0],[0]
See Figure 5.,B Sample Dialog,[0],[0]
"See Algorithm 1.
",C Hierarchical Policy Learning Algorithm,[0],[0]
"Algorithm 1 Hierarchical Policy Learning 1: Initialize Double DQN(online network parameters θ and target network parameters θ−) and
DRRN(network parameters θ+) with small random weights and corresponding replay memory EDQN and EDRRN to capacity N. 2: Initialize game simulator and load dictionary.",C Hierarchical Policy Learning Algorithm,[0],[0]
"3: for episode r = 1, ..., M do 4: Restart game simulator.",C Hierarchical Policy Learning Algorithm,[0],[0]
"5: Receive image caption and candidate images from the simulator, and convert them to represen-
tation via pre-trained visual dialog semantic embedding layer, denoted as initial state Sr,0 6: for t = 1, ..., T do 7: sample high-level action from DQN, At ∼ πDQN (Sr,t) 8: if Ar,t = Q(asking a question) then 9: Compute Q(V Ct, qi) for the list of questions Qr,t using DRRN forward activation and
select the question qr,t with the max Q-value, and keep track the next available question pool Qr,t+1 10: if Ar,t = G (guessing an image) then 11:",C Hierarchical Policy Learning Algorithm,[0],[0]
"Select the image gr,t with the smallest cosine distance between an image vector Ii and
current image belief state VBr,t 12: Execute action qr,t or gr,t in the simulator and get the next visual dialog state representation
Sr,t+1 and reward signal Rr,t 13:",C Hierarchical Policy Learning Algorithm,[0],[0]
"Store the transition (Sr,t, Ar,t, Sr,t+1, Rr,t) into EDQN and if asking a question, also store
the transition (V Cr,t, qr,t, V Cr,t+1, Rr,t, Qr,t+1) into EDRRN 14:",C Hierarchical Policy Learning Algorithm,[0],[0]
"Sample random mini-batch of transitions (Sk, Ak, Sk+1, Rk) from EDQN
15: Set yDQN =",C Hierarchical Policy Learning Algorithm,[0],[0]
{,C Hierarchical Policy Learning Algorithm,[0],[0]
"Rk if terminal state Rk + γQDQN (Sk+1, argmaxa′Q(Sk+1, a
′; θ); θ−) if else 16:",C Hierarchical Policy Learning Algorithm,[0],[0]
"Sample random mini-batch of transitions (V Cl, ql, V Cl+1, Rl, Ql+1) from EDRRN
17: Set yDRRN =",C Hierarchical Policy Learning Algorithm,[0],[0]
{,C Hierarchical Policy Learning Algorithm,[0],[0]
Rl if terminal state Rl + γmaxa′∈Ql+1QDRRN,C Hierarchical Policy Learning Algorithm,[0],[0]
"(V Cl+1, a
′; θ+)",C Hierarchical Policy Learning Algorithm,[0],[0]
"if else 18: Perform gradient steps for DQN with loss ‖ yDQN −QDQN (Sk, Ak; θ) ‖2 with respect to θ
and DRRN with loss ‖ yDRRN −QDRRN",C Hierarchical Policy Learning Algorithm,[0],[0]
"(V Cl, ql; θ+) ‖2 with respect to θ+ 19: Replace target parameters θ− ← θ for every N steps.
",C Hierarchical Policy Learning Algorithm,[0],[0]
end for end for,C Hierarchical Policy Learning Algorithm,[0],[0]
"Creating an intelligent conversational system that understands vision and language is one of the ultimate goals in Artificial Intelligence (AI) (Winograd, 1972).",abstractText,[0],[0]
"Extensive research has focused on vision-tolanguage generation, however, limited research has touched on combining these two modalities in a goal-driven dialog context.",abstractText,[0],[0]
We propose a multimodal hierarchical reinforcement learning framework that dynamically integrates vision and language for task-oriented visual dialog.,abstractText,[0],[0]
The framework jointly learns the multimodal dialog state representation and the hierarchical dialog policy to improve both dialog task success and efficiency.,abstractText,[0],[0]
"We also propose a new technique, state adaptation, to integrate context awareness in the dialog state representation.",abstractText,[0],[0]
We evaluate the proposed framework and the state adaptation technique in an image guessing game and achieve promising results.,abstractText,[0],[0]
Multimodal Hierarchical Reinforcement Learning Policy for Task-Oriented Visual Dialog,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2236–2246 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2236",text,[0],[0]
"Theories of language origin identify the combination of language and nonverbal behaviors (vision and acoustic modality) as the prime form of communication utilized by humans throughout evolution (Müller, 1866).",1 Introduction,[0],[0]
"In natural language processing, this form of language is regarded as human multimodal language.",1 Introduction,[0],[0]
"Modeling multimodal language has recently become a centric research direction in both NLP and multimodal machine learning (Hazarika et al., 2018; Zadeh et al., 2018a; Poria et al., 2017a; Baltrušaitis et al., 2017; Chen et al., 2017).
",1 Introduction,[0],[0]
Studies strive to model the dual dynamics of multimodal language: intra-modal dynamics (dynamics within each modality) and cross-modal dynamics (dynamics across different modalities).,1 Introduction,[0],[0]
"However, from a resource perspective, previous multimodal language datasets have severe shortcomings in the following aspects: Diversity in the training samples: The diversity in training samples is crucial for comprehensive multimodal language studies due to the complexity of the underlying distribution.",1 Introduction,[0],[0]
"This complexity is rooted in variability of intra-modal and crossmodal dynamics for language, vision and acoustic modalities (Rajagopalan et al., 2016).",1 Introduction,[0],[0]
Previously proposed datasets for multimodal language are generally small in size due to difficulties associated with data acquisition and costs of annotations.,1 Introduction,[0],[0]
Variety in the topics: Variety in topics opens the door to generalizable studies across different domains.,1 Introduction,[0],[0]
Models trained on only few topics generalize poorly as language and nonverbal behaviors tend to change based on the impression of the topic on speakers’ internal mental state.,1 Introduction,[0],[0]
"Diversity of speakers: Much like writing styles, speaking styles are highly idiosyncratic.",1 Introduction,[0],[0]
"Training models on only few speakers can lead to degenerate solutions where models learn the identity of speakers as opposed to a generalizable model of multimodal language (Wang et al., 2016).",1 Introduction,[0],[0]
Variety in annotations Having multiple labels to predict allows for studying the relations between labels.,1 Introduction,[0],[0]
"Another positive aspect of having variety of labels is allowing for multi-task learning which has shown excellent performance in past research.
",1 Introduction,[0],[0]
Our first contribution in this paper is to introduce the largest dataset of multimodal sentiment and emotion recognition called CMU Multimodal Opinion Sentiment and Emotion Intensity (CMUMOSEI).,1 Introduction,[0],[0]
"CMU-MOSEI contains 23,453 annotated video segments from 1,000 distinct speakers and
250 topics.",1 Introduction,[0],[0]
Each video segment contains manual transcription aligned with audio to phoneme level.,1 Introduction,[0],[0]
All the videos are gathered from online video sharing websites 1.,1 Introduction,[0],[0]
"The dataset is currently a part of the CMU Multimodal Data SDK and is freely available to the scientific community through Github 2.
",1 Introduction,[0],[0]
Our second contribution is an interpretable fusion model called Dynamic Fusion Graph (DFG) to study the nature of cross-modal dynamics in multimodal language.,1 Introduction,[0],[0]
DFG contains built-in efficacies that are directly related to how modalities interact.,1 Introduction,[0],[0]
These efficacies are visualized and studied in detail in our experiments.,1 Introduction,[0],[0]
"Aside interpretability, DFG achieves superior performance compared to previously proposed models for multimodal sentiment and emotion recognition on CMU-MOSEI.",1 Introduction,[0],[0]
In this section we compare the CMU-MOSEI dataset to previously proposed datasets for modeling multimodal language.,2 Background,[0],[0]
We then describe the baselines and recent models for sentiment analysis and emotion recognition.,2 Background,[0],[0]
We compare CMU-MOSEI to an extensive pool of datasets for sentiment analysis and emotion recognition.,2.1 Comparison to other Datasets,[0],[0]
"The following datasets include a combination of language, visual and acoustic modalities as their input data.",2.1 Comparison to other Datasets,[0],[0]
"CMU-MOSI (Zadeh et al., 2016b) is a collection of 2199 opinion video clips each annotated with sentiment in the range",2.1.1 Multimodal Datasets,[0],[0]
"[-3,3].",2.1.1 Multimodal Datasets,[0],[0]
CMU-MOSEI is the next generation of CMU-MOSI.,2.1.1 Multimodal Datasets,[0],[0]
"The ICT-MMMO (Wöllmer et al., 2013) consists of online social review videos annotated at the video level for sentiment.",2.1.1 Multimodal Datasets,[0],[0]
"YouTube (Morency et al., 2011) contains videos from the social media web site YouTube that span a wide range of product reviews and opinion videos.",2.1.1 Multimodal Datasets,[0],[0]
"MOUD (Perez-Rosas et al., 2013) consists of product review videos in Spanish.",2.1.1 Multimodal Datasets,[0],[0]
"Each video consists of multiple segments labeled to display positive, negative or neutral sentiment.",2.1.1 Multimodal Datasets,[0],[0]
"IEMOCAP (Busso et al., 2008) consists of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset.",2.1.1 Multimodal Datasets,[0],[0]
"Each
1following creative commons license allows for personal unrestricted use and redistribution of the videos
2https://github.com/A2Zadeh/CMUMultimodalDataSDK
segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral) as well as valence, arousal and dominance.",2.1.1 Multimodal Datasets,[0],[0]
"Stanford Sentiment Treebank (SST) (Socher et al., 2013) includes fine grained sentiment labels for phrases in the parse trees of sentences collected from movie review data.",2.1.2 Language Datasets,[0],[0]
"While SST has larger pool of annotations, we only consider the root level annotations for comparison.",2.1.2 Language Datasets,[0],[0]
"Cornell Movie Review (Pang et al., 2002) is a collection of 2000 moviereview documents and sentences labeled with respect to their overall sentiment polarity or subjective rating.",2.1.2 Language Datasets,[0],[0]
"Large Movie Review dataset (Maas et al., 2011) contains text from highly polar movie reviews.",2.1.2 Language Datasets,[0],[0]
"Sanders Tweets Sentiment (STS) consists of 5513 hand-classified tweets each classified with respect to one of four topics of Microsoft, Apple, Twitter, and Google.",2.1.2 Language Datasets,[0],[0]
"The Vera am Mittag (VAM) corpus consists of 12 hours of recordings of the German TV talk-
show “Vera am Mittag” (Grimm et al., 2008).",2.1.3 Visual and Acoustic Datasets,[0],[0]
"This audio-visual data is labeled for continuous-valued scale for three emotion primitives: valence, activation and dominance.",2.1.3 Visual and Acoustic Datasets,[0],[0]
VAM-Audio and VAMFaces are subsets that contain on acoustic and visual inputs respectively.,2.1.3 Visual and Acoustic Datasets,[0],[0]
"RECOLA (Ringeval et al., 2013) consists of 9.5 hours of audio, visual, and physiological (electrocardiogram, and electrodermal activity) recordings of online dyadic interactions.",2.1.3 Visual and Acoustic Datasets,[0],[0]
"Mimicry (Bilakhia et al., 2015) consists of audiovisual recordings of human interactions in two situations: while discussing a political topic and while playing a role-playing game.",2.1.3 Visual and Acoustic Datasets,[0],[0]
"AFEW (Dhall et al., 2012, 2015) is a dynamic temporal facial expressions data corpus consisting of close to real world environment extracted from movies.
",2.1.3 Visual and Acoustic Datasets,[0],[0]
Detailed comparison of CMU-MOSEI to the datasets in this section is presented in Table 1.,2.1.3 Visual and Acoustic Datasets,[0],[0]
CMU-MOSEI has longer total duration as well as larger number of data point in total.,2.1.3 Visual and Acoustic Datasets,[0],[0]
"Furthermore, CMU-MOSEI has a larger variety in number of speakers and topics.",2.1.3 Visual and Acoustic Datasets,[0],[0]
"It has all three modalities provided, as well as annotations for both sentiment and emotions.",2.1.3 Visual and Acoustic Datasets,[0],[0]
Modeling multimodal language has been the subject of studies in NLP and multimodal machine learning.,2.2 Baseline Models,[0],[0]
Notable approaches are listed as follows and indicated with a symbol for reference in the Experiments and Discussion section (Section 5).,2.2 Baseline Models,[0],[0]
"# MFN: (Memory Fusion Network) (Zadeh et al., 2018a) synchronizes multimodal sequences using a multi-view gated memory that stores intraview and cross-view interactions through time.",2.2 Baseline Models,[0],[0]
∎,2.2 Baseline Models,[0],[0]
"MARN: (Multi-attention Recurrent Network) (Zadeh et al., 2018b) models intra-modal and multiple cross-modal interactions by assigning multiple attention coefficients.",2.2 Baseline Models,[0],[0]
Intra-modal and cross-modal interactions are stored in a hybrid LSTM memory component.,2.2 Baseline Models,[0],[0]
"∗ TFN (Tensor Fusion Network) (Zadeh et al., 2017) models inter and intra modal interactions by creating a multi-dimensional tensor that captures unimodal, bimodal and trimodal interactions.",2.2 Baseline Models,[0],[0]
"◇ MV-LSTM (Multi-View LSTM) (Rajagopalan et al., 2016) is a recurrent model that designates regions inside a LSTM to different views of the data.",2.2 Baseline Models,[0],[0]
"§ EF-LSTM (Early Fusion LSTM) concatenates the inputs from different modalities at each time-step and uses that as the input to a single LSTM (Hochreiter and Schmidhuber, 1997;
Graves et al., 2013; Schuster and Paliwal, 1997).",2.2 Baseline Models,[0],[0]
"In case of unimodal models EF-LSTM refers to a single LSTM.
",2.2 Baseline Models,[0],[0]
"We also compare to the following baseline models: † BC-LSTM (Poria et al., 2017b), ♣ C-MKL (Poria et al., 2016), ♭ DF (Nojavanasghari et al., 2016), ♡ SVM (Cortes and Vapnik, 1995; Zadeh et al., 2016b; Perez-Rosas et al., 2013; Park et al., 2014), ● RF (Breiman, 2001), THMM (Morency et al., 2011), SAL-CNN (Wang et al., 2016), 3DCNN (Ji et al., 2013).",2.2 Baseline Models,[0],[0]
"For language only baseline models: ∪ CNN-LSTM (Zhou et al., 2015), RNTN (Socher et al., 2013), ×: DynamicCNN (Kalchbrenner et al., 2014), ⊳ DAN (Iyyer et al., 2015), ≀ DHN (Srivastava et al., 2015), ⊲",2.2 Baseline Models,[0],[0]
"RHN (Zilly et al., 2016).",2.2 Baseline Models,[0],[0]
"For acoustic only baseline models: AdieuNet (Trigeorgis et al., 2016), SERLSTM (Lim et al., 2016).",2.2 Baseline Models,[0],[0]
Understanding expressed sentiment and emotions are two crucial factors in human multimodal language.,3 CMU-MOSEI Dataset,[0],[0]
We introduce a novel dataset for multimodal sentiment and emotion recognition called CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI).,3 CMU-MOSEI Dataset,[0],[0]
"In the following subsections, we first explain the details of the CMU-MOSEI data acquisition, followed by details of annotation and feature extraction.",3 CMU-MOSEI Dataset,[0],[0]
Social multimedia presents a unique opportunity for acquiring large quantities of data from various speakers and topics.,3.1 Data Acquisition,[0],[0]
Users of these social multimedia websites often post their opinions in the forms of monologue videos; videos with only one person in front of camera discussing a certain topic of interest.,3.1 Data Acquisition,[0],[0]
"Each video inherently contains three modalities: language in the form of spoken text, visual via perceived gestures and facial expressions, and acoustic through intonations and prosody.
",3.1 Data Acquisition,[0],[0]
"During our automatic data acquisition process, videos from YouTube are analyzed for the presence of one speaker in the frame using face detection to ensure the video is a monologue.",3.1 Data Acquisition,[0],[0]
We limit the videos to setups where the speaker’s attention is exclusively towards the camera by rejecting videos that have moving cameras (such as camera on bikes or selfies recording while walking).,3.1 Data Acquisition,[0],[0]
We use a diverse set of 250 frequently used topics in online videos as the seed for acquisition.,3.1 Data Acquisition,[0],[0]
"We restrict the
number of videos acquired from each channel to a maximum of 10.",3.1 Data Acquisition,[0],[0]
"This resulted in discovering 1,000 identities from YouTube.",3.1 Data Acquisition,[0],[0]
"The definition of a identity is proxy to the number of channels since accurate identification requires quadratic manual annotations, which is infeasible for high number of speakers.",3.1 Data Acquisition,[0],[0]
"Furthermore, we limited the videos to have manual and properly punctuated transcriptions provided by the uploader.",3.1 Data Acquisition,[0],[0]
"The final pool of acquired videos included 5,000 videos which were then manually checked for quality of video, audio and transcript by 14 expert judges over three months.",3.1 Data Acquisition,[0],[0]
The judges also annotated each video for gender and confirmed that each video is an acceptable monologue.,3.1 Data Acquisition,[0],[0]
A set of 3228 videos remained after manual quality inspection.,3.1 Data Acquisition,[0],[0]
We also performed automatic checks on the quality of video and transcript which are discussed in Section 3.3 using facial feature extraction confidence and forced alignment confidence.,3.1 Data Acquisition,[0],[0]
"Furthermore, we balance the gender in the dataset using the data provided by the judges (57% male to 43% female).",3.1 Data Acquisition,[0],[0]
This constitutes the final set of raw videos in CMU-MOSEI.,3.1 Data Acquisition,[0],[0]
"The topics covered in the final set of videos are shown in Figure 1 as a Venn-style word cloud (Coppersmith and Kelly, 2014) with the size proportional to the number of videos gathered for that topic.",3.1 Data Acquisition,[0],[0]
"The most frequent 3 topics are reviews (16.2%), debate (2.9%) and consulting (1.8%).",3.1 Data Acquisition,[0],[0]
"The remaining topics are almost uniformly distributed 3.
",3.1 Data Acquisition,[0],[0]
"The final set of videos are then tokenized into 3more detailed analysis such as exact percentages and number of videos per topic are available in the supplementary material
sentences using punctuation markers manually provided by transcripts.",3.1 Data Acquisition,[0],[0]
"Due to the high quality of the transcripts, using punctuation markers showed better sentence quality than using the Stanford CoreNLP tokenizer (Manning et al., 2014).",3.1 Data Acquisition,[0],[0]
This was verified on a set of 20 random videos by two experts.,3.1 Data Acquisition,[0],[0]
"After tokenization, a set of 23,453 sentences were chosen as the final sentences in the dataset.",3.1 Data Acquisition,[0],[0]
This was achieved by restricting each identity to contribute at least 10 and at most 50 sentences to the dataset.,3.1 Data Acquisition,[0],[0]
Table 2 shows high-level summary statistics of the CMU-MOSEI dataset.,3.1 Data Acquisition,[0],[0]
"Annotation of CMU-MOSEI follows closely the annotation of CMU-MOSI (Zadeh et al., 2016a) and Stanford Sentiment Treebank (Socher et al., 2013).",3.2 Annotation,[0],[0]
"Each sentence is annotated for sentiment on a [-3,3] Likert scale of:",3.2 Annotation,[0],[0]
"[−3: highly negative, −2 negative, −1 weakly negative, 0 neutral, +1 weakly positive, +2 positive, +3 highly positive].",3.2 Annotation,[0],[0]
"Ekman emotions (Ekman et al., 1980) of {happiness, sadness, anger, fear, disgust, surprise} are annotated on a [0,3]",3.2 Annotation,[0],[0]
Likert scale for presence of emotion x:,3.2 Annotation,[0],[0]
"[0: no evidence of x, 1: weakly x, 2: x, 3: highly x].",3.2 Annotation,[0],[0]
The annotation was carried out by 3 crowdsourced judges from Amazon Mechanical Turk platform.,3.2 Annotation,[0],[0]
"To avert implicitly biasing the judges and to capture the raw perception of the crowd, we avoided extreme annotation training and instead provided the judges with a 5 minutes training video on how to use the annotation system.",3.2 Annotation,[0],[0]
"All the annotations have been carried out by only master workers with higher than 98% approval rate to assure high quality annotations 4.
",3.2 Annotation,[0],[0]
Figure 2 shows the distribution of sentiment and emotions in CMU-MOSEI dataset.,3.2 Annotation,[0],[0]
"The distribution
4Extensive statistics of the dataset including the crawling mechanism, the annotation UI, training procedure for the workers, agreement scores are available in submitted supplementary material available on arXiv.
",3.2 Annotation,[0],[0]
shows a slight shift in favor of positive sentiment which is similar to distribution of CMU-MOSI and SST.,3.2 Annotation,[0],[0]
"We believe that this is an implicit bias in online opinions being slightly shifted towards positive, since this is also present in CMU-MOSI.",3.2 Annotation,[0],[0]
The emotion histogram shows different prevalence for different emotions.,3.2 Annotation,[0],[0]
"The most common category is happiness with more than 12,000 positive sample points.",3.2 Annotation,[0],[0]
The least prevalent emotion is fear with almost 1900 positive sample points which is an acceptable number for machine learning studies.,3.2 Annotation,[0],[0]
Data points in CMU-MOSEI come in video format with one speaker in front of the camera.,3.3 Extracted Features,[0],[0]
"The extracted features for each modality are as follows (for other benchmarks we extract the same features):
Language:",3.3 Extracted Features,[0],[0]
All videos have manual transcription.,3.3 Extracted Features,[0],[0]
"Glove word embeddings (Pennington et al., 2014) were used to extract word vectors from transcripts.",3.3 Extracted Features,[0],[0]
"Words and audio are aligned at phoneme level using P2FA forced alignment model (Yuan and Liberman, 2008).",3.3 Extracted Features,[0],[0]
"Following this, the visual and acoustic modalities are aligned to the words by interpolation.",3.3 Extracted Features,[0],[0]
"Since the utterance duration of words in English is usually short, this interpolation does not lead to substantial information loss.
",3.3 Extracted Features,[0],[0]
Visual: Frames are extracted from the full videos at 30Hz.,3.3 Extracted Features,[0],[0]
"The bounding box of the face is extracted using the MTCNN face detection algorithm (Zhang et al., 2016).",3.3 Extracted Features,[0],[0]
"We extract facial action units through Facial Action Coding System (FACS) (Ekman et al., 1980).",3.3 Extracted Features,[0],[0]
"Extracting these action units allows for accurate tracking and understanding of the facial expressions (Baltrušaitis
et al., 2016).",3.3 Extracted Features,[0],[0]
"We also extract a set of six basic emotions purely from static faces using Emotient FACET (iMotions, 2017).",3.3 Extracted Features,[0],[0]
"MultiComp OpenFace (Baltrušaitis et al., 2016) is used to extract the set of 68 facial landmarks, 20 facial shape parameters, facial HoG features, head pose, head orientation and eye gaze (Baltrušaitis et al., 2016).",3.3 Extracted Features,[0],[0]
"Finally, we extract face embeddings from commonly used facial recognition models such as DeepFace (Taigman et al., 2014), FaceNet (Schroff et al., 2015) and SphereFace (Liu et al., 2017).
",3.3 Extracted Features,[0],[0]
"Acoustic: We use the COVAREP software (Degottex et al., 2014) to extract acoustic features including 12 Mel-frequency cepstral coefficients, pitch, voiced/unvoiced segmenting features (Drugman and Alwan, 2011), glottal source parameters (Drugman et al., 2012; Alku et al., 1997, 2002), peak slope parameters and maxima dispersion quotients (Kane and Gobl, 2013).",3.3 Extracted Features,[0],[0]
All extracted features are related to emotions and tone of speech.,3.3 Extracted Features,[0],[0]
"From the linguistics perspective, understanding the interactions between language, visual and audio modalities in multimodal language is a fundamental research problem.",4 Multimodal Fusion Study,[0],[0]
"While previous works have been successful with respect to accuracy metrics, they have not created new insights on how the fusion is performed in terms of what modalities are related and how modalities engage in an interaction during fusion.",4 Multimodal Fusion Study,[0],[0]
"Specifically, to understand the fusion process one must first understand the n-modal dynamics (Zadeh et al., 2017).",4 Multimodal Fusion Study,[0],[0]
n-modal dynamics state that there exists different combination of modalities and that all of these combinations must be captured to better understand the multimodal language.,4 Multimodal Fusion Study,[0],[0]
"In this paper, we define building the n-modal dynamics as a hierarchical process and propose a new fusion model called the Dynamic Fusion Graph (DFG).",4 Multimodal Fusion Study,[0],[0]
DFG is easily interpretable through what is called efficacies in graph connections.,4 Multimodal Fusion Study,[0],[0]
"To utilize this new fusion model in a multimodal language framework, we build upon Memory Fusion Network (MFN) by replacing the original fusion component in the MFN with our DFG.",4 Multimodal Fusion Study,[0],[0]
We call this resulting model the Graph Memory Fusion Network (Graph-MFN).,4 Multimodal Fusion Study,[0],[0]
"Once the model is trained end to end, we analyze the efficacies in the DFG to study the fusion mechanism learned for modalities in multimodal language.",4 Multimodal Fusion Study,[0],[0]
"In addition to being an interpretable fusion mechanism,
Graph-MFN also outperforms previously proposed state-of-the-art models for sentiment analysis and emotion recognition on the CMU-MOSEI.",4 Multimodal Fusion Study,[0],[0]
In this section we discuss the internal structure of the proposed Dynamic Fusion Graph (DFG) neural model (Figure 3.,4.1 Dynamic Fusion Graph,[0],[0]
"DFG has the following properties: 1) it explicitly models the n-modal interactions, 2) does so with an efficient number of parameters (as opposed to previous approaches such as Tensor Fusion (Zadeh et al., 2017)) and 3) can dynamically alter its structure and choose the proper fusion graph based on the importance of each n-modal dynamics during inference.",4.1 Dynamic Fusion Graph,[0],[0]
"We assume the set of modalities to be M = {(l)anguage, (v)ision, (a)coustic}.",4.1 Dynamic Fusion Graph,[0],[0]
"The unimodal dynamics are denoted as {l},{v},{a}, the bimodal dynamics as {l, v},{v, a},{l, a} and trimodal dynamics as {l, v, a}.",4.1 Dynamic Fusion Graph,[0],[0]
"These dynamics are in the form of latent representations and are each considered as vertices inside a graph G = (V,E) with V the set of vertices and E the set of edges.",4.1 Dynamic Fusion Graph,[0],[0]
A directional neural connection is established between two vertices vi and vj only if vi ⊂ vj .,4.1 Dynamic Fusion Graph,[0],[0]
"For example, {l} ⊂ {l, v} which results in a connection between < language > and < language, vision >.",4.1 Dynamic Fusion Graph,[0],[0]
This connection is denoted as an edge eij .,4.1 Dynamic Fusion Graph,[0],[0]
"Dj takes as input all vi that satisfy the neural connection formula above for vj .
",4.1 Dynamic Fusion Graph,[0],[0]
We define an efficacy for each edge eij denoted as αij .,4.1 Dynamic Fusion Graph,[0],[0]
vi is multiplied by αij before being used as input toDj .,4.1 Dynamic Fusion Graph,[0],[0]
"Each α is a sigmoid activated probabil-
ity neuron which indicates how strong or weak the connection is between vi and vj .",4.1 Dynamic Fusion Graph,[0],[0]
αs are the main source of interpretability in DFG.,4.1 Dynamic Fusion Graph,[0],[0]
"The vector of all αs is inferred using a deep neural network Dα which takes as input singleton vertices in V (l, v, and a).",4.1 Dynamic Fusion Graph,[0],[0]
"We leave it to the supervised training objective to learn parameters of Dα and make good use of efficacies, thus dynamically controlling the structure of the graph.",4.1 Dynamic Fusion Graph,[0],[0]
The singleton vertices are chosen for this purpose since they have no incoming edges thus no efficacy associated with those edges (no efficacy is needed to infer the singleton vertices).,4.1 Dynamic Fusion Graph,[0],[0]
"The same singleton vertices l, v, and a are the inputs to the DFG.",4.1 Dynamic Fusion Graph,[0],[0]
In the next section we discuss how these inputs are given to DFG.,4.1 Dynamic Fusion Graph,[0],[0]
All vertices are connected to the output vertex Tt of the network via edges scaled by their respective efficacy.,4.1 Dynamic Fusion Graph,[0],[0]
"The overall structure of the vertices, edges and respective efficacies is shown in Figure 3.",4.1 Dynamic Fusion Graph,[0],[0]
"There are a total of 8 vertices (counting the output vertex), 19 edges and subsequently 19 efficacies.",4.1 Dynamic Fusion Graph,[0],[0]
"To test the performance of DFG, we use a similar recurrent architecture to Memory Fusion Network (MFN).",4.2 Graph-MFN,[0],[0]
MFN is a recurrent neural model with three main components 1) System of LSTMs: a set of parallel LSTMs with each LSTM modeling a single modality.,4.2 Graph-MFN,[0],[0]
"2) Delta-memory Attention Network is the component that performs multimodal fusion
by assigning coefficients to highlight cross-modal dynamics.",4.2 Graph-MFN,[0],[0]
3) Multiview Gated Memory is a component that stores the output of multimodal fusion.,4.2 Graph-MFN,[0],[0]
We replace the Delta-memory Attention Network with DFG and refer to the modified model as Graph Memory Fusion Network (Graph-MFN).,4.2 Graph-MFN,[0],[0]
"Figure 4 shows the overall architecture of the Graph-MFN.
Similar to MFN, Graph-MFN employs a system of LSTMs for modeling individual modalities.",4.2 Graph-MFN,[0],[0]
"cl, cv, and ca represent the memory of LSTMs for language, vision and acoustic modalities respectively.",4.2 Graph-MFN,[0],[0]
"Dm, m ∈ {l, v, a} is a fully connected deep neural network that takes in hm[t−1,t] the LSTM representation across two consecutive timestamps, which allows the network to track changes in memory dimensions across time.",4.2 Graph-MFN,[0],[0]
"The outputs of Dl, Dv and Da are the singleton vertices for the DFG.",4.2 Graph-MFN,[0],[0]
The DFG models cross-modal interactions and encodes the cross-modal representations in its output vertex Tt for storage in the Multi-view Gated Memory ut.,4.2 Graph-MFN,[0],[0]
The Multi-view Gated Memory functions using a network Du that transforms Tt into a proposed memory update ût. γ1 and γ2 are the Multi-view Gated Memory’s retain and update gates respectively and are learned using networks Dγ1 and Dγ2 .,4.2 Graph-MFN,[0],[0]
"Finally, a network Dz transforms Tt into a multimodal representation zt to update the system of LSTMs.",4.2 Graph-MFN,[0],[0]
"The output of Graph-MFN in all the experiments is the output of each LSTM hmT as well as contents of the Multi-view Gated Memory at time T (last recurrence timestep), uT .",4.2 Graph-MFN,[0],[0]
"This output
is subsequently connected to a classification or regression layer for final prediction (for sentiment and emotion recognition).",4.2 Graph-MFN,[0],[0]
"In our experiments, we seek to evaluate how modalities interact during multimodal fusion by studying the efficacies of DFG through time.
",5 Experiments and Discussion,[0],[0]
Table 3 shows the results on CMU-MOSEI.,5 Experiments and Discussion,[0],[0]
Accuracy is reported as Ax where x is the number of sentiment classes as well as F1 measure.,5 Experiments and Discussion,[0],[0]
For regression we report MAE and correlation (r).,5 Experiments and Discussion,[0],[0]
"For emotion recognition due to the natural imbalances across various emotions, we use weighted accuracy (Tong et al., 2017) and F1 measure.",5 Experiments and Discussion,[0],[0]
Graph-MFN shows superior performance in sentiment analysis and competitive performance in emotion recognition.,5 Experiments and Discussion,[0],[0]
"Therefore, DFG is both an effective and interpretable model for multimodal fusion.
",5 Experiments and Discussion,[0],[0]
"To better understand the internal fusion mechanism between modalities, we visualize the behavior of the learned DFG efficacies in Figure 5 for various cases (deep red denotes high efficacy and deep blue denotes low efficacy).
",5 Experiments and Discussion,[0],[0]
Multimodal Fusion has a Volatile Nature:,5 Experiments and Discussion,[0],[0]
The first observation is that the structure of the DFG is changing case by case and for each case over time.,5 Experiments and Discussion,[0],[0]
"As a result, the model seems to be selectively prioritizing certain dynamics over the others.",5 Experiments and Discussion,[0],[0]
"For example, in case (I) where all modalities are informative, all efficacies seem to be high, imply-
ing that the DFG is able to find useful information in unimodal, bimodal and trimodal interactions.",5 Experiments and Discussion,[0],[0]
"However, in cases (II) and (III) where the visual modality is either uninformative or contradictory, the efficacies of v → l, v and v → l, a, v and l, a→ l, a, v are reduced since no meaningful interactions involve the visual modality.
",5 Experiments and Discussion,[0],[0]
Priors in Fusion: Certain efficacies remain unchanged across cases and across time.,5 Experiments and Discussion,[0],[0]
These are priors from Human Multimodal Language that DFG learns.,5 Experiments and Discussion,[0],[0]
"For example the model always seems to prioritize fusion between language and audio in (l → l, a), and (a → l, a).",5 Experiments and Discussion,[0],[0]
"Subsequently, DFG gives low values to efficacies that rely unilaterally on language or audio alone: the (l → τ) and (a→ τ) efficacies seem to be consistently low.",5 Experiments and Discussion,[0],[0]
"On the other hand, the visual modality appears to have a partially isolated behavior.",5 Experiments and Discussion,[0],[0]
"In the presence of informative visual information, the model increases the efficacies of (v → τ) although the values of other visual efficacies also increase.
",5 Experiments and Discussion,[0],[0]
"Trace of Multimodal Fusion: We trace the dominant path that every modality undergoes during fusion: 1) language tends to first fuse with audio via (l → l, a) and the language and acoustic modalities together engage in higher level fusions such as (l, a → l, a, v).",5 Experiments and Discussion,[0],[0]
"Intuitively, this is aligned with the close ties between language and audio through word intonations.",5 Experiments and Discussion,[0],[0]
2),5 Experiments and Discussion,[0],[0]
The visual modality seems to engage in fusion only if it contains meaningful information.,5 Experiments and Discussion,[0],[0]
"In cases (I) and (IV), all the paths involving the visual modality are relatively active while in cases (II) and (III) the paths involv-
ing the visual modality have low efficacies.",5 Experiments and Discussion,[0],[0]
3),5 Experiments and Discussion,[0],[0]
The acoustic modality is mostly present in fusion with the language modality.,5 Experiments and Discussion,[0],[0]
"However, unlike language, the acoustic modality also appears to fuse with the visual modality if both modalities are meaningful, such as in case (I).
",5 Experiments and Discussion,[0],[0]
"An interesting observation is that in almost all cases the efficacies of unimodal connections to terminal T is low, implying that T prefers to not rely on just one modality.",5 Experiments and Discussion,[0],[0]
"Also, DFG always prefers to perform fusion between language and audio as in most cases both l → l, a and a → l, a have high efficacies; intuitively in most natural scenarios language and acoustic modalities are highly aligned.",5 Experiments and Discussion,[0],[0]
"Both of these cases show unchanging behaviors which we believe DFG has learned as natural priors of human communicative signal.
",5 Experiments and Discussion,[0],[0]
"With these observations, we believe that DFG has successfully learned how to manage its internal structure to model human communication.",5 Experiments and Discussion,[0],[0]
In this paper we presented the largest dataset of multimodal sentiment analysis and emotion recognition called CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI).,6 Conclusion,[0],[0]
"CMUMOSEI consists of 23,453 annotated sentences from more than 1000 online speakers and 250 different topics.",6 Conclusion,[0],[0]
The dataset expands the horizons of Human Multimodal Language studies in NLP.,6 Conclusion,[0],[0]
One such study was presented in this paper where we analyzed the structure of multimodal fusion in sentiment analysis and emotion recognition.,6 Conclusion,[0],[0]
"This was
done using a novel interpretable fusion mechanism called Dynamic Fusion Graph (DFG).",6 Conclusion,[0],[0]
In our studies we investigated the behavior of modalities in interacting with each other using built-in efficacies of DFG.,6 Conclusion,[0],[0]
"Aside analysis of fusion, DFG was trained in the Memory Fusion Network pipeline and showed superior performance in sentiment analysis and competitive performance in emotion recognition.",6 Conclusion,[0],[0]
This material is based upon work partially supported by the National Science Foundation (Award #1833355) and Oculus VR.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or Oculus VR, and no official endorsement should be inferred.",Acknowledgments,[0],[0]
Analyzing human multimodal language is an emerging area of research in NLP.,abstractText,[0],[0]
"Intrinsically human communication is multimodal (heterogeneous), temporal and asynchronous; it consists of the language (words), visual (expressions), and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences.",abstractText,[0],[0]
"From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of multimodal language.",abstractText,[0],[0]
"In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date.",abstractText,[0],[0]
"Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to investigate how modalities interact with each other in human multimodal language.",abstractText,[0],[0]
"Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competitive performance compared to the current state of the art.",abstractText,[0],[0]
Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 150–161 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
150",text,[0],[0]
Computational modeling of human multimodal language is an upcoming research area in natural language processing.,1 Introduction,[0],[0]
"This research area focuses on modeling tasks such as multimodal sentiment analysis (Morency et al., 2011), emotion recognition (Busso et al., 2008), and personality traits recognition (Park et al., 2014).",1 Introduction,[0],[0]
"The multimodal temporal signals include the language (spoken words), visual (facial expressions, gestures) and acoustic modalities (prosody, vocal expressions).",1 Introduction,[0],[0]
"At its core, these multimodal signals are
Recurrent Multistage Fusion
highly structured with two prime forms of interactions: intra-modal and cross-modal interactions (Rajagopalan et al., 2016).",1 Introduction,[0],[0]
"Intra-modal interactions refer to information within a specific modality, independent of other modalities.",1 Introduction,[0],[0]
"For example, the arrangement of words in a sentence according
to the generative grammar of a language (Chomsky, 1957) or the sequence of facial muscle activations for the presentation of a frown.",1 Introduction,[0],[0]
Cross-modal interactions refer to interactions between modalities.,1 Introduction,[0],[0]
"For example, the simultaneous co-occurrence of a smile with a positive sentence or the delayed occurrence of a laughter after the end of a sentence.",1 Introduction,[0],[0]
"Modeling these interactions lie at the heart of human multimodal language analysis and has recently become a centric research direction in multimodal natural language processing (Liu et al., 2018; Pham et al., 2018; Chen et al., 2017), multimodal speech recognition (Sun et al., 2016; Gupta et al., 2017; Harwath and Glass, 2017; Kamper et al., 2017), as well as multimodal machine learning (Tsai et al., 2018; Srivastava and Salakhutdinov, 2012; Ngiam et al., 2011).
",1 Introduction,[0],[0]
"Recent advances in cognitive neuroscience have demonstrated the existence of multistage aggregation across human cortical networks and functions (Taylor et al., 2015), particularly during the integration of multisensory information (Parisi et al., 2017).",1 Introduction,[0],[0]
"At later stages of cognitive processing, higher level semantic meaning is extracted from phrases, facial expressions, and tone of voice, eventually leading to the formation of higher level crossmodal concepts (Parisi et al., 2017; Taylor et al., 2015).",1 Introduction,[0],[0]
"Inspired by these discoveries, we hypothesize that the computational modeling of crossmodal interactions also requires a multistage fusion process.",1 Introduction,[0],[0]
"In this process, cross-modal representations can build upon the representations learned during earlier stages.",1 Introduction,[0],[0]
"This decreases the burden on each stage of multimodal fusion and allows each stage of fusion to be performed in a more specialized and effective manner.
",1 Introduction,[0],[0]
"In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which automatically decomposes the multimodal fusion problem into multiple recursive stages.",1 Introduction,[0],[0]
"At each stage, a subset of multimodal signals is highlighted and fused with previous fusion representations (see Figure 1).",1 Introduction,[0],[0]
"This divide-and-conquer approach decreases the burden on each fusion stage, allowing each stage to be performed in a more specialized and effective way.",1 Introduction,[0],[0]
"This is in contrast with conventional fusion approaches which usually model interactions over multimodal signals altogether in one iteration (e.g., early fusion (Baltrušaitis et al., 2017)).",1 Introduction,[0],[0]
"In RMFN, temporal and intra-modal interactions are modeled by integrating our new multistage fusion process
with a system of recurrent neural networks.",1 Introduction,[0],[0]
"Overall, RMFN jointly models intra-modal and cross-modal interactions for multimodal language analysis and is differentiable end-to-end.
",1 Introduction,[0],[0]
"We evaluate RMFN on three different tasks related to human multimodal language: sentiment analysis, emotion recognition, and speaker traits recognition across three public multimodal datasets.",1 Introduction,[0],[0]
RMFN achieves state-of-the-art performance in all three tasks.,1 Introduction,[0],[0]
"Through a comprehensive set of ablation experiments and visualizations, we demonstrate the advantages of explicitly defining multiple recursive stages for multimodal fusion.",1 Introduction,[0],[0]
"Previous approaches in human multimodal language modeling can be categorized as follows: Non-temporal Models: These models simplify the problem by using feature-summarizing temporal observations (Poria et al., 2017).",2 Related Work,[0],[0]
"Each modality is represented by averaging temporal information through time, as shown for language-based sentiment analysis (Iyyer et al., 2015; Chen et al., 2016) and multimodal sentiment analysis (Abburi et al., 2016; Nojavanasghari et al., 2016; Zadeh et al., 2016; Morency et al., 2011).",2 Related Work,[0],[0]
"Conventional supervised learning methods are utilized to discover intra-modal and cross-modal interactions without specific model design (Wang et al., 2016; Poria et al., 2016).",2 Related Work,[0],[0]
"These approaches have trouble modeling long sequences since the average statistics do not properly capture the temporal intra-modal and cross-modal dynamics (Xu et al., 2013).",2 Related Work,[0],[0]
Multimodal Temporal Graphical Models: The application of graphical models in sequence modeling has been an important research problem.,2 Related Work,[0],[0]
"Hidden Markov Models (HMMs) (Baum and Petrie, 1966), Conditional Random Fields (CRFs) (Lafferty et al., 2001), and Hidden Conditional Random Fields (HCRFs) (Quattoni et al., 2007) were shown to work well on modeling sequential data from the language (Misawa et al., 2017; Ma and Hovy, 2016; Huang et al., 2015) and acoustic (Yuan and Liberman, 2008) modalities.",2 Related Work,[0],[0]
These temporal graphical models have also been extended for modeling multimodal data.,2 Related Work,[0],[0]
"Several methods have been proposed including multi-view HCRFs where the potentials of the HCRF are designed to model data from multiple views (Song et al., 2012), multi-layered CRFs with latent variables to learn hidden spatiotemporal dynamics from multi-view data (Song
et al., 2012), and multi-view Hierarchical Sequence Summarization models that recursively build up hierarchical representations (Song et al., 2013).",2 Related Work,[0],[0]
"Multimodal Temporal Neural Networks: More recently, with the advent of deep learning, Recurrent Neural Networks (Elman, 1990; Jain and Medsker, 1999) have been used extensively for language and speech based sequence modeling (Zilly et al., 2016; Soltau et al., 2016), sentiment analysis (Socher et al., 2013; dos Santos and Gatti, 2014; Glorot et al., 2011; Cambria, 2016), and emotion recognition (Han et al., 2014; Bertero et al., 2016; Lakomkin et al., 2018).",2 Related Work,[0],[0]
"Long-short Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997a) have also been extended for multimodal settings (Rajagopalan et al., 2016) and by learning binary gating mechanisms to remove noisy modalities (Chen et al., 2017).",2 Related Work,[0],[0]
"Recently, more advanced models were proposed to model both intra-modal and cross-modal interactions.",2 Related Work,[0],[0]
"These use Bayesian ranking algorithms (Herbrich et al., 2007) to model both person-independent and person-dependent features (Liang et al., 2018), generative-discriminative objectives to learn either joint (Pham et al., 2018) or factorized multimodal representations (Tsai et al., 2018), external memory mechanisms to synchronize multimodal data (Zadeh et al., 2018a), or lowrank tensors to approximate expensive tensor products (Liu et al., 2018).",2 Related Work,[0],[0]
"All these methods assume that cross-modal interactions should be discovered all at once rather than across multiple stages, where each stage solves a simpler fusion problem.",2 Related Work,[0],[0]
Our empirical evaluations show the advantages of the multistage fusion approach.,2 Related Work,[0],[0]
In this section we describe the Recurrent Multistage Fusion Network (RMFN) for multimodal language analysis (Figure 2).,3 Recurrent Multistage Fusion Network,[0],[0]
"Given a set of modalities{l(anguage), v(isual), a(coustic)}, the signal from each modality m ∈ {l, v, a} is represented as a temporal sequence Xm = {xm1 ,xm2 ,xm3 ,,xmT }, where xmt is the input at time t. Each sequence Xm is modeled with an intra-modal recurrent neural network (see subsection 3.3 for details).",3 Recurrent Multistage Fusion Network,[0],[0]
"At time t, each intra-modal recurrent network will output a unimodal representation hmt .",3 Recurrent Multistage Fusion Network,[0],[0]
The Multistage Fusion Process uses a recursive approach to fuse all unimodal representations hmt into a cross-modal representation zt which is then fed back into each intra-modal recurrent network.,3 Recurrent Multistage Fusion Network,[0],[0]
The Multistage Fusion Process (MFP) is a modular neural approach that performs multistage fusion to model cross-modal interactions.,3.1 Multistage Fusion Process,[0],[0]
"Multistage fusion is a divide-and-conquer approach which decreases the burden on each stage of multimodal fusion, allowing each stage to be performed in a more specialized and effective way.",3.1 Multistage Fusion Process,[0],[0]
"The MFP has three main modules: HIGHLIGHT, FUSE and SUMMARIZE.
",3.1 Multistage Fusion Process,[0],[0]
Two modules are repeated at each stage: HIGHLIGHT and FUSE.,3.1 Multistage Fusion Process,[0],[0]
"The HIGHLIGHT module identifies a subset of multimodal signals from[hlt,hvt ,hat ] that will be used for that stage of fusion.",3.1 Multistage Fusion Process,[0],[0]
The FUSE module then performs two subtasks simultaneously: a local fusion of the highlighted features and integration with representations from previous stages.,3.1 Multistage Fusion Process,[0],[0]
Both HIGHLIGHT and FUSE modules are realized using memorybased neural networks which enable coherence between stages and storage of previously modeled cross-modal interactions.,3.1 Multistage Fusion Process,[0],[0]
"As a final step, the SUMMARIZE module takes the multimodal representation of the final stage and translates it into a cross-modal representation zt.
",3.1 Multistage Fusion Process,[0],[0]
Figure 1 shows an illustrative example for multistage fusion.,3.1 Multistage Fusion Process,[0],[0]
The HIGHLIGHT module selects “neutral words” and “frowning” expression for the first stage.,3.1 Multistage Fusion Process,[0],[0]
The local and integrated fusion at this stage creates a representation reflecting negative emotion.,3.1 Multistage Fusion Process,[0],[0]
"For stage 2, the HIGHLIGHT module identifies the acoustic feature “loud voice”.",3.1 Multistage Fusion Process,[0],[0]
The local fusion at this stage interprets it as an expression of emphasis and is fused with the previous fusion results to represent a strong negative emotion.,3.1 Multistage Fusion Process,[0],[0]
"Finally, the highlighted features of “shrug” and “speech elongation” are selected and are locally interpreted as “ambivalence”.",3.1 Multistage Fusion Process,[0],[0]
The integration with previous stages then gives a representation closer to “disappointed”.,3.1 Multistage Fusion Process,[0],[0]
"In this section, we present the details of the three multistage fusion modules: HIGHLIGHT, FUSE and SUMMARIZE.",3.2 Module Descriptions,[0],[0]
Multistage fusion begins with the concatenation of intra-modal network outputs ht =m∈M hmt .,3.2 Module Descriptions,[0],[0]
"We use superscript [k] to denote the indices of each stage k = 1,,K during K total stages of multistage fusion.",3.2 Module Descriptions,[0],[0]
Let ⇥ denote the neural network parameters across all modules.,3.2 Module Descriptions,[0],[0]
HIGHLIGHT:,3.2 Module Descriptions,[0],[0]
"At each stage k, a subset of the multimodal signals represented in ht will be au-
tomatically highlighted for fusion.",3.2 Module Descriptions,[0],[0]
"Formally, this module is defined by the process function fH : a[k]t = fH(ht ; a[1∶k−1]t ,⇥) (1) where at stage k, a[k]t is a set of attention weights which are inferred based on the previously assigned attention weights a[1∶k−1]t .",3.2 Module Descriptions,[0],[0]
"As a result, the highlights at a specific stage k will be dependent on previous highlights.",3.2 Module Descriptions,[0],[0]
"To fully encapsulate these dependencies, the attention assignment process is performed in a recurrent manner using a LSTM which we call the HIGHLIGHT LSTM.",3.2 Module Descriptions,[0],[0]
"The initial HIGHLIGHT LSTM memory at stage 0, cHIGHLIGHT[0]t , is initialized using a networkM that maps ht into LSTM memory space: cHIGHLIGHT[0]t =M(ht ; ⇥) (2) This allows the memory mechanism of the HIGHLIGHT LSTM to dynamically adjust to the intra-modal representations ht.",3.2 Module Descriptions,[0],[0]
"The output of the HIGHLIGHT LSTM hHIGHLIGHT[k]t is softmax activated to produce attention weights a[k]t at every stage k of the multistage fusion process:
a[k]t j = exp (h HIGHLIGHT[k] t j)
∑hHIGHLIGHT[k]t d=1",3.2 Module Descriptions,[0],[0]
exp,3.2 Module Descriptions,[0],[0]
"(hHIGHLIGHT[k]t d) (3)
and a[k]t is fed as input into the HIGHLIGHT LSTM at stage k + 1.",3.2 Module Descriptions,[0],[0]
"Therefore, the HIGHLIGHT LSTM functions as a decoder LSTM (Sutskever
et al., 2014; Cho et al., 2014) in order to capture the dependencies on previous attention assignments.",3.2 Module Descriptions,[0],[0]
Highlighting is performed by element-wise multiplying the attention weights a[k]t with the concatenated intra-modal representations,3.2 Module Descriptions,[0],[0]
"ht:
h̃[k]t =",3.2 Module Descriptions,[0],[0]
ht ⊙ a[k]t (4) where ⊙ denotes the Hadamard product and h̃[k]t are the attended multimodal signals that will be used for the fusion at stage k. FUSE:,3.2 Module Descriptions,[0],[0]
The highlighted multimodal signals are simultaneously fused in a local fusion and then integrated with fusion representations from previous stages.,3.2 Module Descriptions,[0],[0]
"Formally, this module is defined by the process function fF :
s[k]t = fF (h̃[k]t ; s[1∶k−1]t ,⇥) (5) where s[k]t denotes the integrated fusion representations at stage k.",3.2 Module Descriptions,[0],[0]
We employ a FUSE LSTM to simultaneously perform the local fusion and the integration with previous fusion representations.,3.2 Module Descriptions,[0],[0]
The FUSE LSTM input gate enables a local fusion while the FUSE LSTM forget and output gates enable integration with previous fusion results.,3.2 Module Descriptions,[0],[0]
"The initial FUSE LSTM memory at stage 0, cFUSE[0]t , is initialized using random orthogonal matrices (Arjovsky et al., 2015; Le et al., 2015).",3.2 Module Descriptions,[0],[0]
SUMMARIZE:,3.2 Module Descriptions,[0],[0]
"After completing K recursive stages of HIGHLIGHT and FUSE, the SUMMARIZE operation generates a cross-modal
representation using all final fusion representations s[1∶K]t .",3.2 Module Descriptions,[0],[0]
"Formally, this operation is defined as: zt = S(s[1∶K]t ; ⇥) (6) where zt is the final output of the multistage fusion process and represents all cross-modal interactions discovered at time t. The summarized cross-modal representation is then fed into the intra-modal recurrent networks as described in the subsection 3.3.",3.2 Module Descriptions,[0],[0]
"To integrate the cross-modal representations zt with the temporal intra-modal representations, we employ a system of Long Short-term Hybrid Memories (LSTHMs) (Zadeh et al., 2018b).",3.3 System of Long Short-term Hybrid Memories,[0],[0]
"The LSTHM extends the LSTM formulation to include the cross-modal representation zt in a hybrid memory component:
imt+1 = (Wmi xmt+1",3.3 System of Long Short-term Hybrid Memories,[0],[0]
+Umi hmt +Vmi zt + bmi ) (7) fmt+1 = (Wmf xmt+1 +Umf hmt +Vmf zt + bmf ),3.3 System of Long Short-term Hybrid Memories,[0],[0]
(8) omt+1 = (Wmo xmt+1 +Umo hmt +Vmo zt + bmo ) (9) c̄mt+1,3.3 System of Long Short-term Hybrid Memories,[0],[0]
=Wmc̄ xmt+1 +Umc̄ hmt +Vmc̄ zt + bmc̄ (10) cmt+1,3.3 System of Long Short-term Hybrid Memories,[0],[0]
"= fmt ⊙ cmt + imt ⊙ tanh(c̄mt+1) (11) hmt+1 = omt+1 ⊙ tanh(cmt+1) (12)
where is the (hard-)sigmoid activation function, tanh is the tangent hyperbolic activation function,⊙ denotes the Hadamard product.",3.3 System of Long Short-term Hybrid Memories,[0],[0]
"i, f and o are the input, forget and output gates respectively.",3.3 System of Long Short-term Hybrid Memories,[0],[0]
c̄mt+1 is the proposed update to the hybrid memory cmt at time t + 1 and hmt is the time distributed output of each modality.,3.3 System of Long Short-term Hybrid Memories,[0],[0]
The cross-modal representation zt is modeled by the Multistage Fusion Process as discussed in subsection 3.2.,3.3 System of Long Short-term Hybrid Memories,[0],[0]
The hybrid memory cmt contains both intra-modal interactions from individual modalities xmt as well as the cross-modal interactions captured in zt.,3.3 System of Long Short-term Hybrid Memories,[0],[0]
The multimodal prediction task is performed using a final representation E which integrate (1) the last outputs from the LSTHMs and (2) the last crossmodal representation zT .,3.4 Optimization,[0],[0]
"Formally, E is defined as:
E =",3.4 Optimization,[0],[0]
( m∈M h m T ),3.4 Optimization,[0],[0]
"zT (13)
where denotes vector concatenation.",3.4 Optimization,[0],[0]
E can then be used as a multimodal representation for supervised or unsupervised analysis of multimodal language.,3.4 Optimization,[0],[0]
"It summarizes all modeled intra-modal
and cross-modal representations from the multimodal sequences.",3.4 Optimization,[0],[0]
RMFN is differentiable end-toend which allows the network parameters ⇥ to be learned using gradient descent approaches.,3.4 Optimization,[0],[0]
"To evaluate the performance and generalization of RMFN, three domains of human multimodal language were selected: multimodal sentiment analysis, emotion recognition, and speaker traits recognition.",4 Experimental Setup,[0],[0]
All datasets consist of monologue videos.,4.1 Datasets,[0],[0]
"The speaker’s intentions are conveyed through three modalities: language, visual and acoustic.",4.1 Datasets,[0],[0]
Multimodal Sentiment Analysis involves analyzing speaker sentiment based on video content.,4.1 Datasets,[0],[0]
Multimodal sentiment analysis extends conventional language-based sentiment analysis to a multimodal setup where both verbal and non-verbal signals contribute to the expression of sentiment.,4.1 Datasets,[0],[0]
"We use CMU-MOSI (Zadeh et al., 2016) which consists of 2199 opinion segments from online videos each annotated with sentiment in the range",4.1 Datasets,[0],[0]
"[-3,3].",4.1 Datasets,[0],[0]
Multimodal Emotion Recognition involves identifying speaker emotions based on both verbal and nonverbal behaviors.,4.1 Datasets,[0],[0]
"We perform experiments on the IEMOCAP dataset (Busso et al., 2008) which consists of 7318 segments of recorded dyadic dialogues annotated for the presence of human emotions happiness, sadness, anger and neutral.",4.1 Datasets,[0],[0]
Multimodal Speaker Traits Recognition involves recognizing speaker traits based on multimodal communicative behaviors.,4.1 Datasets,[0],[0]
"POM (Park et al., 2014) contains 903 movie review videos each annotated for 12 speaker traits: confident (con), passionate (pas), voice pleasant (voi), credible (cre), vivid (viv), expertise (exp), reserved (res), trusting (tru), relaxed (rel), thorough (tho), nervous (ner), persuasive (per) and humorous (hum).",4.1 Datasets,[0],[0]
"GloVe word embeddings (Pennington et al., 2014), Facet (iMotions, 2017) and COVAREP (Degottex et al., 2014) are extracted for the language, visual and acoustic modalities respectively 1.",4.2 Multimodal Features and Alignment,[0],[0]
"Forced alignment is performed using P2FA (Yuan and Liberman, 2008) to obtain the exact utterance times
1Details on feature extraction are in supplementary.
of each word.",4.2 Multimodal Features and Alignment,[0],[0]
"We obtain the aligned video and audio features by computing the expectation of their modality feature values over each word utterance time interval (Tsai et al., 2018).",4.2 Multimodal Features and Alignment,[0],[0]
"We compare to the following models for multimodal machine learning: MFN (Zadeh et al., 2018a) synchronizes multimodal sequences using a multi-view gated memory.",4.3 Baseline Models,[0],[0]
It is the current state of the art on CMU-MOSI and POM.,4.3 Baseline Models,[0],[0]
"MARN (Zadeh et al., 2018b) models intra-modal and cross-modal interactions using multiple attention coefficients and hybrid LSTM memory components.",4.3 Baseline Models,[0],[0]
GMELSTM(A),4.3 Baseline Models,[0],[0]
"(Chen et al., 2017) learns binary gating mechanisms to remove noisy modalities that are contradictory or redundant for prediction.",4.3 Baseline Models,[0],[0]
"TFN (Zadeh et al., 2017) models unimodal, bimodal and trimodal interactions using tensor products.
",4.3 Baseline Models,[0],[0]
"BC-LSTM (Poria et al., 2017) performs contextdependent sentiment analysis and emotion recognition, currently state of the art on IEMOCAP.",4.3 Baseline Models,[0],[0]
"EFLSTM concatenates the multimodal inputs and uses that as input to a single LSTM (Hochreiter and Schmidhuber, 1997b).",4.3 Baseline Models,[0],[0]
"We also implement the Stacked, (EF-SLSTM) (Graves et al., 2013)",4.3 Baseline Models,[0],[0]
"Bidirectional (EF-BLSTM) (Schuster and Paliwal, 1997) and Stacked Bidirectional (EF-SBLSTM) LSTMs.",4.3 Baseline Models,[0],[0]
"For descriptions of the remaining baselines, we refer the reader to EF-HCRF (Quattoni et al., 2007), EF/MV-LDHCRF (Morency et al., 2007), MV-HCRF (Song et al., 2012), EF/MVHSSHCRF (Song et al., 2013), MV-LSTM (Rajagopalan et al., 2016), DF (Nojavanasghari et al., 2016), SAL-CNN (Wang et al., 2016), C-MKL (Poria et al., 2015), THMM (Morency et al., 2011), SVM (Cortes and Vapnik, 1995; Park et al., 2014) and RF (Breiman, 2001).",4.3 Baseline Models,[0],[0]
"For classification, we report accuracy Ac where c denotes the number of classes and F1 score.",4.4 Evaluation Metrics,[0],[0]
"For regression, we report Mean Absolute Error MAE and Pearson’s correlation r. For MAE lower values indicate stronger performance.",4.4 Evaluation Metrics,[0],[0]
"For all remaining metrics, higher values indicate stronger performance.",4.4 Evaluation Metrics,[0],[0]
"Results on CMU-MOSI, IEMOCAP and POM are presented in Tables 1, 2 and 3 respectively2.",5.1 Performance on Multimodal Language,[0],[0]
"We achieve state-of-the-art or competitive results for all domains, highlighting RMFN’s capability in human multimodal language analysis.",5.1 Performance on Multimodal Language,[0],[0]
"We observe that RMFN does not improve results on IEMOCAP neutral emotion and the model outperforming RMFN is a memory-based fusion baseline (Zadeh et al., 2018a).",5.1 Performance on Multimodal Language,[0],[0]
We believe that this is because neutral expressions are quite idiosyncratic.,5.1 Performance on Multimodal Language,[0],[0]
"Some people may always look angry given their facial configuration (e.g., natural eyebrow raises of actor Jack Nicholson).",5.1 Performance on Multimodal Language,[0],[0]
"In these situations, it becomes useful to compare the current image with a memorized or aggregated representation of the speaker’s face.",5.1 Performance on Multimodal Language,[0],[0]
"Our proposed multistage fusion approach can easily be extended to memory-based fusion methods.
",5.1 Performance on Multimodal Language,[0],[0]
2Results for all individual baseline models are in supplementary.,5.1 Performance on Multimodal Language,[0],[0]
State-of-the-art (SOTA)1/2/3 represent the three best performing baseline models on each dataset.,5.1 Performance on Multimodal Language,[0],[0]
"To achieve a deeper understanding of the multistage fusion process, we study five research questions.",5.2 Analysis of Multistage Fusion,[0],[0]
(Q1): whether modeling cross-modal interactions across multiple stages is beneficial.,5.2 Analysis of Multistage Fusion,[0],[0]
(Q2): the effect of the number of stages K during multistage fusion on performance.,5.2 Analysis of Multistage Fusion,[0],[0]
(Q3): the comparison between multistage and independent modeling of cross-modal interactions.,5.2 Analysis of Multistage Fusion,[0],[0]
(Q4): whether modeling cross-modal interactions are helpful.,5.2 Analysis of Multistage Fusion,[0],[0]
(Q5): whether attention weights from the HIGHLIGHT module are required for modeling cross-modal interactions.,5.2 Analysis of Multistage Fusion,[0],[0]
"Q1: To study the effectiveness of the multistage fusion process, we test the baseline RMFN-R1 which performs fusion in only one stage instead of across
multiple stages.",5.2 Analysis of Multistage Fusion,[0],[0]
This model makes the strong assumption that all cross-modal interactions can be modeled during only one stage.,5.2 Analysis of Multistage Fusion,[0],[0]
"From Table 4, RMFN-R1 underperforms as compared to RMFN which performs multistage fusion.",5.2 Analysis of Multistage Fusion,[0],[0]
Q2: We test baselines RMFN-RK which perform K stages of fusion.,5.2 Analysis of Multistage Fusion,[0],[0]
"From Table 4, we observe that increasing the number of stages K increases the model’s capability to model cross-modal interactions up to a certain point (K = 3) in our experiments.",5.2 Analysis of Multistage Fusion,[0],[0]
Further increases led to decreases in performance and we hypothesize this is due to overfitting on the dataset.,5.2 Analysis of Multistage Fusion,[0],[0]
"Q3: To compare multistage against independent modeling of cross-modal interactions, we pay close attention to the performance comparison with respect to MARN which models multiple crossmodal interactions all at once (see Table 5).",5.2 Analysis of Multistage Fusion,[0],[0]
"RMFN shows improved performance, indicating that multistage fusion is both effective and efficient for human multimodal language modeling.",5.2 Analysis of Multistage Fusion,[0],[0]
Q4: RMFN (no MFP) represents a system of LSTHMs without the integration of zt from the MFP to model cross-modal interactions.,5.2 Analysis of Multistage Fusion,[0],[0]
"From Table 5, RMFN (no MFP) is outperformed by RMFN, confirming that modeling cross-modal interactions is crucial in analyzing human multimodal language.",5.2 Analysis of Multistage Fusion,[0],[0]
Q5: RMFN (no HIGHLIGHT) removes the HIGHLIGHT module from MFP during multistage fusion.,5.2 Analysis of Multistage Fusion,[0],[0]
"From Table 5, RMFN (no HIGHLIGHT) underperforms, indicating that highlighting multimodal representations using attention weights are important for modeling cross-modal interactions.",5.2 Analysis of Multistage Fusion,[0],[0]
"Using an attention assignment mechanism during the HIGHLIGHT process gives more interpretability to the model since it allows us to visualize the attended multimodal signals at each stage and
time step (see Figure 3).",5.3 Visualizations,[0],[0]
"Using RMFN trained on the CMU-MOSI dataset, we plot the attention weights across the multistage fusion process for three videos in CMU-MOSI.",5.3 Visualizations,[0],[0]
Based on these visualizations we first draw the following general observations on multistage fusion:,5.3 Visualizations,[0],[0]
Across stages: Attention weights change their behaviors across the multiple stages of fusion.,5.3 Visualizations,[0],[0]
Some features are highlighted by earlier stages while other features are used in later stages.,5.3 Visualizations,[0],[0]
This supports our hypothesis that RMFN learns to specialize in different stages of the fusion process.,5.3 Visualizations,[0],[0]
Across time: Attention weights vary over time and adapt to the multimodal inputs.,5.3 Visualizations,[0],[0]
We observe that the attention weights are similar if the input contains no new information.,5.3 Visualizations,[0],[0]
"As soon as new multimodal information comes in, the highlighting mechanism in RMFN adapts to these new inputs.",5.3 Visualizations,[0],[0]
Priors:,5.3 Visualizations,[0],[0]
"Based on the distribution of attention weights, we observe that the language and acoustic modalities seem the most commonly highlighted.",5.3 Visualizations,[0],[0]
"This represents a prior over the expression of sentiment in human multimodal language and is closely related to the strong connections between language and speech in human communication (Kuhl, 2000).",5.3 Visualizations,[0],[0]
Inactivity:,5.3 Visualizations,[0],[0]
Some attention coefficients are not active (always orange) throughout time.,5.3 Visualizations,[0],[0]
We hypothesize that these corresponding dimensions carry only intra-modal dynamics and are not involved in the formation of cross-modal interactions.,5.3 Visualizations,[0],[0]
"In addition to the general observations above, Figure 3 shows three examples where multistage fusion learns cross-modal representations across three different scenarios.",5.4 Qualitative Analysis,[0],[0]
"Synchronized Interactions: In Figure 3(a), the language features are highlighted corresponding to the utterance of the word “fun” that is highly indicative of sentiment (t = 5).",5.4 Qualitative Analysis,[0],[0]
This sudden change is also accompanied by a synchronized highlighting of the acoustic features.,5.4 Qualitative Analysis,[0],[0]
We also notice that the highlighting of the acoustic features lasts longer across the 3 stages since it may take multiple stages to interpret all the new acoustic behaviors (elongated tone of voice and phonological emphasis).,5.4 Qualitative Analysis,[0],[0]
"Asynchronous Trimodal Interactions: In Figure 3(b), the language modality displays ambiguous sentiment: “delivers a lot of intensity” can be inferred as both positive or negative.",5.4 Qualitative Analysis,[0],[0]
We observe that the circled attention units in the visual and acoustic features correspond to the asynchronous presence of a smile (t = 2 ∶ 5) and phonological emphasis (t = 3) respectively.,5.4 Qualitative Analysis,[0],[0]
These nonverbal behaviors resolve ambiguity in language and result in an overall display of positive sentiment.,5.4 Qualitative Analysis,[0],[0]
"We further
note the coupling of attention weights that highlight the language, visual and acoustic features across stages (t = 3 ∶ 5), further emphasizing the coordination of all three modalities during multistage fusion despite their asynchronous occurrences.",5.4 Qualitative Analysis,[0],[0]
"Bimodal Interactions: In Figure 3(c), the language modality is better interpreted in the context of acoustic behaviors.",5.4 Qualitative Analysis,[0],[0]
The disappointed tone and soft voice provide the nonverbal information useful for sentiment inference.,5.4 Qualitative Analysis,[0],[0]
This example highlights the bimodal interactions (t = 4 ∶ 7) in alternating stages: the acoustic features are highlighted more in earlier stages while the language features are highlighted increasingly in later stages.,5.4 Qualitative Analysis,[0],[0]
"This paper proposed the Recurrent Multistage Fusion Network (RMFN) which decomposes the multimodal fusion problem into multiple stages, each focused on a subset of multimodal signals.",6 Conclusion,[0],[0]
Extensive experiments across three publicly-available datasets reveal that RMFN is highly effective in modeling human multimodal language.,6 Conclusion,[0],[0]
"In addition to achieving state-of-the-art performance on all datasets, our comparisons and visualizations reveal that the multiple stages coordinate to capture both synchronous and asynchronous multimodal interactions.",6 Conclusion,[0],[0]
"In future work, we are interested in merging our model with memory-based fusion methods since they have complementary strengths as discussed in subsection 5.1.",6 Conclusion,[0],[0]
This material is based upon work partially supported by the National Science Foundation (Award #1833355) and Samsung.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or Samsung, and no official endorsement should be inferred.",Acknowledgements,[0],[0]
"The authors thank Yao Chong Lim, Venkata Ramana Murthy Oruganti, Zhun Liu, Ying Shen, Volkan Cirik, and the anonymous reviewers for their constructive comments on this paper.",Acknowledgements,[0],[0]
"Computational modeling of human multimodal language is an emerging research area in natural language processing spanning the language, visual and acoustic modalities.",abstractText,[0],[0]
Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions).,abstractText,[0],[0]
"In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion.",abstractText,[0],[0]
Crossmodal interactions are modeled using this multistage fusion approach which builds upon intermediate representations of previous stages.,abstractText,[0],[0]
Temporal and intra-modal interactions are modeled by integrating our proposed fusion approach with a system of recurrent neural networks.,abstractText,[0],[0]
"The RMFN displays state-of-the-art performance in modeling human multimodal language across three public datasets relating to multimodal sentiment analysis, emotion recognition, and speaker traits recognition.",abstractText,[0],[0]
"We provide visualizations to show that each stage of fusion focuses on a different subset of multimodal signals, learning increasingly discriminative multimodal representations.",abstractText,[0],[0]
Multimodal Language Analysis with Recurrent Multistage Fusion,title,[0],[0]
"As a cornerstone of unsupervised learning, clustering has been widely used in knowledge discovery problems (Jain
1Northeastern University, Boston, MA 2Brigham and Women’s Hospital, Harvard Medical School, Boston, MA.",1. Introduction,[0],[0]
"Correspondence to: Yale Chang <ychang@coe.neu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"et al., 1999).",1. Introduction,[0],[0]
"Given data matrix and a notion of similarity between samples, clustering aims to categorize data into different clusters so that samples in the same cluster are similar and samples in different clusters are dissimilar.",1. Introduction,[0],[0]
"Thus, depending on users’ notions of similarity, the same dataset can be clustered in different ways, also known as views (Niu et al., 2010).",1. Introduction,[0],[0]
"For example, face images can be clustered based on pose or identity; marbles can be clustered based on shape or color.",1. Introduction,[0],[0]
"However, how to properly define similarity between samples in a knowledge discovery problem is nontrivial.",1. Introduction,[0],[0]
"To help solve this challenge, semi-supervised clustering utilizes expert supervision to guide the clustering towards the right solution (Wagstaff & Cardie, 2000; Basu et al., 2008).",1. Introduction,[0],[0]
"Supervision is usually in the form of pairwise constraints between samples, including must-link (ML) and cannot-link (CL) constraints.
",1. Introduction,[0],[0]
"Instead of supervision from one expert, it is becoming more common for supervision to be available from multiple experts as data can be shared and processed by increasingly larger audiences (e.g., crowdsourcing (Howe, 2008) mechanisms such as Amazon Mechanical Turk and large collaborative consortiums (Zhang et al., 2011)).",1. Introduction,[0],[0]
"In an exploratory data analysis setting, where ground truths are not known, different experts might provide supervision (pairwise contraints) with varying views in mind.",1. Introduction,[0],[0]
"For example, one expert might be thinking of similarity/clustering based on pose and another expert might be providing inputs based on identity on the face image problem.",1. Introduction,[0],[0]
"Moreover, because experts are not oracles, their inputs are prone to errors as well.",1. Introduction,[0],[0]
"In this paper, we address a new clustering paradigm: how to discover multiple clustering structures in the data given potentially diverse constraints from multiple uncertain experts.
",1. Introduction,[0],[0]
Our objective of finding multiple clustering structures given inputs from multiple experts is motivated by discovering subtypes (clusters) of a complex lung disease called Chronic Obstructive Pulmonary Disease (COPD).,1. Introduction,[0],[0]
"COPD is currently the third leading cause of death in the US (Murphy et al., 2013).",1. Introduction,[0],[0]
"Although traditionally being called one disease, doctors believe there exist multiple disease subtypes and providing personalized clinical care to patients according to their disease subtypes can lead to more effective treatments.",1. Introduction,[0],[0]
We have collected constraints provided by multiple experts in the consortium.,1. Introduction,[0],[0]
"The experts had var-
ied backgrounds: 1) clinicians tend to provide constraints by comparing patients’ clinical measurements; and 2) radiologists tend to provide constraints based on examining patients’ computed tomography (CT) images.",1. Introduction,[0],[0]
"On the one hand, experts have disagreements on whether to put a pair of patients in the same group because they are focusing on different aspects of patients.",1. Introduction,[0],[0]
"On the other hand, experts of similar backgrounds (clinicians, radiologists) tend to provide shared views of clustering data albeit with noisy constraints.",1. Introduction,[0],[0]
"Because the various experts might have different views of clustering data, it does not make sense to learn a single consensus clustering solution; rather, we need to discover the multiple consensus clustering solutions in the different views.
",1. Introduction,[0],[0]
"One naive way to generate multiple clustering solutions from multiple uncertain experts is to separately apply semisupervised clustering (Bilenko et al., 2004; Davis et al., 2007; Basu et al., 2008) using constraints from each expert.",1. Introduction,[0],[0]
"The potential drawbacks of this strategy are 1) the clustering performance often drastically degrades in the presence of noisy constraints due to uncertainty of one expert; and 2) the resulting clustering solutions can be highly redundant due to the existence of similar expert views.
",1. Introduction,[0],[0]
There are a few existing approaches that can combine constraints from multiple experts but they are mostly designed to generate one clustering solution.,1. Introduction,[0],[0]
"Semi-crowdsourced clustering (SemiCrowd) combines constraints provided by multiple experts through filtering out uncertain pairs in the average sample similarity matrix (Yi et al., 2012).",1. Introduction,[0],[0]
"However, this approach can only generate one clustering solution.",1. Introduction,[0],[0]
"Similar to SemiCrowd, most consensus clustering methods are designed to generate one clustering solution (Strehl & Ghosh, 2002; Fern & Brodley, 2004; Topchy et al., 2005; Ghosh & Acharya, 2011).",1. Introduction,[0],[0]
"Another disadvantage is that consensus clustering methods do not work when only a small number of pairwise constraints are available.
",1. Introduction,[0],[0]
"There are a few multiple alternative clustering approaches that can output multiple clustering solutions (Caruana et al., 2006; Cui et al., 2007; Jain et al., 2008; Niu et al., 2010; 2012).",1. Introduction,[0],[0]
"However, all these methods are unsupervised; none of these methods are able to utilize expert inputs.
",1. Introduction,[0],[0]
"There are a few existing crowdsourcing approaches that learns an underlying grouping of experts (Tian & Zhu, 2012; Kajino et al., 2013; Moreno et al., 2015).",1. Introduction,[0],[0]
"However, they are all designed for classification problems, where experts provide labels on samples queried.",1. Introduction,[0],[0]
"In our clustering task, experts can not provide labels because how to define different clusters is still unknown and yet to be discovered.",1. Introduction,[0],[0]
"Instead, they can provide pairwise constraints by comparing sample pairs using their domain knowledge.",1. Introduction,[0],[0]
"Therefore, these classification-based approaches above cannot be used in our clustering task.
",1. Introduction,[0],[0]
Contributions.,1. Introduction,[0],[0]
"To address this new clustering paradigm, we build a Bayesian probabilistic model for learning multiple alternative consensus clustering views from experts’ constraints, we call Multiple Clustering Views from the Crowd (MCVC).",1. Introduction,[0],[0]
Multiple experts are automatically assigned to different latent views and constraints provided by each expert is assumed to be noisy perturbations of the clustering associated with that expert’s view.,1. Introduction,[0],[0]
"Thus, multiple clustering structures can be discovered.",1. Introduction,[0],[0]
"Furthermore, by explicitly modeling the uncertainty of each expert, experts with higher accuracies are assigned higher weights, leading to improved quality of the learned clustering structure in each view.",1. Introduction,[0],[0]
"The clustering structure for each expert view is modeled by a discriminative clustering model (Gomes et al., 2010), which has the advantages of 1) naturally introducing uncertainties in cluster assignments; 2) avoiding making assumptions on the generative process of clusters; and 3) being able to cluster samples that do not appear in the training set.",1. Introduction,[0],[0]
"We demonstrate that our MCVC outperforms competing alternatives on synthetic, benchmark data, and a real-world disease subtyping problem.",1. Introduction,[0],[0]
"We collect data matrix X ∈ Rn×d, where n is the number of samples and d is the number of features, and pairwise constraints provided by M experts S(1:M), where S(m) ∈ {0, 1,NULL}n×n represents the constraints provided by them-th expert.",2. Proposed Approach,[0],[0]
"For sample pair (xi, xj), S (m) ij = 1 means the m-th expert provides must-link (ML) constraint; S(m)ij = 0 means cannot-link (CL) constraint; S (m) ij",2. Proposed Approach,[0],[0]
= NULL means no constraint is provided.,2. Proposed Approach,[0],[0]
Our objective is to utilize constraints collected from these M experts to guide the clustering algorithm to discover multiple clustering structures in the data.,2. Proposed Approach,[0],[0]
We assume there exist multiple alternative expert views and the constraints provided by experts in each view are perturbations of the clustering solution associated with that view 1.,2.1. Multiple Alternative Clustering Views,[0],[0]
Let cm represent the latent view to which the mth expert is assigned to.,2.1. Multiple Alternative Clustering Views,[0],[0]
"Furthermore, Z(cm) are the latent clusters for each cm view.",2.1. Multiple Alternative Clustering Views,[0],[0]
"Since we do not know the underlying number of possible expert views, we automatically learn the number of expert views by assuming a Dirichlet
1Note that “view” in multi-view clustering (Bickel & Scheffer, 2004) means coming from different sources or feature sets; whereas, “view” in our case follows the terminology in multiple alternative clusterings which means different interpretation or point of view of the data.",2.1. Multiple Alternative Clustering Views,[0],[0]
Multi-view clustering only finds ONE clustering solution from multiple feature sets which are given.,2.1. Multiple Alternative Clustering Views,[0],[0]
"In contrast, our goal is to find MULTIPLE clustering solutions/views which are latent.
process (Ferguson, 1973) prior on cm.",2.1. Multiple Alternative Clustering Views,[0],[0]
"Since clustering is widely used for knowledge discovery, experts might not be certain on the constraints they provided.",2.2. Model Uncertainties of Experts’ Constraints,[0],[0]
"To incorporate the assumption that different experts may have different levels of expertises when providing constraints, we assume the uncerntainty of the m-th expert can be characterized by accuracy parameters (αm, βm), where αm represents the m-th expert’s sensitivity and βm represents specificity.",2.2. Model Uncertainties of Experts’ Constraints,[0],[0]
Sensitivity is defined as the probability of providing ML constraints for sample pairs from the same cluster in the ground truth.,2.2. Model Uncertainties of Experts’ Constraints,[0],[0]
Specificity is defined as the probability of providing CL constraints for sample pairs from different clusters in the ground truth.,2.2. Model Uncertainties of Experts’ Constraints,[0],[0]
"We model the conditional likelihood of S(m) given cm (the latent view) and Z(cm) (the latent cluster in each view) by a Bernoulli distribution as follows:
p(S (m) ij = 1|Z (cm)",2.2. Model Uncertainties of Experts’ Constraints,[0],[0]
"i = Z (cm) j , αm) = αm (1)
p(S (m) ij = 0|Z (cm)",2.2. Model Uncertainties of Experts’ Constraints,[0],[0]
i 6=,2.2. Model Uncertainties of Experts’ Constraints,[0],[0]
"Z (cm) j , βm) = βm (2)",2.2. Model Uncertainties of Experts’ Constraints,[0],[0]
"We consider the following when choosing the clustering model: 1) Since we need to model uncertainties of experts, instead of generating hard clustering results, the assignments to clusters should be associated with probabilities; 2) We should avoid making strong assumptions on the generative process of clusters, which can be easily violated in practice; 3) We should be able to cluster samples outside the training set.",2.3. Discriminative Clustering,[0],[0]
"The discriminative clustering model (Gomes et al., 2010) satisfies all these requirements.",2.3. Discriminative Clustering,[0],[0]
"Instead of assuming the generative process of data X , discriminative clustering directly models the conditional distribution of cluster label given data.",2.3. Discriminative Clustering,[0],[0]
"We model the conditional distribution of the latent cluster label, Z(cm), given weight W (cm) and offset b(cm) with a multiple logistic regression model:
p(Z (cm)",2.3. Discriminative Clustering,[0],[0]
i =,2.3. Discriminative Clustering,[0],[0]
"k|W (cm), b(cm);X) =
ew (cm)T k xi+b (cm) k∑K
j=1",2.3. Discriminative Clustering,[0],[0]
e,2.3. Discriminative Clustering,[0],[0]
"w
(cm)T j xi+b (cm) j
(3)
where w(cm)k is the k-th column of W (cm) ∈ Rd×K and b (cm) k is the k-th row of b (cm) ∈ RK×1.",2.3. Discriminative Clustering,[0],[0]
We describe the prior distributions of parameters used in our model.,2.4. Prior Distributions,[0],[0]
"To incorporate the assumptions that experts’ accuracies should be far away from random guess (αm = βm = 0.5), we put Beta priors on αm, βm and set their
parameters to make most of their probability densities be far away from 0.5 and close to 1.
p(αm)",2.4. Prior Distributions,[0],[0]
"= Beta(τ (m) α10 , τ (m) α20 ) (4) p(βm) = Beta(τ (m) β10 , τ (m) β20 ) (5)
To automatically learn the number of expert views, we assume a Dirichlet process prior on cm and utilize the stickbreaking construction as follows (Blei et al., 2006): νg ∼ Beta(1, γ);",2.4. Prior Distributions,[0],[0]
"πg ∼ νg g−1∏ j=1 (1− νj); cm ∼ Cat(π1:∞)
Therefore, p(cm|ν1:∞) can be written as
p(cm|ν1:∞) = ∞∏ g=1 πcmgg = ∞∏ g=1 ( νg g−1∏ j=1 (1− νj) )cmg (6)
where cmg = 1 if cm = g and cmg = 0",2.4. Prior Distributions,[0],[0]
"otherwise.
",2.4. Prior Distributions,[0],[0]
"We assume the prior distributions of both weight W (g) and offset b(g) are factorized Gaussian distributions.
p(W (g))",2.4. Prior Distributions,[0],[0]
= d∏ i=1,2.4. Prior Distributions,[0],[0]
"K∏ j=1 N (µ(g)Wij0, σ",2.4. Prior Distributions,[0],[0]
(,2.4. Prior Distributions,[0],[0]
"g)2 Wij0 ) (7)
p(b(g)) = K∏ i=1",2.4. Prior Distributions,[0],[0]
"N (µ(g)bi0, σ (g)2 bi0 ) (8)",2.4. Prior Distributions,[0],[0]
"The overall joint distribution of observations and latent variables for our model is:
p(S(1:M), α1:M , β1:M , c1:M , ν1:∞, Z (1:∞),W (1:∞), b(1:∞))
",2.5. Joint Distribution,[0],[0]
= M∏ m=1,2.5. Joint Distribution,[0],[0]
"p(S(m)|αm, βm, cm, Z(1:∞))p(αm)p(βm) (9)
p(cm|ν1:∞) ∞∏",2.5. Joint Distribution,[0],[0]
g=1,2.5. Joint Distribution,[0],[0]
"p(Z(g)|W (g), b(g);X)p(W (g))p(b(g))p(νg)
",2.5. Joint Distribution,[0],[0]
The respective graphical model is shown in Figure 1.,2.5. Joint Distribution,[0],[0]
"Our learning objective is to maximize the marginal likelihood of observed constraints, which is intractable.",3. Variational Inference,[0],[0]
"Thus, we apply variational inference.",3. Variational Inference,[0],[0]
"Given variational distribution q(h; θ), where h is the collection of latent variables and θ is their parameters.",3. Variational Inference,[0],[0]
"the log of the marginal likelihood log p(S(1:M)) can be decomposed as
log p(S(1:M))",3. Variational Inference,[0],[0]
= Lq(h;θ) +KL [ q(h; θ) | p(h|S(1:M)) ],3. Variational Inference,[0],[0]
"≥ Lq(h;θ) (10)
where the inequality holds due to the nonnegativity of the the Kullback-Liebler (KL) divergence.",3. Variational Inference,[0],[0]
"Lq(h;θ) is called the evidence lower bound (ELBO) of log p(S(1:M)):
Lq(h;θ) = Eq(h;θ) [ log p(h, S(1:M))− log q(h; θ) ] (11)
",3. Variational Inference,[0],[0]
"In variational inference, the learning objective becomes maximizing Lq(h;θ) w.r.t.",3. Variational Inference,[0],[0]
"variational parameters θ.
Let h = {α1:M , β1:M , c1:M , ν1:G, Z(1:G),W (1:G), b(1:G)}, where G is the number of components of the truncated Dirichlet Process (Blei et al., 2006).",3. Variational Inference,[0],[0]
"We apply mean-field and assume q(h; θ) can be factorized as follows:
q(α1:M , β1:M , c1:M , ν1:G, Z (1:G),W (1:G), b(1:G))
",3. Variational Inference,[0],[0]
= M∏,3. Variational Inference,[0],[0]
m=1,3. Variational Inference,[0],[0]
[q(αm) · q(βm) · q(cm)],3. Variational Inference,[0],[0]
"·
G∏ g=1",3. Variational Inference,[0],[0]
[ q(νg),3. Variational Inference,[0],[0]
n∏ i=1 q(Z (g) i ) d∏ i=1,3. Variational Inference,[0],[0]
K∏ j=1 q(W (g) ij ),3. Variational Inference,[0],[0]
K∏ i=1,3. Variational Inference,[0],[0]
q(b (g) i ) ],3. Variational Inference,[0],[0]
"(12)
where the marginal distribution of each random variable is
q(αm) = Beta(τ (m) α1 , τ (m) α2 )",3. Variational Inference,[0],[0]
"(13) q(βm) = Beta(τ (m) β1 , τ (m) β2 )",3. Variational Inference,[0],[0]
"(14)
q(cm) =",3. Variational Inference,[0],[0]
"Cat(φm,:) (15)
q(νg) = Beta(τ",3. Variational Inference,[0],[0]
"(g) ν1 , τ (g) ν2 )",3. Variational Inference,[0],[0]
"(16)
q(Z (g) i ) =",3. Variational Inference,[0],[0]
"Cat(η (g) i,: ) (17)
q(W (g) ij )",3. Variational Inference,[0],[0]
"= N (µ (g) Wij , σ",3. Variational Inference,[0],[0]
"(g)2 Wij ) (18)
",3. Variational Inference,[0],[0]
"q(b (g) i ) = N (µ (g) bi , σ (g)2 bi ) (19)
We use θ to denote all the variational parameters, which consist of {τ (m)α1 , τ (m) α2 , τ (m) β1 , τ (m) β2 , φm,:} (m = 1 · · ·M ) and {τ (g)ν1 , τ (g) ν2 , η (g) i,: , µ (g) Wij , σ",3. Variational Inference,[0],[0]
"(g) Wij , µ(g)bi , σ (g) bi } (g = 1 · · ·G).",3. Variational Inference,[0],[0]
"Besides the simplex constraints on the parameters of the categorical distribution, both the parameters of the Beta distribution and the standard deviation of Gaussian distribution should have positive constraints.
",3. Variational Inference,[0],[0]
We derive the closed-form formula of Lq(h;θ) as a function of θ and put the detailed steps in the supplementary materials due to space constraints.,3. Variational Inference,[0],[0]
"Since ELBO Lq(h;θ) can be written as a function of variational parameters θ, we can directly maximize Lq(h;θ) using gradient-based optimization approaches.",3. Variational Inference,[0],[0]
"The gradient ∂Lq(h;θ)∂θ can be automatically computed using reverse-mode differentiation (Maclaurin et al., 2015).",3. Variational Inference,[0],[0]
"We choose to use a limited-memory projected quasi-Newton algorithm (PQN) to optimize our objective because it has both superlinear convergence rate and linear memory requirement (Schmidt et al., 2009).",3. Variational Inference,[0],[0]
"Because our objective Lq(h;θ) is not concave, we provide multiple initializations θ(0) to the optimization algorithm and choose the one resulting in the maximal objective value.",3. Variational Inference,[0],[0]
We set the number of random initializations to be 50 in all experiments and the results are stable across different runs.,3. Variational Inference,[0],[0]
"In this section, we aim to demonstrate our MCVC can automatically 1) assign multiple experts to different views; and 2) improve the quality of clustering solution in each view by assigning higher weights to uncertain experts of higher accuracies.",4. Experimental Results,[0],[0]
We also analyze how 3) the settings of the number of clusters; and 4) the constraints provided by irrelevant experts affect the performance of MCVC.,4. Experimental Results,[0],[0]
"For aim 1), we construct two views of experts based on two different ways to cluster the data.",4.1. Competing Alternatives,[0],[0]
These two expert views are treated as the ground truth of assigning multiple experts to different views.,4.1. Competing Alternatives,[0],[0]
"Then we compare our MCVC against an adapted version of meta clustering (Caruana et al., 2006).
",4.1. Competing Alternatives,[0],[0]
Meta Spectral Clustering (MetaClust): The original meta clustering approach cannot handle multiple experts’ constraints.,4.1. Competing Alternatives,[0],[0]
"Instead of using the data matrix to generate multiple clustering solutions as input, we directly use the constraint sets provided by multiple experts as input.",4.1. Competing Alternatives,[0],[0]
"Meta clustering first computes the similarity between experts by computing the rand index (Rand, 1971) between the constraint sets they provide.",4.1. Competing Alternatives,[0],[0]
"Given the resulting similarity matrix between experts, instead of hierarchical clustering as in the original paper, we apply spectral clustering (Ng et al., 2001) to assign multiple experts to different views.",4.1. Competing Alternatives,[0],[0]
"We determine the number of expert views by maximizing the gap between the consecutive eigenvalues of the graph Laplacian (Von Luxburg, 2007).
",4.1. Competing Alternatives,[0],[0]
"For aim 2), we first construct one view of experts based on one way to cluster the data.",4.1. Competing Alternatives,[0],[0]
The underlying clustering structure is treated as the ground truth of clustering samples.,4.1. Competing Alternatives,[0],[0]
"Then we compare our MCVC against the following alternatives in generating clusters of high quality.
",4.1. Competing Alternatives,[0],[0]
"SemiCrowd: SemiCrowd (Yi et al., 2012) combines multiple expert constraints by filtering out uncertain pairs in the average similarity matrix, applying matrix completion, and then learning a distance metric for clustering.
",4.1. Competing Alternatives,[0],[0]
Semi-supervised Clustering:,4.1. Competing Alternatives,[0],[0]
"Given a set of pairwise constraints, semi-supervised clustering either learns a better distance metric for clustering or guides the clustering algorithm to satisfy those constraints.",4.1. Competing Alternatives,[0],[0]
"We use Informationtheoretic Metric Learning (ITML) (Davis et al., 2007) and Metric Pairwise Constrained KMeans (MPCKMeans) (Bilenko et al., 2004) as the representatives of those two strategies due to their superior performances compared to alternatives.",4.1. Competing Alternatives,[0],[0]
"Since semi-supervised clustering can only take one set of constraints as input, we combine constraints from multiple experts through majority voting (a sample pair is given ML constraint if the majority of experts provide ML constraints for them and CL constraint otherwise).
",4.1. Competing Alternatives,[0],[0]
Consensus Clustering: Most consensus clustering algorithms only work with cluster labels instead of pairwise constraints.,4.1. Competing Alternatives,[0],[0]
"However, Cluster-based Similarity Partitioning Algorithm (CSPA) (Strehl & Ghosh, 2002), a consensus clustering approach that only need average similarity matrix between samples as input, can be used in our setup.
",4.1. Competing Alternatives,[0],[0]
We provide the parameter setting details for all methods in the supplementary materials due to space constraint.,4.1. Competing Alternatives,[0],[0]
"Synthetic Dataset: To help understand the algorithms, we generate a synthetic data that has multiple alternative clustering views.",4.2. Synthetic and Benchmark Experiments,[0],[0]
We generate a synthetic dataset containing 600 samples and six features.,4.2. Synthetic and Benchmark Experiments,[0],[0]
The scatterplots between pairwise features in this dataset are shown in Figure 2.,4.2. Synthetic and Benchmark Experiments,[0],[0]
"First, there exist three clusters in the subspace spanned by the first two features, which we denote as Y1.",4.2. Synthetic and Benchmark Experiments,[0],[0]
"In all these three figures, the red, blue and green colors represent the true cluster indicator of Y1.",4.2. Synthetic and Benchmark Experiments,[0],[0]
"Second, there exist an alternative clustering structure in the subspace spanned by the third and fourth features, which we denote as Y2.",4.2. Synthetic and Benchmark Experiments,[0],[0]
Note that Y2 and Y1 are very distinct.,4.2. Synthetic and Benchmark Experiments,[0],[0]
"Third, the subspace spanned by the fifth and sixth feature does not contain well-separated clusters and we consider these as noisy features.
",4.2. Synthetic and Benchmark Experiments,[0],[0]
WebKB,4.2. Synthetic and Benchmark Experiments,[0],[0]
Dataset:,4.2. Synthetic and Benchmark Experiments,[0],[0]
"The WebKB dataset (web, 1998) con-
tains webpages collected from four universities.",4.2. Synthetic and Benchmark Experiments,[0],[0]
"After removing stop words and extracting the top 200 words with most frequent occurrences, we obtain a data matrix with 1041 samples and 200 features.",4.2. Synthetic and Benchmark Experiments,[0],[0]
"The data can be clustered according to either owner types (course, faculty, project, student), which we treat as Y1, or universities (Cornell, Austin, Washington, Wisconsin), which we use as Y2.
",4.2. Synthetic and Benchmark Experiments,[0],[0]
Face Dataset:,4.2. Synthetic and Benchmark Experiments,[0],[0]
"The Face dataset (Lichman, 2013) consists of 640 face images of people taken with varying poses (straight, left, right, up).",4.2. Synthetic and Benchmark Experiments,[0],[0]
Each image has 960 raw pixels.,4.2. Synthetic and Benchmark Experiments,[0],[0]
We apply principal component analysis (PCA) and keep 20 principal components (explaining 80% variance).,4.2. Synthetic and Benchmark Experiments,[0],[0]
"Thus, we obtain a data matrix with 640 samples and 20 features.",4.2. Synthetic and Benchmark Experiments,[0],[0]
"The data can be clustered based on pose (Y1) or identity (Y2).
",4.2. Synthetic and Benchmark Experiments,[0],[0]
"Simulating Constraints from Multiple Experts: For synthetic and benchmark datasets, we do not have access to real-world constraints provided by experts.",4.2. Synthetic and Benchmark Experiments,[0],[0]
"Therefore, we simulate these constraints.",4.2. Synthetic and Benchmark Experiments,[0],[0]
"Given a ground-truth clustering solution Y , the number of ML constraints nML, the number of CL constraints nCL, and accuracy parameters of M experts α1:M , β1:M , we can generate the constraints provided by the m-th expert as follows: 1) randomly sample nML ML constraints and nCL CL constraints from Y ; 2) randomly flip nML(1 − αm) ML pairs to CL pairs and flip nCL(1− βm) CL pairs to ML pairs.",4.2. Synthetic and Benchmark Experiments,[0],[0]
"In this subsection, we test the performance of our MCVC on automatically learning the latent views from multiple experts.",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"We simulate noisy constraints provided by multiple expert from two latent views, Y1 and Y2 as follows: 1) the first view consists of experts 1-5, who provide constraints based on clustering solution Y1 and have accuracy parameters α1:5 = β1:5 = (0.95, 0.9, 0.85, 0.8, 0.75); 2) the second view consists of experts 6-10, who provide constraints based on clustering solution Y2 and have accuracy parameters α6:10 = β6:10 = (0.75, 0.8, 0.85, 0.9, 0.95).
",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"We compare the performance of our MCVC and meta spectral clustering (MetaClust) in recovering the ground-truth expert views as the number of constraints, ncon, is varied from 200 to a large number that makes the performances of both approaches become stable.",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
We repeat the constraints generation process ten times to avoid the randomness of a single run.,4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"As a result, we obtain ten constraint sets for a fixed number of constraints.",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"Given one constraint set, we run MCVC and MetaClust to generate two possibly different ways to group experts, which are denoted as LMCVC and LMetaClust respectively.",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"We measure performance based on the normalized mutual information (NMI) (Strehl & Ghosh, 2002) between LMCVC, LMetaClust
and LTrue, the ground-truth expert views.",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
NMI measures the similarity between two partitions.,4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"In our case, higher NMI values indicate better performance.",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"For a fixed number of constraints, one constraint set, and one approach, we obtain ten NMI values.",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"We plot the mean and standard deviation for every set of ten NMI values of each approach as we vary the number of constraints as shown in Figure 3.
",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"We have the following observations: 1) On the synthetic dataset, our MCVC can consistently recover the groundtruth expert views; 2) On WebKB and Face, the performance of MCVC improves as the number of constraints increases and can recover the ground-truth expert views when the number of constraints becomes large enough; 3) On WebKB and Face, when the number of constraints is too small, as is shown in the left part of each figure, MCVC will be dominated by the priors and therefore cannot perfectly recover the expert views.",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"4) On all three datasets, meta clustering fails to recover the ground-truth expert views.",4.2.1. TASK 1: LEARNING THE VARIOUS LATENT VIEWS FROM MULTIPLE EXPERTS,[0],[0]
"AN EXPERT VIEW
",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"In this subsection, we test the performance of our MCVC against competing methods in learning the clustering structure given constraints provided by multiple experts from one view.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"We simulate noisy constraints provided by multiple expert based on one ground-truth view, Y1.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"We consider two different settings for their accuracy parameters: 1) experts have unequal accuracies α1:5 = β1:5 = (0.95, 0.9, 0.85, 0.8, 0.75); 2) experts have equal and high accuracies α1:5 = β1:5 = (0.95, 0.95, 0.95, 0.95, 0.95).
",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"Our objective is to show that through learning different accuracies of multiple uncertain experts, our MCVC can generate better clustering results compared to competing alternatives (SemiCrowd, ITML, MPCKMeans and CSPA).
",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
We vary the total number of ML/CL constraints provided by each expert from 200 to 2000 (as described in the previous subsection).,4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"For each fixed number of constraints, we randomly generate 10 constraint sets.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"For each constraint set, we run all approaches and obtain their clustering solu-
tions.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
We measure performance based on NMI between the resulting clustering solutions and the ground-truth solution Y1.,4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"For a fixed number of constraints, one constraint set, and one approach, we obtain 10 NMI values.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
We plot the mean and standard deviation for every set of 10 NMI values for each approach as we vary the number of constraints as shown in Figure 4.,4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"To compare the performance of different approaches, we apply the Kruskal-Wallis test (Kruskal & Wallis, 1952), which can be used to test whether two groups of samples are drawn from the same distribution, on their corresponding groups of NMI values.
",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"From the left figures, where the accuracy parameters are set to be unequal, we have the following observations: I)",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
Our MCVC consistently outperforms all competing alternatives.,4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
II),4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"The performances of semi-supervised clustering approaches, including ITML and MPCKMeans, do not consistently improve as the number of constraints increases.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
III),4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
The performance of SemiCrowd increases as the number of constraints increases.,4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"This means it can ef-
fectively reduce the noise in the constraints.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"However, it does not work well when the number of constraints is too small.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"IV) consensus clustering (CSPA) fails because the number of constraints is too small to be used to construct accurate sample similarity matrix.
",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"From the right figures, where the accuracy parameters are set to be equal and have high values, we have the following observations: V) Our MCVC consistently outperforms SemiCrowd, ITML and CSPA but does not consistently outperform MPCKMeans.
",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"By comparing the right against the left figures, we have the following observations: VI) MPCKMeans works well only when all the experts have high accuracies; it fails when not all experts have high accuaracies (as shown on the left figures).",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
VII) Both ITML and SemiCrowd do not benefit much from higher percentage of correct constraints; VIII),4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"Our MCVC perfoms well in both settings.
",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
Explanation:,4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
Our MCVC has good performance in the unequal accuracies case because it can learn the accuracy parameters of different experts and assign higher weights to more accurate experts and lower weights to uncertain experts respectively.,4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"On the synthetic dataset, the posterior distributions of accuracy parameters α1:5, β1:5 computed from MCVC are shown in Figure 5.
",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"As we can see, the modes of their distributions are very close to their true values.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"In our MCVC, the constraints from the m-th expert are assigned weight ωm = log αmβm(1−αm)(1−βm) when combining the constraints fromM experts (we put the derivation details in the supplementary materials).",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"Therefore, higher accuracies (αm, βm) naturally lead to higher weights.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"Thus, our MCVC becomes robust to noisy constraints in the unequal accuracies case.",4.2.2. TASK 2: DISCOVER CLUSTERING SOLUTION IN,[0],[0]
"Number of Clusters: To investigate how the setting of K, the number of clusters, affects the performance of our
MCVC, we vary K from 2 to 8 and run MCVC on the synthetic dataset using two expert views constructed from the procedures described in subsection 4.2.1.",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
The number of constraints is fixed to be 2000 and the constraints generation process is repeated 10 times.,4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"First, for any value of K, MCVC can correctly assign experts to two views and also generate two clustering solutions.",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"Second, to evaluate how the clustering performances are affected by K, we compute the NMI between the two output clustering solutions and Y1, Y2 respectively.
",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
The errorbar plot is shown in Figure 6 (a).,4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"As we can see, our MCVC is very robust to the setting of K as long as K ≥ K∗, where K∗ is the maximal number of clusters among all clustering views and K∗ = 3 for our synthetic data.",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"In problems where K∗ is unknown, we can set K to be some large value to increase the possibility that K ≥ K∗.",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"However, in practice, we also observe that more constraints are needed to effectively train the model when K becomes large because there will be a larger number of parameters.
",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
Irrelevant Experts: We define irrelevant experts as those who provide constraints based on a notion of similarity that is not supported by any feature in the data.,4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"To demonstrate how the existence of irrelevant experts affect the performance of our MCVC, we simulate three expert views using the synthetic data: 1) the first two views are the same as those described in subsection 4.2.1; and 2) the third view consists of five irrelevant experts who provide constraints based on a random clustering solution Y3 and have accuracy parameters",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"α11:15 = β11:15 = (0.95, 0.9, 0.85, 0.8, 0.75).",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
We generate 2000 constraints from each expert.,4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"First, the first two expert views can still be perfectly recovered by our MCVC.",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"Second, the irrelevant experts (experts 11-15) are either assigned to the first two expert views or new expert views.
",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"We plot the posterior distributions of accuracy parameters for all 15 experts in Figure 6 (b,c).",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"As we can see, the
densities of irrelevant experts’ accuracy parameters (red curves) concentrate around 0.5, which indicates the constraints from those experts are treated as random guesses and assigned near zero weights by MCVC.",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"In practice, we can identify irrelevant experts by checking whether the modes of their accuracy parameters’ posteriors are close to 0.5.",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
"Therefore, our MCVC is robust to irrelevant experts.",4.2.3. SENSITIVITY ANALYSIS: TO THE NUMBER OF CLUSTERS AND TO IRRELEVANT EXPERTS,[0],[0]
Chronic Obstructive Pulmonary Disease (COPD) is a complex lung disease characterized by increasing breathlessness.,4.3. COPD Subtyping Experiment,[0],[0]
"Although being called one disease, doctors believe there exist different disease subtypes.",4.3. COPD Subtyping Experiment,[0],[0]
"The identification of different disease subtypes (clusters) can lead to tailored medical care for each patient.
",4.3. COPD Subtyping Experiment,[0],[0]
Dataset:,4.3. COPD Subtyping Experiment,[0],[0]
"We collected 39 features from 987 COPD patients, including clinical measurements, demographics, lung function and measures from CT chest imaging.
",4.3. COPD Subtyping Experiment,[0],[0]
"Experts’ Constraints: We also collected constraints provided by 29 experts, including clinicians and radiologists.",4.3. COPD Subtyping Experiment,[0],[0]
We need to utilize experts’ constraints to guide the clustering algorithm.,4.3. COPD Subtyping Experiment,[0],[0]
"However, the key challenge is that different experts disagree on whether to put a pair of patients in the same cluster.",4.3. COPD Subtyping Experiment,[0],[0]
We suspect there exist different expert views and experts in each view provide constraints based on a shared way to cluster the data.,4.3. COPD Subtyping Experiment,[0],[0]
"The discovery of these expert views and their corresponding clustering solutions can provide more options for further investigation.
",4.3. COPD Subtyping Experiment,[0],[0]
"Evaluation: Since ground truth is not known, we can no longer use NMI to compare the performances of competing approaches.",4.3. COPD Subtyping Experiment,[0],[0]
"However, there are some key genetic variables that are known to be related to COPD, including copdScore (Busch et al., 2017), HHIP (Pillai et al., 2009), MMP12 (Cho et al., 2014).",4.3. COPD Subtyping Experiment,[0],[0]
"A clustering solution is considered as useful/relevant if patients in different clusters show significant differences on these genetic variables.
",4.3. COPD Subtyping Experiment,[0],[0]
We randomly split the dataset into half training set and half testing set.,4.3. COPD Subtyping Experiment,[0],[0]
All approaches are learned using the training set and the constraints from multiple experts.,4.3. COPD Subtyping Experiment,[0],[0]
The learned models can be used to cluster test samples and generate clustering solutions.,4.3. COPD Subtyping Experiment,[0],[0]
"To evaluate each solution, we compute its associated p-values on these genetic variables by applying Kruskal-Wallis test on copdScore (continuous) and χ2 test on HHIP and MMP12 (discrete).",4.3. COPD Subtyping Experiment,[0],[0]
"We use P < 0.05 to identify significant differences in these genetic variables.
Methods and Results: We first apply our MCVC and obtain 12 expert views.",4.3. COPD Subtyping Experiment,[0],[0]
"After removing irrelevant experts and views containing only one expert, there are 5 expert views left.",4.3. COPD Subtyping Experiment,[0],[0]
"Our physician collaborators identify 2 interesting expert views by analyzing the cluster characteristics of their associated clustering solutions, which are de-
noted as MCVC-A and MCVC-B respectively: 1) Solution MCVC-A contains emphysema-dominant and airwaydominant clusters, where emphysema cluster means the destruction of lung tissue and airway cluster means the increase of airway wall thickness.",4.3. COPD Subtyping Experiment,[0],[0]
"2) Solution MCVC-B contains clusters of different levels of disease severity.
",4.3. COPD Subtyping Experiment,[0],[0]
"For competing approaches, we first run meta clustering and all 29 experts were lumped into one view.",4.3. COPD Subtyping Experiment,[0],[0]
"Then we apply SemiCrowd, ITML and MPCKMeans to combine the data matrix and constraints from all experts.
",4.3. COPD Subtyping Experiment,[0],[0]
The p-values of solutions provided by our MCVC and competing approaches are shown in Table 1.,4.3. COPD Subtyping Experiment,[0],[0]
"As we can see, solutions provided by MCVC and MPCKMeans contain different clusters that show significant differences on all three COPD-related genetic variables.",4.3. COPD Subtyping Experiment,[0],[0]
"In contrast, both ITML and SemiCrowd are not significantly correlated with all three genetic variables.",4.3. COPD Subtyping Experiment,[0],[0]
There’s some overlap between solution MPCKMeans and solution MCVC-B (with NMI value 0.39).,4.3. COPD Subtyping Experiment,[0],[0]
"However, solution MCVC-A can only be discovered by our approach.",4.3. COPD Subtyping Experiment,[0],[0]
"This way of clustering COPD patients is consistent with some COPD investigators’ latest discovery of COPD subtypes (Castaldi et al., 2014).",4.3. COPD Subtyping Experiment,[0],[0]
"In this paper, we build a probabilistic model to discover multiple ways to cluster the data given potentially diverse inputs from multiple uncertain experts.",5. Conclusions,[0],[0]
This is achieved by automatically assigning multiple experts to different views and learning the clustering structure associated with each expert view.,5. Conclusions,[0],[0]
The quality of clustering solution in each expert view are improved by assigning higher weights to experts of higher accuracies.,5. Conclusions,[0],[0]
"Experimental results on synthetic data, benchmark datasets and a real-world disease subtyping problem demonstrate that our MCVC outperforms its competing alternatives, including meta clustering, semi-supervised clustering, semi-crowdsourced clustering and consensus clustering.",5. Conclusions,[0],[0]
"We would like to acknowledge support for this project from the NIH grant NIH/NHLBI RO1HL089856, RO1HL089857 and NSF/IIS-1546428.",6. Acknowledgements,[0],[0]
Expert input can improve clustering performance.,abstractText,[0],[0]
"In today’s collaborative environment, the availability of crowdsourced multiple expert input is becoming common.",abstractText,[0],[0]
"Given multiple experts’ inputs, most existing approaches can only discover one clustering structure.",abstractText,[0],[0]
"However, data is multi-faceted by nature and can be clustered in different ways (also known as views).",abstractText,[0],[0]
"In an exploratory analysis problem where ground truth is not known, different experts may have diverse views on how to cluster data.",abstractText,[0],[0]
"In this paper, we address the problem on how to automatically discover multiple ways to cluster data given potentially diverse inputs from multiple uncertain experts.",abstractText,[0],[0]
We propose a novel Bayesian probabilistic model that automatically learns the multiple expert views and the clustering structure associated with each view.,abstractText,[0],[0]
"The benefits of learning the experts’ views include 1) enabling the discovery of multiple diverse clustering structures, and 2) improving the quality of clustering solution in each view by assigning higher weights to experts with higher confidence.",abstractText,[0],[0]
"In our approach, the expert views, multiple clustering structures and expert confidences are jointly learned via variational inference.",abstractText,[0],[0]
"Experimental results on synthetic datasets, benchmark datasets and a real-world disease subtyping problem show that our proposed approach outperforms competing baselines, including meta clustering, semisupervised clustering, semi-crowdsourced clustering and consensus clustering.",abstractText,[0],[0]
Multiple Clustering Views from Multiple Uncertain Experts,title,[0],[0]
"Sentiment analysis has become a fundamental area of research in Natural Language Processing thanks to the proliferation of user-generated content in the form of online reviews, blogs, internet forums, and social media.",1 Introduction,[0],[0]
"A plethora of methods have been proposed in the literature that attempt to distill sentiment information from text, allowing users and service providers to make opinion-driven decisions.
",1 Introduction,[0],[0]
"The success of neural networks in a variety of applications (Bahdanau et al., 2015; Le and Mikolov, 2014; Socher et al., 2013) and the availability of large amounts of labeled data have led to an increased focus on sentiment classification.",1 Introduction,[0],[0]
"Supervised models are typically trained on documents (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b; Tang et al., 2015; Yang et al., 2016), sentences (Kim, 2014), or phrases (Socher et al., 2011;
Socher et al., 2013) annotated with sentiment labels and used to predict sentiment in unseen texts.",1 Introduction,[0],[0]
"Coarse-grained document-level annotations are relatively easy to obtain due to the widespread use of opinion grading interfaces (e.g., star ratings accompanying reviews).",1 Introduction,[0],[0]
"In contrast, the acquisition of sentence- or phrase-level sentiment labels remains a laborious and expensive endeavor despite its relevance to various opinion mining applications, e.g., detecting or summarizing consumer opinions in online product reviews.",1 Introduction,[0],[0]
"The usefulness of finer-grained sentiment analysis is illustrated in the example of Figure 1, where snippets of opposing polarities are extracted from a 2-star restaurant review.",1 Introduction,[0],[0]
"Although, as a whole, the review conveys negative sentiment, aspects of the reviewer’s experience were clearly positive.",1 Introduction,[0],[0]
"This goes largely unnoticed when focusing solely on the review’s overall rating.
",1 Introduction,[0],[0]
"In this work, we consider the problem of segmentlevel sentiment analysis from the perspective of Multiple Instance Learning (MIL; Keeler, 1991).
17
Transactions of the Association for Computational Linguistics, vol. 6, pp.",1 Introduction,[0],[0]
"17–31, 2018.",1 Introduction,[0],[0]
Action Editor: Ani Nenkova.,1 Introduction,[0],[0]
"Submission batch: 7/17; Revision batch: 11/2017; Published 1/2018.
",1 Introduction,[0],[0]
c©2018 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
"Instead of learning from individually labeled segments, our model only requires document-level supervision and learns to introspectively judge the sentiment of constituent segments.",1 Introduction,[0],[0]
"Beyond showing how to utilize document collections of rated reviews to train fine-grained sentiment predictors, we also investigate the granularity of the extracted segments.",1 Introduction,[0],[0]
"Previous research (Tang et al., 2015; Yang et al., 2016; Cheng and Lapata, 2016; Nallapati et al., 2017) has predominantly viewed documents as sequences of sentences.",1 Introduction,[0],[0]
"Inspired by recent work in summarization (Li et al., 2016) and sentiment classification (Bhatia et al., 2015), we also represent documents via Rhetorical Structure Theory’s (Mann and Thompson, 1988) Elementary Discourse Units (EDUs).",1 Introduction,[0],[0]
"Although definitions for EDUs vary in the literature, we follow standard practice and take the elementary units of discourse to be clauses (Carlson et al., 2003).",1 Introduction,[0],[0]
"We employ a state-of-the-art discourse parser (Feng and Hirst, 2012) to identify them.
",1 Introduction,[0],[0]
"Our contributions in this work are three-fold: a novel multiple instance learning neural model which utilizes document-level sentiment supervision to judge the polarity of its constituent segments; the creation of SPOT, a publicly available dataset which contains Segment-level POlariTy annotations (for sentences and EDUs) and can be used for the evaluation of MIL-style models like ours; and the empirical finding (through automatic and human-based evaluation) that neural multiple instance learning is superior to more conventional neural architectures and other baselines on detecting segment sentiment and extracting informative opinions in reviews.1",1 Introduction,[0],[0]
"Our work lies at the intersection of multiple research areas, including sentiment classification, opinion mining and multiple instance learning.",2 Background,[0],[0]
"We review related work in these areas below.
",2 Background,[0],[0]
Sentiment Classification Sentiment classification is one of the most popular tasks in sentiment analysis.,2 Background,[0],[0]
"Early work focused on unsupervised methods and the creation of sentiment lexicons (Turney, 2002; Hu and Liu, 2004; Wiebe et al., 2005; Baccianella et al., 2010) based on which the overall po-
1Our code and SPOT dataset are publicly available at: https://github.com/stangelid/milnet-sent
larity of a text can be computed (e,g., by aggregating the sentiment scores of constituent words).",2 Background,[0],[0]
"More recently, Taboada et al. (2011) introduced SO-CAL, a state-of-the-art method that combines a rich sentiment lexicon with carefully defined rules over syntax trees to predict sentence sentiment.
",2 Background,[0],[0]
"Supervised learning techniques have subsequently dominated the literature (Pang et al., 2002; Pang and Lee, 2005; Qu et al., 2010; Xia and Zong, 2010; Wang and Manning, 2012; Le and Mikolov, 2014) thanks to user-generated sentiment labels or large-scale crowd-sourcing efforts (Socher et al., 2013).",2 Background,[0],[0]
Neural network models in particular have achieved state-of-the-art performance on various sentiment classification tasks due to their ability to alleviate feature engineering.,2 Background,[0],[0]
"Kim (2014) introduced a very successful CNN architecture for sentence-level classification, whereas other work (Socher et al., 2011; Socher et al., 2013) uses recursive neural networks to learn sentiment for segments of varying granularity (i.e., words, phrases, and sentences).",2 Background,[0],[0]
"We describe Kim’s (2014) approach in more detail as it is also used as part of our model.
",2 Background,[0],[0]
Let xi denote a k-dimensional word embedding of the i-th word in text segment s of length n.,2 Background,[0],[0]
"The segment’s input representation is the concatenation of word embeddings x1, . . .",2 Background,[0],[0]
", xn, resulting in word matrix X .",2 Background,[0],[0]
"Let Xi:i+j refer to the concatenation of embeddings xi, . . .",2 Background,[0],[0]
", xi+j .",2 Background,[0],[0]
"A convolution filter W ∈ Rlk, applied to a window of l words, produces a new feature ci = ReLU(W ◦ Xi:i+l + b), where ReLU is the Rectified Linear Unit non-linearity, ‘◦’ denotes the entrywise product followed by a sum over all elements and b ∈ R is a bias term.",2 Background,[0],[0]
"Applying the same filter to every possible window of word vectors in the segment, produces a feature map c =",2 Background,[0],[0]
"[c1, c2, . . .",2 Background,[0],[0]
", cn−l+1].",2 Background,[0],[0]
"Multiple feature maps for varied window sizes are applied, resulting in a fixed-size segment representation v via max-overtime pooling.",2 Background,[0],[0]
"We will refer to the application of convolution to an input word matrix X , as CNN(X).",2 Background,[0],[0]
"A final sentiment prediction is produced using a softmax classifier and the model is trained via backpropagation using sentence-level sentiment labels.
",2 Background,[0],[0]
"The availability of large-scale datasets (Diao et al., 2014; Tang et al., 2015) has also led to the development of document-level sentiment classifiers which exploit hierarchical neural representations.
",2 Background,[0],[0]
"These are obtained by first building representations of sentences and aggregating those into a document feature vector (Tang et al., 2015).",2 Background,[0],[0]
Yang et al. (2016) further acknowledge that words and sentences are deferentially important in different contexts.,2 Background,[0],[0]
"They present a model which learns to attend (Bahdanau et al., 2015) to individual text parts when constructing document representations.",2 Background,[0],[0]
"We describe such an architecture in more detail as we use it as a point of comparison with our own model.
",2 Background,[0],[0]
"Given document d comprising segments (s1, . . .",2 Background,[0],[0]
", sm), a Hierarchical Network with attention (henceforth HIERNET; based on Yang et al., 2016) produces segment representations (v1, . . .",2 Background,[0],[0]
", vm) which are subsequently fed into a bidirectional GRU module (Bahdanau et al., 2015), whose resulting hidden vectors (h1, . . .",2 Background,[0],[0]
",hm) are used to produce attention weights (a1, . . .",2 Background,[0],[0]
", am) (see Section 3.2 for more details on the attention mechanism).",2 Background,[0],[0]
A document is represented as the weighted average of the segments’ hidden vectors vd = ∑ i aihi.,2 Background,[0],[0]
A final sentiment prediction is obtained using a softmax classifier and the model is trained via back-propagation using document-level sentiment labels.,2 Background,[0],[0]
The architecture is illustrated in Figure 2(a).,2 Background,[0],[0]
"In their proposed model, Yang et al. (2016) use bidirectional GRU modules to represent segments as well as documents, whereas we use a more efficient CNN encoder to compose words into segment vectors2 (i.e., vi = CNN(Xi)).",2 Background,[0],[0]
"Note that models like HIERNET do not naturally predict sentiment for individual segments; we discuss how they can be used for segment-level opinion extraction in Section 5.2.
",2 Background,[0],[0]
"Our own work draws inspiration from representation learning (Tang et al., 2015; Kim, 2014), especially the idea that not all parts of a document convey sentiment-worthy clues (Yang et al., 2016).",2 Background,[0],[0]
Our model departs from previous approaches in that it provides a natural way of predicting the polarity of individual text segments without requiring segment-level annotations.,2 Background,[0],[0]
"Moreover, our attention mechanism directly facilitates opinion detection rather than simply aggregating sentence representations into a single document vector.
",2 Background,[0],[0]
"2When applied to the YELP’13 and IMDB document classification datasets, the use of CNNs results in a relative performance decrease of < 2% compared Yang et al’s model (2016).
",2 Background,[0],[0]
"Opinion Mining A standard setting for opinion mining and summarization (Lerman et al., 2009; Carenini et al., 2006; Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014) assumes a set of documents that contain opinions about some entity of interest (e.g., camera).",2 Background,[0],[0]
"The goal of the system is to generate a summary that is representative of the average opinion and speaks to its important aspects (e.g., picture quality, battery life, value).",2 Background,[0],[0]
"Output summaries can be extractive (Lerman et al., 2009) or abstractive (Gerani et al., 2014; Di Fabbrizio et al., 2014) and the underlying systems exhibit varying degrees of linguistic sophistication from identifying aspects (Lerman et al., 2009) to using RSTstyle discourse analysis, and manually defined templates (Gerani et al., 2014; Di Fabbrizio et al., 2014).
",2 Background,[0],[0]
Our proposed method departs from previous work in that it focuses on detecting opinions in individual documents.,2 Background,[0],[0]
"Given a review, we predict the polarity of every segment, allowing for the extraction of sentiment-heavy opinions.",2 Background,[0],[0]
"We explore the usefulness of EDU segmentation inspired by Li et al. (2016), who show that EDU-based summaries align with near-extractive summaries constructed by news editors.",2 Background,[0],[0]
"Importantly, our model is trained in a weakly-supervised fashion on large scale document classification datasets without recourse to finegrained labels or gold-standard opinion summaries.
",2 Background,[0],[0]
Multiple Instance Learning Our models adopt a Multiple Instance Learning (MIL) framework.,2 Background,[0],[0]
"MIL deals with problems where labels are associated with groups of instances or bags (documents in our case), while instance labels (segment-level polarities) are unobserved.",2 Background,[0],[0]
An aggregation function is used to combine instance predictions and assign labels on the bag level.,2 Background,[0],[0]
"The goal is either to label bags (Keeler and Rumelhart, 1992; Dietterich et al., 1997; Maron and Ratan, 1998) or to simultaneously infer bag and instance labels (Zhou et al., 2009; Wei et al., 2014; Kotzias et al., 2015).",2 Background,[0],[0]
"We view segment-level sentiment analysis as an instantiation of the latter variant.
",2 Background,[0],[0]
"Initial MIL efforts for binary classification made the strong assumption that a bag is negative only if all of its instances are negative, and positive otherwise (Dietterich et al., 1997; Maron and Ratan, 1998; Zhang et al., 2002; Andrews and Hofmann, 2004; Carbonetto et al., 2008).",2 Background,[0],[0]
"Subsequent work re-
laxed this assumption, allowing for prediction combinations better suited to the tasks at hand.",2 Background,[0],[0]
"Weidmann et al. (2003) introduced a generalized MIL framework, where a combination of instance types is required to assign a bag label.",2 Background,[0],[0]
"Zhou et al. (2009) used graph kernels to aggregate predictions, exploiting relations between instances in object and text categorization.",2 Background,[0],[0]
"Xu and Frank (2004) proposed a multiple-instance logistic regression classifier where instance predictions were simply averaged, assuming equal and independent contribution toward bag classification.",2 Background,[0],[0]
"More recently, Kotzias et al. (2015) used sentence vectors obtained by a pre-trained hierarchical CNN (Denil et al., 2014) as features under an unweighted average MIL objective.",2 Background,[0],[0]
"Prediction averaging was further extended by Pappas and Popescu-Belis (2014; 2017), who used a weighted summation of predictions, an idea which we also adopt in our work.
",2 Background,[0],[0]
Applications of MIL are many and varied.,2 Background,[0],[0]
"MIL was first explored by Keeler and Rumelhart (1992) for recognizing handwritten post codes, where the position and value of individual digits was unknown.",2 Background,[0],[0]
"MIL techniques have since been applied to drug activity prediction (Dietterich et al., 1997), image retrieval (Maron and Ratan, 1998; Zhang et al., 2002), object detection (Zhang et al., 2006; Carbonetto et al., 2008; Cour et al., 2011), text classification (Andrews and Hofmann, 2004), image captioning (Wu et al., 2015), paraphrase detection (Xu et al., 2014), and information extraction (Hoffmann et al., 2011).
",2 Background,[0],[0]
"When applied to sentiment analysis, MIL takes advantage of supervision signals on the document level in order to train segment-level sentiment predictors.",2 Background,[0],[0]
"Although their work is not couched in the framework of MIL, Täckström and McDonald (2011) show how sentence sentiment labels can be learned as latent variables from document-level annotations using hidden conditional random fields.",2 Background,[0],[0]
Pappas and Popescu-Belis (2014) use a multiple instance regression model to assign sentiment scores to specific aspects of products.,2 Background,[0],[0]
"The Group-Instance Cost Function (GICF), proposed by Kotzias et al. (2015), averages sentence sentiment predictions during trainng, while ensuring that similar sentences receive similar polarity labels.",2 Background,[0],[0]
"Their work uses a pre-trained hierarchical CNN to obtain sentence embeddings, but is not trainable end-to-end, in contrast
with our proposed network.",2 Background,[0],[0]
"Additionally, none of the aforementioned efforts explicitly evaluate opinion extraction quality.",2 Background,[0],[0]
"In this section we describe how multiple instance learning can be used to address some of the drawbacks seen in previous approaches, namely the need for expert knowledge in lexicon-based sentiment analysis (Taboada et al., 2011), expensive finegrained annotation on the segment level (Kim, 2014; Socher et al., 2013) or the inability to naturally predict segment sentiment (Yang et al., 2016).",3 Methodology,[0],[0]
"Under multiple instance learning (MIL), a dataset D is a collection of labeled bags, each of which is a group of unlabeled instances.",3.1 Problem Formulation,[0],[0]
"Specifically, each document d is a sequence (bag) of segments (instances).",3.1 Problem Formulation,[0],[0]
"This sequence d = (s1, s2, . . .",3.1 Problem Formulation,[0],[0]
", sm) is obtained from a document segmentation policy (see Section 4 for details).",3.1 Problem Formulation,[0],[0]
A discrete sentiment label yd ∈,3.1 Problem Formulation,[0],[0]
"[1, C] is associated with each document, where the labelset is ordered and classes 1 and C correspond to maximally negative and maximally positive sentiment.",3.1 Problem Formulation,[0],[0]
"It is assumed that yd is an unknown function of the unobserved segment-level labels:
yd = f(y1, y2, . . .",3.1 Problem Formulation,[0],[0]
", ym) (1)
Probabilistic sentiment classifiers will produce document-level predictions ŷd by selecting the most probable class according to class distribution pd = 〈p(1)d , . . .",3.1 Problem Formulation,[0],[0]
", p (C) d 〉.",3.1 Problem Formulation,[0],[0]
"In a non-MIL framework a classifier would learn to predict the document’s sentiment by directly conditioning on its segments’ feature representations or their aggregate:
pd = f̂θ(v1, v2, . . .",3.1 Problem Formulation,[0],[0]
", vm) (2)
In contrast, a MIL classifier will produce a class distribution pi for each segment and additionally learn to combine these into a document-level prediction:
pi = ĝθs(vi) , (3)
pd = f̂θd(p1, p2, . . .",3.1 Problem Formulation,[0],[0]
",pm) .",3.1 Problem Formulation,[0],[0]
"(4)
In this work, ĝ and f̂ are defined using a single neural network, described below.",3.1 Problem Formulation,[0],[0]
Hierarchical neural models like HIERNET have been used to predict document-level polarity by first encoding sentences and then combining these representations into a document vector.,3.2 Multiple Instance Learning Network,[0],[0]
"Hierarchical vector composition produces powerful sentiment predictors, but lacks the ability to introspectively judge the polarity of individual segments.
",3.2 Multiple Instance Learning Network,[0],[0]
Our Multiple Instance Learning Network (henceforth MILNET) is based on the following intuitive assumptions about opinionated text.,3.2 Multiple Instance Learning Network,[0],[0]
"Each segment conveys a degree of sentiment polarity, ranging from very negative to very positive.",3.2 Multiple Instance Learning Network,[0],[0]
"Additionally, segments have varying degrees of importance, in relation to the overall opinion of the author.",3.2 Multiple Instance Learning Network,[0],[0]
"The overarching polarity of a text is an aggregation of segment polarities, weighted by their importance.",3.2 Multiple Instance Learning Network,[0],[0]
"Thus, our model attempts to predict the polarity of segments and decides which parts of the document are good indicators of its overall sentiment, allowing for the detection of sentiment-heavy opinions.",3.2 Multiple Instance Learning Network,[0],[0]
"An illustration of MILNET is shown in Figure 2(b); the model consists of three components: a CNN segment encoder, a softmax segment classifier and an attentionbased prediction weighting module.
",3.2 Multiple Instance Learning Network,[0],[0]
"Segment Encoding An encoding vi = CNN(Xi) is produced for each segment, using the CNN architecture described in Section 2.
",3.2 Multiple Instance Learning Network,[0],[0]
"Segment Classification Obtaining a separate representation vi for every segment in a document allows us to produce individual segment sentiment predictions pi = 〈p(1)i , . . .",3.2 Multiple Instance Learning Network,[0],[0]
", p (C) i 〉.",3.2 Multiple Instance Learning Network,[0],[0]
"This is achieved using a softmax classifier:
pi = softmax(Wcvi + bc) , (5)
where Wc and bc are the classifier’s parameters, shared across all segments.",3.2 Multiple Instance Learning Network,[0],[0]
"Individual distributions pi are shown in Figure 2(b) as small bar-charts.
",3.2 Multiple Instance Learning Network,[0],[0]
Document Classification,3.2 Multiple Instance Learning Network,[0],[0]
"In the simplest case, document-level predictions can be produced by taking the average of segment class distributions: p (c) d = 1/m ∑ i p (c) i , c ∈",3.2 Multiple Instance Learning Network,[0],[0]
"[1, C].",3.2 Multiple Instance Learning Network,[0],[0]
"This is, however, a crude way of combining segment sentiment, as not all parts of a document convey important sentiment clues.",3.2 Multiple Instance Learning Network,[0],[0]
"We opt for a segment attention mechanism which rewards text units that are more likely to be good sentiment predictors.
",3.2 Multiple Instance Learning Network,[0],[0]
"Our attention mechanism is based on a bidirectional GRU component (Bahdanau et al., 2015) and
inspired by Yang et al. (2016).",3.2 Multiple Instance Learning Network,[0],[0]
"However, in contrast to their work, where attention is used to combine sentence representations into a single document vector, we utilize a similar technique to aggregate individual sentiment predictions.
",3.2 Multiple Instance Learning Network,[0],[0]
"We first use separate GRU modules to produce forward and backward hidden vectors, which are then concatenated:
−→ h i = −−−→ GRU(vi), (6) ←−",3.2 Multiple Instance Learning Network,[0],[0]
"h i = ←−−− GRU(vi), (7)
",3.2 Multiple Instance Learning Network,[0],[0]
hi =,3.2 Multiple Instance Learning Network,[0],[0]
"[ −→ h i, ←−",3.2 Multiple Instance Learning Network,[0],[0]
h,3.2 Multiple Instance Learning Network,[0],[0]
"i], i ∈",3.2 Multiple Instance Learning Network,[0],[0]
"[1,m] .",3.2 Multiple Instance Learning Network,[0],[0]
"(8)
The importance of each segment is measured with the aid of a vector ha, as follows:
h′i = tanh(Wahi + ba) , (9) ai = exp(h′Ti ha)∑",3.2 Multiple Instance Learning Network,[0],[0]
"i exp(h ′T i ha) , (10)
where Equation (9) defines a one-layer MLP that produces an attention vector for the i-th segment.",3.2 Multiple Instance Learning Network,[0],[0]
"Attention weights ai are computed as the normalized similarity of each h′i with ha. Vector ha, which is randomly initialized and learned during training, can be thought of as a trained key, able to recognize sentiment-heavy segments.",3.2 Multiple Instance Learning Network,[0],[0]
"The attention mechanism is depicted in the dashed box of Figure 2, with attention weights shown as shaded circles.
",3.2 Multiple Instance Learning Network,[0],[0]
"Finally, we obtain a document-level distribution over sentiment labels as the weighted sum of segment distributions (see top of Figure 2(b)):
p (c) d =
∑
i
aip (c)",3.2 Multiple Instance Learning Network,[0],[0]
"i , c ∈",3.2 Multiple Instance Learning Network,[0],[0]
"[1, C] .",3.2 Multiple Instance Learning Network,[0],[0]
"(11)
Training The model is trained end-to-end on documents with user-generated sentiment labels.",3.2 Multiple Instance Learning Network,[0],[0]
"We
use the negative log likelihood of the document-level prediction as an objective function:
L = − ∑
d
log p (yd) d (12)",3.2 Multiple Instance Learning Network,[0],[0]
"After training, our model can produce segment-level sentiment predictions for unseen texts in the form of class probability distributions.",4 Polarity-based Opinion Extraction,[0],[0]
"A direct application of our method is opinion extraction, where highly positive and negative snippets are selected from the original document, producing extractive sentiment summaries, as described below.
",4 Polarity-based Opinion Extraction,[0],[0]
"Polarity Scoring In order to extract opinion summaries, we need to rank segments according to their sentiment polarity.",4 Polarity-based Opinion Extraction,[0],[0]
"We introduce a method that takes our model’s confidence in the prediction into account, by reducing each segment’s class probability distribution pi to a single real-valued polarity score.",4 Polarity-based Opinion Extraction,[0],[0]
"To achieve this, we first define a real-valued class weight vector w = 〈w(1), . . .",4 Polarity-based Opinion Extraction,[0],[0]
", w(C) |w(c) ∈",4 Polarity-based Opinion Extraction,[0],[0]
"[−1, 1]〉 that assigns uniformly-spaced weights to the ordered labelset, such that w(c+1) − w(c) = 2C−1 .",4 Polarity-based Opinion Extraction,[0],[0]
"For example, in a 5-class scenario, the class weight vector would be w = 〈−1,−0.5, 0, 0.5, 1〉.",4 Polarity-based Opinion Extraction,[0],[0]
"We compute the polarity score of a segment as the dot-product of the probability distribution pi with vector w:
polarity(si) = ∑
c
p (c) i w (c) ∈",4 Polarity-based Opinion Extraction,[0],[0]
"[−1, 1] (13)
Gated Polarity As a way of increasing the effectiveness of our method, we introduce a gated extension that uses the attention mechanism of our model to further differentiate between segments that carry
significant sentiment cues and those that do not:
gated-polarity(si) = ai · polarity(si) , (14)
where ai is the attention weight assigned to the i-th segment.",4 Polarity-based Opinion Extraction,[0],[0]
"This forces the polarity scores of segments the model does not attend to closer to 0.
",4 Polarity-based Opinion Extraction,[0],[0]
"An illustration of our polarity scoring function is provided in Figure 3, where the class predictions (top) of three restaurant review segments are mapped to their corresponding polarity scores (bottom).",4 Polarity-based Opinion Extraction,[0],[0]
"We observe that our method produces the desired result; segments 1 and 2 convey negative sentiment and receive negative scores, whereas the third segment is mapped to a positive score.",4 Polarity-based Opinion Extraction,[0],[0]
"Although the same discrete class label is assigned to the first two, the second segment’s score is closer to 0 (neutral) as its class probability mass is more evenly distributed.
",4 Polarity-based Opinion Extraction,[0],[0]
"Segmentation Policies As mentioned earlier, one of the hypotheses investigated in this work regards the use of subsentential units as the basis of extraction.",4 Polarity-based Opinion Extraction,[0],[0]
"Specifically, our model was applied to sentences and Elementary Discourse Units (EDUs), obtained from a Rhetorical Structure Theory (RST) parser (Feng and Hirst, 2012).",4 Polarity-based Opinion Extraction,[0],[0]
"According to RST, documents are first segmented into EDUs corresponding roughly to independent clauses which are then recursively combined into larger discourse spans.",4 Polarity-based Opinion Extraction,[0],[0]
"This results in a tree representation of the document, where connected nodes are characterized by discourse relations.",4 Polarity-based Opinion Extraction,[0],[0]
"We only utilize RST’s segmentation, and leave the potential use of the tree structure to future work.
",4 Polarity-based Opinion Extraction,[0],[0]
The example in Figure 3 illustrates why EDUbased segmentation might be beneficial for opinion extraction.,4 Polarity-based Opinion Extraction,[0],[0]
"The second and third EDUs correspond to the sentence: I didn’t enjoy most of them, but the burger was brilliant.",4 Polarity-based Opinion Extraction,[0],[0]
"Taken as a whole, the sentence conveys mixed sentiment, whereas the EDUs clearly convey opposing sentiment.",4 Polarity-based Opinion Extraction,[0],[0]
In this section we describe the data used to assess the performance of our model.,5 Experimental Setup,[0],[0]
We also give details on model training and comparison systems.,5 Experimental Setup,[0],[0]
Our models were trained on two large-scale sentiment classification collections.,5.1 Datasets,[0],[0]
"The Yelp’13 corpus was introduced in Tang et al. (2015) and contains customer reviews of local businesses, each associated with human ratings on a scale from 1 (negative) to 5 (positive).",5.1 Datasets,[0],[0]
The IMDB corpus of movie reviews was obtained from Diao et al. (2014); each review is associated with user ratings ranging from 1 to 10.,5.1 Datasets,[0],[0]
"Both datasets are split into training (80%), validation (10%) and test (10%) sets.",5.1 Datasets,[0],[0]
"A summary of statistics for each collection is provided in Table 1.
",5.1 Datasets,[0],[0]
"In order to evaluate model performance on the segment level, we constructed a new dataset named SPOT (as a shorthand for Segment POlariTy) by annotating documents from the Yelp’13 and IMDB collections.",5.1 Datasets,[0],[0]
"Specifically, we sampled reviews from each collection such that all document-level classes are represented uniformly, and the document lengths are representative of the respective corpus.",5.1 Datasets,[0],[0]
"Documents were segmented into sentences and EDUs, resulting in two segment-level datasets per collection.",5.1 Datasets,[0],[0]
"Statistics are summarized in Table 2.
",5.1 Datasets,[0],[0]
"Each review was presented to three Amazon Mechanical Turk (AMT) annotators who were asked to judge the sentiment conveyed by each segment (i.e., sentence or EDU) as negative, neutral, or pos-
itive.",5.1 Datasets,[0],[0]
We assigned labels using a majority vote or a fourth annotator in the rare cases of no agreement (< 5%).,5.1 Datasets,[0],[0]
Figure 4 shows the distribution of segment labels for each document-level class.,5.1 Datasets,[0],[0]
"As expected, documents with positive labels contain a larger number of positive segments compared to documents with negative labels and vice versa.",5.1 Datasets,[0],[0]
Neutral segments are distributed in an approximately uniform manner across document classes.,5.1 Datasets,[0],[0]
"Interestingly, the proportion of neutral EDUs is significantly higher compared to neutral sentences.",5.1 Datasets,[0],[0]
"The observation reinforces our argument in favor of EDU segmentation, as it suggests that a sentence with positive or negative overall polarity may still contain neutral EDUs.",5.1 Datasets,[0],[0]
"Discarding neutral EDUs, could therefore lead to more concise opinion extraction compared to relying on entire sentences.
",5.1 Datasets,[0],[0]
We further experimented on two collections introduced by Kotzias et al. (2015) which also originate from the YELP’13 and IMDB datasets.,5.1 Datasets,[0],[0]
"Each collection consists of 1,000 randomly sampled sentences annotated with binary sentiment labels.",5.1 Datasets,[0],[0]
"On the task of segment classification we compared MILNET, our multiple instance learning network, against the following methods:
Majority: Majority class applied to all instances.",5.2 Model Comparison,[0],[0]
SO-CAL:,5.2 Model Comparison,[0],[0]
"State-of-the-art lexicon-based system
that classifies segments into positive, neutral, and negative classes (Taboada et al., 2011).
",5.2 Model Comparison,[0],[0]
"Seg-CNN: Fully-supervised CNN segment classifier trained on SPOT’s labels (Kim, 2014).
",5.2 Model Comparison,[0],[0]
GICF:,5.2 Model Comparison,[0],[0]
The Group-Instance Cost Function model introduced in Kotzias et al. (2015).,5.2 Model Comparison,[0],[0]
"This is an unweighted average prediction aggregation MIL method that uses sentence features from a pretrained convolutional neural model.
",5.2 Model Comparison,[0],[0]
HIERNET:,5.2 Model Comparison,[0],[0]
HIERNET does not explicitly generate individual segment predictions.,5.2 Model Comparison,[0],[0]
Segment polarity scores are obtained by assigning the documentlevel prediction to every segment.,5.2 Model Comparison,[0],[0]
"We can then produce finer-grained polarity distinctions via gating, using the model’s attention weights.
",5.2 Model Comparison,[0],[0]
"We further illustrate the differences between HIERNET and MILNET in Figure 5, which includes short descriptions and simplified equations for each model.",5.2 Model Comparison,[0],[0]
"MILNET naturally produces distinct segment polarities, while HIERNET assigns a single polarity score to every segment.",5.2 Model Comparison,[0],[0]
"In both cases, gating is a further means of identifying neutral segments.
",5.2 Model Comparison,[0],[0]
"Finally, we differentiate between variants of HIERNET and MILNET according to: Polarity source: Controls whether we assign polar-
ities via segment-specific or document-wide predictions.",5.2 Model Comparison,[0],[0]
HIERNET only allows for documentwide predictions.,5.2 Model Comparison,[0],[0]
"MILNET can use both.
",5.2 Model Comparison,[0],[0]
"Attention: We use models without gating (no subscript), with gating (gt subscript) as well as models trained with the attention mechanism disabled, falling back to simple averaging (avg subscript).",5.2 Model Comparison,[0],[0]
"We trained MILNET and HIERNET using Adadelta (Zeiler, 2012) for 25 epochs.",5.3 Model Training and Evaluation,[0],[0]
Mini-batches of 200 documents were organized based on the reviews’ segment and document lengths so the amount of padding was minimized.,5.3 Model Training and Evaluation,[0],[0]
We used 300-dimensional pre-trained word2vec embeddings.,5.3 Model Training and Evaluation,[0],[0]
"We tuned hyperparameters on the validation sets of the document classification collections, resulting in the following configuration (unless otherwise noted).",5.3 Model Training and Evaluation,[0],[0]
"For the CNN segment encoder, we used window sizes of 3, 4
and 5 words with 100 feature maps per window size, resulting in 300-dimensional segment vectors.",5.3 Model Training and Evaluation,[0],[0]
The GRU hidden vector dimensions for each direction were set to 50 and the attention vector dimensionality to 100.,5.3 Model Training and Evaluation,[0],[0]
"We used L2-normalization and dropout to regularize the softmax classifiers and additional dropout on the internal GRU connections.
",5.3 Model Training and Evaluation,[0],[0]
"Real-valued polarity scores produced by the two models are mapped to discrete labels using two appropriate thresholds t1 , t2 ∈",5.3 Model Training and Evaluation,[0],[0]
"[−1, 1], so that a segment s is classified as negative if polarity(s) < t1, positive if polarity(s) >",5.3 Model Training and Evaluation,[0],[0]
"t2 or neutral otherwise.3 To evaluate performance, we use macro-averaged F1 which is unaffected by class imbalance.",5.3 Model Training and Evaluation,[0],[0]
"We select optimal thresholds using 10-fold cross-validation and report mean scores across folds.
",5.3 Model Training and Evaluation,[0],[0]
The fully-supervised convolutional segment classifier (Seg-CNN) uses the same window size and feature map configuration as our segment encoder.,5.3 Model Training and Evaluation,[0],[0]
Seg-CNN was trained on SPOT using segment labels directly and 10-fold cross-validation (identical folds as in our main models).,5.3 Model Training and Evaluation,[0],[0]
Seg-CNN is not directly comparable to MILNET (or HIERNET) due to differences in supervision type (segment vs. document labels) and training size (1K-2K segment labels vs. ∼250K document labels).,5.3 Model Training and Evaluation,[0],[0]
"However, the
3The discretization of polarities is only used for evaluation purposes and is not necessary for summary extraction, where we only need a relative ranking of segments.
",5.3 Model Training and Evaluation,[0],[0]
comparison is indicative of the utility of fine-grained sentiment predictors that do not rely on expensive segment-level annotations.,5.3 Model Training and Evaluation,[0],[0]
We evaluated models in two ways.,6 Results,[0],[0]
"We first assessed their ability to classify segment polarity in reviews using the newly created SPOT dataset and, additionally, the sentence corpora of Kotzias et al. (2015).",6 Results,[0],[0]
Our second suite of experiments focused on opinion extraction: we conducted a judgment elicitation study to determine whether extracts produced by MILNET are useful and of higher quality compared to HIERNET and other baselines.,6 Results,[0],[0]
We were also interested to find out whether EDUs provide a better basis for opinion extraction than sentences.,6 Results,[0],[0]
Table 3 summarizes our results.,6.1 Segment Classification,[0],[0]
The first block in the table reports the performance of the majority class baseline.,6.1 Segment Classification,[0],[0]
"The second block considers models that do not utilize segment-level predictions, namely HIERNET which assigns polarity scores to segments using its document-level predictions, as well as the variant of MILNET which similarly uses document-level predictions only (Equation (11)).",6.1 Segment Classification,[0],[0]
"In the third block, MILNET’s segment-level predictions are used.",6.1 Segment Classification,[0],[0]
"Each block further differentiates between three levels of attention integration, as previ-
ously described.",6.1 Segment Classification,[0],[0]
"The final block shows the performance of SO-CAL and the Seg-CNN classifier.
",6.1 Segment Classification,[0],[0]
"When considering models that use documentlevel supervision, MILNET with gated, segmentspecific polarities obtains the best classification performance across all four datasets.",6.1 Segment Classification,[0],[0]
"Interestingly, it performs comparably to Seg-CNN, the fullysupervised segment classifier, which provides additional evidence that MILNET can effectively identify segment polarity without the need for segmentlevel annotations.",6.1 Segment Classification,[0],[0]
Our model also outperforms the strong SO-CAL baseline in all but one datasets which is remarkable given the expert knowledge and linguistic information used to develop the latter.,6.1 Segment Classification,[0],[0]
Document-level polarity predictions result in lower classification performance across the board.,6.1 Segment Classification,[0],[0]
"Differences between the standard hierarchical and multiple instance networks are less pronounced in this case, as MILNET loses the advantage of producing segment-specific sentiment predictions.",6.1 Segment Classification,[0],[0]
Models without attention perform worse in most cases.,6.1 Segment Classification,[0],[0]
"The use of gated polarities benefits all model configurations, indicating the method’s ability to selectively focus on segments with significant sentiment cues.
",6.1 Segment Classification,[0],[0]
"We further analyzed the polarities assigned by MILNET and HIERNET to positive, negative, and
Method Yelp IMDB
GICF 86.3 86.0 GICFHN 92.9 86.5 GICFMN 93.2 91.0
MILNET 94.0 91.9
Table 5: Accuracy scores on the sentence classification datasets introduced in Kotzias et al. (2015).
neutral segments.",6.1 Segment Classification,[0],[0]
Figure 6 illustrates the distribution of polarity scores produced by the two models on the Yelp’13 dataset (sentence segmentation).,6.1 Segment Classification,[0],[0]
"In the case of negative and positive sentences, both models demonstrate appropriately skewed distributions.",6.1 Segment Classification,[0],[0]
"However, the neutral class appears to be particularly problematic for HIERNET, where polarity scores are scattered across a wide range of values.",6.1 Segment Classification,[0],[0]
"In contrast, MILNET is more successful at identifying neutral sentences, as its corresponding distribution has a single mode near zero.",6.1 Segment Classification,[0],[0]
Attention gating addresses this issue by moving the polarity scores of sentiment-neutral segments towards zero.,6.1 Segment Classification,[0],[0]
This is illustrated in Table 4 where we observe that gated variants of both models do a better job at identifying neutral segments.,6.1 Segment Classification,[0],[0]
"The effect is very significant for HIERNET, while MILNET benefits slightly and remains more effective overall.",6.1 Segment Classification,[0],[0]
"Similar trends were observed in all four SPOT datasets.
",6.1 Segment Classification,[0],[0]
"In order to examine the effect of training size, we trained multiple models using subsets of the original document collections.",6.1 Segment Classification,[0],[0]
"We trained on five random
subsets for each training size, ranging from 100 documents to the full training set, and tested segment classification performance on SPOT.",6.1 Segment Classification,[0],[0]
"The results, averaged across trials, are presented in Figure 7.",6.1 Segment Classification,[0],[0]
"With the exception of the IMDB EDU-segmented dataset, MILNET only requires a few thousand training documents to outperform the supervised Seg-CNN.",6.1 Segment Classification,[0],[0]
"HIERNET follows a similar curve, but is inferior to MILNET.",6.1 Segment Classification,[0],[0]
"A reason for MILNET’s inferior performance on the IMDB corpus (EDU-split) can be lowquality EDUs, due to the noisy and informal style of language used in IMDB reviews.
",6.1 Segment Classification,[0],[0]
"Finally, we compared MILNET against the GICF model (Kotzias et al., 2015) on their Yelp and IMDB sentence sentiment datasets.4",6.1 Segment Classification,[0],[0]
Their model requires sentence embeddings from a pre-trained neural model.,6.1 Segment Classification,[0],[0]
"We used the hierarchical CNN from their work (Denil et al., 2014) and, additionally, pre-trained HIERNET and MILNET sentence embeddings.",6.1 Segment Classification,[0],[0]
The results in Table 5 show that MILNET outperforms all variants of GIFC.,6.1 Segment Classification,[0],[0]
"Our models also seem to learn better sentence embeddings, as they improve GICF’s performance on both collections.
",6.1 Segment Classification,[0],[0]
"4GICF only handles binary labels, which makes it unsuitable for the full-scale comparisons in Table 3.",6.1 Segment Classification,[0],[0]
"Here, we binarize our training datasets and use same-sized sentence embeddings for all four models (R150 for Yelp, R72 for IMDB).",6.1 Segment Classification,[0],[0]
"In our opinion extraction experiments, AMT workers (all native English speakers) were shown an original review and a set of extractive, bullet-style summaries, produced by competing systems using a 30% compression rate.",6.2 Opinion Extraction,[0],[0]
"Participants were asked to decide which summary was best according to three criteria: Informativeness (Which summary best captures the salient points of the review?), Polarity (Which summary best highlights positive and negative comments?) and Coherence (Which summary is more coherent and easier to read?).",6.2 Opinion Extraction,[0],[0]
Subjects were allowed to answer “Unsure” in cases where they could not discriminate between summaries.,6.2 Opinion Extraction,[0],[0]
We used all reviews from our SPOT dataset and collected three responses per document.,6.2 Opinion Extraction,[0],[0]
"We ran four judgment elicitation studies: one comparing HIERNET and MILNET when summarizing reviews segmented as sentences, a second one comparing the two models with EDU segmentation, a third which compares EDU- and sentence-based summaries produced by MILNET, and a fourth where EDU-based summaries from MILNET were compared to a LEAD (the first N words from each document) and a RANDOM (random EDUs) baseline.
",6.2 Opinion Extraction,[0],[0]
"Table 6 summarizes our results, showing the proportion of participants that preferred each system.",6.2 Opinion Extraction,[0],[0]
"The first block in the table shows a slight prefer-
ence for MILNET across criteria.",6.2 Opinion Extraction,[0],[0]
"The second block shows significant preference for MILNET against HIERNET on informativeness and polarity, whereas HIERNET was more often preferred in terms of coherence, although the difference is not statistically significant.",6.2 Opinion Extraction,[0],[0]
The third block compares sentence and EDU summaries produced by MILNET.,6.2 Opinion Extraction,[0],[0]
"EDU summaries were perceived as significantly better in terms of informativeness and polarity, but not coherence.",6.2 Opinion Extraction,[0],[0]
This is somewhat expected as EDUs tend to produce more terse and telegraphic text and may seem unnatural due to segmentation errors.,6.2 Opinion Extraction,[0],[0]
In the fourth block we observe that participants find MILNET more informative and better at distilling polarity compared to the LEAD and RANDOM (EDUs) baselines.,6.2 Opinion Extraction,[0],[0]
"We should point out that the LEAD system is not a strawman; it has proved hard to outperform by more sophisticated methods (Nenkova, 2005), particularly on the newswire domain.
",6.2 Opinion Extraction,[0],[0]
"Example EDU- and sentence-based summaries produced by gated variants of HIERNET and MILNET are shown in Figure 8, with attention weights and polarity scores of the extracted segments shown in round and square brackets respectively.",6.2 Opinion Extraction,[0],[0]
"For both granularities, HIERNET’s positive document-level prediction results in a single polarity score assigned to every segment, and further adjusted using the corresponding attention weights.",6.2 Opinion Extraction,[0],[0]
"The extracted segments are informative, but fail to capture the negative sentiment of some segments.",6.2 Opinion Extraction,[0],[0]
"In contrast, MIL-
NET is able to detect positive and negative snippets via individual segment polarities.",6.2 Opinion Extraction,[0],[0]
"Here, EDU segmentation produced a more concise summary with a clearer grouping of positive and negative snippets.",6.2 Opinion Extraction,[0],[0]
"In this work, we presented a neural network model for fine-grained sentiment analysis within the framework of multiple instance learning.",7 Conclusions,[0],[0]
"Our model can be trained on large scale sentiment classification datasets, without the need for segment-level labels.",7 Conclusions,[0],[0]
"As a departure from the commonly used vector-based composition, our model first predicts sentiment at the sentence- or EDU-level and subsequently combines predictions up the document hierarchy.",7 Conclusions,[0],[0]
An attention-weighted polarity scoring technique provides a natural way to extract sentimentheavy opinions.,7 Conclusions,[0],[0]
Experimental results demonstrate the superior performance of our model against more conventional neural architectures.,7 Conclusions,[0],[0]
"Human evaluation studies also show that MILNET opinion extracts are preferred by participants and are effective at capturing informativeness and polarity, especially when using EDU segments.",7 Conclusions,[0],[0]
"In the future, we would like to focus on multi-document, aspect-based extraction (Cao et al., 2017) and ways of improving the coherence of our summaries by taking into account more fine-grained discourse information (Daumé III and Marcu, 2002).",7 Conclusions,[0],[0]
The authors gratefully acknowledge the support of the European Research Council (award number 681760).,Acknowledgments,[0],[0]
"We thank TACL action editor Ani Nenkova and the anonymous reviewers whose feedback helped improve the present paper, as well as Charles Sutton, Timothy Hospedales, and members of EdinburghNLP for helpful discussions and suggestions.",Acknowledgments,[0],[0]
We consider the task of fine-grained sentiment analysis from the perspective of multiple instance learning (MIL).,abstractText,[0],[0]
"Our neural model is trained on document sentiment labels, and learns to predict the sentiment of text segments, i.e. sentences or elementary discourse units (EDUs), without segment-level supervision.",abstractText,[0],[0]
We introduce an attention-based polarity scoring method for identifying positive and negative text snippets and a new dataset which we call SPOT (as shorthand for Segment-level POlariTy annotations) for evaluating MILstyle sentiment models like ours.,abstractText,[0],[0]
"Experimental results demonstrate superior performance against multiple baselines, whereas a judgement elicitation study shows that EDU-level opinion extraction produces more informative summaries than sentence-based alternatives.",abstractText,[0],[0]
Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis,title,[0],[0]
Neural networks have been the driving force behind the success of deep learning applications.,1. Introduction,[0],[0]
Given enough training data they are able to robustly model input-output relationships and as a result provide high predictive accuracy.,1. Introduction,[0],[0]
"However, they do have some drawbacks.",1. Introduction,[0],[0]
"In the absence of enough data they tend to overfit considerably; this restricts them from being applied in scenarios were labeled data are scarce, e.g. in medical applications such as MRI classification.",1. Introduction,[0],[0]
"Even more importantly, deep neural networks trained with maximum likelihood or MAP procedures tend to be overconfident and as a result do not provide accurate confidence intervals, particularly for inputs that are far from the training data distribution.",1. Introduction,[0],[0]
"A simple example can be seen at Figure 1a; the predictive distribution becomes overly overconfident, i.e. assigns a high softmax probability, towards the wrong class for things it hasn’t seen before (e.g. an MNIST 3 rotated by 90 degrees).",1. Introduction,[0],[0]
"This in effect makes them unsuitable for applications where decisions are made, e.g.
1University of Amsterdam, Netherlands 2TNO Intelligent Imaging, Netherlands 3Canadian Institute For Advanced Research (CIFAR).",1. Introduction,[0],[0]
"Correspondence to: Christos Louizos <c.louizos@uva.nl>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"when a doctor determines the disease of a patient based on the output of such a network.
",1. Introduction,[0],[0]
A principled approach to address both of the aforementioned shortcomings is through a Bayesian inference procedure.,1. Introduction,[0],[0]
Under this framework instead of doing a point estimate for the network parameters we infer a posterior distribution.,1. Introduction,[0],[0]
"These distributions capture the parameter uncertainty of the network, and by subsequently integrating over them we can obtain better uncertainties about the predictions of the model.",1. Introduction,[0],[0]
"We can see that this is indeed the case at Figure 1b; the confidence of the network for the unseen digits is drastically reduced when we are using a Bayesian model, thus resulting into more realistic predictive distributions.",1. Introduction,[0],[0]
"Obtaining the posterior distributions is however no easy task, as the nonlinear nature of neural networks makes the problem intractable.",1. Introduction,[0],[0]
"For this reason approximations have to be made.
",1. Introduction,[0],[0]
"Many works have considered the task of approximate Bayesian inference for neural networks using either Markov Chain Monte Carlo (MCMC) with Hamiltonian Dynamics (Neal, 1995), distilling SGD with Langevin Dynamics (Welling & Teh, 2011; Korattikara et al., 2015) or deterministic techniques such as the Laplace Approximation (MacKay, 1992), Expectation Propagation (Hernández-Lobato & Adams, 2015; HernándezLobato et al., 2015) and variational inference (Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016).
",1. Introduction,[0],[0]
In this paper we will also tackle the problem of Bayesian inference in neural networks.,1. Introduction,[0],[0]
"We will adopt a stochastic gradient variational inference (Kingma & Welling, 2014; Rezende et al., 2014) procedure in order to estimate the posterior distribution over the weight matrices of the network.",1. Introduction,[0],[0]
Arguably one of the most important ingredients of variational inference is the flexibility of the approximate posterior distribution; it determines how well we are able to capture the true posterior distribution and thus the true uncertainty of our models.,1. Introduction,[0],[0]
"In Section 2 we will show how we can produce very flexible distributions in an efficient way by employing auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maaløe et al., 2016) and normalizing flows (Rezende & Mohamed, 2015).",1. Introduction,[0],[0]
"In Section 3 we will discuss related
work, whereas in Section 4 we will evaluate and discuss the proposed framework.",1. Introduction,[0],[0]
"Finally we will conclude with Section 5, where we will provide some final thoughts along with promising directions for future research.",1. Introduction,[0],[0]
"Let D be a dataset consisting of input output pairs {(x1,y1), . . .",2.1. Variational inference for Bayesian Neural Networks,[0],[0]
", (xn,yn)} and let W1:L denote the weight matrices of L layers.",2.1. Variational inference for Bayesian Neural Networks,[0],[0]
"Assuming that p(Wi), qφ(Wi) are the prior and approximate posterior over the parameters of the i’th layer we can derive the following lower bound on the marginal log-likelihood of the dataset D using variational Bayes (Peterson, 1987; Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016):
L(φ) = Eqφ(W1:L) [ log p(y|x,W1:L)+
+ log p(W1:L)− log qφ(W1:L) ] , (1)
where p̃(x,y) denotes the training data distribution and φ the parameters of the variational posterior.",2.1. Variational inference for Bayesian Neural Networks,[0],[0]
"For continuous q(·) distributions that allow for the reparametrization trick (Kingma & Welling, 2014) or stochastic backpropagation (Rezende et al., 2014) we can reparametrize the random sampling from q(·) of the lower bound in terms of noise variables and deterministic functions f(φ, ):
L = Ep( ) [ log p(y|x, f(φ, ))",2.1. Variational inference for Bayesian Neural Networks,[0],[0]
"+
+ log p(f(φ, ))",2.1. Variational inference for Bayesian Neural Networks,[0],[0]
"− log qφ(f(φ, ))",2.1. Variational inference for Bayesian Neural Networks,[0],[0]
] .,2.1. Variational inference for Bayesian Neural Networks,[0],[0]
"(2)
This reparametrization allow us to treat approximate parameter posterior inference as a straightforward optimiza-
tion problem that can be optimized with off-the-shelf (stochastic) gradient ascent techniques.",2.1. Variational inference for Bayesian Neural Networks,[0],[0]
For Bayesian neural networks the most common family for the approximate posterior is that of mean field with independent Gaussian distributions for each weight.,2.2. Improving the variational approximation,[0],[0]
"Despite the fact that this leads to a straightforward lower bound for optimization, the approximation capability is quite limiting; it corresponds to just a unimodal “bump” on the very high dimensional space of the parameters of the neural network.",2.2. Improving the variational approximation,[0],[0]
"There have been attempts to improve upon this approximation with works such as (Gal & Ghahramani, 2015b) with mixtures of delta peaks and (Louizos & Welling, 2016) with matrix Gaussians that allow for nontrivial covariances among the weights.",2.2. Improving the variational approximation,[0],[0]
"Nevertheless, both of the aforementioned methods are still, in a sense, limited; the true parameter posterior is more complex than delta peaks or correlated Gaussians.
",2.2. Improving the variational approximation,[0],[0]
"There has been a lot of recent work on ways to improve the posterior approximation in latent variable models with normalizing flows (Rezende & Mohamed, 2015) and auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; Ranganath et al., 2015; Maaløe et al., 2016) being the most prominent.",2.2. Improving the variational approximation,[0],[0]
"Briefly, a normalizing flow is constructed by introducing parametrized bijective transformations, with easy to compute Jacobians, to random variables with simple initial densities.",2.2. Improving the variational approximation,[0],[0]
By subsequently optimizing the parameters of the flow according to the lower bound they can significantly improve the posterior approximation.,2.2. Improving the variational approximation,[0],[0]
"Auxiliary random variables instead construct more flexible distributions by introducing latent variables in the
posterior itself, thus defining the approximate posterior as a mixture of simple distributions.
",2.2. Improving the variational approximation,[0],[0]
"Nevertheless, applying these ideas to the parameters in a neural network has not yet been explored.",2.2. Improving the variational approximation,[0],[0]
"While it is straightforward to apply normalizing flows to a sample of the weight matrix from q(W), this quickly becomes very expensive; for example with planar flows (Rezende & Mohamed, 2015) we will need two extra matrices for each step of the flow.",2.2. Improving the variational approximation,[0],[0]
"Furthermore, by utilizing this procedure we also lose the benefits of local reparametrizations (Kingma et al., 2015; Louizos & Welling, 2016) which are possible with Gaussian approximate posteriors.
",2.2. Improving the variational approximation,[0],[0]
"In order to simultaneously maintain the benefits of local reparametrizations and increase the flexibility of the approximate posteriors in a Bayesian neural network we will rely on auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maaløe et al., 2016); more specifically we will exploit the well known “multiplicative noise” concept, e.g. as in (Gaussian) Dropout (Srivastava et al., 2014), in neural networks and we will parametrize the approximate posterior with the following process:
z ∼ qφ(z); W ∼ qφ(W|z), (3)
where now the approximate posterior becomes a compound distribution, q(W) = ∫ q(W|z)q(z)dz, with z being a vector of random variables distributed according to the mixing density q(z).",2.2. Improving the variational approximation,[0],[0]
To allow for local reparametrizations we will parametrize the conditional distribution for the weights to be a fully factorized Gaussian.,2.2. Improving the variational approximation,[0],[0]
"Therefore we assume the following form for the fully connected layers:
qφ(W|z) = Din∏ i=1",2.2. Improving the variational approximation,[0],[0]
"Dout∏ j=1 N (ziµij , σ2ij), (4)
where Din, Dout is the input and output dimensionality, and the following form for the kernels in convolutional networks:
qφ(W|z) =",2.2. Improving the variational approximation,[0],[0]
Dh∏ i=1,2.2. Improving the variational approximation,[0],[0]
Dw∏ j=1 Df∏ k=1 N,2.2. Improving the variational approximation,[0],[0]
"(zkµijk, σ2ijk), (5)
where Dh, Dw, Df are the height, width and number of filters for each kernel.",2.2. Improving the variational approximation,[0],[0]
"Note that we did not let z affect the variance of the Gaussian approximation; in a pilot study we found that this parametrization was prone to local optima due to large variance gradients, an effect also observed with the multiplicative parametrization of the Gaussian posterior (Kingma et al., 2015; Molchanov et al., 2017).",2.2. Improving the variational approximation,[0],[0]
We have now reduced the problem of increasing the flexibility of the approximate posterior over the weights W to that of increasing the flexibility of the mixing density q(z).,2.2. Improving the variational approximation,[0],[0]
"Since
z is of much lower dimension, compared to W, it is now straightforward to apply normalizing flows to q(z); in this way we can significantly enhance our approximation and allow for e.g. multimodality and nonlinear dependencies between the elements of the weight matrix.",2.2. Improving the variational approximation,[0],[0]
"This will in turn better capture the properties of the true posterior distribution, thus leading to better performance and predictive uncertainties.",2.2. Improving the variational approximation,[0],[0]
We will coin the term multiplicative normalizing flows (MNFs) for this family of approximate posteriors.,2.2. Improving the variational approximation,[0],[0]
"Algorithms 1, 2 describe the forward pass using local reparametrizations for fully connected and convolutional layers with this type of approximate posterior.
",2.2. Improving the variational approximation,[0],[0]
"Algorithm 1 Forward propagation for each fully connected layer h. Mw,Σw are the means and variances of each layer, H is a minibatch of activations and NF(·) is the normalizing flow described at eq. 6.",2.2. Improving the variational approximation,[0],[0]
"For the first layer we have that H = X where X is the minibatch of inputs.
",2.2. Improving the variational approximation,[0],[0]
"Require: H,Mw,Σw 1: Z0 ∼ q(z0) 2: ZTf = NF(Z0) 3:",2.2. Improving the variational approximation,[0],[0]
Mh = (H ZTf ),2.2. Improving the variational approximation,[0],[0]
"Mw 4: Vh = H2Σw 5: E ∼ N (0, 1) 6: return Mh + √ Vh E
Algorithm 2 Forward propagation for each convolutional layer h. Nf are the number of convolutional filters, ∗ is the convolution operator and we assume the [batch, height, width, feature maps] convention.
",2.2. Improving the variational approximation,[0],[0]
"Require: H,Mw,Σw 1: z0 ∼ q(z0) 2: zTf = NF(z0) 3: Mh = H ∗",2.2. Improving the variational approximation,[0],[0]
"(Mw reshape(zTf , [1, 1, Df ])) 4",2.2. Improving the variational approximation,[0],[0]
": Vh = H2 ∗Σw 5: E ∼ N (0, 1) 6: return Mh + √ Vh E
For the normalizing flow of q(z) we will use the masked RealNVP (Dinh et al., 2016) using the numerically stable updates introduced in Inverse Autoregressive Flow (IAF) (Kingma et al., 2016):
m ∼ Bern(0.5); h = tanh(f(m zt)) µ = g(h); σ = σ(k(h))
",2.2. Improving the variational approximation,[0],[0]
zt+1 = m zt+(1−m),2.2. Improving the variational approximation,[0],[0]
"(zt σ + (1− σ) µ) (6)
log ∣∣∣∣∂zt+1∂zt ∣∣∣∣ =",2.2. Improving the variational approximation,[0],[0]
"(1−m)T logσ,
where corresponds to element-wise multiplication, σ(·)
is the sigmoid function1 and f(·), g(·), k(·) are linear mappings.",2.2. Improving the variational approximation,[0],[0]
We resampled the mask m every time in order to avoid a specific splitting over the dimensions of z. For the starting point of the flow q(z0) we used a simple fully factorized Gaussian and we will refer to the final iterate as zTf .,2.2. Improving the variational approximation,[0],[0]
"Unfortunately, parametrizing the posterior distribution as eq. 3 makes the lower bound intractable as generally we do not have a closed form density function for q(W).",2.3. Bounding the entropy,[0],[0]
This makes the calculation of the entropy −Eq(W)[log q(W)] challenging.,2.3. Bounding the entropy,[0],[0]
"Fortunately we can make the lower bound tractable again by further lower bounding the entropy in terms of an auxiliary distribution r(z|W) (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maaløe et al., 2016).",2.3. Bounding the entropy,[0],[0]
"This can be seen as if we are performing variational inference on the augmented probability space p(D,W1:L, z1:L), that maintains the same true posterior distribution p(W|D) (as we can always marginalize out r(z|W) to obtain the original model).",2.3. Bounding the entropy,[0],[0]
"The lower bound in this case becomes:
L(φ, θ) = Eqφ(z1",2.3. Bounding the entropy,[0],[0]
":L,W1:L) [ log p(y|x,W1:L, z1:L)+
+ log p(W1:L) + log rθ(z1:L|W1:L)− − log qφ(W1:L|z1:L)− log qφ(z1:L) ] , (7)
where θ are the parameters of the auxiliary distribution r(·).",2.3. Bounding the entropy,[0],[0]
"This bound is looser than the previous bound, however the extra flexibility of q(W) can compensate and allow for a tighter bound.",2.3. Bounding the entropy,[0],[0]
"Furthermore, the tightness of the bound also depends on the ability of r(z|W) to approximate the “auxiliary” posterior distribution q(z|W) = q(W|z)q(z)q(W) .",2.3. Bounding the entropy,[0],[0]
"Therefore, to allow for a flexible r(z|W) we will follow (Ranganath et al., 2015) and we will parametrize it with inverse normalizing flows as follows:
r(zTb |W) = Dz∏ i=1",2.3. Bounding the entropy,[0],[0]
"N (µ̃i, σ̃2i ), (8)
where for fully connected layers we have that: µ̃i =",2.3. Bounding the entropy,[0],[0]
( b1 ⊗ tanh(cTW) ),2.3. Bounding the entropy,[0],[0]
"(1 D−1out) (9)
σ̃i = σ (( b2 ⊗ tanh(cTW) )",2.3. Bounding the entropy,[0],[0]
"(1 D−1out) ) , (10)
and for convolutional: µ̃i = ( tanh(mat(W)c)⊗ b1 ) (1 (DhDw)−1) (11)
σ̃i = σ",2.3. Bounding the entropy,[0],[0]
"(( tanh(mat(W)c)⊗ b2 ) (1 (DhDw)−1) ) ,
(12)
1f(x) = 1 1+exp(−x)
where b1,b2, c are trainable vectors that have the same dimensionality as z, Dz , 1 corresponds to a vector of 1s, ⊗ corresponds to the outer product and mat(·) corresponds to the matricization2 operator.",2.3. Bounding the entropy,[0],[0]
"The zTb variable corresponds to the fully factorized variable that is transformed by a normalizing flow to zTf or else the variable obtained by the inverse normalizing flow, zTb = NF
−1(zTf ).",2.3. Bounding the entropy,[0],[0]
We will parametrize this inverse directly with the procedure described at eq. 6.,2.3. Bounding the entropy,[0],[0]
"Notice that we can employ local reparametrizations also in eq. 9,10,11,12, so as to avoid sampling the, potentially big, matrix W. With the standard normal prior and the fully factorized Gaussian posterior of eq. 4 the KL-divergence between the prior and the posterior can be computed as follows:
−KL(q(W)||p(W))",2.3. Bounding the entropy,[0],[0]
"= = Eq(W,zT )",2.3. Bounding the entropy,[0],[0]
[−KL(q(W|zTf )||p(W))+ + log r(zTf,2.3. Bounding the entropy,[0],[0]
|W)− log q(zTf ),2.3. Bounding the entropy,[0],[0]
"], (13)
where each of the terms corresponds to:
−KL(q(W|zTf )||p(W))",2.3. Bounding the entropy,[0],[0]
"=
= 1
2 ∑ i,j (− log σ2i,j + σ2i,j + z2Tfiµ 2",2.3. Bounding the entropy,[0],[0]
"i,j − 1) (14)
log r(zTf |W) = log r(zTb |W) + Tf+Tb∑",2.3. Bounding the entropy,[0],[0]
"t=Tf log ∣∣∣∣∂zt+1∂zt ∣∣∣∣
(15)
log q(zTf )",2.3. Bounding the entropy,[0],[0]
"= log q(z0)− Tf∑ t=1 log ∣∣∣∣∂zt+1∂zt ∣∣∣∣. (16)
",2.3. Bounding the entropy,[0],[0]
"It should be noted that this bound is a generalization of the bound proposed by (Gal & Ghahramani, 2015b).",2.3. Bounding the entropy,[0],[0]
"We can arrive at the bound of (Gal & Ghahramani, 2015b) if we trivially parametrize the auxiliary model r(z|W) = q(z) (which provides a less tight bound (Ranganath et al., 2015)) use a standard normal prior for W, a Bernoulli q(z) with probability of success π and then let the variance of our conditional Gaussian q(W|z) go to zero.",2.3. Bounding the entropy,[0],[0]
This will result into the lower bound being infinite due to the log of the variances; nevertheless since we are not optimizing over σ,2.3. Bounding the entropy,[0],[0]
we can simply disregard those terms.,2.3. Bounding the entropy,[0],[0]
"After a little bit of algebra we can show that the only term that will remain in the KL-divergence between q(W) and p(W) will be the expectation of the trace of the square of the mean matrix3, i.e. Eq(z)[ 12 tr((diag(z)M))",2.3. Bounding the entropy,[0],[0]
T (diag(z)M))],2.3. Bounding the entropy,[0],[0]
"= π2 ‖M‖ 2 2, with 1− π being the dropout rate.
",2.3. Bounding the entropy,[0],[0]
"We also found that in general it is beneficial to “constrain” the standard deviations σij of the conditional Gaussian posterior q(W|z) during the forward pass for the computation
2Converting the multidimensional tensor to a matrix.",2.3. Bounding the entropy,[0],[0]
"3The matrix that has M[i, j] = µij
of the likelihood to a lower than the true range, e.g. [0, α] instead of the [0, 1] we have with a standard normal prior.",2.3. Bounding the entropy,[0],[0]
"This results into a small bias and a looser lower bound, however it helps in avoiding bad local minima in the variational objective.",2.3. Bounding the entropy,[0],[0]
"This is akin to the free bits objective described at (Kingma et al., 2016).",2.3. Bounding the entropy,[0],[0]
"Approximate inference for Bayesian neural networks has been pioneered by (MacKay, 1992) and (Neal, 1995).",3. Related work,[0],[0]
"Laplace approximation (MacKay, 1992) provides a deterministic approximation to the posterior that is easy to obtain; it is a Gaussian centered at the MAP estimate of the parameters with a covariance determined by the inverse of the Hessian of the log-likelihood.",3. Related work,[0],[0]
"Despite the fact that it is straightforward to implement, its scalability is limited unless approximations are made, which generally reduces performance.",3. Related work,[0],[0]
"Hamiltonian Monte Carlo (Neal, 1995) is so far the golden standard for approximate Bayesian inference; nevertheless it is also not scalable to large networks and datasets due to the fact that we have to explicitly store the samples from the posterior.",3. Related work,[0],[0]
"Furthermore as it is an MCMC method, assessing convergence is non trivial.",3. Related work,[0],[0]
"Nevertheless there is interesting work that tries to improve upon those issues with stochastic gradient MCMC (Chen et al.) and distillation methods (Korattikara et al., 2015).
",3. Related work,[0],[0]
Deterministic methods for approximate inference in Bayesian neural networks have recently attained much attention.,3. Related work,[0],[0]
"One of the first applications of variational inference in neural networks was in (Peterson, 1987) and (Hinton & Van Camp, 1993).",3. Related work,[0],[0]
"More recently (Graves, 2011) proposed a practical method for variational inference in this setting with a simple (but biased) estimator for a fully factorized posterior distribution.",3. Related work,[0],[0]
"(Blundell et al., 2015) improved upon this work with the unbiased estimator from (Kingma & Welling, 2014) and a scale mixture prior.",3. Related work,[0],[0]
"(Hernández-Lobato & Adams, 2015) proposed to use Expectation Propagation (Minka, 2001) with fully factorized posteriors and showed good results on regression tasks.",3. Related work,[0],[0]
"(Kingma et al., 2015) showed how Gaussian dropout can be interpreted as performing approximate inference with log-uniform priors, multiplicative Gaussian posteriors and local reparametrizations, thus allowing straightforward learning of the dropout rates.",3. Related work,[0],[0]
"Similarly (Gal & Ghahramani, 2015b) showed interesting connections between Bernoulli Dropout (Srivastava et al., 2014) networks and approximate Bayesian inference in deep Gaussian Processes (Damianou & Lawrence, 2013) thus allowing the extraction of uncertainties in a principled way.",3. Related work,[0],[0]
"Similarly (Louizos & Welling, 2016) arrived at the same result through structured posterior approximations via matrix Gaussians and local reparametrizations (Kingma et al.,
2015).
",3. Related work,[0],[0]
"It should also be mentioned that uncertainty estimation in neural networks can also be performed without the Bayesian paradigm; frequentist methods such as Bootstrap (Osband et al., 2016) and ensembles (Lakshminarayanan et al., 2016) have shown that in certain scenarios they can provide reasonable confidence intervals.",3. Related work,[0],[0]
"All of the experiments were coded in Tensorflow (Abadi et al., 2016) and optimization was done with Adam (Kingma & Ba, 2015) using the default hyperparameters.",4. Experiments,[0],[0]
"We used the LeNet 54 (LeCun et al., 1998) convolutional architecture with ReLU (Nair & Hinton, 2010) nonlinearities.",4. Experiments,[0],[0]
"The means M of the conditional Gaussian q(W|z) were initialized with the scheme proposed in (He et al., 2015), whereas the log of the variances were initialized by sampling from N (−9, 0.001).",4. Experiments,[0],[0]
Unless explicitly mentioned otherwise we use flows of length two for q(z) and r(z|W) with 50 hidden units for each step of the flow of q(z) and 100 hidden units for each step of the flow of r(z|W).,4. Experiments,[0],[0]
We used 100 posterior samples to estimate the predictive distribution for all of the models during testing and 1 posterior sample during training.,4. Experiments,[0],[0]
MNIST We trained on MNIST LeNet architectures using the priors and posteriors described at Table 1.,4.1. Predictive performance and uncertainty,[0],[0]
"We trained Dropout with the way described at (Gal & Ghahramani, 2015a) using 0.5 for the dropout rate and for Deep Ensembles (Lakshminarayanan et al., 2016)",4.1. Predictive performance and uncertainty,[0],[0]
we used 10 members and = .25 for the adversarial example generation.,4.1. Predictive performance and uncertainty,[0],[0]
"For the models with the Gaussian prior we constrained the standard deviation of the conditional posterior to be ≤ .5
4The version from Caffe.
during the forward pass.",4.1. Predictive performance and uncertainty,[0],[0]
"The classification performance of each model can be seen at Table 2; while our overall focus is not classification accuracy per se, we see that with the MNF posteriors we improve upon mean field reaching similar accuracies with Deep Ensembles.
",4.1. Predictive performance and uncertainty,[0],[0]
"notMNIST To evaluate the predictive uncertainties of each model we performed the task described at (Lakshminarayanan et al., 2016)",4.1. Predictive performance and uncertainty,[0],[0]
; we estimated the entropy of the predictive distributions on notMNIST5 from the LeNet architectures trained on MNIST.,4.1. Predictive performance and uncertainty,[0],[0]
Since we a-priori know that none of the notMNIST classes correspond to a trained class (since they are letters and not digits),4.1. Predictive performance and uncertainty,[0],[0]
"the ideal predictive distribution is uniform over the MNIST digits, i.e. a maximum entropy distribution.",4.1. Predictive performance and uncertainty,[0],[0]
"Contrary to (Lakshminarayanan et al., 2016)",4.1. Predictive performance and uncertainty,[0],[0]
"we do not plot the histogram of the entropies across the images but we instead use the empirical CDF, which we think is more informative.",4.1. Predictive performance and uncertainty,[0],[0]
"Curves that are closer to the bottom right part of the plot are preferable, as it denotes that the probability of observing a high confidence prediction is low.",4.1. Predictive performance and uncertainty,[0],[0]
"At Figure 2 we show the empirical CDF over the range of possible entropies, [0, 2.5], for all of the models.
",4.1. Predictive performance and uncertainty,[0],[0]
"It is clear from the plot that the uncertainty estimates from MNFs are better than the other approaches, since the probability of a low entropy prediction is overall lower.",4.1. Predictive performance and uncertainty,[0],[0]
"The network trained with just weight decay was, as expected, the most overconfident with an almost zero median entropy while Dropout seems to be in the middle ground.",4.1. Predictive performance and uncertainty,[0],[0]
"The Bayesian neural net with the log-uniform prior also showed overconfidence in this task; we hypothesize that this is due to the induced sparsity (Molchanov et al., 2017) which results into the pruning of almost all irrelevant sources of variation in the parameters thus not providing enough vari-
5Can be found at http://yaroslavvb.blogspot.",4.1. Predictive performance and uncertainty,[0],[0]
"co.uk/2011/09/notmnist-dataset.html
ability to allow for uncertainty in the predictions.",4.1. Predictive performance and uncertainty,[0],[0]
"The sparsity levels6 are 62%, 95.2% for the two convolutional layers and 99.5%, 93.3% for the two fully connected.",4.1. Predictive performance and uncertainty,[0],[0]
Similar effects would probably be also observed if we optimized the dropout rates for Dropout.,4.1. Predictive performance and uncertainty,[0],[0]
The only source of randomness in the neural network is from the Bernoulli random variables (r.v.),4.1. Predictive performance and uncertainty,[0],[0]
z.,4.1. Predictive performance and uncertainty,[0],[0]
"By employing the Central Limit Theorem7 we can express the distribution of the activations as a Gaussian (Wang & Manning, 2013) with variance affected by the variance of the Bernoulli r.v., V(z) = π(1−π).",4.1. Predictive performance and uncertainty,[0],[0]
"The maximum variance of the Bernoulli r.v. is when π = 0.5, therefore any tuning of the Dropout rate will result into a decrease in the variance of the r.v.",4.1. Predictive performance and uncertainty,[0],[0]
and therefore a decrease in the variance of the Gaussian at the hidden units.,4.1. Predictive performance and uncertainty,[0],[0]
"This will subsequently lead into less predictive variance and more confidence.
",4.1. Predictive performance and uncertainty,[0],[0]
"Finally, whereas it was shown at (Lakshminarayanan et al., 2016) that Deep Ensembles provide good uncertainty estimates (better than Dropout) on this task using fully connected networks, this result did not seem to apply for the LeNet architecture we considered.",4.1. Predictive performance and uncertainty,[0],[0]
"We hypothesize that they are sensitive to the hyperparameters (e.g. adversarial noise, number of members in the ensemble) and it requires more tuning in order to improve upon Dropout on this architecture.
",4.1. Predictive performance and uncertainty,[0],[0]
CIFAR 10,4.1. Predictive performance and uncertainty,[0],[0]
We performed a similar experiment on CIFAR 10.,4.1. Predictive performance and uncertainty,[0],[0]
"To artificially create the ”unobserved class” scenario, we hid 5 of the labels (dog, frog, horse, ship, truck) and trained on the rest (airplane, automobile, bird, cat, deer).",4.1. Predictive performance and uncertainty,[0],[0]
"For this task we used the larger LeNet architecture8 described at (Gal & Ghahramani, 2015a).",4.1. Predictive performance and uncertainty,[0],[0]
For the models with the Gaussian prior we similarly constrained the standard deviation during the forward pass to be≤ .4.,4.1. Predictive performance and uncertainty,[0],[0]
For Deep Ensembles we used five members with = .1 for the adversarial example generation.,4.1. Predictive performance and uncertainty,[0],[0]
"The predictive performance on these five classes can be seen in Table 2, with Dropout and MNFs achieving the overall better accuracies.",4.1. Predictive performance and uncertainty,[0],[0]
"We subsequently measured the entropy of the predictive distribution on the classes that were hidden, with the resulting empirical CDFs visualized in Figure 3.
",4.1. Predictive performance and uncertainty,[0],[0]
We similarly observe that the network with just weight decay was the most overconfident.,4.1. Predictive performance and uncertainty,[0],[0]
"Furthermore, Deep Ensembles and Dropout had similar uncertainties, with Deep Ensembles having lower accuracy on the observed classes.",4.1. Predictive performance and uncertainty,[0],[0]
"The networks with the Gaussian priors also had similar uncertainty with the network with the log uniform prior, nevertheless the MNF posterior had much better accuracy on
6Computed by pruning weights where log σ2 − logµ2 ≥ 5 (Molchanov et al., 2017).
",4.1. Predictive performance and uncertainty,[0],[0]
7Assuming that the network is wide enough.,4.1. Predictive performance and uncertainty,[0],[0]
"8192 filters at each convolutional layer and 1000 hidden units
for the fully connected layer.
",4.1. Predictive performance and uncertainty,[0],[0]
the observed classes.,4.1. Predictive performance and uncertainty,[0],[0]
"The sparsity levels for the network with the log-uniform prior now were 94.9%, 99.8% for the convolutional layers and 99.9%, 92.7% for the fully connected.",4.1. Predictive performance and uncertainty,[0],[0]
"Overall, the network with the MNF posteriors seem to provide the better trade-off in uncertainty and accuracy on the observed classes.",4.1. Predictive performance and uncertainty,[0],[0]
"We also measure how robust our models and uncertainties are against adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014) by generating examples using the fast sign method (Goodfellow et al., 2014) for each of the previously trained architectures using Cleverhans (Papernot et al., 2016).",4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
"For this task we do not include Deep Ensembles as they are trained on adversarial examples.
",4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
MNIST On this scenario we observe interesting results if we plot the change in accuracy and entropy by varying the magnitude of the adversarial perturbation.,4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
The resulting plot can be seen in Figure 4.,4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
"Overall Dropout seems to have better accuracies on adversarial examples; nevertheless, those come at an ”overconfident” price since the entropy of the predictive distributions is quite low thus resulting into predictions that have, on average, above 0.7 probability for the dominant class.",4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
"This is in contrast with MNFs; while the accuracy almost immediately drops close to random, the uncertainty simultaneously increases to almost maximum entropy.",4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
This implies that the predictive distribution is more or less uniform over those examples.,4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
"So despite the fact that our model cannot overcome adver-
sarial examples at least it “knows that it doesn’t know”.
",4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
CIFAR We performed the same experiment also on the five class subset of CIFAR 10.,4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
The results can be seen in Figure 5.,4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
"Here we however observe a different picture, compared to MNIST, since all of the methods experienced overconfidence.",4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
"We hypothesize that adversarial examples are harder to escape and be uncertain about in this dataset, due to the higher dimensionality, and therefore further investigation is needed.",4.2. Accuracy and uncertainty on adversarial examples,[0],[0]
"For the final experiment we visualize the predictive distributions obtained with the different models on the toy regression task introduced at (Hernández-Lobato & Adams, 2015).",4.3. Regression on toy dataset,[0],[0]
"We generated 20 training inputs from U [−4, 4] and then obtained the corresponding targets via y = x3 + , where ∼ N (0, 9).",4.3. Regression on toy dataset,[0],[0]
"We fixed the likelihood noise to its true value and then fitted a Dropout network with π = 0.5
for the hidden layer9, an FFLU network and an MNFG.",4.3. Regression on toy dataset,[0],[0]
"We also fitted a Dropout network where we also learned the dropout probability π of the hidden layer according to the bound described at section 2.3 (which is equivalent to the one described at (Gal & Ghahramani, 2015b)) using REINFORCE (Williams, 1992) and a global baseline (Mnih & Gregor, 2014).",4.3. Regression on toy dataset,[0],[0]
"The resulting predictive distributions can be seen at Figure 6.
",4.3. Regression on toy dataset,[0],[0]
"As we can observe, MNF posteriors provide more realistic predictive distributions, closer to the true posterior (which can be seen at (Hernández-Lobato & Adams, 2015)) and with the network being more uncertain on areas where we do not observed any data.",4.3. Regression on toy dataset,[0],[0]
The uncertainties obtained by Dropout with fixed π = 0.5 did not diverge as much in those areas but overall they were better compared to the uncertainties obtained with FFLU.,4.3. Regression on toy dataset,[0],[0]
"We could probably attribute the latter to the sparsification of the network since 95% and 44% of the parameters were pruned for each layer respectively.
",4.3. Regression on toy dataset,[0],[0]
Interestingly the uncertainties obtained with the network with the learned Dropout probability were the most “overfitted”.,4.3. Regression on toy dataset,[0],[0]
This might suggest that Dropout uncertainty is probably not a good posterior approximation since by optimizing the dropout rates we do not seem to move closer to the true posterior predictive distribution.,4.3. Regression on toy dataset,[0],[0]
This is in contrast with MNFs; they are flexible enough to allow for optimizing all of their parameters in a way that does better approximate the true posterior distribution.,4.3. Regression on toy dataset,[0],[0]
This result also empirically verifies the claim we previously made; by learning the dropout rates the entropy of the posterior predictive will decrease thus resulting into more overconfident predictions.,4.3. Regression on toy dataset,[0],[0]
We introduce multiplicative normalizing flows (MNFs); a family of approximate posteriors for the parameters of a variational Bayesian neural network.,5. Conclusion,[0],[0]
"We have shown that through this approximation we can significantly improve upon mean field on both predictive performance as
9No Dropout was used for the input layer since it is 1- dimensional.
",5. Conclusion,[0],[0]
well as predictive uncertainty.,5. Conclusion,[0],[0]
"We compared our uncertainty on notMNIST and CIFAR with Dropout (Srivastava et al., 2014; Gal & Ghahramani, 2015b) and Deep Ensembles (Lakshminarayanan et al., 2016) using convolutional architectures and found that MNFs achieve more realistic uncertainties while providing predictive capabilities on par with Dropout.",5. Conclusion,[0],[0]
We suspect that the predictive capabilities of MNFs can be further improved through more appropriate optimizers that avoid the bad local minima in the variational objective.,5. Conclusion,[0],[0]
"Finally, we also highlighted limitations of Dropout approximations and empirically showed that MNFs can overcome them.
",5. Conclusion,[0],[0]
There are a couple of promising directions for future research.,5. Conclusion,[0],[0]
"One avenue would be to explore how much can MNFs sparsify and compress neural networks under either sparsity inducing priors, such as the log-uniform prior (Kingma et al., 2015; Molchanov et al., 2017), or empirical priors (Ullrich et al., 2017).",5. Conclusion,[0],[0]
Another promising direction is that of designing better priors for Bayesian neural networks.,5. Conclusion,[0],[0]
"For example (Neal, 1995) has identified limitations of Gaussian priors and proposes alternative priors such as the Cauchy.",5. Conclusion,[0],[0]
"Furthermore, the prior over the parameters also affects the type of uncertainty we get in our predictions; for instance we observed in our experiments a significant difference in uncertainty between Gaussian and log-uniform priors.",5. Conclusion,[0],[0]
"Since different problems require different types of uncertainty it makes sense to choose the prior accordingly, e.g. use an informative prior so as to alleviate adversarial examples.",5. Conclusion,[0],[0]
"We would like to thank Klamer Schutte, Matthias Reisser and Karen Ullrich for valuable feedback.",Acknowledgements,[0],[0]
"This research is supported by TNO, NWO and Google.",Acknowledgements,[0],[0]
We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks.,abstractText,[0],[0]
"We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows (Rezende & Mohamed, 2015) while still allowing for local reparametrizations (Kingma et al., 2015) and a tractable lower bound (Ranganath et al., 2015; Maaløe et al., 2016).",abstractText,[0],[0]
In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.,abstractText,[0],[0]
Multiplicative Normalizing Flows for Variational Bayesian Neural Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016–5026 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
5016",text,[0],[0]
"Conversational Artificial Intelligence (Conversational AI) is one of the long-standing challenges in computer science and artificial intelligence since the Dartmouth Proposal (McCarthy et al., 1955).",1 Introduction,[0],[0]
"As human conversation is inherently complex and ambiguous, learning an open-domain conversational AI that can carry on arbitrary tasks is still very far-off (Vinyals and Le, 2015).",1 Introduction,[0],[0]
"As a consequence, instead of focusing on creating ambitious conversational agents that can reach human-level intelligence, industrial practice has focused on building task-oriented dialogue systems (Young et al., 2013) that can help with specific tasks such as flight reservation (Seneff and Polifroni, 2000)
",1 Introduction,[0],[0]
"⇤The work was done while at the University of Cambridge.
or bus information (Raux et al., 2005).",1 Introduction,[0],[0]
"As the need of hands-free use cases continues to grow, building a conversational agent that can handle tasks across different application domains has become more and more prominent (Ram et al., 2018).
",1 Introduction,[0],[0]
"Dialogues systems are inherently hard to build because there are several layers of complexity: the noise and uncertainty in speech recognition (Black et al., 2011); the ambiguity when understanding human language (Williams et al., 2013); the need to integrate third-party services and dialogue context in the decision-making (Traum and Larsson, 2003; Paek and Pieraccini, 2008); and finally, the ability to generate natural and engaging responses (Stent et al., 2005).",1 Introduction,[0],[0]
"These difficulties have led to the same solution of using statistical framework and machine learning for various system components, such as natural language understanding (Henderson et al., 2013; Mesnil et al., 2015; Mrkšić et al., 2017a), dialogue management (Gašić and Young, 2014; Tegho et al., 2018), language generation (Wen et al., 2015; Kiddon et al., 2016), and even end-to-end dialogue modelling (Zhao and Eskenazi, 2016; Wen et al., 2017; Eric et al., 2017).
",1 Introduction,[0],[0]
"To drive the progress of building dialogue systems using data-driven approaches, a number of conversational corpora have been released in the past.",1 Introduction,[0],[0]
"Based on whether a structured annotation scheme is used to label the semantics, these corpora can be roughly divided into two categories: corpora with structured semantic labels (Hemphill et al., 1990; Williams et al., 2013; Asri et al., 2017; Wen et al., 2017; Eric et al., 2017; Shah et al., 2018); and corpora without semantic labels but with an implicit user goal in mind (Ritter et al., 2010; Lowe et al., 2015).",1 Introduction,[0],[0]
"Despite these efforts, aforementioned datasets are usually constrained in one or more dimensions such as missing proper annotations, only available in a limited capacity, lacking multi-domain use cases, or having a negli-
gible linguistic variability.",1 Introduction,[0],[0]
"This paper introduces the Multi-Domain Wizard-of-Oz (MultiWOZ) dataset, a large-scale multi-turn conversational corpus with dialogues spanning across several domains and topics.",1 Introduction,[0],[0]
"Each dialogue is annotated with a sequence of dialogue states and corresponding system dialogue acts (Traum, 1999).",1 Introduction,[0],[0]
"Hence, MultiWOZ can be used to develop individual system modules as separate classification tasks and serve as a benchmark for existing modular-based approaches.",1 Introduction,[0],[0]
"On the other hand, MultiWOZ has around 10k dialogues, which is at least one order of magnitude larger than any structured corpus currently available.",1 Introduction,[0],[0]
"This significant size of the corpus allows researchers to carry on end-to-end based dialogue modelling experiments, which may facilitate a lot of exciting ongoing research in the area.
",1 Introduction,[0],[0]
"This work presents the data collection approach, a summary of the data structure, as well as a series of analyses of the data statistics.",1 Introduction,[0],[0]
"To show the potential and usefulness of the proposed MultiWOZ corpus, benchmarking baselines of belief tracking, natural language generation and end-toend response generation have been conducted and reported.",1 Introduction,[0],[0]
The dataset and baseline models will be freely available online.1,1 Introduction,[0],[0]
"Existing datasets can be roughly grouped into three categories: machine-to-machine, human-tomachine, and human-to-human conversations.",2 Related Work,[0],[0]
"A detailed review of these categories is presented below.
",2 Related Work,[0],[0]
"1 http://dialogue.mi.eng.cam.ac.uk/
index.php/corpus/
Machine-to-Machine Creating an environment with a simulated user enables to exhaustively generate dialogue templates.",2 Related Work,[0],[0]
"These templates can be mapped to a natural language by either pre-defined rules (Bordes et al., 2017) or crowd workers (Shah et al., 2018).",2 Related Work,[0],[0]
Such approach ensures a diversity and full coverage of all possible dialogue outcomes within a certain domain.,2 Related Work,[0],[0]
"However, the naturalness of the dialogue flows relies entirely on the engineered set-up of the user and system bots.",2 Related Work,[0],[0]
This poses a risk of a mismatch between training data and real interactions harming the interaction quality.,2 Related Work,[0],[0]
"Moreover, these datasets do not take into account noisy conditions often experienced in real interactions (Black et al., 2011).
",2 Related Work,[0],[0]
"Human-to-Machine Since collecting dialogue corpus for a task-specific application from scratch is difficult, most of the task-oriented dialogue corpora are fostered based on an existing dialogue system.",2 Related Work,[0],[0]
One famous example of this kind is the Let’s Go Bus Information System which offers live bus schedule information over the phone,2 Related Work,[0],[0]
"(Raux et al., 2005) leading to the first Dialogue State Tracking Challenge (Williams et al., 2013).",2 Related Work,[0],[0]
"Taking the idea of the Let’s Go system forward, the second and third DSTCs (Henderson et al., 2014b,c) have produced bootstrapped human-machine datasets for a restaurant search domain in the Cambridge area, UK.",2 Related Work,[0],[0]
"Since then, DSTCs have become one of the central research topics in the dialogue community (Kim et al., 2016, 2017).
",2 Related Work,[0],[0]
"While human-to-machine data collection is an obvious solution for dialogue system develop-
ment, it is only possible with a provision of an existing working system.",2 Related Work,[0],[0]
"Therefore, this chicken (system)-and-egg (data) problem limits the use of this type of data collection to existing system improvement instead of developing systems in a completely new domain.",2 Related Work,[0],[0]
"What is even worse is that the capability of the initial system introduces additional biases to the collected data, which may result in a mismatch between the training and testing sets (Wen et al., 2016).",2 Related Work,[0],[0]
"The limited understanding capability of the initial system may prompt the users to adapt to simpler input examples that the system can understand but are not necessarily natural in conversations.
",2 Related Work,[0],[0]
"Human-to-Human Arguably, the best strategy to build a natural conversational system may be to have a system that can directly mimic human behaviors through learning from a large amount of real human-human conversations.",2 Related Work,[0],[0]
"With this idea in mind, several large-scale dialogue corpora have been released in the past, such as the Twitter (Ritter et al., 2010) dataset, the Reddit conversations (Schrading et al., 2015), and the Ubuntu technical support corpus (Lowe et al., 2015).",2 Related Work,[0],[0]
"Although previous work (Vinyals and Le, 2015) has shown that a large learning system can learn to generate interesting responses from these corpora, the lack of grounding conversations onto an existing knowledge base or APIs limits the usability of developed systems.",2 Related Work,[0],[0]
"Due to the lack of an explicit goal in the conversation, recent studies have shown that systems trained with this type of corpus not only struggle in generating consistent and diverse responses (Li et al., 2016) but are also extremely hard to evaluate (Liu et al., 2016).
",2 Related Work,[0],[0]
"In this paper, we focus on a particular type of human-to-human data collection.",2 Related Work,[0],[0]
"The Wizardof-Oz framework (WOZ) (Kelley, 1984) was first proposed as an iterative approach to improve user experiences when designing a conversational system.",2 Related Work,[0],[0]
The goal of WOZ data collection is to log down the conversation for future system development.,2 Related Work,[0],[0]
"One of the earliest dataset collected in this fashion is the ATIS corpus (Hemphill et al., 1990), where conversations between a client and an airline help-desk operator were recorded.
",2 Related Work,[0],[0]
"More recently, Wen et al. (2017) have shown that the WOZ approach can be applied to collect high-quality typed conversations where a machine
learning-based system can learn from.",2 Related Work,[0],[0]
"By modifying the original WOZ framework to make it suitable for crowd-sourcing, a total of 676 dialogues was collected via Amazon Mechanical Turk.",2 Related Work,[0],[0]
"The corpus was later extended to additional two languages for cross-lingual research (Mrkšić et al., 2017b).",2 Related Work,[0],[0]
"Subsequently, this approach is followed by Asri et al. (2017) to collect the Frame corpus in a more complex travel booking domain, and Eric et al. (2017) to collect a corpus of conversations for in-car navigation.",2 Related Work,[0],[0]
"Despite the fact that all these datasets contain highly natural conversations comparing to other human-machine collected datasets, they are usually small in size with only a limited domain coverage.",2 Related Work,[0],[0]
"Following the Wizard-of-Oz set-up (Kelley, 1984), corpora of annotated dialogues can be gathered at relatively low costs and with a small time effort.",3 Data Collection Set-up,[0],[0]
"This is in contrast to previous approaches (Henderson et al., 2014a) and such WOZ set-up has been successfully validated by Wen et al. (2017) and Asri et al. (2017).
",3 Data Collection Set-up,[0],[0]
"Therefore, we follow the same process to create a large-scale corpus of natural human-human conversations.",3 Data Collection Set-up,[0],[0]
Our goal was to collect multi-domain dialogues.,3 Data Collection Set-up,[0],[0]
"To overcome the need of relying the data collection to a small set of trusted workers2, the collection set-up was designed to provide an
2Excluding annotation phase.
easy-to-operate system interface for the Wizards and easy-to-follow goals for the users.",3 Data Collection Set-up,[0],[0]
This resulted in a bigger diversity and semantical richness of the collected data (see Section 4.3).,3 Data Collection Set-up,[0],[0]
"Moreover, having a large set of workers mitigates the problem of artificial encouragement of a variety of behavior from users.",3 Data Collection Set-up,[0],[0]
A detailed explanation of the data-gathering process from both sides is provided below.,3 Data Collection Set-up,[0],[0]
"Subsequently, we show how the crowdsourcing scheme can also be employed to annotate the collected dialogues with dialogue acts.",3 Data Collection Set-up,[0],[0]
"The domain of a task-oriented dialogue system is often defined by an ontology, a structured representation of the back-end database.",3.1 Dialogue Task,[0],[0]
The ontology defines all entity attributes called slots and all possible values for each slot.,3.1 Dialogue Task,[0],[0]
"In general, the slots may be divided into informable slots and requestable slots.",3.1 Dialogue Task,[0],[0]
"Informable slots are attributes that allow the user to constrain the search (e.g., area or price range).",3.1 Dialogue Task,[0],[0]
"Requestable slots represent additional information the users can request about a given entity (e.g., phone number).",3.1 Dialogue Task,[0],[0]
"Based on a given ontology spanning several domains, a task template was created for each task through random sampling.",3.1 Dialogue Task,[0],[0]
This results in single and multi-domain dialogue scenarios and domain specific constraints were generated.,3.1 Dialogue Task,[0],[0]
"In domains that allowed for that, an additional booking requirement was sampled with some probability.
",3.1 Dialogue Task,[0],[0]
"To model more realistic conversations, goal changes are encouraged.",3.1 Dialogue Task,[0],[0]
"With a certain probability, the initial constraints of a task may be set to values so that no matching database entry exists.",3.1 Dialogue Task,[0],[0]
"Once informed about that situation by the system, the users only needed to follow the goal which provided alternative values.",3.1 Dialogue Task,[0],[0]
"To provide information to the users, each task template is mapped to natural language.",3.2 User Side,[0],[0]
"Using heuristic rules, the task is then gradually introduced to the user to prevent an overflow of information.",3.2 User Side,[0],[0]
The goal description presented to the user is dependent on the number of turns already performed.,3.2 User Side,[0],[0]
"Moreover, if the user is required to perform a sub-task (for example - booking a venue), these sub-goals are shown straight-away along with the main goal in the given domain.",3.2 User Side,[0],[0]
This makes the dialogues more similar to spoken conversations.3 Figure 1 shows a sampled task description spanning over two domains with booking requirement.,3.2 User Side,[0],[0]
Natural incorporation of co-referencing and lexical entailment into the dialogue was achieved through implicit mentioning of some slots in the goal.,3.2 User Side,[0],[0]
The wizard is asked to perform a role of a clerk by providing information required by the user.,3.3 System Side,[0],[0]
He is given an easy-to-operate graphical user interface to the back-end database.,3.3 System Side,[0],[0]
The wizard conveys the information provided by the current user input through a web form.,3.3 System Side,[0],[0]
This information is persistent across turns and is used to query the database.,3.3 System Side,[0],[0]
"Thus, the annotation of a belief state is performed implicitly while the wizard is allowed to fully focus on providing the required information.",3.3 System Side,[0],[0]
"Given the result of the query (a list of entities satisfying current constraints), the wizard either requests more details or provides the user with the adequate information.",3.3 System Side,[0],[0]
"At each system turn, the wizard starts with the results of the query from the previous turn.
",3.3 System Side,[0],[0]
"To ensure coherence and consistency, the wizard and the user alike first need to go through the
3However, the length of turns are significantly longer than with spoken interaction (Section 4.3).
dialogue history to establish the respective context.",3.3 System Side,[0],[0]
"We found that even though multiple workers contributed to one dialogue, only a small margin of dialogues were incoherent.",3.3 System Side,[0],[0]
"Arguably, the most challenging and timeconsuming part of any dialogue data collection is the process of annotating dialogue acts.",3.4 Annotation of Dialogue Acts,[0],[0]
"One of the major challenges of this task is the definition of a set and structure of dialogue acts (Traum and Hinkelman, 1992; Bunt, 2006).",3.4 Annotation of Dialogue Acts,[0],[0]
"In general, a dialogue act consists of the intent (such as request or inform) and slot-value pairs.",3.4 Annotation of Dialogue Acts,[0],[0]
"For example, the act inform(domain=hotel,price=expensive) has the intent inform, where the user is informing the system to constrain the search to expensive hotels.
",3.4 Annotation of Dialogue Acts,[0],[0]
"Expecting a big discrepancy in annotations between annotators, we initially ran three trial tests over a subset of dialogues using Amazon Mechanical Turk.",3.4 Annotation of Dialogue Acts,[0],[0]
Three annotations per dialogue were gathered resulting in around 750 turns.,3.4 Annotation of Dialogue Acts,[0],[0]
"As this requires a multi-annotator metric over a multi-label task, we used Fleiss’ kappa metric (Fleiss, 1971) per single dialogue act.",3.4 Annotation of Dialogue Acts,[0],[0]
"Although the weighted kappa value averaged over dialogue acts was at a high level of 0.704, we have observed many cases of very poor annotations and an unsatisfactory coverage of dialogue acts.",3.4 Annotation of Dialogue Acts,[0],[0]
"Initial errors in annotations and suggestions from crowd workers gradually helped us to expand and improve the final set of dialogue acts from 8 to 13 - see Table 2.
",3.4 Annotation of Dialogue Acts,[0],[0]
The variation in annotations made us change the initial approach.,3.4 Annotation of Dialogue Acts,[0],[0]
"We ran a two-phase trial to first
identify set of workers that perform well.",3.4 Annotation of Dialogue Acts,[0],[0]
"Turkers were asked to annotate an illustrative, long dialogue which covered many problematic examples that we have observed in the initial run described above.",3.4 Annotation of Dialogue Acts,[0],[0]
All submissions that were of high quality were inspected and corrections were reported to annotators.,3.4 Annotation of Dialogue Acts,[0],[0]
Workers were asked to re-run a new trial dialogue.,3.4 Annotation of Dialogue Acts,[0],[0]
"Having passed the second test, they were allowed to start annotating real dialogues.",3.4 Annotation of Dialogue Acts,[0],[0]
This procedure resulted in a restricted set of annotators performing high quality annotations.,3.4 Annotation of Dialogue Acts,[0],[0]
Appendix A contains a demonstration of a created system.,3.4 Annotation of Dialogue Acts,[0],[0]
Data collection was performed in a two-step process.,3.5 Data Quality,[0],[0]
"First, all dialogues were collected and then the annotation process was launched.",3.5 Data Quality,[0],[0]
"This setup allowed the dialogue act annotators to also report errors (e.g., not following the task or confusing utterances) found in the collected dialogues.",3.5 Data Quality,[0],[0]
"As a result, many errors could be corrected.",3.5 Data Quality,[0],[0]
"Finally, additional tests were performed to ensure that the provided information in the dialogues match the pre-defined goals.
",3.5 Data Quality,[0],[0]
"To estimate the inter-annotator agreement, the averaged weighted kappa value for all dialogue acts was computed over 291 turns.",3.5 Data Quality,[0],[0]
"With  = 0.884, an improvement in agreement between annotators was achieved although the size of action set was significantly larger.",3.5 Data Quality,[0],[0]
"The main goal of the data collection was to acquire highly natural conversations between a tourist and a clerk from an information center in a touristic
city.",4 MultiWOZ Dialogue Corpus,[0],[0]
We considered various possible dialogue scenarios ranging from requesting basic information about attractions through booking a hotel room or travelling between cities.,4 MultiWOZ Dialogue Corpus,[0],[0]
"In total, the presented corpus consists of 7 domains - Attraction, Hospital, Police, Hotel, Restaurant, Taxi, Train.",4 MultiWOZ Dialogue Corpus,[0],[0]
The latter four are extended domains which include the sub-task Booking.,4 MultiWOZ Dialogue Corpus,[0],[0]
"Through a task sampling procedure (Section 3.1), the dialogues cover between 1 and 5 domains per dialogue thus greatly varying in length and complexity.",4 MultiWOZ Dialogue Corpus,[0],[0]
This broad range of domains allows to create scenarios where domains are naturally connected.,4 MultiWOZ Dialogue Corpus,[0],[0]
"For example, a tourist needs to find a hotel, to get the list of attractions and to book a taxi to travel between both places.",4 MultiWOZ Dialogue Corpus,[0],[0]
Table 2 presents the global ontology with the list of considered dialogue acts.,4 MultiWOZ Dialogue Corpus,[0],[0]
"Following data collection process from the previous section, a total of 10, 438 dialogues were collected.",4.1 Data Statistics,[0],[0]
Figure 2 (left) shows the dialogue length distribution grouped by single and multi domain dialogues.,4.1 Data Statistics,[0],[0]
Around 70% of dialogues have more than 10 turns which shows the complexity of the corpus.,4.1 Data Statistics,[0],[0]
"The average number of turns are 8.93 and 15.39 for single and multi-domain dialogues respectively with 115, 434 turns in total.",4.1 Data Statistics,[0],[0]
Figure 2 (right) presents a distribution over the turn lengths.,4.1 Data Statistics,[0],[0]
"As expected, the wizard replies are much longer - the average sentence lengths are 11.75 and 15.12 for users and wizards respectively.",4.1 Data Statistics,[0],[0]
"The responses are also more diverse thus enabling the training of more complex generation models.
",4.1 Data Statistics,[0],[0]
Figure 3 (left) shows the distribution of dialogue acts annotated in the corpus.,4.1 Data Statistics,[0],[0]
We present here a summarized list where different types of actions like inform are grouped together.,4.1 Data Statistics,[0],[0]
The right graph in the Figure 3 presents the distribution of number of acts per turn.,4.1 Data Statistics,[0],[0]
Almost 60% of dialogues turns have more than one dialogue act showing again the richness of system utterances.,4.1 Data Statistics,[0],[0]
"These create a new challenge for reinforcement learning-based models requiring them to operate on concurrent actions.
",4.1 Data Statistics,[0],[0]
"In total, 1, 249 workers contributed to the corpus creation with only few instances of intentional wrongdoing.",4.1 Data Statistics,[0],[0]
"Additional restrictions were added to automatically discover instances of very short utterances, short dialogues or missing single turns during annotations.",4.1 Data Statistics,[0],[0]
All such cases were corrected or deleted from the corpus.,4.1 Data Statistics,[0],[0]
"There are 3, 406 single-domain dialogues that include booking if the domain allows for that and 7, 032 multi-domain dialogues consisting of at least 2 up to 5 domains.",4.2 Data Structure,[0],[0]
"To enforce reproducibility of results, the corpus was randomly split into a train, test and development set.",4.2 Data Structure,[0],[0]
The test and development sets contain 1k examples each.,4.2 Data Structure,[0],[0]
"Even though all dialogues are coherent, some of them were not finished in terms of task description.",4.2 Data Structure,[0],[0]
"Therefore, the validation and test sets only contain fully successful dialogues thus enabling a fair comparison of models.
",4.2 Data Structure,[0],[0]
"Each dialogue consists of a goal, multiple user and system utterances as well as a belief state and
set of dialogue acts with slots per turn.",4.2 Data Structure,[0],[0]
"Additionally, the task description in natural language presented to turkers working from the visitor’s side is added.",4.2 Data Structure,[0],[0]
"To illustrate the contribution of the new corpus, we compare it on several important statistics with the DSTC2 corpus (Henderson et al., 2014a), the SFX corpus (Gašić et al., 2014), the WOZ2.0 corpus (Wen et al., 2017), the FRAMES corpus (Asri et al., 2017), the KVRET corpus (Eric et al., 2017), and the M2M corpus (Shah et al., 2018).",4.3 Comparison to Other Structured Corpora,[0],[0]
"Figure 1 clearly shows that our corpus compares favorably to all other data sets on most of the metrics with the number of total dialogues, the average number of tokens per turn and the total number of unique tokens as the most prominent ones.",4.3 Comparison to Other Structured Corpora,[0],[0]
Especially the latter is important as it is directly linked to linguistic richness.,4.3 Comparison to Other Structured Corpora,[0],[0]
The complexity and the rich linguistic variation in the collected MultiWOZ dataset makes it a great benchmark for a range of dialogue tasks.,5 MultiWOZ as a New Benchmark,[0],[0]
"To show the potential usefulness of the MultiWOZ corpus, we break down the dialogue modelling task into three sub-tasks and report a benchmark result for each of them: dialogue state tracking, dialogue-act-to-text generation, and dialoguecontext-to-text generation.",5 MultiWOZ as a New Benchmark,[0],[0]
These results illustrate new challenges introduced by the MultiWOZ dataset for different dialogue modelling problems.,5 MultiWOZ as a New Benchmark,[0],[0]
A robust natural language understanding and dialogue state tracking is the first step towards building a good conversational system.,5.1 Dialogue State Tracking,[0],[0]
"Since multidomain dialogue state tracking is still in its infancy and there are not many comparable approaches available (Rastogi et al., 2017), we instead report our state-of-the-art result on the restaurant subset of the MultiWOZ corpus as the reference baseline.",5.1 Dialogue State Tracking,[0],[0]
"The proposed method (Ramadan et al., 2018) exploits the semantic similarity between dialogue utterances and the ontology terms which allows the information to be shared across domains.",5.1 Dialogue State Tracking,[0],[0]
"Furthermore, the model parameters are independent of the ontology and belief states, therefore the number of the parameters does not increase with the size of
the domain itself.4
The same model was trained on both the WOZ2.0 and the proposed MultiWOZ datasets, where the WOZ2.0 corpus consists of 1200 single domain dialogues in the restaurant domain.",5.1 Dialogue State Tracking,[0],[0]
"Although not directly comparable, Table 3 shows that the performance of the model is consecutively poorer on the new dataset compared to WOZ2.0.",5.1 Dialogue State Tracking,[0],[0]
These results demonstrate how demanding is the new dataset as the conversations are richer and much longer.,5.1 Dialogue State Tracking,[0],[0]
"After a robust dialogue state tracking module is built, the next challenge becomes the dialogue management and response generation components.",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"These problems can either be addressed separately (Young et al., 2013), or jointly in an end-to-end fashion (Bordes et al., 2017; Wen et al., 2017; Li et al., 2017).",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"In order to establish a clear benchmark where the performance of the composite of dialogue management and response generation is completely independent of the belief tracking, we experimented with a baseline neural response generation model with an oracle beliefstate obtained from the wizard annotations as discussed in Section 3.3.5
Following Wen et al. (2017) which frames the dialogue as a context to response mapping problem, a sequence-to-sequence model (Sutskever et al., 2014) is augmented with a belief tracker and a discrete database accessing component as additional features to inform the word decisions in the decoder.",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"Note, in the original paper the belief tracker was pre-trained while in this work the annotations of the dialogue state are used as an oracle tracker.",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"Figure 4 presents the architecture of the system (Budzianowski et al., 2018).
",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"4The model is publicly available at https://github.com/osmanio2/
multi-domain-belief-tracking
5The model is publicly available at https:// github.com/budzianowski/multiwoz
Training and Evaluation Since often times the evaluation of a dialogue system without a direct interaction with the real users can be misleading (Liu et al., 2016), three different automatic metrics are included to ensure the result is better interpreted.",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"Among them, the first two metrics relate to the dialogue task completion - whether the system has provided an appropriate entity (Inform rate) and then answered all the requested attributes (Success rate); while fluency is measured via BLEU score (Papineni et al., 2002).",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"The best models for both datasets were found through a grid search over a set of hyper-parameters such as the size of embeddings, learning rate and different recurrent architectures.
",5.2 Dialogue-Context-to-Text Generation,[0],[0]
We trained the same neural architecture (taking into account different number of domains) on both MultiWOZ and Cam676 datasets.,5.2 Dialogue-Context-to-Text Generation,[0],[0]
The best results on the Cam676 corpus were obtained with bidirectional GRU cell.,5.2 Dialogue-Context-to-Text Generation,[0],[0]
"In the case of MultiWOZ dataset, the LSTM cell serving as a decoder and an encoder achieved the highest score with the global type of attention (Bahdanau et al., 2014).",5.2 Dialogue-Context-to-Text Generation,[0],[0]
Table 4 presents the results of a various of model architectures and shows several challenges.,5.2 Dialogue-Context-to-Text Generation,[0],[0]
"As expected, the model achieves almost perfect score on the Inform metric on the Cam676 dataset taking the advantage of an oracle belief state signal.",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"However, even with the perfect dialogue state tracking of the user intent, the baseline models obtain almost 30% lower score on the Inform metric on the new corpus.",5.2 Dialogue-Context-to-Text Generation,[0],[0]
The addition of the attention improves the score on the Success metric on the new dataset by less than 1%.,5.2 Dialogue-Context-to-Text Generation,[0],[0]
"Nevertheless, as expected, the
best model on MultiWOZ is still falling behind by a large margin in comparison to the results on the Cam676 corpus taking into account both Inform and Success metrics.",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"As most of dialogues span over at least two domains, the model has to be much more effective in order to execute a successful dialogue.",5.2 Dialogue-Context-to-Text Generation,[0],[0]
"Moreover, the BLEU score on the MultiWOZ is lower than the one reported on the Cam676 dataset.",5.2 Dialogue-Context-to-Text Generation,[0],[0]
This is mainly caused by the much more diverse linguistic expressions observed in the MultiWOZ dataset.,5.2 Dialogue-Context-to-Text Generation,[0],[0]
"Natural Language Generation from a structured meaning representation (Oh and Rudnicky, 2000; Bohus and Rudnicky, 2005) has been a very popular research topic in the community, and the lack of data has been a long standing block for the field to adopt more machine learning methods.",5.3 Dialogue-Act-to-Text Generation,[0],[0]
"Due to the additional annotation of the system acts, the MultiWOZ dataset serves as a new benchmark for studying natural language generation from a structured meaning representation.",5.3 Dialogue-Act-to-Text Generation,[0],[0]
"In order to verify the difficulty of the collected dataset for the language generation task, we compare it to the SFX dataset (see Table 1), which consists of around 5k dialogue act and natural language sentence pairs.",5.3 Dialogue-Act-to-Text Generation,[0],[0]
We trained the same Semantically Conditioned Long Short-term Memory network (SC-LSTM) proposed by Wen et al. (2015) on both datasets and used the metrics as a proxy to estimate the difficulty of the two corpora.,5.3 Dialogue-Act-to-Text Generation,[0],[0]
"To make a fair comparison, we constrained our dataset to only the restaurant sub-domain which contains around 25k dia-
logue turns.",5.3 Dialogue-Act-to-Text Generation,[0],[0]
To give more statistics about the two datasets: the SFX corpus has 9 different act types with 12 slots comparing to 12 acts and 14 slots in our corpus.,5.3 Dialogue-Act-to-Text Generation,[0],[0]
"The best model for both datasets was found through a grid search over a set of hyperparameters such as the size of embeddings, learning rate, and number of LSTM layers.6
Table 5 presents the results on two metrics: BLEU score (Papineni et al., 2002) and slot error rate (SER) (Wen et al., 2015).",5.3 Dialogue-Act-to-Text Generation,[0],[0]
The significantly lower metrics on the MultiWOZ corpus showed that it is much more challenging than the SFX restaurant dataset.,5.3 Dialogue-Act-to-Text Generation,[0],[0]
"This is probably due to the fact that more than 60% of the dialogue turns are composed of at least two system acts, which greatly harms the performance of the existing model.",5.3 Dialogue-Act-to-Text Generation,[0],[0]
"As more and more speech oriented applications are commercially deployed, the necessity of building an entirely data-driven conversational agent becomes more apparent.",6 Conclusions,[0],[0]
Various corpora were gathered to enable data-driven approaches to dialogue modelling.,6 Conclusions,[0],[0]
"To date, however, the available datasets were usually constrained in linguistic variability or lacking multi-domain use cases.",6 Conclusions,[0],[0]
"In this paper, we established a data-collection pipeline entirely based on crowd-sourcing enabling to gather a large scale, linguistically rich corpus of human-human conversations.",6 Conclusions,[0],[0]
"We hope that MultiWOZ offers valuable training data and a new challenging testbed for existing modularbased approaches ranging from belief tracking to
6The model is publicly available at https://github.com/andy194673/
nlg-sclstm-multiwoz
dialogue acts generation.",6 Conclusions,[0],[0]
"Moreover, the scale of the data should help push forward research in the end-to-end dialogue modelling.",6 Conclusions,[0],[0]
"This work was funded by a Google Faculty Research Award (RG91111), an EPSRC studentship (RG80792), an EPSRC grant (EP/M018946/1) and by Toshiba Research Europe Ltd, Cambridge Research Laboratory (RG85875).",Acknowledgments,[0],[0]
The authors thank many excellent Mechanical Turk contributors for building this dataset.,Acknowledgments,[0],[0]
The authors would also like to thank Thang Minh Luong for his support for this project ad Nikola Mrkšić and anonymous reviewers for their constructive feedback.,Acknowledgments,[0],[0]
The data is available at http://dialogue.mi.eng.cam.,Acknowledgments,[0],[0]
ac.uk/index.php/corpus/.,Acknowledgments,[0],[0]
"Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.",abstractText,[0],[0]
"To address this fundamental obstacle, we introduce the MultiDomain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.",abstractText,[0],[0]
"At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.",abstractText,[0],[0]
"The contribution of this work apart from the open-sourced dataset labelled with dialogue belief states and dialogue actions is two-fold: firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided.",abstractText,[0],[0]
"The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators; secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.",abstractText,[0],[0]
MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling,title,[0],[0]
Mutual information is a fundamental quantity for measuring the relationship between random variables.,1. Introduction,[0],[0]
"In data science it has found applications in a wide range of domains and tasks, including biomedical sciences (Maes et al., 1997), blind source separation (BSS, e.g., independent component analysis, Hyvärinen et al., 2004), information bottleneck (IB, Tishby et al., 2000), feature selection (Kwak & Choi, 2002; Peng et al., 2005), and causality (Butte & Kohane, 2000).
",1. Introduction,[0],[0]
"Put simply, mutual information quantifies the dependence of two random variables X and Z. It has the form,
I(X;Z) = ∫ X×Z log dPXZ dPX ⊗ PZ dPXZ , (1)
where PXZ is the joint probability distribution, and PX =∫ Z dPXZ and PZ = ∫ X dPXZ are the marginals.",1. Introduction,[0],[0]
"In con-
1Montréal Institute for Learning Algorithms (MILA), University of Montréal 2Department of Mathematics and Statistics, McGill University 3Canadian Institute for Advanced Research (CIFAR) 4The Institute for Data Valorization (IVADO).",1. Introduction,[0],[0]
"Correspondence to: Mohamed Ishmael Belghazi <ishmael.belghazi@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
trast to correlation, mutual information captures non-linear statistical dependencies between variables, and thus can act as a measure of true dependence (Kinney & Atwal, 2014).
",1. Introduction,[0],[0]
"Despite being a pivotal quantity across data science, mutual information has historically been difficult to compute (Paninski, 2003).",1. Introduction,[0],[0]
"Exact computation is only tractable for discrete variables (as the sum can be computed exactly), or for a limited family of problems where the probability distributions are known.",1. Introduction,[0],[0]
"For more general problems, this is not possible.",1. Introduction,[0],[0]
"Common approaches are non-parametric (e.g., binning, likelihood-ratio estimators based on support vector machines, non-parametric kernel-density estimators; see, Fraser & Swinney, 1986; Darbellay & Vajda, 1999; Suzuki et al., 2008; Kwak & Choi, 2002; Moon et al., 1995; Kraskov et al., 2004), or rely on approximate gaussianity of data distribution (e.g., Edgeworth expansion, Van Hulle, 2005).",1. Introduction,[0],[0]
"Unfortunately, these estimators typically do not scale well with sample size or dimension (Gao et al., 2014), and thus cannot be said to be general-purpose.",1. Introduction,[0],[0]
"Other recent works include Kandasamy et al. (2017); Singh & Pczos (2016); Moon et al. (2017).
",1. Introduction,[0],[0]
"In order to achieve a general-purpose estimator, we rely on the well-known characterization of the mutual information as the Kullback-Leibler (KL-) divergence (Kullback, 1997) between the joint distribution and the product of the marginals (i.e., I(X;Z) = DKL(PXZ ||",1. Introduction,[0],[0]
PX ⊗ PZ)).,1. Introduction,[0],[0]
"Recent work uses a dual formulation to cast the estimation of f -divergences (including the KL-divergence, see Nguyen et al., 2010) as part of an adversarial game between competing deep neural networks (Nowozin et al., 2016).",1. Introduction,[0],[0]
"This approach is at the cornerstone of generative adversarial networks (GANs, Goodfellow et al., 2014), which train a generative model without any explicit assumptions about the underlying distribution of the data.
",1. Introduction,[0],[0]
In this paper we demonstrate that exploiting dual optimization to estimate divergences goes beyond the minimax objective as formalized in GANs.,1. Introduction,[0],[0]
"We leverage this strategy to offer a general-purpose parametric neural estimator of mutual information based on dual representations of the KL-divergence (Ruderman et al., 2012), which we show is valuable in settings that do not necessarily involve an adversarial game.",1. Introduction,[0],[0]
"Our estimator is scalable, flexible, and completely trainable via back-propagation.",1. Introduction,[0],[0]
"The contribu-
tions of this paper are as follows:
• We introduce the Mutual Information Neural Estimator (MINE), which is scalable, flexible, and completely trainable via back-prop, as well as provide a thorough theoretical analysis.
",1. Introduction,[0],[0]
"• We show that the utility of this estimator transcends the minimax objective as formalized in GANs, such that it can be used in mutual information estimation, maximization, and minimization.
",1. Introduction,[0],[0]
"• We apply MINE to palliate mode-dropping in GANs and to improve reconstructions and inference in Adversarially Learned Inference (ALI, Dumoulin et al., 2016) on large scale datasets.
",1. Introduction,[0],[0]
"• We use MINE to apply the Information Bottleneck method (Tishby et al., 2000) in a continuous setting, and show that this approach outperforms variational bottleneck methods (Alemi et al., 2016).",1. Introduction,[0],[0]
Mutual information is a Shannon entropy-based measure of dependence between random variables.,2.1. Mutual Information,[0],[0]
"The mutual information between X and Z can be understood as the decrease of the uncertainty in X given Z:
I(X;Z) := H(X)−H(X | Z), (2)
where H is the Shannon entropy, and H(X |Z) is the conditional entropy of Z given X .",2.1. Mutual Information,[0],[0]
"As stated in Eqn. 1 and the discussion above, the mutual information is equivalent to the Kullback-Leibler (KL-) divergence between the joint, PXZ , and the product of the marginals PX",2.1. Mutual Information,[0],[0]
"⊗ PZ :
I(X,Z) = DKL(PXZ || PX ⊗ PZ), (3)
where DKL is defined as1, DKL(P || Q) := EP [ log
dP dQ
] .",2.1. Mutual Information,[0],[0]
"(4)
whenever P is absolutely continuous with respect to Q2.
",2.1. Mutual Information,[0],[0]
"The intuitive meaning of Eqn. 3 is clear: the larger the divergence between the joint and the product of the marginals, the stronger the dependence between X and Z. This divergence, hence the mutual information, vanishes for fully independent variables.
",2.1. Mutual Information,[0],[0]
"1Although the discussion is more general, we can think of P and Q as being distributions on some compact domain Ω ⊂",2.1. Mutual Information,[0],[0]
"Rd, with density p and q respect the Lebesgue measure λ, so that DKL = ∫ p log p
q dλ.
",2.1. Mutual Information,[0],[0]
2and infinity otherwise.,2.1. Mutual Information,[0],[0]
A key technical ingredient of MINE are dual representations of the KL-divergence.,2.2. Dual representations of the KL-divergence.,[0],[0]
"We will primarily work with the Donsker-Varadhan representation (Donsker & Varadhan, 1983), which results in a tighter estimator; but will also consider the dual f -divergence representation (Keziou, 2003; Nguyen et al., 2010; Nowozin et al., 2016).
",2.2. Dual representations of the KL-divergence.,[0],[0]
The Donsker-Varadhan representation.,2.2. Dual representations of the KL-divergence.,[0],[0]
"The following theorem gives a representation of the KL-divergence (Donsker & Varadhan, 1983):
Theorem 1 (Donsker-Varadhan representation).",2.2. Dual representations of the KL-divergence.,[0],[0]
"The KL divergence admits the following dual representation:
",2.2. Dual representations of the KL-divergence.,[0],[0]
DKL(P || Q) = sup T :Ω→R EP[T ],2.2. Dual representations of the KL-divergence.,[0],[0]
"− log(EQ[eT ]), (5)
where the supremum is taken over all functions T such that the two expectations are finite.
",2.2. Dual representations of the KL-divergence.,[0],[0]
Proof.,2.2. Dual representations of the KL-divergence.,[0],[0]
"See the Supplementary Material.
",2.2. Dual representations of the KL-divergence.,[0],[0]
A straightforward consequence of Theorem 1 is as follows.,2.2. Dual representations of the KL-divergence.,[0],[0]
Let F be any class of functions T : Ω → R satisfying the integrability constraints of the theorem.,2.2. Dual representations of the KL-divergence.,[0],[0]
"We then have the lower-bound3:
DKL(P || Q) ≥ sup T∈F EP[T",2.2. Dual representations of the KL-divergence.,[0],[0]
]− log(EQ[eT ]).,2.2. Dual representations of the KL-divergence.,[0],[0]
"(6)
Note also that the bound is tight for optimal functions T ∗ that relate the distributions to the Gibbs density as,
dP = 1
Z eT
∗ dQ, where Z = EQ[eT ∗ ].",2.2. Dual representations of the KL-divergence.,[0],[0]
"(7)
The f -divergence representation.",2.2. Dual representations of the KL-divergence.,[0],[0]
"It is worthwhile to compare the Donsker-Varadhan representation to the f - divergence representation proposed in Nguyen et al. (2010); Nowozin et al. (2016), which leads to the following bound:
DKL(P || Q) ≥ sup T∈F EP[T",2.2. Dual representations of the KL-divergence.,[0],[0]
],2.2. Dual representations of the KL-divergence.,[0],[0]
− EQ[eT−1].,2.2. Dual representations of the KL-divergence.,[0],[0]
"(8)
Although the bounds in Eqns.",2.2. Dual representations of the KL-divergence.,[0],[0]
"6 and 8 are tight for sufficiently large families F , the Donsker-Varadhan bound is stronger in the sense that, for any fixed T , the right hand side of Eqn. 6 is larger4 than the right hand side of Eqn. 8.",2.2. Dual representations of the KL-divergence.,[0],[0]
We refer to the work by Ruderman et al. (2012) for a derivation of both representations in Eqns.,2.2. Dual representations of the KL-divergence.,[0],[0]
6 and 8 from the unifying perspective of Fenchel duality.,2.2. Dual representations of the KL-divergence.,[0],[0]
"In Section 3 we discuss versions of MINE based on these two representations, and numerical comparisons are performed in Section 4.
3The bound in Eqn. 6 is known as the compression lemma in the PAC-Bayes literature (Banerjee, 2006).
",2.2. Dual representations of the KL-divergence.,[0],[0]
"4To see this, just apply the identity x ≥ e log x with x = EQ[eT ].",2.2. Dual representations of the KL-divergence.,[0],[0]
In this section we formulate the framework of the Mutual Information Neural Estimator (MINE).,3. The Mutual Information Neural Estimator,[0],[0]
We define MINE and present a theoretical analysis of its consistency and convergence properties.,3. The Mutual Information Neural Estimator,[0],[0]
"Using both Eqn. 3 for the mutual information and the dual representation of the KL-divergence, the idea is to choose F to be the family of functions",3.1. Method,[0],[0]
Tθ : X ×Z → R parametrized by a deep neural network with parameters θ ∈,3.1. Method,[0],[0]
Θ. We call this network the statistics network.,3.1. Method,[0],[0]
"We exploit the bound:
I(X;Z) ≥ IΘ(X,Z), (9)
where IΘ(X,Z) is the neural information measure defined as
IΘ(X,Z) = sup θ∈Θ
EPXZ",3.1. Method,[0],[0]
[Tθ]− log(EPX⊗PZ,3.1. Method,[0],[0]
[eTθ ]).,3.1. Method,[0],[0]
"(10)
",3.1. Method,[0],[0]
The expectations in Eqn. 10 are estimated using empirical samples5 from PXZ and PX ⊗ PZ or by shuffling the samples from the joint distribution along the batch axis.,3.1. Method,[0],[0]
"The objective can be maximized by gradient ascent.
",3.1. Method,[0],[0]
"It should be noted that Eqn. 10 actually defines a new class information measures, The expressive power of neural network insures that they can approximate the mutual information with arbitrary accuracy.
",3.1. Method,[0],[0]
"In what follows, given a distribution P, we denote by P̂(n) as the empirical distribution associated to n i.i.d. samples.
",3.1. Method,[0],[0]
Definition 3.1 (Mutual Information Neural Estimator (MINE)).,3.1. Method,[0],[0]
Let F = {Tθ}θ∈Θ be the set of functions parametrized by a neural network.,3.1. Method,[0],[0]
"MINE is defined as,
̂I(X;Z)n",3.1. Method,[0],[0]
= sup θ∈Θ EP(n)XZ,3.1. Method,[0],[0]
[Tθ]− log(EP(n)X ⊗P̂(n)Z,3.1. Method,[0],[0]
[e Tθ ]).,3.1. Method,[0],[0]
"(11)
Details on the implementation of MINE are provided in Algorithm 1.",3.1. Method,[0],[0]
"An analogous definition and algorithm also hold for the f -divergence formulation in Eqn. 8, which we refer to as MINE-f .",3.1. Method,[0],[0]
"Since Eqn. 8 lower-bounds Eqn. 6, it generally leads to a looser estimator of the mutual information, and numerical comparisons of MINE with MINE-f can be found in Section 4.",3.1. Method,[0],[0]
"However, in a mini-batch setting, the SGD gradients of MINE are biased.",3.1. Method,[0],[0]
"We address this in the next section.
",3.1. Method,[0],[0]
"5Note that samples x̄ ∼ PX and z̄ ∼ PZ from the marginals are obtained by simply dropping x, z from samples (x̄, z) and (x, z̄) ∼ PXZ .
",3.1. Method,[0],[0]
Algorithm 1 MINE θ,3.1. Method,[0],[0]
"← initialize network parameters repeat
Draw b minibatch samples from the joint distribution: (x(1), z(1)), . . .",3.1. Method,[0],[0]
", (x(b), z(b))",3.1. Method,[0],[0]
"∼ PXZ Draw n samples from the Z marginal distribution: z̄(1), . . .",3.1. Method,[0],[0]
", z̄(b) ∼ PZ Evaluate the lower-bound: V(θ)← 1
b ∑b i=1",3.1. Method,[0],[0]
"Tθ(x (i),z(i))− log( 1 b ∑b i=1",3.1. Method,[0],[0]
"e Tθ(x (i),z̄(i)))
",3.1. Method,[0],[0]
"Evaluate bias corrected gradients (e.g., moving average): Ĝ(θ)← ∇̃θV(θ) Update the statistics network parameters: θ ← θ + Ĝ(θ)
until convergence",3.1. Method,[0],[0]
"A naive application of stochastic gradient estimation leads to the gradient estimate:
ĜB = EB [∇θTθ]− EB [∇θTθ eTθ ]
EB [eTθ ] .",3.2. Correcting the bias from the stochastic gradients,[0],[0]
"(12)
where, in the second term, the expectations are over the samples of a minibatch B, leads to a biased estimate of the full batch gradient6.
",3.2. Correcting the bias from the stochastic gradients,[0],[0]
"Fortunately, the bias can be reduced by replacing the estimate in the denominator by an exponential moving average.",3.2. Correcting the bias from the stochastic gradients,[0],[0]
"For small learning rates, this improved MINE gradient estimator can be made to have arbitrarily small bias.",3.2. Correcting the bias from the stochastic gradients,[0],[0]
We found in our experiments that this improves all-around performance of MINE.,3.2. Correcting the bias from the stochastic gradients,[0],[0]
In this section we analyze the consistency and convergence properties of MINE.,3.3. Theoretical properties,[0],[0]
All the proofs can be found in the Supplementary Material.,3.3. Theoretical properties,[0],[0]
"MINE relies on a choice of (i) a statistics network and (ii) n samples from the data distribution PXZ .
",3.3.1. CONSISTENCY,[0],[0]
Definition 3.2 (Strong consistency).,3.3.1. CONSISTENCY,[0],[0]
"The estimator ̂I(X;Z)n is strongly consistent if for all > 0, there exists a positive integer N and a choice of statistics network such that:
∀n ≥ N, |I(X,Z)− ̂I(X;Z)n| ≤ , a.e. where the probability is over a set of samples.
",3.3.1. CONSISTENCY,[0],[0]
"6From the optimization point of view, the f -divergence formulation has the advantage of making the use of SGD with unbiased gradients straightforward.
",3.3.1. CONSISTENCY,[0],[0]
"In a nutshell, the question of consistency is divided into two problems: an approximation problem related to the size of the family, F , and an estimation problem related to the use of empirical measures.",3.3.1. CONSISTENCY,[0],[0]
"The first problem is addressed by universal approximation theorems for neural networks (Hornik, 1989).",3.3.1. CONSISTENCY,[0],[0]
"For the second problem, classical consistency theorems for extremum estimators apply (Van de Geer, 2000) under mild conditions on the parameter space.
",3.3.1. CONSISTENCY,[0],[0]
This leads to the two lemmas below.,3.3.1. CONSISTENCY,[0],[0]
"The first lemma states that the neural information measures IΘ(X,Z), defined in Eqn. 10, can approximate the mutual information with arbitrary accuracy:
Lemma 1 (approximation).",3.3.1. CONSISTENCY,[0],[0]
Let > 0.,3.3.1. CONSISTENCY,[0],[0]
There exists a neural network parametrizing functions,3.3.1. CONSISTENCY,[0],[0]
Tθ with parameters θ in some compact domain Θ ⊂,3.3.1. CONSISTENCY,[0],[0]
"Rk, such that
|I(X,Z)− IΘ(X,Z)| ≤ , a.e.
",3.3.1. CONSISTENCY,[0],[0]
"The second lemma states the almost sure convergence of MINE to a neural information measure as the number of samples goes to infinity:
Lemma 2 (estimation).",3.3.1. CONSISTENCY,[0],[0]
Let > 0.,3.3.1. CONSISTENCY,[0],[0]
Given a family of neural network functions,3.3.1. CONSISTENCY,[0],[0]
Tθ with parameters θ in some bounded domain Θ ⊂,3.3.1. CONSISTENCY,[0],[0]
"Rk, there exists an N ∈ N, such that
∀n ≥ N, | ̂I(X;Z)n",3.3.1. CONSISTENCY,[0],[0]
"− IΘ(X,Z) |≤ , a.e. (13)
Combining the two lemmas with the triangular inequality, we have,
Theorem 2.",3.3.1. CONSISTENCY,[0],[0]
MINE is strongly consistent.,3.3.1. CONSISTENCY,[0],[0]
In this section we discuss the sample complexity of our estimator.,3.3.2. SAMPLE COMPLEXITY,[0],[0]
"Since the focus here is on the empirical estimation problem, we assume that the mutual information is well enough approximated by the neural information measure IΘ(X,Z).",3.3.2. SAMPLE COMPLEXITY,[0],[0]
"The theorem below is a refinement of Lemma 2: it gives how many samples we need for an empirical estimation of the neural information measure at a given accuracy and with high confidence.
",3.3.2. SAMPLE COMPLEXITY,[0],[0]
"We make the following assumptions: the functions Tθ are M -bounded (i.e., |Tθ| ≤M ) and L-Lipschitz with respect to the parameters θ.",3.3.2. SAMPLE COMPLEXITY,[0],[0]
The domain Θ ⊂,3.3.2. SAMPLE COMPLEXITY,[0],[0]
"Rd is bounded, so that ‖θ‖ ≤ K for some constant K.",3.3.2. SAMPLE COMPLEXITY,[0],[0]
"The theorem below shows a sample complexity of Õ ( d log d 2 ) , where d is the
dimension of the parameter space.
",3.3.2. SAMPLE COMPLEXITY,[0],[0]
Theorem 3.,3.3.2. SAMPLE COMPLEXITY,[0],[0]
"Given any values , δ of the desired accuracy and confidence parameters, we have,
Pr ( | ̂I(X;Z)n",3.3.2. SAMPLE COMPLEXITY,[0],[0]
"− IΘ(X,Z)| ≤ )",3.3.2. SAMPLE COMPLEXITY,[0],[0]
"≥ 1− δ, (14)
",3.3.2. SAMPLE COMPLEXITY,[0],[0]
"whenever the number n of samples satisfies
n",3.3.2. SAMPLE COMPLEXITY,[0],[0]
"≥ 2M 2(d log(16KL
√ d/ )",3.3.2. SAMPLE COMPLEXITY,[0],[0]
"+ 2dM + log(2/δ))
2 .
(15)",3.3.2. SAMPLE COMPLEXITY,[0],[0]
"Before diving into applications, we perform some simple empirical evaluation and comparisons of MINE.",4. Empirical comparisons,[0],[0]
The objective is to show that MINE is effectively able to estimate mutual information and account for non-linear dependence.,4. Empirical comparisons,[0],[0]
We compare MINE and MINE-f to the k-NN-based nonparametric estimator found in Kraskov et al. (2004).,4.1. Comparing MINE to non-parametric estimation,[0],[0]
"In our experiment, we consider multivariate Gaussian random variables, Xa and Xb, with componentwise correlation, corr(Xia, X j b ) = δij ρ, where ρ ∈ (−1, 1) and δij is Kronecker’s delta.",4.1. Comparing MINE to non-parametric estimation,[0],[0]
"As the mutual information is invariant to continuous bijective transformations of the considered variables, it is enough to consider standardized Gaussians marginals.",4.1. Comparing MINE to non-parametric estimation,[0],[0]
"We also compare MINE (using the DonskerVaradhan representation in Eqn. 6) and MINE-f (based on the f -divergence representation in Eqn. 8).
",4.1. Comparing MINE to non-parametric estimation,[0],[0]
Our results are presented in Figs. 1.,4.1. Comparing MINE to non-parametric estimation,[0],[0]
We observe that both MINE and Kraskov’s estimation are virtually indistinguishable from the ground truth when estimating the mutual information between bivariate Gaussians.,4.1. Comparing MINE to non-parametric estimation,[0],[0]
MINE shows marked improvement over Krakov’s when estimating the mutual information between twenty dimensional random variables.,4.1. Comparing MINE to non-parametric estimation,[0],[0]
We also remark that MINE provides a tighter estimate of the mutual information than MINE-f .,4.1. Comparing MINE to non-parametric estimation,[0],[0]
"An important property of mutual information between random variables with relationship Y = f(X)+σ , where f is a deterministic non-linear transformation and is random noise, is that it is invariant to the deterministic nonlinear transformation, but should only depend on the amount of noise, σ .",4.2. Capturing non-linear dependencies,[0],[0]
"This important property, that guarantees the quantification dependence without bias for the relationship,
is called equitability (Kinney & Atwal, 2014).",4.2. Capturing non-linear dependencies,[0],[0]
Our results (Fig. 2) show that MINE captures this important property.,4.2. Capturing non-linear dependencies,[0],[0]
"In this section, we use MINE to present applications of mutual information and compare to competing methods designed to achieve the same goals.",5. Applications,[0],[0]
"Specifically, by using MINE to maximize the mutual information, we are able to improve mode representation and reconstruction of generative models.",5. Applications,[0],[0]
"Finally, by minimizing mutual information, we are able to effectively implement the information bottleneck in a continuous setting.",5. Applications,[0],[0]
"Mode collapse (Che et al., 2016; Dumoulin et al., 2016; Donahue et al., 2016; Salimans et al., 2016; Metz et al., 2017; Saatchi & Wilson, 2017; Nguyen et al., 2017; Lin et al., 2017; Ghosh et al., 2017) is a common pathology of generative adversarial networks (GANs, Goodfellow et al., 2014), where the generator fails to produces samples with sufficient diversity (i.e., poorly represent some modes).
",5.1. Maximizing mutual information to improve GANs,[0],[0]
"GANs as formulated in Goodfellow et al. (2014) consist of two components: a discriminator, D : X →",5.1. Maximizing mutual information to improve GANs,[0],[0]
"[0, 1] and a generator, G : Z → X , where X is a domain such as a compact subspace of Rn.",5.1. Maximizing mutual information to improve GANs,[0],[0]
"Given Z ∈ Z follows some simple prior distribution (e.g., a spherical Gaussian with density, PZ ), the goal of the generator is to match its output distribution to a target distribution, PX (specified by the data samples).",5.1. Maximizing mutual information to improve GANs,[0],[0]
"The discriminator and generator are optimized through the value function,
min G max D V (D,G) :=
EPX [D(X)]",5.1. Maximizing mutual information to improve GANs,[0],[0]
+ EPZ [log (1−D(G(Z)),5.1. Maximizing mutual information to improve GANs,[0],[0]
].,5.1. Maximizing mutual information to improve GANs,[0],[0]
"(16)
A natural approach to diminish mode collapse would be regularizing the generator’s loss with the neg-entropy of the samples.",5.1. Maximizing mutual information to improve GANs,[0],[0]
"As the sample entropy is intractable, we propose to use the mutual information as a proxy.
",5.1. Maximizing mutual information to improve GANs,[0],[0]
"Following Chen et al. (2016), we write the prior as the concatenation of noise and code variables, Z =",5.1. Maximizing mutual information to improve GANs,[0],[0]
"[ , c].
We propose to palliate mode collapse by maximizing the mutual information between the samples and the code.",5.1. Maximizing mutual information to improve GANs,[0],[0]
"I(G([ , c]); c) = H(G",5.1. Maximizing mutual information to improve GANs,[0],[0]
"([ , c]))",5.1. Maximizing mutual information to improve GANs,[0],[0]
"− H(G([ , c]) | c).",5.1. Maximizing mutual information to improve GANs,[0],[0]
"The generator objective then becomes,
arg max G
E[log(D(G([ , c])))]",5.1. Maximizing mutual information to improve GANs,[0],[0]
+ βI(G,5.1. Maximizing mutual information to improve GANs,[0],[0]
"([ , c]); c).",5.1. Maximizing mutual information to improve GANs,[0],[0]
"(17)
",5.1. Maximizing mutual information to improve GANs,[0],[0]
"As the samples G([ , c]) are differentiable w.r.t.",5.1. Maximizing mutual information to improve GANs,[0],[0]
"the parameters of G, and the statistics network being a differentiable function, we can maximize the mutual information using back-propagation and gradient ascent by only specifying this additional loss term.",5.1. Maximizing mutual information to improve GANs,[0],[0]
"Since the mutual information is theoretically unbounded, we use adaptive gradient clipping (see the Supplementary Material) to ensure that the generator receives learning signals similar in magnitude from the discriminator and the statistics network.
",5.1. Maximizing mutual information to improve GANs,[0],[0]
Related works on mode-dropping Methods to address mode dropping in GANs can readily be found in the literature.,5.1. Maximizing mutual information to improve GANs,[0],[0]
Salimans et al. (2016) use mini-batch discrimination.,5.1. Maximizing mutual information to improve GANs,[0],[0]
"In the same spirit, Lin et al. (2017) successfully mitigates mode dropping in GANs by modifying the discriminator to make decisions on multiple real or generated samples.",5.1. Maximizing mutual information to improve GANs,[0],[0]
Ghosh et al. (2017) uses multiple generators that are encouraged to generate different parts of the target distribution.,5.1. Maximizing mutual information to improve GANs,[0],[0]
Nguyen et al. (2017) uses two discriminators to minimize the KL and reverse KL divergences between the target and generated distributions.,5.1. Maximizing mutual information to improve GANs,[0],[0]
"Che et al. (2016) learns a reconstruction distribution, then teach the generator to sample from it, the intuition being that the reconstruction distribution is a de-noised or smoothed version of the data distribution, and thus easier to learn.",5.1. Maximizing mutual information to improve GANs,[0],[0]
"Srivastava et al. (2017) minimizes the reconstruction error in the latent space of bi-directional GANs (Dumoulin et al., 2016; Donahue et al., 2016).",5.1. Maximizing mutual information to improve GANs,[0],[0]
Metz et al. (2017) includes many steps of the discriminator’s optimization as part of the generator’s objective.,5.1. Maximizing mutual information to improve GANs,[0],[0]
"While Chen et al. (2016) maximizes the mutual information between the code and the samples, it does so by minimizing a variational upper bound on the conditional entropy (Barber & Agakov, 2003) therefore ignoring the entropy of the samples.",5.1. Maximizing mutual information to improve GANs,[0],[0]
"Chen et al. (2016) makes no claim about mode-dropping.
",5.1. Maximizing mutual information to improve GANs,[0],[0]
"Experiments: Spiral, 25-Gaussians datasets We apply MINE to improve mode coverage when training a generative adversarial network (GAN, Goodfellow et al., 2014).",5.1. Maximizing mutual information to improve GANs,[0],[0]
"We demonstrate using Eqn. 17 on the spiral and the 25- Gaussians datasets, comparing two models, one with β = 0 (which corresponds to the orthodox GAN as in Goodfellow et al. (2014)) and one with β = 1.0, which corresponds to mutual information maximization.
",5.1. Maximizing mutual information to improve GANs,[0],[0]
Our results on the spiral (Fig. 3) and the 25-Gaussians (Fig. 4) experiments both show improved mode coverage over the baseline with no mutual information objective.,5.1. Maximizing mutual information to improve GANs,[0],[0]
"This
confirms our hypothesis that maximizing mutual information helps against mode-dropping in this simple setting.
",5.1. Maximizing mutual information to improve GANs,[0],[0]
"Experiment: Stacked MNIST Following Che et al. (2016); Metz et al. (2017); Srivastava et al. (2017); Lin et al. (2017), we quantitatively assess MINE’s ability to diminish mode dropping on the stacked MNIST dataset which is constructed by stacking three randomly sampled MNIST digits.",5.1. Maximizing mutual information to improve GANs,[0],[0]
"As a consequence, stacked MNIST offers 1000 modes.",5.1. Maximizing mutual information to improve GANs,[0],[0]
"Using the same architecture and training protocol as in Srivastava et al. (2017); Lin et al. (2017), we train a GAN on the constructed dataset and use a pre-trained classifier on 26,000 samples to count the number of modes in the samples, as well as to compute the KL divergence between the sample and expected data distributions.",5.1. Maximizing mutual information to improve GANs,[0],[0]
Our results in Table 1 demonstrate the effectiveness of MINE in preventing mode collapse on Stacked MNIST.,5.1. Maximizing mutual information to improve GANs,[0],[0]
"Adversarial bi-directional models were introduced in Adversarially Learned Inference (ALI, Dumoulin et al., 2016) and BiGAN (Donahue et al., 2016) and are an extension of GANs which incorporate a reverse model, F : X → Z jointly trained with the generator.",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"These models formulate the problem in terms of the value function in Eqn. 16 between two joint distributions, p(x, z) =",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"p(z | x)p(x) and q(x, z) = q(x | z)p(z) induced by the forward (encoder)
and reverse (decoder) models, respectively7.
",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
One goal of bi-directional models is to do inference as well as to learn a good generative model.,5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"Reconstructions are one desirable property of a model that does both inference and generation, but in practice ALI can lack fidelity (i.e., reconstructs less faithfully than desired, see Li et al., 2017; Ulyanov et al., 2017; Belghazi et al., 2018).",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"To demonstrate the connection to mutual information, it can be shown (see the Supplementary Material for details) that the reconstruction error,R, is bounded by,
R ≤ DKL(q(x, z) ||",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"p(x, z))− Iq(x, z) +Hq(z) (18)
",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"If the joint distributions are matched,Hq(z) tends toHp(z), which is fixed as long as the prior, p(z), is itself fixed.",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"Subsequently, maximizing the mutual information minimizes the expected reconstruction error.
",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"Assuming that the generator is the same as with GANs in the previous section, the objectives for training a bi-directional adversarial model then become:
arg max D
Eq(x,z)[logD(x, z)]",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"+ Ep(x,z)[log (1−D(x, z))]
arg max F,G
Eq(x,z)[log (1−D(x, z))]",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"+ Ep(x,z)[logD(x, z)]
+ βIq(x, z).",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"(19)
Related works Ulyanov et al. (2017) improves reconstructions quality by forgoing the discriminator and expressing the adversarial game between the encoder and decoder.",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
Kumar et al. (2017) augments the bi-directional objective by considering the reconstruction and the corresponding encodings as an additional fake pair.,5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"Belghazi et al. (2018) shows that a Markovian hierarchical generator in a bi-directional adversarial model provide a hierarchy of
7We switch to density notations for convenience throughout this section.
",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
reconstructions with increasing levels of fidelity (increasing reconstruction quality).,5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
Li et al. (2017) shows that the expected reconstruction error can be diminished by minimizing the conditional entropy of the observables given the latent representations.,5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"The conditional entropy being intractable for general posterior, Li et al. (2017) proposes to augment the generator’s loss with an adversarial cycle consistency loss (Zhu et al., 2017) between the observables and their reconstructions.",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
Experiment: ALI+MINE In this section we compare MINE to existing bi-directional adversarial models.,5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"As the decoder’s density is generally intractable, we use three different metrics to measure the fidelity of the reconstructions with respect to the samples; (i) the euclidean reconstruction error, (ii) reconstruction accuracy, which is the proportion of labels preserved by the reconstruction as identified by a pre-trained classifier; (iii) the Multi-scale structural similarity metric (MS-SSIM, Wang et al., 2004) between the observables and their reconstructions.
",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"We train MINE on datasets of increasing order of complexity: a toy dataset composed of 25-Gaussians, MNIST (LeCun, 1998), and the CelebA dataset (Liu et al., 2015).",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
Fig. 6 shows the reconstruction ability of MINE compared to ALI.,5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"Although ALICE does perfect reconstruction (which is in its explicit formulation), we observe significant mode-dropping in the sample space.",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"MINE does a balanced job of reconstructing along with capturing all the modes of the underlying data distribution.
",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"Next, we measure the fidelity of the reconstructions over ALI, ALICE, and MINE.",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"Tbl. 2 compares MINE to the existing baselines in terms of euclidean reconstruction errors, reconstruction accuracy, and MS-SSIM.",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"On MNIST, MINE outperforms ALI in terms of reconstruction errors by a good margin and is competitive to ALICE with respect to reconstruction accuracy and MS-SSIM.",5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
Our results show that MINE’s effect on reconstructions is even more dramatic when compared to ALI and ALICE on the CelebA dataset.,5.2. Maximizing mutual information to improve inference in bi-directional adversarial models,[0],[0]
"The Information Bottleneck (IB, Tishby et al., 2000) is an information theoretic method for extracting relevant infor-
Model Recons.",5.3. Information Bottleneck,[0],[0]
Error Recons.,5.3. Information Bottleneck,[0],[0]
"Acc.(%) MS-SSIM
MNIST
ALI 14.24 45.95 0.97 ALICE(l2) 3.20 99.03 0.97 ALICE(Adv.)",5.3. Information Bottleneck,[0],[0]
"5.20 98.17 0.98 MINE 9.73 96.10 0.99
CelebA
mation, or yielding a representation, that an input X ∈ X contains about an output Y ∈ Y .",5.3. Information Bottleneck,[0],[0]
An optimal representation of X would capture the relevant factors and compress X by diminishing the irrelevant parts which do not contribute to the prediction of Y .,5.3. Information Bottleneck,[0],[0]
"IB was recently covered in the context of deep learning (Tishby & Zaslavsky, 2015), and as such can be seen as a process to construct an approximation of the minimally sufficient statistics of the data.",5.3. Information Bottleneck,[0],[0]
"IB seeks an encoder, q(Z | X), that induces the Markovian structure X → Z → Y .",5.3. Information Bottleneck,[0],[0]
"This is done by minimizing the IB Lagrangian,
L[q(Z | X)]",5.3. Information Bottleneck,[0],[0]
"= H(Y |Z) + βI(X,Z), (20)
which appears as a standard cross-entropy loss augmented with a regularizer promoting minimality of the representation (Achille & Soatto, 2017).",5.3. Information Bottleneck,[0],[0]
"Here we propose to estimate the regularizer with MINE.
",5.3. Information Bottleneck,[0],[0]
"Related works In the discrete setting, (Tishby et al., 2000) uses the Blahut-Arimoto Algorithm (Arimoto, 1972), which
(a) ALI (b) ALICE (l2) (c) ALICE (A) (d) ALI+MINE Figure 6.",5.3. Information Bottleneck,[0],[0]
Reconstructions and model samples from adversarially learned inference (ALI) and variations intended to increase improve reconstructions.,5.3. Information Bottleneck,[0],[0]
"Shown left to right are the baseline (ALI), ALICE with the l2 loss to minimize the reconstruction error, ALICE with an adversarial loss, and ALI+MINE.",5.3. Information Bottleneck,[0],[0]
Top to bottom are the reconstructions and samples from the priors.,5.3. Information Bottleneck,[0],[0]
"ALICE with the adversarial loss has the best reconstruction, though at the expense of poor sample quality, where as ALI+MINE captures all the modes of the data in sample space.
can be understood as cyclical coordinate ascent in function spaces.",5.3. Information Bottleneck,[0],[0]
"While IB is successful and popular in a discrete setting, its application to the continuous setting was stifled by the intractability of the continuous mutual information.",5.3. Information Bottleneck,[0],[0]
"Nonetheless, IB was applied in the case of jointly Gaussian random variables in (Chechik et al., 2005).
",5.3. Information Bottleneck,[0],[0]
"In order to overcome the intractability of I(X;Z) in the continuous setting, Alemi et al. (2016); Kolchinsky et al. (2017); Chalk et al. (2016) exploit the variational bound of Barber & Agakov (2003) to approximate the conditional entropy in I(X;Z).",5.3. Information Bottleneck,[0],[0]
"These approaches differ only on their treatment of the marginal distribution of the bottleneck variable: Alemi et al. (2016) assumes a standard multivariate normal marginal distribution, Chalk et al. (2016) uses a Student-t distribution, and Kolchinsky et al. (2017) uses non-parametric estimators.",5.3. Information Bottleneck,[0],[0]
"Due to their reliance on a variational approximation, these methods require a tractable density for the approximate posterior, while MINE does not.",5.3. Information Bottleneck,[0],[0]
"Experiment: Permutation-invariant MNIST classification Here, we demonstrate an implementation of the IB objective on permutation invariant MNIST using MINE.",5.3. Information Bottleneck,[0],[0]
"We compare to the Deep Variational Bottleneck (DVB, Alemi et al., 2016) and use the same empirical setup.",5.3. Information Bottleneck,[0],[0]
"As the DVB relies on a variational bound on the conditional entropy, it therefore requires a tractable density.",5.3. Information Bottleneck,[0],[0]
"Alemi et al. (2016) opts for a conditional Gaussian encoder z = µ(x) + σ , where ∼ N (0, I).",5.3. Information Bottleneck,[0],[0]
"As MINE does not require a tractable density, we consider three type of encoders: (i) a Gaussian encoder as in Alemi et al. (2016); (ii) an additive noise encoder, z = enc(x+ σ ); and (iii) a propagated noise encoder, z = enc([x, ]).",5.3. Information Bottleneck,[0],[0]
"Our results can be seen in Tbl. 3, and this shows MINE as being superior in these settings.",5.3. Information Bottleneck,[0],[0]
"We proposed a mutual information estimator, which we called the mutual information neural estimator (MINE), that is scalable in dimension and sample-size.",6. Conclusion,[0],[0]
"We demonstrated
the efficiency of this estimator by applying it in a number of settings.",6. Conclusion,[0],[0]
"First, a term of mutual information can be introduced alleviate mode-dropping issue in generative adversarial networks (GANs, Goodfellow et al., 2014).",6. Conclusion,[0],[0]
"Mutual information can also be used to improve inference and reconstructions in adversarially-learned inference (ALI, Dumoulin et al., 2016).",6. Conclusion,[0],[0]
"Finally, we showed that our estimator allows for tractable application of Information bottleneck methods (Tishby et al., 2000) in a continuous setting.",6. Conclusion,[0],[0]
"We would like to thank Martin Arjovsky, Caglar Gulcehre, Marcin Moczulski, Negar Rostamzadeh, Thomas Boquet, Ioannis Mitliagkas, Pedro Oliveira Pinheiro for helpful comments, as well as Samsung and IVADO for their support.",7. Acknowledgements,[0],[0]
We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks.,abstractText,[0],[0]
"We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent.",abstractText,[0],[0]
We present a handful of applications on which MINE can be used to minimize or maximize mutual information.,abstractText,[0],[0]
We apply MINE to improve adversarially trained generative models.,abstractText,[0],[0]
"We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.",abstractText,[0],[0]
Mutual Information Neural Estimation,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 654–664, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Currently, a growing number of people prefer to express their views and comments online.",1 Introduction,[0],[0]
"These messages, which are updated at the rate of millions per day, become a potentially rich source of information.",1 Introduction,[0],[0]
"From such a large number of texts, entity disambiguation is a critical step when extracting text information from these messages, and for various applications, such as natural language processing and knowledge acquisition (Dredze et al., 2010).",1 Introduction,[0],[0]
Take the application of customer feedback analysis (CFA) as an example.,1 Introduction,[0],[0]
"An enterprise is typically interested in public reviews of its own products as well as those of its competitors’; the identification of these entities is thus critical for further analysis.
",1 Introduction,[0],[0]
The product names comprise a list of entities to be identified.,1 Introduction,[0],[0]
"We refer to these entities of the same domain as target entities, and the identifica-
tion process is called target entity disambiguation (Wang et al., 2012).",1 Introduction,[0],[0]
"All of target entities (e.g., car brands) share a common domain, which we refer to as the target domain.",1 Introduction,[0],[0]
"The target domain is the only constraint to target entities, which implies that the entities are in a specific domain, rather than being general things.",1 Introduction,[0],[0]
"In the case of entity recognition from short texts, the disambiguation can be performed on the document level.",1 Introduction,[0],[0]
"Given a collection of short documents, our goal is to determine which documents contain the target entities.
",1 Introduction,[0],[0]
"Challenge and Related Work In contrast to traditional entity disambiguation tasks, TED in short texts can require as little information as a name list.",1 Introduction,[0],[0]
"There are three types of information that are utilized in Named Entity Disambiguation related tasks: knowledge resources, the context in which a target word occurs and statistical information (Navigli, 2009).",1 Introduction,[0],[0]
"However, the lack of the first two types of information makes this problem more challenging.
",1 Introduction,[0],[0]
"Knowledge Sparsity A large number of methods focus on using knowledge bases (KBs) like Wikipedia or YAGO to enrich the named entities (e.g., in-links and out-links) (Hoffart et al., 2011; Bunescu and Pasca, 2006; Milne and Witten, 2008; Kulkarni et al., 2009; Han et al., 2011; Mihalcea and Csomai, 2007; Shen et al., 2012).",1 Introduction,[0],[0]
These methods compared the context of the entities and their reference pages in the KBs through a similarity measurement.,1 Introduction,[0],[0]
"However, we tested 32 different names of General Motors (GM) car brands, and only four of the brands exist in Wikipedia.",1 Introduction,[0],[0]
This circumstance is not unusual.,1 Introduction,[0],[0]
"For another larger dataset that included 2468 stock names, we found only 340 of them had their reference pages in Wikipedia.",1 Introduction,[0],[0]
"Thus, the methods that rely heavily on KBs might not be appropriate here.
",1 Introduction,[0],[0]
Shortness of the Texts,1 Introduction,[0],[0]
"The context in which a
654
target entity occurs plays an important role in disambiguation.",1 Introduction,[0],[0]
"(Cassidy et al., 2012; Li et al., 2013) applied the underlying topical coherence information to a set of mentions for entity linking.",1 Introduction,[0],[0]
"In (Wang et al., 2012), mentionRank leverages some additional features, such as co-mention; however, people prefer to share their comments in brief or even informal words on social media platforms, such as Twitter, which becomes increasingly important as an information source.",1 Introduction,[0],[0]
"We have counted 350,498 microblogs, 37,379 tweets and 34,018 text snippets in various domains in Chinese and English from Twitter, Sina Weibo and Google.",1 Introduction,[0],[0]
"After preprocessing, their average length are 16, 5 and 11 words, respectively.",1 Introduction,[0],[0]
"To alleviate the shortness issue, additional information has been used to expand the context, such as the user’s interest (Shen et al., 2013) and related documents (Guo et al., 2013).",1 Introduction,[0],[0]
"Such information is still sparse or unavailable in this case, and thus, the existing methods might not be suitable.
",1 Introduction,[0],[0]
"Large Scale Hundreds of millions of tweets are posted daily on Twitter (Liu et al., 2013).",1 Introduction,[0],[0]
"When scaling to a large document collection, the disambiguation task becomes increasingly important as the ambiguity increases (Cucerzan, 2007).",1 Introduction,[0],[0]
"MentionRank, the only state-of-the-art method for TED, is a graph-based model that focuses on a small set of entities (e.g., 50 or 100 entities) and conducts experiments on thousands of documents (Wang et al., 2012).",1 Introduction,[0],[0]
"However, the graph has a quadratic growth as the number of documents increases.",1 Introduction,[0],[0]
"In our experiments, a dataset that included 2,468 target entities and 350,498 microblogs generated a directed graph with billions of edges, which required more computer memory than was available even when using a sparse matrix.",1 Introduction,[0],[0]
"Therefore, the scalability remains a challenge.
",1 Introduction,[0],[0]
"Contributions To address these challenges, we propose a collective method called TremenRank to disambiguate the target entities simultaneously.",1 Introduction,[0],[0]
The main idea is to propagate trust within a graph based on our observations that true mentions are more similar in context than false mentions in a specific domain.,1 Introduction,[0],[0]
"Specifically, inverted indexes is used to construct the graph locally that allows for an arbitrary number of target entities and documents.",1 Introduction,[0],[0]
"Furthermore, a multi-layer directed graph is designed to assign different trust levels to doc-
uments, which significantly improves the performance.",1 Introduction,[0],[0]
"The contributions of our work can be summarized as follows:
• We propose a novel graph-based method, TremenRank, to collectively identify target entities in short texts.",1 Introduction,[0],[0]
"This method constructs the graph locally to avoid storing the entire graph in memory, which provides a linear scale up with respect to the number of target entities and documents.
",1 Introduction,[0],[0]
"• We design a multi-layer directed (MLD) graph for modeling short texts that possess different trust levels during propagation, which significantly improves the performance.
",1 Introduction,[0],[0]
• We conduct a series of experiments on three practical datasets from different domains.,1 Introduction,[0],[0]
"The experimental results demonstrate that TremenRank has a similar performance to the state-of-the-art when addressing the TED problem at a large scale, and the use of MLD graph significantly improves our method.",1 Introduction,[0],[0]
Example Suppose that GM wants to collect tweets that talk about its cars.,2 Problem Definition,[0],[0]
"As shown in Figure 1, we take (i) a list of car brands (target entities), and (ii) a collection of tweets (i.e., short texts) as inputs.",2 Problem Definition,[0],[0]
"“Car” is the target domain, and “Sonic” appears in documents d1 and d2, which can be characterized as two mentions and the latter is a true mention.",2 Problem Definition,[0],[0]
"The goal is to find as many true mentions as possible.
",2 Problem Definition,[0],[0]
Our method models the collection of documents as a directed graph and outputs a trust score for each document via propagation.,2 Problem Definition,[0],[0]
The trust score indicates the likelihood of a document containing a true mention.,2 Problem Definition,[0],[0]
"For flexibility, the trust scores lie within the range of 0 to 1 and are globally comparable; thus, we can obtain the top-k documents or choose an appropriate cut-off value to balance the precision and recall for different application requirements.",2 Problem Definition,[0],[0]
"For example, in the application of CFA, a company expects a higher recall to achieve a comprehensive understanding of its product, whereas a recommendation system must provide as many precise microblogs as possible.
",2 Problem Definition,[0],[0]
"Formally, the problem of TED in short texts is defined as follows:
Target Entity Disambiguation in Short Texts",2 Problem Definition,[0],[0]
"Given a list of target entities E = {ei|i = 1, . . .",2 Problem Definition,[0],[0]
",m}, and a collection of text documents D = {dj |j = 1, . . .",2 Problem Definition,[0],[0]
",",2 Problem Definition,[0],[0]
"n}, Edj = {ej1, . .",2 Problem Definition,[0],[0]
.,2 Problem Definition,[0],[0]
", ejk} is the set of target entities contained in dj .",2 Problem Definition,[0],[0]
The goal is to output the trust score rj ∈,2 Problem Definition,[0],[0]
"[0, 1] for each document dj ∈ D.",2 Problem Definition,[0],[0]
"All the target entities in Edj share the same trust score rj .
",2 Problem Definition,[0],[0]
All of the mentions in one document have the same score because they share a common context.,2 Problem Definition,[0],[0]
"In the graph, the context similarity between documents is computed and used as the edges normally, where the width of the context window that surround the target entity in the document is typically chosen to be 10, 20 or 50 words.",2 Problem Definition,[0],[0]
"However, the documents considered here are limited in length, and a user seldom changes the topic in so few words; thus, we regard the entire document as the context of its target entities.",2 Problem Definition,[0],[0]
"When multiple target entities occur in one document, all of them are more likely to refer to the entities in the target domain.",2 Problem Definition,[0],[0]
"Thus, the trust score for the document is higher, as indicated by d4 (Figure 1).
",2 Problem Definition,[0],[0]
The only constraint for the target entities is that they are in the same target domain.,2 Problem Definition,[0],[0]
This constraint is reasonable in practice.,2 Problem Definition,[0],[0]
The majority of applications identify a set of entities in one domain at a time.,2 Problem Definition,[0],[0]
"For example, a computer company focuses its attentions on the brands in the computer area, whereas an investment company is mainly interested in stocks.",2 Problem Definition,[0],[0]
"Additionally, even if the target domain is general and contains several small domains, an intuitive solution is to split it into several subproblems, where each subproblem focuses
on the target entities in one small domain.",2 Problem Definition,[0],[0]
"Then, the task can be achieved by solving the subproblems individually.",2 Problem Definition,[0],[0]
TremenRank is a graph-based method that identifies target entities collectively.,3 Our Approach,[0],[0]
It propagates trust scores on the graph where each vertex denotes one document and an edge indicates the similarity between them.,3 Our Approach,[0],[0]
"Considering the large scale of the problem, we obtain the neighbors of one vertex when propagating by searching two indexes instead of storing the entire graph in memory.",3 Our Approach,[0],[0]
This approach allows an arbitrary number of target entities and documents to be processed.,3 Our Approach,[0],[0]
"To further improve the performance, a multi-layer directed graph is designed to treat the documents at different trust levels based on prior estimations.",3 Our Approach,[0],[0]
The documents within a domain share the characteristic of unitary similarity.,3.1 Hypotheses,[0],[0]
This characteristic implies that all of the true mentions have a similar context due to the target domain constraint and that false mentions are distinct because their meanings belong to diversified domains.,3.1 Hypotheses,[0],[0]
"We investigated the ambiguity of 2468 stock names (target entities in experiments), and manually labeled 301 of those names.",3.1 Hypotheses,[0],[0]
"As shown in Figure 2, there are many different meanings for these names outside of the target domain, such as plant, bank, media and animal.",3.1 Hypotheses,[0],[0]
"The distribution of meanings are long-tailed; thus, we gathered a group of meanings together (the class “others”).
",3.1 Hypotheses,[0],[0]
"Based on the statistical results, we can make the following hypotheses:
• The context of true mentions are similar to one another.",3.1 Hypotheses,[0],[0]
•,3.1 Hypotheses,[0],[0]
"The context of a false mention is different
from any of the true mention.",3.1 Hypotheses,[0],[0]
"• False mentions have distinct contexts across
different entities.
",3.1 Hypotheses,[0],[0]
"For example, the true mentions in d2, d4 and d6 (Figure 1) describe car brands of GM and share common pieces of text: “drive” or “Chevrolet”.",3.1 Hypotheses,[0],[0]
"However, “sonic”, “express” and “spark” in d1, d3 and d5 are all false mentions; in their contexts, they refer to sound, giving opinions, and a small amount of fire, respectively.",3.1 Hypotheses,[0],[0]
"These false mentions are different in context from one another and from any true mention.
",3.1 Hypotheses,[0],[0]
The assumptions resemble the main insight of MentionRank except the co-mention (multiple entities occur in one document).,3.1 Hypotheses,[0],[0]
"Co-mention seldom happens in short texts, and can be treated as the same because they share a common context.",3.1 Hypotheses,[0],[0]
"In conclusion, the assumptions suggest that a collective method could perform better than a method that disambiguates entities separately, because more comprehensive information on the entities from multiple “collaborators” (i.e., the mentions have similar contexts) has been used (Chen and Ji, 2011; Kulkarni et al., 2009; Pennacchiotti and Pantel, 2009; Will et al., 2010; Fernandez et al., 2010; Cucerzan, 2011; Guo et al., 2011; Han and Sun, 2011; Ratinov et al., 2011; Kozareva et al., 2011; Dalton and Dietz, 2013).",3.1 Hypotheses,[0],[0]
"Based on these assumptions, we build a graph to represent the documents and their relations, and we perform a TrustRank-like algorithm on the graph.",3.2 Graph-based Method,[0],[0]
"We are given a graph G = (V, E) that consists of a set V ofN documents (vertices) and a set E of directed edges, where each edge (di, dj) ∈ E denotes that di points to dj , and o(di) is the outneighbors of di.",3.2 Graph-based Method,[0],[0]
We constructed the edges of the graph according to the similarity relations between documents.,3.2.1 Similarity Measurement,[0],[0]
"Most similarity measurements (Artiles et al., 2010; Miller, 1995) could be used in the proposed method.",3.2.1 Similarity Measurement,[0],[0]
"After some exploration, we found that Jaccard similarity performs better.",3.2.1 Similarity Measurement,[0],[0]
"It can be efficiently calculated through simple set operations:
ωJij = J(di, dj) =",3.2.1 Similarity Measurement,[0],[0]
"∣∣Wdi ⋂Wdj ∣∣∣∣Wdi ⋃Wdj ∣∣ (1) where ωJij denotes the weight of edge (di, dj) using Jaccard similarity, andWdi is the set of words contained in di.",3.2.1 Similarity Measurement,[0],[0]
"ωij varies from 0 to 1, where a value closer to 1 indicates that its two nodes are more similar.",3.2.1 Similarity Measurement,[0],[0]
We link only similar nodes by choosing an appropriate threshold η.,3.2.1 Similarity Measurement,[0],[0]
"In other words, we have (di, dj) ∈ E , only if ωJij > η.",3.2.1 Similarity Measurement,[0],[0]
This is the foundation to construct the graph locally using inverted indexes.,3.2.1 Similarity Measurement,[0],[0]
"When the scale becomes excessively large, such as the 350,000 pieces in our dataset, the number of edges will increase into the billions, producing computational and storage-based difficulties.",3.2.2 Inverted Index,[0],[0]
"Considering that the propagation begins with document di and that we are required to find all of its out-neighbors o(di) through a traversal of the entire dataset, then the complexity is O(n2).",3.2.2 Inverted Index,[0],[0]
"Alternatively, we can represent the entire graph with a matrix; however, its billions of elements would be difficult to store and calculate.
",3.2.2 Inverted Index,[0],[0]
"To address the large scale problem, we construct the graph locally via inverted index technology.",3.2.2 Inverted Index,[0],[0]
"During propagation, the neighbors of the documents are obtained by searching the indexes in real time.",3.2.2 Inverted Index,[0],[0]
Two types of indexes are used: the document-to-word index and the wordto-document index.,3.2.2 Inverted Index,[0],[0]
"The former index recordsWdi of each document di ∈ D; the latter index records the occurrence of each word Dwk = {dj |wk ∈ W}, where W = {w1, . . .",3.2.2 Inverted Index,[0],[0]
", wN} is the word dictionary.",3.2.2 Inverted Index,[0],[0]
"Combining these two indexes, the total out-neighbors of a document can be obtained in constant time as follows:
1.",3.2.2 Inverted Index,[0],[0]
Obtain all of the wordsWdi of di via searching the document-to-word index.,3.2.2 Inverted Index,[0],[0]
2. Find the occurrences of each word,3.2.2 Inverted Index,[0],[0]
wk ∈ Wdi in the word-to-document index:,3.2.2 Inverted Index,[0],[0]
Ddi = {dj | ∪wt∈Wdi Dwt}.,3.2.2 Inverted Index,[0],[0]
Each document dj ∈,3.2.2 Inverted Index,[0],[0]
Ddi shares at least one common word with di. 3.,3.2.2 Inverted Index,[0],[0]
"Count the frequency of dj , which indicates the number of overlapping words between di and dj .",3.2.2 Inverted Index,[0],[0]
"Then, the frequency fij = |Wdi ∩ Wdj |, i 6= j and Equation 1 becomes:
ωJij = fij
|Wdi |+ |Wdj",3.2.2 Inverted Index,[0],[0]
"| − fij (2)
4.",3.2.2 Inverted Index,[0],[0]
Calculate the similarity weight.,3.2.2 Inverted Index,[0],[0]
We obtain the out-neighbors o(di) =,3.2.2 Inverted Index,[0],[0]
{dj |ωJij ≥ η}.,3.2.2 Inverted Index,[0],[0]
"Similar to TrustRank, an individual document propagates the trust score to its neighbors and those who have more neighbors will receive more trust.",3.2.3 Trust Propagation,[0],[0]
"Intuitively, trust attenuates along the edge.",3.2.3 Trust Propagation,[0],[0]
"There are several ways for attenuation to take place, such as trust dampening, trust splitting, and a combination of them (Gyöngyi et al., 2004).",3.2.3 Trust Propagation,[0],[0]
"For example, each document di ∈ D has a trust score ri, and its out-neighbors obtain α · ri (or ri|o(di)| ) through trust dampening (or splitting).",3.2.3 Trust Propagation,[0],[0]
"We adopt the third method, in which a document dampens its split trust with the attenuation coefficient α.
",3.2.3 Trust Propagation,[0],[0]
The final trust score is determined by two parts: the trust from a document’s neighbors and its prior estimation.,3.2.3 Trust Propagation,[0],[0]
"Then, TremenRank can iteratively propagate via the following function:
ri = α · ∑ o(di) rj |o(dj)| + (1− α) · T (di) (3)
where T (di) is the prior estimation of the document di and is a constant during iterative propagation.",3.2.3 Trust Propagation,[0],[0]
Here we simply assign a uniform distribution to all of the documents T (di) = 1|D| .,3.2.3 Trust Propagation,[0],[0]
"A more precise estimation will be discussed later.
",3.2.3 Trust Propagation,[0],[0]
"At the beginning of the propagation, we initialize the trust scores of the documents with the prior estimation.",3.2.3 Trust Propagation,[0],[0]
"Then, the trust scores of the documents are updated iteratively until convergence1.",3.2.3 Trust Propagation,[0],[0]
True mentions receive high scores because they are likely to connect with more trustworthy documents and thus receive more trust through the first term in Equation 3.,3.2.3 Trust Propagation,[0],[0]
"In contrast, false mentions are dissimilar to most other documents; thus, their scores gradually attenuate (the second term in Equation 3).",3.2.3 Trust Propagation,[0],[0]
These scores are globally comparable.,3.2.3 Trust Propagation,[0],[0]
"One can normalize the final scores by dividing them by the largest score, but the relative ordering of the documents will not change.",3.2.3 Trust Propagation,[0],[0]
"Thus, one document that is more likely to contain any
1We select the attenuation coefficient α = 0.85 and the number of iterations to be 20, which have been regarded as the standards in the PageRank literature(Page et al., 1999; Krishnan and Raj, 2006).",3.2.3 Trust Propagation,[0],[0]
"Our experiments also show that 20 iterations are sufficient to achieve convergence.
true mention will receive a higher trust score and be ranked higher.",3.2.3 Trust Propagation,[0],[0]
"During propagation, all of the documents are initialized with the same trust score and attenuate their trust at the same level.",3.3 Multi-layer Directed Graph,[0],[0]
"However, different documents should be treated differently.",3.3 Multi-layer Directed Graph,[0],[0]
"For example, the tweet “I drive a big car suburban” is more trustworthy than “doctors use GMC report system to process harmful patients”, because the former tweet contains the credible context feature “car”, which is the name of the target domain.",3.3 Multi-layer Directed Graph,[0],[0]
The latter clearly irrelevant text should not be trusted or even be regarded as noise.,3.3 Multi-layer Directed Graph,[0],[0]
"In this subsection, we first discuss how to make more precise prior estimation of documents based on some of the characteristics of the target domain.",3.3 Multi-layer Directed Graph,[0],[0]
"Additionally, based on the prior estimation, an MLD graph is built to assign different trust levels to the documents.",3.3 Multi-layer Directed Graph,[0],[0]
"Ideally, a true mention is similar to true mentions only.",3.3.1 Prior Estimation,[0],[0]
"To take advantage of the approximate isolation of true mentions, we first extract a set of true mentions to start the propagation.",3.3.1 Prior Estimation,[0],[0]
"The documents that contain these true mentions are called seeds.
",3.3.1 Prior Estimation,[0],[0]
"Formally, the entire set of documents D is divided into two groups: (i) the seed set Ds = {d1, . . .",3.3.1 Prior Estimation,[0],[0]
", ds}, and (ii) a subset D∗ = {ds+1, . . .",3.3.1 Prior Estimation,[0],[0]
", dn}.",3.3.1 Prior Estimation,[0],[0]
"We estimate a prior trust score for each document via the function T :
T (di) = {
1− |D∗| di ∈ D∗, |Ds| di ∈",3.3.1 Prior Estimation,[0],[0]
"Ds.
(4)
where |Ds| and |D∗| represent the size of the two sets for normalization.",3.3.1 Prior Estimation,[0],[0]
"∈ (0, 1] is used for smoothing and indicates the likelihood that we can trust the seeds that actually contain true mentions.",3.3.1 Prior Estimation,[0],[0]
"In the experiments, we set = 0.9 based on the accuracy of the seeds extraction method.
",3.3.1 Prior Estimation,[0],[0]
"There are several methods for extracting seeds, such as manual annotation and pattern-based method.",3.3.1 Prior Estimation,[0],[0]
"Patterns could be easily derived from the characteristics of the target domain, such as the domain name, product type or unique ID2.",3.3.1 Prior Estimation,[0],[0]
"These methods can typically identify entities with a high accuracy, but their low recall limits the portion of
2The exact patterns used in our datasets are detailed in Section 4
true mentions that can be found.",3.3.1 Prior Estimation,[0],[0]
"We use the results of the pattern-based method as our seeds, which have an accuracy higher than 90%.",3.3.1 Prior Estimation,[0],[0]
We do not present the experimental results here due to space limitations.,3.3.1 Prior Estimation,[0],[0]
"The similarity measurement does not consider directions, and thus, the documents are mutually connected.",3.3.2 Graph Construction,[0],[0]
"In this section, we construct a layered structure in the graph, where each layer denotes a document trust level that contains any true mention.",3.3.2 Graph Construction,[0],[0]
"Thus, we define that the propagation direction from the high trust level to the low trust level.
",3.3.2 Graph Construction,[0],[0]
Figure 3 shows an example of an MLD graph.,3.3.2 Graph Construction,[0],[0]
"The blue nodes of the seeds in the top layer are the most trustworthy, and the other white nodes in the higher layer are less similar to the seeds which implies that they are at lower trust levels.",3.3.2 Graph Construction,[0],[0]
"Thus, as we move farther away from the seeds, trust attenuates at a constant speed α along with the layers.",3.3.2 Graph Construction,[0],[0]
"The nodes in the same layer are also connected.
",3.3.2 Graph Construction,[0],[0]
"The construction algorithm3 is presented in Algorithm 1, where Dl = {dli|l ≥ 0} is the set of nodes in layer l, and the nodes in D̄ = {d̄j |d̄j /∈",3.3.2 Graph Construction,[0],[0]
"Dl, l ≥ 0} are not connected.",3.3.2 Graph Construction,[0],[0]
"To simplify our notation, the seeds are set to be in layer 0.",3.3.2 Graph Construction,[0],[0]
Note that (∩l≥0Dl) ∩ D̄,3.3.2 Graph Construction,[0],[0]
"= D.
TremenRank is different from the standard TrustRank and MentionRank in several respects.",3.3.2 Graph Construction,[0],[0]
"First, TremenRank is designed to process short texts at a large scale.",3.3.2 Graph Construction,[0],[0]
"Second, through a welldesigned MLD graph, we consider documents to consist of different trust levels rather than be represented by a unified distribution.",3.3.2 Graph Construction,[0],[0]
"Third, TrustRank
3A key parameter for the structure of MLD Graph is η, which will be discussed in Section 4.2.
",3.3.2 Graph Construction,[0],[0]
Algorithm 1: Construction of an MLD graph.,3.3.2 Graph Construction,[0],[0]
"Input: Ds, D∗, η, indexesWD,DW Output: G Initialize seeds in layer l = 0, D̄ = D∗; foreach layer l ≥ 0",3.3.2 Graph Construction,[0],[0]
"do
Find the out-neighbors of Dl from D̄; foreach document dli ∈",3.3.2 Graph Construction,[0],[0]
"Dl do
put o(dli) in the next layer D l+1;
update D̄ = D̄ −Dl+1; end if |D̄| = 0",3.3.2 Graph Construction,[0],[0]
"or |Dl+1| = 0 then
break; else
l = l + 1; end
end
randomly samples a set of seeds and checks them manually, which limits the number of seeds available; however, the proposed method extracts the seeds automatically and uses large amounts of seeds to produce better prior estimations.",3.3.2 Graph Construction,[0],[0]
"Finally, disambiguation occurs at the document level in the proposed method; we assign the same scores to the entities that occur in one document, because they typically share common context features in short texts.",3.3.2 Graph Construction,[0],[0]
"Because there is no publicly available benchmark dataset for TED, we constructed three datasets of different domains: Stock, Car and Herb4.",4.1 Data Preparation,[0],[0]
"All of these datasets came from the needs of real-life projects.
1.",4.1 Data Preparation,[0],[0]
"Stock - We collected 2,468 stock names from a stock exchange website, and identified their candidate mentions by string matching from Sina Weibo (Counterpart of Twitter in China).",4.1 Data Preparation,[0],[0]
"Each stock has a unique ID, which has little ambiguity.",4.1 Data Preparation,[0],[0]
"Thus, we used this regular expression pattern to extract seeds.
2.",4.1 Data Preparation,[0],[0]
Car - We collected 32 car brands of General Motors and a group of tweets that contain at least one mention of these brands via the Twitter API.,4.1 Data Preparation,[0],[0]
"In the seeds extraction step, we use the domain name “car” as the patterns.
4All of the dataset related sources used in this study are listed at http://newsminer.org/TEDs.
3.",4.1 Data Preparation,[0],[0]
"Herb - We randomly selected 1,119 Chinese herb names and collected 40 pieces of text descriptions in the search results per entity from Google.",4.1 Data Preparation,[0],[0]
"To extract the seeds, we simply added the domain name to the key words (e.g., “UÁ(Worms)¥ú (Chinese Medical Herb)”).
",4.1 Data Preparation,[0],[0]
Table 1 shows some of the statistics of the datasets created.,4.1 Data Preparation,[0],[0]
"We chose these datasets because (i) the identification of entities in these domains meets the practical requirements of many applications, (ii) the target entities are ambiguous and (iii) there is little information in the existing KBs.",4.1 Data Preparation,[0],[0]
"Before applying TremenRank, we preprocessed the plain texts through word segmentation, low frequency words filtering and stop words filtering.",4.1 Data Preparation,[0],[0]
"Baseline Methods TED in short texts is a relatively new problem, and there are few specific methods for solving it.",4.2 Experiment,[0],[0]
"To validate the performance of TremenRank and the improvement produced by the MLD graph, we selected the baseline from three different perspectives: (i) a context-based method that identifies target entities separately; (ii) a classic supervised method SVM to classify a document by whether it contains any true mentions; and (iii) the only state-of-the-art MentionRank for TED, which is a collective ranking method.
",4.2 Experiment,[0],[0]
"• The Context-based method mines a frequent item set of true mentions and identifies the documents that contain more frequent items.
",4.2 Experiment,[0],[0]
• SVM classifies the documents into two classes: documents that are in the target domain and those that are not.,4.2 Experiment,[0],[0]
"Using context words as features, we train and test SVM on the labeled set with a 10-fold cross validation.
",4.2 Experiment,[0],[0]
• MentionRank is a graph-based method that disambiguates at the entity level.,4.2 Experiment,[0],[0]
"Difficult to apply to the entire large datasets directly, it has been applied to the labeled set.
",4.2 Experiment,[0],[0]
Evaluation Metrics,4.2 Experiment,[0],[0]
"The performance of the disambiguation task is typically evaluated by accuracy, but in TEDs we are also interested in precision, recall and the F1-measure because different applications focus on different aspects.",4.2 Experiment,[0],[0]
"For example, in the application of CFA, a company expects a higher recall to collect as many reviews as possible, while in financial news recommendations, users prefer to read more accurate microblogs.",4.2 Experiment,[0],[0]
"Because the entire dataset is too large to evaluate directly, we randomly sampled 800 mentions for each dataset and labelled them manually to calculate the performance metrics5.
Results and Analysis The overall performances of TremenRank and those of the baseline methods on all of the datasets are shown in Table 2.",4.2 Experiment,[0],[0]
"The following is indicated in the results:
• TremenRank+MLD outperforms all of the baselines with all of the datasets, because it collectively identifies target entities and treats the documents differently based on a precise prior estimation.
",4.2 Experiment,[0],[0]
• The collective methods tend to perform better.,4.2 Experiment,[0],[0]
"Although, on the Car dataset, SVM achieves the second best performance, the use of MLD graph in TremenRank outperforms all of the methods tested.",4.2 Experiment,[0],[0]
"This is because false mentions that occupy a large proportion of the dataset produce too much noise, whose negative impacts can be reduced through the train set in the supervised method or a precise prior estimations.
",4.2 Experiment,[0],[0]
•,4.2 Experiment,[0],[0]
"Compared with MentionRank, TremenRank processes a larger number of entities and documents while achieving a similar performance.",4.2 Experiment,[0],[0]
"Combined with a MLD graph, TremenRank shows significant improvements including an average gain of 24.8% in accuracy and 15.2% in the F1-measure on the three datasets.
",4.2 Experiment,[0],[0]
"We also investigate the influence of the main elements in TremenRank below.
",4.2 Experiment,[0],[0]
"Similarity Threshold Different similarity thresholds result in various structures of the MLD graph, and have a great impact on the performance of our
5A detailed explanation about these metrics can be found in http://en.wikipedia.org/wiki/Precision and Recall.
0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
%
Accuracy Precision Recall F1-measure
0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 η
0.5
0.6
0.7
0.8
0.9
1.0
C o",4.2 Experiment,[0],[0]
"ve
ra g
e (%
)
coverage layer
0
1
2
3
4
5
6
# L
a ye
rs
Figure 4: Selection of the Similarity Threshold
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.40
0.50
0.60
0.70
0.80
0.90
C a r(
% )
",4.2 Experiment,[0],[0]
"Accuracy Precision Recall F1-measure
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
seeds scale(%)
0.70
0.75
0.80
0.85
0.90
H e rb
(% )
",4.2 Experiment,[0],[0]
Figure 5:,4.2 Experiment,[0],[0]
"Influence of Seed Scale
method.",4.2 Experiment,[0],[0]
"In this subsection, we study a heuristical method that help choose the best eta according two factors (Both of them can be obtained before propagation).",4.2 Experiment,[0],[0]
Figure 4 presents the experimental results for different values of η on the dataset of Herb.,4.2 Experiment,[0],[0]
"The performance of TremenRank is showed on the top, and the bottom is the corresponding graph structure represented by two factors: the graph coverage and the number of layers.",4.2 Experiment,[0],[0]
"We can see that precision increases until it becomes steady with the growth of η, and other measurements reach their peaks when η = 0.1.
",4.2 Experiment,[0],[0]
This is in accordance with the change of the graph structure.,4.2 Experiment,[0],[0]
"On one hand, the number of layers l declines sharply when η is less than 0.1, this indicates little difference in the trust level of documents in the propagation.",4.2 Experiment,[0],[0]
"On the other hand, our method has no effect on the vertices outside of the graph, so the performance is directly proportional to the graph coverage rate.",4.2 Experiment,[0],[0]
"Therefore, a proper η should ensure a high coverage as well as adequate layers.",4.2 Experiment,[0],[0]
"In experiments, we choose η as 0.1, 0.15, 0.1 for the datasets of Stock, Car and Herb respectively.
",4.2 Experiment,[0],[0]
"Influence of the Seed Scale As the basis of the prior estimation, the seeds have significant influ-
ence on the performance of the proposed method via the MLD graph.",4.2 Experiment,[0],[0]
"Intuitively, a larger set of seeds should lead to a more precise estimation and thus better performance.",4.2 Experiment,[0],[0]
"In the experiments on the Car and Herb datasets, we split their seed set into ten parts, and add one part each time.",4.2 Experiment,[0],[0]
"As Figure 5 shows, when increasing the percentage of the seeds gradually, the performance has an overall upward trend (e.g., a 14.6% and 4.4% gain in the F1-measure for the Car and Herb datasets, respectively).",4.2 Experiment,[0],[0]
This trend occurs because the potential context features of the seeds utilized for propagation increase as the absolute number of seed documents rises.,4.2 Experiment,[0],[0]
"For example, the tweet “Cadillac Uber airport is classy” obtains a low score of 0.013 with 50% of the seeds, whereas it is identified successfully with the score of 0.588 when all of the seeds are used for propagation because more context features are introduced, such as “airport” and “Uber”.",4.2 Experiment,[0],[0]
"Thus, the proposed method achieves better performance as the seed set grows.",4.2 Experiment,[0],[0]
"This arrangement is helpful for a company that seldom changes its business area and then accumulates seeds continuously to improve performance.
",4.2 Experiment,[0],[0]
"Robustness to Noise Because seeds play an important role in the MLD graph, we further test the
robustness of our method with respect to the quality of the seeds.",4.2 Experiment,[0],[0]
We randomly sample documents outside of the seed set (considered as noise) to replace 20% of the seeds on all the datasets; these results are shown in Figure 6.,4.2 Experiment,[0],[0]
"The introduced noise only leads to a limited decrease in performance (average 1.3%, 1% and 1% in the F1), which is much better than when only using TremenRank (Table 2).",4.2 Experiment,[0],[0]
"More specifically, the experiments that use artificial noise occasionally achieve a higher recall (e.g., on the Stock dataset); this result could occur because the unknown true mentions add some useful edges to the graph, which is helpful when finding more true mentions of the target entities.
",4.2 Experiment,[0],[0]
Cut-off Value,4.2 Experiment,[0],[0]
"According to the globally comparable trust scores, we can (i) rank all of the documents, and trade off the performance metrics by choosing an appropriate cut-off value γ for various applications, or (ii) rank the documents of an individual entity separately and obtain its top-k mentions.
",4.2 Experiment,[0],[0]
"In the experiments, we set γ as different percentage of the ranked documents.",4.2 Experiment,[0],[0]
"As Figure 7 shows, the recommendation system could use γ = 30% to achieve a 93.2% precision and a 64.2% recall, or a company could use γ = 60% for more reviews that have a relatively high precision.
",4.2 Experiment,[0],[0]
Efficiency We implemented TremenRank in Java and ran it on a single PC with the Windows 8.1 64- bit operation system.,4.2 Experiment,[0],[0]
"With an Intel(R) Core(TM) i3-3240 (3.40GHz) CPU and 4GB memory, our program converges within 5 iterations, and only consumes approximately 700MB of memory
when running steadily.",4.2 Experiment,[0],[0]
"The overall identification times of the Stock, Car and Herb datasets are 12h, 5min and 18min, respectively.",4.2 Experiment,[0],[0]
"The computation time increases exponentially with the increase of the amount of data due to the excessive computations required to search the indexes, which can be optimized in future work.",4.2 Experiment,[0],[0]
"In this paper, we addressed a new and increasingly important problem in social content analysis in a challenging scenario: disambiguation of a list of homogenous entities in short texts using names only.",5 Conclusions and Future Work,[0],[0]
We proposed a graph-based method called TremenRank to identify target entities collectively; this method can also hold an arbitrary number of target entities and documents.,5 Conclusions and Future Work,[0],[0]
The performance of this method can be further improved via a welldesigned MLD graph.,5 Conclusions and Future Work,[0],[0]
"The experimental results show that the proposed method has a significant improvement compared to other approaches.
",5 Conclusions and Future Work,[0],[0]
"In the future, we are interested in refining the prior estimation by using the ontology and extending this work to detect the target entities that are not in a list while performing the disambiguation task.
",5 Conclusions and Future Work,[0],[0]
"Acknowledgments We’d like to acknowledge Lei Hou, Chi Wang and Hongzhao Huang for their impressive discussions on this paper.",5 Conclusions and Future Work,[0],[0]
"The work is supported by 973 Program (No. 2014CB340504), NSFC-ANR (No. 61261130588), Tsinghua University Initiative Scientific Research Program (No. 20131089256), Science and Technology Support Program (No. 2014BAK04B00), and THU-NUS NExT Co-Lab.",5 Conclusions and Future Work,[0],[0]
"Target entity disambiguation (TED), the task of identifying target entities of the same domain, has been recognized as a critical step in various important applications.",abstractText,[0],[0]
"In this paper, we propose a graphbased model called TremenRank to collectively identify target entities in short texts given a name list only.",abstractText,[0],[0]
"TremenRank propagates trust within the graph, allowing for an arbitrary number of target entities and texts using inverted index technology.",abstractText,[0],[0]
"Furthermore, we design a multi-layer directed graph to assign different trust levels to short texts for better performance.",abstractText,[0],[0]
The experimental results demonstrate that our model outperforms state-of-the-art methods with an average gain of 24.8% in accuracy and 15.2% in the F1-measure on three datasets in different domains.,abstractText,[0],[0]
Name List Only? Target Entity Disambiguation in Short Texts,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2041–2050 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2041",text,[0],[0]
"The problem of similarity search, also called nearest-neighbor search, consists of finding documents from a large collection of documents, or corpus, which are most similar to a query document of interest.",1 Introduction,[0],[0]
"Fast and accurate similarity search is at the core of many information retrieval applications, such as plagiarism analysis (Stein et al., 2007), collaborative filtering (Koren, 2008), content-based multimedia retrieval (Lew et al., 2006) and caching (Pandey et al., 2009).",1 Introduction,[0],[0]
"Semantic hashing is an effective approach for fast similarity search (Salakhutdinov and Hinton, 2009; Zhang
∗ Equal contribution.
",1 Introduction,[0],[0]
"et al., 2010; Wang et al., 2014).",1 Introduction,[0],[0]
"By representing every document in the corpus as a similaritypreserving discrete (binary) hashing code, the similarity between two documents can be evaluated by simply calculating pairwise Hamming distances between hashing codes, i.e., the number of bits that are different between two codes.",1 Introduction,[0],[0]
"Given that today, an ordinary PC is able to execute millions of Hamming distance computations in just a few milliseconds (Zhang et al., 2010), this semantic hashing strategy is very computationally attractive.
",1 Introduction,[0],[0]
"While considerable research has been devoted to text (semantic) hashing, existing approaches typically require two-stage training procedures.",1 Introduction,[0],[0]
"These methods can be generally divided into two categories: (i) binary codes for documents are first learned in an unsupervised manner, then l binary classifiers are trained via supervised learning to predict the l-bit hashing code (Zhang et al., 2010; Xu et al., 2015); (ii) continuous text representations are first inferred, which are binarized as a second (separate) step during testing (Wang et al., 2013; Chaidaroon and Fang, 2017).",1 Introduction,[0],[0]
"Because the model parameters are not learned in an end-to-end manner, these two-stage training strategies may result in suboptimal local optima.",1 Introduction,[0],[0]
"This happens because different modules within the model are optimized separately, preventing the sharing of information between them.",1 Introduction,[0],[0]
"Further, in existing methods, binary constraints are typically handled adhoc by truncation, i.e., the hashing codes are obtained via direct binarization from continuous representations after training.",1 Introduction,[0],[0]
"As a result, the information contained in the continuous representations is lost during the (separate) binarization process.",1 Introduction,[0],[0]
"Moreover, training different modules (mapping and classifier/binarization) separately often requires additional hyperparameter tuning for each training stage, which can be laborious and timeconsuming.
",1 Introduction,[0],[0]
"In this paper, we propose a simple and generic neural architecture for text hashing that learns binary latent codes for documents in an end-toend manner.",1 Introduction,[0],[0]
"Inspired by recent advances in neural variational inference (NVI) for text processing (Miao et al., 2016; Yang et al., 2017; Shen et al., 2017b), we approach semantic hashing from a generative model perspective, where binary (hashing) codes are represented as either deterministic or stochastic Bernoulli latent variables.",1 Introduction,[0],[0]
The inference (encoder) and generative (decoder) networks are optimized jointly by maximizing a variational lower bound to the marginal distribution of input documents (corpus).,1 Introduction,[0],[0]
"By leveraging a simple and effective method to estimate the gradients with respect to discrete (binary) variables, the loss term from the generative (decoder) network can be directly backpropagated into the inference (encoder) network to optimize the hash function.
",1 Introduction,[0],[0]
"Motivated by the rate-distortion theory (Berger, 1971; Theis et al., 2017), we propose to inject data-dependent noise into the latent codes during the decoding stage, which adaptively accounts for the tradeoff between minimizing rate (number of bits used, or effective code length) and distortion (reconstruction error) during training.",1 Introduction,[0],[0]
"The connection between the proposed method and ratedistortion theory is further elucidated, providing a theoretical foundation for the effectiveness of our framework.
",1 Introduction,[0],[0]
"Summarizing, the contributions of this paper are: (i) to the best of our knowledge, we present the first semantic hashing architecture that can be trained in an end-to-end manner; (ii) we propose a neural variational inference framework to learn compact (regularized) binary codes for documents, achieving promising results on both unsupervised and supervised text hashing; (iii) the connection between our method and rate-distortion theory is established, from which we demonstrate the advantage of injecting data-dependent noise into the latent variable during training.",1 Introduction,[0],[0]
"Models with discrete random variables have attracted much attention in the deep learning community (Jang et al., 2016; Maddison et al., 2016; van den Oord et al., 2017; Li et al., 2017; Shu and Nakayama, 2017).",2 Related Work,[0],[0]
"Some of these structures are more natural choices for language or speech data, which are inherently discrete.",2 Related Work,[0],[0]
"More specifically,
<latexit sha1_base64=""7fXReuSi2AGXHQbFX8oagcVUXco="">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FvRi8cKxhaaWDbbTbp0Nxt3N4VS+ju8eFDx6p/x5r9x2+agrQ8GHu/NMDMvyjjTxnW/nZXVtfWNzdJWeXtnd2+/cnD4oGWuCPWJ5FK1I6wpZyn1DTOctjNFsYg4bUWDm6nfGlKlmUzvzSijocBJymJGsLFSGHCZoECzRODHerdSdWvuDGiZeAWpQoFmt/IV9CTJBU0N4VjrjudmJhxjZRjhdFIOck0zTAY4oR1LUyyoDsezoyfo1Co9FEtlKzVopv6eGGOh9UhEtlNg09eL3lT8z+vkJr4MxyzNckNTMl8U5xwZiaYJoB5TlBg+sgQTxeytiPSxwsTYnMo2BG/x5WXi12tXNffuvNq4LtIowTGcwBl4cAENuIUm+EDgCZ7hFd6cofPivDsf89YVp5g5gj9wPn8AnmWRig==</latexit><latexit sha1_base64=""7fXReuSi2AGXHQbFX8oagcVUXco="">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FvRi8cKxhaaWDbbTbp0Nxt3N4VS+ju8eFDx6p/x5r9x2+agrQ8GHu/NMDMvyjjTxnW/nZXVtfWNzdJWeXtnd2+/cnD4oGWuCPWJ5FK1I6wpZyn1DTOctjNFsYg4bUWDm6nfGlKlmUzvzSijocBJymJGsLFSGHCZoECzRODHerdSdWvuDGiZeAWpQoFmt/IV9CTJBU0N4VjrjudmJhxjZRjhdFIOck0zTAY4oR1LUyyoDsezoyfo1Co9FEtlKzVopv6eGGOh9UhEtlNg09eL3lT8z+vkJr4MxyzNckNTMl8U5xwZiaYJoB5TlBg+sgQTxeytiPSxwsTYnMo2BG/x5WXi12tXNffuvNq4LtIowTGcwBl4cAENuIUm+EDgCZ7hFd6cofPivDsf89YVp5g5gj9wPn8AnmWRig==</latexit><latexit sha1_base64=""7fXReuSi2AGXHQbFX8oagcVUXco="">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FvRi8cKxhaaWDbbTbp0Nxt3N4VS+ju8eFDx6p/x5r9x2+agrQ8GHu/NMDMvyjjTxnW/nZXVtfWNzdJWeXtnd2+/cnD4oGWuCPWJ5FK1I6wpZyn1DTOctjNFsYg4bUWDm6nfGlKlmUzvzSijocBJymJGsLFSGHCZoECzRODHerdSdWvuDGiZeAWpQoFmt/IV9CTJBU0N4VjrjudmJhxjZRjhdFIOck0zTAY4oR1LUyyoDsezoyfo1Co9FEtlKzVopv6eGGOh9UhEtlNg09eL3lT8z+vkJr4MxyzNckNTMl8U5xwZiaYJoB5TlBg+sgQTxeytiPSxwsTYnMo2BG/x5WXi12tXNffuvNq4LtIowTGcwBl4cAENuIUm+EDgCZ7hFd6cofPivDsf89YVp5g5gj9wPn8AnmWRig==</latexit>
van den Oord et al. (2017) combined VAEs with vector quantization to learn discrete latent representation, and demonstrated the utility of these learned representations on images, videos, and speech data.",2 Related Work,[0],[0]
"Li et al. (2017) leveraged both pairwise label and classification information to learn discrete hash codes, which exhibit state-of-the-art performance on image retrieval tasks.
",2 Related Work,[0],[0]
"For natural language processing (NLP), although significant research has been made to learn continuous deep representations for words or documents (Mikolov et al., 2013; Kiros et al., 2015; Shen et al., 2018), discrete neural representations have been mainly explored in learning word embeddings (Shu and Nakayama, 2017; Chen et al., 2017).",2 Related Work,[0],[0]
"In these recent works, words are represented as a vector of discrete numbers, which are very efficient storage-wise, while showing comparable performance on several NLP tasks, relative to continuous word embeddings.",2 Related Work,[0],[0]
"However, discrete representations that are learned in an endto-end manner at the sentence or document level have been rarely explored.",2 Related Work,[0],[0]
Also there is a lack of strict evaluation regarding their effectiveness.,2 Related Work,[0],[0]
Our work focuses on learning discrete (binary) representations for text documents.,2 Related Work,[0],[0]
"Further, we employ semantic hashing (fast similarity search) as a mechanism to evaluate the quality of learned binary latent codes.",2 Related Work,[0],[0]
"Inspired by the recent success of variational autoencoders for various NLP problems (Miao et al., 2016; Bowman et al., 2015; Yang et al., 2017; Miao et al., 2017; Shen et al., 2017b; Wang et al., 2018), we approach the training of discrete (binary) latent variables from a generative perspec-
tive.",3.1 Hashing under the NVI Framework,[0],[0]
"Let x and z denote the input document and its corresponding binary hash code, respectively.",3.1 Hashing under the NVI Framework,[0],[0]
"Most of the previous text hashing methods focus on modeling the encoding distribution p(z|x), or hash function, so the local/global pairwise similarity structure of documents in the original space is preserved in latent space (Zhang et al., 2010; Wang et al., 2013; Xu et al., 2015; Wang et al., 2014).",3.1 Hashing under the NVI Framework,[0],[0]
"However, the generative (decoding) process of reconstructing x from binary latent code z, i.e., modeling distribution p(x|z), has been rarely considered.",3.1 Hashing under the NVI Framework,[0],[0]
"Intuitively, latent codes learned from a model that accounts for the generative term should naturally encapsulate key semantic information from x because the generation/reconstruction objective is a function of p(x|z).",3.1 Hashing under the NVI Framework,[0],[0]
"In this regard, the generative term provides a natural training objective for semantic hashing.
",3.1 Hashing under the NVI Framework,[0],[0]
"We define a generative model that simultaneously accounts for both the encoding distribution, p(z|x), and decoding distribution, p(x|z), by defining approximations qφ(z|x) and qθ(x|z), via inference and generative networks, gφ(x) and gθ(z), parameterized by φ and θ, respectively.",3.1 Hashing under the NVI Framework,[0],[0]
"Specifically, x ∈ Z |V |+ is the bag-of-words (count) representation for the input document, where |V | is the vocabulary size.",3.1 Hashing under the NVI Framework,[0],[0]
"Notably, we can also employ other count weighting schemes as input features x, e.g., the term frequency-inverse document frequency (TFIDF) (Manning et al., 2008).",3.1 Hashing under the NVI Framework,[0],[0]
"For the encoding distribution, a latent variable z is first inferred from the input text x, by constructing an inference network gφ(x) to approximate the true posterior distribution p(z|x) as qφ(z|x).",3.1 Hashing under the NVI Framework,[0],[0]
"Subsequently, the decoder network gθ(z) maps z back into input space to reconstruct the original sequence x as x̂, approximating p(x|z) as qθ(x|z) (as shown in Figure 1).",3.1 Hashing under the NVI Framework,[0],[0]
"This cyclic strategy, x → z → x̂",3.1 Hashing under the NVI Framework,[0],[0]
"≈ x, provides the latent variable z with a better ability to generalize (Miao et al., 2016).
",3.1 Hashing under the NVI Framework,[0],[0]
"To tailor the NVI framework for semantic hashing, we cast z as a binary latent variable and assume a multivariate Bernoulli prior on z: p(z) ∼ Bernoulli(γ) = ∏l i=1",3.1 Hashing under the NVI Framework,[0],[0]
γ zi,3.1 Hashing under the NVI Framework,[0],[0]
"i (1 − γi)1−zi , where γi ∈",3.1 Hashing under the NVI Framework,[0],[0]
"[0, 1] is component i of vector γ.",3.1 Hashing under the NVI Framework,[0],[0]
"Thus, the encoding (approximate posterior) distribution qφ(z|x) is restricted to take the form qφ(z|x) = Bernoulli(h), where h = σ(gφ(x)), σ(·) is the sigmoid function, and gφ(·) is the (nonlinear) inference network specified as a multilayer perceptron (MLP).",3.1 Hashing under the NVI Framework,[0],[0]
"As illustrated in Figure 1, we can obtain
samples from the Bernoulli posterior either deterministically or stochastically.",3.1 Hashing under the NVI Framework,[0],[0]
"Suppose z is a l-bit hash code, for the deterministic binarization, we have, for i = 1, 2, ......, l:
zi = 1σ(giφ(x))>0.5 = sign(σ(giφ(x)− 0.5) + 1 2 ,
(1)
where z is the binarized variable, and zi and giφ(x) denote the i-th dimension of z and gφ(x), respectively.",3.1 Hashing under the NVI Framework,[0],[0]
"The standard Bernoulli sampling in (1) can be understood as setting a hard threshold at 0.5 for each representation dimension, therefore, the binary latent code is generated deterministically.",3.1 Hashing under the NVI Framework,[0],[0]
"Another strategy to obtain the discrete variable is to binarize h in a stochastic manner:
zi = 1σ(giφ(x))>µi = sign(σ(giφ(x))− µi) + 1 2 ,
(2)
",3.1 Hashing under the NVI Framework,[0],[0]
"where µi ∼ Uniform(0, 1).",3.1 Hashing under the NVI Framework,[0],[0]
"Because of this sampling process, we do not have to assume a predefined threshold value like in (1).",3.1 Hashing under the NVI Framework,[0],[0]
"To estimate the parameters of the encoder and decoder networks, we would ideally maximize the marginal distribution p(x) = ∫ p(z)p(x|z)dz.",3.2 Training with Binary Latent Variables,[0],[0]
"However, computing this marginal is intractable in most cases of interest.",3.2 Training with Binary Latent Variables,[0],[0]
"Instead, we maximize a variational lower bound.",3.2 Training with Binary Latent Variables,[0],[0]
"This approach is typically employed in the VAE framework (Kingma and Welling, 2013):
Lvae = Eqφ(z|x) [ log
qθ(x|z)p(z) qφ(z|x)
] , (3)
",3.2 Training with Binary Latent Variables,[0],[0]
"= Eqφ(z|x)[log qθ(x|z)]−DKL(qφ(z|x)||p(z)),
where the Kullback-Leibler (KL) divergence DKL(qφ(z|x)||p(z)) encourages the approximate posterior distribution qφ(z|x) to be close to the multivariate Bernoulli prior p(z).",3.2 Training with Binary Latent Variables,[0],[0]
"In this case, DKL(qφ(z|x)|p(z)) can be written in closed-form as a function of gφ(x):
DKL = gφ(x) log gφ(x)
γ
+ (1− gφ(x)) log 1− gφ(x) 1− γ .",3.2 Training with Binary Latent Variables,[0],[0]
"(4)
Note that the gradient for the KL divergence term above can be evaluated easily.
",3.2 Training with Binary Latent Variables,[0],[0]
"For the first term in (3), we should in principle estimate the influence of µi in (2) on qθ(x|z) by averaging over the entire (uniform) noise distribution.",3.2 Training with Binary Latent Variables,[0],[0]
"However, a closed-form distribution does not exist since it is not possible to enumerate all possible configurations of z, especially when the latent dimension is large.",3.2 Training with Binary Latent Variables,[0],[0]
"Moreover, discrete latent variables are inherently incompatible with backpropagation, since the derivative of the sign function is zero for almost all input values.",3.2 Training with Binary Latent Variables,[0],[0]
"As a result, the exact gradients of Lvae wrt the inputs before binarization would be essentially all zero.
",3.2 Training with Binary Latent Variables,[0],[0]
"To estimate the gradients for binary latent variables, we utilize the straight-through (ST) estimator, which was first introduced by Hinton (2012).",3.2 Training with Binary Latent Variables,[0],[0]
"So motivated, the strategy here is to simply backpropagate through the hard threshold by approximating the gradient ∂z/∂φ",3.2 Training with Binary Latent Variables,[0],[0]
as 1.,3.2 Training with Binary Latent Variables,[0],[0]
"Thus, we have:
dEqφ(z|x)[log qθ(x|z)]",3.2 Training with Binary Latent Variables,[0],[0]
"∂φ
= dEqφ(z|x)[log qθ(x|z)]
dz
dz
dσ(giφ(x))
dσ(giφ(x))
",3.2 Training with Binary Latent Variables,[0],[0]
"dφ
≈ dEqφ(z|x)[log qθ(x|z)]
dz
dσ(giφ(x))
",3.2 Training with Binary Latent Variables,[0],[0]
"dφ (5)
",3.2 Training with Binary Latent Variables,[0],[0]
"Although this is clearly a biased estimator, it has been shown to be a fast and efficient method relative to other gradient estimators for discrete variables, especially for the Bernoulli case (Bengio et al., 2013; Hubara et al., 2016; Theis et al., 2017).",3.2 Training with Binary Latent Variables,[0],[0]
"With the ST gradient estimator, the first loss term in (3) can be backpropagated into the encoder network to fine-tune the hash function gφ(x).
",3.2 Training with Binary Latent Variables,[0],[0]
"For the approximate generator qθ(x|z) in (3), let xi denote the one-hot representation of ith word within a document.",3.2 Training with Binary Latent Variables,[0],[0]
Note that x = ∑,3.2 Training with Binary Latent Variables,[0],[0]
"i xi is thus the bag-of-words representation for document x. To reconstruct the input x from z, we utilize a softmax decoding function written as:
q(xi = w|z) =",3.2 Training with Binary Latent Variables,[0],[0]
exp(zTExw +,3.2 Training with Binary Latent Variables,[0],[0]
"bw)∑|V | j=1 exp(z TExj + bj) , (6)
where q(xi = w|z) is the probability that xi is word w ∈ V , qθ(x|z) = ∏",3.2 Training with Binary Latent Variables,[0],[0]
"i q(xi = w|z) and θ = {E, b1, . . .",3.2 Training with Binary Latent Variables,[0],[0]
", b|V |}.",3.2 Training with Binary Latent Variables,[0],[0]
"Note that E ∈ Rd×|V | can be interpreted as a word embedding matrix to be learned, and {bi}|V |i=1 denote bias terms.",3.2 Training with Binary Latent Variables,[0],[0]
"Intuitively, the objective in (6) encourages the discrete vector z to be close to the embeddings for every word
that appear in the input document x.",3.2 Training with Binary Latent Variables,[0],[0]
"As shown in Section 5.3.1, meaningful semantic structures can be learned and manifested in the word embedding matrix E.",3.2 Training with Binary Latent Variables,[0],[0]
"To reconstruct text data x from sampled binary representation z, a deterministic decoder is typically utilized (Miao et al., 2016; Chaidaroon and Fang, 2017).",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Inspired by the success of employing stochastic decoders in image hashing applications (Dai et al., 2017; Theis et al., 2017), in our experiments, we found that injecting random Gaussian noise into z makes the decoder a more favorable regularizer for the binary codes, which in practice leads to stronger retrieval performance.",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Below, we invoke the rate-distortion theory to perform some further analysis, which leads to interesting findings.
",3.3 Injecting Data-dependent Noise to z,[0],[0]
Learning binary latent codes z to represent a continuous distribution p(x) is a classical information theory concept known as lossy source coding.,3.3 Injecting Data-dependent Noise to z,[0],[0]
"From this perspective, semantic hashing, which compresses an input document into compact binary codes, can be casted as a conventional ratedistortion tradeoff problem (Theis et al., 2017; Ballé et al., 2016):
min − log2R(z)︸ ︷︷ ︸",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Rate +β ·D(x, x̂)︸ ︷︷ ︸ Distortion , (7)
where rate and distortion denote the effective code length, i.e., the number of bits used, and the distortion introduced by the encoding/decoding sequence, respectively.",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Further, x̂ is the reconstructed input and β is a hyperparameter that controls the tradeoff between the two terms.
",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Considering the case where we have a Bernoulli prior on z as p(z) ∼ Bernoulli(γ), and x conditionally drawn from a Gaussian distribution p(x|z) ∼ N (Ez, σ2I).",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Here, E = {ei}|V |i=1, where ei ∈ Rd can be interpreted as a codebook with |V | codewords.",3.3 Injecting Data-dependent Noise to z,[0],[0]
"In our case, E corresponds to the word embedding matrix as in (6).
",3.3 Injecting Data-dependent Noise to z,[0],[0]
"For the case of stochastic latent variable z, the objective function in (3) can be written in a form similar to the rate-distortion tradeoff:
minEqφ(z|x) − log qφ(z|x)︸ ︷︷ ︸",3.3 Injecting Data-dependent Noise to z,[0],[0]
Rate + 1 2σ2︸︷︷︸ β ||x− Ez||22︸ ︷︷ ︸,3.3 Injecting Data-dependent Noise to z,[0],[0]
"Distortion +C  , (8)
where C is a constant that encapsulates the prior distribution p(z) and the Gaussian distribution normalization term.",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Notably, the trade-off hyperparameter β = σ−2/2 is closely related to the variance of the distribution p(x|z).",3.3 Injecting Data-dependent Noise to z,[0],[0]
"In other words, by controlling the variance σ, the model can adaptively explore different trade-offs between the rate and distortion objectives.",3.3 Injecting Data-dependent Noise to z,[0],[0]
"However, the optimal trade-offs for distinct samples may be different.
",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Inspired by the observations above, we propose to inject data-dependent noise into latent variable z, rather than to setting the variance term σ2 to a fixed value (Dai et al., 2017; Theis et al., 2017).",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Specifically, log σ2 is obtained via a one-layer MLP transformation from gφ(x).",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Afterwards, we sample z′ fromN (z, σ2I), which then replace z in (6) to infer the probability of generating individual words (as shown in Figure 1).",3.3 Injecting Data-dependent Noise to z,[0],[0]
"As a result, the variances are different for every input document x, and thus the model is provided with additional flexibility to explore various trade-offs between rate and distortion for different training observations.",3.3 Injecting Data-dependent Noise to z,[0],[0]
"Although our decoder is not a strictly Gaussian distribution, as in (6), we found empirically that injecting data-dependent noise into z yields strong retrieval results, see Section 5.1.",3.3 Injecting Data-dependent Noise to z,[0],[0]
"The proposed Neural Architecture for Semantic Hashing (NASH) can be extended to supervised hashing, where a mapping from latent variable z to labels y is learned, here parametrized by a twolayer MLP followed by a fully-connected softmax layer.",3.4 Supervised Hashing,[0],[0]
"To allow the model to explore and balance between maximizing the variational lower bound in (3) and minimizing the discriminative loss, the following joint training objective is employed:
L = −Lvae(θ, φ;x) + αLdis(η; z, y).",3.4 Supervised Hashing,[0],[0]
"(9)
where η refers to parameters of the MLP classifier and α controls the relative weight between the variational lower bound (Lvae) and discriminative loss (Ldis), defined as the cross-entropy loss.",3.4 Supervised Hashing,[0],[0]
"The parameters {θ, φ, η} are learned end-to-end via Monte Carlo estimation.",3.4 Supervised Hashing,[0],[0]
"We use the following three standard publicly available datasets for training and evaluation:
(i) Reuters21578, containing 10,788 news documents, which have been classified into 90 different categories.",4.1 Datasets,[0],[0]
"(ii) 20Newsgroups, a collection of 18,828 newsgroup documents, which are categorized into 20 different topics.",4.1 Datasets,[0],[0]
"(iii) TMC (stands for SIAM text mining competition), containing air traffic reports provided by NASA.",4.1 Datasets,[0],[0]
"TMC consists 21,519 training documents divided into 22 different categories.",4.1 Datasets,[0],[0]
"To make direct comparison with prior works, we employed the TFIDF features on these datasets supplied by (Chaidaroon and Fang, 2017), where the vocabulary sizes for the three datasets are set to 10,000, 7,164 and 20,000, respectively.",4.1 Datasets,[0],[0]
"For the inference networks, we employ a feedforward neural network with 2 hidden layers (both with 500 units) using the ReLU non-linearity activation function, which transform the input documents, i.e., TFIDF features in our experiments, into a continuous representation.",4.2 Training Details,[0],[0]
"Empirically, we found that stochastic binarization as in (2) shows stronger performance than deterministic binarization, and thus use the former in our experiments.",4.2 Training Details,[0],[0]
"However, we further conduct a systematic ablation study in Section 5.2 to compare the two binarization strategies.
",4.2 Training Details,[0],[0]
"Our model is trained using Adam (Kingma and Ba, 2014), with a learning rate of 1× 10−3 for all parameters.",4.2 Training Details,[0],[0]
"We decay the learning rate by a factor of 0.96 for every 10,000 iterations.",4.2 Training Details,[0],[0]
"Dropout (Srivastava et al., 2014) is employed on the output of encoder networks, with the rate selected from {0.7, 0.8, 0.9} on the validation set.",4.2 Training Details,[0],[0]
"To facilitate comparisons with previous methods, we set the dimension of z, i.e., the number of bits within the hashing code) as 8, 16, 32, 64, or 128.",4.2 Training Details,[0],[0]
We evaluate the effectiveness of our framework on both unsupervised and supervised semantic hashing tasks.,4.3 Baselines,[0],[0]
"We consider the following unsupervised baselines for comparisons: Locality Sensitive Hashing (LSH) (Datar et al., 2004),",4.3 Baselines,[0],[0]
"Stack Restricted Boltzmann Machines (S-RBM) (Salakhutdinov and Hinton, 2009), Spectral Hashing (SpH) (Weiss et al., 2009), Self-taught Hashing (STH) (Zhang et al., 2010) and Variational Deep Semantic Hashing (VDSH) (Chaidaroon and Fang, 2017).
",4.3 Baselines,[0],[0]
"For supervised semantic hashing, we also compare NASH against a number of baselines: Supervised Hashing with Kernels (KSH) (Liu et al., 2012), Semantic Hashing using Tags and Topic Modeling (SHTTM) (Wang et al., 2013) and Supervised VDSH (Chaidaroon and Fang, 2017).",4.3 Baselines,[0],[0]
"It is worth noting that unlike all these baselines, our NASH model is trained end-to-end in one-step.",4.3 Baselines,[0],[0]
"To evaluate the hashing codes for similarity search, we consider each document in the testing set as a query document.",4.4 Evaluation Metrics,[0],[0]
"Similar documents to the query in the corresponding training set need to be retrieved based on the Hamming distance of their hashing codes, i.e. number of different bits.",4.4 Evaluation Metrics,[0],[0]
"To facilitate comparison with prior work (Wang et al., 2013; Chaidaroon and Fang, 2017), the performance is measured with precision.",4.4 Evaluation Metrics,[0],[0]
"Specifically, during testing, for a query document, we first retrieve the 100 nearest/closest documents according to the Hamming distances of the corresponding hash codes (i.e., the number of different bits).",4.4 Evaluation Metrics,[0],[0]
We then examine the percentage of documents among these 100 retrieved ones that belong to the same label (topic) with the query document (we consider documents having the same label as relevant pairs).,4.4 Evaluation Metrics,[0],[0]
The ratio of the number of relevant documents to the number of retrieved documents (fixed value of 100) is calculated as the precision score.,4.4 Evaluation Metrics,[0],[0]
The precision scores are further averaged over all test (query) documents.,4.4 Evaluation Metrics,[0],[0]
We experimented with four variants for our NASH model: (i) NASH: with deterministic decoder; (ii) NASH-N: with fixed random noise injected to decoder; (iii) NASH-DN: with data-dependent noise injected to decoder; (iv) NASH-DN-S: NASH-DN with supervised information during training.,5 Experimental Results,[0],[0]
Table 1 presents the results of all models on Reuters dataset.,5.1 Semantic Hashing Evaluation,[0],[0]
"Regarding unsupervised semantic hashing, all the NASH variants consistently outperform the baseline methods by a substantial margin, indicating that our model makes the most effective use of unlabeled data and manage to assign similar hashing codes, i.e., with small Hamming distance to each other, to documents that belong to the same label.",5.1 Semantic Hashing Evaluation,[0],[0]
"It can be also observed that the injection of noise into the decoder networks has improved the robustness of learned binary representations, resulting in better retrieval performance.",5.1 Semantic Hashing Evaluation,[0],[0]
"More importantly, by making the variances of noise adaptive to the specific input, our NASH-DN achieves even better results, compared with NASH-N, highlighting the importance of exploring/learning the trade-off between rate and distortion objectives by the data itself.",5.1 Semantic Hashing Evaluation,[0],[0]
"We observe the same trend and superiority of our NASH-DN models on the other two benchmarks, as shown in Tables 3 and 4.
",5.1 Semantic Hashing Evaluation,[0],[0]
"Another observation is that the retrieval results tend to drop a bit when we set the length of hashing codes to be 64 or larger, which also happens for some baseline models.",5.1 Semantic Hashing Evaluation,[0],[0]
"This phenomenon has been reported previously in Wang et al. (2012); Liu et al. (2012); Wang et al. (2013); Chaidaroon and Fang (2017), and the reasons could be twofold: (i) for longer codes, the number of data points that are assigned to a certain binary code decreases exponentially.",5.1 Semantic Hashing Evaluation,[0],[0]
"As a result, many queries may fail to return any neighbor documents (Wang et al., 2012); (ii) considering the size of training data, it is likely that the model may overfit with long hash codes (Chaidaroon and Fang, 2017).",5.1 Semantic Hashing Evaluation,[0],[0]
"However, even with longer hashing codes,
our NASH models perform stronger than the baselines in most cases (except for the 20Newsgroups dataset), suggesting that NASH can effectively allocate documents to informative/meaningful hashing codes even with limited training data.
",5.1 Semantic Hashing Evaluation,[0],[0]
"We also evaluate the effectiveness of NASH in a supervised scenario on the Reuters dataset,
where the label or topic information is utilized during training.",5.1 Semantic Hashing Evaluation,[0],[0]
"As shown in Figure 2, our NASHDN-S model consistently outperforms several supervised semantic hashing baselines, with various choices of hashing bits.",5.1 Semantic Hashing Evaluation,[0],[0]
"Notably, our model exhibits higher Top-100 retrieval precision than VDSH-S and VDSH-SP, proposed by Chaidaroon and Fang (2017).",5.1 Semantic Hashing Evaluation,[0],[0]
"This may be attributed to the fact that in VDSH models, the continuous embeddings are not optimized with their future binarization in mind, and thus could hurt the relevance of learned binary codes.",5.1 Semantic Hashing Evaluation,[0],[0]
"On the contrary, our model is optimized in an end-to-end manner, where the gradients are directly backpropagated to the inference network (through the binary/discrete latent variable), and thus gives rise to a more robust hash function.",5.1 Semantic Hashing Evaluation,[0],[0]
"As described in Section 3, the binary latent variables z in NASH can be either deterministically (via (1)) or stochastically (via (2)) sampled.",5.2.1 The effect of stochastic sampling,[0],[0]
We compare these two types of binarization functions in the case of unsupervised hashing.,5.2.1 The effect of stochastic sampling,[0],[0]
"As illustrated in Figure 3, stochastic sampling shows stronger retrieval results on all three datasets, indicating that endowing the sampling process of latent variables with more stochasticity improves the learned representations.",5.2.1 The effect of stochastic sampling,[0],[0]
"Under the variational framework introduced here, the encoder network, i.e., hash function, and decoder network are jointly optimized to abstract semantic features from documents.",5.2.2 The effect of encoder/decoder networks,[0],[0]
An interesting question concerns what types of network should be leveraged for each part of our NASH model.,5.2.2 The effect of encoder/decoder networks,[0],[0]
"In this regard, we further investigate the effect of
using an encoder or decoder with different nonlinearity, ranging from a linear transformation to two-layer MLPs.",5.2.2 The effect of encoder/decoder networks,[0],[0]
"We employ a base model with an encoder of two-layer MLPs and a linear decoder (the setup described in Section 3), and the ablation study results are shown in Table 6.
",5.2.2 The effect of encoder/decoder networks,[0],[0]
"It is observed that for the encoder networks, increasing the non-linearity by stacking MLP layers leads to better empirical results.",5.2.2 The effect of encoder/decoder networks,[0],[0]
"In other words, endowing the hash function with more modeling capacity is advantageous to retrieval tasks.",5.2.2 The effect of encoder/decoder networks,[0],[0]
"However, when we employ a non-linear network for the decoder, the retrieval precision drops dramatically.",5.2.2 The effect of encoder/decoder networks,[0],[0]
"It is worth noting that the only difference between linear transformation and one-layer MLP is whether a non-linear activation function is employed or not.
",5.2.2 The effect of encoder/decoder networks,[0],[0]
"This observation may be attributed the fact that the decoder networks can be considered as a sim-
ilarity measure between latent variable z and the word embeddings Ek for every word, and the probabilities for words that present in the document is maximized to ensure that z is informative.",5.2.2 The effect of encoder/decoder networks,[0],[0]
"As a result, if we allow the decoder to be too expressive (e.g., a one-layer MLP), it is likely that we will end up with a very flexible similarity measure but relatively less meaningful binary representations.",5.2.2 The effect of encoder/decoder networks,[0],[0]
"This finding is consistent with several image hashing methods, such as SGH (Dai et al., 2017) or binary autoencoder (Carreira-Perpinán and Raziperchikolaei, 2015), where a linear decoder is typically adopted to obtain promising retrieval results.",5.2.2 The effect of encoder/decoder networks,[0],[0]
"However, our experiments may not speak for other choices of encoder-decoder architectures, e.g., LSTM-based sequence-to-sequence models (Sutskever et al., 2014) or DCNN-based autoencoder (Zhang et al., 2017).",5.2.2 The effect of encoder/decoder networks,[0],[0]
"To understand what information has been learned in our NASH model, we examine the matrix E ∈ Rd×l in (6).",5.3.1 Analysis of Semantic Information,[0],[0]
"Similar to (Miao et al., 2016; Larochelle and Lauly, 2012), we select the 5 nearest words according to the word vectors learned from NASH and compare with the corresponding results from NVDM.
",5.3.1 Analysis of Semantic Information,[0],[0]
"As shown in Table 2, although our NASH model contains a binary latent variable, rather than a continuous one as in NVDM, it also effectively group semantically-similar words together in the learned vector space.",5.3.1 Analysis of Semantic Information,[0],[0]
This further demonstrates that the proposed generative framework manages to bypass the binary/discrete constraint and is able to abstract useful semantic information from documents.,5.3.1 Analysis of Semantic Information,[0],[0]
"In Table 5, we show some examples of the learned binary hashing codes on 20Newsgroups
dataset.",5.3.2 Case Study,[0],[0]
"We observe that for both 8-bit and 16- bit cases, NASH typically compresses documents with shared topics into very similar binary codes.",5.3.2 Case Study,[0],[0]
"On the contrary, the hashing codes for documents with different topics exhibit much larger Hamming distance.",5.3.2 Case Study,[0],[0]
"As a result, relevant documents can be efficiently retrieved by simply computing their Hamming distances.",5.3.2 Case Study,[0],[0]
"This paper presents a first step towards end-to-end semantic hashing, where the binary/discrete constraints are carefully handled with an effective gradient estimator.",6 Conclusions,[0],[0]
A neural variational framework is introduced to train our model.,6 Conclusions,[0],[0]
"Motivated by the connections between the proposed method and rate-distortion theory, we inject data-dependent noise into the Bernoulli latent variable at the training stage.",6 Conclusions,[0],[0]
"The effectiveness of our framework is demonstrated with extensive experiments.
",6 Conclusions,[0],[0]
Acknowledgments We would like to thank the ACL reviewers for their insightful suggestions.,6 Conclusions,[0],[0]
"This research was supported in part by DARPA, DOE, NIH, NSF and ONR.",6 Conclusions,[0],[0]
Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems.,abstractText,[0],[0]
"While fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc.",abstractText,[0],[0]
"In this paper, we present an end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli latent variables.",abstractText,[0],[0]
"A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent variable to optimize the hash function.",abstractText,[0],[0]
"We also draw connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of the proposed framework.",abstractText,[0],[0]
Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsupervised and supervised scenarios.,abstractText,[0],[0]
NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing,title,[0],[0]
"We study the problem of composite non-convex minimization:
min x∈Rd
{ F (x) := ψ(x) + f(x) := ψ(x) + 1
n n∑ i=1",1 Introduction,[0],[0]
"fi(x) }
(1.1) where each fi(x) is nonconvex but smooth, and ψ(·) is proper convex, possibly nonsmooth, but relatively simple.",1 Introduction,[0],[0]
"We are interested in finding a point x that is an approximate local minimum of F (x).
",1 Introduction,[0],[0]
• The finite-sum structure f(x) =,1 Introduction,[0],[0]
1n,1 Introduction,[0],[0]
∑n i=1,1 Introduction,[0],[0]
"fi(x) arises
prominently in large-scale machine learning tasks.",1 Introduction,[0],[0]
"In particular, when minimizing loss over a training set, each example i corresponds to one loss function fi(·) in the summation.",1 Introduction,[0],[0]
"This finite-sum structure allows one to perform stochastic gradient descent with respect to a
Future version of this paper shall be found at http:// arxiv.org/abs/1702.00763.",1 Introduction,[0],[0]
1Microsoft Research.,1 Introduction,[0],[0]
"Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
random∇fi(x).",1 Introduction,[0],[0]
"• The so-called proximal term ψ(x) adds more general-
ity to the model.",1 Introduction,[0],[0]
"For instance, if ψ(x) is the indicator function of a convex set, then problem (1.1) becomes constraint minimization; if ψ(x) =",1 Introduction,[0],[0]
"‖x‖1, then we can allow problem (1.1) to perform feature selection.",1 Introduction,[0],[0]
"In general, ψ(x) has to be a simple function where the projection operation arg minx{ψ(x) + 12η‖x − x0‖
2} is efficiently computable.",1 Introduction,[0],[0]
"At a first reading of this paper, one can assume ψ(x) ≡ 0 for simplicity.
",1 Introduction,[0],[0]
Many non-convex machine learning problems fall into problem (1.1).,1 Introduction,[0],[0]
"Most notably, training deep neural networks and classifications with sigmoid loss correspond to (1.1) where neither fi(x) or f(x) is convex.",1 Introduction,[0],[0]
"However, our understanding to this challenging non-convex problem is very limited.",1 Introduction,[0],[0]
"LetL be the smoothness parameter for each fi(x), meaning all the eigenvalues of∇2fi(x) lie in [−L,L].1
We denote by σ ∈",1.1 Strongly Non-Convex Optimization,[0],[0]
"[0, L] the strong-nonconvexity parameter of f(x) = 1n",1.1 Strongly Non-Convex Optimization,[0],[0]
∑n i=1,1.1 Strongly Non-Convex Optimization,[0],[0]
"fi(x), meaning that
all the eigenvalues of∇2f(x) lie in [−σ, L].
",1.1 Strongly Non-Convex Optimization,[0],[0]
"We emphasize that parameter σ is analogous to the strongconvexity parameter µ for convex optimization, where all the eigenvalues of ∇2f(x) lie in [µ,L] for some µ > 0.",1.1 Strongly Non-Convex Optimization,[0],[0]
"We wish to find an ε-approximate stationary point (a.k.a. critical point) of F (x), that is
a point x satisfying ‖G(x)‖ ≤ ε
where G(x) is the so-called gradient mapping of F (x) (see Section 2 for a formal definition).",1.1 Strongly Non-Convex Optimization,[0],[0]
"In the special case of ψ(·) ≡ 0, gradient mapping G(x) is the same as gradient ∇f(x), so x satisfies ‖∇f(x)‖ ≤ ε.",1.1 Strongly Non-Convex Optimization,[0],[0]
"Since f(·) is σ-strongly nonconvex, any ε-approximate stationary point is automatically also an (ε, σ)-approximate local minimum — meaning that the Hessian of the output point ∇2f(x) −σI is approximately positive semidefinite (PSD).
",1.1 Strongly Non-Convex Optimization,[0],[0]
"1This definition also applies to functions f(x) that are not twice differentiable, see Section 2 for details.",1.1 Strongly Non-Convex Optimization,[0],[0]
• We focus on strongly non-convex optimization because introducing this parameter σ allows us to perform a more refined study of non-convex optimization.,1.2 Motivations and Remarks,[0],[0]
"If σ equals L then L-strongly nonconvex optimization is equivalent to the general non-convex optimization.
",1.2 Motivations and Remarks,[0],[0]
"• We focus only on finding stationary points as opposed to local minima, because in a recent study — see Appendix A— researchers have shown that finding (ε, δ)-approximate local minima reduces to finding εapproximate stationary points in an O(δ)-strongly nonconvex function.
",1.2 Motivations and Remarks,[0],[0]
"• Parameter σ is often not constant and can be much smaller than L. For instance, second-order methods often find (ε, √ ε)-approximate local minima (Nesterov,
2008) and this corresponds to σ = √ ε.",1.2 Motivations and Remarks,[0],[0]
"Despite the widespread use of nonconvex models in machine learning and related fields, our understanding to nonconvex optimization is still very limited.",1.3 Known Results,[0],[0]
"Until recently, nearly all research papers have been mostly focusing on either σ = 0",1.3 Known Results,[0],[0]
"or σ = L:
•",1.3 Known Results,[0],[0]
"If σ = 0, the accelerated SVRG method (ShalevShwartz, 2016; Allen-Zhu & Yuan, 2016) finds x satisfying F (x)",1.3 Known Results,[0],[0]
"− F (x∗) ≤ ε, in gradient complexity Õ",1.3 Known Results,[0],[0]
( n + n3/4,1.3 Known Results,[0],[0]
√ L/ε ),1.3 Known Results,[0],[0]
.2,1.3 Known Results,[0],[0]
"This result is irrelevant to this
paper because f(x) is simply convex.
",1.3 Known Results,[0],[0]
•,1.3 Known Results,[0],[0]
"If σ = L, the SVRG method (Allen-Zhu & Hazan, 2016) finds an ε-approximate stationary point of F (x) in gradient complexity O(n+ n2/3L/ε2).
",1.3 Known Results,[0],[0]
•,1.3 Known Results,[0],[0]
"If σ = L, gradient descent finds an ε-approximate stationary point in gradient complexity O(nL/ε2).
",1.3 Known Results,[0],[0]
•,1.3 Known Results,[0],[0]
"If σ = L, stochastic gradient descent finds an ε-approx.",1.3 Known Results,[0],[0]
"stationary point in gradient complexity O(L2/ε4).
",1.3 Known Results,[0],[0]
"Throughout this paper, we refer to gradient complexity as the total number of stochastic gradient computations ∇fi(x) and proximal computations y ← Proxψ,η(x) := arg miny{ψ(y) + 12η‖y",1.3 Known Results,[0],[0]
− x‖ 2}.3,1.3 Known Results,[0],[0]
"Very recently, it was observed by two independent groups (Agarwal et al., 2017; Carmon et al., 2016) — although implicitly, see Section 2.1— that for solving the σ-strongly nonconvex problem, one can repeatedly regularize F (x) to make it σ-strongly convex, and then apply the accelerated SVRG method to minimize this regularized
2We use Õ to hide poly-logarithmic factors in n,L, 1/ε.",1.3 Known Results,[0],[0]
3Some authors also refer to them as incremental first-order oracle (IFO) and proximal oracle (PO) calls.,1.3 Known Results,[0],[0]
"In most machine learning applications, each IFO and PO call can be implemented to run in time O(d) where d is the dimension of the model, or even in time O(s) if s is the average sparsity of the data vectors.
function.",1.3 Known Results,[0],[0]
"Under mild assumption σ ≥ ε2, this approach • finds an ε-approximate stationary point in gradient
complexity Õ",1.3 Known Results,[0],[0]
"( nσ+n3/4 √ Lσ
ε2
) .
",1.3 Known Results,[0],[0]
We call this method repeatSVRG in this paper.,1.3 Known Results,[0],[0]
"Unfortunately, repeatSVRG is even slower than the vanilla SVRG for σ = L by a factor n1/3, see Figure 1.
Remark on SGD.",1.3 Known Results,[0],[0]
"Stochastic gradient descent (SGD) has a slower convergence rate (i.e., in terms of 1/ε4) than other cited first-order methods (i.e., in terms of 1/ε2), see for instance (Ghadimi & Lan, 2015).",1.3 Known Results,[0],[0]
"However, the complexity of SGD does not depend on n and thus is incomparable to gradient descent, SVRG, or repeatSVRG.4",1.3 Known Results,[0],[0]
"This is one of the main motivations to study how to reduce the complexity of non-SGD methods, especially in terms of n.",1.3 Known Results,[0],[0]
"In this paper, we identify an interesting dichotomy with respect to the spectrum of the nonconvexity parameter σ ∈",1.4 Our New Results,[0],[0]
"[0, L].",1.4 Our New Results,[0],[0]
"In particular, we showed that if σ ≥ L/",1.4 Our New Results,[0],[0]
"√ n, then our new method Natasha finds an ε-approximate stationary point of F (x) in gradient complexity
O ( n log 1
ε + n2/3(L2σ)1/3",1.4 Our New Results,[0],[0]
"ε2
) .
",1.4 Our New Results,[0],[0]
"In other words, together with repeatSVRG, we have improved the gradient complexity for σ-stringly nonconvex optimization to5
Õ ( min {n3/4√Lσ
ε2 , n2/3(L2σ)1/3 ε2 }) and the first term in the min is smaller if σ < L/",1.4 Our New Results,[0],[0]
√ n and the second term is smaller if σ > L/,1.4 Our New Results,[0],[0]
√ n.,1.4 Our New Results,[0],[0]
"We illustrate our
4In practice, there are examples in non-convex empirical risk minimization (Allen-Zhu & Hazan, 2016) and in training neural networks (Allen-Zhu & Hazan, 2016; Reddi et al., 2016) where SVRG can outperform SGD.",1.4 Our New Results,[0],[0]
"Of course, for deep learning tasks, SGD remains to be the best practical method of choice.
",1.4 Our New Results,[0],[0]
5We remark here that this is under mild assumptions for ε being sufficiently small.,1.4 Our New Results,[0],[0]
"For instance, the result of (Agarwal et al., 2017; Carmon et al., 2016) requires ε2 ≤ σ.",1.4 Our New Results,[0],[0]
"In our result, the term n log 1
ε disappears when ε6 ≤ L2σ/n.
performance improvement in Figure 1.",1.4 Our New Results,[0],[0]
"Our result matches that of SVRG for σ = L, and has a much simpler analysis.
",1.4 Our New Results,[0],[0]
Additional Results.,1.4 Our New Results,[0],[0]
"One can take a step further and ask what if each function fi(x) is (`1, `2)-smooth for parameters `1, `2 ≥ σ, meaning that all the eigenvalues of∇2fi(x) lie in [−`2, `1].",1.4 Our New Results,[0],[0]
"We show that a variant of our method, which we call Natashafull, solves this more refined problem of (1.1) with total gradient complexity",1.4 Our New Results,[0],[0]
O ( n log 1ε + n2/3(`1`2σ) 1/3 ε2 ) as long as `1`2σ2 ≤ n 2.,1.4 Our New Results,[0],[0]
Remark 1.1.,1.4 Our New Results,[0],[0]
"In applications, `1 and `2 can be of very different magnitudes.",1.4 Our New Results,[0],[0]
The most influential example is finding the leading eigenvector of a symmetric matrix.,1.4 Our New Results,[0],[0]
"Using the so-called shift-and-invert reduction (Garber et al., 2016), computing the leading eigenvector reduces to the convex version of problem (1.1), where each fi(x) is (λ, 1)- smooth for λ 1.",1.4 Our New Results,[0],[0]
"Other examples include all the applications that are built on shift-and-invert, including high rank SVD/PCA (Allen-Zhu & Li, 2016), canonical component analysis (Allen-Zhu & Li, 2017a), online matrix learning (Allen-Zhu & Li, 2017b), and approximate local minima algorithms (Agarwal et al., 2017; Carmon et al., 2016).
",1.4 Our New Results,[0],[0]
Mini-Batch.,1.4 Our New Results,[0],[0]
"Our result generalizes trivially to the minibatch stochastic setting, where in each iteration one computes∇fi(x) for b random choices of index i ∈",1.4 Our New Results,[0],[0]
[n] and average them.,1.4 Our New Results,[0],[0]
The stated gradient complexities of Natasha and Natashafull can be adjusted so that the factor n2/3 is replaced with n2/3b1/3.,1.4 Our New Results,[0],[0]
"Let us first recall the main idea behind stochastic variancereduced methods, such as SVRG (Johnson & Zhang, 2013).
",1.5 Our Techniques,[0],[0]
"The SVRG method divides iterations into epochs, each of length n.",1.5 Our Techniques,[0],[0]
"It maintains a snapshot point x̃ for each epoch, and computes the full gradient ∇f(x̃) only for snapshots.",1.5 Our Techniques,[0],[0]
"Then, in each iteration t at point xt, SVRG defines gradient estimator ∇̃ = ∇fi(xt)−∇fi(x̃)+∇f(x̃) which satisfies Ei[∇̃] = ∇f(xt), and performs proximal update xt+1 ← Proxψ,α ( xt − α∇̃ ) for some learning rate α.",1.5 Our Techniques,[0],[0]
(Recall that if ψ(·) ≡ 0,1.5 Our Techniques,[0],[0]
then we would have xt+1,1.5 Our Techniques,[0],[0]
← xt,1.5 Our Techniques,[0],[0]
− α∇̃.),1.5 Our Techniques,[0],[0]
"In nearly all the aforementioned results for nonconvex optimization, researchers have either directly applied SVRG (Allen-Zhu & Hazan, 2016) (for the case σ = L), or repeatedly applied SVRG (Agarwal et al., 2017; Carmon et al., 2016) (for general σ ∈",1.5 Our Techniques,[0],[0]
"[0, L]).",1.5 Our Techniques,[0],[0]
"This puts some limitation in the algorithmic design, because SVRG requires each epoch to be of length exactly n.6
6The epoch length of SVRG is always n (or a constant multiple of n in practice), because this ensures the computation of ∇̃ is of amortized gradient complexity O(1).",1.5 Our Techniques,[0],[0]
"The per-iteration complexity of SVRG is thus the same as the traditional stochastic
Our New Idea.",1.5 Our Techniques,[0],[0]
"In this paper, we propose Natasha and Natashafull, two methods that are no longer black-box reductions to SVRG.",1.5 Our Techniques,[0],[0]
"Both of them still divide iterations into epochs of length n, and compute gradient estimators ∇̃",1.5 Our Techniques,[0],[0]
the same way as SVRG.,1.5 Our Techniques,[0],[0]
"However, we do not apply compute xt",1.5 Our Techniques,[0],[0]
− α∇̃ directly.,1.5 Our Techniques,[0],[0]
•,1.5 Our Techniques,[0],[0]
"In our base algorithm Natasha, we divide each epoch
into p sub-epochs, each with a starting vector x̂.",1.5 Our Techniques,[0],[0]
Our theory suggests the choice p,1.5 Our Techniques,[0],[0]
"≈ ( σ 2
L2n) 1/3.",1.5 Our Techniques,[0],[0]
"Then, we
replace the use of ∇̃ with ∇̃ + 2σ(xt − x̂).",1.5 Our Techniques,[0],[0]
"This is equivalent to replacing f(x) with its regularized version f(x)+σ‖x− x̂‖2, where the center x̂ varies across subepochs.",1.5 Our Techniques,[0],[0]
"We provide pseudocode in Algorithm 1 and illustrate it in Figure 2.
",1.5 Our Techniques,[0],[0]
"We view this additional term 2σ(xt − x̂) as a type of retraction, which stabilizes the algorithm by moving the vector a bit in the backward direction towards x̂.
•",1.5 Our Techniques,[0],[0]
"In our full algorithm Natashafull, we add one more ingredient on top of Natasha.",1.5 Our Techniques,[0],[0]
"That is, we perform updates zt+1 ← Proxψ,α(zt − α∇̃) with respect to a different sequence {zt}, and then define xt = 12zt",1.5 Our Techniques,[0],[0]
"+ 1 2 x̂
and compute gradient estimators ∇̃ at points xt.",1.5 Our Techniques,[0],[0]
"We provide pseudocode in Algorithm 2 in the appendix.
",1.5 Our Techniques,[0],[0]
We view this averaging xt = 12zt,1.5 Our Techniques,[0],[0]
+ 1 2 x̂,1.5 Our Techniques,[0],[0]
"as another type of retraction, which stabilizes the algorithm by moving towards x̂. The technique of computing gradients at points xt but moving a different sequence of points zt is related to the Katyusha momentum recently developed for convex optimization (Allen-Zhu, 2017).",1.5 Our Techniques,[0],[0]
Methods based on variance-reduced stochastic gradients were first introduced for convex optimization.,1.6 Other Related Work,[0],[0]
"The first such method is SAG by Schmidt et al (Schmidt et al., 2013).",1.6 Other Related Work,[0],[0]
"The two most popular choices for gradient estimators are the SVRG-like one we adopted in this paper (independently introduced by (Johnson & Zhang, 2013; Zhang et al., 2013), and the SAGA-like one introduced by (Defazio et al., 2014).",1.6 Other Related Work,[0],[0]
"In nearly all applications, the results proven for SVRG-like estimators and SAGA-like estimators are simply exchangeable (therefore, the results of this paper naturally generalize to SAGA-like estimators).
",1.6 Other Related Work,[0],[0]
"The first “non-convex use” of variance reduction is by Shalev-Shwartz (Shalev-Shwartz, 2016) who assumes that each fi(x) is non-convex but their average f(x) is still convex.",1.6 Other Related Work,[0],[0]
"This result has been slightly improved to several more refined settings (Allen-Zhu & Yuan, 2016).",1.6 Other Related Work,[0],[0]
"The first truly non-convex use of variance reduction (i.e., for f(x) being also non-convex) is independently by (Allen-Zhu & Hazan, 2016) and (Reddi et al., 2016).",1.6 Other Related Work,[0],[0]
"First-order methods only
gradient descent (SGD).
find stationary points (unless there is extra assumption on the randomness of the data), and converge no faster than 1/ε2.
",1.6 Other Related Work,[0],[0]
"When the second-order Hessian information is used, one can (1) find local minima instead of stationary points, and (2) improve the 1/ε2 rate to 1/ε1.5.",1.6 Other Related Work,[0],[0]
"The first such result is by cubic-regularized Newton’s method (Nesterov, 2008); however, its per-iteration complexity is very high.",1.6 Other Related Work,[0],[0]
"Very recently, two independent groups of authors tackled this problem from a somewhat similar viewpoint (Carmon et al., 2016; Agarwal et al., 2017): if the computation of Hessian-vector multiplications (i.e., ( ∇2fi(x) ) v) is on the same order of the computation of gradients ∇fi(x),7 then one can obtain a (ε, √ ε)-approximate local minimum in
gradient complexity Õ ( n ε1.5 + n3/4 ε1.75 ) , if we use big-O to also hide dependencies on the smoothness parameters.
",1.6 Other Related Work,[0],[0]
"Other related papers include Ge et al. (Ge et al., 2015) where the authors showed that a noise-injected version of SGD converges to local minima instead of critical points, as long as the underlying function is “strict-saddle.”",1.6 Other Related Work,[0],[0]
Their theoretical running time is a large polynomial in the dimension.,1.6 Other Related Work,[0],[0]
"Lee et al. (Lee et al., 2016) showed that gradient descent, starting from a random point, almost surely converges to a local minimum if the function is “strict-saddle”.",1.6 Other Related Work,[0],[0]
The rate of convergence required is somewhat unknown.,1.6 Other Related Work,[0],[0]
"Throughout this paper, we denote by ‖ · ‖ the Euclidean norm.",2 Preliminaries,[0],[0]
We use i ∈R,2 Preliminaries,[0],[0]
"[n] to denote that i is generated from [n] = {1, 2, . . .",2 Preliminaries,[0],[0]
", n}",2 Preliminaries,[0],[0]
uniformly at random.,2 Preliminaries,[0],[0]
"We denote by ∇f(x) the full gradient of function f if it is differentiable, and ∂f(x) any subgradient if f is only Lipschitz continuous at point x.",2 Preliminaries,[0],[0]
"We let x∗ be any minimizer of F (x).
",2 Preliminaries,[0],[0]
"Recall some definitions on strong convexity (SC), strongly nonconvexity, and smoothness.
",2 Preliminaries,[0],[0]
Definition 2.1.,2 Preliminaries,[0],[0]
"For a function f : Rd → R, 7A lot of interesting problems satisfy this property, including training neural nets.
",2 Preliminaries,[0],[0]
"• f is σ-strongly convex if ∀x, y ∈ Rd, it satisfies
f(y) ≥ f(x) + 〈∂f(x), y",2 Preliminaries,[0],[0]
"− x〉+ σ 2 ‖x− y‖2 .
",2 Preliminaries,[0],[0]
"• f is σ-strongly nonconvex if ∀x, y ∈ Rd, it satisfies
f(y) ≥ f(x) + 〈∂f(x), y",2 Preliminaries,[0],[0]
"− x〉 − σ 2 ‖x− y‖2 .
",2 Preliminaries,[0],[0]
"• f is (`1, `2)-smooth if ∀x, y ∈ Rd, it satisfies
f(x) +",2 Preliminaries,[0],[0]
"〈∇f(x), y",2 Preliminaries,[0],[0]
"− x〉+ `12 ‖x− y‖ 2 ≥ f(y)
≥ f(x) +",2 Preliminaries,[0],[0]
"〈∇f(x), y − x〉 − `2 2 ‖x− y‖2 .
• f is L-smooth if it is (L,L)-smooth.
",2 Preliminaries,[0],[0]
"The (`1, `2)-smoothness parameters were introduced in (Allen-Zhu & Yuan, 2016) to tackle the convex setting of problem (1.1).",2 Preliminaries,[0],[0]
"The notion of strong nonconvexity is also known as “almost convexity (Carmon et al., 2016)” or “lower smoothness (Allen-Zhu & Yuan, 2016).”",2 Preliminaries,[0],[0]
"We refrain from using the name “almost convexity” because it coincides with several other non-equivalent definitions in optimization literatures.
",2 Preliminaries,[0],[0]
Definition 2.2.,2 Preliminaries,[0],[0]
"Given a parameter η > 0, the gradient mapping of F (·) in (1.1) at point x is
Gη(x) :",2 Preliminaries,[0],[0]
"= 1
η
( x− x′ ) where x′ = arg miny { ψ(y) + 〈∇f(x), y〉+ 12η‖y−x‖ 2 }
.",2 Preliminaries,[0],[0]
"The following theorem for the SVRG method can be found for instance in (Allen-Zhu & Yuan, 2016), which is built on top of the results (Shalev-Shwartz, 2016; Lin et al., 2015; Frostig et al., 2015):
Theorem 2.3 (SVRG).","In particular, if ψ(·) ≡ 0, then Gη(x) ≡ ∇f(x).",[0],[0]
Let G(y) := ψ(y) +,"In particular, if ψ(·) ≡ 0, then Gη(x) ≡ ∇f(x).",[0],[0]
1n,"In particular, if ψ(·) ≡ 0, then Gη(x) ≡ ∇f(x).",[0],[0]
"∑n i=1 gi(y) be σ-strongly convex, then the SVRG method finds a point y satisfying G(y)−G(y∗)","In particular, if ψ(·) ≡ 0, then Gη(x) ≡ ∇f(x).",[0],[0]
≤,"In particular, if ψ(·) ≡ 0, then Gη(x) ≡ ∇f(x).",[0],[0]
"ε
• with gradient complexity O ( (n + L 2
σ2 ) log 1 ε
) , if each
gi(·) is L-smooth (for L ≥ σ); or
• with gradient complexity","In particular, if ψ(·) ≡ 0, then Gη(x) ≡ ∇f(x).",[0],[0]
"O ( (n + `1`2σ2 ) log 1 ε ) , if each
gi(·) is (`1, `2)-smooth (for `1, `2 ≥ σ).","In particular, if ψ(·) ≡ 0, then Gη(x) ≡ ∇f(x).",[0],[0]
Õ ( n+ n3/4 √ L/σ ) and Õ ( n+ n3/4(`1`2σ 2)1/4 ) .,"If one performs acceleration, the running times become",[0],[0]
We recall the idea behind a simple algorithm —that we call repeatSVRG— which finds the ε-approximate stationary points for problem (1.1) when f(x) is σ-strongly nonconvex.,2.1 RepeatSVRG,[0],[0]
The algorithm is divided into stages.,2.1 RepeatSVRG,[0],[0]
"In each stage t, consider a modified function Ft(x)",2.1 RepeatSVRG,[0],[0]
:= F (x)+σ‖x−xt‖2.,2.1 RepeatSVRG,[0],[0]
"It is easy to see that Ft(x) is σ-strongly convex, so one can apply the accelerated SVRG method to minimize Ft(x).",2.1 RepeatSVRG,[0],[0]
"Let xt+1 be any sufficiently accurate approximate minimizer of Ft(x).8
Now, one can prove (c.f. Section 4) that xt+1 is an O(σ‖xt − xt+1‖)-approximate stationary point for F (x).",2.1 RepeatSVRG,[0],[0]
"Therefore, if σ‖xt − xt+1‖ ≤ ε we can stop the algorithm because we have already found an O(ε)-approximate stationary point.",2.1 RepeatSVRG,[0],[0]
"If σ‖xt − xt+1‖ > ε , then it must satisfy that F (xt) − F (xt+1) ≥ σ‖xt",2.1 RepeatSVRG,[0],[0]
"− xt+1‖2 ≥ Ω(ε2/σ),",2.1 RepeatSVRG,[0],[0]
but this cannot happen for more than T = O ( σ ε2 (F (x0),2.1 RepeatSVRG,[0],[0]
− F ∗) stages.,2.1 RepeatSVRG,[0],[0]
"Therefore, the total gradient complexity is T multiplied with the complexity of accelerated SVRG in each stage (which is Õ(n + n3/4",2.1 RepeatSVRG,[0],[0]
√ L/σ),2.1 RepeatSVRG,[0],[0]
according to Theorem 2.3).,2.1 RepeatSVRG,[0],[0]
Remark 2.4.,2.1 RepeatSVRG,[0],[0]
"The complexity of repeatSVRG can be inferred from (Agarwal et al., 2017; Carmon et al., 2016), but is not explicitly stated.",2.1 RepeatSVRG,[0],[0]
"For instance, the paper (Carmon et al., 2016) does not allow F (x) to have a non-smooth proximal term ψ(x), and applies accelerated gradient descent instead of accelerated SVRG.",2.1 RepeatSVRG,[0],[0]
"We introduce two variants of our algorithms: (1) the base method Natasha targets on the simple regime when f(x) and each fi(x) are both L-smooth, and (2) the full method Natashafull targets on the more refined regime when f(x) is L-smooth but each fi(x) is (`1, `2)-smooth.
",3 Our Algorithms,[0],[0]
"Both methods follow the general idea of variance-reduced stochastic gradient descent: in each inner-most iteration, they compute a gradient estimator ∇̃",3 Our Algorithms,[0],[0]
that is of the form ∇̃ = ∇f(x̃)−∇fi(x̃)+∇fi(x) and satisfies Ei∈R[n][∇̃] = ∇f(x).,3 Our Algorithms,[0],[0]
"Here, x̃ is a snapshot point that is changed once every n iterations (i.e., for each different k = 1, 2, . . .",3 Our Algorithms,[0],[0]
", T ′ in the pseudocode), and we call it a full epoch for every distinct k. Notice that the amortized gradient complexity for computing ∇̃ is O(1) per-iteration.
",3 Our Algorithms,[0],[0]
Base Method.,3 Our Algorithms,[0],[0]
"In Natasha (see Algorithm 1), as illustrated by Figure 2, we divide each full epoch into p subepochs s = 0, 1, . . .",3 Our Algorithms,[0],[0]
", p",3 Our Algorithms,[0],[0]
"− 1, each of length m",3 Our Algorithms,[0],[0]
=,3 Our Algorithms,[0],[0]
n,3 Our Algorithms,[0],[0]
/p.,3 Our Algorithms,[0],[0]
"In
8Since the accelerated SVRG method has a linear convergence rate for strongly convex functions, the complexity to find such xt+1 only depends logarithmically on this accuracy.
",3 Our Algorithms,[0],[0]
"each sub-epoch s, we start with a point x0 = x̂, and replace f(x) with its regularized version fs(x) := f(x) + σ‖x",3 Our Algorithms,[0],[0]
− x̂‖2.,3 Our Algorithms,[0],[0]
"Then, in each iteration t of the sub-epoch s, we • compute gradient estimator ∇̃",3 Our Algorithms,[0],[0]
"with respect to fs(xt), • perform update xt+1 = arg miny { ψ(y) +",3 Our Algorithms,[0],[0]
"〈∇̃, y〉 +
1 2α‖y − xt‖
2 }
with learning rate α.
",3 Our Algorithms,[0],[0]
"Effectively, the introduction of the regularizer σ‖x",3 Our Algorithms,[0],[0]
"− x̂‖2 makes sure that when performing update xt ← xt+1, we also move a bit towards point x̂",3 Our Algorithms,[0],[0]
"(i.e., retraction by regularization).",3 Our Algorithms,[0],[0]
"Finally, when the sub-epoch is done, we define x̂ to be a random one from {x0, . . .",3 Our Algorithms,[0],[0]
", xm−1}.
",3 Our Algorithms,[0],[0]
Full Method.,3 Our Algorithms,[0],[0]
"In Natashafull (see full version), we also divide each full epoch into p sub-epochs.",3 Our Algorithms,[0],[0]
"In each sub-epoch s, we start with a point x0 = z0 = x̂ and define fs(x) := f(x) + σ‖x",3 Our Algorithms,[0],[0]
− x̂‖2.,3 Our Algorithms,[0],[0]
"However, this time in each iteration t, we
• compute gradient estimator ∇̃ with respect to fs(xt), • perform update zt+1 = arg miny { ψ(y) +",3 Our Algorithms,[0],[0]
"〈∇̃, y〉 +
1 2α‖y − zt‖
2 }
with learning rate α, and
• choose xt+1 = 12zt+1 + 1 2 x̂.
Effectively, the regularizer σ‖x− x̂‖2 makes sure that when performing updates, we move a bit towards point x̂ (i.e., retraction by regularization); at the same time, the choice xt+1 = 1 2zt+1 + 1 2 x̂ also helps us move towards point x̂",3 Our Algorithms,[0],[0]
"(i.e., retraction by the so-called “Katyusha momentum”9).",3 Our Algorithms,[0],[0]
"Finally, when the sub-epoch is over, we define x̂ to be a random one from the set {x0, . . .",3 Our Algorithms,[0],[0]
", xm−1}, and move to the next sub-epoch.",3 Our Algorithms,[0],[0]
"In this section, we present a sufficient condition for finding approximate stationary points in a σ-strongly nonconvex function.",4 A Sufficient Stopping Criterion,[0],[0]
"Lemma 4.1 below states that, if we regularize the original function and define G(x) := F (x) + σ‖x",4 A Sufficient Stopping Criterion,[0],[0]
"− x̂‖2 for an arbitrary point x̂, then the minimizer of G(x) is an approximate saddle-point for F (x).
",4 A Sufficient Stopping Criterion,[0],[0]
Lemma 4.1.,4 A Sufficient Stopping Criterion,[0],[0]
"Suppose G(y) = F (y) +σ‖y− x̂‖2 for some given point x̂, and let x∗ be the minimizer of G(y).",4 A Sufficient Stopping Criterion,[0],[0]
"If we minimize G(y) and obtain a point x satisfying
G(x)−G(x∗) ≤ δ2σ , then for every η ∈ ( 0, 1max{L,4σ} ] we have the gradient mapping
‖Gη(x)‖2 ≤ 12σ2‖x∗",4 A Sufficient Stopping Criterion,[0],[0]
−,4 A Sufficient Stopping Criterion,[0],[0]
"x̂‖2 +O ( δ2 ) .
",4 A Sufficient Stopping Criterion,[0],[0]
Notice that when ψ(x) ≡ 0,4 A Sufficient Stopping Criterion,[0],[0]
"this lemma is trivial, and can be found for instance in (Carmon et al., 2016).",4 A Sufficient Stopping Criterion,[0],[0]
"The main
9The idea for this second kind of retraction, and the idea of having the updates on a sequence zt but computing gradients at points xt, is largely motivated by our recent work on the Katyusha momentum and the Katyusha acceleration (Allen-Zhu, 2017).
",4 A Sufficient Stopping Criterion,[0],[0]
"Algorithm 1 Natasha(x∅, p, T ′, α)
Input: starting vector x∅, sub-epoch count p ∈",4 A Sufficient Stopping Criterion,[0],[0]
"[n], epoch count T ′, learning rate α > 0.",4 A Sufficient Stopping Criterion,[0],[0]
"Output: vector xout.
1: x̂← x∅; m← n/p; X ←",4 A Sufficient Stopping Criterion,[0],[0]
[]; 2: for k ← 1 to T ′,4 A Sufficient Stopping Criterion,[0],[0]
do T ′,4 A Sufficient Stopping Criterion,[0],[0]
full epochs 3: x̃← x̂; µ← ∇f(x̃); 4: for s← 0,4 A Sufficient Stopping Criterion,[0],[0]
to p− 1,4 A Sufficient Stopping Criterion,[0],[0]
do p sub,4 A Sufficient Stopping Criterion,[0],[0]
-epochs in each epoch 5: x0 ← x̂;,4 A Sufficient Stopping Criterion,[0],[0]
"X ← [X, x̂]; 6: for t← 0",4 A Sufficient Stopping Criterion,[0],[0]
"to m− 1 do m iterations in each sub-epoch 7: i← a random choice from {1, · · · , n}.",4 A Sufficient Stopping Criterion,[0],[0]
8: ∇̃,4 A Sufficient Stopping Criterion,[0],[0]
← ∇fi(xt)−∇fi(x̃) + µ+ 2σ(xt − x̂),4 A Sufficient Stopping Criterion,[0],[0]
Ei[∇̃] = ∇ ( f(x) + σ‖x−,4 A Sufficient Stopping Criterion,[0],[0]
x̂‖2 )∣∣,4 A Sufficient Stopping Criterion,[0],[0]
"xt
9: xt+1 = arg miny∈Rd { ψ(y) + 12α‖y − xt‖ 2 + 〈∇̃, y〉 }
10: end for 11: x̂←",4 A Sufficient Stopping Criterion,[0],[0]
"a random choice from {x0, x1, . .",4 A Sufficient Stopping Criterion,[0],[0]
.,4 A Sufficient Stopping Criterion,[0],[0]
", xm−1}; for practitioners, choose the average 12: end for 13: end for 14: x̂← a random vector in X; for practitioners, choose the last 15: xout ← an approximate minimizer of G(y)",4 A Sufficient Stopping Criterion,[0],[0]
:= F (y) + σ‖y,4 A Sufficient Stopping Criterion,[0],[0]
− x̂‖2 using SVRG. 16: return xout.,4 A Sufficient Stopping Criterion,[0],[0]
"it suffices to run SVRG for O(n log 1
ε ) iterations.
",4 A Sufficient Stopping Criterion,[0],[0]
technical difficulty arises in order to deal with ψ(x) 6= 0.,4 A Sufficient Stopping Criterion,[0],[0]
The proof is included in the full version.,4 A Sufficient Stopping Criterion,[0],[0]
"In this section, we consider problem (1.1) where each fi(x) is L-smooth and F (x) is σ-strongly nonconvex.",5 Base Method: Analysis for One Full Epoch,[0],[0]
"We use our base method Natasha to minimize F (x), and analyze its behavior for one full epoch in this section.",5 Base Method: Analysis for One Full Epoch,[0],[0]
"We assume σ ≤ L without loss of generality, because any L-smooth function is also L-strongly nonconvex.
Notations.",5 Base Method: Analysis for One Full Epoch,[0],[0]
"We introduce the following notations for analysis purpose only.
",5 Base Method: Analysis for One Full Epoch,[0],[0]
• Let x̂s be the vector x̂ at the beginning of sub-epoch s. •,5 Base Method: Analysis for One Full Epoch,[0],[0]
Let xst be the vector xt in sub-epoch s. •,5 Base Method: Analysis for One Full Epoch,[0],[0]
Let ist be the index,5 Base Method: Analysis for One Full Epoch,[0],[0]
i ∈,5 Base Method: Analysis for One Full Epoch,[0],[0]
[n] in sub-epoch s at iteration t. • Let fs(x) := f(x) + σ‖x,5 Base Method: Analysis for One Full Epoch,[0],[0]
"− x̂s‖2, F s(x)",5 Base Method: Analysis for One Full Epoch,[0],[0]
":= F (x) + σ‖x− x̂s‖2, and xs∗",5 Base Method: Analysis for One Full Epoch,[0],[0]
":= arg minx{F s(x)}.
",5 Base Method: Analysis for One Full Epoch,[0],[0]
•,5 Base Method: Analysis for One Full Epoch,[0],[0]
Let ∇̃fs(xst ),5 Base Method: Analysis for One Full Epoch,[0],[0]
":= ∇fi(xst )−∇fi(x̃)+∇f(x̃)+2σ(xt− x̂) where i = ist .
• Let ∇̃f(xst ) := ∇fi(xst )",5 Base Method: Analysis for One Full Epoch,[0],[0]
− ∇fi(x̃) + ∇f(x̃) where i = ist .,5 Base Method: Analysis for One Full Epoch,[0],[0]
"We obviously have that fs(x) and F s(x) are σ-strongly convex, and fs(x) is (L+ 2σ)-smooth.",5 Base Method: Analysis for One Full Epoch,[0],[0]
"The following lemma gives an upper bound on the variance of the gradient estimator ∇̃fs(xst ):
Lemma 5.1.",5.1 Variance Upper Bound,[0],[0]
We have Eist [ ‖∇̃fs(xst ),5.1 Variance Upper Bound,[0],[0]
"− ∇fs(xst )‖2 ] ≤
pL2‖xst − x̂s‖2 + pL2 ∑s−1",5.1 Variance Upper Bound,[0],[0]
k=0,5.1 Variance Upper Bound,[0],[0]
‖x̂k,5.1 Variance Upper Bound,[0],[0]
"− x̂k+1‖2 .
",5.1 Variance Upper Bound,[0],[0]
Proof.,5.1 Variance Upper Bound,[0],[0]
We have Eist [ ‖∇̃fs(xst )−∇fs(xst ),5.1 Variance Upper Bound,[0],[0]
‖2 ] = Eist [ ‖∇̃f(xst ),5.1 Variance Upper Bound,[0],[0]
−∇f(xst ),5.1 Variance Upper Bound,[0],[0]
"‖2 ] = Ei∈R[n]
[∥∥(∇fi(xst )−∇fi(x̃))− (∇f(xst )−∇f(x̃)))∥∥2] ¬
≤ Ei∈R[n]",5.1 Variance Upper Bound,[0],[0]
"[∥∥∇fi(xst )−∇fi(x̃)∥∥2]
 ≤ pEi∈R[n]",5.1 Variance Upper Bound,[0],[0]
[∥∥∇fi(xst ),5.1 Variance Upper Bound,[0],[0]
"−∇fi(x̂s)∥∥2]
+ p ∑s−1 k=0 Ei∈R[n]",5.1 Variance Upper Bound,[0],[0]
"[∥∥∇fi(x̂k)−∇fi(x̂k+1)∥∥2] ®
≤ pL2‖xst",5.1 Variance Upper Bound,[0],[0]
− x̂s‖2 + pL2 ∑s−1,5.1 Variance Upper Bound,[0],[0]
k=0 ‖x̂ k,5.1 Variance Upper Bound,[0],[0]
"− x̂k+1‖2 .
",5.1 Variance Upper Bound,[0],[0]
"Above, inequality ¬ is because for any random vector ζ ∈ Rd, it holds that E‖ζ−Eζ‖2 = E‖ζ‖2−‖Eζ‖2; inequality  is because x̂0 = x̃ and for any p vectors a1, a2, . .",5.1 Variance Upper Bound,[0],[0]
.,5.1 Variance Upper Bound,[0],[0]
", ap ∈ Rd, it holds that ‖a1+· · ·+",5.1 Variance Upper Bound,[0],[0]
ap‖2 ≤ p‖a1‖2+· · ·+p‖ap‖2; and inequality ® is because each fi(·) is L-smooth.,5.1 Variance Upper Bound,[0],[0]
"The following inequality is classically known as the “regret inequality” for mirror descent (Allen-Zhu & Orecchia, 2017), and its proof is classical (see full version):
Fact 5.2.",5.2 Analysis for One Sub-Epoch,[0],[0]
"〈∇̃fs(xst ), xst+1",5.2 Analysis for One Sub-Epoch,[0],[0]
− u〉 + ψ(xst+1),5.2 Analysis for One Sub-Epoch,[0],[0]
"− ψ(u) ≤ ‖xst−u‖ 2
2α − ‖xst+1−u‖ 2 2α − ‖xst+1−x s t‖ 2 2α for every u ∈ R d.
The following lemma is our main contribution for the base method Natasha.
",5.2 Analysis for One Sub-Epoch,[0],[0]
Lemma 5.3.,5.2 Analysis for One Sub-Epoch,[0],[0]
"As long as α ≤ 12L+4σ , we have E",5.2 Analysis for One Sub-Epoch,[0],[0]
[( F s(x̂s+1)− F s(xs∗) )],5.2 Analysis for One Sub-Epoch,[0],[0]
≤ E,5.2 Analysis for One Sub-Epoch,[0],[0]
"[F s(x̂s)− F s(xs∗) σαm/2 +αpL2 ( s∑ k=0 ‖x̂k−x̂k+1‖2 )] .
",5.2 Analysis for One Sub-Epoch,[0],[0]
Proof.,5.2 Analysis for One Sub-Epoch,[0],[0]
"We first compute that F s(xst+1)− F s(u) = fs(xst+1)− fs(u) + ψ(xst+1)− ψ(u)
¬ ≤ fs(xst ) +",5.2 Analysis for One Sub-Epoch,[0],[0]
"〈∇fs(xst ), xst+1",5.2 Analysis for One Sub-Epoch,[0],[0]
− xst 〉,5.2 Analysis for One Sub-Epoch,[0],[0]
"+ L+ 2σ
2 ‖xst",5.2 Analysis for One Sub-Epoch,[0],[0]
"− xst+1‖2
− fs(u) + ψ(xst+1)− ψ(u)  ≤ 〈∇fs(xst ), xst+1",5.2 Analysis for One Sub-Epoch,[0],[0]
− xst 〉,5.2 Analysis for One Sub-Epoch,[0],[0]
"+ L+ 2σ
2 ‖xst",5.2 Analysis for One Sub-Epoch,[0],[0]
"− xst+1‖2
+ 〈∇fs(xst ),",5.2 Analysis for One Sub-Epoch,[0],[0]
xst,5.2 Analysis for One Sub-Epoch,[0],[0]
− u〉+ ψ(xst+1)− ψ(u) .,5.2 Analysis for One Sub-Epoch,[0],[0]
"(5.1)
Above, inequality ¬ uses the fact that fs(·) is (L + 2σ)smooth; and inequality  uses the convexity of fs(·).",5.2 Analysis for One Sub-Epoch,[0],[0]
"Now, we take expectation with respect to ist on both sides of (5.1), and derive that:
Eist [ F s(xst+1) ]",5.2 Analysis for One Sub-Epoch,[0],[0]
"− F s(u)
¬ ≤",5.2 Analysis for One Sub-Epoch,[0],[0]
"Eist [ 〈∇̃fs(xst )−∇fs(xst ), xst",5.2 Analysis for One Sub-Epoch,[0],[0]
"− xst+1〉+ 〈∇̃fs(xst ), xst+1",5.2 Analysis for One Sub-Epoch,[0],[0]
"− u〉
+ L+ 2σ
2 ‖xst",5.2 Analysis for One Sub-Epoch,[0],[0]
"− xst+1‖2 + ψ(xst+1)− ψ(u) ] 
≤",5.2 Analysis for One Sub-Epoch,[0],[0]
"Eist [ 〈∇̃fs(xst )−∇fs(xst ),",5.2 Analysis for One Sub-Epoch,[0],[0]
xst,5.2 Analysis for One Sub-Epoch,[0],[0]
− xst+1〉+ ‖xst,5.2 Analysis for One Sub-Epoch,[0],[0]
"− u‖2
2α
− ‖x s t+1",5.2 Analysis for One Sub-Epoch,[0],[0]
− u‖2 2α − ( 1 2α − L+ 2σ 2 ) ‖xst+1,5.2 Analysis for One Sub-Epoch,[0],[0]
"− xst‖2 ] ®
≤",5.2 Analysis for One Sub-Epoch,[0],[0]
Eist [ α ∥∥∇̃fs(xst )−∇fs(xst )∥∥2,5.2 Analysis for One Sub-Epoch,[0],[0]
+,5.2 Analysis for One Sub-Epoch,[0],[0]
‖xst,5.2 Analysis for One Sub-Epoch,[0],[0]
"− u‖2 2α − ‖x s t+1 − u‖2 2α ] ¯
≤",5.2 Analysis for One Sub-Epoch,[0],[0]
Eist [ αpL2‖xst − x̂s‖2 + αpL2 s−1∑ k=0 ‖x̂k,5.2 Analysis for One Sub-Epoch,[0],[0]
"− x̂k+1‖2
+ ‖xst",5.2 Analysis for One Sub-Epoch,[0],[0]
"− u‖2 2α − ‖x s t+1 − u‖2 2α
] .",5.2 Analysis for One Sub-Epoch,[0],[0]
"(5.2)
Above, inequality ¬ is follows from (5.1) together with the fact that",5.2 Analysis for One Sub-Epoch,[0],[0]
Eist [∇̃f s(xst )],5.2 Analysis for One Sub-Epoch,[0],[0]
"= ∇fs(xst ) implies
Eist [ 〈∇fs(xst ), xst+1",5.2 Analysis for One Sub-Epoch,[0],[0]
"− xst 〉+ 〈∇fs(xst ), xst",5.2 Analysis for One Sub-Epoch,[0],[0]
"− u〉 ] = Eist [ 〈∇̃fs(xst )−∇fs(xst ), xst−xst+1〉+〈∇̃fs(xst ), xst+1−u〉 ] ;
inequality  uses Fact 5.2; inequality ® uses α ≤ 12L+4σ together with Young’s inequality 〈a, b〉 ≤ 12‖a‖ 2 + 12‖b‖ 2; and inequality ¯ uses Lemma 5.1.",5.2 Analysis for One Sub-Epoch,[0],[0]
"Finally, choosing u = xs∗ to be the (unique) minimizer of F s(·) = fs(·) + ψ(·), and telescoping inequality (5.2) for t = 0, 1, . . .",5.2 Analysis for One Sub-Epoch,[0],[0]
",m− 1, we have
E [m−1∑ t=1 ( F s(xst )",5.2 Analysis for One Sub-Epoch,[0],[0]
− F s(xs∗) ),5.2 Analysis for One Sub-Epoch,[0],[0]
],5.2 Analysis for One Sub-Epoch,[0],[0]
≤,5.2 Analysis for One Sub-Epoch,[0],[0]
E,5.2 Analysis for One Sub-Epoch,[0],[0]
"[‖xs0 − xs∗‖2 2α + m−1∑ t=0 ( αpL2‖xst − x̂s‖2
+ αpL2 s−1∑ k=0 ‖x̂k − x̂k+1‖2 )]
",5.2 Analysis for One Sub-Epoch,[0],[0]
≤ E,5.2 Analysis for One Sub-Epoch,[0],[0]
"[F s(x̂s)− F s(xs∗)
",5.2 Analysis for One Sub-Epoch,[0],[0]
σα + αpmL2 ( s∑ k=0 ‖x̂k,5.2 Analysis for One Sub-Epoch,[0],[0]
"− x̂k+1‖2 )] .
",5.2 Analysis for One Sub-Epoch,[0],[0]
"Above, the second inequality uses the fact that x̂s+1 is chosen from {xs0, . . .",5.2 Analysis for One Sub-Epoch,[0],[0]
", xsm−1} uniformly at random, as well as the σ-strong convexity of F s(·).
",5.2 Analysis for One Sub-Epoch,[0],[0]
"Dividing both sides by m and rearranging the terms (using 1 2σα ≥ 1), we have
E",5.2 Analysis for One Sub-Epoch,[0],[0]
[( F s(x̂s+1)− F s(xs∗) )],5.2 Analysis for One Sub-Epoch,[0],[0]
≤ E,5.2 Analysis for One Sub-Epoch,[0],[0]
[F s(x̂s)− F s(xs∗) σαm/2 + αpL2 ( s∑ k=0 ‖x̂k,5.2 Analysis for One Sub-Epoch,[0],[0]
− x̂k+1‖2 )] .,5.2 Analysis for One Sub-Epoch,[0],[0]
"One can telescope Lemma 5.3 for an entire epoch and arrive at the following lemma (see full version):
Lemma 5.4.",5.3 Analysis for One Full Epoch,[0],[0]
"If α ≤ 12L+4σ , α ≥ 4 σm and α ≤ σ",5.3 Analysis for One Full Epoch,[0],[0]
"p2L2 , we have p−1∑ s=0 E",5.3 Analysis for One Full Epoch,[0],[0]
[( F s(x̂s)− F s(xs∗) )],5.3 Analysis for One Full Epoch,[0],[0]
≤ 2E [ F (x̂0)− F (x̂p) ] .,5.3 Analysis for One Full Epoch,[0],[0]
We are now ready to state and prove our main convergence theorem for Natasha: Theorem 1.,6 Base Method: Final Theorem,[0],[0]
"Suppose in (1.1), each fi(x) is L-smooth and F (x) is σ-strongly nonconvex for σ ≤",6 Base Method: Final Theorem,[0],[0]
"L. Then, if L2
σ2 ≤ n, p = Θ ( ( σ 2 L2n) 1/3 )
and α = Θ( σp2L2 ), our base method Natasha outputs a point xout satisfying
E[‖Gη(xout)‖2] ≤",6 Base Method: Final Theorem,[0],[0]
"O ( (L2σ)1/3n2/3
T ′n
) ·",6 Base Method: Final Theorem,[0],[0]
"(F (x∅)− F ∗) .
",6 Base Method: Final Theorem,[0],[0]
"for every η ∈ ( 0, 1max{L,4σ} ] .",6 Base Method: Final Theorem,[0],[0]
"In other words, to obtain E[‖Gη(xout)‖2] ≤ ε2, we need gradient complexity
O ( n log 1
ε +
(L2σ)1/3n2/3
ε2 · (F (x∅)− F ∗)
) .
",6 Base Method: Final Theorem,[0],[0]
"In the above theorem, we have assumed σ ≤ L without loss of generality because any L-smooth function is also L-strongly nonconvex.",6 Base Method: Final Theorem,[0],[0]
"Also, we have assumed L 2
σ2 ≤ n",6 Base Method: Final Theorem,[0],[0]
"and if this inequality does not hold, then one should apply repeatSVRG for a faster running time (see Figure 1).",6 Base Method: Final Theorem,[0],[0]
Proof of Theorem 1.,6 Base Method: Final Theorem,[0],[0]
"We choose p = ( σ2 24L2n )1/3
, m = n/p, and α = 4σm = σ 6p2L2 ≤ 1 2L+4σ , so we can apply Lemma 5.4.",6 Base Method: Final Theorem,[0],[0]
"If we telescope Lemma 5.4 for the entire algorithm (which has T ′ full epochs), and use the fact that x̂p of the previous epoch equals x̂0 of the next epoch, we conclude that if we choose a random epoch and a random subepoch s, we will have
E[F s(x̂s)− F s(xs∗)] ≤ 2 pT ′",6 Base Method: Final Theorem,[0],[0]
"(F (x∅)− F ∗) .
",6 Base Method: Final Theorem,[0],[0]
"By the σ-strong convexity of F s(·), we have E[σ‖x̂s − xs∗‖2] ≤",6 Base Method: Final Theorem,[0],[0]
4pT ′,6 Base Method: Final Theorem,[0],[0]
"(F (x ∅)− F ∗).
",6 Base Method: Final Theorem,[0],[0]
"Now, F s(x) =",6 Base Method: Final Theorem,[0],[0]
F (x)+σ‖x− x̂s‖2 satisfies the assumption of G(x) in Lemma 4.1.,6 Base Method: Final Theorem,[0],[0]
"If we use the SVRG method (see Theorem 2.3) to minimize the convex function F s(x), we
get an output xout satisfying F s(xout)",6 Base Method: Final Theorem,[0],[0]
− F s(xs∗) ≤ ε2σ in gradient complexity,6 Base Method: Final Theorem,[0],[0]
"O ( (n+ L 2
σ2 ) log 1 ε )",6 Base Method: Final Theorem,[0],[0]
"≤ O(n log 1ε ).
",6 Base Method: Final Theorem,[0],[0]
"We can therefore apply Lemma 4.1 and conclude that this output xout satisfies
E[‖Gη(xout)‖2] ≤",6 Base Method: Final Theorem,[0],[0]
O ( σ pT ′ ) ·,6 Base Method: Final Theorem,[0],[0]
"(F (x∅)− F ∗)
",6 Base Method: Final Theorem,[0],[0]
"= O ( (L2σ)1/3n2/3
T ′n
) ·",6 Base Method: Final Theorem,[0],[0]
"(F (x∅)− F ∗) .
",6 Base Method: Final Theorem,[0],[0]
"In other words, we obtain E[‖Gη(xout)‖2] ≤ ε2 using T ′n",6 Base Method: Final Theorem,[0],[0]
=,6 Base Method: Final Theorem,[0],[0]
"O ( n+ (L 2σ)1/3n2/3
ε2 · (F (x ∅)− F ∗) )",6 Base Method: Final Theorem,[0],[0]
computations of the stochastic gradients.,6 Base Method: Final Theorem,[0],[0]
"Here, the additive term n is because T ′",6 Base Method: Final Theorem,[0],[0]
≥ 1.,6 Base Method: Final Theorem,[0],[0]
"Finally, adding this withO(n log 1ε ), the gradient complexity for the application of SVRG in the last line of Natasha, we finish the proof of the total gradient complexity.",6 Base Method: Final Theorem,[0],[0]
We analyze and state the main theorems for our full method Natashafull in the full version of this paper.,7 Full Method: Final Theorem,[0],[0]
Stochastic gradient descent and gradient descent (including alternating minimization) have become the canonical methods for solving non-convex machine learning tasks.,8 Conclusion,[0],[0]
"However, can we design new non-convex methods to run even faster than SGD or GD?
This present paper tries to tackle this general question, by providing a new Natashamethod which is intrinsically different from GD or SGD.",8 Conclusion,[0],[0]
It runs faster than GD and SVRGbased methods at least in theory.,8 Conclusion,[0],[0]
"We hope that this could be a non-negligible step towards our better understanding of non-convex optimization.
",8 Conclusion,[0],[0]
"Finally, our results give rise to an interesting dichotomy in the best-known complexity of first-order non-convex optimization: the complexity scales with n3/4 for σ < L/",8 Conclusion,[0],[0]
√ n and with n2/3 for σ > L/,8 Conclusion,[0],[0]
√ n.,8 Conclusion,[0],[0]
"It remains open to investigate whether this dichotomy is intrinsic, or we can design a more efficient algorithm that outperforms both.",8 Conclusion,[0],[0]
"Given a non-convex function f(x) that is an average of n smooth functions, we design stochastic first-order methods to find its approximate stationary points.",abstractText,[0],[0]
The performance of our new methods depend on the smallest (negative) eigenvalue −σ of the Hessian.,abstractText,[0],[0]
"This parameter σ captures how strongly non-convex f(x) is, and is analogous to the strong convexity parameter for convex optimization.",abstractText,[0],[0]
"At least in theory, our methods outperform known results for a range of parameter σ, and can also be used to find approximate local minima.",abstractText,[0],[0]
Our result implies an interesting dichotomy: there exists a threshold σ0 so that the (currently) fastest methods for σ > σ0 and for σ < σ0 have different behaviors: the former scales with n and the latter scales with n.,abstractText,[0],[0]
Natasha: Faster Non-Convex Stochastic Optimization  via Strongly Non-Convex Parameter,title,[0],[0]
"Acquisition of vocabulary and semantic knowledge of a second language, including appropriate word choice and awareness of subtle word meaning contours, are recognized as a notoriously hard task, even for advanced non-native speakers.",1 Introduction,[0],[0]
"When nonnative authors produce utterances in a foreign language (L2), these utterances are marked by traces of their native language (L1).",1 Introduction,[0],[0]
"Such traces are known as transfer effects, and they can be phonological (a foreign accent), morphological, lexical, or syntactic.",1 Introduction,[0],[0]
"Specifically, psycholinguistic research has shown that the choice of lexical items is influenced by the author’s L1, and that non-native speakers tend to choose words that happen to have cognates in their native language.
",1 Introduction,[0],[0]
"Cognates are words in two languages that share both a similar meaning and a similar phonetic (and,
sometimes, also orthographic) form, due to a common ancestor in some protolanguage.",1 Introduction,[0],[0]
The definition is sometimes also extended to words that have similar forms and meanings due to borrowing.,1 Introduction,[0],[0]
"Most studies on cognate facilitation have been conducted with few human subjects, focusing on few words, and the experimental setup was such that participants were asked to produce lexical choices in an artificial setting.",1 Introduction,[0],[0]
"We demonstrate that cognates affect lexical choice in L2 spontaneous production on a much larger scale.
",1 Introduction,[0],[0]
"Using a new and unique large corpus of nonnative English that we introduce as part of this work, we identify a focus set of over 1000 words, and show that they are distributed very differently across the “Englishes” of authors with various L1s.",1 Introduction,[0],[0]
"Importantly, we go to great lengths to guarantee that these words do not reflect specific properties of the various native languages, the cultures associated with them, or the topics that may be relevant for particular geographic regions.",1 Introduction,[0],[0]
"Rather, these are “ordinary” words, with very little culture-specific weight, that happen to have synonyms in English that may reflect cognates in some L1s, but not all of them.",1 Introduction,[0],[0]
"Consequently, they are used differently by authors with different linguistic backgrounds, to the extent that the authors’ L1s can be identified through their use of the words in the focus set.",1 Introduction,[0],[0]
"The signal of L1 is so powerful, that we are able to reconstruct a linguistic typology tree from the distribution of these words in the Englishes witnessed in the corpus.
",1 Introduction,[0],[0]
"We propose a methodology for creating a focus set of highly frequent, unbiased words that we expect to be distributed differently across different Englishes simply because they happen to have synonyms with different etymologies, even though they
329
Transactions of the Association for Computational Linguistics, vol. 6, pp. 329–342, 2018.",1 Introduction,[0],[0]
Action Editor: Ivan Titov.,1 Introduction,[0],[0]
"Submission batch: 1/2018; Revision batch: 3/2018; Published 5/2018.
",1 Introduction,[0],[0]
c©2018 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
carry very limited cultural weight.",1 Introduction,[0],[0]
"Then, we show that simple lexical semantic features (based on the focus set of words) suffice for clustering together English texts authored by speakers of “closer” languages; we generate a phylogenetic tree of 31 languages solely by looking at lexical semantic properties of the English spoken by non-native speakers from 31 countries.
",1 Introduction,[0],[0]
The contribution of this work is twofold.,1 Introduction,[0],[0]
"First, we introduce the L2-Reddit corpus: a large corpus of highly-advanced, fluent, diverse, non-native English, with sentence-level annotations of the native language of each author.",1 Introduction,[0],[0]
"Second, we lay out sound empirical foundations for the theoretical hypothesis on the cognate effect in L2 of non-native English speakers, highlighting the cognate facilitation phenomenon as one of the important factors shaping the language of non-native speakers.
",1 Introduction,[0],[0]
"After discussing related work in Section 2, we describe the L2-Reddit corpus in Section 3.",1 Introduction,[0],[0]
Section 4 details the methodology we use and our results.,1 Introduction,[0],[0]
"We analyze these results in Section 5, and conclude with suggestions for future research.",1 Introduction,[0],[0]
The language of bilinguals is different.,2 Related Work,[0],[0]
"The mutual presence of two linguistic systems in the mind of the bilingual speaker involves a significant cognitive load (Shlesinger, 2003; Hvelplund, 2014; Prior, 2014; Kroll et al., 2014); this burden is likely to have a bearing on the linguistic productions of the bilingual speaker.",2 Related Work,[0],[0]
"Moreover, the presence of more than one linguistic system gives rise to transfer: traces of one linguistic system may be observed in the other language (Jarvis and Pavlenko, 2008).
",2 Related Work,[0],[0]
"Several works addressed the translation choices of bilingual speakers, either within a rich linguistic context (e.g., given a source sentence), or decontextualized.",2 Related Work,[0],[0]
"For example, de Groot (1992) demonstrated that cognate translations are produced more rapidly and accurately than translations that do not exhibit phonetic or orthographic similarity with a source word.",2 Related Work,[0],[0]
"This observation was further articulated by Prior et al. (2007), who showed that translation choices of L2 speakers were positively correlated with cross-linguistic form overlap of a stimulus word with its target language translations.",2 Related Work,[0],[0]
"Prior et al. (2011) emphasized that “bilinguals are sensi-
tive to the degree of form overlap between the translation equivalents in the two languages, and show a preference toward producing a cognate translation”.",2 Related Work,[0],[0]
"As an example, they showed that the preferred translation of the Spanish incidente to English was incident, and not the alternative translation event, despite the much higher frequency of the latter.
",2 Related Work,[0],[0]
"More recent work is consistent with previous research and advances it by highlighting phonologically mediated cross-lingual influences on visual word processing of same- and different-script bilinguals (Degani and Tokowicz, 2010; Degani et al., 2017).",2 Related Work,[0],[0]
"Cognate facilitation was also studied using eye tracking (Libben and Titone, 2009; Cop et al., 2017), demonstrating that the reading of bilinguals is influenced by orthographic similarity of words with their translation equivalents in another language.",2 Related Work,[0],[0]
"Crucially, much of this research has been conducted in a laboratory experimental setup; this implies a small number of participants, a small number of target words, and focus on a very limited set of languages.",2 Related Work,[0],[0]
"While our research questions are similar, we present a computational analysis of the effects of cognates on L2 productions on a completely different scale: 31 languages, over 1000 words, and thousands of speakers whose spontaneous language production is recorded in a very large corpus.
Corpus-based investigation of non-native language has been a prolific field of recent research.",2 Related Work,[0],[0]
Numerous studies address syntactic transfer effects on L2.,2 Related Work,[0],[0]
"Such influences from L1 facilitate various computational tasks, including automatic detection of highly competent non-native writers (Tomokiyo and Jones, 2001; Bergsma et al., 2012), identification of the mother tongue of English learners (Koppel et al., 2005; Tetreault et al., 2013; Tsvetkov et al., 2013; Malmasi et al., 2017) and typology-driven error prediction in learners’ speech (Berzak et al., 2015).",2 Related Work,[0],[0]
"English texts produced by native speakers of a variety of languages have been used to reconstruct phylogenetic trees, with varying degrees of success (Nagata and Whittaker, 2013; Berzak et al., 2014).",2 Related Work,[0],[0]
"Syntactic preferences of professional translators were exploited to reconstruct the Indo-European language tree (Rabinovich et al., 2017).",2 Related Work,[0],[0]
"Our study is also corpus-based; but it stands out as it focuses not on the distribution of function words or (shallow) syntactic structures, but rather on the unique use of
cognates in L2.",2 Related Work,[0],[0]
"From the lexical perspective, L2 writers have been shown to produce more overgeneralizations, use more frequent words and words with a lower degree of ambiguity (Hinkel, 2002; Crossley and McNamara, 2011).",2 Related Work,[0],[0]
"Several studies addressed crosslinguistic influences on semantic acquisition in L2, investigating the distribution of collocations (Siyanova-Chanturia, 2015; Kochmar and Shutova, 2017) and formulaic language (Paquot and Granger, 2012) in learner corpora.",2 Related Work,[0],[0]
"We, in contrast, address highly-fluent, advanced non-natives in this work.
",2 Related Work,[0],[0]
Nastase and Strapparava (2017) presented the first attempt to leverage etymological information for the task of native language identification of English learners.,2 Related Work,[0],[0]
"They sowed the seeds for exploitation of etymological clues in the study of non-native language, but their results were very inconclusive.
",2 Related Work,[0],[0]
"In contrast to the learner corpora that dominate studies in this field (Granger, 2003; Geertzen et al., 2013; Blanchard et al., 2013), our corpus contains spontaneous productions of advanced, highly proficient non-native speakers, spanning over 80K topical threads, by 45K distinct users from 50 countries (with 46 native languages).",2 Related Work,[0],[0]
"To the best of our knowledge, this is the first attempt to computationally study the effect of L1 cognates on L2 lexical choice in productions of competent non-native English speakers, certainly at such a large scale.
",2 Related Work,[0],[0]
"3 The L2-Reddit corpus One contribution of this work is the collection, organization and annotation of a large corpus of highlyfluent non-native English.",2 Related Work,[0],[0]
We describe this new and unique corpus in this section.,2 Related Work,[0],[0]
"Reddit1 is an online community-driven platform consisting of numerous forums for news aggregation, content rating, and discussions.",3.1 Corpus mining,[0],[0]
"As of 2017, it has over 200 million unique users, ranking the fourth most visited website in the US.",3.1 Corpus mining,[0],[0]
"Content entries are organized by areas of interest called subreddits,2 ranging from main forums that receive much attention to smaller ones that foster discussion on
1https://www.reddit.com/ 2Subreddits are typically denoted with a leading r/, for ex-
ample r/linguistics is the ‘linguistics’ subreddit.
niche areas.",3.1 Corpus mining,[0],[0]
"Subreddit topics include news, science, movies, books, music, fitness and many others.
",3.1 Corpus mining,[0],[0]
Collection of author metadata We collected a large dataset of posts (both initial submissions and subsequent comments) using an API especially designed for providing search capabilities on Reddit content.3,3.1 Corpus mining,[0],[0]
"We focused on several subreddits (r/Europe, r/AskEurope, r/EuropeanCulture, r/EuropeanFederalists, r/Eurosceptics) whose content is generated by users who specified their country as a flair (metadata attribute).",3.1 Corpus mining,[0],[0]
"Although categorized as ‘European’, these subreddits are used by people from all over the world, expressing views on politics, legislation, economics, culture, etc.
",3.1 Corpus mining,[0],[0]
"In the absence of a restrictive policy, multiple flair alternatives often exist for the same country, e.g., ‘CROA’ and ‘Croatia’ for Croatia.",3.1 Corpus mining,[0],[0]
"Additionally, distinct flairs are sometimes used for regions, cities, or states of big European countries, e.g., ‘Bavaria’ for Germany.",3.1 Corpus mining,[0],[0]
"We (manually) grouped flairs representing the same country into a single cluster, reducing 489 distinct flairs into 50 countries, from Albania to Vietnam.",3.1 Corpus mining,[0],[0]
"The posts in the Europe-related subreddits constitute our seed corpus, comprising 9M sentences (160M tokens) by over 45K distinct users.
",3.1 Corpus mining,[0],[0]
"Dataset expansion A typical user activity in Reddit is not limited to a single thread, but rather spreads across multiple, not necessarily related, areas of interest.",3.1 Corpus mining,[0],[0]
"Once the authors’ country is determined based on their European submissions, their entire Reddit footprint can be associated with their profile, and, therefore, with their country of origin.",3.1 Corpus mining,[0],[0]
"We extended our seed corpus by mining all submissions of users whose country flair is known, querying all Reddit data spanning years 2005-2017.",3.1 Corpus mining,[0],[0]
"The final dataset thus contains over 250M sentences (3.8B tokens) of native and non-native English speakers, where each sentence is annotated with its author’s country of origin.",3.1 Corpus mining,[0],[0]
"The data covers posts by over 45K authors and spans over 80K subreddits.4
Focus on “large” languages For the sake of robustness, we limited the scope of this work to (coun-
3https://github.com/pushshift/api 4The annotated dataset will freely available at http: //cl.haifa.ac.il/projects/L2.",3.1 Corpus mining,[0],[0]
"To protect the anonymity of Reddit users, the released dataset does not expose any author identifying information.
",3.1 Corpus mining,[0],[0]
tries whose L1s are) the Indo-European (IE) languages; and only to those countries whose users had at least 500K sentences in the corpus.,3.1 Corpus mining,[0],[0]
"Additionally, we excluded multilingual countries, such as Belgium and Switzerland.",3.1 Corpus mining,[0],[0]
"Consequently, the final set of Reddit authors considered in this work originate from 31 countries, which represent the three main IE language families: Germanic (Austria, Denmark, Germany, Iceland, Netherlands, Norway, Sweden); Romance (France, Italy, Mexico, Portugal, Romania, Spain); and Balto-Slavic (Bosnia, Bulgaria, Croatia, Czech, Latvia, Lithuania, Poland, Russia, Serbia, Slovakia, Slovenia, Ukraine).",3.1 Corpus mining,[0],[0]
"In addition, we have data authored by native English speakers from Australia, Canada, Ireland, New Zealand, the UK and the US.
",3.1 Corpus mining,[0],[0]
"Correlation of country annotation with L1 We view the country information as an accurate, albeit not perfect, proxy for the native language of the author.5",3.1 Corpus mining,[0],[0]
We acknowledge that the L1 information is noisy and may occasionally be inaccurate.,3.1 Corpus mining,[0],[0]
"We therefore evaluated the correlation of the country flair with L1 by means of supervised classification: our assumption is that if we can accurately distinguish among users from various countries using features that reflect language, rather than culture or content, then such a correlation indeed exists.
",3.1 Corpus mining,[0],[0]
We assume that the native language of speakers “shines through” mainly in their syntactic choices.,3.1 Corpus mining,[0],[0]
"Consequently, we opted for (shallow) syntactic structures, realized by function words (FW) and ngrams of part-of-speech (POS) tags, rather than geographical and topical markers, that are reflected best by content words.",3.1 Corpus mining,[0],[0]
"Aiming to disentangle the effect of native language we randomly shuffled texts produced by all authors from each country, thereby “blurring out” any topical (i.e., subreddit-specific) or authorial trace.",3.1 Corpus mining,[0],[0]
"Consequently, we assume that the separability of texts by country can be attributed to the only distinguishing linguistic variable left: the dimension of the native language of a speaker.
",3.1 Corpus mining,[0],[0]
"We classified 200 chunks of 100 randomly sampled sentences from each country into (i) native vs. non-native English speakers, (ii) the three IE language families, and (iii) 45 individual L1s, where
5We therefore use the terms ‘user country’, ‘native language’ and ‘L1’ interchangeably henceforth.
",3.1 Corpus mining,[0],[0]
the six English-speaking countries are unified under the native-English umbrella.,3.1 Corpus mining,[0],[0]
"Using over 400 function words and top-300 most frequent POS-trigrams, we obtained 10-fold cross-validation accuracy of 90.8%, 82.5% and 60.8%, for the three scenarios, respectively.",3.1 Corpus mining,[0],[0]
"We conclude, therefore, that the country flair can be viewed as a plausible proxy for the native language of Reddit authors.
",3.1 Corpus mining,[0],[0]
Initial preprocessing Several preprocessing steps were applied on the dataset.,3.1 Corpus mining,[0],[0]
We (i) removed text by users who changed their country flair within their period of activity; (ii) excluded non-English sentences;6 and (iii) eliminated sentences containing single non-alphabetic tokens.,3.1 Corpus mining,[0],[0]
The final corpus comprises over 230M sentences and 3.5B tokens.,3.1 Corpus mining,[0],[0]
"Unlike most corpora of non-native speakers, which focus on learners (e.g., ICLE (Granger, 2003), EFCAMDAT (Geertzen et al., 2013), or the TOEFL dataset (Blanchard et al., 2013)), our corpus is unique in that it is composed by fluent, advanced non-native speakers of English.",3.2 Evaluation of author proficiency,[0],[0]
"We verified that, on average, Reddit users possess excellent, near-native command of English by comparing three distinct populations: (i) Reddit native English authors, defined as those tagged for one of the English-speaking countries: Australia, Canada, Ireland, New Zealand, and the UK.",3.2 Evaluation of author proficiency,[0],[0]
"We excluded texts produced by US authors due to the high ratio of the US immigrant population; (ii) Reddit non-native English authors; and (iii) A population of English learners, using the TOEFL dataset (Blanchard et al., 2013); here, the proficiency of authors is classified as low, intermediate, or high.
",3.2 Evaluation of author proficiency,[0],[0]
"We compared these populations across various indices, assessing their proficiency with several commonly accepted lexical and syntactic complexity measures (Lu and Ai, 2015; Kyle and Crossley, 2015).",3.2 Evaluation of author proficiency,[0],[0]
"Lexical richness was evaluated through typeto-token ratio (TTR), average age-of-acquisition (in years) of lexical items (Kuperman et al., 2012), and mean word rank, where the rank was retrieved from a list of the entire Reddit dataset vocabulary, sorted by word frequency in the corpus.",3.2 Evaluation of author proficiency,[0],[0]
"Syntactic com-
6We used the polyglot language detection tool (http:// polyglot.readthedocs.io).
",3.2 Evaluation of author proficiency,[0],[0]
"plexity was assessed using mean length of T-units (TU; the minimal terminable unit of language that can be considered a grammatical sentence), and the ratio of complex T-units (those containing a dependent clause) to all T-units in a sentence.
",3.2 Evaluation of author proficiency,[0],[0]
Table 1 reports the results.,3.2 Evaluation of author proficiency,[0],[0]
"Across almost all indices, the level of Reddit non-natives is much higher than even the advanced TOEFL learners, and almost on par with Reddit natives.",3.2 Evaluation of author proficiency,[0],[0]
Cognates are words in two languages that share both a similar meaning and a similar form.,4.1 Hypotheses,[0],[0]
"Our main hypothesis is that non-native speakers, when required to pick an English word that has a set of synonyms, are more likely to select a lexical item that has a cognate in their L1.",4.1 Hypotheses,[0],[0]
We therefore expect the effect of L1 cognates to be reflected in the frequency of their English counterparts in the spontaneous productions of L2 speakers.,4.1 Hypotheses,[0],[0]
"Moreover, we expect similar effects, perhaps to a lesser extent, in the contextual usage of certain words, reflecting collocations and subtle contours of word meanings that are transferred from L1.",4.1 Hypotheses,[0],[0]
"The different contexts that certain words are embedded in (in the Englishes of speakers with different L1 backgrounds) can be captured by the means of distributional semantics.
",4.1 Hypotheses,[0],[0]
"Furthermore, we hypothesize that the effect of L1 is powerful to an extent that facilitates clustering of Englishes produced by non-natives with “similar” L1s; specifically, L1s that belong to the same language family.",4.1 Hypotheses,[0],[0]
"“Similar” L1s may reflect both typological and areal closeness: for example, we expect the English spoken by Romanians to be similar both to the English of Italians (as both are Romance languages) and to the English of Bulgarians (as both are Balkan languages).",4.1 Hypotheses,[0],[0]
"Ultimately, we aim to reconstruct the IE language phylogeny, reflecting historical and areal evolution of the subsets of Germanic, Romance and Balto-Slavic languages over thousands of years, from non-native English only.
",4.1 Hypotheses,[0],[0]
"While lexical transfer from L1 is a known phenomenon in learner language, we hypothesize that its signal is present also in the language of highly competent non-native speakers.",4.1 Hypotheses,[0],[0]
"Mastering the nuances of lexical choice, including subtle contours
of word meaning and the correct context in which words tend to occur, are key factors in advanced language competence.",4.1 Hypotheses,[0],[0]
The L2-Reddit corpus provides a perfect environment for testing this hypothesis.,4.1 Hypotheses,[0],[0]
Our goal is to investigate non-native speakers’ choice of lexical items in English.,4.2 Selection of a focus set of words,[0],[0]
"We address this task by defining a set of English words that have at least one synonym; ideally, we would like the various synonyms to have different etymologies, and in particular, to have different cognates in different language families.",4.2 Selection of a focus set of words,[0],[0]
"English happens to be a particularly good choice for this task, since in spite of its Germanic origins, much of its vocabulary evolved from Romance, as a great number of words were borrowed from Old French during the Norman occupation of Britain in the 11th century.
",4.2 Selection of a focus set of words,[0],[0]
"To trace the etymological history of English words we used Etymological WordNet (EW), a database that contains information about the ancestors of over 100K English words, about 25K of them in contemporary English (de Melo, 2014).",4.2 Selection of a focus set of words,[0],[0]
"For each word recorded in EW, the full path to its root can be reconstructed.",4.2 Selection of a focus set of words,[0],[0]
"Intuitively, an English word with Latin roots may exhibit higher (phonetic and orthographic) proximity to its Romance languages’ counterparts.",4.2 Selection of a focus set of words,[0],[0]
"Conversely, an English word with a ProtoGermanic ancestor may better resemble its equivalents in Germanic languages.
",4.2 Selection of a focus set of words,[0],[0]
"We selected from EW all the nouns, verbs, and adjectives.",4.2 Selection of a focus set of words,[0],[0]
"For each such word w, we identified the synset of w in WordNet, choosing only the first (i.e., most prominent) sense of w (and, in particular, corresponding to the most frequent partof-speech (POS) category of w in the L2-Reddit dataset).",4.2 Selection of a focus set of words,[0],[0]
"Then, we retained only those words that had synonyms, and only those whose synonyms had at least two different etymological paths, i.e., synonyms rooted in different ancestors.",4.2 Selection of a focus set of words,[0],[0]
"For example, we retained the synset {heaven, paradise}, since the former is derived from Proto-Germanic *himin- , while the latter is derived from Greek παράδεισος (via Latin and Old French).
",4.2 Selection of a focus set of words,[0],[0]
"Furthermore, to capture the bias of non-native speakers toward their L1 cognate, it makes sense to focus on a set of easily interchangeable synonyms, e.g., {divide, split}.",4.2 Selection of a focus set of words,[0],[0]
"In contrast, consider an unbal-
anced synset {kiss, buss, osculation}: presumably, the prevalent alternative kiss is likely to be used by all speakers, regardless of their native language.",4.2 Selection of a focus set of words,[0],[0]
"To eliminate such cases, we excluded synsets that were dominated by a single alternative (with a frequency of over 90% in our corpus), compared to other synonymous choices.",4.2 Selection of a focus set of words,[0],[0]
"Table 2 illustrates a few examples of synonym sets with their etymological origins.
",4.2 Selection of a focus set of words,[0],[0]
"Eliminating cultural bias Although our Reddit corpus spans over 80K topical threads and 45K users, posts produced by authors from neighboring countries may carry over markers with similar geographical or cultural flavor.",4.2 Selection of a focus set of words,[0],[0]
"For example, we may expect to encounter soviet more frequently in posts by Russians and Ukrainians, wine in texts of French or Italian authors, and refugees in posts by German users.",4.2 Selection of a focus set of words,[0],[0]
"While they may be typical to a certain population group, such terms are totally unrelated to the phenomenon we address here, and we therefore wish to eliminate them from the focus set of words.
",4.2 Selection of a focus set of words,[0],[0]
"A common way to identify elements that are statistically over-represented in a particular population, compared to another, is log-odds ratio informative",4.2 Selection of a focus set of words,[0],[0]
"Dirichlet prior (Monroe et al., 2008).",4.2 Selection of a focus set of words,[0],[0]
"We employed this approach to discover words that were overused by authors of a certain country, where posts from each country (a category under test) were compared to all the others (the background).",4.2 Selection of a focus set of words,[0],[0]
We used the strict log-odds score of −5 as a threshold for filtering out terms associated with a certain country.7,4.2 Selection of a focus set of words,[0],[0]
"Among the terms eliminated by this procedure were genocide for Armenia, hockey for Canada and independence for the UK.",4.2 Selection of a focus set of words,[0],[0]
"The final focus set of words thus consists of neutral, ubiquitous sets of synonyms, varying in their etymological roots.",4.2 Selection of a focus set of words,[0],[0]
"It comprises 540 synonym sets and 1143 distinct words.
7The threshold was set by preliminary experiments, without any further tuning.",4.2 Selection of a focus set of words,[0],[0]
"We hypothesize (Section 4.1) that L1 effects on lexical choice are so powerful, even with advanced non-native speakers, that it is possible to reconstruct the IE language phylogeny, reflecting historical and areal evolution over thousands of years, from nonnative English only.",4.3 Model,[0],[0]
"We now describe a simple yet effective framework for clustering the Englishes of authors with different L1s, integrating both word frequencies and semantic word representations of the words in our focus set (Section 4.2).",4.3 Model,[0],[0]
"Aiming to learn word representations for the lexical items in our focus set, we want the contextual information to be as free as possible from strong geographical and cultural cues.",4.3.1 Data cleanup and abstraction,[0],[0]
We therefore process the corpus further.,4.3.1 Data cleanup and abstraction,[0],[0]
"First, we identified named entities (NEs) and systematically replaced them by their type.",4.3.1 Data cleanup and abstraction,[0],[0]
"We used the implementation available in the spacy Python package,8 which supports a wide range of entities (e.g., names of people, nationalities, countries, products, events, book titles, etc.), at state-of-the-art accuracy.",4.3.1 Data cleanup and abstraction,[0],[0]
"Like other web-based user generated content, the Reddit corpus does not adhere to strict casing rules, which has detrimental effects on the accuracy of NE identification.",4.3.1 Data cleanup and abstraction,[0],[0]
"To improve the tagging accuracy, we applied a preprocessing step of ‘truecasing’, where each token w was assigned the case (lower, upper, or upper-initial) that maximized the likelihood of the consecutive tri-gram 〈wpre, w, wpost〉 in the Corpus of Contemporary American English (COCA).9 For example, the trigram ‘the us people’ was converted to ‘the US people’, but ‘let us know’ remained unchanged.",4.3.1 Data cleanup and abstraction,[0],[0]
"When a tri-gram was not found in the COCA n-gram corpus,
8https://spacy.io 9https://www.ngrams.info
we employed fallback to unigram probability estimation.",4.3.1 Data cleanup and abstraction,[0],[0]
"Additionally, we replaced all non-English words with the token ‘UNK’; and all web links, subreddit (e.g., r/compling) and user (u/userid) pointers with the ‘URL’ token.10",4.3.1 Data cleanup and abstraction,[0],[0]
Bamman et al. (2014) introduced a model for incorporating contextual information (such as geography) in learning vector representations.,4.3.2 Distance estimation and clustering,[0],[0]
"They proposed a joint model for learning word representations in a situated language, a model that “includes information about a subject (i.e., the speaker), allowing to learn the contours of a word’s meaning that are shaped by the context in which it is uttered”.",4.3.2 Distance estimation and clustering,[0],[0]
"Using a large corpus of tweets, their joint model learned word representations that were sensitive to geographical factors, demonstrating that the usage of wicked in the United States (meaning bad or evil ) differs from that in New England, where it is used as an adverbial intensifier (my boy’s wicked smart).
",4.3.2 Distance estimation and clustering,[0],[0]
We leveraged this model to uncover linguistic variation grounded in the different L1 backgrounds of non-native Reddit speakers.,4.3.2 Distance estimation and clustering,[0],[0]
We used equal-sized random samples of 500K sentences from each country to train a model of vector representations.,4.3.2 Distance estimation and clustering,[0],[0]
"The model comprises representation of every vocabulary item in each of the 31 Englishes; e.g., 31 vectors are generated for the word fatigue, presumably reflecting the subtle divergences of word semantics, rooted in the various L1 backgrounds of the authors.
",4.3.2 Distance estimation and clustering,[0],[0]
"In order to cluster together Englishes of speakers with “similar” L1s, we need a measure of distance between two English texts.",4.3.2 Distance estimation and clustering,[0],[0]
"This measure is based
10The cleaned, abstracted subset of the corpus is also available at http://cl.haifa.ac.il/projects/L2.",4.3.2 Distance estimation and clustering,[0],[0]
"The cleanup code is available at https://github.com/ ellarabi/reddit-l2.
on two constituents: word frequencies and word embeddings.",4.3.2 Distance estimation and clustering,[0],[0]
"Given two English texts originating from different countries, we computed for each word w in our focus set (i) the difference in the frequency of w in the two texts; and (ii) the distance between the vector representations of w in these texts, estimated by cosine similarity of the two corresponding word vectors.",4.3.2 Distance estimation and clustering,[0],[0]
We employed the popular weighted product model to integrate the two arguments.,4.3.2 Distance estimation and clustering,[0],[0]
The word vector component was assigned a higher weight as the frequency of w in the collection increases; this is motivated by the intuition that learning the semantic relationships of a word benefits from vast usage examples.,4.3.2 Distance estimation and clustering,[0],[0]
"We therefore weigh the embedding constituent proportionally to the word’s frequency in the dataset, and assign the complementary weight to the difference of frequencies.
",4.3.2 Distance estimation and clustering,[0],[0]
"Formally, given two English texts ELi and ELj , with Li and Lj native languages, and given a word w in the focus set, let fi and fj denote the frequencies of w in ELi and ELj , respectively.",4.3.2 Distance estimation and clustering,[0],[0]
Let pw be the probability of w in the entire collection.,4.3.2 Distance estimation and clustering,[0],[0]
"We further denote the vector space representation of w in ELi by vi, and the representation of w in ELj by vj .",4.3.2 Distance estimation and clustering,[0],[0]
"Then, the distance between ELi and ELj with respect to the word w is:
Dij(w) = (|fi−fj |)1−pw×(1−cos(vi, vj))pw .",4.3.2 Distance estimation and clustering,[0],[0]
"(1)
The final distance between ELi and ELj is given by averaging Dij over all words in the focus set FS:
Dij = ( ∑
w∈FS Dij(w)) |FS| .
",4.3.2 Distance estimation and clustering,[0],[0]
"Finally, we constructed a symmetric distance matrix (31× 31)M by settingM",4.3.2 Distance estimation and clustering,[0],[0]
"[i, j] = Dij .",4.3.2 Distance estimation and clustering,[0],[0]
"We used
Ward’s hierarchical clustering11 with the Euclidean distance metric to derive a tree from the distance matrix M.
We considered several other weighting alternatives, including assignment of constant weights to the two factors in Equation 1; they all resulted in inferior outcomes.",4.3.2 Distance estimation and clustering,[0],[0]
We also corroborated the relative contribution of the two components by using each of them alone.,4.3.2 Distance estimation and clustering,[0],[0]
"While considering only frequencies resulted in a slightly inferior outcome (see Section 4.5), using word representations alone produced a completely arbitrary result.",4.3.2 Distance estimation and clustering,[0],[0]
The resulting tree is depicted in Figure 1.,4.4 Results,[0],[0]
The reconstructed language typology reveals several interesting observations.,4.4 Results,[0],[0]
"First, and much expectedly, all native English speakers are grouped together into a single, distant sub-tree, implying that similarities exhibited by the lexical choices of native speakers go beyond geographical and cultural differences.",4.4 Results,[0],[0]
"The Englishes of non-native speakers are clustered into three main language families: Germanic, Romance, and Balto-Slavic.",4.4 Results,[0],[0]
"Notably, Spanish-speaking Mexico is clustered with its Romance counterparts.",4.4 Results,[0],[0]
"The firm Balto-Slavic cluster reveals historical relations between languages by generating coherent sub-branches: the Czech Republic and Slovakia, Latvia and Lithuania, as well as the relative proximity of Serbia and Croatia.",4.4 Results,[0],[0]
"In fact, former Yugoslavia is clustered together, except for Bosnia, which is somewhat detached.",4.4 Results,[0],[0]
"Similar close ties can be seen between Austria and Germany, and between Portugal and Spain.
",4.4 Results,[0],[0]
"Another interesting phenomenon is captured by English texts of authors from Romania: their language is assigned to the Balto-Slavic family, implying that the deep-rooted areal and cultural Balkan influences left their traces in the Romanian language, which in turn, is reflected in the English productions of native Romanian authors.",4.4 Results,[0],[0]
"Unfortunately, we cannot explain the location of Iceland.
",4.4 Results,[0],[0]
A geographical view mirroring the language phylogeny is presented in Figure 3.,4.4 Results,[0],[0]
"Flat clusters were obtained from the hierarchy using the scipy fcluster
11https://docs.scipy.org/doc/scipy/ reference/generated/scipy.cluster.",4.4 Results,[0],[0]
"hierarchy.linkage.html
method12 with defaults.
",4.4 Results,[0],[0]
"This outcome, obtained using only lexical semantic properties (word frequencies and word embeddings) of English authored by various non-native speakers, is a strong indication of the power of L1 influence on L2 speakers, even highly fluent ones.",4.4 Results,[0],[0]
"These results are strongly dependent on the choice of focus words: we carefully selected words that on one hand lack any cultural or geographical bias toward one group of non-natives, but on the other hand have synonyms with different etymologies.",4.4 Results,[0],[0]
"As
12https://docs.scipy.org/doc/scipy/ reference/generated/scipy.cluster.",4.4 Results,[0],[0]
"hierarchy.fcluster.html
an additional validation step, we generated a language tree using exactly the same methodology but a different set of focus words.",4.4 Results,[0],[0]
"We randomly sampled 1143 words from the corpus, controlling for country-specific bias but not for the existence of synonyms with different etymologies.",4.4 Results,[0],[0]
"Although some of the intra-family ties were captured (in particular, all native speakers were clustered together), the resulting tree (Figure 2) is far inferior.
",4.4 Results,[0],[0]
"We also conducted an additional experiment, including multilingual Belgium and Switzerland in the set of countries.",4.4 Results,[0],[0]
"While the L1 of speakers cannot be determined for these two countries, presumably Belgium is dominated by Dutch and French, and Switzerland by German and French.",4.4 Results,[0],[0]
"Indeed, both countries were assigned into the Germanic language family in our clustering experiments.",4.4 Results,[0],[0]
To better assess the quality of the reconstructed trees we now provide a quantitative evaluation of the language typologies obtained by the various experiments.,4.5 Evaluation,[0],[0]
"We adopt the evaluation approach of Rabinovich et al. (2017), who introduced a distance metric between two trees, defined as the sum of the square differences between all leaf-pair distances in the two trees.",4.5 Evaluation,[0],[0]
"More specifically, given a tree of N leaves, li, i ∈",4.5 Evaluation,[0],[0]
"[1..N ], the distance between two leaves li, lj in a tree τ , denoted Dτ (li, lj), is defined as the length of the shortest path between li and lj .",4.5 Evaluation,[0],[0]
"The distance Dist(τ, g) between a generated tree τ and the gold tree g is then calculated by summing the square differences between all leaf-pair distances in the two trees:
Dist(τ, g) =",4.5 Evaluation,[0],[0]
"∑
i,j∈[1..N ];i 6=j (Dτ (li, lj)−Dg(li, lj))2.
",4.5 Evaluation,[0],[0]
"We used the Indo-European tree in Glottolog13 as our gold standard, pruning it to contain the set of 31 languages considered in this work.",4.5 Evaluation,[0],[0]
"For the sake of comparison, we also present the distance obtained for a completely random tree, generated by sampling a random distance matrix from the uniform (0, 1) distribution.",4.5 Evaluation,[0],[0]
"The reported random tree evaluation score is averaged over 100 experiments.
",4.5 Evaluation,[0],[0]
Table 3 presents the results.,4.5 Evaluation,[0],[0]
"All distances are normalized to a zero-one scale, where the bounds, zero and one, represent the identical and the most distant tree with respect to the gold standard, respectively.",4.5 Evaluation,[0],[0]
"Much expectedly, the random tree is the worst one, followed closely by the tree reconstructed from a random sample of over 1000 words sampled from the corpus (Figure 2).",4.5 Evaluation,[0],[0]
"The best result is obtained by considering both word frequencies and representations, being only slightly superior to the tree reconstructed using word frequencies alone.",4.5 Evaluation,[0],[0]
"The latter result corroborates the aforementioned observation (Section 4.3.2) and further posits word frequencies as the major factor affecting the shape of the obtained phylogeny.
",4.5 Evaluation,[0],[0]
13http://glottolog.org/,4.5 Evaluation,[0],[0]
The results described in Section 4.4 empirically support the intuition that cognates are one of the factors that shape lexical choice in productions of nonnative authors.,5 Analysis,[0],[0]
"In this section we perform a closer analysis of the data, aiming to capture the subtle yet systematic distortions that help distinguish between English texts of speakers with different L1s.
",5 Analysis,[0],[0]
"Quantitative analysis Given a synonym set s ∈ FS, consisting of words 〈w1, w2, ..., wn〉, and two English texts with two different L1s, ELi and ELj , we computed the counts of the synset words in these texts, and further normalized the counts by the total sum, yielding probabilities.",5 Analysis,[0],[0]
"We denote the probability distribution of a synset s = 〈w1, w2, ..., wn〉 in ELi by:
P si = 〈pi(w1), pi(w2), ..., pi(wn)〉.",5 Analysis,[0],[0]
"The different usage patterns of a synonym set s across two Englishes can then be estimated using the Jensen-Shannon divergence (JSD) between the two probability distributions:
divij(s) = JSD(P s",5 Analysis,[0],[0]
"i , P s j ).",5 Analysis,[0],[0]
"(2)
We expect that “close” L1s will have lower divergence, whereas L1s from different language families will exhibit higher divergences.
",5 Analysis,[0],[0]
"Table 4 presents the top twenty synonym sets for the arbitrarily chosen Germany–Spain country pair, ranked by divergence (Equation 2).",5 Analysis,[0],[0]
"The overuse of hinder by German authors may be attributed to its German behindern cognate, whereas Spanish users’ preference of impede is probably attributable to its Spanish impedir equivalent.",5 Analysis,[0],[0]
"A Spanish cognate for plantation, plantación, possibly explains the clear preference of Spanish native speakers for this alternative, compared to the more popular choice of German authors, grove, which has Germanic etymological origins.
",5 Analysis,[0],[0]
"The {weariness, tiredness, fatigue} synset reveals the preference of Spanish native speakers for fatigue, whose Spanish equivalent fatiga resembles it to a great extent; weariness, however, is slightly more frequent in the texts of German speakers, potentially reflecting its Proto-Germanic *wōrı̄gaz ancestor.",5 Analysis,[0],[0]
"An interesting phenomenon is revealed by the synset {conceivable, imaginable}: while both words have Latin origins, imaginable is more ubiquitous in the English language, rendering it more frequent in texts of German native speakers, compared to the more balanced choice of Spanish authors.",5 Analysis,[0],[0]
"Usage patterns in {overdo, exaggerate} and {inspect, audit, scrutinize} can be attributed to the same phe-
nomenon, where the German equivalent for inspect (inspizieren) resembles its English counterpart despite a different etymological root.
",5 Analysis,[0],[0]
"Usage examples Table 5 presents example sentences written by Reddit authors with French and Italian L1s, further illustrating discrepancies in lexical choice (presumably) stemming from cognate facilitation effects.",5 Analysis,[0],[0]
"The French rapide is a translation equivalent of the English synset {rapid, quick, fast}, but its English rapid cognate is more constrained to contexts of movement or growth, rendering the collocation rapid check somewhat marked.",5 Analysis,[0],[0]
The French noun approbation is more frequent in contemporary French than its English (practically unused) equivalent approbation; this makes its use in English sound unnatural.,5 Analysis,[0],[0]
"In our Reddit corpus, approbation appears 48 times in L1-French texts, compared to 5, 4, and 4 in equal-sized texts by authors from the UK, Ireland and Canada, respectively.",5 Analysis,[0],[0]
"One of the frequent English synonym alternatives {approval, acceptance} would better fit this context.",5 Analysis,[0],[0]
"Finally, while the Italian expression sera precedente
is common, its English equivalent precedent evening is very infrequent, yet it is used in English productions of Italian speakers.",5 Analysis,[0],[0]
We presented an investigation of L1 cognate effects on the productions of advanced non-native Reddit authors.,6 Conclusion,[0],[0]
"The results are accompanied by a large dataset of native and non-native English speakers, annotated for author country (and, presumably, also L1) at the sentence level.
",6 Conclusion,[0],[0]
Several open questions remain for future research.,6 Conclusion,[0],[0]
"From a theoretical perspective, we would like to extend this work by studying whether the tendency to choose an English cognate is more powerful in L1s with both phonetic and orthographic similarity to English (Roman script) than in L1s with phonetic similarity only (e.g., Cyrillic script).",6 Conclusion,[0],[0]
"We also plan to more carefully investigate productions of speakers from multilingual countries, like Belgium and Switzerland.",6 Conclusion,[0],[0]
"Another extension of this work may broaden the analysis to include additional language families.
",6 Conclusion,[0],[0]
There are also various potential practical applications to this work.,6 Conclusion,[0],[0]
"First, we plan to exploit the potential benefits of our findings to the task of native language identification of (highly advanced) non-native authors, in various domains.",6 Conclusion,[0],[0]
"Second, our results will be instrumental for personalization of language learning applications, based on the L1 background of the learner.",6 Conclusion,[0],[0]
"For example, error correction systems can be enhanced with the native language of the author to offer root cause analysis of subtle discrepancies in the usage of lexical items, considering both their frequencies and context.",6 Conclusion,[0],[0]
"Given the L1 of the target audience, lexical simplification systems can also benefit from cognate cues, e.g., by providing an informed choice of potentially challenging candidates for substitution with a simplified alternative.",6 Conclusion,[0],[0]
We leave such applications for future research.,6 Conclusion,[0],[0]
This work was partially supported by the National Science Foundation through award IIS-1526745.,Acknowledgments,[0],[0]
We would like to thank Anat Prior and Steffen Eger for valuable suggestions.,Acknowledgments,[0],[0]
We are also grateful to Sivan Rabinovich for much advise and helpful comments.,Acknowledgments,[0],[0]
"Finally, we are thankful to our action editor, Ivan Titov, and three anonymous reviewers for their constructive feedback.",Acknowledgments,[0],[0]
We present a computational analysis of cognate effects on the spontaneous linguistic productions of advanced non-native speakers.,abstractText,[0],[0]
"Introducing a large corpus of highly competent non-native English speakers, and using a set of carefully selected lexical items, we show that the lexical choices of non-natives are affected by cognates in their native language.",abstractText,[0],[0]
This effect is so powerful that we are able to reconstruct the phylogenetic language tree of the Indo-European language family solely from the frequencies of specific lexical items in the English of authors with various native languages.,abstractText,[0],[0]
"We quantitatively analyze nonnative lexical choice, highlighting cognate facilitation as one of the important phenomena shaping the language of non-native speakers.",abstractText,[0],[0]
Native Language Cognate Effects on Second Language Lexical Choice,title,[0],[0]
"When humans reason about the world, we tend to formulate a variety of hypotheses and counterfactuals, then test them in turn by physical or thought experiments.",1 Introduction,[0],[0]
"The philosopher Epicurus first formalized this idea in his Principle of Multiple Explanations: if several theories are consistent with the observed data, retain them all until more data is observed.",1 Introduction,[0],[0]
"In this paper, we argue that the same principle can be applied to machine comprehension of natural language.",1 Introduction,[0],[0]
"We propose a deep, end-to-end, neural comprehension model that we call the EpiReader.
",1 Introduction,[0],[0]
"Comprehension of natural language by machines, at a near-human level, is a prerequisite for an extremely broad class of useful applications of artificial intelligence.",1 Introduction,[0],[0]
"Indeed, most human knowledge is collected in the natural language of text.",1 Introduction,[0],[0]
Machine comprehension (MC) has therefore garnered significant attention from the machine learning research community.,1 Introduction,[0],[0]
"Machine comprehension is typically evaluated by posing a set of questions based on a supporting text passage, then scoring a system’s answers to those questions.",1 Introduction,[0],[0]
We all took similar tests in school.,1 Introduction,[0],[0]
"Such tests are objectively gradable and may assess a range of abilities, from basic understanding to causal reasoning to inference (Richardson et al., 2013).
",1 Introduction,[0],[0]
"In the past year, two large-scale MC datasets have been released: the CNN/Daily Mail corpus, consisting of news articles from those outlets (Hermann et al., 2015), and the Children’s Book Test (CBT), consisting of short excerpts from books available through Project Gutenberg (Hill et al., 2015).",1 Introduction,[0],[0]
The size of these datasets (on the order of 105 distinct questions) makes them amenable to data-intensive deep learning techniques.,1 Introduction,[0],[0]
"Both corpora use Clozestyle questions (Taylor, 1953), which are formulated by replacing a word or phrase in a given sentence with a placeholder token.",1 Introduction,[0],[0]
"The task is then to find the answer that “fills in the blank”.
",1 Introduction,[0],[0]
"In tandem with these corpora, a host of neural machine comprehension models has been developed (Weston et al., 2014; Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016).",1 Introduction,[0],[0]
"We compare the EpiReader to these earlier models through training and evaluation on the CNN and CBT
ar X
iv :1
60 6.
02 27
0v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
0 Ju
n 20
datasets.1
The EpiReader factors into two components.",1 Introduction,[0],[0]
The first component extracts a small set of potential answers based on a shallow comparison of the question with its supporting text; we call this the Extractor.,1 Introduction,[0],[0]
The second component reranks the proposed answers based on deeper semantic comparisons with the text; we call this the Reasoner.,1 Introduction,[0],[0]
We can summarize this process as Extract → Hypothesize → Test2.,1 Introduction,[0],[0]
"The semantic comparisons implemented by the Reasoner are based on the concept of recognizing textual entailment (RTE) (Dagan et al., 2006), also known as natural language inference.",1 Introduction,[0],[0]
This process is computationally demanding.,1 Introduction,[0],[0]
"Thus, the Extractor serves the important function of filtering a large set of potential answers down to a small, tractable set of likely candidates for more thorough testing.
",1 Introduction,[0],[0]
"The Extractor follows the form of a pointer network (Vinyals et al., 2015), and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question.",1 Introduction,[0],[0]
"This approach was used (on its own) for question answering with the Attention Sum Reader (Kadlec et al., 2016).",1 Introduction,[0],[0]
The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness.,1 Introduction,[0],[0]
"The Reasoner forms hypotheses by inserting the candidate answers into the question, then estimates the concordance of each hypothesis with each sentence in the supporting text.",1 Introduction,[0],[0]
"We use these estimates as a measure of the evidence for a hypothesis, and aggregate evidence over all sentences.",1 Introduction,[0],[0]
"In the end, we combine the Reasoner’s evidence with the Extractor’s probability estimates to produce a final ranking of the answer candidates.
",1 Introduction,[0],[0]
This paper is organized as follows.,1 Introduction,[0],[0]
In Section 2 we formally define the problem to be solved and give some background on the datasets used in our tests.,1 Introduction,[0],[0]
"In Section 3 we describe the EpiReader, focusing on its two components and how they combine.",1 Introduction,[0],[0]
"Section 4 discusses related work, and Section 5 details our experimental results and analysis.",1 Introduction,[0],[0]
"We conclude in Section 6.
1The CNN and Daily Mail datasets were released together and have the same form.",1 Introduction,[0],[0]
"The Daily Mail dataset is significantly larger, and our tests with this data are still in progress.
",1 Introduction,[0],[0]
"2The Extractor performs extraction, while the Reasoner both hypothesizes and tests.",1 Introduction,[0],[0]
The task of the EpiReader is to answer a Cloze-style question by reading and comprehending a supporting passage of text.,"2 Problem definition, notation, datasets",[0],[0]
"The training and evaluation data consist of tuples (Q, T , a∗, A), where Q is the question (a sequence of words {q1, ...q|Q|}), T is the text (a sequence of words {t1, ..., t|T |}), A is a set of possible answers {a1, ..., a|A|}, and a∗ ∈","2 Problem definition, notation, datasets",[0],[0]
A is the correct answer.,"2 Problem definition, notation, datasets",[0],[0]
"All words come from a vocabulary V , and A ⊂ T .","2 Problem definition, notation, datasets",[0],[0]
"In each question, there is a placeholder token indicating the missing word to be filled in.","2 Problem definition, notation, datasets",[0],[0]
CNN,2.1 Datasets,[0],[0]
This corpus is built using articles scraped from the CNN website.,2.1 Datasets,[0],[0]
"The articles themselves form the text passages, and questions are generated synthetically from short summary statements that accompany each article.",2.1 Datasets,[0],[0]
These summary points are (presumably) written by human authors.,2.1 Datasets,[0],[0]
Each question is created by replacing a named entity in a summary point with a placeholder token.,2.1 Datasets,[0],[0]
"All named entities in the articles and questions are replaced with anonymized tokens that are shuffled for each (Q, T ) pair.",2.1 Datasets,[0],[0]
"This forces the model to rely only on the text, rather than learning world knowledge about the entities during training.",2.1 Datasets,[0],[0]
"The CNN corpus (henceforth CNN) was presented by Hermann et al. (2015).
",2.1 Datasets,[0],[0]
Children’s Book Test,2.1 Datasets,[0],[0]
"This corpus is constructed similarly to CNN, but from children’s books available through Project Gutenberg.",2.1 Datasets,[0],[0]
"Rather than articles, the text passages come from book excerpts of 20 sentences.",2.1 Datasets,[0],[0]
"Since no summaries are provided, a question is generated by replacing a single word in the next (i.e. 21st) sentence.",2.1 Datasets,[0],[0]
"The corpus distinguishes questions based on the type of word that is replaced: named entity, common noun, verb, or preposition.",2.1 Datasets,[0],[0]
"Like Kadlec et al. (2016), we focus only on the first two classes since Hill et al. (2015) showed that standard LSTM language models already achieve humanlevel performance on the latter two.",2.1 Datasets,[0],[0]
"Unlike in the CNN corpora, named entities are not anonymized and shuffled in the Children’s Book Test (CBT).",2.1 Datasets,[0],[0]
"The CBT was presented by Hill et al. (2015).
",2.1 Datasets,[0],[0]
"Due to the construction of questions in each corpus, CNN and CBT assess different aspects of machine comprehension.",2.1 Datasets,[0],[0]
"The summary points of CNN are
condensed paraphrasings of information from the text, so determining the correct answer relies more on recognizing textual entailment.",2.1 Datasets,[0],[0]
"On the other hand, a CBT question, generated from a sentence which continues the text passage (rather than summarizes it), is more about making a prediction based on context.",2.1 Datasets,[0],[0]
The EpiReader explicitly leverages the observation that the answer to a question is often a word or phrase from the related text passage.,3.1 Overview and intuition,[0],[0]
This condition holds for the CNN and CBT datasets.,3.1 Overview and intuition,[0],[0]
"EpiReader’s first module, the Extractor, can thus select a small set of candidate answers by pointing to their locations in the supporting passage.",3.1 Overview and intuition,[0],[0]
"This mechanism is detailed in Section 3.2, and was used previously by the Attention Sum Reader (Kadlec et al., 2016).",3.1 Overview and intuition,[0],[0]
"Pointing to candidate answers removes the need to apply a softmax over the entire vocabulary as in Weston et al. (2014), which is computationally more costly and uses less-direct information about the context of a predicted answer in the supporting text.
EpiReader’s second module, the Reasoner, begins by formulating hypotheses using the extracted answer candidates.",3.1 Overview and intuition,[0],[0]
It generates each hypothesis by replacing the placeholder token in the question with an answer candidate.,3.1 Overview and intuition,[0],[0]
"Cloze-style questions are ideally-suited to this process, because inserting the correct answer at the placeholder location produces a well-formed, grammatical statement.",3.1 Overview and intuition,[0],[0]
"Thus, the correct hypothesis will “make sense” to a language model.
",3.1 Overview and intuition,[0],[0]
The Reasoner then tests each hypothesis individually.,3.1 Overview and intuition,[0],[0]
"It compares a hypothesis to the text, split into sentences, to measure textual entailment, and then aggregates entailment over all sentences.",3.1 Overview and intuition,[0],[0]
This computation uses a pair of convolutional encoder networks followed by a recurrent neural network.,3.1 Overview and intuition,[0],[0]
The convolutional encoders generate abstract representations of the hypothesis and each text sentence; the recurrent network estimates and aggregates entailment.,3.1 Overview and intuition,[0],[0]
This is described formally in Section 3.3.,3.1 Overview and intuition,[0],[0]
"The end-toend EpiReader model, combining the Extractor and Reasoner modules, is depicted in Figure 1.
",3.1 Overview and intuition,[0],[0]
"Throughout our model, words will be represented with trainable embeddings (Bengio et al., 2000).",3.1 Overview and intuition,[0],[0]
"We represent these embeddings using a matrix W ∈
RD×|V |, where D is the embedding dimension and |V",3.1 Overview and intuition,[0],[0]
| is the vocabulary size.,3.1 Overview and intuition,[0],[0]
"The Extractor is a Pointer Network (Vinyals et al., 2015).",3.2 The Extractor,[0],[0]
"It uses a pair of bidirectional recurrent neural networks, f(θT ,T) and g(θQ,Q), to encode the text passage and the question.",3.2 The Extractor,[0],[0]
"θT represents the parameters of the text encoder, and T ∈ RD×N is a matrix representation of the text (comprising N words), whose columns are individual word embeddings ti.",3.2 The Extractor,[0],[0]
"Likewise, θQ represents the parameters of the question encoder, and Q ∈ RD×NQ is a matrix representation of the question (comprisingNQ words), whose columns are individual word embeddings qj .
",3.2 The Extractor,[0],[0]
"We use a recurrent neural network with gated recurrent units (GRU) (Bahdanau et al., 2014) to scan over the columns (i.e. word embeddings) of the input matrix.",3.2 The Extractor,[0],[0]
"We selected the GRU because it is computationally simpler than Long Short-Term Memory (Hochreiter and Schmidhuber, 1997), while still avoiding the problem of vanishing/exploding gradients often encountered when training recurrent networks.
",3.2 The Extractor,[0],[0]
The GRU’s hidden state gives a representation of the ith word conditioned on preceding words.,3.2 The Extractor,[0],[0]
"To include context from proceeding words, we run a second GRU over T in the reverse direction.",3.2 The Extractor,[0],[0]
We refer to the combination as a biGRU.,3.2 The Extractor,[0],[0]
"At each step the biGRU outputs two d-dimensional encoding vectors, one for the forward direction and one for the backward direction.",3.2 The Extractor,[0],[0]
We concatenate these to yield a vector f(ti) ∈ R2d.,3.2 The Extractor,[0],[0]
"The question biGRU is similar, but we get a single-vector representation of the question by concatenating the final forward state with the initial backward state, which we denote g(Q) ∈ R2d.
",3.2 The Extractor,[0],[0]
"As in Kadlec et al. (2016), we model the probability that the ith word in text T answers question Q using
si ∝ exp(f(ti) · g(Q)), (1)
which takes the inner product of the text and question representations followed by a softmax.",3.2 The Extractor,[0],[0]
In many cases unique words repeat in a text.,3.2 The Extractor,[0],[0]
"Therefore, we compute the total probability that word w is the correct answer using a sum:
P (w | T ,Q) =",3.2 The Extractor,[0],[0]
"∑
i: ti=w
si.",3.2 The Extractor,[0],[0]
"(2)
This probability is evaluated for each unique word in T .",3.2 The Extractor,[0],[0]
"Finally, the Extractor outputs the set {p1, ..., pK} of the K highest word probabilities from 2, along with the corresponding set of K most probable answer words {â1, ..., âK}.",3.2 The Extractor,[0],[0]
"The indicial selection involved in gathering {â1, ..., âK} is not a smooth operation.",3.3 The Reasoner,[0],[0]
"To construct an end-to-end differentiable model, we bypass this by propagating the probability estimates of the Extractor directly through the Reasoner.
",3.3 The Reasoner,[0],[0]
"The Reasoner begins by inserting the answer candidates, which are single words or phrases, into the
question sequence Q at the placeholder location.",3.3 The Reasoner,[0],[0]
"This forms K hypotheses {H1, ...,HK}.",3.3 The Reasoner,[0],[0]
"At this point, we consider each hypothesis to have probability p(Hk)",3.3 The Reasoner,[0],[0]
"≈ pk, as estimated by the Extractor.",3.3 The Reasoner,[0],[0]
"The Reasoner updates and refines this estimate.
",3.3 The Reasoner,[0],[0]
"The hypotheses represent new information in some sense—they are statements we have constructed, albeit from words already present in the question and text passage.",3.3 The Reasoner,[0],[0]
The Reasoner estimates entailment between the statements Hk and the passage T .,3.3 The Reasoner,[0],[0]
"We denote these estimates using ek = F (Hk, T ), with F to be defined.",3.3 The Reasoner,[0],[0]
"We start by reorganizing T into a sequence of Ns sentences: T = {t1, . . .",3.3 The Reasoner,[0],[0]
", tN} → {S1, . . .",3.3 The Reasoner,[0],[0]
",SNs}, where Si is a sequence of words.
",3.3 The Reasoner,[0],[0]
"For each hypothesis and each sentence of the text, Reasoner input consists of two matrices: Si ∈ RD×|Si|, whose columns are the embedding vectors for each word of sentence Si, and Hk ∈ RD×|Hk|, whose columns are the embedding vectors for each word in the hypothesisHk.",3.3 The Reasoner,[0],[0]
"The embedding vectors themselves come from matrix W, as before.
",3.3 The Reasoner,[0],[0]
These matrices feed into a convolutional architecture based on that of Severyn and Moschitti (2016).,3.3 The Reasoner,[0],[0]
The architecture first augments Si with matrix M ∈ R2×|Si|.,3.3 The Reasoner,[0],[0]
"The first row of M contains the inner product of each word embedding in the sentence with the candidate answer embedding, and the second row contains the maximum inner product of each sentence word embedding with any word embedding in the question.",3.3 The Reasoner,[0],[0]
"These word-matching features were inspired by similar approaches in Wang and Jiang (2015) and Trischler et al. (2016), where they were shown to improve entailment estimates.
",3.3 The Reasoner,[0],[0]
"The augmented Si is then convolved with a bank of filters FS ∈ R(D+2)×m, while Hk is convolved with filters FH ∈ RD×m, where m is the convolutional filter width.",3.3 The Reasoner,[0],[0]
We add a bias term and apply a nonlinearity (we use a ReLU) following the convolution.,3.3 The Reasoner,[0],[0]
"Maxpooling over the sequences then yields two vectors: the representation of the text sentence, rSi ∈ RNF , and the representation of the hypothesis, rHk ∈ RNF , where NF is the number of filters.
",3.3 The Reasoner,[0],[0]
"We then compute a scalar similarity score between these vector representations using the bilinear form
ς = rTSiRrHk , (3)
where R ∈ RNF×NF is a matrix of trainable parameters.",3.3 The Reasoner,[0],[0]
"We then concatenate the similarity score with the sentence and hypothesis representations to get a vector, xik =",3.3 The Reasoner,[0],[0]
[ς; rSi ;,3.3 The Reasoner,[0],[0]
"rHk ]
T .",3.3 The Reasoner,[0],[0]
There are more powerful models of textual entailment that could have been used in place of this convolutional architecture.,3.3 The Reasoner,[0],[0]
"We adopted the approach of Severyn and Moschitti (2016) for computational efficiency.
",3.3 The Reasoner,[0],[0]
"The resulting sequence of Ns vectors feeds into yet another GRU for synthesis, of hidden dimension dS .",3.3 The Reasoner,[0],[0]
"Intuitively, it is often the case that evidence for a particular hypothesis is distributed over several sentences.",3.3 The Reasoner,[0],[0]
"For instance, if we hypothesize that the football is in the park, perhaps it is because one sentence tells us that Sam picked up the football and a
later one tells us that Sam ran to the park.3",3.3 The Reasoner,[0],[0]
"The Reasoner synthesizes distributed information by running a GRU network over xik, where i indexes sentences and represents the step dimension.4",3.3 The Reasoner,[0],[0]
"The final hidden state of the GRU is fed through a fully-connected layer, yielding a single scalar yk.",3.3 The Reasoner,[0],[0]
This value represents the collected evidence forHk based on the text.,3.3 The Reasoner,[0],[0]
"In practice, the Reasoner processes all K hypotheses in parallel and the estimated entailment of each is normalized by a softmax, ek ∝ exp(yk).
",3.3 The Reasoner,[0],[0]
The reranking step performed by the Reasoner helps mitigate a significant weakness of most existing attention mechanisms.,3.3 The Reasoner,[0],[0]
"Specifically, these mechanisms blend representations of all possible outcomes together using “soft” attention, rather than considering them discretely using “hard” attention.",3.3 The Reasoner,[0],[0]
"This is like exploring a maze by generating an average path out of the several before you, and then attempting to follow it by walking through a wall.",3.3 The Reasoner,[0],[0]
"Examining possibilities individually, as in the Reasoner module, is more natural.",3.3 The Reasoner,[0],[0]
"Finally, we combine the evidence from the Reasoner with the probability from the Extractor.",3.4 Combining components,[0],[0]
"We compute the output probability of each hypothesis, πk, according to the product
πk ∝",3.4 Combining components,[0],[0]
"ekpk, (4)
whereby the evidence of the Reasoner can be interpreted as a correction to the Extractor probabilities, applied as an additive shift in log-space.",3.4 Combining components,[0],[0]
"We experimented with other combinations of the Extractor and Reasoner, but we found the multiplicative approach to yield the best performance.
",3.4 Combining components,[0],[0]
"After combining results from the Extractor and Reasoner to get the probabilities πk described in Eq. 4, we optimize the parameters of the full EpiReader to minimize a cost comprising two terms, LE and LR.",3.4 Combining components,[0],[0]
"The first term is a standard negative loglikelihood objective, which encourages the Extractor to rate the correct answer above other answers.",3.4 Combining components,[0],[0]
"This
3This example is characteristic of the bAbI dataset (Weston et al., 2015).
",3.4 Combining components,[0],[0]
"4Note a benefit of forming the hypothesis: it renders bidirectional aggregation unnecessary, since knowing both the question and the putative answer ""closes the loop"" the same way that a bidirectional encoding would.
is the same loss term used in Kadlec et al. (2016).",3.4 Combining components,[0],[0]
"This loss is given by:
LE = E (Q,T ,a∗,A)
",3.4 Combining components,[0],[0]
"[− logP (a∗ | T ,Q)] , (5)
where P (a∗ | T ,Q) is as defined in Eq. 2, and a∗ denotes the true answer.",3.4 Combining components,[0],[0]
The second term is a marginbased loss on the end-to-end probabilities πk.,3.4 Combining components,[0],[0]
We define π∗ as the probability πk corresponding to the true answer word a∗.,3.4 Combining components,[0],[0]
"This term is given by:
LR = E (Q,T ,a∗,A)  ∑ âi∈{â1,...,âK}\a∗",3.4 Combining components,[0],[0]
"[γ − π∗ + πâi ]+  , (6) where γ is a margin hyperparameter, {â1, ..., âK} is the set of K answers proposed by the Extractor, and [x]+ indicates truncating x to be non-negative.",3.4 Combining components,[0],[0]
"Intuitively, this loss says that we want the end-to-end probability π∗ for the correct answer to be at least γ larger than the probability πâi for any other answer proposed by the Extractor.",3.4 Combining components,[0],[0]
"During training, the correct answer is occasionally missed by the Extractor, especially in early epochs.",3.4 Combining components,[0],[0]
We counter this issue by forcing the correct answer into the top K set while training.,3.4 Combining components,[0],[0]
"When evaluating the model on validation and test examples we rely fully on the top K answers proposed by the Extractor.
To get the final loss term LER, minus `2 regularization terms on the model parameters, we take a weighted combination of LE and LR:
LER = LE + λLR, (7)
where λ is a hyperparameter for weighting the relative contribution of the Extractor and Reasoner losses.",3.4 Combining components,[0],[0]
"In practice, we found that λ should be fairly large (e.g. 10 < λ < 100).",3.4 Combining components,[0],[0]
"Empirically, we observed that the output probabilities from the Extractor often peak and saturate the first softmax; hence, the Extractor term can come to dominate the Reasoner term without the weight λ (we discuss the Extractor’s propensity to overfit in Section 5).",3.4 Combining components,[0],[0]
The Impatient and Attentive Reader models were proposed by Hermann et al. (2015).,4 Related Work,[0],[0]
"The Attentive Reader applies bidirectional recurrent encoders to the
question and supporting text.",4 Related Work,[0],[0]
"It then uses the attention mechanism described in Bahdanau et al. (2014) to compute a fixed-length representation of the text based on a weighted sum of the text encoder’s output, guided by comparing the question representation to each location in the text.",4 Related Work,[0],[0]
"Finally, a joint representation of the question and supporting text is formed by passing their separate representations through a feedforward MLP and an answer is selected by comparing the MLP output to a representation of each possible answer.",4 Related Work,[0],[0]
"The Impatient Reader operates similarly, but computes attention over the text after processing each consecutive word of the question.",4 Related Work,[0],[0]
"The two models achieved similar performance on the CNN and Daily Mail datasets.
",4 Related Work,[0],[0]
Memory Networks were first proposed by Weston et al. (2014) and later applied to machine comprehension by Hill et al. (2015).,4 Related Work,[0],[0]
"This model builds fixed-length representations of the question and of windows of text surrounding each candidate answer, then uses a weighted-sum attention mechanism to combine the window representations.",4 Related Work,[0],[0]
"As in the previous Readers, the combined window representation is then compared with each possible answer to form a prediction about the best answer.",4 Related Work,[0],[0]
What distinguishes Memory Networks is how they construct the question and text window representations.,4 Related Work,[0],[0]
"Rather than a recurrent network, they use a specially-designed, trainable transformation of the word embeddings.
",4 Related Work,[0],[0]
"Most of the details for the very recent AS Reader are provided in the description of our Extractor module in Section 3.2, so we do not summarize it further here.",4 Related Work,[0],[0]
"This model (Kadlec et al., 2016) set the previous state-of-the-art on the CBT dataset.
",4 Related Work,[0],[0]
"During the write-up of this paper, another very recent model came to our attention.",4 Related Work,[0],[0]
"Chen et al. (2016) propose using a bilinear term instead of a tanh layer to compute the attention between question and passage words, and also uses the attended word encodings for direct, pointer-style prediction as in Kadlec et al. (2016).",4 Related Work,[0],[0]
This model set the previous state-of-theart on the CNN dataset.,4 Related Work,[0],[0]
"However, this model used embedding vectors pretrained on a large external corpus (Pennington et al., 2014).
",4 Related Work,[0],[0]
The EpiReader borrows ideas from other models as well.,4 Related Work,[0],[0]
The Reasoner’s convolutional architecture is based on Severyn and Moschitti (2016) and Kalchbrenner et al. (2014).,4 Related Work,[0],[0]
"Our use of word-level match-
ing was inspired by the Parallel-Hierarchical model of Trischler et al. (2016) and the natural language inference model of Wang and Jiang (2015).",4 Related Work,[0],[0]
"Finally, the idea of formulating and testing hypotheses for question-answering was used to great effect in IBM’s DeepQA system for Jeopardy!",4 Related Work,[0],[0]
"(Ferrucci et al., 2010), although that was a more traditional information retrieval pipeline rather than an end-to-end neural model.",4 Related Work,[0],[0]
"To train our model we used stochastic gradient descent with the ADAM optimizer (Kingma and Ba, 2014), with an initial learning rate of 0.001.",5.1 Implementation and training details,[0],[0]
"The word embeddings were initialized randomly, drawing from the uniform distribution over",5.1 Implementation and training details,[0],[0]
"[−0.05, 0.05).",5.1 Implementation and training details,[0],[0]
"We used batches of 32 examples, and early stopping with a patience of 2 epochs.",5.1 Implementation and training details,[0],[0]
"Our model was implement in Theano (Bergstra et al., 2010) using the Keras framework (Chollet, 2015).
",5.1 Implementation and training details,[0],[0]
The results presented below for the EpiReader were obtained by searching over a small grid of hyperparameter settings.,5.1 Implementation and training details,[0],[0]
"We selected the model that, on each dataset, maximized accuracy on the validation set, then evaluated it on the test set.",5.1 Implementation and training details,[0],[0]
We record the best settings for each dataset in Table 1.,5.1 Implementation and training details,[0],[0]
"As has been done previously, we train separate models on CBT’s named entity (CBT-NE) and common noun (CBTCN) splits.",5.1 Implementation and training details,[0],[0]
"All our models used `2-regularization at 0.001, λ = 50, and γ = 0.04.",5.1 Implementation and training details,[0],[0]
We did not use dropout but plan to investigate its effect in the future.,5.1 Implementation and training details,[0],[0]
Hill et al. (2015) and Kadlec et al. (2016) also present results for ensembles of their models.,5.1 Implementation and training details,[0],[0]
"Time did not
permit us to generate an ensemble of EpiReaders on the CNN dataset so we omit those measures; however, EpiReader ensembles (of seven models) demonstrated improved performance on the CBT dataset.",5.1 Implementation and training details,[0],[0]
"In Table 2, we compare the performance of the EpiReader against that of several baselines, on the validation and test sets of the CBT and CNN corpora.",5.2 Results,[0],[0]
We measure EpiReader performance at the output of both the Extractor and the Reasoner.,5.2 Results,[0],[0]
The EpiReader achieves state-of-the-art performance across the board for both datasets.,5.2 Results,[0],[0]
"On CNN, we score 2.2% higher on test than the best previous model of Chen et al. (2016).",5.2 Results,[0],[0]
"Interestingly, an analysis of the CNN dataset by Chen et al. (2016) suggests that approximately 25% of the test examples contain coreference errors or questions which are “ambiguous/hard” even for a human analyst.",5.2 Results,[0],[0]
"If this estimate is accurate, then the EpiReader, achieving an absolute test accuracy of 74.0%, is operating close to expected human performance.",5.2 Results,[0],[0]
"On the other hand, ambiguity is unlikely to be distributed evenly over entities, so a good model should be able to perform at better-thanchance levels even on questions where the correct answer is uncertain.",5.2 Results,[0],[0]
"If, on the 25% of “noisy” questions, the model can shift its hit rate from, e.g., 1/10 to 1/3, then there is still a fair amount of performance to gain.
",5.2 Results,[0],[0]
On CBT-CN our single model scores 4.0% higher than the previous best of the AS Reader.,5.2 Results,[0],[0]
The improvement on CBT-NE is more modest at 1.1%.,5.2 Results,[0],[0]
"Looking more closely at our CBT-NE results, we found that the validation and test accuracies had relatively high variance even in late epochs of training.",5.2 Results,[0],[0]
"We discovered that many of the validation and test questions were asked about the same named entity, which may explain this issue.",5.2 Results,[0],[0]
"Aside from achieving state-of-the-art results at its final output, the EpiReader framework gives a boost to its Extractor component through the joint training process.",5.3 Analysis,[0],[0]
"In Table 2, we provide accuracy scores evaluated at the output of the Extractor.",5.3 Analysis,[0],[0]
"These are all higher than the analogous scores reported for the AS Reader, which we verified ourselves to within negligible difference.",5.3 Analysis,[0],[0]
"Based on our own work with that
Table 2: Model comparison on the CBT and CNN datasets.",5.3 Analysis,[0],[0]
"Results marked with 1 are from Hill et al. (2016), those marked with 2 are from Kadlec et al. (2016), those marked with 3 are from Hermann et al. (2015), and those marked with 4 are from Chen et al. (2016).
",5.3 Analysis,[0],[0]
"CBT-NE CBT-CN
Model valid test valid test
Humans (context + query) 1 - 81.6 - 81.6
LSTMs (context + query) 1 51.2 41.8 62.6 56.0
MemNNs 1 70.4 66.6 64.2 63.0
AS Reader 2 73.8 68.6 68.8 63.4
EpiReader Extractor 73.2 69.4 69.9 66.7 EpiReader 75.3 69.7 71.5 67.4
AS Reader (ensemble) 2 74.5 70.6 71.1 68.9 EpiReader (ensemble) 76.6 71.8 73.6 70.6
CNN
Model valid test
Deep LSTM Reader 3 55.0 57.0 Attentive Reader 3 61.6 63.0 Impatient Reader 3 61.8 63.8
MemNNs 1 63.4 66.8
AS Reader 2 68.6 69.5
Stanford AR 4 72.4 72.4
EpiReader Extractor 71.8 72.0 EpiReader 73.4 74.0
model, we found it to overfit the training set rapidly and significantly, achieving training accuracy scores upwards of 98% after only 2 epochs.",5.3 Analysis,[0],[0]
"We suspect that the Reasoner module had a regularizing effect on the Extractor, but leave the verification for future work.",5.3 Analysis,[0],[0]
An analysis by Kadlec et al. (2016) indicates that the trained AS Reader includes the correct answer among its five most probable candidates on approximately 95% of test examples for both datasets.,5.3 Analysis,[0],[0]
"We verified that our Extractor achieved a similar rate, and of course this is vital for performance of the full system, since the Reasoner cannot recover when the correct answer is not among its inputs.
",5.3 Analysis,[0.9652886388301825],['Our attempts demonstrate that the learning of the large scale neural network systems is still not good enough.']
Our results demonstrate that the Reasoner often corrects erroneous answers from the Extractor.,5.3 Analysis,[0],[0]
Figure 2 gives an example of this correction.,5.3 Analysis,[0],[0]
"In the text passage, from CBT-NE, Mr. Blacksnake is pursuing Mr. Toad, presumably to eat him.",5.3 Analysis,[0],[0]
"The dialogue in the question sentence refers to both: Mr. Toad is its subject, referred to by the pronoun “he”, and Mr. Blacksnake is its object.",5.3 Analysis,[0],[0]
"In the preceding sentences, it is clear (to a human) that Jimmy is worried about Mr. Toad and his potential encounter with Mr. Blacksnake.",5.3 Analysis,[0],[0]
"The Extractor, however, points most strongly to “Toad”, possibly because he has been referred to most recently.",5.3 Analysis,[0],[0]
The Reasoner corrects this error and selects “Blacksnake” as the answer.,5.3 Analysis,[0],[0]
This relies on a deeper understanding of the text.,5.3 Analysis,[0],[0]
"The named entity can, in this case, be inferred through an alternation of the entities most recently referred to.",5.3 Analysis,[0],[0]
"This kind alternation is typical of dialogues, when two actors
Mr. Blacksnake grinned and started after him, not very fast because he knew that he wouldn't have to run very fast to catch old Mr. Toad, and he thought the exercise would do him good.",5.3 Analysis,[0],[0]
"…
“Still, the green meadows wouldn't be quite the same without old Mr. Toad.",5.3 Analysis,[0],[0]
I should miss him if anything happened to him.,5.3 Analysis,[0],[0]
"I suppose it would be partly my fault, too, for if I hadn't pulled over that piece of bark, he probably would have stayed there the rest of the day and been safe.”
",5.3 Analysis,[0],[0]
QUESTION:,5.3 Analysis,[0],[0]
"“Maybe he won't meet Mr. XXXXX,” said a little voice inside of Jimmy.
",5.3 Analysis,[0],[0]
EXTRACTOR:,5.3 Analysis,[0],[0]
"Toad REASONER: Blacksnake
1.
18.
21.
19. 20.
",5.3 Analysis,[0],[0]
"Figure 2: An abridged example from CBT-NE demonstrating corrective reranking by the Reasoner.
interact in turns.",5.3 Analysis,[0],[0]
The Reasoner can capture this behavior because it examines sentences in sequence.,5.3 Analysis,[0],[0]
"In this article we presented the novel EpiReader framework for machine comprehension, and evaluated it on two large, complex datasets: CNN and CBT.",6 Conclusion,[0],[0]
"Our model achieves state-of-the-art results on these corpora, outperforming all previous approaches.",6 Conclusion,[0],[0]
"In future work, we plan to augment our framework with a more powerful model for natural language inference, and explore the effect of pretraining such a model specifically on an inference task.",6 Conclusion,[0],[0]
We also plan to try simplifying the model by reusing the Extractor’s biGRU encodings in the Reasoner.,6 Conclusion,[0],[0]
"We present the EpiReader, a novel model for machine comprehension of text.",abstractText,[0],[0]
"Machine comprehension of unstructured, real-world text is a major research goal for natural language processing.",abstractText,[0],[0]
"Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model’s response to the questions.",abstractText,[0],[0]
"The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text.",abstractText,[0],[0]
"We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children’s Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.",abstractText,[0],[0]
Natural Language Comprehension with the EpiReader,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2879–2885 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
Deep and recurrent neural networks with large network capacity have become increasingly accurate for challenging language processing tasks.,1 Introduction,[0],[0]
"For example, machine translation models have been able to attain impressive accuracies, with models that use hundreds of millions (Bahdanau et al., 2014; Wu et al., 2016) or billions (Shazeer et al., 2017) of parameters.",1 Introduction,[0],[0]
"These models, however, may not be feasible in all computational settings.",1 Introduction,[0],[0]
"In particular, models running on mobile devices are often constrained in terms of memory and computation.
",1 Introduction,[0],[0]
"Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997) have achieved good results with small memory footprints by using character-based input representations: e.g., the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters.",1 Introduction,[0],[0]
"Latency, however, can still be an issue with LSTMs, due to the large number of matrix multiplications they require (eight per LSTM cell): Kim and Rush (2016) report speeds of only 8.8 words/second when running a two-layer LSTM translation system on an Android phone.
",1 Introduction,[0],[0]
Feed-forward neural networks have the potential to be much faster.,1 Introduction,[0],[0]
"In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach.
",1 Introduction,[0],[0]
We begin by introducing the network model structure and the character-based representations we use throughout all tasks (§2).,1 Introduction,[0],[0]
"The four tasks that we address are: language identification (LangID), part-of-speech (POS) tagging, word segmentation, and preordering for translation.",1 Introduction,[0],[0]
"In order to use feed-forward networks for structured prediction tasks, we use transition systems (Titov and Henderson, 2007, 2010) with feature embeddings as proposed by Chen and Manning (2014), and introduce two novel transition systems for the last two tasks.",1 Introduction,[0],[0]
"We focus on budgeted models and ablate four techniques (one on each task) for improving accuracy for a given memory budget:
1.",1 Introduction,[0],[0]
"Quantization: Using more dimensions and less precision (Lang-ID: §3.1).
",1 Introduction,[0],[0]
2.,1 Introduction,[0],[0]
"Word clusters: Reducing the network size to allow for word clusters and derived features (POS tagging: §3.2).
",1 Introduction,[0],[0]
3.,1 Introduction,[0],[0]
"Selected features: Adding explicit feature conjunctions (segmentation: §3.3).
",1 Introduction,[0],[0]
4.,1 Introduction,[0],[0]
"Pipelines: Introducing another task in a pipeline and allocating parameters to the auxiliary task instead (preordering: §3.4).
",1 Introduction,[0],[0]
We achieve results at or near state-of-the-art with small (< 3 MB) models on all four tasks.,1 Introduction,[0],[0]
The network architectures are designed to limit the memory and runtime of the model.,2 Small Feed-Forward Network Models,[0],[0]
"Figure 1 illustrates the model architecture:
2879
1.",2 Small Feed-Forward Network Models,[0],[0]
"Discrete features are organized into groups (e.g., Ebigrams), with one embedding matrix Eg ∈ RVg×Dg per group.
2.",2 Small Feed-Forward Network Models,[0],[0]
Embeddings of features extracted for each group are reshaped into a single vector and concatenated to define the output of the embedding layer as h0 =,2 Small Feed-Forward Network Models,[0],[0]
"[XgEg | ∀g].
3.",2 Small Feed-Forward Network Models,[0],[0]
"A single hidden layer, h1, with M rectified linear units (Nair and Hinton, 2010) is fully connected to h0.
4.",2 Small Feed-Forward Network Models,[0],[0]
A softmax function models the probability of an output class y: P (y) ∝,2 Small Feed-Forward Network Models,[0],[0]
"exp(βTy h1 + by), where βy ∈ RM and by are the weight vector and bias, respectively.
",2 Small Feed-Forward Network Models,[0],[0]
"Memory needs are dominated by the embedding matrix sizes ( ∑ g VgDg, where Vg and Dg are the vocabulary sizes and dimensions respectively for each feature group g), while runtime is strongly influenced by the hidden layer dimensions.
",2 Small Feed-Forward Network Models,[0],[0]
"Hashed Character n-grams Previous applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015).",2 Small Feed-Forward Network Models,[0],[0]
"However, for word embeddings to be effective, they usually need to cover large vocabularies (100,000+) and dimensions (50+).",2 Small Feed-Forward Network Models,[0],[0]
"Inspired by the success of character-based representations (Ling et al., 2015), we use features defined over character n-grams instead of relying on word embeddings, and learn their embeddings from scratch.
",2 Small Feed-Forward Network Models,[0],[0]
"We use a distinct feature group g for each ngram length N , and control the size Vg directly by applying random feature mixing (Ganchev and Dredze, 2008).",2 Small Feed-Forward Network Models,[0],[0]
"That is, we define the feature value v for an n-gram string x as v = H(x) mod Vg, where H is a well-behaved hash function.",2 Small Feed-Forward Network Models,[0],[0]
"Typical values for Vg are in the 100-5000 range, which is far smaller than the exponential number of unique raw n-grams.",2 Small Feed-Forward Network Models,[0],[0]
"A consequence of these small feature vocabularies is that we can also use small feature embeddings, typically Dg=16.
",2 Small Feed-Forward Network Models,[0],[0]
"Quantization A commonly used strategy for compressing neural networks is quantization, using less precision to store parameters (Han et al., 2015).",2 Small Feed-Forward Network Models,[0],[0]
We compress the embedding weights (the vast majority of the parameters for these shallow models) by storing scale factors for each embedding (details in the supplementary material).,2 Small Feed-Forward Network Models,[0],[0]
"In §3.1, we contrast devoting model size to higher
precision and lower dimensionality versus lower precision and more network dimensions.
",2 Small Feed-Forward Network Models,[0],[0]
Training Our objective function combines the cross-entropy loss for model predictions relative to the ground truth with L2 regularization of the biases and hidden layer weights.,2 Small Feed-Forward Network Models,[0],[0]
"For optimization, we use mini-batched averaged stochastic gradient descent with momentum (Bottou, 2010; Hinton, 2012) and exponentially decaying learning rates.",2 Small Feed-Forward Network Models,[0],[0]
"The mini-batch size is fixed to 32 and we perform a grid search for the other hyperparameters, tuning against the task-specific evaluation metric on held-out data, with early stopping.",2 Small Feed-Forward Network Models,[0],[0]
Full feature templates and optimal hyperparameter settings are given in the supplementary material.,2 Small Feed-Forward Network Models,[0],[0]
"We experiment with small feed-forward networks for four diverse NLP tasks: language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation.
",3 Experiments,[0],[0]
Evaluation Metrics,3 Experiments,[0],[0]
"In addition to standard task-specific quality metrics, our evaluations also consider model size and computational cost.",3 Experiments,[0],[0]
We skirt implementation details by calculating size as the number of kilobytes (1KB=1024 bytes) needed to represent all model parameters and resources.,3 Experiments,[0],[0]
We approximate the computational cost as the number of floating-point operations (FLOPs) performed for one forward pass through the network given an embedding vector h0.,3 Experiments,[0],[0]
"This cost is dominated by the matrix multiplications to compute (unscaled) activation unit values, hence our metric excludes the non-linearities and softmax normal-
ization, but still accounts for the final layer logits.",3 Experiments,[0],[0]
"To ground this metric, we also provide indicative absolute speeds for each task, as measured on a modern workstation CPU (3.50GHz Intel Xeon E5-1650 v3).",3 Experiments,[0],[0]
"Recent shared tasks on code-switching (Molina et al., 2016) and dialects (Malmasi et al., 2016) have generated renewed interest in language identification.",3.1 Language Identification,[0],[0]
"We restrict our focus to single language identification across diverse languages, and compare to the work of Baldwin and Lui (2010) on predicting the language of Wikipedia text in 66 languages.",3.1 Language Identification,[0],[0]
"For this task, we obtain the input h0 by separately averaging the embeddings for each ngram length (N =",3.1 Language Identification,[0],[0]
"[1, 4]), as summation did not produce good results.
",3.1 Language Identification,[0],[0]
Table 1 shows that we outperform the lowmemory nearest-prototype model of Baldwin and Lui (2010).,3.1 Language Identification,[0],[0]
"Their nearest neighbor model is the most accurate but its memory scales linearly with the size of the training data.
",3.1 Language Identification,[0],[0]
"Moreover, we can apply quantization to the embedding matrix without hurting prediction accuracy: it is better to use less precision for each dimension, but to use more dimensions.",3.1 Language Identification,[0],[0]
Our subsequent models all use quantization.,3.1 Language Identification,[0],[0]
There is no noticeable variation in processing speed when performing dequantization on-the-fly at inference time.,3.1 Language Identification,[0],[0]
"Our 16-dim Lang-ID model runs at 4450 documents/second (5.6 MB of text per second) on the preprocessed Wikipedia dataset.
",3.1 Language Identification,[0],[0]
"Relationship to Compact Language Detector These techniques back the open-source Compact Language Detector v3 (CLD3)1 that runs in Google Chrome browsers.2 Our experimental Lang-ID model uses the same overall architecture as CLD3, but uses a simpler feature set, less involved preprocessing, and covers fewer languages.",3.1 Language Identification,[0],[0]
"We apply our model as an unstructured classifier to predict a POS tag for each token independently, and compare its performance to that of the byteto-span (BTS) model (Gillick et al., 2016).",3.2 POS Tagging,[0],[0]
"BTS is a 4-layer LSTM network that maps a sequence of bytes to a sequence of labeled spans, such as tokens and their POS tags.",3.2 POS Tagging,[0],[0]
"Both approaches limit
1github.com/google/cld3 2As of the date of this writing in 2017.
model size by using small input vocabularies: byte values in the case of BTS, and hashed character ngrams and (optionally) cluster ids in our case.
",3.2 POS Tagging,[0],[0]
Bloom Mapped Word Clusters,3.2 POS Tagging,[0],[0]
"It is well known that word clusters can be powerful features in linear models for a variety of tasks (Koo et al., 2008; Turian et al., 2010).",3.2 POS Tagging,[0],[0]
"Here, we show that they can also be useful in neural network models.",3.2 POS Tagging,[0],[0]
"However, naively introducing word cluster features drastically increases the amount of memory required, as a word-to-cluster mapping file with hundreds of thousands of entries can be several megabytes on its own.3 By representing word clusters with a Bloom map (Talbot and Talbot, 2008), a key-value based generalization of Bloom filters, we can reduce the space required by a factor of∼15 and use 300KB to (approximately) represent the clusters for 250,000 word types.
",3.2 POS Tagging,[0],[0]
"In order to compare against the monolingual setting of Gillick et al. (2016), we train models for the same set of 13 languages from the Universal Dependency treebanks v1.1 (Nivre et al., 2016) corpus, using the standard predefined splits.
",3.2 POS Tagging,[0],[0]
"As shown in Table 2, our best models are 0.3% more accuate on average across all languages than the BTS monolingual models, while using 6x fewer parameters and 36x fewer FLOPs.",3.2 POS Tagging,[0],[0]
"The cluster features play an important role, providing a 15% relative reduction in error over our vanilla model, but also increase the overall size.",3.2 POS Tagging,[0],[0]
"Halv-
3For example, the commonly used English clusters from the BLLIP corpus is over 7 MB – people.csail.mit.",3.2 POS Tagging,[0],[0]
"edu/maestro/papers/bllip-clusters.gz
ing all feature embedding dimensions (except for the cluster features) still gives a 12% reduction in error and trims the overall size back to 1.1x the vanilla model, staying well under 1MB in total.",3.2 POS Tagging,[0],[0]
"This halved model configuration has a throughput of 46k tokens/second, on average.
",3.2 POS Tagging,[0],[0]
"Two potential advantages of BTS are that it does not require tokenized input and has a more accurate multilingual version, achieving 95.85% accuracy.",3.2 POS Tagging,[0],[0]
"From a memory perspective, one multilingual BTS model will take less space than separate FF models.",3.2 POS Tagging,[0],[0]
"However, from a runtime perspective, a pipeline of our models doing language identification, word segmentation, and then POS tagging would still be faster than a single instance of the deep LSTM BTS model, by about 12x in our FLOPs estimate.4",3.2 POS Tagging,[0],[0]
Word segmentation is critical for processing Asian languages where words are not explicitly separated by spaces.,3.3 Segmentation,[0],[0]
"Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015).",3.3 Segmentation,[0],[0]
"We use a structured model based on the transition system in Table 3, and similar to the one proposed by Zhang and Clark (2007).",3.3 Segmentation,[0],[0]
We conduct the segmentation experiments on the Chinese Treebank 6.0 with the recommended data splits.,3.3 Segmentation,[0],[0]
No external resources or pretrained embeddings are used.,3.3 Segmentation,[0],[0]
"Hashing was detrimental to quality in our preliminary experiments, hence we do not use it for this task.",3.3 Segmentation,[0],[0]
"To learn an embedding for unknown characters, we cast characters occurring only once in the training set to a special symbol.
",3.3 Segmentation,[0],[0]
Selected Features,3.3 Segmentation,[0],[0]
"Because we are not using hashing here, we need to be careful about the size of the input vocabulary.",3.3 Segmentation,[0],[0]
"The neural network with its non-linearity is in theory able to learn bigrams by conjoining unigrams, but it has been
4Our calculation of BTS FLOPs is very conservative and favorable to BTS, as detailed in the supplementary material.
shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014).",3.3 Segmentation,[0],[0]
Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (‘Zhang et al. (2016)-combo’ in Table 4).,3.3 Segmentation,[0],[0]
"However, such embeddings could easily lead to a model size explosion and thus are not considered in this work.
",3.3 Segmentation,[0],[0]
"The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size.",3.3 Segmentation,[0],[0]
"Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second.",3.3 Segmentation,[0],[0]
"Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015).",3.4 Preordering,[0],[0]
"We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations.",3.4 Preordering,[0],[0]
"Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans.",3.4 Preordering,[0],[0]
"The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n",3.4 Preordering,[0],[0]
− 1 times to form a single span).,3.4 Preordering,[0],[0]
"For training and evaluation, we use the English-Japanese manual word alignments from Nakagawa (2015).
",3.4 Preordering,[0],[0]
"Pipelines For preordering, we experiment with either spending all of our memory budget on reordering, or spending some of the memory budget on features over predicted POS tags, which also requires an additional neural network to predict these tags.",3.4 Preordering,[0],[0]
Full feature templates are in the supplementary material.,3.4 Preordering,[0],[0]
"As the POS tagger network uses features based on a three word window around the token, another possibility is to add all of the features that would have affected the POS tag of a token to the reorderer directly.
",3.4 Preordering,[0],[0]
"Table 6 shows results with or without using the predicted POS tags in the preorderer, as well as including the features used by the tagger in the reorderer directly and only training the downstream task.",3.4 Preordering,[0],[0]
The preorderer that includes a separate network for POS tagging and then extracts features over the predicted tags is more accurate and smaller than the model that includes all the features that contribute to a POS tag in the reorderer directly.,3.4 Preordering,[0],[0]
"This pipeline processes 7k tokens/second when taking pretokenized text as input, with the POS tagger accounting for 23% of the computation time.",3.4 Preordering,[0],[0]
This paper shows that small feed-forward networks are sufficient to achieve useful accuracies on a variety of tasks.,4 Conclusions,[0],[0]
"In resource-constrained environments, speed and memory are important metrics to optimize as well as accuracies.",4 Conclusions,[0],[0]
"While large and deep recurrent models are likely to be the most accurate whenever they can be afforded, feed-foward networks can provide better value in terms of runtime and memory, and should be considered a strong baseline.",4 Conclusions,[0],[0]
"We thank Kuzman Ganchev, Fernando Pereira, and the anonymous reviewers for their useful comments.",Acknowledgments,[0],[0]
We show that small and shallow feedforward neural networks can achieve near state-of-the-art results on a range of unstructured and structured language processing tasks while being considerably cheaper in memory and computational requirements than deep recurrent models.,abstractText,[0],[0]
"Motivated by resource-constrained environments like mobile phones, we showcase simple techniques for obtaining such small neural network models, and investigate different tradeoffs when deciding how to allocate a small memory budget.",abstractText,[0],[0]
Natural Language Processing with Small Feed-Forward Networks,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 929–938 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1086",text,[0],[0]
"In tasks such as analyzing and plotting data (Gulwani and Marron, 2014), querying databases (Zelle and Mooney, 1996; Berant et al., 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al., 2017) and robots (Tellex et al., 2011), people need computers to perform well-specified but complex actions.",1 Introduction,[0],[0]
"To accomplish this, one route is to use a programming language, but this is inaccessible to most and can be tedious even for experts because the syntax is uncompromising and all statements have to be precise.",1 Introduction,[0],[0]
"Another route is to convert natural language into a formal lan-
Cubes: initial – select left 6 – select front 8 – black 10x10x10 frame – black 10x10x10 frame – move front 10 – red cube size 6 – move bot 2 – blue cube size 6 – green cube size 4 – (some steps are omitted)
",1 Introduction,[0],[0]
"Monsters, Inc: initial – move forward – add green monster – go down 8 – go right and front – add brown floor – add girl – go back and down – add door – add black column 30 – go up 9 – finish door – (some steps for moving are omitted)",1 Introduction,[0],[0]
"Deer: initial – bird’s eye view – deer head; up; left 2; back 2; { left antler }; right 2; {right antler} – down 4; front 2; left 3; deer body; down 6; {deer leg front}; back 7; {deer leg back}; left 4; {deer leg back}; front 7; {deer leg front} – (some steps omitted)
",1 Introduction,[0],[0]
"Figure 1: Some examples of users building structures using a naturalized language in Voxelurn:
http://www.voxelurn.com
guage, which has been the subject of work in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011, 2013; Pasupat and Liang, 2015).",1 Introduction,[0],[0]
"However, the capability of semantic parsers is still quite primitive compared to the power one wields with a programming language.",1 Introduction,[0],[0]
"This gap is increasingly limiting the potential of
929
both text and voice interfaces as they become more ubiquitous and desirable.
",1 Introduction,[0],[0]
"In this paper, we propose bridging this gap with an interactive language learning process which we call naturalization.",1 Introduction,[0],[0]
"Before any learning, we seed a system with a core programming language that is always available to the user.",1 Introduction,[0],[0]
"As users instruct the system to perform actions, they augment the language by defining new utterances — e.g., the user can explicitly tell the computer that ‘X’ means ‘Y’.",1 Introduction,[0],[0]
"Through this process, users gradually and interactively teach the system to understand the language that they want to use, rather than the core language that they are forced to use initially.",1 Introduction,[0],[0]
"While the first users have to learn the core language, later users can make use of everything that is already taught.",1 Introduction,[0],[0]
"This process accommodates both users’ preferences and the computer action space, where the final language is both interpretable by the computer and easier to produce by human users.
",1 Introduction,[0],[0]
"Compared to interactive language learning with weak denotational supervision (Wang et al., 2016), definitions are critical for learning complex actions (Figure 1).",1 Introduction,[0],[0]
Definitions equate a novel utterance to a sequence of utterances that the system already understands.,1 Introduction,[0],[0]
"For example, ‘go left 6 and go front’ might be defined as ‘repeat 6 [go left]; go front’, which eventually can be traced back to the expression ‘repeat 6",1 Introduction,[0],[0]
[select left of this]; select front of this’ in the core language.,1 Introduction,[0],[0]
"Unlike function definitions in programming languages, the user writes concrete values rather than explicitly declaring arguments.",1 Introduction,[0],[0]
The system automatically extracts arguments and learns to produce the correct generalizations.,1 Introduction,[0],[0]
"For this, we propose a grammar induction algorithm tailored to the learning from definitions setting.",1 Introduction,[0],[0]
"Compared to standard machine learning, say from demonstrations, definitions provide a much more powerful learning signal: the system is told directly that ‘a 3 by 4 red square’ is ‘3 red columns of height 4’, and does not have to infer how to generalize from observing many structures of different sizes.
",1 Introduction,[0],[0]
"We implemented a system called Voxelurn, which is a command language interface for a voxel world initially equipped with a programming language supporting conditionals, loops, and variable scoping etc.",1 Introduction,[0],[0]
We recruited 70 users from Amazon Mechanical Turk to build 230 voxel structures using our system.,1 Introduction,[0],[0]
"All users teach the system at once, and what is learned from one user
can be used by another user.",1 Introduction,[0],[0]
"Thus a community of users evolves the language to becomes more efficient over time, in a distributed way, through interaction.",1 Introduction,[0],[0]
"We show that the user community defined many new utterances—short forms, alternative syntax, and also complex concepts such as ‘add green monster, add yellow plate 3 x 3’.",1 Introduction,[0],[0]
"As the system learns, users increasingly prefer to use the naturalized language over the core language: 85.9% of the last 10K accepted utterances are in the naturalized language.",1 Introduction,[0],[0]
World.,2 Voxelurn,[0],[0]
"A world state in Voxelurn contains a set of voxels, where each voxel has relations ‘row’, ‘col’, ‘height’, and ‘color’.",2 Voxelurn,[0],[0]
"There are two domainspecific actions, ‘add’ and ‘move’, one domainspecific relation ‘direction’.",2 Voxelurn,[0],[0]
"In addition, the state contains a selection, which is a set of positions.",2 Voxelurn,[0],[0]
"While our focus is Voxelurn, we can think more generally about the world as a set of objects equiped with relations — events on a calendar, cells of a spreadsheet, or lines of text.
",2 Voxelurn,[0],[0]
Core language.,2 Voxelurn,[0],[0]
"The system is born understanding a core language called Dependency-based Action Language (DAL), which we created (see Table 1 for an overview).
",2 Voxelurn,[0],[0]
"The language composes actions using the usual but expressive control primitives such as ‘if’, ‘foreach’, ‘repeat’, etc.",2 Voxelurn,[0],[0]
"Actions usually take sets as arguments, which are represented using lambda dependency-based compositional semantics (lambda DCS) expressions (Liang, 2013).",2 Voxelurn,[0],[0]
"Besides standard set operations like union, intersec-
tion and complement, lambda DCS leverages the tree dependency structure common in natural language: for the relation ‘color’, ‘has color red’ refers to the set of voxels that have color red, and its reverse ‘color of has row 1’ refers to the set of colors of voxels having row number 1.",2 Voxelurn,[0],[0]
"Treestructured joins can be chained without using any variables, e.g., ‘has color [yellow or color of has row 1]’.
",2 Voxelurn,[0],[0]
We protect the core language from being redefined so it is always precise and usable.1,2 Voxelurn,[0],[0]
"In addition to expressivity, the core language interpolates well with natural language.",2 Voxelurn,[0],[0]
"We avoid explicit variables by using a selection, which serves as the default argument for most actions.2 For example, ‘select has color red; add yellow top; remove’ adds yellow on top of red voxels and then removes the red voxels.
",2 Voxelurn,[0],[0]
"To enable the building of more complex struc1Not doing so resulted in ambiguities that propagated uncontrollably, e.g., once ‘red’ can mean many different colors.",2 Voxelurn,[0],[0]
"2The selection is like the turtle in LOGO, but can be a set.
",2 Voxelurn,[0],[0]
"tures in a more modular way, we introduce a notion of scoping.",2 Voxelurn,[0],[0]
Suppose one is operating on one of the palm trees in Figure 2.,2 Voxelurn,[0],[0]
The user might want to use ‘select all’ to select only the voxels in that tree rather than all of the voxels in the scene.,2 Voxelurn,[0],[0]
"In general, an action A can be viewed as taking a set of voxels v and a selection s, and producing an updated set of voxels v′ and a modified selection s′.",2 Voxelurn,[0],[0]
"The default scoping is ‘[A]’, which is the same as ‘A’ and returns (v′, s′).",2 Voxelurn,[0],[0]
"There are two constructs that alter the flow: First, ‘{A}’ takes (v, s) and returns (v′, s), thus restoring the selection.",2 Voxelurn,[0],[0]
This allows A to use the selection as a temporary variable without affecting the rest of the program.,2 Voxelurn,[0],[0]
"Second, ‘isolate [A]’ takes (v, s), calls A with (s, s) (restricting the set of voxels to just the selection) and returns (v′′, s), where v′′ consists of voxels in v′ and voxels in v that occupy empty locations in v′.",2 Voxelurn,[0],[0]
"This allows A to focus only on the selection (e.g., one of the palm trees).",2 Voxelurn,[0],[0]
"Although scoping can be explicitly controlled via
‘[ ]’, ‘isolate’, and ‘{ }’, it is an unnatural concept for non-programmers.",2 Voxelurn,[0],[0]
"Therefore when the choice is not explicit, the parser generates all three possible scoping interpretations, and the model learns which is intended based on the user, the rule, and potentially the context.",2 Voxelurn,[0],[0]
The goal of the user is to build a structure in Voxelurn.,3 Learning interactively from definitions,[0],[0]
"In Wang et al. (2016), the user provided interactive supervision to the system by selecting from a list of candidates.",3 Learning interactively from definitions,[0],[0]
"This is practical when there are less than tens of candidates, but is completely infeasible for a complex action space such as Voxelurn.",3 Learning interactively from definitions,[0],[0]
"Roughly, 10 possible colors over the 3× 3× 4 box containing the palm tree in Figure 2 yields 1036 distinct denotations, and many more programs.",3 Learning interactively from definitions,[0],[0]
"Obtaining the structures in Figure 1 by selecting candidates alone would be infeasible.
",3 Learning interactively from definitions,[0],[0]
This work thus uses definitions in addition to selecting candidates as the supervision signal.,3 Learning interactively from definitions,[0],[0]
"Each definition consists of a head utterance and a body, which is a sequence of utterances that the system understands.",3 Learning interactively from definitions,[0],[0]
"One use of definitions is paraphrasing and defining alternative syntax, which helps naturalize the core language (e.g., defining ‘add brown top 3 times’ as ‘repeat 3 add brown top’).",3 Learning interactively from definitions,[0],[0]
The second use is building up complex concepts hierarchically.,3 Learning interactively from definitions,[0],[0]
"In Figure 2, ‘add yellow palm tree’ is defined as a sequence of steps for building the palm tree.",3 Learning interactively from definitions,[0],[0]
"Once the system understands an utterance, it can be used in the body of other definitions.",3 Learning interactively from definitions,[0],[0]
"For example, Figure 3 shows the full definition tree of ‘add palm tree’.",3 Learning interactively from definitions,[0],[0]
"Unlike function definitions in a programming language, our definitions do not specify the exact arguments; the system has to learn to extract arguments to achieve the correct generalization.
",3 Learning interactively from definitions,[0],[0]
The interactive definition process is described in Figure 4.,3 Learning interactively from definitions,[0],[0]
"When the user types an utterance x, the system parses x into a list of candidate programs.",3 Learning interactively from definitions,[0],[0]
"If the user selects one of them (based on its denotation), then the system executes the resulting program.",3 Learning interactively from definitions,[0],[0]
"If the utterance is unparsable or the user rejects all candidate programs, the user is asked to provide the definition body for x. Any utterances in the body not yet understood can be defined recursively.",3 Learning interactively from definitions,[0],[0]
"Alternatively, the user can first execute a sequence of commands X , and then provide a head utterance for body X .
",3 Learning interactively from definitions,[0],[0]
"When constructing the definition body, users
can type utterances with multiple parses; e.g., ‘move forward’ could either modify the selection (‘select front’) or move the voxel (‘move front’).",3 Learning interactively from definitions,[0],[0]
"Rather than propagating this ambiguity to the head, we force the user to commit to one interpretation by selecting a particular candidate.",3 Learning interactively from definitions,[0],[0]
Note that we are using interactivity to control the exploding ambiguity.,3 Learning interactively from definitions,[0],[0]
Let us turn to how the system learns and predicts.,4 Model and learning,[0],[0]
"This section contains prerequisites before we describe definitions and grammar induction in Section 5.
",4 Model and learning,[0],[0]
Semantic parsing.,4 Model and learning,[0],[0]
"Our system is based on a semantic parser that maps utterances x to programs z, which can be executed on the current state s (set of voxels and selection) to produce the next state s′ = JzKs.",4 Model and learning,[0],[0]
"Our system is implemented as the interactive package in SEMPRE (Berant et al., 2013);
see Liang (2016) for a gentle exposition.",4 Model and learning,[0],[0]
A derivation d represents the process by which an utterance x turns into a program z = prog(d).,4 Model and learning,[0],[0]
"More precisely, d is a tree where each node contains the corresponding span of the utterance (start(d), end(d)), the grammar rule rule(d), the grammar category cat(d), and a list of child derivations",4 Model and learning,[0],[0]
"[d1, . . .",4 Model and learning,[0],[0]
", dn].
",4 Model and learning,[0],[0]
"Following Zettlemoyer and Collins (2005), we define a log-linear model over derivations d given an utterance x produced by the user u:
pθ(d | x, u) ∝",4 Model and learning,[0],[0]
"exp(θTφ(d, x, u)), (1)
where φ(d, x, u) ∈",4 Model and learning,[0],[0]
Rp is a feature vector and θ ∈,4 Model and learning,[0],[0]
Rp is a parameter vector.,4 Model and learning,[0],[0]
"The user u does not appear in previous work on semantic parsing, but we use it to personalize the semantic parser trained on the community.
",4 Model and learning,[0],[0]
We use a standard chart parser to construct a chart.,4 Model and learning,[0],[0]
"For each chart cell, indexed by the start and end indices of a span, we construct a list of partial derivations recursively by selecting child derivations from subspans and applying a grammar rule.",4 Model and learning,[0],[0]
The resulting derivations are sorted by model score and only the top K are kept.,4 Model and learning,[0],[0]
We use chart(x) to denote the set of all partial derivations across all chart cells.,4 Model and learning,[0],[0]
"The set of grammar rules starts with the set of rules for the core language (Table 1), but grows via grammar induction when users add definitions (Section 5).",4 Model and learning,[0],[0]
"Rules in the grammar are stored in a trie based on the righthand side to enable better scalability to a large number of rules.
",4 Model and learning,[0],[0]
Features.,4 Model and learning,[0],[0]
Derivations are scored using a weighted combination of features.,4 Model and learning,[0],[0]
"There are three types of features, summarized in Table 2.
",4 Model and learning,[0],[0]
Rule features fire on each rule used to construct a derivation.,4 Model and learning,[0],[0]
ID features fire on specific rules (by ID).,4 Model and learning,[0],[0]
"Type features track whether a rule is part of the core language or induced, whether it has been
used again after it was defined, if it was used by someone other than its author, and if the user and the author are the same (5 + #rules features).
",4 Model and learning,[0],[0]
Social features fire on properties of rules that capture the unique linguistic styles of different users and their interaction with each other.,4 Model and learning,[0],[0]
"Author features capture the fact that some users provide better, and more generalizable definitions that tend to be accepted.",4 Model and learning,[0],[0]
"Friends features are cross products of author ID and user ID, which captures whether rules from a particular author are systematically preferred or not by the current user, due to stylistic similarities or differences (#users+#users×#users features).
",4 Model and learning,[0],[0]
Span features include conjunctions of the category of the derivation and the leftmost/rightmost token on the border of the span.,4 Model and learning,[0],[0]
"In addition, span features include conjunctions of the category of the derivation and the 1 or 2 adjacent tokens just outside of the left/right border of the span.",4 Model and learning,[0],[0]
"These capture a weak form of context-dependence that is generally helpful (<≈ V 4 × #cats features for a vocabulary of size V ).
",4 Model and learning,[0],[0]
"Scoping features track how the community, as well as individual users, prefer each of the 3 scoping choices (none, selection only ‘{A}’, and voxels+selection ‘isolate {A}’), as described in Section 2.",4 Model and learning,[0],[0]
"3 global indicators, and 3 indicators for each user fire every time a particular scoping choice is made (3 + 3× #users features).
",4 Model and learning,[0],[0]
Parameter estimation.,4 Model and learning,[0],[0]
"When the user types an utterance, the system generates a list of candidate next states.",4 Model and learning,[0],[0]
"When the user chooses a particular next state s′ from this list, the system performs an online AdaGrad update (Duchi et al., 2010) on the parameters θ according to the gradient of the following loss function:
− log ∑
d:Jprog(d)Ks=s′ pθ(d | x, u) + λ||θ||1,
which attempts to increase the model probability on derivations whose programs produce the next state s′.",4 Model and learning,[0],[0]
"Recall that the main form of supervision is via user definitions, which allows creation of user-defined concepts.",5 Grammar induction,[0],[0]
"In this section, we show how to turn
these definitions into new grammar rules that can be used by the system to parse new utterances.
",5 Grammar induction,[0],[0]
"Previous systems of grammar induction for semantic parsing were given utterance-program pairs (x, z).",5 Grammar induction,[0],[0]
"Both the GENLEX (Zettlemoyer and Collins, 2005) and higher-order unification (Kwiatkowski et al., 2010) algorithms overgenerate rules that liberally associate parts of x with parts of z.",5 Grammar induction,[0],[0]
"Though some rules are immediately pruned, many spurious rules are undoubtedly still kept.",5 Grammar induction,[0],[0]
"In the interactive setting, we must keep the number of candidates small to avoid a bad user experience, which means a higher precision bar for new rules.
",5 Grammar induction,[0],[0]
"Fortunately, the structure of definitions makes the grammar induction task easier.",5 Grammar induction,[0],[0]
"Rather than being given an utterance-program (x, z) pair, we are given a definition, which consists of an utterance x (head) along with the body X =",5 Grammar induction,[0],[0]
"[x1, . . .",5 Grammar induction,[0],[0]
", xn], which is a sequence of utterances.",5 Grammar induction,[0],[0]
"The body X is fully parsed into a derivation d, while the head x is likely only partially parsed.",5 Grammar induction,[0],[0]
"These partial derivations are denoted by chart(x).
",5 Grammar induction,[0],[0]
"At a high-level, we find matches—partial derivations chart(x) of the head x that also occur in the full derivation d of the body X .",5 Grammar induction,[0],[0]
A grammar rule is produced by substituting any set of nonoverlapping matches by their categories.,5 Grammar induction,[0],[0]
"As an example, suppose the user defines
‘add red top times 3’ as ‘repeat 3",5 Grammar induction,[0],[0]
"[add red top]’.
",5 Grammar induction,[0],[0]
"Then we would be able to induce the following two grammar rules:
A→ add C D times N :",5 Grammar induction,[0],[0]
λCDN.repeat N,5 Grammar induction,[0],[0]
"[add C D]
",5 Grammar induction,[0],[0]
A→ A times N :,5 Grammar induction,[0],[0]
λAN.repeat N,5 Grammar induction,[0],[0]
"[A]
The first rule substitutes primitive values (‘red’, ‘top’, and ‘3’) with their respective pre-terminal categories (C, D, N ).",5 Grammar induction,[0],[0]
"The second rule contains compositional categories like actions (A), which require some care.",5 Grammar induction,[0],[0]
"One might expect that greedily substituting the largest matches or the match that covers the largest portion of the body would work, but the following example shows that this is not the case:
",5 Grammar induction,[0],[0]
A1 A1 A1︷,5 Grammar induction,[0],[0]
︸︸,5 Grammar induction,[0],[0]
︷,5 Grammar induction,[0],[0]
︷,5 Grammar induction,[0],[0]
︸︸,5 Grammar induction,[0],[0]
︷ ︷,5 Grammar induction,[0],[0]
︸︸,5 Grammar induction,[0],[0]
"︷ add red left and here = add red left; add red︸ ︷︷ ︸ ︸ ︷︷ ︸
A2 A2
Here, both the highest coverage substitution (A1: ‘add red’, which covers 4 tokens of the body), and the largest substitution available (A2: ‘add red left’) would generalize incorrectly.",5 Grammar induction,[0],[0]
"The correct grammar rule only substitutes the primitive values (‘red’, ‘left’).",5 Grammar induction,[0],[0]
We now propose a grammar induction procedure that optimizes a more global objective and uses the learned semantic parsing model to choose substitutions.,5.1 Highest scoring abstractions,[0],[0]
"More formally, let M be the set of partial derivations in the head whose programs appear in the derivation dX of the body X:
M def = {d ∈ chart(x) : ∃d′ ∈ desc(dX)",5.1 Highest scoring abstractions,[0],[0]
"∧ prog(d) = prog(d′)},
where desc(dX) are the descendant derivations of dX .",5.1 Highest scoring abstractions,[0],[0]
"Our goal is to find a packing P ⊆ M , which is a set of derivations corresponding to nonoverlapping spans of the head.",5.1 Highest scoring abstractions,[0],[0]
"We say that a packing P is maximal if no other derivations may be added to it without creating an overlap.
",5.1 Highest scoring abstractions,[0],[0]
"Let packings(M) denote the set of maximal packings, we can frame our problem as finding the maximal packing that has the highest score under our current semantic parsing model:
P ∗L = argmax P∈packings(M);
∑ d∈P score(d).",5.1 Highest scoring abstractions,[0],[0]
"(2)
Finding the highest scoring packing can be done using dynamic programming on P ∗i for i = 0, 1, . . .",5.1 Highest scoring abstractions,[0],[0]
", L, whereL is the length of x and P ∗0",5.1 Highest scoring abstractions,[0],[0]
= ∅.,5.1 Highest scoring abstractions,[0],[0]
"Since d ∈M , start(d) and end(d) (exclusive) refer to span in the head x. To obtain this dynamic program, let Di be the highest scoring maximal packing containing a derivation ending exactly at position i (if it exists):
Di = {di} ∪ P ∗start(di), (3) di = argmax
d∈M ;end(d)=i score(d ∪ P ∗start(d)).",5.1 Highest scoring abstractions,[0],[0]
"(4)
Then the maximal packing of up to i can be defined recursively as
P ∗i = argmax D∈{Ds(i)+1,Ds(i)+2,...,Di} score(D) (5)
s(i) = max d:end(d)≤i start(d), (6)
",5.1 Highest scoring abstractions,[0],[0]
"Input : x, dX , P ∗ Output: rule r ← x;",5.1 Highest scoring abstractions,[0],[0]
"f ← dX ; for d ∈ P ∗ do
r ← r[cat(d)/ span(d)]",5.1 Highest scoring abstractions,[0],[0]
f ← λ cat(d).f,5.1 Highest scoring abstractions,[0],[0]
"[cat(d)/d] return rule (cat(dX)→ r : f)
Algorithm 1: Extract a rule r from a derivation dX of body X and a packing P ∗. Here, f [t/s] means substituting s by t in f , with the usual care about names of bound variables.
where s(i) is the largest index such thatDs(i) is no longer maximal for the span (0, i) (i.e. there is a d ∈M on the span start(d) ≥ s(i)",5.1 Highest scoring abstractions,[0],[0]
"∧ end(d) ≤ i.
",5.1 Highest scoring abstractions,[0],[0]
"Once we have a packing P ∗ = P ∗L, we can go through d ∈ P ∗ in order of start(d), as in Algorithm 1.",5.1 Highest scoring abstractions,[0],[0]
This generates one high precision rule per packing per definition.,5.1 Highest scoring abstractions,[0],[0]
"In addition to the highest scoring packing, we also use a “simple packing”, which includes only primitive values (in Voxelurn, these are colors, numbers, and directions).",5.1 Highest scoring abstractions,[0],[0]
"Unlike the simple packing, the rule induced from the highest scoring packing does not always generalize correctly.",5.1 Highest scoring abstractions,[0],[0]
"However, a rule that often generalizes incorrectly should be down-weighted, along with the score of its packings.",5.1 Highest scoring abstractions,[0],[0]
"As a result, a different rule might be induced next time, even with the same definition.",5.1 Highest scoring abstractions,[0],[0]
"Algorithm 1 yields high precision rules, but fails to generalize in some cases.",5.2 Extending the chart via alignment,[0],[0]
"Suppose that ‘move up’ is defined as ‘move top’, where ‘up’ does not parse, and does not match anything.",5.2 Extending the chart via alignment,[0],[0]
We would like to infer that ‘up’ means ‘top’.,5.2 Extending the chart via alignment,[0],[0]
"To handle this, we leverage a property of definitions that we have not used thus far: the utterances themselves.",5.2 Extending the chart via alignment,[0],[0]
"If we align the head and body, then we would intuitively expect aligned phrases to correspond to the same derivations.",5.2 Extending the chart via alignment,[0],[0]
"Under this assumption, we can then transplant these derivations from dX to chart(x) to create new matches.",5.2 Extending the chart via alignment,[0],[0]
"This is more constrained than the usual alignment problem (e.g., in machine translation) since we only need to consider spans of X which corresponds to derivations in desc(dX).
",5.2 Extending the chart via alignment,[0],[0]
Algorithm 2 provides the algorithm for extending the chart via alignments.,5.2 Extending the chart via alignment,[0],[0]
"The aligned function is implemented using the following two heuristics:
Input : x,X, dX for d ∈ desc(dX), x′ ∈ spans(x) do
if aligned(x′, d, (x,X)) then d′ ← d; start(d′)← start(x′); end(d′)← end(x′); chart(x)← chart(x) ∪ d′
end end
Algorithm 2: Extending the chart by alignment: If d is aligned with x′ based on the utterance, then we pretend that x′ should also parse to d, and d is transplanted to chart(x) as if it parsed from x′.
• exclusion: if all but 1 pair of short spans (1 or 2 tokens) are matched, the unmatched pair is considered aligned.
",5.2 Extending the chart via alignment,[0],[0]
"• projectivity: if d1, d2 ∈ desc(dX) ∩ chart(x), then ances(d1, d2) is aligned to the corresponding span in x.
With the extended chart, we can run the algorithm from Section 5.1 to induce rules.",5.2 Extending the chart via alignment,[0],[0]
"The transplanted derivations (e.g., ‘up’) might now form new matches which allows the grammar induction to induce more generalizable rules.",5.2 Extending the chart via alignment,[0],[0]
"We only perform this extension when the body consists of one utterance, which tend to be a paraphrase.",5.2 Extending the chart via alignment,[0],[0]
"Bodies with multiple utterances tend to be new concepts (e.g., ‘add green monster’), for which alignment is impossible.",5.2 Extending the chart via alignment,[0],[0]
"Because users have to select from candidates parses in the interactive setting, inducing low precision rules that generate many parses degrade the user experience.",5.2 Extending the chart via alignment,[0],[0]
"Therefore, we induce alignment-based rules conservatively—only when all but 1 or 2 tokens of the head aligns to the body and vice versa.",5.2 Extending the chart via alignment,[0],[0]
Setup.,6 Experiments,[0],[0]
Our ultimate goal is to create a community of users who can build interesting structures in Voxelurn while naturalizing the core language.,6 Experiments,[0],[0]
We created this community using Amazon Mechanical Turk (AMT) in two stages.,6 Experiments,[0],[0]
"First, we had qualifier tasks, in which an AMT worker was instructed to replicate a fixed target exactly (Figure 5), ensuring that the initial users are familiar with at least some of the core language, which is the starting point of the naturalization process.
",6 Experiments,[0],[0]
"Next, we allowed the workers who qualified to enter the second freebuilding task, in which they were asked to build any structure they wanted in 30 minutes.",6 Experiments,[0],[0]
This process was designed to give users freedom while ensuring quality.,6 Experiments,[0],[0]
"The analogy of this scheme in a real system is that early users (or a small portion of expert users) have to make some learning investment, so the system can learn and become easier for other users.
",6 Experiments,[0],[0]
Statistics.,6 Experiments,[0],[0]
"70 workers passed the qualifier task, and 42 workers participated in the final freebuilding experiment.",6 Experiments,[0],[0]
They built 230 structures.,6 Experiments,[0],[0]
"There were over 103,000 queries consisting of 5,388 distinct token types.",6 Experiments,[0],[0]
"Of these, 64,075 utterances were tried and 36,589 were accepted (so an action was performed).",6 Experiments,[0],[0]
"There were 2,495 definitions combining over 15,000 body utterances with 6.5 body utterances per head on average (96 max).",6 Experiments,[0],[0]
"From these definitions, 2,817 grammar rules were induced, compared to less than 100 core rules.",6 Experiments,[0],[0]
"Over all queries, there were 8.73 parses per utterance on average (starting from 1 for core).
",6 Experiments,[0],[0]
Is naturalization happening?,6 Experiments,[0],[0]
"The answer is yes according to Figure 6, which plots the cummulative percentage of utterances that are core, induced, or unparsable.",6 Experiments,[0],[0]
"To rule out that more induced utterances are getting rejected, we consider only accepted utterances in the middle of Figure 6, which plots the percentage of induced rules among accepted utterances for the entire community, as well as for the 5 heaviest users.",6 Experiments,[0],[0]
"Since unparsable utterances cannot be accepted, accepted core (which is not shown) is the complement of accepted induced.",6 Experiments,[0],[0]
"At the conclusion of the experiment, 72.9% of all accepted utterances are induced—this becomes 85.9% if we only consider the final 10,000 accepted utterances.
",6 Experiments,[0],[0]
Three modes of naturalization are outlined in Table 3.,6 Experiments,[0],[0]
"For very common operations, like moving the selection, people found ‘select left’ too verbose and shorterned this to l, left, >, sel l. One user preferred ‘go down and right’ instead of ‘select bot; select right’ in core and defined it as ‘go down; go right’.",6 Experiments,[0],[0]
"Definitions for high-level
concepts tend to be whole objects that are not parameterized (e.g., ‘dancer’).",6 Experiments,[0],[0]
"The bottom plot of Figure 6 suggests that users are defining and using higher level concepts, since programs become longer relative to utterances over time.
",6 Experiments,[0],[0]
"As a result of the automatic but implicit grammar induction, some concepts do not generalize correctly.",6 Experiments,[0],[0]
"In definition head ‘3 tall 9 wide white tower centered here’, arguments do not match the body; for ‘black 10x10x10 frame’, we failed to tokenize.
",6 Experiments,[0],[0]
Learned parameters.,6 Experiments,[0],[0]
"Training using L1 regularization, we obtained 1713 features with nonzero parameters.",6 Experiments,[0],[0]
"One user defined many concepts consisting of a single short token, and the Social.",6 Experiments,[0],[0]
Author feature for that user has the most negative weight overall.,6 Experiments,[0],[0]
"With user compatibility (Social.Friends), some pairs have large positive weights and others large negative weights.",6 Experiments,[0],[0]
"The ‘isolate’ scoping choice (which allows easier hierarchical building) received the most positive weights, both overall and for many users.",6 Experiments,[0],[0]
"The 2 highest scoring induced rules correspond to ‘add row red right 5’ and ‘select left 2’.
",6 Experiments,[0],[0]
Incentives.,6 Experiments,[0],[0]
Having complex structures show that the actions in Voxelurn are expressive and that hierarchical definitions are useful.,6 Experiments,[0],[0]
"To incentivize this behavior, we created a leaderboard which ranked structures based on recency and upvotes (like Hacker News).",6 Experiments,[0],[0]
"Over the course of 3 days, we picked three prize categories to be released daily.",6 Experiments,[0],[0]
"The prize categories for each day were bridge, house, animal; tower, monster, flower; ship, dancer, and castle.
",6 Experiments,[0],[0]
"To incentivize more definitions, we also track citations.",6 Experiments,[0],[0]
"When a rule is used in an accepted utterance by another user, the rule (and its author) receives a citation.",6 Experiments,[0],[0]
We pay bonuses to top users according to their h-index.,6 Experiments,[0],[0]
Most cited definitions are also displayed on the leaderboard.,6 Experiments,[0],[0]
"Our qualitative results should be robust to the incentives scheme, because the users do not overfit to the incentives—e.g., around 20% of the structures are
not in the prize categories and users define complex concepts that are rarely cited.",6 Experiments,[0],[0]
"This work is an evolution of Wang et al. (2016), but differs crucially in several ways: While Wang et al. (2016) starts from scratch and relies on selecting candidates, this work starts with a programming language (PL) and additionally relies on definitions, allowing us to scale.",7 Related work and discussion,[0],[0]
"Instead of having a private language for each user, the user community in this work shares one language.
",7 Related work and discussion,[0],[0]
"Azaria et al. (2016) presents Learning by Instruction Agent (LIA), which also advocates learning from users.",7 Related work and discussion,[0],[0]
"They argue that developers cannot anticipate all the actions that users want, and that the system cannot understand the corresponding natural language even if the desired action is built-in.",7 Related work and discussion,[0],[0]
"Like Jia et al. (2017), Azaria et al. (2016) starts with an ad-hoc set of initial slot-filling commands in natural language as the basis of further instructions—our approach starts with a more expressive core PL designed to interpolate with natural language.",7 Related work and discussion,[0],[0]
"Compared to previous work, this work studied interactive learning in a shared community setting and hierarchical definitions resulting in more complex concepts.
",7 Related work and discussion,[0],[0]
Allowing ambiguity and a flexible syntax is a key reason why natural language is easier to produce—this cannot be achieved by PLs such as Inform and COBOL which look like natural language.,7 Related work and discussion,[0],[0]
"In this work, we use semantic parsing techniques that can handle ambiguity (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Liang et al., 2011; Pasupat and Liang, 2015).",7 Related work and discussion,[0],[0]
"In semantic parsing, the semantic representation and action space is usually designed to accommodate the natural language that is considered constant.",7 Related work and discussion,[0],[0]
"In contrast, the action space is considered constant in the naturalizing PL approach, and the language adapts to be more natural while accommodating the action space.
",7 Related work and discussion,[0],[0]
Our work demonstrates that interactive definitions is a strong and usable form of supervision.,7 Related work and discussion,[0],[0]
"In the future, we wish to test these ideas in more domains, naturalize a real PL, and handle paraphrasing and implicit arguments.",7 Related work and discussion,[0],[0]
"In the process of naturalization, both data and the semantic grammar have important roles in the evolution of a language that is easier for humans to produce while still parsable by computers.
Acknowledgments.",7 Related work and discussion,[0],[0]
"We thank our reviewers, Panupong (Ice) Pasupat for helpful suggestions and discussions on lambda DCS, DARPA Communicating with Computers (CwC) program under ARO prime contract no.",7 Related work and discussion,[0],[0]
"W911NF-15-1-0462, and NSF CAREER Award",7 Related work and discussion,[0],[0]
no.,7 Related work and discussion,[0],[0]
"IIS-1552635.
Reproducibility.",7 Related work and discussion,[0],[0]
"All code, data, and experiments for this paper are available on the CodaLab platform: https://worksheets.
",7 Related work and discussion,[0],[0]
"codalab.org/worksheets/
0xbf8f4f5b42e54eba9921f7654b3c5c5d and a demo: http://www.voxelurn.com",7 Related work and discussion,[0],[0]
"Our goal is to create a convenient natural language interface for performing wellspecified but complex actions such as analyzing data, manipulating text, and querying databases.",abstractText,[0],[0]
"However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language.",abstractText,[0],[0]
"To bridge this gap, we start with a core programming language and allow users to “naturalize” the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones.",abstractText,[0],[0]
"In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures.",abstractText,[0],[0]
"Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9% of the last 10K utterances.",abstractText,[0],[0]
Naturalizing a Programming Language via Interactive Learning,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 284–290 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Currently, the most effective GEC systems are based on phrase-based statistical machine translation (Rozovskaya and Roth, 2016; JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017).",1 Introduction,[0],[0]
"Systems that rely on neural machine translation (Yuan and Briscoe, 2016; Xie et al., 2016; Schmaltz et al., 2017; Ji et al., 2017) are not yet able to achieve as high performance as SMT systems according to automatic evaluation metrics (see Table 1 for comparison on the CoNLL-2014 test set).",1 Introduction,[0],[0]
"However, it has been shown that the neural approach can produce more fluent output, which might be desirable by human evaluators (Napoles et al., 2017).",1 Introduction,[0],[0]
"In this work, we combine both MT flavors within a hybrid GEC system.",1 Introduction,[0],[0]
Such a GEC system preserves the accuracy of SMT output and at the same time generates more fluent sentences achieving new state-of-the-art results on two different benchmarks: the annotationbased CoNLL-2014 and the fluency-based JFLEG benchmark.,1 Introduction,[0],[0]
"Moreover, comparison with human gold standards shows that the created systems are
closer to reaching human-level performance than any other GEC system described in the literature so far.
",1 Introduction,[0],[0]
"Using consistent training data and preprocessing (§ 2), we first create strong SMT (§ 3) and NMT (§ 4) baseline systems.",1 Introduction,[0],[0]
"Then, we experiment with system combinations through pipelining and reranking (§ 5).",1 Introduction,[0],[0]
"Finally, we compare the performance with human annotations and identify issues with current state-of-the-art systems (§ 6).",1 Introduction,[0],[0]
"Our main training data is NUCLE (Dahlmeier et al., 2013).",2 Data and preprocessing,[0],[0]
"English sentences from the publicly available Lang-8 Corpora (Mizumoto et al., 2012) serve as additional training data.
",2 Data and preprocessing,[0],[0]
"We use official test sets from two CoNLL shared tasks from 2013 and 2014 (Ng et al., 2013, 2014) as development and test data, and evaluate using M2 (Dahlmeier and Ng, 2012).",2 Data and preprocessing,[0],[0]
"We also report results on JFLEG (Napoles et al., 2017) with the
284
GLEU metric (Napoles et al., 2015).",2 Data and preprocessing,[0],[0]
The data set is provided with a development and test set split.,2 Data and preprocessing,[0],[0]
"All data sets are listed in Table 1.
",2 Data and preprocessing,[0],[0]
"We preprocess Lang-8 with the NLTK tokenizer (Bird and Loper, 2004) and preserve the original tokenization in NUCLE and JFLEG.",2 Data and preprocessing,[0],[0]
"Sentences are truecased with scripts from Moses (Koehn et al., 2007).",2 Data and preprocessing,[0],[0]
"For dealing with out-of-vocabulary words, we split tokens into 50k subword units using Byte Pair Encoding (BPE) by Sennrich et al. (2016b).",2 Data and preprocessing,[0],[0]
BPE codes are extracted only from correct sentences from Lang-8 and NUCLE.,2 Data and preprocessing,[0],[0]
"For our SMT-based systems, we follow recipes proposed by Junczys-Dowmunt and Grundkiewicz (2016), and use a phrase-based SMT system with a log-linear combination of task-specific features.",3 SMT systems,[0],[0]
"We use word-level Levenshtein distance and edit operation counts as dense features (Dense), and correction patterns on words with one word left/right context on Word Classes (WC) as sparse features (Sparse).",3 SMT systems,[0],[0]
We also experiment with additional character-level dense features (Char. ops).,3 SMT systems,[0],[0]
"All systems use a 5-gram Language Model (LM) and OSM (Durrani et al., 2011) both estimated from the target side of the training data, and a 5-gram LM and 9-gram WCLM trained on Common Crawl data (Buck et al., 2014).
",3 SMT systems,[0],[0]
"Experiment settings Translation models are trained with Moses (Koehn et al., 2007), wordalignment models are produced with MGIZA++",3 SMT systems,[0],[0]
"(Gao and Vogel, 2008), and no reordering models are used.",3 SMT systems,[0],[0]
"Language models are built using KenLM (Heafield, 2011), while word classes are trained with word2vec1.
",3 SMT systems,[0],[0]
We tune the systems separately for M2 and GLEU metrics.,3 SMT systems,[0],[0]
"MERT (Och, 2003) is used for tuning dense features and Batch Mira (Cherry and Foster, 2012) for sparse features.",3 SMT systems,[0],[0]
"For M2 tunning
1https://github.com/dav/word2vec
we follow the 4-fold cross-validation on NUCLE with adapted error rate recommended by JunczysDowmunt and Grundkiewicz (2016).",3 SMT systems,[0],[0]
"Models evaluated on GLEU are optimized on JFLEG Dev using the GLEU scorer, which we added to Moses.",3 SMT systems,[0],[0]
"We report results for models using feature weights averaged over 4 tuning runs.
",3 SMT systems,[0],[0]
"Results Other things being equal, using the original tokenization, applying subword units, and extending edit-based features result in a similar system to Junczys-Dowmunt and Grundkiewicz (2016): 49.82 vs 49.49 M2 (Table 2).
",3 SMT systems,[0],[0]
"The phrase-based SMT systems do not deal well with orthographic errors (Napoles et al., 2017) — if a source word has not been seen in the training corpus, it is likely copied as a target word.",3 SMT systems,[0],[0]
Subword units can help to solve this problem partially.,3 SMT systems,[0],[0]
"Adding features based on character-level edit counts increases the results on both test sets.
",3 SMT systems,[0],[0]
"A result of 55.79 GLEU on JFLEG Test is already 2 points better than the GLEU-tuned NMT system of Sakaguchi et al. (2017) and only 1 point worse than the best reported result by Chollampatt and Ng (2017) with their M2-tuned SMT system, even though no additional spelling correction has been used at this point.",3 SMT systems,[0],[0]
We experiment with specialized spell-checking methods in later sections.,3 SMT systems,[0],[0]
"The model architecture we choose for our NMTbased systems is an attentional encoder-decoder model with a bidirectional single-layer encoder and decoder, both using GRUs as their RNN variants (Sennrich et al., 2017).",4 NMT systems,[0],[0]
"A similar architecture has been already tested for the GEC task by Sakaguchi et al. (2017), but we use different hyperparameters.
",4 NMT systems,[0],[0]
"To improve the performance of our NMT models, similarly to Xie et al. (2016) and Ji et al. (2017), we combine them with an additional large-scale language model.",4 NMT systems,[0],[0]
"In contrast to previous studies, which use an n-gram probabilistic LM, we build a 2-layer Recurrent Neural Network Language Model (RNN
LM) with GRU cells which we train again on English Common Crawl data (Buck et al., 2014).
",4 NMT systems,[0],[0]
"Experimental settings We train with the Marian toolkit (Junczys-Dowmunt et al., 2018) on the same data we used for the SMT baselines, i.e. NUCLE and Lang-8.",4 NMT systems,[0],[0]
"The RNN hidden state size is set to 1024, embedding size to 512.",4 NMT systems,[0],[0]
"Source and target vocabularies as well as subword units are the same.
",4 NMT systems,[0],[0]
"Optimization is performed with Adam (Kingma and Ba, 2014) and the mini-batch size fitted into 4GB of GPU memory.",4 NMT systems,[0],[0]
"We regularize the model with scaling dropout (Gal and Ghahramani, 2016) with a dropout probability of 0.2 on all RNN inputs and states.",4 NMT systems,[0],[0]
Apart from that we dropout entire source and target words with probabilities of 0.2 and 0.1 respectively.,4 NMT systems,[0],[0]
We use early stopping with a patience of 10 based on the cross-entropy cost on the CoNLL-2013 test set.,4 NMT systems,[0],[0]
"Models are validated and saved every 10,000 mini-batches.",4 NMT systems,[0],[0]
"As final models we choose the one with the best performance on the development set among the last ten model check-points based on the M2 or GLEU metrics.
",4 NMT systems,[0],[0]
"Size of RNN hidden state and embeddings, target vocabulary, and optimization options for the RNN LM are identical to those used for our NMT models.",4 NMT systems,[0],[0]
Decoding is done by beam search with a beam size of 12.,4 NMT systems,[0],[0]
"We normalize scores for each hypothesis by sentence length.
",4 NMT systems,[0],[0]
Results A single NMT model achieves lower performance than the SMT baselines (Table 3).,4 NMT systems,[0],[0]
"However, the M2 score of 42.76 for CoNLL-2014 is already higher than the best published result of 41.53 M2 for a strictly neural GEC system of Ji et al. (2017) that has not been enhanced by an additional language model.
",4 NMT systems,[0],[0]
"Our RNN LM is integrated with NMT models through ensemble decoding (Sennrich et al., 2016a).",4 NMT systems,[0],[0]
"Similarly to Ji et al. (2017), we choose the weight of the language model using grid search on the development set2.",4 NMT systems,[0],[0]
"This strongly improves recall,
2Used weights are 0.2 and 0.25 for M2 and GLEU evalua-
and thus boosts the results significantly on both test sets (+5.8 M2 and +5.96 GLEU).
",4 NMT systems,[0],[0]
"An ensemble of four independently trained models3 (NMT×4), on the other hand, increases precision at the expense of recall, which may even lead to a performance drop.",4 NMT systems,[0],[0]
"Adding the RNN LM to that ensemble balances this negative effect, resulting in 50.19 M2.",4 NMT systems,[0],[0]
"These are by far the highest results reported on both benchmarks for pure neural GEC systems.
",4 NMT systems,[0],[0]
"Comparison to SMT systems With model ensembling, the neural systems achieve performance similar to SMT baselines (Figure 2).",4 NMT systems,[0],[0]
"A strippeddown SMT system without CCLM, quite surprisingly gives better results on JFLEG than the NMT system, and the opposite is true for CoNLL-2014.",4 NMT systems,[0],[0]
"The reason for the lower performance on JFLEG might be a large amount of spelling errors, which are more efficiently corrected by the SMT system using subword units.
",4 NMT systems,[0],[0]
"If both systems are enhanced by a large-scale language model, the neural system outperforms the SMT system on JFLEG and it is competitive with SMT systems on CoNLL-2014.",4 NMT systems,[0],[0]
"However, it is not known if the results would preserve if the NMT model is combined with a probabilistic ngram LM instead as it has been proposed in the previous works (Xie et al., 2016; Ji et al., 2017).
tion, respectively.",4 NMT systems,[0],[0]
3Each model is weighted equally during decoding.,4 NMT systems,[0],[0]
"We experiment with pipelining and rescoring methods in order to combine our best SMT and NMT GEC systems4.
SMT-NMT pipelines The output corrected by an SMT system is passed as an input to the NMT ensemble with or without RNN LM5.",5 Hybrid SMT-NMT systems,[0],[0]
In this case the NMT system serves as an automatic post-editing system.,5 Hybrid SMT-NMT systems,[0],[0]
Pipelining improves the results on both test sets by increasing recall (Table 4).,5 Hybrid SMT-NMT systems,[0],[0]
"As the performance of the NMT system without a RNN LM is much lower than the performance of the SMT system alone, this implies that both approaches produce complementary corrections.
",5 Hybrid SMT-NMT systems,[0],[0]
"Rescoring with NMT Rescoring of an n-best list obtained from one system by another is a commonly used technique in GEC, which allows to combine multiple different systems or even different approaches (Hoang et al., 2016; Yannakoudakis et al., 2017; Chollampatt and Ng, 2017; Ji et al., 2017).",5 Hybrid SMT-NMT systems,[0],[0]
"In our experiments, we generate a 1000 n-best list with the SMT system and add separate scores from each neural component.",5 Hybrid SMT-NMT systems,[0],[0]
Scores of NMT models and the RNN LM are added in the form of probabilities in negative log space.,5 Hybrid SMT-NMT systems,[0],[0]
"The re-scored weights are obtained from a single run of the Batch Mira algorithm (Cherry and Foster, 2012) on the development set.
",5 Hybrid SMT-NMT systems,[0],[0]
"As opposed to pipelining, rescoring improves precision at the expense of recall and is more effective for the CoNLL data resulting in up to 54.95 M2.",5 Hybrid SMT-NMT systems,[0],[0]
"On JFLEG, rescoring only with the RNN LM
4The best system combinations are chosen again based on the development sets, i.e. CoNLL-2013 and JFLEG Dev.",5 Hybrid SMT-NMT systems,[0],[0]
"We omit these results as they are highly overestimated.
",5 Hybrid SMT-NMT systems,[0],[0]
"5We did not observed any improvements if the order of the systems is reversed.
produces similar results as rescoring with the NMT ensemble.",5 Hybrid SMT-NMT systems,[0],[0]
"However, the best result for rescoring is lower than for pipelining on that test set.",5 Hybrid SMT-NMT systems,[0],[0]
"It seems the SMT system is not able to produce as diversified corrections in an n-best list as those generated by the NMT ensemble.
",5 Hybrid SMT-NMT systems,[0],[0]
Spelling correction and final results Pipelining the NMT-rescored SMT system and the NMT system leads to further improvement.,5 Hybrid SMT-NMT systems,[0],[0]
"We believe this can be explained by different contributions to precision and recall trade-offs for the two methods, similar to effects observed for the combination of the NMT ensemble and our RNN LM.
",5 Hybrid SMT-NMT systems,[0],[0]
"On top of our final hybrid system we add a spellchecking component, which is run before pipelining.",5 Hybrid SMT-NMT systems,[0],[0]
We use a character-level SMT system following Chollampatt and Ng (2017) which they deploy for unknown words in their word-based SMT system.,5 Hybrid SMT-NMT systems,[0],[0]
"As our BPE-based SMT does not really suffer from unknown words, we run the spell-checking component on words that would have been segmented by the BPE algorithm.",5 Hybrid SMT-NMT systems,[0],[0]
This last system achieves the best results reported in this paper: 56.25 M2 on CoNLL-2014 and 61.50 GLEU on JFLEG Test.,5 Hybrid SMT-NMT systems,[0],[0]
"For both benchmarks our systems are close to automatic evaluation results that have been claimed to correspond to human-level performance on the CoNLL-2014 test set and on JFLEG Test.
",6 Analysis and future work,[0],[0]
Example outputs Table 5 shows system outputs for an example source sentence from the JFLEG Test corpus that illustrate the complementarity of the statistical and neural approaches.,6 Analysis and future work,[0],[0]
The SMT and NMT systems produce different corrections.,6 Analysis and future work,[0],[0]
"Rescoring is able to generate a unique correction (is change→has changed), but it fails in generating some corrections from the neural system, e.g. misspellings (becom and dificullty).",6 Analysis and future work,[0],[0]
"Pipelining, on the other hand, may not improve a local correction made by the SMT system (is changed).",6 Analysis and future work,[0],[0]
"The combination of the two methods produces output, which is most similar to the references.
",6 Analysis and future work,[0],[0]
"Comparison with human annotations Bryant and Ng (2015) created an extension of the CoNLL2014 test set with 10 annotators in total, JFLEG already incorporates corrections from 4 annotators.",6 Analysis and future work,[0],[0]
"Human-level results for M2 and GLEU were calculated by averaging the scores for each annotator with regard to the remaining 9 (CoNLL) or 3 (JFLEG) annotators, respectively.
",6 Analysis and future work,[0],[0]
"Figure 3 contains human level scores, our results, and previously best reported results by Chollampatt and Ng (2017).",6 Analysis and future work,[0],[0]
"Our best system reaches nearly 100% of the average human score according to M2 and nearly 99% for GLEU being much closer to that bound than previous works6.
",6 Analysis and future work,[0],[0]
"6During the camera-ready preparation, Chollampatt and Ng (2018) have published a GEC system based on a multilayer convolutional encoder-decoder neural network with a character-based spell-checking module improving the previous best result to 54.79 M2 on CoNLL-2014 and 57.47 GLEU on JFLEG Test.
",6 Analysis and future work,[0],[0]
"Further inspection reveals, however, that the precision/recall trade-off for the automatic system indicates lower coverage compared to human corrections — lower recall is compensated with high precision7.",6 Analysis and future work,[0],[0]
"Automatic systems might, for example, miss some obvious error corrections and therefore easily be distinguishable from human references.",6 Analysis and future work,[0],[0]
Future work would require a human evaluation effort to draw more conclusions.,6 Analysis and future work,[0],[0]
This work was partially funded by Facebook.,Acknowledgments,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Facebook.",Acknowledgments,[0],[0]
We combine two of the most popular approaches to automated Grammatical Error Correction (GEC): GEC based on Statistical Machine Translation (SMT) and GEC based on Neural Machine Translation (NMT).,abstractText,[0],[0]
The hybrid system achieves new state-of-the-art results on the CoNLL-2014 and JFLEG benchmarks.,abstractText,[0],[0]
"This GEC system preserves the accuracy of SMT output and, at the same time, generates more fluent sentences as it typical for NMT.",abstractText,[0],[0]
Our analysis shows that the created systems are closer to reaching human-level performance than any other GEC system reported so far.,abstractText,[0],[0]
Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation,title,[0],[0]
"For large-scale matrix computations, exact algorithms are often too slow, so a large body of works focus on designing fast randomized approximation algorithms.",1. Introduction,[0],[0]
"To speedup the computation, matrix sketching is a commonly used technique, e.g. (Sarlos, 2006; Clarkson & Woodruff, 2013; Avron et al., 2013; Chierichetti et al., 2017).",1. Introduction,[0],[0]
"In real-world applications, the data often arrives in a streaming fashion and it is often impractical or impossible to store the entire data set in the main memory.
",1. Introduction,[0],[0]
"In this paper, we study online streaming algorithms for maintaining matrix sketches with small covariance errors.",1. Introduction,[0],[0]
"In the streaming model, the rows of the input matrix arrive one at a time; the algorithm is only allowed to make one pass over the stream with severely limited working space, which is required to maintain a sketch continuously.",1. Introduction,[0],[0]
"This problem has
1School of Data Science, Fudan University, China.",1. Introduction,[0],[0]
"Correspondence to: Zengfeng Huang <huangzf@fudan.edu.cn>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
received lots of attention recently (Liberty, 2013; Ghashami & Phillips, 2014; Woodruff, 2014; Ghashami et al., 2016; Wei et al., 2016).
",1. Introduction,[0],[0]
"The popular Frequent Directions algorithms (Liberty, 2013; Ghashami et al., 2016) achieve optimal tradeoff between space usage and approximation error (Woodruff, 2014), which have found lots of applications in online learning, e.g., (Boutsidis et al., 2015; Karnin & Liberty, 2015; Leng et al., 2015; Huang & Kasiviswanathan, 2015; Luo et al., 2016; Calandriello et al., 2017), and other problems (Song et al., 2015; Ye et al., 2016; Kim et al., 2016).",1. Introduction,[0],[0]
"However, it is unclear whether their running times can be improved; one might hope to get linear (in sparsity) time algorithms, which is possible for many matrix problems, e.g. (Clarkson & Woodruff, 2013).",1. Introduction,[0],[0]
"This paper is motivated by the following question:
• Is there an input sparsity time Frequent Directions, which achieves the same optimal space-error tradeoff ?",1. Introduction,[0],[0]
"Given a matrix A ∈ Rn×d, we want to compute a much smaller matrix B ∈ R`×d, which has low covariance error, i.e., ‖ATA−BTB‖2.",1.1. Problem definitions,[0],[0]
Definition 1 (Covariance Sketch).,1.1. Problem definitions,[0],[0]
"For any 0 < α < 1, and integer 0 ≤ k ≤ rank(A), we will call B an (α, k)-covsketch of A, if the covariance error1
‖ATA−BTB‖2 ≤ α‖A− [A]k‖2F .",1.1. Problem definitions,[0],[0]
"(1)
Here ‖ · ‖2 and ‖ · ‖F are the spectral norm and Frobenius norm of matrices;",1.1. Problem definitions,[0],[0]
"[A]k is the best rank-k approximation to A. We will use πkB(A) to denote the projection of A on the top-k singular vectors of B, i.e. πkB(A) = AV V
T , where the columns of V are the top-k right singular vectors of B. Definition 2 (Projection error).",1.1. Problem definitions,[0],[0]
"The projection error of B with respect to A is defined as ‖A− πkB(A)‖2F .
",1.1. Problem definitions,[0],[0]
"Note πkB(A) is a rank-k matrix, thereby the projection error is at least ‖A−[A]k‖2F .",1.1. Problem definitions,[0],[0]
"It is proved in (Ghashami & Phillips, 2014) that one can obtain relative projection error from small covariance error.",1.1. Problem definitions,[0],[0]
"We include a proof of the next lemma in the supplementary material.
",1.1. Problem definitions,[0],[0]
"1for k = 0, we define [A]0 = 0
Lemma 1 (covariance error to projection error (Ghashami & Phillips, 2014) (modified)).
‖A− πkB(A)‖2F ≤ ‖A−",1.1. Problem definitions,[0],[0]
"[A]k‖2F + 2k · ‖ATA−BTB‖2.
",1.1. Problem definitions,[0],[0]
"Therefore, any ( ε2k , k)-cov-sketch B has projection error
‖A− πkB(A)‖2F ≤ (1 + ε)‖A− [A]k‖2F .",1.1. Problem definitions,[0],[0]
"(2)
We will often refer to such sketches as (ε, k)-proj-sketches.
",1.1. Problem definitions,[0],[0]
Modern data matrices are often large and sparse.,1.1. Problem definitions,[0],[0]
"So we will always assume n and d are very large, typically d n, and nnz(A) nd, where nnz(A) in the number of nonzero entries in A. Moreover, we assume that each entry of A is representable by O(log(nd)) bits.",1.1. Problem definitions,[0],[0]
"To simplify the analysis, we assume the entries of A are integers of magnitude at most poly(nd); the general case can be reduced to this, see e.g. (Boutsidis et al., 2016).",1.1. Problem definitions,[0],[0]
"In the row-wise update streaming model, Liberty’s Frequent Direction (FD) algorithm (Liberty, 2013), with an improved analysis in (Ghashami & Phillips, 2014), maintains an (α, k)-cov-sketch B ∈ R`×d at any time, where ` = O(k+α−1).",1.2. Previous results,[0],[0]
The algorithm uses O(d`) space and runs inO(nd`) time.,1.2. Previous results,[0],[0]
"For sparse matrices, the running time of FD is improved to O(nnz(A)` log d+ nnz(A) log n+ n`2) by Ghashami et al. (Ghashami et al., 2016).",1.2. Previous results,[0],[0]
"Set α = ε/2k (or ` = O(k/ε)), and by Lemma 1, B is a (ε, k)-projsketch.",1.2. Previous results,[0],[0]
"Now, B contains O(k/ε) rows, the space and the running time become O(dk/ε) and O(nnz(A)kε−1 · log d+ nnz(A) log n+nk2ε−2) respectively.",1.2. Previous results,[0],[0]
"It was shown by Woodruff (Woodruff, 2014) that the space used by FD is optimal for both covariance error and projection error.",1.2. Previous results,[0],[0]
A natural question is if the running time can be improved.,1.2. Previous results,[0],[0]
"In particular,
• Is there an input sparsity time algorithm, i.e., in time O(nnz(A)",1.2. Previous results,[0],[0]
"+ (n + d) · poly(kα−1)), which achieves the same guarantee as FD?",1.2. Previous results,[0],[0]
This paper almost settles the above question.,1.3. Our contributions,[0],[0]
"Our main contributions are summarized as follows.
1.",1.3. Our contributions,[0],[0]
"We show that o(nnz(A)k) time is likely very difficult to achieve, as it will imply a breakthrough in fast matrix multiplication.",1.3. Our contributions,[0],[0]
"In particular, we prove that computing an (O(1), k)-cov-sketch B ∈ RO(k)×d of A is as hard as left multiplying A by an arbitrary matrix C ∈ Rk×n.
2.",1.3. Our contributions,[0],[0]
"We give a new space-optimal streaming algorithm with O(ndk) + Õ(dα−3) running time to compute (α, k)-covsketches for dense matrices, which improves the original
FD algorithm for small α.",1.3. Our contributions,[0],[0]
"The running time is optimal up to lower order terms, provided matrix multiplication cannot be improved significantly.
3.",1.3. Our contributions,[0],[0]
We then give a new space-optimal streaming algorithm withO(nnz(A)k+nnz(A) log n)+Õ(nk3+dα−3),1.3. Our contributions,[0],[0]
"running time to compute (α, k)-cov-sketches for sparse matrices.",1.3. Our contributions,[0],[0]
"We separate the dependence of 1/α from nnz(A), which improves the results of (Ghashami et al., 2016) for small α.",1.3. Our contributions,[0],[0]
"In particular, computing an (ε, k)-proj-sketch, our algorithm only needs O(nnz(A)k) time (ignoring lower order terms) as opposed to O(nnz(A)kε−1 · log d) in (Ghashami et al., 2016) (see Table 1).",1.3. Our contributions,[0],[0]
"Even when α is small, our algorithm improves a log d factor.",1.3. Our contributions,[0],[0]
"Moreover, for k = Ω(log n), the running time of our algorithm matches the lower bound.",1.3. Our contributions,[0],[0]
"The problem of computing (α, k)-cov-sketches was also studied in the sliding window streaming model (Wei et al., 2016) and distributed models (Ghashami et al., 2014; Huang et al., 2017).",1.4. Other related works,[0],[0]
"A closely related problem, namely approximate PCA, was studied in (Kannan et al., 2014; Liang et al., 2014; Boutsidis et al., 2016; Zhang et al., 2015).",1.4. Other related works,[0],[0]
"(Clarkson & Woodruff, 2009) studied other streaming numerical linear algebra problems.",1.4. Other related works,[0],[0]
"We always use n for the number rows, and d for the dimension of each row.",1.5. Matrix preliminaries and notations,[0],[0]
"For a d-dimensional vector x, ‖x‖ is the `2 norm of x.",1.5. Matrix preliminaries and notations,[0],[0]
"We use xi to denote the ith entry of x, and Diag(x) ∈ Rd×d is a diagonal matrix such that the ith diagonal entry is xi.",1.5. Matrix preliminaries and notations,[0],[0]
"Let A ∈ Rn×d with n > d, we use Ai to denote the ith row of A, and ai,j for the (i, j)-th entry of A. nnz(A) is the number of non-zero entries in A, and rows(A) is the number of rows in A.",1.5. Matrix preliminaries and notations,[0],[0]
"We write the (reduced) singular value decomposition of A as (U,Σ, V ) = SVD(A).",1.5. Matrix preliminaries and notations,[0],[0]
The computation time of standard SVD algorithms is O(nd2).,1.5. Matrix preliminaries and notations,[0],[0]
"We use ‖A‖2 or ‖A‖ to denote the spectral norm of A, which is the largest singular value of A, and ‖A‖F for the Frobenius Norm, which is √∑ i,j a 2 i,j .",1.5. Matrix preliminaries and notations,[0],[0]
"For k ≤ rank(A), we use [A]k to denote the best rank k approximation of A.",1.5. Matrix preliminaries and notations,[0],[0]
We define [A]0 = 0.,1.5. Matrix preliminaries and notations,[0],[0]
[A;B] is the matrix formed by concatenating the rows of A and B. We use Õ() to hide polylog(ndk) factors.,1.5. Matrix preliminaries and notations,[0],[0]
Frequent Directions.,1.6. Tools,[0],[0]
"We will use the Frequent Directions (FD) algorithm by Liberty (Liberty, 2013), denoted as FD(A,α, k), and the main result is summarized in the following theorem.
",1.6. Tools,[0],[0]
"Theorem 1 ((Liberty, 2013)).",1.6. Tools,[0],[0]
"Given A ∈ Rn×d, in one pass, FD(A,α, k) processes A in O(nd(k + α−1)) time
and O(d(k + α−1)) space.",1.6. Tools,[0],[0]
"It maintains a matrix B ∈ RO(k+α−1)×d such that ‖ATA−BTB‖2 ≤ α‖A−[A]k‖2F .
",1.6. Tools,[0],[0]
Row sampling.,1.6. Tools,[0],[0]
"We provide a result about row sampling, which is analogous to a result from (Drineas et al., 2006).",1.6. Tools,[0],[0]
"The difference is that they use iid sampling, i.e., each row of B is an iid sample from the rows ofA.",1.6. Tools,[0],[0]
"On the other hand, we use Bernoulli sampling, i.e., sample each Ai independently with some probability qi, and B is the set of sampled rows.",1.6. Tools,[0],[0]
Bernoulli sampling can easily be combined with FD in the streaming model.,1.6. Tools,[0],[0]
"The proof is essentially the same as that for iid sampling, which can be found in the supplementary material.",1.6. Tools,[0],[0]
Theorem 2.,1.6. Tools,[0],[0]
"For any A ∈ Rn×d and F > 0, we sample each row Ai with probability pi ≥",1.6. Tools,[0],[0]
"‖Ai‖ 2
α2F ; if it is sampled, scale it by 1/ √ pi.",1.6. Tools,[0],[0]
"Let B be the (rescaled) sampled rows, then w.p. 0.99, ‖ATA",1.6. Tools,[0],[0]
"− BTB‖2 ≤ 10α √ F‖A‖F , and ‖B‖F ≤ 10‖A‖F .",1.6. Tools,[0],[0]
"The expected number of rows sampled is O(‖A‖ 2 F
α2F ).
",1.6. Tools,[0],[0]
Input-sparsity time lower rank approximation algorithm.,1.6. Tools,[0],[0]
"There are nnz(A) (omitting lower order terms) time algorithms (Clarkson & Woodruff, 2013), which output a matrix Z consists of k orthonormal rows such that ‖A",1.6. Tools,[0],[0]
− ZTZA‖2F ≤,1.6. Tools,[0],[0]
(1 + ε)‖A,1.6. Tools,[0],[0]
−,1.6. Tools,[0],[0]
[A]k‖2F .,1.6. Tools,[0],[0]
"For our application, we only need a constant approximation, and only require Z to contain O(k) rows.",1.6. Tools,[0],[0]
"For this purpose, we give a simplified algorithm with slightly better running time than directly applying the results from (Clarkson & Woodruff, 2013).",1.6. Tools,[0],[0]
"We require high success probability, which can be achieved using similar techniques as in (Boutsidis et al., 2016).",1.6. Tools,[0],[0]
The proof of the following theorem can be found in the supplementary file.,1.6. Tools,[0],[0]
Theorem 3 (weak low rank approximation).,1.6. Tools,[0],[0]
"For any integers `, d, given A ∈ R`×d, there is an algorithm that uses O(nnz(A) log(1/δ))",1.6. Tools,[0],[0]
"+ Õ(`k3) time and O(`(k2 + log 1δ )) space, and outputs a matrix Z ∈ RO(k)×` with orthonormal rows such that with probability 1 − δ, ‖A",1.6. Tools,[0],[0]
− ZTZA‖2F ≤ O(1)‖A−,1.6. Tools,[0],[0]
[A]k‖2F .,1.6. Tools,[0],[0]
"In this section, we provide a conditional lower bound for our problem based on the idea of (Musco & Woodruff,
2017).",2. Time lower bound,[0],[0]
"We prove that the existence of algorithms which compute an (O(1), k)-cov-sketch in time o(nnz(A)k) implies a breakthrough in matrix multiplication, which is likely very difficult.",2. Time lower bound,[0],[0]
"In fact, the lower bound holds even for offline algorithms without constraints on working space.
",2. Time lower bound,[0],[0]
Theorem 4.,2. Time lower bound,[0],[0]
"Assume there is an algorithm A , which, given any A ∈ Rn×d with polynomially bounded integer entries, returns B ∈ RO(k)×d in time o(nnz(A)k) such that
‖ATA−BTB‖2 ≤ ∆‖A−",2. Time lower bound,[0],[0]
"[A]k‖2F ,
for some constant error parameter ∆. Then there is an o(nnz(M)k) + O(dk2) time algorithm for multiplying arbitrary polynomially bounded integer matrices MT ∈ R(d−k)×n, C ∈ Rn×k.
Proof.",2. Time lower bound,[0],[0]
"For any matrices M ∈ Rn×(d−k) and C ∈ Rn×k with integer entries in [−U,U ], let A ∈ Rn×d be the matrix which is a concatenation of the columns of M and wC, i.e., A = [M,wC] .",2. Time lower bound,[0],[0]
Here w is large number will be determined later.,2. Time lower bound,[0],[0]
We have ‖A−,2. Time lower bound,[0],[0]
"[A]k‖2F ≤ ‖M‖2F ≤ ndU2 and
ATA =
[ MTM wMTC
wCTM w2CTC
] .
",2. Time lower bound,[0],[0]
"We assume A is an algorithm with running time T , which can compute a sketch matrix B ∈ RO(k)×d of A such that
‖ATA−BTB‖2 ≤ ∆‖A−",2. Time lower bound,[0],[0]
[A]k‖2F ≤,2. Time lower bound,[0],[0]
"∆U2nd,
for some constant error parameter ∆.
The spectral norm of a matrix N is the largest singular value, which can be equivalently defined as ‖N‖2 = maxx,y:‖x‖=‖y‖=1",2. Time lower bound,[0],[0]
"x
TNy, therebyNi,j = eTi Nej ≤",2. Time lower bound,[0],[0]
"‖N‖2 for all i, j. It follows that (ATA − BTB)i,j ≤ ∆U2nd, meaning the corresponding block of BTB is an entry-wise approximation to wMTC within additive error ∆U2nd.
",2. Time lower bound,[0],[0]
"Now if w is a large integer, say w = d3∆U2nde, we can recover MTC from BTB exactly by rounding the numbers in BTB to their nearest integers (as MTC is an integer matrix).",2. Time lower bound,[0],[0]
"Note BTB can be computed in time O(dk2) given B and the rounding can be done in O(dk), so using A, the exact integer matrix multiplication MTC can be computed in time O(T + dk2).",2. Time lower bound,[0],[0]
"Therefore, we have proved that if T = o(nnz(A)k) = o(nnz(M)k + dk2), then MTC can
be computed in time o(nnz(M)k) +O(dk2), which will be a breakthrough in fast matrix multiplication.",2. Time lower bound,[0],[0]
"We remark that all the integers in our reduction are at most poly(nd) in magnitude, as long as U = poly(nd), so our reduction works for anyM,C with polynomially bounded entries.",2. Time lower bound,[0],[0]
"Theorem 5 (FFDdense(A,α, k)).",3. Algorithm for dense matrices,[0],[0]
"Given a matrix A ∈ Rn×d, 0 < α < 1 and 0 ≤ k ≤ d, FFDdense(A,α, k) processes A in one pass using O(ndk) + Õ(dα−3) time and O(dk + dα−1)) space, and maintains a matrix B. With probability 0.99, it holds that ‖ATA",3. Algorithm for dense matrices,[0],[0]
"− BTB‖2 ≤ α‖A− [A]k‖2F .
",3. Algorithm for dense matrices,[0],[0]
Overview of the algorithm.,3. Algorithm for dense matrices,[0],[0]
"To speed up FD, we will use the idea of adaptive random sampling.",3. Algorithm for dense matrices,[0],[0]
Let us first review the standard FD algorithm.,3. Algorithm for dense matrices,[0],[0]
"Given an integer parameter ` ≤ d, the algorithm always maintains a matrix B with at most 2` rows at any time.",3. Algorithm for dense matrices,[0],[0]
"When a new row v arrives, it processes the row using FDShrink(B, v, `) (Algorithm 3).",3. Algorithm for dense matrices,[0],[0]
"In this procedure, we first append a after B; if B has no more than 2` rows we do nothing, and otherwise we do a DenseShrink operation (Algorithm 1) on B, which halves the number of rows in B (after removing zero rows).",3. Algorithm for dense matrices,[0],[0]
"It was proved in (Liberty, 2013) and (Ghashami & Phillips, 2014)",3. Algorithm for dense matrices,[0],[0]
"that for ` = k + α−1, we have
‖ATA−BTB‖2 ≤ α‖A− [A]k‖2F .
",3. Algorithm for dense matrices,[0],[0]
"Since each SVD computation in DenseShrink takes O(d`2) time, and there are totally n/` SVD computations (SVD is applied every ` rows), the running time is O(nd`) = O(nd(k + α−1)).",3. Algorithm for dense matrices,[0],[0]
"Our goal is to separate nd from α−1 in the running time.
",3. Algorithm for dense matrices,[0],[0]
"Algorithm 1 DenseShrink Input: B ∈ R2`×d.
",3. Algorithm for dense matrices,[0],[0]
"1: Compute [U,Σ, V ] = SVD(B), and σ = Σ`,`. 2: Σ̂ = √ max(Σ2",3. Algorithm for dense matrices,[0],[0]
"− σ2I`, 0) I ReLu(x) = max(x, 0) 3: return B = Σ̂V T
Algorithm 2 DenseShrinkR Input: B ∈ R2`×d.
",3. Algorithm for dense matrices,[0],[0]
"1: Compute [U,Σ, V ] = SVD(B), and σ = Σ`,`. 2: Σ̂ = √ max(Σ2",3. Algorithm for dense matrices,[0],[0]
"− σ2I`, 0)
3: Σ̄ = √
Σ2 − Σ̂2",3. Algorithm for dense matrices,[0],[0]
"I Σ2 = Σ̄2 + Σ̂2 4: return B = Σ̂V T , Σ̄ and V T
To achieve this, we first compute a coarse approximation using FD by invoking B = FD(A, k, 12k ), which takes
Algorithm 3 FDShrink Input: B ∈ R`′×d, v ∈ Rd, and an integer `
I it always holds that `′ < 2`.",3. Algorithm for dense matrices,[0],[0]
1: B =,3. Algorithm for dense matrices,[0],[0]
"[B;v] 2: If `′ + 1 = 2`, then B = DenseShrink(B, `).",3. Algorithm for dense matrices,[0],[0]
"3: return B
O(ndk) time.",3. Algorithm for dense matrices,[0],[0]
"The key idea here is that in each DenseShrink operation, after shrinking B, we also return the residual; we call this modified shrinking operation DenseShrinkR (see Algorithm 2).",3. Algorithm for dense matrices,[0],[0]
Let C be the matrix which is the concatenation of all residuals return from DenseShrinkR. We will showATA = BTB+CTC and ‖C‖2F ≤ ‖A−[A]k‖2F .,3. Algorithm for dense matrices,[0],[0]
"We then refine the answer by computing an approximation to C. Since the norm of C is small, random sampling suffices.",3. Algorithm for dense matrices,[0],[0]
"To save space, the sampled rows will be fed to a standard FD algorithm.",3. Algorithm for dense matrices,[0],[0]
"See Algorithm 4 for detailed description of the algorithm.
",3. Algorithm for dense matrices,[0],[0]
"Algorithm 4 FFDdense Input: A ∈ Rn×d, 0",3. Algorithm for dense matrices,[0],[0]
"< α < 1, and integer k ≤ d.
1: F = 0, ` = 3k, Q = empty 2: for i = 1 to n do 3:",3. Algorithm for dense matrices,[0],[0]
"Append Ai after B 4: if rows(B) = 2` then 5: [B,Σ, V ] = DenseShrinkR(B) I Here ` = 3k, thus B = FD(A, 12k , k) 6: F = F + ‖Σ‖2F ,
I ### Next: subsample C := ΣV , and then compressed the sampled rows using standard FD
7: for j = 1 to 2` do 8: pj = Σ2j α2F
9:",3. Algorithm for dense matrices,[0],[0]
Sample Cj with probability pj .,3. Algorithm for dense matrices,[0],[0]
10: if Cj is sampled then 11: Set v = Cj√pj .,3. Algorithm for dense matrices,[0],[0]
"12: Q = FDShrink([Q : v], 1α )",3. Algorithm for dense matrices,[0],[0]
"I Invoking FD with k = 0 13: end if 14: end for 15: end if 16: end for 17: return [B;Q]
Correctness.",3. Algorithm for dense matrices,[0],[0]
"We note that, at the end of Algorithm 4, B = FD(A, 12k , k), so ‖A
TA−BTB‖2 ≤ ‖A−",3. Algorithm for dense matrices,[0],[0]
"[A]k‖2F /2k, or equivalently
max x:‖x‖=1
| ‖Ax‖2 − ‖Bx‖2 | ≤ ‖A−",3. Algorithm for dense matrices,[0],[0]
[A]k‖2F /2k.,3. Algorithm for dense matrices,[0],[0]
"(3)
Let Σ(i), V (i), and B(i) be the value of Σ, V , and B respectively returned by ith DenseShrinkR operation (line 5).",3. Algorithm for dense matrices,[0],[0]
"Let
C(i) = Σ(i)V",3. Algorithm for dense matrices,[0],[0]
(i).,3. Algorithm for dense matrices,[0],[0]
We use B′(i) to denote the value of B right before the ith DenseShrinkR operation (or the input of the ith DenseShrinkR operation) .,3. Algorithm for dense matrices,[0],[0]
"From Algorithm 2, we have that
B′(i)TB′(i) = B(i)TB(i) + V",3. Algorithm for dense matrices,[0],[0]
"(i)TΣ(i)2V (i)
= B(i)TB(i) + C(i)TC(i).
",3. Algorithm for dense matrices,[0],[0]
"Let A(i) be the rows of A arrived between the (i − 1)th and the ith DenseShrinkR operation, which means B′(i) =",3. Algorithm for dense matrices,[0],[0]
"[B(i−1);A(i)], and thus
B′(i)TB′(i) = B(i−1)TB(i−1) +A(i)TA(i).
",3. Algorithm for dense matrices,[0],[0]
"Combined with the previous equality, we get
A(i)TA(i) +B(i−1)TB(i−1) −B(i)TB(i) = C(i)TC(i).
",3. Algorithm for dense matrices,[0],[0]
Let t be the total number of iterations.,3. Algorithm for dense matrices,[0],[0]
"We define B(0) = 0, and C =",3. Algorithm for dense matrices,[0],[0]
[C(1); · · · ;C(t)].,3. Algorithm for dense matrices,[0],[0]
"Summing the above equality over i = 1, · · · , t, we have
CTC = ∑ i C(i)TC(i)
= ∑ i ( A(i)TA(i) +B(i−1)TB(i−1) −B(i)TB(i) )",3. Algorithm for dense matrices,[0],[0]
"= ATA−BTB.
",3. Algorithm for dense matrices,[0],[0]
"It follows that
‖C‖2F = trace(CTC) = trace(ATA)− trace(BTB) = ‖A‖2F − ‖B‖2F .
",3. Algorithm for dense matrices,[0],[0]
"Now we bound ‖A‖2F − ‖B‖2F using similar ideas as in (Ghashami & Phillips, 2014).",3. Algorithm for dense matrices,[0],[0]
"Let wj be the jth singular vector of A, we have ‖C‖2F = ‖A‖2F",3. Algorithm for dense matrices,[0],[0]
"− ‖B‖2F
= k∑ j=1 ‖Awj‖2 + d∑ j=k+1 ‖Awj‖2",3. Algorithm for dense matrices,[0],[0]
− ‖B‖2F ≤ k∑ j=1 ‖Awj‖2,3. Algorithm for dense matrices,[0],[0]
+ ‖A−,3. Algorithm for dense matrices,[0],[0]
"[A]k‖2F − k∑ j=1 ‖Bwj‖2
because k∑ j=1 ‖Bwj‖2 ≤ ‖B‖2F
≤ ‖A−",3. Algorithm for dense matrices,[0],[0]
[A]k‖2F + k · ‖A−,3. Algorithm for dense matrices,[0],[0]
[A]k‖2F /2k,3. Algorithm for dense matrices,[0],[0]
by Eq (3) = 1.5‖A− [A]k‖2F .,3. Algorithm for dense matrices,[0],[0]
"(4)
In the algorithm, each row of C is sampled with probability ‖Cj‖2",3. Algorithm for dense matrices,[0],[0]
"α2F , where F is the current squared F-norm of C. Let Cs be the sampled rows.",3. Algorithm for dense matrices,[0],[0]
"Given Eq (4), we can prove the following using Theorem 2
‖CTC",3. Algorithm for dense matrices,[0],[0]
"− CTs Cs‖2 ≤ α‖A− [A]k‖2F , and
‖Cs‖2F = O(1) · ‖A−",3. Algorithm for dense matrices,[0],[0]
"[A]k‖2F .
",3. Algorithm for dense matrices,[0],[0]
"At the end of the algorithm, Q = FD(Cs, α, 0), then
‖CTs Cs −QTQ‖2 ≤",3. Algorithm for dense matrices,[0],[0]
α‖Cs‖2F ≤ O(α),3. Algorithm for dense matrices,[0],[0]
"· ‖A− [A]k‖2F .
",3. Algorithm for dense matrices,[0],[0]
"Applying triangle inequality, we have ‖CTC −QTQ‖2 ≤ O(α) · ‖A−",3. Algorithm for dense matrices,[0],[0]
"[A]k‖2F , and thus
‖ATA−BTB −QTQ‖2 = ‖CTC −QTQ‖2 ≤ O(α) · ‖A− [A]k‖2F ,
which proves the correctness.
",3. Algorithm for dense matrices,[0],[0]
Space and running time.,3. Algorithm for dense matrices,[0],[0]
"The space is dominated by maintainingB = FD(A, 1/2k, k)",3. Algorithm for dense matrices,[0],[0]
"andQ = FD(Cs, α, 0), which is O(dk + d/α) in total.
",3. Algorithm for dense matrices,[0],[0]
"The running time of computing B is O(ndk), and the running time for Q is O(rows(Cs)d/α).",3. Algorithm for dense matrices,[0],[0]
"To bound rows(Cs), we divide the stream into epochs, where F roughly doubles in each epoch.",3. Algorithm for dense matrices,[0],[0]
"This means the total number of epochs is bounded by O(log(nd)), since we assume each real number in the input can be represented by O(log(nd)) bits2.",3. Algorithm for dense matrices,[0],[0]
"Applying Theorem 2 on the submatrix in each epoch, it is easy to check the expected number of rows sampled in each epoch is O(1/α2), so rows(Cs) =",3. Algorithm for dense matrices,[0],[0]
O( log(nd) α2 ).,3. Algorithm for dense matrices,[0],[0]
Thus the total running time is O(ndk) + Õ(dα−3).,3. Algorithm for dense matrices,[0],[0]
"We remark that the residual return by DenseShrinkR is in the form of C = ΣV T , where Σ is diagonal and V has orthonormal columns.",3. Algorithm for dense matrices,[0],[0]
"Therefore, the row norms of C are simply the diagonals of Σ.",3. Algorithm for dense matrices,[0],[0]
"Our approach is quite different from (Ghashami et al., 2016).",4.1. Overview of our algorithm,[0],[0]
"Their main idea is to use fast approximate SVD (Musco & Musco, 2015) in the original FD, which leads to suboptimal time.",4.1. Overview of our algorithm,[0],[0]
"Our approach is summarized as follows.
1.",4.1. Overview of our algorithm,[0],[0]
"Decompose ATA = A′TA′ +RTR, such that A′ contains small number of rows and ‖A′−[A′]k‖2F = O(1)· ‖A−",4.1. Overview of our algorithm,[0],[0]
[A]k‖2F .,4.1. Overview of our algorithm,[0],[0]
"Moreover, ‖R‖2F = O(1) ·‖A− [A]k‖2F .
2.",4.1. Overview of our algorithm,[0],[0]
"Compute a sketch B of A′ using fast FD algorithm for dense matrices (Theorem 5), which satisfies that ‖A′TA′",4.1. Overview of our algorithm,[0],[0]
− BTB‖2 ≤ α‖A′,4.1. Overview of our algorithm,[0],[0]
"− [A′]k‖2F ≤ α‖A − [A]k‖2F .
3.",4.1. Overview of our algorithm,[0],[0]
Compute a sketch matrix C of R such that ‖RTR,4.1. Overview of our algorithm,[0],[0]
− CTC‖2 ≤ α‖R‖2F ≤ O(α) ·,4.1. Overview of our algorithm,[0],[0]
‖A−,4.1. Overview of our algorithm,[0],[0]
"[A]k‖2F , which can be done via random sampling (Theorem 2) combined with FD.
",4.1. Overview of our algorithm,[0],[0]
"2A rigorous analysis on this will be more subtle; see discussions in the proof of Lemma 6 below.
4.",4.1. Overview of our algorithm,[0],[0]
The final sketch is S =,4.1. Overview of our algorithm,[0],[0]
"[B;C].
Note that S =",4.1. Overview of our algorithm,[0],[0]
[B;C] approximate,4.1. Overview of our algorithm,[0],[0]
"[A′;R] in the sense that
‖A′TA′ +RTR−BTB − CTC‖2 ≤",4.1. Overview of our algorithm,[0],[0]
"‖A′TA′ −BTB‖2 + ‖RTR− CTC‖2 ≤ O(α) · ‖A− [A]k‖2F .
",4.1. Overview of our algorithm,[0],[0]
"From step (1), we have ATA = A′TA′ + RTR, and thus [B;C] is a good approximation of A.",4.1. Overview of our algorithm,[0],[0]
"Next we briefly describe how to implement this in one pass and small space.
",4.1. Overview of our algorithm,[0],[0]
"To achieve (1), we use the following new idea.",4.1. Overview of our algorithm,[0],[0]
Let Z ∈ RO(k)×d be an orthonormal matrix satisfying ‖A,4.1. Overview of our algorithm,[0],[0]
− ZTZA‖2F ≤ O(1)‖A,4.1. Overview of our algorithm,[0],[0]
− [A]k‖2F .,4.1. Overview of our algorithm,[0],[0]
Let A′ = ZA and R = (I − ZTZ)A.,4.1. Overview of our algorithm,[0],[0]
It is easy to check that A′ and R satisfy the requirement of (1).,4.1. Overview of our algorithm,[0],[0]
"In the streaming model, we divide A into blocks, each of which contains roughly dk non-zero entries, and thus there are at most t = nnz(A)dk blocks.",4.1. Overview of our algorithm,[0],[0]
We use the above idea for each of the blocks and concatenate the results together.,4.1. Overview of our algorithm,[0],[0]
"More precisely, for each block A(i) ∈ R`i×d, we use an input-sparsity time algorithm (Theorem 3) to compute a matrix Z(i) ∈ RO(k)×`i such that
‖A(i)",4.1. Overview of our algorithm,[0],[0]
− Z(i)TZ(i)A(i)‖2F ≤ 4‖A(i),4.1. Overview of our algorithm,[0],[0]
"− [A(i)]k‖2F .
",4.1. Overview of our algorithm,[0],[0]
"Let A′(i) = Z(i)A(i), and R(i) = (I − Z(i)TZ(i))A(i).",4.1. Overview of our algorithm,[0],[0]
We then set A′ =,4.1. Overview of our algorithm,[0],[0]
[A′(1); · · · ;A′(t)] and R =,4.1. Overview of our algorithm,[0],[0]
"[R(1); · · · ;R(t)], and prove that A′ and R satisfy the requirement of (1), where A′ only has t×O(k) =",4.1. Overview of our algorithm,[0],[0]
O( nnz(A)d ) rows (since each block of A′ has O(k) rows).,4.1. Overview of our algorithm,[0],[0]
"Here we do not compute R explicitly, as we will sample a subset of the rows from R. Note that the running time of this step is dominated by computing Z(i)A(i), which is O(nnz(A(i))k), and thus O(nnz(A)k) in total.
",4.1. Overview of our algorithm,[0],[0]
"To compute B of step (2), we may use the standard FD(A′, α, k) (Theorem 1).",4.1. Overview of our algorithm,[0],[0]
"Since A′ has at most O( nnz(A)d ) rows, B can be computed in O(nnz(A)(k + α−1)) time.",4.1. Overview of our algorithm,[0],[0]
"However, it still has an nnz(A)α−1 term.",4.1. Overview of our algorithm,[0],[0]
"So we apply our faster FD algorithm for dense matrices (Theorem 5) on A′, which only takes O(nnz(A)k) + Õ(dα−3) time.
",4.1. Overview of our algorithm,[0],[0]
"In order to compute a sketch C of R in step (3), we first subsample the rows of R using streaming Bernoulli sampling.",4.1. Overview of our algorithm,[0],[0]
"One difficulty is that R could be dense, and it may take nd time to compute the norms of the rows.",4.1. Overview of our algorithm,[0],[0]
"Fortunately, constant approximations of the norms are good enough, and thus we can use Johnson-Lindenstrauss (JL) (Johnson & Lindenstrauss, 1984) transform to reduce the dimensionality of R from d to O(log n).",4.1. Overview of our algorithm,[0],[0]
"Let Φ ∈ RO(logn)×d be a JL transform, then RΦT can be computed in time O(nnz(A) log n+nk log n).",4.1. Overview of our algorithm,[0],[0]
"Now we only need to compute the norms of the rows in RΦT , which are constant approximations to the row norms inR (by JL Lemma).",4.1. Overview of our algorithm,[0],[0]
"LetQ be the
sampled rows, with rows(Q) = Õ(1/α2), and each row of Q can be computed in time O(kd) as Z(i)A(i) has already been computed in step (1).",4.1. Overview of our algorithm,[0],[0]
"We finally use FD(Q,α, 0) to compute a sketch matrix C of Q in time Õ(dα−3).
",4.1. Overview of our algorithm,[0],[0]
"In all, the running time is roughly O(nnz(A)(k + log n))",4.1. Overview of our algorithm,[0],[0]
+ Õ(dα−3 + dkα−2).,4.1. Overview of our algorithm,[0],[0]
"Algorithm 5 FFDsparse Input: A ∈ Rn×d, α ∈ (0, 1), and integers k ≤",4.2. Our algorithm,[0],[0]
"d.
1: F = η, F ′ = 0, B = 0, Q = 0",4.2. Our algorithm,[0],[0]
"I η will be determined in Lemma 6 2: Divide the rows of A into continuous blocks A(1), · · · , A(t): we will put new rows into the current block until: a) the number of non-zero entries exceeds dk, or b) the number of rows is dk log(nd) .",4.2. Our algorithm,[0],[0]
"When either a) or b) happens, we start a new block.",4.2. Our algorithm,[0],[0]
Note that the total number of blocks t ≤ nnz(A)dk + nk log(nd) d .,4.2. Our algorithm,[0],[0]
3: for i = 1 to t do 4: Use Theorem 3 to compute Z(i) such that ‖A(i),4.2. Our algorithm,[0],[0]
"−
Z(i)TZ(i)A(i)‖2F ≤",4.2. Our algorithm,[0],[0]
O(1)‖A(i),4.2. Our algorithm,[0],[0]
−,4.2. Our algorithm,[0],[0]
[A(i)]k‖2F with probability 1− 1/n2.,4.2. Our algorithm,[0],[0]
"Let `i = rows(A(i)).
5: Compute A′(i) = ZA(i).",4.2. Our algorithm,[0],[0]
6: Compute W =,4.2. Our algorithm,[0],[0]
"(I − Z(i)TZ(i))A(i)Φ, where Φ ∈
Rd×O(logn) is a dense JL transform (each entry is an iid Gaussian).",4.2. Our algorithm,[0],[0]
"Compute all the rows norms of W , and let w ∈ R`i be the vector of these norms.
7: F ′",4.2. Our algorithm,[0],[0]
"= F ′ + ‖w‖2, and if F ′",4.2. Our algorithm,[0],[0]
≥,4.2. Our algorithm,[0],[0]
"2F , F = F ′. I F = O(1) · ∑ i(I − Z(i)TZ(i))A(i) by JL 8: Let p ∈ R`i such that pj = w 2",4.2. Our algorithm,[0],[0]
"i
α2F for j = 1, · · · , `i. Let x ∈ R`i be a random vector with iid entries.",4.2. Our algorithm,[0],[0]
"For each j, xj = 1/pj w.p. pj , and xj = 0 w.p. 1− pj .
9: Let R(i) =",4.2. Our algorithm,[0],[0]
(I − Z(i)TZ(i))A(i) and Q(i) = Diag(x) ·R(i) (no need to compute R(i) explicitly).,4.2. Our algorithm,[0],[0]
10: B = FFDdense([B;A′(i),4.2. Our algorithm,[0],[0]
"], α, k).",4.2. Our algorithm,[0],[0]
I Sketching A′ =,4.2. Our algorithm,[0],[0]
"[A′(1); · · · ;A′(t)] using Theorem 5 11: C = FD([C;Q(i)], α, 0).",4.2. Our algorithm,[0],[0]
I Sketching Q =,4.2. Our algorithm,[0],[0]
[Q(1); · · · ;Q(t)] using FD.,4.2. Our algorithm,[0],[0]
"12: end for 13: return [B;C]
Theorem 6 (FFDsparse).",4.2. Our algorithm,[0],[0]
"Given any matrix A ∈ Rn×d, 0 < α < 1 and 0 ≤ k ≤ d, FFDsparse(A,α, k) (Algorithm 5) maintains a matrix S in a streaming fashion, such that
‖ATA− STS‖2 ≤ α‖A− [A]k‖2F .
",4.2. Our algorithm,[0],[0]
"The algorithm uses O(d(k + α−1)) space and runs in O(nnz(A)k + nnz(A) log n) + Õ(nk3 + d · poly(kα−1)).
",4.2. Our algorithm,[0],[0]
"By Lemma 1, we also have the following result.
",4.2. Our algorithm,[0],[0]
Theorem 7.,4.2. Our algorithm,[0],[0]
"Given any matrix A ∈ Rn×d, 0 < ε < 1 and 0 < k ≤ d, there is a streaming algorithm which maintains a strong (ε, k)-proj-sketch S ∈ RO(k/ε)×d.",4.2. Our algorithm,[0],[0]
The algorithm uses O(dk/ε) space and runs in O(nnz(A)k + nnz(A) log n) + Õ(nk3 + d · poly(kε−1)).,4.2. Our algorithm,[0],[0]
Proof of Theorem 6.,4.3. Proof of Theorem 6,[0],[0]
"The detail of our fast algorithm for sparse matrix is described in Algorithm 5.
",4.3. Proof of Theorem 6,[0],[0]
We let A′ =,4.3. Proof of Theorem 6,[0],[0]
"[A′(1); · · · ;A′(t)], R =",4.3. Proof of Theorem 6,[0],[0]
"[R(1); · · · ;R(t)], and Q =",4.3. Proof of Theorem 6,[0],[0]
[Q(1); · · · ;Q(t)].,4.3. Proof of Theorem 6,[0],[0]
We use w(i) to denote the vector w in ith iteration.,4.3. Proof of Theorem 6,[0],[0]
"We need some technical lemmas.
",4.3. Proof of Theorem 6,[0],[0]
Lemma 2.,4.3. Proof of Theorem 6,[0],[0]
With probability at least 1,4.3. Proof of Theorem 6,[0],[0]
"− 1/n, (1) ‖A′",4.3. Proof of Theorem 6,[0],[0]
− [A′]k‖F ≤ ‖A−,4.3. Proof of Theorem 6,[0],[0]
"[A]k‖F ; (2) ‖R‖2F ≤ O(1) ·‖A− [A]k‖2F .
Proof.",4.3. Proof of Theorem 6,[0],[0]
"We divide A and A′ into blocks as defined in Algorithm 5, i.e. A = [A(1); · · · ;A(t)] and A′ =",4.3. Proof of Theorem 6,[0],[0]
[A′(1); · · · ;A(t)].,4.3. Proof of Theorem 6,[0],[0]
"For each i, we have A′(i) = Z(i)A(i) for some matrix Z(i) with O(k) orthonormal rows such that, with probability at least 1− 1/n2,
‖A(i)−Z(i)TZ(i)A(i)‖2F ≤ O(1) ·‖A(i)− [A(i)]k‖2F .",4.3. Proof of Theorem 6,[0],[0]
"(5)
",4.3. Proof of Theorem 6,[0],[0]
"By union bound, with probability at least 1− 1/n, eqn (5) holds for all i simultaneously.",4.3. Proof of Theorem 6,[0],[0]
Let P be the projection matrix onto the subspace spanned by the top-k right singular vectors of A.,4.3. Proof of Theorem 6,[0],[0]
"So we have
‖A′",4.3. Proof of Theorem 6,[0],[0]
−,4.3. Proof of Theorem 6,[0],[0]
"[A′]k‖2F ≤ ‖A′ −A′P‖2F
= t∑ i=1 ‖A′(i) −A′(i)P‖2F",4.3. Proof of Theorem 6,[0],[0]
"P has rank k
= t∑ i=1 ‖Z(i)A(i)",4.3. Proof of Theorem 6,[0],[0]
"− Z(i)A(i)P‖2F
≤ t∑ i=1 ‖A(i) −A(i)P‖2F Z(i) is a orthogonal",4.3. Proof of Theorem 6,[0],[0]
= ‖A−AP‖2F = ‖A−,4.3. Proof of Theorem 6,[0],[0]
"[A]k‖2F , by definition of P
which proves (1).
",4.3. Proof of Theorem 6,[0],[0]
"As defined in Algorithm 5, R(i) = (I − Z(i)TZ(i))A(i), where Z(i) satisfies (5).",4.3. Proof of Theorem 6,[0],[0]
"Therefore,
‖R‖2F = t∑ i=1 ‖(I",4.3. Proof of Theorem 6,[0],[0]
"− Z(i)TZ(i))A(i)‖2F
≤ O(1) · t∑ i=1 ‖A(i)",4.3. Proof of Theorem 6,[0],[0]
− [A(i)]k‖2F = O(1) · ‖A−,4.3. Proof of Theorem 6,[0],[0]
"[A]k‖2F
which proves (2).
",4.3. Proof of Theorem 6,[0],[0]
Lemma 3.,4.3. Proof of Theorem 6,[0],[0]
ATA = RTR+A′TA′; with probability 1−1/n,4.3. Proof of Theorem 6,[0],[0]
it holds that ‖ATA−A′TA′‖F ≤ O(1) · ‖A−,4.3. Proof of Theorem 6,[0],[0]
"[A]k‖2F .
",4.3. Proof of Theorem 6,[0],[0]
Proof.,4.3. Proof of Theorem 6,[0],[0]
"To prove the first part, we only need to prove A(i)TA(i) = R(i)TR(i) +A′(i)TA′(i) holds for all i. Recall that A′(i) = Z(i)A(i).",4.3. Proof of Theorem 6,[0],[0]
"For each i, we have
R(i)TR(i) = A(i)T (I − Z(i)TZ(i)) · (I − Z(i)TZ(i))A(i)
= A(i)T (I − Z(i)TZ(i))A(i)
= A(i)TA(i) −A′(i)TA′(i).
",4.3. Proof of Theorem 6,[0],[0]
"This proves the first part, from which, we also get
‖ATA−A′TA′‖F = ‖RTR‖F ≤ ‖R‖2F ,
where the inequality is from the submultiplicative of matrix norms.",4.3. Proof of Theorem 6,[0],[0]
"Then the second part follows from Lemma 2.
",4.3. Proof of Theorem 6,[0],[0]
Lemma 4.,4.3. Proof of Theorem 6,[0],[0]
"If the entries of A are integers bounded in magnitude by poly(nd) and rank(A) ≥ 1.1k, then ‖A − [A]k‖2F ≥ 1/poly(nd).
",4.3. Proof of Theorem 6,[0],[0]
Proof.,4.3. Proof of Theorem 6,[0],[0]
"The lemma directly follows from a result of (Clarkson & Woodruff, 2009), and here we use the restated version from (Boutsidis et al., 2016).
",4.3. Proof of Theorem 6,[0],[0]
"Lemma 5 (Lemma 37 of (Boutsidis et al., 2016))",4.3. Proof of Theorem 6,[0],[0]
.,4.3. Proof of Theorem 6,[0],[0]
"If an n× d matrix A has integer entries bounded in magnitude by γ, and has rank ρ, then the k-th largest singular value of A satisfies
σk ≥ (ndγ2)−k/2(ρ−k).
",4.3. Proof of Theorem 6,[0],[0]
Lemma 6.,4.3. Proof of Theorem 6,[0],[0]
"We set η = poly−1(nd), then with probability at least 0.99, it holds that
‖Q‖2F = O(‖R‖2F ), ‖RTR−QTQ‖2 = O(α)·‖A−[A]k‖2F ,
and rows(Q) = O(log(nd)/α2).
",4.3. Proof of Theorem 6,[0],[0]
Proof.,4.3. Proof of Theorem 6,[0],[0]
Let us first assume rank(A) ≥ 1.1k.,4.3. Proof of Theorem 6,[0],[0]
"Each row Ri ofR is sampled with probability Θ(‖Ri‖ 2
α2F )",4.3. Proof of Theorem 6,[0],[0]
"(by JL property), with F initialized to be η.",4.3. Proof of Theorem 6,[0],[0]
We have η ≤ ‖A−,4.3. Proof of Theorem 6,[0],[0]
[A]k‖2F (by Lemma 4).,4.3. Proof of Theorem 6,[0],[0]
"When ‖R‖2F ≥ η, the probability is at least Ω( ‖Ri‖ 2
α2‖R‖2F ) (since F will be a constant approximation of ‖R‖2F by JL), so the first two parts directly follow from Theorem 2 and Lemma 2.",4.3. Proof of Theorem 6,[0],[0]
"Otherwise if ‖R‖2F ≤ η, then the probability is Ω(‖Ri‖ 2
α2η )",4.3. Proof of Theorem 6,[0],[0]
"= Ω( ‖Ri‖2
α2‖A−[A]k‖2F ) the first two
parts follow by Theorem 2.
",4.3. Proof of Theorem 6,[0],[0]
"To bound the number of rows sampled, we divide the stream into epochs, where F roughly doubles in each epoch.",4.3. Proof of Theorem 6,[0],[0]
"So the total number of epochs is bounded by O(log ‖R‖Fη ), as the final value of F is at most O(‖R‖2F ).",4.3. Proof of Theorem 6,[0],[0]
"As we assume each
entry of A is integer bounded in magnitude by poly(nd), which implies the number of epochs is
O(log ‖R‖2F η
) ≤",4.3. Proof of Theorem 6,[0],[0]
O ( log ( ‖A‖2F · poly(nd) )),4.3. Proof of Theorem 6,[0],[0]
"= O(log(nd)).
",4.3. Proof of Theorem 6,[0],[0]
"The number of rows sampled in each epoch is at most O(1/α2): let a1, · · · , at be the rows of R in the epoch, and thus ∑ j ‖aj‖2 ≤ O(F )",4.3. Proof of Theorem 6,[0],[0]
"(otherwise the epoch ends); each row aj is sampled with probability Θ( ‖aj‖2 α2F ), which implies the total number of rows sampled is O(1/α2).",4.3. Proof of Theorem 6,[0],[0]
"This proves the third part.
",4.3. Proof of Theorem 6,[0],[0]
"The case rank(A) ≤ 1.1k is easier: we can set the rank parameter k a little larger (say k′ = 2k) in our algorithm so that R is always 0 by Lemma 2, and thus rows(Q) = 0.",4.3. Proof of Theorem 6,[0],[0]
"In this case, our algorithm is essentially exact.
",4.3. Proof of Theorem 6,[0],[0]
Correctness.,4.3. Proof of Theorem 6,[0],[0]
"By union bound, with probability 0.9 all the above lemmas hold simultaneously, and we will assume this happens.",4.3. Proof of Theorem 6,[0],[0]
"Since C = FD(Q,α, 0), by Theorem 1 and Lemma 6, we have
‖QTQ− CTC‖2 ≤ α‖Q‖2F ≤ O(α) · ‖R‖2F .",4.3. Proof of Theorem 6,[0],[0]
"(6)
Since B is a matrix such that B = FFDdense(A′, α, k), by Theorem 5, we have
‖A′TA′ −BTB‖2 ≤ α‖A′",4.3. Proof of Theorem 6,[0],[0]
− [A′]k‖2F ≤,4.3. Proof of Theorem 6,[0],[0]
α‖A−,4.3. Proof of Theorem 6,[0],[0]
"[A]k‖2F , (7)
where the last inequality is from Lemma 2. Let S =",4.3. Proof of Theorem 6,[0],[0]
"[B;C],
‖ATA− STS‖2 = ‖ATA−BTB",4.3. Proof of Theorem 6,[0],[0]
− CTC‖2 ≤ ‖ATA−A′TA′,4.3. Proof of Theorem 6,[0],[0]
− CTC‖2 + ‖A′TA′ −BTB‖2 ≤ ‖ATA−A′TA′,4.3. Proof of Theorem 6,[0],[0]
− CTC‖2 + α‖A−,4.3. Proof of Theorem 6,[0],[0]
[A]k‖2F by (7),4.3. Proof of Theorem 6,[0],[0]
= ‖RTR−,4.3. Proof of Theorem 6,[0],[0]
"CTC‖2 + α‖A− [A]k‖2F ≤ ‖RTR−QTQ‖2 + ‖QTQ− CTC‖2 + α‖A− [A]k‖2F
triangle inequality
≤ O(α) ·",4.3. Proof of Theorem 6,[0],[0]
‖A−,4.3. Proof of Theorem 6,[0],[0]
"[A]k‖2F +O(α) · ‖R‖2F + α‖A− [A]k‖2F by (6) and Lemma 6
≤ O(α) · ‖A−",4.3. Proof of Theorem 6,[0],[0]
"[A]k‖2F , by Lemma 2
which proves the error bound after adjusting α by a constant.
",4.3. Proof of Theorem 6,[0],[0]
Space and running time.,4.3. Proof of Theorem 6,[0],[0]
"For space, we need a buffer to store a new block ofA, the size of which is at most dk+d, as the nnz(A(i))",4.3. Proof of Theorem 6,[0],[0]
"is at most dk+ d. When applying Theorem 3, we set δ = 1/n2, and the input matrix has at most dk log(nd) rows, so we need O(dk) space to compute and store Z. A′(i) is of dimension O(k)× d, which needs O(dk) space to compute and store.",4.3. Proof of Theorem 6,[0],[0]
"To naively compute W = (I − ZTZ)A(i)Φ, we need O(dk + d log n) space.",4.3. Proof of Theorem 6,[0],[0]
"However,
observe that we do not have to compute W explicitly, we only need to know its row norms, i.e. the vector w. To save space, we compute one column of W at a time, i.e., generate columns of Φ one by one, and update w iteratively, and thus the extra space used is O(d).",4.3. Proof of Theorem 6,[0],[0]
"From Theorem 5, the space used by FFDdense(A′, α, k) is O(d(k + α−1)).",4.3. Proof of Theorem 6,[0],[0]
"Note that, in line 11 of Algorithm 5, the rows of Q(i) can be computed one by one, and thus compute C = FD(Q,α, 0) uses O(d/α) space by theorem 1.",4.3. Proof of Theorem 6,[0],[0]
"So the total space usage is bounded by O(d(k + α−1)).
",4.3. Proof of Theorem 6,[0],[0]
Let `i be the number of rows in ith block A(i).,4.3. Proof of Theorem 6,[0],[0]
The time to compute Z using Theorem 3 is O(nnz(A(i) log n) +,4.3. Proof of Theorem 6,[0],[0]
"Õ(`ik
3).",4.3. Proof of Theorem 6,[0],[0]
Hence the total time used on this step is∑ i O(nnz(A(i) log n),4.3. Proof of Theorem 6,[0],[0]
+,4.3. Proof of Theorem 6,[0],[0]
"Õ(`ik 3) =
O(nnz(A) log n)",4.3. Proof of Theorem 6,[0],[0]
+,4.3. Proof of Theorem 6,[0],[0]
"Õ(nk3).
",4.3. Proof of Theorem 6,[0],[0]
"The step to compute the matrix multiplicationA′(i) = ZA(i) takes O(nnz(A(i))k) time, since Z has O(k) rows.",4.3. Proof of Theorem 6,[0],[0]
So the total time spent on this step is O(nnz(A)k).,4.3. Proof of Theorem 6,[0],[0]
"By the definition of blocks, there are at most O( nnz(A)dk + nk log(nd) d ) blocks.",4.3. Proof of Theorem 6,[0],[0]
"After left multiplying Z, each block contributes O(k) rows to A′, and thus the total number of rows in A′ is at most O( nnz(A)d + nk2 log(nd)
d ).",4.3. Proof of Theorem 6,[0],[0]
"Computing B by invoking B = FFDdense(A′, α, k) needs O(rows(A′)dk)",4.3. Proof of Theorem 6,[0],[0]
+ Õ(d/α3),4.3. Proof of Theorem 6,[0],[0]
= O(nnz(A)k) + Õ(nk3 + d/α3) time.,4.3. Proof of Theorem 6,[0],[0]
"Finally, each row of Q can be computed in time O(dk) given A′ (which has been computed in line 5).",4.3. Proof of Theorem 6,[0],[0]
"Invoking C = FD(Q,α, 0) needs Õ(d/α3 + dk/α2) since rows(Q) = Õ(1/α2) by Lemma 6.",4.3. Proof of Theorem 6,[0],[0]
The total time is thus O(nnz(A)k + nnz(A) log n),4.3. Proof of Theorem 6,[0],[0]
+ Õ(nk3,4.3. Proof of Theorem 6,[0],[0]
+,4.3. Proof of Theorem 6,[0],[0]
dα−3 + dkα−2).,4.3. Proof of Theorem 6,[0],[0]
"In this paper, we study covariance sketches for matrices in the streaming model.",5. Conclusion,[0],[0]
We provide new space-optimal algorithms with improved running time.,5. Conclusion,[0],[0]
We also prove that our running times cannot be significantly improved unless the state-of-the-art matrix multiplication algorithms can.,5. Conclusion,[0],[0]
"Thus, we almost settle the time complexity of this problem.",5. Conclusion,[0],[0]
This work is supported by Shanghai Science and Technology Commission (Grant No. 17JC1420200) and Shanghai Sailing Program (Grant No. 18YF1401200).,Acknowledgments,[0],[0]
"Given a large matrix A ∈ Rn×d, we consider the problem of computing a sketch matrix B ∈ R`×d which is significantly smaller than but still well approximates A.",abstractText,[0],[0]
We are interested in minimizing the covariance error ‖AA−BB‖2.,abstractText,[0],[0]
"We consider the problems in the streaming model, where the algorithm can only make one pass over the input with limited working space.",abstractText,[0],[0]
"The popular Frequent Directions algorithm of (Liberty, 2013) and its variants achieve optimal space-error tradeoff.",abstractText,[0],[0]
"However, whether the running time can be improved remains an unanswered question.",abstractText,[0],[0]
"In this paper, we almost settle the time complexity of this problem.",abstractText,[0],[0]
"In particular, we provide new space-optimal algorithms with faster running times.",abstractText,[0],[0]
"Moreover, we also show that the running times of our algorithms are near-optimal unless the state-of-the-art running time of matrix multiplication can be improved significantly.",abstractText,[0],[0]
Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices,title,[0],[0]
"Experimental design is an important problem in statistics and machine learning research (Pukelsheim, 2006).",1. Introduction,[0],[0]
"Consider a linear regression model
y = Xβ0 +w, (1)
where X ∈ Rn×p is a pool of n design points, y is the response vector, β0 is a p-dimensional unknown regression model and w is a vector of i.i.d.",1. Introduction,[0],[0]
noise variables satisfying Ewi = 0 and Ew2i,1. Introduction,[0],[0]
< ∞.,1. Introduction,[0],[0]
"The experimental design problem is to select a small subset of rows (i.e., design points) XS from the design pool X so that the statistical power of estimating β0 is maximized from noisy response yS on the selected designs XS .
",1. Introduction,[0],[0]
"As an example, consider a material synthesis application where p is the number of variables (e.g., temperature, pressure, duration) that are hypothesized to affect the quality of the synthesized material and n is the total number of combinations of different parameters of experimental conditions.",1. Introduction,[0],[0]
"As experiments are expensive and time-consuming,
*Author names listed in alphabetic order.",1. Introduction,[0],[0]
"1Microsoft Research, Redmond, USA 2Princeton University, Princeton, USA 3Carnegie Mellon University, Pittsburgh, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Yining Wang <yiningwa@cs.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"one wishes to select k n experimental settings from X that are the most statistically efficient for establishing a model that connects experimental parameters with synthesized material quality, y.",1. Introduction,[0],[0]
"The experimental design problem is also related to many machine learning tasks, such as linear bandits (Deshpande & Montanari, 2012; Huang et al., 2016), diversity sampling (Kulesza & Taskar, 2012) and active learning (Ma et al., 2013; Chaudhuri et al., 2015; Hazan & Karnin, 2015; Balcan & Long, 2013; Wang & Singh, 2016).
",1. Introduction,[0],[0]
"Since statistical efficiency can be measured in various ways, there exist a number of optimality criteria to guide the selection of experiments.",1. Introduction,[0],[0]
"We review some optimality criteria in Sec. 2 and interested readers are referred to Sec. 6 of (Pukelsheim, 2006) for a comprehensive review.
",1. Introduction,[0],[0]
"Typically, an optimality criterion is a function f :",1. Introduction,[0],[0]
S+p → R that maps from the p-dimensional positive definite cone to a real number.,1. Introduction,[0],[0]
"The experimental design problem can then be formulated as a combinatorial optimization problem:
S∗(k) = arg min S∈S(n,k) f(X>SXS), (2)
where S is either a set or a multi-set of size k, and XS ∈ Rk×p is formed by stacking the rows of X that are in S.",1. Introduction,[0],[0]
"The constraint set S1/2(n, k) is defined as follows: 1.",1. Introduction,[0],[0]
"With replacement: S1(n, k) = {S multi-set : S ⊆
[n], |S| ≤ k}.",1. Introduction,[0],[0]
"Under this setting, XS may contain duplicate rows of the design pool X;
2.",1. Introduction,[0],[0]
"Without replacement: S2(n, k) = {S standard set : S ⊆",1. Introduction,[0],[0]
"[n], |S| ≤ k}.",1. Introduction,[0],[0]
"Under this setting, XS only contains distinct rows of the design pool X.
The “with replacement” setting is classical in statistics literature, where the multiple measurements in y with respect to the same design point lead to different values with statistically independent noise.",1. Introduction,[0],[0]
"The “without replacement” setting, on the other hand, is more relevant in machine learning applications, because labels are not likely to change if the same data point (e.g., the same image) is considered twice.",1. Introduction,[0],[0]
"Finally, it is worth pointing out that the “with replacement” setting is easier, because it can be reduced (in polynomial time) to the “without replacement” setting by replicating each row of X for k times.
",1. Introduction,[0],[0]
"For many popular choices of f , the exact optimization
problem in Eq. (2) is NP-hard (Çivril & Magdon-Ismail, 2009; Černỳ & Hladı́k, 2012).",1. Introduction,[0],[0]
"In this paper, we propose a computationally tractable algorithm that approximately computes Eq.",1. Introduction,[0],[0]
"(2) for a wide range of optimality criteria, and under very weak conditions on n, k and p.
",1. Introduction,[0],[0]
"Below is our main theorem:
Theorem 1.1.",1. Introduction,[0],[0]
"Suppose b ∈ {1, 2}, n > k",1. Introduction,[0],[0]
> p and let,1. Introduction,[0],[0]
f : S+p → R be a regular optimality criterion (cf.,1. Introduction,[0],[0]
Definition 2.1).,1. Introduction,[0],[0]
There exists a polynomial-time algorithm that outputs Ŝ ∈,1. Introduction,[0],[0]
"Sb(n, k) for any input matrix X ∈ Rn×p with full column rank, and Ŝ satisfies the following:
1.",1. Introduction,[0],[0]
"For b = 1 (with replacement), there exists an absolute constant C0 ≤ 32 such that, for any ε ∈ (0, 1), if k ≥ C0p/ε 2 then
f(X> Ŝ XŜ)",1. Introduction,[0],[0]
≤ (1 + ε) ·,1. Introduction,[0],[0]
"min S∈S1(n,k) f(X>SXS) .",1. Introduction,[0],[0]
"(3)
2.",1. Introduction,[0],[0]
"For b = 2 (without replacement) and any ξ > 2, there exists constant C1(ξ)",1. Introduction,[0],[0]
> 0,1. Introduction,[0],[0]
"depending only on ξ such that, if k ≥ ξp then
f(X> Ŝ XŜ)",1. Introduction,[0],[0]
≤ C1(ξ) ·,1. Introduction,[0],[0]
"min S∈S2(n,k) f(X>SXS) .",1. Introduction,[0],[0]
"(4)
Moreover, for ξ ≥ 4 we have C1(ξ) ≤ 32.",1. Introduction,[0],[0]
3.,1. Introduction,[0],[0]
"For b = 2 (without replacement) and any ε ∈ (0, 1/2),
if k, r satisfy k ≥ 4(1 + 7ε)r and r ≥ p/ε2, then
f(X> Ŝ XŜ)",1. Introduction,[0],[0]
≤ (1 + ε) ·,1. Introduction,[0],[0]
"min S∈S2(n,r) f(X>SXS).",1. Introduction,[0],[0]
"(5)
We interpret the significance of Theorem 1.1 as follows.
",1. Introduction,[0],[0]
"• Under a very mild condition of k > 2p, our polynomialtime algorithm finds a set Ŝ ⊂",1. Introduction,[0],[0]
"[n] of size k, with objective value f(X>
Ŝ XŜ) being at most O(1) a constant
times the optimum.",1. Introduction,[0],[0]
"See Eq. (4).
",1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"If replacement (b = 1) or over-sampling (k > r) is allowed, the approximation ratio can be tightened to 1+ ε for arbitrarily small ε > 0.",1. Introduction,[0],[0]
"See Eq. (3) and (5).
",1. Introduction,[0],[0]
"• In all of the three cases, we only require k to grow linearly in p.",1. Introduction,[0],[0]
"Recall that k ≥ p is necessary to ensure the singularity of X>
Ŝ XŜ .",1. Introduction,[0],[0]
"In contrast, no polynomial-
time algorithm has achieved O(1) approximation in the regime k = O(p) for non-submodular optimality criteria (e.g., A- and V-optimality) under the without replacement setting.
",1. Introduction,[0],[0]
• Our algorithm works for any regular optimality criterion.,1. Introduction,[0],[0]
"To the best of our knowledge, no known polynomial-time algorithm can achieve a (1 + ε) approximation for the D- and T-optimality criteria, or even an O(1) approximation for the E- and G-optimality criteria.",1. Introduction,[0],[0]
"See Table 1 for a comparison.
",1. Introduction,[0],[0]
The key idea behind our proof of Theorem 1.1 is a regret minimization characterization of the least eigenvalue of positive semidefinite (PSD) matrices.,1. Introduction,[0],[0]
"Similar ideas were developed in (Allen-Zhu et al., 2015; Silva et al., 2016) to construct efficient algorithms for linear-sized graph sparsifiers.",1. Introduction,[0],[0]
In this paper we adopt the regret minimization framework and present novel potential function analysis for the specific application of experimental design.,1. Introduction,[0],[0]
S+p is the positive definite cone of p × p matrices: a p × p symmetric matrix A belongs to S+p if and only if v>Av > 0 for all v ∈ Rp\{0}.,1.1. Notations,[0],[0]
"For symmetric matrices A and B, we write A B if v>(A",1.1. Notations,[0],[0]
− B)v ≥ 0 for all v ∈,1.1. Notations,[0],[0]
Rp.,1.1. Notations,[0],[0]
"The inner product 〈A,B〉 is defined as 〈A,B〉 = tr(B>A) = ∑pi,j=1 AijBij .",1.1. Notations,[0],[0]
"We use ‖A‖2 = supv∈Rp\{0} ‖Av‖2/‖v‖2 to denote the spectral norm, and ‖A‖F = √∑p",1.1. Notations,[0],[0]
"i,j=1 A 2 ij = √ 〈A,A〉 to denote
the Frobenius norm of A. For A 0, we write B = A1/2 as the unique B 0 that satisfies B2 = A.",1.1. Notations,[0],[0]
"For a design pool X ∈ Rn×p, we use xi ∈",1.1. Notations,[0],[0]
Rp to denote the i-th row of X. We use σmin(A) for the least (smallest) singular value of a PSD matrix A.,1.1. Notations,[0],[0]
"Experimental design is an old topic in statistics research (Pukelsheim, 2006; Fedorov, 1972).",1.2. Related work,[0],[0]
"Computationally efficient experimental design algorithms (with provable guarantees) are, however, a less studied field.",1.2. Related work,[0],[0]
"In the case of submodular optimality criteria (e.g., D- and T-optimality), the classical pipage rounding method (Ageev & Sviridenko, 2004; Horel et al., 2014; Ravi et al., 2016) combined with semi-definite programming results in computationally efficient algorithms that enjoy a constant approximation ratio.",1.2. Related work,[0],[0]
Bouhtou et al. (2010) improves the approximation ratio when k is very close to n. Deshpande & Rademacher (2010); Li et al. (2017) considered polynomial-time algorithms for sampling from a D-optimality criterion.,1.2. Related work,[0],[0]
"These algorithms are not applicable to non-submodular criteria, such as A-, V-, E- or G-optimality.
",1.2. Related work,[0],[0]
"For the particular A-optimality criterion, (Avron & Boutsidis, 2013) proposed a greedy algorithm with an approximation ratio of O(n/k) with respect to f(X>X).",1.2. Related work,[0],[0]
It was shown that in the worst case min|S|≤k f(X>SXS),1.2. Related work,[0],[0]
≈ O(n/k) ·,1.2. Related work,[0],[0]
f(X>X) and hence the bound is tight.,1.2. Related work,[0],[0]
"However, for general design pool min|S|≤k f(X>SXS) could be far smaller than O(n/k) ·",1.2. Related work,[0],[0]
"f(X>X), making the theoretical results powerless in such scenarios.",1.2. Related work,[0],[0]
"Wang et al. (2016) considered a variant of the greedy method and showed an approximation ratio quadratic in design dimension p and independent of pool size n.
Wang et al. (2016) derived algorithms based on effective resistance sampling (Spielman & Srivastava, 2011) that attain (1 + ε) approximation ratio if k = Ω(p log p/ε2) and repetitions of design points are allowed.",1.2. Related work,[0],[0]
The algorithm fundamentally relies on the capability of “re-weighting” (repeating) design points and cannot be adapted to the more general “without replacement” setting.,1.2. Related work,[0],[0]
"Naive sampling based methods were considered in (Wang et al., 2016; Chaudhuri et al., 2015; Dhillon et al., 2013), which also achieve (1+ε) approximation but requires the subset size k to be much larger than the condition number of X.
A related however different topic is low-rank matrix column subset selection and CUR approximation, which seeks column subset C and row subset R such that",1.2. Related work,[0],[0]
‖X,1.2. Related work,[0],[0]
"− CC†X‖F and/or ‖X − CUR‖F are minimized (Drineas et al., 2008; Boutsidis & Woodruff, 2014; Wang & Singh, 2015b; Drineas & Mahoney, 2005; Wang & Zhang, 2013; Wang & Singh, 2015a).",1.2. Related work,[0],[0]
"These problems are unsupervised in nature and do not in general correspond to statistical
properties under supervised regression settings.",1.2. Related work,[0],[0]
Pilanci & Wainwright (2016); Raskutti & Mahoney (2014); Woodruff (2014) considered fast methods for solving ordinary least squares (OLS) problems.,1.2. Related work,[0],[0]
"They are computationally oriented and typically require knowledge of the full response vector y, which is different from the experimental design problem.",1.2. Related work,[0],[0]
"We start with the definition of regular optimality criteria:
Definition 2.1 (Regular criteria).",2. Regular criteria and continuous relaxation,[0],[0]
An optimality criterion f : S+p → R is regular if it satisfies the followig properties: 1.,2. Regular criteria and continuous relaxation,[0],[0]
Convexity: 1 f(λA + (1 − λ)B) ≤ λf(A) + (1 − λ)f(B) for all λ ∈,2. Regular criteria and continuous relaxation,[0],[0]
"[0, 1] and A,B ∈ S+p ;
2.",2. Regular criteria and continuous relaxation,[0],[0]
Monotonicity:,2. Regular criteria and continuous relaxation,[0],[0]
If A B then f(A) ≥ f(B); 3.,2. Regular criteria and continuous relaxation,[0],[0]
"Reciprocal multiplicity: f(tA) = t−1f(A) for all t >
0 and A ∈ S+p .
",2. Regular criteria and continuous relaxation,[0],[0]
Almost all optimality criteria used in the experimental design literature are regular.,2. Regular criteria and continuous relaxation,[0],[0]
"Below we list a few popular examples; their statistical implications can be found in (Pukelsheim, 2006):
- A-optimality (Average): fA(Σ) =",2. Regular criteria and continuous relaxation,[0],[0]
"1p tr(Σ −1);
- D-optimality (Determinant): fD(Σ) =",2. Regular criteria and continuous relaxation,[0],[0]
"(det |Σ|)− 1 p ;
- T-optimality (Trace): fT (Σ) = p/tr(Σ);
- E-optimality (Eigenvalue): fE(Σ) = ‖Σ−1‖2; - V-optimality (Variance): fV (Σ) =",2. Regular criteria and continuous relaxation,[0],[0]
"1n tr(XΣ −1X>); - G-optimality: fG(Σ) = max diag(XΣ−1X>).
",2. Regular criteria and continuous relaxation,[0],[0]
"The (A-, D-, T-, E-) criteria concern estimates of regression coefficients and the (V-, G-) criteria are about insample predictions.",2. Regular criteria and continuous relaxation,[0],[0]
All criteria listed above are regular.,2. Regular criteria and continuous relaxation,[0],[0]
Note that for D-optimality the proxy function gD(Σ) =,2. Regular criteria and continuous relaxation,[0],[0]
− log det(Σ) is considered to satisfy the convexity property.,2. Regular criteria and continuous relaxation,[0],[0]
"In addition, by the standard arithmetic inequality we have that fT ≤ fD ≤",2. Regular criteria and continuous relaxation,[0],[0]
fA ≤ fE and that fV ≤,2. Regular criteria and continuous relaxation,[0],[0]
"fG. Although exact optimization of the combinatorial problem Eq. (2) is intractable, it is nevertheless easy to solve a continuous relaxation of Eq.",2. Regular criteria and continuous relaxation,[0],[0]
(2) given the convexity property in Definition 2.1.,2. Regular criteria and continuous relaxation,[0],[0]
"We consider the following continuous optimization problem:
π∗(b) = arg min π=(π1,··· ,πn) f
( n∑
i=1
πixix >",2. Regular criteria and continuous relaxation,[0],[0]
"i
) , (6)
s.t.",2. Regular criteria and continuous relaxation,[0],[0]
"π ≥ 0, ‖π‖1",2. Regular criteria and continuous relaxation,[0],[0]
"≤ r, I[b = 2] · ‖π‖∞ ≤ 1.",2. Regular criteria and continuous relaxation,[0],[0]
"1This property could be relaxed to allow a proxy function g :
S+p → R being convex, where g(A) ≤ g(B)⇔ f(A) ≤ f(B).
",2. Regular criteria and continuous relaxation,[0],[0]
"The ‖π‖1 ≤ r constraint makes sure only r rows of X are “selected”, where r ≤ k is a parameter that controls the degree of oversampling.",2. Regular criteria and continuous relaxation,[0],[0]
The 0 ≤ πi ≤ 1 constraint enforces that each row of X is “selected” at most once and is only applicable to the without replacement setting (b = 2).,2. Regular criteria and continuous relaxation,[0],[0]
Eq. (6) is a relaxation of the original combinatorial problem Eq.,2. Regular criteria and continuous relaxation,[0],[0]
"(2), which we formalize below: Fact 2.1.",2. Regular criteria and continuous relaxation,[0],[0]
"For b ∈ {1, 2} we have f(∑ni=1 π∗i (b)xix>i ) ≤",2. Regular criteria and continuous relaxation,[0],[0]
"minS∈Sb(n,r) f(X > SXS)
",2. Regular criteria and continuous relaxation,[0],[0]
"In addition, because of the monotonicity property of f the sum constraint must bind: Fact 2.2.",2. Regular criteria and continuous relaxation,[0],[0]
"For b ∈ {1, 2} it holds that∑ni=1 π∗i (b) =",2. Regular criteria and continuous relaxation,[0],[0]
"r.
Proofs of Facts 2.1 and 2.2 are straightforward and are placed in the supplementary material.
",2. Regular criteria and continuous relaxation,[0],[0]
Both the objective function and the constraint set in Eq.,2. Regular criteria and continuous relaxation,[0],[0]
"(6) are convex, and hence it can be efficiently solved to global optimality by conventional convex optimization algorithms.",2. Regular criteria and continuous relaxation,[0],[0]
"In particular, for differentiable f we suggest the following projected gradient descent (PGD) procedure:
π(t+1) = PC ( π(t) − γt∇f(π(t)) ) , (7)
where PC(x) = arg miny∈C",2. Regular criteria and continuous relaxation,[0],[0]
‖x,2. Regular criteria and continuous relaxation,[0],[0]
− y‖2 is the projection operator onto the feasible set C = {π ∈,2. Regular criteria and continuous relaxation,[0],[0]
"Rp : π ≥ 0, ‖π‖1",2. Regular criteria and continuous relaxation,[0],[0]
"≤ r, I[b = 2] · ‖π‖∞ ≤ 1} and {",2. Regular criteria and continuous relaxation,[0],[0]
γt}t≥1 > 0 is a sequence of step sizes typically chosen by backtracking line search.,2. Regular criteria and continuous relaxation,[0],[0]
"When f is not differentiable everywhere, projected subgradient descent could be used with either constant or diminishing step sizes.",2. Regular criteria and continuous relaxation,[0],[0]
We defer detailed gradient computations to the supplementary material.,2. Regular criteria and continuous relaxation,[0],[0]
"It was shown in (Wang et al., 2016; Su et al., 2012) that the projection operator PC(x) could be efficiently computed up to precision δ in O(n log(‖x‖∞/δ)) operations.",2. Regular criteria and continuous relaxation,[0],[0]
The optimal solution π∗ of Eq.,3. Sparsification via regret minimization,[0],[0]
(6) does not naturally lead to a valid approximation of the combinatorial problem in Eq.,3. Sparsification via regret minimization,[0],[0]
"(2), because the number of non-zero components in π∗ may far exceed k.",3. Sparsification via regret minimization,[0],[0]
The primary focus of this section is to design efficient algorithms that sparsify the optimal solution π∗ into s ∈,3. Sparsification via regret minimization,[0],[0]
"[k]n (with replacement) or s ∈ {0, 1}n (without replacement), while at the same time bounding the increase in the objective.
",3. Sparsification via regret minimization,[0],[0]
"Due to the monotonicity and reciprocal multiplicity properties of f , it suffices to find a sparsifier s that satisfies
( n∑
i=1
sixix > i
) τ · ( n∑
i=1
π∗i",3. Sparsification via regret minimization,[0],[0]
xix >,3. Sparsification via regret minimization,[0],[0]
"i
) (8)
for some constant τ ∈ (0, 1).",3. Sparsification via regret minimization,[0],[0]
"By Definition 2.1, Eq. (8) immediately implies f( ∑n i=1",3. Sparsification via regret minimization,[0],[0]
"sixix > i ) ≤
τ−1f( ∑n i=1",3. Sparsification via regret minimization,[0],[0]
π ∗,3. Sparsification via regret minimization,[0],[0]
i,3. Sparsification via regret minimization,[0],[0]
xix,3. Sparsification via regret minimization,[0],[0]
>,3. Sparsification via regret minimization,[0],[0]
i ).,3. Sparsification via regret minimization,[0],[0]
"The key idea behind our algorithm is a regret-minimization interpretation of the least eigenvalue of a positive definite matrix, which arises from recent progress in the spectral graph sparsification literature (Silva et al., 2016; Allen-Zhu et al., 2015).
",3. Sparsification via regret minimization,[0],[0]
"In the rest of this section, we adopt the notation that Π = diag(π∗) and S = diag(s), both being n×n non-negative diagonal matrices.",3. Sparsification via regret minimization,[0],[0]
"We also use I to denote the identity matrix, whose dimension should be clear from the context.",3. Sparsification via regret minimization,[0],[0]
Consider the linear transform xi 7→ (XΠX>)−1/2xi =: x̃i.,3.1. The whitening trick,[0],[0]
It is easy to verify that ∑n i=1,3.1. The whitening trick,[0],[0]
π ∗,3.1. The whitening trick,[0],[0]
i x̃ix̃,3.1. The whitening trick,[0],[0]
>,3.1. The whitening trick,[0],[0]
i = I.,3.1. The whitening trick,[0],[0]
"Such a transform is usually referred to as whitening, because the sample covariance of the transformed data is the identity matrix.",3.1. The whitening trick,[0],[0]
Define W = ∑n i=1 six̃ix̃,3.1. The whitening trick,[0],[0]
>,3.1. The whitening trick,[0],[0]
i .,3.1. The whitening trick,[0],[0]
"We then have the following:
Proposition 3.1.",3.1. The whitening trick,[0],[0]
"For τ > 0, W τI",3.1. The whitening trick,[0],[0]
if and only if ( ∑n i=1,3.1. The whitening trick,[0],[0]
sixix > i ) τ,3.1. The whitening trick,[0],[0]
( ∑n i=1,3.1. The whitening trick,[0],[0]
π ∗,3.1. The whitening trick,[0],[0]
i,3.1. The whitening trick,[0],[0]
xix,3.1. The whitening trick,[0],[0]
>,3.1. The whitening trick,[0],[0]
"i ).
",3.1. The whitening trick,[0],[0]
Proof.,3.1. The whitening trick,[0],[0]
"The proposition holds because W τI if and only if (XΠX>)1/2W(XΠX>)1/2 τXΠX>, and that (XΠX>)1/2W(XΠX>)1/2 = XSX>.
",3.1. The whitening trick,[0],[0]
"Proposition 3.1 shows that, without loss of generality, we may assume ∑n i=1",3.1. The whitening trick,[0],[0]
π ∗,3.1. The whitening trick,[0],[0]
i,3.1. The whitening trick,[0],[0]
xix,3.1. The whitening trick,[0],[0]
>,3.1. The whitening trick,[0],[0]
"i = XΠX
> = I.",3.1. The whitening trick,[0],[0]
"The question of proving W = XSX> τI is then reduced to lower bounding the smallest eigenvalue of W.
Recall that W can be written as a sum of rank-1 PSD matrices W = ∑k t=1",3.1. The whitening trick,[0],[0]
"Ft, where Ft = xix >",3.1. The whitening trick,[0],[0]
i for some i ∈,3.1. The whitening trick,[0],[0]
[n].,3.1. The whitening trick,[0],[0]
In the next section we give a novel characterization of the least eigenvalue of W from a regret minimization perspective.,3.1. The whitening trick,[0],[0]
"The problem of lower bounding the least eigenvalue of W can then be reduced to bounding the regret of a particular Follow-The-Regularized-Leader (FTRL) algorithm, which is a much easier task as FTRL admits closed-form solutions.",3.1. The whitening trick,[0],[0]
We first review the concept of regret minimization in a classical linear bandit setting.,3.2. Smallest eigenvalue as regret minimization,[0],[0]
"Let ∆p = {A ∈ Rp×p : A 0, tr(A) = 1} be an action space that consists of positive semi-definite matrices of dimension p and unit trace norm.",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"Consider the linear bandit problem, which operates in k iterations.",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"At iteration t, the player chooses an action At ∈ ∆p; afterwards, a “reference” action Ft 0 is observed and the loss 〈Ft,At〉 is incurred.",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"The objective of the player is to minimize his/her regret:
R({At}kt=1) := k∑
t=1
〈Ft,At〉 − inf U∈∆p
k∑
t=1
〈Ft,U〉,
which is the “excess loss” of {At}kt=1 compared to the single optimal action U ∈ ∆p in hindsight, knowing all the reference actions {Ft}kt=1.",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"A popular algorithm for regret minimization is Follow-The-Regularized-Leader (FTRL), also known to be equivalent to Mirror Descent (MD) (McMahan, 2011), which solves for
At = arg min A∈∆p
{ w(A) + α · t−1∑
`=1
〈F`,A〉 } .",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"(9)
Here w(A) is a regularization term and α > 0 is a parameter that balances model fitting and regularization.",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"For the proof of our purpose we adopt the `1/2-regularizerw(A) = −2tr(A1/2) introduced in (Allen-Zhu et al., 2015), which leads to the closed-form solution
At = ( ctI + α t−1∑
`=1
F`
)−2 , (10)
where ct ∈ R is the unique constant that ensures At ∈ ∆p.",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"The following lemma from (Allen-Zhu et al., 2015) bounds the regret of FTRL using the particular `1/2-regularizer:
Lemma 3.1 (Theorem 3.2 of (Allen-Zhu et al., 2015), specialized to `1/2-regularization).",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"Suppose α > 0, rank(Ft) = 1 and let {At}kt=1 be FTRL solutions defined in Eq. (10).",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"If α〈Ft,A1/2t 〉 >",3.2. Smallest eigenvalue as regret minimization,[0],[0]
−1,3.2. Smallest eigenvalue as regret minimization,[0],[0]
"for all t, then
R({At}kt=1) := k∑
t=1
〈Ft,At〉 − inf U∈∆p
k∑
t=1
〈Ft,U〉
≤ α k∑
t=1 〈Ft,At〉〈Ft,A1/2t 〉 1 + α〈Ft,A1/2t 〉
+ 2 √ p
α .
",3.2. Smallest eigenvalue as regret minimization,[0],[0]
Now consider each Ft = xitx >,3.2. Smallest eigenvalue as regret minimization,[0],[0]
"it
to be the outer product of a design point selected from the design pool X. One remarkable consequence of Lemma 3.1 is that, in order to lower bound the smallest eigenvalue of ∑k t=1 Ft, which by defi-
nition is infU∈∆p〈 ∑k t=1 Ft,U〉, it suffices to lower bound∑k
t=1 〈Ft,At〉.",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"Because At admits closed-form expression in Eq. (10), choosing a sequence of {Ft}kt=1 with large∑k t=1 〈Ft,At〉 becomes a much more manageable analytical task, which we shall formalize in the next section.",3.2. Smallest eigenvalue as regret minimization,[0],[0]
"Re-organizing terms in Lemma 3.1 we obtain
inf U∈∆p
k∑
t=1
〈Ft,U〉 ≥ k∑
t=1 〈Ft,At〉 1 + α〈Ft,A1/2t 〉
− 2 √ p
α .",3.3. Proof of Theorem 1.1,[0],[0]
"(11)
The k near-optimal design points are selected in a sequential manner.",3.3. Proof of Theorem 1.1,[0],[0]
"Let Λt ∈ Sb(n, t) be the set of selected design points at or prior to iteration t (Λ0 = ∅), and define
Ft = xitx >",3.3. Proof of Theorem 1.1,[0],[0]
"it
, where it is the design point selected at iteration t. Define also Λt = ∑t",3.3. Proof of Theorem 1.1,[0],[0]
"`=1 F` = ∑ i∈Λt xix > i .
",3.3. Proof of Theorem 1.1,[0],[0]
We first consider the with replacement setting b = 1.,3.3. Proof of Theorem 1.1,[0],[0]
Lemma 3.2.,3.3. Proof of Theorem 1.1,[0],[0]
Suppose ∑n i=1,3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
i,3.3. Proof of Theorem 1.1,[0],[0]
xix,3.3. Proof of Theorem 1.1,[0],[0]
>,3.3. Proof of Theorem 1.1,[0],[0]
"i = I where π ∗ i ≥ 0
and ∑n i=1",3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
i =,3.3. Proof of Theorem 1.1,[0],[0]
r. Then for 1 ≤ t ≤,3.3. Proof of Theorem 1.1,[0],[0]
k,3.3. Proof of Theorem 1.1,[0],[0]
"we have that maxi∈[n] 〈xix>i ,At〉
1+α〈xix>i ,A 1/2 t 〉 ≥ 1r+α√p .
",3.3. Proof of Theorem 1.1,[0],[0]
Proof.,3.3. Proof of Theorem 1.1,[0],[0]
Recall that tr(At) = 1 and ∑n i=1,3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
i,3.3. Proof of Theorem 1.1,[0],[0]
xix,3.3. Proof of Theorem 1.1,[0],[0]
>,3.3. Proof of Theorem 1.1,[0],[0]
"i =
I. Subsequently, ∑n i=1",3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
"i 〈xix>i ,At〉 = 1.",3.3. Proof of Theorem 1.1,[0],[0]
"On the other hand, we have that ∑n i=1",3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
"i (1 + α〈xix>i ,A 1/2 t 〉) =",3.3. Proof of Theorem 1.1,[0],[0]
∑n i=1,3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
i + α · tr(A 1/2 t ) (a) ≤,3.3. Proof of Theorem 1.1,[0],[0]
"r + α · tr(A1/2t ) (b)
≤",3.3. Proof of Theorem 1.1,[0],[0]
r + α √ p.,3.3. Proof of Theorem 1.1,[0],[0]
"Here (a) is due to the optimization constraint that ‖π∗‖1 ≤ r, and (b) is because tr(A1/2t ) = ‖σ(A1/2t )‖1 ≤√ p‖σ(A1/2t )",3.3. Proof of Theorem 1.1,[0],[0]
‖2 = √ p √ ‖σ(At)‖1 = √p,3.3. Proof of Theorem 1.1,[0],[0]
"√ tr(At) =√
p, where σ(·) is the vector of all eigenvalues of a PSD matrix.",3.3. Proof of Theorem 1.1,[0],[0]
"Combining both inequalities we have that maxi∈[n]
〈xix>i ,At〉 1+α〈xix>i ,A 1/2 t 〉
≥ ∑n i=1",3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
"i 〈xix>i ,At〉∑n
i=1",3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
"i (1+α〈xix>i ,A 1/2 t 〉)
,
where the right-hand side is lower bounded by 1/(r + α √ p).
",3.3. Proof of Theorem 1.1,[0],[0]
"Let it = arg maxi∈[n] 〈xix>i ,At〉
1+α〈xix>i ,A 1/2 t 〉
be the design point
selected at iteration t. Combining Eq.",3.3. Proof of Theorem 1.1,[0],[0]
"(11) and Lemma 3.2,
Λk = ∑
i∈Λk xix
>",3.3. Proof of Theorem 1.1,[0],[0]
"i
( k r + α √ p − 2 √ p α ) I. (12)
",3.3. Proof of Theorem 1.1,[0],[0]
To prove Eq.,3.3. Proof of Theorem 1.1,[0],[0]
"(3), set α = 8 √ p/ε.",3.3. Proof of Theorem 1.1,[0],[0]
"Because k = r ≥
C0p/ε 2, we have that kr+α√p",3.3. Proof of Theorem 1.1,[0],[0]
"−
2 √ p
α ≥ 11+8ε/C0 − ε 4 .",3.3. Proof of Theorem 1.1,[0],[0]
"With
C0 = 32 the right-hand side is lower bounded by 1− ε/2.",3.3. Proof of Theorem 1.1,[0],[0]
Eq. (3) is thus proved because (1− ε/2)−1 ≤ 1 + ε.,3.3. Proof of Theorem 1.1,[0],[0]
"We next consider the without replacement setting b = 2.
",3.3. Proof of Theorem 1.1,[0],[0]
Lemma 3.3.,3.3. Proof of Theorem 1.1,[0],[0]
"Fix arbitrary β ∈ (0, 1] and suppose∑n i=1",3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
i,3.3. Proof of Theorem 1.1,[0],[0]
xix,3.3. Proof of Theorem 1.1,[0],[0]
>,3.3. Proof of Theorem 1.1,[0],[0]
i = I where π ∗ i ∈,3.3. Proof of Theorem 1.1,[0],[0]
"[0, β] and ∑n i=1",3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
i =,3.3. Proof of Theorem 1.1,[0],[0]
r. Then for all 1 ≤ t ≤,3.3. Proof of Theorem 1.1,[0],[0]
"k,
max i/∈Λt−1 〈xix>i ,At〉 1 + α〈xix>i ,A 1/2 t 〉
≥ 1− βσmin(Λt−1)− √ p/α
r + α √ p
.
",3.3. Proof of Theorem 1.1,[0],[0]
Proof.,3.3. Proof of Theorem 1.1,[0],[0]
"On one hand, we have ∑ i/∈Λt−1 π ∗",3.3. Proof of Theorem 1.1,[0],[0]
"i 〈xix>i ,At〉 (a)
",3.3. Proof of Theorem 1.1,[0],[0]
"≥ 〈At, I− βΛt−1〉 (b) = 1 − tr [ (αΛt−1 + ctI) −2 βΛt−1",3.3. Proof of Theorem 1.1,[0],[0]
"] =
1+ βctα",3.3. Proof of Theorem 1.1,[0],[0]
− β α tr [ (αΛt−1 + ctI) −1,3.3. Proof of Theorem 1.1,[0],[0]
] = 1+ βctα − tr(A 1/2 t ),3.3. Proof of Theorem 1.1,[0],[0]
"α (c)
≥ 1 + βctα − √ p α .",3.3. Proof of Theorem 1.1,[0],[0]
Here (a) is due to ∑n i=1,3.3. Proof of Theorem 1.1,[0],[0]
π ∗,3.3. Proof of Theorem 1.1,[0],[0]
i,3.3. Proof of Theorem 1.1,[0],[0]
xix,3.3. Proof of Theorem 1.1,[0],[0]
>,3.3. Proof of Theorem 1.1,[0],[0]
i = I and π∗i ∈,3.3. Proof of Theorem 1.1,[0],[0]
"[0, β]; (b) is due to 〈At, I〉 = tr(At) = 1 and (c) is proved in the proof of Lemma 3.2.",3.3. Proof of Theorem 1.1,[0],[0]
"Because αΛt−1 +ctI 0, we conclude that ct ≥ −ασmin(Λt−1) and therefore∑ i/∈Λt−1 π ∗",3.3. Proof of Theorem 1.1,[0],[0]
"i 〈xix>i ,At〉 ≥ 1 − βσmin(Λt−1)",3.3. Proof of Theorem 1.1,[0],[0]
− √ p/α.,3.3. Proof of Theorem 1.1,[0],[0]
"On the other hand, ∑ i/∈Λt−1 π ∗",3.3. Proof of Theorem 1.1,[0],[0]
"i (1 + α〈xix>i ,A 1/2 t 〉) ≤
r + α √ p by the same argument as in the proof of Lemma 3.2.",3.3. Proof of Theorem 1.1,[0],[0]
"Subsequently, maxi/∈Λt−1 〈xix>i ,At〉
1+α〈xix>i ,A 1/2 t 〉 ≥
∑ i/∈Λt−1 π ∗",3.3. Proof of Theorem 1.1,[0],[0]
"i 〈xix>i ,At〉
∑ i/∈Λt−1 π ∗",3.3. Proof of Theorem 1.1,[0],[0]
"i (1+α〈xix>i ,A 1/2 t 〉)
≥ 1−βσmin(Λt−1)− √ p/α
r+α √ p .
Let it = arg maxi/∈Λt−1 〈xix>i ,At〉
1+α〈xix>i ,A 1/2 t 〉
.",3.3. Proof of Theorem 1.1,[0],[0]
"Combining
Eq. (11) and Lemma 3.3 with β = 1, we have that
Λk ( k∑
t=1
1− κt",3.3. Proof of Theorem 1.1,[0],[0]
−√p/α r + α,3.3. Proof of Theorem 1.1,[0],[0]
√ p − 2 √ p α ),3.3. Proof of Theorem 1.1,[0],[0]
"I, (13)
where κt := σmin(Λt).",3.3. Proof of Theorem 1.1,[0],[0]
We are now ready to prove Eqs.,3.3. Proof of Theorem 1.1,[0],[0]
"(4,5) in Theorem 1.1.
",3.3. Proof of Theorem 1.1,[0],[0]
Proof of Eq.,3.3. Proof of Theorem 1.1,[0],[0]
(4).,3.3. Proof of Theorem 1.1,[0],[0]
"Note that
Λk sup u>0 min
{ u, 1− u−√p/α r + α √ p · k",3.3. Proof of Theorem 1.1,[0],[0]
"− 2 √ p α } I. (14)
Eq.",3.3. Proof of Theorem 1.1,[0],[0]
(14) can be proved by a case analysis: if u ≤ κt for some 1 ≤ t ≤,3.3. Proof of Theorem 1.1,[0],[0]
k,3.3. Proof of Theorem 1.1,[0],[0]
then σmin(Λk),3.3. Proof of Theorem 1.1,[0],[0]
≥ σmin(Λt−1) ≥ u; otherwise 1−κt−√p/α ≥ 1−u−√p/α for all 1 ≤ t ≤ k.,3.3. Proof of Theorem 1.1,[0],[0]
"Suppose k = r ≥ ξp for some ξ > 2. and let α = ν√p, u = (1−2/ξ)ν−3ν(2+ν/ξ) , where ν > 1 is some parameter to be specified later.",3.3. Proof of Theorem 1.1,[0],[0]
"Eq. (14) then yields Λk (1−2/ξ)ν−3ν(2+ν/ξ) I. Because ξ > 2, it is possible to select ν",3.3. Proof of Theorem 1.1,[0],[0]
> 0,3.3. Proof of Theorem 1.1,[0],[0]
"such that C1(ξ)
−1 =",3.3. Proof of Theorem 1.1,[0],[0]
(1−2/ξ)ν−3ν(2+ν/ξ),3.3. Proof of Theorem 1.1,[0],[0]
> 0.,3.3. Proof of Theorem 1.1,[0],[0]
"Finally, for ξ ≥ 4 and ν = 8 we have C1(ξ)−1 ≥ 1/32.",3.3. Proof of Theorem 1.1,[0],[0]
"Eq. (4) is thus proved.
",3.3. Proof of Theorem 1.1,[0],[0]
Proof of Eq.,3.3. Proof of Theorem 1.1,[0],[0]
(5).,3.3. Proof of Theorem 1.1,[0],[0]
"Let β ∈ (0, 1) be a parameter to be specified later, and define Σ∗β := ∑ π∗i≥β π ∗",3.3. Proof of Theorem 1.1,[0],[0]
i,3.3. Proof of Theorem 1.1,[0],[0]
xix,3.3. Proof of Theorem 1.1,[0],[0]
>,3.3. Proof of Theorem 1.1,[0],[0]
"i and Σ̄ ∗ β :=
I − Σ∗β = ∑ π∗i<β π∗i xix >",3.3. Proof of Theorem 1.1,[0],[0]
i .,3.3. Proof of Theorem 1.1,[0],[0]
"Let Ŝ be constructed such that it includes all points in S∗β := {i : π∗i ≥ β}, plus the resulting set by running Algorithm 1 on the remaining weights smaller than β, with subset size k−k′ = k−|S∗β |.",3.3. Proof of Theorem 1.1,[0],[0]
"Define α = 2 √ p/ε, r′ :=
∑ π∗i≥β π ∗",3.3. Proof of Theorem 1.1,[0],[0]
"i , k̃ := k",3.3. Proof of Theorem 1.1,[0],[0]
"− k′ and
r̃ := r− r′+α√p = r− r′+ 2p/ε.",3.3. Proof of Theorem 1.1,[0],[0]
Let Λ = ∑i∈Ŝ,3.3. Proof of Theorem 1.1,[0],[0]
xix,3.3. Proof of Theorem 1.1,[0],[0]
>,3.3. Proof of Theorem 1.1,[0],[0]
i be the sample covariance of the selected subset.,3.3. Proof of Theorem 1.1,[0],[0]
"By the definition of Ŝ and Lemma 3.3, together with the whitening trick (Sec. 3.1) on Σ̄∗β , we have
Λ Σ∗β + sup u>0
min { u, (1− βu− ε/2)k̃/r̃",3.3. Proof of Theorem 1.1,[0],[0]
"− ε } Σ̄∗β
sup u>0
min { u, (1− βu− ε/2)k̃/r̃",3.3. Proof of Theorem 1.1,[0],[0]
− ε },3.3. Proof of Theorem 1.1,[0],[0]
"I,
where the second line holds because Σ∗β + Σ̄ ∗ β =",3.3. Proof of Theorem 1.1,[0],[0]
I and u ≤ 1.,3.3. Proof of Theorem 1.1,[0],[0]
Now set β = 0.5 and note that k′ ≤ r′/β ≤ 2r′ by definition of S∗β .,3.3. Proof of Theorem 1.1,[0],[0]
"Subsequently, r ≥ p/ε2 and k ≥ 4(1 + 7ε)r for ε ∈ (0, 1/2) implies that k̃r̃ ≥ 1+2ε(1−ε/2)(1−β) , which yields u ≥ 1 − ε/2 and hence f(X>
Ŝ XŜ) ≤",3.3. Proof of Theorem 1.1,[0],[0]
"(1 +
ε)f(X>S∗XS∗).",3.3. Proof of Theorem 1.1,[0],[0]
"Eq. (5) is thus proved.
",3.3. Proof of Theorem 1.1,[0],[0]
"Algorithm 1 Near-optimal experimental design 1: Input: design pool X ∈ Rn×p, budget parameters k ≥ r ≥ p, algorithmic parameter α > 0.
2: Solve the convex optimization problem Eq.",3.3. Proof of Theorem 1.1,[0],[0]
(6) with parameter s; Let π∗ be the optimal solution; 3: Whitening: X← X(X>diag(π∗)X)−1/2; 4: Initialization: Λ0 = ∅; 5: for t = 1 to k do 6: ct ← FINDCONSTANT( ∑ i∈Λt−1 xix >,3.3. Proof of Theorem 1.1,[0],[0]
"i , α);
7:",3.3. Proof of Theorem 1.1,[0],[0]
At ← (ctI + ∑ i∈Λt−1 xix >,3.3. Proof of Theorem 1.1,[0],[0]
i ) −2; 8:,3.3. Proof of Theorem 1.1,[0],[0]
If b = 1 then Γt =,3.3. Proof of Theorem 1.1,[0],[0]
"[n]; else Γt = [n]\Λt−1; 9: it ← arg maxi∈Γt
〈xix>i ,At〉 1+α〈xix>i ,A 1/2 t 〉 ;
10: Λt = Λt−1 ∪ {it}; 11: end for 12: Output: Ŝ = Λk.
Algorithm 2",3.3. Proof of Theorem 1.1,[0],[0]
"FINDCONSTANT(Z, α) 1: Initialization: c` = −σmin(Z), cu = √p; = 10−9; 2: while |c` − cu| > do 3: c̄← (c` + cu)/2; 4: If tr[(c̄I + Z)−2] > 1 then c` ← c̄; else cu ← c̄; 5: end while 6: Output: c = (c` + cu)/2.
",3.3. Proof of Theorem 1.1,[0],[0]
"Our proof of Theorem 1.1 is constructive and yields a computationally efficient iterative algorithm which finds subset Ŝ ∈ Sb(n, k) that satisfies the approximation results in Theorem 1.1.",3.3. Proof of Theorem 1.1,[0],[0]
"In Algorithm 1 we give a pseducode description of the algorithm, which makes use of a binary search routine (Algorithm 2) that finds the unique constant ct for which tr(At) = tr[(ctI + ∑ i∈Λt−1 xix >",3.3. Proof of Theorem 1.1,[0],[0]
i ) −2,3.3. Proof of Theorem 1.1,[0],[0]
] = 1.,3.3. Proof of Theorem 1.1,[0],[0]
Note that for Eq.,3.3. Proof of Theorem 1.1,[0],[0]
"(5) to be valid, it is necessary to run Algorithm 2 on the remaining set of π∗ after including all points xi with π∗i ≥ 1/2 in Ŝ.",3.3. Proof of Theorem 1.1,[0],[0]
The experimental algorithm presented in this paper could be easily extended beyond the linear regression model.,4. Extension to generalized linear models,[0],[0]
"For this purpose we consider the Generalized Linear Model (GLM), which assumes that
y|x i.i.d.∼ p(y|x>β0), where p(·|·) is a known distribution and β0 is an unknown p-dimensional regression model.",4. Extension to generalized linear models,[0],[0]
Examples include the logistic regression model p(y = 1|x) =,4. Extension to generalized linear models,[0],[0]
"exp(x
>β0)",4. Extension to generalized linear models,[0],[0]
"1+exp(x>β0) , the
Possion count model p(yi = y|x)",4. Extension to generalized linear models,[0],[0]
= exp(yx >β0−e−x,4. Extension to generalized linear models,[0],[0]
>,4. Extension to generalized linear models,[0],[0]
"β0 ) y! , and many others.
Let S ∈ Sb(n, k) be the set of selected design points from X. Under the classical statistics regime,
the maximum likelihood (ML) estimator β̂",4. Extension to generalized linear models,[0],[0]
"ML
= arg minβ ∑ i∈S log p(yi|x>i β) is asymptotically efficient, and its asymptotic variance equals the Fisher’s information
I(XS ;β0)",4. Extension to generalized linear models,[0],[0]
":= ∑
i∈S Ey|x>i β0
[ −∂
2 log p(y|xi;β0) ∂β∂β>
]
ηi=x > i β0=
∑ i∈S Ey|ηi
[ −∂
2 log p(y|ηi) ∂η2i
] · xix>i .
",4. Extension to generalized linear models,[0],[0]
Here the second equality is due to the sufficiency of x>i β0 in a GLM.,4. Extension to generalized linear models,[0],[0]
"Note that for the linear regression model y = Xβ0 + w, the ML estimator is the ordinary least squares (OLS) β̂",4. Extension to generalized linear models,[0],[0]
"= (X>SXS)
−1XSyS and its Fisher’s information equals the sample covariance X>SXS .",4. Extension to generalized linear models,[0],[0]
"The experimental design problem can then be formalized as follows: 2
min S∈Sb(n,k) f(I(XS ;β0))",4. Extension to generalized linear models,[0],[0]
=,4. Extension to generalized linear models,[0],[0]
"min S∈Sb(n,k) f
(∑
i∈S ziz > i
) ;
(15)
zi = √ −Ey|ηi [ −∂
2 log p(yi|ηi) ∂η2i
] , ηi = x > i β0.
",4. Extension to generalized linear models,[0],[0]
"Suppose β̌ is a “pilot” estimate of β0, obtained from a uniformly sampled design subset S1.",4. Extension to generalized linear models,[0],[0]
A near-optimal design set S2 can then be constructed by minimizing Eq.,4. Extension to generalized linear models,[0],[0]
(15) using η̌i =,4. Extension to generalized linear models,[0],[0]
"x>i β̌. Such an approach was adopted in sequential design and active learning for ML estimators (Chaudhuri et al., 2015; Khuri et al., 2006); however, with our algorithm the quality of S2 is greatly improved.
",4. Extension to generalized linear models,[0],[0]
"2Under very mild conditions E[− ∂2 log p ∂η2 ] = E[( ∂ log p ∂η )2] is non-negative (Van der Vaart, 2000).",4. Extension to generalized linear models,[0],[0]
We compare the proposed method with several baseline methods on both synthetic and real-world data sets.,5. Numerical results,[0],[0]
"We only consider the harder “without replacement” setting, where each row of X can be selected at most once.",5. Numerical results,[0],[0]
"We compare our algorithm with three simple heuristic methods that apply to all optimality criteria:
1.",5.1. Methods and their implementation,[0],[0]
"Uniform sampling: Ŝ is sampled uniformly at random without replacement from the design pool X;
2.",5.1. Methods and their implementation,[0],[0]
Weighted sampling: first the optimal solution π∗ of Eq.,5.1. Methods and their implementation,[0],[0]
"(6) is computed with r = k; afterwards, Ŝ is sampled without replacement according to the distribution specified by π∗/k.",5.1. Methods and their implementation,[0],[0]
"Recall that (Wang et al., 2016) proved that weighted sampling works when k is sufficiently large compared to p (cf. Table 1).",5.1. Methods and their implementation,[0],[0]
"3
3.",5.1. Methods and their implementation,[0],[0]
"Fedorov’s exchange (Miller & Nguyen, 1994): the algorithm starts with a random subset S0 ∈ Sb(n, k) and iteratively exchanges two coordinates i ∈ S0, j /∈",5.1. Methods and their implementation,[0],[0]
S0 such that the objective is minimized after the exchange.,5.1. Methods and their implementation,[0],[0]
"The algorithm terminates if no such exchange can reduce the objective, or T iterations are reached.
",5.1. Methods and their implementation,[0],[0]
"All algorithms are implemented in MATLAB, except for the Fedorov’s exchange algorithm, which is implemented in C due to efficiency concerns.",5.1. Methods and their implementation,[0],[0]
We also apply the ShermanMorrison formula (A+λuu>)−1 = A−1 + λA,5.1. Methods and their implementation,[0],[0]
"−1uu>A−1
1+λu>A−1u
and the matrix determinant lemma det(A + λuu>) =
3Fact 2.2 ensures that π∗/k is a valid probability distribution.
",5.1. Methods and their implementation,[0],[0]
Table 3.,5.1. Methods and their implementation,[0],[0]
"Results on the Minnesota wind speed dataset (n =
2642, p = 15, k = 30).",5.1. Methods and their implementation,[0],[0]
"MSE is defined as
√
1 n ‖y −Vβ̂‖22.
fV MSE fG MSE
UNIFORM SAMPLING 94.1 1.10 3093 1.34 WEIGHTED SAMPLING 21.4 0.89 2451 1.13
FEDOROV’S EXCHANGE 10.0 0.86 29.2 0.78 (running time /secs) 15 - 1857 -
ALGORITHM 1 10.8 0.72 29.2 0.76 (running time /secs)",5.1. Methods and their implementation,[0],[0]
"< 1 - < 1 -
FULL-SAMPLE OLS - 0.55 - 0.55
(1 + λu>A−1u>)",5.1. Methods and their implementation,[0],[0]
det(A),5.1. Methods and their implementation,[0],[0]
to accelerate computations of rank-1 updates of matrix inverse and determinant.,5.1. Methods and their implementation,[0],[0]
"For uniform sampling and weighted sampling, we report the median objective of 50 indpendent trials.",5.1. Methods and their implementation,[0],[0]
We only report the objective for one trial of Fedorov’s exchange method due to time constraints.,5.1. Methods and their implementation,[0],[0]
The maximum number of iterations T for Fedorov’s exchange is set at T = 100.,5.1. Methods and their implementation,[0],[0]
"We always set k = r in the optimization problem Eq. (6), and details of solving Eq.",5.1. Methods and their implementation,[0],[0]
(6) are placed in the appendix.,5.1. Methods and their implementation,[0],[0]
In Algorithm 1 we set α = 10; our similuations suggest that the algorithm is not sensitive to α.,5.1. Methods and their implementation,[0],[0]
"We synthesize a 1000× 50 design pool X as follows:
X =
[ XA 0500×25
0500×25 XB
] .
",5.2. Synthetic data,[0],[0]
"XA is a 500 × 25 random Gaussian matrix, re-scaled so that the eigenvalues of X>AXA satisfy a quadratic decay: σj(X > AXA) ∝",5.2. Synthetic data,[0],[0]
j−2; XB is a 500 × 25 Gaussian matrix with i.i.d. standard Normal variables.,5.2. Synthetic data,[0],[0]
"Both XA and XB have comparable Frobenius norm.
",5.2. Synthetic data,[0],[0]
"In Table 2 we report results on all 6 optimality criteria (fA, fD, fT , fE , fV , fG) for k ∈ {2p, 3p, 5p, 10p}.",5.2. Synthetic data,[0],[0]
We also report the running time (measured in seconds) of Algorithm 1 and the Fedorov’s exchange algorithm.,5.2. Synthetic data,[0],[0]
The other two sampling based algorithms are very efficient and always terminate within one second.,5.2. Synthetic data,[0],[0]
"We observe that our algorithm has the best performance for fE and fG, while still achieving comparable results for the other optimality criteria.",5.2. Synthetic data,[0],[0]
"It is also robust when k is small compared to p, while sampling based methods occasionally produce designs that are not even full rank.",5.2. Synthetic data,[0],[0]
"Finally, Algorithm 1 is computationally efficient and terminates within seconds for all settings.",5.2. Synthetic data,[0],[0]
"The Minnesota wind dataset collects wind speed information across n = 2642 locations in Minnesota, USA for a
period of 24 months (for the purpose of this experiment, we only use wind speed data for one month).",5.3. The Minnesota wind speed dataset,[0],[0]
"The 2642 locations are connected with 3304 bi-directional roads, which form an n× n sparse unweighted undirected graph G. Let L = diag(d)?G be the n×n Laplacian of G, where d is a vector of node degrees, and let V ∈ Rn×p be an orthonormal eigenbasis corresponding to the smallest p eigenvalues of L. (Chen et al., 2015) shows that the relatively smooth wind speed signal y ∈ Rn can be well approximated by using only p = 15 graph Laplacian basis.
",5.3. The Minnesota wind speed dataset,[0],[0]
"In Table 3 we compare the mean-square error (MSE) for prediction on the full design pool V: MSE =√
1 n‖y −Vβ̂‖22.",5.3. The Minnesota wind speed dataset,[0],[0]
"Because the objective is prediction based, we only consider the two prediction related criteria: fV (Σ) = tr(VΣ−1V>) and fG(Σ) = max diag(VΣ−1V>).",5.3. The Minnesota wind speed dataset,[0],[0]
"The subset size k is set as k = 2p = 30, which is much smaller than n = 2642.",5.3. The Minnesota wind speed dataset,[0],[0]
"We observe that Algorithm 1 consistently outperforms the other heuristic methods, and is so efficient that its running time is negligible.",5.3. The Minnesota wind speed dataset,[0],[0]
It is also interesting that by using k = 30 samples Algorithm 1 already achieves an MSE that is comparable to the OLS on the entire n = 2642 design pool.,5.3. The Minnesota wind speed dataset,[0],[0]
"We proposed a computationally efficient algorithm that approximately computes optimal solutions for the experimental design problem, with near-optimal requirement on k (i.e., the number of experiments to choose).",6. Concluding remarks and open questions,[0],[0]
"In particular, we obtained a constant approximation under the very weak condition k > 2p, and a (1 + ε) approximation if replacement or over-sampling is allowed.",6. Concluding remarks and open questions,[0],[0]
"Our algorithm works for all regular optimality criteria.
",6. Concluding remarks and open questions,[0],[0]
"An important open question is to achieve (1 + ε) relative approximation ratio under the “proper sampling” regime k = r, or the “slight over-sampling” regime k = (1 + δ)r, for the without replacement model.",6. Concluding remarks and open questions,[0],[0]
"It was shown in (Wang et al., 2016) that a simple greedy method achieves (1 + ε) approximation ratio for A- and V-optimality provided that k = Ω(p2/ε).",6. Concluding remarks and open questions,[0],[0]
"Whether such analysis can be extended to other optimality criteria and whether the p2 term can be further reduced to a near linear function of p remain open.
",6. Concluding remarks and open questions,[0],[0]
Another practical question is to develop fast-converging optimization methods for the continuous problem in Eq.,6. Concluding remarks and open questions,[0],[0]
"(6), especially for criteria that are not differentiable such as the E- and G-optimality, where subgradient methods have very slow convergence rate.
",6. Concluding remarks and open questions,[0],[0]
Acknowledgement This work is supported by NSF grants CAREER IIS-1252412 and CCF-1563918.,6. Concluding remarks and open questions,[0],[0]
"We thank Adams Wei Yu for providing an efficient implementation of the projection step, and other useful discussions.",6. Concluding remarks and open questions,[0],[0]
"We consider computationally tractable methods for the experimental design problem, where k out of n design points of dimension p are selected so that certain optimality criteria are approximately satisfied.",abstractText,[0],[0]
"Our algorithm finds a (1 + ε)approximate optimal design when k is a linear function of p; in contrast, existing results require k to be super-linear in p.",abstractText,[0],[0]
"Our algorithm also handles all popular optimality criteria, while existing ones only handle one or two such criteria.",abstractText,[0],[0]
Numerical results on synthetic and real-world design problems verify the practical effectiveness of the proposed algorithm.,abstractText,[0],[0]
Near-Optimal Design of Experiments via Regret Minimization,title,[0],[0]
"In this work, we study the robust subspace tracking (RST) problem and obtain one of the first two provable guarantees for it. The goal of RST is to track sequentially arriving data vectors that lie in a slowly changing low-dimensional subspace, while being robust to corruption by additive sparse outliers. It can also be interpreted as a dynamic (time-varying) extension of robust PCA (RPCA), with the minor difference that RST also requires a short tracking delay. We develop a recursive projected compressive sensing algorithm that we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its tracking delay is nearly optimal. We prove that NORST solves both the RST and the dynamic RPCA problems under weakened standard RPCA assumptions, two simple extra assumptions (slow subspace change and most outlier magnitudes lower bounded), and a few minor assumptions.
Our guarantee shows that NORST enjoys a near optimal tracking delay of O(r log n log(1/ )). Its required delay between subspace change times is the same, and its memory complexity is n times this value. Thus both these are also nearly optimal. Here n is the ambient space dimension, r is the subspaces’ dimension, and is the tracking accuracy. NORST also has the best outlier tolerance compared with all previous RPCA or RST methods, both theoretically and empirically (including for real videos), without requiring any model on how the outlier support is generated. This is possible because of the extra assumptions it uses.",text,[0],[0]
Principal Components Analysis (PCA) is one of the most widely used dimension reduction techniques.,1 Introduction,[0],[0]
"It finds a small number of orthogonal basis vectors, called principal components, along which most of the variability of the dataset lies.",1 Introduction,[0],[0]
"According to its modern definition [3], robust PCA (RPCA) is the problem of decomposing a given data matrix into the sum of a low-rank matrix (true data) and a sparse matrix (outliers).",1 Introduction,[0],[0]
The column space of the low-rank matrix then gives the desired principal subspace (PCA solution).,1 Introduction,[0],[0]
A common application of RPCA is in video analytics in separating a video into a slow-changing background image sequence (modeled as a low-rank matrix) and a foreground image sequence consisting of moving objects or people (sparse),1 Introduction,[0],[0]
[3].,1 Introduction,[0],[0]
Robust Subspace Tracking (RST) can be simply interpreted as a time-varying extension of RPCA.,1 Introduction,[0],[0]
"It assumes that the true data lies in a low-dimensional subspace that can change with time, albeit slowly.",1 Introduction,[0],[0]
The goal is to track this changing subspace over time in the presence of additive sparse outliers.,1 Introduction,[0],[0]
The offline version of this problem can be called dynamic (or time-varying) RPCA.,1 Introduction,[0],[0]
"RST requires the tracking delay to be small, while dynamic RPCA does not.",1 Introduction,[0],[0]
"Time-varying subspace is a more appropriate model for long data sequences, e.g., long surveillance videos, since if a
∗A shorter version of this manuscript [1] will be presented at ICML, 2018.",1 Introduction,[0],[0]
"Another small part, Corollary 5.18, will appear
in [2].
",1 Introduction,[0],[0]
"ar X
iv :1
71 2.
06 06
1v 4
[ cs
.I T
] 6
J ul
2 01
single subspace model is used the resulting matrix may not be sufficiently low-rank.",1 Introduction,[0],[0]
"Moreover the RST problem setting (short tracking delay) is most relevant for applications where real-time or near real-time estimates are needed, e.g., video-based surveillance (object tracking)",1 Introduction,[0],[0]
"[4], monitoring seismological activity [5], or detection of anomalous behavior in dynamic social networks [6].
",1 Introduction,[0],[0]
"In recent years, RPCA has since been extensively studied.",1 Introduction,[0],[0]
"Many fast and provably correct approaches now exist: PCP introduced in [3] and studied in [3, 7, 8], AltProj",1 Introduction,[0],[0]
"[9], RPCA-GD [10] and NO-RMC [11].",1 Introduction,[0],[0]
"There is much lesser work on provable dynamic RPCA and RST: original-ReProCS [12, 13, 14] for dynamic RPCA and simple-ReProCS [15] for both.",1 Introduction,[0],[0]
"The subspace tracking (ST) problem (without outliers), and with or without missing data, has been studied for much longer in [16, 17, 18, 19].",1 Introduction,[0],[0]
"However, all existing guarantees for it only consider the statistically stationary setting of data being generated from a single unknown subspace.",1 Introduction,[0],[0]
"Of course, the most general nonstationary model that allows the subspace to change at each time is not even identifiable since at least r data points are needed to compute an r-dimensional subspace even in the no noise or missing entries case.
",1 Introduction,[0],[0]
"In this work, we make the subspace tracking problem identifiable by assuming a piecewise constant model on subspace change.",1 Introduction,[0],[0]
"We show that it is possible to track the changing subspace to within accuracy as long as the subspace remains constant for at least O(r log n log(1/ )) time instants, and some other assumptions hold.",1 Introduction,[0],[0]
This is more than r by only log factors.,1 Introduction,[0],[0]
"Here n is the ambient space dimension.
",1 Introduction,[0],[0]
Notation.,1 Introduction,[0],[0]
"We use the interval notation [a, b] to refer to all integers between a and b, inclusive, and we use [a, b) := [a, b − 1].",1 Introduction,[0],[0]
"‖.‖ denotes the l2 norm for vectors and induced l2 norm for matrices unless specified otherwise, and ′ denotes transpose.",1 Introduction,[0],[0]
We use MT to denote a sub-matrix of M formed by its columns indexed by entries in the set T .,1 Introduction,[0],[0]
For a matrix P we use P (i) to denote its i-th row.,1 Introduction,[0],[0]
"In our algorithm statements, we use L̂t;α := [ ˆ̀t−α+1, · · · , ˆ̀t] and SV Dr[M ] to refer to the matrix of top of r left singular vectors of the matrix M .",1 Introduction,[0],[0]
A matrix P with mutually orthonormal columns is referred to as a basis matrix and is used to represent the subspace spanned by its columns.,1 Introduction,[0],[0]
"For basis matrices P1,P2, we use SE(P1,P2) := ‖(I − P1P1′)P2‖ as a measure of Subspace Error (distance) between their respective subspaces.",1 Introduction,[0],[0]
This is equal to the sine of the largest principal angle between the subspaces.,1 Introduction,[0],[0]
It is also called “projection distance” [20].,1 Introduction,[0],[0]
"If P1 and P2 are of the same dimension, SE(P1,P2) = SE(P2,P1).
",1 Introduction,[0],[0]
"We reuse the letters C, c to denote different numerical constants in each use.",1 Introduction,[0],[0]
Robust Subspace Tracking (RST) and Dynamic RPCA Problem Setting.,1 Introduction,[0],[0]
"At each time t,
we get a data vector yt ∈",1 Introduction,[0],[0]
"Rn that satisfies
yt := `t + xt + νt, for t = 1, 2, . . .",1 Introduction,[0],[0]
", d
where νt is small unstructured noise, xt is the sparse outlier vector, and `t is the true data vector that lies in a fixed or slowly changing low-dimensional subspace of Rn, i.e., `t = P(t)at where P(t) is an n× r basis matrix with r n and with ‖(I",1 Introduction,[0],[0]
− P(t−1)P(t−1)′)P(t)‖ small compared to ‖P(t)‖ = 1.,1 Introduction,[0],[0]
We use Tt to denote the support set of xt.,1 Introduction,[0],[0]
"Given an initial subspace estimate, P̂0, the goal is to track span(P(t)) and `t either immediately or within a short delay.",1 Introduction,[0],[0]
"A by-product is that `t, xt, and Tt can also be tracked on-the-fly.",1 Introduction,[0],[0]
"The initial subspace estimate, P̂0, can be computed by applying any of the existing RPCA solutions, e.g., PCP or AltProj, for the first roughly r data points, i.e., for Y[1,ttrain], with ttrain = Cr.
",1 Introduction,[0],[0]
Dynamic RPCA is the offline version of the above problem.,1 Introduction,[0],[0]
"Define matrices L,X,W ,Y with L =",1 Introduction,[0],[0]
"[`1, `2, . .",1 Introduction,[0],[0]
.,1 Introduction,[0],[0]
"`d] and Y ,X,W similarly defined.",1 Introduction,[0],[0]
The goal is to recover the matrix L and its column space with error.,1 Introduction,[0],[0]
"We use rL to denote the rank of L. The maximum fraction of nonzeros in any row (column) of the outlier matrix X is denoted by max-outlier-frac-row (max-outlier-frac-col).
",1 Introduction,[0],[0]
Identifiability and other assumptions.,1 Introduction,[0],[0]
The above problem definition does not ensure identifiability since either of L or X can be both low-rank and sparse.,1 Introduction,[0],[0]
"Moreover, if the subspace changes at every time, it is impossible to correctly estimate all the subspaces.",1 Introduction,[0],[0]
One way to ensure that L is not sparse is by requiring that its left and right singular vectors are dense (non-sparse) or “incoherent” w.r.t.,1 Introduction,[0],[0]
"a sparse vector [3, 8, 9].
",1 Introduction,[0],[0]
Definition 1.1.,1 Introduction,[0],[0]
"An n× r basis matrix P is µ-incoherent if maxi=1,2,..,n ‖P (i)‖22 ≤ µr/n. Here µ is called the coherence parameter.",1 Introduction,[0],[0]
"It quantifies the non-denseness of P .
",1 Introduction,[0],[0]
"A simple way to ensure that X is not low-rank is by imposing upper bounds on max-outlier-frac-row and max-outlier-frac-col [8, 9].",1 Introduction,[0],[0]
"One way to ensure identifiability of the changing subspaces is to assume that they are piecewise constant:
P(t) =",1 Introduction,[0],[0]
Pj for all t ∈,1 Introduction,[0],[0]
"[tj , tj+1), j = 1, 2, . . .",1 Introduction,[0],[0]
", J,
and to lower bound tj+1",1 Introduction,[0],[0]
− tj .,1 Introduction,[0],[0]
"Let t0 = 1 and tJ+1 = d. With this model, rL = rJ in general (except if subspace directions are repeated).",1 Introduction,[0],[0]
"The union of the column spans of all the Pj ’s is equal to the span of the left singular vectors of L. Thus, assuming that the Pj ’s are µ-incoherent implies their incoherence.",1 Introduction,[0],[0]
"We also assume that the subspace coefficients at are mutually independent over time, have identical and diagonal covariance matrices denoted by Λ, and are element-wise bounded.",1 Introduction,[0],[0]
"Element-wise bounded-ness of at’s, along with the statistical assumptions, is similar to incoherence of right singular vectors of L (right incoherence); see Remark 3.5.",1 Introduction,[0],[0]
"Because tracking requires an online algorithm that processes data vectors one at a time or in mini-batches, we need these statistical assumptions on the at’s.",1 Introduction,[0],[0]
"For the same reason, we also need to re-define max-outlier-frac-row as the maximum fraction of nonzeroes in any row of any α-consecutive-column sub-matrix of X.",1 Introduction,[0],[0]
Here α is the mini-batch size used by the RST algorithm.,1 Introduction,[0],[0]
"We will refer to it as max-outlier-frac-rowα to indicate this difference.
",1 Introduction,[0],[0]
Contributions.,1 Introduction,[0],[0]
(1) We develop a recursive projected compressive sensing (ReProCS) algorithm for RST that we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its tracking delay is nearly optimal.,1 Introduction,[0],[0]
We will refer to it as just “NORST” in the sequel.,1 Introduction,[0],[0]
NORST has a significantly improved (and simpler) subspace update step compared to all previous ReProCS-based methods.,1 Introduction,[0],[0]
"Our most important contribution is one of the first two provable guarantees for RST, and the first that ensures near optimal tracking delay, needs a near optimal lower bound on how long a subspace should remain constant, and needs minimal assumptions on subspace change.",1 Introduction,[0],[0]
"Moreover, our guarantee also shows that NORST is online (after initialization), fast (has the same complexity as vanilla r-SVD), and has memory complexity of order nr log n log(1/ ).",1 Introduction,[0],[0]
Here “online” means the following.,1 Introduction,[0],[0]
"After each subspace change, the algorithm detects the change in at most 2α = Cf2r log n time instants, and after that, it improves the subspace estimate every α time instants1.",1 Introduction,[0],[0]
"The improvement after each step is exponential and thus, one can get an -accurate estimate within K = C log(1/ ) such steps.",1 Introduction,[0],[0]
"Offline NORST also provably solves dynamic RPCA.
",1 Introduction,[0],[0]
(2) Our guarantees for both NORST and offline NORST (Theorem 2.2) essentially hold under “weakened” standard RPCA assumptions and two simple extra assumptions: (i) slow subspace change and (ii) a lower bound on most outlier magnitudes.,1 Introduction,[0],[0]
"(i) is a natural assumption for static camera videos (with no sudden scene changes) and (ii) is also easy because, by definition, an “outlier” is a large magnitude corruption.",1 Introduction,[0],[0]
The small magnitude ones get classified as νt.,1 Introduction,[0],[0]
"Besides these, we also need that at’s are mutually independent, have identical and diagonal covariance matrix Λ, and are element-wise bounded.",1 Introduction,[0],[0]
"Element-wise bounded-ness, along with the statistical assumptions on at, is similar to right incoherence of L; see Remark 3.5.",1 Introduction,[0],[0]
"For the initial Cr samples, NORST needs the outlier fractions to be O(1/r) (needed to apply AltProj).",1 Introduction,[0],[0]
"As explained in Sec. 2.3, the extra assumptions help ensure that, after initialization, NORST can tolerate a constant maximum fraction of outliers per row in any α-column-sub-matrix of the data matrix without assuming any outlier support generation model.",1 Introduction,[0],[0]
This statement assumes that the condition number of the covariance of at is a constant (with n).,1 Introduction,[0],[0]
"As is evident from Table 1, this
1The reason that just O(r logn) samples suffice for each update is because we assume that the at’s are element-wise bounded, νt is very small and with effective dimension r or smaller (see Theorem 2.2).",1 Introduction,[0],[0]
"These, along with the specific structure of the PCA problem we encounter (noise/error seen by the PCA step depends on the `t’s and thus has “effective dimension” r), is why so few samples suffice.
is better than what all existing RPCA approaches can tolerate.",1 Introduction,[0],[0]
"For the video application, this implies that NORST tolerates slow moving and occasionally static foreground objects much better than all other approaches.",1 Introduction,[0],[0]
"This is also corroborated by our experiments on real videos, e.g., see Fig 1 and Sec. 6.
(3) Unlike simple-ReProCS [15] or original-ReProCS [12, 13, 14], NORST needs only a coarse initialization which can be computed using just C log r iterations of any batch RPCA method such as AltProj applied to Cr initial samples.",1 Introduction,[0],[0]
"In fact, if the outlier magnitudes were very large for an initial set of O(r log n log r) time instants, or if the outliers were absent for this much time, even a random initialization would suffice.",1 Introduction,[0],[0]
This simple fact has two important implications.,1 Introduction,[0],[0]
"First, NORST with the subspace change detection step removed also provides an online, fast, memory-efficient, and provably correct approach for static RPCA (our problem with J = 1, i.e., with `t = Pat).",1 Introduction,[0],[0]
The other online solution for such a problem is ORPCA which comes with only a partial guarantee [21] (the guarantee requires intermediate algorithm estimates to be satisfying certain properties).,1 Introduction,[0],[0]
"Moreover, a direct corollary of our result is a guarantee that a minor modification of NORST-random (NORST with random initialization) also solves the subspace tracking with missing data (ST-missing) and the dynamic matrix completion (MC) problems.",1 Introduction,[0],[0]
"All existing guarantees for ST-missing [18, 19] hold only for the case of a single unknown subspace and are only partial guarantees.",1 Introduction,[0],[0]
"From the MC perspective, NORST-random does not assume any model on the set of observed entries.",1 Introduction,[0],[0]
"However, the tradeoff is that it needs many more observed entries.",1 Introduction,[0],[0]
"Both these results are given in Sec. 5.
Paper Organization.",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
In Sec. 2 we explain the main ideas of the NORST algorithm and present our main result for it (Theorem 2.2).,1 Introduction,[0],[0]
"We also discuss the implications of our guarantee, and provide detailed comparison with related work.",1 Introduction,[0],[0]
"In Sec. 3, we give the complete NORST algorithm and carefully explain the subspace change detection approach.",1 Introduction,[0],[0]
"In Sec. 4, we give the proof outline, the three main lemmas leading to the proof of Theorem 2.2, then also prove the lemmas.",1 Introduction,[0],[0]
"In Sec. 5, we provide useful corollaries for (a) Static RPCA, (b) Subspace Tracking with missing entries and (c) a simple extension to recover the guarantee of s-ReProCS from [15].",1 Introduction,[0],[0]
Empirical evaluation on synthetic and real-world datasets is described in Sec. 6.,1 Introduction,[0],[0]
"The complete proof of Theorem 2.2, of two auxiliary lemmas, and of the extensions is given in the Appendix.",1 Introduction,[0],[0]
NORST starts with a “good” estimate of the initial subspace.,2.1 NORST: Nearly-Optimal RST,[0],[0]
"This can be obtained by C log r iterations2 of AltProj applied to Y[1,ttrain] with ttrain = Cr.",2.1 NORST: Nearly-Optimal RST,[0],[0]
"It then iterates between (a) Projected Compressive
2Using C log r iterations helps ensure that the initialization error is O(1/ √ r).
",2.1 NORST: Nearly-Optimal RST,[0],[0]
Sensing (CS) /,2.1 NORST: Nearly-Optimal RST,[0],[0]
"Robust Regression3 in order to estimate the sparse outliers, xt’s, and hence the `t’s, and (b) Subspace Update to update the subspace estimate P̂(t).",2.1 NORST: Nearly-Optimal RST,[0],[0]
Projected CS proceeds as follows.,2.1 NORST: Nearly-Optimal RST,[0],[0]
"At time t, if the previous subspace estimate, P̂(t−1), is accurate enough, because of slow subspace change, projecting yt onto its orthogonal complement will nullify most of `t. We compute ỹt :",2.1 NORST: Nearly-Optimal RST,[0],[0]
= Ψyt where Ψ := I − P̂(t−1)P̂(t−1)′.,2.1 NORST: Nearly-Optimal RST,[0],[0]
"Thus, ỹt = Ψxt + Ψ(`t + νt) and ‖Ψ(`t +",2.1 NORST: Nearly-Optimal RST,[0],[0]
νt)‖ is small due to slow subspace change and small νt.,2.1 NORST: Nearly-Optimal RST,[0],[0]
Recovering xt from ỹt is now a CS / sparse recovery problem in small noise [22].,2.1 NORST: Nearly-Optimal RST,[0],[0]
"We compute x̂t,cs using noisy l1 minimization followed by thresholding based support estimation to obtain T̂t.",2.1 NORST: Nearly-Optimal RST,[0],[0]
A Least Squares (LS) based debiasing step on T̂t returns the final x̂t.,2.1 NORST: Nearly-Optimal RST,[0],[0]
We then estimate `t as ˆ̀t = yt− x̂t.,2.1 NORST: Nearly-Optimal RST,[0],[0]
"The ˆ̀t’s are then used for the Subspace Update step which involves (i) detecting subspace change, and (ii) obtaining improved estimates of the new subspace by K steps of r-SVD, each done with a new set of α samples of ˆ̀t.",2.1 NORST: Nearly-Optimal RST,[0],[0]
"While this step is designed under the piecewise constant subspace assumption (needed for identifiability of P(t)’s), if the goal is only to get good estimates of `t or xt, the method works even when this assumption may not hold, e.g., for real videos.",2.1 NORST: Nearly-Optimal RST,[0],[0]
"For ease of understanding, we present a basic version of NORST in Algorithm 1.",2.1 NORST: Nearly-Optimal RST,[0],[0]
This assumes the change times tj are known.,2.1 NORST: Nearly-Optimal RST,[0],[0]
"The actual algorithm, that we study and implement, detects these automatically.",2.1 NORST: Nearly-Optimal RST,[0],[0]
It is given as Algorithm 2 in Sec. 3.,2.1 NORST: Nearly-Optimal RST,[0],[0]
"Before stating the result, we precisely define max-outlier-frac-col and max-outlier-frac-rowα.",2.2 Main Result,[0],[0]
"Since NORST is an online approach that performs outlier support recovery one data vector at a time, it needs different bounds on both.",2.2 Main Result,[0],[0]
Let max-outlier-frac-col := maxt |Tt|/n.,2.2 Main Result,[0],[0]
We define max-outlier-frac-rowα as the maximum fraction of outliers (nonzeros) per row of any sub-matrix of X with α consecutive columns.,2.2 Main Result,[0],[0]
"To understand
this precisely, for a time interval, J , define γ(J ) :",2.2 Main Result,[0],[0]
"= maxi=1,2,...,n 1|J | ∑ t∈J 1{i∈Tt} where 1S is the indicator
function for statement S. Thus, ∑
t∈J 1{i∈Tt} counts the number of outliers (nonzeros) in row i of XJ ,
and so γ(J ) is the maximum outlier fraction in any row of the sub-matrix XJ of X.",2.2 Main Result,[0],[0]
Let J α denote a time interval of duration α.,2.2 Main Result,[0],[0]
"Then max-outlier-frac-rowα := maxJ α⊆[1,d] γ(J α).
",2.2 Main Result,[0],[0]
"We use t̂j to denote the time instant at which the j-th subspace change time is detected by Algorithm
2.
Theorem 2.2.",2.2 Main Result,[0],[0]
Consider Algorithm 2 given in the next section.,2.2 Main Result,[0],[0]
"Let α := Cf2r log n, Λ := E[a1a1′], λ+ := λmax(Λ), λ − := λmin(Λ), f",2.2 Main Result,[0],[0]
":= λ +/λ−, and let xmin := mint mini∈Tt(xt)i denote the minimum outlier magnitude.",2.2 Main Result,[0],[0]
"Pick an ε ≤ min(0.01, 0.03 minj SE(Pj−1,Pj)2/f).",2.2 Main Result,[0],[0]
Let K := C log(1/ε).,2.2 Main Result,[0],[0]
"If
1.",2.2 Main Result,[0],[0]
"Pj’s are µ-incoherent; and at’s are zero mean, mutually independent over time t, have identical
covariance matrices, i.e. E[atat′] = Λ, are element-wise uncorrelated (Λ is diagonal), are elementwise bounded (for a numerical constant η, (at) 2 i ≤ ηλi(Λ)), and are independent of all outlier supports Tt;
2. ‖νt‖2 ≤",2.2 Main Result,[0],[0]
"cr‖E[νtνt′]‖, ‖E[νtνt′]‖ ≤",2.2 Main Result,[0],[0]
"cε2λ−, νt’s are zero mean, mutually independent, and independent of xt, `t;
3.",2.2 Main Result,[0],[0]
"max-outlier-frac-col ≤ c1/µr, max-outlier-frac-rowα ≤ b0 := c2f2 ;
4. subspace change: let ∆ := maxj SE(Pj−1,Pj), assume that
3Robust Regression (with a sparsity model on the outliers) assumes that observed data vector y satisfies y = P̂ a+x+ b where P̂ is a tall matrix (given), a is the vector of (unknown) regression coefficients, x is the (unknown) sparse outliers, b is (unknown) small noise/modeling error.",2.2 Main Result,[0],[0]
"An obvious way to solve this is by solving mina,x λ‖x‖1 + ‖y",2.2 Main Result,[0],[0]
− P̂ a− x‖2.,2.2 Main Result,[0],[0]
"In this, one can solve for a in closed form to get â = P̂ ′(y−x).",2.2 Main Result,[0],[0]
"Substituting this, the minimization simplifies to minx λ‖x‖1 +‖(I− P̂ P̂ ′)(y",2.2 Main Result,[0],[0]
− x)‖2.,2.2 Main Result,[0],[0]
"This is equivalent to the Lagrangian version of the projected CS problem that NORST solves (see line 7 of Algorithm 1).
",2.2 Main Result,[0],[0]
(a) tj+1,2.2 Main Result,[0],[0]
"− tj > (K + 2)α, and (b) ∆ ≤ 0.8 and C1 √ rλ+(∆ + 2ε) ≤ xmin;
5. initialization4: SE(P̂0,P0) ≤ 0.25, C1 √ rλ+SE(P̂0,P0) ≤ xmin;
and (6) algorithm parameters are set as given in Algorithm 2; then, with probability (w.p.) at least 1− 10dn−10,
SE(P̂(t),P(t))",2.2 Main Result,[0],[0]
≤  (ε+ ∆) if t ∈,2.2 Main Result,[0],[0]
"[tj , t̂j + α), (0.3)k−1(ε+ ∆) if t ∈",2.2 Main Result,[0],[0]
[t̂j +,2.2 Main Result,[0],[0]
"(k − 1)α, t̂j + kα), ε if t ∈",2.2 Main Result,[0],[0]
"[t̂j +Kα+ α, tj+1).
",2.2 Main Result,[0],[0]
"Treating f as a numerical constant, the memory complexity is O(nα) = O(nr log n) and time complexity is O(ndr log(1/ε)).
",2.2 Main Result,[0],[0]
Corollary 2.3.,2.2 Main Result,[0],[0]
"Under Theorem 2.2 assumptions, the following also hold:
1. ‖x̂t",2.2 Main Result,[0],[0]
− xt‖ = ‖ ˆ̀t,2.2 Main Result,[0],[0]
"− `t‖ ≤ 1.2(SE(P̂(t),P(t))",2.2 Main Result,[0],[0]
"+ ε)‖`t‖ with SE(P̂(t),P(t)) bounded as above,
2.",2.2 Main Result,[0],[0]
"at all times, t, T̂t = Tt,
3.",2.2 Main Result,[0],[0]
tj ≤,2.2 Main Result,[0],[0]
t̂j ≤,2.2 Main Result,[0],[0]
"tj + 2α,
4.",2.2 Main Result,[0],[0]
"Offline-NORST (last few lines of Algorithm 2): SE(P̂ offline(t) ,P(t))",2.2 Main Result,[0],[0]
"≤ ε, ‖x̂ offline t −xt‖ = ‖",2.2 Main Result,[0],[0]
"ˆ̀ offline t −
`t‖ ≤ ε‖`t‖ at all t.",2.2 Main Result,[0],[0]
"Its memory complexity is O(Knα) = O(nr log n log(1/ε)).
",2.2 Main Result,[0],[0]
Remark 2.4 (Relaxing outlier magnitudes lower bound).,2.2 Main Result,[0],[0]
The assumption on xmin (outlier magnitudes) required by Theorem 2.2 can be significantly relaxed to the following which only requires that most outlier magnitudes are lower bounded.,2.2 Main Result,[0],[0]
"Assume that the outlier magnitudes are such that the following holds: xt can be split as xt = (xt)small + (xt)large with the two components being such that, in the k-th subspace
update interval 5, ‖(xt)small‖ ≤ 0.3k−1(ε + ∆)",2.2 Main Result,[0],[0]
"√ rλ+ and the smallest nonzero entry of (xt)large is larger
than C1 · 0.3k−1(ε + ∆) √ rλ+.",2.2 Main Result,[0],[0]
"For the case of j = 0, we need the bound to hold with ∆ replaced by ∆init = SE(P̂0,P0), and ε replaced by zero.
",2.2 Main Result,[0],[0]
"If there were a way to bound the element-wise error of the CS step (instead of the l2 norm error), one
could relax the above requirement even more.
Discussion.",2.2 Main Result,[0],[0]
"This discussion assumes that f is a constant (does not increase with n), i.e., it is O(1).",2.2 Main Result,[0],[0]
"Theorem 2.2 shows that, with high probability (whp), when using NORST, the subspace change gets detected within a delay of at most 2α = Cf2(r log n) time instants, and the subspace gets estimated to ε error within at most (K + 2)α",2.2 Main Result,[0],[0]
= Cf2(r log n) log(1/ε) time instants.,2.2 Main Result,[0],[0]
"The same is also true for the recovery error of xt and `t. Both the detection and tracking delay are within log factors of the optimal since r is the minimum delay needed even in the noise-free, i..e, xt = νt = 0, case.",2.2 Main Result,[0],[0]
"The fact that NORST can detect subspace change within a short delay can be an important feature for certain applications, e.g., this feature is used in [6] to detect structural changes in a dynamic social network.",2.2 Main Result,[0],[0]
"Moreover, if offline processing is allowed, we can guarantee recovery within normalized error ε at all time instants.",2.2 Main Result,[0],[0]
"This implies that offline-NORST solves the dynamic RPCA problem.
",2.2 Main Result,[0],[0]
"Observe that Theorem 2.2 allows a constant maximum fraction of outliers per row (after initialization), without making any assumption on how the outlier support is generated, as long as the extra assumptions
4This can be satisfied by applying C log r iterations of AltProj",2.2 Main Result,[0],[0]
[9] on the first Cr data samples and assuming that these have outlier fractions in any row or column bounded by c/r. 5k-th subspace update interval refers to Jk :=,2.2 Main Result,[0],[0]
"[t̂j + (k − 1)α, t̂j + kα) for k > 1 and J1 =",2.2 Main Result,[0],[0]
"[tj , t̂j + α) for k = 1.",2.2 Main Result,[0],[0]
"The first interval also includes the subspace detection interval, [tj , t̂j), since the analysis of the projected CS step for this interval is the same as for [t̂j , t̂j + α).
",2.2 Main Result,[0],[0]
Algorithm 1 Basic-NORST (with tj known).,2.2 Main Result,[0],[0]
The actual algorithm that detects tj automatically is Algorithm 2.,2.2 Main Result,[0],[0]
"Obtain P̂0 by C(log r) iterations of AltProj on Y[1,ttrain] with ttrain = Cr followed by SVD on the output L̂.
1: Input: yt, Output: x̂t, ˆ̀t, P̂(t), T̂t 2: Parameters: K ← C log(1/ε), α← Cf2r log n, ωsupp ← xmin/2, ξ ← xmin/15, r 3: Initialize: j ← 1, k ← 1 P̂(ttrain)",2.2 Main Result,[0],[0]
"← P̂0 4: for t > ttrain do 5: Ψ← I − P̂(t−1)P̂(t−1)′ 6: ỹt ← Ψyt. 7: x̂t,cs ← arg minx̃ ‖x̃‖1 s.t. ‖ỹt −Ψx̃‖ ≤ ξ. 8",2.2 Main Result,[0],[0]
": T̂t ← {i : |x̂t,cs| > ωsupp}.",2.2 Main Result,[0],[0]
"9: x̂t ← IT̂t(ΨT̂t ′ΨT̂t) −1ΨT̂t ′ỹt.
10: ˆ̀t ← yt",2.2 Main Result,[0],[0]
− x̂t.,2.2 Main Result,[0],[0]
"11: if t = tj + kα− 1 then 12: P̂j,k ← SV Dr[L̂t;α], P̂(t) ← P̂j,k, k ← k + 1. 13: else 14: P̂(t) ← P̂(t−1).",2.2 Main Result,[0],[0]
"15: end if 16: if t = tj +Kα− 1 then 17: P̂j ← P̂(t), k ← 1, j ← j + 1 18: end if 19: end for
Projected-CS
(Robust
Regression).
",2.2 Main Result,[0],[0]
"Subspace
Update.
discussed below hold.",2.2 Main Result,[0],[0]
We explain why this is possible in Sec. 2.3.,2.2 Main Result,[0],[0]
"Of course, for the initial Cr samples, NORST needs max-outlier-frac-rowCr ∈ O(1/r) (needed to apply AltProj).",2.2 Main Result,[0],[0]
"Also, the memory complexity guaranteed by Theorem 2.2 is nearly d/r times better than that of all existing RPCA solutions; see Table 1.",2.2 Main Result,[0],[0]
"The time complexity is worse than that of only NO-RMC6, but NO-RMC needs d ≥ cn",2.2 Main Result,[0],[0]
(unreasonable requirement for videos which often have much fewer frames d than the image size n).,2.2 Main Result,[0],[0]
"Finally, NORST also needs outlier fraction per column to be O(1/r) instead of O(1/rL).",2.2 Main Result,[0],[0]
"If J is large, e.g. if J = d/(r log n), it is possible that rL",2.2 Main Result,[0],[0]
r.,2.2 Main Result,[0],[0]
We should clarify that NORST allows max-outlier-frac-rowα ∈ O(1) but this does not necessarily imply that the number of outliers in each row can be this high.,2.2 Main Result,[0],[0]
The reason is it only allows the fraction per column to only be O(1/r).,2.2 Main Result,[0],[0]
"Thus, for a matrix of size n × α, it allows the total number of outliers to be O(min(nα, nα/r))",2.2 Main Result,[0],[0]
= O(nα/r).,2.2 Main Result,[0],[0]
"Thus the average fraction allowed is only O(1/r).
",2.2 Main Result,[0],[0]
NORST needs the following extra assumptions.,2.2 Main Result,[0],[0]
"The main extra requirement is that xmin be lower bounded as given in the last two assumptions of Theorem 2.2, or as stated in Remark 2.4.",2.2 Main Result,[0],[0]
"The lower bound on xmin is reasonable 7 as long as the initial subspace estimate is accurate enough and the subspace changes slowly enough so that both ∆ and SE(P̂0,P0) are O(1/ √ r).",2.2 Main Result,[0],[0]
This requirement may seem restrictive on first glance but actually is not.,2.2 Main Result,[0],[0]
The reason is that SE(.) is only measuring the largest principal angle.,2.2 Main Result,[0],[0]
This bound on SE still allows the chordal distance between the two subspaces to be O(1).,2.2 Main Result,[0],[0]
Chordal distance [20] is the l2 norm of the vector containing the sine of all principal angles.,2.2 Main Result,[0],[0]
The second related extra requirement is an upper bound on ∆ (slow subspace change) which depends on the value of xmin.,2.2 Main Result,[0],[0]
We discuss this point next.,2.2 Main Result,[0],[0]
"Other than these two, NORST only needs simple statistical assumptions on
6NO-RMC is so fast because it is actually a robust matrix completion solution and it deliberately undersamples the entire
data matrix Y to get a faster RPCA algorithm.",2.2 Main Result,[0],[0]
"7requires xmin to be C √ λ+ or larger.
",2.2 Main Result,[0],[0]
at’s.,2.2 Main Result,[0],[0]
The zero-mean assumption is a minor one.,2.2 Main Result,[0],[0]
The assumption that Λ be diagonal is also minor 8.,2.2 Main Result,[0],[0]
"In the video setting, zero-mean can be ensured by subtracting the empirical average of the background images computing using the first ttrain frames.",2.2 Main Result,[0],[0]
Mutual independence of at’s holds if the changes in each background image w.r.t.,2.2 Main Result,[0],[0]
"a “mean” background are independent, when conditioned on their subspace.",2.2 Main Result,[0],[0]
"This is valid, for example, if the background changes are due to illumination variations or due to moving curtains (see Fig. 5).",2.2 Main Result,[0],[0]
"Moreover, by using the approach of [14], it is possible to relax this to just requiring that the at’s satisfy an autoregressive model over time.",2.2 Main Result,[0],[0]
"Element-wise boundedness, along with the above, is similar to right incoherence (see Remark 3.5).
",2.2 Main Result,[0],[0]
Outlier v/s Subspace Assumptions.,2.2 Main Result,[0],[0]
"When there are fewer outliers in the data or when outliers are easy to detect, one would expect to need weaker assumptions on the true data’s subspace and/or on its rate of change.",2.2 Main Result,[0],[0]
This is indeed true.,2.2 Main Result,[0],[0]
The max-outlier-frac-col bound relates max-outlier-frac-col to µ (not-denseness parameter) and r (subspace dimension).,2.2 Main Result,[0],[0]
"The upper bound on ∆ implies that, if xmin is
8It only implies that Pj is the matrix of principal components of E[LjL′j ]",2.2 Main Result,[0],[0]
"where Lj := [`tj , `tj+1, . . .",2.2 Main Result,[0],[0]
", `tj+1−1].
larger (outliers are easier to detect), a larger amount of subspace change ∆ can be tolerated.",2.2 Main Result,[0],[0]
"The relation of max-outlier-frac-row to rate of subspace change is not evident from the way the guarantee is stated above because we have assumed max-outlier-frac-row ≤ b0 := c/f2 with c being a numerical constant, and used this to get a simple expression for K. If we did not do this, we would get K = Cd 1− log(√b0f) log( c∆ 0.8ε)e, see Remark A.1.",2.2 Main Result,[0],[0]
Since we need tj+1,2.2 Main Result,[0],[0]
"− tj ≥ (K + 2)α, a smaller b0 means a larger ∆ can be tolerated for the same delay, or vice versa.
",2.2 Main Result,[0],[0]
Algorithm Parameters.,2.2 Main Result,[0],[0]
"Algorithm 2 assumes knowledge of 4 model parameters: r, λ+, λ− and xmin to set the algorithm parameters.",2.2 Main Result,[0],[0]
"The initial dataset used for estimating P̂0 (using AltProj) can be used to get an accurate estimate of r, λ− and λ+ using standard techniques.",2.2 Main Result,[0],[0]
Thus one really only needs to set xmin.,2.2 Main Result,[0],[0]
"If continuity over time is assumed, we can let it be time-varying and set it as mini∈T̂t−1 |(x̂t−1)i| at t.
Related Work.",2.2 Main Result,[0],[0]
"For a summary of comparisons, see Table 1.",2.2 Main Result,[0],[0]
"In terms of other solutions for provably correct RST or dynamic RPCA, there is very little work.",2.2 Main Result,[0],[0]
"For RST, there is only one other provable algorithm, simple-ReProCS (s-ReProCS)",2.2 Main Result,[0],[0]
[15].,2.2 Main Result,[0],[0]
"This has the same tracking delay and memory complexity as NORST, however, it assumes that only one subspace direction can change at each change time.",2.2 Main Result,[0],[0]
This is a more restrictive model than ours.,2.2 Main Result,[0],[0]
"Moreover, it implies that the tracking delay of s-ReProCS is r-times sub-optimal.",2.2 Main Result,[0],[0]
"Also, s-ReProCS uses a projection-SVD step for subspace update (as opposed to simple SVD in NORST).",2.2 Main Result,[0],[0]
These two facts imply that it needs an -accurate subspace initialization in order to ensure that the later changed subspaces can be tracked with -accuracy.,2.2 Main Result,[0],[0]
"Thus, it does not provide a static RPCA or subspace tracking with missing data solution.
",2.2 Main Result,[0],[0]
"For dynamic RPCA, the earliest result was a partial guarantee (a guarantee that depended on intermediate algorithm estimates satisfying certain assumptions) for the original reprocs approach (originalReProCS) [12].",2.2 Main Result,[0],[0]
"This was followed up by two complete guarantees for reprocs-based approaches with minor modifications [13, 14].",2.2 Main Result,[0],[0]
For simplicity we will still call these “original-ReProCS”.,2.2 Main Result,[0],[0]
These guarantees needed very strong assumptions and their tracking delay was O(nr2/ 2).,2.2 Main Result,[0],[0]
"Since can be very small, this factor can be quite large, and hence one cannot claim that original-ReProCS solves RST.",2.2 Main Result,[0],[0]
Our work is a very significant improvement over all these works.,2.2 Main Result,[0],[0]
"(i) The guaranteed memory complexity, tracking delay, and required delay between subspace change times of NORST are all r/ 2 times lower than that of original-ReProCS.",2.2 Main Result,[0],[0]
(ii) All the original-ReProCS guarantees needed a very specific assumption on how the outlier support could change.,2.2 Main Result,[0],[0]
"They required an outlier support model inspired by a video moving object that moves in one direction for a long time; and whenever it moves, it must move by a fraction of s := maxt |Tt|.",2.2 Main Result,[0],[0]
This is very specific model with the requirement of moving by a fraction of s being the most restrictive.,2.2 Main Result,[0],[0]
Our result removes this model and replaces it with just a bound on max-outlier-frac-row.,2.2 Main Result,[0],[0]
We explain in the last para of Sec. 4.1 why this is possible.,2.2 Main Result,[0],[0]
"(iii) The subspace change model assumed in [13, 14] required a few new directions, that were orthogonal to Pj−1, to be added at time tj and some others to be removed.",2.2 Main Result,[0],[0]
"This is an unrealistic model for slow subspace change, e.g., in 3D, it implies that the subspace needs to change from the x-y plane to the y-z plane.",2.2 Main Result,[0],[0]
"Moreover because of this model, their results needed the “energy” (eigenvalues) along the newly added directions to be small for a period of time after each subspace change.",2.2 Main Result,[0],[0]
This is a strong (and not easy to interpret) requirement.,2.2 Main Result,[0],[0]
"Our result removes all these requirements and replaces them with a bound on SE(Pj−1,Pj) which is much more realistic.",2.2 Main Result,[0],[0]
"Thus, in 3D, our result allows the x-y plane to change to a slightly tilted x-y plane.
",2.2 Main Result,[0],[0]
An approach called modified-PCP (mod-PCP) was proposed to solve the problem of RPCA with partial subspace knowledge [23].,2.2 Main Result,[0],[0]
A corollary of its guarantee shows that it can also be used to solve dynamic RPCA [23].,2.2 Main Result,[0],[0]
"However, since it adapted the PCP proof techniques from [3], its pros and cons are similar to those of PCP, e.g., it also needs a uniformly randomly generated outlier support.",2.2 Main Result,[0],[0]
"As can be seen from Table 1, its pros and cons are similar to those of the PCP result by [3] (PCP(C)) discussed below.
",2.2 Main Result,[0],[0]
We also provide a comparison with provably correct RPCA approaches in Table 1.,2.2 Main Result,[0],[0]
"In summary,
NORST has significantly better memory complexity than all of them, all of which are batch; it has the best outlier tolerance (after initialization), and the second-best time complexity, as long as its extra assumptions hold.",2.2 Main Result,[0],[0]
"It can also detect subspace change quickly, which can be a useful feature.",2.2 Main Result,[0],[0]
Consider outlier tolerance.,2.2 Main Result,[0],[0]
"PCP(H), AltProj, RPCA-GD, and NO-RMC need both max-outlier-frac-row and max-outlier-frac-col to be O(1/rL); PCP(C)",2.2 Main Result,[0],[0]
[3] and modified-PCP [23] need the outlier support to uniformly random (strong requirement: for video it implies that objects are very small sized and jumping around randomly); and original-ReProCS needs it to satisfy a very specific moving object model described above (restrictive).,2.2 Main Result,[0],[0]
"Instead, after initialization, NORST only needs max-outlier-frac-rowα ∈ O(1) and max-outlier-frac-col ∈ O(1/r).",2.2 Main Result,[0],[0]
"As noted in [9], the standard RPCA problem (that only assumes left and right incoherence of L and nothing else) cannot tolerate a bound on outlier fractions in any row or any column that is larger than 1/rL 9.",2.3 The need for extra assumptions,[0],[0]
The reason NORST can tolerate a constant max-outlier-frac-rowα bound is because it uses extra assumptions.,2.3 The need for extra assumptions,[0],[0]
We explain the need for these here.,2.3 The need for extra assumptions,[0],[0]
"It recovers xt first and then `t and does this at each time t. When recovering xt, it exploits “good” knowledge of the subspace of `t (either from initialization or from the previous subspace’s estimate and slow subspace change), but it has no way to deal with the residual error, bt := (I − P̂(t−1)P̂(t−1)′)`t, in this knowledge.",2.3 The need for extra assumptions,[0],[0]
"Since the individual vector bt does not have any structure that can exploited10, the error in recovering xt cannot be lower than C‖bt‖.",2.3 The need for extra assumptions,[0],[0]
"This means that, to correctly recover the support of xt, xmin needs to be larger than C‖bt‖.",2.3 The need for extra assumptions,[0],[0]
This is where the xmin lower bound comes from.,2.3 The need for extra assumptions,[0],[0]
"As we will see in Sec. 4, correct support recovery is needed to ensure that the subspace estimate can be improved with each update.",2.3 The need for extra assumptions,[0],[0]
"In particular, it helps ensure that the error vectors et := xt− x̂t in a given subspace update interval are mutually independent, when conditioned on the yt’s from all past intervals.",2.3 The need for extra assumptions,[0],[0]
This step also uses element-wise boundedness of the at’s along with their mutual independence and identical covariances.,2.3 The need for extra assumptions,[0],[0]
We present the actual NORST algorithm (automatic NORST) in Algorithm 2.,3 Automatic NORST,[0],[0]
The main idea why automatic NORST works is the same as that of the basic algorithm with the exception of the additional subspace detection step.,3 Automatic NORST,[0],[0]
"The subspace detection idea is borrowed from [15], although its correctness proof has differences because we assume a much simpler subspace change model.",3 Automatic NORST,[0],[0]
"In Algorithm 2, the subspace update stage toggles between the “detect” phase and the “update” phase.",3 Automatic NORST,[0],[0]
It starts in the “update” phase with t̂0 = ttrain.,3 Automatic NORST,[0],[0]
We then perform K r-SVD steps with the k-th one done at t = t̂0 + kα,3 Automatic NORST,[0],[0]
− 1.,3 Automatic NORST,[0],[0]
"Each such step uses the last α estimates, i.e., uses L̂t;α.",3 Automatic NORST,[0],[0]
Thus at t = t̂0 +,3 Automatic NORST,[0],[0]
Kα,3 Automatic NORST,[0],[0]
"− 1, the subspace update of P0 is complete.",3 Automatic NORST,[0],[0]
"At this point, the algorithm enters the “detect” phase.
",3 Automatic NORST,[0],[0]
"For any j, if the j-th subspace change is detected at time t, we set t̂j = t.",3 Automatic NORST,[0],[0]
"At this time, the algorithm enters the “update” (subspace update) phase.",3 Automatic NORST,[0],[0]
We then perform K r-SVD steps with the k-th r-SVD step done at t = t̂j,3 Automatic NORST,[0],[0]
+,3 Automatic NORST,[0],[0]
kα− 1 (instead of at t = tj + kα− 1).,3 Automatic NORST,[0],[0]
"Each such step uses the last α estimates, i.e., uses 9The reason is this: let b0 = max-outlier-frac-row, one can construct a matrix X with b0 outliers in some rows that has rank equal to 1/b0.",3 Automatic NORST,[0],[0]
A simple way to do this would be to let the support and nonzero entries of X be constant for b0d columns before letting either of them change.,3 Automatic NORST,[0],[0]
Then the rank of X will be d/(b0d).,3 Automatic NORST,[0],[0]
A similar argument can be used for max-outlier-frac-col. 10However the bt’s arranged into a matrix do form a low-rank matrix whose approximate rank is r or even lower (if not all directions change).,3 Automatic NORST,[0],[0]
If we try to exploit this structure we end up with a modified-PCP [23] type approach.,3 Automatic NORST,[0],[0]
This needs the uniform random support assumption (used in its guarantee).,3 Automatic NORST,[0],[0]
"Or, if the [8] approach were used for its guaratee, for identifiability reasons similar to the one described above, it will still not tolerate outlier fractions larger than 1/rnew where rnew is the (approximate) rank of the matrix formed by the bt’s.
",3 Automatic NORST,[0],[0]
Algorithm 2 Automatic-NORST.,3 Automatic NORST,[0],[0]
"Obtain P̂0 by C(log r) iterations of AltProj on Y[1,ttrain] with ttrain = Cr followed by SVD on the output L̂.
1: Input: P̂0, yt, Output: x̂t, ˆ̀t, P̂(t) 2: Parameters: K ← C log(1/ε), α←",3 Automatic NORST,[0],[0]
"Cf2r log n, ωsupp ← xmin/2, ξ ← xmin/15, ωevals ← 2ε2λ+, r. 3: P̂(ttrain)",3 Automatic NORST,[0],[0]
"← P̂0; j ← 1, k ← 1 4: phase← update; t̂0 ← ttrain; 5: for t > ttrain do 6: Lines 5− 10 of Algorithm 1 7: if phase = detect and t = t̂j−1,fin + uα then 8: Φ← (I − P̂j−1P̂j−1′).",3 Automatic NORST,[0],[0]
"9: B ← ΦL̂t,α
10: if λmax(BB ′) ≥ αωevals then 11: phase← update, t̂j ← t, 12: end if 13: end if 14: if phase = update then 15: if t = t̂j + uα− 1 for u = 1, 2, · · · , then 16: P̂j,k ← SV Dr[L̂t;α], P̂(t) ← P̂j,k, k ← k + 1. 17: else 18: P̂(t) ← P̂(t−1) 19: end if 20: if t = t̂j",3 Automatic NORST,[0],[0]
"+Kα− 1 then 21: t̂j,fin ← t, P̂j ← P̂(t) 22:",3 Automatic NORST,[0],[0]
"k ← 1, j ← j + 1, phase← detect.",3 Automatic NORST,[0],[0]
23: end if 24: end if 25: end for 26:,3 Automatic NORST,[0],[0]
Offline NORST:,3 Automatic NORST,[0],[0]
At t = t̂j,3 Automatic NORST,[0],[0]
"+Kα, for all t ∈ [t̂j−1 +Kα, t̂j +Kα− 1], 27: P̂ offline(t)",3 Automatic NORST,[0],[0]
←,3 Automatic NORST,[0],[0]
"[P̂j−1, (I − P̂j−1P̂j−1 ′)P̂j ] 28: Ψ← I − P̂ offline(t) P̂ offline (t) ′",3 Automatic NORST,[0],[0]
29: x̂offlinet,3 Automatic NORST,[0],[0]
← IT̂t(ΨT̂t ′ΨT̂t) −1ΨT̂t,3 Automatic NORST,[0],[0]
"′yt 30: ˆ̀offlinet ← yt − x̂offlinet .
",3 Automatic NORST,[0],[0]
"Projected CS.
",3 Automatic NORST,[0],[0]
"Subspace
Detect Phase.
Subspace
Update Phase.
",3 Automatic NORST,[0],[0]
"Offline
NORST.
",3 Automatic NORST,[0],[0]
L̂t;α,3 Automatic NORST,[0],[0]
"Thus, at t = t̂j,fin = t̂j +",3 Automatic NORST,[0],[0]
"Kα − 1, the update is complete.",3 Automatic NORST,[0],[0]
"At this time, the algorithm enters the “detect” phase again.
",3 Automatic NORST,[0],[0]
"To understand the change detection strategy, consider the j-th subspace change.",3 Automatic NORST,[0],[0]
"Assume that the previous subspace Pj−1 has been accurately estimated by t = t̂j−1,fin = t̂j−1 +Kα−1 and that t̂j−1,fin < tj .",3 Automatic NORST,[0],[0]
Let P̂j−1 denote this estimate.,3 Automatic NORST,[0],[0]
"At this time, the algorithm enters the “detect” phase in order to detect the next (j-th) change.",3 Automatic NORST,[0],[0]
Let Bt := (I − P̂j−1P̂j−1′)L̂t;α.,3 Automatic NORST,[0],[0]
"For every t = t̂j−1,fin + uα− 1, u = 1, 2, . . .",3 Automatic NORST,[0],[0]
", we detect change by checking if the maximum singular value of Bt is above a pre-set threshold, √ ωevalsα, or not.
",3 Automatic NORST,[0],[0]
"We claim that, whp, under assumptions of Theorem 2.2, this strategy has no “false subspace detections” and correctly detects change within a delay of at most 2α samples.",3 Automatic NORST,[0],[0]
"The former is true because, for any t for which [t − α + 1, t] ⊆",3 Automatic NORST,[0],[0]
"[t̂j−1,fin, tj), all singular values of the matrix Bt will be close to
zero (will be of order ε √ λ+) and hence its maximum singular value will be below √ ωevalsα.",3 Automatic NORST,[0],[0]
"Thus, whp, t̂j ≥ tj .",3 Automatic NORST,[0],[0]
"To understand why the change is correctly detected within 2α samples, first consider t = t̂j−1,fin + d tj−t̂j−1,fin α eα := tj,∗. Since we assumed that t̂j−1,fin < tj (the previous subspace update is complete before the next change), tj lie in the interval [tj,∗ − α + 1, tj,∗].",3 Automatic NORST,[0],[0]
"Thus, not all of the `t’s in this interval lie in the new subspace.",3 Automatic NORST,[0],[0]
"Depending on where in the interval tj lies, the algorithm may or may not detect the change at this time.",3 Automatic NORST,[0],[0]
"However, in the next interval, i.e., for t ∈",3 Automatic NORST,[0],[0]
"[tj,∗ + 1, tj,∗ + α], all of the `t’s lie in the new subspace.",3 Automatic NORST,[0],[0]
We can prove that Bt for this time t will have maximum singular value that is above the threshold.,3 Automatic NORST,[0],[0]
"Thus, if the change is not detected at tj,∗, whp, it will get detected at tj,∗ + α.",3 Automatic NORST,[0],[0]
"Hence one can show that, whp, either t̂j = tj,∗, or t̂j =",3 Automatic NORST,[0],[0]
"tj,∗ + α, i.e., tj ≤",3 Automatic NORST,[0],[0]
t̂j ≤,3 Automatic NORST,[0],[0]
tj + 2α (see Appendix A).,3 Automatic NORST,[0],[0]
Time complexity.,3 Automatic NORST,[0],[0]
Consider initialization.,3 Automatic NORST,[0],[0]
"To ensure that SE(P̂0,P0) ∈ O(1/ √ r), we need to use C log r iterations of AltProj.",3 Automatic NORST,[0],[0]
Since there is no lower bound in the AltProj guarantee on the required number of matrix columns (except the trivial lower bound of rank),3 Automatic NORST,[0],[0]
"[9], we can use ttrain = Cr frames for initialization.",3 Automatic NORST,[0],[0]
Thus the initialization complexity is O(nttrainr 2 log( √ r) = O(nr3 log r),3 Automatic NORST,[0],[0]
[9].,3 Automatic NORST,[0],[0]
The projectedCS step complexity is equal to the cost of a matrix vector multiplication with the measurement matrix times negative logarithm of the desired accuracy in solving the l1 minimization problem.,3 Automatic NORST,[0],[0]
"Since the measurement matrix for the CS step is I−P̂(t−1)P̂(t−1)′, the cost per CS step (per frame) is O(nr log(1/ ))",3 Automatic NORST,[0],[0]
[24] and so the total cost is O((d−ttrain)nr log(1/ )).,3 Automatic NORST,[0],[0]
The subspace update involves at most ((d−ttrain)/α) rank r-SVD’s on n×αmatrices all of which have constant eigen-gap (this is proved in the proof of tTheorem 4.14 from [25] which we use to show correctness of this step).,3 Automatic NORST,[0],[0]
Thus the total time for subspace update steps is at most ((d − ttrain)/α) ∗ O(nαr log(1/ )),3 Automatic NORST,[0],[0]
= O((d,3 Automatic NORST,[0],[0]
− ttrain)nr log(1/ )),3 Automatic NORST,[0],[0]
[26].,3 Automatic NORST,[0],[0]
Thus the running time of the complete algorithm is O(ndr log(1/ ) +,3 Automatic NORST,[0],[0]
nr3 log r).,3 Automatic NORST,[0],[0]
"As long as r2 log r ≤ d log(1/ ), the time complexity of the entire algorithm is O(ndr log(1/ )).
",3 Automatic NORST,[0],[0]
"Remark 3.5 (Relating our assumptions to right incoherence of Lj := L[tj ,tj+1)",3 Automatic NORST,[0],[0]
[8]).,3 Automatic NORST,[0],[0]
"From our assumptions, Lj = PjAj with Aj :=",3 Automatic NORST,[0],[0]
"[atj ,atj+1, . .",3 Automatic NORST,[0],[0]
".atj+1−1], the columns of Aj are zero mean, mutually independent, have identical covariance Λ, Λ is diagonal, and are element-wise bounded as specified by Theorem
2.2.",3 Automatic NORST,[0],[0]
Let dj := tj+1,3 Automatic NORST,[0],[0]
− tj.,3 Automatic NORST,[0],[0]
"Define a diagonal matrix Σ with (i, i)-th entry σi and with σ2i := ∑ t(at) 2",3 Automatic NORST,[0],[0]
"i /dj.
Define a dj × r matrix Ṽ with the t-th entry of the i-th column being (ṽi)t := (at)i/(σi √ dj).",3 Automatic NORST,[0],[0]
"Then, Lj = PjΣṼ ′ and each column of Ṽ is unit 2-norm.",3 Automatic NORST,[0],[0]
"Also, from the bounded-ness assumption, (ṽi)
2 t ≤ η",3 Automatic NORST,[0],[0]
"λiσ2i · 1 dj
where η is a numerical constant.
Observe that PjΣṼ ′ is not exactly the SVD of Lj since the columns of Ṽ are not necessarily exactly mutually orthogonal.",3 Automatic NORST,[0],[0]
"However, if dj is large enough, one can argue using any law of large numbers’ result (e.g., Hoeffding inequality), that the columns of Ṽ are approximately mutually orthogonal whp.",3 Automatic NORST,[0],[0]
"Also, whp, σ2i ≥ 0.99λi.",3 Automatic NORST,[0],[0]
This also follows using Hoeffding11.,3 Automatic NORST,[0],[0]
"Thus, our assumptions imply that, whp, (ṽi)2t ≤ C/dj.",3 Automatic NORST,[0],[0]
"If one interprets Ṽ as an “approximation” to the right singular vectors of Lj, this is the right incoherence assumed by [8] and slightly stronger than what is assumed by [3, 9] and others (these require that the squared norm of each row of the matrix of right singular vectors be bounded by Cr/dj).
",3 Automatic NORST,[0],[0]
The claim that “Ṽ can be interpreted as an “approximation” to the right singular vectors of Lj” is not rigorous.,3 Automatic NORST,[0],[0]
But it is also not clear how to make it rigorous since our work uses statistical assumptions on the at’s.,3 Automatic NORST,[0],[0]
"To get the exact SVD of Lj, we need the SVD of Aj.",3 Automatic NORST,[0],[0]
"Suppose Aj SVD = RΣV ′, then Lj SVD = (PjR)ΣV
′. Here R will be an r × r orthonormal matrix.",3 Automatic NORST,[0],[0]
"Now it is not clear how to relate the element-wise bounded-ness assumption on at’s to an assumption on entries of V , since now there is no easy expression for each entry of V or of the entries of Σ in terms of at (since R is unknown).
",3 Automatic NORST,[0],[0]
11The first claim uses all the four assumptions on at; the second claim uses all assumptions except diagonal Λ,3 Automatic NORST,[0],[0]
In this section we first give the main ideas of the proof (without formal lemmas).,4 Proof Outline and (most of the) Proof,[0],[0]
We then state the three main lemmas and explain how they help prove Theorem 2.2.,4 Proof Outline and (most of the) Proof,[0],[0]
"After this, we prove the three lemmas.",4 Proof Outline and (most of the) Proof,[0],[0]
It is not hard to see that the “noise” bt := Ψ(`t + νt) seen by the projected CS step is proportional the error between the subspace estimate from (t − 1) and the current subspace.,4.1 Main idea of the proof,[0],[0]
"Moreover, incoherence (denseness) of the P(t)’s and slow subspace change together",4.1 Main idea of the proof,[0],[0]
imply that Ψ satisfies the restricted isometry property (RIP),4.1 Main idea of the proof,[0],[0]
[12].,4.1 Main idea of the proof,[0],[0]
"Using this, a result for noisy l1 minimization [22], and the lower bound assumption on outlier magnitudes, one can ensure that the CS step output is accurate enough and the outlier support Tt is correctly recovered.",4.1 Main idea of the proof,[0],[0]
"With this, we have that ˆ̀t =",4.1 Main idea of the proof,[0],[0]
`t + νt,4.1 Main idea of the proof,[0],[0]
− et where et := xt,4.1 Main idea of the proof,[0],[0]
− x̂t satisfies et = ITt(ΨTt ′ΨTt),4.1 Main idea of the proof,[0],[0]
"−1ITt
′Ψ′`t and ‖et‖ ≤",4.1 Main idea of the proof,[0],[0]
C‖bt‖. Consider subspace update.,4.1 Main idea of the proof,[0],[0]
"Every time the subspace changes, one can show that the change can be detected within a short delay.",4.1 Main idea of the proof,[0],[0]
"After that, the K SVD steps help get progressively improved estimates of the changed subspace.",4.1 Main idea of the proof,[0],[0]
"To understand this, observe that, after a subspace change, but before the first update step, bt is the largest and hence, et, is also the largest for this interval.",4.1 Main idea of the proof,[0],[0]
"However, because of good initialization or because of slow subspace change and previous subspace correctly recovered (to error ε), neither is too large.",4.1 Main idea of the proof,[0],[0]
"Both are proportional to (ε+ ∆), or to the initialization error.",4.1 Main idea of the proof,[0],[0]
"Using the idea below, we can show that we get a “good” first estimate of the changed subspace.
",4.1 Main idea of the proof,[0],[0]
The input to the PCA step is ˆ̀t and the noise seen by it is et.,4.1 Main idea of the proof,[0],[0]
"Notice that et depends on the true data `t. Hence this is a setting of PCA in data-dependent noise [27, 25].",4.1 Main idea of the proof,[0],[0]
"From [25], it is known that the subspace recovery error of the PCA step is proportional to the ratio between the time averaged noise
power plus time-averaged signal-noise correlation, (‖ ∑ t E[etet′]‖ + ‖ ∑
t E[`tet′‖)/α, and the minimum signal space eigenvalue,",4.1 Main idea of the proof,[0],[0]
λ−.,4.1 Main idea of the proof,[0],[0]
The instantaneous value of noise power is (∆ + ε)2 times λ+ while that of signal-noise correlation is of order (∆ + ε) times λ+.,4.1 Main idea of the proof,[0],[0]
"However, using the fact that et is sparse with support Tt that changes enough over time so that max-outlier-frac-rowα is bounded, one can argue (using Cauchy-Schwartz) that their time averaged values are √ max-outlier-frac-rowα times smaller.",4.1 Main idea of the proof,[0],[0]
"As a result, after the first subspace update, the subspace recovery error is at most 4 √ max-outlier-frac-rowα(λ+/λ−) times (∆ + ε).",4.1 Main idea of the proof,[0],[0]
"Since max-outlier-frac-rowα(λ+/λ−)2 is bounded by a constant c2 < 1, this means that, after the first subspace update, the subspace error is at most √ c2 times (∆ + ε).
",4.1 Main idea of the proof,[0],[0]
"This, in turn, implies that ‖bt‖, and hence ‖et‖, is also √ c2 times smaller in the second subspace update interval compared to the first.",4.1 Main idea of the proof,[0],[0]
"This, along with repeating the above argument, helps show that the second estimate of the changed subspace is √ c2 times better than the first and hence its error is ( √ c2) 2 times (∆ + ε).",4.1 Main idea of the proof,[0],[0]
"Repeating the argument K times, the K-th estimate has error ( √ c2) K times (∆ + ε).",4.1 Main idea of the proof,[0],[0]
"Since K = C log(1/ε), this is an ε accurate estimate of the changed subspace.
",4.1 Main idea of the proof,[0],[0]
"A careful application of the result of [25] is the reason why we are able to remove the moving object model assumption on the outlier support needed by the earlier guarantees for original-ReProCS [13, 14].
",4.1 Main idea of the proof,[0],[0]
"Applied to our problem, this result requires ‖ ∑
t∈J α ITtITt ′/α‖ to be bounded by a constant less than one.",4.1 Main idea of the proof,[0],[0]
"It is not hard to see that maxJ α∈[1,d] ‖ ∑ t∈J α ITtITt ′/α‖ = max-outlier-frac-rowα.",4.1 Main idea of the proof,[0],[0]
"To understand
this simply, the matrix ∑
t∈J α ITtITt ′ is diagonal, and the i-th diagonal entry counts the number of
time the index i appears in the support set Tt in the interval J α which is precisely the definition of max-outlier-frac-rowα ·α.",4.1 Main idea of the proof,[0],[0]
This is also why a constant bound on max-outlier-frac-rowα suffices for our setting.,4.1 Main idea of the proof,[0],[0]
"On the other hand the guarantees of [13, 14] required that, for any sequence of positive semi-definite
(p.s.d.)",4.1 Main idea of the proof,[0],[0]
"matrices, At, ‖ ∑ t∈J α ITtAtITt ′/α‖, be bounded by a constant less than one.",4.1 Main idea of the proof,[0],[0]
This is a much more stringent requirement; one way to satisfy it is using the moving object model on outlier supports assumed there.,4.1 Main idea of the proof,[0],[0]
"For simplicity, we give the proof for the νt = 0 case.",4.2 Main Lemmas,[0],[0]
"The changes with νt 6= 0 are minor, see Appendix A.
First consider the simpler case when tj ’s are known, i.e., consider Algorithm 1.",4.2 Main Lemmas,[0],[0]
"In this case, t̂j = tj .
",4.2 Main Lemmas,[0],[0]
Definition 4.6.,4.2 Main Lemmas,[0],[0]
"Define
1.",4.2 Main Lemmas,[0],[0]
"the constants used in Theorem 2.2: c1 = 0.01, c2 = 0.01, and C1 = 15 √ η
2.",4.2 Main Lemmas,[0],[0]
"s := max-outlier-frac-col · n
3.",4.2 Main Lemmas,[0],[0]
"φ+ = 1.2
4. bound on max-outlier-frac-rowα: b0 := 0.01/f 2.
5.",4.2 Main Lemmas,[0],[0]
"q0 := 1.2(ε+ SE(Pj−1,Pj)), qk = (0.3) kq0
6.",4.2 Main Lemmas,[0],[0]
et,4.2 Main Lemmas,[0],[0]
:,4.2 Main Lemmas,[0],[0]
= x̂t,4.2 Main Lemmas,[0],[0]
− xt.,4.2 Main Lemmas,[0],[0]
"Since νt = 0, et = `t − ˆ̀t
7.",4.2 Main Lemmas,[0],[0]
"Events: Γ0,0 := {assumed bound on SE(P̂0,P0)}, Γ0,k := Γ0,k−1 ∩ {SE(P̂0,k,P0) ≤ 0.3kSE(P̂0,P0)}, Γj,0 := Γj−1,K , Γj,k := Γj,k−1 ∩ {SE(P̂j,k,Pj) ≤ qk−1/4} for j = 1, 2, . . .",4.2 Main Lemmas,[0],[0]
", J and k = 1, 2, . . .",4.2 Main Lemmas,[0],[0]
",K.
8.",4.2 Main Lemmas,[0],[0]
"Using the expression for K given in the theorem, and since P̂j = P̂j,k (from the Algorithm), it
follows that Γj,K implies SE(P̂j ,Pj) = SE(P̂j,K ,Pj) ≤ ε.
Observe that, if we can show that Pr(ΓJ,K |Γ0,0) ≥ 1",4.2 Main Lemmas,[0],[0]
− dn−10 we will have obtained all the subspace recovery bounds of Theorem 2.2.,4.2 Main Lemmas,[0],[0]
The next two lemmas applied sequentially help show that this is true for Algorithm 1 (tj known).,4.2 Main Lemmas,[0],[0]
"The correctness of the actual algorithm (Algorithm 2) follows using these, Corollary 4.10, and Lemma 4.12.",4.2 Main Lemmas,[0],[0]
"The Theorem’s proof is in Appendix A.
Lemma 4.7 (first subspace update interval).",4.2 Main Lemmas,[0],[0]
"Under the conditions of Theorem 2.2, conditioned on Γj,0, 1.",4.2 Main Lemmas,[0],[0]
for all t ∈,4.2 Main Lemmas,[0],[0]
"[t̂j , t̂j + α), ‖Ψ`t‖ ≤ (ε + ∆)",4.2 Main Lemmas,[0],[0]
"√ ηrλ+ < xmin/15, ‖x̂t,cs − xt‖ ≤ 7xmin/15",4.2 Main Lemmas,[0],[0]
"< xmin/2,
T̂t = Tt, and the error et :",4.2 Main Lemmas,[0],[0]
= x̂t,4.2 Main Lemmas,[0],[0]
"− xt = `t − ˆ̀t satisfies
et = ITt ( ΨTt ′ΨTt )−1 ITt ′Ψ`t, (1)
and ‖et‖ ≤ 1.2(ε+ ∆) √ ηrλ+.
2.",4.2 Main Lemmas,[0],[0]
"w.p. at least 1 − 10n−10, the first subspace estimate P̂j,1 satisfies SE(P̂j,1,Pj)",4.2 Main Lemmas,[0],[0]
"≤ (q0/4), i.e., Γj,1 holds.
",4.2 Main Lemmas,[0],[0]
Lemma 4.8 (k-th subspace update interval).,4.2 Main Lemmas,[0],[0]
"Under the conditions of Theorem 2.2, conditioned on Γj,k−1,
1.",4.2 Main Lemmas,[0],[0]
for all t ∈,4.2 Main Lemmas,[0],[0]
"[t̂j + (k − 1)α, t̂j + kα − 1), all claims of the first part of Lemma 4.7 holds, ‖Ψ`t‖ ≤ 0.3k−1(ε+ ∆)",4.2 Main Lemmas,[0],[0]
"√ ηrλ+, and ‖et‖ ≤ (0.3)k−1 · 1.2(ε+ ∆) √ ηrλ+.
2. w.p.",4.2 Main Lemmas,[0],[0]
"at least 1−10n−10 the subspace estimate P̂j,k satisfies SE(P̂j,k,Pj) ≤ (qk−1/4), i.e., Γj,k holds.
",4.2 Main Lemmas,[0],[0]
Remark 4.9.,4.2 Main Lemmas,[0],[0]
"For the case of j = 0, in both the lemmas above, ∆ gets replaced by SE(P̂0,P0) and ε by zero.
",4.2 Main Lemmas,[0],[0]
Corollary 4.10.,4.2 Main Lemmas,[0],[0]
"Under the conditions of Theorem 2.2 the following hold
1.",4.2 Main Lemmas,[0],[0]
For all t ∈,4.2 Main Lemmas,[0],[0]
"[tj , t̂j), conditioned on Γj−1,K , all claims of the first item of Lemma 4.7 hold.
2.",4.2 Main Lemmas,[0],[0]
For all t ∈,4.2 Main Lemmas,[0],[0]
"[t̂j +Kα, tj+1), conditioned on Γj,K , the first item of Lemma 4.8 holds with k = K.
Thus, for all t, by the above two claims and Lemmas 4.7, 4.8, under appropriate conditioning, et satisfies (1).
",4.2 Main Lemmas,[0],[0]
We prove these lemmas in the next few subsections.,4.2 Main Lemmas,[0],[0]
"The projected CS proof (item one of both lemmas) uses the following lemma from [12] that relates the s-Restricted Isometry Constant (RIC), δs(.),",4.2 Main Lemmas,[0],[0]
"[22] of a projection matrix to the incoherence of its orthogonal complement.
",4.2 Main Lemmas,[0],[0]
Lemma 4.11.,4.2 Main Lemmas,[0],[0]
"[[12]] For an n × r basis matrix P , (1) δs(I − PP ′) = max|T |≤s",4.2 Main Lemmas,[0],[0]
‖IT ′P ‖2; and (2) max|T |≤s,4.2 Main Lemmas,[0],[0]
"‖IT ′P ‖2 ≤ smaxi=1,2,...,n ‖Ii′P ‖2 ≤ sµr/n.
The last bound of the above lemma is a consequence of Definition 1.1.",4.2 Main Lemmas,[0],[0]
We apply this lemma with s = max-outlier-frac-col · n.,4.2 Main Lemmas,[0],[0]
"The subspace update step proof (item 2 of both the above lemmas) uses a guarantee for PCA in sparse data-dependent noise, Theorem 4.14, due to [25].",4.2 Main Lemmas,[0],[0]
Notice that et = `t− ˆ̀t is the noise/error seen by the subspace update step.,4.2 Main Lemmas,[0],[0]
"By (1), this is sparse and depends on the true data `t.
Consider the actual tj unknown case.",4.2 Main Lemmas,[0],[0]
"The following lemma is used to show that, whp, we can detect subspace change within 2α time instants.",4.2 Main Lemmas,[0],[0]
"This lemmas assumes detection threshold ωevals = 2ε 2λ+ (see Algorithm 2).
",4.2 Main Lemmas,[0],[0]
Lemma 4.12 (Subspace Change Detection).,4.2 Main Lemmas,[0],[0]
Consider an α-length time interval J α ⊂,4.2 Main Lemmas,[0],[0]
"[tj , tj+1] (so that `t = Pjat).
",4.2 Main Lemmas,[0],[0]
1.,4.2 Main Lemmas,[0],[0]
"If Φ := I − P̂j−1P̂j−1′ and SE(P̂j−1,Pj−1) ≤ ε, with probability at least 1− 10n−10,
λmax
( 1
α ∑ t∈J α Φ ˆ̀t ˆ̀t",4.2 Main Lemmas,[0],[0]
"′Φ
)",4.2 Main Lemmas,[0],[0]
"≥ 0.28λ−SE2(Pj−1,Pj) >",4.2 Main Lemmas,[0],[0]
"ωevals
2.",4.2 Main Lemmas,[0],[0]
If Φ := I − P̂jP̂j ′,4.2 Main Lemmas,[0],[0]
"and SE(P̂j ,Pj) ≤ ε, with probability at least 1− 10n−10,
λmax
( 1
α ∑ t∈J α Φ ˆ̀t ˆ̀t",4.2 Main Lemmas,[0],[0]
"′Φ
) ≤",4.2 Main Lemmas,[0],[0]
1.37ε2λ+ < ωevals,4.2 Main Lemmas,[0],[0]
We first state a simple lemma.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"This is proved in Appendix B.
Lemma 4.13.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Let Q1, Q2 and Q3 be r-dimensional subspaces in Rn such that SE(Q1,Q2) ≤",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"∆1 and SE(Q2,Q3) ≤ ∆2.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Then, SE(Q1,Q3) ≤ ∆1 + ∆2.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Proof of Lemma 4.7.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Recall from Definition 4.6 that s := max-outlier-frac-col ·n and φ+ = 1.2.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Recall also that, for simplicity, we are considering the νt = 0 case.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Proof of item 1.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
First consider j > 0.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"We have conditioned on the event Γj,0 := Γj−1,K .",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"This implies that SE(P̂j−1,Pj−1) ≤ ε.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
We consider the interval t ∈,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"[t̂j , t̂j +α).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"For this interval, P̂(t−1) = P̂j−1 and thus Ψ = I − P̂j−1P̂j−1′ (from Algorithm).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"For the sparse recovery step, we first need to bound the 2s-RIC of Ψ. To do this, we first obtain bound on max|T |≤2s ‖IT ′P̂j−1‖ as follows.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Consider any set T such that |T | ≤ 2s.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Then,∥∥∥IT ′P̂j−1∥∥∥ ≤ ∥∥∥IT ′(I",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"− Pj−1Pj−1′)P̂j−1∥∥∥+ ∥∥∥IT ′Pj−1Pj−1′P̂j−1∥∥∥ ≤ SE(Pj−1, P̂j−1) + ∥∥IT ′Pj−1∥∥ = SE(P̂j−1,Pj−1) + ∥∥IT ′Pj−1∥∥
Using Lemma 4.11, and the bound on max-outlier-frac-col from Theorem 2.2,
max |T |≤2s ‖IT ′Pj−1‖2 ≤ 2smax i ‖Ii′Pj−1‖2 ≤
2sµr
n ≤ 0.01 (2)
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, using SE(P̂j−1,Pj−1) ≤ ε,
max |T |≤2s ∥∥∥IT ′P̂j−1∥∥∥ ≤ ε+ max |T |≤2s ∥∥IT ′Pj−1∥∥ ≤",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"ε+ 0.1 Finally, using Lemma 4.11, δ2s(Ψ) ≤ 0.112 < 0.15.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Hence∥∥∥(ΨTt ′ΨTt)−1∥∥∥ ≤ 11− δs(Ψ) ≤ 11− δ2s(Ψ) ≤ 11− 0.15 < 1.2 = φ+.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"When j = 0, there are some minor changes.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"From the initialization assumption, we have SE(P̂0,P0) ≤ 0.25.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, max|T |≤2s ∥∥∥IT ′P̂0∥∥∥",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
≤ 0.25 + 0.1 = 0.35.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, using Lemma 4.11, δ2s(Ψ0) ≤ 0.352 < 0.15.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"The rest of the proof given below is the same for j = 0 and j > 0.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Next we bound norm of bt := Ψ`t.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"This and the RIC bound will then be used to bound ‖x̂t,cs − xt‖. ‖bt‖ = ‖Ψ`t‖ =",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"∥∥∥(I − P̂j−1P̂j−1′)Pjat∥∥∥ ≤ SE(P̂j−1,Pj)",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"‖at‖
(a) ≤ (ε+ SE(Pj−1,Pj))",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"√ ηrλ+ := bb
where (a) follows from Lemma 4.13 with Q1 = P̂j−1, Q2 = Pj−1 and Q3 = Pj .",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Under the assumptions of Theorem 2.2, bb < xmin/15.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
This is why we have set ξ = xmin/15 in the Algorithm.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Using these facts, and δ2s(Ψ) ≤ 0.15, the CS guarantee from [22, Theorem 1.3] implies that
‖x̂t,cs − xt‖ ≤ 7ξ = 7xmin/15",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"< xmin/2
Consider support recovery.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"From above,
|(x̂t,cs − xt)i| ≤ ‖x̂t,cs − xt‖ ≤ 7xmin/15",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"< xmin/2
The Algorithm sets ωsupp = xmin/2.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Consider an index i ∈ Tt.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Since |(xt)i| ≥ xmin,
xmin − |(x̂t,cs)i| ≤ |(xt)i|",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"− |(x̂t,cs)i| ≤ |(xt − x̂t,cs)i| < xmin
2
Thus, |(x̂t,cs)i| > xmin2",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
= ωsupp which means i ∈ T̂t.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Hence Tt ⊆ T̂t.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Next, consider any j /∈",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Tt.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Then, (xt)j = 0 and so
|(x̂t,cs)j | = |(x̂t,cs)j)| − |(xt)j | ≤ |(x̂t,cs)j − (xt)j | ≤ bb < xmin
2
which implies j /∈ T̂t and T̂t ⊆ Tt implying that T̂t = Tt.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Finally, we get an expression for et and bound it.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
With T̂t =,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Tt and since Tt is the support of xt, xt = ITtITt ′xt, and so
x̂t = ITt ( ΨTt ′ΨTt )−1 ΨTt ′(Ψ`t + Ψxt) = ITt ( ΨTt ′ΨTt )−1",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
ITt,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"′Ψ`t + xt
since ΨTt ′Ψ = I ′TtΨ ′Ψ = ITt ′Ψ. Thus et = x̂t",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"− xt satisfies
et = ITt ( ΨTt ′ΨTt )−1 ITt ′Ψ`t and
‖et‖ ≤ ∥∥∥(ΨTt ′ΨTt)−1∥∥∥∥∥ITt ′Ψ`t∥∥ ≤ φ+ ∥∥ITt ′Ψ`t∥∥ ≤",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"1.2bb
Proof of Item 2 : We will use the following result from [25, Remark 4.18].
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Theorem 4.14 (PCA in sparse data-dependent noise (PCA-SDDN)).,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"We are given data vectors yt := `t + wt with wt = ITtMs,t`t, t = 1, 2, . . .",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
", α, where Tt is the support set of wt, and `t = Pat with at satisfying the assumptions of Theorem 2.2.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Pick an εSE > 0.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Assume that maxt ‖Ms,tP ‖2 ≤ q < 1, the fraction of non-zeroes in any row of the noise matrix [w1,w2, . . .",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
",wα] is bounded by b, and 3 √ bqf ≤ 0.9εSE/(1 + εSE).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Define
α0 := Cη q2f2
ε2SE r log n
For an α ≥ α0, let P̂ be the matrix of top r eigenvectors of D := 1α ∑α t=1 yty ′",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"t. With probability at least 1− 10n−10, SE(P̂ ,P ) ≤ εSE.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Since ˆ̀t = `t − et with et satisfying (1), updating P̂(t) from the ˆ̀t’s is a problem of PCA in sparse data-dependent noise (SDDN), et.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"To analyze this, we use Theorem 4.14 given above.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Recall from Item 1 of this lemma that, for t ∈",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"[t̂j , t̂j + α), et satisfies (1).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Recall from the Algorithm that we compute the first estimate of the j-th subspace, P̂j,1, as the top r eigenvectors of 1 α ∑t̂j+α−1 t=t̂j ˆ̀ t",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
ˆ̀,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"t ′. In the notation of Theorem 4.14, yt ≡ ˆ̀t, wt ≡ et, `t ≡",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"`t, Ms,t = − (ΨTt ′ΨTt) −1",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"ΨTt ′, P̂ = P̂j,1, P = Pj , and so ‖Ms,tP ‖ = ‖ (ΨTt ′ΨTt) −1",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"ΨTt
′Pj‖ ≤ 1.2(ε + SE(Pj−1,Pj))",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
:= q0.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Also, b ≡ b0 which is the upper bound on max-outlier-frac-rowα (see Definition 4.6).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Applying Theorem 4.14 with q ≡ q0, b ≡ b0 and using εSE = q0/4, observe that we require √
b0q0f ≤ 0.9(q0/4)
1 + (q0/4) .
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Since q0 = 1.2(ε + SE(Pj−1,Pj))",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"< 1.2(0.01 + 0.8) < 0.98 (follows from the bounds on ε and on ∆ given in Theorem 2.2), this holds if √ b0f ≤ 0.18.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
This is true since we have assumed b0 = 0.01/f2 (see Definition 4.6).,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, from Theorem 4.14, with probability at least 1 − 10n−10, SE(P̂j,1,Pj) ≤ q0/4.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, conditioned on Γj,0, with this probability, Γj,1 holds.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Remark 4.15 (Clarification about conditioning).,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"In the proof above we have used Theorem 4.14 which assumes that, for t ∈ J α, the at’s are mutually independent and the matrices Ms,t are either nonrandom or are independent of the at’s for this interval.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"When we apply the theorem for our proof, we are conditioning on Γj,0.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"This does not cause any problem since the event Γj,0 is a function of the random variable",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
yold,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
:,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"= {y1,y2, . . .",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
",yt̂j−1} where as our summation is over J α :=",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"[t̂j , t̂j + α).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Also, by Theorem 2.2, at’s are independent of the outlier supports Tt.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"To be precise, we are applying Theorem 4.14 conditioned on yold, for any yold ∈ Γj,0.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Even conditioned on yold, clearly, the matrices Ms,t used above are independent of the at’s for this interval.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Also, even conditioned on yold, the at’s for t ∈",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"[t̂j , t̂j+α) are clearly mutually independent.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, the theorem can be applied.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Its conclusion then tells us that, for any yold ∈ Γj,0, conditioned on yold, with probability at least 1−10n−10, SE(P̂j,1,Pj) ≤ q0/4.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Since this holds with the same probability for all yold ∈ Γj,0, it also holds with the same probability when we condition on Γj,0.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, conditioned on Γj,0, with this probability, Γj,1 holds.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"An analogous argument will also apply to the following proofs.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"4.4 Proof of Lemma 4.8: lemma for projected CS and subspace update in k-th update interval
Proof of Lemma 4.8.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"We first present the proof for the k = 2 case and then generalize it for an arbitrary k.
(A) k",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"= 2: We have conditioned on Γj,1.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"This implies that SE(P̂j,1,Pj) ≤",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"q0/4.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Proof of Item 1 : We consider the interval t ∈,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"[t̂j + α, t̂j + 2α).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"For this interval, P̂(t−1) = P̂j,1 and thus Ψ = I",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"− P̂j,1P̂j,1′ (from Algorithm).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"For the sparse recovery step, we need to bound the 2s-RIC of Ψ. Consider any set T such that |T | ≤ 2s.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"We have∥∥∥IT ′P̂j,1∥∥∥ ≤ ∥∥∥IT ′(I",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"− PjPj ′)P̂j,1∥∥∥+ ∥∥∥IT ′PjPj ′P̂j,1∥∥∥
≤ SE(Pj , P̂j,1) + ∥∥IT ′Pj∥∥ = SE(P̂j,1,Pj) + ∥∥IT ′Pj∥∥
The equality holds since SE is symmetric for subspaces of the same dimension.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Using SE(P̂j,1,Pj)",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
≤,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"q0/4, (2),
max |T |≤2s ∥∥∥IT ′P̂j,1∥∥∥ ≤ q0/4 + max |T |≤2s ∥∥IT ′Pj∥∥ ≤ q0/4 + 0.1.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Finally, from using the assumptions of Theorem 2.2, q0 ≤ 0.96.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Using this and Lemma 4.11,
δ2s(Ψj) = max |T |≤2s ∥∥∥IT ′P̂j,1∥∥∥2 ≤ 0.352 < 0.15.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"From, this ∥∥∥(ΨTt ′ΨTt)−1∥∥∥ ≤ 11− δs(Ψ) ≤ 11− δ2s(Ψ) ≤ 11− 0.15 < 1.2 = φ+.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Consider ‖bt‖.
‖bt‖ = ‖Ψ`t‖ = ∥∥∥(I − P̂j,1P̂j,1′)Pjat∥∥∥ ≤ SE(P̂j,1,Pj)",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"‖at‖ ≤ (q0/4)√ηrλ+
(a) ≤ 0.3(ε+ SE(Pj−1,Pj))",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"√ ηrλ+ := 0.3bb
We have 0.3bb < bb < xmin/15",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
as in the proof of Lemma 4.7.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
The rest of the proof is the same too.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Notice here that, we could have loosened the required lower bound on xmin for this interval.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Proof of Item 2 : Again, updating P̂(t) using ˆ̀t’s is a problem of PCA in sparse data-dependent noise (SDDN), et.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
We use the result of Theorem 4.14.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Recall from Item 1 of this lemma that, for
t ∈",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"[t̂j + α, t̂j + 2α), et satisfies (1).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"We compute P̂j,2 as the top r eigenvectors of 1α ∑t̂j+2α−1 t=t̂j+α ˆ̀ t ˆ̀ t ′.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"In notation of Theorem 4.14, yt ≡ ˆ̀t, wt ≡ et, `t ≡",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"`t, P ≡",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Pj , P̂ ≡ Pj,2, and Ms,t = − (ΨTt ′ΨTt) −1",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"ΨTt ′. So ‖Ms,tPj‖ = ‖ (ΨTt ′ΨTt) −1",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"ΨTt
′Pj‖ ≤ (φ+/4)q0 := q1.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Applying Theorem 4.14 with q ≡ q1, b ≡ b0 (b0 bounds max-outlier-frac-rowα), and setting εSE = q1/4, observe that we require√
b0q1f ≤ 0.9(q1/4)
1 + (q1/4)
which holds if √ b0f ≤ 0.18.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
This is ensured since b0 = 0.01/f2.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, from Theorem 4.14, with probability at least 1− 10n−10, SE(P̂j,2,Pj) ≤ (q1/4) = 0.25 · 0.3q0.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, with this probability, conditioned on Γj,1, Γj,2 holds.
",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
(B) General,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"k: We have conditioned on Γj,k−1.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"This implies that SE(P̂j,k−1,Pj)",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
≤ qk−1/4.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Proof of Item 1 : Consider the interval,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"[t̂j + (k − 1)α, t̂j + kα).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"In this interval, P̂(t−1) = P̂j,k−1 and thus Ψ = I − P̂j,k−1P̂j,k−1′. Using the same idea as for the k = 2 case, we have that for the k-th interval, qk−1 = (φ +/4)k−1q0.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Pick εSE = (qk−1/4).,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"From this it is easy to see that
δ2s(Ψ) ≤",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"(
max |T |≤2s ∥∥∥IT",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"′P̂j,k−1∥∥∥)2 ≤",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"(SE(P̂j,k−1,Pj) +",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"max |T |≤2s ∥∥IT ′Pj∥∥)2 (a) ≤ (SE(P̂j,k−1,Pj) + 0.1)2 ≤ ((φ+/4)k−1(ε+ SE(Pj−1,Pj) + 0.1)2 < 0.15
where (a) follows from (2).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Using the approach Lemma 4.7, ‖Ψ`t‖ ≤ SE(P̂j,k−1,Pj)",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"‖at‖ ≤ (φ+/4)k−1(ε+ SE(Pj−1,Pj))",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"√ ηrλ+
(a) ≤ (φ+/4)k−1(ε+ ∆)",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"√ ηrλ+ := (φ+/4)k−1bb
Proof of Item 2 : Again, updating P̂(t) from ˆ̀t’s is a problem of PCA in sparse data-dependent noise given in Theorem 4.14.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"From Item 1 of this lemma that, for t ∈",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"[t̂j + (k − 1)α, t̂j +",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"kα], et satisfies (1).",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"We update the subspace, P̂j,k as the top r eigenvectors of 1 α ∑t̂j+kα−1",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
t=t̂j+(k−1)α ˆ̀ t ˆ̀ t ′.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"In the setting above yt ≡ ˆ̀t, wt ≡ et, `t ≡ `t, and Ms,t = − (ΨTt ′ΨTt) −1",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"ΨTt ′, and so ‖Ms,tPj‖ = ‖ (ΨTt ′ΨTt) −1",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"ΨTt
′Pj‖ ≤",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
(φ+/4)k−1q0 := qk−1.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Applying Theorem 4.14 with q ≡ qk−1, b ≡ b0 (b0 bounds max-outlier-frac-rowα), and setting εSE = qk−1/4, we require √
b0qk−1f ≤ 0.9(qk−1/4)
1 + (qk−1/4)
which holds if √ b0f ≤ 0.12.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
This is true by our assumption.,4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, from Theorem 4.14, with probability at least 1− 10n−10, SE(P̂j,k,Pj) ≤ (φ+/4)k−1q1.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
"Thus, with this probability, conditioned on Γj,k−1, Γj,k holds.",4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval,[0],[0]
Proof of Lemma 4.12.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
The proof uses the following lemma.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"It is proved in Appendix B. The proof uses Cauchy-Schwartz for sums of matrices, followed by either matrix Bernstein [28] or Vershynin’s subGaussian result",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"[29].
Lemma 4.16 (Concentration Bounds).",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Assume that the assumptions of Theorem 2.2 hold.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"For this lemma assume that `t = Pat, Φ := I − P̂ P̂ ′, et = M2,tM1,t`t, with ‖ 1α ∑ t∈J αM2,tM2,t ′‖",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"≤ b0 and ‖M1,tP ‖ ≤ q. Assume that event E0 holds.",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Then,
Pr (∥∥∥∥∥ 1α∑ t atat ′ −Λ ∥∥∥∥∥ ≤ 0λ− )",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"≥ 1− 10n−10
Pr (∥∥∥∥∥ 1α∑ t Φ`tet",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
′Φ ∥∥∥∥∥ ≤,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"(1 + 1)SE(P̂ ,P )√b0qλ+ ) ≥ 1− 10n−10
Pr (∥∥∥∥∥ 1α∑ t Φetet ′Φ ∥∥∥∥∥ ≤ (1 + 2)√b0q2λ+ ) ≥ 1− 10n−10
Pr (∥∥∥∥∥ 1α∑ t Φ`tνt ′Φ ∥∥∥∥∥ ≤",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"l,vλ− ) ≥ 1− 2n−10
Pr (∥∥∥∥∥ 1α∑ t Φνtνt ′Φ ∥∥∥∥∥ ≤",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"(ε2 + v,v)λ− )",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"≥ 1− 2n−10
",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Proof of Item (a): First from Corollary 4.10, note that for t ∈",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"[tj , t̂j ], the error et satisfies (1).",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"We
have
λmax
( 1
α ∑ t∈J α Φ[Pjatat ′Pj ′",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
+,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
etet ′,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"+ `tet ′ + et`t ′]Φ ) (a) ≥ λmax ( 1
α ∑ t∈J α ΦPjatat ′Pj",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"′Φ
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"+ λmin ( 1
α ∑ t∈J α Φ[etet ′",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"+ `tet ′ + et`t ′]Φ
)
",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
≥,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"λmax
( 1
α ∑ t∈J α ΦPjatat ′Pj",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"′Φ
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
− ∥∥∥∥∥ 1α ∑ t∈J α Φetet ′Φ ∥∥∥∥∥− 2 ∥∥∥∥∥ 1α ∑ t∈J α Φ`tet ′Φ ∥∥∥∥∥ := λmax(T ),4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"−
∥∥∥∥∥ 1α ∑ t∈J α Φetet ′Φ ∥∥∥∥∥− 2 ∥∥∥∥∥ 1α ∑ t∈J α Φ`tet",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
′Φ ∥∥∥∥∥ (3) where (a) follows from Weyl’s Inequality.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Now we bound the second and third terms by invoking Lemma 4.16 with E0 := {SE(P̂j−1,Pj−1) ≤ ε}, P̂ ≡ P̂j−1, P ≡",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Pj , M2,t ≡ ITt and M1,t ≡ (ΨTt ′ΨTt) −1",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"ΨTt ′, where Ψ = I − P̂j−1P̂j−1′. Thus, b0 ≡ b0, q ≡ q0.",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Thus, with probability at least 1− 4n−10,∥∥∥∥∥ 1α ∑ t∈J α Φetet ′Φ ∥∥∥∥∥+ 2 ∥∥∥∥∥ 1α ∑ t∈J α Φ`tet ′Φ ∥∥∥∥∥ ≤√b0q20λ+(1",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"+ 2) + 2√b0q0(SE(Pj−1,Pj) + ε)λ+(1 + 1) (4)
",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"The above equation uses the fact that SE(P̂j−1,Pj) ≤ ε + SE(Pj−1,Pj) which is a direct consequence of using Lemma 4.13.",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
We bound the first term of (3) as follows.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Let ΦPj QR = EjRj be its reduced QR decomposition.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Thus Ej is an n×r matrix with orthonormal columns and Rj is an r×r upper triangular matrix.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Let
A := Rj
( 1
α ∑ t∈J α atat ′
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Rj ′.
Observe that T can also be written as T =",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"[ Ej Ej,⊥ ]",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
[A 0 0 0 ],4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"[ Ej ′ Ej,⊥ ′ ] (5)
and thus λmax(A) = λmax(T ).",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
We work with λmax(A) in the sequel.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"We will use the following simple claim.
",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Claim 4.17.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"If X 0 (i.e., X is a p.s.d matrix), where X ∈ Rr×r, then RXR′ 0 for all R ∈",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Rr×r.
",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Proof.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Since X is p.s.d., y′Xy ≥ 0 for any vector y. Use this with y = R′z for any z ∈",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Rr.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
We get z′RXR′z ≥ 0.,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Since this holds for all z, RXR′ 0.
",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Using Lemma 4.16, it follows that
Pr
( 1
α ∑ t atat ′",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"− (λ− − 0)I 0
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"≥ 1− 2n−10.
",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Using Claim 4.17, with probability 1− 2n−10,
Rj
( 1
α ∑ t atat ′",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"− (λ− − 0)I
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Rj ′ 0
=⇒ λmin ( Rj ( 1
α ∑ t atat ′",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"− (λ− − 0)I
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Rj ′ ),4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"≥ 0
Using Weyl’s inequality [30], with the same probability,
λmin ( Rj ( 1
α ∑ t atat ′",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"− (λ− − 0)I
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Rj ′ ) ≤,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"λmax ( Rj ( 1
α ∑ t atat ′
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Rj ′ ),4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"− (λ− − 0)λmax ( RjRj
′) and so,
λmax(A) ≥ (λ− − 0)λmax(RjRj ′).
",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"We now obtain a lower bound on the second term in the rhs above.
λmax(RjRj ′)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
= λmax(Pj ′(I,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
− P̂j−1P̂j−1′)Pj)),4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
= λmax(Pj ′(I,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
− Pj−1Pj−1′ + Pj−1Pj−1′,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
− P̂j−1P̂j−1′)Pj)),4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
≥ λmax(Pj ′(I,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
− Pj−1Pj−1′)Pj),4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
+,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
λmin(Pj ′(Pj−1Pj−1′ − P̂j−1P̂j−1′)Pj) = σ2max((I,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
− Pj−1Pj−1′)Pj) +,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"λmin(Pj ′(Pj−1Pj−1′ − P̂j−1P̂j−1′)Pj)
≥ SE2(Pj−1,Pj)− ∥∥∥Pj",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"′(Pj−1Pj−1′ − P̂j−1P̂j−1′)Pj∥∥∥
(a) ≥ SE2(Pj−1,Pj)− ∥∥∥Pj−1Pj−1′",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"− P̂j−1P̂j−1′∥∥∥ ≥ SE2(Pj−1,Pj)−",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"2ε (6)
where we have used [12, Lemma 2.10].",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Thus, combining (3), (4), (6), and using 0 = 0.01, 1 = 2 = 0.01, with probability at least 1− 10n−10,
λmax
( 1
α ∑ t∈J α Φ ˆ̀t ˆ̀t",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"′Φ
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
≥,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"0.99λ−(SE2(Pj−1,Pj)− 2ε)− λ+[ √ b0q0(1.01q0 + 2.02(ε+ SE(Pj−1,Pj)))",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"]
(a) ≥ λ− [ 0.91SE2(Pj−1,Pj)− 2.7 √ b0f(ε+ SE(Pj−1,Pj) 2 ]
(b) ≥",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"λ− [ 0.91SE2(Pj−1,Pj)− 0.54(ε2 + SE2(Pj−1,Pj) ]",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"≥ λ−SE2(Pj−1,Pj)(0.91− 0.54 · 1.16) ≥ 0.28λ−SE2(Pj−1,Pj)
where (a) uses q0 = 1.2(ε + SE(Pj−1,Pj)) and ε ≤ 0.03SE2(Pj−1,Pj)/f2 < 0.4SE2(Pj−1,Pj), (b) uses√ b0f = 0.1 and (a+ b)
2 ≤ 2(a2 + b2).",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"and the last inequality again uses ε ≤ 0.03SE2(Pj−1,Pj)/f2.",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Proof of Item (b): First, we recall that from Corollary 4.10, for t ∈",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"[t̂j + Kα, tj+1), the error et satisfies (7).
",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"λmax
( 1
α ∑ t∈J α Φ ˆ̀t ˆ̀t",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"′Φ
) ≤ λmax ( 1
α ∑ t∈J α Φ`t`t",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"′Φ
) +",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"λmax ( 1
α ∑ t∈J α Φ[`tet ′ + et`t ′",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
+,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"etet ′]Φ
)
≤",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"λmax
( 1
α ∑ t∈J α",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
Φ`t`t,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"′Φ
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
+ ∥∥∥∥∥ 1α ∑ t∈J α Φetet,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
′Φ ∥∥∥∥∥+ 2 ∥∥∥∥∥ 1α ∑ t∈J α Φ`tet,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
′Φ ∥∥∥∥∥ :,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
= λmax(T ),4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"+
∥∥∥∥∥",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
1α ∑ t∈J α Φetet ′Φ ∥∥∥∥∥+ 2 ∥∥∥∥∥ 1α ∑ t∈J α Φ`tet,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"′Φ ∥∥∥∥∥ To obtain bounds on the second and third terms in the equation above we invoked Lemma 4.16 with E0 := {SE(P̂j ,Pj) ≤ ε}, P̂ ≡ P̂j , P ≡",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Pj , M2,t ≡ ITt , M1,t ≡ (ΨTt ′ΨTt) −1",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"ΨTt
′, where Ψ = I",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"− P̂jP̂j ′ and b0 ≡ b0, ≡ qK , .",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Thus, with probability at least 1− 10n−10,∥∥∥∥∥ 1α ∑
t∈J α Φetet
′Φ ∥∥∥∥∥+ 2 ∥∥∥∥∥ 1α ∑
t∈J α Φ`tet
′Φ ∥∥∥∥∥ ≤√b0qKλ+(qK(1 + 2) + 2(1 + 1)ε)
The above equation also uses SE(P̂j ,Pj) ≤ ε.",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Proceeding as before to bound λmax(T ), define ΦPj QR = EjRj , define A as before, we know λmax(T ) =",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
λmax(Ej ′TEj) = λmax(A).,4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Further,
λmax(A) = λmax ( Rj ( 1
α ∑ t∈J α atat ′
)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Rj ′ ) (a) ≤ λmax ( 1
α ∑ t∈J α atat
) λmax(RjRj ′)
where (a) uses Ostrowski’s theorem [30, Theorem 5.4.9].",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"We have
λmax(RjRj ′)",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"= σ2max(Rj) = σ 2 max((I − P̂jP̂j ′)Pj) ≤ ε2
and we can bound λmax( 1 α ∑ t∈J α atat ′) using the first item of Lemma 4.16 with 0 = 0.01.",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Combining all of the above, and setting 1 = 2 = 0.01, when the subspace has not changed, with probability at least 1− 10n−10,
λmax
( 1
α ∑ t∈J α Φ ˆ̀t ˆ̀t",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"′Φ
) ≤",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
λ+[1.01ε2 + √ b0qK(1.01qK + 2.01ε)],4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"(a) ≤ 1.37ε2λ+
where (a) uses qK ≤ ε and b0f2 = 0.01.",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
"Under the condition of Theorem 2.2, recall that ωevals = 2ε2λ+ and thus, with high probability, 1.37ε2λ+ < ωevals < 0.28λ −SE2(Pj−1,Pj).",4.5 Proof of Lemma 4.12: subspace change detection lemma,[0],[0]
A useful corollary of our result for RST is that NORST is also the first online algorithm that provides a provable finite sample guarantee for the static Robust PCA problem.,5.1 Static Robust PCA,[0],[0]
"Static RPCA is our problem setting with J = 1, or in other words, with `t = Pat.",5.1 Static Robust PCA,[0],[0]
"A recent work, [21], developed an online stochastic optimization based reformulation of PCP, called ORPCA, to solve this.",5.1 Static Robust PCA,[0],[0]
"Their paper provides only a partial guarantee because the guarantee assumes that the intermediate algorithm estimates, P̂(t), are fullrank.",5.1 Static Robust PCA,[0],[0]
Moreover the guarantee is only asymptotic.,5.1 Static Robust PCA,[0],[0]
"Instead our result given below is a complete guarantee and is non-asymptotic.
",5.1 Static Robust PCA,[0],[0]
Corollary 5.18 (Static RPCA).,5.1 Static Robust PCA,[0],[0]
Consider Algorithm 1 with t2 = ∞. Theorem 2.2 holds with following modification: replace the slow subspace change assumption with a fixed subspace P .,5.1 Static Robust PCA,[0],[0]
"Everything else remains as is, but with r ≡ rL.",5.1 Static Robust PCA,[0],[0]
"Under the assumptions of Theorem 2.2, all the conclusions hold with same probability.",5.1 Static Robust PCA,[0],[0]
"The time and memory complexity are O(ndr log(1/ )) and O(nr log n log(1/ )).
",5.1 Static Robust PCA,[0],[0]
"In applications such as “robust” dimensionality reduction [31, 32] where the objective is to just obtain the top-r directions along which the variability of data is maximized, we only need the first Kα = Cf2r log n log(1/ε) samples to obtain an ε-accurate subspace estimate.",5.1 Static Robust PCA,[0],[0]
"If only these are used, the time complexity reduces to O(nKαr log(1/ε))",5.1 Static Robust PCA,[0],[0]
= O(nr2 log n log2(1/ )).,5.1 Static Robust PCA,[0],[0]
This is faster than even NO-RMC [11] and does not require d ≈ n; of course it requires the other extra assumptions discussed earlier.,5.1 Static Robust PCA,[0],[0]
Another useful corollary of our result is a guarantee for the ST-missing problem.,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
Consider the subspace tracking with missing data (ST-missing) problem.,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"By setting the missing entries at time t to zero, and by defining Tt to be the set of missing entries at time t, we observe n-dimensional vectors that satisfy
yt := `t − ITtITt ′`t, for t = 1, 2, . . .",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
", d. (7)
Algorithm 3 NORST-Random for subspace tracking with missing data (ST-missing) Algorithm 2 with the following changes
1.",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
Replace line 3 with: compute P̂0 ←,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
P̂init ←,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"Generate an n× r basis matrix from the random orthogonal model; j ← 1, k ← 1
2.",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"Replace line 6 with the following
• Ψ← I",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
− P̂(t−1)P̂(t−1)′; ỹt ← Ψyt; x̂t,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
← ITt(ΨTt ′ΨTt)−1ΨTt ′ỹt; ˆ̀t,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
← yt,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"− x̂t.
with xt ≡ ITtITt ′`t.",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
This can be interpreted as a special case of the RST problem where the set Tt is known.,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"Because there are no sparse corruptions (outliers), there is no xmin.",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
Thus the initialization error need not be O(1/ √ r) (needed in the RST result to ensure a reasonable lower bound on xmin) and so one can even use random initialization.,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
We assume that the initialization is obtained using the Random Orthogonal Model described in [33].,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"As explained in [33], a basis matrix generated from this model is already µ-incoherent.",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
We have the following corollary.,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"The only change in its proof is the proof of the first subspace update interval for the j = 0 case.
",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
Corollary 5.19 (ST-missing).,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
Consider NORST-Random (Algorithm 3).,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"If the assumptions of Theorem 2.2 on at and νt hold, Pj’s are µ-incoherent, tj+1",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"− tj > (K + 2)α, ∆ < 0.8, the outlier fraction bounds given in Theorem 2.2 hold, and if, for t ∈",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"[t0, t1], max-outlier-frac-col ≤ c/(log n), then all conclusions of Theorem 2.2 on P̂(t) and on `t hold with the same probability.
",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"To our knowledge, the above is the first complete non-asymptotic guarantee for ST-missing; and the first result that allows changing subspaces.",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"All existing guarantees are either partial guarantees (make assumptions on intermediate algorithm estimates), e.g., [19], or provide only asymptotic results",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"[16, 18].",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
"Moreover, from a dynamic matrix completion viewpoint, it is also giving a matrix completion solution without assuming that the set of observed entries is generated from a uniform or a Bernoulli model.",5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
Of course the tradeoff is that it needs many more observed entries.,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
All these points will be discussed in detail in follow-up work where we will also numerically evaluate NORST-random for this problem.,5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion,[0],[0]
It is possible to relax the lower bound on outlier magnitudes if not all of the subspace directions change at a given subspace change time.,5.3 Fewer than r directions change,[0],[0]
Suppose that only rch < r directions change.,5.3 Fewer than r directions change,[0],[0]
"When rch = 1, we recover the guarantee of [15] but for NORST (which is a simpler algorithm than s-reprocs).
",5.3 Fewer than r directions change,[0],[0]
"Let Pj−1,fix denote a basis for the fixed component of Pj−1 and let Pj−1,ch denote a basis for its
changing component.",5.3 Fewer than r directions change,[0],[0]
"Thus, Pj−1R =",5.3 Fewer than r directions change,[0],[0]
"[Pj−1,fix,Pj−1,ch], where R is a r × r rotation matrix.",5.3 Fewer than r directions change,[0],[0]
"We have
Pj = [Pj−1,fix,Pj,chd] (8)
where Pj,chd is the changed component and has the same dimension as Pj−1,ch.",5.3 Fewer than r directions change,[0],[0]
"Thus,
SE(Pj−1,Pj) = SE(Pj−1,ch,Pj,chd) (9)
and so ∆ = maxj SE(Pj−1,Pj) = maxj SE(Pj−1,ch,Pj,chd).",5.3 Fewer than r directions change,[0],[0]
"Let λ + ch denote the largest eigenvalue along any direction in Pj,chd.
",5.3 Fewer than r directions change,[0],[0]
Corollary 5.20.,5.3 Fewer than r directions change,[0],[0]
"In Algorithm 2, replace line 17 by P̂(t) ← basis(P̂j−1, P̂j,k).",5.3 Fewer than r directions change,[0],[0]
"For basis matrices P1,P2, we use P = basis(P1,P2) to mean that P is a basis matrix with column span equal to the column span of
[P1,P2].",5.3 Fewer than r directions change,[0],[0]
Assume that (8) and (9) hold.,5.3 Fewer than r directions change,[0],[0]
Also assume that the conditions of Theorem 2.2 holds with the lower bound on xmin relaxed to xmin ≥ C(ε √ η(r − rch)λ+ + (ε+ ∆) √ ηrchλ + ch).,5.3 Fewer than r directions change,[0],[0]
"Then, all conclusions of Theorem 2.2 hold.",5.3 Fewer than r directions change,[0],[0]
In this section we present the results for extensive numerical experiments on synthetic and real data to validate our theoretical claims.,6 Empirical Evaluation,[0],[0]
All time comparisons are performed on a Desktop Computer with Intel R© Xeon E3-1240 8-core CPU @ 3.50GHz and 32GB RAM and all synthetic data experiments are averaged over 100 independent trials.,6 Empirical Evaluation,[0],[0]
The codes are available at https://github.com/praneethmurthy/NORST.,6 Empirical Evaluation,[0],[0]
"We perform three experiments on synthetic data to corroborate our theoretical claims.
",6.1 Synthetic Data,[0],[0]
Experiment 1.,6.1 Synthetic Data,[0],[0]
"We compare the results of NORST and Offline-NORST with static RPCA algorithms, and Robust Subspace Tracking/Online RPCA methods proposed in literature.",6.1 Synthetic Data,[0],[0]
"For our first experiment, we generate the changing subspaces using Pj = e γjBjPj−1 as done in [34] where γj controls the subspace change and Bj ’s are skew-symmetric matrices.",6.1 Synthetic Data,[0],[0]
In the first experiment we used the following parameters. n,6.1 Synthetic Data,[0],[0]
"= 1000, d = 12000, J = 2, t1 = 3000, t2 = 8000, r = 30, γ1 = 0.001, γ2 = γ1 and the matrices B1 and B2 are generated as B1 = (B̃1 − B̃1) and B2 = (B̃2 − B̃2) where the entries of B̃1, B̃2 are generated independently from a standard normal distribution.",6.1 Synthetic Data,[0],[0]
We set α = 300.,6.1 Synthetic Data,[0],[0]
This gives us the basis matrices P(t) for all t. To obtain the low-rank matrix L from this we generate the coefficients at ∈,6.1 Synthetic Data,[0],[0]
"Rr as independent zero-mean, bounded random variables.",6.1 Synthetic Data,[0],[0]
"They are (at)i i.i.d∼ unif [−qi, qi] where qi = √ f",6.1 Synthetic Data,[0],[0]
"− √ f(i − 1)/2r for i = 1, 2, · · · , r − 1 and qr = 1.",6.1 Synthetic Data,[0],[0]
thus the condition number is f and we selected f = 50.,6.1 Synthetic Data,[0],[0]
"For the sparse supports, we considered two models according to which the supports are generated.",6.1 Synthetic Data,[0],[0]
First we use Model G.24,6.1 Synthetic Data,[0],[0]
[15] which simulates a moving object pacing in the video.,6.1 Synthetic Data,[0],[0]
"For the first ttrain = 100 frames, we used a smaller fraction of outliers with parameters s/n = 0.01, b0 = 0.01.",6.1 Synthetic Data,[0],[0]
For t > ttrain we used s/n = 0.05 and b0 = 0.3.,6.1 Synthetic Data,[0],[0]
"Secondly, we used the Bernoulli model to simulate sampling uniformly at random, i.e., each entry of the matrix, is independently selected with probability ρ or not selected with probability 1−ρ.",6.1 Synthetic Data,[0],[0]
We generate the sparse supports using the Bernoulli model using ρ = 0.01 for the first ttrain frames and ρ = 0.3 for the subsequent frames.,6.1 Synthetic Data,[0],[0]
The sparse outlier magnitudes for both support models are generated uniformly at random from the interval,6.1 Synthetic Data,[0],[0]
"[xmin, xmax] with xmin = 10 and xmax = 20.
We initialized the s-ReProCS and NORST algorithms using AltProj applied to Y[1,ttrain] with ttrain = 100.",6.1 Synthetic Data,[0],[0]
"For the parameters to AltProj we used used the true value of r, 15 iterations and a threshold of 0.01.",6.1 Synthetic Data,[0],[0]
"This, and the choice of γ1 and γ2 ensure that SE(P̂init,P0)",6.1 Synthetic Data,[0],[0]
"≈ SE(P1,P0)",6.1 Synthetic Data,[0],[0]
"≈ SE(P2,P1)",6.1 Synthetic Data,[0],[0]
≈ 0.01.,6.1 Synthetic Data,[0],[0]
"The other algorithm parameters are set as mentioned in the theorem, i.e., K = dlog(c/ε)e = 8, α = Cr log n = 300, ω = xmin/2 = 5 and ξ = 7xmin/15 = 0.67, ωevals = 2ε
2λ+ = 7.5 × 10−4.",6.1 Synthetic Data,[0],[0]
For l1 minimization we used the YALL-1 toolbox,6.1 Synthetic Data,[0],[0]
[35] and set the tolerance to 10−4.,6.1 Synthetic Data,[0],[0]
For the least-squares step we use the Conjugate Gradient Least Squares instead of the well-known “backslash” operator in MATLAB since this is a well conditioned problem.,6.1 Synthetic Data,[0],[0]
For this we set the tolerance as 10−10 and the number of iterations as 10.,6.1 Synthetic Data,[0],[0]
We have not done any code optimization such as use of MEX files for various sub-routines to speed up our algorithm.,6.1 Synthetic Data,[0],[0]
For the other online methods we implement the algorithms without modifications.,6.1 Synthetic Data,[0],[0]
The regularization parameter for ORPCA was set as with λ1 = 1/ √ n and λ2 = 1/ √ d according to [21].,6.1 Synthetic Data,[0],[0]
Wherever possible we set the tolerance as 10−6 and 100 iterations to match that of our algorithm.,6.1 Synthetic Data,[0],[0]
"As shown in Fig. 2, NORST is significantly better than all the RST methods - s-ReProCS [15], and two popular heuristics from literature - ORPCA",6.1 Synthetic Data,[0],[0]
[21] and GRASTA,6.1 Synthetic Data,[0],[0]
"[34].
We also provide a comparison of offline techniques in Table 2.",6.1 Synthetic Data,[0],[0]
"We must mention here that we implemented the static RPCA methods once on the entire data matrix, Y .",6.1 Synthetic Data,[0],[0]
We do this to provide a roughly equal comparison of the time taken.,6.1 Synthetic Data,[0],[0]
"In principle, we could also implement the static techniques on disjoint batches of size α, but we observed that this did not yield significant improvement in terms of reconstruction accuracy, while being considerably slower, and thus we report only the latter setting.",6.1 Synthetic Data,[0],[0]
"As can be seen, offline NORST outperforms all static RPCA methods, both for the moving object outlier support model and for the commonly used random Bernoulli support model.",6.1 Synthetic Data,[0],[0]
"For the batch comparison we used PCP, AltProj and RPCA-GD.",6.1 Synthetic Data,[0],[0]
We set the regularization parameter for PCP 1/ √ n in accordance with [3].,6.1 Synthetic Data,[0],[0]
"The other known parameters, r for Alt-Proj, outlier-fraction for RPCA-GD, are set using the true values.",6.1 Synthetic Data,[0],[0]
"Furthermore, for all algorithms (the IALM solver in case of PCP)",6.1 Synthetic Data,[0],[0]
we set the threshold as 10−6 and the number of iterations to 100 as opposed to 10−3 and 50 iterations which were set as default to provide a fair comparison with NORST and Offline-NORST.,6.1 Synthetic Data,[0],[0]
"All results are averaged over 100 independent runs.
",6.1 Synthetic Data,[0],[0]
Experiment 2.,6.1 Synthetic Data,[0],[0]
Next we perform an experiment to validate our claim of NORST admitting a higher fraction of outliers per row than the state of the art.,6.1 Synthetic Data,[0],[0]
"In particular, since AltProj has the highest tolerance to max-outlier-frac-row, we only compare with it.",6.1 Synthetic Data,[0],[0]
The experiment proceeded as follows.,6.1 Synthetic Data,[0],[0]
We chose 10 different values of each of r and b0 (we slightly misuse notation here to let b0 := max-outlier-frac-row for this section only).,6.1 Synthetic Data,[0],[0]
"For each pair of b0 and r we implemented NORST and ALtProj over 100 independent trials and computed the relative error in recovering L, i.e., we computed ‖L̂ − L‖F /‖L‖F for each run.",6.1 Synthetic Data,[0],[0]
"We computed the empirical probability of success, i.e., we enumerated the number of times out of 100 the error seen by each algorithm was less than a threshold, 0.5.
",6.1 Synthetic Data,[0],[0]
"For each pair of {b0, r} we used the Bernoulli model for sparse support generation, the low rank matrix is generated exactly as done in the previous experiments with the exception that again to provide an equal footing, we increased the “subspace change” by setting γ1 and γ2 to 10 times the value that was used in the previous experiment.",6.1 Synthetic Data,[0],[0]
For the first ttrain frames we used b0 = 0.02.,6.1 Synthetic Data,[0],[0]
"We provide the phase transition
plots for both algorithm in Fig. 2.",6.1 Synthetic Data,[0],[0]
"Here, white represents success while black represents failure.",6.1 Synthetic Data,[0],[0]
"As can be seen, NORST is able to tolerate a much larger fraction of outlier-per-row as compared to AltProj.
Experiment 3.",6.1 Synthetic Data,[0],[0]
Finally we perform an experiment to analyze the effect of the lower bound on the outlier magnitude xmin with the performance of NORST and AltProj.,6.1 Synthetic Data,[0],[0]
We show the results in Fig. 4.,6.1 Synthetic Data,[0],[0]
"In the first stage, we generate the data exactly as done in the Moving Object sparse support model of the first experiment.",6.1 Synthetic Data,[0],[0]
"The only change to the data generation parameters is that we now choose three different values of xmin = {0.5, 5, 10}.",6.1 Synthetic Data,[0],[0]
"Furthermore, we set all the non-zero entries of the sparse matrix to be equal to xmin.",6.1 Synthetic Data,[0],[0]
This is actually harder than allowing the sparse outliers to take on any value since for a moderately low value of xmin the outlier-lower magnitude bound of Theorem 2.2 is violated.,6.1 Synthetic Data,[0],[0]
This is indeed confirmed by the numerical results presented in Fig. 4.,6.1 Synthetic Data,[0],[0]
(i),6.1 Synthetic Data,[0],[0]
"When xmin = 0.5, NORST works well since now all the outliers get classified as the small unstructured noise νt.",6.1 Synthetic Data,[0],[0]
"(ii) When xmin = 10, NORST still works well because now xmin is large enough so that the outlier support is mostly correctly recovered.",6.1 Synthetic Data,[0],[0]
"(iii) But when xmin = 5 the NORST reconstruction error stagnates around 10 −3.
",6.1 Synthetic Data,[0],[0]
All AltProj errors are much worse than those of NORST because the outlier fraction per row is the same as in the first experiment.,6.1 Synthetic Data,[0],[0]
What can be noticed though is that the variation with varying xmin is not that significant.,6.1 Synthetic Data,[0],[0]
In this section we provide empirical results on real video for the task of Background Subtraction.,6.2 Real Data,[0],[0]
For the AltProj algorithm we set r = 40.,6.2 Real Data,[0],[0]
The remaining parameters were used with default setting.,6.2 Real Data,[0],[0]
"For NORST, we set α = 60, K = 3, ξt = ‖Ψ ˆ̀t−1‖2.",6.2 Real Data,[0],[0]
"We found that these parameters work for most videos that we
verified our algorithm on.",6.2 Real Data,[0],[0]
"For RPCA-GD we set the “corruption fraction” α = 0.2 as described in their paper.
",6.2 Real Data,[0],[0]
Meeting Room (MR) dataset:,6.2 Real Data,[0],[0]
The meeting room sequence is set of 1964 images of resolution 64× 80.,6.2 Real Data,[0],[0]
The first 1755 frames consists of outlier-free data.,6.2 Real Data,[0],[0]
"Henceforth, we consider only the last 1209 frames.",6.2 Real Data,[0],[0]
"For NORST, we used ttrain = 400.",6.2 Real Data,[0],[0]
"In the first 400 frames, a person wearing a black shirt walks in, writes something on the board and goes back.",6.2 Real Data,[0],[0]
"In the subsequent frames, the person walks in with a white shirt.",6.2 Real Data,[0],[0]
This is a challenging video sequence because the color of the person and the color of the curtain are hard to distinguish.,6.2 Real Data,[0],[0]
NORST is able to perform the separation at around 43 frames per second.,6.2 Real Data,[0],[0]
"We present the results in Fig. 5
Lobby (LB) dataset:",6.2 Real Data,[0],[0]
This dataset contains 1555 images of resolution 128× 160.,6.2 Real Data,[0],[0]
The first 341 frames are outlier free.,6.2 Real Data,[0],[0]
Here we use the first 400 “noisy” frames as training data.,6.2 Real Data,[0],[0]
"The Alt Proj algorithm is used to obtain an initial estimate with rank, r = 40.",6.2 Real Data,[0],[0]
The parameters used in all algorithms are exactly the same as above.,6.2 Real Data,[0],[0]
NORST achieves a “test” processing rate of 16 frames-per-second.,6.2 Real Data,[0],[0]
We present the results in Fig. 6,6.2 Real Data,[0],[0]
The authors would like to thank Praneeth Netrapalli and Prateek Jain of Microsoft Research India for fruitful discussions on strengthening the guarantee by removing assumptions on subspace change model.,Acknowledgments,[0],[0]
We divide the proof into 3 parts for better clarity.,A Proof of Theorem 2.2,[0],[0]
"We first prove the νt = 0 case for NORST (Algorithm 2), then prove the correctness of Offline NORST, and finally explain the changes needed when νt 6= 0.
A.1 Proof with νt = 0
Remark A.1 (Deriving the long expression forK given in the discussion).",A Proof of Theorem 2.2,[0],[0]
We have used max-outlier-frac-rowα ≤ b0 with b0 = 0.01/f 2 throughout the analysis in order to simplify the proof.,A Proof of Theorem 2.2,[0],[0]
"If we were not to do this, and if we used [25], it is possible to show that the “decay rate” qk is of the form qk = (c2 √ b0f) kq0 from which it follows that to obtain an ε-accurate approximation of the subspace it suffices to have
K =
⌈ log (c1∆/ε)
",A Proof of Theorem 2.2,[0],[0]
"− log(c2 √ b0f)
⌉ .
",A Proof of Theorem 2.2,[0],[0]
"We first prove Theorem 2.2 for the case when tj ’s are known, i.e., correctness of Algorithm 1.
",A Proof of Theorem 2.2,[0],[0]
Proof of Theorem 2.2 with assuming tj known.,A Proof of Theorem 2.2,[0],[0]
In this case t̂j = tj .,A Proof of Theorem 2.2,[0],[0]
The proof is an easy consequence of Lemmas 4.7 and 4.8.,A Proof of Theorem 2.2,[0],[0]
"Recall that Γj,K ⊆ Γj,K−1 ⊆ · · ·Γj,0 and ΓJ,K ⊆ ΓJ−1,K ⊆ · · · ⊆ Γ1,K .",A Proof of Theorem 2.2,[0],[0]
"To show that the conclusions of the Theorem hold, it suffices to show that Pr(ΓJ,K |Γ0,0) ≥ 1− 10dn−10.",A Proof of Theorem 2.2,[0],[0]
"Using the chain rule of probability,
Pr(ΓJ,K |Γ1,0) = Pr(ΓJ,K ,ΓJ−1,K , · · · ,Γ1,K |Γ1,0)
= J∏ j=1 Pr(Γj,K |Γj,0) = J∏ j=1 Pr(Γj,K ,Γj,K−1, · · · ,Γj,1|Γj,0) = J∏ j=1 K∏ k=1 Pr(Γj,k|Γj,k−1) (a) ≥ (1− 10n−10)JK ≥ 1− 10JKn−10.
where (a) used Pr(Γj,1|Γj,0) ≥ 1−10n−10 from Lemma 4.7 and Pr(Γj,k|Γj,k−1) ≥ 1−10n−10 from Lemma 4.8.
",A Proof of Theorem 2.2,[0],[0]
Proof of Theorem 2.2.,A Proof of Theorem 2.2,[0],[0]
"Define
t̂j−1,fin := t̂j−1 +Kα, tj,∗ = t̂j−1,fin +
⌈ tj − t̂j−1,fin
α
⌉ α
Thus, t̂j−1,fin is the time at which the (j − 1)-th subspace update is complete; w.h.p., this occurs before tj .",A Proof of Theorem 2.2,[0],[0]
"With this assumption, tj,∗ is such that tj lies in the interval",A Proof of Theorem 2.2,[0],[0]
"[tj,∗ − α+ 1, tj,∗].",A Proof of Theorem 2.2,[0],[0]
Recall from the algorithm that we increment j to j + 1 at t = t̂j,A Proof of Theorem 2.2,[0],[0]
"+Kα := t̂j,fin.",A Proof of Theorem 2.2,[0],[0]
"Define the events
1. Det0",A Proof of Theorem 2.2,[0],[0]
":= {t̂j = tj,∗} = {λmax( 1α ∑tj,∗ t=tj,∗−α+1 Φ ˆ̀ t ˆ̀′ tΦ) > ωevals} and
Det1 := {t̂j = tj,∗ + α} = {λmax( 1α ∑tj,∗+α t=tj,∗+1",A Proof of Theorem 2.2,[0],[0]
"Φ ˆ̀t ˆ̀ ′ tΦ) > ωevals},
2.",A Proof of Theorem 2.2,[0],[0]
SubUpd :,A Proof of Theorem 2.2,[0],[0]
"= ∩Kk=1SubUpdk where SubUpdk := {SE(P̂j,k,Pj) ≤ qk},
3.",A Proof of Theorem 2.2,[0],[0]
NoFalseDets := {for all,A Proof of Theorem 2.2,[0],[0]
J α ⊆,A Proof of Theorem 2.2,[0],[0]
"[t̂j,fin, tj+1), λmax( 1α ∑ t∈J α Φ ˆ̀ t ˆ̀′ tΦ) ≤ ωevals}
4.",A Proof of Theorem 2.2,[0],[0]
"Γ0,end",A Proof of Theorem 2.2,[0],[0]
":= {SE(P̂0,P0) ≤ 0.25}, 5.",A Proof of Theorem 2.2,[0],[0]
"Γj,end := Γj−1,end ∩ ( (Det0 ∩ SubUpd ∩NoFalseDets) ∪ (Det0 ∩Det1 ∩ SubUpd ∩NoFalseDets) ) .
",A Proof of Theorem 2.2,[0],[0]
"Let p0 denote the probability that, conditioned on Γj−1,end, the change got detected at t = tj,∗, i.e.,
let
p0 := Pr(Det0|Γj−1,end).
",A Proof of Theorem 2.2,[0],[0]
"Thus, Pr(Det0|Γj−1,end) = 1 − p0.",A Proof of Theorem 2.2,[0],[0]
It is not easy to bound p0.,A Proof of Theorem 2.2,[0],[0]
"However, as we will see, this will not be needed.",A Proof of Theorem 2.2,[0],[0]
"Assume that Γj−1,end∩Det0 holds.",A Proof of Theorem 2.2,[0],[0]
"Consider the interval J α := [tj,∗, tj,∗+α).",A Proof of Theorem 2.2,[0],[0]
"This interval starts at or after tj , so, for all t in this interval, the subspace has changed.",A Proof of Theorem 2.2,[0],[0]
"For this interval, Φ = I− P̂j−1P̂j−1′. Applying the first item of Lemma 4.12, w.p. at least 1− 10n−10,
λmax
( 1
α ∑ t∈J α Φ ˆ̀t ˆ̀ ′ tΦ
) ≥ ωevals
and thus t̂j = tj,∗ + α.",A Proof of Theorem 2.2,[0],[0]
"In other words,
Pr(Det1|Γj−1,end ∩Det0) ≥ 1− 10n−10.
",A Proof of Theorem 2.2,[0],[0]
"Conditioned on Γj−1,end ∩ Det0 ∩ Det1, the first SVD step is done at t = t̂j",A Proof of Theorem 2.2,[0],[0]
"+ α = tj,∗ + 2α and the subsequent steps are done every α samples.",A Proof of Theorem 2.2,[0],[0]
"We can prove Lemma 4.7 with Γj,0 replaced by Γj,end∩Det0∩ Det1 and Lemma 4.8 with Γj,k−1 replaced by Γj,end ∩Det0∩Det1∩SubUpd1 ∩ · · · ∩SubUpdk−1 and with the k-th SVD interval being Jk :=",A Proof of Theorem 2.2,[0],[0]
"[t̂j + (k − 1)α, t̂j + kα).",A Proof of Theorem 2.2,[0],[0]
"Applying Lemmas 4.7, and 4.8 for each k, we get
Pr(SubUpd|Γj−1,end ∩Det0 ∩Det1) ≥ (1− 10n−10)K+1.
",A Proof of Theorem 2.2,[0],[0]
"We can also do a similar thing for the case when the change is detected at tj,∗, i.e. when Det0 holds.",A Proof of Theorem 2.2,[0],[0]
"In this case, we replace Γj,0 by Γj,end ∩ Det0 and Γj,k by Γj,end ∩ Det0 ∩ SubUpd1 ∩ · · · ∩ SubUpdk−1 and conclude that
Pr(SubUpd|Γj−1,end ∩Det0)",A Proof of Theorem 2.2,[0],[0]
"≥ (1− 10n−10)K .
",A Proof of Theorem 2.2,[0],[0]
Finally consider the NoFalseDets event.,A Proof of Theorem 2.2,[0],[0]
"First, assume that Γj−1,end ∩Det0∩SubUpd holds.",A Proof of Theorem 2.2,[0],[0]
Consider any interval J α ⊆,A Proof of Theorem 2.2,[0],[0]
"[t̂j,fin, tj+1).",A Proof of Theorem 2.2,[0],[0]
"In this interval, P̂(t) = P̂j , Φ = I − P̂jP̂j ′",A Proof of Theorem 2.2,[0],[0]
"and SE(P̂j ,Pj) ≤ ε.",A Proof of Theorem 2.2,[0],[0]
"Using the second part of Lemma 4.12 we conclude that w.p. at least 1− 10n−10,
λmax
( 1
α ∑ t∈J α Φ ˆ̀t ˆ̀ ′ tΦ
)",A Proof of Theorem 2.2,[0],[0]
"< ωevals
Since Det0 holds, t̂j = tj,∗.",A Proof of Theorem 2.2,[0],[0]
"Thus, we have a total of b tj+1−tj,∗−Kα−αα c intervals J α that are subsets of [t̂j,fin, tj+1).",A Proof of Theorem 2.2,[0],[0]
"Moreover, b tj+1−tj,∗−Kα−α α c ≤ b",A Proof of Theorem 2.2,[0],[0]
tj+1−tj−Kα−α α c ≤ b tj+1−tj α c,A Proof of Theorem 2.2,[0],[0]
− (K + 1) since α ≤ α.,A Proof of Theorem 2.2,[0],[0]
"Thus,
Pr(NoFalseDets|Γj−1,end ∩Det0 ∩ SubUpd) ≥ (1− 10n−10)b tj+1−tj α c−(K)
",A Proof of Theorem 2.2,[0],[0]
"On the other hand, if we condition on Γj−1,end ∩Det0 ∩Det1 ∩ SubUpd, then t̂j = tj,∗ + α.",A Proof of Theorem 2.2,[0],[0]
"Thus,
Pr(NoFalseDets|Γj−1,end ∩Det0 ∩Det1 ∩ SubUpd) ≥",A Proof of Theorem 2.2,[0],[0]
"(1− 10n−10)b tj+1−tj α c−(K+1)
",A Proof of Theorem 2.2,[0],[0]
"We can now combine the above facts to bound Pr(Γj,end|Γj−1,end).",A Proof of Theorem 2.2,[0],[0]
"Recall that p0 := Pr(Det0|Γj−1,end).",A Proof of Theorem 2.2,[0],[0]
"Clearly, the events (Det0∩SubUpd∩NoFalseDets) and (Det0∩Det1∩SubUpd∩NoFalseDets) are disjoint.
",A Proof of Theorem 2.2,[0],[0]
"Thus,
Pr(Γj,end|Γj−1,end) = p0 Pr(SubUpd ∩NoFalseDets|Γj−1,end ∩Det0)",A Proof of Theorem 2.2,[0],[0]
"+ (1− p0) Pr(Det1|Γj−1,end ∩Det0)",A Proof of Theorem 2.2,[0],[0]
"Pr(SubUpd ∩NoFalseDets|Γj−1,end ∩Det0 ∩Det1) ≥",A Proof of Theorem 2.2,[0],[0]
p0(1− 10n−10)K(1− 10n−10)b tj+1−tj α c−(K) + (1− p0)(1− 10n−10)(1− 10n−10)K(1− 10n−10)b tj+1−tj α c−(K+1) =,A Proof of Theorem 2.2,[0],[0]
(1− 10n−10)b tj+1−tj α,A Proof of Theorem 2.2,[0],[0]
c,A Proof of Theorem 2.2,[0],[0]
≥,A Proof of Theorem 2.2,[0],[0]
"(1− 10n−10)tj+1−tj .
",A Proof of Theorem 2.2,[0],[0]
"Since the events Γj,end are nested, the above implies that
Pr(ΓJ,end|Γ0,end) =",A Proof of Theorem 2.2,[0],[0]
∏,A Proof of Theorem 2.2,[0],[0]
"j Pr(Γj,end|Γj−1,end) ≥ ∏ j (1− 10n−10)tj+1−tj = (1− 10n−10)d
≥ 1− 10dn−10.
",A Proof of Theorem 2.2,[0],[0]
"A.2 Proof of Offline NORST
We now provide the proof of the Offline Algorithm (lines 26-30 of Algorithm 2).
",A Proof of Theorem 2.2,[0],[0]
Proof of Offline NORST.,A Proof of Theorem 2.2,[0],[0]
The proof of this follows from the conclusions of the online counterpart.,A Proof of Theorem 2.2,[0],[0]
Note that the subspace estimate in this case is not necessarily r dimensional.,A Proof of Theorem 2.2,[0],[0]
"This is essentially done to ensure that in the time intervals when the subspace has changed, but has not yet been updated, the output of the algorithm is still an ε-approximate solution to the true subspace.",A Proof of Theorem 2.2,[0],[0]
"In other words, for t ∈ [t̂j−1 +Kα, tj ], the true subspace is Pj−1 and so in this interval
SE(P̂",A Proof of Theorem 2.2,[0],[0]
"offline(t) ,Pj−1) = SE([P̂j−1, (I − P̂j−1P̂j−1 ′)P̂j ],Pj−1) (a) = ∥∥∥[I",A Proof of Theorem 2.2,[0],[0]
− (I − P̂j−1P̂j−1′)P̂jP̂j ′(I,A Proof of Theorem 2.2,[0],[0]
− P̂j−1P̂j−1′)][I,A Proof of Theorem 2.2,[0],[0]
"− P̂j−1P̂j−1′]Pj−1∥∥∥
≤ ∥∥∥[I",A Proof of Theorem 2.2,[0],[0]
− (I − P̂j−1P̂j−1′)P̂jP̂j ′(I,A Proof of Theorem 2.2,[0],[0]
"− P̂j−1P̂j−1′)]∥∥∥SE(P̂j−1,Pj−1) ≤ ε
where (a) follows because for orthogonal matrices P1 and P2,
I − P1P1′ − P2P2′ = (I − P1P1′)(I − P2P2′) = (I − P2P2′)(I − P1P1′)
Now consider the interval t ∈",A Proof of Theorem 2.2,[0],[0]
"[tj , t̂j +",A Proof of Theorem 2.2,[0],[0]
Kα].,A Proof of Theorem 2.2,[0],[0]
"In this interval, the true subspace is Pj and we have back propagated the ε-approximate subspace P̂j in this interval.",A Proof of Theorem 2.2,[0],[0]
"We first note that span([P̂j−1, (I − P̂j−1P̂j−1
′)P̂j ]) = span([P̂j , (I − P̂jP̂j ′)P̂j−1]).",A Proof of Theorem 2.2,[0],[0]
"And so we use the latter to quantify the error in this interval as
SE(P̂ offline(t) ,Pj) = SE([P̂j , (I − P̂jP̂j ′)P̂j−1],Pj) = ∥∥∥[I",A Proof of Theorem 2.2,[0],[0]
− (I − P̂jP̂j ′)P̂j−1P̂j−1′(I,A Proof of Theorem 2.2,[0],[0]
− P̂jP̂j ′)][I,A Proof of Theorem 2.2,[0],[0]
"− P̂jP̂j ′]Pj∥∥∥
≤ ∥∥∥[I",A Proof of Theorem 2.2,[0],[0]
− (I − P̂jP̂j ′)P̂j−1P̂j−1′(I,A Proof of Theorem 2.2,[0],[0]
"− P̂jP̂j ′)]∥∥∥SE(P̂j ,Pj) ≤ ε
A.3 Proof with νt 6= 0
In this section we analyze the “stable” version of RST, i.e., we let νt 6= 0.
",A Proof of Theorem 2.2,[0],[0]
Proof.,A Proof of Theorem 2.2,[0],[0]
The proof is very similar to that of the noiseless case but there are two differences due to the additional noise term.,A Proof of Theorem 2.2,[0],[0]
The first is the effect of the noise on the sparse recovery step.,A Proof of Theorem 2.2,[0],[0]
The approach to address this is straightforward.,A Proof of Theorem 2.2,[0],[0]
"We note that the error now seen in the sparse recovery step is bounded by ‖Ψ(`t + νt)‖ and using the bound on ‖νt‖, we observe that the error only changes by a constant factor.",A Proof of Theorem 2.2,[0],[0]
"In particular, we can show that ‖et‖ ≤ 2.4(2ε + ∆) √ ηrλ+.",A Proof of Theorem 2.2,[0],[0]
The other crucial difference is in updating subspace estimate.,A Proof of Theorem 2.2,[0],[0]
"To deal with the additional uncorrelated noise, we use the following result.
",A Proof of Theorem 2.2,[0],[0]
Remark 4.18 of [25] states the following for the case where the data contains unstructured noise νt that satisfies the assumptions of Theorem 2.2.,A Proof of Theorem 2.2,[0],[0]
"Thus, in the notation of [25], λ+v ≤ cε2λ+ and rv = r.",A Proof of Theorem 2.2,[0],[0]
"The following result also assumes r, n large enough so that (r + log n) ≤",A Proof of Theorem 2.2,[0],[0]
"r log n.
Corollary A.2 (Noisy PCA-SDDN).",A Proof of Theorem 2.2,[0],[0]
Given data vectors yt := `t + wt,A Proof of Theorem 2.2,[0],[0]
"+ zt = `t + ITtMs,t`t + zt, t = 1, 2, . .",A Proof of Theorem 2.2,[0],[0]
.,A Proof of Theorem 2.2,[0],[0]
", α, where Tt is the support set of wt, and `t satisfying the model detailed above.",A Proof of Theorem 2.2,[0],[0]
"Furthermore, maxt ‖Ms,tP ‖2 ≤ q",A Proof of Theorem 2.2,[0],[0]
< 1.,A Proof of Theorem 2.2,[0],[0]
"zt is small uncorrelated noise such that E[ztzt′] = Σz, maxt ‖zt‖2 := b2z < ∞.",A Proof of Theorem 2.2,[0],[0]
Define λ+z := λmax(Σz) and rz as the “effective rank” of zt such that b 2 z = rzλ + z .,A Proof of Theorem 2.2,[0],[0]
"Then for any α ≥ α0, where
α0 := C
ε2SE max
{ ηq2f2r log n,
b2z λ− f log n } the fraction of nonzeroes in any row of the noise matrix [w1,w2, . . .",A Proof of Theorem 2.2,[0],[0]
",wα] is bounded by b, and
3 √ bqf + λ+z /λ
− ≤ 0.9εSE 1 + εSE
For an α ≥ α0, let P̂ be the matrix of top r eigenvectors of D := 1α ∑ t yty ′",A Proof of Theorem 2.2,[0],[0]
"t. With probability at least 1− 10n−10, SE(P̂ ,P ) ≤ εSE.
",A Proof of Theorem 2.2,[0],[0]
We illustrate how applying Corollary A.2 changes the subspace update step.,A Proof of Theorem 2.2,[0],[0]
"Consider the first subspace estimate, i.e., we are trying to get an estimate P̂j,1 in the j-th subspace change time interval.",A Proof of Theorem 2.2,[0],[0]
Define (e`)t = ITt ′,A Proof of Theorem 2.2,[0],[0]
(ΨTt ′ΨTt) −1,A Proof of Theorem 2.2,[0],[0]
ΨTt ′`t and (eν)t = ITt ′,A Proof of Theorem 2.2,[0],[0]
(ΨTt ′ΨTt) −1 ΨTt ′νt.,A Proof of Theorem 2.2,[0],[0]
"We estimate the new subspace, P̂j,1
as the top r eigenvectors of 1α ∑t̂j+α−1 t=t̂j",A Proof of Theorem 2.2,[0],[0]
ˆ̀ t ˆ̀ t ′. In the setting above,A Proof of Theorem 2.2,[0],[0]
", yt ≡ ˆ̀t, wt ≡ (e`)t, zt ≡ (eν)t, `t ≡",A Proof of Theorem 2.2,[0],[0]
"`t and Ms,t = − (ΨTt ′ΨTt) −1 ΨTt ′",A Proof of Theorem 2.2,[0],[0]
"and so ‖Ms,tP ‖ = ‖ (ΨTt ′ΨTt) −1",A Proof of Theorem 2.2,[0],[0]
"ΨTt
′Pj‖ ≤ φ+(ε+ SE(Pj−1,Pj))",A Proof of Theorem 2.2,[0],[0]
:= q0.,A Proof of Theorem 2.2,[0],[0]
"Applying Corollary A.2 with q ≡ q0, and recalling that the support, Tt satisfies the assumptions similar to that of the noiseless case and hence b0 ≡ max-outlier-frac-rowα.",A Proof of Theorem 2.2,[0],[0]
"Now, setting εSE,1 = q0/4, observe that we require
(i) √ b0q0f ≤
0.5 · 0.9εSE,1 1 + εSE,1 , and, (ii) λ+z",A Proof of Theorem 2.2,[0],[0]
"λ− ≤ 0.5 · 0.9εSE,1 1 + εSE,1 .
which holds if (i) √ b0f ≤ 0.12, and (ii) is satisfied as follows from using the assumptions on νt as follows.",A Proof of Theorem 2.2,[0],[0]
It is immediate to see that λ+z /λ,A Proof of Theorem 2.2,[0],[0]
"− ≤ ε2 ≤ .2εSE,1.",A Proof of Theorem 2.2,[0],[0]
"Furthermore, the sample complexity term remains unchanged due to the choice of νt.",A Proof of Theorem 2.2,[0],[0]
"To see this, notice that the only extra term in the α0 expression is b2zf log n/(ε 2 SEλ −) which simplifies to ε2f2r log n/ε2SE which is what was required even in the noiseless case.",A Proof of Theorem 2.2,[0],[0]
"Thus, from Corollary A.2, with probability at least 1 − 10n−10, SE(P̂j,1,Pj) ≤ εSE,1 = q0/4.",A Proof of Theorem 2.2,[0],[0]
"The argument in other subspace update stages will require the same changes and follows without any further differences.
",A Proof of Theorem 2.2,[0],[0]
The final difference is in the subspace detection step.,A Proof of Theorem 2.2,[0],[0]
"Notice that here too, in general, there will be some extra assumption required to provably detect the subspace change.",A Proof of Theorem 2.2,[0],[0]
"However, due to the bounds assumed on ‖νt‖ and the bounds on using l,v = v,v = 0.01ε, we see that (i) the extra sample complexity term is the same as that required in the noiseless case.",A Proof of Theorem 2.2,[0],[0]
Proof of Lemma 4.13.,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"The proof follows from triangle inequality as SE(Q1,Q3) = ∥∥(I",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"−Q1Q1′)Q3∥∥ = ∥∥(I −Q1Q1′)(I −Q2Q2′ +Q2Q2′)Q3∥∥
≤ ∥∥(I −Q1Q1′)(I −Q2Q2′)Q3∥∥+ ∥∥(I −Q1Q1′)Q2Q2′Q3∥∥
≤ ∥∥(I −Q1Q1′)∥∥SE(Q2,Q3) + SE(Q1,Q2)∥∥Q2′Q3∥∥ ≤ ∆1 + ∆2
We need the following results for proving Lemma 4.16.
",B Proof of Lemmas 4.13 and 4.16,[0],[0]
Theorem B.1 (Cauchy-Schwartz for sums of matrices [12]).,B Proof of Lemmas 4.13 and 4.16,[0],[0]
For matrices X and Y we have∥∥∥∥∥ 1α∑ t XtYt ′ ∥∥∥∥∥ 2 ≤ ∥∥∥∥∥,B Proof of Lemmas 4.13 and 4.16,[0],[0]
1α∑ t XtXt ′ ∥∥∥∥∥ ∥∥∥∥∥,B Proof of Lemmas 4.13 and 4.16,[0],[0]
1α∑ t YtYt ′ ∥∥∥∥∥,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"(10) The following theorem is adapted from [28].
",B Proof of Lemmas 4.13 and 4.16,[0],[0]
Theorem B.2 (Matrix Bernstein [28]).,B Proof of Lemmas 4.13 and 4.16,[0],[0]
Given an α-length sequence of n1× n2 dimensional random matrices and a r.v.,B Proof of Lemmas 4.13 and 4.16,[0],[0]
X. Assume the following holds.,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"For all X ∈ C, (i) conditioned on X, the matrices Zt are mutually independent, (ii) P(‖Zt‖ ≤ R|X) = 1, and (iii) max {∥∥ 1 α ∑ t E[Zt′Zt|X] ∥∥ , ∥∥ 1α∑t E[ZtZt′|X]∥∥} ≤ σ2.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Then, for an > 0 and for all X ∈ C,
P (∥∥∥∥∥ 1α∑ t",B Proof of Lemmas 4.13 and 4.16,[0],[0]
Zt ∥∥∥∥∥ ≤ ∥∥∥∥∥,B Proof of Lemmas 4.13 and 4.16,[0],[0]
1α∑ t E[Zt|X] ∥∥∥∥∥+ ∣∣∣∣X ),B Proof of Lemmas 4.13 and 4.16,[0],[0]
≥ 1− (n1 + n2) exp ( −α 2 2 (σ2 +R ) ) .,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"(11)
The following theorem is adapted from [29].
",B Proof of Lemmas 4.13 and 4.16,[0],[0]
Theorem B.3 (Sub-Gaussian Rows [29]).,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Given an N -length sequence of sub-Gaussian random vectors wi in Rnw , an r.v X, and a set C. Assume the following holds.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"For all X ∈ C, (i) wi are conditionally independent given X; (ii) the sub-Gaussian norm of wi is bounded by K for all i.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
Let,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"W := [w1,w2, . . .",B Proof of Lemmas 4.13 and 4.16,[0],[0]
",wN ] ′. Then for an ∈ (0, 1) and for all X ∈ C
P (∥∥∥∥ 1NW ′W − 1N E[W ′W |X] ∥∥∥∥ ≤ ∣∣∣∣X)",B Proof of Lemmas 4.13 and 4.16,[0],[0]
≥ 1− 2 exp(nw log 9− c 2N4K4 ) .,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"(12)
Proof of Lemma 4.16.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"The proof approach is similar to that of [15, Lemma 7.17] but the details are different since we use a simpler subspace model.
",B Proof of Lemmas 4.13 and 4.16,[0],[0]
Item 1 : Recall that the (at)i are bounded,B Proof of Lemmas 4.13 and 4.16,[0],[0]
r.v.’s satisfying |(at)i| ≤ √ ηλi.,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Thus, the vectors, at are sub-Gaussian with ‖at‖ψ2",B Proof of Lemmas 4.13 and 4.16,[0],[0]
= maxi ‖(at)i‖ψ2 = √ ηλ+.,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"We now apply Theorem B.3 with K ≡ √ ηλ+,
= 0λ",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"−, N ≡ α and nw ≡ r to conclude the following: For an α ≥ α(0) := C(r log 9 + 10 log n)f2,
Pr (∥∥∥∥∥ 1α∑ t atat ′ −Λ ∥∥∥∥∥ ≤ 0λ− )",B Proof of Lemmas 4.13 and 4.16,[0],[0]
≥ 1−,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"10n−10
The Lemma statement assumes α = Cf2r log n. For large r, n, this α > α(0) =",B Proof of Lemmas 4.13 and 4.16,[0],[0]
Cf 2(r+ log n).,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Thus, the above holds under the Lemma statement.
",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Item 2 : For the second term, we proceed as follows.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Since ‖Φ‖ = 1,∥∥∥∥∥ 1α∑ t Φ`tet",B Proof of Lemmas 4.13 and 4.16,[0],[0]
′Φ ∥∥∥∥∥ ≤ ∥∥∥∥∥,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"1α∑ t Φ`tet ′ ∥∥∥∥∥ .
",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"To bound the RHS above, we will apply Theorem B.2 with Zt = Φ`tet ′. Conditioned on {P̂∗, Z}, the Zt’s are mutually independent.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
We first bound obtain a bound on the expected value of the time average of the Zt’s and then compute R and σ 2.,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"By Cauchy-Schwartz,∥∥∥∥∥E [ 1 α ∑ t Φ`tet ′",B Proof of Lemmas 4.13 and 4.16,[0],[0]
],B Proof of Lemmas 4.13 and 4.16,[0],[0]
∥∥∥∥∥ 2 = ∥∥∥∥∥,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"1α∑ t ΦPΛP ′M1,t ′M2,t ′",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"∥∥∥∥∥ 2
(a) ≤ ∥∥∥∥∥",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"1α∑ t ( ΦPΛP ′M1,t ′)",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"(M1,tPΛP ′Φ) ∥∥∥∥∥ ∥∥∥∥∥",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"1α∑ t M2,tM2,t ′ ∥∥∥∥∥ (b)
≤ b0",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"[ max t ∥∥ΦPΛP ′M1,t′∥∥2] ≤ b0SE2(P̂ ,P )q2(λ+)2",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"(13)
where (a) follows by Cauchy-Schwartz (Theorem B.1) withXt = ΦPΛP ′M1,t ′",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"and Yt = M2,t, (b) follows from the assumption on M2,t.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"To compute R
‖Zt‖ ≤ ‖Φ`t‖ ‖et‖ ≤ SE(P̂ ,P )qηrλ+",B Proof of Lemmas 4.13 and 4.16,[0],[0]
":= R
Next we compute σ2.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Since wt’s are bounded r.v.’s, we have∥∥∥∥∥ 1α∑ t E[ZtZt′] ∥∥∥∥∥ = ∥∥∥∥∥",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"1α∑ t E [ Φ`tet ′et`t ′Φ ]∥∥∥∥∥ = ∥∥∥∥ 1αE[‖et‖2 Φ`t`t′Φ] ∥∥∥∥
≤ (
max et ‖et‖2 )∥∥∥∥∥",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"1α∑ t E [ Φ`t`t ′Φ ]∥∥∥∥∥
≤ q2SE2(P̂ ,P )ηr(λ+)2· := σ2
it can also be seen that ∥∥ 1 α ∑ t E[Zt′Zt] ∥∥ evaluates to the same expression.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Thus, applying Theorem B.2 Pr
(∥∥∥∥∥ 1α∑ t Φ`tet ′ ∥∥∥∥∥ ≤ SE(P̂ ,P )√b0qλ+ + )
",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"≥ 1− 2n exp  −α 4 max { σ2 2 , R }  .
",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Let = 1λ −, then, σ2/ 2 = cηf2r and R/ = cηfr.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"Hence, for the probability to be of the form 1−2n−10 we require that α ≥ α(1)",B Proof of Lemmas 4.13 and 4.16,[0],[0]
:= C · ηf2(r log n).,B Proof of Lemmas 4.13 and 4.16,[0],[0]
Item 3,B Proof of Lemmas 4.13 and 4.16,[0],[0]
: We use Theorem B.2 with Zt := Φetet ′Φ. The proof is analogous to the previous item.,B Proof of Lemmas 4.13 and 4.16,[0],[0]
First we bound the norm of the expectation of the time average of Zt:∥∥∥∥E,B Proof of Lemmas 4.13 and 4.16,[0],[0]
[ 1α∑Φetet′Φ ]∥∥∥∥ = ∥∥∥∥,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"1α∑ΦM2,tM1,tPΛP ′M1,t′M2,t′Φ
∥∥∥∥ ≤ ∥∥∥∥",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"1α∑M2,tM1,tPΛP ′M1,t′M2,t′
∥∥∥∥ (a) ≤
(∥∥∥∥∥ 1α∑ t M2,tM2,t ′ ∥∥∥∥∥",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"[maxt ∥∥M2,tM1,tPΛP ′M1,t(·)′∥∥2] )1/2
(b) ≤ √ b0 [ max t ∥∥M1,tPΛP ′M1,t′M2,t′∥∥] ≤√b0q2λ+
where (a) follows from Theorem B.1 with Xt = M2,t and Yt = M1,tPΛP ′M1,t ′M2,t ′",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"and (b) follows from the assumption on M2,t.",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"To obtain R,
‖Zt‖ = ∥∥Φetet′Φ∥∥ ≤ max
t ‖ΦMtPat‖2 ≤",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"q2rηλ+ := R
To obtain σ2, ∥∥∥∥∥",B Proof of Lemmas 4.13 and 4.16,[0],[0]
1α∑ t E [ Φet(Φet) ′(Φet)et ′Φ ]∥∥∥∥∥ = ∥∥∥∥∥,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"1α∑ t E [ Φetet ′Φ ‖Φet‖2 ]∥∥∥∥∥
≤ (
max et ‖Φet‖2 )∥∥ΦMtPΛP",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"′Mt′Φ∥∥ ≤ q2rηλ+ · q2λ+ := σ2
Applying Theorem",B Proof of Lemmas 4.13 and 4.16,[0],[0]
B.2,B Proof of Lemmas 4.13 and 4.16,[0],[0]
", we have
Pr (∥∥∥∥∥ 1α∑ t Φetet ′Φ ∥∥∥∥∥ ≤√b0q2λ+ + )",B Proof of Lemmas 4.13 and 4.16,[0],[0]
≥ 1− n exp ( −α 2 2(σ2 +R ) ),B Proof of Lemmas 4.13 and 4.16,[0],[0]
Letting = 2λ,B Proof of Lemmas 4.13 and 4.16,[0],[0]
− we get R/ = cηrf and σ2/ 2 = cηrf2.,B Proof of Lemmas 4.13 and 4.16,[0],[0]
"For the success probability to be of the form 1− 2n−10 we require α ≥ α(2) := Cη · 11f2(r log n).
",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"The proof of the last two items follow from using [25, Lemma 7.19].",B Proof of Lemmas 4.13 and 4.16,[0],[0]
"In this section we present the proof of the extensions stated in Sec. 5.
",C Proof of Extensions,[0],[0]
"C.1 Static Robust PCA
The proof follows directly from Theorem 2.2 by setting J = 1.
",C Proof of Extensions,[0],[0]
"C.2 Subspace Tracking with missing data and dynamic Matrix Completion
Here we present the proof of the subspace tracking with missing data problem.",C Proof of Extensions,[0],[0]
"The only changes needed for this proof are in the initialization step, i.e., for j = 0.",C Proof of Extensions,[0],[0]
"For this we use the following lemma.
",C Proof of Extensions,[0],[0]
"Lemma C.1 (Lemma 2.1, [33]).",C Proof of Extensions,[0],[0]
"Set r̄ = max(r, log n).",C Proof of Extensions,[0],[0]
"Then there exist constants C and c such that
the random orthogonal model with left singular vectors P̂init obeys Pr ( maxi ∥∥∥Ii′P̂init∥∥∥2 ≤ Cr̄/n)",C Proof of Extensions,[0],[0]
"≥ 1 − cn−β log n.
Thus,
Pr ( max i ∥∥∥Ii′P̂init∥∥∥2 ≤ µr̄/n)",C Proof of Extensions,[0],[0]
"≥ 1− n−10 Consider the two scenarios (i) if r ≥ log n, then everything discussed before remains true, whereas, if (ii) r ≤ log n, we redefine µ2 = C log n/r and thus in the interval [t0, t1] we require max-outlier-frac-col ≤ 0.01logn .",C Proof of Extensions,[0],[0]
"Further, using the bound on max-outlier-frac-col it follows from triangle inequality that
max T",C Proof of Extensions,[0],[0]
≤2s ∥∥∥IT,C Proof of Extensions,[0],[0]
′P̂init∥∥∥2 ≤ 2smax,C Proof of Extensions,[0],[0]
i ∥∥∥Ii′P̂init∥∥∥2 ≤ 2sµr n <,C Proof of Extensions,[0],[0]
"0.01
We only mention the changes needed for Lemma 4.7 for when j = 0 since the initialization is different.",C Proof of Extensions,[0],[0]
The rest of the argument of recursively applying the lemmas hold exactly as before.,C Proof of Extensions,[0],[0]
"First, ttrain = 1 since we use random initialization.",C Proof of Extensions,[0],[0]
"Thus from the Algorithm, t̂0 = ttrain = 1.
",C Proof of Extensions,[0],[0]
Proof.,C Proof of Extensions,[0],[0]
Proof of item 1.,C Proof of Extensions,[0],[0]
"Since the support of xt is known, the LS step gives x̂t = ITt ( ΨTt ′ΨTt )−1",C Proof of Extensions,[0],[0]
ΨTt ′(Ψ`t + Ψxt) = ITt ( ΨTt ′ΨTt )−1,C Proof of Extensions,[0],[0]
ITt,C Proof of Extensions,[0],[0]
"′Ψ`t + xt
Thus et = x̂t",C Proof of Extensions,[0],[0]
"− xt satisfies
et = ITt ( ΨTt ′ΨTt )−1 ITt ′Ψ`t
Now, from the incoherence assumption on P̂init, Lemma 4.11, the bound on max-outlier-frac-col, and recalling that in this interval, Ψ = I",C Proof of Extensions,[0],[0]
"− P̂initP̂init′ we have
max |T |≤2s ∥∥∥IT",C Proof of Extensions,[0],[0]
"′P̂init∥∥∥2 ≤ 2sµr n ≤ 0.09 =⇒ δ2s(Ψ) ≤ 0.32 < 0.15,∥∥∥(ΨTt",C Proof of Extensions,[0],[0]
"′ΨTt)−1∥∥∥ ≤ 11− δs(Ψ) ≤ 11− δ2s(Ψ) ≤ 11− 0.15 < 1.2 = φ+.
",C Proof of Extensions,[0],[0]
"Secondly, ∥∥ITt ′ΨP0∥∥ ≤ (∥∥ITt ′P0∥∥+ ∥∥∥ITt ′P̂init∥∥∥) ≤ 0.3 + 0.3 = 0.6 (14)",C Proof of Extensions,[0],[0]
"Thus, ‖ITt ′Ψ`t‖ ≤ 0.6 √ ηrλ+.
",C Proof of Extensions,[0],[0]
"Proof of Item 2 : Since ˆ̀t = `t − et with et satisfying the above equation, updating P̂(t) from the ˆ̀t’s is a problem of PCA in sparse data-dependent noise (SDDN), et.",C Proof of Extensions,[0],[0]
"To analyze this, we use the PCA-SDDN result of Theorem 4.14 (this is taken from [25]).",C Proof of Extensions,[0],[0]
"Recall from above that, for t ∈",C Proof of Extensions,[0],[0]
"[t̂0, t̂0 + α], T̂t = Tt, and ˆ̀ t = `t − et.",C Proof of Extensions,[0],[0]
"Recall from the algorithm that we compute the first estimate of the j-th subspace, P̂j,1,
as the top r eigenvectors of 1α ∑t0+α−1 t=t0 ˆ̀ t ˆ̀ t ′.",C Proof of Extensions,[0],[0]
"In the notation of Theorem 4.14, yt ≡ ˆ̀t, wt ≡ et, `t ≡",C Proof of Extensions,[0],[0]
"`t and Ms,t = − (ΨTt ′ΨTt) −1 ΨTt ′",C Proof of Extensions,[0],[0]
"and so ‖Ms,tP ‖ = ‖ (ΨTt ′ΨTt) −1",C Proof of Extensions,[0],[0]
"ΨTt
′P0‖ ≤ φ+ · 0.6 = 0.72 := q0.",C Proof of Extensions,[0],[0]
This follows using (14).,C Proof of Extensions,[0],[0]
"Also, b0 ≡ max-outlier-frac-rowα.
",C Proof of Extensions,[0],[0]
"Applying Theorem 4.14 with q ≡ q0, b0 ≡ max-outlier-frac-rowα and setting εSE = q0/4, observe that we require √
b0q0f ≤ 0.9(q0/4)
1 + (q0/4) .
",C Proof of Extensions,[0],[0]
This holds if √ b0f ≤ 0.12 as provided by Theorem 2.2.,C Proof of Extensions,[0],[0]
"Thus, from Corollary 4.14, with probability at least 1− 10n−10, SE(P̂j,1,P0) ≤ q0/4.
",C Proof of Extensions,[0],[0]
"C.3 Fewer than r directions change
Proof of Corollary 5.20.",C Proof of Extensions,[0],[0]
The proof is very similar to that of Theorem 2.2.,C Proof of Extensions,[0],[0]
The only changes occur in the (a) Projected CS step.,C Proof of Extensions,[0],[0]
"With the subspace change model, we define `t = P(t)at := [ Pj−1,fix Pj,chd ]",C Proof of Extensions,[0],[0]
"[at,fix at,chd ] where at,fix is a (r − rch)× 1 dimensional vector and at,chd is a rch × 1 dimensional vector.",C Proof of Extensions,[0],[0]
"In the first α frames after the j-th subspace changes (or, the j-th subspace change is detected in the automatic case), recall that P̂(t) = P̂j−1.",C Proof of Extensions,[0],[0]
"Thus, SE(P̂(t),Pj−1,fix) = SE(P̂j−1,Pj−1,fix) ≤ SE(P̂j−1,Pj−1) ≤ ε and so, the error can be bounded as
‖Ψ`t‖ ≤",C Proof of Extensions,[0],[0]
"‖ΨPj−1,fixat,fix‖+ ‖ΨPj,chdat,chd‖ ≤ ε",C Proof of Extensions,[0],[0]
√ η(r − rch)λ+ +,C Proof of Extensions,[0],[0]
(,C Proof of Extensions,[0],[0]
"ε+ SE(Pj−1,Pj))",C Proof of Extensions,[0],[0]
"√ ηrchλ + ch
In the second α frames, have P̂(t) = basis(P̂j−1, P̂j,1).",C Proof of Extensions,[0],[0]
"Thus SE(P̂(t),Pj−1,fix) ≤ SE(P̂j−1,Pj−1,fix) ≤ SE(P̂j−1,Pj−1) ≤ ε and SE(P̂(t),Pj,chd) ≤",C Proof of Extensions,[0],[0]
"SE(P̂j,1,Pj,chd) ≤ SE(P̂j,1,Pj)",C Proof of Extensions,[0],[0]
"≤ 0.3 · (ε + SE(Pj−1,Pj))",C Proof of Extensions,[0],[0]
.,C Proof of Extensions,[0],[0]
"Thus, the error in the sparse recovery step in the interval after the first subspace update is performed is given as
‖Ψ`t‖ ≤ ε √ η(r − rch)λ+ + 0.3 · (ε+ SE(Pj−1,Pj))",C Proof of Extensions,[0],[0]
"√ ηrchλ + ch
The rest of the proof follows as before.",C Proof of Extensions,[0],[0]
"The error after the k-th subspace update is also bounded using the above idea.
",C Proof of Extensions,[0],[0]
(b) Subspace Detection step: The proof of the subspace detection step follows exactly analogous to Lemma 4.12.,C Proof of Extensions,[0],[0]
"One minor observation is noting that SE(Pj−1,PJ) = SE(Pj−1,ch,Pj,chd) in the proof of part (a) of Lemma 4.12.",C Proof of Extensions,[0],[0]
The rest of the argument is exactly the same.,C Proof of Extensions,[0],[0]
"In this work, we study the robust subspace tracking (RST) problem and obtain one of the first two provable guarantees for it.",abstractText,[0],[0]
"The goal of RST is to track sequentially arriving data vectors that lie in a slowly changing low-dimensional subspace, while being robust to corruption by additive sparse outliers.",abstractText,[0],[0]
"It can also be interpreted as a dynamic (time-varying) extension of robust PCA (RPCA), with the minor difference that RST also requires a short tracking delay.",abstractText,[0],[0]
We develop a recursive projected compressive sensing algorithm that we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its tracking delay is nearly optimal.,abstractText,[0],[0]
"We prove that NORST solves both the RST and the dynamic RPCA problems under weakened standard RPCA assumptions, two simple extra assumptions (slow subspace change and most outlier magnitudes lower bounded), and a few minor assumptions.",abstractText,[0],[0]
Our guarantee shows that NORST enjoys a near optimal tracking delay of O(r log n log(1/ )).,abstractText,[0],[0]
"Its required delay between subspace change times is the same, and its memory complexity is n times this value.",abstractText,[0],[0]
Thus both these are also nearly optimal.,abstractText,[0],[0]
"Here n is the ambient space dimension, r is the subspaces’ dimension, and is the tracking accuracy.",abstractText,[0],[0]
"NORST also has the best outlier tolerance compared with all previous RPCA or RST methods, both theoretically and empirically (including for real videos), without requiring any model on how the outlier support is generated.",abstractText,[0],[0]
This is possible because of the extra assumptions it uses.,abstractText,[0],[0]
Nearly Optimal Robust Subspace Tracking,title,[0],[0]
"Generative models for graphs have a longstanding history, with applications including data augmentation, anomaly detection and recommendation (Chakrabarti & Faloutsos, 2006).",1. Introduction,[0],[0]
"Explicit probabilistic models such as Barabási-Albert or stochastic blockmodels are the de-facto standard in this field (Goldenberg et al., 2010).",1. Introduction,[0],[0]
"However, it has also been shown on multiple occasions that our intuitions about structure and behavior of graphs may be misleading.",1. Introduction,[0],[0]
"For instance, heavy-tailed degree distributions in real graphs were in strong disagreement with the models existing at the time of their discovery (Barabási & Albert, 1999).",1. Introduction,[0],[0]
More recent works like Dong et al. (2017) and,1. Introduction,[0],[0]
Broido & Clauset (2018) keep bringing up other surprising characteristics of realworld networks that question the validity of the established models.,1. Introduction,[0],[0]
"This leads us to the question: “How do we define a model that captures all the essential (potentially still unknown) properties of real graphs?”
",1. Introduction,[0],[0]
"*Equal contribution 1Technical University of Munich, Germany.",1. Introduction,[0],[0]
"Correspondence to: Daniel Zügner <zuegnerd@in.tum.de>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
An increasingly popular way to address this issue in other fields is by switching from explicit (prescribed) models to implicit ones.,1. Introduction,[0],[0]
"This transition is especially notable in computer vision, where generative adversarial networks (GANs) (Goodfellow et al., 2014) significantly advanced the state of the art over the classic prescribed approaches like mixtures of Gaussians (Blanken et al., 2007).",1. Introduction,[0],[0]
"GANs achieve unparalleled results in scenarios such as image and 3D objects generation (e.g., Karras et al., 2017; Berthelot et al., 2017; Wu et al., 2016).",1. Introduction,[0],[0]
"However, despite their massive success when dealing with real-valued data, adapting GANs to handle discrete objects like graphs or text remains an open research problem (Goodfellow, 2016).",1. Introduction,[0],[0]
"In fact, discreteness is only one of the obstacles when applying GANs to network data.",1. Introduction,[0],[0]
Large repositories of graphs that all come from the same distribution are not available.,1. Introduction,[0],[0]
This means that in a typical setting one has to learn from a single graph.,1. Introduction,[0],[0]
"Additionally, any model operating on a graph necessarily has to be permutation invariant, as graphs are isomorphic under node reordering.
",1. Introduction,[0],[0]
In this work we introduce NetGAN – the first implicit generative model for graphs and networks that tackles all of the above challenges.,1. Introduction,[0],[0]
We formulate the problem of learning the graph topology as learning the distribution of biased random walks over the graph.,1. Introduction,[0],[0]
"Like in the typical GAN setting, the generator G – in our case defined as a stochastic neural network with discrete output samples – learns to generate random walks that are plausible in the real graph, while the discriminator D then has to distinguish them from the true ones that are sampled from the original graph.
",1. Introduction,[0],[0]
The main requirement for a graph generative model is the ability to generate realistic graphs.,1. Introduction,[0],[0]
In the experimental section we compare NetGAN to other established prescribed models on this task.,1. Introduction,[0],[0]
"We observe that our proposed method consistently reproduces most known patterns inherent to real-world networks without explicitly specifying any of them in the model definition (e.g., degree distribution, as seen in Fig. 1).",1. Introduction,[0],[0]
"However, a model that simply replicates the original graph would also trivially fulfill this requirement, which clearly isn’t our goal.",1. Introduction,[0],[0]
In order to prove that this is not the case we examine the generalization properties of NetGAN by evaluating its link prediction performance.,1. Introduction,[0],[0]
"As our experiments show, our model exhibits competitive performance in this task and even achieves state-of-the-art
results on some datasets.",1. Introduction,[0],[0]
"This result is especially impressive, since NetGAN is not trained explicitly for performing link prediction.",1. Introduction,[0],[0]
"To summarize, our main contributions are:
• We introduce NetGAN1 – the first of its kind GAN architecture that generates graphs via random walks.",1. Introduction,[0],[0]
"Our model tackles the associated challenges of staying permutation invariant, learning from a single graph and generating discrete output.
",1. Introduction,[0],[0]
"• We show that our method preserves important topological properties, without having to explicitly specifying them in the model definition.",1. Introduction,[0],[0]
"Moreover, we demonstrate how latent space interpolation leads to producing graphs with smoothly changing characteristics.
",1. Introduction,[0],[0]
"• We highlight the generalization properties of NetGAN by its link prediction performance that is competitive with the state of the art on real-word datasets, despite the model not being trained explicitly for this task.",1. Introduction,[0],[0]
"So far, no GAN architectures applicable to real-world networks have been proposed.",2. Related Work,[0],[0]
Liu et al. (2017) propose a GAN architecture for learning topological features of subgraphs.,2. Related Work,[0],[0]
Tavakoli et al. (2017) apply GANs to graph data by trying to directly generate adjacency matrices.,2. Related Work,[0],[0]
Because their model produces the entire adjacency matrix – including the zero entries – it requires computations and memory quadratic in the number of nodes.,2. Related Work,[0],[0]
"Such quadratic complexity is infeasible in practice, allowing to process only small graphs, with reported runtime of over 60 hours for a graph with only 154 nodes.",2. Related Work,[0],[0]
"In contrast, NetGAN operates on random walks – it considers only the non-zero entries of the adjacency matrix efficiently exploiting the sparsity of real-world graphs – and is readily applicable to graphs with thousands of nodes.
",2. Related Work,[0],[0]
"1 Code available at: https://www.kdd.in.tum.de/netgan
Deep learning methods for graph data have mostly been studied in the context of node embeddings (Perozzi et al., 2014; Grover & Leskovec, 2016; Kipf & Welling, 2016).",2. Related Work,[0],[0]
"The main idea behind these approaches is that of modeling the probabilities of each individual edge’s existence, p(Auv), as some function of the respective node embeddings, f(hu,hv), where f is represented by a neural network.",2. Related Work,[0],[0]
"The recently proposed GraphGAN (Wang et al., 2017) is another instance of such prescribed edge-level probabilistic models, where f is optimized using the GAN objective instead of the traditional cross-entropy.",2. Related Work,[0],[0]
Deep embedding based methods achieve state-of-the-art scores in tasks like link prediction and node classification.,2. Related Work,[0],[0]
"Nevertheless, as we show in Sec. 3.2, using such approaches for generating entire graphs produces samples that don’t preserve any of the patterns inherent to real-world networks.
",2. Related Work,[0],[0]
Prescribed generative models for graphs have a long history and are well-studied.,2. Related Work,[0],[0]
For a survey we refer the reader to Chakrabarti & Faloutsos (2006) and Goldenberg et al. (2010).,2. Related Work,[0],[0]
"Typically, prescribed generative approaches are designed to capture and reproduce some predefined subset of graph properties (e.g., degree distribution, community structure, clustering coefficient).",2. Related Work,[0],[0]
"Notable examples include the configuration model (Bender & Canfield, 1978; Molloy & Reed, 1995), variants of the degree-corrected stochastic blockmodel (Karrer & Newman, 2011; Bojchevski & Günnemann), Exponential Random Graph Models (Holland & Leinhardt, 1981), Multiplicative Attribute Graph model (Kim & Leskovec, 2011), and the block two-level ErdősRéniy random graph model (Seshadhri et al., 2012).",2. Related Work,[0],[0]
"In Sec. 4 we compare with some of these prescribed models on the tasks of graph generation and link prediction.
",2. Related Work,[0],[0]
"Due to the challenging nature of the problem, only few approaches able to generate discrete data using GANs exist.",2. Related Work,[0],[0]
"Most approaches focus on generating discrete sequences such as text, with some of them using reinforcement learn-
ing techniques to enable backpropagation through sampling discrete random variables (Yu et al., 2017; Kusner & Hernández-Lobato, 2016; Li et al., 2017; Liang et al., 2017).",2. Related Work,[0],[0]
"Other approaches modify the GAN objective to tackle the same challenge (Che et al., 2017; Hjelm et al., 2017).",2. Related Work,[0],[0]
"Focusing on non-sequential discrete data, Choi et al. (2017) generate high-dimensional discrete features (e.g. binary indicators, counts) in patient records.",2. Related Work,[0],[0]
None of these methods have considered graph structured data.,2. Related Work,[0],[0]
In this section we introduce NetGAN - a Generative Adversarial Network model for graph / network data.,3. Model,[0],[0]
Its core idea lies in capturing the topology of a graph by learning a distribution over the random walks.,3. Model,[0],[0]
"Given an input graph of N nodes, defined by a binary adjacency matrix A ∈ {0, 1}N×N , we first sample a set of random walks of length T from A.",3. Model,[0],[0]
This collection of random walks serves as a training set for our model.,3. Model,[0],[0]
"We use the biased secondorder random walk sampling strategy described in Grover & Leskovec (2016), as it better captures both local and global graph structure.",3. Model,[0],[0]
An important advantage of using random walks is their invariance under node reordering.,3. Model,[0],[0]
"Additionally, random walks only include the nonzero entries of A, thus efficiently exploiting the sparsity of real-world graphs.
",3. Model,[0],[0]
"Like any typical GAN architecture, NetGAN consists of two main components - a generator G and a discriminator D. The goal of the generator is to generate synthetic random walks that are plausible in the input graph.",3. Model,[0],[0]
"At the same time, the discriminator learns to distinguish the synthetic random walks from the real ones that come from the training set.",3. Model,[0],[0]
BothG andD are trained end-to-end using backpropagation.,3. Model,[0],[0]
"At any point of the training process it is possible to use G to generate a set of random walks, which can then be used to produce an adjacency matrix of a new generated graph.",3. Model,[0],[0]
In the rest of this section we describe each stage of this process and our design choices in more detail.,3. Model,[0],[0]
An overview of our model’s complete architecture can be seen in Fig. 2.,3. Model,[0],[0]
Generator.,3.1. Architecture,[0],[0]
"The generator G defines an implicit probabilistic model for generating random walks: (v1, ...,vT )",3.1. Architecture,[0],[0]
∼ G. We model G as a sequential process based on a neural network fθ parametrized by θ.,3.1. Architecture,[0],[0]
"At each step t, fθ produces two values: the probability distribution over the next node to be sampled, parametrized by the logits pt, and the current memory state of the model, denoted as mt.",3.1. Architecture,[0],[0]
"The next node vt, represented as a one-hot vector, is sampled from a categorical distribution vt ∼ Cat(σ(pt)), where σ(·) denotes the softmax function, and together with mt is passed into fθ at the next step t+ 1.",3.1. Architecture,[0],[0]
"Similarly to the classic GAN setting, a latent code z drawn from a multivariate standard normal
distribution is passed through a parametric function gθ′ to initialize m0.",3.1. Architecture,[0],[0]
"The generative process of G is summarized in the box below.
",3.1. Architecture,[0],[0]
"z ∼ N (0, Id) m0 = gθ′(z)
v1 ∼ Cat(σ(p1)), (p1,m1) = fθ(m0,0) v2 ∼ Cat(σ(p2)), (p2,m2) = fθ(m1,v1)
... ...
vT ∼ Cat(σ(pT )), (pT ,mT ) = fθ(mT−1,vT−1)
",3.1. Architecture,[0],[0]
"In this work we focus our attention on the Long short-term memory (LSTM) architecture for fθ, introduced by Hochreiter & Schmidhuber (1997).",3.1. Architecture,[0],[0]
"The memory state mt of an LSTM is represented by the cell state Ct, and the hidden state ht.",3.1. Architecture,[0],[0]
"The latent code z goes through two separate streams, each consisting of two fully connected layers with tanh activation, and then used to initialize (C0,h0).
",3.1. Architecture,[0],[0]
"A natural question might arise: ”Why use a model with memory and temporal dependencies, when the random walks are Markov processes?”",3.1. Architecture,[0],[0]
(2nd order Markov for biased RWs).,3.1. Architecture,[0],[0]
"Or put differently, what’s the benefit of using random walks of length greater than 2?",3.1. Architecture,[0],[0]
"In theory, a model with large enough capacity could simply memorize all existing edges in the graph and recreate them.",3.1. Architecture,[0],[0]
"However, for large graphs achieving this in practice is not feasible.",3.1. Architecture,[0],[0]
"More importantly, pure memorization is not the goal of NetGAN, rather we want to have generalization and to generate graphs with similar properties, not exact replicas.",3.1. Architecture,[0],[0]
"Having longer random walks combined with memory helps the model to learn the topology and general patterns in the data (e.g., community structure).",3.1. Architecture,[0],[0]
"Our experiments in Sec. 4.2 confirm this, showing that longer random walks are indeed beneficial.
",3.1. Architecture,[0],[0]
"After each time step, to generate the next node in the random walk, the network fθ should output the logits pt of lengthN .",3.1. Architecture,[0],[0]
"However, operating in such high dimensional space leads to an unnecessary computational overhead.",3.1. Architecture,[0],[0]
"To tackle this issue, the LSTM outputs ot ∈ RH instead, with H N , which is then up-projected to RN using the matrix W up ∈ RH×N .",3.1. Architecture,[0],[0]
"This enables us to efficiently handle large-scale graphs.
",3.1. Architecture,[0],[0]
Sampling the next node in the random walk vt presents another challenge.,3.1. Architecture,[0],[0]
Since sampling from a categorical distribution is a non-differentiable operation it blocks the flow of gradients and precludes backpropagation.,3.1. Architecture,[0],[0]
We solve this problem by using the Straight-Through Gumbel estimator by Jang et al. (2016).,3.1. Architecture,[0],[0]
"More specifically, we perform the following transformation: First, we let v∗t = σ",3.1. Architecture,[0],[0]
"((pt + g)/τ)), where τ is a temperature parameter, and gi’s are i.i.d.",3.1. Architecture,[0],[0]
samples from a Gumbel distribution with zero mean and unit scale.,3.1. Architecture,[0],[0]
"Then, the next sample is
chosen as vt = onehot(arg maxv∗t ).",3.1. Architecture,[0],[0]
"While the one-hot sample vt is passed as input to the next time step, during the backward pass the gradients will flow through the differentiable v∗t .",3.1. Architecture,[0],[0]
"The choice of τ allows to trade-off between better flow of gradients (large τ , more uniform v∗t ) and more exact calculations (small τ , v∗t",3.1. Architecture,[0],[0]
"≈ vt).
",3.1. Architecture,[0],[0]
"Now that a new node vt is sampled, it needs to be projected back to a lower-dimensional representation before feeding into the LSTM.",3.1. Architecture,[0],[0]
This is done by means of down-projection matrix W down ∈,3.1. Architecture,[0],[0]
"RN×H .
",3.1. Architecture,[0],[0]
Discriminator.,3.1. Architecture,[0],[0]
The discriminator D is based on the standard LSTM architecture.,3.1. Architecture,[0],[0]
"At every time step t, a one-hot vector vt, denoting the node at the current position, is fed as input.",3.1. Architecture,[0],[0]
"After processing the entire sequence of T nodes, the discriminator outputs a single score that represents the probability of the random walk being real.",3.1. Architecture,[0],[0]
Wasserstein GAN.,3.2. Training,[0],[0]
"We train our model based on the Wasserstein GAN (WGAN) framework (Arjovsky et al., 2017), as it prevents mode collapse and leads to more stable training overall.",3.2. Training,[0],[0]
"To enforce the Lipschitz constraint of the discriminator, we use the gradient penalty as in Gulrajani et al. (2017).",3.2. Training,[0],[0]
"The model parameters {θ, θ′} are trained using stochastic gradient descent with Adam (Kingma & Ba, 2014).",3.2. Training,[0],[0]
"Weights are regularized with an L2 penalty.
",3.2. Training,[0],[0]
Early stopping.,3.2. Training,[0],[0]
"Because we are interested in generalizing the input graph, the “trivial” solution where the generator has memorized all existing edges is of no interest to us.",3.2. Training,[0],[0]
This means that we need to control how closely the generated graphs resemble the original one.,3.2. Training,[0],[0]
"To achieve this, we propose two possible early stopping strategies, either of which can be used depending on the task at hand.",3.2. Training,[0],[0]
"The
first strategy, named VAL-CRITERION is concerned with the generalization properties of NetGAN.",3.2. Training,[0],[0]
"During training, we keep a sliding window of the random walks generated in the last 1,000 iterations and use them to construct a matrix of transition counts.",3.2. Training,[0],[0]
"This matrix is then used to evaluate the link prediction performance on a validation set (i.e. ROC and AP scores, for more details see Sec. 4.2).",3.2. Training,[0],[0]
"We stop with training when the validation performance stops improving.
",3.2. Training,[0],[0]
"The second strategy, named EO-CRITERION makes NetGAN very flexible and gives the user control over the graph generation.",3.2. Training,[0],[0]
We stop training when we achieve a user specified edge overlap between the generated graphs (see next section) and the original one at a given iteration.,3.2. Training,[0],[0]
"Based on her end task the user can choose to generate graphs with either small or large edge overlap with the original, while maintaining structural similarity.",3.2. Training,[0],[0]
"This will lead to generated graphs that either generalize better or are closer replicas respectively, yet still capture the properties of the original.",3.2. Training,[0],[0]
"After finishing the training, we use the generator G to construct a score matrix S of transition counts, i.e. we count how often an edge appears in the set of generated random walks (typically, using a much larger number of random walks than for early stopping, e.g., 500K).",3.3. Assembling the Adjacency Matrix,[0],[0]
"While the raw counts matrix S is sufficient for link prediction purposes, we need to convert it to a binary adjacency matrix Â",3.3. Assembling the Adjacency Matrix,[0],[0]
if we wish to reason about the synthetic graph.,3.3. Assembling the Adjacency Matrix,[0],[0]
"First, S is symmetrized by setting sij = sji = max{sij , sji}.",3.3. Assembling the Adjacency Matrix,[0],[0]
"Because we cannot explicitly control the starting node of the random walks generated by G, some high-degree nodes will likely be overrepresented.",3.3. Assembling the Adjacency Matrix,[0],[0]
"Thus, a simple binarization strategy like thresholding or choosing top-k entries might lead to leaving out the low-degree nodes and producing singletons.
",3.3. Assembling the Adjacency Matrix,[0],[0]
"To address this issue, we use the following approach: (i) We ensure that every node i has at least one edge by sampling a neighbor j with probability pij =
sij∑ v siv
.",3.3. Assembling the Adjacency Matrix,[0],[0]
"If an edge was already sampled before, we repeat the procedure; (ii) We continue sampling edges without replacement using for each edge (i, j) the probability pij =
sij∑ u,v suv
, until we reach the desired amount of edges (e.g., as many edges as in the original graph).",3.3. Assembling the Adjacency Matrix,[0],[0]
"To obtain an undirected graph for every edge (i, j) we also include (j, i).",3.3. Assembling the Adjacency Matrix,[0],[0]
Note that this procedure is not guaranteed to produce a fully connected graph.,3.3. Assembling the Adjacency Matrix,[0],[0]
In this section we evaluate the quality of the graphs generated by NetGAN by computing various graph statistics.,4. Experiments,[0],[0]
We quantify the generalization power of the proposed model by evaluating its link prediction performance.,4. Experiments,[0],[0]
"Furthermore, we demonstrate how we can generate graphs with smoothly changing properties via latent space interpolation.",4. Experiments,[0],[0]
Additional experiments are provided in the supp.,4. Experiments,[0],[0]
"mat.
Datasets.",4. Experiments,[0],[0]
For the experiments we use five well-known citation datasets and the Political Blogs dataset.,4. Experiments,[0],[0]
For the large CORA dataset and its commonly used subset of machine learning papers denoted with CORA-ML we use the same preprocessing as in Bojchevski & Günnemann (2018).,4. Experiments,[0],[0]
For all the experiments we treat the graphs as undirected and only consider the largest connected component (LCC).,4. Experiments,[0],[0]
Information about the datasets is listed in Table 2.,4. Experiments,[0],[0]
Setup.,4.1. Graph Generation,[0],[0]
"In this task, we fit NetGAN to the CORA-ML and CITESEER citation networks in order to evaluate quality of the generated graphs.",4.1. Graph Generation,[0],[0]
"We compare to the following baselines: configuration model (Molloy & Reed, 1995), degree-corrected stochastic blockmodel (DC-SBM) (Karrer & Newman, 2011), exponential random graph model (ERGM) (Holland & Leinhardt, 1981) and the block twolevel Erdős-Réniy random graph model (BTER) (Seshadhri et al., 2012).",4.1. Graph Generation,[0],[0]
"Additionally, we use the variational graph autoencoder (VGAE) (Kipf & Welling, 2016) as a representative of network embedding approaches.",4.1. Graph Generation,[0],[0]
We randomly hide 15% of the edges (which are used for the stopping criterion; see Sec. 3.2) and fit all the models on the remaining graph.,4.1. Graph Generation,[0],[0]
We sample 5 graphs from each of the trained models and report their average statistics in Table 1.,4.1. Graph Generation,[0],[0]
"Definitions of the statistics, additional metrics, standard deviations and details about the baselines are given in the supplementary material.
",4.1. Graph Generation,[0],[0]
Evaluation.,4.1. Graph Generation,[0],[0]
"The general trend that becomes apparent from the results in Table 1 (and Table 2 in supplementary material) is that prescribed models excel at recovering the statistics that they directly model (e.g., degree sequence for DC-SBM).",4.1. Graph Generation,[0],[0]
"At the same time, these models struggle when dealing with graph properties that they don’t account for (e.g., assortativity for BTER).",4.1. Graph Generation,[0],[0]
"On the other hand, NetGAN is able to capture all the graph properties well, although none of them are explicitly specified in its model definition.",4.1. Graph Generation,[0],[0]
We also see that VGAE is not able to produce realistic graphs.,4.1. Graph Generation,[0],[0]
"This is expected, since the main purpose of VGAE is learning node embeddings, and not generating entire graphs.
",4.1. Graph Generation,[0],[0]
"The final column shows the average rank of each method for all statistics, with NetGAN performing the best.",4.1. Graph Generation,[0],[0]
"ERGM seems to be performing surprisingly well, however it suffers from severe overfitting – using the same fitted ERGM for the link prediction task we get both AUC and AP scores close to 0.5 (worst possible value).",4.1. Graph Generation,[0],[0]
"In contrast, NetGAN does a good job both at preserving properties in generated graphs, as well as generalizing, as we see in Sec. 4.2.
",4.1. Graph Generation,[0],[0]
Is the good performance of NetGAN in this experiment only due to the overlapping edges (existing in the input graph)?,4.1. Graph Generation,[0],[0]
"To rule out this possibility we perform the following experiment: We take the graph generated by NetGAN, fix the overlapping edges and rewire the rest according to the configuration model.",4.1. Graph Generation,[0],[0]
The properties of the resulting graph (row #3 in Table 1) deviate strongly from the input graph.,4.1. Graph Generation,[0],[0]
"This confirms that NetGAN does not simply memorize some edges and generates the rest at random, but rather captures the underlying structure of the network.
",4.1. Graph Generation,[0],[0]
"In line with our intuition, we can see that higher EO leads to generated graphs with statistics closer to the original.",4.1. Graph Generation,[0],[0]
Figs.,4.1. Graph Generation,[0],[0]
3b and 3c show how the graph statistics evolve during the training process.,4.1. Graph Generation,[0],[0]
Fig.,4.1. Graph Generation,[0],[0]
3c shows that the edge overlap smoothly increasing with the number of epochs.,4.1. Graph Generation,[0],[0]
We provide plots for other statistics and for CITESEER in the supp.,4.1. Graph Generation,[0],[0]
mat.,4.1. Graph Generation,[0],[0]
Setup.,4.2. Link Prediction,[0],[0]
Link prediction is a common graph mining task where the goal is to predict the existence of unobserved links in a given graph.,4.2. Link Prediction,[0],[0]
We use it to evaluate the generalization properties of NetGAN.,4.2. Link Prediction,[0],[0]
"We hold out 10% of edges from the graph for validation and 5% as the test set, along with the same amount of randomly selected non-edges, while ensuring that the training network remains connected.",4.2. Link Prediction,[0],[0]
We measure the performance with two commonly used metrics: area under the ROC curve (AUC) and average precision (AP).,4.2. Link Prediction,[0],[0]
"To evaluate NetGAN’s performance, we sample a given number of random walks (500K/100M) from the trained generator and we use the observed transition counts between any two nodes as a measure of how likely there is an edge between them.",4.2. Link Prediction,[0],[0]
"We compare with DC-SBM, node2vec and VGAE, as well as Adamic/Adar(Adamic & Adar, 2003).
",4.2. Link Prediction,[0],[0]
Evaluation.,4.2. Link Prediction,[0],[0]
The results are listed in Table 3.,4.2. Link Prediction,[0],[0]
"There is no overall dominant method, with different methods achieving
best results on different datasets.",4.2. Link Prediction,[0],[0]
"NetGAN shows competitive performance for all datasets, even achieving state-of-theart results for some of them (CITESEER and POLBLOGS), despite not being explicitly trained for this task.
",4.2. Link Prediction,[0],[0]
"Interestingly, the NetGAN performance increases when increasing the number of random walks sampled from the generator.",4.2. Link Prediction,[0],[0]
"This is especially true for the larger networks (CORA, DBLP, PUBMED), since given their size we need more random walks to cover the entire graph.",4.2. Link Prediction,[0],[0]
This suggests that for an additional computational cost one can get significant gains in link prediction performance.,4.2. Link Prediction,[0],[0]
"Note, that while 100M may seem like a large number, the sampling procedure can be trivially parallelized.
Sensitivity analysis.",4.2. Link Prediction,[0],[0]
"Although NetGAN has many hyperparameters – typical for a GAN model – in practice most of them are not critical for performance, as long as they are within a reasonable range (e.g. H ≥ 30).
",4.2. Link Prediction,[0],[0]
One important exception is the the random walk length T .,4.2. Link Prediction,[0],[0]
"To choose the optimal value, we evaluate the change in link prediction performance as we vary T on CORAML.",4.2. Link Prediction,[0],[0]
"We train multiple models with different random walk
lengths, and evaluate the scores ensuring each one observes equal number of transitions.",4.2. Link Prediction,[0],[0]
Results averaged over 5 runs are given in Fig. 6.,4.2. Link Prediction,[0],[0]
We empirically confirm that the model benefits from using longer random walks as opposed to just edges (i.e. T=2).,4.2. Link Prediction,[0],[0]
"The performance gain for T = 20 over T = 16 is marginal and does not outweigh the additional computational cost, thus we set T = 16 for all experiments.",4.2. Link Prediction,[0],[0]
Setup.,4.3. Latent Variable Interpolation,[0],[0]
Latent space interpolation is a good way to gain insight into what kind of structure the generator was able to capture.,4.3. Latent Variable Interpolation,[0],[0]
To be able to visualize the properties of the generated graphs we train our model using a 2-dimensional noise vector z drawn as before from a bivariate standard normal distribution.,4.3. Latent Variable Interpolation,[0],[0]
This corresponds to a 2-dimensional latent space Ω = R2.,4.3. Latent Variable Interpolation,[0],[0]
"Then, instead of sampling z from the entire latent space Ω, we now sample from subregions of Ω and visualize the results.",4.3. Latent Variable Interpolation,[0],[0]
"Specifically, we divide Ω into 20× 20 subregions (bins) of equal probability mass using the standard normal cumulative distribution function Φ. For each bin we generate 62.5K random walks.",4.3. Latent Variable Interpolation,[0],[0]
"We evaluate properties of both the generated random walks themselves, as well as properties of the resulting graphs obtained by sampling a binary adjacency matrix for each bin.
Evaluation.",4.3. Latent Variable Interpolation,[0],[0]
In Fig.,4.3. Latent Variable Interpolation,[0],[0]
4a and 4b we see properties of the generated random walks; in Fig.,4.3. Latent Variable Interpolation,[0],[0]
"4c and 4d, we visualize properties of graphs sampled from the random walks in the respective bins.",4.3. Latent Variable Interpolation,[0],[0]
"In all four heatmaps, we see distinct patterns, e.g. higher average degree of starting nodes for the bottom right region of Fig.",4.3. Latent Variable Interpolation,[0],[0]
"4a, or higher degree distribution inequality in the top-right area of Fig.",4.3. Latent Variable Interpolation,[0],[0]
4c.,4.3. Latent Variable Interpolation,[0],[0]
While Fig.,4.3. Latent Variable Interpolation,[0],[0]
"4c and 4d show that certain regions of z correspond to generated graphs with very different degree distributions, recall that sampling from the entire latent space (Ω) yields graphs with degree distribution similar to the original graph (see Fig. 1c).",4.3. Latent Variable Interpolation,[0],[0]
The model was trained on CORA-ML.,4.3. Latent Variable Interpolation,[0],[0]
"More heatmaps for other metrics (16 in total) and visualizations for CITESEER can be found in the supplementary material.
",4.3. Latent Variable Interpolation,[0],[0]
This experiment clearly demonstrates that by interpolating in the latent space we can obtain graphs with smoothly changing properties.,4.3. Latent Variable Interpolation,[0],[0]
"The smooth transitions in the heatmaps provide evidence that our model learns to map specific parts of the latent space to specific properties of the graph.
",4.3. Latent Variable Interpolation,[0],[0]
We can also see this mapping from latent space to the generated graph properties in the community distribution histograms on a 10 × 10 grid in Fig. 5.,4.3. Latent Variable Interpolation,[0],[0]
"Marked by (*) and (Ω) we see the community distributions for the input graph and the graph obtained by sampling on the complete latent
space respectively.",4.3. Latent Variable Interpolation,[0],[0]
In Fig.,4.3. Latent Variable Interpolation,[0],[0]
"5b and 5c, we see the evolution of selected community shares when following a trajectory from top to bottom, and left to right, respectively.",4.3. Latent Variable Interpolation,[0],[0]
"The community histograms resulting from sampling random walks from opposing regions of the latent space are very different; again the transitions between these histograms are smooth, as can be seen in the trajectories in Fig.",4.3. Latent Variable Interpolation,[0],[0]
5b and 5c.,4.3. Latent Variable Interpolation,[0],[0]
"When evaluating different graph generative models in Sec. 3.2, we observed a major limitation of explicit models.",5. Discussion and Future Work,[0],[0]
"While the prescribed approaches excel at recovering the properties directly included in their definition, they perform significantly worse with respect to the rest.",5. Discussion and Future Work,[0],[0]
This clearly indicates the need for implicit graph generators such as NetGAN.,5. Discussion and Future Work,[0],[0]
"Indeed, we notice that our model is able to consistently capture all the important graph characteristics (see Table 1).",5. Discussion and Future Work,[0],[0]
"Moreover, NetGAN generalizes beyond the input graph, as can be seen by its strong link prediction performance in Sec. 4.2.",5. Discussion and Future Work,[0],[0]
"Still, being the first model of its kind, NetGAN possesses certain limitations, and a number of related questions could be addressed in follow-up works:
Scalability.",5. Discussion and Future Work,[0],[0]
We have observed in Sec. 4.2 that it takes a large number of generated random walks to get representative transition counts for large graphs.,5. Discussion and Future Work,[0],[0]
"While sampling random walks from NetGAN is trivially parallelizable, a possible extension of our model is to use a conditional generator, i.e. the generator can be provided a desired starting node, thus ensuring a more even coverage.",5. Discussion and Future Work,[0],[0]
"On the other hand, the sampling procedure itself can be sped up by incorporating a hierarchical softmax output layer - a method commonly used in natural language processing.
",5. Discussion and Future Work,[0],[0]
Evaluation.,5. Discussion and Future Work,[0],[0]
"It is nearly impossible to judge whether a graph is realistic by visually inspecting it (unlike images, for example).",5. Discussion and Future Work,[0],[0]
In this work we already quantitatively evaluate the performance of NetGAN on a large number of standard graph statistics.,5. Discussion and Future Work,[0],[0]
"However, developing new measures applicable to (implicit) graph generative models will deepen our understanding of their behavior, and is an important direction for future work.
",5. Discussion and Future Work,[0],[0]
Experimental scope.,5. Discussion and Future Work,[0],[0]
In the current work we focus on the setting of a single large graph.,5. Discussion and Future Work,[0],[0]
"Adaptation to other scenarios, such as a collection of smaller i.i.d.",5. Discussion and Future Work,[0],[0]
"graphs, that frequently occur in other fields (e.g., chemistry, biology), would be an important extension of our model.",5. Discussion and Future Work,[0],[0]
"Studying the influence of the graph topology (e.g., sparsity, diameter) on NetGAN’s performance will shed more light on the model’s properties.
",5. Discussion and Future Work,[0],[0]
Other types of graphs.,5. Discussion and Future Work,[0],[0]
"While plain graphs are ubiquitous, many of important applications deal with attributed, k-partite or heterogeneous networks.",5. Discussion and Future Work,[0],[0]
Adapting the NetGAN model to handle these other modalities of the data is a promising direction for future research.,5. Discussion and Future Work,[0],[0]
"Especially important would be an adaptation to the dynamic / inductive setting, where new nodes are added over time.",5. Discussion and Future Work,[0],[0]
In this work we introduce NetGAN - an implicit generative model for network data.,6. Conclusion,[0],[0]
"NetGAN is able to generate graphs that capture important topological properties of complex networks, such as community structure and degree distribution, without having to manually specify any of them.",6. Conclusion,[0],[0]
"Moreover, our proposed model shows strong generalization properties, as highlighted by its competitive link prediction performance on a number of datasets.",6. Conclusion,[0],[0]
NetGAN can also be used for generating graphs with continuously varying characteristics using latent space interpolation.,6. Conclusion,[0],[0]
Combined our results provide strong evidence that implicit generative models for graphs are well-suited for capturing the complex nature of real-world networks.,6. Conclusion,[0],[0]
"This research was supported by the German Research Foundation, Emmy Noether grant GU 1409/2-1, and by the Technical University of Munich - Institute for Advanced Study, funded by the German Excellence Initiative and the European Union Seventh Framework Programme under grant agreement no 291763, co-funded by the European Union.",Acknowledgments,[0],[0]
We propose NetGAN – the first implicit generative model for graphs able to mimic real-world networks.,abstractText,[0],[0]
We pose the problem of graph generation as learning the distribution of biased random walks over the input graph.,abstractText,[0],[0]
The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein GAN objective.,abstractText,[0],[0]
NetGAN is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition.,abstractText,[0],[0]
"At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task.",abstractText,[0],[0]
"Being the first approach to combine both of these desirable properties, NetGAN opens exciting avenues for further research.",abstractText,[0],[0]
NetGAN: Generating Graphs via Random Walks,title,[0],[0]
"Given a large symmetrical network, we are interested in the global testing problem where we use the adjacency matrix of the network to test whether the network consists of only one community or that it consists of multiple communities, where some nodes may have mixed memberships.
",1. Introduction,[0],[0]
Real networks frequently have severe degree heterogeneity.,1. Introduction,[0],[0]
"The Stochastic Block Model (SBM) is well-known, but does not accommodate severe degree heterogeneity.",1. Introduction,[0],[0]
"To tackle the problem, Karrer and Newman (2011) proposed the DegreeCorrected Block Model (DCBM).",1. Introduction,[0],[0]
"DCBM strikes a better balance between theory and practice than SBM, and has become increasingly more popular recently.
",1. Introduction,[0],[0]
"We adopt a Degree-Corrected Mixed-Membership (DCMM) model (Jin et al., 2017).",1. Introduction,[0],[0]
"DCMM can be viewed as an extension of DCBM, but allows for mixed memberships.",1. Introduction,[0],[0]
"Suppose the network has n nodes and K perceivable communities
C1, C2, . . .",1. Introduction,[0],[0]
", CK .",1. Introduction,[0],[0]
"*Equal contribution 1Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, USA 2Department of Statistics, University of Chicago, Chicago, USA.",1. Introduction,[0],[0]
"Correspondence to: Jiashun Jin <jiashun@stat.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"For each node, we assign a Probability Mass Function (PMF) πi = (πi(1), πi(2), · · · , πi(K))′, where πi(k) is the “weight” that node i puts on community Ck, 1 ≤ k ≤",1. Introduction,[0],[0]
K.,1. Introduction,[0],[0]
We call node i “pure” if πi is degenerate and “mixed” otherwise.,1. Introduction,[0],[0]
"Let A ∈ Rn,n be the adjacency matrix, where Aij = 1 if nodes i and j have an edge, and Aij = 0 otherwise (all diagonal entries of A are 0 as we don’t treat a node as connecting to itself).",1. Introduction,[0],[0]
"In DCMM, we think the upper triangle of A contains independent Bernoulli random variables.",1. Introduction,[0],[0]
"Moreover, for n degree heterogeneity parameters θ1, θ2, . . .",1. Introduction,[0],[0]
", θn and a non-singular, irreducible matrix P ∈ RK,K ,
P(Aij = 1) = θiθj K∑ k=1 K∑ `=1 πi(k)πj(`)Pk`.",1. Introduction,[0],[0]
"(1)
Also, we assume (a) all diagonals of P are 1, and (b) each of the K communities has at least one pure node.",1. Introduction,[0],[0]
"With such constraints, DCMM is identifiable (Jin et al., 2017).
",1. Introduction,[0],[0]
"Let Θ be the n× n diagonal matrix Θ = diag(θ1, . . .",1. Introduction,[0],[0]
", θn) and let Π be the n×K matrix Π =",1. Introduction,[0],[0]
"[π1, π2, . . .",1. Introduction,[0],[0]
", πn]′. Let Ω = ΘΠPΠ′Θ (recall that P ∈ RK,K):
Ω = θ1 . . .",1. Introduction,[0],[0]
θn  π ′ 1 ... π′n P,1. Introduction,[0],[0]
"[π1, . . .",1. Introduction,[0],[0]
", πn] θ1 . . .",1. Introduction,[0],[0]
θn  .,1. Introduction,[0],[0]
"Let diag(Ω) be the diagonal matrix where the i-th diagonal entry is Ωii, and let W = A− [Ω− diag(Ω)].",1. Introduction,[0],[0]
"We have
A = [Ω− diag(Ω)]",1. Introduction,[0],[0]
"+W = “signal” + “noise”.
",1. Introduction,[0],[0]
Remark 1.,1. Introduction,[0],[0]
"Many recent works use the “Random Degree Parameter (RDP)” model (which is narrower than ours): fixing a scaling parameter αn > 0 and a density function f over (0,∞) where the first a few moments of f are finite, and especially the second moment is 1, we assume (θi/αn)
iid∼ f , i = 1, 2, . . .",1. Introduction,[0],[0]
", n. We call the resultant DCMM model the DCMM-RDP model.",1. Introduction,[0],[0]
"In applications where we don’t know how θi’s are correlated, it is preferable to treat θ as nonrandom as before, and it is safer to use the original DCMM than DCMM-RDP.
",1. Introduction,[0],[0]
"The testing problem can be cast as testing a null hypothesis H
(n) 0 against a specific hypothesis H (n) 1 in its complement:
H (n) 0",1. Introduction,[0],[0]
": K = 1 vs. H (n) 1 : K > 1, (2)
where under H(n)1 , DCMM holds for some eligible (P, θ1, ..., θn, π1, ..., πn).",1. Introduction,[0],[0]
"Note that under H (n) 0 , P = 1, π1, ..., πn are all degenerate, and P(A(i, j) = 1) = θiθj .
",1. Introduction,[0],[0]
"Most existing literatures for the testing problem (2) have been focused on the special case where
•",1. Introduction,[0],[0]
No degree heterogeneity.,1. Introduction,[0],[0]
θ1 = θ2 = . . .,1. Introduction,[0],[0]
= θn. •,1. Introduction,[0],[0]
No mixed-membership.,1. Introduction,[0],[0]
"All πi are degenerate PMFs.
",1. Introduction,[0],[0]
"Many tests were proposed for this special case, including but are not limited to the likelihood ratio test approach (Wang & Bickel, 2017) and the spectral approach (Bickel & Sarkar, 2016; Lei, 2016; Banerjee & Ma, 2017).
",1. Introduction,[0],[0]
"However, in our setting, θ1, θ2, . .",1. Introduction,[0],[0]
.,1. Introduction,[0],[0]
", θn vary significantly from one to another, and it is unclear how to extend the methods above to the current setting.",1. Introduction,[0],[0]
"The likelihood ratio test is not applicable, for there are many unknown parameters (θ1, π1), (θ2, π2), ..., (θn, πn).",1. Introduction,[0],[0]
"The spectral approach also faces challenges, because the distributions of such test statistics depend on unknown parameters in a complicated way, and it is nontrivial to figure out the rejection region.
",1. Introduction,[0],[0]
"Also, it may be tempting to adapt the recent approaches on estimating K to our testing problem (Saldana et al., 2017; Chen & Lei, 2017; Le & Levina, 2015), but similarly, due to severe degree heterogeneity, the null distributions of such statistics are not tractable, so they cannot be used directly.
",1. Introduction,[0],[0]
"Recently, Gao and Lafferty (2017) (see also Bubeck et al. (2016) which is related but on different settings) proposed a new test called the Erdős-Zuckerberg (EZ) test for DCMM, with the following constraints.
",1. Introduction,[0],[0]
• (GL1) No mixed-membership: all πi are degenerate.,1. Introduction,[0],[0]
• (GL2),1. Introduction,[0],[0]
"The community labels are uniformly drawn so
the K communities have roughly equal sizes.
",1. Introduction,[0],[0]
"• (GL3) The DCMM-RDP holds (i.e., θi are iid samples), and the K ×K matrix P has the special form of
P = a b · · · b... . . .",1. Introduction,[0],[0]
...,1. Introduction,[0],[0]
b b · · · a  .,1. Introduction,[0],[0]
"Note that the last bullet point is particularly restrictive.
",1. Introduction,[0],[0]
"Gao and Lafferty (2017) made an interesting observation that the effect of the degree heterogeneity parameters θ1, θ2, . . .",1. Introduction,[0],[0]
", θn is largely canceled out in the EZ test, and the test statistic approximately equals to 0 under the null, regardless of what θ1, θ2, . . .",1. Introduction,[0],[0]
", θn are.",1. Introduction,[0],[0]
"This allows us to find a convenient way to map out the rejection region.
",1. Introduction,[0],[0]
"Unfortunately, the authors did not make it clear whether the cancellation is “coincidental” and is due to the symmetry they imposed on the model (see GL1-GL3), or is “inherent” and holds for much broader settings.
",1. Introduction,[0],[0]
"In this paper, we introduce a class of test statistics by counting the number of graphlets in the network.",1. Introduction,[0],[0]
"Fixing a small m ≥ 1, we count two kinds of graphlets: length-m paths and m-cycles.",1. Introduction,[0],[0]
"Our main contributions are as follows:
• Ideation.",1. Introduction,[0],[0]
"For a K-vector η and a K × K matrix G1/2PG1/2 to be introduced, we derive succinct proxies for the number of length-m paths and mcycles, using η and the eigenvalues and eigenvectors of G1/2PG1/2.",1. Introduction,[0],[0]
"The proxies motivate a systematic way of constructing tests where the degree heterogeneity is largely removed so the distributions are more tractable.
",1. Introduction,[0],[0]
"To the best of our knowledge, such proxies have not been discovered in the literature.",1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
Methods and theory.,1. Introduction,[0],[0]
"We propose a class of graphlet count (GC) test statistics, and derive their asymptotic distributions, under the null and alternative hypotheses.
",1. Introduction,[0],[0]
"We try to be as general as possible, and our methods and theory are for DCMM with minimal constraints.
",1. Introduction,[0],[0]
"The way we construct our statistics is to use the proxies aforementioned, and thus is different from that in Gao and Lafferty (2017), which uses calculations that heavily depend on the imposed constraints GL1-GL3.",1. Introduction,[0],[0]
"See Section 3.2 for more comparison.
",1. Introduction,[0],[0]
Our findings support the philosophy of Jin (2015) which introduced the community detection algorithm SCORE.,1. Introduction,[0],[0]
"Jin (2015) pointed out that θ1, θ2, . . .",1. Introduction,[0],[0]
", θn are required to model severe degree heterogeneity, but they turn out to be nuisance parameters, the effects of which can be largely removed with a proper construction of statistics.
",1. Introduction,[0],[0]
"While our tests are designed for global testing, the idea is also useful for tackling other problems.",1. Introduction,[0],[0]
"For example, we can combine our idea with those in community detection (e.g. Jin (2015), Chen et al. (2018), Qin and Rohe (2013)) to estimate the number of communities K.",1. Introduction,[0],[0]
"The testing problem (2) is hard for there are so many unknown parameters: P, θ1, . . .",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
", θn, π1, . . .",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
", πn.",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"The parameters θ1, θ2, . . .",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
", θn, which are required to model the severe degree heterogeneity of real networks, are especially hard to deal with for they vary significantly from one to another.",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"What we need is therefore a smart test statistic that
• does not vary significantly as θ = (θ1, θ2, . . .",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
", θn)′ varies, and has a tractable limiting distribution (so it is easy to map out the rejection region),
• is powerful in differentiating the null and alternative.
",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
Our idea is to use the graphlet-count statistics.,2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"In a network, we say a path is a “self-avoiding path” if it doesn’t intersect with itself, and a “cycle” if it is a closed path that does not intersect with itself.
",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"Definition 2.1 For 0 ≤ m ≤ n, let Bn,m = ∏m−1 s=0",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
(n− s).,2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"Definition 2.2 For m ≥ 1, we define the “density of lengthm self-avoiding paths” by
L̂m = 1
Bn,m+1 ∑ 1≤i1,...,im+1≤n
i1,··· ,im+1 are distinct
Ai1i2Ai2i3 · · ·Aimim+1 .
",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"and for m ≥ 3, we define the “density of m-cycles” by
Ĉm = 1
Bn,m ∑ 1≤i1,...,im≤n
i1,··· ,im are distinct
Ai1i2Ai2i3 · · ·Aimi1 .
",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"We propose the family of test statistics, called the Graphlet Count (GC) test statistics:
χ̂(m)gc = Ĉm − (L̂m−1/L̂m−2)m, m = 3, 4, . . . .",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"(3)
Remark 2.",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"Using the adjacency matrix A, Ĉm and L̂m can be conveniently computed (e.g., Ĉ4 = 124(n4)
[ tr(A4)−
2(1′nA 21n)",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
+,2. A Class of Graphlet Count (GC) Statistics,[0],[0]
1 ′,2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"nA1n
] ).",2. A Class of Graphlet Count (GC) Statistics,[0],[0]
See supplemental material.,2. A Class of Graphlet Count (GC) Statistics,[0],[0]
"Recall that Ω = ΘΠPΠ′Θ. Let 1n be the n-dimensional vector of 1’s and let θ = (θ1, θ2, . . .",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
", θn)′. We use (·, ·) to denote the inner product of two vectors.",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"The K ×K matrix G ≡ Π′Θ2Π and the vector η ∈ RK play a key role.
",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"Definition 2.3 Denote the vector G−1/2Π′Θ1n by η.
",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
Definition 2.4 For 1 ≤,2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"k ≤ K, let λk be the k-th largest (in absolute value) eigenvalue of G1/2PG1/2, and let ξk be the corresponding eigenvector.
",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"It turns out that λ1, . . .",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
", λK (eigenvalues of G1/2PG1/2) are the nonzero eigenvalues of Ω. The following results, which will be made precise in Theorems 3.1-3.2, play the key role:
nm · Ĉm ≈ tr(Ωm) =",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"K∑ k=1 λmk , (4)
nm+1 · L̂m",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"≈ 1′nΩm1n = K∑ k=1 (η, ξk) 2λmk .",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"(5)
We explain why these equations motivate the test statistic χ̂ (m) gc .",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"Recall that we hope to have a statistic that does not vary too much as θ varies, so first, it is desirable to remove the terms (η, ξk)2, which not only depend on θ but are also not very tractable.",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"Under the alternative hypothesis, it is unclear how to cancel these terms, but under the null hypothesis, K = 1, and the right hand side of (5) reduces to (η, ξ1)
2λm1 , and there are many ways to do the cancellation.",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"One such way is to use the following ratio:
nmL̂m−1 nm−1L̂m−2",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"≈ ∑K k=1(η, ξk) 2λm−1k∑K",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"k=1(η, ξk) 2λm−2k { = λ1, under H (n) 0 ,
≤ λ1, under H(n)1 .",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"(6)
Therefore, at least under H(n)0 , we have managed to cancel the terms (η, ξk)2.
",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"Next, it is also desirable to cancel the term λ1, at least under the null hypothesis.",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"Comparing (4) and (6), there are many ways to do this, and one such way is to use the χ̂(m)gc statistic aforementioned:
nmχ̂(m)gc = n mĈm",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"− [ ( nmL̂m−1)/(n m−1L̂m−2) ]m .
",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"In fact, by (4) and (6), we have that up to a negligible term (i.e., of a smaller order than that of the standard deviation of the statistic under H(n)0 ),
χ̂(m)gc { = 0, under H(n)0 , ≥ 1nm ∑K k=2",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"λ m k , under H (n) 1 .
",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"On the one hand, the effects of degree heterogeneity are largely canceled in the statistic, so it does not vary significantly as the vector θ varies (this is particularly important for we wish to have a rejection region that is relatively insensitive to θ).",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"On the other hand, the statistic χ̂(m)gc is able to differentiate the null hypothesis and the alternative hypothesis, through the term ∑K k=2",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
λ m k .,2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"This suggests that χ̂ (m) gc is a reasonable test statistic.
",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"Note that χ̂(m)gc is only one of many test statistics with the desired properties above, but seemingly one of the simplest.
",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
Remark 3.,2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"Recall that λ1, . . .",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
", λK are the eigenvalues of G1/2PG1/2, and they are also the eigenvalues of PG (nonnegative, irreducible).",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"By Perron’s theorem (Horn & Johnson, 1985), λ1 is positive and λ1 > |λk| for all 2 ≤",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
k ≤,2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"K.
Remark 4.",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"Our tests include the EZ test as a special case (i.e., χ̂(m)gc with m = 3), but our idea is by no means a straightforward extension of that in Gao and Lafferty (2017).",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"The EZ test was derived by calculations that depend heavily on the constraints GL1-GL3 imposed on P , θ, etc., and it was unclear whether the core idea of the EZ test is only valid when these constraints hold, or in more general settings.",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"The GC tests are derived by (4)-(6), where the relationship between the test statistics and θ, η, and the eigenvalues and eigenvectors of G1/2PG1/2 has not been discovered before, even in cases where the constraints GL1-GL3 hold.
",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
Remark 5.,2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
Can we simply use ∑K k=2 λ̂,2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"m k as the test statistic, where λ̂k is the k-th eigenvalue of A?",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"We can not, as K is unknown.",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
Can we simply use λ̂2 as the test statistic?,2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"We can, but the asymptotic distribution of λ̂2 is much harder to derive than that of χ̂(m)gc",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"(which is Gaussian; see below), so it is challenging to determine the rejection region.",2.1. The Key Idea: Why the GC Test Statistics Work,[0],[0]
"In theory, we use n as the driving asymptotic parameter, let the matrices (Θ,Π, P ) change with n, and consider a
sequence of problems where we test H(n)0 : K = 1 vs. H
(n) 1 : K > 1 (K is unknown but does not change with n).",3. Main Results,[0],[0]
Recall node i is pure if πi is degenerate.,3. Main Results,[0],[0]
A pure node can be in any of the K communities.,3. Main Results,[0],[0]
"For 1 ≤ k ≤ K, we let",3. Main Results,[0],[0]
"be the set of all pure nodes in the community k. Assume
‖θ‖ → ∞, ‖θ‖3 → 0.",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"(7)
",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"Since θmax ≤ ‖θ‖3, this implies θmax → 0.",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
Suppose there is a constant c1 > 0,Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"so that for any 1 ≤ k ≤ K,∑
i∈Nk θ 2 i∑n
i=1",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"θ 2 i
≥ c1.",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"(8)
(this roughly says each community has sufficiently many pure nodes).",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"Also, assume for some constant c2 ∈ (0, 1),
all singular values of P fall between c2 and c−12 .",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"(9)
Denote Cm = E[Ĉm] and Lm = E[L̂m] and introduce a non-stochastic counterpart of χ̂(m)gc by
χ(m)gc = Cm − (Lm−1/Lm−2)m.",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"(10)
Theorem 3.1 Consider the DCMM model (1) where (7)-(9) hold.",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"As n→∞,
χ(m)gc = 1
nm { K∑ k=1 λmk",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"− [∑K k=1(η, ξk) 2λm−1k∑K",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"k=1(η, ξk) 2λm−2k",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"]m} +O(n−m‖θ‖44‖θ‖2m−4).
",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
The last term is of a smaller order of the standard deviation of χ̂(m)gc and is thus negligible.,Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"Theorem 3.1 solidifies what we mentioned in Section 2.1 and is proved in Section 6.
",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
Theorem 3.2 Consider the DCMM model (1) where (7)-(9) hold.,Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"As n→∞, for m = 3, 4, under either H(n)0 or H
(n) 1 ,√
Bn,m 2m
· Ĉ−1/2m [ χ̂(m)gc − χ(m)gc ] d→ N(0, 1).
",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"The proof for other fixed m is similar, but significantly more tedious so we leave it as future work.",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
Theorem 3.2 is proved in the supplemental material.,Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"Compared to existing literature, our theorems are for a much broader setting where existing works have very little theory and understanding.
",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
Remark 6.,Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
Conditions (8)-(9) are only for H(n)1 since they naturally hold under H(n)0 ; the conditions are only mild.,Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
Condition (7) is also only mild.,Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"Take the DCMM-RDP model for example (see Remark 1): ‖θ‖ √ nαn and ‖θ‖3 n1/3αn, so Condition (7) requires n−1/2 αn n−1/3.",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"The case αn n−1/3 corresponds to the “strong signal” case, the analysis of which is different and we leave it to the forthcoming manuscript.",Nk = {1 ≤ i ≤ n : πi is degenerate and πi(k) = 1},[0],[0]
"By Theorems 3.1-3.2, we expect to see that in distribution,√ Bn,m 2m · Ĉ−1/2m χ̂(m)gc",3.1. Testing Power,[0],[0]
"≈ N (√ Bn,m 2m C−1/2m χ (m) gc , 1 ) .
",3.1. Testing Power,[0],[0]
"Motivated by Theorem 3.1 and equation (4), we introduce a proxy of √
Bn,m 2m · C −1/2 m χ (m) gc , defined as
δ(m)gc = (2m)−1/2√∑K
k=1",3.1. Testing Power,[0],[0]
"λ m k
[ K∑",3.1. Testing Power,[0],[0]
"k=1 λmk − (∑K k=1(η, ξk) 2λm−1k∑K",3.1. Testing Power,[0],[0]
"k=1(η, ξk) 2λm−2k )",3.1. Testing Power,[0],[0]
"m] ,
(11) and we expect to see that in distribution,√
Bn,m 2m · Ĉ−1/2m χ̂(m)gc",3.1. Testing Power,[0],[0]
"≈ N(δ(m)gc , 1).",3.1. Testing Power,[0],[0]
"(12)
It is noteworthy",3.1. Testing Power,[0],[0]
"that under H(n)0 , δ (m) gc = 0.
Fixing 0",3.1. Testing Power,[0],[0]
"< α < 1, let zα be the (1−α)-quantile ofN(0, 1).",3.1. Testing Power,[0],[0]
"Consider the Graphlet Count (GC) test where we
reject H(n)0 ⇐⇒ √ Bn,m 2m · Ĉ−1/2m χ̂(m)gc > zα.",3.1. Testing Power,[0],[0]
"(13)
",3.1. Testing Power,[0],[0]
"The theorem below is proved in the supplemental material.
",3.1. Testing Power,[0],[0]
Theorem 3.3 Consider the DCMM model (1) where (7)-(9) hold.,3.1. Testing Power,[0],[0]
"As n → ∞, for m = 3, 4, the level and the power of the Graphlet Count test are respectively α + o(1) and Φ ( δ (m) gc",3.1. Testing Power,[0],[0]
− zα ),3.1. Testing Power,[0],[0]
+ o(1).,3.1. Testing Power,[0],[0]
"Moreover, if δ(m)gc →∞ as n→∞, then the power→ 1.",3.1. Testing Power,[0],[0]
"One of the key messages is that, χ̂(3)gc may be powerless for some non-null cases, even when the “signals” are strong and the test is relatively easy.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"In comparison, χ̂(4)gc has successfully avoided such a pitfall.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"Note that translated to our terms, the EZ test by Gao and Lafferty (2017) is χ̂(3)gc .
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"In detail, by Remark 3, λ1 is positive and has the largest absolute value among all λk’s, so
[ K∑ k=1 (η, ξk) 2λm−1k ]/",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"[ K∑ k=1 (η, ξk) 2λm−2k ] ≤ λ1. (14)
It follows that under H(n)1
δ(m)gc ≥ ( K∑ k=2",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"λmk )/[2m K∑ k=1 λmk ] 1/2, (15)
where “=” is achievable; see Section 3.3 for examples.
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"It can be shown that ∑K k=1 λ m k > 0, no matter m is odd or
even.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
The numerator ∑K k=2,3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"λ m k , however, is more tricky.
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"• When m is even, the numerator of (15) is positive.
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"• When m is odd, the numerator of (15) can be negative or 0.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"In fact, δ(m)gc may be 0 under some H (n) 1 , even
when signals are strong; see Section 3.3 for an example.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"If additionally P (and so G1/2PG1/2) is positive definite, then δ(m)gc is positive, either m is odd or even.
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
Corollary 3.1 Consider the DCMM model (1) where (7)- (9) hold and the Graphlet Count test (13).,3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"As n→∞.
• When m = 3, for some configurations of the non-null case, δ(3)gc = 0 and the power of the test is α+ o(1).",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"If additionally the K ×K matrix P is positive definite, then there is a constant c3 > 0",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"such that δ (3) gc ≥ c3‖θ‖3
and so the power of the test & Φ ( c3‖θ‖3− zα ) , which tends to 1 since ‖θ‖ → ∞ in our setting.
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
•,3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"If m = 4, then there is a constant c4 > 0",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"such that δ (4) gc ≥ c4‖θ‖4 and the power of the test& Φ ( c4‖θ‖4−
zα ) , which tends to 1 as ‖θ‖ → ∞ in our setting.
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
See the supplement for the proof.,3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"In comparison, χ̂(3)gc has a two-fold disadvantage: it may lose power in some non-null configurations even when ‖θ‖ is large, and as ‖θ‖ → ∞, its power grows to 1 in a speed slower than that of χ̂(4)gc .
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
Remark 7.,3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"In the most subtle case where ‖θ‖ 1, for any m ≥ 3, χ̂(m)gc has a non-trivial power since the signal to noise ratio δ(m)gc ‖θ‖m.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"The form is reminiscent of the results on likelihood ratio (Mossel et al., 2015) which is unfortunately only for SBM (much narrower than DCMM).
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
Remark 8.,3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"The computational complexity of the GC test is O(nd2) for m = 3 and O(nd3) for m = 4 (Schank & Wagner, 2005), where d is the maximum degree.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"Many large networks are reasonably sparse where d n, and the complexity is reasonably modest in such cases.
",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
Remark 9.,3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"Our work is connected to Maugis et al. (2017), which characterizes the expected number of closed walks.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"However, their work does not study the expected number non-closed walks, and the standard deviations of close and non-closed walks, so how to apply their results to our setting is unclear.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"Our work is also closed to the notion of clustering coefficient (Holland & Leinhardt, 1971; Watts & Strogatz, 1998), which in our notation equals to 3Ĉ3/L̂2.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"To use this as a test, the challenge is how to normalize the statistic properly so the limiting distribution is more tractable.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"Also, the test may lose power in some settings.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
"In Section 3.3, we provide an example where C3/L2 = λ1 + ( ∑K k=2 λ 3 k)/λ 2 1,
so when ∑K k=2 λ 3 k = 0, asymptotically, the test is powerless while the GC test may still have good power.",3.2. Comparison of χ̂(3)gc and χ̂(4)gc,[0],[0]
It is instructive to consider an example where δ(m)gc can be further simplified.,3.3. An Example,[0],[0]
"Consider a setting where • the DCMM-RDP model holds (see Remark 1), i.e.,
(θi/αn) i.i.d.∼",3.3. An Example,[0],[0]
"f , where the 2nd moment of f is 1,
• all πi’s are degenerate, dividing to K equal-size communities,
• the rows of P have an equal sum.
",3.3. An Example,[0],[0]
It follows that approximately: (a).,3.3. An Example,[0],[0]
"‖θ‖ = √ nαn, (b).",3.3. An Example,[0],[0]
G ∝,3.3. An Example,[0],[0]
"the K ×K identity matrix, so ξ1 (the first eigenvector of G1/2PG1/2) is approximately proportional to the vector of ones 1K .",3.3. An Example,[0],[0]
(c).,3.3. An Example,[0],[0]
"η = G−1/2Π′Θ1n ∝ 1K , due to the random model for θi.",3.3. An Example,[0],[0]
"Therefore, (η, ξk) = (ξ1, ξk), which equals to 1 if k = 1 and 0 otherwise, and so by basic linear algebra,
δ(m)gc = (2m)−1/2√∑K
k=1",3.3. An Example,[0],[0]
"λ m k
[ K∑ k=1",3.3. An Example,[0],[0]
"λmk − λm1 ] =
∑K k=2",3.3. An Example,[0],[0]
"λ m k√
2m ∑K k=1 λ m k .
",3.3. An Example,[0],[0]
We now consider a setting where we can spell out λk more explicitly.,3.3. An Example,[0],[0]
"Suppose K is even and the K × K matrix P calibrating the community structure is a 2× 2 block-wise
matrix having the form of P =",3.3. An Example,[0],[0]
"[ D C C D ] , where C,D ∈ RK/2,K/2 and
D = 1 a · · · a... . . .",3.3. An Example,[0],[0]
...,3.3. An Example,[0],[0]
a a · · · 1  and C = b b · · · b... . . .,3.3. An Example,[0],[0]
...,3.3. An Example,[0],[0]
b b · · · b  .,3.3. An Example,[0],[0]
"In this case, λ1 = nα2n K { (1− a) + K2 (a+ b) } , and the other (K− 1) eigenvalues are (which one is λ2 depends on (a, b))
",3.3. An Example,[0],[0]
nα2n K,3.3. An Example,[0],[0]
[(1− a) +,3.3. An Example,[0],[0]
"K 2 (a− b)], nα 2 n K (1− a), . .",3.3. An Example,[0],[0]
.,3.3. An Example,[0],[0]
", nα 2 n K (1− a).
",3.3. An Example,[0],[0]
"If we let AK(a, b) =",3.3. An Example,[0],[0]
"(1− a) + K2 (a− b) and BK(a, b) = (1− a) + K2 (a+ b), recalling ‖θ‖ = √ nαn, then
δ(3)gc = (K− 3 2 ‖θ‖3) ·",3.3. An Example,[0],[0]
"[A3K(a, b) + (K − 2)(1− a)3]√
[A3K(a, b) +B 3 K(a, b) + (K − 2)(1− a)3
,
δ(4)gc = (K−2‖θ‖4) ·",3.3. An Example,[0],[0]
"[A4K(a, b) + (K − 2)(1− a)4]√ A4K(a, b) +B 4 K(a, b) +",3.3. An Example,[0],[0]
"(K − 2)(1− a)4 .
",3.3. An Example,[0],[0]
"Note that δ(4)gc is always positive, but δ (3) gc can be either positive or negative, and in particular,
δ (3) gc = 0 when b = w + (1− w)a, where w = 1+(K−2)
1/3
K/2 .
",3.3. An Example,[0],[0]
Figure 1 compares δ(3)gc,3.3. An Example,[0],[0]
"and δ (4) gc for (K, a) =",3.3. An Example,[0],[0]
"(10, .25) and various (b, ‖θ‖).",3.3. An Example,[0],[0]
"Regardless of ‖θ‖, δ(3)gc gets small when b is close to 0.7.",3.3. An Example,[0],[0]
"However, δ(4)gc has no such issue.",3.3. An Example,[0],[0]
"The lower bound is not discussed here but is studied in the extended version (Jin et al., 2018), where we show that under DCMM, if ‖θ‖ ≤ c for a sufficiently small constant, then the risk (sum of type-I and type-II errors) of any test converges to 1 as n → ∞.",3.4. The Lower Bound,[0],[0]
"See also Massoulié (2014), Mossel (2015), Abbe & Sandon (2016), Gao & Lafferty (2017).",3.4. The Lower Bound,[0],[0]
"Note by Corollary 3.1, if the level α→ 0, then the risk of the GC test→ 0 as ‖θ‖ → ∞. A closely related work is Jin and Ke (2017).",3.4. The Lower Bound,[0],[0]
"Under the DCMM and the assumption of θmax ≤ Cθmin, they showed that when ‖θ‖ 6→ ∞ it is impossible to successfully estimate the mixed memberships.",3.4. The Lower Bound,[0],[0]
"We investigate χ̂(m)gc for m = 3, 4.",4. Simulations,[0],[0]
"Recall that when m = 3, it coincides with the EZ test (Gao & Lafferty, 2017).",4. Simulations,[0],[0]
"The methods (Bickel & Sarkar, 2016; Lei, 2016; Banerjee & Ma, 2017; Wang & Bickel, 2017) are for SBM, which do not apply to our settings and so we skip them for study.
",4. Simulations,[0],[0]
Experiment 1 (checking for asymptotic normality).,4. Simulations,[0],[0]
"Fixing n = 200, we consider a null setting where θi’s are from√
10θi iid∼ Pareto(4, 0.375);1 (note: severe degree heterogeneity!)",4. Simulations,[0],[0]
"We also consider an alternative case where θi’s are from √ 2θi
iid∼ Pareto(4, 0.375), K = 3, and P is the matrix with unit diagonals and all off-diagonals are 1/3.",4. Simulations,[0],[0]
"Among the 200 nodes, 180 are pure with 60 in each community, and the remaining 20 nodes have a mixed membership (1/3, 1/3, 1/3).",4. Simulations,[0],[0]
"The results of 500 repetitions are in Figure 2, which suggest that the claimed asymptotic normality is valid even for relatively small n.
Experiment 2 (power comparison).",4. Simulations,[0],[0]
"Fix (n,K) =",4. Simulations,[0],[0]
"(300, 10).",4. Simulations,[0],[0]
"All nodes are pure with 30 in each commu-
1In Pareto(α, xm), α is for shape, xm is for scale.
",4. Simulations,[0],[0]
Figure 2.,4. Simulations,[0],[0]
"Histograms of
√
(Bn,4/8) ·",4. Simulations,[0],[0]
"Ĉ−1/24 χ̂ (4) gc for the null hy-
pothesis (light blue) and the alternative hypothesis (yellow).",4. Simulations,[0],[0]
"The blue and red curves are densities of N(0, 1) and N(δ(4)gc , 1), respectively, which support our results on asymptotical normality.
nity.",4. Simulations,[0],[0]
"For (a, b) and h > 0, let the matrix P be the same as in Section 3.3.",4. Simulations,[0],[0]
"Set θi = (h/‖θ̃‖)θ̃i, where θ̃i iid∼ Pareto(4, 0.375); we note that ‖θ‖ = h.
For ‖θ‖ ranging in {5, 6, . . .",4. Simulations,[0],[0]
", 10}, we consider three different settings for (a, b), (i)-(iii).",4. Simulations,[0],[0]
See Table 1 for the results.,4. Simulations,[0],[0]
"In (i), (a, b) = (.15, .52) and the second eigenvalue λ2 of G1/2PG1/2 is moderately large, and χ̂(4)gc uniformly outperforms χ̂(3)gc (the EZ test).",4. Simulations,[0],[0]
"In (ii), (a, b) = (.30, .54), λ2 is relatively small and the testing problem is more challenging than (i).",4. Simulations,[0],[0]
"In this case, χ̂(4)gc performs better again.",4. Simulations,[0],[0]
"In (iii), we investigate a case where δ(3)gc = 0 and see how χ̂ (3) gc deals with this most challenging case (the case poses no challenge to χ̂(4)gc as δ (4) gc is always > 0; see Section 3.3).",4. Simulations,[0],[0]
"In this case, χ̂ (3) gc loses power, and significantly underperforms χ̂ (4) gc .
",4. Simulations,[0],[0]
"These results are consistent with our theoretical results, especially those of Section 3.3.",4. Simulations,[0],[0]
"In the college football network (Girvan & Newman, 2002), each node is a Division I-A college team and two nodes have an edge if and only if they played ≥ 1 games during the Fall 2000 season.",5. Application to a Football Network,[0],[0]
There are a total of 115 teams.,5. Application to a Football Network,[0],[0]
"Except for 5 “independent” teams, all teams are manually divided into 11 conferences for administration purposes; we treat these “manually labeled communities” as the ground truth.
",5. Application to a Football Network,[0],[0]
"First, we consider a relatively easy setting where we test whether the whole network has only 1 community or has multiple communities.",5. Application to a Football Network,[0],[0]
"As expected, for both m = 3 and 4, our test χ̂(m)gc rejects the null with extremely small p-values.
",5. Application to a Football Network,[0],[0]
"Next, we consider a more subtle problem, where for each of the 11 manually labeled communities aforementioned, we test whether it can’t be further divided into multiple communities (null) or it can (alternative).",5. Application to a Football Network,[0],[0]
"The results of all 11 testing settings are in Table 2.
",5. Application to a Football Network,[0],[0]
"For the first 10 test settings, despite some differences in the p-values, two tests, χ̂(3)gc and χ̂ (4) gc , agree with each other and both accept the null.",5. Application to a Football Network,[0],[0]
"For the last setting (corresponding to the Western Athletic Conference (WAC)), however, χ̂(4)gc votes for rejection and χ̂(3)gc votes for acceptance.",5. Application to a Football Network,[0],[0]
"It turns out that one team (“BioseState”) in the WAC is an outlier, which did not play any game in the data range.",5. Application to a Football Network,[0],[0]
"After removing the outlier, both tests vote for acceptance.",5. Application to a Football Network,[0],[0]
"These results are consistent with the ground truth, suggesting (a) both tests yield reasonable testing results even for small-size networks, and (b) χ̂(4)gc is more effective in detecting outliers.
",5. Application to a Football Network,[0],[0]
"m χ̂
(m)
gc and corre-",5. Application to a Football Network,[0],[0]
"First, we show that for any m ≥ 3,
Bn,mCm = K∑ k=1 λmk +O ( ‖θ‖44‖θ‖2m−4 ) , (16)
and for any m ≥ 1, Bn,m+1Lm equals to K∑ k=1 λmk (η ′ξk) 2 +O ( ‖θ‖21‖θ‖44‖θ‖2m−6 ) .",6. Proof of Theorem 3.1,[0],[0]
"(17)
Consider (16).",6. Proof of Theorem 3.1,[0],[0]
"By definition, Bn,mCm = Bn,mE[Ĉm] =∑ i1,...,im E[Ai1i2 · · ·Aimi1 ] = ∑ i1,...,im
Ωi1i2 · · ·Ωimi1 , where the sum is over distinct indices i1, ..., im. As a result,
Bn,mCm = tr(Ω m)− ∑ non-distinct i1,...,im Ωi1i2Ωi2i3 · · ·Ωimi1 .
",6. Proof of Theorem 3.1,[0],[0]
We calculate the term tr(Ωm).,6. Proof of Theorem 3.1,[0],[0]
"From the DCMM model, Ω = ΘΠPΠ′Θ.",6. Proof of Theorem 3.1,[0],[0]
"It follows that
Ωm = ΘΠP (Π′Θ2Π)P (Π′Θ2Π) · · ·P (Π′Θ2Π)PΠ′Θ
= ΘΠ(PGPG · · ·PGP )Π′Θ = (ΘΠG−1/2)(G1/2PG1/2)m(G−1/2Π′Θ).
",6. Proof of Theorem 3.1,[0],[0]
"For any matrices A and B, tr(AB) = tr(BA).",6. Proof of Theorem 3.1,[0],[0]
"As a result, tr(Ωm) = tr [ (P 1/2GP 1/2)m(G−1/2Π′Θ)(ΘΠG−1/2) ] = tr [ (P 1/2GP 1/2)m
] = ∑ k λmk .",6. Proof of Theorem 3.1,[0],[0]
"(18)
We then bound the remainder term.",6. Proof of Theorem 3.1,[0],[0]
Note that Ωij = θiθj(π ′ iPπj) ≤,6. Proof of Theorem 3.1,[0],[0]
"Cθiθj , where the last inequality is from Condition (9).",6. Proof of Theorem 3.1,[0],[0]
"Hence,∑ non-distinct i1,...,im Ωi1i2Ωi2i3 · · ·Ωimi1 ≤ ∑ non-distinct i1,...,im Cθ2i1θ 2 i2 · · · θ 2 im
≤ ∑
i1,...,im−1
Cθ4i1θ 2 i2 · · · θ 2 im−1 ≤ C‖θ‖ 4 4‖θ‖2(m−2).
",6. Proof of Theorem 3.1,[0],[0]
"Combining the above gives (16).
",6. Proof of Theorem 3.1,[0],[0]
Consider (17).,6. Proof of Theorem 3.1,[0],[0]
"Similarly, we have
Bn,m+1Lm = 1 ′",6. Proof of Theorem 3.1,[0],[0]
"nΩ m1n− ∑
non-distinct i1,...,im+1
Ωi1i2Ωi2i3 · · ·Ωimim+1 .
",6. Proof of Theorem 3.1,[0],[0]
"Since Ω = ΘΠPΠ′Θ, it follows that
1′nΩ m1n = 1 ′",6. Proof of Theorem 3.1,[0],[0]
nΘΠP (Π ′Θ2Π)P (Π′Θ2Π) · · ·PΠ′Θ1n = 1′nΘΠ(PGPG · · ·P )Π′Θ1n,6. Proof of Theorem 3.1,[0],[0]
"= 1′nΘΠG −1/2(G1/2PG1/2)mG−1/2Π′Θ1n
= η′(G1/2PG1/2)mη.
",6. Proof of Theorem 3.1,[0],[0]
The eigen-decomposition G1/2PG1/2 = ∑K k=1 λkξkξ ′,6. Proof of Theorem 3.1,[0],[0]
"k im-
plies that (G1/2PG1/2)m = ∑ k=1 λ m k ξkξ ′",6. Proof of Theorem 3.1,[0],[0]
k.,6. Proof of Theorem 3.1,[0],[0]
"As a result,
1′nΩ m1n = η ′[ K∑ k=1 λmk ξkξ ′",6. Proof of Theorem 3.1,[0],[0]
k ],6. Proof of Theorem 3.1,[0],[0]
"η = K∑ k=1 λmk (η ′ξk) 2. (19)
We then bound the remainder term.",6. Proof of Theorem 3.1,[0],[0]
"Since Ωij ≤ Cθiθj , Ωi1i2 · · ·Ωimim+1 ≤ Cθi1θim+1θ2i2 · · · θ 2 im
.",6. Proof of Theorem 3.1,[0],[0]
"It follows that∑ non-distinct i1,...,im+1 Ωi1i2Ωi2i3 · · ·Ωimim+1
≤ ( ∑ i1=im+1 + ∑ i2=im+1 + ∑
non-distinct i2,...,im
)",6. Proof of Theorem 3.1,[0],[0]
"Cθi1θim+1θ 2 i2 · · · θ 2 im
≤ ∑
i1,...,im
Cθ2i1 · · · θ 2 im + ∑ i1,...,im Cθi1θ 3 i2θ 2 i3 · · · θ 2 im
+ ∑
i1,...,im−1,im+1
θi1θim+1θ 4 i2θ 2 i3 · · · θ 2 im−1
≤C",6. Proof of Theorem 3.1,[0],[0]
"[ ‖θ‖2m + ‖θ‖1‖θ‖33‖θ‖2(m−2) + ‖θ‖21‖θ‖44‖θ‖2(m−3) ]
≤C‖θ‖2(m−3) ( ‖θ‖6 + ‖θ‖1‖θ‖33‖θ‖2 + ‖θ‖21‖θ‖44 ) .
",6. Proof of Theorem 3.1,[0],[0]
We need to compare the three terms in the brackets.,6. Proof of Theorem 3.1,[0],[0]
"First, applying Holder’s inequality with p = 3 and q = 3/2,
we have ∑ i θ 2 i = ∑ i θ 4 3 i θ 2 3 i ≤ ( ∑ i θ 4p 3 i ) 1 p",6. Proof of Theorem 3.1,[0],[0]
( ∑ i θ 2q 3 i ) 1 q .,6. Proof of Theorem 3.1,[0],[0]
It implies ‖θ‖2 ≤ ‖θ‖4/34 ‖θ‖ 2/3 1 .,6. Proof of Theorem 3.1,[0],[0]
"As a result, ‖θ‖6 ≤ ‖θ‖44‖θ‖21.",6. Proof of Theorem 3.1,[0],[0]
This means the first term above is dominated by the last term.,6. Proof of Theorem 3.1,[0],[0]
"Second, by Cauchy-Schwartz inequality, ∑ i θ 3 i ≤
( ∑ i θ 2 i ) 1/2( ∑ i θ 4 i )
1/2, which means ‖θ‖33 ≤ ‖θ‖‖θ‖24.",6. Proof of Theorem 3.1,[0],[0]
"As a result, ‖θ‖1‖θ‖33‖θ‖2 ≤ ‖θ‖1‖θ‖24‖θ‖3.",6. Proof of Theorem 3.1,[0],[0]
"Furthermore, we have proved ‖θ‖3 ≤ ‖θ‖24‖θ‖1.",6. Proof of Theorem 3.1,[0],[0]
Combining the above gives ‖θ‖1‖θ‖33‖θ‖2 ≤ ‖θ‖21‖θ‖44.,6. Proof of Theorem 3.1,[0],[0]
"Hence, the second term is dominated by the last term.",6. Proof of Theorem 3.1,[0],[0]
"In summary, the remainder term is O(‖θ‖21‖θ‖44‖θ‖2(m−3)).",6. Proof of Theorem 3.1,[0],[0]
"This proves (17).
",6. Proof of Theorem 3.1,[0],[0]
"Next, we use (16)-(17) to show the claim.",6. Proof of Theorem 3.1,[0],[0]
"Write for short
χ (m) gc,0 =
1
Bn,m {∑ k λmk",6. Proof of Theorem 3.1,[0],[0]
"− [∑ k(η, ξk) 2λm−1k∑ k(η, ξk) 2λm−2k",6. Proof of Theorem 3.1,[0],[0]
"]m} .
",6. Proof of Theorem 3.1,[0],[0]
"Write C̃m = Bn,mCm, C̃0m = tr(Ω m), L̃m = Bn,m+1Lm, and L̃0m = 1 ′",6. Proof of Theorem 3.1,[0],[0]
"nΩ m1n, for all m. By definition and (18)-(19),
Bn,mχ (m) gc,0 = C̃ 0",6. Proof of Theorem 3.1,[0],[0]
m,6. Proof of Theorem 3.1,[0],[0]
"− (L̃0m−1/L̃0m−2)m,
Bn,mχ (m) gc = Bn,m[Cm",6. Proof of Theorem 3.1,[0],[0]
"− (Lm−1/Lm−2)m]
= C̃m − (Bn,m−1)",6. Proof of Theorem 3.1,[0],[0]
"m
(Bn,m)m−1 · (L̃m−1/L̃m−2)m.
",6. Proof of Theorem 3.1,[0],[0]
"As a result,
Bn,m|χ(m)gc − χ (m) gc,0| ≤",6. Proof of Theorem 3.1,[0],[0]
|C̃m − C̃0m|+ ∣∣∣,6. Proof of Theorem 3.1,[0],[0]
"(L̃m−1)m (L̃m−2)m − (L̃ 0 m−1) m (L̃0m−2) m ∣∣∣ + (L̃m−1) m
(L̃m−2)m ∣∣∣ (Bn,m−1)m(Bn,m)m−1 − 1∣∣∣ ≡",6. Proof of Theorem 3.1,[0],[0]
I1 + I2 + I3.,6. Proof of Theorem 3.1,[0],[0]
"(20)
We now bound these three terms.",6. Proof of Theorem 3.1,[0],[0]
"By (16)-(17),
|C̃m − C̃0m| = O(‖θ‖44‖θ‖2m−4), |L̃m − L̃0m| = O(‖θ‖21‖θ‖44‖θ‖2m−6).",6. Proof of Theorem 3.1,[0],[0]
"(21)
",6. Proof of Theorem 3.1,[0],[0]
"Hence, I1 = O(‖θ‖44‖θ‖2m−4).
",6. Proof of Theorem 3.1,[0],[0]
"To bound I2, we need the following lemma, which is proved in the supplemental material.
",6. Proof of Theorem 3.1,[0],[0]
Lemma 6.1,6. Proof of Theorem 3.1,[0],[0]
"Under conditions of Theorem 3.3, |λk| ‖θ‖2 for 1 ≤ k ≤ K, and max1≤k≤K |η′ξk| ‖θ‖−1‖θ‖1.
",6. Proof of Theorem 3.1,[0],[0]
"By (19), L̃0m = ∑ k(η ′ξk)
2λmk .",6. Proof of Theorem 3.1,[0],[0]
"For m even, it then follows from Lemma 6.1 that
L̃0m ≥ c‖θ‖2m−2‖θ‖21 (22)
for a constant c > 0.",6. Proof of Theorem 3.1,[0],[0]
"For m odd, this is still true; see the proof of Lemma B.1 in the supplemental material.",6. Proof of Theorem 3.1,[0],[0]
"Additionally, by Cauchy-Schwarz inequality, ‖θ‖4 ≤ n‖θ‖44; hence, for m ≤ 3, |L̃m − L̃0m| is negligible compared to the order of L̃0m, so we also have L̃m ≥ c‖θ‖2m−2‖θ‖21.",6. Proof of Theorem 3.1,[0],[0]
"Now,∣∣∣ L̃0m−1",6. Proof of Theorem 3.1,[0],[0]
L̃0m−2,6. Proof of Theorem 3.1,[0],[0]
− L̃m−1 L̃m−2 ∣∣∣ ≤,6. Proof of Theorem 3.1,[0],[0]
|L̃m−1−L̃0m−1| L̃0m−2,6. Proof of Theorem 3.1,[0],[0]
"+ L̃m−1 L̃m−2 |L̃m−2−L̃0m−2| L̃0m−2
= O (‖θ‖21‖θ‖44‖θ‖2m−8 ‖θ‖2m−6‖θ‖21 )",6. Proof of Theorem 3.1,[0],[0]
+,6. Proof of Theorem 3.1,[0],[0]
O ( ‖θ‖2,6. Proof of Theorem 3.1,[0],[0]
"‖θ‖ 2 1‖θ‖44‖θ‖2m−10 ‖θ‖2m−6‖θ‖21 ) = O(‖θ‖44‖θ‖−2).
",6. Proof of Theorem 3.1,[0],[0]
"Since |xm − ym| ≤ C|x− y|(|x|+ |y|)m−1,
I2 ≤ C ∣∣∣ L̃0m−1 L̃0m−2",6. Proof of Theorem 3.1,[0],[0]
"− L̃m−1 L̃m−2 ∣∣∣ · ∣∣∣ L̃0m−1 L̃0m−2 ∣∣∣m−1 = O(‖θ‖44‖θ‖−2 · ‖θ‖2m−2) = O(‖θ‖44‖θ‖2m−4).
",6. Proof of Theorem 3.1,[0],[0]
Consider I3.,6. Proof of Theorem 3.1,[0],[0]
"Note that (Bn,m−1)
m (Bn,m)m−1 = n(n−1)···(n−m+2)(n−m+1)m−1 =∏m−1
j=1 (1 + j n−m+1 )",6. Proof of Theorem 3.1,[0],[0]
= 1,6. Proof of Theorem 3.1,[0],[0]
+O( 1 n ).,6. Proof of Theorem 3.1,[0],[0]
"So,
I3 = O(‖θ‖2m · n−1) = O(‖θ‖44‖θ‖2m−4),
where we have used the universal inequality ‖θ‖4 ≤ n‖θ‖44.
",6. Proof of Theorem 3.1,[0],[0]
"We plug the above results into (20) and note that Bn,m ∼ nm.",6. Proof of Theorem 3.1,[0],[0]
The claim follows immediately.,6. Proof of Theorem 3.1,[0],[0]
We consider a hard testing problem in the rather general DCMM model where the challenge is severe degree heterogeneity.,7. Conclusion,[0],[0]
"We discover a systematic way to cancel the effects of degree heterogeneity, and propose a family of tests, with careful analysis and numerical support.",7. Conclusion,[0],[0]
"Compared to literature, our tests have competitive powers and are applicable in much broader settings.",7. Conclusion,[0],[0]
Our theory is also for very broad settings where existing works have very limited understanding.,7. Conclusion,[0],[0]
"We point out an unappealing feature of the EZ test (Gao & Lafferty, 2017), and shows a new test in our family has successfully overcome the problem that the EZ test faces.
",7. Conclusion,[0],[0]
"In our theorems, we require K to be fixed, but the results continue to hold if K → ∞ reasonably slowly.",7. Conclusion,[0],[0]
"We also require the singular values of P are in the same order, but this is mostly for simplicity in presentation and can be replaced by weaker conditions.",7. Conclusion,[0],[0]
We also assume ‖θ‖3 → 0.,7. Conclusion,[0],[0]
"The case ‖θ‖3 →∞ is related to the “dense network” case, the analysis of which can be done but is different and we leave it as future work.",7. Conclusion,[0],[0]
Consider a large social network with possibly severe degree heterogeneity and mixedmemberships.,abstractText,[0],[0]
We are interested in testing whether the network has only one community or there are more than one communities.,abstractText,[0],[0]
"The problem is known to be non-trivial, partially due to the presence of severe degree heterogeneity.",abstractText,[0],[0]
"We construct a class of test statistics using the numbers of short paths and short cycles, and the key to our approach is a general framework for canceling the effects of degree heterogeneity.",abstractText,[0],[0]
The tests compare favorably with existing methods.,abstractText,[0],[0]
We support our methods with careful analysis and numerical study with simulated data and a real data example.,abstractText,[0],[0]
Network Global Testing by Counting Graphlets,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 146–157 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1014
Neural AMR: Sequence-to-Sequence Models for Parsing and Generation
Ioannis Konstas† Srinivasan Iyer† Mark Yatskar† Yejin Choi† Luke Zettlemoyer†‡
†Paul G. Allen School of Computer Science & Engineering, Univ. of Washington, Seattle, WA {ikonstas,sviyer,my89,yejin,lsz}@cs.washington.edu
‡Allen Institute for Artificial Intelligence, Seattle, WA lukez@allenai.org
Abstract
Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the nonsequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions.",text,[0],[0]
Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text.,1 Introduction,[0],[0]
"As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text.",1 Introduction,[0],[0]
"AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016).",1 Introduction,[0],[0]
"While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use
Obama was elected and his voters celebrated
of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016).
",1 Introduction,[0],[0]
"In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation.",1 Introduction,[0],[0]
"Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015).",1 Introduction,[0],[0]
"However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges.",1 Introduction,[0],[0]
"We show that these challenges can be easily overcome, by demonstrating that seq2seq models can be trained using any graph-isomorphic linearization and that unlabeled text can be used to significantly reduce sparsity.
",1 Introduction,[0],[0]
Our approach is two-fold.,1 Introduction,[0],[0]
"First, we introduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator.",1 Introduction,[0],[0]
"More concretely, first we use self-training to
146
bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences (Napoles et al., 2012) and then use the automatically parsed AMR graphs to pre-train an AMR generator.",1 Introduction,[0],[0]
"This paired training allows both the parser and generator to learn high quality representations of fluent English text from millions of weakly labeled examples, that are then fine-tuned using human annotated AMR data.
",1 Introduction,[0],[0]
"Second, we propose a preprocessing procedure for the AMR graphs, which includes anonymizing entities and dates, grouping entity categories, and encoding nesting information in concise ways, as illustrated in Figure 2(d).",1 Introduction,[0],[0]
This preprocessing procedure helps overcoming the data sparsity while also substantially reducing the complexity of the AMR graphs.,1 Introduction,[0],[0]
"Under such a representation, we show that any depth first traversal of the AMR is an effective linearization, and it is even possible to use a different random order for each example.
",1 Introduction,[0],[0]
Experiments on the LDC2015E86 AMR corpus (SemEval-2016 Task 8) demonstrate the effectiveness of the overall approach.,1 Introduction,[0],[0]
"For parsing, we are able to obtain competitive performance of 62.1 SMATCH without using any external annotated examples other than the output of a NER system, an improvement of over 10 points relative to neural models with a comparable setup.",1 Introduction,[0],[0]
"For generation, we substantially outperform previous best results, establishing a new state of the art of 33.8 BLEU.",1 Introduction,[0],[0]
"We also provide extensive ablative and qualitative analysis, quantifying the contributions that come from preprocessing and the paired training procedure.",1 Introduction,[0],[0]
Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm.,2 Related Work,[0],[0]
Zhou et al. (2016) extend JAMR by performing the concept and relation identification tasks jointly with an incremental model.,2 Related Work,[0],[0]
Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules.,2 Related Work,[0],[0]
"In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities.
",2 Related Work,[0],[0]
"Grammar-based Parsing Wang et al. (2016) (CAMR) perform a series of shift-reduce transformations on the output of an externally-trained dependency parser, similar to Damonte et al. (2017),
Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016).",2 Related Work,[0],[0]
"Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016).",2 Related Work,[0],[0]
"Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources.",2 Related Work,[0],[0]
"Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities.
",2 Related Work,[0],[0]
"Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017).",2 Related Work,[0],[0]
"Similar to our approach, Peng et al. (2017) deal with sparsity by anonymizing named entities and typing low frequency words, resulting in a very compact vocabulary (2k tokens).",2 Related Work,[0],[0]
"However, we avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6).
",2 Related Work,[0],[0]
AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system.,2 Related Work,[0],[0]
"Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder.",2 Related Work,[0],[0]
"Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order.",2 Related Work,[0],[0]
"Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph.",2 Related Work,[0],[0]
"Finally, all three systems intersect with a large language model trained on Gigaword.",2 Related Work,[0],[0]
"We show that our seq2seq model has the capacity to learn the same information as a language model, especially after pretraining on the external corpus.
Data Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016).",2 Related Work,[0],[0]
"They improve neural MT performance for low resource language pairs by using a back-translation MT system for a large monolingual corpus of the target language in order to create synthetic output,
and mixing it with the human translations.",2 Related Work,[0],[0]
"We instead pre-train on the external corpus first, and then fine-tune on the original dataset.",2 Related Work,[0],[0]
"In this section, we first provide the formal definition of AMR parsing and generation (section 3.1).",3 Methods,[0],[0]
"Then we describe the sequence-to-sequence models we use (section 3.2), graph-to-sequence conversion (section 3.3), and our paired training procedure (section 3.4).",3 Methods,[0],[0]
We assume access to a training dataset D where each example pairs a natural language sentence s with an AMR a. The AMR is a rooted directed acylical graph.,3.1 Tasks,[0],[0]
"It contains nodes whose names correspond to sense-identified verbs, nouns, or AMR specific concepts, for example elect.01, Obama, and person in Figure 1.",3.1 Tasks,[0],[0]
"One of these nodes is a distinguished root, for example, the node and in Figure 1.",3.1 Tasks,[0],[0]
"Furthermore, the graph contains labeled edges, which correspond to PropBank-style (Palmer et al., 2005) semantic roles for verbs or other relations introduced for AMR, for example, arg0 or op1 in Figure 1.",3.1 Tasks,[0],[0]
"The set of node and edge names in an AMR graph is drawn from a set of tokens C, and every word in a sentence is drawn from a vocabulary W .
",3.1 Tasks,[0],[0]
"We study the task of training an AMR parser, i.e., finding a set of parameters θP for model f , that predicts an AMR graph â, given a sentence s:
â = argmax a
f ( a|s; θP ) (1)
We also consider the reverse task, training an AMR generator by finding a set of parameters θG, for a model f that predicts a sentence ŝ, given an AMR graph a:
ŝ = argmax s
f ( s|a; θG )",3.1 Tasks,[0],[0]
"(2)
In both cases, we use the same family of predictors f , sequence-to-sequence models that use global attention, but the models have independent parameters, θP and θG.",3.1 Tasks,[0],[0]
"For both tasks, we use a stacked-LSTM sequenceto-sequence neural architecture employed in neural machine translation (Bahdanau et al., 2015; Wu
et al.,",3.2 Sequence-to-sequence Model,[0],[0]
"2016).1 Our model uses a global attention decoder and unknown word replacement with small modifications (Luong et al., 2015).
",3.2 Sequence-to-sequence Model,[0],[0]
The model uses a stacked bidirectional-LSTM encoder to encode an input sequence and a stacked LSTM to decode from the hidden states produced by the encoder.,3.2 Sequence-to-sequence Model,[0],[0]
"We make two modifications to the encoder: (1) we concatenate the forward and backward hidden states at every level of the stack instead of at the top of the stack, and (2) introduce dropout in the first layer of the encoder.",3.2 Sequence-to-sequence Model,[0],[0]
The decoder predicts an attention vector over the encoder hidden states using previous decoder states.,3.2 Sequence-to-sequence Model,[0],[0]
The attention is used to weigh the hidden states of the encoder and then predict a token in the output sequence.,3.2 Sequence-to-sequence Model,[0],[0]
"The weighted hidden states, the decoded token, and an attention signal from the previous time step (input feeding) are then fed together as input to the next decoder state.",3.2 Sequence-to-sequence Model,[0],[0]
"The decoder can optionally choose to output an unknown word symbol, in which case the predicted attention is used to copy a token directly from the input sequence into the output sequence.",3.2 Sequence-to-sequence Model,[0],[0]
Our seq2seq models require that both the input and target be presented as a linear sequence of tokens.,3.3 Linearization,[0],[0]
We define a linearization order for an AMR graph as any sequence of its nodes and edges.,3.3 Linearization,[0],[0]
A linearization is defined as (1) a linearization order and (2) a rendering function that generates any number of tokens when applied to an element in the linearization order (see Section 4.2 for implementation details).,3.3 Linearization,[0],[0]
"Furthermore, for parsing, a valid AMR graph must be recoverable from the linearization.",3.3 Linearization,[0],[0]
Obtaining a corpus of jointly annotated pairs of sentences and AMR graphs is expensive and current datasets only extend to thousands of examples.,3.4 Paired Training,[0],[0]
Neural sequence-to-sequence models suffer from sparsity with so few training pairs.,3.4 Paired Training,[0],[0]
"To reduce the effect of sparsity, we use an external unannotated corpus of sentences Se, and a procedure which pairs the training of the parser and generator.
",3.4 Paired Training,[0],[0]
"Our procedure is described in Algorithm 1, and first trains a parser on the datasetD of pairs of sentences and AMR graphs.",3.4 Paired Training,[0],[0]
"Then it uses self-training
1We extended the Harvard NLP seq2seq framework from http://nlp.seas.harvard.edu/code.
Algorithm 1 Paired Training Procedure Input: Training set of sentences and AMR graphs (s, a) ∈
D, an unannotated external corpus of sentences Se, a number of self training iterations,N , and an initial sample size k. Output: Model parameters for AMR parser θP and AMR generator θG.
1: θP ← Train parser on D .",3.4 Paired Training,[0],[0]
Self-train AMR parser.,3.4 Paired Training,[0],[0]
2: S1e ← sample k sentences from Se 3: for i = 1 to N do 4:,3.4 Paired Training,[0],[0]
"Aie ← Parse Sie using parameters θP
.",3.4 Paired Training,[0],[0]
Pre-train AMR parser.,3.4 Paired Training,[0],[0]
"5: θP ← Train parser on (Aie, Sie)
.",3.4 Paired Training,[0],[0]
Fine tune AMR parser.,3.4 Paired Training,[0],[0]
6: θP ← Train parser on D with initial parameters θP 7: Si+1e ← sample k · 10i,3.4 Paired Training,[0],[0]
"new sentences from Se 8: end for 9: SNe ← sample k · 10N new sentences from Se
.",3.4 Paired Training,[0],[0]
Pre-train AMR generator.,3.4 Paired Training,[0],[0]
10:,3.4 Paired Training,[0],[0]
Ae ← Parse SNe using parameters θP 11: θG,3.4 Paired Training,[0],[0]
"← Train generator on (ANe , SNe )
.",3.4 Paired Training,[0],[0]
Fine tune AMR generator.,3.4 Paired Training,[0],[0]
12: θG,3.4 Paired Training,[0],[0]
"← Train generator on D using initial parameters θG 13: return θP , θG
to improve the initial parser.",3.4 Paired Training,[0],[0]
"Every iteration of self-training has three phases: (1) parsing samples from a large, unlabeled corpus Se, (2) creating a new set of parameters by training on Se, and (3) fine-tuning those parameters on the original paired data.",3.4 Paired Training,[0],[0]
"After each iteration, we increase the size of the sample from Se by an order of magnitude.",3.4 Paired Training,[0],[0]
"After we have the best parser from self-training, we use it to label AMRs for Se and pre-train the generator.",3.4 Paired Training,[0],[0]
The final step of the procedure fine-tunes the generator on the original dataset D.,3.4 Paired Training,[0],[0]
"We use a series of preprocessing steps, including AMR linerization, anonymization, and other modifications we make to sentence-graph pairs.",4 AMR Preprocessing,[0],[0]
"Our methods have two goals: (1) reduce the complexity of the linearized sequences to make learning easier while maintaining enough original information, and (2) address sparsity from certain open class vocabulary entries, such as named entities (NEs) and quantities.",4 AMR Preprocessing,[0],[0]
"Figure 2(d) contains example inputs and outputs with all of our preprocessing techniques.
",4 AMR Preprocessing,[0],[0]
"Graph Simplification In order to reduce the overall length of the linearized graph, we first remove variable names and the instance-of relation ( / ) before every concept.",4 AMR Preprocessing,[0],[0]
In case of re-entrant nodes we replace the variable mention with its co-referring concept.,4 AMR Preprocessing,[0],[0]
"Even though this replacement incurs loss of information, often the
surrounding context helps recover the correct realization, e.g., the possessive role :poss in the example of Figure 1 is strongly correlated with the surface form his.",4 AMR Preprocessing,[0],[0]
Following Pourdamghani et al. (2016) we also remove senses from all concepts for AMR generation only.,4 AMR Preprocessing,[0],[0]
Figure 2(a) contains an example output after this stage.,4 AMR Preprocessing,[0],[0]
"Open-class types including NEs, dates, and numbers account for 9.6% of tokens in the sentences of the training corpus, and 31.2% of vocabulary W .",4.1 Anonymization of Named Entities,[0],[0]
83.4% of them occur fewer than 5 times in the dataset.,4.1 Anonymization of Named Entities,[0],[0]
"In order to reduce sparsity and be able to account for new unseen entities, we perform extensive anonymization.
",4.1 Anonymization of Named Entities,[0],[0]
"First, we anonymize sub-graphs headed by one of AMR’s over 140 fine-grained entity types that contain a :name role.",4.1 Anonymization of Named Entities,[0],[0]
"This captures structures referring to entities such as person, country, miscellaneous entities marked with *-enitity, and typed numerical values, *-quantity.",4.1 Anonymization of Named Entities,[0],[0]
We exclude date entities (see the next section).,4.1 Anonymization of Named Entities,[0],[0]
"We then replace these sub-graphs with a token indicating fine-grained type and an index, i, indicating it is the ith occurrence of that type.2",4.1 Anonymization of Named Entities,[0],[0]
"For example, in Figure 2 the sub-graph headed by country gets replaced with country 0.
",4.1 Anonymization of Named Entities,[0],[0]
"On the training set, we use alignments obtained using the JAMR aligner (Flanigan et al., 2014) and the unsupervised aligner of Pourdamghani et al. (2014) in order to find mappings of anonymized subgraphs to spans of text and replace mapped text with the anonymized token that we inserted into the AMR graph.",4.1 Anonymization of Named Entities,[0],[0]
We record this mapping for use during testing of generation models.,4.1 Anonymization of Named Entities,[0],[0]
"If a generation model predicts an anonymization token, we find the corresponding token in the AMR graph and replace the model’s output with the most frequent mapping observed during training for the entity name.",4.1 Anonymization of Named Entities,[0],[0]
"If the entity was never observed, we copy its name directly from the AMR graph.
",4.1 Anonymization of Named Entities,[0],[0]
"Anonymizing Dates For dates in AMR graphs, we use separate anonymization tokens for year, month-number, month-name, day-number and day-name, indicating whether the date is mentioned by word or by number.3 In AMR gener-
2In practice we only used three groups of ids: a different one for NEs, dates and constants/numbers.
",4.1 Anonymization of Named Entities,[0],[0]
"3We also use three date format markers that appear in the text as: YYYYMMDD, YYMMDD, and YYYY-MM-DD.
ation, we render the corresponding format when predicted.",4.1 Anonymization of Named Entities,[0],[0]
"Figure 2(b) contains an example of all preprocessing up to this stage.
",4.1 Anonymization of Named Entities,[0],[0]
"Named Entity Clusters When performing AMR generation, each of the AMR fine-grained entity types is manually mapped to one of the four coarse entity types used in the Stanford NER system (Finkel et al., 2005): person, location, organization and misc.",4.1 Anonymization of Named Entities,[0],[0]
This reduces the sparsity associated with many rarely occurring entity types.,4.1 Anonymization of Named Entities,[0],[0]
"Figure 2 (c) contains an example with named entity clusters.
NER for Parsing When parsing, we must normalize test sentences to match our anonymized training data.",4.1 Anonymization of Named Entities,[0],[0]
"To produce fine-grained named entities, we run the Stanford NER system and first try to replace any identified span with a fine-grained category based on alignments observed during training.",4.1 Anonymization of Named Entities,[0],[0]
"If this fails, we anonymize the sentence using the coarse categories predicted by the NER system, which are also categories in AMR.",4.1 Anonymization of Named Entities,[0],[0]
"After parsing, we deterministically generate AMR for anonymizations using the corresponding text span.",4.1 Anonymization of Named Entities,[0],[0]
Linearization Order,4.2 Linearization,[0],[0]
"Our linearization order is defined by the order of nodes visited by depth first search, including backward traversing steps.",4.2 Linearization,[0],[0]
"For example, in Figure 2, starting at meet the order contains meet, :ARG0, person, :ARG1-of, expert, :ARG2-of,
group, :ARG2-of, :ARG1-of, :ARG0.4 The order traverses children in the sequence they are presented in the AMR.",4.2 Linearization,[0],[0]
"We consider alternative orderings of children in Section 7 but always follow the pattern demonstrated above.
",4.2 Linearization,[0],[0]
Rendering Function,4.2 Linearization,[0],[0]
"Our rendering function marks scope, and generates tokens following the pre-order traversal of the graph: (1) if the element is a node, it emits the type of the node.",4.2 Linearization,[0],[0]
"(2) if the element is an edge, it emits the type of the edge and then recursively emits a bracketed string for the (concept) node immediately after it.",4.2 Linearization,[0],[0]
"In case the node has only one child we omit the scope markers (denoted with left “(”, and right “)” parentheses), thus significantly reducing the number of generated tokens.",4.2 Linearization,[0],[0]
Figure 2(d) contains an example showing all of the preprocessing techniques and scope markers that we use in our full model.,4.2 Linearization,[0],[0]
"We conduct all experiments on the AMR corpus used in SemEval-2016 Task 8 (LDC2015E86), which contains 16,833/1,368/1,371 train/dev/test examples.",5 Experimental Setup,[0],[0]
"For the paired training procedure of Algorithm 1, we use Gigaword as our external corpus and sample sentences that only contain words from the AMR corpus vocabulary W .",5 Experimental Setup,[0],[0]
We subsampled the original sentence to ensure there is no overlap with the AMR training or test sets.,5 Experimental Setup,[0],[0]
"Table 2
4Sense, instance-of and variable information has been removed at the point of linearization.
summarizes statistics about the original dataset and the extracted portions of Gigaword.",5 Experimental Setup,[0],[0]
"We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002)5.
",5 Experimental Setup,[0],[0]
We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 – line 1).,5 Experimental Setup,[0],[0]
"We searched over the set {128, 256, 500, 1024} for the best combinations of sizes and set both to 500.",5 Experimental Setup,[0],[0]
"Models were trained by optimizing cross-entropy loss with stochastic gradient descent, using a batch size of 100 and dropout rate of 0.5.",5 Experimental Setup,[0],[0]
"Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8.
",5 Experimental Setup,[0],[0]
"For the initial parser trained on the AMR corpus, (Algorithm 1 – line 1), we use a single stack version of our model, set initial learning rate to 0.5 and train for 60 epochs, taking the best performing model on the development set.",5 Experimental Setup,[0],[0]
"All subsequent models benefited from increased depth and we used 2-layer stacked versions, maintaining the same embedding sizes.",5 Experimental Setup,[0],[0]
"We set the initial Gigaword sample size to k = 200, 000 and executed a maximum of 3 iterations of self-training.",5 Experimental Setup,[0],[0]
"For pretraining the parser and generator, (Algorithm 1 – lines 4 and 9), we used an initial learning rate of 1.0, and ran for 20 epochs.",5 Experimental Setup,[0],[0]
"We attempt to fine-tune the parser and generator, respectively, after every epoch of pre-training, setting the initial learning rate to 0.1.",5 Experimental Setup,[0],[0]
"We select the best performing model on the development set among all of these fine-tuning
5We use the multi-BLEU script from the MOSES decoder suite (Koehn et al., 2007).
attempts.",5 Experimental Setup,[0],[0]
During prediction we perform decoding using beam search and set the beam size to 5 both for parsing and generation.,5 Experimental Setup,[0],[0]
"Parsing Results Table 1 summarizes our development results for different rounds of self-training and test results for our final system, self-trained on 200k, 2M and 20M unlabeled Gigaword sentences.",6 Results,[0],[0]
"Through every round of self-training, our
parser improves.",6 Results,[0],[0]
Our final parser outperforms comparable seq2seq and character LSTM models by over 10 points.,6 Results,[0],[0]
"While much of this improvement comes from self-training, our model without Gigaword data outperforms these approaches by 3.5 points on F1.",6 Results,[0],[0]
We attribute this increase in performance to different handling of preprocessing and more careful hyper-parameter tuning.,6 Results,[0],[0]
"All other models that we compare against use semantic resources, such as WordNet, dependency parsers or CCG parsers (models marked with * were trained with less data, but only evaluate on newswire text; the rest evaluate on the full test set, containing text from blogs).",6 Results,[0],[0]
"Our full models outperform JAMR, a graph-based model but still lags behind other parser-dependent systems (CAMR6), and resource heavy approaches (SBMT).
",6 Results,[0],[0]
Generation Results Table 3 summarizes our AMR generation results on the development and test set.,6 Results,[0],[0]
We outperform all previous state-of-theart systems by the first round of self-training and further improve with the next rounds.,6 Results,[0],[0]
"Our final model trained on GIGA-20M outperforms TSP and TREETOSTR trained on LDC2015E86, by over 9 BLEU",6 Results,[0],[0]
"points.7 Overall, our model incorporates less data than previous approaches as all reported methods train language models on the whole Gigaword corpus.",6 Results,[0],[0]
"We leave scaling our models to all of Gigaword for future work.
",6 Results,[0],[0]
"Sparsity Reduction Even after anonymization of open class vocabulary entries, we still encounter a great deal of sparsity in vocabulary given the small size of the AMR corpus, as shown in Table 2.",6 Results,[0],[0]
"By incorporating sentences from Gigaword we are able to reduce vocabulary sparsity dramatically, as we increase the size of sampled sentences: the out-of-vocabulary rate with a threshold of 5 reduces almost 5 times for GIGA-20M.
Preprocessing Ablation Study We consider the contribution of each main component of our preprocessing stages while keeping our linearization order identical.",6 Results,[0],[0]
Figure 2 contains examples for each setting of the ablations we evaluate on.,6 Results,[0],[0]
"First we evaluate using linearized graphs without paren-
6Since we are currently not using any Wikipedia resources for the prediction of named entities, we compare against the no-wikification version of the CAMR system.
",6 Results,[0],[0]
"7We also trained our generator on GIGA-2M and finetuned on LDC2014T12 in order to have a direct comparison with PBMT, and achieved a BLEU score of 29.7, i.e., 2.8 points of improvement.
theses for indicating scope, Figure 2(c), then without named entity clusters, Figure 2(b), and additionally without any anonymization, Figure 2(a).
",6 Results,[0],[0]
Tables 4 summarizes our evaluation on the AMR generation.,6 Results,[0],[0]
"Each components is required, and scope markers and anonymization contribute the most to overall performance.",6 Results,[0],[0]
We suspect without scope markers our seq2seq models are not as effective at capturing long range semantic relationships between elements of the AMR graph.,6 Results,[0],[0]
We also evaluated the contribution of anonymization to AMR parsing (Table 5).,6 Results,[0],[0]
"Following previous work, we find that seq2seq-based AMR parsing is largely ineffective without anonymization (Peng et al., 2017).",6 Results,[0],[0]
In this section we evaluate three strategies for converting AMR graphs into sequences in the context of AMR generation and show that our models are largely agnostic to linearization orders.,7 Linearization Evaluation,[0],[0]
"Our results argue, unlike SMT-based AMR generation methods (Pourdamghani et al., 2016), that seq2seq models can learn to ignore artifacts of the conversion of graphs to linear sequences.",7 Linearization Evaluation,[0],[0]
"All linearizations we consider use the pattern described in Section 4.2, but differ on the order in which children are visited.",7.1 Linearization Orders,[0],[0]
"Each linearization generates anonymized, scope-marked output (see Section 4), of the form shown in Figure 2(d).
",7.1 Linearization Orders,[0],[0]
Human,7.1 Linearization Orders,[0],[0]
"The proposal traverses children in the order presented by human authored AMR annotations exactly as shown in Figure 2(d).
",7.1 Linearization Orders,[0],[0]
Global-Random We construct a random global ordering of all edge types appearing in AMR graphs and re-use it for every example in the dataset.,7.1 Linearization Orders,[0],[0]
"We traverse children based on the position in the global ordering of the edge leading to a child.
",7.1 Linearization Orders,[0],[0]
Random,7.1 Linearization Orders,[0],[0]
For each example in the dataset we traverse children following a different random order of edge types.,7.1 Linearization Orders,[0],[0]
We present AMR generation results for the three proposed linearization orders in Table 6.,7.2 Results,[0],[0]
Random linearization order performs somewhat worse than traversing the graph according to Human linearization order.,7.2 Results,[0],[0]
"Surprisingly, a per example random linearization order performs nearly identically to a global random order, arguing seq2seq models can learn to ignore artifacts of the conversion of graphs to linear sequences.
",7.2 Results,[0],[0]
Human-authored AMR leaks information The small difference between random and globalrandom linearizations argues that our models are largely agnostic to variation in linearization order.,7.2 Results,[0],[0]
"On the other hand, the model that follows the human order performs better, which leads us to suspect it carries extra information not apparent in the graphical structure of the AMR.
",7.2 Results,[0],[0]
"To further investigate, we compared the relative ordering of edge pairs under the same parent to the relative position of children nodes derived from those edges in a sentence, as reported by JAMR alignments.",7.2 Results,[0],[0]
"We found that the majority of pairs of AMR edges (57.6%) always occurred in the same relative order, therefore revealing no extra generation order information.8",7.2 Results,[0],[0]
"Of the examples corresponding to edge pairs that showed variation, 70.3% appeared in an order consistent with the order they were realized in the sentence.",7.2 Results,[0],[0]
"The relative ordering of some pairs of AMR edges was
8This is consistent with constraints encoded in the annotation tool used to collect AMR.",7.2 Results,[0],[0]
"For example, :ARG0 edges are always ordered before :ARG1 edges.
particularly indicative of generation order.",7.2 Results,[0],[0]
"For example, the relative ordering of edges with types location and time, was 17% more indicative of the generation order than the majority of generated locations before time.9
To compare to previous work we still report results using human orderings.",7.2 Results,[0],[0]
"However, we note that any practical application requiring a system to generate an AMR representation with the intention to realize it later on, e.g., a dialog agent, will need to be trained either using consistent, or randomderived linearization orders.",7.2 Results,[0],[0]
"Arguably, our models are agnostic to this choice.",7.2 Results,[0],[0]
Figure 3 shows example outputs of our full system.,8 Qualitative Results,[0],[0]
The generated text for the first graph is nearly perfect with only a small grammatical error due to anonymization.,8 Qualitative Results,[0],[0]
"The second example is more challenging, with a deep right-branching structure, and a coordination of the verbs stabilize and push in the subordinate clause headed by state.",8 Qualitative Results,[0],[0]
"The model omits some information from the graph, namely the concepts terrorist and virus.",8 Qualitative Results,[0],[0]
"In the third example there are greater parts of the graph that are missing, such as the whole sub-graph headed by expert.",8 Qualitative Results,[0],[0]
"Also the model makes wrong attachment decisions in the last two sub-graphs (it is the evidence that is unimpeachable and irrefutable, and not the equipment), mostly due to insufficient annotation (thing) thus making their generation harder.
",8 Qualitative Results,[0],[0]
"Finally, Table 7 summarizes the proportions of error types we identified on 50 randomly selected examples from the development set.",8 Qualitative Results,[0],[0]
"We found that the generator mostly suffers from coverage issues,
9Consider the sentences “She went to school in New York two years ago”, and “Two years ago, she went to school in New York”, where “two year ago” is the time modifying constituent for the verb went and “New York” is the location modifying constituent of went.
",8 Qualitative Results,[0],[0]
"an inability to mention all tokens in the input, followed by fluency mistakes, as illustrated above.",8 Qualitative Results,[0],[0]
"Attachment errors are less frequent, which supports our claim that the model is robust to graph linearization, and can successfully encode long range dependency information between concepts.",8 Qualitative Results,[0],[0]
"We applied sequence-to-sequence models to the tasks of AMR parsing and AMR generation, by carefully preprocessing the graph representation and scaling our models via pretraining on millions of unlabeled sentences sourced from Gigaword corpus.",9 Conclusions,[0],[0]
"Crucially, we avoid relying on resources such as knowledge bases and externally trained parsers.",9 Conclusions,[0],[0]
"We achieve competitive results for the parsing task (SMATCH 62.1) and state-of-theart performance for generation (BLEU 33.8).
",9 Conclusions,[0],[0]
"For future work, we would like to extend our work to different meaning representations such as the Minimal Recursion Semantics (MRS; Copestake et al. (2005)).",9 Conclusions,[0],[0]
"This formalism tackles certain linguistic phenomena differently from AMR (e.g., negation, and co-reference), contains explicit annotation on concepts for number, tense and case, and finally handles multiple languages10 (Bender, 2014).",9 Conclusions,[0],[0]
"Taking a step further, we would like to apply our models on Semantics-Based Machine Translation using MRS as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as English and Japanese (Siegel, 2000).",9 Conclusions,[0],[0]
"The research was supported in part by DARPA under the DEFT program through AFRL (FA875013-2-0019) and the CwC program through ARO (W911NF-15-1-0543), the ARO (W911NF-16-10121), the NSF (IIS-1252835, IIS-1562364, IIS1524371), an Allen Distinguished Investigator Award, and gifts by Google and Facebook.",Acknowledgments,[0],[0]
"The authors thank Rik Koncel-Kedziorski and the UW NLP group for helpful discussions, and the anonymous reviewers for their thorough and helpful comments.
10A list of actively maintained languages can be found here: http://moin.delph-in.net/ GrammarCatalogue
limit :arg0 ( treaty :arg0-of ( control :arg1 arms ) )",Acknowledgments,[0],[0]
":arg1 ( number :arg1 ( weapon :mod conventional :arg1-of ( deploy :arg2 ( relative-pos :op1 loc_0 :dir west ) :arg1-of possible ) ) )
",Acknowledgments,[0],[0]
"SYS: the arms control treaty limits the number of conventional weapons that can be deployed west of Ural Mountains .
",Acknowledgments,[0],[0]
REF: the arms control treaty limits the number of conventional weapons that can be deployed west of the Ural Mountains .,Acknowledgments,[0],[0]
Sequence-to-sequence models have shown strong performance across a broad range of applications.,abstractText,[0],[0]
"However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the nonsequential nature of the AMR graphs.",abstractText,[0],[0]
We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs.,abstractText,[0],[0]
"For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources.",abstractText,[0],[0]
"For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8.",abstractText,[0],[0]
We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions.,abstractText,[0],[0]
Neural AMR: Sequence-to-Sequence Models for Parsing and Generation,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 38–44 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2007",text,[0],[0]
"In this work, we address multilingual semantic parsing – the task of mapping natural language sentences coming from multiple different languages into their corresponding formal semantic representations.",1 Introduction,[0],[0]
"We consider two multilingual scenarios: 1) the single-source setting, where the input consists of a single sentence in a single language, and 2) the multi-source setting, where the input consists of parallel sentences in multiple languages.",1 Introduction,[0],[0]
"Previous work handled the former by means of monolingual models (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012), while the latter has only been explored by Jie and Lu (2014) who ensembled many monolingual models together.",1 Introduction,[0],[0]
"Unfortunately, training a model for each language separately ignores the shared information among the source languages, which may be potentially beneficial for typologically related languages.",1 Introduction,[0],[0]
"Practically, it is also inconvenient to train, tune, and configure a new model for each language, which can be a laborious process.
",1 Introduction,[0],[0]
"In this work, we propose a parsing architecture that accepts as input sentences in several
languages.",1 Introduction,[0],[0]
"We extend an existing sequence-totree model (Dong and Lapata, 2016) to a multitask learning framework, motivated by its success in other fields, e.g., neural machine translation (MT) (Dong et al., 2015; Firat et al., 2016).",1 Introduction,[0],[0]
"Our model consists of multiple encoders, one for each language, and one decoder that is shared across source languages for generating semantic representations.",1 Introduction,[0],[0]
"In this way, the proposed model potentially benefits from having a generic decoder that works well across languages.",1 Introduction,[0],[0]
"Intuitively, the model encourages each source language encoder to find a common structured representation for the decoder.",1 Introduction,[0.9603170509683625],['The encoder-decoder architecture provides a general paradigm for learning machine translation from the source language to the target language.']
"We further modify the attention mechanism (Bahdanau et al., 2015) to integrate multisource information, such that it can learn where to focus during parsing; i.e., which input positions in which languages.
",1 Introduction,[0],[0]
"Our contributions are as follows:
• We investigate semantic parsing in two multilingual scenarios that are relatively unexplored in past research,
• We present novel extensions to the sequenceto-tree architecture that integrates multilingual information for semantic parsing, and
• We release a new ATIS semantic dataset annotated in two new languages.",1 Introduction,[0],[0]
"In this section, we summarize semantic parsing approaches from previous works.",2 Related Work,[0],[0]
"Wong and Mooney (2006) created WASP, a semantic parser based on statistical machine translation.",2 Related Work,[0],[0]
"Lu et al. (2008) proposed generative hybrid tree structures, which were augmented with a discriminative reranker.",2 Related Work,[0],[0]
"CCG-based semantic parsing systems have been developed, such as ZC07 (Zettlemoyer and Collins, 2007) and UBL (Kwiatkowski et al.,
38
2010).",2 Related Work,[0],[0]
"Researchers have proposed sequence-tosequence parsing models (Jia and Liang, 2016; Dong and Lapata, 2016; Kočiskỳ et al., 2016).",2 Related Work,[0],[0]
"Recently, Susanto and Lu (2017) extended the hybrid tree with neural features.
",2 Related Work,[0],[0]
"Recent progress in multilingual NLP has moved towards building a unified model that can work across different languages, such as in multilingual dependency parsing (Ammar et al., 2016), multilingual MT (Firat et al., 2016), and multilingual word embedding (Guo et al., 2016).",2 Related Work,[0],[0]
"Nonetheless, multilingual approaches for semantic parsing are relatively unexplored, which motivates this work.",2 Related Work,[0],[0]
Jones et al. (2012) evaluated an individuallytrained tree transducer on a multilingual semantic dataset.,2 Related Work,[0],[0]
Jie and Lu (2014) ensembled monolingual hybrid tree models on the same dataset.,2 Related Work,[0],[0]
"In this section, we describe our approach to multilingual semantic parsing, which extends the sequence-to-tree model by Dong and Lapata (2016).",3 Model,[0],[0]
"Unlike the mainstream approach that trains one monolingual parser per source language, our approach integrates N encoders, one for each language, into a single model.",3 Model,[0],[0]
"This model encodes a sentence from the n-th language X = x1, x2, ..., x|X| as a vector and then uses a shared decoder to decode the encoded vector into its corresponding logical form Y = y1, y2, ..., y|Y |.",3 Model,[0],[0]
We consider two types of input: 1) a single sentence in one of N languages in the single-source setting and 2) parallel sentences in N languages in the multi-source setting.,3 Model,[0],[0]
"We elaborate on each setting in Section 3.1 and 3.2, respectively.
",3 Model,[0],[0]
"The encoder is implemented as a unidirectional RNN with long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997), which takes a sequence of natural language tokens as input.",3 Model,[0],[0]
"Similar to previous multi-task frameworks, e.g., in neural MT (Firat et al., 2016; Zoph and Knight, 2016), we create one encoder per source language, i.e., {Ψnenc}Nn=1.",3 Model,[0],[0]
"For the n-th language, it updates the hidden vector at time step t by:
hnt = Ψ n enc(h n t−1,E n x[xt]) (1)
where Ψnenc is the LSTM function and E n x ∈ R|V |×d is an embedding matrix containing row vectors of the source tokens in the n-th language.",3 Model,[0],[0]
"Each encoder may be configured differently, such
as by the number of hidden units and the embedding dimension for the source symbol.
",3 Model,[0],[0]
"In the basic sequence-to-sequence model, the decoder generates each target token in a linear fashion.",3 Model,[0],[0]
"However, in semantic parsing, such a model ignores the hierarchical structure of logical forms.",3 Model,[0],[0]
"In order to alleviate this issue, Dong and Lapata (2016) proposed a decoder that generates logical forms in a top-down manner, where they define a “non-terminal” token <n> to indicate subtrees.",3 Model,[0],[0]
"At each depth in the tree, logical forms are generated sequentially until the end-ofsequence token is output.
",3 Model,[0],[0]
"Unlike in the single language setting, here we define a single, shared decoder Ψdec as opposed to one decoder per source language.",3 Model,[0],[0]
"We augment the parent non-terminal’s information p when computing the decoder state zt, as follows:
zt = Ψdec(zt−1,Ey[ỹt−1],p) (2)
where Ψdec is the LSTM function and ỹt−1 is the previous target symbol.
",3 Model,[0],[0]
"The attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) computes a timedependent context vector ct (as defined later in Section 3.1 and 3.2), which is subsequently used for computing the probability distribution over the next symbol, as follows:
z̃t = tanh(Uzt + Vct) (3) p(yt|y<t, X) ∝",3 Model,[0],[0]
"exp(Wz̃t) (4)
where U, V, and W are weight matrices.",3 Model,[0],[0]
"Finally, the model is trained to maximize the following conditional log-likelihood:
L(θ) = ∑
(X,Y )∈D
|Y |∑
t=1
log p(yt|y<t, X) (5)
where (X,Y ) refers to a ground-truth sentencesemantics pair in the training data D.
We use the same formulation above for the encoders and the decoder in both multilingual settings.",3 Model,[0],[0]
"Each setting differs in terms of: 1) the decoder state initialization, 2) the computation of the context vector ct, and 3) the training procedure, which are described in the following sections.",3 Model,[0],[0]
"In this setting, the input is a source sentence coming from the n-th language.",3.1 Single-Source Setting,[0],[0]
"Figure 1 (a) depicts a scenario where the model is parsing Indonesian input, with English and Chinese being non-active.
",3.1 Single-Source Setting,[0],[0]
The last state of the n-th encoder is used to initialize the first state of the decoder.,3.1 Single-Source Setting,[0],[0]
"We may need to first project the encoder vector into a suitable dimension for the decoder, i.e., z0 = φndec(h n |X|), where φndec can be an affine transformation.",3.1 Single-Source Setting,[0],[0]
"Similarly, we may do so before computing the attention scores, i.e., h̃nk = φ n att(h n k).",3.1 Single-Source Setting,[0],[0]
"Then, we compute the context vector cnt as a weighted sum of the hidden vectors in the n-th encoder:
αnk,t = exp(h̃nk · zt)∑|X|
k′=1 exp(h̃ n k′ · zt)
(6)
",3.1 Single-Source Setting,[0],[0]
"cnt =
|X|∑
k=1
αnk,th̃ n",3.1 Single-Source Setting,[0],[0]
"k (7)
",3.1 Single-Source Setting,[0],[0]
We set ct = cnt for computing Equation 3.,3.1 Single-Source Setting,[0],[0]
We propose two variants of the model under this setting.,3.1 Single-Source Setting,[0],[0]
"In the first version, we define separate weight matrices for each language, i.e., {Un,Vn,Wn}Nn=1.",3.1 Single-Source Setting,[0],[0]
"In the second version, the three weight matrices are shared across languages, essentially reducing the number of parameters by a factor of N .
",3.1 Single-Source Setting,[0],[0]
"The training data consists of the union of sentence-semantics pairs in N languages, where the source sentences are not necessarily parallel.",3.1 Single-Source Setting,[0],[0]
"We implement a scheduling mechanism that cycles through all languages during training, one language at a time.",3.1 Single-Source Setting,[0],[0]
"Specifically, model parameters are updated after one batch from one language before moving to the next one.",3.1 Single-Source Setting,[0],[0]
"Similar to Firat et al. (2016), this mechanism prevents excessive updates from a specific language.",3.1 Single-Source Setting,[0],[0]
"In this setting, the input are semantically equivalent sentences in N languages.",3.2 Multi-Source Setting,[0],[0]
"Figure 1 (b) depicts a scenario where the model is parsing English, Indonesian, and Chinese simultaneously.",3.2 Multi-Source Setting,[0],[0]
"It
includes a combiner module (denoted by the grey box), which we will explain next.
",3.2 Multi-Source Setting,[0],[0]
"The decoder state at the first time step is initialized by first combining the N final states from each encoder, i.e., z0 = φinit(h1|X|, · · · ,hN|X|), where we implement φinit by max-pooling.
",3.2 Multi-Source Setting,[0],[0]
We propose two ways of computing ct that integrates source-side information from multiple encoders.,3.2 Multi-Source Setting,[0],[0]
"First, we consider word-level combination, where we combine N encoder states at every time step, as follows:
αnk,t = exp(h̃nk · zt)∑N
n′=1 ∑|X| k′=1 exp(h̃ n′ k′ ·",3.2 Multi-Source Setting,[0],[0]
"zt)
(8)
ct = N∑
n=1
|X|∑
k=1
αnk,th̃ n k",3.2 Multi-Source Setting,[0],[0]
"(9)
Alternatively, in sentence-level combination, we first compute the context vector for each language in the same way as Equation 6 and 7.",3.2 Multi-Source Setting,[0],[0]
"Then, we perform a simple concatenation of N context vectors: ct = [ c1t ; · · · ; cNt ] .
",3.2 Multi-Source Setting,[0],[0]
"Unlike the single-source setting, the training data consists of N -way parallel sentencesemantics pairs.",3.2 Multi-Source Setting,[0],[0]
"That is, each training instance consists of N semantically equivalent sentences and their corresponding logical form.",3.2 Multi-Source Setting,[0],[0]
"We conduct our experiments on two multilingual benchmark datasets, which we describe below.",4.1 Datasets and Settings,[0],[0]
"Both datasets use a meaning representation based on lambda calculus.
",4.1 Datasets and Settings,[0],[0]
The GeoQuery (GEO) dataset is a standard benchmark evaluation for semantic parsing.,4.1 Datasets and Settings,[0],[0]
"The multilingual version consists of 880 instances of natural language queries related to US geography facts in four languages (English, German, Greek, and Thai) (Jones et al., 2012).",4.1 Datasets and Settings,[0],[0]
"We use the standard split which consists of 600 training examples and 280 test examples.
",4.1 Datasets and Settings,[0],[0]
The ATIS dataset contains natural language queries to a flight database.,4.1 Datasets and Settings,[0],[0]
"The data is split into 4,434 instances for training, 491 for development, and 448 for evaluation, same as Zettlemoyer and Collins (2007).",4.1 Datasets and Settings,[0],[0]
The original version only includes English.,4.1 Datasets and Settings,[0],[0]
"In this work, we annotate the corpus in Indonesian and Chinese.",4.1 Datasets and Settings,[0],[0]
"The Chinese corpus was
annotated (with segmentations) by hiring professional translation service.",4.1 Datasets and Settings,[0],[0]
"The Indonesian corpus was annotated by a native Indonesian speaker.
",4.1 Datasets and Settings,[0],[0]
"We use the same pre-processing as Dong and Lapata (2016), where entities and numbers are replaced with their type names and unique IDs.1 English words are stemmed using NLTK (Bird et al., 2009).",4.1 Datasets and Settings,[0],[0]
"Each query is paired with its corresponding semantic representation in lambda calculus (Zettlemoyer and Collins, 2005).
",4.1 Datasets and Settings,[0],[0]
"In all experiments, following Dong and Lapata (2016), we use a one-layer LSTM with 200- dimensional cells and embeddings.",4.1 Datasets and Settings,[0],[0]
"We use a minibatch size of 20 with RMSProp updates (Tieleman and Hinton, 2012) for a fixed number of epochs, with gradient clipping at 5.",4.1 Datasets and Settings,[0],[0]
"Parameters are uniformly initialized at [-0.08,0.08] and regularized using dropout (Srivastava et al., 2014).",4.1 Datasets and Settings,[0],[0]
Input sequences are reversed.,4.1 Datasets and Settings,[0],[0]
"See Appendix A for detailed experimental settings.
",4.1 Datasets and Settings,[0],[0]
"For each model configuration, all experiments are repeated 3 times with different random seed values, in order to make sure that our findings are reliable.",4.1 Datasets and Settings,[0],[0]
We found empirically that the random seed may affect SEQ2TREE performance.,4.1 Datasets and Settings,[0],[0]
This is especially important due to the relatively small dataset.,4.1 Datasets and Settings,[0],[0]
"As previously done in multitask sequence-to-sequence learning (Luong et al., 2016), we report the average performance for the baseline and our model.",4.1 Datasets and Settings,[0],[0]
The evaluation metric is defined in terms of exact match accuracy with the ground-truth logical forms.,4.1 Datasets and Settings,[0],[0]
See Appendix B for the accuracy of individual runs.,4.1 Datasets and Settings,[0],[0]
"Table 1 compares the performance of the monolingual sequence-to-tree model (Dong and Lapata, 2016), SINGLE, and our multilingual model, MULTI, with separate and shared output parameters under the single-source setting as described in Section 3.1.",4.2 Results,[0],[0]
"On average, both variants of the multilingual model outperform the monolingual model by up to 1.34% average accuracy on GEO.",4.2 Results,[0],[0]
"Parameter sharing is shown to be helpful, in particular for GEO.",4.2 Results,[0],[0]
We observe that the average performance increase on ATIS mainly comes from Chinese and Indonesian.,4.2 Results,[0],[0]
"We also learn that although including English is often helpful for the other languages, it may affect its individual performance.
",4.2 Results,[0],[0]
"Table 2 shows the average performance on
1See Section 3.6 of (Dong and Lapata, 2016).
",4.2 Results,[0],[0]
multi,4.2 Results,[0],[0]
-source parsing by combining 3 to 4 languages for GEO and 2 to 3 languages for ATIS.,4.2 Results,[0],[0]
"For RANKING, we combine the predictions from each language by selecting the one with the highest probability.",4.2 Results,[0],[0]
"Indeed, we observe that system combination at the model level is able to give better performance on average (up to 4.29% on GEO) than doing so at the output level.",4.2 Results,[0],[0]
Combining at the word level and sentence level shows comparable performance on both datasets.,4.2 Results,[0],[0]
"It can be seen that the benefit is more apparent when we include English in the system combination.
",4.2 Results,[0],[0]
"Regarding comparison to previous monolingual works, we want to highlight that there exist two different versions of the GeoQuery dataset annotated with completely different semantic representations: semantic tree and lambda calculus.",4.2 Results,[0],[0]
"As noted in Section 5 of Lu (2014), results obtained from these two versions are not comparable.",4.2 Results,[0],[0]
We use lambda calculus same as Dong and Lapata (2016).,4.2 Results,[0],[0]
"Under the multilingual setting, the closest work is Jie and Lu (2014).",4.2 Results,[0],[0]
"Nonetheless, they used the semantic tree version of GeoQuery.",4.2 Results,[0],[0]
"They eval-
uated extrinsically on a database query task while we use exact match accuracy, so their work is not directly comparable to ours.",4.2 Results,[0],[0]
"In this section, we report a qualitative analysis of our multilingual model.",5 Analysis,[0],[0]
"Table 3 shows example output from the monolingual model, SINGLE, trained on the three languages in ATIS and the multilingual model, MULTI, with sentence-level combination.",5 Analysis,[0],[0]
"This example demonstrates a scenario when the multilingual model successfully parses the three input sentences into the correct logical form, whereas the individual models are unable to do so.
",5 Analysis,[0],[0]
Figure 2 shows the alignments produced by MULTI (sentence) when parsing ATIS in the multisource setting.,5 Analysis,[0],[0]
"Each cell in the alignment matrix corresponds to αnk,t which is computed by Equation 6.",5 Analysis,[0],[0]
"Semantically related words are strongly aligned, such as the alignments between ground (en), darat (id), 地面 (zh) and ground transport.",5 Analysis,[0],[0]
"This shows that such correspondences can be jointly learned by our multilingual model.
",5 Analysis,[0],[0]
"In Table 4, we summarize the number of parameters in the baseline and our multilingual model.",5 Analysis,[0],[0]
The number of parameters in SINGLE and RANKING is equal to the sum of the number of parameters in their monolingual components.,5 Analysis,[0],[0]
It can be seen that the size of our multilingual model is about 50-60% smaller than that of the baseline.,5 Analysis,[0],[0]
We have presented a multilingual semantic parser that extends the sequence-to-tree model to a multitask learning framework.,6 Conclusion,[0],[0]
"Through experiments, we show that our multilingual model performs better on average than 1) monolingual models in the single-source setting and 2) ensemble ranking in the multi-source setting.",6 Conclusion,[0],[0]
We hope that this work will stimulate further research in multilingual semantic parsing.,6 Conclusion,[0],[0]
Our code and data is available at http://statnlp.org/research/sp/.,6 Conclusion,[0],[0]
"We would like to thank Christopher Bryant, Li Dong, and the anonymous reviewers for helpful comments and feedback.",Acknowledgments,[0],[0]
"This work is supported by MOE Tier 1 grant SUTDT12015008, and is partially supported by project 61472191 under the National Natural Science Foundation of China.",Acknowledgments,[0],[0]
"Table 5 lists the number of training epochs and the dropout probability used in the LSTM cell and the hidden layers before the softmax classifiers, which were chosen based on preliminary experiments on a held-out dataset.",A Hyperparameters,[0],[0]
We use a training schedule where we switch to the next language after training one mini-batch for GEO and 500 for ATIS.,A Hyperparameters,[0],[0]
"For
all multilingual models, we initialize the encoders using the encoder weights learned by the monolingual models.",A Hyperparameters,[0],[0]
"For the multi-source setting, we also initialize the decoder using the first language in the list of the combined languages.",A Hyperparameters,[0],[0]
"In Table 6 and 7, we report the accuracy of the 3 runs for each model and dataset.",B Additional Experimental Results,[0],[0]
"In both settings, we observe that the best accuracy on both datasets is often achieved by MULTI.",B Additional Experimental Results,[0],[0]
This is the same conclusion that we reached when averaging the results over all runs.,B Additional Experimental Results,[0],[0]
"In this paper, we address semantic parsing in a multilingual context.",abstractText,[0],[0]
We train one multilingual model that is capable of parsing natural language sentences from multiple different languages into their corresponding formal semantic representations.,abstractText,[0],[0]
We extend an existing sequence-to-tree model to a multi-task learning framework which shares the decoder for generating semantic representations.,abstractText,[0],[0]
We report evaluation results on the multilingual GeoQuery corpus and introduce a new multilingual version of the ATIS corpus.,abstractText,[0],[0]
Neural Architectures for Multilingual Semantic Parsing,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 219–230 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
219",text,[0],[0]
"Generating high quality arguments plays a crucial role in decision-making and reasoning processes (Bonet and Geffner, 1996; Byrnes, 2013).",1 Introduction,[0],[0]
"A multitude of arguments and counter-arguments are constructed on a daily basis, both online and offline, to persuade and inform us on a wide range of issues.",1 Introduction,[0],[0]
"For instance, debates are often conducted in legislative bodies to secure enough votes for bills to pass.",1 Introduction,[0],[0]
"In another example, online deliberation has become a popular way of soliciting public opinions on new policies’ pros and cons (Albrecht, 2006; Park et al., 2012).",1 Introduction,[0],[0]
"Nonetheless, constructing persuasive arguments is a daunting task, for both human and computers.",1 Introduction,[0],[0]
"We believe that developing effective argument generation models will enable a broad range of compelling applications, including debate coaching, improving students’ essay writing skills, and pro-
viding context of controversial issues from different perspectives.",1 Introduction,[0],[0]
"As a consequence, there exists a pressing need for automating the argument construction process.
",1 Introduction,[0],[0]
"To date, progress made in argument generation has been limited to retrieval-based methods— arguments are ranked based on relevance to a given topic, then the top ones are selected for inclusion in the output (Rinott et al., 2015; Wachsmuth et al., 2017; Hua and Wang, 2017).",1 Introduction,[0],[0]
"Although sentence ordering algorithms are developed for information structuring (Sato et al., 2015; Reisert et al., 2015), existing methods lack the ability of synthesizing information from different resources, leading to redundancy and incoherence in the output.
",1 Introduction,[0],[0]
"In general, the task of argument generation presents numerous challenges, ranging from aggregating supporting evidence to generating text with coherent logical structure.",1 Introduction,[0],[0]
"One particular hurdle comes from the underlying natural language generation (NLG) stack, whose success has been limited to a small set of domains.",1 Introduction,[0],[0]
"Especially, most previous NLG systems rely on tem-
plates that are either constructed by rules (Hovy, 1993; Belz, 2008; Bouayad-Agha et al., 2011), or acquired from a domain-specific corpus (Angeli et al., 2010) to enhance grammaticality and coherence.",1 Introduction,[0],[0]
"This makes them unwieldy to be adapted for new domains.
",1 Introduction,[0],[0]
"In this work, we study the following novel problem: given a statement on a controversial issue, generate an argument of an alternative stance.",1 Introduction,[0],[0]
"To address the above challenges, we present a neural network-based argument generation framework augmented with externally retrieved evidence.",1 Introduction,[0],[0]
"Our model is inspired by the observation that when humans construct arguments, they often collect references from external sources, e.g., Wikipedia or research papers, and then write their own arguments by synthesizing talking points from the references.",1 Introduction,[0],[0]
Figure 1 displays sample arguments by users from Reddit subcommunity /r/ChangeMyView 1 who argue against the motion that “government should be allowed to view private emails”.,1 Introduction,[0],[0]
"Both replies leverage information drawn from Wikipedia, such as “political corruption” and “Fourth Amendment on protections of personal privacy”.
",1 Introduction,[0],[0]
"Concretely, our neural argument generation model adopts the popular encoder-decoderbased sequence-to-sequence (seq2seq) framework (Sutskever et al., 2014), which has achieved significant success in various text generation tasks (Bahdanau et al., 2015; Wen et al., 2015; Wang and Ling, 2016; Mei et al., 2016; Wiseman et al., 2017).",1 Introduction,[0],[0]
"Our encoder takes as input a statement on a disputed issue, and a set of relevant evidence automatically retrieved from English Wikipedia2.",1 Introduction,[0],[0]
"Our decoder consists of two separate parts, one of which first generates keyphrases as intermediate representation of “talking points”, and the other then generates an argument based on both input and keyphrases.
",1 Introduction,[0],[0]
"Automatic evaluation based on BLEU (Papineni et al., 2002) shows that our framework generates better arguments than directly using retrieved sentences or popular seq2seq-based generation models (Bahdanau et al., 2015) that are also trained with retrieved evidence.",1 Introduction,[0],[0]
"We further design a novel evaluation procedure to measure whether the arguments are on-topic by predicting their relevance to the given statement based on a separately trained
1 https://www.reddit.com/r/changemyview 2 https://en.wikipedia.org/
relevance estimation model.",1 Introduction,[0],[0]
"Results suggest that our model generated arguments are more likely to be predicted as on-topic, compared to other seq2seq-based generations models.
",1 Introduction,[0],[0]
The rest of this paper is organized as follows.,1 Introduction,[0],[0]
Section 2 highlights the roadmap of our system.,1 Introduction,[0],[0]
The dataset used for our study is introduced in Section 3.,1 Introduction,[0],[0]
The model formulation and retrieval methods are detailed in Sections 4 and 5.,1 Introduction,[0],[0]
"We then describe the experimental setup and results in Sections 6 and 7, followed by further analysis and future directions in Section 8.",1 Introduction,[0],[0]
Related work is discussed in Section 9.,1 Introduction,[0],[0]
"Finally, we conclude in Section 10.",1 Introduction,[0],[0]
"Our argument generation pipeline, consisting of evidence retrieval and argument construction, is depicted in Figure 2.",2 Framework,[0],[0]
"Given a statement, a set of queries are constructed based on its topic signature words (e.g., “government” and “national security”) to retrieve a list of relevant articles from Wikipedia.",2 Framework,[0],[0]
"A reranking component further extracts sentences that may contain supporting evidence, which are used as additional input information for the neural argument generation model.
",2 Framework,[0],[0]
The generation model then encodes the statement and the evidence with a shared encoder in sequence.,2 Framework,[0],[0]
"Two decoders are designed: the keyphrase decoder first generates an intermediate representation of talking points in the form of keyphrases (e.g., “right to privacy”, “political corruption”), followed by a separate argument decoder which produces the final argument.",2 Framework,[0],[0]
"We draw data from Reddit subcommunity /r/ChangeMyView (henceforth CMV), which focuses on facilitating open discussions on a wide range of disputed issues.",3 Data Collection and Processing,[0],[0]
"Specifically, CMV is structured as discussion threads, where the original post (OP) starts with a viewpoint on a controversial topic, followed with detailed reasons, then other users reply with counter-arguments.",3 Data Collection and Processing,[0],[0]
"Importantly, when a user believes his view has been changed by an argument, a delta is often awarded to the reply.
",3 Data Collection and Processing,[0],[0]
"In total, 26,761 threads from CMV are downloaded, dating from January 2013 to June 20173.
",3 Data Collection and Processing,[0],[0]
"3Dataset used in this paper is available at http:// xinyuhua.github.io/Resources/.
Only root replies (i.e., replies directly addressing OP) that meet all of the following requirements are included: (1) longer than 5 words, (2) without offensive language4, (3) awarded with delta or with more upvotes than downvotes, and (4) not generated by system moderators.
",3 Data Collection and Processing,[0],[0]
"After filtering, the resultant dataset contains 26,525 OPs along with 305,475 relatively high quality root replies.",3 Data Collection and Processing,[0],[0]
"We treat each OP as the input statement, and the corresponding root replies as target arguments, on which our model is trained and evaluated.",3 Data Collection and Processing,[0],[0]
A Focused Domain Dataset.,3 Data Collection and Processing,[0],[0]
The current dataset contains diverse domains with unbalanced numbers of arguments.,3 Data Collection and Processing,[0],[0]
"We therefore choose samples from the politics domain due to its large volume of discussions and good coverage of popular arguments in the domain.
",3 Data Collection and Processing,[0],[0]
"However, topic labels are not available for the discussions.",3 Data Collection and Processing,[0],[0]
"We thus construct a domain classifier for politics vs. non-politics posts based on a logistic regression model with unigram features, trained from our heuristically labeled Wikipedia abstracts5.",3 Data Collection and Processing,[0],[0]
"Concretely, we manually collect two lists of keywords that are indicative of politics and non-politics.",3 Data Collection and Processing,[0],[0]
"Each abstract is labeled as politics
4 We use offensive words collected by Google’s",3 Data Collection and Processing,[0],[0]
"What Do You Love project: https://gist.github.com/ jamiew/1112488, last accessed on February 22nd, 2018.
",3 Data Collection and Processing,[0],[0]
"5About 1.3 million English Wikipedia abstracts are downloaded from http://dbpedia.org/page/.
or non-politics if its title only matches keywords from one category.6",3 Data Collection and Processing,[0],[0]
"In total, 264,670 politics abstracts and 827,437 of non-politics are labeled.",3 Data Collection and Processing,[0],[0]
"Starting from this dataset, our domain classifier is trained in a bootstrapping manner by gradually adding OPs predicted as politics or non-politics.7 Finally, 12,549 OPs are labeled as politics, each of which is paired with 9.4 high-quality target arguments on average.",3 Data Collection and Processing,[0],[0]
"The average length for OPs is 16.1 sentences of 356.4 words, and 7.7 sentences of 161.1 words for arguments.",3 Data Collection and Processing,[0],[0]
"In this section, we present our argument generation model, which jointly learns to generate talking points in the form of keyphrases and produce arguments based on the input and keyphrases.",4 Model,[0],[0]
"Extended from the successful seq2seq attentional model (Bahdanau et al., 2015), our proposed model is novel in the following ways.",4 Model,[0],[0]
"First, two separate decoders are designed, one for generating keyphrases, the other for argument construction.",4 Model,[0],[0]
"By sharing the encoder with keyphrase generation, our argument decoder is better aware of salient talking points in the input.",4 Model,[0],[0]
"Second, a novel
6Sample keywords for politics: “congress”, “election”, “constitution”; for non-politics: “art”, “fashion”,“music”.",4 Model,[0],[0]
"Full lists are provided in the supplementary material.
",4 Model,[0],[0]
"7More details about our domain classifier are provided in the supplementary material.
attention mechanism is designed for argument decoding by attending both input and the previously generated keyphrases.",4 Model,[0],[0]
"Finally, a reranking-based beam search decoder is introduced to promote topic-relevant generations.",4 Model,[0],[0]
"Our model takes as input a sequence of tokens x = {xO;xE}, where xO is the statement sequence and xE contains relevant evidence that is extracted from Wikipedia based on a separate retrieval module.",4.1 Model Formulation,[0],[0]
A special token <evd> is inserted between xO and xE .,4.1 Model Formulation,[0],[0]
"Our model then first generates a set of keyphrases as a sequence yp = {ypl }, followed by an argument ya = {yat }, by maximizing logP (y|x), where y = {yp;ya}.",4.1 Model Formulation,[0],[0]
"The objective is further decomposed into∑ t logP (yt|y1:t−1,x), with each term estimated by a softmax function over a non-linear transformation of decoder hidden states sat and s p t , for argument decoder and keyphrase decoder, respectively.",4.1 Model Formulation,[0],[0]
"The hidden states are computed as done in Bahdanau et al. (2015) with attention:
st = g(st−1, ct, yt) (1)
ct = T∑ j=1 αtjhj (2) αtj = exp(etj)∑T k=1 exp(etk) (3) etj = v T tanh(Whhj +Wsst + battn)",4.1 Model Formulation,[0],[0]
"(4)
Notice that two sets of parameters and different state update functions g(·) are learned for separate decoders: {W ah , W as , baattn, ga(·)} for the argument decoder; {W ph , W p s , b p attn, g
p(·)} for the keyphrase decoder.",4.1 Model Formulation,[0],[0]
Encoder.,4.1 Model Formulation,[0],[0]
"A two-layer bidirectional LSTM (biLSTM) is used to obtain the encoder hidden states hi for each time step i. For biLSTM, the hidden state is the concatenation of forward and backward hidden states:",4.1 Model Formulation,[0],[0]
hi =,4.1 Model Formulation,[0],[0]
[ −→ hi; ←− hi].,4.1 Model Formulation,[0],[0]
"Word representations are initialized with 200-dimensional pre-trained GloVe embeddings (Pennington et al., 2014), and updated during training.",4.1 Model Formulation,[0],[0]
The last hidden state of encoder is used to initialize both decoders.,4.1 Model Formulation,[0],[0]
In our model the encoder is shared by argument and keyphrase decoders.,4.1 Model Formulation,[0],[0]
Decoders.,4.1 Model Formulation,[0],[0]
"Our model is equipped with two decoders: keyphrase decoder and argument decoder, each is implemented with a separate two-layer unidirectional LSTM, in a similar spirit with one-
to-many multi-task sequence-to-sequence learning (Luong et al., 2015).",4.1 Model Formulation,[0],[0]
"The distinction is that our training objective is the sum of two loss functions:
L(θ) =− α Tp ∑ (x,yp)∈D logP (yp|x; θ)
− (1− α)",4.1 Model Formulation,[0],[0]
"Ta ∑ (x,ya)∈D",4.1 Model Formulation,[0],[0]
"logP (ya|x; θ) (5)
where Tp and Ta denote the lengths of reference keyphrase sequence and argument sequence.",4.1 Model Formulation,[0],[0]
"α is a weighting parameter, and it is set as 0.5 in our experiments.
",4.1 Model Formulation,[0],[0]
Attention over Both Input and Keyphrases.,4.1 Model Formulation,[0],[0]
"Intuitively, the argument decoder should consider the generated keyphrases as talking points during the generation process.",4.1 Model Formulation,[0],[0]
We therefore propose an attention mechanism that can attend both encoder hidden states and the keyphrase decoder hidden states.,4.1 Model Formulation,[0],[0]
"Additional context vector c′t is then computed over keyphrase decoder hidden states spj , which is used for computing the new argument decoder state:
sat = g ′(sat−1, [ct; c ′ t], y a t )",4.1 Model Formulation,[0],[0]
"(6)
c′t = Tp∑ j=1 α′tjs p j (7) α′tj = exp(e′tj)∑Tp k=1 exp(e ′",4.1 Model Formulation,[0],[0]
tk) (8) e′tj = v ′T tanh(W ′ps p j,4.1 Model Formulation,[0],[0]
+W ′,4.1 Model Formulation,[0],[0]
"as a t + b ′ attn) (9)
where spj is the hidden state of keyphrase decoder at position j, sat is the hidden state of argument decoder at timestep t, and ct is computed in Eq. 2.
",4.1 Model Formulation,[0],[0]
Decoder Sharing.,4.1 Model Formulation,[0],[0]
We also experiment with a shared decoder between keyphrase generation and argument generation: the last hidden state of the keyphrase decoder is used as the initial hidden state for the argument decoder.,4.1 Model Formulation,[0],[0]
"A special token <arg> is inserted between the two sequences, indicating the start of argument generation.",4.1 Model Formulation,[0],[0]
Here we describe our decoding strategy on the argument decoder.,4.2 Hybrid Beam Search Decoding,[0],[0]
"We design a hybrid beam expansion method combined with segment-based reranking to promote diversity of beams and informativeness of the generated arguments.
",4.2 Hybrid Beam Search Decoding,[0],[0]
Hybrid Beam Expansion.,4.2 Hybrid Beam Search Decoding,[0],[0]
"In the standard beam search, the top k words of highest probability are
selected deterministically based on the softmax output to expand each hypothesis.",4.2 Hybrid Beam Search Decoding,[0],[0]
"However, this may lead to suboptimal output for text generation (Wiseman and Rush, 2016), e.g., one beam often dominates and thus inhibits hypothesis diversity.",4.2 Hybrid Beam Search Decoding,[0],[0]
"Here we only pick the top n words (n < k), and randomly draw",4.2 Hybrid Beam Search Decoding,[0],[0]
another k− n words based on the multinomial distribution after removing the n expanded words from the candidates.,4.2 Hybrid Beam Search Decoding,[0],[0]
This leads to a more diverse set of hypotheses.,4.2 Hybrid Beam Search Decoding,[0],[0]
Segment-based Reranking.,4.2 Hybrid Beam Search Decoding,[0],[0]
We also propose to rerank the beams every p steps based on beam’s coverage of content words from input.,4.2 Hybrid Beam Search Decoding,[0],[0]
"Based on our observation that likelihood-based reranking often leads to overly generic arguments (e.g., “I don’t agree with you”), this operation has the potential of encouraging more informative generation.",4.2 Hybrid Beam Search Decoding,[0],[0]
k,4.2 Hybrid Beam Search Decoding,[0],[0]
"= 10, n = 3, and p = 10 are used for experiments.",4.2 Hybrid Beam Search Decoding,[0],[0]
The effect of parameter selection is studied in Section 7.,4.2 Hybrid Beam Search Decoding,[0],[0]
"We take a two-step approach for retrieving evidence sentences: given a statement, (1) constructing one query per sentence and retrieving relevant articles from Wikipedia, and (2) reranking paragraphs and then sentences to create the final set of evidence sentences.",5.1 Retrieval Methodology,[0],[0]
Wikipedia is used as our evidence source mainly due to its objective perspective and broad coverage of topics.,5.1 Retrieval Methodology,[0],[0]
"A dump of December 21, 2016 was downloaded.",5.1 Retrieval Methodology,[0],[0]
"For training, evidence sentences are retrieved with queries constructed from target user arguments.",5.1 Retrieval Methodology,[0],[0]
"For test, queries are constructed from OP.",5.1 Retrieval Methodology,[0],[0]
Article Retrieval.,5.1 Retrieval Methodology,[0],[0]
We first create an inverted index lookup table for Wikipedia as done in Chen et al. (2017).,5.1 Retrieval Methodology,[0],[0]
"For a given statement, we construct one query per sentence to broaden the diversity of retrieved articles.",5.1 Retrieval Methodology,[0],[0]
"Therefore, multiple passes of retrieval will be conducted if more than one query is created.",5.1 Retrieval Methodology,[0],[0]
"Specifically, we first collect topic signature words of the post.",5.1 Retrieval Methodology,[0],[0]
"Topic signatures (Lin and Hovy, 2000) are terms strongly correlated with a given post, measured by log-likelihood ratio against a background corpus.",5.1 Retrieval Methodology,[0],[0]
We treat posts from other discussions in our dataset as background.,5.1 Retrieval Methodology,[0],[0]
"For each sentence, one query is constructed based on the noun phrases and verbs containing at least one topic signature word.",5.1 Retrieval Methodology,[0],[0]
"For instance, a query “the government, my e-mails,
national security” is constructed for the first sentence of OP in the motivating example (Figure 2).",5.1 Retrieval Methodology,[0],[0]
Top five retrieved articles with highest TF-IDF similarity scores are kept per query.,5.1 Retrieval Methodology,[0],[0]
Sentence Reranking.,5.1 Retrieval Methodology,[0],[0]
"The retrieved articles are first segmented into paragraphs, which are reranked by TF-IDF similarity to the given statement.",5.1 Retrieval Methodology,[0],[0]
Up to 100 top ranked paragraphs with positive scores are retained.,5.1 Retrieval Methodology,[0],[0]
"These paragraphs are further segmented into sentences, and reranked according to TF-IDF similarity again.",5.1 Retrieval Methodology,[0],[0]
We only keep up to 10 top sentences with positive scores for inclusion in the evidence set.,5.1 Retrieval Methodology,[0],[0]
"To create training data for the keyphrase decoder, we use the following rules to identify keyphrases from evidence sentences that are reused by human writers for argument construction: • Extract noun phrases and verb phrases
from evidence sentences using Stanford CoreNLP (Manning et al., 2014).",5.2 Gold-Standard Keyphrase Construction,[0],[0]
"• Keep phrases of length between 2 and 10 that
overlap with content words in the argument.",5.2 Gold-Standard Keyphrase Construction,[0],[0]
•,5.2 Gold-Standard Keyphrase Construction,[0],[0]
"If there is span overlap between phrases, the
longer one is kept if it has more content word coverage of the argument; otherwise the shorter one is retained.
",5.2 Gold-Standard Keyphrase Construction,[0],[0]
The resultant phrases are then concatenated with a special delimiter <phrase> and used as gold-standard generation for training.,5.2 Gold-Standard Keyphrase Construction,[0],[0]
Encoding the full set of evidence by our current decoder takes a huge amount of time.,6.1 Final Dataset Statistics,[0],[0]
"We there propose a sampling strategy to allow the encoder to finish encoding within reasonable time
by considering only a subset of the evidence: For each sentence in the statement, up to three evidence sentences are randomly sampled from the retrieved set; then the sampled sentences are concatenated.",6.1 Final Dataset Statistics,[0],[0]
"This procedure is repeated three times per statement, where a statement is an user argument for training data and an OP for test set.",6.1 Final Dataset Statistics,[0],[0]
"In our experiments, we remove duplicates samples and the ones without any retrieved evidence sentence.",6.1 Final Dataset Statistics,[0],[0]
"Finally, we break down the augmented data into a training set of 224,553 examples (9,737 unique OPs), 13,911 for validation (640 OPs), and 30,417 retained for test (1,892 OPs).",6.1 Final Dataset Statistics,[0],[0]
"For all models, we use a two-layer biLSTM as encoder and a two-layer unidirectional LSTM as decoder, with 200-dimensional hidden states in each layer.",6.2 Training Setup,[0],[0]
"We apply dropout (Gal and Ghahramani, 2016) on RNN cells with a keep probability of 0.8.",6.2 Training Setup,[0],[0]
"We use Adam (Kingma and Ba, 2015) with an initial learning rate of 0.001 to optimize the cross-entropy loss.",6.2 Training Setup,[0],[0]
Gradient clipping is also applied with the maximum norm of 2.,6.2 Training Setup,[0],[0]
The input and output vocabulary sizes are both 50k.,6.2 Training Setup,[0],[0]
Curriculum Training.,6.2 Training Setup,[0],[0]
We train the models in three stages where the truncated input and output lengths are gradually increased.,6.2 Training Setup,[0],[0]
Details are listed in Table 2.,6.2 Training Setup,[0],[0]
"Importantly, this strategy allows model training to make rapid progress during early stages.",6.2 Training Setup,[0],[0]
Training each of our full models takes about 4 days on a Quadro P5000 GPU card with a batch size of 32.,6.2 Training Setup,[0],[0]
"The model converges after about 10 epochs in total with pre-training initialization, which is described below.
",6.2 Training Setup,[0],[0]
Adding Pre-training.,6.2 Training Setup,[0],[0]
We pre-train a two-layer seq2seq model with OP as input and target argument as output from our training set.,6.2 Training Setup,[0],[0]
"After 20 epochs (before converging), parameters for the
first layer are used to initialize the first layer of all comparison models and our models (except for the keyphrase decoder).",6.2 Training Setup,[0],[0]
"Experimental results show that pre-training boosts all methods by roughly 2 METEOR (Denkowski and Lavie, 2014) points.",6.2 Training Setup,[0],[0]
We describe more detailed results in the supplementary material.,6.2 Training Setup,[0],[0]
"We first consider a RETRIEVAL-based baseline, which concatenates retrieved evidence sentences to form the argument.",6.3 Baseline and Comparisons,[0],[0]
"We further compare with three seq2seq-based generation models with different training data: (1) SEQ2SEQ: training with OP as input and the argument as output; (2) SEQ2SEQ + encode evd: augmenting input with evidence sentences as in our model; (3) SEQ2SEQ + encode KP: augmenting input with gold-standard keyphrases, which assumes some of the talking points are known.",6.3 Baseline and Comparisons,[0],[0]
"All seq2seq models use a regular beam search decoder with the same beam size as ours.
",6.3 Baseline and Comparisons,[0],[0]
Variants of Our Models.,6.3 Baseline and Comparisons,[0],[0]
We experiment with variants of our models based on the proposed separate decoder model (DEC-SEPARATE) or using a shared decoder (DEC-SHARED).,6.3 Baseline and Comparisons,[0],[0]
"For each, we further test whether adding keyphrase attention for argument decoding is helpful (+ attend KP).
",6.3 Baseline and Comparisons,[0],[0]
System vs. Oracle Retrieval.,6.3 Baseline and Comparisons,[0],[0]
"For test time, evidence sentences are retrieved with queries constructed from OP (System Retrieval).",6.3 Baseline and Comparisons,[0],[0]
"We also experiment with an Oracle Retrieval setup, where the evidence is retrieved based on user arguments, to indicate how much gain can be expected with better retrieval results.",6.3 Baseline and Comparisons,[0],[0]
"For automatic evaluation, we use BLEU (Papineni et al., 2002), an n-gram precision-based metric (up to bigrams are considered), and METEOR (Denkowski and Lavie, 2014), measuring unigram recall and precision by considering paraphrases, synonyms, and stemming.",7.1 Automatic Evaluation,[0],[0]
Human arguments are used as the gold-standard.,7.1 Automatic Evaluation,[0],[0]
"Because each OP may be paired with more than one highquality arguments, we compute BLEU and METEOR scores for the system argument compared against all arguments, and report the best.",7.1 Automatic Evaluation,[0],[0]
"We do not use multiple reference evaluation because
the arguments are often constructed from different angles and cover distinct aspects of the issue.",7.1 Automatic Evaluation,[0],[0]
"For models that generate more than one arguments based on different sets of sampled evidence, the one with the highest score is considered.
",7.1 Automatic Evaluation,[0],[0]
"As can be seen from Table 3, our models produce better BLEU scores than almost all the comparisons.",7.1 Automatic Evaluation,[0],[0]
"Especially, our models with separate decoder yield significantly higher BLEU and METEOR scores than all seq2seq-based models (approximation randomization testing, p < 0.0001) do.",7.1 Automatic Evaluation,[0],[0]
"Better METEOR scores are achieved by the RETRIEVAL baseline, mainly due to its significantly longer arguments.
",7.1 Automatic Evaluation,[0],[0]
"Moreover, utilizing attention over both input and the generated keyphrases further boosts our models’ performance.",7.1 Automatic Evaluation,[0],[0]
"Interestingly, utilizing system retrieved evidence yields better BLEU scores than using oracle retrieval for testing.",7.1 Automatic Evaluation,[0],[0]
The reason could be that arguments generated based on system retrieval contain less topic-specific words and more generic argumentative phrases.,7.1 Automatic Evaluation,[0],[0]
"Since the later is often observed in human written arguments, it may lead to higher precision and thus better BLEU scores.
",7.1 Automatic Evaluation,[0],[0]
Decoder Strategy Comparison.,7.1 Automatic Evaluation,[0],[0]
We also study the effect of our reranking-based decoder by varying the reranking step size (p) and the number of top words expanded to beam hypotheses deterministically (k).,7.1 Automatic Evaluation,[0],[0]
"From the results in Figure 3, we find that reranking with a smaller step size, e.g.,
p = 5, can generally lead to better METEOR scores.",7.1 Automatic Evaluation,[0],[0]
"Although varying the number of top words for beam expansion does not yield significant difference, we do observe more diverse beams from the system output if more candidate words are selected stochastically (i.e. with a smaller k).",7.1 Automatic Evaluation,[0],[0]
"During our pilot study, we observe that generic arguments, such as “I don’t agree with you” or “this is not true”, are prevalent among generations by seq2seq models.",7.2 Topic-Relevance Evaluation,[0],[0]
We believe that good arguments should include content that addresses the given topic.,7.2 Topic-Relevance Evaluation,[0],[0]
"Therefore, we design a novel evaluation method to measure whether the generated arguments contain topic-relevant information.
",7.2 Topic-Relevance Evaluation,[0],[0]
"To achieve the goal, we first train a topicrelevance estimation model inspired by the latent semantic model in Huang et al. (2013).",7.2 Topic-Relevance Evaluation,[0],[0]
"A pair of OP and argument, each represented as the average of word embeddings, are separately fed into a twolayer transformation model.",7.2 Topic-Relevance Evaluation,[0],[0]
"A dot-product is computed over the two projected low-dimensional vectors, and then a sigmoid function outputs the relevance score.",7.2 Topic-Relevance Evaluation,[0],[0]
"For model learning, we further divide our current training data into training, developing, and test sets.",7.2 Topic-Relevance Evaluation,[0],[0]
"For each OP and argument pair, we first randomly sample 100 arguments from other threads, and then pick the top 5 dissimilar ones, measured by Jaccard distance, as negative training samples.",7.2 Topic-Relevance Evaluation,[0],[0]
This model achieves a Mean Reciprocal Rank (MRR) score of 0.95 on the test set.,7.2 Topic-Relevance Evaluation,[0],[0]
"Descriptions about model formulation and related training
details are included in the supplementary material.",7.2 Topic-Relevance Evaluation,[0],[0]
We then take this trained model to evaluate the relevance between OP and the corresponding system arguments.,7.2 Topic-Relevance Evaluation,[0],[0]
Each system argument is treated as positive sample; we then select five negative samples from arguments generated for other OPs whose evidence sentences most similar to that of the positive sample.,7.2 Topic-Relevance Evaluation,[0],[0]
"Intuitively, if an argument contains more topic relevant information, then the relevance estimation model will output a higher score for it; otherwise, the argument will receive a lower similarity score, and thus cannot be easily distinguished from negative samples.",7.2 Topic-Relevance Evaluation,[0],[0]
"Ranking metrics of MRR and Precision at 1 (P@1) are utilized, with results reported in Table 4.",7.2 Topic-Relevance Evaluation,[0],[0]
"The ranker yields significantly better scores over arguments generated from models trained with evidence, compared to arguments generated by SEQ2SEQ model.
",7.2 Topic-Relevance Evaluation,[0],[0]
"Moreover, we manually pick 29 commonly used generic responses (e.g., “I don’t think so”) and count their frequency in system outputs.",7.2 Topic-Relevance Evaluation,[0],[0]
"For the seq2seq model, more than 75% of its outputs contain at least one generic argument, compared to 16.2% by our separate decoder model with attention over keyphrases.",7.2 Topic-Relevance Evaluation,[0],[0]
This further implies that our model generates more topic-relevant content.,7.2 Topic-Relevance Evaluation,[0],[0]
"We also hire three trained human judges who are fluent English speakers to rate system arguments for the following three aspects on a scale of 1
to 5 (with 5 as best): Grammaticality—whether an argument is fluent, informativeness—whether the argument contains useful information and is not generic, and relevance—whether the argument contains information of a different stance or offtopic.",7.3 Human Evaluation,[0],[0]
"30 CMV threads are randomly selected, each of which is presented with randomly-shuffled OP statement and four system arguments.
",7.3 Human Evaluation,[0],[0]
Table 5 shows that our model with separate decoder and attention over keyphrases produce significantly more informative and relevant arguments than seq2seq trained without evidence.8,7.3 Human Evaluation,[0],[0]
"However, we also observe that human judges prefer the retrieved arguments over generation-based models, illustrating the gap between system arguments and human edited text.",7.3 Human Evaluation,[0],[0]
Sample arguments are displayed in Figure 4.,7.3 Human Evaluation,[0],[0]
Keyphrase Generation Analysis.,8 Further Discussion,[0],[0]
Here we provide further analysis over the generated keyphrases by our separate decoder model.,8 Further Discussion,[0],[0]
"First, about 10% of the keyphrases output by our model also appear in the gold-standard (i.e., used by human arguments).",8 Further Discussion,[0],[0]
"Furthermore, 36% of generated keyphrases are reused by our system arguments.",8 Further Discussion,[0],[0]
"With human inspection, we find that although some keyphrases are not directly reused by the argument decoder, they represent high level talking points in the argument.",8 Further Discussion,[0],[0]
"For instance, in the first sample argument by our model in Figure 4, keyphrases “the motive” and “russian” are generated.",8 Further Discussion,[0],[0]
"Although not used, they suggest the topics that the argument should stay on.
",8 Further Discussion,[0],[0]
Sample Arguments and Future Directions.,8 Further Discussion,[0],[0]
"As can be seen from the sample outputs in Figure 4, our model generally captures more relevant concepts, e.g., “military army” and “wars
8Inter-rater agreement scores for these three aspects are 0.50, 0.60, and 0.48 by Krippendorff’s α.
of the world”, as discussed in the first example.",8 Further Discussion,[0],[0]
"Meanwhile, our model also acquires argumentative style language, though there is still a noticeable gap between system arguments and human constructed arguments.",8 Further Discussion,[0],[0]
"As discovered by our prior work (Wang et al., 2017), both topical content and language style are essential elements for high quality arguments.",8 Further Discussion,[0],[0]
"For future work, generation models with a better control on linguistic style need to be designed.",8 Further Discussion,[0],[0]
"As for improving coherence, we believe that discourse-aware generation models (Ji et al., 2016) should also be explored in the future work to enhance text planning.",8 Further Discussion,[0],[0]
"There is a growing interest in argumentation mining from the natural language processing research
community (Park and Cardie, 2014; Ghosh et al., 2014; Palau and Moens, 2009; Niculae et al., 2017; Eger et al., 2017).",9 Related Work,[0],[0]
"While argument understanding has received increasingly more attention, the area of automatic argument generation is much less studied.",9 Related Work,[0],[0]
"Early work on argument construction investigates the design of argumentation strategies (Reed et al., 1996; Carenini and Moore, 2000; Zukerman et al., 2000).",9 Related Work,[0],[0]
"For instance, Reed (1999) describes the first full natural language argument generation system, called Rhetorica.",9 Related Work,[0],[0]
"It however only outputs a text plan, mainly relying on heuristic rules.",9 Related Work,[0],[0]
"Due to the difficulty of text generation, none of the previous work represents a fully automated argument generation system.",9 Related Work,[0],[0]
"This work aims to close the gap by proposing an end-to-end trained argument construction framework.
",9 Related Work,[0],[0]
"Additionally, argument retrieval and extraction are investigated (Rinott et al., 2015; Hua and Wang, 2017) to deliver relevant arguments for user-specified queries.",9 Related Work,[0],[0]
Wachsmuth et al. (2017) build a search engine from arguments collected from various online debate portals.,9 Related Work,[0],[0]
"After the retrieval step, sentence ordering algorithms are often applied to improve coherence (Sato et al., 2015; Reisert et al., 2015).",9 Related Work,[0],[0]
"Nevertheless, simply merging arguments from different resources inevitably introduces redundancy.",9 Related Work,[0],[0]
"To the best of our knowledge, this is the first automatic argument generation system that can synthesize retrieved content from different articles into fluent arguments.",9 Related Work,[0],[0]
We studied the novel problem of generating arguments of a different stance for a given statement.,10 Conclusion,[0],[0]
We presented a neural argument generation framework enhanced with evidence retrieved from Wikipedia.,10 Conclusion,[0],[0]
"Separate decoders were designed to first produce a set of keyphrases as talking points, and then generate the final argument.",10 Conclusion,[0],[0]
Both automatic evaluation against human arguments and human assessment showed that our model produced more informative arguments than popular sequence-to-sequence-based generation models.,10 Conclusion,[0],[0]
"This work was partly supported by National Science Foundation Grant IIS-1566382, and a GPU gift from Nvidia.",Acknowledgements,[0],[0]
We thank three anonymous reviewers for their insightful suggestions on various aspects of this work.,Acknowledgements,[0],[0]
High quality arguments are essential elements for human reasoning and decision-making processes.,abstractText,[0],[0]
"However, effective argument construction is a challenging task for both human and machines.",abstractText,[0],[0]
"In this work, we study a novel task on automatically generating arguments of a different stance for a given statement.",abstractText,[0],[0]
We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia.,abstractText,[0],[0]
"Our model first generates a set of talking point phrases as intermediate representation, followed by a separate decoder producing the final argument based on both input and the keyphrases.",abstractText,[0],[0]
Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topicrelevant content than a popular sequence-tosequence generation model according to both automatic evaluation and human assessments.,abstractText,[0],[0]
Neural Argument Generation Augmented with Externally Retrieved Evidence,title,[0],[0]
Audio synthesis is important for a large range of applications including text-to-speech (TTS) systems and music generation.,1. Introduction,[0],[0]
"Audio generation algorithms, know as vocoders in TTS and synthesizers in music, respond to higher-level control signals to create fine-grained audio waveforms.",1. Introduction,[0],[0]
"Synthesizers have a long history of being hand-designed instruments, accepting control signals such as ‘pitch’, ‘velocity’, and filter parameters to shape the tone, timbre, and dynamics of a sound (Pinch et al., 2009).",1. Introduction,[0],[0]
"In spite of their limitations, or perhaps because of them, synthesizers have had a profound effect on the course of music and culture in the past half century (Punk, 2014).
",1. Introduction,[0],[0]
*Equal contribution 1Google Brain 2DeepMind.,1. Introduction,[0],[0]
"Correspondence to: Jesse Engel <jesseengel@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we outline a data-driven approach to audio synthesis.",1. Introduction,[0],[0]
"Rather than specifying a specific arrangement of oscillators or an algorithm for sample playback, such as in FM Synthesis or Granular Synthesis (Chowning, 1973; Xenakis, 1971), we show that it is possible to generate new types of expressive and realistic instrument sounds with a neural network model.",1. Introduction,[0],[0]
"Further, we show that this model can learn a semantically meaningful hidden representation that can be used as a high-level control signal for manipulating tone, timbre, and dynamics during playback.
",1. Introduction,[0],[0]
"Explicitly, our two contributions to advance the state of generative audio modeling are:
• A WaveNet-style autoencoder that learns temporal hidden codes to effectively capture longer term structure without external conditioning.
",1. Introduction,[0],[0]
"• NSynth: a large-scale dataset for exploring neural audio synthesis of musical notes.
",1. Introduction,[0],[0]
"The primary motivation for our novel autoencoder structure follows from the recent advances in autoregressive models like WaveNet (van den Oord et al., 2016a) and SampleRNN (Mehri et al., 2016).",1. Introduction,[0],[0]
"They have proven to be effective at modeling short and medium scale (∼500ms) signals, but rely on external conditioning for longer-term dependencies.",1. Introduction,[0],[0]
Our autoencoder removes the need for that external conditioning.,1. Introduction,[0],[0]
It consists of a WaveNet-like encoder that infers hidden embeddings distributed in time and a WaveNet decoder that uses those embeddings to effectively reconstruct the original audio.,1. Introduction,[0],[0]
"This structure allows the size of an embedding to scale with the size of the input and encode over much longer time scales.
",1. Introduction,[0],[0]
"Recent breakthroughs in generative modeling of images (Kingma & Welling, 2013; Goodfellow et al., 2014; van den Oord et al., 2016b) have been predicated on the availability of high-quality and large-scale datasets such as MNIST (LeCun et al., 1998), SVHN (Netzer et al., 2011), CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Deng et al., 2009).",1. Introduction,[0],[0]
"While generative models are notoriously hard to evaluate (Theis et al., 2015), these datasets provide a common test bed for consistent qualitative and quantitative
evaluation, such as with the use of the Inception score (Salimans et al., 2016).
",1. Introduction,[0],[0]
We recognized the need for an audio dataset that was as approachable as those in the image domain.,1. Introduction,[0],[0]
"Audio signals found in the wild contain multi-scale dependencies that prove particularly difficult to model (Raffel, 2016; BertinMahieux et al., 2011; King et al., 2008; Thickstun et al., 2016), leading many previous efforts at data-driven audio synthesis to focus on more constrained domains such as texture synthesis or training small parametric models (Sarroff & Casey, 2014; McDermott et al., 2009).
",1. Introduction,[0],[0]
"Inspired by the large, high-quality image datasets, NSynth is an order of magnitude larger than comparable public datasets (Humphrey, 2016).",1. Introduction,[0],[0]
"It consists of ∼300k foursecond annotated notes sampled at 16kHz from ∼1k harmonic musical instruments.
",1. Introduction,[0],[0]
"After introducing the models and describing the dataset, we evaluate the performance of the WaveNet autoencoder over a baseline convolutional autoencoder model trained on spectrograms.",1. Introduction,[0],[0]
"We examine the tasks of reconstruction and interpolation, and analyze the learned space of embeddings.",1. Introduction,[0],[0]
"For qualitative evaluation, we include supplemental audio files for all examples mentioned in this paper.",1. Introduction,[0],[0]
"Despite our best efforts to convey analysis in plots, listening to the samples is essential to understanding this paper and we strongly encourage the reader to listen along as they read.",1. Introduction,[0],[0]
"WaveNet (van den Oord et al., 2016a) is a powerful generative approach to probabilistic modeling of raw audio.",2.1. WaveNet Autoencoder,[0],[0]
In this section we describe our novel WaveNet autoencoder structure.,2.1. WaveNet Autoencoder,[0],[0]
The primary motivation for this approach is to attain consistent long-term structure without external conditioning.,2.1. WaveNet Autoencoder,[0],[0]
"A secondary motivation is to use the learned encodings for applications such as meaningful audio interpolation.
",2.1. WaveNet Autoencoder,[0],[0]
"Recalling the original WaveNet architecture described in (van den Oord et al., 2016a), at each step a stack of dilated convolutions predicts the next sample of audio from a fixed-size input of prior sample values.",2.1. WaveNet Autoencoder,[0],[0]
"The joint probability of the audio x is factorized as a product of conditional probabilities:
p(x) = N∏ i=1",2.1. WaveNet Autoencoder,[0],[0]
"p(xi|x1, ..., xN−1)
Unconditional generation from this model manifests as “babbling” due to the lack of longer term structure (see Supplemental for an audio example).",2.1. WaveNet Autoencoder,[0],[0]
"However, (van den Oord et al., 2016a) showed in the context of speech that long-range structure can be enforced by conditioning on temporally aligned linguistic features.
",2.1. WaveNet Autoencoder,[0],[0]
"Our autoencoder removes the need for that external con-
ditioning.",2.1. WaveNet Autoencoder,[0],[0]
It works by taking raw audio waveform as input from which the encoder produces an embedding Z = f(x).,2.1. WaveNet Autoencoder,[0],[0]
"Next, we causally shift the same input and feed it into the decoder, which reproduces the input waveform.",2.1. WaveNet Autoencoder,[0],[0]
"The joint probablity is now:
p(x) = N∏ i=1",2.1. WaveNet Autoencoder,[0],[0]
"p(xi|x1, ..., xN−1, f(x))
",2.1. WaveNet Autoencoder,[0],[0]
"We could parameterize Z as a latent variable p(Z|x) that we would have to marginalize over (Gulrajani et al., 2016), but in practice we have found this to be less effective.",2.1. WaveNet Autoencoder,[0],[0]
"As discussed in (Chen et al., 2016), this may be due to the decoder being so powerful that it can ignore the latent variables unless they encode a much larger context that’s otherwise inaccessible.
",2.1. WaveNet Autoencoder,[0],[0]
Note that the decoder could completely ignore the deterministic encoding and degenerate to a standard unconditioned WaveNet.,2.1. WaveNet Autoencoder,[0],[0]
"However, because the encoding is a strong signal for the supervised output, the model learns to utilize it.
",2.1. WaveNet Autoencoder,[0],[0]
"During inference, the decoder autoregressively generates a single output sample at a time conditioned on an embedding and a starting palette of zeros.",2.1. WaveNet Autoencoder,[0],[0]
"The embedding can be inferred deterministically from audio or drawn from other points in the embedding space, e.g. through interpolation or analogy (White, 2016).
",2.1. WaveNet Autoencoder,[0],[0]
Figure 1b depicts the model architecture in more detail.,2.1. WaveNet Autoencoder,[0],[0]
The temporal encoder model is a 30-layer nonlinear residual network of dilated convolutions followed by 1x1 convolutions.,2.1. WaveNet Autoencoder,[0],[0]
Each convolution has 128 channels and precedes a ReLU nonlinearity.,2.1. WaveNet Autoencoder,[0],[0]
The output feed into another 1x1 convolution before downsampling with average pooling to get the encoding Z.,2.1. WaveNet Autoencoder,[0],[0]
We call it a ‘temporal encoding’ because the result is a sequence of hidden codes with separate dimensions for time and channel.,2.1. WaveNet Autoencoder,[0],[0]
The time resolution depends on the stride of the pooling.,2.1. WaveNet Autoencoder,[0],[0]
"We tune the stride, keeping total size of the embedding constant (∼32x compression).",2.1. WaveNet Autoencoder,[0],[0]
"In the trade-off between temporal resolution and embedding expressivity, we find a sweet spot at a stride of 512 (32ms) with 16 dimensions per timestep, yielding a 125x16 embedding for each NSynth note.",2.1. WaveNet Autoencoder,[0],[0]
"We additionally explore models that condition on global attributes by utilizing a one-hot pitch embedding.
",2.1. WaveNet Autoencoder,[0],[0]
"The WaveNet decoder model is similar to that presented in (van den Oord et al., 2016a).",2.1. WaveNet Autoencoder,[0],[0]
We condition it by biasing every layer with a different linear projection of the temporal embeddings.,2.1. WaveNet Autoencoder,[0],[0]
"Since the decoder does not downsample anywhere in the network, we upsample the temporal encodings
to the original audio rate with nearest neighbor interpolation.",2.1. WaveNet Autoencoder,[0],[0]
"As in the original design, we quantize our input audio using 8-bit mu-law encoding and predict each output step with a softmax over the resulting 256 values.
",2.1. WaveNet Autoencoder,[0],[0]
"This WaveNet autoencoder is a deep and expressive network, but has the trade-off of being limited in temporal context to the chunk-size of the training audio.",2.1. WaveNet Autoencoder,[0],[0]
"While this is sufficient for consistently encoding the identity of a sound and interpolating among many sounds, achieving larger context would be better and is an area of ongoing research.",2.1. WaveNet Autoencoder,[0],[0]
"As a point of comparison, we set out to create a straightforward yet strong baseline for the our neural audio synthesis experiments.",2.2. Baseline: Spectral Autoencoder,[0],[0]
"Inspired by image models (Vincent et al., 2010), we explore convolutional autoencoder structures with a bottleneck that forces the model to find a compressed representation for an entire note.",2.2. Baseline: Spectral Autoencoder,[0],[0]
Figure 1a shows a block diagram of our baseline architecture.,2.2. Baseline: Spectral Autoencoder,[0],[0]
The convolutional encoder and decoder are each 10 layers deep with 2x2 strides and 4x4 kernels.,2.2. Baseline: Spectral Autoencoder,[0],[0]
"Every layer is followed by a leaky-ReLU (0.1) nonlinearity and batch normalization (Ioffe & Szegedy, 2015).",2.2. Baseline: Spectral Autoencoder,[0],[0]
"The number of channels grows from 128 to 1024 before a linear fully-connected layer creates a single 19841 dimensional hidden vector (Z) to match that of the WaveNet autoencoder.
",2.2. Baseline: Spectral Autoencoder,[0],[0]
"Given the simplicity of the architecture, we examined a range of input representations.",2.2. Baseline: Spectral Autoencoder,[0],[0]
Using the raw waveform as input with a mean-squared error (MSE) cost proved difficult to train and highlighted the inadequacy of the independent Gaussian assumption.,2.2. Baseline: Spectral Autoencoder,[0],[0]
"Spectral representations such as the real and imaginary components of the Fast Fourier Transform (FFT) fared better, but suffered from low perceptual quality despite achieving low MSE cost.",2.2. Baseline: Spectral Autoencoder,[0],[0]
"We found that training on the log magnitude of the power spectra, peak normalized to be between 0 and 1, correlated better with perceptual distortion.
",2.2. Baseline: Spectral Autoencoder,[0],[0]
"We also explored several representations of phase, including instantaneous frequency and circular normal cost functions (see Supplemental), but in each case independently estimating phase and magnitude led to poor sample quality due to phase errors.",2.2. Baseline: Spectral Autoencoder,[0],[0]
"We find a large improvement by estimating only the magnitude and using a well established iterative technique to reconstruct the phase (Griffin & Lim, 1984).",2.2. Baseline: Spectral Autoencoder,[0],[0]
"To get the best results, we used a large FFT size (1024) relative to the hop size (256) and ran the algorithm for 1000 iterations.",2.2. Baseline: Spectral Autoencoder,[0],[0]
"As a final heuristic, we weighted the MSE loss, starting at 10 for 0Hz and decreasing linearly to
1This size was aligned with a WaveNet autoencoder that had a pooling stride of 1024 and a 62x32 embedding.
1 at 4000Hz and above.",2.2. Baseline: Spectral Autoencoder,[0],[0]
"At the expense of some precision in timbre, this created more phase coherence for the fundamentals of notes, where errors in the linear spectrum lead to a larger relative error in frequency.",2.2. Baseline: Spectral Autoencoder,[0],[0]
"We train all models with stochastic gradient descent with an Adam optimizer (Kingma & Ba, 2014).",2.3. Training,[0],[0]
"The baseline models commonly use a learning rate of 1e-4, while the WaveNet models use a schedule, starting at 2e-4 and descending to 6e-5, 2e-5, and 6e-6 at iterations 120k, 180k, and 240k respectively.",2.3. Training,[0],[0]
The baseline models train asynchronously for 1800k iterations with a batch size of 8.,2.3. Training,[0],[0]
The WaveNet models train synchronously for 250k iterations with a batch size of 32.,2.3. Training,[0],[0]
"To evaluate our WaveNet autoencoder model, we wanted an audio dataset that let us explore the learned embeddings.",3. The NSynth Dataset,[0],[0]
"Musical notes are an ideal setting for this study as we hypothesize that the embeddings will capture structure such as pitch, dynamics, and timbre.",3. The NSynth Dataset,[0],[0]
"While several smaller datasets currently exist (Goto et al., 2003; Romani Picas et al., 2015), deep networks train better on abundant, highquality data, motivating the development of a new dataset.",3. The NSynth Dataset,[0],[0]
"NSynth consists of 306 043 musical notes, each with a unique pitch, timbre, and envelope.",3.1. A Dataset of Musical Notes,[0],[0]
"For 1006 instruments from commercial sample libraries, we generated four second, monophonic 16kHz audio snippets, referred to as notes, by ranging over every pitch of a standard MIDI piano (21-108) as well as five different velocities2 (25, 50, 75, 100, 127).",3.1. A Dataset of Musical Notes,[0],[0]
The note was held for the first three seconds and allowed to decay for the final second.,3.1. A Dataset of Musical Notes,[0],[0]
"Some instruments are not capable of producing all 88 pitches in this range, resulting in an average of 65.4 pitches per instrument.",3.1. A Dataset of Musical Notes,[0],[0]
"Furthermore, the commercial sample packs occasionally contain duplicate sounds across multiple velocities, leaving an average of 4.75 unique velocities per pitch.",3.1. A Dataset of Musical Notes,[0],[0]
"We also annotated each of the notes with three additional pieces of information based on a combination of human evaluation and heuristic algorithms:
• Source: The method of sound production for the note’s instrument.",3.2. Annotations,[0],[0]
"This can be one of ‘acoustic’ or
2MIDI velocity is similar to volume control",3.2. Annotations,[0],[0]
and they have a direct relationship.,3.2. Annotations,[0],[0]
"For physical intuition, higher velocity corresponds to pressing a piano key harder.
",3.2. Annotations,[0],[0]
"‘electronic’ for instruments that were recorded from acoustic or electronic instruments, respectively, or ‘synthetic’ for synthesized instruments.
•",3.2. Annotations,[0],[0]
Family: The high-level family of which the note’s instrument is a member.,3.2. Annotations,[0],[0]
Each instrument is a member of exactly one family.,3.2. Annotations,[0],[0]
"See Supplemental for the complete list.
",3.2. Annotations,[0],[0]
• Qualities: Sonic qualities of the note.,3.2. Annotations,[0],[0]
See Supplemental for the complete list of classes and their cooccurrences.,3.2. Annotations,[0],[0]
Each note is annotated with zero or more qualities.,3.2. Annotations,[0],[0]
The full NSynth dataset will be made publicly available in a serialized format after publication.,3.3. Availability,[0],[0]
is available for download at http://download.magenta.tensorflow.org/hans as TFRecord files split into training and holdout sets.,3.3. Availability,[0],[0]
Each note is represented by a serialized TensorFlow Example protocol buffer containing the note and annotations.,3.3. Availability,[0],[0]
Details of the format can be found in the README.,3.3. Availability,[0],[0]
"We evaluate and analyze our models on the tasks of note reconstruction, instrument interpolation, and pitch interpolation.
",4. Evaluation,[0],[0]
Audio is notoriously hard to represent visually.,4. Evaluation,[0],[0]
"Magnitude spectrograms capture many aspects of a signal for analytics, but two spectrograms that appear very similar to the eye can correspond to audio that sound drastically different due to phase differences.",4. Evaluation,[0],[0]
"We have included supplemental audio examples of every plot and encourage the reader to listen along as they read.
",4. Evaluation,[0],[0]
"That said, in our analysis we present examples as plots of the constant-q transform (CQT) (Brown, 1991; Schörkhuber & Klapuri, 2010), which is useful because it is shift invariant to changes in the fundamental frequency.",4. Evaluation,[0],[0]
"In this way, the structure and envelope of the overtone series (higher harmonics) determines the dynamics and timbre of a note, regardless of its base frequency.",4. Evaluation,[0],[0]
"However, due to the logarithmic binning of frequencies, transient noise-like impulses appear as rainbow “pyramidal spikes” rather than straight broadband lines.",4. Evaluation,[0],[0]
"We display CQTs with a pitch range of 24-96 (C2-C8), hop size of 256, 40 bins per octave, and a filter scale of 0.8.
",4. Evaluation,[0],[0]
"As phase plays such an essential part in sample quality, we have attempted to show both magnitude and phase on the same plot.",4. Evaluation,[0],[0]
"The intensity of lines is proportional to the log magnitude of the power spectrum while the color
is given by the derivative of the unrolled phase (‘instantaneous frequency’)",4. Evaluation,[0],[0]
"(Boashash, 1992).",4. Evaluation,[0],[0]
We display the derivative of the phase because it creates a solid continuous line for a harmonic of a consistent frequency.,4. Evaluation,[0],[0]
"We can understand this because if the instantaneous frequency of a harmonic (fharm) and an FFT bin (fbin) are not exactly equal, each timestep will introduce a constant phase shift, ∆φ = (fbin − fharm) hopsizesamplerate .",4. Evaluation,[0],[0]
"Figure 2 displays CQT spectrograms for notes from 3 different instruments in the holdout set, where the original note spectrograms are on the first column and the model reconstruction spectrograms are on the second and third columns.",4.1. Reconstruction,[0],[0]
"Each note has a similar structure with some noise on onset, a fundamental frequency with a series of harmonics, and a decay.",4.1. Reconstruction,[0],[0]
"For all the WaveNet models, there is a slight built-in distortion due to the compression of the mulaw encoding.",4.1. Reconstruction,[0],[0]
"It is a minor effect for many samples, but is more pronounced for lower frequencies.",4.1. Reconstruction,[0],[0]
"Using different representations without this distortion is an ongoing area of research.
",4.1. Reconstruction,[0],[0]
"While each spectrogram matches the general contour of the original note, we can hear a pronounced difference in sample quality that we can ascribe to certain features.",4.1. Reconstruction,[0],[0]
"For the Glockenspiel, we can see that the WaveNet autoencoder reproduces the magnitude and phase of the fundamental (solid blue stripe, (A)), and also the noise on attack (vertical rainbow spike (B)).",4.1. Reconstruction,[0],[0]
"There is a slight error in the fun-
damental as it starts a little high and quickly descends to the correct pitch (C).",4.1. Reconstruction,[0],[0]
"In contrast, the baseline has a more percussive, multitonal sound, similar to a bell or gong.",4.1. Reconstruction,[0],[0]
"The fundamental is still present, but so are other frequencies, and the phases estimated from the Griffin-Lim procedure are noisy as indicated by the blurred horizontal rainbow texture (D).
",4.1. Reconstruction,[0],[0]
"The electric piano has a more clearly defined harmonic series (the horizontal rainbow solid lines, (E)) and a noise on the beginning and end of the note (vertical rainbow spikes, (F)).",4.1. Reconstruction,[0],[0]
"Listening to the sound, we hear that it is slightly distorted, which promotes these upper harmonics.",4.1. Reconstruction,[0],[0]
"Both the WaveNet autoencoder and the baseline produce spectrograms with similar shapes to the original, but with different types of phase artifacts.",4.1. Reconstruction,[0],[0]
"The WaveNet model has sufficient phase structure to model the distortion, but has a slight wavering of the instantaneous frequency of some harmonics, as seen in the color change in harmonic stripes (G).",4.1. Reconstruction,[0],[0]
"In contrast, the baseline lacks the structure in phase to maintain the punchy character of the original note, and produces a duller sound that is slightly out of tune.",4.1. Reconstruction,[0],[0]
"This is represented in the less brightly colored harmonics due to phase noise (H).
",4.1. Reconstruction,[0],[0]
The flugelhorn displays perhaps the starkest difference between the two models.,4.1. Reconstruction,[0],[0]
"The sound combines rich harmonics (many lines), non-tonal wind and lip noise (background color), and vibrato - oscillation of pitch that results in a corresponding rainbow of color in all of the harmonics.",4.1. Reconstruction,[0],[0]
"While the WaveNet autoencoder does not replicate the ex-
act trace of the vibrato (I), it creates a very similar spectrogram with oscillations in the instantaneous frequency at all levels synced across the harmonics (J).",4.1. Reconstruction,[0],[0]
This results in a rich and natural sounding reconstruction with all three aspects of the original sound.,4.1. Reconstruction,[0],[0]
"The baseline, by comparison, is unable to model such structure.",4.1. Reconstruction,[0],[0]
"It creates a more or less correct harmonic series, but the phase has lots of random perturbations.",4.1. Reconstruction,[0],[0]
"Visually this shows up as colors which are faded and speckled with rainbow noise (K), which contrasts with the bright colors of the original and WaveNet examples.",4.1. Reconstruction,[0],[0]
"Acoustically, this manifests as an unappealing buzzing sound laid over an inexpressive and consistent series of harmonics.",4.1. Reconstruction,[0],[0]
The WaveNet model also produces a few inaudible discontinuities visually evidenced by the vertical rainbow spikes (L).,4.1. Reconstruction,[0],[0]
"Inspired by the use of the Inception Score for images (Salimans et al., 2016), we train a multi-task classification network to perform a quantitative comparison of the model reconstructions by predicting pitch and quality labels on the NSynth dataset (details in the Supplemental).",4.1.1. QUANTITATIVE COMPARISON,[0],[0]
"The network configuration is the same as the baseline encoder and testing is done on reconstructions of a randomly chosen subset of 4096 examples from the held-out set.
",4.1.1. QUANTITATIVE COMPARISON,[0],[0]
The results in Table 1 confirm our qualititive observation that the WaveNet reconstructions are of superior quality.,4.1.1. QUANTITATIVE COMPARISON,[0],[0]
"The classifier is ∼70% more successful at extracting pitch from the reconstructed WaveNet samples than the baseline
and several points higher for predicting quality information, giving an accuracy roughly equal to the original audio.",4.1.1. QUANTITATIVE COMPARISON,[0],[0]
"Given the limited factors of variation in the dataset, we know that a successful embedding space (Z) should span the range of timbre and dynamics in its reconstructions.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"In Figure 3, we show reconstructions from linear interpolations (0.5:0.5) in the Z space among three different instruments and additionally compare these to interpolations in the original audio space.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
The latter are simple superpositions of the individual instruments’ spectrograms.,4.2. Interpolation in Timbre and Dynamics,[0],[0]
"This is perceptually equivalent to the two instruments being played at the same time.
",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"In contrast, we find that the generative models fuse aspects of the instruments.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"As we saw in Section 4.1, the WaveNet autoencoder models the data much more realistically than the baseline, so it is no surprise that it also learns a manifold of codes that yield more perceptually interesting re-
constructions.
",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"For example, in the interpolated note between the bass and flute (Figure 3, column 2), we can hear and see that both the baseline and WaveNet models blend the harmonic structure of the two instruments while imposing the amplitude envelope of the bass note onto the upper harmonics of the flute note.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"However, the WaveNet model goes beyond this to create a dynamic mixing of the overtones in time, even jumping to a higher harmonic at the end of the note (A).",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"This sound captures expressive aspects of the timbre and dynamics of both the bass and flute, but is distinctly separate from either original note.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"This contrasts with the interpolation in audio space, where the dynamics and timbre of the two notes is independent.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"The baseline model also introduces phase distortion similar to those in the reconstructions of the bass and flute.
",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"We see this phenomenon again in the interpolation between flute and organ (Figure 3, column 4).",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"Both models also seem to create new harmonic structure, rather than just overlay the original harmonics.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"The WaveNet model adds additional harmonics as well as a sub-harmonic to the original flute note, all while preserving phase relationships (B).",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"The resulting sound has the breathiness of a flute, with the upper frequency modulation of an organ.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"By contrast, the lack of phase structure in the baseline leads to a new harmonic yet dull sound lacking a unique character.
",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"The WaveNet model additionally has a tendency to exaggerate amplitude modulation behavior, while the baseline suppresses it.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
"If we examine the original organ sound (Figure 3, column 5), we can see a subtle modulation signified by the blue harmonics periodically fading to black (C).",4.2. Interpolation in Timbre and Dynamics,[0],[0]
The baseline model misses this behavior completely as it is washed out.,4.2. Interpolation in Timbre and Dynamics,[0],[0]
"Conversely, the WaveNet model amplifies the behavior, adding in new harmonics not present in the original note and modulating all the harmonics.",4.2. Interpolation in Timbre and Dynamics,[0],[0]
This is seen in the figure by four vertical black stripes that align with the four modulations of the original signal (D).,4.2. Interpolation in Timbre and Dynamics,[0],[0]
"By conditioning on pitch during training, we hypothesize that we should be able to generate multiple pitches from a single Z vector that preserve the identity of timbre and dynamics.",4.3. Entanglement of Pitch and Timbre,[0],[0]
"Our initial attempts were unsuccessful, as it seems our models had learned to ignore the conditioning variable.",4.3. Entanglement of Pitch and Timbre,[0],[0]
We investigate this further with classification and correlation studies.,4.3. Entanglement of Pitch and Timbre,[0],[0]
One way to study the entanglement of pitch andZ is to consider the pitch classification accuracy from embeddings.,4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"If training with pitch conditioning disentangles the represen-
tation of pitch and timbre, then we would expect a linear pitch classifier trained on the embeddings to drop in accuracy.",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"To test this, we train a series of baseline autoencoder models with different embedding sizes, both with and without pitch conditioning.",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"For each model, we then train a logistic regression pitch classifier on its embeddings and test on a random sample of 4096 held-out embeddings.
",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
The first two rows of Table 2 demonstrate that the baseline and WaveNet models decrease in classification accuracy by 13-30% when adding pitch conditioning during training.,4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
This is indicative a reduced presence of pitch information in the latent code and thus a decoupling of pitch and timbre information.,4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"Further, as the total embedding size decreases below 512, the accuracy drop becomes much more pronounced, reaching a 75% relative decrease.",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"This is likely due to the greater expressivity of larger embeddings, where there is less to be gained from utilizing the pitch conditioning.",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"However, as the embedding size decreases, so too does reconstruction quality.",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"This is more pronounced for the WaveNet models, which have farther to fall in terms of sample quality.
",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"As a proof of principle, we find that for a baseline model with an embedding size of 128, we are able to successfully balance reconstruction quality and response to conditioning.",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"Figure 4 demonstrates two octaves of a C major chord created from a single embedding of an electric piano note, but conditioned on different pitches.",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
The resulting harmonic structure of the original note is only partially preserved across the range.,4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"As we shift the pitch upwards, a sub-harmonic emerges (A) such that the pitch +12 note is similar to the original except that the harmonics of the octave are accentuated in amplitude.",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
"This aligns with our pitch classification results, where we find that pitches are most commonly confused with those one octave away (see Supplemental).",4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
These errors can account for as much as 20% absolute classification error.,4.3.1. PITCH CLASSIFICATION FROM Z,[0],[0]
We can gain further insight into the relationship between timbre and pitch by examining the correlation of WaveNet embeddings among pitches for a given instrument.,4.3.2. Z CORRELATION ACROSS PITCH,[0],[0]
Figure 5 shows correlations for several instruments across their entire 88 note range at velocity 127.,4.3.2. Z CORRELATION ACROSS PITCH,[0],[0]
We see that each instrument has a unique partitioning into two or more registers over which notes of different pitches have similar embeddings.,4.3.2. Z CORRELATION ACROSS PITCH,[0],[0]
Even the average over all instruments shows a broad distinction between high and low registers.,4.3.2. Z CORRELATION ACROSS PITCH,[0],[0]
"On reflection, this is unsurprising as the timbre and dynamics of an instrument can vary dramatically across its range.",4.3.2. Z CORRELATION ACROSS PITCH,[0],[0]
"In this paper, we have introduced a WaveNet autoencoder model that captures long term structure without the need for external conditioning and demonstrated its effectiveness on the new NSynth dataset for generative modeling of audio.
",5. Conclusion and Future Directions,[0],[0]
The WaveNet autoencoder that we describe is a powerful representation for which there remain multiple avenues of exploration.,5. Conclusion and Future Directions,[0],[0]
It builds upon the fine-grained local understanding of the original WaveNet work and provides access to a useful hidden space.,5. Conclusion and Future Directions,[0],[0]
"However, due to memory constraints, it is unable to fully capture global context.",5. Conclusion and Future Directions,[0],[0]
"Overcoming this limitation is an important open problem.
",5. Conclusion and Future Directions,[0],[0]
NSynth was inspired by image recognition datasets that have been core to recent progress in deep learning.,5. Conclusion and Future Directions,[0],[0]
We encourage the broader community to use NSynth as a benchmark and entry point into audio machine learning.,5. Conclusion and Future Directions,[0],[0]
We also view NSynth as a building block for future datasets and envision a high-quality multi-note dataset for tasks like generation and transcription that involve learning complex language-like dependencies.,5. Conclusion and Future Directions,[0],[0]
"A huge thanks to Hans Bernhard with the dataset, Colin Raffel for crucial conversations, and Sageev Oore for thoughtful analysis.",Acknowledgments,[0],[0]
Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets.,abstractText,[0],[0]
"In this paper, we offer contributions in both these areas to enable similar progress in audio modeling.",abstractText,[0],[0]
"First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform.",abstractText,[0],[0]
"Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets.",abstractText,[0],[0]
"Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline.",abstractText,[0],[0]
"Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.",abstractText,[0],[0]
Neural Audio Synthesis of Musical Notes  with WaveNet Autoencoders,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 395–400 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
395",text,[0],[0]
"Cross-lingual models for NLP tasks are important since they can be used on data from a new language without requiring annotation from the new language (Ji et al., 2014, 2015).",1 Introduction,[0],[0]
"This paper investigates the use of multi-lingual embeddings (Faruqui and Dyer, 2014; Upadhyay et al., 2016) for building cross-lingual models for the task of coreference resolution (Ng and Cardie, 2002; Pradhan et al., 2012).",1 Introduction,[0],[0]
"Consider the following text from a Spanish news article:
“Tormenta de nieve afecta a 100 millones de personas en EEUU.",1 Introduction,[0],[0]
"Unos 100 millones de personas enfrentaban el sábado nuevas dificultades tras la enorme tormenta de nieve de hace dı́as en la costa este de Estados Unidos.”
",1 Introduction,[0],[0]
The mentions “EEUU” (“US” in English) and “Estados Unidos” (“United States” in English) are coreferent.,1 Introduction,[0],[0]
"A coreference model trained on English data is unlikely to coreference these two
mentions in Spanish since these mentions did not appear in English data and a regular English style abbreviation of “Estados Unidos” will be “EU” instead of “EEUU”.",1 Introduction,[0],[0]
"But in the bilingual EnglishSpanish word embedding space, the word embedding of “EEUU” sits close to the word embedding of “US” and the sum of word embeddings of “Estados Unidos” sit close to the sum of word embeddings of “United States”.",1 Introduction,[0],[0]
"Therefore, a coreference model trained using English-Spanish bilingual word embeddings on English data has the potential to make the correct coreference decision between “EEUU” and “Estados Unidos” without ever encountering these mentions in training data.
",1 Introduction,[0],[0]
The contributions of this paper are two-fold.,1 Introduction,[0],[0]
"Firstly, we propose an entity-centric neural crosslingual coreference model.",1 Introduction,[0],[0]
"This model, when trained on English and tested on Chinese and Spanish from the TAC 2015 Trilingual Entity Discovery and Linking (EDL) Task (Ji et al., 2015), achieves competitive results to models trained directly on Chinese and Spanish respectively.",1 Introduction,[0],[0]
"Secondly, a pipeline consisting of this coreference model and an Entity Linking (henceforth EL) model can achieve superior linking accuracy than the official top ranking system in 2015 on Chinese and Spanish test sets, without using any supervision in Chinese or Spanish.
",1 Introduction,[0],[0]
"Although most of the active coreference research is on solving the problem of noun phrase coreference resolution in the Ontonotes data set, invigorated by the 2011 and 2012 CoNLL shared task (Pradhan et al., 2011, 2012), there are many important applications/end tasks where the mentions of interest are not noun phrases.",1 Introduction,[0],[0]
"Consider the sentence,
“(U.S. president Barack Obama who started ((his) political career) in (Illinois)), was born in (Hawaii).”
",1 Introduction,[0],[0]
"The bracketing represents the Ontonotes style
noun phrases and underlines represent the phrases that should be linked to Wikipedia by an EL system.",1 Introduction,[0],[0]
Note that mentions like “U.S.” and “Barack Obama” do not align with any noun phrase.,1 Introduction,[0],[0]
"Therefore, in this work, we focus on coreference on mentions that arise in our end task of entity linking and conduct experiments on TAC TriLingual 2015 data sets consisting of English, Chinese and Spanish.",1 Introduction,[0],[0]
"Each mention has a mention type (m type) of either name or nominal and an entity type (e type) of Person (PER) / Location (LOC) / GPE / facility (FAC) / organization (ORG) (following standard TAC (Ji et al., 2015) notations).
",2 Coreference Model,[0],[0]
The objective of our model is to compute a function that can decide whether two partially constructed entities should be coreferenced or not.,2 Coreference Model,[0],[0]
We gradually merge the mentions in the given document to form entities.,2 Coreference Model,[0],[0]
"Mentions are considered in the order of names and then nominals and within each group, mentions are arranged in the order they appear in the document.",2 Coreference Model,[0],[0]
"Suppose, the sorted order of mentions are m1, . . ., mN1 , mN1+1, . . .",2 Coreference Model,[0],[0]
", mN1+N2 where N1 and N2 are respectively the number of the named and nominal mentions.",2 Coreference Model,[0],[0]
A singleton entity is created from each mention.,2 Coreference Model,[0],[0]
"Let the order of entities be e1, . . .",2 Coreference Model,[0],[0]
", eN1 , eN1+1, . . .",2 Coreference Model,[0],[0]
", eN1+N2 .",2 Coreference Model,[0],[0]
"We merge the named entities with other named entities, then nominal entities with named entities in the same sentence and finally we merge nominal entities across sentences as follows:",2 Coreference Model,[0],[0]
"Step 1: For each named entity ei (1 ≤ i ≤ N1), antecedents are all entities ej (1 ≤ j ≤",2 Coreference Model,[0],[0]
i,2 Coreference Model,[0],[0]
− 1) such that ej and ei have same e type.,2 Coreference Model,[0],[0]
"Training examples are triplets of the form (ei, ej , yij).",2 Coreference Model,[0],[0]
"If ei and ej are coreferent (meaning, yij=1), they are merged.",2 Coreference Model,[0],[0]
Step 2: For each nominal entity ei (N1 + 1 ≤,2 Coreference Model,[0],[0]
"i ≤ N1 + N2), we consider antecedents ej such that ei and ej have the same e type and ej has some mention that appears in the same sentence as some mention in ei. Training examples are generated and entities are merged as in the previous step.",2 Coreference Model,[0],[0]
Step 3:,2 Coreference Model,[0],[0]
"This is similar to previous step, except ei and ej have no sentence restriction.",2 Coreference Model,[0],[0]
Features:,2 Coreference Model,[0],[0]
"For each training triplet (e1, e2, y12), the network takes the entity pair (e1, e2) as input and tries to predict y12 as output.",2 Coreference Model,[0],[0]
"Since each entity
represents a set of mentions, the entity-pair embedding is obtained from the embeddings of mention pairs generated from the cross product of the entity pair.",2 Coreference Model,[0],[0]
"Let M(e1, e2) be the set {(mi,mj)",2 Coreference Model,[0],[0]
"| (mi,mj)∈ e1 × e2} .",2 Coreference Model,[0],[0]
"For each (mi,mj) ∈ M(e1, e2), a feature vector φmi,mj is computed.",2 Coreference Model,[0],[0]
"Then, every feature in φmi,mj is embedded as a vector in the real space.",2 Coreference Model,[0],[0]
"Let vmi,mj dentote the concatenation of embeddings of all features in φmi,mj .",2 Coreference Model,[0],[0]
Embeddings of all features except the words are learned in the training process.,2 Coreference Model,[0],[0]
Word embeddings are pre-trained.,2 Coreference Model,[0],[0]
"vmi,mj includes the following language independent features: String match: whether mi is a substring or exact match of mj and vice versa (e.g. mi = “Barack Obama” and mj = “Obama”) Distance: word distance and sentence distance between mi and mj discretized into bins m type: concatenation of m types for mi and mj e type: concatenation of e types for mi and mj Acronym: whether mi is an acronym of mj or vice versa (e.g. mi = “United States” and mj = “US”)",2 Coreference Model,[0],[0]
First name mismatch: whether mi and mj belong to e type of PERSON with the same last name but different first name (e.g. mi=“Barack Obama” and mj = “Michelle Obama”),2 Coreference Model,[0],[0]
"Speaker detection: whether mi and mj both occur in the context of words indicating speech e.g. “say”, “said”",2 Coreference Model,[0],[0]
"In addition, vmi,mj includes the average of the word embeddings of mi and average of the word embeddings of mj .",2 Coreference Model,[0],[0]
The network architecture from the input to the output is shown in figure 1.,2.1 Network Architecture,[0],[0]
Embedding Layer:,2.1 Network Architecture,[0],[0]
"For each training triplet (e1, e2, y), a sequence of vectors vmi,mj (for each ((mi,mj) ∈ M(e1, e2))) is given as input to the network.",2.1 Network Architecture,[0],[0]
"Relu Layer: vrmi,mj = max(0,W
(1)vmi",2.1 Network Architecture,[0],[0]
",mj ) Attention Layer: To generate the entity-pair embedding, we need to combine the embeddings of mention pairs generated from the entity-pair.",2.1 Network Architecture,[0],[0]
"Consider two entities e1 = (President1, Obama)} and e2 = {(President2, Clinton)}.",2.1 Network Architecture,[0],[0]
Here the superscripts are used to indicate two different mentions with the same surface form.,2.1 Network Architecture,[0],[0]
"Since the named mention pair (Obama, Clinton) has no string overlap, e1 and e2 should not be coreferenced even though the
nominal mention pair (President1, President2) has full string overlap.",2.1 Network Architecture,[0],[0]
"So, while combining the embeddings for the mention pairs, mention pairs with m type (name, name) should get higher weight than mention pairs with m type (nominal, nominal).",2.1 Network Architecture,[0],[0]
The entity pair embedding is the weighted sum of the mention-pair embeddings.,2.1 Network Architecture,[0],[0]
"We introduce 4 parameters aname,name, aname,nominal, anominal,nominal and anominal,name as weights for mention pair embeddings with m types of (name, name), (name, nominal), (nominal, nominal) and (nominal, name) respectively.",2.1 Network Architecture,[0],[0]
"The entity pair embedding is computed as follows:
vae1,e2 =∑ (mi,mj)∈M(e1,e2) am type(mi),m type(mj) N vrmi,mj
Here N is a normalizing constant given by:
N = √ ∑ (mi,mj)∈M(e1,e2) a2m type(mi),m type(mj)
",2.1 Network Architecture,[0],[0]
This layer represents attention over the mention pair embeddings where attention weights are based on the m types of the mention pairs.,2.1 Network Architecture,[0],[0]
"Sigmoid Layer: vse1,e2 = σ(W
(2)vae1,e2) Output Layer:
P (y12 = 1|e1, e2) = 1
1 + e−w s.vse1,e2
The training objective is to maximize L. L = ∏ d∈D ∏ (e1,e2,y12)∈Sd P (y12|e1, e2;W (1),W (2), a, ws) (1) Here D is the corpus and Sd is the training triplets generated from document d.
Decoding proceeds similarly to training algorithm, except at each of the three steps, for each entity ei, the highest scoring antecdent ej is selected and if the score is above a threshold, ei and ej are merged.",2.1 Network Architecture,[0],[0]
"We use our recently proposed cross-lingual EL model, described in (Sil et al., 2018), where our target is to perform “zero shot learning” (Socher et al., 2013; Palatucci et al., 2009).",3 A Zero-shot Entity Linking model,[0],[0]
"We train an EL model on English and use it to decode on any other language, provided that we have access to multi-lingual embeddings from English and the target language.",3 A Zero-shot Entity Linking model,[0],[0]
We briefly describe our techniques here and direct the interested readers to the paper.,3 A Zero-shot Entity Linking model,[0],[0]
"The EL model computes several similarity/coherence scores S in a “feature abstraction layer” which computes several measures of similarity between the context of the mention m in the query document and the context of the candidate link’s Wikipedia page which are fed to a
feed-forward neural layer which acts as a binary classifier to predict the correct link for m. Specifically, the feature abstraction layer computes cosine similarities (Sil and Florian, 2016) between the representations of the source query document and the target Wikipedia pages over various granularities.",3 A Zero-shot Entity Linking model,[0],[0]
These representations are computed by performing CNNs and LSTMs over the context of the entities.,3 A Zero-shot Entity Linking model,[0],[0]
Then these similarities are fed into a Multi-perspective Binning layer which maps each similarity into a higher dimensional vector.,3 A Zero-shot Entity Linking model,[0],[0]
"We also train fine-grained similarities and dissimilarities between the query and candidate document from multiple perspectives, combined with convolution and tensor networks.
",3 A Zero-shot Entity Linking model,[0],[0]
The model achieves state-of-the-art (SOTA) results on English benchmark EL datasets and also performs surprisingly well on Spanish and Chinese.,3 A Zero-shot Entity Linking model,[0],[0]
"However, although the EL model is “zeroshot”, the within-document coreference resolution in the system is a language-dependent SOTA coreference system that has won multiple TACKBP (Ji et al., 2015; Sil et al., 2015) evaluations but is trained on the target language.",3 A Zero-shot Entity Linking model,[0],[0]
"Hence, our aim is to apply our proposed coreference model to the EL system to perform an extrinsic evaluation of our proposed algorithm.",3 A Zero-shot Entity Linking model,[0],[0]
We evaluate cross-lingual transfer of coreference models on the TAC 2015 Tri-Lingual EL datasets.,4 Experiments,[0],[0]
"It contains mentions annotated with their grounded Freebase 1 links (if such links exist) or corpus-wide clustering information for 3 languages: English (henceforth, En), Chinese (henceforth, Zh) and Spanish (henceforth, Es).",4 Experiments,[0],[0]
Table 1 shows the size of the training and test sets for the three languages.,4 Experiments,[0],[0]
The documents come from two genres of newswire and discussion forums.,4 Experiments,[0],[0]
"The mentions in this dataset are either named entities or nominals that belong to five types: PER, ORG, GPE, LOC and FAC.",4 Experiments,[0],[0]
Hyperparameters: Every feature is embedded in a 50 dimensional space except the words which reside in a 300 dimensional space.,4 Experiments,[0],[0]
The Relu and Sigmoid layers have 100 and 500 neurons respectively.,4 Experiments,[0],[0]
"We use SGD for optimization with an initial learning rate of 0.05 which is linearly reduced to
1TAC uses BaseKB, which is a snapshot of Freebase.",4 Experiments,[0],[0]
"SIL18 links entities to Wikipedia and in-turn links them to BaseKB.
0.0001.",4 Experiments,[0],[0]
Our mini batch size is 32 and we train for 50 epochs and keep the best model based on dev set.,4 Experiments,[0],[0]
"Coreference Results: For each language, we follow the official train-test splits made in the TAC 2015 competition.",4 Experiments,[0],[0]
"Except, a small portion of the training set is held out as development set for tuning the models.",4 Experiments,[0],[0]
All experimental results on all languages reported in this paper were obtained on the official test sets.,4 Experiments,[0],[0]
"We used the official CoNLL 2012 evaluation script and report MUC, B3 and CEAF scores and their average (CONLL score).",4 Experiments,[0],[0]
"See Pradhan et al. (2011, 2012).
",4 Experiments,[0],[0]
"To test the competitiveness of our model with other SOTA models, we train the publicly available system of Clark and Manning (2016) (henceforth, C&M16) on the TAC 15 En training set and test on the TAC 15 En test set.",4 Experiments,[0],[0]
The C&M16 system normally outputs both noun phrase mentions and their coreference and is trained on Ontonotes.,4 Experiments,[0],[0]
"To ensure a fair comparison, we changed the configuration of the system to accept gold mention boundaries both during training and testing.",4 Experiments,[0],[0]
"Since the system was unable to deal with partially overlapping mentions, we excluded such mentions in the evaluation.",4 Experiments,[0],[0]
"Table 2 shows that our model outperforms C&M16 by 8 points.
",4 Experiments,[0],[0]
"For cross-lingual experiments, we build monolingual embeddings for En, Zh and Es using the widely used CBOW word2vec model (Mikolov et al., 2013a).",4 Experiments,[0],[0]
"Recently Canonical Correlation Analysis (CCA) (Faruqui and Dyer, 2014), MultiCCA (Ammar et al., 2016) and Weighted Regression (Mikolov et al., 2013b) have been proposed for building the multi-lingual embedding space from monolingual embedding.",4 Experiments,[0],[0]
"In our prelimi-
nary experiments, the technique of Mikolov et al. (2013b) performed the best and so we used it to project the embeddings of Zh and Es onto En.
",4 Experiments,[0],[0]
"In Table 3, “En Model” refers to the model that was trained on the En training set of TAC 15 using multi-lingual embeddings and tested on the Es and Zh testing set of TAC 15.",4 Experiments,[0],[0]
“Es Model” refers to the model trained on Es training set of TAC 15 using Es embeddings.,4 Experiments,[0],[0]
“Zh Model” refers to the model trained on the Zh training set of TAC 15 using Zh embeddings.,4 Experiments,[0],[0]
The En model performs 0.5 point below the Es model on the Es test set.,4 Experiments,[0],[0]
"On the Zh test set, the En model performs only 0.3 point below the Zh model.",4 Experiments,[0],[0]
"Hence, we show that without using any target language training data, the En model with multi-lingual embeddings gives comparable results to models trained on the target language.",4 Experiments,[0],[0]
EL Results: We replace the in-document coreference system (trained on the target language) of SIL18 with our En model to investigate the performance of our proposed algorithm on an extrinsic task.,4 Experiments,[0],[0]
Table 4 shows the EL results on Es and Zh test sets respectively.,4 Experiments,[0],[0]
“EL - Coref” refers to the case where the first step of coreference is not used and EL is used to link the mentions directly to Freebase.,4 Experiments,[0],[0]
“EL + En Coref” refers to the case where the neural english coreference model is first used on Zh or Es data followed by the EL model.,4 Experiments,[0],[0]
"The former is 3 points below the latter on Es and 2.6 points below Zh, implying coreference is a vital task for EL.",4 Experiments,[0],[0]
"Our “EL + En Coref” outperforms the 2015 TAC best system by 0.7 points on Es and 0.8 points on Zh, without requiring any training data for coreference on Es and Zh respectively.",4 Experiments,[0],[0]
"Finally, we show the SOTA results on these two data sets recently reported by SIL18.",4 Experiments,[0],[0]
"Although their EL model does not use any supervision from Es or Zh, their coreference resolution model is trained on a large internal data set on the same language as
the test set .Without",4 Experiments,[0],[0]
"using any in-language training data, our results are competitive to their results (1.2% below on Es and 0.5% below on Zh).",4 Experiments,[0],[0]
"Rule based (Raghunathan et al., 2010) and statistical coreference models (Bengtson and Roth, 2008; Rahman and Ng, 2009; Fernandes et al., 2012; Durrett et al., 2013; Clark and Manning, 2015; Martschat and Strube, 2015; Björkelund and Kuhn, 2014) are hard to transfer across languages due to their use of lexical features or patterns in the rules.",5 Related Work,[0],[0]
Neural coreference is promising since it allows cross-lingual transfer using multilingual embedding.,5 Related Work,[0],[0]
"However, most of the recent neural coreference models (Wiseman et al., 2015, 2016; Clark and Manning, 2015, 2016; Lee et al., 2017) have focused on training and testing on the same language.",5 Related Work,[0],[0]
"In contrast, our model performs cross-lingual coreference.",5 Related Work,[0],[0]
"There have been some recent promising results regarding such cross-lingual models for other tasks, most notably mention detection(Ni et al., 2017) and EL (Tsai and Roth, 2016; Sil and Florian, 2016).",5 Related Work,[0],[0]
"In this work, we show that such promise exists for coreference also.
",5 Related Work,[0],[0]
"The tasks of EL and coreference are intrinsically related, prompting joint models (Durrett and Klein, 2014; Hajishirzi et al., 2013).",5 Related Work,[0],[0]
"However, the recent SOTA was obtained using pipeline models of coreference and EL (Sil et al., 2018).",5 Related Work,[0],[0]
"Compared to a joint model, pipeline models are easier to implement, improve and adapt to a new domain.",5 Related Work,[0],[0]
The proposed cross-lingual coreference model was found to be empirically strong in both intrinsic and extrinsic evaluations in the context of an entity linking task.,6 Conclusion,[0],[0]
We propose an entity-centric neural crosslingual coreference model that builds on multi-lingual embeddings and languageindependent features.,abstractText,[0],[0]
We perform both intrinsic and extrinsic evaluations of our model.,abstractText,[0],[0]
"In the intrinsic evaluation, we show that our model, when trained on English and tested on Chinese and Spanish, achieves competitive results to the models trained directly on Chinese and Spanish respectively.",abstractText,[0],[0]
"In the extrinsic evaluation, we show that our English model helps achieve superior entity linking accuracy on Chinese and Spanish test sets than the top 2015 TAC system without using any annotated data from Chinese or Spanish.",abstractText,[0],[0]
Neural Cross-Lingual Coreference Resolution And Its Application To Entity Linking,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 996–1005 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1092",text,[0],[0]
"Advances in text categorization have the potential to improve systems for analyzing sentiment, inferring authorship or author attributes, making predictions, and many more.",1 Introduction,[0],[0]
"Several past researchers have noticed that methods that reason about the relative salience or importance of passages within a text can lead to improvements (Ko et al., 2004).",1 Introduction,[0],[0]
"Latent variables (Yessenalina et al., 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al., 2016) have all been explored.
",1 Introduction,[0],[0]
"Discourse structure, which represents the organization of a text as a tree (for an example, see Figure 1), might provide cues for the importance of different parts of a text.",1 Introduction,[0],[0]
"Some promising results on sentiment classification tasks support this idea: Bhatia et al. (2015) and Hogenboom et al. (2015) applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification.
",1 Introduction,[0],[0]
"In this paper, we investigate the value of discourse structure for text categorization more broadly, considering five tasks, through the use of a recursive neural network built on an
automatically-derived document parse from a topperforming, open-source discourse parser, DPLP (Ji and Eisenstein, 2014).",1 Introduction,[0],[0]
"Our models learn to weight the importance of a document’s sentences, based on their positions and relations in the discourse tree.",1 Introduction,[0],[0]
"We introduce a new, unnormalized attention mechanism to this end.
",1 Introduction,[0],[0]
Experimental results show that variants of our model outperform prior work on four out of five tasks considered.,1 Introduction,[0],[0]
"Our method unsurprisingly underperforms on the fifth task, making predictions about legislative bills—a genre in which discourse conventions are quite different from those in the discourse parser’s training data.",1 Introduction,[0],[0]
"Further experiments show the effect of discourse parse quality on text categorization performance, suggesting that future improvements to discourse parsing will pay off for text categorization, and validate our new attention mechanism.
996
Our implementation is available at https:// github.com/jiyfeng/disco4textcat.",1 Introduction,[0],[0]
"Rhetorical Structure Theory (RST; Mann and Thompson, 1988) is a theory of discourse that has enjoyed popularity in NLP.",2 Background: Rhetorical Structure Theory,[0],[0]
"RST posits that a document can be represented by a tree whose leaves are elementary discourse units (EDUs, typically clauses or sentences).",2 Background: Rhetorical Structure Theory,[0],[0]
Internal nodes in the tree correspond to spans of sentences that are connected via discourse relations such as CONTRAST and ELABORATION.,2 Background: Rhetorical Structure Theory,[0],[0]
"In most cases, a discourse relation links adjacent spans denoted “nucleus” and “satellite,” with the former more essential to the writer’s purpose than the latter.1
An example of a manually constructed RST parse for a restaurant review is shown in Figure 1.",2 Background: Rhetorical Structure Theory,[0],[0]
"The six EDUs are indexed from A to F ; the discourse tree organizes them hierarchically into increasingly larger spans, with the last CONTRAST relation resulting in a span that covers the whole review.",2 Background: Rhetorical Structure Theory,[0],[0]
"Within each relation, the RST tree indicates the nucleus pointed by an arrow from its satellite (e.g., in the ELABORATION relation, A is the nucleus and B is the satellite).
",2 Background: Rhetorical Structure Theory,[0],[0]
"The information embedded in RST trees has motivated many applications in NLP research, including document summarization (Marcu, 1999), argumentation mining (Azar, 1999), and sentiment analysis (Bhatia et al., 2015).",2 Background: Rhetorical Structure Theory,[0],[0]
"In most applications, RST trees are built by automatic discourse parsing, due to the expensive cost of manual annotation.",2 Background: Rhetorical Structure Theory,[0],[0]
"In this work, we use a state-of-the-art open-source RST-style discourse parser, DPLP (Ji and Eisenstein, 2014).2
We follow recent work that suggests transforming the RST tree into a dependency structure (Yoshida et al., 2014).3 Figure 2(a) shows the corresponding dependency structure of the RST tree in Figure 1.",2 Background: Rhetorical Structure Theory,[0],[0]
"It is clear that C is the root of the tree, and in fact this clause summarizes the review and suffices to categorize it as negative.",2 Background: Rhetorical Structure Theory,[0],[0]
"This dependency representation of the RST tree offers a
1There are also a few exceptions in which a relation can be realized with multiple nuclei.
",2 Background: Rhetorical Structure Theory,[0],[0]
2https://github.com/jiyfeng/DPLP 3The transformation is trivial and deterministic given the nucleus-satellite mapping for each relation.,2 Background: Rhetorical Structure Theory,[0],[0]
"The procedure is analogous to the transformation of a headed phrase-structure parse in syntax into a dependency tree (e.g., Yamada and Matsumoto, 2003).
form of inductive bias for our neural model, helping it to discern the most salient parts of a text in order to assign it a label.",2 Background: Rhetorical Structure Theory,[0],[0]
Our model is a recursive neural network built on a discourse dependency tree.,3 Model,[0],[0]
"It includes a distributed representation computed for each EDU, and a composition function that combines EDUs and partial trees into larger trees.",3 Model,[0],[0]
"At the top of the tree, the representation of the complete document is used to make a categorization decision.",3 Model,[0],[0]
"Our approach is analogous to (and inspired by) the use of recursive neural networks on syntactic dependency trees, with word embeddings at the leaves (Socher et al., 2014).",3 Model,[0],[0]
Let e be the distributed representation of an EDU.,3.1 Representation of Sentences,[0],[0]
"We use a bidirectional LSTM on the words’ embeddings within each EDU (details of word embeddings are given in section 4), concatenating the last hidden state vector from the forward LSTM (−→e ) with that of the backward LSTM (←−e ) to get e.
There is extensive recent work on architectures for embedding representations of sentences and other short pieces of text, including, for example, (bi)recursive neural networks (Paulus et al., 2014) and convolutional neural networks (Kalchbrenner et al., 2014).",3.1 Representation of Sentences,[0],[0]
Future work might consider alternatives; we chose the bidirectional LSTM due to its effectiveness in many settings.,3.1 Representation of Sentences,[0],[0]
"Given the discourse dependency tree for an input text, our recursive model builds a vector representation through composition at each arc in the tree.",3.2 Full Recursive Model,[0],[0]
Let vi denote the vector representation of EDU i and its descendants.,3.2 Full Recursive Model,[0],[0]
"For the base case where EDU i is a leaf in the tree, we let vi = tanh(ei), which is the elementwise hyperbolic tangent function.
",3.2 Full Recursive Model,[0],[0]
"For an internal node i, the composition function considers a parent and all of its children, whose indices are denoted by children(i).",3.2 Full Recursive Model,[0],[0]
"In defining this composition function, we seek for (i.) the contribution of the parent node ei to be central; and (ii.)",3.2 Full Recursive Model,[0],[0]
the contribution of each child node ej be determined by its content as well as the discourse relation it holds with the parent.,3.2 Full Recursive Model,[0],[0]
"We therefore define
vi = tanh  ei +
∑
j∈children(i) αi,jWri,jvj
  ,
(1) where Wri,j is a relation-specific composition matrix indexed by the relation between i and j, ri,j .",3.2 Full Recursive Model,[0],[0]
"αi,j is an “attention” weight, defined as
αi,j = σ",3.2 Full Recursive Model,[0],[0]
"( e>i Wαvj ) , (2)
where σ is the elementwise sigmoid and Wα contains attention parameters (these are relationindependent).",3.2 Full Recursive Model,[0],[0]
"Our attention mechanism differs from prior work (Bahdanau et al., 2015), in which attention weights are normalized to sum to one across competing candidates for attention.",3.2 Full Recursive Model,[0],[0]
"Here, αi,j does not depend on node i’s other children.",3.2 Full Recursive Model,[0],[0]
"This is motivated by RST, in which the presence of a node does not signify lesser importance to its siblings.",3.2 Full Recursive Model,[0],[0]
"Consider, for example, EDU D and text span E-F in Figure 1, which in parallel provide EXPLANATION for EDU C. This scenario differs from machine translation, where attention isused to implicitly and softly align output-language words to relatively few input-language words.",3.2 Full Recursive Model,[0],[0]
"It also differs from attention in composition functions used in syntactic parsing (Kuncoro et al., 2017), where attention can mimic head rules that follow from an endocentricity hypothesis of syntactic phrase representation.
",3.2 Full Recursive Model,[0],[0]
"Our recursive composition function, through the attention mechanism and the relation-specific weight matrices, is designed to learn how to differently weight EDUs for the categorization task.",3.2 Full Recursive Model,[0],[0]
"This idea of using a weighting scheme along with discourse structure is explored in prior works (Bhatia et al., 2015; Hogenboom et al., 2015), although they are manually designed, rather than learned from training data.
",3.2 Full Recursive Model,[0],[0]
"Once we have vroot of a text, the prediction of its category is given by softmax (Wovroot + b).
",3.2 Full Recursive Model,[0],[0]
"We refer to this model as the FULL model, since it makes use of the entire discourse dependency tree.",3.2 Full Recursive Model,[0],[0]
The FULL model based on Equation 1 uses a dependency discourse tree with relations.,3.3 Unlabeled Model,[0],[0]
"Because alternate discourse relation labels have been proposed (e.g., Prasad et al., 2008), we seek to measure the effect of these labels.",3.3 Unlabeled Model,[0],[0]
"We therefore consider an UNLABELED model based only on the tree structure, without the relations:
",3.3 Unlabeled Model,[0],[0]
"vi = tanh  ei +
∑
j∈children(i) αi,jvj
  .",3.3 Unlabeled Model,[0],[0]
"(3)
Here, only attention weights are used to compose the children nodes’ representations, significantly reducing the number of model parameters.
",3.3 Unlabeled Model,[0],[0]
"This UNLABELED model is similar to the depth weighting scheme introduced by Bhatia et al. (2015), which also uses an unlabeled discourse dependency tree, but our attention weights are computed by a function whose parameters are learned.",3.3 Unlabeled Model,[0],[0]
This approach sits squarely between Bhatia et al. (2015) and the flat document structure used by Yang et al. (2016); the UNLABELED model still uses discourse to bias the model toward some content (that which is closer to the tree’s root).,3.3 Unlabeled Model,[0],[0]
We consider two additional baselines that are even simpler.,3.4 Simpler Variants,[0],[0]
"The first, ROOT, uses the discourse dependency structure only to select the root EDU, which is used to represent the entire text: vroot = eroot.",3.4 Simpler Variants,[0],[0]
No composition function is needed.,3.4 Simpler Variants,[0],[0]
"This model variant is motivated by work on document summarization (Yoshida et al., 2014), where the
most central EDU is used to represent the whole text.
",3.4 Simpler Variants,[0],[0]
"The second variant, ADDITIVE, uses all the EDUs with a simple composition function, and does not depend on discourse structure at all: vroot = 1 N ∑N i=1",3.4 Simpler Variants,[0],[0]
"ei, where N is the total number of EDUs.",3.4 Simpler Variants,[0],[0]
"This serves as a baseline to test the benefits of discourse, controlling for other design decisions and implementation choices.",3.4 Simpler Variants,[0],[0]
"Although sentence representations ei are built in a different way from the work of Yang et al. (2016), this model is quite similar to their HN-AVE model on building document representations.",3.4 Simpler Variants,[0],[0]
"The parameters of all components of our model (top-level classification, composition, and EDU representation) are learned end-to-end using standard methods.",4 Implementation Details,[0],[0]
"We implement our learning procedure with the DyNet package (Neubig et al., 2017).
Preprocessing.",4 Implementation Details,[0],[0]
"For all datasets, we use the same preprocessing steps, mostly following recent work on language modeling (e.g., Mikolov et al., 2010).",4 Implementation Details,[0],[0]
We lowercased all the tokens and removed tokens that contain only punctuation symbols.,4 Implementation Details,[0],[0]
We replaced numbers in the documents with a special number token.,4 Implementation Details,[0],[0]
Low-frequency word types were replaced by UNK; we reduce the vocabulary for each dataset until approximately 5% of tokens are mapped to UNK.,4 Implementation Details,[0],[0]
"The vocabulary sizes after preprocessing are also shown in Table 1.
",4 Implementation Details,[0],[0]
Discourse parsing.,4 Implementation Details,[0],[0]
Our model requires the discourse structure for each document.,4 Implementation Details,[0],[0]
"We used DPLP, the RST parser from Ji and Eisenstein (2014), which is one of the best discourse parsers on the RST discourse treebank benchmark (Carlson et al., 2001).",4 Implementation Details,[0],[0]
"It employs a greedy decoding algorithm for parsing, producing 2,000 parses per minute on average on a single CPU.",4 Implementation Details,[0],[0]
"DPLP provides discourse segmentation, breaking a text into EDUs, typically clauses or sentences, based on syntactic parses provided by Stanford CoreNLP.",4 Implementation Details,[0],[0]
RST trees are converted to dependencies following the method of Yoshida et al. (2014).,4 Implementation Details,[0],[0]
"DPLP as distributed is trained on 347 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993).
",4 Implementation Details,[0],[0]
Word embeddings.,4 Implementation Details,[0],[0]
"In cases where there are 10,000 or fewer training examples, we used
pretrained GloVe word embeddings (Pennington et al., 2014), following previous work on neural discourse processing (Ji and Eisenstein, 2015).",4 Implementation Details,[0],[0]
"For larger datasets, we randomly initialized word embeddings and trained them alongside other model parameters.
",4 Implementation Details,[0],[0]
Learning and hyperparameters.,4 Implementation Details,[0],[0]
Online learning was performed with the optimization method and initial learning rate as hyperparameters.,4 Implementation Details,[0],[0]
"To avoid the exploding gradient problem, we used the norm clipping trick with a threshold of τ = 5.0.",4 Implementation Details,[0],[0]
"In addition, dropout rate 0.3 was used on both input and hidden layers to avoid overfitting.",4 Implementation Details,[0],[0]
"We performed grid search over the word vector representation dimensionality, the LSTM hidden state dimensionality (both {32, 48, 64, 128, 256}), the initial learning rate ({0.1, 0.01, 0.001}), and the update method (SGD and Adam, Kingma and Ba, 2015).",4 Implementation Details,[0],[0]
"For each corpus, the highest-accuracy combination of these hyperparameters is selected using development data or ten-fold cross validation, which will be specified in section 5.",4 Implementation Details,[0],[0]
We selected five datasets of different sizes and corresponding to varying categorization tasks.,5 Datasets,[0],[0]
"Some information about these datasets is summarized in Table 1.
",5 Datasets,[0],[0]
Sentiment analysis on Yelp reviews.,5 Datasets,[0],[0]
"Originally from the Yelp Dataset Challenge in 2015, this dataset contains 1.5 million examples.",5 Datasets,[0],[0]
"We used the preprocessed dataset from Zhang et al. (2015), which has 650,000 training and 50,000 test examples.",5 Datasets,[0],[0]
The task is to predict an ordinal rating (1–5) from the text of the review.,5 Datasets,[0],[0]
"To select the best combination of hyperparameters, we randomly sampled 10% training examples as the development data.",5 Datasets,[0],[0]
"We compared with hierarchical attention networks (Yang et al., 2016), which use the normalized attention mechanism on both word and sentence layers with a flat document structure, and provide the state-of-the-art result on this corpus.
",5 Datasets,[0],[0]
Framing dimensions in news articles.,5 Datasets,[0],[0]
"The Media Frames Corpus (MFC; Card et al., 2015) includes around 4,200 news articles about immigration from 13 U.S. newspapers over the years 1980–2012.",5 Datasets,[0],[0]
"The annotations of these articles are in terms of a set of 15 general-purpose labels, such as ECONOMICS and MORALITY, designed to categorize the emphasis framing applied to the
immigration issue within the articles.",5 Datasets,[0],[0]
We focused on predicting the single primary frame of each article.,5 Datasets,[0],[0]
"The state-of-the-art result on this corpus is from Card et al. (2016), where they used logistic regression together with unigrams, bigrams and Bamman-style personas (Bamman et al., 2014) as features.",5 Datasets,[0],[0]
"The best feature combination in their model alongside other hyperparameters was identified by a Bayesian optimization method (Bergstra et al., 2015).",5 Datasets,[0],[0]
"To select hyperparameters, we used a small set of examples from the corpus as a development set.",5 Datasets,[0],[0]
"Then, we report average accuracy across 10-fold cross validation as in (Card et al., 2016).
",5 Datasets,[0],[0]
Congressional floor debates.,5 Datasets,[0],[0]
"The corpus was originally collected by Thomas et al. (2006), and the data split we used was constructed by Yessenalina et al. (2010).",5 Datasets,[0],[0]
The goal is to predict the vote (“yea” or “nay”) for the speaker of each speech segment.,5 Datasets,[0],[0]
"The most recent work on this corpus is from Yogatama and Smith (2014), which proposed structured regularization methods based on linguistic components, e.g., sentences, topics, and syntactic parses.",5 Datasets,[0],[0]
"Each regularization method induces a linguistic bias to improve text classification accuracy, where the best result we repeated here is from the model with sentence regularizers.
",5 Datasets,[0],[0]
Movie reviews.,5 Datasets,[0],[0]
"This classic movie review corpus was constructed by Pang and Lee (2004) and includes 1,000 positive and 1,000 negative reviews.",5 Datasets,[0],[0]
"On this corpus, we used the standard tenfold data split for cross validation and reported the average accuracy across folds.",5 Datasets,[0],[0]
"We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis.",5 Datasets,[0],[0]
"Bha-
",5 Datasets,[0],[0]
tia et al. (2015) used a hand-crafted weighting scheme to bias the bag-of-word representations on sentences.,5 Datasets,[0],[0]
"Hogenboom et al. (2015) also considered manually-designed weighting schemes and a lexicon-based model as classifier, achieving performance inferior to fully-supervised methods like Bhatia et al. (2015) and ours.
",5 Datasets,[0],[0]
Congressional bill corpus.,5 Datasets,[0],[0]
"This corpus, collected by Yano et al. (2012), includes 51,762 legislative bills from the 103rd to 111th U.S. Congresses.",5 Datasets,[0],[0]
The task is to predict whether a bill will survive based on its content.,5 Datasets,[0],[0]
We randomly sampled 10% training examples as development data to search for the best hyperparameters.,5 Datasets,[0],[0]
"To our knowledge, the best published results are due to Yogatama and Smith (2014), which is the same baseline as for the congressional floor debates corpus.",5 Datasets,[0],[0]
"We evaluated all variants of our model on the five datasets presented in section 5, comparing in each case to the published state of the art as well as the most relevant works.
Results.",6 Experiments,[0],[0]
See Table 2.,6 Experiments,[0],[0]
"On four out of five datasets, our UNLABELED model (line 8) outperforms past methods.",6 Experiments,[0],[0]
"In the case of the very large Yelp dataset, our FULL model (line 9) gives even stronger performance, but not elsewhere, suggesting that it is overparameterized for the smaller datasets.",6 Experiments,[0],[0]
"Indeed, on the MFC and Movies tasks, the discourse-ignorant ADDITIVE outperforms the FULL model.",6 Experiments,[0],[0]
"On these datasets, the selected FULL model had nearly 20 times as many parameters as the UNLABELED model, which in turn had twice as many parameters as the ADDITIVE.
",6 Experiments,[0],[0]
This finding demonstrates the benefit of explicit discourse structure—even the output from an imperfect parser—for text categorization in some genres.,6 Experiments,[0],[0]
"This benefit is supported by both UNLABELED and FULL, since both of them use discourse structures of texts.",6 Experiments,[0],[0]
The advantage of using discourse information varies on different genres and different corpus sizes.,6 Experiments,[0],[0]
"Even though the discourse parser is trained on news text, it still offers benefit to restaurant and movie reviews and to the genre of congressional debates.",6 Experiments,[0],[0]
"Even for news text, if the training dataset is small (e.g., MFC), a lighter-weight variant of discourse (UNLABELED) is preferred.
",6 Experiments,[0],[0]
"Legislative bills, which have technical legal content and highly specialized conventions (see the supplementary material for an example), are arguably the most distant genre from news among those we considered.",6 Experiments,[0],[0]
"On that task, we see discourse working against accuracy.",6 Experiments,[0],[0]
"Note that the corpus of bills is more than ten times larger than three cases where our UNLABELED model outperformed past methods, suggesting that the drop in performance is not due to lack of data.
",6 Experiments,[0],[0]
It is also important to notice that the ROOT model performs quite poorly in all cases.,6 Experiments,[0],[0]
"This implies that discourse structure is not simply helping by finding a single EDU upon which to make the categorization decision.
",6 Experiments,[0],[0]
Qualitative analysis.,6 Experiments,[0],[0]
"Figure 3 shows some example texts from the Yelp Review corpus with their discourse structures produced by DPLP, where the weights were generated with the FULL model.",6 Experiments,[0],[0]
"Figures 3(a) and 3(b) are two successful
examples of the FULL model.",6 Experiments,[0],[0]
Figure 3(a) shows a simple case with respect to the discourse structure.,6 Experiments,[0],[0]
"Figure 3(b) is slightly different—the text in this example may have more than one reasonable discourse structure, e.g., 2D could be a child of 2C instead of 2A.",6 Experiments,[0],[0]
"In both cases, discourse structures help the FULL model bias to the important sentences.
",6 Experiments,[0],[0]
"Figure 3(c), on the other hand, presents a negative example, where DPLP failed to identify the most salient sentence 3F .",6 Experiments,[0],[0]
"In addition, the weights produced by the FULL model do not make much sense, which we suspect the model was confused by the structure.",6 Experiments,[0],[0]
Figure 3(c) also presents a manually-constructed discourse structure on the same text for reference.,6 Experiments,[0],[0]
"A more accurate prediction is expected if we use this manuallyconstructed discourse structure, because it has the appropriate dependency between sentences.",6 Experiments,[0],[0]
"In addition, the annotated discourse relations are able to select the right relation-specific composition matrices in FULL model, which are consistent with the training examples.
",6 Experiments,[0],[0]
Effect of parsing performance.,6 Experiments,[0],[0]
A natural question is whether further improvements to RST discourse parsing would lead to even greater gains in text categorization.,6 Experiments,[0],[0]
"While advances in discourse parsing are beyond the scope of this paper, we can gain some insight by exploring degradation to the DPLP parser.",6 Experiments,[0],[0]
An easy way to do this is to train it on subsets of the RST discourse treebank.,6 Experiments,[0],[0]
"We repeated the conditions described above for our FULL model, training DPLP on 25%, 50%, and 75% of the training set (randomly selected in
each case) before re-parsing the data for the sentiment analysis task.",6 Experiments,[0],[0]
We did not repeat the hyperparameter search.,6 Experiments,[0],[0]
"In Figure 4, we plot accuracy of the classifier (y-axis) against the F1 performance of the discourse parser",6 Experiments,[0],[0]
(x-axis).,6 Experiments,[0],[0]
"Unsurprisingly, lower parsing performance implies lower classification accuracy.",6 Experiments,[0],[0]
"Notably, if the RST discourse treebank were reduced to 25% of its size, our method would underperform the discourseignorant model of Yang et al. (2016).",6 Experiments,[0],[0]
"While we cannot extrapolate with certainty, these findings suggest that further improvements to discourse parsing, through larger annotated datasets or improved models, could lead to greater gains.
Attention mechanism.",6 Experiments,[0],[0]
"In section 3, we contrasted our new attention mechanism (Equation 2), which is inspired by RST’s lack of “competition” for salience among satellites, with the attention mechanism used in machine translation (Bahdanau et al., 2015).",6 Experiments,[0],[0]
"We consider here a variant of our model with normalized attention:
α′i = softmax     ...",6 Experiments,[0],[0]
"v>j
...   j∈children(i)",6 Experiments,[0],[0]
"Wα · ei   .
(4) The result here is a vector α′i, with one element for each child node j ∈",6 Experiments,[0],[0]
"children(i), and which sums to one.
",6 Experiments,[0],[0]
"On Yelp dateset, this variant of the FULL model achieves 70.3% accuracy (1.5% absolute behind our FULL model), giving empirical support to our
theoretically-motivated design decision not to normalize attention.",6 Experiments,[0],[0]
"Of course, further architecture improvements may yet be possible.
",6 Experiments,[0],[0]
Discussion.,6 Experiments,[0],[0]
Our findings in this work show the benefit of using discourse structure for text categorization.,6 Experiments,[0],[0]
"Although discourse structure strongly improves the performance on most of corpora in our experiments, its benefit is limited particularly by two factors: (1) the state-of-the-art performance on RST discourse parsing; and (2) domain mismatch between the training corpus for a discourse parser and the domain where the discourse parser is used.",6 Experiments,[0],[0]
"For the first factor, discourse parsing is still an active research topic in NLP, and may yet improve.",6 Experiments,[0],[0]
The second factor suggests exploring domain adaptation methods or even direct discourse annotation for genres of interest.,6 Experiments,[0],[0]
"Early work on text categorization often treated text as a bag of words (e.g., Joachims, 1998; Yang and Pedersen, 1997).",7 Related Work,[0],[0]
"Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data.
",7 Related Work,[0],[0]
The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are considered.,7 Related Work,[0],[0]
"Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation.",7 Related Work,[0],[0]
"Xiao and Cho (2016) extended that architecture by inserting a recurrent neural network layer between the convolutional layer and the classification layer.
",7 Related Work,[0],[0]
"In contrast, our contributions follow Ko et al. (2004), who sought to weight the influence of different parts of an input text on the task.",7 Related Work,[0],[0]
Two works that sought to learn the importance of sentences in a document are Yessenalina et al. (2010) and Yang et al. (2016).,7 Related Work,[0],[0]
"The former used a latent variable for the informativeness of each sentence, and the latter used a neural network to learn an attention function.",7 Related Work,[0],[0]
"Neither used any linguistic bias, relying only on task supervision to discover the latent variable distribution or attention function.",7 Related Work,[0],[0]
"Our work builds the neural network directly on a discourse dependency tree, favoring the most central EDUs over the others but giving the model the ability to overcome this bias.
",7 Related Work,[0],[0]
"Another way to use linguistic information was
presented by Yogatama and Smith (2014), who used a bag-of-words model.",7 Related Work,[0],[0]
The novelty in their approach was a data-driven regularization method that encouraged the model to collectively ignore groups of features found to coocur.,7 Related Work,[0],[0]
"Most related to our work is their “sentence regularizer,” which encouraged the model to try to ignore training-set sentences that were not informative for the task.",7 Related Work,[0],[0]
"Discourse structure was not considered.
",7 Related Work,[0],[0]
Discourse for sentiment analysis.,7 Related Work,[0],[0]
"Recently, discourse structure has been considered for sentiment analysis, which can be cast as a text categorization problem.",7 Related Work,[0],[0]
Bhatia et al. (2015) proposed two discourse-motivated models for sentiment polarity prediction.,7 Related Work,[0],[0]
"One of the models is also based on discourse dependency trees, but using a handcrafted weighting scheme.",7 Related Work,[0],[0]
Our method’s attention mechanism automates the weighting.,7 Related Work,[0],[0]
"We conclude that automatically-derived discourse structure can be helpful to text categorization, and the benefit increases with the accuracy of discourse parsing.",8 Conclusion,[0],[0]
"We did not see a benefit for categorizing legislative bills, a text genre whose discourse structure diverges from that of news.",8 Conclusion,[0],[0]
"These findings motivate further improvements to discourse parsing, especially for new genres.",8 Conclusion,[0],[0]
We thank anonymous reviewers and members of Noah’s ARK for helpful feedback on this work.,Acknowledgments,[0],[0]
We thank Dallas Card and Jesse Dodge for helping prepare the Media Frames Corpus and the Congressional bill corpus.,Acknowledgments,[0],[0]
This work was made possible by a University of Washington Innovation Award.,Acknowledgments,[0],[0]
"We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization.",abstractText,[0],[0]
"Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task.",abstractText,[0],[0]
Experiments consider variants of the approach and illustrate its strengths and weaknesses.,abstractText,[0],[0]
Neural Discourse Structure for Text Categorization,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 11–22 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1002",text,[0],[0]
Computational argumentation mining (AM) deals with finding argumentation structures in text.,1 Introduction,[0],[0]
"This involves several subtasks, such as: (a) separating argumentative units from non-argumentative units, also called ‘component segmentation’; (b) classifying argument components into classes such as “Premise” or “Claim”; (c) finding relations between argument components; (d) classifying relations into classes such as “Support” or “Attack” (Persing and Ng, 2016; Stab and Gurevych, 2017).
",1 Introduction,[0],[0]
"Thus, AM would have to detect claims and premises (reasons) in texts such as the following, where premise P supports claim C:
Since it killed many marine livesP , ::::::: tourism ::: has :::::::::: threatened :::::: natureC .
",1 Introduction,[0],[0]
"Argument structures in real texts are typically much more complex, cf.",1 Introduction,[0],[0]
"Figure 1.
",1 Introduction,[0],[0]
"While different research has addressed different subsets of the AM problem (see below), the ultimate goal is to solve all of them, starting from unannotated plain text.",1 Introduction,[0],[0]
Two recent approaches to this end-to-end learning scenario are Persing and Ng (2016) and Stab and Gurevych (2017).,1 Introduction,[0],[0]
"Both solve the end-to-end task by first training independent models for each subtask and then defining an integer linear programming (ILP) model that encodes global constraints such as that each premise has a parent, etc.",1 Introduction,[0],[0]
"Besides their pipeline architecture the approaches also have in common that they heavily rely on hand-crafted features.
",1 Introduction,[0],[0]
"Hand-crafted features pose a problem because AM is to some degree an “arbitrary” problem in that the notion of “argument” critically relies on the underlying argumentation theory (Reed et al., 2008; Biran and Rambow, 2011; Habernal and Gurevych, 2015; Stab and Gurevych, 2017).",1 Introduction,[0],[0]
"Accordingly, datasets typically differ with respect to their annotation of (often rather complex) argument structure.",1 Introduction,[0],[0]
"Thus, feature sets would have to be manually adapted to and designed for each new sample of data, a challenging task.",1 Introduction,[0],[0]
The same critique applies to the designing of ILP constraints.,1 Introduction,[0],[0]
"Moreover, from a machine learning perspective, pipeline approaches are problematic because they solve subtasks independently and thus lead to error propagation rather than exploiting interrelationships between variables.",1 Introduction,[0],[0]
"In contrast to this, we investigate neural techniques for end-to-end learning in computational AM, which do not require the hand-crafting of features or constraints.",1 Introduction,[0],[0]
The models we survey also all capture some notion of “joint”—rather than “pipeline”—learning.,1 Introduction,[0],[0]
"We investigate several approaches.
",1 Introduction,[0],[0]
"First, we frame the end-to-end AM problem as a dependency parsing problem.",1 Introduction,[0],[0]
"Dependency parsing may be considered a natural choice for AM, because argument structures often form trees,
11
or closely resemble them (see §3).",1 Introduction,[0],[0]
"Hence, it is not surprising that ‘discourse parsing’ (Muller et al., 2012) has been suggested for AM (Peldszus and Stede, 2015).",1 Introduction,[0],[0]
"What distinguishes our approach from these previous ones is that we operate on the token level, rather than on the level of components, because we address the end-toend framework and, thus, do not assume that nonargumentative units have already been sorted out and/or that the boundaries of argumentative units are given.
",1 Introduction,[0],[0]
"Second, we frame the problem as a sequence tagging problem.",1 Introduction,[0],[0]
"This is a natural choice especially for component identification (segmentation and classification), which is a typical entity recognition problem for which BIO tagging is a standard approach, pursued in AM, e.g., by Habernal and Gurevych (2016).",1 Introduction,[0],[0]
"The challenge in the end-to-end setting is to also include relations into the tagging scheme, which we realize by coding the distances between linked components into the tag label.",1 Introduction,[0],[0]
"Since related entities in AM are oftentimes several dozens of tokens apart from each other, neural sequence tagging models are in principle ideal candidates for such a framing because they can take into account long-range dependencies—something that is inherently difficult to capture with traditional feature-based tagging models such as conditional random fields (CRFs).
",1 Introduction,[0],[0]
"Third, we frame AM as a multi-task (tagging) problem (Caruana, 1997; Collobert and Weston, 2008).",1 Introduction,[0],[0]
"We experiment with subtasks of AM—e.g., component identification—as auxiliary tasks and investigate whether this improves performance on the AM problem.",1 Introduction,[0],[0]
"Adding such subtasks can be seen as analogous to de-coupling, e.g., component identification from the full AM problem.
",1 Introduction,[0],[0]
"Fourth, we evaluate the model of Miwa and Bansal (2016) that combines sequential (entity) and tree structure (relation) information and is in principle applicable to any problem where the aim is to extract entities and their relations.",1 Introduction,[0],[0]
"As such, this model makes fewer assumptions than our dependency parsing and tagging approaches.
",1 Introduction,[0],[0]
The contributions of this paper are as follows.,1 Introduction,[0],[0]
(1) We present the first neural end-to-end solutions to computational AM.,1 Introduction,[0],[0]
(2) We show that several of them perform better than the state-of-theart joint ILP model.,1 Introduction,[0],[0]
"(3) We show that a framing of AM as a token-based dependency parsing problem is ineffective—in contrast to what has been
proposed for systems that operate on the coarser component level and that (4) a standard neural sequence tagging model that encodes distance information between components performs robustly in different environments.",1 Introduction,[0],[0]
"Finally, (5) we show that a multi-task learning setup where natural subtasks of the full AM problem are added as auxiliary tasks improves performance.1",1 Introduction,[0],[0]
"AM has applications in legal decision making (Palau and Moens, 2009; Moens et al., 2007), document summarization, and the analysis of scientific papers (Kirschner et al., 2015).",2 Related Work,[0],[0]
"Its importance for the educational domain has been highlighted by recent work on writing assistance (Zhang and Litman, 2016) and essay scoring (Persing and Ng, 2015; Somasundaran et al., 2016).
",2 Related Work,[0],[0]
"Most works on AM address subtasks of AM such as locating/classifying components (Florou et al., 2013; Moens et al., 2007; Rooney et al., 2012; Knight et al., 2003; Levy et al., 2014; Rinott et al., 2015).",2 Related Work,[0],[0]
Relatively few works address the full AM problem of component and relation identification.,2 Related Work,[0],[0]
Peldszus and Stede (2016) present a corpus of microtexts containing only argumentatively relevant text of controlled complexity.,2 Related Work,[0],[0]
"To our best knowledge, Stab and Gurevych (2017) created the only corpus of attested high quality which annotates the AM problem in its entire complexity: it contains token-level annotations of components, their types, as well as relations and their types.",2 Related Work,[0],[0]
"We use the dataset of persuasive essays (PE) from Stab and Gurevych (2017), which contains student essays written in response to controversial topics such as “competition or cooperation—which is better?”
",3 Data,[0],[0]
"As Table 1 details, the corpus consists of 402 essays, 80 of which are reserved for testing.",3 Data,[0],[0]
"The an-
1Scripts that document how we ran our experiments are available from https://github.com/UKPLab/ acl2017-neural_end2end_AM.
notation distinguishes between major claims (the central position of an author with respect to the essay’s topic), claims (controversial statements that are either for or against the major claims), and premises, which give reasons for claims or other premises and either support or attack them.",3 Data,[0],[0]
"Overall, there are 751 major claims, 1506 claims, and 3832 premises.",3 Data,[0],[0]
"There are 5338 relations, most of which are supporting relations (>90%).
",3 Data,[0],[0]
"The corpus has a special structure, illustrated in Figure 1.",3 Data,[0],[0]
"First, major claims relate to no other components.",3 Data,[0],[0]
"Second, claims always relate to all other major claims.2 Third, each premise relates to exactly one claim or premise.",3 Data,[0],[0]
"Thus, the argument structure in each essay is—almost—a tree.",3 Data,[0],[0]
"Since there may be several major claims, each claim potentially connects to multiple targets, violating the tree structure.",3 Data,[0],[0]
"This poses no problem, however, since we can “loss-lessly” re-link the claims to one of the major claims (e.g., the last major claim in a document) and create a special root node to which the major claims link.",3 Data,[0],[0]
"From this tree, the actual graph can be uniquely reconstructed.
",3 Data,[0],[0]
There is another peculiarity of this data.,3 Data,[0],[0]
"Each essay is divided into paragraphs, of which there are 2235 in total.",3 Data,[0],[0]
"The argumentation structure is completely contained within a paragraph, except, possibly, for the relation from claims to major claims.",3 Data,[0],[0]
"Paragraphs have an average length of 66 tokens and are therefore much shorter than essays, which have an average length of 368 tokens.",3 Data,[0],[0]
"Thus, prediction on the paragraph level is easier than
2All MCs are considered as equivalent in meaning.
",3 Data,[0],[0]
"prediction on the essay level, because there are fewer components in a paragraph and hence fewer possibilities of source and target components in argument relations.",3 Data,[0],[0]
"The same is true for component classification: a paragraph can never contain premises only, for example, since premises link to other components.",3 Data,[0],[0]
"This section describes our neural network framings for end-to-end AM.
",4 Models,[0],[0]
Sequence Tagging is the problem of assigning each element in a stream of input tokens a label.,4 Models,[0],[0]
"In a neural context, the natural choice for tagging problems are recurrent neural nets (RNNs) in which a hidden vector representation ht at time point t depends on the previous hidden vector representation ht−1 and the input xt.",4 Models,[0],[0]
"In this way, an infinite window (“long-range dependencies”) around the current input token xt can be taken into account when making an output prediction yt.",4 Models,[0],[0]
"We choose particular RNNs, namely, LSTMs (Hochreiter and Schmidhuber, 1997), which are popular for being able to address vanishing/exploding gradients problems.",4 Models,[0],[0]
"In addition to considering a left-to-right flow of information, bidirectional LSTMs (BL) also capture information to the right of the current input token.
",4 Models,[0],[0]
"The most recent generation of neural tagging models add label dependencies to BLs, so that successive output decisions are not made independently.",4 Models,[0],[0]
"This class of models is called BiLSTM-
CRF (BLC) (Huang et al., 2015).",4 Models,[0],[0]
"The model of Ma and Hovy (2016) adds convolutional neural nets (CNNs) on the character-level to BiLSTMCRFs, leading to BiLSTM-CRF-CNN (BLCC) models.",4 Models,[0],[0]
"The character-level CNN may address problems of out-of-vocabulary words, that is, words not seen during training.
AM as Sequence Tagging: We frame AM as the following sequence tagging problem.",4 Models,[0],[0]
"Each input token has an associated label from Y , where
Y = {(b, t, d, s)",4 Models,[0],[0]
"| b ∈ {B, I,O}, t ∈ {P,C,MC,⊥}, d ∈ {. . .",4 Models,[0],[0]
",−2,−1, 1, 2, . .",4 Models,[0],[0]
.,4 Models,[0],[0]
",⊥}, s ∈ {Supp,Att, For,Ag,⊥}}.",4 Models,[0],[0]
"(1)
In other words, Y consists of all four-tuples (b, t, d, s) where b is a BIO encoding indicating whether the current token is non-argumentative (O) or begins (B) or continues (I) a component; t indicates the type of the component (claim C, premise P, or major claim MC for our data).",4 Models,[0],[0]
"Moreover, d encodes the distance—measured in number of components—between the current component and the component it relates to.",4 Models,[0],[0]
We encode the same d value for each token in a given component.,4 Models,[0],[0]
"Finally, s is the relation type (“stance”) between two components and its value may be Support (Supp), Attack (Att), or For or Against (Ag).",4 Models,[0],[0]
"We also have a special symbol ⊥ that indicates when a particular slot is not filled: e.g., a nonargumentative unit (b = O) has neither component type, nor relation, nor relation type.",4 Models,[0],[0]
"We refer to this framing as STagT (for “Simple Tagging”), where T refers to the tagger used.",4 Models,[0],[0]
"For the example from §1, our coding would hence be:
Since it killed many (O,⊥,⊥,⊥) (B,P,1,Supp) (I,P,1,Supp)",4 Models,[0],[0]
"(I,P,1,Supp) marine lives , tourism (I,P,1,Supp) (I,P,1,Supp) (O,⊥,⊥,⊥) (B,C,⊥,For) has threatened nature .",4 Models,[0],[0]
"(I,C,⊥,For) (I,C,⊥,For) (I,C,⊥,For) (O,⊥, ⊥, ⊥)
",4 Models,[0],[0]
"While the size of the label set Y is potentially infinite, we would expect it to be finite even in a potentially infinitely large data set, because humans also have only finite memory and are therefore expected to keep related components close in textual space.",4 Models,[0],[0]
"Indeed, as Figure 2 shows, in our PE essay data set about 30% of all relations between components have distance −1, that is, they follow the claim or premise that they attach to.",4 Models,[0],[0]
"Overall, around 2/3 of all relation distances d lie
in {−2,−1, 1}.",4 Models,[0],[0]
"However, the figure also illustrates that there are indeed long-range dependencies: distance values between −11 and +10 are observed in the data.
",4 Models,[0],[0]
"Multi-Task Learning Recently, there has been a lot of interest in so-called multi-task learning (MTL) scenarios, where several tasks are learned jointly (Søgaard and Goldberg, 2016; Peng and Dredze, 2016; Yang et al., 2016; Rusu et al., 2016; Héctor and Plank, 2017).",4 Models,[0],[0]
It has been argued that such learning scenarios are closer to human learning because humans often transfer knowledge between several domains/tasks.,4 Models,[0],[0]
"In a neural context, MTL is typically implemented via weight sharing: several tasks are trained in the same network architecture, thereby sharing a substantial portion of network’s parameters.",4 Models,[0],[0]
"This forces the network to learn generalized representations.
",4 Models,[0],[0]
In the MTL framework of Søgaard and Goldberg (2016) the underlying model is a BiLSTM with several hidden layers.,4 Models,[0],[0]
"Then, given different tasks, each task k ‘feeds’ from one of the hidden layers in the network.",4 Models,[0],[0]
"In particular, the hidden states encoded in a specific layer are fed into a multiclass classifier fk.",4 Models,[0],[0]
"The same work has demonstrated that this MTL protocol may be successful when there is a hierarchy between tasks and ‘lower’ tasks feed from lower layers.
AM as MTL:",4 Models,[0],[0]
We use the same framework STagT for modeling AM as MTL.,4 Models,[0],[0]
"However, we in addition train auxiliary tasks in the network— each with a distinct label set Y ′.
Dependency Parsing methods can be classified into graph-based and transition-based approaches (Kiperwasser and Goldberg, 2016).",4 Models,[0],[0]
"Transitionbased parsers encode the parsing problem as a sequence of configurations which may be modified by application of actions such as shift, reduce,
etc.",4 Models,[0],[0]
"The system starts with an initial configuration in which sentence elements are on a buffer and a stack, and a classifier successively decides which action to take next, leading to different configurations.",4 Models,[0],[0]
"The system terminates after a finite number of actions, and the parse tree is read off the terminal configuration.",4 Models,[0],[0]
"Graph-based parsers solve a structured prediction problem in which the goal is learning a scoring function over dependency trees such that correct trees are scored above all others.
",4 Models,[0],[0]
"Traditional dependency parsers used handcrafted feature functions that look at “core” elements such as “word on top of the stack”, “POS of word on top of the stack”, and conjunctions of core features such as “word is X and POS is Y” (see McDonald et al. (2005)).",4 Models,[0],[0]
Most neural parsers have not entirely abandoned feature engineering.,4 Models,[0],[0]
"Instead, they rely, for example, on encoding the core features of parsers as low-dimensional embedding vectors (Chen and Manning, 2014) but ignore feature combinations.",4 Models,[0],[0]
Kiperwasser and Goldberg (2016) design a neural parser that uses only four features: the BiLSTM vector representations of the top 3 items on the stack and the first item on the buffer.,4 Models,[0],[0]
"In contrast, Dyer et al. (2015)’s neural parser associates each stack with a “stack LSTM” that encodes their contents.",4 Models,[0],[0]
"Actions are chosen based on the stack LSTM representations of the stacks, and no more feature engineering is necessary.",4 Models,[0],[0]
"Moreover, their parser has thus access to any part of the input, its history and stack contents.
",4 Models,[0],[0]
"AM as Dependency Parsing: To frame a problem as a dependency parsing problem, each instance of the problem must be encoded as a directed tree, where tokens have heads, which in turn are labeled.",4 Models,[0],[0]
"For end-to-end AM, we propose the framing illustrated in Figure 3.",4 Models,[0],[0]
"We highlight two design decisions, the remaining are analogous and/or can be read off the figure.
",4 Models,[0],[0]
"• The head of each non-argumentative text token is the document terminating token END, which is a punctuation mark in all our cases.",4 Models,[0],[0]
"The label of this link is O, the symbol for non-argumentative units.
",4 Models,[0],[0]
• The head of each token in a premise is the first token of the claim or premise that it links to.,4 Models,[0],[0]
"The label of each of these links is (b,P,Supp) or (b,P,Att) depending on whether a premise “supports” or “attacks” a claim or premise; b ∈ {B, I}.
",4 Models,[0],[0]
LSTM-ER Miwa and Bansal (2016) present a neural end-to-end system for identifying both entities as well as relations between them.,4 Models,[0],[0]
Their entity detection system is a BLC-type tagger and their relation detection system is a neural net that predicts a relation for each pair of detected entities.,4 Models,[0],[0]
This relation module is a TreeLSTM model that makes use of dependency tree information.,4 Models,[0],[0]
"In addition to de-coupling entity and relation detection but jointly modeling them,3 pretraining on entities and scheduled sampling (Bengio et al., 2015) is applied to prevent low performance at early training stages of entity detection and relation classification.",4 Models,[0],[0]
"To adapt LSTM-ER for the argument structure encoded in the PE dataset, we model three types of entities (premise, claim, major claim) and four types of relations (for, against, support, attack).
",4 Models,[0],[0]
We use the feature-based ILP model from Stab and Gurevych (2017) as a comparison system.,4 Models,[0],[0]
"This system solves the subtasks of AM—component segmentation, component classification, relation detection and classification— independently.",4 Models,[0],[0]
"Afterwards, it defines an ILP model with various constraints to enforce valid argumentation structure.",4 Models,[0],[0]
"As features it uses structural, lexical, syntactic and context features, cf.",4 Models,[0],[0]
"Stab and Gurevych (2017) and Persing and Ng (2016).
",4 Models,[0],[0]
"Summarizing, we distinguish our framings in terms of modularity and in terms of their constraints.",4 Models,[0],[0]
Modularity: Our dependency parsing framing and LSTM-ER are more modular than STagT because they de-couple relation information from entity information.,4 Models,[0],[0]
"However, (part of)
3By ‘de-coupling’, we mean that both tasks are treated separately rather than merging entity and relation information in the same tag label (output space).",4 Models,[0],[0]
"Still, a joint model like that of Miwa and Bansal (2016) de-couples the two tasks in such a way that many model parameters are shared across the tasks, similarly as in MTL.
",4 Models,[0],[0]
this modularity can be regained by using STagT in an MTL setting.,4 Models,[0],[0]
"Moreover, since entity and relation information are considerably different, such a de-coupling may be advantageous.",4 Models,[0],[0]
"Constraints: LSTM-ER can, in principle, model any kind of— even many-to-many—relationships between detected entities.",4 Models,[0],[0]
"Thus, it is not guaranteed to produce trees, as we observe in AM datasets.",4 Models,[0],[0]
"STagT also does not need to produce trees, but it more severely restricts search space than does LSTMER: each token/component can only relate to one (and not several) other tokens/components.",4 Models,[0],[0]
The same constraint is enforced by the dependency parsing framing.,4 Models,[0],[0]
"All of the tagging modelings, including LSTM-ER, are local models whereas our parsing framing is a global model: it globally enforces a tree structure on the token-level.
",4 Models,[0],[0]
"Further remarks: (1) part of the TreeLSTM modeling inherent to LSTM-ER is ineffective for our data because this modeling exploits dependency tree structures on the sentence level, while relationships between components are almost never on the sentence level.",4 Models,[0],[0]
"In our data, roughly 92% of all relationships are between components that appear in different sentences.",4 Models,[0],[0]
"Secondly, (2) that a model enforces a constraint does not necessarily mean that it is more suitable for a respective task.",4 Models,[0],[0]
"It has frequently been observed that models tend to produce output consistent with constraints in their training data in such situations (Zhang et al., 2017; Héctor and Plank, 2017); thus, they have learned the constraints.",4 Models,[0],[0]
This section presents and discusses the empirical results for the AM framings outlined in §4.,5 Experiments,[0],[0]
"We relegate issues of pre-trained word embeddings, hyperparameter optimization and further practical issues to the supplementary material.",5 Experiments,[0],[0]
"Links to software used as well as some additional error analysis can also be found there.
",5 Experiments,[0],[0]
Evaluation Metric We adopt the evaluation metric suggested in Persing and Ng (2016).,5 Experiments,[0],[0]
"This computes true positives TP, false positives FP, and false negatives FN, and from these calculates component and relation F1 scores as F1 = 2TP2TP+FP+FN .",5 Experiments,[0],[0]
"For space reasons, we refer to Persing and Ng (2016) for specifics, but to illustrate, for components, true positives are defined as the set of components in the gold standard for which there exists a predicted component with the same type that
‘matches’.",5 Experiments,[0],[0]
"Persing and Ng (2016) define a notion of what we may term ‘level α matching’: for example, at the 100% level (exact match) predicted and gold components must have exactly the same spans, whereas at the 50% level they must only share at least 50% of their tokens (approximate match).",5 Experiments,[0],[0]
"We refer to these scores as C-F1 (100%) and C-F1 (50%), respectively.",5 Experiments,[0],[0]
"For relations, an analogous F1 score is determined, which we denote by R-F1 (100%) and R-F1 (50%).",5 Experiments,[0],[0]
We note that R-F1 scores depend on C-F1 scores because correct relations must have correct arguments.,5 Experiments,[0],[0]
"We also define a ‘global’ F1 score, which is the F1score of C-F1 and R-F1.
",5 Experiments,[0],[0]
"Most of our results are shown in Table 2.
(a) Dependency Parsing We show results for the two feature-based parsers MST (McDonald et al., 2005), Mate (Bohnet and Nivre, 2012) as well as the neural parsers by Dyer et al. (2015) (LSTM-Parser) and Kiperwasser and Goldberg (2016) (Kiperwasser).",5 Experiments,[0],[0]
"We train and test all parsers on the paragraph level, because training them on essay level was typically too memory-exhaustive.
MST mostly labels only non-argumentative units correctly, except for recognizing individual major claims, but never finds their exact spans (e.g., “tourism can create negative impacts on” while the gold major claim is “international tourism can create negative impacts on the destination countries”).",5 Experiments,[0],[0]
Mate is slightly better and in particular recognizes several major claims correctly.,5 Experiments,[0],[0]
"Kiperwasser performs decently on the approximate match level, but not on exact level.",5 Experiments,[0],[0]
"Upon inspection, we find that the parser often predicts ‘too large’ component spans, e.g., by including following punctuation.",5 Experiments,[0],[0]
The best parser by far is the LSTM-Parser.,5 Experiments,[0],[0]
"It is over 100% better than Kiperwasser on exact spans and still several percentage points on approximate spans.
",5 Experiments,[0],[0]
How does performance change when we switch to the essay level?,5 Experiments,[0],[0]
"For the LSTM-Parser, the best performance on essay level is 32.84%/47.44% CF1 (100%/50% level), and 9.11%/14.45% on RF1, but performance result varied drastically between different parametrizations.",5 Experiments,[0],[0]
"Thus, the performance drop between paragraph and essay level is in any case immense.
",5 Experiments,[0],[0]
Since the employed features of modern featurebased parsers are rather general—such as distance between words or word identities—we had expected them to perform much better.,5 Experiments,[0],[0]
"The mini-
mal feature set employed by Kiperwasser is apparently not sufficient for accurate AM but still a lot more powerful than the hand-crafted feature approaches.",5 Experiments,[0],[0]
"We hypothesize that the LSTM-Parser’s good performance, relative to the other parsers, is due to its encoding of the whole stack history— rather than just the top elements on the stack as in Kiperwasser— which makes it aware of much larger ‘contexts’.",5 Experiments,[0],[0]
"While the drop in performance from paragraph to essay level is expected, the LSTM-Parser’s deterioration is much more severe than the other models’ surveyed below.",5 Experiments,[0],[0]
"We believe that this is due to a mixture of the following: (1) ‘capacity’, i.e., model complexity, of the parsers— that is, risk of overfitting; and (2) few, but very long sequences on essay level—that is, little training data (trees), paired with a huge search space on each train/test instance, namely, the number of possible trees on n tokens.",5 Experiments,[0],[0]
"See also our discussions below, particularly, our stability analysis.
",5 Experiments,[0],[0]
"(b) Sequence Tagging For these experiments, we use the BLCC tagger from Ma and Hovy (2016) and refer to the resulting system as STagBLCC.",5 Experiments,[0],[0]
"Again, we observe that paragraph level is considerably easier than essay level; e.g., for relations, there is ∼5% points increase from essay to paragraph level.",5 Experiments,[0],[0]
"Overall, STagBLCC is ∼13% better than the best parser for C-F1 and ∼11% better for R-F1 on the paragraph level.",5 Experiments,[0],[0]
"Our explanation is that taggers are simpler local models, and thus need less training data and are less prone to overfitting.",5 Experiments,[0],[0]
"Moreover, they can much better deal with the long sequences because they are largely invariant to length: e.g., it does in principle not matter, from a parameter estimation perspective, whether we train our taggers on two sequences of lengths n and m, respectively, or on
one long sequence of length n+m.
",5 Experiments,[0],[0]
"(c) MTL As indicated, we use the MTL tagging framework from Søgaard and Goldberg (2016) for multi-task experiments.",5 Experiments,[0],[0]
The underlying tagging framework is weaker than that of BLCC: there is no CNN which can take subword information into account and there are no dependencies between output labels: each tagging prediction is made independently of the other predictions.,5 Experiments,[0],[0]
"We refer to this system as STagBL.
",5 Experiments,[0],[0]
"Accordingly, as Table 3 shows for the essay level (paragraph level omitted for space reasons), results are generally weaker: For exact match, C-F1 values are about ∼10% points below those of STagBLCC, while approximate match performances are much closer.",5 Experiments,[0],[0]
"Hence, the independence assumptions of the BL tagger apparently lead to more ‘local’ errors such as exact argument span identification (cf. error analysis).",5 Experiments,[0],[0]
"An analogous trend holds for argument relations.
",5 Experiments,[0],[0]
Additional Tasks: We find that when we train STagBL with only its main task—with label set Y as in Eq.,5 Experiments,[0],[0]
(1)—the overall result is worst.,5 Experiments,[0],[0]
"In contrast, when we include the ‘natural subtasks’ “C” (label set YC consists of the projection on the coordinates (b, t) in Y) and/or “R” (label set YR consists of the projection on the coordinates (d, s)), performance increases typically by a few percentage points.",5 Experiments,[0],[0]
"This indicates that complex sequence tagging may benefit when we train a “sublabeler” in the same neural architecture, a finding that may be particularly relevant for morphological POS tagging (Müller et al., 2013).",5 Experiments,[0],[0]
"Unlike Søgaard and Goldberg (2016), we do not find that the optimal architecture is the one in which “lower” tasks (such as C or R) feed from lower layers.",5 Experiments,[0],[0]
"In fact, in one of the best parametrizations
the C task and the full task feed from the same layer in the deep BiLSTM.",5 Experiments,[0],[0]
"Moreover, we find that the C task is consistently more helpful as an auxiliary task than the R task.
",5 Experiments,[0],[0]
"On essay level, (d) LSTM-ER performs very well on component identification (+5% C-F1 compared to STagBLCC), but rather poor on relation identification (-18% R-F1).",5 Experiments,[0],[0]
"Hence, its overall F1 on essay level is considerably below that of STagBLCC.",5 Experiments,[0],[0]
"In contrast, LSTM-ER trained and tested on paragraph level substantially outperforms all other systems discussed, both for component as well as for relation identification.
",5 Experiments,[0],[0]
We think that its generally excellent performance on components is due to LSTM-ER’s de-coupling of component and relation tasks.,5 Experiments,[0],[0]
"Our findings indicate that a similar result can be achieved for STagT via MTL when components and relations are included as auxiliary tasks,",5 Experiments,[0],[0]
cf. Table 3.,5 Experiments,[0],[0]
"For example, the improvement of LSTM-ER over STagBLCC, for C-F1, roughly matches the increase for STagBL when including components and relations separately (Y-3:YC-3:YR-3) over not including them as auxiliary tasks (Y-3).",5 Experiments,[0],[0]
"Lastly, the better performance of LSTM-ER over STagBLCC for relations on paragraph level appears to be a consequence of its better performance on components.",5 Experiments,[0],[0]
"E.g., when both arguments are correctly predicted, STagBLCC has even higher chance of getting their relation correct than LSTM-ER (95.34% vs. 94.17%).
",5 Experiments,[0],[0]
Why does LSTM-ER degrade so much on essay level for R-F1?,5 Experiments,[0],[0]
"As said, text sequences are much longer on essay level than on paragraph level— hence, there are on average many more entities on essay level.",5 Experiments,[0],[0]
"Thus, there are also many more possible relations between all entities discovered in a text—namely, there are O(2m 2 ) possible relations between m discovered components.",5 Experiments,[0],[0]
"Due to its
generality, LSTM-ER considers all these relations as plausible, while STagT does not (for any of choice of T ): e.g., our coding explicitly constrains each premise to link to exactly one other component, rather than to 0, . . .",5 Experiments,[0],[0]
",m possible components, as LSTM-ER allows.",5 Experiments,[0],[0]
"In addition, our explicit coding of distance values d biases the learner T to reflect the distribution of distance values found in real essays—namely, that related components are typically close in terms of the number of components between them.",5 Experiments,[0],[0]
"In contrast, LSTM-ER only mildly prefers short-range dependencies over long-range dependencies, cf.",5 Experiments,[0],[0]
"Figure 4.
",5 Experiments,[0],[0]
The (e) ILP has access to both paragraph and essay level information and thus has always more information than all neural systems compared to.,5 Experiments,[0],[0]
"Thus, it also knows in which paragraph in an essay it is.",5 Experiments,[0],[0]
"This is useful particularly for major claims, which always occur in first or last paragraphs in our data.",5 Experiments,[0],[0]
"Still, its performance is equal to or lower than that of LSTM-ER and STagBLCC when both are evaluated on paragraph level.
",5 Experiments,[0],[0]
"Stability Analysis Table 4 shows averages and standard deviations of two selected models, namely, the STagBLCC tagging framework as well as the LSTM-Parser over several different runs (different random initializations as well as different hyperparameters as discussed in the supplementary material).",5 Experiments,[0],[0]
These results detail that the taggers have lower standard deviations than the parsers.,5 Experiments,[0],[0]
"The difference is particularly striking on the essay level where the parsers often completely fail to learn, that is, their performance scores are close to 0%.",5 Experiments,[0],[0]
"As discussed above, we attribute this to the parsers’ increased model capacity relative to the taggers, which makes them more prone to overfitting.",5 Experiments,[0],[0]
"Data scarcity is another very likely source of error in this context, as the parsers only observe 322 (though very rich) trees
in the training data, while the taggers are always roughly trained on 120K tokens.",5 Experiments,[0],[0]
"On paragraph level, they do observe more trees, namely, 1786.
",5 Experiments,[0],[0]
"Error analysis
A systematic source of errors for all systems is detecting exact argument spans (segmentation).",5 Experiments,[0],[0]
"For instance, the ILP system predicts the following premise: “As a practical epitome , students should be prepared to present in society after their graduation”, while the gold premise omits the preceding discourse marker, and hence reads: “students should be prepared to present in society after their graduation”.",5 Experiments,[0],[0]
"On the one hand, it has been observed that even humans have problems exactly identifying such entity boundaries (Persing and Ng, 2016; Yang and Cardie, 2013).",5 Experiments,[0],[0]
"On the other hand, our results in Table 2 indicate that the neural taggers BLCC and BLC (in the LSTMER model) are much better at such exact identification than either the ILP model or the neural parsers.",5 Experiments,[0],[0]
"While the parsers’ problems are most likely due to model complexity, we hypothesize that the ILP model’s increased error rates stem from a weaker underlying tagging model (featurebased CRF vs. BiLSTM) and/or",5 Experiments,[0],[0]
its features.4,5 Experiments,[0],[0]
"In fact, as Table 5 shows, the macro-F1 scores5 on only the component segmentation tasks (BIO labeling) are substantially higher for both LSTMER and STagBLCC than for the ILP model.",5 Experiments,[0],[0]
"Noteworthy, the two neural systems even outperform the human upper bound (HUB) in this context, reported as 88.6% in Stab and Gurevych (2017).",5 Experiments,[0],[0]
We present the first study on neural end-to-end AM.,6 Conclusion,[0],[0]
"We experimented with different framings,
4The BIO tagging task is independent and thus not affected by the ILP constraints in the model of Stab and Gurevych (2017).",6 Conclusion,[0],[0]
"The same holds true for the model of Persing and Ng (2016).
",6 Conclusion,[0],[0]
"5Denoted FscoreM in Sokolova and Lapalme (2009).
such as encoding AM as a dependency parsing problem, as a sequence tagging problem with particular label set, as a multi-task sequence tagging problem, and as a problem with both sequential and tree structure information.",6 Conclusion,[0],[0]
"We show that (1) neural computational AM is as good or (substantially) better than a competing feature-based ILP formulation, while eliminating the need for manual feature engineering and costly ILP constraint designing.",6 Conclusion,[0],[0]
"(2) BiLSTM taggers perform very well for component identification, as demonstrated for our STagT frameworks, for T = BLCC and T = BL, as well as for LSTM-ER (BLC tagger).",6 Conclusion,[0],[0]
"(3) (Naively) coupling component and relation identification is not optimal, but both tasks should be treated separately, but modeled jointly.",6 Conclusion,[0],[0]
"(4) Relation identification is more difficult: when there are few entities in a text (“short documents”), a more general framework such as that provided in LSTM-ER performs reasonably well.",6 Conclusion,[0],[0]
"When there are many entities (“long documents”), a more restrained modeling is preferable.",6 Conclusion,[0],[0]
These are also our policy recommendations.,6 Conclusion,[0],[0]
"Our work yields new state-of-the-art results in end-to-end AM on the PE dataset from Stab and Gurevych (2017).
",6 Conclusion,[0],[0]
"Another possible framing, not considered here, is to frame AM as an encoder-decoder problem (Bahdanau et al., 2015; Vinyals et al., 2015).",6 Conclusion,[0],[0]
This is an even more general modeling than LSTM-ER.,6 Conclusion,[0],[0]
"Its suitability for the end-to-end learning task is scope for future work, but its adequacy for component classification and relation identification has been investigated in Potash et al. (2016).",6 Conclusion,[0],[0]
"We thank Lucie Flekova, Judith Eckle-Kohler, Nils Reimers, and Christian Stab for valuable feedback and discussions.",Acknowledgments,[0],[0]
We also thank the anonymous reviewers for their suggestions.,Acknowledgments,[0],[0]
The second author was supported by the German Federal Ministry of Education and Research (BMBF) under the promotional reference 01UG1416B,Acknowledgments,[0],[0]
(CEDIFOR).,Acknowledgments,[0],[0]
"2016, Osaka, Japan.","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"pages 1568–1578.
","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Christian Stab and Iryna Gurevych.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
2017.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Parsing argumentation structures in persuasive essays.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"Computational Linguistics (in press), preprint: http://arxiv.org/abs/1604.07370).
","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015.","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Pointer networks.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages 2692–2700.
","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Bishan Yang and Claire Cardie.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
2013.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Joint inference for fine-grained opinion extraction.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"Association for Computational Linguistics, Sofia, Bulgaria, pages 1640–1649.","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"http://www.aclweb.org/anthology/P13-1161.
","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"Zhilin Yang, Ruslan Salakhutdinov, and William W. Cohen. 2016.","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Multi-task cross-lingual sequence tagging from scratch.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"CoRR abs/1603.06270.
","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Fan Zhang and Diane J. Litman. 2016.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Using context to predict the purpose of argumentative writing revisions.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
In The Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"pages 1424–1430.
","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
"Xingxing Zhang, Jianpeng Cheng, and Mirella Lapata.","In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
2017.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Dependency parsing as head selection.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
In Proceedings of EACL 2017 (long papers).,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
Association for Computational Linguistics.,"In COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16,",[0],[0]
We investigate neural techniques for endto-end computational argumentation mining (AM).,abstractText,[0],[0]
"We frame AM both as a tokenbased dependency parsing and as a tokenbased sequence tagging problem, including a multi-task learning setup.",abstractText,[0],[0]
"Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results.",abstractText,[0],[0]
"In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch longrange dependencies inherent to the AM problem.",abstractText,[0],[0]
"Moreover, we find that jointly learning ‘natural’ subtasks, in a multi-task learning setup, improves performance.",abstractText,[0],[0]
Neural End-to-End Learning for Computational Argumentation Mining,title,[0],[0]
"Deep reinforcement learning agents have achieved state-ofthe-art results in a variety of complex environments (Mnih et al., 2015; 2016), often surpassing human performance (Silver et al., 2016).",1. Introduction,[0],[0]
"Although the final performance of these agents is impressive, these techniques usually require several orders of magnitude more interactions with their environment than a human in order to reach an equivalent level of expected performance.",1. Introduction,[0],[0]
"For example, in the Atari 2600 set of environments (Bellemare et al., 2013), deep Q-networks (Mnih et al., 2016) require more than 200 hours of gameplay in order to achieve scores similar to those a human player achieves after two hours (Lake et al., 2016).
",1. Introduction,[0],[0]
"The glacial learning speed of deep reinforcement learning has several plausible explanations and in this work we focus on addressing these:
1.",1. Introduction,[0],[0]
Stochastic gradient descent optimisation requires the use of small learning rates.,1. Introduction,[0],[0]
"Due to the global approximation nature of neural networks, high learning rates cause catastrophic interference (McCloskey & Cohen, 1989).",1. Introduction,[0],[0]
"Low learning rates mean that experience can only be incorporated into a neural network slowly.
",1. Introduction,[0],[0]
2.,1. Introduction,[0],[0]
Environments with a sparse reward signal can be difficult for a neural network to model as there may be very few instances where the reward is non-zero.,1. Introduction,[0],[0]
This can be viewed as a form of class imbalance where low-reward samples outnumber high-reward samples by an unknown number.,1. Introduction,[0],[0]
"Consequently, the neural network disproportionately underperforms at predicting larger rewards, making it difficult for an agent to take the most rewarding actions.
",1. Introduction,[0],[0]
3.,1. Introduction,[0],[0]
"Reward signal propagation by value-bootstrapping techniques, such as Q-learning, results in reward information being propagated one step at a time through the history of previous interactions with the environment.",1. Introduction,[0],[0]
This can be fairly efficient if updates happen in reverse order in which the transitions occur.,1. Introduction,[0],[0]
"However, in order to train on uncorrelated minibatches DQN-style, algorithms train on randomly selected transitions, and, in order to further stabilise training, require the use of a slowly updating target network further slowing down reward propagation.
",1. Introduction,[0],[0]
"In this work we shall focus on addressing the three concerns listed above; we must note, however, that other recent advances in exploration (Osband et al., 2016), hierarchical reinforcement learning (Vezhnevets et al., 2016) and transfer learning (Rusu et al., 2016; Fernando et al., 2017) also make substantial contributions to improving data efficiency in deep reinforcement learning over baseline agents.
",1. Introduction,[0],[0]
"In this paper we propose Neural Episodic Control (NEC), a method which tackles the limitations of deep reinforcement learning listed above and demonstrates dramatic im-
ar X
iv :1
70 3.
01 98
8v 1
[ cs
.L",1. Introduction,[0],[0]
"G
] 6
M ar
provements on the speed of learning for a wide range of environments.",1. Introduction,[0],[0]
"Critically, our agent is able to rapidly latch onto highly successful strategies as soon as they are experienced, instead of waiting for many steps of optimisation (e.g., stochastic gradient descent) as is the case with DQN (Mnih et al., 2015) and A3C (Mnih et al., 2016).
",1. Introduction,[0],[0]
"Our work is in part inspired by the hypothesised role of the Hippocampus in decision making (Lengyel & Dayan, 2007; Blundell et al., 2016) and also by recent work on one-shot learning (Vinyals et al., 2016) and learning to remember rare events with neural networks (Kaiser et al., 2016).",1. Introduction,[0],[0]
"Our agent uses a semi-tabular representation of its experience of the environment possessing several of the features of episodic memory such as long term memory, sequentiality, and context-based lookups.",1. Introduction,[0],[0]
The semi-tabular representation is an append-only memory that binds slow-changing keys to fast updating values and uses a context-based lookup on the keys to retrieve useful values during action selection by the agent.,1. Introduction,[0],[0]
Thus the agent’s memory operates in much the same way that traditional table-based RL methods map from state and action to value estimates.,1. Introduction,[0],[0]
A unique aspect of the memory in contrast to other neural memory architectures for reinforcement learning (explained in more detail in Section 3) is that the values retrieved from the memory can be updated much faster than the rest of the deep neural network.,1. Introduction,[0],[0]
"This helps alleviate the typically slow weight updates of stochastic gradient descent applied to the whole network and is reminiscent of work on fast weights (Ba et al., 2016; Hinton & Plaut, 1987), although the architecture we present is quite different.",1. Introduction,[0],[0]
"Another unique aspect of the memory is that unlike other memory architectures such as LSTM and the differentiable neural computer (DNC; Graves et al., 2016), our architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time.",1. Introduction,[0],[0]
"Instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing memory architectures (in contrast to Oh et al. (2015); Graves et al. (2016) where the memory is wiped at the end of each episode).",1. Introduction,[0],[0]
"Reading from this large memory is made efficient using kd-tree based nearest neighbour (Bentley, 1975).
",1. Introduction,[0],[0]
"The remainder of the paper is organised as follows: in Section 2 we review deep reinforcement learning, in Section 3 the Neural Episodic Control algorithm is described, in Section 4 we report experimental results in the Atari Learning Environment, in Section 5 we discuss other methods that use memory for reinforcement learning, and finally in Section 6 we outline future work and summarise the main advantages of the NEC algorithm.",1. Introduction,[0],[0]
"The action-value function of a reinforcement learning agent (Sutton & Barto, 1998) is defined as Qπ(s, a) =",2. Deep Reinforcement Learning,[0],[0]
"Eπ [ ∑ t γ
trt | s, a], where a is the initial action taken by the agent in the initial state s and the expectation denotes that the policy π is followed thereafter.",2. Deep Reinforcement Learning,[0],[0]
"The discount factor γ ∈ (0, 1) trades off favouring short vs. long term rewards.
",2. Deep Reinforcement Learning,[0],[0]
"Deep Q-Network agents (DQN; Mnih et al., 2015) use Qlearning (Watkins & Dayan, 1992) to learn a value function Q(st, at) to rank which action at is best to take in each state st at step t. The agent then executes an -greedy policy based upon this value function to trade-off exploration and exploitation: with probability the agent picks an action uniformly at random, otherwise it picks the action at = argmaxaQ(st, a).
",2. Deep Reinforcement Learning,[0],[0]
"In DQN, the action-value function Q(st, at) is parameterised by a convolutional neural network that takes a 2D pixel representation of the state st as input, and outputs a vector containing the value of each action at that state.",2. Deep Reinforcement Learning,[0],[0]
"When the agent observes a transition, DQN stores the (st, at, rt, st+1) tuple in a replay buffer, the contents of which are used for training.",2. Deep Reinforcement Learning,[0],[0]
This neural network is trained by minimizing the squared error between the network’s output and the Q-learning target yt =,2. Deep Reinforcement Learning,[0],[0]
"rt + γmaxa Q̃(st+1, a), for a subset of transitions sampled at random from the replay buffer.",2. Deep Reinforcement Learning,[0],[0]
"The target network Q̃(st+1, a) is an older version of the value network that is updated periodically.",2. Deep Reinforcement Learning,[0],[0]
"The use of a target network and uncorrelated samples from the replay buffer are critical for stable training.
",2. Deep Reinforcement Learning,[0],[0]
A number of extensions have been proposed that improve DQN.,2. Deep Reinforcement Learning,[0],[0]
"Double DQN (Van Hasselt et al., 2016) reduces bias on the target calculation.",2. Deep Reinforcement Learning,[0],[0]
"Prioritised Replay (Schaul et al., 2015b) further improves Double DQN by optimising the replay strategy.",2. Deep Reinforcement Learning,[0],[0]
"Several authors have proposed methods of improving reward propagation and the back up mechanism of Q learning (Harutyunyan et al., 2016; Munos et al., 2016; He et al., 2016) by incorporating on-policy rewards or by adding constraints to the optimisation.",2. Deep Reinforcement Learning,[0],[0]
Q∗(λ),2. Deep Reinforcement Learning,[0],[0]
"(Harutyunyan et al., 2016) and Retrace(λ) (Munos et al., 2016) change the form of the Q-learning target to incorporate on-policy samples and fluidly switch between on-policy learning and off-policy learning.",2. Deep Reinforcement Learning,[0],[0]
"Munos et al. (2016) show that by incorporating on-policy samples allows an agent to learn faster in Atari environments, indicating that reward propagation is indeed a bottleneck to efficiency in deep reinforcement learning.
",2. Deep Reinforcement Learning,[0],[0]
"A3C (Mnih et al., 2016) is another well known deep reinforcement learning algorithm that is very different from DQN.",2. Deep Reinforcement Learning,[0],[0]
"It is based upon a policy gradient, and learns both a policy and its associated value function, which is learned
entirely on-policy (similar to the λ",2. Deep Reinforcement Learning,[0],[0]
= 1 case of Q(λ)).,2. Deep Reinforcement Learning,[0],[0]
"Interestingly, Mnih et al. (2016) also added an LSTM memory to the otherwise convolutional neural network architecture to give the agent a notion of memory, although this did not have significant impact on the performance on Atari games.",2. Deep Reinforcement Learning,[0],[0]
"Our agent consists of three components: a convolutional neural network that processes pixel images s, a set of memory modules (one per action), and a final network that converts read-outs from the action memories into Q(s, a) values.",3. Neural Episodic Control,[0],[0]
"For the convolutional neural network we use the same architecture as DQN (Mnih et al., 2015).",3. Neural Episodic Control,[0],[0]
"For each action a ∈ A, NEC has a simple memory module Ma = (Ka, Va), where Ka and Va are dynamically sized arrays of vectors, each containing the same number of vectors.",3.1. Differentiable Neural Dictionary,[0],[0]
"The memory module acts as an arbitrary association from keys to corresponding values, much like the dictionary data type found in programs.",3.1. Differentiable Neural Dictionary,[0],[0]
Thus we refer to this kind of memory module as a differentiable neural dictionary (DND).,3.1. Differentiable Neural Dictionary,[0],[0]
"There are two operations possible on a DND: lookup and write, as depicted in Figure 1.",3.1. Differentiable Neural Dictionary,[0],[0]
"Performing a lookup on a DND maps a key h to an output value o:
o = ∑ i wivi, (1)
where vi is the ith element of the array Va and wi = k(h, hi)/ ∑ j k(h, hj), (2)
where hi is the ith element of the array Ka and k(x, y) is a kernel between vectors x and y, e.g., Gaussian or inverse kernels.",3.1. Differentiable Neural Dictionary,[0],[0]
"Thus the output of a lookup in a DND is a weighted sum of the values in the memory, whose weights are given by normalised kernels between the lookup key and the corresponding key in memory.",3.1. Differentiable Neural Dictionary,[0],[0]
"To make queries into very large memories scalable we shall make two approximations in practice: firstly, we shall limit (1) to the top p-nearest neighbours (typically p = 50).",3.1. Differentiable Neural Dictionary,[0],[0]
"Secondly, we use an approximate nearest neighbours algorithm to perform the lookups, based upon kd-trees (Bentley, 1975).
",3.1. Differentiable Neural Dictionary,[0],[0]
"After a DND is queried, a new key-value pair is written into the memory.",3.1. Differentiable Neural Dictionary,[0],[0]
The key written corresponds to the key that was looked up.,3.1. Differentiable Neural Dictionary,[0],[0]
The associated value is application-specific (below we specify the update for the NEC agent).,3.1. Differentiable Neural Dictionary,[0],[0]
Writes to a DND are append-only: keys and values are written to the memory by appending them onto the end of the arrays Ka and Va respectively.,3.1. Differentiable Neural Dictionary,[0],[0]
"If a key already exists in the memory, then its corresponding value is updated, rather than being duplicated.
",3.1. Differentiable Neural Dictionary,[0],[0]
Note that a DND is a differentiable version of the memory module described in Blundell et al. (2016).,3.1. Differentiable Neural Dictionary,[0],[0]
"It is also a generalisation to the memory and lookup schemes described in (Vinyals et al., 2016; Kaiser et al., 2016) for classification.",3.1. Differentiable Neural Dictionary,[0],[0]
"Figure 2 shows a DND as part of the NEC agent for a single action, whilst Algorithm 1 describes the general outline of the NEC algorithm.",3.2. Agent Architecture,[0],[0]
The pixel state s is processed by a convolutional neural network to produce a key h.,3.2. Agent Architecture,[0],[0]
"The key h is then used to lookup a value from the DND, yielding weights wi in the process for each element of the memory arrays.",3.2. Agent Architecture,[0],[0]
"Finally, the output is a weighted sum of the values in the DND.",3.2. Agent Architecture,[0],[0]
"The values in the DND, in the case of an NEC agent, are the Q values corresponding to the state that originally resulted in the corresponding key-value pair to be written to the memory.",3.2. Agent Architecture,[0],[0]
"Thus this architecture produces an estimate of Q(s, a) for a single given action a. The architecture is replicated once for each action a the agent can take, with the convolutional part of the network shared among each separate DND Ma.",3.2. Agent Architecture,[0],[0]
The NEC agent acts by taking the action with the highest Q-value estimate at each time step.,3.2. Agent Architecture,[0],[0]
"In practice, we use -greedy policy during training with a low .
",3.2. Agent Architecture,[0],[0]
Algorithm 1 Neural Episodic Control D: replay memory.,3.2. Agent Architecture,[0],[0]
Ma: a DND for each action a. N : horizon for N -step Q estimate.,3.2. Agent Architecture,[0],[0]
"for each episode do
for t = 1, 2, . . .",3.2. Agent Architecture,[0],[0]
", T do Receive observation st from environment with embedding h. Estimate Q(st, a) for each action a via (1) from Ma at ← -greedy policy based on Q(st, a) Take action at, receive reward rt+1",3.2. Agent Architecture,[0],[0]
"Append (h,Q(N)(st, at)) to Mat .",3.2. Agent Architecture,[0],[0]
"Append (st, at, Q(N)(st, at)) to D. Train on a random minibatch from D.
end for end for
3.3.",3.2. Agent Architecture,[0],[0]
"Adding (s, a) pairs to memory
As an NEC agent acts, it continually adds new key-value pairs to its memory.",3.2. Agent Architecture,[0],[0]
"Keys are appended to the memory of the corresponding action, taking the value of the query key h encoded by the convolutional neural network.",3.2. Agent Architecture,[0],[0]
We now turn to the question of an appropriate corresponding value.,3.2. Agent Architecture,[0],[0]
"In Blundell et al. (2016), Monte Carlo returns were written to memory.",3.2. Agent Architecture,[0],[0]
We found that a mixture of Monte Carlo returns (on-policy) and off-policy backups worked better and so for NEC we elect to use N -step,3.2. Agent Architecture,[0],[0]
"Q-learning as in Mnih et al.
(2016) (see also Watkins, 1989; Peng & Williams, 1996).",3.2. Agent Architecture,[0],[0]
"This adds the following N on-policy rewards and bootstraps the sum of discounted rewards for the rest of the trajectory, off-policy.",3.2. Agent Architecture,[0],[0]
"The N -step Q-value estimate is then
Q(N)(st, a) = N−1∑ j=0 γjrt+j",3.2. Agent Architecture,[0],[0]
"+ γ N max a′ Q(st+N , a ′) .",3.2. Agent Architecture,[0],[0]
"(3)
The bootstrap term of (3), maxa′ Q(st+N , a′) is found by querying all memories Ma for each action a and taking the highest estimated Q-value returned.",3.2. Agent Architecture,[0],[0]
"Note that the earliest such values can be added to memory is N steps after a particular (s, a) pair occurs.
",3.2. Agent Architecture,[0],[0]
"When a state-action value is already present in a DND (i.e the exact same key h is already in Ka), the corresponding value present in Va, Qi, is updated in the same way as the classic tabular Q-learning algorithm:
Qi ← Qi + α(Q(N)(s, a)−Qi) .",3.2. Agent Architecture,[0],[0]
"(4)
where α is the learning rate of the Q update.",3.2. Agent Architecture,[0],[0]
"If the state is not already present Q(N)(st, a) is appended to Va and h is appended to Ka.",3.2. Agent Architecture,[0],[0]
"Note that our agent learns the value function in much the same way that a classic tabular Q-learning agent does, except that the Q-table grows with time.",3.2. Agent Architecture,[0],[0]
"We found that α could take on a high value, allowing repeatedly visited states with a stable representation to rapidly update their value function estimate.",3.2. Agent Architecture,[0],[0]
"Additionally, batching up memory updates (e.g., at the end of the episode) helps with computational performance.",3.2. Agent Architecture,[0],[0]
We overwrite the item that has least recently shown up as a neighbour when we reach the memory’s maximum capacity.,3.2. Agent Architecture,[0],[0]
Agent parameters are updated by minimising the L2 loss between the predicted Q value for a given action and the Q(N) estimate on randomly sampled mini-batches from a replay buffer.,3.4. Learning,[0],[0]
"In particular, we store tuples (st, at, Rt) in
the replay buffer, where N is the horizon of the N -step Q rule, and Rt = Q(N)(st, a) plays the role of the target network seen in DQN (our replay buffer is significantly smaller than DQN’s).",3.4. Learning,[0],[0]
"These (st, at, Rt)-tuples are then sampled uniformly at random to form minibatches for training.",3.4. Learning,[0],[0]
Note that the architecture in Figure 2 is entirely differentiable and so we can minimize this loss by gradient descent.,3.4. Learning,[0],[0]
"Backpropagation updates the the weights and biases of the convolutional embedding network and the keys and values of each action-specific memory using gradients of this loss, using a lower learning rate than is used for updating pairs after queries (α).",3.4. Learning,[0],[0]
We investigated whether neural episodic control allows for more data efficient learning in practice in complex domains.,4. Experiments,[0],[0]
"As a problem domain we chose the Atari Learning Environment(ALE; Bellemare et al., 2013).",4. Experiments,[0],[0]
"We tested our method on the 57 Atari games used by Schaul et al. (2015a), which form an interesting set of tasks as they contain diverse challenges such as sparse rewards and vastly different magnitudes of scores across games.",4. Experiments,[0],[0]
"Most common algorithms applied in these domains, such as variants of DQN and A3C, require in the thousands of hours of in-game time, i.e. they are data inefficient.
",4. Experiments,[0],[0]
"We consider 5 variants of A3C and DQN as baselines as well as MFEC (Blundell et al., 2016).",4. Experiments,[0],[0]
"We compare to the basic implementations of A3C (Mnih et al., 2016) and DQN (Mnih et al., 2015).",4. Experiments,[0],[0]
"We also compare to two algorithms incorporating λ returns (Sutton, 1988) aiming at more data efficiency by faster propagation of credit assignments, namely Q∗(λ)",4. Experiments,[0],[0]
"(Harutyunyan et al., 2016) and Retrace(λ) (Munos et al., 2016).",4. Experiments,[0],[0]
"We also compare to DQN with Prioritised Replay, which improves data efficiency by replaying more salient transitions more frequently.",4. Experiments,[0],[0]
"We did not directly compare to DRQN (Hausknecht & Stone, 2015) nor FRMQN (Oh et al., 2016) as results were not available
for all Atari games.",4. Experiments,[0],[0]
"Note that in the case of DRQN, reported performance is lower than that of Prioritised Replay.
",4. Experiments,[0],[0]
"All algorithms were trained using discount rate γ = 0.99, except MFEC that uses γ = 1.",4. Experiments,[0],[0]
"In our implementation of MFEC we used random projections as an embedding function, since in the original publication it obtained better performance on the Atari games tested.
",4. Experiments,[0],[0]
"In terms of hyperparameters for NEC, we chose the same convolutional architecture as DQN, and store up to 5 × 105 memories per action.",4. Experiments,[0],[0]
"We used the RMSProp algorithm (Tieleman & Hinton, 2012) for gradient descent training.",4. Experiments,[0],[0]
"We apply the same preprocessing steps as (Mnih et al., 2015), including repeating each action four times.",4. Experiments,[0],[0]
For the N -step Q estimates we picked a horizon of N = 100.,4. Experiments,[0],[0]
Our replay buffer stores the only last 105 states (as opposed to 106 for DQN) observed and their N -step Q estimates.,4. Experiments,[0],[0]
We do one replay update for every 16 observed frames with a minibatch of size 32.,4. Experiments,[0],[0]
We set the number of nearest neighbours p = 50 in all our experiments.,4. Experiments,[0],[0]
"For the kernel function we chose a function that interpolates between the mean for short distances and weighted inverse distance for large distances, more precisely:
k(h, hi) = 1
‖h− hi‖22",4. Experiments,[0],[0]
+ δ .,4. Experiments,[0],[0]
"(5)
Intuitively, when all neighbours are far away we want to avoid putting all weight onto one data point.",4. Experiments,[0],[0]
"A Gaussian kernel, for example, would exponentially suppress all neighbours except for the closest one.",4. Experiments,[0],[0]
The kernel we chose has the advantage of having heavy tails.,4. Experiments,[0],[0]
This makes the algorithm more robust and we found it to be less sensitive to kernel hyperparameters.,4. Experiments,[0],[0]
"We set δ = 10−3.
",4. Experiments,[0],[0]
"In order to tune the remaining hyperparameters (SGD learning-rate, fast-update learning-rate α in Equation 4, dimensionality of the embeddings, Q(N) in Equation 3, and - greedy exploration-rate) we ran a hyperparameter sweep on six games: Beam Rider, Breakout, Pong, Q*Bert, Seaquest and Space Invaders.",4. Experiments,[0],[0]
"We picked the hyperparameter values that performed best on the median for this subset of games (a
common cross validation procedure described by Bellemare et al. (2013), and adhered to by Mnih et al. (2015)).
",4. Experiments,[0],[0]
Data efficiency results are summarised in Table 1.,4. Experiments,[0],[0]
In the small data regime (less than 20 million frames) NEC clearly outperforms all other algorithms.,4. Experiments,[0],[0]
The difference is especially pronounced before 5 million frames have been observed.,4. Experiments,[0],[0]
"Only at 40 million frames does DQN with Prioritised Replay outperform NEC on average; note that this corresponds to 185 hours of gameplay.
",4. Experiments,[0],[0]
"In order to provide a more detailed picture of NEC’s performance, Figures 3 to 7 show learning curves on 6 games (Alien, Bowling, Boxing, Frostbite, HERO, Ms. Pac-Man, Pong), where several stereotypical cases of NEC’s performance can be observed.",4. Experiments,[0],[0]
All learning curves show the average performance over 5 different initial random seeds.,4. Experiments,[0],[0]
"We evaluate MFEC and NEC every 200.000 frames, and the other algorithms are evaluated every million steps.
",4. Experiments,[0],[0]
"Across most games, NEC is significantly faster at learning in the initial phase (see also Table 1), only comparable to MFEC, which also uses an episodic-like Q-function.
",4. Experiments,[0],[0]
NEC also outperforms MFEC on average (see Table 2).,4. Experiments,[0],[0]
"In contrast with MFEC, NEC uses the reward signal to learn an embedding adequate for value interpolation.",4. Experiments,[0],[0]
This difference is especially significant in games where a few pixels determine the value of each action.,4. Experiments,[0],[0]
"The simpler version of MFEC uses an approximation to L2 distances in pixel-space by means of random projections, and cannot focus on the small but most relevant details.",4. Experiments,[0],[0]
"Another version of MFEC calculated distances on the latent representation of a variational autoencoder (Kingma & Welling, 2013) trained to model frames.",4. Experiments,[0],[0]
"This latent representation does not depend on rewards and will be subject to irrelevant details like, for example, the display of the current score.
",4. Experiments,[0],[0]
"A3C, DQN and related algorithms require rewards to be clipped to the range [−1, 1] for training stability1(Mnih
1See Pop–Art (van Hasselt et al., 2016) for a DQN-like algorithm that does not require reward-clipping.",4. Experiments,[0],[0]
"NEC also outperforms
et al., 2015).",4. Experiments,[0],[0]
"NEC and MFEC do not require reward clipping, which results in qualitative changes in behaviour and better performance relative to other algorithms on games requiring clipping (Bowling, Frostbite, H.E.R.O., Ms. PacMan, Alien out of the seven shown).
",4. Experiments,[0],[0]
Figure 3.,4. Experiments,[0],[0]
"Learning curve on Bowling.
",4. Experiments,[0],[0]
"Alien and Ms. Pac-Man both involve controlling a character, where there is an easy way to collect small rewards by collecting items of which there are plenty, while avoiding enemies, which are invulnerable to the agent.",4. Experiments,[0],[0]
"On the other hand the agent can pick up a special item making enemies vulnerable, allowing the agent to attack them and get significantly larger rewards than from collecting the small rewards.",4. Experiments,[0],[0]
Agents trained using existing parametric methods tend to show little interest in this as clipping implies there is no difference between large and small rewards.,4. Experiments,[0],[0]
"Therefore, as NEC does not need reward clipping, it can strongly
Pop–Art.
",4. Experiments,[0],[0]
Figure 4.,4. Experiments,[0],[0]
"Learning curve on Frostbite.
outperform other algorithms, since NEC is maximising the non-clipped score (the true score).",4. Experiments,[0],[0]
"This can also be seen when observing the agents play: parametric methods will tend to collect small rewards, while NEC will try to actively make the enemies vulnerable and attack them to get large rewards.
",4. Experiments,[0],[0]
NEC also outperforms the other algorithms on Pong and Boxing where reward clipping does not affect any of the algorithms as all original rewards are in the range,4. Experiments,[0],[0]
"[−1, 1]; as can be expected, NEC does not outperform others in terms of maximally achieved score, but it is vastly more data efficient.
",4. Experiments,[0],[0]
In Figure 10 we show a chart of human-normalised scores across all 57 Atari games at 10 million frames comparing to Prioritised Replay and MFEC.,4. Experiments,[0],[0]
"We rank the games independently for each algorithm, and on the y-axis the deciles are
Figure 5.",4. Experiments,[0],[0]
"Learning curve on H.E.R.O.
Figure 6.",4. Experiments,[0],[0]
"Learning curve on Ms. Pac-Man.
shown.
",4. Experiments,[0],[0]
We can see that NEC gets to a human level performance in about 25% of the games within 10 million frames.,4. Experiments,[0],[0]
As we can see NEC outperforms MFEC and Prioritised Replay.,4. Experiments,[0],[0]
"There has been much recent work on memory architectures for neural networks (LSTM; Hochreiter & Schmidhuber, 1997), DNC (Graves et al., 2016), memory networks (Sukhbaatar et al., 2015; Miller et al., 2016)).",5. Related work,[0],[0]
"Recurrent neural network representations of memory (LSTMs and DNCs) are trained by truncated backpropagation through time, and are subject to the same slow learning of non-recurrent neural networks.
",5. Related work,[0],[0]
"Some of these models have been adapted to their use in RL agents (LSTMs; Bakker et al., 2003; Hausknecht & Stone, 2015), DNCs (Graves et al., 2016), memory networks (Oh et al., 2016).",5. Related work,[0],[0]
"However, the contents of these memories is typically reset at the beginning of every episode.",5. Related work,[0],[0]
"This is ap-
Figure 7.",5. Related work,[0],[0]
"Learning curve on Alien.
",5. Related work,[0],[0]
Figure 8.,5. Related work,[0],[0]
"Learning curve on Pong.
propriate when the goal of the memory is tracking previous observations in order to maximise rewards in partially observable or non-Markovian environments.",5. Related work,[0],[0]
"Therefore, these implementations can be thought of as a type of working memory, and solve a different problem than the one addressed in this work.
RNNs can learn to quickly write highly rewarding states into memory and may even be able to learn entire reinforcement learning algorithms (Wang et al., 2016; Duan et al., 2016).",5. Related work,[0],[0]
"However, doing so can take an arbitrarily long time and the learning time likely scales strongly with the complexity of the task.
",5. Related work,[0],[0]
The work of Oh et al. (2016) is also reminiscent of the ideas presented here.,5. Related work,[0],[0]
"They introduced (FR)MQN, an adaptation of memory networks used in the top layers of a Q-network.
",5. Related work,[0],[0]
Kaiser et al. (2016) introduced a differentiable layer of keyvalue pairs that can be plugged into a neural network.,5. Related work,[0],[0]
This layer uses cosine similarity to calculate a weighted average of the values associated with the k most similar memories.,5. Related work,[0],[0]
"Their use of a moving average update rule is reminiscent of
the one presented in Section 3.",5. Related work,[0],[0]
"The authors reported results on a set of supervised tasks, however they did not consider applications to reinforcement learning.",5. Related work,[0],[0]
Other deep RL methods keep a history of previous experience.,5. Related work,[0],[0]
"Indeed, DQN itself has an elementary form of memory: the replay buffer central to its stable training can be viewed as a memory that is frequently replayed to distil the contents into DQN’s value network.",5. Related work,[0],[0]
Kumaran et al. (2016) suggest that training on replayed experiences from the replay buffer in DQN is similar to the replay of experiences from episodic memory during sleep in animals.,5. Related work,[0],[0]
"DQN’s replay buffer differs from most other work on memory for deep reinforcement learning in its sheer scale: it is common for DQN’s replay buffer to hold millions of (s, a, r, s′) tuples.",5. Related work,[0],[0]
The use of local regression techniques for Q-function approximation has been suggested before: Santamarı́a,5. Related work,[0],[0]
et al. (1997) proposed the use of k-nearest-neighbours regression with a heuristic for adding memories based on the distance to previous memories.,5. Related work,[0],[0]
"Munos & Moore (1998) proposed barycentric interpolators to model the value function and proved their convergence to the optimal value function under mild conditions, but no empirical results were presented.",5. Related work,[0],[0]
"Gabel & Riedmiller (2005) also suggested the use of local regression, under the paradigm of case-based-reasoning that included heuristics for the deletion of stored cases.",5. Related work,[0],[0]
"Blundell et al. (2016, MFEC) recently used local regression for Q-function estimation using the mean of the k-nearest neighbours, except in the case of an exact match of the query point, in which case the stored value was returned.",5. Related work,[0],[0]
"They also propose the use of the latent variable obtained from a variational autoencoder (Rezende et al., 2014) as an embedding space, but showed random projections often obtained better results.",5. Related work,[0],[0]
"In contrast with the ideas presented here, none of the localregression work aforementioned uses the reward signal to learn an embedding space of covariates in which to perform the local-regression.",5. Related work,[0],[0]
"We learn this embedding space using temporal-difference learning; a crucial difference, as we
showed in the experimental comparison to MFEC.",5. Related work,[0],[0]
We have proposed Neural Episodic Control (NEC): a deep reinforcement learning agent that learns significantly faster than other baseline agents on a wide range of Atari 2600 games.,6. Discussion,[0],[0]
"At the core of NEC is a memory structure: a Differentiable Neural Dictionary (DND), one for each potential action.",6. Discussion,[0],[0]
"NEC inserts recent state representations paired with corresponding value functions into the appropriate DND.
",6. Discussion,[0],[0]
"Our experiments show that NEC requires an order of magnitude fewer interactions with the environment than agents previously proposed for data efficiency, such as Prioritised Replay (Schaul et al., 2015b) and Retrace(λ) (Munos et al., 2016).",6. Discussion,[0],[0]
"We speculate that NEC learns faster through a com-
bination of three features of the agent: the memory architecture (DND), the use of N -step Q estimates, and a state representation provided by a convolutional neural network.
",6. Discussion,[0],[0]
"The memory architecture, DND, rapidly integrates recent experience—state representations and corresponding value estimates—allowing this information to be rapidly integrated into future behaviour.",6. Discussion,[0],[0]
"Such memories persist across many episodes, and we use a fast approximate nearest neighbour algorithm (kd-trees) to ensure that such memories can be efficiently accessed.",6. Discussion,[0],[0]
Estimating Q-values by using the N -step Q value function interpolates between Monte Carlo value estimates and backed up off-policy estimates.,6. Discussion,[0],[0]
"Monte Carlo value estimates reflect the rewards an agent is actually receiving, whilst backed up off-policy estimates should be more representative of the value function at the optimal policy, but evolve much slower.",6. Discussion,[0],[0]
"By using both estimates, NEC can trade-off between these two estimation procedures and their relative strengths and weaknesses (speed of reward propagation vs optimality).",6. Discussion,[0],[0]
"Finally, by having a slow changing, stable representation provided by a convolutional neural network, keys stored in the DND remain relative stable.
",6. Discussion,[0],[0]
"Our work suggests that non-parametric methods are a promising addition to the deep reinforcement learning toolbox, especially where data efficiency is paramount.",6. Discussion,[0],[0]
In our experiments we saw that at the beginning of learning NEC outperforms other agents in terms of learning speed.,6. Discussion,[0],[0]
We saw that later in learning Prioritised Replay has higher performance than NEC.,6. Discussion,[0],[0]
We leave it to future work to further improve NEC so that its long term final performance is significantly superior to parametric agents.,6. Discussion,[0],[0]
"Another avenue of further research would be to apply the method discussed in this paper to a wider range of tasks such as visually more complex 3D worlds or real world tasks where data efficiency is of great importance due to the high cost of acquiring data.
",6. Discussion,[0],[0]
"Acknowledgements The authors would like to thank Daniel Zoran, Dharshan Kumaran, Jane Wang, Dan Belov, Ruiqi Guo, Yori Zwols, Jack Rae, Andreas Kirsch, Peter Dayan, David Silver and many others at DeepMind for insightful discussions and feedback.",6. Discussion,[0],[0]
"We also thank Georg Ostrovski, Tom Schaul, and Hubert Soyer for providing baseline learning curves.",6. Discussion,[0],[0]
Deep reinforcement learning methods attain super-human performance in a wide range of environments.,abstractText,[0],[0]
"Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance.",abstractText,[0],[0]
We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them.,abstractText,[0],[0]
Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function.,abstractText,[0],[0]
"We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.",abstractText,[0],[0]
Neural Episodic Control,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 16–25 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Fine-grained Entity Type Classification (FETC) aims at labeling entity mentions in context with one or more specific types organized in a hierarchy (e.g., actor as a subtype of artist, which in turn is a subtype of person).",1 Introduction,[0],[0]
"Fine-grained types help in many applications, including relation extraction (Mintz et al., 2009), question answering (Li and Roth, 2002), entity linking (Lin et al., 2012), knowledge base completion (Dong et al., 2014) and entity recommendation (Yu et al., 2014).",1 Introduction,[0],[0]
"Because of the high cost in labeling large training corpora with fine-grained types, current FETC systems resort to distant supervision (Mintz et al., 2009) and annotate mentions in the training corpus with all types associated with the entity in a knowledge graph.",1 Introduction,[0],[0]
"This is illustrated in
Figure 1, with three training sentences about entity Steve Kerr.",1 Introduction,[0],[0]
"Note that while the entity belongs to three fine-grained types (person, athlete, and coach), some sentences provide evidence of only some of the types: person and coach from S1, person and athlete from S2, and just person for S3.",1 Introduction,[0],[0]
"Clearly, direct distant supervision leads to noisy training data which can hurt the accuracy of the FETC model.
",1 Introduction,[0],[0]
One kind of noise introduced by distant supervision is assigning labels that are out-of-context (athlete in S1 and coach in S2) for the sentence.,1 Introduction,[0],[0]
Current FETC systems sidestep the issue by either ignoring out-of-context labels or using simple pruning heuristics like discarding training examples with entities assigned to multiple types in the knowledge graph.,1 Introduction,[0],[0]
"However, both strategies are inelegant and hurt accuracy.",1 Introduction,[0],[0]
Another source of noise introduced by distant supervision is when the type is overly-specific for the context.,1 Introduction,[0],[0]
"For instance, example S3 does not support the inference that Mr. Kerr is either an athlete or a coach.",1 Introduction,[0],[0]
"Since existing knowledge graphs give more attention to notable entities with more specific types, overly-specific labels bias the model towards popular subtypes instead of generic ones, i.e., preferring athlete over person.",1 Introduction,[0],[0]
"Instead of correcting for this bias, most existing FETC systems ignore the problem and treat each type equally and independently, ignoring that many types are semantically related.
",1 Introduction,[0],[0]
Besides failing to handle noisy training data there are two other limitations of previous FETC approaches we seek to address.,1 Introduction,[0],[0]
"First, they rely on hand-crafted features derived from various NLP tools; therefore, the inevitable errors introduced by these tools propagate to the FETC systems via the training data.",1 Introduction,[0],[0]
"Second, previous systems treat FETC as a multi-label classification problem: during type inference they predict a plausibility score for each type, and, then, either classify types
16
with scores above a threshold (Mintz et al., 2009; Gillick et al., 2014; Shimaoka et al., 2017) or perform a top-down search in the given type hierarchy (Ren et al., 2016a; Abhishek et al., 2017).
",1 Introduction,[0],[0]
Contributions: We propose a neural network based model to overcome the drawbacks of existing FETC systems mentioned above.,1 Introduction,[0],[0]
"With publicly available word embeddings as input, we learn two different entity representations and use bidirectional long-short term memory (LSTM) with attention to learn the context representation.",1 Introduction,[0],[0]
We propose a variant of cross entropy loss function to handle out-of-context labels automatically during the training phase.,1 Introduction,[0],[0]
"Also, we introduce hierarchical loss normalization to adjust the penalties for correlated types, allowing our model to understand the type hierarchy and alleviate the negative effect of overly-specific labels.
",1 Introduction,[0],[0]
"Moreover, in order to simplify the problem and take advantage of previous research on hierarchical classification, we transform the multi-label classification problem to a single-label classification problem.",1 Introduction,[0],[0]
"Based on the assumption that each mention can only have one type-path depending on the context, we leverage the fact that type hierarchies are forests, and represent each type-path uniquely by the terminal type (which might not be a leaf node).",1 Introduction,[0],[0]
"For Example, type-path rootperson-coach can be represented as just coach, while root-person can be unambiguously represented as the non-leaf person.
",1 Introduction,[0],[0]
"Finally, we report on an experimental validation against the state-of-the-art on established bench-
marks that shows that our model can adapt to noise in training data and consistently outperform previous methods.",1 Introduction,[0],[0]
"In summary, we describe a single, much simpler and more elegant neural network model that attempts FETC “end-to-end” without post-processing or ad-hoc features and improves on the state-of-the-art for the task.",1 Introduction,[0],[0]
"Fine-Grained Entity Type Classification: The first work to use distant supervision (Mintz et al., 2009) to induce a large - but noisy - training set and manually label a significantly smaller dataset to evaluate their FETC system, was Ling and Weld (2012) who introduced both a training and evaluation dataset FIGER (GOLD).",2 Related Work,[0],[0]
They used a linear classifier perceptron for multi-label classification.,2 Related Work,[0],[0]
"While initial work largely assumed that mention assignments could be done independently of the mention context, Gillick et al. (2014) introduced the concept of context-dependent FETC where the types of a mention are constrained to what can be deduced from its context and introduced a new OntoNotes-derived (Weischedel et al., 2011) manually annotated evaluation dataset.",2 Related Work,[0],[0]
"In addition, they addressed the problem of label noise induced by distant supervision and proposed three label cleaning heuristics.",2 Related Work,[0],[0]
Yogatama et al. (2015) proposed an embedding-based model where userdefined features and labels were embedded into a low dimensional feature space to facilitate information sharing among labels.,2 Related Work,[0],[0]
"Ma et al. (2016) presented a label embedding method that incor-
porates prototypical and hierarchical information to learn pre-trained label embeddings and adpated a zero-shot framework that can predict both seen and previously unseen entity types.
",2 Related Work,[0],[0]
Shimaoka et al. (2016) proposed an attentive neural network model that used LSTMs to encode the context of an entity mention and used an attention mechanism to allow the model to focus on relevant expressions in such context.,2 Related Work,[0],[0]
Shimaoka et al. (2017) summarizes many neural architectures for FETC task.,2 Related Work,[0],[0]
"These models ignore the outof-context noise, that is, they assume that all labels obtained via distant supervision are “correct” and appropriate for every context in the training corpus.",2 Related Work,[0],[0]
"In our paper, a simple yet effective variant of cross entropy loss function is proposed to handle the problem of out-of-context noise.
",2 Related Work,[0],[0]
"Ren et al. (2016a) have proposed AFET, an FETC system, that separates the loss function for clean and noisy entity mentions and uses labellabel correlation information obtained by given data in its parametric loss function.",2 Related Work,[0],[0]
"Considering the noise reduction aspects for FETC systems, Ren et al. (2016b) introduced a method called LNR to reduce label noise without data loss, leading to significant performance gains on both the evaluation dataset of FIGER(GOLD) and OntoNotes.",2 Related Work,[0],[0]
"Although these works consider both out-of-context noise and overly-specific noise, they rely on handcrafted features which become an impediment to further improvement of the model performance.",2 Related Work,[0],[0]
"For LNR, because the noise reduction step is separated from the FETC model, the inevitable errors introduced by the noise reduction will be propagated into the FETC model which is undesirable.",2 Related Work,[0],[0]
"In our FETC system, we handle the problem induced from irrelevant noise and overly-specific noise seamlessly inside the model and avoid the usage of hand-crafted features.
",2 Related Work,[0],[0]
"Most recently, following the idea from AFET, Abhishek et al. (2017) proposed a simple neural network model which incorporates noisy label information using a variant of non-parametric
hinge loss function and gain great performance improvement on FIGER(GOLD).",2 Related Work,[0],[0]
"However, their work overlooks the effect of overly-specific noise, treating each type label equally and independently when learning the classifiers and ignores possible correlations among types.
",2 Related Work,[0],[0]
"Hierarchical Loss Function: Due to the intrinsic type hierarchy existing in the task of FETC, it is natural to adopt the idea of hierarchical loss function to adjust the penalties for FETC mistakes depending on how far they are in the hierarchy.",2 Related Work,[0],[0]
The penalty for predicting person instead of athlete should less than the penalty for predicting organization.,2 Related Work,[0],[0]
"To the best of our knowledge, the first use of a hierarchical loss function was originally introduced in the context of document categorization with support vector machines (Cai and Hofmann, 2004).",2 Related Work,[0],[0]
"However, that work assumed that weights to control the hierarchical loss would be solicited from domain experts, which is inapplicable for FETC.",2 Related Work,[0],[0]
"Instead, we propose a method called hierarchical loss normalization which can overcome the above limitations and be incorporated with cross entropy loss used in our neural architecture.
",2 Related Work,[0],[0]
Table 1 provides a summary comparison of our work against the previous state-of-the-art in fine grained entity typing.,2 Related Work,[0],[0]
Our task is to automatically reveal the type information for entity mentions in context.,3 Background and Problem,[0],[0]
"The input is a knowledge graph Ψ with schema YΨ, whose types are organized into a type hierarchy Y , and an automatically labeled training corpus D obtained by distant supervision with Y .",3 Background and Problem,[0],[0]
"The output is a type-path in Y for each named entity mentioned in a test sentence from a corpus Dt.
More precisely, a labeled corpus for entity type classification consists of a set of extracted entity mentions {mi}Ni=1 (i.e., token spans representing entities in text), the context (e.g., sentence, paragraph) of each mention {ci}Ni=1, and the candidate
type sets {Yi}Ni=1 automatically generated for each mention.
",3 Background and Problem,[0],[0]
"We represent the training corpus using a set of mention-based triples D = {(mi, ci,Yi)}Ni=1.
",3 Background and Problem,[0],[0]
"If Yi is free of out-of-context noise, the type labels for each mi should form a single type-path in Yi.",3 Background and Problem,[0],[0]
"However, Yi may contain type-paths that are irrelevant to mi in ci if there exists out-of-context noise.
",3 Background and Problem,[0],[0]
We denote the type set including all terminal types for each type-path as the target type set Yti .,3 Background and Problem,[0],[0]
"In the example type hierarchy shown in Figure 1, if Yi contains types person, athlete, coach, Yti should contain athlete, coach, but not person.",3 Background and Problem,[0],[0]
"In order to understand the trade-off between the effect of out-of-context noise and the size of the training set, we report on experiments with two different training sets: Dfiltered only with triples whose Yi form a single type-path in D, and Draw with all triples.
",3 Background and Problem,[0],[0]
"We formulate fine-grained entity classification problem as follows:
Definition 1",3 Background and Problem,[0],[0]
"Given an entity mention mi = (wp, . . .",3 Background and Problem,[0],[0]
", wt) (p, t ∈",3 Background and Problem,[0],[0]
"[1, T ], p ≤ t) and its context ci = (w1, . . .",3 Background and Problem,[0],[0]
", wT ) where T is the context length, our task is to predict its most specific type ŷi depending on the context.
",3 Background and Problem,[0],[0]
"In practice, ci is generated by truncating the original context with words beyond the context window size C both to the left and to the right of mi.",3 Background and Problem,[0],[0]
"Specifically, we compute a probability distribution over all theK = |Y| types in the target type hierarchy Y .",3 Background and Problem,[0],[0]
The type with the highest probability is classified as the predicted type ŷi which is the terminal type of the predicted type-path.,3 Background and Problem,[0],[0]
This section details our Neural Fine-Grained Entity Type Classification (NFETC) model.,4 Methodology,[0],[0]
"As stated in Section 3, the input is an entity mention mi with its context ci.",4.1 Input Representation,[0],[0]
"First, we transform each word in the context ci into a real-valued vector to provide lexical-semantic features.",4.1 Input Representation,[0],[0]
"Given a word embedding matrix Wwrd of size dw × |V |, where V is the input vocabulary and dw is the size of word embedding, we map every wi to a column vector wdi ∈ Rdw .
To additionally capture information about the relationship to the target entities, we incorporate
word position embeddings (Zeng et al., 2014) to reflect relative distances between the i-th word to the entity mention.",4.1 Input Representation,[0],[0]
"Every relative distance is mapped to a randomly initialized position vector in Rdp , where dp is the size of position embedding.",4.1 Input Representation,[0],[0]
"For a given word, we obtain the position vector wpi .",4.1 Input Representation,[0],[0]
The overall embedding for the i-th word is wEi =,4.1 Input Representation,[0],[0]
[(w d i ),4.1 Input Representation,[0],[0]
">, (wpi )",4.1 Input Representation,[0],[0]
>,4.1 Input Representation,[0],[0]
]>.,4.1 Input Representation,[0],[0]
"For the context ci, we want to apply a non-linear transformation to the vector representation of ci to derive a context feature vector hi = f(ci; θ) given a set of parameters θ.",4.2 Context Representation,[0],[0]
"In this paper, we adopt bidirectional LSTM with ds hidden units as f(ci; θ).",4.2 Context Representation,[0],[0]
The network contains two sub-networks for the forward pass and the backward pass respectively.,4.2 Context Representation,[0],[0]
"Here, we use element-wise sum to combine the forward and backward pass outputs.",4.2 Context Representation,[0],[0]
"The output of the i-th word in shown in the following equation:
hi =",4.2 Context Representation,[0],[0]
[ −→ hi ⊕ ←−,4.2 Context Representation,[0],[0]
"hi ] (1)
",4.2 Context Representation,[0],[0]
"Following Zhou et al. (2016), we employ word-level attention mechanism, which makes our model able to softly select the most informative words during training.",4.2 Context Representation,[0],[0]
"Let H be a matrix consisting of output vectors [h1, h2, . . .",4.2 Context Representation,[0],[0]
", hT ] that the LSTM produced.",4.2 Context Representation,[0],[0]
"The context representation r is formed by a weighted sum of these output vectors:
G = tanh(H) (2) α = softmax(w>G) (3)
rc = Hα > (4)
where H ∈ Rds×T , w is a trained parameter vector.",4.2 Context Representation,[0],[0]
"The dimension ofw,α, rc are ds, T, ds respectively.",4.2 Context Representation,[0],[0]
Averaging encoder:,4.3 Mention Representation,[0],[0]
"Given the entity mention mi = (wp, . . .",4.3 Mention Representation,[0],[0]
", wt) and its length L = t",4.3 Mention Representation,[0],[0]
"− p + 1, the averaging encoder computes the average word embedding of the words in mi.",4.3 Mention Representation,[0],[0]
"Formally, the averaging representation ra of the mention is computed as follows:
ra = 1
L
t∑
i=p
wdi (5)
",4.3 Mention Representation,[0],[0]
"This relatively simple method for composing the mention representation is motivated by it being less prone to overfitting (Shimaoka et al., 2017).
",4.3 Mention Representation,[0],[0]
"LSTM encoder: In order to capture more semantic information from the mentions, we add one token before and another after the target entity to the mention.",4.3 Mention Representation,[0],[0]
"The extended mention can be represented as m∗i = (wp−1, wp, . . .",4.3 Mention Representation,[0],[0]
", wt, wt+1).",4.3 Mention Representation,[0],[0]
"The standard LSTM is applied to the mention sequence from left to right and produces the outputs hp−1, . . .",4.3 Mention Representation,[0],[0]
", ht+1.",4.3 Mention Representation,[0],[0]
The last output ht+1 then serves as the LSTM representation rl of the mention.,4.3 Mention Representation,[0],[0]
We concatenate context representation and two mention representations together to form the overall feature representation of the input R =,4.4 Optimization,[0],[0]
"[rc, ra, rl].",4.4 Optimization,[0],[0]
"Then we use a softmax classifier to predict ŷi from a discrete set of classes for a entity mention m and its context c with R as input:
p̂(y|m, c) = softmax(WR+ b) (6) ŷ",4.4 Optimization,[0],[0]
"= arg max
y p̂(y|m, c) (7)
where W can be treated as the learned type embeddings and b is the bias.
",4.4 Optimization,[0],[0]
"The traditional cross-entropy loss function is represented as follows:
J(θ) =",4.4 Optimization,[0],[0]
"− 1 N
N∑
i=1
log(p̂(yi|mi, ci))",4.4 Optimization,[0],[0]
"+ λ‖Θ‖2 (8)
where yi is the only element in Yti and (mi, ci,Yi) ∈ Dfiltered.",4.4 Optimization,[0],[0]
"λ is an L2 regularization hyperparameter and Θ denotes all parameters of the considered model.
",4.4 Optimization,[0],[0]
"In order to handle data with out-of-context noise (in other words, with multiple labeled types) and take full advantage of them, we introduce a simple yet effective variant of the cross-entropy loss:
J(θ) =",4.4 Optimization,[0],[0]
"− 1 N
N∑
i=1
log(p̂(y∗i |mi, ci))",4.4 Optimization,[0],[0]
"+ λ‖Θ‖2 (9)
where y∗i = arg maxy∈Yti p̂(y|mi, ci) and (mi, ci,Yi) ∈",4.4 Optimization,[0],[0]
Draw.,4.4 Optimization,[0],[0]
"With this loss function, we assume that the type with the highest probability among Yti during training as the correct type.",4.4 Optimization,[0],[0]
"If there is only one element in Yti , this loss function is equivalent to the cross-entropy loss function.",4.4 Optimization,[0],[0]
"Wherever there are multiple elements, it can filter the less probable types based on the local context automatically.",4.4 Optimization,[0],[0]
"Since the fine-grained types tend to form a forest of type hierarchies, it is unreasonable to treat every type equally.",4.5 Hierarchical Loss Normalization,[0],[0]
"Intuitively, it is better to predict an ancestor type of the true type than some other unrelated type.",4.5 Hierarchical Loss Normalization,[0],[0]
"For instance, if one example is labeled as athlete, it is reasonable to predict its type as person.",4.5 Hierarchical Loss Normalization,[0],[0]
"However, predicting other high level types like location or organization would be inappropriate.",4.5 Hierarchical Loss Normalization,[0],[0]
"In other words, we want the loss function to penalize less the cases where types are related.",4.5 Hierarchical Loss Normalization,[0],[0]
"Based on the above idea, we adjust the estimated probability as follows:
p∗(ŷ|m, c) = p(ŷ|m, c) + β ∗ ∑
t∈Γ p(t|m, c) (10)
where Γ is the set of ancestor types along the type-path of ŷ, β is a hyperparameter to tune the penalty.",4.5 Hierarchical Loss Normalization,[0],[0]
"Afterwards, we re-normalize it back to a probability distribution, a process which we denote as hierarchical loss normalization.
",4.5 Hierarchical Loss Normalization,[0],[0]
"As discussed in Section 1, there exists overlyspecific noise in the automatically labeled training sets which hurt the model performance severely.",4.5 Hierarchical Loss Normalization,[0],[0]
"With hierarchical loss normalization, the model will get less penalty when it predicts the actual type for one example with overly-specific noise.",4.5 Hierarchical Loss Normalization,[0],[0]
"Hence, it can alleviate the negative effect of overly-specific noise effectively.",4.5 Hierarchical Loss Normalization,[0],[0]
"Generally, hierarchical loss normalization can make the model somewhat understand the given type hierarchy and learn to detect those overly-specific cases.",4.5 Hierarchical Loss Normalization,[0],[0]
"During classification, it will make the models prefer generic types unless there is a strong indicator for a more specific type in the context.",4.5 Hierarchical Loss Normalization,[0],[0]
"Dropout, proposed by Hinton et al. (2012), prevents co-adaptation of hidden units by randomly omitting feature detectors from the network during forward propagation.",4.6 Regularization,[0],[0]
We employ both input and output dropout on LSTM layers.,4.6 Regularization,[0],[0]
"In addition, we constrain L2-norms for the weight vectors as shown in Equations 8, 9 and use early stopping to decide when to stop training.",4.6 Regularization,[0],[0]
This section reports an experimental evaluation of our NFETC approach using the previous state-ofthe-art as baselines.,5 Experiments,[0],[0]
"We evaluate the proposed model on two standard and publicly available datasets, provided in a preprocessed tokenized format by Shimaoka et al. (2017).",5.1 Datasets,[0],[0]
Table 2 shows statistics about the benchmarks.,5.1 Datasets,[0],[0]
"The details are as follows:
• FIGER(GOLD):",5.1 Datasets,[0],[0]
"The training data consists of Wikipedia sentences and was automatically generated with distant supervision, by mapping Wikipedia identifiers to Freebase ones.",5.1 Datasets,[0],[0]
"The test data, mainly consisting of sentences from news reports, was manually annotated as described by Ling and Weld (2012).
",5.1 Datasets,[0],[0]
• OntoNotes:,5.1 Datasets,[0],[0]
"The OntoNotes dataset consists of sentences from newswire documents present in the OntoNotes text corpus (Weischedel et al., 2013).",5.1 Datasets,[0],[0]
"DBpedia spotlight (Daiber et al., 2013) was used to automatically link entity mention in sentences to Freebase.",5.1 Datasets,[0],[0]
"Manually annotated test data was shared by Gillick et al. (2014).
",5.1 Datasets,[0],[0]
"Because the type hierarchy can be somewhat understood by our proposed model, the quality of the type hierarchy can also be a key factor to the performance of our model.",5.1 Datasets,[0],[0]
We find that the type hierarchy for FIGER(GOLD) dataset following Freebase has some flaws.,5.1 Datasets,[0],[0]
"For example, software is not a subtype of product and government is not a subtype of organization.",5.1 Datasets,[0],[0]
"Following the proposed type hierarchy of Ling and Weld (2012), we refine the Freebase-based type hierarchy.",5.1 Datasets,[0],[0]
The process is a one-to-one mapping for types in the original dataset and we didn’t add or drop any type or sentence in the original dataset.,5.1 Datasets,[0],[0]
"As a result, we can directly compare the results of our proposed model with or without this refinement.
",5.1 Datasets,[0],[0]
"Aside from the advantages brought by adopting the single label classification setting, we can see one disadvantage of this setting based on Table 2.",5.1 Datasets,[0],[0]
"That is, the performance upper bounds of
our proposed model are no longer 100%: for example, the best strict accuracy we can get in this setting is 88.28% for FIGER(GOLD).",5.1 Datasets,[0],[0]
"However, as the strict accuracy of state-of-the-art methods are still nowhere near 80% (Table 3), the evaluation we perform is still informative.",5.1 Datasets,[0],[0]
"We compared the proposed model with state-ofthe-art FETC systems 1: (1) Attentive (Shimaoka et al., 2017); (2) AFET (Ren et al., 2016a); (3) LNR+FIGER (Ren et al., 2016b); (4) AAA (Abhishek et al., 2017).
",5.2 Baselines,[0],[0]
We compare these baselines with variants of our proposed model: (1) NFETC(f): basic neural model trained on Dfiltered (recall Section 4.4); (2) NFETC-hier(f): neural model with hierarichcal loss normalization trained on Dfiltered.,5.2 Baselines,[0],[0]
(3) NFETC(r): neural model with proposed variant of cross-entropy loss trained on Draw; (4) NFETC-hier(r): neural model with proposed variant of cross-entropy loss and hierarchical loss normalization trained on Draw.,5.2 Baselines,[0],[0]
"For evaluation metrics, we adopt the same criteria as Ling and Weld (2012), that is, we evaluate the model performance by strict accuracy, loose macro, and loose micro F-scores.",5.3 Experimental Setup,[0],[0]
"These measures are widely used in existing FETC systems (Shimaoka et al., 2017; Ren et al., 2016b,a; Abhishek et al., 2017).
",5.3 Experimental Setup,[0],[0]
We use pre-trained word embeddings that were not updated during training to help the model generalize to words not appearing in the training set.,5.3 Experimental Setup,[0],[0]
"For this purpose, we used the freely available 300-dimensional cased word embedding trained on 840 billion tokens from the Common Crawl supplied by Pennington et al. (2014).",5.3 Experimental Setup,[0],[0]
"For both datasets, we randomly sampled 10% of the test set as a development set, on which we do the hyperparameters tuning.",5.3 Experimental Setup,[0],[0]
The remaining 90% is used for final evaluation.,5.3 Experimental Setup,[0],[0]
"We run each model with the welltuned hyperparameter setting five times and report their average strict accuracy, macro F1 and micro F1 on the test set.",5.3 Experimental Setup,[0],[0]
The proposed model was implemented using the TensorFlow framework.,5.3 Experimental Setup,[0],[0]
"2
1The results of the baselines are all as reported in their corresponding papers.
",5.3 Experimental Setup,[0],[0]
2The code to replicate the work is available at: https: //github.com/billy-inn/NFETC,5.3 Experimental Setup,[0],[0]
"In this paper, we search different hyperparameter settings for FIGER(GOLD) and OntoNotes separately, considering the differences between the two datasets.",5.4 Hyperparameter Setting,[0],[0]
"The hyperparameters include the learning rate lr for Adam Optimizer, size of word position embeddings (WPE) dp, state size for LSTM layers ds, input dropout keep probability pi and output dropout keep probability po for LSTM layers 3, L2 regularization parameter λ and parameter to tune hierarchical loss normalization β.",5.4 Hyperparameter Setting,[0],[0]
"The values of these hyperparameters, obtained by evaluating the model performance on the development set, for each dataset can be found in Table 4.",5.4 Hyperparameter Setting,[0],[0]
Table 3 compares our models with other stateof-the-art FETC systems on FIGER(GOLD) and OntoNotes.,5.5 Performance comparison and analysis,[0],[0]
"The proposed model performs better than the existing FETC systems, consistently on both datasets.",5.5 Performance comparison and analysis,[0],[0]
"This indicates benefits of the proposed representation scheme, loss function and hierarchical loss normalization.
",5.5 Performance comparison and analysis,[0],[0]
"Discussion about Out-of-context Noise: For dataset FIGER(GOLD), the performance of our model with the proposed variant of cross-entropy loss trained onDraw is significantly better than the basic neural model trained on Dfiltered, suggesting that the proposed variant of the cross-entropy loss function can make use of the data with outof-context noise effectively.",5.5 Performance comparison and analysis,[0],[0]
"On the other hand, the improvement introduced by our proposed variant of cross-entropy loss is not as significant for the OntoNotes benchmark.",5.5 Performance comparison and analysis,[0],[0]
"This may be caused by the fact that OntoNotes is much smaller than FIGER(GOLD) and proportion of examples without out-of-context noise are also higher, as shown in Table 2.
",5.5 Performance comparison and analysis,[0],[0]
"3Following TensorFlow terminology.
",5.5 Performance comparison and analysis,[0],[0]
"Investigations on Overly-Specific Noise: With hierarchical loss normalization, the performance of our models are consistently better no matter whether trained on Draw or Dfiltered on both datasets, demonstrating the effectiveness of this hierarchical loss normalization and showing that overly-specific noise has a potentially significant influence on the performance of FETC systems.",5.5 Performance comparison and analysis,[0],[0]
"By visualizing the learned type embeddings (Figure 3), we can observe that the parent types are mixed with their subtypes and forms clear distinct clusters without hierarchical loss normalization, making it hard for the model to distinguish subtypes like actor or athlete from their parent types person.",5.6 T-SNE Visualization of Type Embeddings,[0],[0]
This also biases the model towards the most popular subtype.,5.6 T-SNE Visualization of Type Embeddings,[0],[0]
While the parent types tend to cluster together and the general pattern is more complicated with hierarchical loss normalization.,5.6 T-SNE Visualization of Type Embeddings,[0],[0]
"Although it’s not as easy to interpret, it hints that our model can learn rather subtle intricacies and correlations among types latent in the data with the help of hierarchical loss normalization, instead of sticking to a pre-defined hierarchy.",5.6 T-SNE Visualization of Type Embeddings,[0],[0]
"Since there are only 563 sentences for testing in FIGER(GOLD), we look into the predictions for
all the test examples of all variants of our model.",5.7 Error Analysis on FIGER(GOLD),[0],[0]
Table 5 shows 5 examples of test sentence.,5.7 Error Analysis on FIGER(GOLD),[0],[0]
"Without hierarchical loss normalization, our model will make too aggressive predictions for S1 with Politician and for S2 with Software.",5.7 Error Analysis on FIGER(GOLD),[0],[0]
This kind of mistakes are very common and can be effectively reduced by introducing hierarchical loss normalization leading to significant improvements on the model performance.,5.7 Error Analysis on FIGER(GOLD),[0],[0]
Using the changed loss function to handle multi-label (noisy) training data can help the model distinguish ambiguous cases.,5.7 Error Analysis on FIGER(GOLD),[0],[0]
"For example, our model trained on Dfiltered will misclassify S5 as Title, while the model trained on Draw can make the correct prediction.
",5.7 Error Analysis on FIGER(GOLD),[0],[0]
"However, there are still some errors that can’t be fixed with our model.",5.7 Error Analysis on FIGER(GOLD),[0],[0]
"For example, our model cannot make correct predictions for S3 and S4 due to the fact that our model doesn’t know that UW is an abbreviation of University of Washington and Washington state is the name of a province.",5.7 Error Analysis on FIGER(GOLD),[0],[0]
"In addition, the influence of overly-specific noise can only be alleviated but not eliminated.",5.7 Error Analysis on FIGER(GOLD),[0],[0]
"Sometimes, our model will still make too aggressive or conservative predictions.",5.7 Error Analysis on FIGER(GOLD),[0],[0]
"Also, mixing up very ambiguous entity names is inevitable in this task.",5.7 Error Analysis on FIGER(GOLD),[0],[0]
"In this paper, we studied two kinds of noise, namely out-of-context noise and overly-specific noise, for noisy type labels and investigate their effects on FETC systems.",6 Conclusion and Further Work,[0],[0]
We proposed a neural network based model which jointly learns representations for entity mentions and their context.,6 Conclusion and Further Work,[0],[0]
A variant of cross-entropy loss function was used to handle out-of-context noise.,6 Conclusion and Further Work,[0],[0]
Hierarchical loss normalization was introduced into our model to alleviate the effect of overly-specific noise.,6 Conclusion and Further Work,[0],[0]
"Experimental results on two publicly available datasets demonstrate that the proposed model is robust to these two kind of noise and outperforms previous state-of-the-art methods significantly.
",6 Conclusion and Further Work,[0],[0]
More work can be done to further develop hierarchical loss normalization since currently it’s very simple.,6 Conclusion and Further Work,[0],[0]
"Considering type information is valuable in various NLP tasks, we can incorporate results produced by our FETC system to other tasks, such as relation extraction, to check our model’s effectiveness and help improve other tasks’ per-
formance.",6 Conclusion and Further Work,[0],[0]
"In addition, tasks like relation extraction are complementary to the task of FETC and therefore may have potentials to be digged to help improve the performance of our system in return.",6 Conclusion and Further Work,[0],[0]
This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC).,Acknowledgments,[0],[0]
The task of Fine-grained Entity Type Classification (FETC) consists of assigning types from a hierarchy to entity mentions in text.,abstractText,[0],[0]
Existing methods rely on distant supervision and are thus susceptible to noisy labels that can be out-of-context or overly-specific for the training sentence.,abstractText,[0],[0]
Previous methods that attempt to address these issues do so with heuristics or with the help of hand-crafted features.,abstractText,[0],[0]
"Instead, we propose an end-to-end solution with a neural network model that uses a variant of crossentropy loss function to handle out-of-context labels, and hierarchical loss normalization to cope with overly-specific ones.",abstractText,[0],[0]
"Also, previous work solve FETC a multi-label classification followed by ad-hoc post-processing.",abstractText,[0],[0]
"In contrast, our solution is more elegant: we use public word embeddings to train a single-label that jointly learns representations for entity mentions and their context.",abstractText,[0],[0]
We show experimentally that our approach is robust against noise and consistently outperforms the state-of-theart on established benchmarks for the task.,abstractText,[0],[0]
Neural Fine-Grained Entity Type Classification with Hierarchy-Aware Loss,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 377–382 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
377",text,[0],[0]
"Attention-based neural translation models (Bahdanau et al., 2015; Luong et al., 2015) attend to specific positions on the source side to generate translation.",1 Introduction,[0],[0]
"Using the attention component provides significant improvements over the pure encoder-decoder sequence-to-sequence approach (Sutskever et al., 2014) that uses no such attention mechanism.",1 Introduction,[0],[0]
"In this work, we aim to compare the performance of attention-based models to another baseline, namely, neural hidden Markov models.
",1 Introduction,[0],[0]
"The neural HMM has been successfully applied in the literature on top of conventional phrasebased systems (Wang et al., 2017).",1 Introduction,[0],[0]
"In this work, our purpose is to explore its application in standalone decoding, i.e. the model is used to generate and score candidates without assistance from a phrase-based system.",1 Introduction,[0],[0]
"Because translation is done standalone using only neural models, we still refer to this as NMT.",1 Introduction,[0],[0]
"In addition, while Wang et al. (2017) applied feedforward networks to model alignment and translation, the recurrent structures proposed in this work surpass the feedforward variants by up to 1.3% in BLEU.
",1 Introduction,[0],[0]
"By comparing neural HMM and attention-based NMT, we shed light on the role of the attention component.",1 Introduction,[0],[0]
"To this end, we use an alignmentbased model that has a recurrent bidirectional encoder and a recurrent decoder, but use no attention component.",1 Introduction,[0],[0]
We replace the attention mechanism by a first-order HMM alignment model.,1 Introduction,[0],[0]
Attention levels are deterministic normalized similarity scores part of the architecture design of an otherwise fully supervised classifier.,1 Introduction,[0],[0]
HMM-style alignments on the other hand are discrete random variables and (unlike attention levels) must be marginalized.,1 Introduction,[0],[0]
"Once alignments are marginalized, which is tractable for a first-order HMM, parameters can be estimated to attain a local optimum of log-likelihood of observations as usual.",1 Introduction,[0],[0]
"In attention-based approaches, the alignment distribution is used to select the positions in the source sentence that the decoder attends to during translation.",2 Motivation,[0],[0]
Thus the alignment model can be considered as an implicit part of the translation model.,2 Motivation,[0],[0]
"On the other hand, separating the alignment model from the lexicon model has its own advantages: First of all, this leads to more flexibility in modeling and training: The models can not only be trained separately, but they can also have different model types, such as neural models, count-based models, etc.",2 Motivation,[0],[0]
"Second, the separation avoids propagating errors from one model to another.",2 Motivation,[0],[0]
"In attention-based systems, the translation score is based on the alignment distribution, in which errors can be propagated from the alignment part to the translation part.",2 Motivation,[0],[0]
"Third, probabilistic treatment to alignments in NMT typically implies an extended degree of interpretability (e.g. one can inspect posteriors) and control over the model (e.g. one can impose priors over alignments
and lexical distributions).",2 Motivation,[0],[0]
"Given a source sentence fJ1 = f1...fj ...fJ and a target sentence eI1 = e1...ei...eI , where j = bi is the source position aligned to the target position i, we model translation using an alignment model and a lexicon model:
p(eI1|fJ1 ) =",3 Neural Hidden Markov Model,[0],[0]
"∑ bI1 p(eI1, b I 1|fJ1 )",3 Neural Hidden Markov Model,[0],[0]
"(1)
:= ∑ bI1 I∏ i=1",3 Neural Hidden Markov Model,[0],[0]
"p(ei|bi1, ei−10 , f J 1 )︸",3 Neural Hidden Markov Model,[0],[0]
"︷︷ ︸ lexicon model · p(bi|bi−11 , e i−1 0 , f J 1 )︸",3 Neural Hidden Markov Model,[0],[0]
"︷︷ ︸ alignment model
(2)
",3 Neural Hidden Markov Model,[0],[0]
"Instead of predicting the absolute source position bi, we use an alignment model p(∆i|bi−11 , e i−1 0 , f J 1 ) that predicts the jump ∆i = bi − bi−1.",3 Neural Hidden Markov Model,[0],[0]
Wang et al. (2017) applied feedforward neural networks for modeling the lexicon and alignment probabilities.,3 Neural Hidden Markov Model,[0],[0]
"In this work, we would like to model these distributions using recurrent neural networks (RNN).",3 Neural Hidden Markov Model,[0],[0]
RNNs have been shown to outperform feedforward variants in language and translation modeling.,3 Neural Hidden Markov Model,[0],[0]
This is mainly due to that RNN can handle arbitrary input lengths and thus include unbounded context information.,3 Neural Hidden Markov Model,[0],[0]
"Unfortunately, the recurrent hidden layer cannot be easily applied for the neural hidden Markov model, since it will significantly complicate the computation of forward-backward messages when running Baum-Welch.",3 Neural Hidden Markov Model,[0],[0]
"Nevertheless, we can apply long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) structure for source and target words embedding.",3 Neural Hidden Markov Model,[0],[0]
"With this technique we can take the essence of LSTM RNN and do not break any sequential generative model assumptions.
",3 Neural Hidden Markov Model,[0],[0]
"Our models are close in structure to the model proposed in Luong et al. (2015), where we have a component that encodes the source sentence, and another that encodes the target sentence.",3 Neural Hidden Markov Model,[0],[0]
"As shown in Figure 1, we use a source side bidirectional LSTM embedding hj = −→ h j + ←−",3 Neural Hidden Markov Model,[0],[0]
"h j , where −→ h j = LSTM(W, fj , −→ h j−1) and ←− h j = LSTM(V, fj , ←− h j+1), as well as a target side LSTM embedding si−1 = LSTM(U, ei−1, si−2).",3 Neural Hidden Markov Model,[0],[0]
"hj , −→ h j , ←− h j and si−1, si−2 are vectors, W , V and U are weight matrices.",3 Neural Hidden Markov Model,[0],[0]
"Before the non-linear hidden layers, there is a projection layer which
concatenates hj , si−1 and ei−1.",3 Neural Hidden Markov Model,[0],[0]
"Then the neural network-based lexicon model is given by
p(ei|bi1, ei−10 , f J 1 ) := p(ei|hj , si−1, ei−1) (3)
and the neural network-based alignment model
p(bi|bi−11 , e i−1 0 , f J 1 ) := p(∆i|hj′ , si−1, ei−1) (4)
where j′ = bi−1.",3 Neural Hidden Markov Model,[0],[0]
"The training criterion is the logarithm of sentence posterior probabilities over training sentence pairs (Fr, Er), r = 1, ..., R:
arg max θ {∑ r log pθ(Er|Fr) } (5)
The derivative for a single sentence pair (F,E) =",3 Neural Hidden Markov Model,[0],[0]
"(fJ1 , e I 1) is:
∂
∂θ log pθ(E|F )",3 Neural Hidden Markov Model,[0],[0]
"= ∑ j′,j ∑ i pi(j ′, j|fJ1 , eI1; θ)
· ∂ ∂θ log p(j, ei|j′, ei−10 , f J 1 ; θ)
(6)
with HMM posterior weights pi(j′, j|fJ1 , eI1; θ), which can be computed using the forwardbackward algorithm.
",3 Neural Hidden Markov Model,[0],[0]
"The entire training procedure can be summarized as backpropagation in an EM framework:
",3 Neural Hidden Markov Model,[0],[0]
"1. compute: • the posterior HMM weights • the local gradients (backpropagation)
2. update neural network weights",3 Neural Hidden Markov Model,[0],[0]
"In the decoding stage we still calculate the sum over alignments and apply a target-synchronous beam search for the target string.
",4 Decoding,[0],[0]
"The auxiliary quantity for each unknown partial string ei0 is specified as Q(i, j; e i 0).",4 Decoding,[0],[0]
"During search, the partial hypothesis is extended from ei−10 to e i 0:
Q(i, j; ei0) = ∑ j′",4 Decoding,[0],[0]
"[ p(j, ei|j′, ei−10 , f J 1 ) ·Q(i− 1, j′; ei−10 ) ]",4 Decoding,[0],[0]
"(7)
The decoder is shown in Algorithm 1.",4 Decoding,[0],[0]
"In the innermost loop (line 11-13), alignments are hypothesized and used to calculate the auxiliary quantity Q(i, j; ei0).",4 Decoding,[0],[0]
"Then for each source position j, the lexical distribution over the full target vocabulary is computed (line 14).",4 Decoding,[0],[0]
"The distributions are accumulated (Q(i; ei0) = ∑ j Q(i, j; e i 0), line 16), then sorted (line 18) and the best candidate translations (arg maxei Q(i; e i 0)) lying within the beam are used to expand the partial hypotheses (line 19-23).",4 Decoding,[0],[0]
"cache is a two-dimensional list of size J × |Vsrc| (source vocabulary size), which is used to cache the current quantities.
",4 Decoding,[0],[0]
"Whenever a partial hypothesis in the beam ends with the sentence end symbol (<EOF>), the counter will be increased by 1 (line 26-28).",4 Decoding,[0],[0]
The translation is terminated if the counter reaches the beam size or hypothesis sentence length reaches three times the source sentence length (line 6).,4 Decoding,[0],[0]
"If a hypothesis stops but its score is worse than other hypotheses, it is eliminated from the beam, but it still contests non-terminated hypotheses.",4 Decoding,[0],[0]
During comparison the scores are normalized by hypothesis sentence length.,4 Decoding,[0],[0]
Note that we have no explicit coverage constraints.,4 Decoding,[0],[0]
"This means that a source position can be revisited many times, whereby creating one-to-many alignment cases.",4 Decoding,[0],[0]
"This also allows unaligned source words.
",4 Decoding,[0],[0]
"In the neural HMM decoder, word alignments are estimated and scored according to the distribution calculated by the neural network alignment model, leading alignment decisions to become part of the beam search.",4 Decoding,[0],[0]
The search space consists of both alignment and translation decisions.,4 Decoding,[0],[0]
"In contrast, the search space in attentionbased decoding consists only of translation decisions.
",4 Decoding,[0],[0]
"The decoding complexity is O(J2 · I) (J = source sentence length, I = target sentence length)
",4 Decoding,[0],[0]
"Algorithm 1 Neural HMM Decoder
1: function TRANSLATE(fJ1 , beam size) 2: count = 0 3: i = 1 4: hyps = {e0} 5: new hyps = {} 6: while count < beam size and i < 3 · J do 7: for hyp in hyps do 8: sum dist =",4 Decoding,[0],[0]
"[0] ∗ |V |src 9: for j from 1 to J do
10: sum = 0 11: for j′ from 1 to J do 12: sum = sum + SCORES(hyp, j′) ·palign(fj′ , j − j′) 13: end for 14: cache[j] = sum · lex dist(fj) 15:",4 Decoding,[0],[0]
"#Element wise addition 16: sum dist = sum dist⊕ cache[j] 17: end for 18: dist = SORT(sum dist, beam size) 19: for word in dist[:beam size] do 20: new hyp = EXTEND(hyp, word) 21: SETSCORES(new hyp, cache) 22: new hyps.",4 Decoding,[0],[0]
"INSERT(new hyp) 23: end for 24: end for 25: PRUNE(new hyps, beam size) 26: for <EOF> in new hyps do 27: count = count + 1 28: end for 29: hyps = new hyps 30: i = i+ 1 31: end while 32: return GETBEST(hyps) 33: end function
compared to O(J · I) for attention-based models.",4 Decoding,[0],[0]
These are theoretical complexities of decoding on a CPU only considering source and target sentence lengths.,4 Decoding,[0],[0]
"In practice, the size of the neural network must also be taken into account, and there are some optimized matrix multiplications for decoding on a GPU.",4 Decoding,[0],[0]
"In general, the decoding speed of our model is about 3 times slower than that of a standard attention model (1.07 sentences per second vs. 3.00 sentences per second) on a single GPU.",4 Decoding,[0],[0]
This is still an initial decoder and we did not spend much time on accelerating its decoding yet.,4 Decoding,[0],[0]
The optimization of our decoder would be a promising future work.,4 Decoding,[0],[0]
"The experiments are conducted on the WMT 2017 German↔English and Chinese→English translation tasks, which consist of 5M and 23M parallel sentence pairs respectively.",5 Experiments,[0],[0]
"Translation quality is measured with the case sensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metric on newstests 2017, which contain 3004 (German↔English) and 2001 (Chinese→English) sentence pairs.
",5 Experiments,[0],[0]
"For German and English preprocessing, we use the Moses tokenizer with hyphen splitting, and perform truecasing with Moses scripts (Koehn et al., 2007).",5 Experiments,[0],[0]
"For German↔English subword segmentation (Sennrich et al., 2016), we use 20K joint BPE operations.",5 Experiments,[0],[0]
"For the Chinese data, we segment it using the Jieba1 segmenter.",5 Experiments,[0],[0]
"We then learn a BPE model on the segmented Chinese, also using 20K merge operations.",5 Experiments,[0],[0]
"During training, sentences with a length greater than 50 subwords are filtered out.",5 Experiments,[0],[0]
"The attention-based systems are trained with Sockeye (Hieber et al., 2017), which implement an attentional encoder-decoder with small modifications to the model in Bahdanau et al. (2015).",5.1 Attention-Based System,[0],[0]
The encoder and decoder word embeddings are of size 620.,5.1 Attention-Based System,[0],[0]
The encoder consists of a bidirectional layer with 1000 LSTMs with peephole connections to encode the source side.,5.1 Attention-Based System,[0],[0]
"We use Adam (Kingma and Ba, 2015) as optimizer with a learning rate of 0.001, and a batch size of 50.",5.1 Attention-Based System,[0],[0]
The network is trained with 30% dropout for up to 500K iterations and evaluated every 10K iterations on the development set with BLEU.,5.1 Attention-Based System,[0],[0]
Decoding is done using beam search with a beam size of 12.,5.1 Attention-Based System,[0],[0]
"In the end the four best models are averaged as described in
1https://github.com/fxsjy/jieba
the beginning of Junczys-Dowmunt et al. (2016).",5.1 Attention-Based System,[0],[0]
"The entire neural hidden Markov model is implemented in TensorFlow (Abadi et al., 2016).",5.2 Neural Hidden Markov Model,[0],[0]
"The feedforward models have three hidden layers of sizes 1000, 1000 and 500 respectively, with a 5- word source window and a 3-gram target history.",5.2 Neural Hidden Markov Model,[0],[0]
"200 nodes are used for word embeddings.
",5.2 Neural Hidden Markov Model,[0],[0]
"The output layer of the neural lexicon model consists of around 25K nodes for all subword units, while the neural alignment model has a small output layer with 201 nodes, which reflects that the aligned position can jump within the scope from −100 to 100.
",5.2 Neural Hidden Markov Model,[0],[0]
"Apart from the basic projection layer, we also applied LSTM layers for the source and target words embedding.",5.2 Neural Hidden Markov Model,[0],[0]
"The embedding layers have 350 nodes and the size of the projection layer is 800 (400 + 200 + 200, Figure 1).",5.2 Neural Hidden Markov Model,[0],[0]
We use Adam as optimizer with a learning rate of 0.001.,5.2 Neural Hidden Markov Model,[0],[0]
"Neural lexicon and alignment models are trained with 30% dropout and the norm of the gradient is clipped with a threshold 1 (Pascanu et al., 2014).",5.2 Neural Hidden Markov Model,[0],[0]
In decoding we use a beam size of 12 and the element-wise average of all weights of the four best models also results in better performance.,5.2 Neural Hidden Markov Model,[0],[0]
We compare the neural HMM approach (Subsection 5.2) with the state-of-the-art attention-based approach (Subsection 5.1) on different translation tasks.,5.3 Results,[0],[0]
The results are presented in Table 1.,5.3 Results,[0],[0]
"Compare to the model presented in Wang et al. (2017), switching to LSTM models has a clear advantage, which improves the FFNN-based system by up to 1.3% BLEU and 1.8% TER.",5.3 Results,[0],[0]
"It seems that the HMM model benefits from richer features, such as LSTM states, which are very similar to what an attention mechanism would require.",5.3 Results,[0],[0]
"We actually
expected it to do with less, the reason being that alignment distributions get refined a posteriori",5.3 Results,[0],[0]
and so they do not have to be as strong a priori.,5.3 Results,[0],[0]
We can also observe that the performance of our approach is comparable with the state-of-the-art attentionbased system with 25M more parameters on all three tasks.,5.3 Results,[0],[0]
"We show an example from the German→English newstest 2017 in Figure 2, along with the attention and alignment matrices.",5.4 Alignment Analysis,[0],[0]
We can observe that the neural network-based HMM could generate a more clear alignment path compared to the attention weights.,5.4 Alignment Analysis,[0],[0]
"In this example, it can exactly estimate the alignment positions for words wanted and of.",5.4 Alignment Analysis,[0],[0]
"We described a novel formulation for a neural network-based machine translation system, which applied neural networks to the conventional hidden Markov model.",6 Discussion,[0],[0]
"The training is end-to-end, the model is monolithic and can be used as a standalone decoder.",6 Discussion,[0],[0]
"This results in a more modern and efficient way to use HMM in machine translation and enables neural networks to benefit from HMMs.
",6 Discussion,[0],[0]
Experiments show that replacing attention with alignment does not improve the translation performance of NMT significantly.,6 Discussion,[0],[0]
One possible reason is that alignment may fail to capture relevant contexts as attention does.,6 Discussion,[0],[0]
"While alignment aims to identify translation equivalents between two lan-
guages, attention is designed to find relevant context for predicting the next target word.",6 Discussion,[0],[0]
Source words with high attention weights are not necessarily translation equivalents of the target word.,6 Discussion,[0],[0]
"Although using alignment does not lead to significant improvements in terms of BLEU over attention, we think alignment-based NMT models are still useful for automatic post editing and developing coverage-based models.",6 Discussion,[0],[0]
These might be interesting future directions to explore.,6 Discussion,[0],[0]
"This project has received funding from the European Union’s Horizon 2020 research and innovation programme under
grant agreement no 645452 (QT21), and from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme, grant agreement no 694537 (SEQCLAS).",Acknowledgments,[0],[0]
The work reflects only the authors’ views and neither the European Research Council Executive Agency nor the European Commission is responsible for any use that may be made of the information it contains.,Acknowledgments,[0],[0]
The GPU cluster used for the experiments was partially funded by Deutsche Forschungsgemeinschaft (DFG) Grant INST 222/1168-1.,Acknowledgments,[0],[0]
"Tamer Alkhouli was partly funded by the 2016 Google PhD Fellowship for North America, Europe and the Middle East.",Acknowledgments,[0],[0]
This work aims to investigate alternative neural machine translation (NMT) approaches and thus proposes a neural hidden Markov model (HMM) consisting of neural network-based alignment and lexicon models.,abstractText,[0],[0]
"The neural models make use of encoder and decoder components, but drop the attention component.",abstractText,[0],[0]
The training is end-to-end and the standalone decoder is able to provide comparable performance with the state-of-the-art attention-based models on three different translation tasks.,abstractText,[0],[0]
Neural Hidden Markov Model for Machine Translation,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1204–1214 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1111",text,[0],[0]
"Dependency parsers have been enhanced by the use of neural networks and embedding vectors (Chen and Manning, 2014; Weiss et al., 2015; Zhou et al., 2015; Alberti et al., 2015; Andor et al., 2016; Dyer et al., 2015).",1 Introduction,[0],[0]
"When these dependency parsers process sentences in English and other languages that use symbols for word separations, they can be very accurate.",1 Introduction,[0],[0]
"However, for languages that do not contain word separation symbols, dependency parsers are used in pipeline processes with word segmentation and POS tagging models, and encounter serious problems because of error propagations.",1 Introduction,[0],[0]
"In particular, Chinese word segmentation is notoriously difficult because sentences are written without word dividers and Chinese words are not clearly defined.",1 Introduction,[0],[0]
"Hence, the pipeline of word segmentation, POS tagging and dependency parsing always suffers from word seg-
mentation errors.",1 Introduction,[0],[0]
"Once words have been wronglysegmented, word embeddings and traditional onehot word features, used in dependency parsers, will mistake the precise meanings of the original sentences.",1 Introduction,[0],[0]
"As a result, pipeline models achieve dependency scores of around 80% for Chinese.
",1 Introduction,[0],[0]
A traditional solution to this error propagation problem is to use joint models.,1 Introduction,[0],[0]
Many Chinese words play multiple grammatical roles with only one grammatical form.,1 Introduction,[0],[0]
"Therefore, determining the word boundaries and the subsequent tagging and dependency parsing are closely correlated.",1 Introduction,[0],[0]
"Transition-based joint models for Chinese word segmentation, POS tagging and dependency parsing are proposed by Hatori et al. (2012) and Zhang et al. (2014).",1 Introduction,[0],[0]
"Hatori et al. (2012) state that dependency information improves the performances of word segmentation and POS tagging, and develop the first transition-based joint word segmentation, POS tagging and dependency parsing model.",1 Introduction,[0],[0]
"Zhang et al. (2014) expand this and find that both the inter-word dependencies and intraword dependencies are helpful in word segmentation and POS tagging.
",1 Introduction,[0],[0]
"Although the models of Hatori et al. (2012) and Zhang et al. (2014) perform better than pipeline models, they rely on the one-hot representation of characters and words, and do not assume the similarities among characters and words.",1 Introduction,[0],[0]
"In addition, not only words and characters but also many incomplete tokens appear in the transitionbased joint parsing process.",1 Introduction,[0],[0]
"Such incomplete or unknown words (UNK) could become important cues for parsing, but they are not listed in dictionaries or pre-trained word embeddings.",1 Introduction,[0],[0]
"Some recent studies show that character-based embeddings are effective in neural parsing (Ballesteros et al., 2015; Zheng et al., 2015), but their models could not be directly applied to joint models because they use given word segmentations.",1 Introduction,[0],[0]
"To solve
1204
these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing.",1 Introduction,[0],[0]
"We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens.
",1 Introduction,[0],[0]
Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering.,1 Introduction,[0],[0]
"Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016).",1 Introduction,[0],[0]
"In their models, the bi-LSTM is used to represent the tokens including their context.",1 Introduction,[0],[0]
"Indeed, such neural networks can observe whole sentence through the bi-LSTM.",1 Introduction,[0],[0]
This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014).,1 Introduction,[0],[0]
"As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models.",1 Introduction,[0],[0]
"We also develop joint models with ngram character string bi-LSTM.
",1 Introduction,[0],[0]
"In the experiments, we obtain state-of-the-art Chinese word segmentation and POS tagging scores, and the pipeline of the dependency model achieves the better dependency scores than the previous joint models.",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first model to use embeddings and neural networks for Chinese full joint parsing.
",1 Introduction,[0],[0]
"Our contributions are summarized as follows: (1) we propose the first embedding-based fully joint parsing model, (2) we use character string embeddings for UNK and incomplete tokens.",1 Introduction,[0],[0]
(3) we also explore bi-LSTM models to avoid the detailed feature engineering in previous approaches.,1 Introduction,[0],[0]
"(4) in experiments using Chinese corpus, we achieve state-of-the-art scores in word segmentation, POS tagging and dependency parsing.",1 Introduction,[0],[0]
All full joint parsing models we present in this paper use the transition-based algorithm in Section 2.1 and the embeddings of character strings in Section 2.2.,2 Model,[0],[0]
We present two neural networks: the feed-forward neural network models in Section 2.3 and the bi-LSTM models in Section 2.4.,2 Model,[0],[0]
"Based on Hatori et al. (2012), we use a modified arc-standard algorithm for character transi-
技术有了新的进展。
Technology have made new progress.
tions (Figure 1).","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
The model consists of one buffer and one stack.,"2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"The buffer contains characters in the input sentence, and the stack contains words shifted from the buffer.","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
The stack words may have their child nodes.,"2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"The words in the stack are formed by the following transition operations.
","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"• SH(t) (shift): Shift the first character of the buffer to the top of the stack as a new word.
","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"• AP (append): Append the first character of the buffer to the end of the top word of the stack.
","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"• RR (reduce-right): Reduce the right word of the top two words of the stack, and make the right child node of the left word.
","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"• RL (reduce-left): Reduce the left word of the top two words of the stack, and make the left child node of the right word.
","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"The RR and RL operations are the same as those of the arc-standard algorithm (Nivre, 2004a).","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
SH makes a new word whereas AP makes the current word longer by adding one character.,"2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"The POS tags are attached with the SH(t) transition.
","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"In this paper, we explore both greedy models and beam decoding models.","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
This parsing algorithm works in both types.,"2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"We also develop a joint model of word segmentation and POS tagging, along with a dependency parsing model.","2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
The joint model of word segmentation and POS tagging does not have RR and RL transitions.,"2.1 Transition-based Algorithm for Joint Segmentation, POS Tagging, and Dependency Parsing",[0],[0]
"First, we explain the embeddings used in the neural networks.",2.2 Embeddings of Character Strings,[0],[0]
"Later, we explain details of the neural networks in Section 2.3 and 2.4.
",2.2 Embeddings of Character Strings,[0],[0]
Both meaningful words and incomplete tokens appear during transition-based joint parsing.,2.2 Embeddings of Character Strings,[0],[0]
"Although embeddings of incomplete tokens are not used in previous work, they could become useful features in several cases.",2.2 Embeddings of Character Strings,[0],[0]
"For example, “南京 东路” (Nanjing East Road, the famous shopping street of Shanghai) is treated as a single Chinese word in the Penn Chinese Treebank (CTB) corpus.",2.2 Embeddings of Character Strings,[0],[0]
"There are other named entities of this form in CTB, e.g, “北京西路” (Beijing West Road) and “湘西路” (Hunan West Road).",2.2 Embeddings of Character Strings,[0],[0]
"In these cases, “南京” (Nanjing) and “北京” (Beijing) are location words, while “东路” (East Road) and “西 路” (West Road) are sub-words.",2.2 Embeddings of Character Strings,[0],[0]
"“东路” and “西 路” are similar in terms of their character composition and usage, which is not sufficiently considered in the previous work.",2.2 Embeddings of Character Strings,[0],[0]
"Moreover, representations of incomplete tokens are helpful for compensating the segmentation ambiguity.",2.2 Embeddings of Character Strings,[0],[0]
Suppose that the parser makes over-segmentation errors and segments “南京东路” to “南京” and “东 路”.,2.2 Embeddings of Character Strings,[0],[0]
"In this case, “东路” becomes UNK.",2.2 Embeddings of Character Strings,[0],[0]
"However, the models could infer that “东路” is also a location, from its character composition and neighboring words.",2.2 Embeddings of Character Strings,[0],[0]
This could give models robustness of segmentation errors.,2.2 Embeddings of Character Strings,[0],[0]
"In our models, we prepare the word and character embeddings in the pretraining.",2.2 Embeddings of Character Strings,[0],[0]
"We also use the embeddings of character strings for sub-words and UNK which are not in the pre-trained embeddings.
",2.2 Embeddings of Character Strings,[0],[0]
The characters and words are embedded in the same vector space during pre-training.,2.2 Embeddings of Character Strings,[0],[0]
We prepare the same training corpus with the segmented word files and the segmented character files.,2.2 Embeddings of Character Strings,[0],[0]
"Both files are concatenated and learned by word2vec (Mikolov et al., 2013).",2.2 Embeddings of Character Strings,[0],[0]
We use the embeddings of 1M frequent words and characters.,2.2 Embeddings of Character Strings,[0],[0]
Words and characters that are in the training set and do not have pre-trained embeddings are given randomly initialized embeddings.,2.2 Embeddings of Character Strings,[0],[0]
"The development set and the test set have out-of-vocabulary (OOV) tokens for these embeddings.
",2.2 Embeddings of Character Strings,[0],[0]
The embeddings of the unknown character strings are generated in the neural computation graph when they are required.,2.2 Embeddings of Character Strings,[0],[0]
Consider a character string c1c2 · · · cn consisting of characters ci.,2.2 Embeddings of Character Strings,[0],[0]
"When this character string is not in the pretrained embeddings, the model obtains the embeddings v(c1c2 · · · cn) by the mean of each character embeddings ∑n i=1 v(ci).",2.2 Embeddings of Character Strings,[0],[0]
"Embeddings of words, characters and character strings have the same di-
mension and are chosen in the neural computation graph.",2.2 Embeddings of Character Strings,[0],[0]
"We avoid using the “UNK” vector as far as possible, because this degenerates the information about unknown tokens.",2.2 Embeddings of Character Strings,[0],[0]
"However, models use the “UNK” vector if the parser encounters characters that are not in the pre-trained embeddings, though this is quite uncommon.",2.2 Embeddings of Character Strings,[0],[0]
We present a feed-forward neural network model in Figure 2.,2.3.1 Neural Network,[0],[0]
The neural network for greedy training is based on the neural networks of Chen and Manning (2014) and Weiss et al. (2015).,2.3.1 Neural Network,[0],[0]
"We add the dynamic generation of the embeddings of character strings for unknown tokens, as described in Section 2.2.",2.3.1 Neural Network,[0],[0]
"This neural network has two hidden layers with 8,000 dimensions.",2.3.1 Neural Network,[0],[0]
"This is larger than Chen and Manning (2014) (200 dimensions) or Weiss et al. (2015) (1,024 or 2,048 dimensions).",2.3.1 Neural Network,[0],[0]
"We use the ReLU for the activation function of the hidden layers (Nair and Hinton, 2010) and the softmax function for the output layer of the greedy
neural network.",2.3.1 Neural Network,[0],[0]
There are three randomly initialized weight matrices between the embedding layers and the softmax function.,2.3.1 Neural Network,[0],[0]
"The loss function L(θ) for the greedy training is
L(θ) =",2.3.1 Neural Network,[0],[0]
"− ∑
s,t
log pgreedys,t + λ
2 ||θ||2,
pgreedys,t (β) ∝",2.3.1 Neural Network,[0],[0]
"exp
 ∑
j
wtjβj + bt
  ,
where t denotes one transition among the transition set T ( t ∈ T ).",2.3.1 Neural Network,[0],[0]
s denotes one element of the single mini-batch.,2.3.1 Neural Network,[0],[0]
β denotes the output of the previous layer.,2.3.1 Neural Network,[0],[0]
w and b denote the weight matrix and the bias term.,2.3.1 Neural Network,[0],[0]
θ contains all parameters.,2.3.1 Neural Network,[0],[0]
We use the L2 penalty term and the Dropout.,2.3.1 Neural Network,[0],[0]
The backprop is performed including the word and character embeddings.,2.3.1 Neural Network,[0],[0]
"We use Adagrad (Duchi et al., 2010) to optimize learning rate.",2.3.1 Neural Network,[0],[0]
"We also consider Adam (Kingma and Ba, 2015) and SGD, but find that Adagrad performs better in this model.",2.3.1 Neural Network,[0],[0]
"The other learning parameters are summarized in Table 1.
",2.3.1 Neural Network,[0],[0]
"In our model implementation, we divide all sentences into training batches.",2.3.1 Neural Network,[0],[0]
Sentences in the same training batches are simultaneously processed by the neural mini-batches.,2.3.1 Neural Network,[0],[0]
"By doing so, the model can parse all sentences of the training batch in the number of transitions required to parse the longest sentence in the batch.",2.3.1 Neural Network,[0],[0]
"This allows the model to parse more sentences at once, as long as the neural mini-batch can be allocated to the GPU memory.",2.3.1 Neural Network,[0],[0]
This can be applied to beam decoding.,2.3.1 Neural Network,[0],[0]
The features of this neural network are listed in Table 2.,2.3.2 Features,[0],[0]
"We use three kinds of features: (1) features obtained from Hatori et al. (2012) by removing combinations of features, (2) features obtained from Chen and Manning (2014), (3) original features related to character strings.",2.3.2 Features,[0],[0]
"In particular,
Type Features
Stack word and tags s0w, s1w, s2w s0p, s1p, s2p Stack 1 children and tags s0l0w, s0r0w, s0l1w, s0r1w s0l0p, s0r0p, s0l1p, s0r1p Stack 2 children s1l0w, s1r0w, s1l1w, s1r1w",2.3.2 Features,[0],[0]
"Children of children s0l0lw, s0r0rw, s1l0lw, s1r0rw Buffer characters b0c, b1c, b2c, b3c Previously shifted words q0w, q1w Previously shifted tags q0p, q1p Character of q0 q0e",2.3.2 Features,[0],[0]
"Parts of q0 word q0f1, q0f2, q0f3 Strings across q0 and buf. q0b1, q0b2, q0b3 Strings of buffer characters b0-2, b0-3, b0-4
the original features include sub-words, character strings across the buffer and the stack, and character strings in the buffer.",2.3.2 Features,[0],[0]
Character strings across the buffer and stack could capture the currentlysegmented word.,2.3.2 Features,[0],[0]
"To avoid using character strings that are too long, we restrict the length of character string to a maximum of four characters.",2.3.2 Features,[0],[0]
"Unlike Hatori et al. (2012), we use sequential characters of sentences for features, and avoid handengineered combinations among one-hot features, because such combinations could be automatically generated in the neural hidden layers as distributed representations (Hinton et al., 1986).
",2.3.2 Features,[0],[0]
"In the later section, we evaluate a joint model for word segmentation and POS tagging.",2.3.2 Features,[0],[0]
This model does not use the children and children-ofchildren of stack words as features.,2.3.2 Features,[0],[0]
"Structured learning plays an important role in previous joint parsing models for Chinese.1 In this paper, we use the structured learning model proposed by Weiss et al. (2015) and Andor et al. (2016).
",2.3.3 Beam Search,[0],[0]
"In Figure 2, the output layer for the beam decoding is at the top of the network.",2.3.3 Beam Search,[0],[0]
There are a perceptron layer which has inputs from the two hidden layers and the greedy output layer:,2.3.3 Beam Search,[0],[0]
"[h1,h2,pgreedy(y)].",2.3.3 Beam Search,[0],[0]
"This layer is learned by the following cost function (Andor et al., 2016):
L(d∗1:j ; θ) =",2.3.3 Beam Search,[0],[0]
"− j∑
i=1
ρ(d∗1:i−1, d ∗",2.3.3 Beam Search,[0],[0]
"i ; θ)
+ ln ∑
d′1:",2.3.3 Beam Search,[0],[0]
"j∈B1:j exp
j∑
i=1
ρ(d′1:i−1, d ′",2.3.3 Beam Search,[0],[0]
"i; θ),
where d1:j denotes the transition path and d∗1:j denotes the gold transition path.",2.3.3 Beam Search,[0],[0]
B1:j is the set of transition paths from 1 to j step in beam.,2.3.3 Beam Search,[0],[0]
ρ is the value of the top layer in Figure 2.,2.3.3 Beam Search,[0],[0]
This training can be applied throughout the network.,2.3.3 Beam Search,[0],[0]
"However, we separately train the last beam layer and the previous greedy network in practice, as in Andor et al. (2016).",2.3.3 Beam Search,[0],[0]
"First, we train the last perceptron layer using the beam cost function freezing the previous greedy-trained layers.",2.3.3 Beam Search,[0],[0]
"After the last layer has been well trained, backprop is performed including the previous layers.",2.3.3 Beam Search,[0],[0]
"We notice that training the embedding layer at this stage could make the results worse, and thus we exclude it.",2.3.3 Beam Search,[0],[0]
Note that this whole network backprop requires considerable GPU memory.,2.3.3 Beam Search,[0],[0]
"Hence, we exclude particularly large batches from the training, because they cannot be on GPU memory.",2.3.3 Beam Search,[0],[0]
We use multiple beam sizes for training because models can be trained faster with small beam sizes.,2.3.3 Beam Search,[0],[0]
"After the small beam size training, we use larger beam sizes.",2.3.3 Beam Search,[0],[0]
"The test of this fully joint model takes place with a beam size of 16.
",2.3.3 Beam Search,[0],[0]
Hatori et al. (2012) use special alignment steps in beam decoding.,2.3.3 Beam Search,[0],[0]
"The AP transition has size-2 steps, whereas the other transitions have a size-1 step.",2.3.3 Beam Search,[0],[0]
"Using this alignment, the total number of steps for an N -character sentence is guaranteed to be 2N − 1 (excluding the root arc) for any transition path.",2.3.3 Beam Search,[0],[0]
"This can be interpreted as the AP transition doing two things: appending characters and
1Hatori et al. (2012) report that structured learning with a beam size of 64 is optimal.
resolving intra-word dependencies.",2.3.3 Beam Search,[0],[0]
This alignment stepping assumes that the intra-word dependencies of characters to the right of the characters exist in each Chinese word.,2.3.3 Beam Search,[0],[0]
"In Section 2.3, we describe a neural network model with feature extraction.",2.4 Bi-LSTM Model,[0],[0]
"Unfortunately, although this model is fast and very accurate, it has two problems: (1) the neural network cannot see the whole sentence information.",2.4 Bi-LSTM Model,[0],[0]
(2) it relies on feature engineering.,2.4 Bi-LSTM Model,[0],[0]
"To solve these problems, Kiperwasser and Goldberg (2016) propose a bi-LSTM neural network parsing model.",2.4 Bi-LSTM Model,[0],[0]
"Surprisingly, their model uses very few features, and bi-LSTM is applied to represent the context of the features.",2.4 Bi-LSTM Model,[0],[0]
"Their neural network consists of three parts: bi-LSTM, a feature extraction function and a multilayer perceptron (MLP).",2.4 Bi-LSTM Model,[0],[0]
"First, all tokens in the sentences are converted to embeddings.",2.4 Bi-LSTM Model,[0],[0]
"Second, the bi-LSTM reads all embeddings of the sentence.",2.4 Bi-LSTM Model,[0],[0]
"Third, the feature function extracts the feature representations of tokens from the bi-LSTM layer.",2.4 Bi-LSTM Model,[0],[0]
"Finally, an MLP with one hidden layer outputs the transition scores of the transition-based parser.
",2.4 Bi-LSTM Model,[0],[0]
"In this paper, we propose a Chinese joint parsing model with simple and global features using n-gram bi-LSTM and a simple feature extraction function.",2.4 Bi-LSTM Model,[0],[0]
The model is described in Figure 3.,2.4 Bi-LSTM Model,[0],[0]
"We consider that Chinese sentences consist of tokens, including words, UNKs and incomplete tokens, which can have some meanings and are useful for parsing.",2.4 Bi-LSTM Model,[0],[0]
Such tokens appear in many parts of the sentence and have arbitrary lengths.,2.4 Bi-LSTM Model,[0],[0]
"To capture them, we propose the n-gram bi-LSTM.",2.4 Bi-LSTM Model,[0],[0]
The n-gram bi-LSTM read through characters ci · · · ci+n−1 of the sentence (ci is the i-th character).,2.4 Bi-LSTM Model,[0],[0]
"For example, the 1-gram bi-LSTM reads each character, and the 2-gram bi-LSTM reads two consecutive characters cici+1.",2.4 Bi-LSTM Model,[0],[0]
"After the n-gram forward LSTM reads character string ci · · · ci+n−1, it next reads ci+1 · · · ci+n.",2.4 Bi-LSTM Model,[0],[0]
The backward LSTM reads from ci+1 · · · ci+n toward ci · · · ci+n−1.,2.4 Bi-LSTM Model,[0],[0]
"This allows models to capture any n-gram character strings in the input sentence.2 All n-gram inputs to bi-LSTM are given by the embeddings of words and characters or the dynamically generated embeddings of character strings, as described in
2At the end of the sentence of length N , character strings ci · · · cN",2.4 Bi-LSTM Model,[0],[0]
"(N < i+n−1), which are shorter than n characters, are used.
",2.4 Bi-LSTM Model,[0],[0]
Section 2.2.,2.4 Bi-LSTM Model,[0],[0]
"Although these arbitrary n-gram tokens produce UNKs, character string embeddings can capture similarities among them.",2.4 Bi-LSTM Model,[0],[0]
"Following the bi-LSTM layer, the feature function extracts the corresponding outputs of the bi-LSTM layer.",2.4 Bi-LSTM Model,[0],[0]
We summarize the features in Table 3.,2.4 Bi-LSTM Model,[0],[0]
"Finally, MLP and the softmax function outputs the transition probability.",2.4 Bi-LSTM Model,[0],[0]
We use an MLP with three hidden layers as for the model in Section 2.3.,2.4 Bi-LSTM Model,[0],[0]
We train this neural network with the loss function for the greedy training.,2.4 Bi-LSTM Model,[0],[0]
"We use the Penn Chinese Treebank 5.1 (CTB5) and 7 (CTB-7) datasets to evaluate our models, following the splitting of Jiang et al. (2008) for CTB-5 and Wang et al. (2011) for CTB-7.",3.1 Experimental Settings,[0],[0]
The statistics of datasets are presented in Table 4.,3.1 Experimental Settings,[0],[0]
We use the Chinese Gigaword Corpus for embedding pre-training.,3.1 Experimental Settings,[0],[0]
Our model is developed for unlabeled dependencies.,3.1 Experimental Settings,[0],[0]
The development set is used for parameter tuning.,3.1 Experimental Settings,[0],[0]
"Following Hatori et al. (2012) and Zhang et al. (2014), we use the standard word-level evaluation with F1-measure.",3.1 Experimental Settings,[0],[0]
"The POS tags and dependencies cannot be correct unless the corresponding words are correctly segmented.
",3.1 Experimental Settings,[0],[0]
"We trained three models: SegTag, SegTagDep and Dep.",3.1 Experimental Settings,[0],[0]
SegTag is the joint word segmentation and POS tagging model.,3.1 Experimental Settings,[0],[0]
"SegTagDep is the full joint segmentation, tagging and dependency parsing model.",3.1 Experimental Settings,[0],[0]
"Dep is the dependency parsing model which is similar to Weiss et al. (2015) and Andor et al. (2016), but uses the embeddings of character strings.",3.1 Experimental Settings,[0],[0]
Dep compensates for UNKs and segmentation errors caused by previous word segmentation using embeddings of character strings.,3.1 Experimental Settings,[0],[0]
"We will examine this effect later.
",3.1 Experimental Settings,[0],[0]
"Most experiments are conducted on GPUs, but some of beam decoding processes are performed on CPUs because of the large mini-batch size.",3.1 Experimental Settings,[0],[0]
The neural network is implemented with Theano.,3.1 Experimental Settings,[0],[0]
"First, we evaluate the joint segmentation and POS tagging model (SegTag).",3.2.1 Joint Segmentation and POS Tagging,[0],[0]
Table 5 compares the performance of segmentation and POS tagging using the CTB-5 dataset.,3.2.1 Joint Segmentation and POS Tagging,[0],[0]
We train two modles: a greedy-trained model and a model trained with beams of size 4.,3.2.1 Joint Segmentation and POS Tagging,[0],[0]
"We compare our model to three previous approaches: Hatori et al. (2012), Zhang et al. (2014) and Zhang et al. (2015).",3.2.1 Joint Segmentation and POS Tagging,[0],[0]
"Our SegTag joint model is superior to these previous models, including Hatori et al. (2012)’s model with rich dictionary information, in terms of both segmentation and POS tagging accuracy.",3.2.1 Joint Segmentation and POS Tagging,[0],[0]
Table 6 presents the results of our full joint model.,"3.2.2 Joint Segmentation, POS Tagging and Dependency Parsing",[0],[0]
We employ the greedy trained full joint model SegTagDep(g) and the beam decoding model SegTagDep.,"3.2.2 Joint Segmentation, POS Tagging and Dependency Parsing",[0],[0]
All scores for the existing models in this table are taken from Zhang et al. (2014).,"3.2.2 Joint Segmentation, POS Tagging and Dependency Parsing",[0],[0]
"Though our model surpasses the previous best end-to-end joint models in terms of segmentation and POS tagging, the dependency score is slightly lower than the previous models.","3.2.2 Joint Segmentation, POS Tagging and Dependency Parsing",[0],[0]
"The greedy model SegTagDep(g) achieves slightly lower scores than beam models, although this model works considerably fast because it does not use beam decoding.","3.2.2 Joint Segmentation, POS Tagging and Dependency Parsing",[0],[0]
We use our joint SegTag model for the pipeline input of the Dep model (SegTag+Dep).,3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
Both SegTag and Dep models are trained and tested by the beam cost function with beams of size 4.,3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
Table 7 presents the results.,3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
Our SegTag+Dep model performs best in terms of the dependency and word segmentation.,3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
The SegTag+Dep model is better than the full joint model.,3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
This is because most segmentation errors of these models occur around named entities.,3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
"Hatori et al. (2012)’s alignment step assumes the intra-word dependencies in words, while named entities do not always have them.",3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
"For example, SegTag+Dep model treats named entity “海赛克”, a company name, as one word, while the SegTagDep model divides this to “海” (sea) and “赛克”, where “赛克” could be used for foreigner’s name.",3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
"For such words, SegTagDep prefers SH because AP has size-2 step of the character appending and intra-word dependency resolution, which does not exist for named entities.",3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
"This problem could be solved by adding a special transition AP_named_entity which is similar to AP but with size-1 step and used
only for named entities.",3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
"Additionally, Zhang et al. (2014)’s STD (arc-standard) model works slightly better than Hatori et al. (2012)’s fully joint model in terms of the dependency score.",3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
Zhang et al. (2014)’s STD model is similar to our SegTag+Dep because they combine a word segmentator and a dependency parser using “deque” of words.,3.2.3 Pipeline of Our Joint SegTag and Dep Model,[0],[0]
"Finally, we compare the two pipeline models of SegTag+Dep to show the effectiveness of using character string representations instead of “UNK” embeddings.",3.2.4 Effect of Character String Embeddings,[0],[0]
We use two dependency models with greedy training: Dep(g) for dependency model and Dep(g)-cs for dependency model without the character string embeddings .,3.2.4 Effect of Character String Embeddings,[0],[0]
"In the Dep(g)-cs model, we use the “UNK” embedding when the embeddings of the input features are unavailable, whereas we use the character string embeddings in model Dep(g).",3.2.4 Effect of Character String Embeddings,[0],[0]
The results are presented in Table 8.,3.2.4 Effect of Character String Embeddings,[0],[0]
"When the models encounter unknown tokens, using the embeddings of character strings is better than using the “UNK” embedding.",3.2.4 Effect of Character String Embeddings,[0],[0]
We test the effect of special features: q0bX in Table 2.,3.2.5 Effect of Features across the Buffer and Stack,[0],[0]
The q0bX features capture the tokens across the buffer and stack.,3.2.5 Effect of Features across the Buffer and Stack,[0],[0]
Joint transition-based parsing models by Hatori et al. (2012) and Chen and Manning (2014) decide POS tags of words before corresponding word segmentations are determined.,3.2.5 Effect of Features across the Buffer and Stack,[0],[0]
"In our model, the q0bX features capture words even if their segmentations are not determined.",3.2.5 Effect of Features across the Buffer and Stack,[0],[0]
We examine the effectiveness of these features by training greedy full joint models with and without them.,3.2.5 Effect of Features across the Buffer and Stack,[0],[0]
The results are shown in Table 9.,3.2.5 Effect of Features across the Buffer and Stack,[0],[0]
The q0bX features boost not only POS tagging scores but also word segmentation scores.,3.2.5 Effect of Features across the Buffer and Stack,[0],[0]
We also test the SegTagDep and SegTag+Dep models on CTB-7.,3.2.6 CTB-7 Experiments,[0],[0]
"In these experiments, we no-
tice that the MLP with four hidden layers performs better than the MLP with three hidden layers, but we could not find definite differences in the experiments in CTB-5.",3.2.6 CTB-7 Experiments,[0],[0]
We speculate that this is caused by the difference in the training set size.,3.2.6 CTB-7 Experiments,[0],[0]
We present the final results with four hidden layers in Table 10.,3.2.6 CTB-7 Experiments,[0],[0]
We experiment the n-gram bi-LSTMs models with four and eight features listed in Table 3.,3.2.7 Bi-LSTM Model,[0],[0]
We summarize the result in Table 11.,3.2.7 Bi-LSTM Model,[0],[0]
"The greedy biLSTM models perform slightly worse than the previous models, but they do not rely on feature engineering.",3.2.7 Bi-LSTM Model,[0],[0]
Zhang and Clark (2008) propose an incremental joint word segmentation and POS tagging model driven by a single perceptron.,4 Related Work,[0],[0]
Zhang and Clark (2010) improve this model by using both character and word-based decoding.,4 Related Work,[0],[0]
Hatori et al. (2011) propose a transition-based joint POS tagging and dependency parsing model.,4 Related Work,[0],[0]
Zhang et al. (2013) propose a joint model using character structures of words for constituency parsing.,4 Related Work,[0],[0]
Wang et al. (2013) also propose a lattice-based joint model for constituency parsing.,4 Related Work,[0],[0]
"Zhang et al. (2015) propose joint segmentation, POS tagging and dependency re-ranking system.",4 Related Work,[0],[0]
"This system requires
base parsers.",4 Related Work,[0],[0]
"In neural joint models, Zheng et al. (2013) propose a neural network-based Chinese word segmentation model based on tag inferences.",4 Related Work,[0],[0]
They extend their models for joint segmentation and POS tagging.,4 Related Work,[0],[0]
Zhu et al. (2015) propose the re-ranking system of parsing results with recursive convolutional neural network.,4 Related Work,[0],[0]
We propose the joint parsing models by the feedforward and bi-LSTM neural networks.,5 Conclusion,[0],[0]
Both of them use the character string embeddings.,5 Conclusion,[0],[0]
The character string embeddings help to capture the similarities of incomplete tokens.,5 Conclusion,[0],[0]
We also explore the neural network with few features using n-gram bi-LSTMs.,5 Conclusion,[0],[0]
"Our SegTagDep joint model achieves better scores of Chinese word segmentation and POS tagging than previous joint models, and our SegTag and Dep pipeline model achieves state-of-the-art score of dependency parsing.",5 Conclusion,[0],[0]
The bi-LSTM models reduce the cost of feature engineering.,5 Conclusion,[0],[0]
"We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing.",abstractText,[0],[0]
Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models.,abstractText,[0],[0]
"Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work.",abstractText,[0],[0]
"To address this problem, we propose embeddings of character strings, in addition to words.",abstractText,[0],[0]
"Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing.",abstractText,[0],[0]
We also explore bi-LSTM models with fewer features.,abstractText,[0],[0]
Neural Joint Model for Transition-based Chinese Syntactic Analysis,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 594–600 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
594",text,[0],[0]
"Information on the semantic relations of words is important for many natural language processing tasks, such as recognizing textual entailment, discourse classification, and question answering.",1 Introduction,[0],[0]
"There are two main approaches to obtain the distributed relational representations of word pairs.
",1 Introduction,[0],[0]
"One is the vector offset method (Mikolov et al., 2013a,b).",1 Introduction,[0],[0]
This approach represents word pairs as the vector offsets of their word embeddings.,1 Introduction,[0],[0]
Another approach exploits lexico-syntactic patterns to obtain word pair representations.,1 Introduction,[0],[0]
"As a pioneer work, Turney (2005) introduced latent relational analysis (LRA), based on the latent relation hypothesis.",1 Introduction,[0],[0]
"It states that word pairs that co-occur in similar lexico-syntactic patterns tend to have similar semantic relations (Turney, 2008b; Turney and Pantel, 2010).",1 Introduction,[0],[0]
"LRA is expected to complement
the vector offset model because word embeddings do not contain information on lexico-syntactic patterns that connect word pairs in a corpus (Shwartz et al., 2016).
",1 Introduction,[0],[0]
"However, LRA cannot obtain the representations of word pairs that do not co-occur in a corpus.",1 Introduction,[0],[0]
"Even with a large corpus, observing a cooccurrence of all semantically related word pairs is nearly impossible because of Zipf’s law, which states that most content words rarely occur.",1 Introduction,[0],[0]
"This data sparseness problem is a major bottleneck of pattern-based models such as LRA.
",1 Introduction,[0],[0]
"In this paper, we propose neural latent relational analysis (NLRA) to solve that data sparseness problem.",1 Introduction,[0],[0]
NLRA unsupervisedly learns the embeddings of target word pairs and co-occurring patterns from a corpus.,1 Introduction,[0],[0]
"In addition, it jointly learns the mapping from the word embedding space to the word-pair embedding space.",1 Introduction,[0],[0]
"By this mapping, NLRA can generalize the cooccurrences of word pairs and patterns, and obtain the relational embeddings for arbitrary word pairs even if they do not co-occur in the corpus.
",1 Introduction,[0],[0]
"Our experimental results on the task of measuring relational similarity show that NLRA significantly outperforms LRA, and it can also capture semantic relations of word pairs without cooccurrences.",1 Introduction,[0],[0]
"Moreover, we show that combining NLRA and the vector offset model improves the performance and leads to competitive results to those of the state-of-the-art method that exploits additional semantic relational data.",1 Introduction,[0],[0]
"The vector offset model (Mikolov et al., 2013a,b; Levy and Goldberg, 2014) obtains word embeddings from a corpus and represents each word pair (a, b) as the vector offset of their embedding as
follows: v(a,b) =",2.1 Vector Offset Model,[0],[0]
"vb − va (1)
where va and vb are the word embeddings of a and b respectively.
",2.1 Vector Offset Model,[0],[0]
"This method regards relational information as the change in multiple topicality dimensions from one word to the other in the word embedding space (Zhila et al., 2013).",2.1 Vector Offset Model,[0],[0]
"Meanwhile, it does not contain the information of lexico-syntactic patterns that were shown to capture complementary information with word embeddings in previous studies on the lexical semantic relation detection (Levy et al., 2015; Shwartz et al., 2016).",2.1 Vector Offset Model,[0],[0]
"LRA takes a set of word pairs as input and generates the distributed representations of those word pairs based on their co-occurring patterns.
",2.2 Latent Relational Analysis,[0],[0]
"Given target word pairs W = {(a1, b1), . . .",2.2 Latent Relational Analysis,[0],[0]
", (an, bn)}, LRA constructs a list of lexico-syntactic patterns that connect those pairs, such as is a or in the, from the corpus for each word pair.",2.2 Latent Relational Analysis,[0],[0]
"Then, those patterns are generalized by replacing any or all or none of the intervening words with wildcards.",2.2 Latent Relational Analysis,[0],[0]
"As a feature selection, the generalized patterns generated from many word pairs are used as features.",2.2 Latent Relational Analysis,[0],[0]
"We define the set of these target feature patterns as C = {p1, . . .",2.2 Latent Relational Analysis,[0],[0]
", pm}.",2.2 Latent Relational Analysis,[0],[0]
"Then, the 2n × 2m matrix M is constructed.",2.2 Latent Relational Analysis,[0],[0]
"The rows of M correspond to pairs (ai, bi) and reversed pairs (bi, ai).",2.2 Latent Relational Analysis,[0],[0]
"The columns of M correspond to patterns XpiY and swapped patterns Y piX , where X and Y are the slots for the words of the word pairs.",2.2 Latent Relational Analysis,[0],[0]
"The value of Mij represents the strength of the association between the corresponding word pair and pattern, which is calculated using weighting methods such as positive pointwise mutual information
(PPMI).",2.2 Latent Relational Analysis,[0],[0]
"After these processes, the singular value decomposition (SVD) is applied to M , and the vector v(a,b) is assigned to each word pair (a, b).
",2.2 Latent Relational Analysis,[0],[0]
"Although pattern-based approaches such as LRA have achieved promising results in some semantic relational tasks (Turney, 2008a,b), they have a crucial problem that a co-occurrence of all semantically related word pairs cannot be observed because of Zipf’s law, which states that the frequency distribution of words has a long tail.",2.2 Latent Relational Analysis,[0],[0]
"In other words, most words occur very rarely (Hanks, 2009).",2.2 Latent Relational Analysis,[0],[0]
"For the word pairs without co-occurrences, LRA cannot obtain their vector representations.",2.2 Latent Relational Analysis,[0],[0]
"We introduce NLRA, based on the latent relation hypothesis.",3 Neural Latent Relational Analysis,[0],[0]
NLRA represents the target word pairs and lexico-syntactic patterns as embeddings.,3 Neural Latent Relational Analysis,[0],[0]
"Similar to the skip-gram model (Mikolov et al., 2013a), NLRA updates those representations unsupervisedly, such that the inner products of the word pairs and patterns in which they co-occur in a corpus have high values.",3 Neural Latent Relational Analysis,[0],[0]
"Through this learning, the word pairs that co-occur in similar patterns have similar embeddings.",3 Neural Latent Relational Analysis,[0],[0]
"Moreover, NLRA can generalize the co-occurrences of the word pairs and patterns by constructing the embeddings of the word pairs from their word embeddings, thus solving the data sparseness problem of word cooccurrences.",3 Neural Latent Relational Analysis,[0],[0]
"Therefore, NLRA can provide representations that capture the information of lexicosyntactic patterns even for the word pairs that do not co-occur in a sentence.
",3 Neural Latent Relational Analysis,[0],[0]
Figure 1 is an illustration of our model.,3 Neural Latent Relational Analysis,[0],[0]
"NLRA encodes a word pair (a, b) into a dense vector as follows:
h(a,b) =MLP ([va;vb;vb − va]) (2)
where [va;vb;vb − va] is the concatenation of the word embeddings of a and b and their vector offsets; MLP is a multilayer perceptron with nonlinear activation functions.
",3 Neural Latent Relational Analysis,[0],[0]
"A pattern p is a sequence of the words w1, . . .",3 Neural Latent Relational Analysis,[0],[0]
", wk.",3 Neural Latent Relational Analysis,[0],[0]
"The sequence of the corresponding word embeddings w1, . . .",3 Neural Latent Relational Analysis,[0],[0]
",wk are encoded using long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997).",3 Neural Latent Relational Analysis,[0],[0]
"Then, the final output vector vp is used as the pattern embedding.
",3 Neural Latent Relational Analysis,[0],[0]
"For unsupervised learning, we use the negative sampling objective (Mikolov et al., 2013a).",3 Neural Latent Relational Analysis,[0],[0]
"Given a set of observed triples (a, b, p) ∈ D, where a and b are words such that (a, b) ∈ W , or (b, a) ∈ W and p is a co-occurring pattern from a corpus, the objective is as follows:
L = ∑
(a,b,p)∈D
log σ(vp · h(a,b))
",3 Neural Latent Relational Analysis,[0],[0]
"+ ∑
(a,b,p′)∈D′ log σ(−vp′ · h(a,b))",3 Neural Latent Relational Analysis,[0],[0]
"(3)
where D′ is a set of randomly generated negative samples and σ is the sigmoid function.",3 Neural Latent Relational Analysis,[0],[0]
We sampled 10 negative patterns for each word pair.,3 Neural Latent Relational Analysis,[0],[0]
"This objective is maximized using the stochastic gradient descent.
",3 Neural Latent Relational Analysis,[0],[0]
"After unsupervised learning, we can obtain word pair representations v(a,b) as follows:
v(a,b) =",3 Neural Latent Relational Analysis,[0],[0]
"[h(a,b);h(b,a)] (4)",3 Neural Latent Relational Analysis,[0],[0]
"In our evaluation, we used the SemEval-2012 Task 2 dataset (Jurgens et al., 2012) for the task of measuring relational similarity.",4.1 Dataset,[0],[0]
This dataset contains a collection of 79 fine-grained semantic relations.,4.1 Dataset,[0],[0]
"For each relation, there are a few prototypical word pairs and a set of several dozen target word pairs.",4.1 Dataset,[0],[0]
The task is to rank the target pairs based on the extent to which they exhibit the relation.,4.1 Dataset,[0],[0]
"In our experiment, we calculated the score of a target word pair with the average cosine similarity between it and each prototypical word pair.",4.1 Dataset,[0],[0]
The models are evaluated in terms of the MaxDiff accuracy and Spearman’s correlation.,4.1 Dataset,[0],[0]
"Following previous works (Rink and Harabagiu, 2012; Zhila et al., 2013), we used the test set that includes 69 semantic relations to evaluate the performance.",4.1 Dataset,[0],[0]
VecOff.,4.2 Baselines,[0],[0]
"We used the 300-dimensional pre-trained GloVe (Pennington et al., 2014)1 and represented word pairs as described in Section 2.1.
LRA.",4.2 Baselines,[0],[0]
We implemented LRA as described in Section 2.2.,4.2 Baselines,[0],[0]
We set W as the lemmatized word pairs of the dataset.,4.2 Baselines,[0],[0]
We used the English Wikipedia as a corpus.,4.2 Baselines,[0],[0]
"For each word pair, we searched for patterns of from one to three words.",4.2 Baselines,[0],[0]
"When searching for patterns, the left word and right word adjacent to the patterns were lemmatized to ignore their inflections.",4.2 Baselines,[0],[0]
"Following (Turney, 2008b), we selected C as the top 20|W | generalized patterns.",4.2 Baselines,[0],[0]
"Then, M was constructed using PPMI weighting, and its dimensionality was reduced to 300 using SVD.",4.2 Baselines,[0],[0]
NLRA.,4.3 Our methods,[0],[0]
"For each word pair in the dataset, cooccurring patterns were extracted from the same corpus in the same manner as with LRA, resulting in D. For word embeddings, we used the same pre-trained GloVe as VecOff.",4.3 Our methods,[0],[0]
These embeddings were updated during the training.,4.3 Our methods,[0],[0]
"For MLP , we used three affine transformations followed by the batch normalization (Ioffe and Szegedy, 2015) and tanh activation.",4.3 Our methods,[0],[0]
The size of each hidden layer of the MLP was 300.,4.3 Our methods,[0],[0]
"To encode the patterns, we used LSTM with the 300-dimensional hidden state.",4.3 Our methods,[0],[0]
"The objective was optimized by AdaGrad (Duchi et al., 2011) (whose learning rate was 0.01).",4.3 Our methods,[0],[0]
"We trained the model for 50 epochs.
NLRA+VecOff.",4.3 Our methods,[0],[0]
This method combines NLRA and VecOff by averaging their score for a target word pair.,4.3 Our methods,[0],[0]
"Table 1 displays the overall result.
",4.4 Result and Analysis,[0],[0]
"NLRA vs. LRA First, NLRA outperformed LRA in terms of both the average accuracy and correlation.",4.4 Result and Analysis,[0],[0]
These differences were statistically significant (p < 0.01) with the paired t-test.,4.4 Result and Analysis,[0],[0]
These results indicate that generalizing patterns with LSTM is better than by using wildcards.,4.4 Result and Analysis,[0],[0]
"Moreover, NLRA can successfully calculate the relational similarity for the word pairs that do not co-occur in the corpus.",4.4 Result and Analysis,[0],[0]
"Table 2 shows an example of the Reference–Express relation, where the middle-score pair handshake:cordiality
1https://nlp.stanford.edu/projects/glove/
and the low-score pair friendliness:wink have no co-occurring pattern.",4.4 Result and Analysis,[0],[0]
"In these cases, LRA could not obtain the representations of those word pairs nor correctly assign the score.",4.4 Result and Analysis,[0],[0]
"By contrast, NLRA could accomplish both because it could generalize the co-occurrences of word pairs and patterns.
NLRA+VecOff vs. Other Models Second, NLRA+VecOff outperformed the other models.",4.4 Result and Analysis,[0],[0]
These differences were statistically significant (the correlation difference between NLRA+Vecoff and NLRA: p < 0.05; the other differences: p < 0.01).,4.4 Result and Analysis,[0],[0]
These results indicate that lexico-syntactic patterns and the vector offset of word embeddings capture complementary information for measuring relational similarity.,4.4 Result and Analysis,[0],[0]
This is inconsistent with the findings of Zhila et al. (2013).,4.4 Result and Analysis,[0],[0]
"That work combined heterogeneous models, such as the vector offset model, patternbased model, etc., and stated that the pattern-based model was less significant than the vector offset model, based on their ablation study.",4.4 Result and Analysis,[0],[0]
We believe that this was because their pattern-based model did not generalize patterns with wildcards nor select useful features.,4.4 Result and Analysis,[0],[0]
Their pattern-based model seemed to suffer from sparse feature space.,4.4 Result and Analysis,[0],[0]
"In our experiment, NLRA helped VecOff, for example, for the Part-Whole relation, Cause Purpose rela-
tion, and Space-Time relation, where there seemed to be prototypical patterns indicating those relations.",4.4 Result and Analysis,[0],[0]
"Meanwhile, VecOff helped NLRA for the Attribute relation, where the relational patterns seemed to be diverse.",4.4 Result and Analysis,[0],[0]
These results showed that the combined model is robust.,4.4 Result and Analysis,[0],[0]
We compared the results of our models to other published results.,4.5 Comparison to other systems,[0],[0]
Table 3 displays those results.,4.5 Comparison to other systems,[0],[0]
Rink and Harabagiu (2012) is the pattern-based model with naive Bayes.,4.5 Comparison to other systems,[0],[0]
"Mikolov et al. (2013b), Levy and Goldberg (2014), and Iacobacci et al. (2015) are the vector offset models.",4.5 Comparison to other systems,[0],[0]
Zhila et al. (2013) is the model composed of various features.,4.5 Comparison to other systems,[0],[0]
"Turney (2013) extracts the statistical features of two word pairs from a word-context co-occurrence matrix and trains the classifier with additional semantic relational data to assign a relational similarity for two word pairs.
",4.5 Comparison to other systems,[0],[0]
NLRA+VecOff achieved a competitive performance to the state-of-the-art method of Turney (2013).,4.5 Comparison to other systems,[0],[0]
"Note that our method learns unsupervisedly and does not exploit additional resources, and the method of Turney (2013) cannot obtain the distributed representation of word pairs.
",4.5 Comparison to other systems,[0],[0]
"A work similar to ours, Bollegala et al. (2015), represented lexico-syntactic patterns as the vector offset of co-occurring word pairs and updated the
vector offsets of word pairs such that word pairs that co-occur in similar patterns have similar offsets.",4.5 Comparison to other systems,[0],[0]
They evaluated their model on all 79 semantic relations of the dataset and achieved 0.449 accuracy.,4.5 Comparison to other systems,[0],[0]
"In their setting, NLRA+VecOff achieved 0.47 accuracy, outperforming their model.",4.5 Comparison to other systems,[0],[0]
Hearst (1992) detected the hypernymy relation of word pairs from a corpus using several handcrafted lexico-syntactic patterns.,5.1 Word Pairs and Co-occurring Patterns,[0],[0]
Turney and Littman (2005) used 64 handcrafted lexicosyntactic patterns as features of word pairs to represent word pairs as vectors.,5.1 Word Pairs and Co-occurring Patterns,[0],[0]
"To obtain word-pair embeddings, Turney (2005) extended the method of Turney and Littman (2005) as LRA.",5.1 Word Pairs and Co-occurring Patterns,[0],[0]
"Our work is a neural extension of LRA.
",5.1 Word Pairs and Co-occurring Patterns,[0],[0]
Washio and Kato (2018) proposed the method similar to ours in lexical semantic relation detection.,5.1 Word Pairs and Co-occurring Patterns,[0],[0]
Their neural method modeled the cooccurrences of word pairs and dependency paths connecting two words to alleviate the data sparseness problem of pattern-based lexical semantic relation detection.,5.1 Word Pairs and Co-occurring Patterns,[0],[0]
"While they assigned randomly initialized embeddings to each dependency path, our work encodes co-occurring patterns with LSTM for better generalization.",5.1 Word Pairs and Co-occurring Patterns,[0],[0]
Jameel et al. (2018) embedded word pairs with the context words occurring around word pairs instead of lexico-syntactic patterns.,5.1 Word Pairs and Co-occurring Patterns,[0],[0]
Their method cannot obtain embeddings of word pairs that do not co-occur in a corpus because they directly assigned embeddings to word pairs.,5.1 Word Pairs and Co-occurring Patterns,[0],[0]
"By contrast, NLRA can obtain embeddings for those word pairs.
",5.1 Word Pairs and Co-occurring Patterns,[0],[0]
"In another research area, relation extraction, several works have explored an idea similar to the latent relation hypothesis (Riedel et al., 2013; Toutanova et al., 2015; Verga et al., 2017).",5.1 Word Pairs and Co-occurring Patterns,[0],[0]
"They factorized a matrix of entity pairs and co-occurring patterns, while they focused on named entity pairs instead of word pairs and did not consider cooccurrence frequencies.",5.1 Word Pairs and Co-occurring Patterns,[0],[0]
"Knowledge graph embedding (KGE) embeds entities and relations in knowledge graph (KG), where entities and relations corresponds to nodes and edges respectively (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2014;
Lin et al., 2015; Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Liu et al., 2017; Wang et al., 2017; Ishihara et al., 2018).",5.2 Relation to Knowledge Graph Embedding,[0],[0]
"By considering words and lexico-syntactic patterns as nodes and edges, respectively, a corpus can be viewed as a graph, i.e., corpus graph (CG).",5.2 Relation to Knowledge Graph Embedding,[0],[0]
"Thus, NLRA can be regarded as corpus graph embedding (CGE) models based on the latent relation hypothesis.
",5.2 Relation to Knowledge Graph Embedding,[0],[0]
"Although KGE models can be easily applied to CG, several differences exist between KG and CG.",5.2 Relation to Knowledge Graph Embedding,[0],[0]
"First, the nodes and edges of CG are (sequences of) linguistic expressions, such as tokens, lemmas, phrases, etc.",5.2 Relation to Knowledge Graph Embedding,[0],[0]
"Thus, the nodes and edges of CG might exhibit compositionality and ambiguity, while KG does not have those properties.",5.2 Relation to Knowledge Graph Embedding,[0],[0]
"Second, the edges of CG have weights based on cooccurrence frequencies unlike the edges of KG.",5.2 Relation to Knowledge Graph Embedding,[0],[0]
"Finally, CG might have a large number of edges types while the number of KG edges is at most several thousands.",5.2 Relation to Knowledge Graph Embedding,[0],[0]
An interesting research direction is exploring models suitable for CGE to capture the property of linguistic expressions and their relations in the embedding space.,5.2 Relation to Knowledge Graph Embedding,[0],[0]
"We presented NLRA, which learns the distributed representation of word pairs capturing semantic relational information through co-occurring patterns encoded by LSTM.",6 Conclusion,[0],[0]
This model jointly learns the mapping from the word embedding space into the word-pair embedding space to generalize co-occurrences of word pairs and patterns.,6 Conclusion,[0],[0]
"Our experiment on measuring relational similarity demonstrated that NLRA outperforms LRA and can successfully solve the data sparseness problem of word co-occurrences, which is a major bottleneck in pattern-based approaches.",6 Conclusion,[0],[0]
"Moreover, combining the vector offset model and NLRA yielded competitive performance to the state-ofthe-art method, though our method relied only on unsupervised learning.",6 Conclusion,[0],[0]
"This combined model exploits the complementary information of lexicosyntactic patterns and word embeddings.
",6 Conclusion,[0],[0]
"In our future work, we will apply word-pair embeddings from NLRA to various downstream tasks related to lexical relational information.",6 Conclusion,[0],[0]
This work was supported by JSPS KAKENHI Grant numbers JP17H01831.,Acknowledgments,[0],[0]
We thank Satoshi Sekine and Kentaro Inui for helpful discussion.,Acknowledgments,[0],[0]
Capturing the semantic relations of words in a vector space contributes to many natural language processing tasks.,abstractText,[0],[0]
One promising approach exploits lexico-syntactic patterns as features of word pairs.,abstractText,[0],[0]
"In this paper, we propose a novel model of this pattern-based approach, neural latent relational analysis (NLRA).",abstractText,[0],[0]
"NLRA can generalize co-occurrences of word pairs and lexicosyntactic patterns, and obtain embeddings of the word pairs that do not co-occur.",abstractText,[0],[0]
This overcomes the critical data sparseness problem encountered in previous pattern-based models.,abstractText,[0],[0]
Our experimental results on measuring relational similarity demonstrate that NLRA outperforms the previous pattern-based models.,abstractText,[0],[0]
"In addition, when combined with a vector offset model, NLRA achieves a performance comparable to that of the state-of-theart model that exploits additional semantic relational data.",abstractText,[0],[0]
Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 850–860 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1079",text,[0],[0]
"When handling broad or open domains, machine translation systems usually have to handle a large vocabulary as their inputs and outputs.",1 Introduction,[0],[0]
"This is particularly a problem in neural machine translation (NMT) models (Sutskever et al., 2014), such as the attention-based models (Bahdanau et al., 2014; Luong et al., 2015) shown in Figure 1.",1 Introduction,[0],[0]
"In these models, the output layer is required to generate a specific word from an internal vector, and a large vocabulary size tends to require a large amount of computation to predict each of the candidate word probabilities.
",1 Introduction,[0],[0]
"Because this is a significant problem for neural language and translation models, there are a number of methods proposed to resolve this problem, which we detail in Section 2.2.",1 Introduction,[0],[0]
"However, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in NMT systems:
Memory efficiency: The method should not require large memory to store the parameters and calculated vectors to maintain scalability in resource-constrained environments.
",1 Introduction,[0],[0]
"Time efficiency: The method should be able to train the parameters efficiently, and possible to perform decoding efficiently with choosing the candidate words from the full probability distribution.",1 Introduction,[0],[0]
"In particular, the method should be performed fast on general CPUs to suppress physical costs of computational resources for actual production systems.
",1 Introduction,[0],[0]
"Compatibility with parallel computation: It should be easy for the method to be minibatched and optimized to run efficiently on GPUs, which are essential for training large NMT models.
",1 Introduction,[0],[0]
"In this paper, we propose a method that satisfies all of these conditions: requires significantly less memory, fast, and is easy to implement minibatched on GPUs.",1 Introduction,[0],[0]
"The method works by not predicting a softmax over the entire output vocab-
850
ulary, but instead by encoding each vocabulary word as a vector of binary variables, then independently predicting the bits of this binary representation.",1 Introduction,[0],[0]
"In order to represent a vocabulary size of 2n, the binary representation need only be at least n bits long, and thus the amount of computation and size of parameters required to select an output word is only O(log V ) in the size of the vocabulary V , a great reduction from the standard linear increase of O(V ) seen in the original softmax.
",1 Introduction,[0],[0]
"While this idea is simple and intuitive, we found that it alone was not enough to achieve competitive accuracy with real NMT models.",1 Introduction,[0],[0]
"Thus we make two improvements: First, we propose a hybrid model, where the high frequency words are predicted by a standard softmax, and low frequency words are predicted by the proposed binary codes separately.",1 Introduction,[0],[0]
"Second, we propose the use of convolutional error correcting codes with Viterbi decoding (Viterbi, 1967), which add redundancy to the binary representation, and even in the face of localized mistakes in the calculation of the representation, are able to recover the correct word.
",1 Introduction,[0],[0]
"In experiments on two translation tasks, we find that the proposed hybrid method with error correction is able to achieve results that are competitive with standard softmax-based models while reducing the output layer to a fraction of its original size.",1 Introduction,[0],[0]
"Most of current NMT models use one-hot representations to represent the words in the output vocabulary – each word w is represented by a unique sparse vector eid(w) ∈ RV , in which only one element at the position corresponding to the word ID id(w) ∈ {x ∈ N",2.1 Formulation and Standard Softmax,[0],[0]
"| 1 ≤ x ≤ V } is 1, while others are 0.",2.1 Formulation and Standard Softmax,[0],[0]
V represents the vocabulary size of the target language.,2.1 Formulation and Standard Softmax,[0],[0]
"NMT models optimize network parameters by treating the one-hot representation eid(w) as the true probability distribution, and minimizing the cross entropy between it and the softmax probability v:
LH(v, id(w))",2.1 Formulation and Standard Softmax,[0],[0]
":= H(eid(w),v), (1) = log sum expu− uid(w), (2)
v := expu/ sum expu, (3)
",2.1 Formulation and Standard Softmax,[0],[0]
"u := Whuh+ βu, (4)
where sumx represents the sum of all elements in x, xi represents the i-th element of x, Whu ∈
RV×H and βu ∈ RV are trainable parameters and H is the total size of hidden layers directly connected to the output layer.
",2.1 Formulation and Standard Softmax,[0],[0]
"According to Equation (4), this model clearly requires time/space computation in proportion to O(HV ), and the actual load of the computation of the output layer is directly affected by the size of vocabulary V , which is typically set around tens of thousands (Sutskever et al., 2014).",2.1 Formulation and Standard Softmax,[0],[0]
Several previous works have proposed methods to reduce computation in the output layer.,2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"The hierarchical softmax (Morin and Bengio, 2005) predicts each word based on binary decision and reduces computation time to O(H log V ).",2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"However, this method still requires O(HV ) space for the parameters, and requires calculation much more complicated than the standard softmax, particularly at test time.
",2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"The differentiated softmax (Chen et al., 2016) divides words into clusters, and predicts words using separate part of the hidden layer for each word clusters.",2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"This method make the conversion matrix of the output layer sparser than a fully-connected softmax, and can reduce time/space computation amount by ignoring zero part of the matrix.",2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"However, this method restricts the usage of hidden layer, and the size of the matrix is still in proportion to V .
",2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"Sampling-based approximations (Mnih and Teh, 2012; Mikolov et al., 2013) to the denominator of the softmax have also been proposed to reduce calculation at training.",2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax.
",2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"Vocabulary selection approaches (Mi et al., 2016; L’Hostis et al., 2016) can also reduce the vocabulary size at testing, but these methods abandon full search over the target space and the quality of picked vocabularies directly affects the translation quality.
",2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"Other methods using characters (Ling et al., 2015) or subwords (Sennrich et al., 2016; Chitnis and DeNero, 2015) can be applied to suppress the vocabulary size, but these methods also make for longer sequences, and thus are not a direct solution to problems of computational efficiency.",2.2 Prior Work on Suppressing Complexity of NMT Models,[0],[0]
"Figure 2(a) shows the conventional softmax prediction, and Figure 2(b) shows the binary code prediction model proposed in this study.",3.1 Representing Words using Bit Arrays,[0],[0]
"Unlike the conventional softmax, the proposed method predicts each output word indirectly using dense bit arrays that correspond to each word.",3.1 Representing Words using Bit Arrays,[0],[0]
"Let b(w) := [b1(w), b2(w), · · · , bB(w)] ∈ {0, 1}B be the target bit array obtained for word w, where each bi(w) ∈",3.1 Representing Words using Bit Arrays,[0],[0]
"{0, 1} is an independent binary function given w, and B is the number of bits in whole array.",3.1 Representing Words using Bit Arrays,[0],[0]
"For convenience, we introduce some constraints on b. First, a wordw is mapped to only one bit array b(w).",3.1 Representing Words using Bit Arrays,[0],[0]
"Second, all unique words can be discriminated by b, i.e., all bit arrays satisfy that:1
id(w) 6= id(w′)⇒ b(w) 6= b(w′).",3.1 Representing Words using Bit Arrays,[0],[0]
"(5)
Third, multiple bit arrays can be mapped to the same word as described in Section 3.5.",3.1 Representing Words using Bit Arrays,[0],[0]
"By considering second constraint, we can also constrain B ≥ dlog2 V e, because b should have at least V unique representations to distinguish each word.",3.1 Representing Words using Bit Arrays,[0],[0]
The output layer of the network independently predicts B probability values q,3.1 Representing Words using Bit Arrays,[0],[0]
":= [q1(h), q2(h), · · · , qB(h)] ∈",3.1 Representing Words using Bit Arrays,[0],[0]
"[0, 1]B using the
1We designed this injective condition using the id(·) function to ignore task-specific sensitivities between different word surfaces (e.g. cases, ligatures, etc.).
",3.1 Representing Words using Bit Arrays,[0],[0]
"current hidden values h by logistic regressions:
q(h) = σ(Whqh+ βq), (6)
σ(x) :",3.1 Representing Words using Bit Arrays,[0],[0]
"= 1/(1 + exp(−x)), (7) where Whq ∈ RB×H and βq ∈ RB are trainable parameters.",3.1 Representing Words using Bit Arrays,[0],[0]
"When we assume that each qi is the probability that “the i-th bit becomes 1,” the joint probability of generating word w can be represented as:
Pr(b(w)|q(h))",3.1 Representing Words using Bit Arrays,[0],[0]
":= B∏
i=1
( biqi + b̄iq̄i ) , (8)
where x̄ := 1 − x. We can easily obtain the maximum-probability bit array from q by simply assuming the i-th bit is 1 if qi ≥ 1/2, or 0 otherwise.",3.1 Representing Words using Bit Arrays,[0],[0]
"However, this calculation may generate invalid bit arrays which do not correspond to actual words according to the mapping between words and bit arrays.",3.1 Representing Words using Bit Arrays,[0],[0]
"For now, we simply assume that w = UNK (unknown) when such bit arrays are obtained, and discuss alternatives later in Section 3.5.
",3.1 Representing Words using Bit Arrays,[0],[0]
"The constraints described here are very general requirements for bit arrays, which still allows us to choose between a wide variety of mapping functions.",3.1 Representing Words using Bit Arrays,[0],[0]
"However, designing the most appropriate mapping method for NMT models is not a trivial problem.",3.1 Representing Words using Bit Arrays,[0],[0]
"In this study, we use a simple mapping method described in Algorithm 1, which was empirically effective in preliminary experiments.2 Here, V is the set of V target words including 3 extra markers: UNK, BOS (begin-of-sentence), and EOS (end-of-sentence), and rank(w) ∈",3.1 Representing Words using Bit Arrays,[0],[0]
N>0 is the rank of the word according to their frequencies in the training corpus.,3.1 Representing Words using Bit Arrays,[0],[0]
"Algorithm 1 is one of the minimal mapping methods (i.e., satisfying B = dlog2 V e), and generated bit arrays have the characteristics that their higher bits roughly represents the frequency of corresponding words (e.g., if w is frequently appeared in the training corpus, higher bits in b(w) tend to become 0).",3.1 Representing Words using Bit Arrays,[0],[0]
"For learning correct binary representations, we can use any loss functions that is (sub-)differentiable and satisfies a constraint that:
LB(q, b) {
= L, if q = b, ≥ L, otherwise, (9)
2Other methods examined included random codes, Huffman codes (Huffman, 1952) and Brown clustering (Brown et al., 1992) with zero-padding to adjust code lengths, and some original allocation methods based on the word2vec embeddings (Mikolov et al., 2013).
",3.2 Loss Functions,[0],[0]
Algorithm 1 Mapping words to bit arrays.,3.2 Loss Functions,[0],[0]
"Require: w ∈ V Ensure: b ∈ {0, 1}B = Bit array representing w
x :=    0, if w = UNK 1, if w = BOS 2, if w = EOS 2 + rank(w), otherwise bi := bx/2i−1c mod 2 b←",3.2 Loss Functions,[0],[0]
"[b1, b2, · · · , bB ]
where L is the minimum value of the loss function which typically does not affect the gradient descent methods.",3.2 Loss Functions,[0],[0]
"For example, the squareddistance:
LB(q, b) := B∑
i=1
(qi − bi)2, (10)
or the cross-entropy:
LB(q, b) := − B∑
i=1
( bi log qi + b̄i log q̄i ) , (11)
are candidates for the loss function.",3.2 Loss Functions,[0],[0]
"We also examined both loss functions in the preliminary experiments, and in this paper, we only used the squared-distance function (Equation (10)), because this function achieved higher translation accuracies than Equation (11).3",3.2 Loss Functions,[0],[0]
The computational complexity for the parameters Whq and βq is O(HB).,3.3 Efficiency of the Binary Code Prediction,[0],[0]
"This is equal to O(H log V ) when using a minimal mapping method like that shown in Algorithm 1, and is significantly smaller than O(HV ) when using standard softmax prediction.",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"For example, if we chose V = 65536 = 216 and use Algorithm 1’s mapping method, then B = 16 and total amount of computation in the output layer could be suppressed to 1/4096 of its original size.
",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"On a different note, the binary code prediction model proposed in this study shares some ideas with the hierarchical softmax (Morin and Bengio, 2005) approach.",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"Actually, when we used a binarytree based mapping function for b, our model can be interpreted as the hierarchical softmax with two
3In terms of learning probabilistic models, we should remind that using Eq.",3.3 Efficiency of the Binary Code Prediction,[0],[0]
(10) is an approximation of Eq.,3.3 Efficiency of the Binary Code Prediction,[0],[0]
(11).,3.3 Efficiency of the Binary Code Prediction,[0],[0]
"The output bit scores trained by Eq. (10) do not represent actual word perplexities, and this characteristics imposes some practical problems when comparing multiple hypotheses (e.g., reranking, beam search, etc.).",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"We could ignore this problem in this paper because we only evaluated the one-best results in experiments.
",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"strong constraints for guaranteeing independence between all bits: all nodes in the same level of the hierarchy share their parameters, and all levels of the hierarchy are predicted independently of each other.",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"By these constraints, all bits in b can be calculated in parallel.",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"This is particularly important because it makes the model conducive to being calculated on parallel computation backends such as GPUs.
",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"However, the binary code prediction model also introduces problems of robustness due to these strong constraints.",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"As the experimental results show, the simplest prediction model which directly maps words into bit arrays seriously decreases translation quality.",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"In Sections 3.4 and 3.5, we introduce two additional techniques to prevent reductions of translation quality and improve robustness of the binary code prediction model.",3.3 Efficiency of the Binary Code Prediction,[0],[0]
"According to the Zipf’s law (Zipf, 1949), the distribution of word appearances in an actual corpus is biased to a small subset of the vocabulary.",3.4 Hybrid Softmax/Binary Model,[0],[0]
"As a result, the proposed model mostly learns characteristics for frequent words and cannot obtain enough opportunities to learn for rare words.",3.4 Hybrid Softmax/Binary Model,[0],[0]
"To alleviate this problem, we introduce a hybrid model using both softmax prediction and binary code prediction as shown in Figure 2(c).",3.4 Hybrid Softmax/Binary Model,[0],[0]
"In this model, the output layer calculates a standard softmax for the N − 1 most frequent words and an OTHER marker which indicates all rare words.",3.4 Hybrid Softmax/Binary Model,[0],[0]
"When the softmax layer predicts OTHER, then the binary code layer is used to predict the representation of rare words.",3.4 Hybrid Softmax/Binary Model,[0],[0]
"In this case, the actual probability of generating a particular word can be separated into two equations according to the frequency of words:
Pr(w|h) ' { v′id(w), if id(w) < N, v′N · π(w,h), otherwise, (12)
v′",3.4 Hybrid Softmax/Binary Model,[0],[0]
":= expu′/ sum expu′, (13) u′ := Whu′h+ βu′ , (14) π(w,h) := Pr(b(w)|q(h)), (15)
",3.4 Hybrid Softmax/Binary Model,[0],[0]
"where Whu′ ∈ RN×H and βu′ ∈ RN are trainable parameters, and id(w) assumes that the value corresponds to the rank of frequency of each word.",3.4 Hybrid Softmax/Binary Model,[0],[0]
"We also define the loss function for the hybrid
model using both softmax and binary code losses:
L := { lH(id(w)), if id(w) < N, lH(N) + lB, otherwise, (16) lH(i) := λHLH(v′, i), (17)
lB := λBLB(q, b), (18)
where λH and λB are hyper-parameters to determine strength of both softmax/binary code losses.",3.4 Hybrid Softmax/Binary Model,[0],[0]
"These also can be adjusted according to the training data, but in this study, we only used λH = λB = 1 for simplicity.
",3.4 Hybrid Softmax/Binary Model,[0],[0]
"The computational complexity of the hybrid model is O(H(N + log V )), which is larger than the original binary code modelO(H log V ).",3.4 Hybrid Softmax/Binary Model,[0],[0]
"However,N can be chosen asN V because the softmax prediction is only required for a few frequent words.",3.4 Hybrid Softmax/Binary Model,[0],[0]
"As a result, we can control the actual computation for the hybrid model to be much smaller than the standard softmax complexity O(HV ),
The idea of separated prediction of frequent words and rare words comes from the differentiated softmax (Chen et al., 2016) approach.",3.4 Hybrid Softmax/Binary Model,[0],[0]
"However, our output layer can be configured as a fullyconnected network, unlike the differentiated softmax, because the actual size of the output layer is still small after applying the hybrid model.",3.4 Hybrid Softmax/Binary Model,[0],[0]
"The 2 methods proposed in previous sections impose constraints for all bits in q, and the value of each bit must be estimated correctly for the correct word to be chosen.",3.5 Applying Error-correcting Codes,[0],[0]
"As a result, these models may generate incorrect words due to even a single bit error.",3.5 Applying Error-correcting Codes,[0],[0]
"This problem is the result of dense mapping between words and bit arrays, and can be avoided by creating redundancy in the bit array.",3.5 Applying Error-correcting Codes,[0],[0]
Figure 3 shows a simple example of how this idea works when discriminating 2 words using 3 bits.,3.5 Applying Error-correcting Codes,[0],[0]
"In this case, the actual words are obtained by
estimating the nearest centroid bit array according to the Hamming distance between each centroid and the predicted bit array.",3.5 Applying Error-correcting Codes,[0],[0]
"This approach can predict correct words as long as the predicted bit arrays are in the set of neighbors for the correct centroid (gray regions in the Figure 3), i.e., up to a 1-bit error in the predicted bits can be corrected.",3.5 Applying Error-correcting Codes,[0],[0]
"This ability to be robust to errors is a central idea behind error-correcting codes (Shannon, 1948).",3.5 Applying Error-correcting Codes,[0],[0]
"In general, an error-correcting code has the ability to correct up to b(d−1)/2c bit errors when all centroids differ d bits from each other (Golay, 1949).",3.5 Applying Error-correcting Codes,[0],[0]
d is known as the free distance determined by the design of error-correcting codes.,3.5 Applying Error-correcting Codes,[0],[0]
"Errorcorrecting codes have been examined in some previous work on multi-class classification tasks, and have reported advantages from the raw classification (Dietterich and Bakiri, 1995; Klautau et al., 2003; Liu, 2006; Kouzani and Nasireding, 2009; Kouzani, 2010; Ferng and Lin, 2011, 2013).",3.5 Applying Error-correcting Codes,[0],[0]
"In this study, we applied an error-correcting algorithm to the bit array obtained from Algorithm 1 to improve robustness of the output layer in an NMT system.",3.5 Applying Error-correcting Codes,[0],[0]
"A challenge in this study is trying a large classification (#classes > 10,000) with error-correction, unlike previous studies focused on solving comparatively small tasks (#classes < 100).",3.5 Applying Error-correcting Codes,[0],[0]
And this study also tries to solve a generation task unlike previous studies.,3.5 Applying Error-correcting Codes,[0],[0]
"As shown in the experiments, we found that this approach is highly effective in these tasks.
",3.5 Applying Error-correcting Codes,[0],[0]
Figure 4 (a) and (b) illustrate the training and generation processes for the model with errorcorrecting codes.,3.5 Applying Error-correcting Codes,[0],[0]
"In the training, we first convert the original bit arrays b(w) to a center bit array b′",3.5 Applying Error-correcting Codes,[0],[0]
in the space of error-correcting code: b′(b),3.5 Applying Error-correcting Codes,[0],[0]
:=,3.5 Applying Error-correcting Codes,[0],[0]
"[b′1(b), b ′ 2(b), · · · , b′B′(b)] ∈ {0, 1}B ′ , where B′(B) ≥ B is the number of bits in the error-correcting code.",3.5 Applying Error-correcting Codes,[0],[0]
"The NMT model learns its
Algorithm 2 Encoding into a convolutional code.",3.5 Applying Error-correcting Codes,[0],[0]
"Require: b ∈ {0, 1}B",3.5 Applying Error-correcting Codes,[0],[0]
Ensure:,3.5 Applying Error-correcting Codes,[0],[0]
"b′ ∈ {0, 1}2(B+6) =
Redundant bit array
x[t] := { bt, if 1 ≤ t ≤ B 0, otherwise y1t := x[t− 6 .. t] ·",3.5 Applying Error-correcting Codes,[0],[0]
[1001111] mod 2 y2t := x[t− 6 .. t] ·,3.5 Applying Error-correcting Codes,[0],[0]
[1101101] mod 2 b′,3.5 Applying Error-correcting Codes,[0],[0]
"← [y11, y21, y12, y22, · · · , y1B+6, y2B+6]
parameters based on the loss between predicted probabilities q and b′.",3.5 Applying Error-correcting Codes,[0],[0]
"Note that typical errorcorrecting codes satisfy O(B′/B) = O(1), and this characteristic efficiently suppresses the increase of actual computation cost in the output layer due to the application of the error-correcting code.",3.5 Applying Error-correcting Codes,[0],[0]
"In the generation of actual words, the decoding method of the error-correcting code converts the redundant predicted bits q into a dense representation q̃ := [q̃1(q), q̃2(q), · · · , q̃B(q)], and uses q̃ as the bits to restore the word, as is done in the method described in the previous sections.
",3.5 Applying Error-correcting Codes,[0],[0]
It should be noted that the method for performing error correction directly affects the quality of the whole NMT model.,3.5 Applying Error-correcting Codes,[0],[0]
"For example, the mapping shown in Figure 3 has only 3 bits and it is clear that these bits represent exactly the same information as each other.",3.5 Applying Error-correcting Codes,[0],[0]
"In this case, all bits can be estimated using exactly the same parameters, and we can not expect that we will benefit significantly from applying this redundant representation.",3.5 Applying Error-correcting Codes,[0],[0]
"Therefore, we need to choose an error correction method in which the characteristics of original bits should be distributed in various positions of the resulting bit arrays so that errors in bits are not highly correlated with each-other.",3.5 Applying Error-correcting Codes,[0],[0]
"In addition, it is desirable that the decoding method of the applied error-correcting code can directly utilize the probabilities of each bit, because q generated by the network will be a continuous probabilities between zero and one.
",3.5 Applying Error-correcting Codes,[0],[0]
"In this study, we applied convolutional codes (Viterbi, 1967) to convert between original and redundant bits.",3.5 Applying Error-correcting Codes,[0],[0]
Convolutional codes perform a set of bit-wise convolutions between original bits and weight bits (which are hyper-parameters).,3.5 Applying Error-correcting Codes,[0],[0]
"They are well-suited to our setting here because they distribute the information of original bits in different places in the resulting bits, work robustly for random bit errors, and can be decoded using
Algorithm 3 Decoding from a convolutional code.",3.5 Applying Error-correcting Codes,[0],[0]
"Require: q ∈ (0, 1)2(B+6)",3.5 Applying Error-correcting Codes,[0],[0]
"Ensure: q̃ ∈ {0, 1}B = Restored bit array g(q, b) := b log q +",3.5 Applying Error-correcting Codes,[0],[0]
(1− b) log(1− q) φ0[s,3.5 Applying Error-correcting Codes,[0],[0]
"| s ∈ {0, 1}6]← { 0, if s = [000000] −∞, otherwise
for t = 1→ B + 6 do for scur ∈ {0, 1}6 do sprev(x) :=",3.5 Applying Error-correcting Codes,[0],[0]
[x] ◦ scur[1 .. 5] o1(x) := ([x] ◦ scur) ·,3.5 Applying Error-correcting Codes,[0],[0]
[1001111] mod 2 o2(x) := ([x] ◦ scur) ·,3.5 Applying Error-correcting Codes,[0],[0]
"[1101101] mod 2 g′(x) := g(q2t−1, o1(x))",3.5 Applying Error-correcting Codes,[0],[0]
+,3.5 Applying Error-correcting Codes,[0],[0]
"g(q2t, o2(x)) φ′(x)",3.5 Applying Error-correcting Codes,[0],[0]
:,3.5 Applying Error-correcting Codes,[0],[0]
= φt−1[sprev(x)],3.5 Applying Error-correcting Codes,[0],[0]
"+ g′(x) x̂← arg maxx∈{0,1} φ′(x) rt[s
cur]← sprev(x̂) φt[s
cur]← φ′(x̂) end for
end for s′",3.5 Applying Error-correcting Codes,[0],[0]
←,3.5 Applying Error-correcting Codes,[0],[0]
"[000000] for t = B → 1 do s′ ← rt+6[s′] q̃t ← s′1 end for q̃ ← [q̃1, q̃2, · · · , q̃B]
bit probabilities directly.
",3.5 Applying Error-correcting Codes,[0],[0]
"Algorithm 2 describes the particular convolutional code that we applied in this study, with two convolution weights [1001111] and [1101101] as fixed hyper-parameters.4 Where x[i ..",3.5 Applying Error-correcting Codes,[0],[0]
"j] := [xi, · · · , xj ] and x · y := ∑ i xiyi.",3.5 Applying Error-correcting Codes,[0],[0]
"On the other hand, there are various algorithms to decode convolutional codes with the same format which are based on different criteria.",3.5 Applying Error-correcting Codes,[0],[0]
"In this study, we use the decoding method described in Algorithm 3, where x ◦ y represents the concatenation of vectors x and y.",3.5 Applying Error-correcting Codes,[0],[0]
"This method is based on the Viterbi algorithm (Viterbi, 1967) and estimates original bits by directly using probability of redundant bits.",3.5 Applying Error-correcting Codes,[0],[0]
"Although Algorithm 3 looks complicated, this algorithm can be performed efficiently on CPUs at test time, and is not necessary at training time when we are simply performing calculation of Equation (6).",3.5 Applying Error-correcting Codes,[0],[0]
"Algorithm 2 increases the number of bits from B intoB′ = 2(B+6), but does not restrict the actual value of B.
4We also examined many configurations of convolutional codes which have different robustness and computation costs, and finally chose this one.",3.5 Applying Error-correcting Codes,[0],[0]
"We examined the performance of the proposed methods on two English-Japanese bidirectional translation tasks which have different translation difficulties: ASPEC (Nakazawa et al., 2016) and BTEC (Takezawa, 1999).",4.1 Experimental Settings,[0],[0]
Table 1 describes details of two corpora.,4.1 Experimental Settings,[0],[0]
"To prepare inputs for training, we used tokenizer.perl in Moses (Koehn et al., 2007) and KyTea (Neubig et al., 2011) for English/Japanese tokenizations respectively, applied lowercase.perl from Moses, and replaced out-of-vocabulary words such that rank(w) >",4.1 Experimental Settings,[0],[0]
"V − 3 into the UNK marker.
",4.1 Experimental Settings,[0],[0]
"We implemented each NMT model using C++ in the DyNet framework (Neubig et al., 2017) and trained/tested on 1 GPU (GeForce GTX TITAN X).",4.1 Experimental Settings,[0],[0]
Each test is also performed on CPUs to compare its processing time.,4.1 Experimental Settings,[0],[0]
"We used a bidirectional RNN-based encoder applied in Bahdanau et al. (2014), unidirectional decoder with the same style of (Luong et al., 2015), and the concat global attention model also proposed in Luong et al. (2015).",4.1 Experimental Settings,[0],[0]
"Each recurrent unit is constructed using a 1-layer LSTM (input/forget/output gates and nonpeepholes) (Gers et al., 2000) with 30% dropout (Srivastava et al., 2014) for the input/output vectors of the LSTMs.",4.1 Experimental Settings,[0],[0]
"All word embeddings, recurrent states and model-specific hidden states are designed with 512-dimentional vectors.",4.1 Experimental Settings,[0],[0]
"Only output layers and loss functions are replaced, and other network architectures are identical for the conventional/proposed models.",4.1 Experimental Settings,[0],[0]
"We used the Adam optimizer (Kingma and Ba, 2014) with fixed hyperparameters α = 0.001, β1 = 0.9β2 = 0.999, ε = 10−8, and mini-batches with 64 sentences sorted according to their sequence lengths.",4.1 Experimental Settings,[0],[0]
"For evaluating the quality of each model, we calculated case-insensitive BLEU (Papineni et al., 2002)",4.1 Experimental Settings,[0],[0]
every 1000 mini,4.1 Experimental Settings,[0],[0]
-,4.1 Experimental Settings,[0],[0]
batches.,4.1 Experimental Settings,[0],[0]
Table 2 lists summaries of all methods we examined in experiments.,4.1 Experimental Settings,[0],[0]
"Table 3 shows the BLEU on the test set (bold and italic faces indicate the best and second places in each task), number of bits B (or B′) for the binary code, actual size of the output layer #out, number of parameters in the output layer #W,β, as well as the ratio of #W,β or amount of whole parameters compared with Softmax, and averaged processing time at training (per mini-batch on GPUs) and test (per sentence on GPUs/CPUs), respectively.",4.2 Results and Discussion,[0],[0]
"Figure 5(a) and 5(b) shows training curves up to 180,000 epochs about some English→Japanese settings.",4.2 Results and Discussion,[0],[0]
"To relax instabilities of translation qualities while training (as shown in Figure 5(a) and 5(b)), each BLEU in Table 3 is calculated by averaging actual test BLEU of 5 consecutive results
Table 3:",4.2 Results and Discussion,[0],[0]
"Comparison of BLEU, size of output layers, number of parameters and processing time.
",4.2 Results and Discussion,[0],[0]
Corpus Method BLEU,4.2 Results and Discussion,[0],[0]
"% B #out #W,β
Ratio of #params Time (En→Ja)",4.2 Results and Discussion,[0],[0]
"[ms] EnJa JaEn #W,β All Train Test: GPU / CPU
ASPEC Softmax 31.13 21.14 — 65536 33.6 M 1/1 1 1026.",4.2 Results and Discussion,[0],[0]
121.6 / 2539.,4.2 Results and Discussion,[0],[0]
Binary 13.78 6.953 16 16 8.21 k,4.2 Results and Discussion,[0],[0]
1/4.10 k 0.698 711.2 73.08 / 122.3 Hybrid-512 22.81 13.95 16 528 271.,4.2 Results and Discussion,[0],[0]
k 1/124.,4.2 Results and Discussion,[0],[0]
0.700 843.6 81.28 / 127.5 Hybrid-2048 27.73 16.92 16 2064 1.06 M 1/31.8 0.707 837.1 82.28 / 159.3 Binary-EC 25.95 18.02 44 44 22.6 k 1/1.49 k 0.698 712.0 78.75 / 164.0 Hybrid-512-EC 29.07 18.66 44 556 285.,4.2 Results and Discussion,[0],[0]
k,4.2 Results and Discussion,[0],[0]
"1/118. 0.700 850.3 80.30 / 180.2 Hybrid-2048-EC 30.05 19.66 44 2092 1.07 M 1/31.4 0.707 851.6 77.83 / 201.3
BTEC Softmax 47.72 45.22 — 25000 12.8 M 1/1 1 325.0 34.35 / 323.3 Binary 31.83 31.90 15 15 7.70 k 1/1.67 k 0.738 250.7 27.98 / 54.62 Hybrid-512 44.23 43.50 15 527 270.",4.2 Results and Discussion,[0],[0]
k,4.2 Results and Discussion,[0],[0]
1/47.4 0.743,4.2 Results and Discussion,[0],[0]
300.7 28.83 / 66.13 Hybrid-2048 46.13 45.76 15 2063 1.06 M 1/12.1 0.759 307.7 28.25 / 67.40 Binary-EC 44.48 41.21 42 42 21.5 k 1/595. 0.738,4.2 Results and Discussion,[0],[0]
255.6 28.02 / 69.76 Hybrid-512-EC 47.20 46.52 42 554 284.,4.2 Results and Discussion,[0],[0]
"k 1/45.1 0.744 307.8 28.44 / 56.98 Hybrid-2048-EC 48.17 46.58 42 2090 1.07 M 1/12.0 0.760 311.0 28.47 / 69.44
Figure 6: BLEU changes in the Hybrid-N methods according to the softmax size (En→Ja).
",4.2 Results and Discussion,[0],[0]
"around the epoch that has the highest dev BLEU.
",4.2 Results and Discussion,[0],[0]
"First, we can see that each proposed method largely suppresses the actual size of the output layer from ten to one thousand times compared with the standard softmax.",4.2 Results and Discussion,[0],[0]
"By looking at the total number of parameters, we can see that the proposed models require only 70% of the actual memory, and the proposed model reduces the total number of parameters for the output layers to a practically negligible level.",4.2 Results and Discussion,[0],[0]
Note that most of remaining parameters are used for the embedding lookup at the input layer in both encoder/decoder.,4.2 Results and Discussion,[0],[0]
"These still occupy O(EV ) memory, where E represents the size of each embedding layer and usually O(E/H) = O(1).",4.2 Results and Discussion,[0],[0]
"These are not targets to be reduced in this study because these values rarely are accessed at test time because we only need to access them for input words, and do not need them to always be in the physical memory.",4.2 Results and Discussion,[0],[0]
"It might be
possible to apply a similar binary representation as that of output layers to the input layers as well, then express the word embedding by multiplying this binary vector by a word embedding matrix.",4.2 Results and Discussion,[0],[0]
"This is one potential avenue of future work.
",4.2 Results and Discussion,[0],[0]
"Taking a look at the BLEU for the simple Binary method, we can see that it is far lower than other models for all tasks.",4.2 Results and Discussion,[0],[0]
"This is expected, as described in Section 3, because using raw bit arrays causes many one-off estimation errors at the output layer due to the lack of robustness of the output representation.",4.2 Results and Discussion,[0],[0]
"In contrast, Hybrid-N and Binary-EC models clearly improve BLEU from Binary, and they approach that of Softmax.",4.2 Results and Discussion,[0],[0]
This demonstrates that these two methods effectively improve the robustness of binary code prediction models.,4.2 Results and Discussion,[0],[0]
"Especially, Binary-EC generally achieves higher quality than Hybrid-512 despite the fact that it suppress the number of parameters by about 1/10.",4.2 Results and Discussion,[0],[0]
These results show that introducing redundancy to target bit arrays is more effective than incremental prediction.,4.2 Results and Discussion,[0],[0]
"In addition, the Hybrid-NEC model achieves the highest BLEU in all proposed methods, and in particular, comparative or higher BLEU than Softmax in BTEC.",4.2 Results and Discussion,[0],[0]
"This behavior clearly demonstrates that these two methods are orthogonal, and combining them together can be effective.",4.2 Results and Discussion,[0],[0]
"We hypothesize that the lower quality of Softmax in BTEC is caused by an over-fitting due to the large number of parameters required in the softmax prediction.
",4.2 Results and Discussion,[0],[0]
The proposed methods also improve actual computation time in both training and test.,4.2 Results and Discussion,[0],[0]
"In particular on CPU, where the computation speed is directly affected by the size of the output layer, the proposed methods translate significantly faster
than Softmax by x5 to x20.",4.2 Results and Discussion,[0],[0]
"In addition, we can also see that applying error-correcting code is also effictive with respect to the decoding speed.
",4.2 Results and Discussion,[0],[0]
Figure 6 shows the trade-off between the translation quality and the size of softmax layers in the hybrid prediction model (Figure 2(c)) without error-correction.,4.2 Results and Discussion,[0],[0]
"According to the model definition in Section 3.4, the softmax prediction and raw binary code prediction can be assumed to be the upper/lower-bound of the hybrid prediction model.",4.2 Results and Discussion,[0],[0]
"The curves in Figure 6 move between Softmax and Binary models, and this behavior intuitively explains the characteristics of the hybrid prediction.",4.2 Results and Discussion,[0],[0]
"In addition, we can see that the BLEU score in BTEC quickly improves, and saturates at N = 1024 in contrast to the ASPEC model, which is still improving at N = 2048.",4.2 Results and Discussion,[0],[0]
"We presume that the shape of curves in Figure 6 is also affected by the difficulty of the corpus, i.e., when we train the hybrid model for easy datasets (e.g., BTEC is easier than ASPEC), it is enough to use a small softmax layer (e.g. N ≤ 1024).",4.2 Results and Discussion,[0],[0]
"In this study, we proposed neural machine translation models which indirectly predict output words via binary codes, and two model improvements: a hybrid prediction model using both softmax and binary codes, and introducing error-correcting codes to introduce robustness of binary code prediction.",5 Conclusion,[0],[0]
"Experiments show that the proposed model can achieve comparative translation qualities to standard softmax prediction, while significantly suppressing the amount of parameters in the output layer, and improving calculation speeds while training and especially testing.
",5 Conclusion,[0],[0]
One interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here.,5 Conclusion,[0],[0]
"In Algorithms 2 and 3 we use convolutions that were determined heuristically, and it is likely that learning these along with the model could result in improved accuracy or better compression capability.",5 Conclusion,[0],[0]
"Part of this work was supported by JSPS KAKENHI Grant Numbers JP16H05873 and JP17H00747, and Grant-in-Aid for JSPS Fellows Grant Number 15J10649.",Acknowledgments,[0],[0]
"In this paper, we propose a new method for calculating the output layer in neural machine translation systems.",abstractText,[0],[0]
The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case.,abstractText,[0],[0]
"In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes.",abstractText,[0],[0]
"Experiments on two English ↔ Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.",abstractText,[0],[0]
Neural Machine Translation via Binary Code Prediction,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2846–2852 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014) relies heavily on source representations, which encode implicitly semantic information of source words by neural networks (Mikolov et al., 2013a,b).",1 Introduction,[0],[0]
"Recently, several research works have been proposed to learn richer source representation, such as multisource information (Zoph and Knight, 2016; Firat et al., 2016), and particularly source syntactic information (Eriguchi et al., 2016; Li et al., 2017; Huadong et al., 2017; Eriguchi et al., 2017), thus improving the performance of NMT.
",1 Introduction,[0],[0]
"In this paper, we enhance source representations by dependency information, which can capture source long-distance dependency constraints for word prediction.",1 Introduction,[0],[0]
"Actually, source dependency information has been shown greatly effective in
∗Kehai Chen was an internship research fellow at NICT when conducting this work.
†Corresponding author.
",1 Introduction,[0],[0]
"Statistical Machine Translation (SMT) (Garmash and Monz, 2014; Kazemi et al., 2015; Hadiwinoto et al., 2016; Chen et al., 2017; Hadiwinoto and Ng, 2017).",1 Introduction,[0],[0]
"In NMT, there has been a quite recent preliminary exploration (Sennrich and Haddow, 2016), in which vector representations of source word and its dependency label are simply concatenated as source input, achieving state-ofthe-art performance in NMT (Bojar et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we propose a novel NMT with source dependency representation to improve translation performance.",1 Introduction,[0],[0]
"Compared with the simple approach of vector concatenation, we learn the Source Dependency Representation (SDR) to compute dependency context vectors and alignment matrices in a more sophisticated manner, which has the potential to make full use of source dependency information.",1 Introduction,[0],[0]
"To this end, we create a dependency unit for each source word to capture long-distance dependency constraints.",1 Introduction,[0],[0]
"Then we design an Encoder with convolutional architecture to jointly learn SDRs (Section 3) and source dependency annotations, thus computing dependency context vectors and hidden states by a novel double-context based Decoder for word prediction (Section 4).",1 Introduction,[0],[0]
"Empirical results on NIST Chinese-to-English translation task show that the proposed approach achieves significant gains over the method by Sennrich and Haddow (2016), and thus delivers substantial improvements over the standard attentional NMT (Section 5).",1 Introduction,[0],[0]
"An NMT model consists of an Encoder process and a Decoder process, and hence it is often called Encoder-Decoder model (Sutskever et al., 2014; Bahdanau et al., 2014).",2 Background,[0],[0]
"Typically, each unit of source input xj ∈ (x1, . . .",2 Background,[0],[0]
", xJ) is firstly embedded as a vector Vxj , and then represented as
2846
an annotation vector hj by
hj = fenc(Vxj , hj−1), (1)
where fenc is a bidirectional Recurrent Neural Network (RNN) (Bahdanau et al., 2014).",2 Background,[0],[0]
"These annotation vectors H = (h1, . . .",2 Background,[0],[0]
", hJ) are used to generate the target word in the Decoder.
",2 Background,[0],[0]
"An RNN Decoder is used to compute the target word yi probability by a softmax layer g:
p(yi|y<i, x) = g(ŷi−1, si, ci), (2)
where ŷi−1 is the previously emitted word, and si is an RNN hidden state for the current time step:
si = ϕ(ŷi−1, si−1, ci), (3)
and the context vector ci is computed as a weighted sum of these source annotations hj :
ci = J∑
j=1
αijhj , (4)
where the normalized alignment weight αij is computed by
αij = exp(eij)∑J
k=1 exp(eik) , (5)
where eij is an alignment which indicates how well the inputs around position j and the output at the position i match:
eij = f(si−1, hj).",2 Background,[0],[0]
"(6)
where f is a feedforward neural network.",2 Background,[0],[0]
"In order to capture source long-distance dependency constraints, we extract a dependency unit Uj for each source word xj from dependency tree, inspired by a dependency-based bilingual composition sequence for SMT (Chen et al., 2017).",3 Source Dependency Representation,[0],[0]
"The extracted Uj is defined as the following:
Uj = 〈PAxj , SIxj , CHxj 〉, (7)
where PAxj , SIxj , CHxj denote the parent, siblings and children words of source word xj in a dependency tree.",3 Source Dependency Representation,[0],[0]
"Take x2 in Figure 2 as an example, the blue solid box U2 denotes its dependency unit: PAx2 = 〈x3〉, SIx2 = 〈x1, x4, x7〉 and CHx2 = 〈ε〉 (no child), that is, U2 = 〈x3, x1, x4, x7, ε〉.
",3 Source Dependency Representation,[0],[0]
"We design a simplified neural network following Chen et al. (2017)’s Convolutional Neural Network (CNN) method, to learn the SDR for each source dependency unit Uj , as shown in Figure 1.",3 Source Dependency Representation,[0],[0]
"Our neural network consists of an input layer, two convolutional layers, two pooling layers and an output layer:
• Input layer: the input layer takes words of a dependency unitUj in the form of embedding vectors n×d, where n is the number of words in a dependency unit and d is vector dimension of each word.",3 Source Dependency Representation,[0],[0]
"In our experiments, we set n to 10,1 and d is 620.",3 Source Dependency Representation,[0],[0]
"For dependency units shorter than 10, we perform “/” padding at the ending of Uj .",3 Source Dependency Representation,[0],[0]
"For example, the padded U2 is 〈x3, x1, x4, x7, ε, /, /, /, /, /〉.
",3 Source Dependency Representation,[0],[0]
1We find that 99% of all the source dependency units contain no more than 10 words.,3 Source Dependency Representation,[0],[0]
"So if the length is more than 10, the extra words are abandoned; if the length is less than 10, the rest positions are padded with “/”.
",3 Source Dependency Representation,[0],[0]
"• Convolutional layer: the first convolution consists of one 3×d convolution kernels (the stride is 1) to output an (n-2)×d matrix; the second convolution consists of one 3×d convolution kernels to output a n−22 ×d matrix.
• Max-Pooling layer: the first pooling layer performs row-wise max over the two consecutive rows to output a n−24 ×d matrix; the second pooling layer performs row-wise max over the two consecutive rows to output a n−2
8 ×d matrix.",3 Source Dependency Representation,[0],[0]
"• Output layer: the output layer performs
row-wise average based on the output of the second pooling layer to learn a compact d-dimension vector VUj for Uj .",3 Source Dependency Representation,[0],[0]
"In our experiment, the output of the output layer is 1× d-dimension vector.
",3 Source Dependency Representation,[0],[0]
It should be noted that the dependency unit is similar to the source dependency feature of Sennrich and Haddow (2016) and the SDR is the same to the source-side representation of Chen et al. (2017).,3 Source Dependency Representation,[0],[0]
"In comparison with Sennrich and Haddow (2016), who concatenate the source dependency labels and word together to enhance the Encoder of NMT, we adapt a separate attention mechanism together with a CNN dependency Encoder.",3 Source Dependency Representation,[0],[0]
"Compared with Chen et al. (2017), which expands the famous neural network joint model (Devlin et al., 2014) with source dependency information to improve the phrase pair translation probability estimation for SMT, we focus on source dependency information to enhance attention probability estimation and to learn corresponding dependency context and RNN hidden state for improving translation.",3 Source Dependency Representation,[0],[0]
"In this section, we propose two novel NMT models SDRNMT-1 and SDRNMT-2, both of which can make use of source dependency information SDR to enhance Encoder and Decoder of NMT.
4.1 SDRNMT-1",4 NMT with SDR,[0],[0]
"Compared with standard attentional NMT, the Encoder of SDRNMT-1 model consists of a convolutional architecture and an bidirectional RNN, as shown in Figure 2.",4 NMT with SDR,[0],[0]
"Therefore, the proposed Encoder can not only learn compositional representations for dependency units but also
greatly tackle the sparsity issues associated with large dependency units.
",4 NMT with SDR,[0],[0]
"Motivated by (Sennrich and Haddow, 2016), we concatenate the Vxj and VUj as input of the Encoder, as shown in the black dotted box in Figure 2.",4 NMT with SDR,[0],[0]
"Source annotation vectors are learned based on the concatenated representation with dependency information:
hj = fenc(Vxj : VUj , hj−1), (8)
where “:” denotes the operation of vectors concatenation.",4 NMT with SDR,[0],[0]
"Finally, these learned annotation vectors are as the input of the standard NMT Decoder to jointly learn alignment and translation.",4 NMT with SDR,[0],[0]
"The only difference between our method and (Sennrich and Haddow, 2016)’s method is that they only use dependency label representation instead of VUj .
",4 NMT with SDR,[0],[0]
"4.2 SDRNMT-2
In SDRNMT-1, a single annotation, learned over concatenating word representation and SDR, is used to compute the context vector and the RNN hidden state for the current time step.",4 NMT with SDR,[0],[0]
"To relieve more translation performance for NMT from the SDR, we propose a double-context mechanism, as shown in Figure 3.
",4 NMT with SDR,[0],[0]
"First, the Encoder of SDRNMT-2 consists of two independent annotations hj and dj :
hj = fenc(Vxj , hj−1), dj = fenc(VUj , dj−1),
(9)
where H = [h1, · · · , hJ ] and D =",4 NMT with SDR,[0],[0]
"[d1, · · · , dJ ] encode source sequential and long-distance dependency information, respectively.
",4 NMT with SDR,[0],[0]
"The Decoder learns the corresponding alignment matrices and context vectors over the H and D, respectively.",4 NMT with SDR,[0],[0]
"That is, according to eq.(6), given the previous hidden state ssi−1 and s d i−1, the current alignments esi,j and e d i,j are computed over source annotation vectors hj and dj , respectively:
esi,j = f(s s i−1 + hj), edi,j = f(s d i−1 + dj).
(10)
",4 NMT with SDR,[0],[0]
"According to eq.(5), we further compute the current alignment α̃:
α̃i,j = exp(λesi,j + (1− λ)edi,j)∑J
j=1 exp(λe s",4 NMT with SDR,[0],[0]
"i,j + (1− λ)edi,j)
, (11)
where λ is a hyperparameter2 to control the importance of H and D. Note",4 NMT with SDR,[0],[0]
"that compared with the original alignment model only depending on the sequential annotation vectorsH , the alignment weight α̃i,j jointly compute statistic over source sequential annotation vectors H and dependency annotation vectors",4 NMT with SDR,[0],[0]
"D.
The current context vector csi and c d",4 NMT with SDR,[0],[0]
"i are
compute by eq.(4), respectively:
csi = J∑
j=1
α̃i,jhj , and cdi = J∑
j=1
α̃i,jdj .",4 NMT with SDR,[0],[0]
"(12)
",4 NMT with SDR,[0],[0]
"The current hidden state ssi and s d i are computed by eq.(3), respectively:
ssi = ϕ(s s i−1, yi−1, c s i ), sdi = ϕ(s d i−1, yi−1, c d i ).
",4 NMT with SDR,[0],[0]
"(13)
Finally, according to eq.(2), the probabilities for the next target word are computed using two hidden states ssi and s d i , the previously emitted word ŷi−1, the current sequential context vector csi and dependency context vector cdi :
p(yi|y<i, x, T ) =",4 NMT with SDR,[0],[0]
"g(ŷi−1, ssi , sdi , csi , cdi ).",4 NMT with SDR,[0],[0]
(14),4 NMT with SDR,[0],[0]
We carry out experiments on Chinese-to-English translation.,5.1 Setting up,[0],[0]
"The training dataset consists of 1.42M
2λ can be tuned according to a subset FBIS of training data and be set as 0.6 in the experiments.
sentence pairs extract from LDC corpora.3",5.1 Setting up,[0],[0]
"We use the Stanford dependency parser (Chang et al., 2009) to generate the dependency tree for Chinese.",5.1 Setting up,[0],[0]
"We choose the NIST 2002 (MT02) and the NIST 2003-2008 (MT03-08) datasets as the validation set and test sets, respectively.",5.1 Setting up,[0],[0]
"Case-insensitive 4- gram NIST BLEU score (Papineni et al., 2002) is used as an evaluation metric, and signtest (Collins et al., 2005) is as statistical significance test.
",5.1 Setting up,[0],[0]
"The baseline systems include the standard Phrase-Based Statistical Machine Translation (PBSMT) implemented in Moses (Koehn et al., 2007) and the standard Attentional NMT (AttNMT) (Bahdanau et al., 2014), where only source word representation is utilized.",5.1 Setting up,[0],[0]
"We also compare with a state-of-the-art syntax enhanced NMT method (Sennrich and Haddow, 2016).",5.1 Setting up,[0],[0]
"For a fair comparison, we only utilize dependency information for (Sennrich and Haddow, 2016), called Sennrich-deponly.",5.1 Setting up,[0],[0]
"We try our best to re-implement the baseline methods on Nematus toolkit 4 (Sennrich et al., 2017).
",5.1 Setting up,[0],[0]
"For all NMT systems, we limit the source and target vocabularies to 30K, and the maximum sentence length is 80.",5.1 Setting up,[0],[0]
"The word embedding dimension is 620,5 and the hidden layer dimension
3LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06.
",5.1 Setting up,[0],[0]
"4https://github.com/EdinburghNLP/nematus 5For SDRNMT-1, the 360 dimensions are from Vxj and
the 260 dimensions are from VUj .
is 1000, and all the layers use the dropout training technique (Hinton et al., 2012).",5.1 Setting up,[0],[0]
We shuffle training set before training and the mini-batch size is 80.,5.1 Setting up,[0],[0]
Training is conducted on a single Tesla P100 GPU.,5.1 Setting up,[0],[0]
"All NMT models train for 15 epochs using ADADELTA (Zeiler, 2012), and the train time is 6 days, which is 25% slower than the standard NMT.",5.1 Setting up,[0],[0]
Table 1 shows the translation performances on test sets measured in BLEU score.,5.2 Results and Analyses,[0],[0]
"The AttNMT significantly outperforms PBSMT by 2.74 BLEU points on average, indicating that it is a strong baseline NMT system.",5.2 Results and Analyses,[0],[0]
The baseline Sennrichdeponly improves the performance over the AttNMT by 0.58 BLEU points on average.,5.2 Results and Analyses,[0],[0]
"This indicates that the proposed source dependency constraint is beneficial for improving the performance of NMT.
",5.2 Results and Analyses,[0],[0]
"Moreover, SDRNMT-1 gains improvements of 0.92 and 0.34 BLEU points on average than the AttNMT and Sennrich-deponly.",5.2 Results and Analyses,[0],[0]
These show that the proposed SDR can more effectively capture source dependency information than vector concatenation.,5.2 Results and Analyses,[0],[0]
"Especially, the proposed SDRNMT2 outperforms the AttNMT and Sennrich-deponly on average by 1.64 and 1.03 BLEU points.",5.2 Results and Analyses,[0],[0]
These verify that the proposed double-context method is effective for word prediction.,5.2 Results and Analyses,[0],[0]
"We follow (Bahdanau et al., 2014) to group sentences of similar lengths all the test sets (MT03-08), for example, “40” indicates that the length of sentences is between 30 and 40, and compute a BLEU score per group.",5.3 Effect of Translating Long Sentences,[0],[0]
"As demonstrated in Figure 4, the proposed models outperform other baseline systems, especially in translating long sentences.",5.3 Effect of Translating Long Sentences,[0],[0]
These results show that the proposed models can effective encode longdistance dependencies to improve translation.,5.3 Effect of Translating Long Sentences,[0],[0]
"In this paper, we explored the source dependency information to improve the performance of NMT.",6 Conclusion and Future Work,[0],[0]
We proposed a novel attentional NMT with source dependency representation to capture source longdistance dependencies.,6 Conclusion and Future Work,[0],[0]
"In the future, we will try to exploit a general framework for utilizing richer syntax knowledge.",6 Conclusion and Future Work,[0],[0]
We are grateful to the anonymous reviewers for their insightful comments and suggestions.,Acknowledgments,[0],[0]
"This work is partially supported by the program “Promotion of Global Communications Plan: Research, Development, and Social Demonstration of Multilingual Speech Translation Technology” of MIC, Japan.",Acknowledgments,[0],[0]
Tiejun Zhao is supported by the National Natural Science Foundation of China (NSFC) via grant 91520204 and National High Technology Research & Development Program of China (863 program) via grant 2015AA015405.,Acknowledgments,[0],[0]
Source dependency information has been successfully introduced into statistical machine translation.,abstractText,[0],[0]
"However, there are only a few preliminary attempts for Neural Machine Translation (NMT), such as concatenating representations of source word and its dependency label together.",abstractText,[0],[0]
"In this paper, we propose a novel attentional NMT with source dependency representation to improve translation performance of NMT, especially on long sentences.",abstractText,[0],[0]
Empirical results on NIST Chinese-toEnglish translation task show that our method achieves 1.6 BLEU improvements on average over a strong NMT system.,abstractText,[0],[0]
Neural Machine Translation with Source Dependency Representation,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 125–135 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Neural Machine Translation (NMT) is an active area of research due to its outstanding empirical results (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014).",1 Introduction,[0],[0]
"Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy (Eriguchi et al., 2016b, 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016).",1 Introduction,[0],[0]
"The existing syntax-based NMT models employ a syntactic parser trained by supervised learning in advance, and hence the parser is not adapted to the translation tasks.",1 Introduction,[0],[0]
"An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences
All the calculated electronic band structures are metallic .
0.42 0.34 0.20
0.44
along with the target task (Socher et al., 2011; Yogatama et al., 2017).
",1 Introduction,[0],[0]
"Motivated by the promising results of recent joint learning approaches, we present a novel NMT model that can learn a task-specific latent graph structure for each source-side sentence.",1 Introduction,[0],[0]
"The graph structure is similar to the dependency structure of the sentence, but it can have cycles and is learned specifically for the translation task.",1 Introduction,[0],[0]
"Unlike the aforementioned approach of learning single syntactic trees, our latent graphs are composed of “soft” connections, i.e., the edges have realvalued weights (Figure 1).",1 Introduction,[0],[0]
"Our model consists of two parts: one is a task-independent parsing component, which we call a latent graph parser, and the other is an attention-based NMT model.",1 Introduction,[0],[0]
"The latent parser can be independently pre-trained with human-annotated treebanks and is then adapted to the translation task.
",1 Introduction,[0],[0]
"In experiments, we demonstrate that our model can be effectively pre-trained by the treebank annotations, outperforming a state-of-the-art sequential counterpart and a pipelined syntax-based model.",1 Introduction,[0],[0]
Our final ensemble model outperforms the previous best results by a large margin on the WAT English-to-Japanese dataset.,1 Introduction,[0],[0]
We model the latent graph parser based on dependency parsing.,2 Latent Graph Parser,[0],[0]
"In dependency parsing, a sentence is represented as a tree structure where each node corresponds to a word in the sentence and
125
a unique root node (ROOT) is added.",2 Latent Graph Parser,[0],[0]
"Given a sentence of length N , the parent node Hwi ∈ {w1, . . .",2 Latent Graph Parser,[0],[0]
", wN , ROOT} (Hwi 6= wi) of each word wi (1 ≤ i ≤ N) is called its head.",2 Latent Graph Parser,[0],[0]
"The sentence is thus represented as a set of tuples (wi, Hwi , `wi), where `wi is a dependency label.
",2 Latent Graph Parser,[0],[0]
"In this paper, we remove the constraint of using the tree structure and represent a sentence as a set of tuples (wi, p(Hwi |wi), p(`wi |wi)), where p(Hwi |wi) is the probability distribution of wi’s parent nodes, and p(`wi |wi) is the probability distribution of the dependency labels.",2 Latent Graph Parser,[0],[0]
"For example, p(Hwi = wj |wi) is the probability that wj is the parent node of wi.",2 Latent Graph Parser,[0],[0]
"Here, we assume that a special token 〈EOS〉 is appended to the end of the sentence, and we treat the 〈EOS〉 token as ROOT.",2 Latent Graph Parser,[0],[0]
"This approach is similar to that of graph-based dependency parsing (McDonald et al., 2005) in that a sentence is represented with a set of weighted arcs between the words.",2 Latent Graph Parser,[0],[0]
"To obtain the latent graph representation of the sentence, we use a dependency parsing model based on multi-task learning proposed by Hashimoto et al. (2017).",2 Latent Graph Parser,[0],[0]
The i-th input word wi is represented with the concatenation of its d1-dimensional word embedding vdp(wi) ∈ Rd1 and its character n-gram embedding c(wi) ∈ Rd1 : x(wi) =,2.1 Word Representation,[0],[0]
[vdp(wi); c(wi)].,2.1 Word Representation,[0],[0]
c(wi) is computed as the average of the embeddings of the character n-grams in wi.,2.1 Word Representation,[0],[0]
"Our latent graph parser builds upon multilayer bi-directional Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units (Graves and Schmidhuber, 2005).",2.2 POS Tagging Layer,[0],[0]
"In the first layer, POS tagging is handled by computing a hidden state h(1)i =",2.2 POS Tagging Layer,[0],[0]
[ −→ h (1) i ; ←− h (1),2.2 POS Tagging Layer,[0],[0]
"i ] ∈ R2d1 for wi,
where −→ h
(1)",2.2 POS Tagging Layer,[0],[0]
"i = LSTM(
−→ h
(1) i−1, x(wi)) ∈ Rd1 and←−
h (1)",2.2 POS Tagging Layer,[0],[0]
"i = LSTM(
←− h
(1) i+1, x(wi))",2.2 POS Tagging Layer,[0],[0]
"∈ Rd1 are hidden
states of the forward and backward LSTMs, respectively.",2.2 POS Tagging Layer,[0],[0]
"h(1)i is then fed into a softmax classifier to predict a probability distribution p(1)i ∈ RC (1) for word-level tags, where C(1) is the number of POS classes.",2.2 POS Tagging Layer,[0],[0]
"The model parameters of this layer can be learned not only by human-annotated data, but also by backpropagation from higher layers, which are described in the next section.",2.2 POS Tagging Layer,[0],[0]
Dependency parsing is performed in the second layer.,2.3 Dependency Parsing Layer,[0],[0]
"A hidden state h(2)i ∈ R2d1 is computed by −→ h
(2)",2.3 Dependency Parsing Layer,[0],[0]
"i = LSTM(
−→ h
(2) i−1, [x(wi); y(wi);
−→ h
(1) i ])
and ←− h
(2)",2.3 Dependency Parsing Layer,[0],[0]
"i = LSTM(
←− h
(2) i+1, [x(wi); y(wi);
←− h
(1) i ]),
where y(wi) = W (1) ` p (1)",2.3 Dependency Parsing Layer,[0],[0]
"i ∈ Rd2 is the POS information output from the first layer, and W (1)` ∈ Rd2×C(1) is a weight matrix.
",2.3 Dependency Parsing Layer,[0],[0]
"Then, (soft) edges of our latent graph representation are obtained by computing the probabilities
p(Hwi = wj |wi)",2.3 Dependency Parsing Layer,[0],[0]
"= exp (m(i, j))∑
k 6=i exp (m(i, k)) , (1)
where m(i, k) = h(2)Tk",2.3 Dependency Parsing Layer,[0],[0]
"Wdph (2) i (1 ≤ k ≤ N + 1, k 6= i) is a scoring function with a weight matrix Wdp ∈",2.3 Dependency Parsing Layer,[0],[0]
R2d1×2d1 .,2.3 Dependency Parsing Layer,[0],[0]
"While the models of Hashimoto et al. (2017), Zhang et al. (2017), and Dozat and Manning (2017) learn the model parameters of their parsing models only by humanannotated data, we allow the model parameters to be learned by the translation task.
",2.3 Dependency Parsing Layer,[0],[0]
"Next, [h(2)i ; z(Hwi)] is fed into a softmax classifier to predict the probability distribution p(`wi |wi), where z(Hwi) ∈ R2d1 is the weighted average of the hidden states of the parent nodes: ∑ j 6=i p(Hwi = wj |wi)h(2)j .",2.3 Dependency Parsing Layer,[0],[0]
"This results in the latent graph representation (wi, p(Hwi |wi), p(`wi |wi)) of the input sentence.",2.3 Dependency Parsing Layer,[0],[0]
"The latent graph representation described in Section 2 can be used for any sentence-level tasks, and here we apply it to an Attention-based NMT (ANMT) model (Luong et al., 2015).",3 NMT with Latent Graph Parser,[0],[0]
We modify the encoder and the decoder in the ANMT model to learn the latent graph representation.,3 NMT with Latent Graph Parser,[0],[0]
The ANMT model first encodes the information about the input sentence and then generates a sentence in another language.,3.1 Encoder with Dependency Composition,[0],[0]
The encoder represents the word wi with a word embedding venc(wi) ∈ Rd3 .,3.1 Encoder with Dependency Composition,[0],[0]
It should be noted that venc(wi) is different from vdp(wi) because each component is separately modeled.,3.1 Encoder with Dependency Composition,[0],[0]
"The encoder then takes the word embedding venc(wi) and the hidden state h (2) i as the input to a uni-directional LSMT:
h (enc)",3.1 Encoder with Dependency Composition,[0],[0]
"i = LSTM(h (enc) i−1 , [venc(wi); h (2) i ]), (2)
where h(enc)i ∈ Rd3 is the hidden state corresponding to wi.",3.1 Encoder with Dependency Composition,[0],[0]
"That is, the encoder of our model is a three-layer LSTM network, where the first two layers are bi-directional.
",3.1 Encoder with Dependency Composition,[0],[0]
"In the sequential LSTMs, relationships between words in distant positions are not explicitly considered.",3.1 Encoder with Dependency Composition,[0],[0]
"In our model, we explicitly incorporate such relationships into the encoder by defining a dependency composition function:
dep(wi) = tanh(Wdep[henci ; h(Hwi); p(`wi |wi)]), (3)
where h(Hwi) = ∑
j 6=i p(Hwi = wj |wi)h(enc)j is the weighted average of the hidden states of the parent nodes.
",3.1 Encoder with Dependency Composition,[0],[0]
"Note on character n-gram embeddings In NMT models, sub-word units are widely used to address rare or unknown word problems (Sennrich et al., 2016).",3.1 Encoder with Dependency Composition,[0],[0]
"In our model, the character n-gram embeddings are fed through the latent graph parsing component.",3.1 Encoder with Dependency Composition,[0],[0]
"To the best of our knowledge, the character n-gram embeddings have never been used in NMT models.",3.1 Encoder with Dependency Composition,[0],[0]
"Wieting et al. (2016), Bojanowski et al. (2017), and Hashimoto et al. (2017) have reported that the character n-gram embeddings are useful in improving several NLP tasks by better handling unknown words.",3.1 Encoder with Dependency Composition,[0],[0]
"The decoder of our model is a single-layer LSTM network, and the initial state is set with h(enc)N+1 and its corresponding memory cell.",3.2 Decoder with Attention Mechanism,[0],[0]
"Given the t-th hidden state h(dec)t ∈ Rd3 , the decoder predicts the t-th word in the target language using an attention mechanism.",3.2 Decoder with Attention Mechanism,[0],[0]
"The attention mechanism in Luong et al. (2015) computes the weighted average of the hidden states h(enc)i of the encoder:
s(i, t) = exp (h (dec) t ·h(enc)i )∑N+1
j=1 exp (h (dec) t ·h(enc)j )
, (4)
at = ∑N+1 i=1",3.2 Decoder with Attention Mechanism,[0],[0]
"s(i, t)h (enc) i , (5)
where s(i, t) is a scoring function which specifies how much each source-side hidden state contributes to the word prediction.
",3.2 Decoder with Attention Mechanism,[0],[0]
"In addition, like the attention mechanism over constituency tree nodes (Eriguchi et al., 2016b), our model uses attention to the dependency composition vectors:
s′(i, t) = exp (h (dec) t ·dep(wi))∑N
j=1 exp (h (dec) t ·dep(wj))
, (6)
a′t = ∑N i=1 s ′(i, t)dep(wi), (7)
To predict the target word, a hidden state h̃(dec)t ∈ Rd3 is then computed as follows:
h̃ (dec) t = tanh(W̃ [h (dec) t ; at; a ′ t]), (8)
where W̃ ∈ Rd3×3d3 is a weight matrix.",3.2 Decoder with Attention Mechanism,[0],[0]
h̃(dec)t is fed into a softmax classifier to predict a target word distribution.,3.2 Decoder with Attention Mechanism,[0],[0]
"h̃(dec)t is also used in the transition of the decoder LSTMs along with a word embedding vdec(wt) ∈ Rd3 of the target word wt:
h (dec) t+1 = LSTM(h (dec) t , [vdec(wt); h̃ (dec) t ]), (9)
where the use of h̃(dec)t is called input feeding proposed by Luong et al. (2015).
",3.2 Decoder with Attention Mechanism,[0],[0]
"The overall model parameters, including those of the latent graph parser, are jointly learned by minimizing the negative log-likelihood of the prediction probabilities of the target words in the training data.",3.2 Decoder with Attention Mechanism,[0],[0]
"To speed up the training, we use BlackOut sampling (Ji et al., 2016).",3.2 Decoder with Attention Mechanism,[0],[0]
"By this joint learning using Equation (3) and (7), the latent graph representations are automatically learned according to the target task.
",3.2 Decoder with Attention Mechanism,[0],[0]
"Implementation Tips Inspired by Zoph et al. (2016), we further speed up BlackOut sampling by sharing noise samples across words in the same sentences.",3.2 Decoder with Attention Mechanism,[0],[0]
"This technique has proven to be effective in RNN language modeling, and we have found that it is also effective in the NMT model.",3.2 Decoder with Attention Mechanism,[0],[0]
"We have also found it effective to share the model parameters of the target word embeddings and the softmax weight matrix for word prediction (Inan et al., 2016; Press and Wolf, 2017).",3.2 Decoder with Attention Mechanism,[0],[0]
"Also, we have found that a parameter averaging technique (Hashimoto et al., 2013) is helpful in improving translation accuracy.
",3.2 Decoder with Attention Mechanism,[0],[0]
"Translation At test time, we use a novel beam search algorithm which combines statistics of sentence lengths (Eriguchi et al., 2016b) and length normalization (Cho et al., 2014).",3.2 Decoder with Attention Mechanism,[0],[0]
"During the beam search step, we use the following scoring function for a generated word sequence y = (y1, y2, . . .",3.2 Decoder with Attention Mechanism,[0],[0]
", yLy)",3.2 Decoder with Attention Mechanism,[0],[0]
given a source word sequence x =,3.2 Decoder with Attention Mechanism,[0],[0]
"(x1, x2, . . .",3.2 Decoder with Attention Mechanism,[0],[0]
", xLx):
1 Ly  Ly∑ i=1",3.2 Decoder with Attention Mechanism,[0],[0]
"log p(yi|x, y1:i−1) + log p(Ly|Lx)  , (10)
where p(Ly|Lx) is the probability that sentences of length Ly are generated given source-side sentences of length Lx.",3.2 Decoder with Attention Mechanism,[0],[0]
The statistics are taken by using the training data in advance.,3.2 Decoder with Attention Mechanism,[0],[0]
"In our experiments, we have empirically found that this beam search algorithm helps the NMT models to avoid generating translation sentences that are too short.",3.2 Decoder with Attention Mechanism,[0],[0]
"We used an English-to-Japanese translation task of the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016b) used in the Workshop on Asian Translation (WAT), since it has been shown that syntactic information is useful in English-to-Japanese translation (Eriguchi et al., 2016b; Neubig et al., 2015).",4.1 Data,[0],[0]
We followed the data preprocessing instruction for the English-toJapanese task in Eriguchi et al. (2016b).,4.1 Data,[0],[0]
"The English sentences were tokenized by the tokenizer in the Enju parser (Miyao and Tsujii, 2008), and the Japanese sentences were segmented by the KyTea tool1.",4.1 Data,[0],[0]
"Among the first 1,500,000 translation pairs in the training data, we selected 1,346,946 pairs where the maximum sentence length is 50.",4.1 Data,[0],[0]
"In what follows, we call this dataset the large training dataset.",4.1 Data,[0],[0]
"We further selected the first 20,000 and 100,000 pairs to construct the small and medium training datasets, respectively.",4.1 Data,[0],[0]
"The development data include 1,790 pairs, and the test data 1,812 pairs.
",4.1 Data,[0],[0]
"For the small and medium datasets, we built the vocabulary with words whose minimum frequency is two, and for the large dataset, we used words whose minimum frequency is three for English and five for Japanese.",4.1 Data,[0],[0]
"As a result, the vocabulary of the target language was 8,593 for the small dataset, 23,532 for the medium dataset, and 65,680 for the large dataset.",4.1 Data,[0],[0]
A special token 〈UNK〉 was used to replace words which were not included in the vocabularies.,4.1 Data,[0],[0]
"The character ngrams (n = 2, 3, 4) were also constructed from each training dataset with the same frequency settings.",4.1 Data,[0],[0]
We turned hyper-parameters of the model using development data.,4.2 Parameter Optimization and Translation,[0],[0]
"We set (d1, d2) = (100, 50) for the latent graph parser.",4.2 Parameter Optimization and Translation,[0],[0]
"The word and character n-gram embeddings of the latent graph parser
1http://www.phontron.com/kytea/.
were initialized with the pre-trained embeddings in Hashimoto et al. (2017).2 The weight matrices in the latent graph parser were initialized with uniform random values in [− √ 6√
row+col , +
√ 6√
row+col ],
where row and col are the number of rows and columns of the matrices, respectively.",4.2 Parameter Optimization and Translation,[0],[0]
"All the bias vectors and the weight matrices in the softmax layers were initialized with zeros, and the bias vectors of the forget gates in the LSTMs were initialized by ones (Jozefowicz et al., 2015).
",4.2 Parameter Optimization and Translation,[0],[0]
"We set d3 = 128 for the small training dataset, d3 = 256 for the medium training dataset, and d3 = 512 for the large training dataset.",4.2 Parameter Optimization and Translation,[0],[0]
"The word embeddings and the weight matrices of the NMT model were initialized with uniform random values in [−0.1, +0.1].",4.2 Parameter Optimization and Translation,[0],[0]
The training was performed by mini-batch stochastic gradient descent with momentum.,4.2 Parameter Optimization and Translation,[0],[0]
"For the BlackOut objective (Ji et al., 2016), the number of the negative samples was set to 2,000 for the small and medium training datasets, and 2,500 for the large training dataset.",4.2 Parameter Optimization and Translation,[0],[0]
"The mini-batch size was set to 128, and the momentum rate was set to 0.75 for the small and medium training datasets and 0.70 for the large training dataset.",4.2 Parameter Optimization and Translation,[0],[0]
A gradient clipping technique was used with a clipping value of 1.0.,4.2 Parameter Optimization and Translation,[0],[0]
"The initial learning rate was set to 1.0, and the learning rate was halved when translation accuracy decreased.",4.2 Parameter Optimization and Translation,[0],[0]
We used the BLEU scores obtained by greedy translation as the translation accuracy and checked it at every half epoch of the model training.,4.2 Parameter Optimization and Translation,[0],[0]
We saved the model parameters at every half epoch and used the saved model parameters for the parameter averaging technique.,4.2 Parameter Optimization and Translation,[0],[0]
"For regularization, we used L2-norm regularization with a coefficient of 10−6 and applied dropout (Hinton et al., 2012) to Equation (8) with a dropout rate of 0.2.
",4.2 Parameter Optimization and Translation,[0],[0]
"The beam size for the beam search algorithm was 12 for the small and medium training datasets, and 50 for the large training dataset.",4.2 Parameter Optimization and Translation,[0],[0]
"We used BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), and perplexity scores as our evaluation metrics.",4.2 Parameter Optimization and Translation,[0],[0]
Note that lower perplexity scores indicate better accuracy.,4.2 Parameter Optimization and Translation,[0],[0]
The latent graph parser in our model can be optionally pre-trained by using human annotations for dependency parsing.,4.3 Pre-Training of Latent Graph Parser,[0],[0]
"In this paper we used
2The pre-trained embeddings can be found at https: //github.com/hassyGo/charNgram2vec.
the widely-used Wall Street Journal (WSJ) training data to jointly train the POS tagging and dependency parsing components.",4.3 Pre-Training of Latent Graph Parser,[0],[0]
We used the standard training split (Section 0-18) for POS tagging.,4.3 Pre-Training of Latent Graph Parser,[0],[0]
We followed Chen and Manning (2014) to generate the training data (Section 2-21) for dependency parsing.,4.3 Pre-Training of Latent Graph Parser,[0],[0]
"From each training dataset, we selected the first K sentences to pre-train our model.",4.3 Pre-Training of Latent Graph Parser,[0],[0]
"The training dataset for POS tagging includes 38,219 sentences, and that for dependency parsing includes 39,832 sentences.
",4.3 Pre-Training of Latent Graph Parser,[0],[0]
"The parser including the POS tagger was first trained for 10 epochs in advance according to the multi-task learning procedure of Hashimoto et al. (2017), and then the overall NMT model was trained.",4.3 Pre-Training of Latent Graph Parser,[0],[0]
"When pre-training the POS tagging and dependency parsing components, we did not apply dropout to the model and did not fine-tune the word and character n-gram embeddings to avoid strong overfitting.",4.3 Pre-Training of Latent Graph Parser,[0],[0]
"LGP-NMT is our proposed model that learns the Latent Graph Parsing for NMT.
",4.4 Model Configurations,[0],[0]
"LGP-NMT+ is constructed by pre-training the latent parser in LGP-NMT as described in Section 4.3.
",4.4 Model Configurations,[0],[0]
"SEQ is constructed by removing the dependency composition in Equation (3), forming a sequential NMT model with the multi-layer encoder.
",4.4 Model Configurations,[0],[0]
DEP is constructed by using pre-trained dependency relations rather than learning them.,4.4 Model Configurations,[0],[0]
"That is, p(Hwi = wj |wi) is fixed to 1.0 such that wj is the head of wi.",4.4 Model Configurations,[0],[0]
"The dependency labels are also given by the parser which was trained by using all the training samples for parsing and tagging.
",4.4 Model Configurations,[0],[0]
UNI is constructed by fixing p(Hwi = wj |wi) to 1 N for all the words in the same sentence.,4.4 Model Configurations,[0],[0]
"That is, the uniform probability distributions are used for equally connecting all the words.",4.4 Model Configurations,[0],[0]
We first show our translation results using the small and medium training datasets.,5 Results on Small and Medium Datasets,[0],[0]
We report averaged scores with standard deviations across five different runs of the model training.,5 Results on Small and Medium Datasets,[0],[0]
Table 1 shows the results of using the small training dataset.,5.1 Small Training Dataset,[0],[0]
"LGP-NMT performs worse than SEQ
and UNI, which shows that the small training dataset is not enough to learn useful latent graph structures from scratch.",5.1 Small Training Dataset,[0],[0]
"However, LGP-NMT+ (K = 10,000) outperforms SEQ and UNI, and the standard deviations are the smallest.",5.1 Small Training Dataset,[0],[0]
"Therefore, the results suggest that pre-training the parsing and tagging components can improve the translation accuracy of our proposed model.",5.1 Small Training Dataset,[0],[0]
We can also see that DEP performs the worst.,5.1 Small Training Dataset,[0],[0]
"This is not surprising because previous studies, e.g., Li et al. (2015), have reported that using syntactic structures do not always outperform competitive sequential models in several NLP tasks.
",5.1 Small Training Dataset,[0],[0]
"Now that we have observed the effectiveness of pre-training our model, one question arises naturally:
how many training samples for parsing and tagging are necessary for improving the translation accuracy?
",5.1 Small Training Dataset,[0],[0]
Table 2 shows the results of using different numbers of training samples for parsing and tagging.,5.1 Small Training Dataset,[0],[0]
"The results of K= 0 and K= 10,000 correspond to those of LGP-NMT and LGP-NMT+ in Table 1, respectively.",5.1 Small Training Dataset,[0],[0]
We can see that using the small amount of the training samples performs better than using all the training,5.1 Small Training Dataset,[0],[0]
samples.3,5.1 Small Training Dataset,[0],[0]
One possible reason is that the domains of the translation dataset and the parsing (tagging) dataset are considerably different.,5.1 Small Training Dataset,[0],[0]
"The parsing and tagging datasets come from WSJ, whereas the translation dataset comes from abstract text of scientific papers in a wide range of domains, such as
3We did not observe such significant difference when using the larger datasets, and we used all the training samples in the remaining part of this paper.
biomedicine and computer science.",5.1 Small Training Dataset,[0],[0]
These results suggest that our model can be improved by a small amount of parsing and tagging datasets in different domains.,5.1 Small Training Dataset,[0],[0]
"Considering the recent universal dependency project4 which covers more than 50 languages, our model has the potential of being applied to a variety of language pairs.",5.1 Small Training Dataset,[0],[0]
Table 3 shows the results of using the medium training dataset.,5.2 Medium Training Dataset,[0],[0]
"In contrast with using the small training dataset, LGP-NMT is slightly better than SEQ.",5.2 Medium Training Dataset,[0],[0]
"LGP-NMT significantly outperforms UNI, which shows that our adaptive learning is more effective than using the uniform graph weights.",5.2 Medium Training Dataset,[0],[0]
"By pre-training our model, LGP-NMT+ significantly outperforms SEQ in terms of the BLEU score.",5.2 Medium Training Dataset,[0],[0]
"Again, DEP performs the worst among all the models.
",5.2 Medium Training Dataset,[0],[0]
"By using our beam search strategy, the Brevity Penalty (BP) values of our translation results are equal to or close to 1.0, which is important when evaluating the translation results using the BLEU scores.",5.2 Medium Training Dataset,[0],[0]
"A BP value ranges from 0.0 to 1.0, and larger values mean that the translated sentences have relevant lengths compared with the reference translations.",5.2 Medium Training Dataset,[0],[0]
"As a result, our BLEU evaluation results are affected only by the word n-gram precision scores.",5.2 Medium Training Dataset,[0],[0]
"BLEU scores are sensitive to the BP values, and thus our beam search strategy leads to more solid evaluation for NMT models.",5.2 Medium Training Dataset,[0],[0]
Table 4 shows the BLEU and RIBES scores on the development data achieved with the large training dataset.,6 Results on Large Dataset,[0],[0]
Here we focus on our models and SEQ because UNI and DEP consistently perform worse than the other models as shown in Table 1 and 3.,6 Results on Large Dataset,[0],[0]
"The averaging technique and attentionbased unknown word replacement (Jean et al., 2015; Hashimoto et al., 2016) improve the scores.
",6 Results on Large Dataset,[0],[0]
"4http://universaldependencies.org/.
Again, we see that the translation scores of our model can be further improved by pre-training the model.
",6 Results on Large Dataset,[0],[0]
"Table 5 shows our results on the test data, and the previous best results summarized in Nakazawa et al. (2016a) and the WAT website5 are also shown.",6 Results on Large Dataset,[0],[0]
"Our proposed models, LGP-NMT and LGP-NMT+, outperform not only SEQ but also all of the previous best results.",6 Results on Large Dataset,[0],[0]
"Notice also that our implementation of the sequential model (SEQ) provides a very strong baseline, the performance of which is already comparable to the previous state of the art, even without using ensemble techniques.",6 Results on Large Dataset,[0],[0]
"The confidence interval (p ≤ 0.05) of the RIBES score of LGP-NMT+ estimated by bootstrap resampling (Noreen, 1989) is (82.27, 83.37), and thus the RIBES score of LGP-NMT+ is significantly better than that of SEQ, which shows that our latent parser can be effectively pre-trained with the human-annotated treebank.
",6 Results on Large Dataset,[0],[0]
The sequential NMT model in Cromieres et al. (2016) and the tree-to-sequence NMT model in Eriguchi et al. (2016b) rely on ensemble techniques while our results mentioned above are obtained using single models.,6 Results on Large Dataset,[0],[0]
"Moreover, our model is more compact6 than the previous best NMT model in Cromieres et al. (2016).",6 Results on Large Dataset,[0],[0]
"By applying the ensemble technique to LGP-NMT, LGP-NMT+,
5http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation/list.php?t=1&o=1.
6Our training time is within five days on a c4.8xlarge machine of Amazon Web Service by our CPU-based C++ code, while it is reported that the training time is more than two weeks in Cromieres et al. (2016) by their GPU code.
and SEQ, the BLEU and RIBES scores are further improved, and both of the scores are significantly better than the previous best scores.",6 Results on Large Dataset,[0],[0]
"Figure 2 shows two translation examples7 to see how the proposed model works and what is missing in the state-of-the-art sequential NMT model, SEQ.",6.1 Analysis on Translation Examples,[0],[0]
"Besides the reference translation, the outputs of our models with and without pre-training, SEQ, and Google Translation8 are shown.
",6.1 Analysis on Translation Examples,[0],[0]
"Selectional Preference In the translation example (1) in Figure 2, we see that the adverb “obliquely” is interpreted differently across the systems.",6.1 Analysis on Translation Examples,[0],[0]
"As in the reference translation, “obliquely” is a modifier of the verb “crosses”.",6.1 Analysis on Translation Examples,[0],[0]
"Our models correctly capture the relationship between the two words, whereas Google Translation and SEQ treat “obliquely” as a modifier of the verb “existed”.",6.1 Analysis on Translation Examples,[0],[0]
This error is not a surprise since the verb “existed” is located closer to “obliquely” than the verb “crosses”.,6.1 Analysis on Translation Examples,[0],[0]
A possible reason for the correct interpretation by our models is that they can better capture long-distance dependencies and are less susceptible to surface word distances.,6.1 Analysis on Translation Examples,[0],[0]
"This is an indication of our models’ ability of capturing domain-specific selectional preference that cannot be captured by purely sequential
7These English sentences were created by manual simplification of sentences in the development data.
",6.1 Analysis on Translation Examples,[0],[0]
"8The translations were obtained at https: //translate.google.com in Feb. and Mar. 2017.
models.",6.1 Analysis on Translation Examples,[0],[0]
"It should be noted that simply using standard treebank-based parsers does not necessarily address this error, because our pre-trained dependency parser interprets that “obliquely” is a modifier of the verb “existed”.
",6.1 Analysis on Translation Examples,[0],[0]
Adverb or Adjective,6.1 Analysis on Translation Examples,[0],[0]
The translation example (2) in Figure 2 shows another example where the adverb “negatively” is interpreted as an adverb or an adjective.,6.1 Analysis on Translation Examples,[0],[0]
"As in the reference translation, “negatively” is a modifier of the verb “controls”.",6.1 Analysis on Translation Examples,[0],[0]
"Only LGP-NMT+ correctly captures the adverb-verb relationship, whereas “negatively” is interpreted as the adjective “negative” to modify the noun “ImRNA” in the translation results from Google Translation and LGP-NMT.",6.1 Analysis on Translation Examples,[0],[0]
"SEQ interprets “negatively” as both an adverb and an adjective, which leads to the repeated translations.",6.1 Analysis on Translation Examples,[0],[0]
This error suggests that the state-of-the-art NMT models are strongly affected by the word order.,6.1 Analysis on Translation Examples,[0],[0]
"By contrast, the pre-training strategy effectively embeds the information about the POS tags and the dependency relations into our model.",6.1 Analysis on Translation Examples,[0],[0]
Without Pre-Training We inspected the latent graphs learned by LGP-NMT.,6.2 Analysis on Learned Latent Graphs,[0],[0]
Figure 1 shows an example of the learned latent graph obtained for a sentence taken from the development data of the translation task.,6.2 Analysis on Learned Latent Graphs,[0],[0]
It has long-range dependencies and cycles as well as ordinary left-to-right dependencies.,6.2 Analysis on Learned Latent Graphs,[0],[0]
We have observed that the punctuation mark “.” is often pointed to by other words with large weights.,6.2 Analysis on Learned Latent Graphs,[0],[0]
"This is primarily because the hidden state corresponding to the mark in each sentence has rich information about the sentence.
",6.2 Analysis on Learned Latent Graphs,[0],[0]
"To measure the correlation between the latent graphs and human-defined dependencies, we parsed the sentences on the development data of the WSJ corpus and converted the graphs into dependency trees by Eisner’s algorithm (Eisner, 1996).",6.2 Analysis on Learned Latent Graphs,[0],[0]
"For evaluation, we followed Chen and Manning (2014) and measured Unlabeled Attachment Score (UAS).",6.2 Analysis on Learned Latent Graphs,[0],[0]
"The UAS is 24.52%, which shows that the implicitly-learned latent graphs are partially consistent with the human-defined syntactic structures.",6.2 Analysis on Learned Latent Graphs,[0],[0]
Similar trends have been reported by Yogatama et al. (2017) in the case of binary constituency parsing.,6.2 Analysis on Learned Latent Graphs,[0],[0]
We checked the most dominant gold dependency labels which were assigned for the dependencies detected by LGPNMT.,6.2 Analysis on Learned Latent Graphs,[0],[0]
"The labels whose ratio is more than 3% are
All the calculated electronic band structures are metallic .",6.2 Analysis on Learned Latent Graphs,[0],[0]
"0.86 0.97 0.99
1.0
0.85 1.0
All the calculated electronic band structures are metallic .
0.26 0.60 0.99
0.86 1.0
ROOT 1.0
0.23
0.95 0.82
(a)
(b)
nn, amod, prep, pobj, dobj, nsubj, num, det, advmod, and poss.",6.2 Analysis on Learned Latent Graphs,[0],[0]
"We see that dependencies between words in distant positions, such as subject-verb-object relations, can be captured.
",6.2 Analysis on Learned Latent Graphs,[0],[0]
With Pre-Training We also inspected the pretrained latent graphs.,6.2 Analysis on Learned Latent Graphs,[0],[0]
Figure 3-(a) shows the dependency structure output by the pre-trained latent parser for the same sentence in Figure 1.,6.2 Analysis on Learned Latent Graphs,[0],[0]
"This is an ordinary dependency tree, and the head selection is almost deterministic; that is, for each word, the largest weight of the head selection is close to 1.0.",6.2 Analysis on Learned Latent Graphs,[0],[0]
"By contrast, the weight values are more evenly distributed in the case of LGP-NMT as shown in Figure 1.",6.2 Analysis on Learned Latent Graphs,[0],[0]
"After the overall NMT model training, the latent parser is adapted to the translation task, and Figure 3-(b) shows the adapted latent graph.",6.2 Analysis on Learned Latent Graphs,[0],[0]
"Again, we can see that the adapted weight values are also distributed and different from the original pre-trained weight values, which suggests that human-defined syntax is not always optimal for the target task.
",6.2 Analysis on Learned Latent Graphs,[0],[0]
"The UAS of the pre-trained dependency trees is 92.52%9, and that of the adapted latent graphs is 18.94%.",6.2 Analysis on Learned Latent Graphs,[0],[0]
"Surprisingly, the resulting UAS (18.94%) is lower than the UAS of our model without pretraining (24.52%).",6.2 Analysis on Learned Latent Graphs,[0],[0]
"However, in terms of the translation accuracy, our model with pre-training is better than that without pre-training.",6.2 Analysis on Learned Latent Graphs,[0],[0]
"These results suggest that human-annotated treebanks can provide useful prior knowledge to guide the overall model training by pre-training, but the resulting sentence structures adapted to the target task do not need to highly correlate with the treebanks.
",6.2 Analysis on Learned Latent Graphs,[0],[0]
9The UAS is significantly lower than the reported score in Hashimoto et al. (2017).,6.2 Analysis on Learned Latent Graphs,[0],[0]
The reason is described in Section 4.3.,6.2 Analysis on Learned Latent Graphs,[0],[0]
"While initial studies on NMT treat each sentence as a sequence of words (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014), researchers have recently started investigating into the use of syntactic structures in NMT models (Bastings et al., 2017; Chen et al., 2017; Eriguchi et al., 2016a,b, 2017; Li et al., 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016; Yang et al., 2017).",7 Related Work,[0],[0]
"In particular, Eriguchi et al. (2016b) introduced a tree-to-sequence NMT model by building a tree-structured encoder on top of a standard sequential encoder, which motivated the use of the dependency composition vectors in our proposed model.",7 Related Work,[0],[0]
"Prior to the advent of NMT, the syntactic structures had been successfully used in statistical machine translation systems (Neubig and Duh, 2014; Yamada and Knight, 2001).",7 Related Work,[0],[0]
"These syntax-based approaches are pipelined; a syntactic parser is first trained by supervised learning using a treebank such as the WSJ dataset, and then the parser is used to automatically extract syntactic information for machine translation.",7 Related Work,[0],[0]
"They rely on the output from the parser, and therefore parsing errors are propagated through the whole systems.",7 Related Work,[0],[0]
"By contrast, our model allows the parser to be adapted to the translation task, thereby providing a first step towards addressing ambiguous syntactic and semantic problems, such as domain-specific selectional preference and PP attachments, in a task-oriented fashion.
",7 Related Work,[0],[0]
Our model learns latent graph structures in a source-side language.,7 Related Work,[0],[0]
Eriguchi et al. (2017) have proposed a model which learns to parse and translate by using automatically-parsed data.,7 Related Work,[0],[0]
"Thus, it is also an interesting direction to learn latent structures in a target-side language.
",7 Related Work,[0],[0]
"As for the learning of latent syntactic structure, there are several studies on learning task-oriented syntactic structures.",7 Related Work,[0],[0]
Yogatama et al. (2017) used a reinforcement learning method on shift-reduce action sequences to learn task-oriented binary constituency trees.,7 Related Work,[0],[0]
"They have shown that the learned trees do not necessarily highly correlate with the human-annotated treebanks, which is consistent with our experimental results.",7 Related Work,[0],[0]
Socher et al. (2011) used a recursive autoencoder model to greedily construct a binary constituency tree for each sentence.,7 Related Work,[0],[0]
The autoencoder objective works as a regularization term for sentiment classification tasks.,7 Related Work,[0],[0]
"Prior to these deep learning approaches,
Wu (1997) presented a method for bilingual parsing.",7 Related Work,[0],[0]
"One of the characteristics of our model is directly using the soft connections of the graph edges with the real-valued weights, whereas all of the above-mentioned methods use one best structure for each sentence.",7 Related Work,[0],[0]
"Our model is based on dependency structures, and it is a promising future direction to jointly learn dependency and constituency structures in a task-oriented fashion.
",7 Related Work,[0],[0]
"Finally, more related to our model, Kim et al. (2017) applied their structured attention networks to a Natural Language Inference (NLI) task for learning dependency-like structures.",7 Related Work,[0],[0]
They showed that pre-training their model by a parsing dataset did not improve accuracy on the NLI task.,7 Related Work,[0],[0]
"By contrast, our experiments show that such a parsing dataset can be effectively used to improve translation accuracy by varying the size of the dataset and by avoiding strong overfitting.",7 Related Work,[0],[0]
"Moreover, our translation examples show the concrete benefit of learning task-oriented latent graph structures.",7 Related Work,[0],[0]
We have presented an end-to-end NMT model by jointly learning translation and source-side latent graph representations.,8 Conclusion and Future Work,[0],[0]
"By pre-training our model using treebank annotations, our model significantly outperforms both a pipelined syntax-based model and a state-of-the-art sequential model.",8 Conclusion and Future Work,[0],[0]
"On English-to-Japanese translation, our model outperforms the previous best models by a large margin.",8 Conclusion and Future Work,[0],[0]
"In future work, we investigate the effectiveness of our approach in different types of target tasks.",8 Conclusion and Future Work,[0],[0]
We thank the anonymous reviewers and Akiko Eriguchi for their helpful comments and suggestions.,Acknowledgments,[0],[0]
We also thank Yuchen Qiao and Kenjiro Taura for their help in speeding up our training code.,Acknowledgments,[0],[0]
"This work was supported by CREST, JST, and JSPS KAKENHI Grant Number 17J09620.",Acknowledgments,[0],[0]
This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences.,abstractText,[0],[0]
"Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, and thus the parser is optimized according to the translation objective.",abstractText,[0],[0]
"In experiments, we first show that our model compares favorably with state-of-the-art sequential and pipelined syntax-based NMT models.",abstractText,[0],[0]
We also show that the performance of our model can be further improved by pretraining it with a small amount of treebank annotations.,abstractText,[0],[0]
Our final ensemble model significantly outperforms the previous best models on the standard Englishto-Japanese translation dataset.,abstractText,[0],[0]
Neural Machine Translation with Source-Side Latent Graph Parsing,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 136–145 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"The encoder-decoder based neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014) have been developing rapidly.",1 Introduction,[0],[0]
"Sutskever et al. (2014) propose to encode the source sentence as a fixed-length vector representation, based on which the decoder generates the target sequence, where both the encoder and decoder are recurrent neural networks (RNN) (Sutskever et al., 2014) or their variants (Cho et al., 2014; Chung et al., 2014; Bahdanau et al., 2014).",1 Introduction,[0],[0]
"In this framework, the fixedlength vector plays the crucial role of transition-
ing the information of the sentence from the source side to the target side.",1 Introduction,[0],[0]
"Later, attention mechanisms are proposed to enhance the source side representations (Bahdanau et al., 2014; Luong et al., 2015b).",1 Introduction,[0],[0]
"The source side context is computed at each time-step of decoding, based on the attention weights between the source side representations and the current hidden state of the decoder.",1 Introduction,[0],[0]
"However, the hidden states in the recurrent decoder still originate from the single fixed-length representation (Luong et al., 2015b), or the average of the bi-directional representations (Bahdanau et al., 2014).",1 Introduction,[0],[0]
Here we refer to the representation as initial state.,1 Introduction,[0],[0]
"Interestingly, Britz et al. (2017) find that the value of initial state does not affect the translation performance, and prefer to set the initial state to be a zero vector.",1 Introduction,[0],[0]
"On the contrary, we argue that initial state still plays an important role of translation, which is currently neglected.",1 Introduction,[0],[0]
"We notice that beside the end-to-end error back propagation for the initial and transition parameters, there is no direct control of the initial state in the current NMT architectures.",1 Introduction,[0],[0]
"Due to the large number of parameters, it may be difficult for the NMT system to learn the proper sentence representation as the initial state.",1 Introduction,[0],[0]
"Thus, themodel is very likely to get stuck in local minimums, making the translation process arbitrary and unstable.",1 Introduction,[0],[0]
"In this paper, we propose to augment the current NMT architecture with a word prediction mechanism.",1 Introduction,[0],[0]
"More specifically, we require the initial state of the decoder to be able to predict all the words in the target sentence.",1 Introduction,[0],[0]
"In this way, there is a specific objective for learning the initial state.",1 Introduction,[0],[0]
Thus the learnt source side representation will be better constrained.,1 Introduction,[0],[0]
We further extend this idea by applying the word predictions mechanism to all the hidden states of the decoder.,1 Introduction,[0],[0]
"So the transition between different decoder states could be controlled
136
as well.",1 Introduction,[0],[0]
Our mechanism is simple and requires no additional data or annotation.,1 Introduction,[0],[0]
The proposed word predictions mechanism could be used as a training method and brings no extra computing cost during decoding.,1 Introduction,[0],[0]
Experiments on the Chinese-English and German-English translation tasks show that both the constraining of the initial state and the decoder hidden states bring significant improvement over the baseline systems.,1 Introduction,[0],[0]
"Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality.",1 Introduction,[0],[0]
Many previous works have noticed the problem of training an NMT system with lots of parameters.,2 Related Work,[0],[0]
"Some of them prefer to use the dropout technique (Srivastava et al., 2014; Luong et al., 2015b; Meng et al., 2016).",2 Related Work,[0],[0]
"Another possible choice is to ensemble several models with random starting points (Sutskever et al., 2014; Jean et al., 2015; Luong and Manning, 2016).",2 Related Work,[0],[0]
Both techniques could bring more stable and better results.,2 Related Work,[0],[0]
"But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours.",2 Related Work,[0],[0]
We will make empirical comparison with them in the experiments.,2 Related Work,[0],[0]
The way we add the word prediction is similar to the research of multi-task learning.,2 Related Work,[0],[0]
Dong et al. (2015) propose to share an encoder between different translation tasks.,2 Related Work,[0],[0]
"Luong et al. (2015a) propose to jointly learn the translation task for different languages, the parsing task and the image captioning task, with a shared encoder or decoder.",2 Related Work,[0],[0]
Zhang and Zong (2016) propose to use multitask learning for incorporating source sidemonolingual data.,2 Related Work,[0],[0]
"Different from these attempts, our method focuses solely on the current translation task, and does not require any extra data or annotation.",2 Related Work,[0],[0]
"In the other sequence to sequence tasks, Suzuki and Nagata (2017) propose the idea for predicting words by using encoder information.",2 Related Work,[0],[0]
"However, the purpose and the way of our mechanism are different from them.",2 Related Work,[0],[0]
"The word prediction technique has been applied in the research of both statistical machine transla-
tion (SMT) (Bangalore et al., 2007; Mauser et al., 2009; Jeong et al., 2010; Tran et al., 2014) and NMT (Mi et al., 2016; L’Hostis et al., 2016).",2 Related Work,[0],[0]
"In these research, word prediction mechanisms are employed to decide the selection of words or constrain the target vocabulary, while in this paper, we use word prediction as a control mechanism for neural model training.",2 Related Work,[0],[0]
"We present a popular NMT framework with the encoder-decoder architecture (Cho et al., 2014; Bahdanau et al., 2014) and the attention networks (Luong et al., 2015b), based on which we propose our word prediction mechanism.",3 Notations and Backgrounds,[0],[0]
"Denote a source-target sentence pair as {x, y} from the training set, where x is the source word sequence (x1, x2, · · · , x|x|) and y is the target word sequence (y1, y2, · · · , y|y|), |x| and |y| are the length of x and y, respectively.",3 Notations and Backgrounds,[0],[0]
"In the encoding stage, a bi-directional recurrent neural network is used (Bahdanau et al., 2014) to encode x into a sequence of vectors (h1,h2, · · · ,h|x|).",3 Notations and Backgrounds,[0],[0]
"For each xi, the representation hi is:
hi =",3 Notations and Backgrounds,[0],[0]
"[ −→hi ;←−hi ] (1)
where [·; ·] denotes the concatenation of column vectors;
−→hi and ←−hi denote the hidden vectors for the word xi in the forward and backward RNNs, respectively.",3 Notations and Backgrounds,[0],[0]
"The gated recurrent unit (GRU) is used as the recurrent unit in each RNN, which is shown to have promising results in speech recognition and machine translation (Cho et al., 2014).",3 Notations and Backgrounds,[0],[0]
"Formally, the hidden state hi at time step i of the forward RNN encoder is defined by the GRU function",3 Notations and Backgrounds,[0],[0]
"g−→e (·, ·), as follows:
−→h",3 Notations and Backgrounds,[0],[0]
i =,3 Notations and Backgrounds,[0],[0]
"g−→e ( −→h i−1, embxi) (2)
= (1−−→z i)⊙−→h i−1 +−→z",3 Notations and Backgrounds,[0],[0]
i ⊙,3 Notations and Backgrounds,[0],[0]
−→ h′,3 Notations and Backgrounds,[0],[0]
"i
−→z",3 Notations and Backgrounds,[0],[0]
i = σ(−→Wz[embxi ; −→h i−1]) (3) −→ h′ i = tanh( −→W[embxi ; (−→r i ⊙ −→h i−1)]),3 Notations and Backgrounds,[0],[0]
"(4) −→r i = σ(−→Wr[embxi ; −→h i−1]) (5)
where ⊙ denotes element-wise product between vectors and embxi is the word embedding of the xi. tanh(·) and σ(·) are the tanh and sigmoid transformation functions that can be applied elementwise on vectors, respectively.",3 Notations and Backgrounds,[0],[0]
"For simplicity, we
omit the bias term in each network layer.",3 Notations and Backgrounds,[0],[0]
The backward RNN encoder is defined likewise.,3 Notations and Backgrounds,[0],[0]
"In the decoding stage, the decoder starts with the initial state s0, which is the average of source representations (Bahdanau et al., 2014).
",3 Notations and Backgrounds,[0],[0]
s0 = σ(Ws 1 |x| |x|∑ i=1,3 Notations and Backgrounds,[0],[0]
"hi) (6)
",3 Notations and Backgrounds,[0],[0]
"At each time step j, the decoder maximizes the conditional probability of generating the jth target word, which is defined as:
P (yj |y<j , x) = fd(td([embyj−1 ; sj ; cj ]))",3 Notations and Backgrounds,[0],[0]
"(7) fd(u) = softmax(Wfu) (8) td(v) = tanh(Wtv) (9)
where sj is the decoder’s hidden state, which is computed by another GRU (as in Equation 2):
sj = gd(sj−1, [embyj−1 ; cj ]) (10)
and the context vector cj is from the attention mechanism (Luong et al., 2015b):
cj = |x|∑ i=1",3 Notations and Backgrounds,[0],[0]
"ajihi (11)
aji = exp(eji)∑|x|
k=1 exp(ejk) (12)
eji = tanh(Wattd [sj−1;hi]).",3 Notations and Backgrounds,[0],[0]
(13),3 Notations and Backgrounds,[0],[0]
The decoder starts the generation of target sentence from the initial state s0 (Equation 6) generated by the encoder.,4.1 Word Prediction for the Initial State,[0],[0]
"Currently, the update for the encoder
only happens when a translation error occurs in the decoder.",4.1 Word Prediction for the Initial State,[0],[0]
The error is propagated through multiple time steps in the recurrent structure until it reaches the encoder.,4.1 Word Prediction for the Initial State,[0],[0]
"As there are hundreds of millions of parameters in the NMT system, it is hard for the model to learn the exact representation of source sentences.",4.1 Word Prediction for the Initial State,[0],[0]
"As a result, the values of initial state may not be exact during the translation process, leading to poor translation performances.",4.1 Word Prediction for the Initial State,[0],[0]
We propose word prediction as a mechanism to control the values of initial state.,4.1 Word Prediction for the Initial State,[1.0],['We propose word prediction as a mechanism to control the values of initial state.']
"The intuition is that since the initial state is responsible for the translation of whole target sentence, it should at least contain information of each word in the target sentence.",4.1 Word Prediction for the Initial State,[0],[0]
"Thus, we optimize the initial state by making prediction for all target words.",4.1 Word Prediction for the Initial State,[0],[0]
"For simplicity, we assume each target word is independent of each other.",4.1 Word Prediction for the Initial State,[0],[0]
"Here the word prediction mechanism is a simpler sub-task of translation, where the order of words is not considered.",4.1 Word Prediction for the Initial State,[0],[0]
"The prediction task could be trained jointly with the translation task in a multi-task learningway (Luong et al., 2015a; Dong et al., 2015; Zhang and Zong, 2016), where both tasks share the same encoder.",4.1 Word Prediction for the Initial State,[0],[0]
"In other words, word prediction for the initial state could be interpreted as an improvement for the encoder.",4.1 Word Prediction for the Initial State,[0],[0]
We denote this mechanism as WPE .,4.1 Word Prediction for the Initial State,[0],[0]
"As shown in Figure 1, a prediction network is added to the initial state.",4.1 Word Prediction for the Initial State,[0],[0]
"We define the conditional probability of WPE as follows:
PWPE(y|x) = |y|∏
j=1
PWPE(yj |x)",4.1 Word Prediction for the Initial State,[0],[0]
"(14)
PWPE(yj |x) = fp(tp([s0; cp]))",4.1 Word Prediction for the Initial State,[0],[0]
"(15) where fp(·) and tp(·) are the softmax layer and non-linear layer as defined in Equation 8-9, with different parameters; cp is defined similar as the
attention network, so the source side information could be enhanced.
cp = |x|∑ i=1",4.1 Word Prediction for the Initial State,[0],[0]
"aihi (16) ai = exp(ei)∑|x|
k=1 exp(ek) (17)
",4.1 Word Prediction for the Initial State,[0],[0]
ei = tanh(Wattp,4.1 Word Prediction for the Initial State,[0],[0]
"[s0,hi]).",4.1 Word Prediction for the Initial State,[0],[0]
(18),4.1 Word Prediction for the Initial State,[0],[0]
Similar intuition is also applied for the decoder.,4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
"Because the hidden states of the decoder are responsible for the translation of target words, they should be able to predict the target words as well.",4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
The only difference is that we remove the already generated words from the prediction task.,4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
So each hidden state in the decoder is required to predict the target words which remain untranslated.,4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
"For the first state s1 of the decoder, the prediction task is similar with the task for the initial state.",4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
"Since then, the prediction is no longer a separate training task, but integrated into each time step of the training process.",4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
We denote this mechanism as WPD.,4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
"As shown in Figure 2, for each time step j in the decoder, the hidden state sj is used for the prediction of (yj , yj+1, · · · , y|y|).",4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
"The conditional probability of WPD is defined as:
PWPD(yj , yj+1, · · · , y|y||y<j , x) (19)
= |y|∏
k=j
PWPD(yk|y<j , x)
",4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
"PWPD(yk|y<j , x) =fd(p(td([embyj−1 ; sj ; cj ])))",4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
"(20)
where fd(·) and td(·) are the softmax layer and non-linear layer as defined in Equation 8-9; p(·)
is another non-linear transformation layer, which prepares the current state for the prediction:
p(u) = tanh(Wpu).",4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
(21),4.2 Word Predictions for Decoder’s Hidden States,[0],[0]
"NMT models optimize the networks by maximizing the likelihood of the target translation y given source sentence x, denoted by LT.
LT = 1 |y| |y|∑ j=1 logP (yj |y<j , x) (22)
where P (yj |y<j , x) is defined in Equation 7.",4.3 Training,[1.000000053821755],"['NMT models optimize the networks by maximizing the likelihood of the target translation y given source sentence x, denoted by LT. LT = 1 |y| |y|∑ j=1 logP (yj |y<j , x) (22) where P (yj |y<j , x) is defined in Equation 7.']"
"To optimize the word prediction mechanism, we propose to add extra likelihood functionsLWPE and LWPD into the training procedure.",4.3 Training,[0],[0]
"For the WPE, we directly optimize the likelihood of translation and word prediction:
",4.3 Training,[0],[0]
"L1 = LT + LWPE (23) LWPE = logPWPE (24)
where PWPE is defined in Equation 14.",4.3 Training,[0],[0]
"For the WPD, we optimize the likelihood as:
L2 = LT + LWPD (25)
LWPD = |y|∑
j=1
1 |y| − j + 1 logPWPD (26)
where PWPD is defined in Equation 19; the coefficient of the logarithm is used to calculate the average probability of each prediction.",4.3 Training,[0],[0]
"The two mechanisms could also work together, so that both the encoder and the decoder could be improved:
L3 = LT + LWPE + LWPD .",4.3 Training,[0],[0]
(27),4.3 Training,[0],[0]
"The previously proposed word prediction mechanism could be used only as a extra training objective, which will not be computed during the translation.",4.4 Making Use of the Word Predictor,[0],[0]
Thus the computational complexity of our models for translation stays exactly the same.,4.4 Making Use of the Word Predictor,[0],[0]
"On the other hand, using a smaller and specific vocabulary for each sentence or batch will improve translation efficiency.",4.4 Making Use of the Word Predictor,[1.0],"['On the other hand, using a smaller and specific vocabulary for each sentence or batch will improve translation efficiency.']"
"If the vocabulary is accurate enough, there is also a chance to improve the translation quality (Jean et al., 2015; Mi et al., 2016; L’Hostis et al., 2016).",4.4 Making Use of the Word Predictor,[0],[0]
Our word prediction mechanismWPE provides a natural solution for generating a possible set of target words at sentence level.,4.4 Making Use of the Word Predictor,[0],[0]
"The prediction could be made from the initial state s0, without using extra resources such as word dictionaries, extracted phrases or frequent word lists, as in Mi et al. (2016).",4.4 Making Use of the Word Predictor,[0],[0]
We perform experiments on the Chinese-English (CH-EN) and German-English (DE-EN) machine translation tasks.,5.1 Data,[1.0],['We perform experiments on the Chinese-English (CH-EN) and German-English (DE-EN) machine translation tasks.']
"For the CH-EN, the training data consists of about 8million sentence pairs 1.",5.1 Data,[0],[0]
"We use NIST MT02 as our validation set, and the NIST MT03, MT04 and MT05 as our test sets.",5.1 Data,[1.0],"['We use NIST MT02 as our validation set, and the NIST MT03, MT04 and MT05 as our test sets.']"
"These sets have 878, 919, 1597 and 1082 source sentences, respectively, with 4 references for each sentence.",5.1 Data,[0],[0]
"For the DE-EN, the experiments trained on the standard benchmark WMT14, and it has about 4.5 million sentence pairs.",5.1 Data,[0],[0]
"We use newstest 2013 (NST13) as validation set, and newstest 2014(NST14) as test set.",5.1 Data,[0],[0]
"These sets have 3000 and 2737 source sentences, respectively, with 1 reference for each sentence.",5.1 Data,[0],[0]
"Sentences were encoded using byte-pair encoding (BPE) (Britz et al., 2017).",5.1 Data,[0],[0]
"We implement a baseline system with the bidirectional encoder (Bahdanau et al., 2014) and the attention mechanism (Luong et al., 2015b) as described in Section 3, denoted as baseNMT.",5.2 Systems and Techniques,[0],[0]
"Then our proposed word prediction mechanism on initial state and hidden states of decoder are implemented on the baseNMT system, denoted as WPE and WPD, respectively.",5.2 Systems and Techniques,[0],[0]
"We denote the system
1includes LDC2002E18, LDC2003E07, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T06, LDC2005T10, LDC2006E26 and LDC2007T09
use both techniques as WPED.",5.2 Systems and Techniques,[0],[0]
"We implement systemswith variable-sized vocabulary following (Mi et al., 2016).",5.2 Systems and Techniques,[0],[0]
"For comparison, we also implement systems with dropout (with dropout rate 0.5 on the output layer) and ensemble (ensemble of 4 systems at the output layer) techniques.",5.2 Systems and Techniques,[0],[0]
"Both our CH-EN and DE-EN experiments are implemented on the open source toolkit dl4mt 2, with most default parameter settings kept the same.",5.3 Implementation Details,[0],[0]
We train the NMT systemswith the sentences of length up to 50 words.,5.3 Implementation Details,[0],[0]
"The source and target vocabularies are limited to the most frequent 30K words for both Chinese and English, respectively, with the out-of-vocabulary words mapped to a special token UNK.",5.3 Implementation Details,[0],[0]
The dimension of word embedding is set to 512 and the size of the hidden layer is 1024.,5.3 Implementation Details,[0],[0]
"The recurrent weight matrices are initialized as random orthogonal matrices, and all the bias vectors as zero.",5.3 Implementation Details,[0],[0]
"Other parameters are initialized by sampling from the Gaussian distribution N (0, 0.01).",5.3 Implementation Details,[0],[0]
"We use the mini-batch stochastic gradient descent (SGD) approach to update the parameters, with a batch size of 32.",5.3 Implementation Details,[0],[0]
"The learning rate is controlled by AdaDelta (Zeiler, 2012).",5.3 Implementation Details,[0],[0]
"For efficient training of our system, we adopt a simple pre-train strategy.",5.3 Implementation Details,[0],[0]
"Firstly, the baseNMT system is trained.",5.3 Implementation Details,[0],[0]
The training results are used as the initial parameters for pre-training our proposed models with word predictions.,5.3 Implementation Details,[0],[0]
"For decoding during test time, we simply decode until the end-of-sentence symbol eos occurs, using a beam search with a beam width of 5.",5.3 Implementation Details,[0],[0]
"To see the effect of word predictions in translation, we evaluate these systems in case-insensitive IBM-BLEU (Papineni et al., 2002) on both CH-EN and DE-EN tasks.",5.4 Translation Experiments,[0],[0]
The detailed results are show in the Table 1 and Table 2.,5.4 Translation Experiments,[0],[0]
"Compared to the baseNMT system, all of our models achieve significant improvements.",5.4 Translation Experiments,[0],[0]
"On the CH-EN experiments, simply adding word predictions to the initial state (WPE) already brings considerable improvements.",5.4 Translation Experiments,[0],[0]
"The average improvement on test set is 2.53 BLEU, showing that constraining the initial state does lead to a higher translation quality.",5.4 Translation Experiments,[0],[0]
"Adding word predic-
2https://github.com/nyu-dl/dl4mt-tutorial
tions to the hidden states in the decoder (WPD) leads to further improvements against baseNMT (4.15 BLEU), because WPD adds constraints to the state transitions through different time steps in the decoder.",5.4 Translation Experiments,[0],[0]
Using both techniques improves the baseline by 4.53 BLEU.,5.4 Translation Experiments,[1.0],['Using both techniques improves the baseline by 4.53 BLEU.']
"On the DE-EN experiments, the improvement of WPE model is 0.41 BLEU and WPD model is 0.86 BLEU on test set.",5.4 Translation Experiments,[0],[0]
"When use both techniques, the WPED improves on the test set is 1.3 BLEU.",5.4 Translation Experiments,[0],[0]
We compare our models with systems using dropout and ensemble techniques.,5.4 Translation Experiments,[1.0],['We compare our models with systems using dropout and ensemble techniques.']
The results show in Table 3 and 4.,5.4 Translation Experiments,[1.0],['The results show in Table 3 and 4.']
"On the CH-EN experiments, the dropout method successfully improves the baseNMT system by 2.06 BLEU.",5.4 Translation Experiments,[0],[0]
"However, it does not work on our WPED system.",5.4 Translation Experiments,[0],[0]
The ensemble technique improves the baseNMT system by 2.75 BLEU.,5.4 Translation Experiments,[0],[0]
"It still improves WPED by 1.26
BLEU, but the improvement is smaller than on the baseNMT.",5.4 Translation Experiments,[0],[0]
"On the DE-EN experiments, the phenomenon of experiments is similar to CH-EN experiments.",5.4 Translation Experiments,[0],[0]
The baseNMT system improves 0.94 through dropout method and 0.9 BLEU through ensemble method.,5.4 Translation Experiments,[0],[0]
The dropout technique also does not work on WPED and the ensemble technique improves 1.79 BLEU.,5.4 Translation Experiments,[0],[0]
"These comparisons suggests that our system already learns better and stable values for the parameters, enjoying some of the benefits of general training techniques like dropout and ensemble.",5.4 Translation Experiments,[0],[0]
"Compared to dropout and ensemble, our method WPED achieves the highest improvement against the baseline system on both CH-EN and DE-EN experiments.",5.4 Translation Experiments,[0],[0]
"Along with ensemble method, the improvement could be up to 5.79 BLEU and 1.79 BLEU respectively.",5.4 Translation Experiments,[0],[0]
"Since we include an explicit word prediction mechanism during the training of NMT systems, we also evaluate the prediction performance on the CH-EN experiments to see how the training is improved.",5.5 Word Prediction Experiments,[0],[0]
"For each sentence in the test set, we use the initial state of the given model to make prediction about the possible words.",5.5 Word Prediction Experiments,[0],[0]
"We denote the set of top nwords as Tn, the set of words in all the references
as R. We define the precision, recall of the word prediction as follows:
precision = |Tn ∩R| |Tn| ∗ 100% (28)
recall = |Tn ∩R| |R| ∗ 100% (29)
We compare the prediction performance of baseNMT and WPE.",5.5 Word Prediction Experiments,[0],[0]
"WPED has similar prediction results withWPE, so we omit its results.",5.5 Word Prediction Experiments,[0],[0]
"As shown in Table 5, baseNMT system has a relatively lower prediction precision, for example, 45% in top 10 prediction.",5.5 Word Prediction Experiments,[0],[0]
"With an explicit training, the WPE could achieve a much higher precision in all conditions.",5.5 Word Prediction Experiments,[0],[0]
"Specifically, the precision reaches 73% in top 10.",5.5 Word Prediction Experiments,[1.0],"['Specifically, the precision reaches 73% in top 10.']"
"This indicates that the initial state in WPE contains more specific information about the prediction of the target words, which may be a step towards better semantic representation, and leads to better translation quality.",5.5 Word Prediction Experiments,[0],[0]
"Because the total words in the references are limited (around 50), the precision goes down, as expected, when a larger prediction set is considered.",5.5 Word Prediction Experiments,[0],[0]
"On the other hand, the recall of WPE is also much higher than baseNMT.",5.5 Word Prediction Experiments,[0],[0]
"When given 1k predictions, WPE could successfully predict 89% of the words in the reference.",5.5 Word Prediction Experiments,[0],[0]
"The recall goes up to 95% with 5k predictions, which is only 1/6 of the current vocabulary.",5.5 Word Prediction Experiments,[0],[0]
"To analyze the process of word prediction, we draw the attention heatmap (Equation 16) between the initial state s0 and the bi-directional representation of each source side word hi for an example sentence.",5.5 Word Prediction Experiments,[0],[0]
"As shown in Figure 3, both examples show that the initial states have a very strong attention with all the content words in the source sentence.",5.5 Word Prediction Experiments,[0],[0]
"The blank cells are mostly functions words
or high frequent tokens such as “的 (’s)”, “是 (is)”, “而 (and)”, “它 (it)”, comma and period.",5.5 Word Prediction Experiments,[0],[0]
"This indicates that the initial state successfully encodes information about most of the content words in the source sentence, which contributes for a high prediction performance and leads to better translation.",5.5 Word Prediction Experiments,[0],[0]
"To make use of the word prediction, we conduct experiments using the predicted vocabulary, with different vocabulary size (1k to 10k) on the CHEN experiments, denoted as WPE-V and WPED-V. The comparison is made in both translation quality and decoding time.",5.6 Improving Decoding Efficiency,[0],[0]
"As all our models with fixed vocabulary size have exactly the same number of parameters for decoding (extra mechanism is used only for training), we only plot the decoding time of the WPED for comparison.",5.6 Improving Decoding Efficiency,[0],[0]
Figure 4 and 5 show the results.,5.6 Improving Decoding Efficiency,[0],[0]
"When we start the experiments with top 1k vocabulary (1/30 of the baseline settings), the translation quality of both WPE-V and WPED-V are already higher than the baseNMT; while their decoding time is less than 1/3 of an NMT system with 30k vocabulary.",5.6 Improving Decoding Efficiency,[0],[0]
"When the size of vocabulary increases, the translation quality improves as well.",5.6 Improving Decoding Efficiency,[0],[0]
"With a 6k predicted vocabulary (1/5 of the baseline settings), the decoding time is about 60% of a full-
vocabulary system; the performances of both systems with variable size vocabulary are comparable their corresponding fixed-vocabulary systems, which is higher than the baseNMT by 2.53 and 4.53 BLEU, respectively.",5.6 Improving Decoding Efficiency,[0],[0]
"Although the comparison may not be fair enough due to the language pair and training conditions, the above relative improvements (e.g. WPED-V v.s. baseNMT) is much higher than previous research of manipulating the vocabularies (Jean et al., 2015; Mi et al., 2016; L’Hostis et al., 2016).",5.6 Improving Decoding Efficiency,[0],[0]
"This is because our mechanism is not only about reducing the vocabulary itself for each sentence or batch, it also brings improvement to the overall translation model.",5.6 Improving Decoding Efficiency,[0],[0]
"Please note that un-
like these research, we keep the target vocabulary to be 30k in all our experiments, becausewe are not focusing on increasing the vocabulary size in this paper.",5.6 Improving Decoding Efficiency,[0],[0]
It will be interesting to combine our mechanism with larger vocabulary to further enhance the translation performance.,5.6 Improving Decoding Efficiency,[0],[0]
"Again, our mechanism requires no extra annotation, dictionary, alignment or separate discriminative predictor, etc.",5.6 Improving Decoding Efficiency,[0],[0]
We also analyze real-case translations to see the difference between different systems (Table 6).,5.7 Translation Analysis,[0],[0]
"It is easy to see that the baseNMT systemmisses the translations of several important words, such as “advertising”, “1.5”, which are marked with underline in the reference.",5.7 Translation Analysis,[0],[0]
It also wrongly translates the company name “time warner inc.”,5.7 Translation Analysis,[0],[0]
as the redundant information “internet company”; “america online” as “us line”.,5.7 Translation Analysis,[0.983958705003602],['It also wrongly translates the company name “time warner inc.” as the redundant information “internet company”; “america online” as “us line”.']
The results of dropout or ensemble show improvement compared to the baseNMT.,5.7 Translation Analysis,[0],[0]
But they still make mistakes about the translation of “online” and the company name “time warner inc.”.,5.7 Translation Analysis,[1.0],['But they still make mistakes about the translation of “online” and the company name “time warner inc.”.']
"WithWPED, most of these errors no longer exist, because we force the encoder and decoder to carry the exact information during translation.",5.7 Translation Analysis,[0],[0]
The encoder-decoder architecture provides a general paradigm for learning machine translation from the source language to the target language.,6 Conclusions,[0],[0]
"However, due to the large amount of parameters and relatively small training data set, the end-toend learning of an NMT model may not be able to learn the best solution.",6 Conclusions,[0],[0]
"We argue that at least part of the problem is caused by the long error backpropagation pipeline of the recurrent structures in multiple time steps, which provides no direct control of the information carried by the hidden states in both the encoder and decoder.",6 Conclusions,[0],[0]
"Instead of looking for other annotated data, we notice that the words in the target language sentence could be viewed as a natural annotation.",6 Conclusions,[0],[0]
We propose to use the word prediction mechanism to enhance the initial state generated by the encoder and extend the mechanism to control the hidden states of decoder as well.,6 Conclusions,[0],[0]
Experiments show promising results on the Chinese-English and German-English translation tasks.,6 Conclusions,[0],[0]
"As a byproduct, the word predictor could be used to improve the efficiency of decoding, which may be
crucial for large scale applications.",6 Conclusions,[0],[0]
Our attempts demonstrate that the learning of the large scale neural network systems is still not good enough.,6 Conclusions,[0],[0]
"In the future, it might be helpful to analyze the benefits of jointly learning other related tasks together with machine translation, to provide further control of the learning process.",6 Conclusions,[0],[0]
It is interesting to demonstrate the effectiveness of the proposed mechanism on other sequence to sequence learning tasks as well.,6 Conclusions,[0],[0]
We would like to thank the anonymous reviewers for their insightful comments.,Acknowledgments,[0],[0]
Shujian Huang is the corresponding author.,Acknowledgments,[0],[0]
"This work is supported by the National Science Foundation of China (No. 61672277, 61472183), the Jiangsu Provincial Research Foundation for Basic Research (No. BK20170074).",Acknowledgments,[0],[0]
"In the encoder-decoder architecture for neural machine translation (NMT), the hidden states of the recurrent structures in the encoder and decoder carry the crucial information about the sentence.",abstractText,[0],[0]
These vectors are generated by parameters which are updated by back-propagation of translation errors through time.,abstractText,[0],[0]
We argue that propagating errors through the end-to-end recurrent structures are not a direct way of control the hidden vectors.,abstractText,[0],[0]
"In this paper, we propose to use word predictions as a mechanism for direct supervision.",abstractText,[0],[0]
"More specifically, we require these vectors to be able to predict the vocabulary in target sentence.",abstractText,[0],[0]
Our simple mechanism ensures better representations in the encoder and decoder without using any extra data or annotation.,abstractText,[0],[0]
It is also helpful in reducing the target side vocabulary and improving the decoding efficiency.,abstractText,[0],[0]
"Experiments on Chinese-English and German-English machine translation tasks show BLEU improvements by 4.53 and 1.3, respectively.",abstractText,[0],[0]
Neural Machine Translation with Word Predictions,title,[0],[0]
