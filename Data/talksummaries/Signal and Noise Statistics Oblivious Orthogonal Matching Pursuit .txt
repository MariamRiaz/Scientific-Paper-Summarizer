1	15	Throughout this article, we assume that the entries of the noise w are independent, zero mean and Gaussian distributed with variance σ2.
3	13	This problem of estimating high dimensional vectors in sample starved scenarios is ill-posed even in the absence of noise unless strong structural assumptions are made on X and β.
5	30	The vector β ∈ Rp is sparse if the support of β given by S = supp(β) = {k : βk 6= 0} has cardinality k0 = card(S) p. A number of algorithms like least absolute shrinkage and selection operator (LASSO)(Tropp, 2006; Tibshirani, 1996), Dantzig selector (DS)(Candes & Tao, 2007), subspace pursuit (SP)(Dai & Milenkovic, 2009), OMP (Pati et al., 1993; Mallat & Zhang, 1993; Tropp, 2004; Cai & Wang, 2011), elastic net (Zou & Hastie, 2005) etc.
6	43	are proposed to efficiently estimate β. Tuning the hyper parameters of aforementioned algorithms to achieve optimal performance require a priori knowledge of signal parameters like sparsity k0 or noise statistics like σ2 etc.
8	44	To the best of our knowledge, no computationally efficient technique to estimate k0 is reported in open literature.
27	20	Further, RRT also delivered a highly competitive performance when applied to identify outliers in real data sets, an increasingly popular application of sparse estimation algorithms(Mitra et al., 2010; 2013).
42	31	Then a LS estimate of β restricted to the current support Skomp is Algorithm 1 Orthogonal matching pursuit Input: Observation y, matrix X Initialize Somp0 = φ. k = 1 and residual r0 = y repeat Identify the next column tk = arg max j |XTj rk−1| Expand current support Skomp = Sk−1omp ∪ tk Restricted LS estimate: β̂Skomp = X † Skomp y. β̂{1,...,p}/Skomp = 0p−k.
43	23	Update residual: rk = y −Xβ̂ = (In −Pk)y. Increment k ← k + 1. until stopping condition (SC) is true Output: Support estimate Ŝ = Skomp.
49	17	The monotonicity of Skomp in turn implies that the residual norm ‖rk‖2 is a non increasing function of k, i.e, ‖rk+1‖2 ≤ ‖rk‖2.
50	15	Most of the theoretical properties of OMP are derived assuming a priori knowledge of true sparsity level k0 in which case OMP stops after exactly k0 iterations(Tropp, 2004; Wang, 2015).
51	24	When k0 is not known, one has to rely on stopping conditions (SC) based on the properties of the residual rk as k varies.
55	18	Consequently, one can stop OMP iterations in Gaussian noise once ‖rk‖2 ≤ σ .
58	27	RIC of order j denoted by δj is defined as the smallest value of δ such that (1− δ)‖b‖22 ≤ ‖Xb‖22 ≤ (1 + δ)‖b‖22 (2) hold true for all b ∈ Rp with ‖b‖0 = card(supp(b)) ≤ j.
59	18	A smaller value of δj implies that X act as a near orthogonal matrix for all j sparse vectors b.
84	12	To summarize, exact support recovery using any OMP based scheme including the signal and noise statistics aware schemes is possible only if kmin = k0.
94	12	Consequently, at k = k0, the numerator ‖rk0‖2 ofRR(k0) contains contribution only from the noise term ‖(In−Pk0)w‖2, whereas, the denominator ‖rk0−1‖2 in RR(k0) contain contributions from both the signal term i.e., (In−Pk)XSβS and the noise term (In−Pk)w. This behaviour of RR(k0) along with the fact that ‖w‖2 P→ 0 as σ2 → 0 implies the following theorem.
101	27	The absence of signal terms in numerator and the denominator of RR(k) = ‖(In−Pk)w‖2‖(In−Pk−1)w‖2 for k > kmin implies that even when ‖w‖2 → 0 or σ2 → 0, RR(k) for k > kmin does not converge to zero.
103	71	Let Fa,b(x) denotes the cumulative distribution function of a B(a, b) random variable.
104	27	(3) Theorem 2 states that the residual ratio statistic RR(k) for k > kmin is lower bounded by the deterministic sequence {ΓαRRT (k)} kmax k=kmin+1 with a high probability (for small values of α).
112	28	An important aspect regarding the RRT in Algorithm 2 is the choice of kRRT when the set {k : RR(k) ≤ ΓαRRT (k)} = φ.
113	27	This situation happens only at very low SNR.
115	19	Mathematically, we set kRRT = max{k : RR(k) < ΓαnewRRT (k)}, where αnew = min a>α {a : {k : RR(k) ≤ ΓαRRT (k)} 6= φ}.
121	43	As we will see later, a good choice of kmax is kmax = [0.5(n + 1)] which results in a complexity order O(n2p).
123	25	This is the computational cost being paid for not knowing k0 or σ2 a priori.
125	18	RRT algorithm is developed only assuming that the support sequence generated by the sparse recovery algorithm is monotonically increasing.
126	72	Apart from OMP, algorithms such as orthogonal least squares(Wen et al., 2017) and OMP with thresholding(Yang & de Hoog, 2015) also produce monotonic support sequences.
128	14	In this section we present support recovery guarantees for RRT and compare it with the results available for OMP with a priori knowledge of k0 or σ2.
129	37	The first result in this section deals with the finite sample and finite SNR performance for RRT.
130	43	Let kmax ≥ k0 and suppose that the matrix X satisfies δk0+1 < 1√ k0+1 .
134	21	Also please note that the RRT framework does not impose any extra conditions on the design matrix X. Consequently, the only appreciable difference between RRT and OMP with a priori knowledge of k0 and σ2 is in the extra SNR required by RRT which is quantified next using the metric extra = omp/ rrt.
