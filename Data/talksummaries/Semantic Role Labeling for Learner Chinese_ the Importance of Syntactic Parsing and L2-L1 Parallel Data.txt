0	33	A learner language (interlanguage) is an idiolect developed by a learner of a second or foreign language which may preserve some features of his/her first language.
2	77	In this paper, we study semantic parsing for interlanguage, taking semantic role labeling (SRL) as a case task and learner Chinese as a case language.
36	40	Following Mizumoto et al. (2011), we collected a large dataset of L2-L1 parallel texts of Mandarin Chinese by exploring “language exchange” social networking services (SNS), i.e., Lang-8, a language-learning website where native speakers can freely correct the sentences written by foreign learners.
49	25	The dataset includes four typologically different mother tongues, i.e., English (ENG), Japanese (JPN), Russian (RUS) and Arabic (ARA).
51	19	We take the mother languages of the learners into consideration, which have a great impact on grammatical errors and hence automatic semantic analysis.
52	40	We hope that four selected mother tongues guarantee a good coverage of typologies.
54	13	Semantic role labeling (SRL) is the process of assigning semantic roles to constituents or their head words in a sentence according to their relationship to the predicates expressed in the sentence.
55	15	Typical semantic roles can be divided into core arguments and adjuncts.
69	12	For inter-annotator agreement, we evaluate the precision (P), recall (R), and F1-score (F) of the semantic labels given by the two annotators.
77	14	Table 2 further reports agreements on each argument (AN) and adjunct (AM) in detail, according to which the high scores are attributed to the high agreement on arguments (AN).
84	22	To evaluate the robustness of state-of-the-art SRL algorithms, we evaluate two representative SRL frameworks.
86	18	This system first collects all c-commanders of a predicate in question from the output of a parser and puts them in order.
87	11	It then employs a first order linear-chain global linear model to perform semantic tagging.
88	38	For constituent parsing, we use two parsers for comparison, one is Berkeley parser3 (Petrov et al., 2006), a well-known implementation of the unlexicalized latent variable PCFG model, the other is a minimal span-based neural parser based on independent scoring of labels and spans (Stern et al., 2017).
89	34	As proposed in Stern et al. (2017), the second parser is capable of achieving state-of-the-art single-model performance on the Penn Treebank.
96	20	To train the three SRL systems as well as the supporting parsers, we use the CTB and CPB data 4.
127	30	The Chinese word “也” (also) usually serves as an adjunct but is now used for linking the parallel structure “用汉语也说话快” (using Chinese also speaking quickly) in this sentence, which is ill-formed to native speakers and negatively affects the boundary detection of A0 for both systems.
128	48	On the other hand, the neural system incorrectly takes the whole part before “很难” (very hard) as A0, regardless of the adjunct “对 我 来说” (for me), while this can be figured out by exploiting syntactic analysis, as illustrated in Figure 3c.
130	23	This shows that by providing information of some well-formed sub-trees associated with correct semantic roles, the syntactic system can perform better than the neural one on SRL for learner texts.
133	10	Besides, the deep end-to-end system is also likely to incorrectly attach adjuncts AM to the predicates.
134	27	The Chinese verb “做饭” (cook-meal) is intransitive while this sentence takes it as a transitive verb, which is very common in L2.
146	11	Based on a word alignment, we define the shared tuple as a mutual tuple between two SRL results of an L2-L1 sentence pair, meaning that both the predicate and argument words are aligned respectively, and their role relations are the same.
148	10	Sentences whose L1 and L2 recall are both greater than a threshold p are taken as good ones.
149	20	A parser-based SRL system consists of two essential modules: a syntactic parser and a semantic classifier.
150	17	To enhance the syntactic parser, the automatically generated syntactic trees of the sentence pairs that exhibit high semantic consistency are directly used to extend training data.
151	10	To improve a semantic classifier, besides the consistent semantic analysis, we also use the outputs of the L1 but not L2 data which are generated by the neural syntax-agnostic SRL system.
153	14	We separate them into three data sets.
164	35	Since both the syntactic parser and the SRL classifier can be retrained and thus enhanced, we report the individual impact as well as the combined one.
168	11	Fortunately, combining both results in further improvement.
173	83	Though for standard in-domain test, the neural parser performs better and thus is more and more popular, for some other scenarios, the PCFGLA model is stronger.
