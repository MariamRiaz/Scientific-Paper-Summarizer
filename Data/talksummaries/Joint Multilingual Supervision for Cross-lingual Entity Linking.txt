1	43	Recently, the task of Cross-lingual Entity Linking (XEL) has gained attention (McNamee et al., 2011; Ji et al., 2015; Tsai and Roth, 2016) with the goal of grounding entity mentions written in any language to the English Wikipedia.
3	54	XEL involves grounding the Tamil mention (which translates to ‘Liverpool’) to the football club Liverpool_F.C., and not the city or the university.
4	76	XEL enables knowledge acquisition directly from documents in any language, without resorting to machine translation.
5	22	Training an EL model requires grounded mentions, i.e. mentions of entities that are grounded to a Knowledge Base (KB), as supervision (Figure 1).
6	65	While millions of such mentions are available in English, by virtue of hyperlinks in the English Wikipedia, this is not the case for most languages.
33	14	To learn from mention contexts in multiple languages, we generate mention context representations using a language-agnostic mention context encoder.
35	46	Below we describe the components of the mention context encoder, namely multilingual word embeddings and local and document context encoders.
36	53	Multilingual Word Embeddings (Ammar et al., 2016b; Smith et al., 2017; Duong et al., 2017) jointly encode words in multiple (≥2) languages in the same vector space such that semantically similar words in the same language, and translationally equivalent words in different languages are close (per cosine similarity).
41	16	ReLU Oi <latexit sha1_base64="8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit><latexit sha1_base64="8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit><latexit sha1_base64="8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit><latexit sha1_base64="8XCJfZLXz6f3NZoZnnjJ2Skt/Dw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxp0V7AOaUCbTSTt0MgkzN0IJ/Q03LhRx68+482+ctllo64GBwzn3cs+cMJXCoOt+O6W19Y3NrfJ2ZWd3b/+genjUNkmmGW+xRCa6G1LDpVC8hQIl76aa0ziUvBOOb2d+54lrIxL1iJOUBzEdKhEJRtFKvh9THIVRfj/ti3615tbdOcgq8QpSgwLNfvXLHyQsi7lCJqkxPc9NMcipRsEkn1b8zPCUsjEd8p6lisbcBPk885ScWWVAokTbp5DM1d8bOY2NmcShnZxlNMveTPzP62UYXQe5UGmGXLHFoSiTBBMyK4AMhOYM5cQSyrSwWQkbUU0Z2poqtgRv+curpH1R99y693BZa9wUdZThBE7hHDy4ggbcQRNawCCFZ3iFNydzXpx352MxWnKKnWP4A+fzB0Chkc8=</latexit> r <latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit><latexit sha1_base64="juw6eQUUFM3i37Bw8Ysq5tbV1Ao=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+8A2lMl00g6dTMLMjVBC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHFtRKwecJpwP6IjJULBKFrpsR9RHAdhpmeDas2tu3OQVeIVpAYFmoPqV38YszTiCpmkxvQ8N0E/oxoFk3xW6aeGJ5RN6Ij3LFU04sbP5oln5MwqQxLG2j6FZK7+3shoZMw0CuxkntAse7n4n9dLMbz2M6GSFLlii4/CVBKMSX4+GQrNGcqpJZRpYbMSNqaaMrQlVWwJ3vLJq6R9Uffcund/WWvcFHWU4QRO4Rw8uIIG3EETWsBAwTO8wptjnBfn3flYjJacYucY/sD5/AH0G5EW</latexit> Average Pooling … …wi <latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit><latexit sha1_base64="UBwr2R+8MpY5RFOsjT7bXwdRlsw=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiQi6LLoxmUF+4CmlMn0ph06mYSZiVJCf8ONC0Xc+jPu/BsnbRbaemDgcM693DMnSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWBym/udR1Sax/LBTBPsR3QkecgZNVby/YiacRBmT7MBH1Rrbt2dg6wSryA1KNAcVL/8YczSCKVhgmrd89zE9DOqDGcCZxU/1ZhQNqEj7FkqaYS6n80zz8iZVYYkjJV90pC5+nsjo5HW0yiwk3lGvezl4n9eLzXhdT/jMkkNSrY4FKaCmJjkBZAhV8iMmFpCmeI2K2FjqigztqaKLcFb/vIqaV/UPbfu3V/WGjdFHWU4gVM4Bw+uoAF30IQWMEjgGV7hzUmdF+fd+ViMlpxi5xj+wPn8AX25kfc=</latexit> Multilingual Token Embeddings of the Context on the Right … k <latexit sha1_base64="FN4g9GFe+3ziaTgM0DMwdUeUKp0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit><latexit sha1_base64="FN4g9GFe+3ziaTgM0DMwdUeUKp0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit><latexit sha1_base64="FN4g9GFe+3ziaTgM0DMwdUeUKp0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit><latexit sha1_base64="FN4g9GFe+3ziaTgM0DMwdUeUKp0=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipORmUK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4A0oWM7w==</latexit> CNN CNN ReLU Figure 3: Local Context Encoder, for the right context.
58	18	The local and document context vectors c and d are combined to get the mention context vector g = F2h,h(c⊕ d).
62	31	For instance, knowing the correct type of mention [Liverpool] as sports_team and constraining linking to entities with the relevant type, encourages disambiguation to the correct entity.
63	70	To make the mention context representation g type-aware, we predict the set of fine-grained types of m, T(m) = {t1, ..., t|T(m)|} using g. Each ti belongs to a pre-defined type vocabulary Γ.2 The probability of a type t belonging to T(m) given the mention context is defined as P(t | m) =σ(tTg), where σ is the sigmoid function and t is the learnable embedding for type t. We define a Type-Context loss (TC-LOSS) as, TC-LOSS = BCE(T(m),P(t | m)) (7) where BCE is the Binary Cross-Entropy Loss, − ∑ t∈T(m) log P(t | m)− ∑ t6∈T(m) log(1− P(t | m)) We also incorporate the entity-type information in the entity representations, and define a similar Type-Entity loss (TE-LOSS).
65	13	Gold fine-grained types of the entities can be acquired from resources like Freebase (Bollacker et al., 2008) or YAGO (Hoffart et al., 2013).
67	43	Candidate generation identifies a small number of plausible entities for a mention m to avoid brute force comparison with all KB entities.
68	33	Given m, candidate generation outputs a list of candidate entities C(m) = {e1, e2, · · · , eK} of size at most K (we use K=20), each associated with a prior probability Pprior(ei | m) indicating the probability of m referring to ei, given only m’s surface.
74	21	5) and prior probability Pprior(e | m) by taking their union: Pmodel(e | m) = Pprior(e | m) + Pcontext(e | m) − Pprior(e | m)× Pcontext(e | m) Inference for the mention m picks the entity, ê = arg max e∈C(m) Pmodel(e | m) (8)
77	13	This way, the model learns the relative weight for each loss term.
79	23	That is, if two languages have training data sizes proportional to α : β, at any time during training, mini-batches seen from them are in the ratio 1α : 1 β .
92	24	TH-Test A subset of the dataset used in (Tsai and Roth, 2016), derived from Wikipedia.3 The mentions in the dataset fall in two categories – easy and hard, where hard mentions are those for which the most likely candidate according to the prior probability (i.e., arg max Pprior(e | m)) is not the correct title.
96	19	It contains documents from discussion forum articles and news.
106	13	Sil et al. (2018) (English Only) uses multilingual embeddings to transfer a pre-trained English entity linking model to perform XEL for Spanish and Chinese.
112	14	In Table 3 and 4 we compare XELMS(mono), which uses monolingual supervision in the target language only, and XELMS(joint), which uses supervi- sion from English in addition to the monolingual supervision, with the state-of-the-art approaches.
118	43	To demonstrate this capability, we train a model, henceforth referred as XELMS(multi), jointly on 5 related languages – Spanish, German, French, Italian and En- glish.
119	13	We compare XELMS(multi) to the respective XELMS(joint) model for each language.
123	18	XELMS(mono+type) and XELMS(joint+type) both improve compared to XELMS(mono) and XELMS(joint) on MCN-TEST and TH-TEST (Table 6 vs Table 3), showing the benefit of using structured knowledge in the form of fine-grained types.
124	15	Similar trends are also seen on TAC15TEST (Table 4), where XELMS(joint+type) improves on the SoTA for Spanish and Chinese.
150	14	We analyze the behavior of XELMS in a lowresource setting, i.e. when some supervision is available in the target language.
154	21	Figure 4 also shows the best results achieved using all available target language supervision (denoted by Lbest).
155	81	For comparison with the mono-lingually supervised model, we also plot the performance of XELMS(mono), which only uses the target language supervision.
157	25	For comparison, a XELMS(mono) model trained on the same number of training mentions is 5-10% behind the respective XELMS(joint) model, showing better utilization of target language supervision by XELMS(joint).
