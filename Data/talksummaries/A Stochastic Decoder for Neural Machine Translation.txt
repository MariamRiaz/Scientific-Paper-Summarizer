6	47	In this work, we propose a more expressive latent variable model that extends the attentionbased architecture of Bahdanau et al. (2014).
7	109	Our model is motivated by the following observation: translations by professional translators vary across translators but also within a single translator (the same translator may produce different translations on different days, depending on his state of health, concentration etc.).
8	59	Neural machine translation (NMT) models are incapable of capturing this variation, however.
10	18	(1) Our proposal is to augment this model with latent sources of variation that are able to represent more of the variation present in the training data.
18	51	We briefly describe its architecture.
22	46	, hm ] = RNN (xm1 ) (2a) t̃i = RNN (ti−1, yi−1) (2b) eij = v ⊤ a tanh ( Wa[t̃i, hj ] ⊤ + ba ) (2c) αij = exp (eij)∑m j=1 exp (eij) (2d) ci = m∑ j=1 αijhj (2e) ti = Wt[t̃i, ci] ⊤ + bt (2f) ϕi = softmax(Woti + bo) (2g) The parameters {Wa,Wt,Wo, ba, bt, bo, va} ⊆ θ are learned during training.
23	39	The model is trained usingmaximum likelihood estimation.
32	43	That translation corpora contain variation is acknowledged by the machine translation community in the design of their evaluation metrics which are geared towards comparing onemachinegenerated translation against several human translations (see e.g. Papineni et al., 2002).
56	19	Since the model contains latent variables and is parametrised by a neural network, it falls into the class of deep generative models (DGMs).
63	32	Let us denote the functions that these networks compute by f .
69	16	This makes the decoder state a function of a random variable and thus the decoder state is itself random.
90	58	Concretely, samples from the inference network condition on the information available to the generation network (Section 3.3) and also on the target words that are yet to be processed by the generative decoder.
104	37	Notice that since the inference network conditions on representations produced by the generator network, a naïve application of backpropagation would update parts of the generator network with gradients computed for the inference network.
118	19	This is especially likely at the beginning of training when the variational approximation does not yet encode much useful information.
126	49	Thus moving the variational approximation back to the prior would likely reduce the reconstruction term since the standard normal prior is not useful for inference purposes.
127	34	This is in stark contrast to Bowman et al. (2016) whose prior was a fixed standard normal distribution.
130	30	Moving the prior towards the variational approximation has another desirable effect.
131	51	The prior can now learn to emulate the variational “lookahead” mechanism without having access to future contexts itself (recall that the inference model has access to future target tokens).
133	20	We report experiments on the IWSLT 2016 data set which contains transcriptions of TED talks and their respective translations.
150	22	In our experiments we tested different latent variable sizes and used KL scaling (see Section 4.1).
175	129	A non-stochastic NMT system would always yield the same translation in this scenario.
176	24	Interestingly, when we applied the sampling procedure to the SENT model it did not produce any variation at all, thus behaving like a deterministic NMT system.
178	32	Like the model of Bowman et al. (2016), SENT presumably tends to ignore the latent variable.
196	17	Wehave presented a recurrent decoder formachine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translation corpora.
201	31	• Latent factor models: our model only contains one source of variation per word.
202	135	A latent factor model such as DARN (Gregor et al., 2014) would consider several sources simultaneously.
203	149	This would also allow us to perform a better analysis of the model behaviour as we could correlate the factors with observed linguistic phenomena.
204	108	• Richer prior and variational distributions: The diagonal Gaussian is likely too simple a distribution to appropriately model the variation in our data.
205	115	Richer distributions computed by normalising flows (Rezende and Mohamed, 2015; Kingma et al., 2016) will likely improve our model.
206	44	• Extension to other architectures: Introducing latent variables into non-autoregressive translation models such as the transformer (Vaswani et al., 2017) should increase their translation ability further.
