0	107	In machine learning and social network problems, information is often encoded in matrix form.
1	37	User profiles in social networks can be embedded into feature matrices; item profiles in recommendation systems can also be modeled as matrices.
2	176	Many medical imaging modalities, such as MRI and CT, also represent data as a stack of images.
5	14	Therefore we propose matrix networks as a general model for data representation.
6	61	A matrix network is defined by a weighted graph whose nodes are matrices.
11	30	As an example, in the following MRI image sequence (figure 1(a)), we sample each frame of the MRI images with i.i.d.
13	123	If we perform matrix completion by nuclear norm minimization on individual frames, we are not able to recover the completely unobserved matrices (figure 1(c)).
14	37	When we build a network on the image frames, in this case, an one-dimensional chain representing the sequence, and assume that the matrices following graph Fourier transform are low-rank, we are able to recover the missing frames, as shown in figure 1(d).
15	9	The ability to recover all matrices from partial observations, especially inferring matrices that are totally unobserved, is crucial to many applications such as the cold start problem in networks.
16	79	Illustrated in figure 2, new items or users in a network, which does not have much information available, need to aggregate information from the network to have an initial estimate of their feature matrices, in order to support inference and decisions.
17	24	Since we model the matrices as nodes on a graph, information from other matrices makes it possible to recover the missing ones.
20	10	For all the matrices after the graph Fourier transform, singular values quickly decrease to almost zero, demonstrating that they are in fact low-rank.
56	36	For a weighted undirected graph G and its adjacent matrix W , the normalized graph Laplacian is defined as L = I − D−1/2WD−1/2, where D is a diagonal matrix with entries Dii = ∑ jWij .
66	25	When ν(U) is close to 1√ N , the eigenvec- tors are non-local, for example, the discrete Fourier transform case, different classes of random graphs(14; 17; 47), and non-random regular graph(5).
71	29	For two matrix networks X,Y on the same graph, we define their convolution as ̂(X ?
72	19	Y is a stack of matrices where each matrix is the matrix multiplication of X̂(k) and Ŷ (k).
73	20	Convolution on a graph is defined as multiplication in the spectral space by generalizing the convolution theorem since it is not clear how to define convolution in the original space.
74	16	Imagine that we observe a few entries Ω(i) of each matrix A(i).
76	18	The projection operator PΩ is defined to project the full matrix network to our partial observation by only retaining entries in the set Ω = ⋃ Ω(i).
79	32	This sampling scheme almost has not been discussed in depth in the literature.
82	84	To recover missing entries, we need structural assumptions about the matrix network A.
83	21	We propose the assumption that A can be well-approximated by the convolution X ?
84	29	Y of two matrix networks X,Y of size m × r and r × n, for some r much smaller than m and n. We will show that under this assumption, accurate completion is possible even if a significant fraction of the matrices are completely unobserved.
85	31	We formulate the completion problem as follows.
87	24	After the graph Fourier transform, Â0(k) are rank r matrices.
89	17	We first consider the noiseless setting where σ = 0.
91	11	We can also consider the bi-convex formulation, which is to minimize the following objective function, Lλ(X,Y ) = ‖AΩ − PΩ(X ?
92	22	This formulation is non-convex but it is computationally efficient in large-scale applications.
94	49	Let us now analyze the theoretical problem: what condition is needed for the non-uniform sampling Ω and for the rank of Â such that our algorithm is guaranteed to perform accurate recovery with high probability?
96	27	It is worth pointing out that the condition is only about the average sampling rate, therefore it includes the interesting case that a subset of matrices is completely unobserved.
