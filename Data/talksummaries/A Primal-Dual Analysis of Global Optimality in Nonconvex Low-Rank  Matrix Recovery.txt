0	39	Low-rank matrix recovery has received increasing attention in recent years, due to its wide range of applications including signal processing, computer vision and collaborative filtering (Rennie & Srebro, 2005; Ahmed & Romberg, 2015).
1	22	The objective is to estimate an unknown rank-r matrix X∗ ∈ Rd1×d2 based on partially observed measurements.
2	24	More formally, low-rank matrix recovery can be formulated as the following optimization problem min X∈C Fn(X) subject to rank(X) ≤ r, (1.1) where Fn : Rd1×d2 → R denotes a general sample loss function with respect to n measurements, and C denotes a constraint set such that X∗ ∈ C. For example, C is set to be Rd1×d2 in matrix sensing (Recht et al., 2010; Negahban & Wainwright, 2011), and is chosen to be the set of incoherent matrices in matrix completion (Rohde et al., 2011; Koltchinskii et al., 2011; Negahban & Wainwright, 2012) and one-bit matrix completion (Cai & Zhou, 2013; Davenport et al., 2014).
3	20	Tremendous efforts have been made to efficiently solve (1.1), among which the most popular ones are nuclear norm relaxation based methods (Srebro et al., 2004; Candès & Tao, 2010; Rohde et al., 2011; Recht et al., 2010; Recht, 2011; Negahban & Wainwright, 2011; 2012; Gui & Gu, 2015).
5	49	To avoid using SVD, the most commonly-used technique is based on BurerMonteiro factorization (Burer & Monteiro, 2003), which reparameterizes the low-rank matrix X as the product of two smaller matrices U ∈ Rd1×r and V ∈ Rd2×r such that X = UV>.
6	93	Instead of optimizing (1.1) directly, we turn to solve the following nonconvex optimization problem min U∈C1,V∈C2 Fn(UV>), (1.2) where C1 ⊆ Rd1×r, C2 ⊆ Rd2×r are some constraint sets induced by C (c.f.
7	20	Note that (1.2) automatically ensures the low-rankness of the estimated matrix.
8	31	A line of research (Bach et al., 2008; Keshavan et al., 2009; Lee et al., 2013; Jain et al., 2013; Bach, 2013; Hardt, 2014; Hardt & Wootters, 2014; Netrapalli et al., 2014; Jain & Netrapalli, 2014; Haeffele et al., 2014; Sun & Luo, 2015; Bhojanapalli et al., 2015; Chen & Wainwright, 2015; Zhao et al., 2015; Tu et al., 2015; Chen & Wainwright, 2015; Zheng & Lafferty, 2015; 2016; Park et al., 2016b; Jin et al., 2016; Gu et al., 2016; Wang et al., 2017a;b; Xu et al., 2017; Zhang et al., 2018) proposed to solve (1.2) based on gradient descent and/or alternating minimization, and established a locally linear convergence property provided that the initial solution falls into a basin of attraction, i.e., a small neighbourhood around the optimum.
9	24	Recently, another line of research (Bhojanapalli et al., 2016; Ge et al., 2016; Park et al., 2016c; Li et al., 2016; Zhu et al., 2017a; Ge et al., 2017) directly characterized the optimization landscape of (1.2) and proved that various low-rank matrix recovery problems, including matrix sensing (Bhojanapalli et al., 2016; Park et al., 2016c; Zhu et al., 2017a), matrix completion (Ge et al., 2016), and robust PCA (Ge et al., 2017), have no spurious local minima, i.e., all local minima are global ones.
10	31	Based on existing results on finding local minimum for certain nonconvex problems (Ge et al., 2015; Carmon et al., 2016; Agarwal et al., 2016; Jin et al., 2017), they further showed that (1.2) can be successfully solved by saddle-avoiding algorithms, such as perturbed gradient descent.
11	28	However, none of the aforementioned work is generic enough to cover objective functions beyond square loss, e.g., the sample loss function for one-bit matrix completion (Davenport et al., 2014).
12	11	Following the second line of research, we propose a primaldual analysis to characterize the landscape of general objective functions in nonconvex low-rank matrix recovery including both square loss and beyond.
13	29	By using restricted strongly convex and smooth conditions (Negahban et al., 2009; Negahban & Wainwright, 2011), we are able to characterize a large family of low-rank problems.
14	49	To incorporate the widely-used incoherence constraints for low-rank matrix estimation, we propose to analyze the corresponding Lagrangian function rather than the primal objective function and use the Karush-Kuhn-Tucker (KKT) condition (Nocedal & Wright, 2006) to characterize the local minima of (1.2).
17	3	Our major contributions are further highlighted as follows.
18	3	• Our general framework can be applied to any loss function that satisfies the restricted strongly convex and smooth conditions (c.f.
20	5	All the existing theoretical analyses (Bhojanapalli et al., 2016; Ge et al., 2016; Park et al., 2016c; Li et al., 2016; Zhu et al., 2017a; Ge et al., 2017) are limited to square loss, thus we resolve an open problem raised in Ge et al. (2017) regarding the characterization of global geometry for one-bit matrix completion.
22	13	In particular, our analysis suggests there are no spurious local minima in noisy matrix completion, provided that the number of observations is O(r2d log d).
23	54	Compared with existing studies (Ge et al., 2016; 2017) whose sample complexity scales to the fourth power with the rank, the sample requirement of our method matches the best-known sample complexity of matrix completion using nonconvex optimization algorithm (Zheng & Lafferty, 2016) under the incoherence condition.
24	67	• Compared with the seminal work (Ge et al., 2016; 2017) along this line that makes use of ad hoc regularizer to deal with incoherence constraints, our primaldual analytic framework directly characterizes the global geometry of constrained nonconvex optimization problem for low-rank matrix recovery using duality theory.
25	40	We believe the Lagrangian based proof technique is of independent interest, which can be extended to handle more general inequality constraints in other nonconvex problems.
41	4	The remainder of this paper is organized as follows.
42	7	We formally state the general low-rank matrix recovery problem and introduce two specific applications in Section 2.
