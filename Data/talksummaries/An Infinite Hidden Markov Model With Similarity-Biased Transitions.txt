0	37	The hierarchical Dirichlet process hidden Markov model (HDP-HMM) (Beal et al., 2001; Teh et al., 2006) is a Bayesian model for time series data that generalizes the conventional hidden Markov Model to allow a countably infinite state space.
1	14	The hierarchical structure ensures that, despite the infinite state space, a common set of destination states will be reachable with positive probability from each source state.
2	26	The HDP-HMM can be characterized by the following generative process.
4	28	A top-level sequence of state weights, β = (β1, β2, .
11	11	The actual transition distribution, πj , from state j, is drawn from another DP with concentration α and base measure G0: πj i.i.d.∼ DP(αG0) j = 0, 1, 2, .
18	42	(5) An alternative approach that treats self-transitions as special is the HDP Hidden Semi-Markov Model (HDPHSMM; Johnson & Willsky (2013)), wherein state duration distributions are modeled separately, and ordinary selftransitions are ruled out.
19	61	However, while both of these models have the ability to privilege self-transitions, they contain no notion of similarity for pairs of states that are not identical: in both cases, when the transition matrix is integrated out, the prior probability of transitioning to state j′ depends only on the top-level stick weight associated with state j′, and not on the identity or parameters of the previous state j.
20	13	The two main contributions of this paper are (1) a generalization of the HDP-HMM, which we call the HDP-HMM with local transitions (HDP-HMM-LT) that allows for a geometric structure to be defined on the latent state space, so that “nearby” states are a priori more likely to have transitions between them, and (2) a simple Gibbs sampling algorithm for this model.
31	13	Factorial HMMs (Ghahramani et al., 1997) are commonly used in this setting, but this ignores dependence among chains, and hence may do poorly when some combinations of states are much more probable than suggested by the chain-wise dynamics.
33	27	This property is arguably present in musical harmony, where consecutive chords are often (near-)neighbors in the “circle of fifths”, and small steps along the circle are more common than large ones.
40	16	We wish to add to the transition model the concept of a transition to a “nearby” state, where transitions between states j and j′ are more likely a priori to the extent that they are “nearby” in some similarity space.
41	18	In order to accomplish this, we first consider an alternative construction of the transition distributions, based on the Normalized Gamma Process representation of the DP (Ishwaran & Zarepour, 2002; Ferguson, 1973).
49	11	In the HDP prior, the rows of the transition matrix are conditionally independent.
50	14	We wish to relax this assumption, to incorporate possible prior knowledge that certain pairs of states are “nearby” in some sense and thus more likely than others to produce large transition weights between them (in both directions); that is, transitions are likely to be “local”.
69	34	If we do not observe the jumps directly, but instead an observation is generated once per jump from a distribution that depends on the state being jumped to, then we have an ordinary HMM whose transition matrix is obtained by normalizing π; that is, we have the HDP-HMM.
71	35	Suppose each jump attempt from state j to state j′ has probability (1 − φjj′) of failing, in which case no transition occurs and no observation is generated.
73	19	The probability that the first successful jump is to state j′ (that is, that zt+1 = j′) is proportional to the rate of successful jump attempts to j′, which is πjj′φjj′ .
74	39	Conditioned on zt, the holding time, ũt, is independent of zt+1 and is distributed as Exp(Tzt).
75	24	We denote the total time spent in state j by uj = ∑ t:zt=j ũt, where, as the sum of i.i.d.
76	21	Exponentials, uj | z,π,θ ind.∼ Gamma(nj·, Tj) (11) During this period there will be qjj′ failed attempts to jump to state j′, where qjj′ ∼ Poisson(ujπjj′(1− φjj′)) are independent.
83	14	according to a state-specific duration distribution, and sample the latent state sequence using a suitable semi-Markov message passing algorithm (Johnson & Willsky, 2013).
90	12	Nonparametric extensions of the factorial HMM, such as the infinite factorial hidden Markov Model (Gael et al., 2009) and the infinite factorial dynamic model (Valera et al., 2015), have been developed in recent years by making use of the Indian Buffet Process (Ghahramani & Griffiths, 2005) as a state prior.
106	13	We sample the hidden state sequence, z, jointly with the auxiliary variables, which consist of u, Q, M, r and w. The joint conditional distribution of these variables is defined directly by the generative model: p(D) = p(z)p(u | z)p(Q |u)p(M | z,Q)p(r |M)p(w |M) Since we are conditioning on the transition matrix, we can sample the entire sequence z jointly with the forwardbackward algorithm, as in an ordinary HMM.
108	11	Having done this, we can sample u, Q, M, r andw from their forward distributions.
112	20	There is no general-purpose method for sampling `, or for sampling θ in the dependent case, due to the dependence on the form of φ and on the emission model, but specific instances are illustrated in the experiments below.
143	13	Sampling W and Σ Conditioned on Y and θ∗, W and Σ can be sampled as in Bayesian linear regression.
146	10	Results We attempted to infer the binary speaker matrices using five models: (1) a binary-state Factorial HMM (Ghahramani et al., 1997), where individual binary speaker sequences are modeled as independent, (2) an ordinary HDP-HMM without local transitions (Teh et al., 2006), where the latent states are binary vectors, (3) a Sticky HDPHMM (Fox et al., 2008), (4) our HDP-HMM-LT model, and (5) a model that combines the Sticky and LT properties3.
149	10	We evaluated the models at each iteration using both the Hamming distance between inferred and ground truth state matrices and F1 score.
174	17	Although the LT model does not achieve as close a fit to the training data, its generalization performance is better, suggesting that the vanilla HDP-HMM is overfitting.
177	13	We have defined a new probabilistic model, the Hierarchical Dirichlet Process Hidden Markov Model with Local Transitions (HDP-HMM-LT), which generalizes the HDP-HMM by allowing state space geometry to be represented via a similarity kernel, making transitions between “nearby” pairs of states (“local” transitions), more likely a priori.
