{"sections": [{"heading": "1 Introduction", "text": "Hierarchical clustering (HC) is a widely used data analysis tool, ubiquitous in information retrieval, data mining, and machine learning (see a survey by Berkhin [2006]). This clustering technique represents a given dataset as a binary tree; each leaf represents an individual data point and each internal node represents a cluster on the leaves of its descendants. HC has become the most popular method for gene expression data analysis Eisen et al. [1998], and also has been used in the analysis of social networks Leskovec et al. [2014], Mann et al. [2008], bioinformatics Diez et al. [2015], image and text classification Steinbach et al. [2000], and even in analysis of financial markets Tumminello et al. [2010]. It is attractive because it provides richer information at all levels of granularity simultaneously, compared to more traditional flat clustering approaches like k-means or k-median.\nRecently, Dasgupta [2016] formulated HC as a combinatorial optimization problem, giving a principled way to compare the performance of different HC algorithms. This optimization viewpoint has since received a lot of attention Roy and Pokutta [2016], Charikar and Chatziafratis [2017], Cohen-Addad et al. [2017], Moseley and Wang [2017], Cohen-Addad et al. [2018] that has led not only to the development of new algorithms but also to theoretical justifications for the observed success of popular HC algorithms (e.g. average-linkage).\nHowever, in real applications of clustering, the user often has background knowledge about the data that may not be captured by the input to the clustering algorithm. There is a rich body of work on constrained (flat) clustering formulations that take into account such user input in the form of \u201ccannot link\u201d and \u201cmust link\u201d constraints Wagstaff and Cardie [2000], Wagstaff et al. [2001],\nar X\niv :1\n80 5.\n09 47\n6v 2\n[ cs\n.D S]\n1 4\nJu l 2\nBilenko et al. [2004], Rangapuram and Hein [2012]. Very recently, \u201csemi-supervised\u201d versions of HC that incorporate additional constraints have been studied Vikram and Dasgupta [2016], where the natural form of such constraints is triplet (or \u201cmust link before\u201d) constraints ab|c1: these require that valid solutions contain a sub-cluster with a, b together and c previously separated from them.2 Such triplet constraints, as we formally show later, can encode more general structural constraints in the form of rooted subtrees. Surprisingly, such simple triplet constraints already pose significant challenges for bottom-up linkage methods. (Figure 1).\nOur work is motivated by applying the optimization lens to study the interaction of hierarchical clustering algorithms with structural constraints. Constraints can be fairly naturally incorporated into top-down (i.e. divisive) algorithms for hierarchical clustering; but can we establish guarantees on the quality of the solution they produce? Another issue is that incorporating constraints from multiple experts may lead to a conflicting set of constraints; can the optimization viewpoint of hierarchical clustering still help us obtain good solutions even in the presence of infeasible constraints? Finally, different objective functions for HC have been studied in the literature; do algorithms designed for these objectives behave similarly in the presence of constraints? To the best of our knowledge, this is the first work to propose a unified approach for constrained HC through the lens of optimization and to give provable approximation guarantees for a collection of fast and simple top-down algorithms that have been used for unconstrained HC in practice (e.g. community detection in social networks Mann et al. [2008]).\nBackground on Optimization View of HC. Dasgupta [2016] introduced a natural optimization framework for HC. Given a weighted graph G(V,E,w) and pairwise similarities wij \u2265 0 between the n data points i, j \u2208 V , the goal is to find a hierarchical tree T \u2217 such that\nT \u2217 = arg min all trees T \u2211 (i,j)\u2208E wij \u00b7 |Tij | (1)\nwhere Tij is the subtree rooted at the lowest common ancestor of i, j in T and |Tij | is the number 1Hierarchies on data imply that all datapoints are linked at the highest level and all are separated at the lowest level, hence \u201ccannot link\u201d and \u201cmust link\u201d constraints are not directly meaningful. 2For a concrete example from taxonomy of species, a triplet constraint may look like (Tuna,Salmon|Lion).\nof leaves it contains.3 We denote (1) as similarity-HC. For applications where the geometry of the data is given by dissimilarities, again denoted by {wij}(i,j)\u2208E , Cohen-Addad et al. [2018] proposed an analogous approach, where the goal is to find a hierarchical tree T \u2217 such that\nT \u2217 = arg max all trees T \u2211 (i,j)\u2208E wij \u00b7 |Tij | (2)\nWe denote (2) as dissimilarity-HC. A comprehensive list of desirable properties of the aformentioned objectives can be found in Dasgupta [2016], Cohen-Addad et al. [2018]. In particular, if there is an underlying ground-truth hierarchical structure in the data, then T \u2217 can recover the ground-truth. Also, both objectives are NP-hard to optimize, so the focus is on approximation algorithms.\nOur Results. i) We design algorithms that take into account both the geometry of the data, in the form of similarities, and the structural constraints imposed by the users. Our algorithms emerge as the natural extensions of Dasgupta\u2019s original recursive sparsest cut algorithm and the recursive balanced cut suggested in Charikar and Chatziafratis [2017]. We generalize previous analyses to handle constraints and we prove an O(k\u03b1n)-approximation guarantee4, thus surprisingly matching the best approximation guarantee of the unconstrained HC problem for constantly many constraints.\nii) In the case of infeasible constraints, we extend the similarity-HC optimization framework, and we measure the quality of a possible tree T by a constraint-based regularized objective. The regularization naturally favors solutions with as few constraint violations as possible and as far down the tree as possible (similar to the motivation behind similarity-HC objective). For this problem, we provide a top-down O(k\u03b1n)-approximation algorithm by drawing an interesting connection to an instance of the hypergraph sparsest cut problem.\niii) We then change gears and study the dissimilarity-HC objective. Surprisingly, we show that known top-down techniques do not cope well with constraints, drawing a contrast with the situation for similarity-HC. Specifically, the (locally) densest cut heuristic performs poorly even if there is only one triplet constraint, blowing up its approximation factor to O(n). Moreover, we improve upon the state-of-the-art in Cohen-Addad et al. [2018], by showing a simple randomized partitioning is a 23 -approximation algorithm. We also give a deterministic local-search algorithm with the same worst-case guarantee. Furthermore, we show that our randomized algorithm is robust under constraints, mainly because of its \u201cexploration\u201d behavior. In fact, besides the number of constraints, we propose an inherent notion of dependency measure among constraints to capture this behavior quantitatively. This helps us not only to explain why \u201cnon-exploring\u201d algorithms may perform poorly, but also gives tight guarantees for our randomized algorithm.\nExperimental Results. We run experiments on the Zoo dataset [Lichman, 2013] to demonstrate our approach and the performance of our algorithms for a taxonomy application. We consider a setup where there is a ground-truth tree and extra information regarding this tree is provided for the algorithm in the form of triplet constraints. The upshot is we believe specific variations of our algorithms can exploit this information; In this practical application, our algorithms have around %9 imrpvements in the objective compared to the naive recursive sparsest cut proposed in Dasgupta [2016] that does not use this information. See Appendix B for more details on the setup and precise conclusions of our experiments.\n3Observe that in HC, all edges get cut eventually. Therefore it is better to postpone cutting \u201cheavy\u201d edges to when the clusters become small, i.e .as far down the tree as possible.\n4For n data points, \u03b1n = O( \u221a logn) is the best approximation factor for the sparsest cut and k is the number of\nconstraints.\nConstrained HC work-flow in Practice. Throughout this paper, we develop different tools to handle user-defined structural constraints for hierarchical clustering. Here we describe a recipe on how to use our framework in practice.\n(1) Preprocessing constraints to form triplets. User-defined structural constraints as rooted binary subtrees are convenient for the user and hence for the usability of our algorithm. The following proposition (whose proof is in the supplement) allows us to focus on studying HC with just triplet constraints.\nProposition 1. Given constraints as a rooted binary subtree T on k data points (k \u2265 3), there is linear time algorithm that returns an equivalent set of at most k triplet constraints.\n(2) Detecting feasibility. The next step is to see if the set of triplet constraints is consistent, i.e. whether there exists a HC satisfying all the constraints. For this, we use a simple linear time algorithm called BUILD Aho et al. [1981].\n(3) Hard constraints vs. regularization. BUILD can create a hierarchical decomposition that satisfies triplet constraints, but ignores the geometry of the data, whereas our goal here is to consider both simultaneously. Moreover, in the case that the constraints are infeasible, we aim to output a clustering that minimizes the cost of violating constraints combined with the cost of the clustering itself. \u2022 Feasible instance: to output a feasible HC, we propose using Constrained Recursive Sparsest Cut (CRSC) or Constrained Recursive Balanced Cut (CRBC): two simple top-down algorithms which are natural adaptations of recursive sparsest cut [Mann et al., 2008, Dasgupta, 2016] or recursive balanced cut Charikar and Chatziafratis [2017] to respect constraints (Section 2). \u2022 Infeasible instance: in this case, we turn our attention to a regularized version of HC, where the cost of violating constraints is added to the tree cost. We then propose an adaptation of CRSC, namely Hypergraph Recursive Sparsest Cut (HRSC) for the regularized problem (Section 3).\nReal-world application example. In phylogenetics, which is the study of the evolutionary history and relationships among species, an end-user usually has access to whole genomes data of a group of organisms. There are established methods in phylogeny to infer similarity scores between pairs of datapoints, which give the user the similarity weights wij . Often the user also has access to rare structural footprints of a common ancestry tree (e.g. through gene rearrangement data, gene inversions/transpositions etc., see Patan\u00e9 et al. [2018]). These rare, yet informative, footprints play the role of the structural constraints. The user can follow our pre-processing step to get triplet constraints from the given rare footprints, and then use Aho\u2019s BUILD algorithm to choose between regularized or hard version of the HC problem. The above illustrates how to use our workflow and why using our algorithms facilitates HC when expert domain knowledge is available.\nFurther related work. Similar to Vikram and Dasgupta [2016], constraints in the form of triplet queries have been used in an (adaptive) active learning framework by Tamuz et al. [2011], Emamjomeh-Zadeh and Kempe [2018], showing that approximately O(n log n) triplet queries are enough to learn an underlying HC. Other forms of user interaction in order to improve the quality of the produced clusterings have been used in Balcan and Blum [2008], Awasthi et al. [2014] where they prove that interactive feedback in the form of cluster split/merge requests can lead to significant improvements. Robust algorithms for HC in the presence of noise were studied in Balcan et al. [2014] and a variety of sufficient conditions on the similarity function that would allow linkage-style methods to produce good clusters was explored in Balcan et al. [2008]. On a different setting, the notion of triplets has been used as a measure of distance between hierarchical decomposition trees\non the same data points Brodal et al. [2013]. More technically distant analogs of how to use relations among triplets points have recently been proposed in Kleindessner and von Luxburg [2017] for defining kernel functions corresponding to high-dimensional embeddings."}, {"heading": "2 Constrained Sparsest (Balanced) Cut", "text": "Given an instance of the constrained hierarchical clustering, our proposed CRSC algorithm uses a blackbox \u03b1n-approximation algorithm for the sparsest cut problem (the best-known approximation factor for this problem is O( \u221a log n) due to Arora et al. [2009]). Moreover, it also maintains the feasibility of the solution in a top-down approach by recursive partitioning of what we call the supergraph G\u2032. Informally speaking, the supergraph is a simple data structure to track the progress of the algorithm and the resolved constraints.\nMore formally, for every constraint ab|c we merge the nodes a and b into a supernode {a, b} while maintaining the edges in G (now connecting to their corresponding supernodes). Note that G\u2032 may have parallel edges, but this can easily be handled by grouping edges together and replacing them with the sum of their weights. We repeatedly continue this merging procedure until there are no more constraints. Observe that any feasible solution needs to start splitting the original graph G by using a cut that is also present in G\u2032. When cutting the graph G\u2032 = (G1, G2), if a constraint ab|c is resolved,5 then we can safely unpack the supernode {a, b} into two nodes again (unless there is another constraint ab|c\u2032 in which case we should keep the supernode). By continuing and recursively finding approximate sparsest cuts on the supergraph G1 and G2, we can find a feasible hierarchical decomposition of G respecting all triplet constraints. Next, we show the approximation guarantees for our algorithm.\nAlgorithm 1 CRSC 1: Given G and the triplet constraints ab|c, run BUILD to create the supergraph G\u2032. 2: Use a blackbox access to an \u03b1n-approximation oracle for the sparsest cut problem, i.e.\narg minS\u2286V wG\u2032 (S,S\u0304) |S|\u00b7|S\u0304| .\n3: Given the output cut (S, S\u0304), separate the graph G\u2032 into two pieces G1(S,E1) and G2(V \\S,E2).\n4: Recursively compute a HC T1 for G1 using only G1\u2019s active constraints. Similarly compute T2 for G2. 5: Output T = (T1, T2).\nAnalysis of CRSC Algorithm. The main result of this section is the following theorem:\nTheorem 1. Given a weighted graph G(V,E,w) with k triplet constraints ab|c for a, b, c \u2208 V , the CRSC algorithm outputs a HC respecting all triplet constraints and achieves an O(k\u03b1n)-approximation for the HC-similarity objective as in (1).\nNotations and Definitions. We slightly abuse notation by having OPT denote the optimum hierarchical decomposition or its optimum value as measured by (1). Similarly for CRSC. For t \u2208 [n], OPT(t) denotes the maximal clusters in OPT of size at most t. Note that OPT(t) induces a partitioning of V . We use OPT(t) to denote edges cut by OPT(t) (i.e. edges with endpoints in different clusters in OPT(t)) or their total weight; the meaning will be clear from context. For convenience, we define\n5A constraint ab|c is resolved, if c gets separated from a, b.\nOPT(0) = \u2211\n(i,j)\u2208E wij . For a cluster A created by CRSC, a constraint ab|c is active if a, b, c \u2208 A, otherwise ab|c is resolved and can be discarded.\nOverview of the Analysis. There are three main ingredients: The first is to view a HC of n datapoints as a collection of partitions, one for each level t = n \u2212 1, . . . , 1, as in [Charikar and Chatziafratis, 2017]. For a level t, the partition consists of maximal clusters of size at most t. The total cost incurred by OPT is then a combination of costs incurred at each level of this partition. This is useful for comparing our CRSC cost with OPT. The second idea is in handling constraints and it is the main obstacle where previous analyses Charikar and Chatziafratis [2017], Cohen-Addad et al. [2018] break down: constraints inevitably limit the possible cuts that are feasible at any level, and since the set of active constraints6 differ for CRSC and OPT, a direct comparison between them is impossible. If we have no constraints, we can charge the cost of partitioning a cluster A to lower levels of the OPT decomposition. However, when we have triplet constraints, the partition induced by the lower levels of OPT in a cluster A will not be feasible in general (Figure 2). The natural way to overcome this obstacle is merging pieces of this partition so as to respect constraints and using higher levels of OPT, but it still may be impossible to compare CRSC with OPT if all pieces are merged. We overcome this difficulty by an indirect comparison between the CRSC cost and lower levels r6kA of OPT, where kA is the number of active constraints in A. Finally, after a cluster-by-cluster analysis bounding the CRSC cost for each cluster, we exploit disjointness of clusters of the same level in the CRSC partition allowing us to combine their costs.\nProof of Theorem 1. We start by borrowing the following facts from [Charikar and Chatziafratis, 2017], modified slightly for the purpose of our analysis (proofs are provided in the supplementary materials).\nFact 1 (Decomposition of OPT). The total cost paid by OPT can be decomposed into costs of the different levels in the OPT partition, i.e. OPT = \u2211n t=0w(OPT(t)). Fact 2 (OPT at scaled levels). Let k \u2264 n6 be the number of constraints. Then, OPT \u2265 1 6k \u00b7\u2211n\nt=0w(OPT(b t 6kc)).\n6All constraints are active in the beginning of CRSC.\nGiven the above facts, we look at any cluster A of size r produced by the algorithm. Here is the main technical lemma that allows us to bound the cost of CRSC for partitioning A.\nLemma 1. Suppose CRSC partitions a cluster A (|A| = r) in two clusters (B1, B2) (w.l.o.g. |B1| = s, |B2| = r\u2212 s, s \u2264 b r2c \u2264 r\u2212 s). Let the size r \u2265 6k and let l = 6kA, where kA denotes the number of active constraints for A. Then: r \u00b7 w(B1, B2) \u2264 4\u03b1n \u00b7 s \u00b7 w(OPT(b rl c) \u2229A).\nProof. The cost incurred by CRSC for partitioning A is r \u00b7 w(B1, B2). Now consider OPT(b rl c). This induces a partitioning of A into pieces {Ai}i\u2208[m], where by design |Ai| = \u03b3i|A|, \u03b3i \u2264 1l , \u2200i \u2208 [m]. Now, consider the cuts {(Ai, A \\ Ai)}. Even though all m cuts are allowed for OPT, for CRSC some of them are forbidden: for example, in Figure 2, the constraints ab|c, de|f render 4 out of the 6 cuts infeasible. But how many of them can become infeasible with kA active constraints? Since every constraint is involved in at most 2 cuts, we may have at most 2kA infeasible cuts. Let F \u2286 [m] denote the index set of feasible cuts, i.e. if i \u2208 F , the cut (Ai, A \\Ai) is feasible for CRSC. To cut A, we use an \u03b1n-approximation of sparsest cut, whose sparsity is upper bounded by any feasible cut:\nw(B1, B2)\ns(r \u2212 s) \u2264 \u03b1n \u00b7 SP.CUT(A) \u2264 \u03b1n min i\u2208F w(Ai, A \\Ai) |Ai||A \\Ai|\n\u2264 \u03b1n \u2211\ni\u2208F w(Ai, A \\Ai)\u2211 i\u2208F |Ai||A \\Ai|\nwhere for the last inequality we used the standard fact that mini \u00b5i\u03bdi \u2264 \u2211 i \u00b5i\u2211 i \u03bdi\nfor \u00b5i \u2265 0 and \u03bdi > 0. We also have the following series of inequalities:\n\u03b1n \u2211 i\u2208F w(Ai, A \\Ai)\u2211 i\u2208F |Ai||A \\Ai| \u2264 \u03b1n 2w(OPT(b rl c) \u2229A) r2 \u2211 i\u2208F \u03b3i(1\u2212 \u03b3i) \u2264 4\u03b1n w(OPT(b rl c) \u2229A) r2\nwhere the first inequality holds because we double count some (potentially all) edges of OPT(b rl c)\u2229A (these are the edges cut by OPT(b rl c) that are also present in cluster A, i.e. they have both endpoints in A) and the second inequality holds because \u03b3i \u2264 16k =\u21d2 1\u2212 \u03b3i \u2265\n6k\u22121 6k and\u2211 i\u2208F \u03b3i(1\u2212 \u03b3i) \u2265 m\u2211 i=1 \u03b3i(1\u2212 \u03b3i)\u2212 2 \u2211 i\u2208[m]\\F 1 6k \u2265 6k \u2212 1 6k m\u2211 i=1 \u03b3i \u2212 2k 6k = 4k \u2212 1 6k \u2265 1/2\nFinally, we are ready to prove the lemma by combining the above inequalities ( r\u2212sr \u2264 1):\nr \u00b7 w(B1, B2) = r \u00b7 s(r \u2212 s) \u00b7 w(B1, B2)\ns(r \u2212 s)\n\u2264 r \u00b7 s(r \u2212 s) \u00b7 4\u03b1n w(OPT(b rl c) \u2229A)\nr2 \u2264 4\u03b1n \u00b7 s \u00b7 w(OPT(b rl c) \u2229A).\nIt is clear that we exploited the charging to lower levels of OPT, since otherwise if all pieces in A were merged, the denominator with the |Ai|\u2019s would become 0. The next lemma lets us combine the costs incurred by CRSC for different clusters A (proof is in the supplementary materials)\nLemma 2 (Combining the costs of clusters in CRSC). The total CRSC cost for partitioning all clusters A into (B1, B2) (with |A| = rA, |B1| = sA) is bounded by:\n(1) \u2211\nA:|A|\u22656k rA \u00b7 w(B1, B2) \u2264 O(\u03b1n) \u00b7 n\u2211 t=0 w(OPT(b t6kc))\n(2) \u2211\nA:|A|<6k\nrAw(B1, B2) \u2264 6k \u00b7 OPT\nCombining Fact 2 and Lemma 2 finishes the proof.\nRemark 1. In the supplementary material, we prove how one can use balanced cut, i.e. finding a cut S such that\narg min S\u2286V :|S|\u2265n/3,|S\u0304|\u2265n/3 wG\u2032(S, S\u0304) (3)\ninstead of sparsest cut, and using approximation algorithms for this problem achieves the same approximation factor as in Theorem 1, but with better running time.\nRemark 2. Optimality of the CRSC algorithm: Note that complexity theoretic lower-bounds for the unconstrained version of HC from Charikar and Chatziafratis [2017] also apply to our setting; more specifically, they show that no constant factor approximation exists for HC assuming the Small-Set Expansion Hypothesis.\nTheorem 2 (The divisive algorithm using balanced cut). Given a weighted graph G(V,E,w) with k triplet constraints ab|c for a, b, c \u2208 V , the constrained recursive balanced cut algorithm CRBC (same as CRSC, but using balanced cut instead of sparsest cut) outputs a HC respecting all triplet constraints and achieves an O(k\u03b1n)-approximation for Dasgupta\u2019s HC objective. Moreover, the running time is almost linear time."}, {"heading": "3 Constraints and Regularization", "text": "Previously, we assumed that constraints were feasible. However, in many practical applications, users/experts may disagree, hence our algorithm may receive conflicting constraints as input. Here we want to explore how to still output a satisfying HC that is a good in terms of objective (1) (similarity-HC) and also respects the constraints as much as possible. To this end, we propose a regularized version of Dasgupta\u2019s objective, where the regularizer measures quantitatively the degree by which constraints get violated.\nInformally, the idea is to penalize a constraint more if it is violated at top levels of the decomposition compared to lower levels. We also allow having different violation weights for different constraints (potentially depending on the expertise of the users providing the constraints). More concretely, inspired by the Dasgupta\u2019s original objective function, we consider the following optimization problem:\nmin T\u2208T ( \u2211 (i,j)\u2208E wij |Tij |+ \u03bb \u00b7 \u2211 ab|c\u2208K cab|c|Tab| \u00b7 1{ab|c is violated} ) , (4)\nwhere T is the set of all possible binary HC trees for the given data points, K is the set of the k triplet constraints, Tab is the size of the subtree rooted at the least common ancestor of a, b, and cab|c is defined as the base cost of violating triplet constraint ab|c. Note that the regularization parameter \u03bb \u2265 0 allows us to interpolate between satisfying the constraints or respecting the geometry of the data.\nHypergraph Recursive Sparsest Cut In order to design approximation algorithms for the regularized objective, we draw an interesting connection to a different problem, which we call 3- Hypergraph Hierarchical Clustering (3HHC). An instance of this problem consists of a hypergraph GH = (V,E,EH) with edges E, and hyperedges of size 3, EH, together with similarity weights for\nedges, {wij}(i,j)\u2208E , and similarity weights for 3-hyperedges,7 {wij|k}(i,j,k)\u2208EH . We now think of HC on the hypergraph GH, where for every binary tree T we define the cost to be the natural extension of Dasgupta\u2019s objective: \u2211\n(i,j)\u2208E\nwij |Tij |+ \u2211\n(i,j,k)\u2208EH wTijk|Tijk| (5)\nwhere wTijk is either equal to wij|k, wjk|i or wki|j , and Tijk is either the subtree rooted at LCA(i, j), 8 LCA(i, k) or LCA(k, j), all depending on how T cuts the 3-hyperedge {i, j, k}. The goal is to find a hierarchical clustering of this hypergraph, so as to minimize the cost (5) of the tree.\nReduction from Regularization to 3HHC. Given an instance of HC with constraints (with their costs of violations) and a parameter \u03bb, we create a hypergraph GH so that the total cost of any binary clustering tree in the 3HHC problem (5) corresponds to the regularized objective of the same tree as in (4). GH has exactly the same set of vertices, (normal) edges and (normal) edge weights as in the original instance of the HC problem. Moreover, for every constraint ab|c (with cost cab|c) it has a hyperedge {a, b, c}, to which we assign three weights wab|c = 0, wac|b = wbc|a = \u03bb \u00b7cab|c. Therefore, we ensure that any divisive algorithm for the 3HHC problem avoids the cost |Tabc|\u00b7\u03bb\u00b7cab|c only if it chops {a, b, c} into {a, b} and {c} at some level, which matches the regularized objective.\nReduction from 3HHC to Hypergraph Sparsest Cut. A natural generalization of the sparsest cut problem for our hypergraphs, which we call Hyper Sparsest Cut (HSC), is the following problem:\narg min S\u2286V\n( w(S, S\u0304) + \u2211 (i,j,k)\u2208EH w S ijk\n|S||S\u0304|\n) ,\nwhere w(S, S\u0304) is the weight of the cut (S, S\u0304) and wSijk is either equal to wij|k, wjk|i or wki|j , depending on how (S, S\u0304) chops the hyperedge {i, j, k}. Now, similar to Charikar and Chatziafratis [2017], Dasgupta [2016], we can recursively run a blackbox approximation algorithm for HSC to solve 3HHC. The main result of this section is the following technical proposition, whose proof is analogous to that of Theorem 1 (provided in the supplementary materials).\nProposition 2. Given the hypergraph GH with k hyperedges, and given access to an algorithm which is \u03b1n-approximation for HSC, the Recursive Hypergraph Sparsest Cut (R-HSC) algorithm achieves an O(k\u03b1n)-approximation.\nReduction from HSC back to Sparsest Cut. We now show how to get an \u03b1n-approximation oracle for our instance of the HSC problem by a general reduction to sparsest cut. Our reduction is simple: given a hypergraph GH and all the weights, create an instance of sparsest cut with the same vertices, (normal) edges and (normal) edge weights. Moreover, for every 3-hyperedge {a, b, c}\n7We have 3 different weights corresponding to the 3 possible ways of partitioning {i, j, k} in two parts: wij|k, wjk|i and wki|j .\n8LCA(i, j) denotes the lowest common ancestor of i, j \u2208 T .\nconsider adding a triangle to the graph, i.e. three weighted edges connecting {a, b, c}, where:\nw\u2032ab = wbc|a + wac|b \u2212 wab|c\n2 = \u03bb \u00b7 cab|c,\nw\u2032ac = wbc|a + wab|c \u2212 wac|b\n2 = 0,\nw\u2032bc = wac|b + wab|c \u2212 wbc|a\n2 = 0.\nThis construction can be seen in Figure 3. The important observation is that w\u2032ab+w \u2032 ac = wbc|a, w\u2032ab+ w\u2032bc = wac|b and w \u2032 bc +w \u2032 ac = wab|c, which are exactly the weights associated with the corresponding splits of the 3-hyperedge {a, b, c}. So, correctness of the reduction9 follows as the weight of each cut is preserved between the hypergraph and the graph after adding the triangles. For a discussion on extending this gadget more generally, see the supplement.\nRemark 3. Reduction to hypergraphs: we would like to emphasize the necessity of the hypergraph version in order for the reduction to work. One might think that just adding extra heavy edges would be sufficient, but there is a technical difficulty with this approach. Consider a triplet constraint ab|c; once c is separated from a and b at some level, there is no extra tendency anymore to keep a and b together (i.e. only the similarity weight should play role after this point). This behavior cannot be captured by only adding heavy-weight edges. Instead, one needs to add a heavy edge between a and b that disappears once c is separated, and this is exactly why we need the hyperedge gadget. One can replace the reduction for a one-shot proof, but we believe it will be less modular and less transparent."}, {"heading": "4 Variations on a Theme", "text": "In this section we study dissimilarity-HC, and we look into the problem of designing approximation algorithms for both unconstrained and constrained hierarchical clustering. In Cohen-Addad et al. [2017], they show that average linkage is a 12 -approximation for this problem and they propose a top-\ndown approach based on locally densest cut achieving a (23\u2212 )-approximation in time O\u0303 ( n2(n+m) ) . Notably, when gets small the running time blows up. Here, we prove that the most natural randomized algorithm for this problem, i.e. recursive random cutting, is a 23 -approximation with expected running time O(n log n). We further derandomize this algorithm to get a simple deterministic local-search style 23 -approximation algorithm.\nIf we also have structural constraints for the dissimilarity-HC, we show that the existing approaches fail. In fact we show that they lead to an \u2126(n)-approximation factor due to the lack of \u201cexploration\u201d (e.g. recursive densest cut). We then show that recursive random cutting is robust to adding user constraints, and indeed it preserves a constant approximation factor when there are, roughly speaking, constantly many user constraints.\n9Since all weights in the final graph are non-negative, standard techniques for Sparsest Cut can be used.\nRandomized 23-approximation. Consider the most natural randomized algorithm for hierarchical clustering, i.e. recursively partition each cluster into two, where each point in the current cluster independently flips an unbiased coin and based on the outcome, it is put in one of the two parts.\nTheorem 3. Recursive-Random-Cutting is a 23 -approximation for maximizing dissimilarity-HC objective.\nProof sketch. An alternative view of Dasgupta\u2019s objective is to divide the reward of the clustering tree between all possible triples {i, j, k}, where (i, j) \u2208 E and k is another point (possibly equal to i or j). Now, in any hierarchical clustering tree, if at the moment right before i and j become separated the vertex k has still been in the same cluster as {i, j}, then this triple contributes wij to the objective function. We claim this event happens with probability exactly 23 . To see this, consider an infinite independent sequence of coin flips for i, j, and k. Without loss of generality, condition on i\u2019s sequence to be all heads. The aforementioned event happens only if j\u2019s first tales in its sequence happens no later than k\u2019s first tales in its sequence. This happens with probability\u2211\ni\u22651 1 2( 1 4) i\u22121 = 23 . Therefore, the algorithm gets the total reward 2n 3 \u2211 (i,j)\u2208E wij in expectation.\nMoreover, the total reward of any hierarchical clustering is upper-bounded by n \u2211\n(i,j)\u2208E wij , which completes the proof of the 23 -approximation.\nRemark 4. This algorithm runs in time O(n log n) in expectation, due to the fact that the binary clustering tree has expected depth O(log n) (see for example Cormen et al. [2009]) and at each level we only perform n operations.\nWe now derandomize the recursive random cutting algorithm using the method of conditional expectations. At every recursion, we go over the points in the current cluster one by one, and decide whether to put them in the \u201cleft\u201d partition or \u201cright\u201d partition for the next recursion. Once we make a decision for a point, we fix that point and go to the next one. Roughly speaking, these local improvements can be done in polynomial time, which will result in a simple local-search style deterministic algorithm.\nTheorem 4. There is a deterministic local-search style 23 -approximation algorithm for maximizing dissimilarity-HC objective that runs in time O(n2(n+m)).\nMaximizing the Objective with User Constraints From a practical point of view, one can think of many settings in which the output of the hierarchical clustering algorithm should satisfy user-defined hard constraints. Now, combining the new perspective of maximizing Dasgupta\u2019s objective with this practical consideration raises a natural question: which algorithms are robust to adding user constraints, in the sense that a simple variation of these algorithms still achieve a decent approximation factor?\n\u2022 Failure of \u201cNon-exploring\u201d Approaches. Surprisingly enough, there are convincing reasons that adapting existing algorithms for maximizing Dasgupta\u2019s objective (e.g. those proposed in Cohen-Addad et al. [2018]) to handle user constraints is either challenging or hopeless. First, bottom-up algorithms, e.g. average-linkage, fail to output a feasible outcome if they only consider each constraint separately and not all the constraints jointly (as we saw in Figure 1). Second, maybe more surprisingly, the natural extension of (locally) Recursive-Densest-Cut10 algorithm proposed in Cohen-Addad et al. [2018] to handle user constraints performs poorly in the worst-case, even\n10While a locally densest cut can be found in poly-time, desnest cut is NP-hard, making our negative result stronger.\nwhen we have only one constraint. Recursive-Densest-Cut proceeds by repeatedly picking the cut that has maximum density, i.e. arg maxS\u2286V w(S,S\u0304) |S|\u00b7|S\u0304| and making two clusters. To handle the user constraints, we run it recursively on the supergraph generated by the constraints, similar to the approach in Section 2. Note that once the algorithm resolves a triplet constraint, it also breaks its corresponding supernode.\nNow consider the following example in Figure 4, in which there is just one triplet constraint ab|c. The weight W should be thought of as large and as small. By choosing appropriate weights on the edges of the clique Kn, we can fool the algorithm into cutting the dense parts in the clique, without ever resolving the ab|c constraint until it is too late. The algorithm gets a gain of O(n3+W ) whereas OPT gets \u2126(nW ) by starting with the removal of the edge (b, c) and then removing (a, b), thus enjoying a gain of \u2248 nW .\n\u2022 Constrained Recursive Random Cutting. The example in Figure 4, although a bit pathological, suggests that a meaningful algorithm for this problem should explore cutting low-weight edges that might lead to resolving constraints, maybe randomly, with the hope of unlocking rewarding edges that were hidden before this exploration.\nFormally, our approach is showing that the natural extension of recursive random cutting for the constrained problem, i.e. by running it on the supergraph generated by constraints and unpacking supernodes as we resolve the constraints (in a similar fashion to CSC), achieves a constant factor approximation when the constraints have bounded dependency. In the remaining of this section, we define an appropriate notion of dependency between the constraints, under the name of dependency measure and analyze the approximation factor of constrained recursive random cutting (Constrained-RRC ) based on this notion.\nSuppose we are given an instance of hierarchical clustering with triplet constraints {c1, . . . , ck}, where ci = xi|yizi,\u2200i \u2208 [k]. For any triplet constraint ci, lets call the pair {yi, zi} the base, and zi the key of the constraint. We first partition our constraints into equivalence classes C1, . . . , CN , where Ci \u2286 {c1, . . . , ck}. For every i, j, the constraints ci and cj belong to the same class C if they share the same base (see Figure 5).\nDefinition 1 (Dependency digraph). The Dependency digraph is a directed graph with vertex set {C1, . . . , CL}. For every i, j, there is a directed edge Ci \u2192 Cj if \u2203 c = x|yz, c\u2032 = x\u2032|y\u2032z\u2032, such that c \u2208 Ci, c\u2032 \u2208 Cj , and either {x, z} = {y\u2032, z\u2032} or {x, y} = {y\u2032, z\u2032} (see Figure 6).\nThe dependency digraph captures how groups of constraints impact each other. Formally, the existence of the edge Ci \u2192 Cj implies that all the constraints in Cj should be resolved before one can separate the two endpoints of the (common) base edge of the constraints in Ci. Remark 5. If the constraints {c1, . . . , ck} are feasible, i.e. there exists a hierarchical clustering that can respect all the constraints, the dependency digraph is clearly acyclic.\nDefinition 2 (Layered dependency subgraph). Given any class C, the layered dependency subgraph of C is the induced subgraph in the dependency digraph by all the classes that are reachable from C. Moreover, the vertex set of this subgraph can be partitioned into layers {I0, I1, . . . , IL}, where L is the maximum length of any directed path leaving C and Il is a subset of classes where the length of the longest path from C to each of them is exactly equal to l (see Figure 7).\nWe are now ready to define a crisp quantity for every dependency graph. This will later help us give a more meaningful and refined beyond-worst-case guarantee for the approximation factor of the Constrained-RRC algorithm.\nDefinition 3 (Dependency measure). Given any class C, the dependency measure of C is defined as\nDM(C) , L\u220f l=0 (1 + \u2211 C\u2032\u2208Il |C\u2032|),\nwhere I0, . . . , IL are the layers of the dependency subgraph of C, as in Definition 2. Moreover, the dependency measure of a set of constraints DMC({c1, . . . , ck}) is defined as maxC DM(C), where the maximum is taken over all the classes generated by {c1, . . . , ck}.\nIntuitively speaking, the notion of the dependency measure quantitatively expresses how \u201cdeeply\u201d the base of a constraint is protected by the other constraints, i.e. how many constraints need to be resolved first before the base of a particular constraint is unpacked and the Constrained-RRC algorithm can enjoy its weight. This intuition is formalized through the following theorem, whose proof is deferred to the supplementary materials.\nTheorem 5. The constrained recursive random cutting (Constrained-RRC ) algorithm is an \u03b1approximation algorithm for maximizing dissimilarity-HC objective objective given a set of feasible constraints {c1, . . . , ck}, where\n\u03b1 = 2(1\u2212 k/n) 3 \u00b7DMC({c1, . . . , ck}) \u2264 2(1\u2212 k/n) 3 \u00b7maxC DM(C)\nCorollary 1. Constrained-RRC is an O(1)-approximation for maximizing dissimilarity-HC objective, given feasible constraints of constant dependency measure."}, {"heading": "5 Conclusion", "text": "We studied the problem of hierarchical clustering when we have structural constraints on the feasible hierarchies. We followed the optimization viewpoint that was recently developed in Dasgupta [2016], Cohen-Addad et al. [2018] and we analyzed two natural top-down algorithms giving provable approximation guarantees. In the case where the constraints are infeasible, we proposed and analyzed a regularized version of the HC objective by using the hypergraph version of the sparsest cut problem. Finally, we also explored a variation of Dasgupta\u2019s objective and improved upon previous techniques, both in the unconstrained and in the constrained setting."}, {"heading": "Acknowledgements", "text": "Vaggos Chatziafratis was partially supported by ONR grant N00014-17-1-2562. Rad Niazadeh was supported by Stanford Motwani fellowship. Moses Charikar was supported by NSF grant CCF1617577 and a Simons Investigator Award. We would also like to thank Leo Keselman, Aditi Raghunathan and Yang Yuan for providing comments on an earlier draft of the paper. We also thank the anonymous reviewers for their helpful comments and suggestions."}, {"heading": "A Supplementary Materials", "text": "A.1 Missing proofs and discussion in Section 2\nProof of Proposition 1. For nodes u, v \u2208 T , let P (u) denote the parent of u in the tree and LCA(u, v) denote the lowest common ancestor of u, v. For a leaf node li, i \u2208 [k], we say that its label is li, whereas for an internal node of T , we say that its label is the label of any of its two children. As long as there are any two nodes a, b that are siblings (i.e. P (a) \u2261 P (b)), we create a constraint ab|c where c is the label of the second child of P (P (a)). We delete leaves a, b from the tree and repeat until there are fewer than 3 leaves left. To see why the above procedure will only create at most k constraints, notice that every time a new constraint is created, we delete two nodes of the given tree T . Since T has k leaves and is binary, it can have at most 2k \u2212 1 nodes in total. It follows that we create at most 2k\u221212 < k triplet constraints. For the equivalence between the constraints imposed by T and the created triplet constraints, observe that all triplet constraints we create are explicitly imposed by the given tree (since we only create constraints for two leaves that are siblings) and that for any three datapoints a, b, c \u2208 T with LCA(a, c)=LCA(b, c), our set of triplet constraints will indeed imply ab|c, because LCA(a, b) appears further down the tree than LCA(a, c) and hence a, b become siblings before a, c or b, c.\nProof of Fact 1 from Charikar and Chatziafratis [2017]. We will measure the contribution of an edge e = (u, v) \u2208 E to the RHS and to the LHS. Suppose that r denotes the size of the minimal cluster in OPT that contains both u and v. Then the contribution of the edge e = (u, v) to the LHS is by definition r \u00b7 we. On the other hand, (u, v) \u2208 OPT(t),\u2200t \u2208 {0, ..., r \u2212 1}. Hence the contribution to the RHS is also r \u00b7 we.\nProof of Fact 2 from Charikar and Chatziafratis [2017]. We rewrite OPT using the fact that\nw(OPT(t)) \u2265 0\nat every level t \u2208 [n]:\n6k \u00b7 OPT = 6k n\u2211 t=0 w(OPT(t))\n= 6k(w(OPT(0)) + \u00b7 \u00b7 \u00b7+ w(OPT(n))) \u2265 6k(w(OPT(0)) + \u00b7 \u00b7 \u00b7+ w(OPT(b n6kc)))\n= n\u2211 t=0 w(OPT(b t6kc))\nProof of Lemma 2. By using the previous lemma we have: CRSC = \u2211 A rAw(B1, B2) \u2264\u2264 O(\u03b1n) \u2211 A sAw(OPT(b rA6kA c) \u2229A)\nObserve that w(OPT(t)) is a decreasing function of t, since as t decreases, more and more edges are getting cut. Hence we can write:\n\u2211 A sA \u00b7 w(OPT(b rA6k c) \u2229A) \u2264 \u2211 A rA\u2211 t=rA\u2212sA+1 w(OPT(b rA6kA c) \u2229A)\nTo conclude with the proof of the first part all that remains to be shown is that:\n\u2211 A rA\u2211 t=rA\u2212sA+1 w(OPT(b t6kA c) \u2229A) \u2264 n\u2211 t=0 w(OPT(b t6kc))\nTo see why this is true consider the clusters A with a contribution to the LHS. We have that rA\u2212sA+1 \u2264 t \u2264 rA, hence |B2| < tmeaning thatA is aminimal cluster of size |A| \u2265 t > |B2| \u2265 |B1|, i.e. if both A\u2019s children are of size less than t, then this cluster A contributes such a term. The set of all such A form a disjoint partition of V because of the definition for minimality (in order for them to overlap in the hierarchical clustering, one of them needs to be ancestor of the other and this cannot happen because of minimality). Since OPT(b t6kc)\u2229A for all such A forms a disjoint partition of OPT(b t6kc), the claim follows by summing up over all t.\nNote that so far our analysis handles clusters A with size rA \u2265 6k. However, for clusters with smaller size rA < 6k we can get away by using a crude bound for bounding the total cost and still not affecting the approximation guarantee that will be dominated by O(k\u03b1n):\u2211\n|A|<6k rAw(B1, B2) < 6k \u00b7 \u2211 ij\u2208E wij = 6k \u00b7 OPT(1) \u2264 6k \u00b7 OPT\nTheorem 6 (The divisive algorithm using balanced cut). Given a weighted graph G(V,E,w) with k triplet constraints ab|c for a, b, c \u2208 V , the constrained recursive balanced cut algorithm (same as CRSC, but using balanced cut instead of sparsest cut) outputs a HC respecting all triplet constraints and achieves an O(k\u03b1n)-approximation for the HC objective (1).\nProof. It is not hard to show that one can use access to balanced cut rather than sparsest cut and achieve the same approximation factor by the recursive balanced cut algorithm.\nWe will follow the same notation as in the sparsest cut analysis and we will use some of the facts and inequalities we previously proved about OPT(t). Again, for a cluster A of size r, the important observation is that the partition A1, . . . , Al (at the end, we will again choose l = 6kA) induced inside the cluster A by OPT( rl ) can be separated into two groups, let\u2019s say (C1, C2) such that r/3 \u2264 |C1|, |C2| \u2264 2r/3. In other words we can demonstrate a Balanced Cut with ratio 13 : 2 3 for the cluster A. Since we cut fewer edges when creating C1, C2 compared to the partitioning of OPT( rl ):\nw(C1, C2) \u2264 w(OPT(b rl c) \u2229A) By the fact we used an \u03b1n-approximation to balanced cut we can get the following inequality\n(similarly to Lemma 1):\nr \u00b7 w(C1, C2) \u2264 O(\u03b1n) \u00b7 s \u00b7 w(OPT(b rl c) \u2229A)\nFinally, we have to sum up over all the clusters A (now in the summation we should write rA, sA instead of just r, s, since there is dependence in A) produced by the constrained recursive balanced cut algorithm for Hierarchical Clustering and we get that we can approximate the HC objective function up to O(k\u03b1n).\nRemark 6. Using balanced-cut can be useful for two reasons. First, the runtime of sparsest and balanced cut on a graph with n nodes and m edges are O\u0303(m+n1+ ). When run recursively however as in our case, taking recursive sparsest cuts might be worse off by a factor of n (in case of unbalanced splits at every step) in the worst case. However, recursive balanced cut is still O\u0303(m+n1+ ). Second, it is known that an \u03b1-approximation for the sparsest cut yields an O(\u03b1)-approximation for balanced cut, but not the other way. This gives more flexibility to the balanced cut algorithm, and there is a chance it can achieve a better approximation factor (although we don\u2019t study it further in this paper)."}, {"heading": "A.2 Missing proofs in Section 3", "text": "Proof sketch of Proposition 2. Here the main obstacle is similar to the one we handled when proving Theorem (1): for a given cluster A created by the R-HSC algorithm, different constraints are, in general, active compared to the OPT decomposition for this cluster A. Note of course, that OPT itself will not respect all constraints, but because we don\u2019t know which constraints are active for OPT, we still need to use a charging argument to low levels of OPT. Observe that here we are allowed to cut an edge ab even if we had the ab|c constraint (incurring the corresponding cost cab|c), however we cannot possibly hope to charge this to the OPT solution, as OPT, for all we know, may have respected this constraint. In the analysis, we crucially use a merging procedure between sub-clusters of A having active constraints between them and this allows us to compare the cost of our R-HSC with the cost of OPT .\n3-hyperedges to triangles for general weights . Even though the general reduction presented in Section 3 (Figure 3) to transform a 3-hyperedge to a triangle is valid even for general instances of HSC with 3-hyperedges and arbitrary weights, the reduced sparsest cut problem may have negative weights, e.g. when wbc|a + wac|b < wab|c. To the best of our knowledge, sparsest cut with negative weights has not been studied. Notice however that if the original weights wbc|a, wac|b, wab|c satisfy the triangle inequality (or as a special case, if two of them are zero which is usually the case when we have a triplet constraints), then we can actually solve (approximately) the HSC instance, as the sparsest cut instance will only have non-negative weights."}, {"heading": "A.3 Missing proofs in Section 4", "text": "Proof of Theorem 3. We start by looking at the objective value of any algorithm as the summation of contributions of different triples i, j and k to the objective, where (i, j) \u2208 E and k is some other point (possibly equal to i or j).\nOBJ = \u2211\n(i,j)\u2208E\nwij |Tij | = \u2211\n(i,j)\u2208E,k\u2208V\nwij1{k \u2208 leaves(Tij)} = \u2211\n(i,j)\u2208E \u2211 k\u2208V Yi,j,k,\nwhere random variable Yi,j,k denotes the contribution of the edge (i, j) and vertex k to the objective value. The vertex k is a leaf of Tij if and only if right before the time that i and j gets separated k is still in the same cluster as i and j. Therefore,\nYi,j,k = wij1{i separates from k no earlier than j }\nWe now show that E [Yi,j,k] = 23wij . Given this, the expected objective value of recursive random cutting algorithm will be at least 2n3 \u2211 (i,j)\u2208E wij . Moreover, the objective value of the optimal\nhierarchical clustering, i.e. maximizer of the Dasgupta\u2019s objective, is no more than n \u2211\n(i,j)\u2208E wij , and we conclude that recursive random cutting is a 23 -approximation. To see why E [Yi,j,k] = 2 3wij , think of randomized cutting as flipping an independent unbiased coin for each vertex, and then deciding on which side of the cut this vertex belongs to based on the outcome of its coin. Look at the sequence of the coin flips of i, j and k. Our goal is to find the probability of the event that for the first time that i and j sequences are not matched, still i\u2019s sequence and k\u2019s sequence are matched up to this point, or still j\u2019s sequence and k\u2019s sequence are matched up to this. The probability of each of these events is equal to 13 . To see this for the first event, suppose i\u2019s sequence is all heads (H). We then need the pair of coin flips of (j, k) to be a sequence of (H,H)\u2019s ending with a (T,H), and this happens with probability \u2211 i\u22651( 1 4) i = 13 . The probability of the second event is similarly calculated. Now, these events are disjoint. Hence, the probability that i is separated from k no earlier than j is exactly 23 , as desired.\nProof of Theorem 4. We derandomize the recursive random cutting algorithm using the method of conditional expectations. At every recursion, we go over the points in the current cluster one by one, and decide whether to put them in the \u201cleft\u201d partition or \u201cright\u201d partition for the next recursion. Once we make a decision for a point, we fix that point and go to the next one. Now suppose for a cluster C we have already fixed points S \u2286 C, and now we want to make a decision for i \u2208 C \\ S. The reward of assigning to left(right) partition is now defined as the expected value of recursive random cutting restricted to C, when the points in S are fixed (i.e. it is already decided which points in S are going to the left partition and which ones are going to the right partition), i goes to the left(right) partition and j \u2208 C \\ ({i} \u222a S) are randomly assigned to either the left or right. Note that these two rewards (or the difference of the two rewards) can be calculated exactly in polynomial time by considering all triples consisting of an edge and another vertex, and then calculating the probability that this triple contributes to the objective function (this is similar to the proof of Theorem 3, and we omit the details for brevity here). Because we know the randomized assignment of i gives a 23 -approximation (Theorem 3), we conclude that assigning to the better of left or right partition for every vertex will remain to be at least a 23 -approximation. For running time, we have at most n clusters to investigate. Moreover, a careful counting argument shows that the total number of operations required to calculate the differences of the rewards of assigning to left and right partitions for all vertices is at most n(n+ 2m). Hence, the running time is bounded by O(n2(n+m)).\nProof sketch of Theorem 5. Before starting to prove the theorem, we prove the following simple lemma.\nLemma 3. There is no edge between any two classes in the same layer Il.\nProof of Lemma 3. If such an edge exists, then there is a path of length l + 1 from C to a class in Il, a contradiction.\nNow, similar to the proof of Theorem 3, we consider every triple {x, y, z}, where (x, y) \u2208 E and z is another point , but this time we only consider z\u2019s that are not involved in any triplet constraint (there are at least n\u2212 k such points). We claim with probability at least 23\u00b7DMC({c1,...,ck}) the supernode containing z is still in the same cluster as supernodes containing x and y right before x and y gets separated. By summing over all such triples, we show that the algorithm gets a gain of at least 2(n\u2212k)3\u00b7DMC({c1,...,ck}) \u2211 (x,y)\u2208E wxy, which proves the \u03b1-approximation as the optimal clustering\nhas a reward bounded by n \u2211\n(x,y)\u2208E wxy. To prove the claim, if (x, y) is not the base of any triplet constraint then a similar argument as in the proof of Theorem 3 shows the desired probability is exactly 23 (with a slight adaptation, i.e. by looking at the coin sequences of supernodes containing x and y, which are going to be disjoint in this case at all iterations, and the coin sequence of z). Now suppose (x, y) is the base of any constraint c and suppose c belongs to a class C. Consider the layered dependency subgraph of C as in Definition 2 and let the layers to be I0, . . . , IL. In order for z to be in the same cluster as x and y when they get separated, a chain of L + 1 independent events needs to happen. These events are defined inductively; for the first event, consider the coin sequence of z, coin sequence of (the supernode containing all the bases of) constraints in \u222aLl=0Il and coin sequences of all the keys of constraints in IL (there are \u2211 C\u2032\u2208IL |C\n\u2032| of them). Without loss of generality, suppose the coin sequence of (the supernode containing) \u222aLl=0Il is all heads. Now the event happens only if at the time z flips its first tales all keys of IL have already flipped at least one tales. Conditioned on this event happening, all the constraints in IL will be resolved and z remains in the same cluster as x and y. Now, remove IL from the dependency subgraph and repeat the same process to define the events 2, . . . , L in a similar fashion. For the lth event to happen, we need to look at 1 + \u2211 C\u2032\u2208IL |C\n\u2032| number of i.i.d. symmetric geometric random variable, and calculate the probability that first of\nthem is no smaller than the rest. This event happens with a probability at least ( 1 + \u2211 C\u2032\u2208IL |C \u2032| )\u22121\n. Moreover the events are independent, as there is no edge between any two classes in Il for l \u2208 [L], and different classes have different keys. After these L events, the final event that needs to happen is when all the constraints are unlocked, and z needs to remain in the same cluster as x and y at the time they get separated. This event happens with probability 23 . Multiplying all of these probabilities due to independence implies the desired approximation factor."}, {"heading": "B Experiments", "text": "The purpose of this section is to present the benefits of incorporating triplet constraints when performing Hierarchical Clustering. We will focus on real data using the Zoo dataset (Lichman [2013]) for a taxonomy application. We demonstrate that using our approach, the performance of simple recursive spectral clustering algorithms can be improved by approximately 9% as measured by the Dasgupta\u2019s Hierarchical Clustering cost function (1). More specifically:\n\u2022 The Zoo dataset : It contains 100 animals forming 7 different categories (e.g. mammals, amphibians etc.). The features of each animal are provided by a 16-dimensional vector containing\ninformation such as if the animal has hair or feathers etc.\n\u2022 Evaluation method : Given the feature vectors, we can create a similarity matrixM(\u00b7, \u00b7) indexed by the labels of the animals. We choose the widely used cosine similarity to create M .\n\u2022 Algorithms: We use a simple implementation of spectral clustering based on the second eigenvector of the normalized Laplacian of M . By applying the spectral clustering algorithm once, we can create two clusters; by applying it recursively we can create a complete hierarchical decomposition, which is ultimately the output of the HC algorithm.\n\u2022 Baseline comparison: Since triplet constraints are especially useful when there is noisy information (i.e. noisy features), we simulate this situation by hiding some of the features of our Zoo dataset. Specifically, when we want to find the target HC tree T \u2217, we use the full 16-dimensional feature vectors, but for the comparison between the unconstrained and the constrained HC algorithms we will use a noisy version of the feature vectors which consists of only the first 10 coordinates from every vector.\nIn more detail, the first step in our experiments is to evaluate the cost of the target clustering T \u2217. For this, we use the full feature vectors and perform repeated spectral clustering to get a hierarchical decomposition (without incorporating any constraints). We call this cost OPT.\nThe second step is to perform unconstrained HC but with noisy information, i.e. to run the spectral clustering algorithm repeatedly on the 10-dimensional feature vectors (again without taking into account any triplet constraints). This will output a hierarchical tree that has cost in terms of the Dasgupta\u2019s HC cost Unconstrained_Noisy_Cost.11\nThe final step is to choose some structural constraints (that are valid in T \u2217)12 and perform again HC with noisy information. We again use the 10-dimensional feature vectors but the spectral clustering algorithm is allowed only cuts that do not violate any of the given structural constraints. Repeating until we get a decomposition gives us the final output which will have cost in terms of the Dasgupta\u2019s HC cost Constrained_Noisy_Cost.\nThe first main result of our experimental evaluation is that the Constrained_Noisy_Cost is surprisingly close to OPT, even though to get the Constrained_Noisy_Cost the features used were noisy and the second main result is that incorporating the structural constraints yields \u2248 9% improvement over the noisy unconstrained version of HC with cost Unconstrained_Noisy_Cost. Now that we have presented the experimental set-up, we can proceed by describing our results and final observations in greater depth."}, {"heading": "B.1 Experimental Results", "text": "We ran our experiments for 20, 50, 80 and 100 animals from the Zoo dataset and for the evaluation of the % improvement in terms of the Dasgupta\u2019s HC cost (1), we used the following formula:"}, {"heading": "Unconstrained_Noisy_Cost\u2212 Constrained_Noisy_Cost OPT", "text": "The improvements obtained due to the constrained version are presented in Table 1.\n11The cost of the trees are always evaluated using the actual similarities obtained from the full feature vectors. 12Here we chose triplet constraints that will induce the same first cut as T \u2217 and required no constraints after that. This corresponds to a high-level separation of the animals, for example to those that are \u201cland\u201d animals versus those that are \u201cwater\u201d animals.\n#animals OPT Unconstrained_Noisy_Cost Constrained_Noisy_Cost % Improvement\n20 1137 1286 1142 12.63 50 23088 25216 23443 7.68 80 89256 99211 90419 9.85 100 171290 190205 173499 9.75"}], "year": 2018, "references": [{"title": "Inferring a tree from lowest common ancestors with an application to the optimization of relational expressions", "authors": ["Alfred V. Aho", "Yehoshua Sagiv", "Thomas G. Szymanski", "Jeffrey D. Ullman"], "venue": "SIAM Journal on Computing,", "year": 1981}, {"title": "Expander flows, geometric embeddings and graph partitioning", "authors": ["Sanjeev Arora", "Satish Rao", "Umesh Vazirani"], "venue": "Journal of the ACM (JACM),", "year": 2009}, {"title": "Local algorithms for interactive clustering", "authors": ["Pranjal Awasthi", "Maria Balcan", "Konstantin Voevodski"], "venue": "In International Conference on Machine Learning,", "year": 2014}, {"title": "Clustering with interactive feedback", "authors": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "In International Conference on Algorithmic Learning Theory,", "year": 2008}, {"title": "A discriminative framework for clustering via similarity functions", "authors": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "year": 2008}, {"title": "Robust hierarchical clustering", "authors": ["Maria-Florina Balcan", "Yingyu Liang", "Pramod Gupta"], "venue": "The Journal of Machine Learning Research,", "year": 2014}, {"title": "A survey of clustering data mining techniques", "authors": ["Pavel Berkhin"], "venue": "In Grouping multidimensional data,", "year": 2006}, {"title": "Integrating constraints and metric learning in semi-supervised clustering", "authors": ["Mikhail Bilenko", "Sugato Basu", "Raymond J Mooney"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "year": 2004}, {"title": "Efficient algorithms for computing the triplet and quartet distance between trees of arbitrary degree", "authors": ["Gerth St\u00f8lting Brodal", "Rolf Fagerberg", "Thomas Mailund", "Christian NS Pedersen", "Andreas Sand"], "venue": "In Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms,", "year": 2013}, {"title": "Approximate hierarchical clustering via sparsest cut and spreading metrics", "authors": ["Moses Charikar", "Vaggos Chatziafratis"], "venue": "In Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2017}, {"title": "Hierarchical clustering beyond the worst-case", "authors": ["Vincent Cohen-Addad", "Varun Kanade", "Frederik Mallmann-Trenn"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Hierarchical clustering: Objective functions and algorithms", "authors": ["Vincent Cohen-Addad", "Varun Kanade", "Frederik Mallmann-Trenn", "Claire Mathieu"], "venue": "In Proceedings of the Twenty-Ninth Annual ACMSIAM Symposium on Discrete Algorithms,", "year": 2018}, {"title": "Introduction to Algorithms, Third Edition", "authors": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest", "Clifford Stein"], "year": 2009}, {"title": "A cost function for similarity-based hierarchical clustering", "authors": ["Sanjoy Dasgupta"], "venue": "In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing,", "year": 2016}, {"title": "A novel brain partition highlights the modular skeleton shared by structure and function", "authors": ["Ibai Diez", "Paolo Bonifazi", "I\u00f1aki Escudero", "Beatriz Mateos", "Miguel A Mu\u00f1oz", "Sebastiano Stramaglia", "Jesus M Cortes"], "venue": "Scientific reports,", "year": 2015}, {"title": "Cluster analysis and display of genome-wide expression patterns", "authors": ["Michael B Eisen", "Paul T Spellman", "Patrick O Brown", "David Botstein"], "venue": "Proceedings of the National Academy of Sciences,", "year": 1998}, {"title": "Adaptive hierarchical clustering using ordinal queries", "authors": ["Ehsan Emamjomeh-Zadeh", "David Kempe"], "venue": "In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2018}, {"title": "Kernel functions based on triplet comparisons", "authors": ["Matth\u00e4us Kleindessner", "Ulrike von Luxburg"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Uci machine learning repository, zoo", "authors": ["Moshe Lichman"], "venue": "URL http://archive.ics. uci.edu/ml/datasets/zoo", "year": 2013}, {"title": "The use of sparsest cuts to reveal the hierarchical community structure of social networks", "authors": ["Charles F Mann", "David W Matula", "Eli V Olinick"], "venue": "Social Networks,", "year": 2008}, {"title": "Approximation bounds for hierarchical clustering: Average linkage, bisecting k-means, and local search", "authors": ["Benjamin Moseley", "Joshua Wang"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Phylogenomics. In Comparative Genomics, pages 103\u2013187", "authors": ["Jos\u00e9 SL Patan\u00e9", "Joaquim Martins", "Jo\u00e3o C Setubal"], "year": 2018}, {"title": "Constrained 1-spectral clustering", "authors": ["Syama Sundar Rangapuram", "Matthias Hein"], "venue": "In Artificial Intelligence and Statistics,", "year": 2012}, {"title": "Hierarchical clustering via spreading metrics", "authors": ["Aurko Roy", "Sebastian Pokutta"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "A comparison of document clustering techniques", "authors": ["Michael Steinbach", "George Karypis", "Vipin Kumar"], "venue": "In KDD workshop on text mining,", "year": 2000}, {"title": "Adaptively learning the crowd kernel", "authors": ["Omer Tamuz", "Ce Liu", "Serge Belongie", "Ohad Shamir", "Adam Tauman Kalai"], "venue": "In Proceedings of the 28th International Conference on International Conference on Machine Learning,", "year": 2011}, {"title": "Correlation, hierarchies, and networks in financial markets", "authors": ["Michele Tumminello", "Fabrizio Lillo", "Rosario N Mantegna"], "venue": "Journal of Economic Behavior & Organization,", "year": 2010}, {"title": "Interactive bayesian hierarchical clustering", "authors": ["Sharad Vikram", "Sanjoy Dasgupta"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Clustering with instance-level constraints", "authors": ["Kiri Wagstaff", "Claire Cardie"], "year": 2000}, {"title": "Constrained k-means clustering with background knowledge", "authors": ["Kiri Wagstaff", "Claire Cardie", "Seth Rogers", "Stefan Schr\u00f6dl"], "venue": "In ICML,", "year": 2001}], "id": "SP:5ec31c2946d14be31d773b38db4747cea5cc0248", "authors": [{"name": "Vaggos Chatziafratis", "affiliations": []}, {"name": "Rad Niazadeh", "affiliations": []}, {"name": "Moses Charikar", "affiliations": []}], "abstractText": "Hierarchical clustering is a popular unsupervised data analysis method. For many real-world applications, we would like to exploit prior information about the data that imposes constraints on the clustering hierarchy, and is not captured by the set of features available to the algorithm. This gives rise to the problem of hierarchical clustering with structural constraints. Structural constraints pose major challenges for bottom-up approaches like average/single linkage and even though they can be naturally incorporated into top-down divisive algorithms, no formal guarantees exist on the quality of their output. In this paper, we provide provable approximation guarantees for two simple top-down algorithms, using a recently introduced optimization viewpoint of hierarchical clustering with pairwise similarity information [Dasgupta, 2016]. We show how to find good solutions even in the presence of conflicting prior information, by formulating a constraint-based regularization of the objective. We further explore a variation of this objective for dissimilarity information [Cohen-Addad et al., 2018] and improve upon current techniques. Finally, we demonstrate our approach on a real dataset for the taxonomy application.", "title": "Hierarchical Clustering with Structural Constraints"}