14	7	Unlike other simplex parameterizations, the reduced-mean one does not make heuristic pseudolikelihood assumptions.
17	11	The generative model of the Poisson gamma belief network (PGBN) (Zhou et al., 2016a) with L hidden layers, from top to bottom, is expressed as θ (L) j ∼ Gam ( r, 1/c (L+1) j ) , · · · θ (l) j ∼ Gam ( Φ(l+1)θ (l+1) j , 1/c (l+1) j ) , · · · x (1) j ∼ Pois ( Φ(1)θ (1) j ) , θ (1) j ∼ Gam ( Φ(2)θ (2) j , p (2) j 1−p(2)j ) , (1) where the jth observed or latent V -dimensional count vectorsx(1)j ∈ ZV , where Z := {0, 1, .
20	16	The PGBN in (1) can be further extended un- der the Bernoulli-Poisson link as b(1)j = 1 ( x (1) j > 0 ) , and under the Poisson randomized gamma link as y(1)j ∼ Gam ( x (1) j , 1/aj ) , where aj ∼ Gam(e0, 1/f0).
22	7	The information of the whole data set is compressed by the PGBN into the inferred sparse network {Φ(1), .
26	19	To make its inference scalable to allow processing a large amount of data sufficiently fast on a regular personal computer, we resort to SG-MCMC that subsamples the data and utilizes stochastic gradients in each MCMC iteration to generate posterior samples for globally shared model parameters.
27	9	Let us denote the posterior of model parameters z given the data X = {xj}1,J as p (z |X ) ∝ e−H(z), with potential function H (z) = − ln p (z) − ∑ j ln p (xj |z ).
29	66	Thus one has a mini-batch update rule as zt+1 =zt + εt { − [ D(zt)+Q(zt) ] ∇H̃(zt)+Γ(zt) } +N ( 0, εt [ 2D (zt)− εtB̂t ]) , (4) where εt denotes step sizes, H̃ (z) = − ln p (z) − ρ ∑ x∈X̃ ln p (x |z ), X̃ the mini-batch, ρ the ratio of the dataset size |X| to the mini-batch size |X̃|, and B̂t an estimate of the stochastic gradient noise variance satisfying a positive definite constraint as 2D (zt)− εtB̂t 0.
31	52	SGRLD is designed to solve the inference on the probability simplex, where four different parameterizations of the simplex-constrained basis vectors are discussed, including reduced-mean, expanded-mean, reduced-natural, and expanded-natural.
32	49	Here, we consider both expandedmean, previously shown to provide the best overall results, and reduced-mean, which, although discarded in Patterson & Teh (2013) due to its unstable gradients, is used in this paper to produce state-of-the-art results.
33	140	Let us denote φk ∈ RV+ as a vector on the probability simplex, φ̂k ∈ RV+ as a nonnegative vector, and ϕk ∈ RV−1+ as a nonnegative vector constrained with ϕ·k := ∑V−1 v=1 ϕvk ≤ 1.
34	35	For convenience, the symbol “·” will denote the operation of summing over the corresponding index.
35	134	SGRLD focuses on a single-layer model with a multinomial likelihood nk ∼ Mult (n·k,φk) and a Dirichlet distributed prior φk ∼ Dir (η1V ).
36	122	For inference, it adopts the expanded-mean parameterization of φk and makes a heuristic assumption that n·k ∼ Pois ( φ̂·k ) .
37	60	While that heuristic pseudolikelihood assumption of SGRLD is neither supported by the original generative model nor rigorously justified in theory, it converts a Dirichlet-multinomial model into a gamma-Poisson one, allowing a simple sampling equation for φ̂k as( φ̂k ) t+1 = ∣∣∣(φ̂k)t+εt[(nk+η)−(n·k+φ̂·k)(φk)t] +N ( 0, 2εtdiag [( φ̂k ) t ])∣∣∣ , (5) where the absolute operation |·| is used to ensure positivevalued φ̂k.
38	4	Below we show how to eliminate that heuristic assumption by parameterizing φk with reduced-mean, and develop efficient SG-MCMC for the PGBN, which reduces to LDA when the number of hidden layers is one.
39	20	While the original construction of PGBN in (1) makes it seemingly impossible to compute the FIM, as shown in Appendix A, we find that, by exploiting data augmentation and marginalization techniques, the PGBN generative model can be rewritten under an alternative representation that marginalizes out all the gamma distributed hidden units, as shown in the following Lemma, where Log(·) denotes the logarithmic distribution (Johnson et al., 1997), m ∼ SumLog(x, p) represents the sum-logarithmic distribution generated with m = ∑x i=1 ui, ui ∼ Log(p) (Zhou et al., 2016b).
