{"sections": [{"heading": "1. Introduction", "text": "Matrix balancing is the problem of rescaling a given square nonnegative matrix A \u2208 Rn\u00d7n\u22650 to a doubly stochastic matrix RAS, where every row and column sums to one, by multiplying two diagonal matrices R and S. This is a fundamental process for analyzing and comparing matrices in a wide range of applications, including input-output analysis in economics, called the RAS approach (Parikh, 1979; Miller & Blair, 2009; Lahr & de Mesnard, 2004), seat assignments in elections (Balinski, 2008; Akartunal\u0131 &\n1National Institute of Informatics 2JST PRESTO 3RIKEN Brain Science Institute 4Graduate School of Frontier Sciences, The University of Tokyo 5RIKEN AIP 6NIMS. Correspondence to: Mahito Sugiyama <mahito@nii.ac.jp>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nEvery ber sums to 1\nGiven tensor A\nMultistochastic tensor A\u2019 Submanifold (\u03b2)\nProbability distribution P\nStatistical manifold (dually at Riemannian manifold)\nProjectionTensor balancing\nProjected distribution P\u03b2\nFigure 1. Overview of our approach.\nKnight, 2016), Hi-C data analysis (Rao et al., 2014; Wu & Michor, 2016), the Sudoku puzzle (Moon et al., 2009), and the optimal transportation problem (Cuturi, 2013; Frogner et al., 2015; Solomon et al., 2015). An excellent review of this theory and its applications is given by Idel (2016).\nThe standard matrix balancing algorithm is the SinkhornKnopp algorithm (Sinkhorn, 1964; Sinkhorn & Knopp, 1967; Marshall & Olkin, 1968; Knight, 2008), a special case of Bregman\u2019s balancing method (Lamond & Stewart, 1981) that iterates rescaling of each row and column until convergence. The algorithm is widely used in the above applications due to its simple implementation and theoretically guaranteed convergence. However, the algorithm converges linearly (Soules, 1991), which is prohibitively slow for recently emerging large and sparse matrices. Although Livne & Golub (2004) and Knight & Ruiz (2013) tried to achieve faster convergence by approximating each step of Newton\u2019s method, the exact Newton\u2019s method with quadratic convergence has not been intensively studied yet.\nAnother open problem is tensor balancing, which is a generalization of balancing from matrices to higher-order multidimentional arrays, or tensors. The task is to rescale an N th order nonnegative tensor to a multistochastic tensor, in which every fiber sums to one, by multiplying (N \u22121)th order N tensors. There are some results about mathematical properties of multistochastic tensors (Cui et al., 2014; Chang et al., 2016; Ahmed et al., 2003). However, there is no result for tensor balancing algorithms with guaranteed convergence that transforms a given tensor to a multistochastic tensor until now.\nHere we show that Newton\u2019s method with quadratic convergence can be applied to tensor balancing while avoiding solving a linear system on the full tensor. Our strategy is to realize matrix and tensor balancing as projection onto a dually flat Riemmanian submanifold (Figure 1), which is a statistical manifold and known to be the essential structure for probability distributions in information geometry (Amari, 2016). Using a partially ordered outcome space, we generalize the log-linear model (Agresti, 2012) used to model the higher-order combinations of binary variables (Amari, 2001; Ganmor et al., 2011; Nakahara & Amari, 2002; Nakahara et al., 2003), which allows us to model tensors as probability distributions in the statistical manifold. The remarkable property of our model is that the gradient of the manifold can be analytically computed using the Mo\u0308bius inversion formula (Rota, 1964), the heart of combinatorial mathematics (Ito, 1993), which enables us to directly obtain the Jacobian matrix in Newton\u2019s method. Moreover, we show that (n \u2212 1)N entries for the size nN of a tensor are invariant with respect to one of the two coordinate systems of the statistical manifold. Thus the number of equations in Newton\u2019s method is O(nN\u22121).\nThe remainder of this paper is organized as follows: We begin with a low-level description of our matrix balancing algorithm in Section 2 and demonstrate its efficiency in numerical experiments in Section 3. To guarantee the correctness of the algorithm and extend it to tensor balancing, we provide theoretical analysis in Section 4. In Section 4.1, we introduce a generalized log-linear model associated with a partial order structured outcome space, followed by introducing the dually flat Riemannian structure in Section 4.2. In Section 4.3, we show how to use Newton\u2019s method to compute projection of a probability distribution onto a submanifold. Finally, we formulate the matrix and tensor balancing problem in Section 5 and summarize our contributions in Section 6."}, {"heading": "2. The Matrix Balancing Algorithm", "text": "Given a nonnegative square matrix A = (aij) \u2208 Rn\u00d7n\u22650 , the task of matrix balancing is to find r, s \u2208 Rn that satisfy\n(RAS)1 = 1, (RAS)T1 = 1, (1)\nwhere R = diag(r) and S = diag(s). The balanced matrix A\u2032 = RAS is called doubly stochastic, in which each entry a\u2032ij = aijrisj and all the rows and columns sum to one. The most popular algorithm is the Sinkhorn-Knopp algorithm, which repeats updating r and s as r = 1/(As) and s = 1/(ATr). We denote by [n] = {1, 2, . . . , n} hereafter.\nIn our algorithm, instead of directly updating r and s, we update two parameters \u03b8 and \u03b7 defined as\nlog pij = \u2211 i\u2032\u2264i \u2211 j\u2032\u2264j \u03b8i\u2032j\u2032 , \u03b7ij = \u2211 i\u2032\u2265i \u2211 j\u2032\u2265j pi\u2032j\u2032 (2)\nMatrix Constraints for balancing\nfor each i, j \u2208 [n], where we normalized entries as pij = aij/ \u2211 ij aij so that \u2211 ij pij = 1. We assume for simplicity that each entry is strictly larger than zero. The assumption will be removed in Section 5.\nThe key to our approach is that we update \u03b8(t)ij with i = 1 or j = 1 by Newton\u2019s method at each iteration t = 1, 2, . . . while fixing \u03b8ij with i, j \u0338= 1 so that \u03b7(t)ij satisfies the following condition (Figure 2):\n\u03b7 (t) i1 = (n\u2212 i+ 1)/n, \u03b7 (t) 1j = (n\u2212 j + 1)/n.\nNote that the rows and columns sum not to 1 but to 1/n due to the normalization. The update formula is described as \u03b8 (t+1) 11 ... \u03b8 (t+1) 1n \u03b8 (t+1) 21\n... \u03b8 (t+1) n1\n =  \u03b8 (t) 11 ... \u03b8 (t) 1n\n\u03b8 (t) 21 ... \u03b8 (t) n1\n \u2212 J\u22121  \u03b7 (t) 11 \u2212 (n\u2212 1 + 1)/n ... \u03b7 (t) 1n \u2212 (n\u2212 n+ 1)/n \u03b7 (t) 21 \u2212 (n\u2212 2 + 1)/n\n... \u03b7 (t) n1 \u2212 (n\u2212 n+ 1)/n\n , (3)\nwhere J is the Jacobian matrix given as\nJ(ij)(i\u2032j\u2032)= \u2202\u03b7\n(t) ij \u2202\u03b8 (t) i\u2032j\u2032 = \u03b7max{i,i\u2032}max{j,j\u2032}\u2212n2\u03b7ij\u03b7i\u2032j\u2032 , (4)\nwhich is derived from our theoretical result in Theorem 3. Since J is a (2n\u22121)\u00d7(2n\u22121) matrix, the time complexity of each update is O(n3), which is needed to compute the inverse of J .\nAfter updating to \u03b8(t+1)ij , we can compute p (t+1) ij and \u03b7 (t+1) ij by Equation (2). Since this update does not ensure the condition \u2211 ij p (t+1) ij = 1, we again update \u03b8 (t+1) 11 as \u03b8 (t+1) 11 = \u03b8 (t+1) 11 \u2212 log \u2211 ij p (t+1) ij and recompute p (t+1) ij and \u03b7(t+1)ij for each i, j \u2208 [n].\nBy iterating the above update process in Equation (3) until convergence, A = (aij) with aij = npij becomes doubly stochastic."}, {"heading": "3. Numerical Experiments", "text": "We evaluate the efficiency of our algorithm compared to the two prominent balancing methods, the standard SinkhornKnopp algorithm (Sinkhorn, 1964) and the state-of-the-art\nalgorithm BNEWT (Knight & Ruiz, 2013), which uses Newton\u2019s method-like iterations with conjugate gradients. All experiments were conducted on Amazon Linux AMI release 2016.09 with a single core of 2.3 GHz Intel Xeon CPU E5-2686 v4 and 256 GB of memory. All methods were implemented in C++ with the Eigen library and compiled with gcc 4.8.31. We have carefully implemented BNEWT by directly translating the MATLAB code provided in (Knight & Ruiz, 2013) into C++ with the Eigen library for fair comparison, and used the default parameters. We measured the residual of a matrix A\u2032 = (a\u2032ij) by the squared norm \u2225(A\u20321\u22121, A\u2032T1\u22121)\u22252, where each entry a\u2032ij is obtained as npij in our algorithm, and ran each of three algorithms until the residual is below the tolerance threshold 10\u22126.\nHessenberg Matrix. The first set of experiments used a Hessenberg matrix, which has been a standard benchmark for matrix balancing (Parlett & Landis, 1982; Knight & Ruiz, 2013). Each entry of an n \u00d7 n Hessenberg matrix Hn = (hij) is given as hij = 0 if j < i \u2212 1 and hij = 1 otherwise. We varied the size n from 10 to 5, 000, and measured running time (in seconds) and the number of iterations of each method.\nResults are plotted in Figure 3. Our balancing algorithm with the Newton\u2019s method (plotted in blue in the figures)\n1An implementation of algorithms for matrices and third order tensors is available at: https://github.com/ mahito-sugiyama/newton-balancing\nis clearly the fastest: It is three to five orders of magnitude faster than the standard Sinkhorn-Knopp algorithm (plotted in red). Although the BNEWT algorithm (plotted in green) is competitive if n is small, it suddenly fails to converge whenever n \u2265 200, which is consistent with results in the original paper (Knight & Ruiz, 2013) where there is no result for the setting n \u2265 200 on the same matrix. Moreover, our method converges around 10 to 20 steps, which is about three and seven orders of magnitude smaller than BNEWT and Sinkhorn-Knopp, respectively, at n = 100.\nTo see the behavior of the rate of convergence in detail, we plot the convergence graph in Figure 4 for n = 20, where we observe the slow convergence rate of the SinkhornKnopp algorithm and unstable convergence of the BNEWT algorithm, which contrasts with our quick convergence.\nTrefethen Matrix. Next, we collected a set of Trefethen matrices from a collection website2, which are nonnegative diagonal matrices with primes. Results are plotted in Figure 5, where we observe the same trend as before: Our algorithm is the fastest and about four orders of magnitude faster than the Sinkhorn-Knopp algorithm. Note that larger matrices with n > 300 do not have total support, which is the necessary condition for matrix balancing (Knight & Ruiz, 2013), while the BNEWT algorithm fails to converge if n = 200 or n = 300."}, {"heading": "4. Theoretical Analysis", "text": "In the following, we provide theoretical support to our algorithm by formulating the problem as a projection within a statistical manifold, in which a matrix corresponds to an element, that is, a probability distribution, in the manifold.\nWe show that a balanced matrix forms a submanifold and matrix balancing is projection of a given distribution onto the submanifold, where the Jacobian matrix in Equation (4) is derived from the gradient of the manifold.\n2http://www.cise.ufl.edu/research/sparse/ matrices/"}, {"heading": "4.1. Formulation", "text": "We introduce our log-linear probabilistic model, where the outcome space is a partially ordered set, or a poset (Gierz et al., 2003). We prepare basic notations and the key mathematical tool for posets, the Mo\u0308bius inversion formula, followed by formulating the log-linear model."}, {"heading": "4.1.1. MO\u0308BIUS INVERSION", "text": "A poset (S,\u2264), the set of elements S and a partial order \u2264 on S, is a fundamental structured space in computer science. A partial order \u201c\u2264\u201d is a relation between elements in S that satisfies the following three properties: For all x, y, z \u2208 S, (1) x \u2264 x (reflexivity), (2) x \u2264 y, y \u2264 x \u21d2 x = y (antisymmetry), and (3) x \u2264 y, y \u2264 z \u21d2 x \u2264 z (transitivity). In what follows, S is always finite and includes the least element (bottom) \u22a5 \u2208 S; that is, \u22a5 \u2264 x for all x \u2208 S. We denote S \\ {\u22a5} by S+.\nRota (1964) introduced the Mo\u0308bius inversion formula on posets by generalizing the inclusion-exclusion principle. Let \u03b6 :S \u00d7 S \u2192 {0, 1} be the zeta function defined as\n\u03b6(s, x) = { 1 if s \u2264 x, 0 otherwise.\nThe Mo\u0308bius function \u00b5 :S\u00d7S \u2192 Z satisfies \u03b6\u00b5 = I , which is inductively defined for all x, y with x \u2264 y as\n\u00b5(x, y) =  1 if x = y, \u2212 \u2211\nx\u2264s<y \u00b5(x, s) if x < y, 0 otherwise.\nFrom the definition, it follows that\u2211 s\u2208S \u03b6(s, y)\u00b5(x, s) = \u2211 x\u2264s\u2264y\n\u00b5(x, s) = \u03b4xy,\u2211 s\u2208S \u03b6(x, s)\u00b5(s, y) = \u2211 x\u2264s\u2264y \u00b5(s, y) = \u03b4xy (5)\nwith the Kronecker delta \u03b4 such that \u03b4xy = 1 if x = y and \u03b4xy = 0 otherwise. Then for any functions f , g, and h with the domain S such that\ng(x) = \u2211 s\u2208S \u03b6(s, x)f(s) = \u2211 s\u2264x f(s),\nh(x) = \u2211 s\u2208S \u03b6(x, s)f(s) = \u2211 s\u2265x f(s),\nf is uniquely recovered with the Mo\u0308bius function: f(x) = \u2211 s\u2208S \u00b5(s, x)g(s), f(x) = \u2211 s\u2208S \u00b5(x, s)h(s).\nThis is called the Mo\u0308bius inversion formula and is at the heart of enumerative combinatorics (Ito, 1993)."}, {"heading": "4.1.2. LOG-LINEAR MODEL ON POSETS", "text": "We consider a probability vector p on (S,\u2264) that gives a discrete probability distribution with the outcome space S.\nA probability vector is treated as a mapping p :S \u2192 (0, 1) such that \u2211 x\u2208S p(x) = 1, where every entry p(x) is assumed to be strictly larger than zero.\nUsing the zeta and the Mo\u0308bius functions, let us introduce two mappings \u03b8 :S \u2192 R and \u03b7 :S \u2192 R as\n\u03b8(x) = \u2211 s\u2208S \u00b5(s, x) log p(s), (6)\n\u03b7(x) = \u2211 s\u2208S \u03b6(x, s)p(s) = \u2211 s\u2265x p(s). (7)\nFrom the Mo\u0308bius inversion formula, we have log p(x) = \u2211 s\u2208S \u03b6(s, x)\u03b8(s) = \u2211 s\u2264x \u03b8(s), (8)\np(x) = \u2211 s\u2208S \u00b5(x, s)\u03b7(s). (9)\nThey are generalization of the log-linear model (Agresti, 2012) that gives the probability p(x) of an n-dimensional binary vector x = (x1, . . . , xn) \u2208 {0, 1}n as\nlog p(x) = \u2211 i \u03b8ixi + \u2211 i<j \u03b8ijxixj + \u2211 i<j<k \u03b8ijkxixjxk\n+ \u00b7 \u00b7 \u00b7+ \u03b81...nx1x2 . . . xn \u2212 \u03c8,\nwhere \u03b8 = (\u03b81, . . . , \u03b812...n) is a parameter vector, \u03c8 is a normalizer, and \u03b7 = (\u03b71, . . . , \u03b712...n) represents the expectation of variable combinations such that\n\u03b7i = E[xi] = Pr(xi = 1),\n\u03b7ij = E[xixj ] = Pr(xi = xj = 1), i < j, . . .\n\u03b71...n = E[x1 . . . xn] = Pr(x1 = \u00b7 \u00b7 \u00b7 = xn = 1).\nThey coincide with Equations (8) and (7) when we let S = 2V with V = {1, 2, . . . , n}, each x \u2208 S as the set of indices of \u201c1\u201d of x, and the order \u2264 as the inclusion relationship, that is, x \u2264 y if and only if x \u2286 y. Nakahara et al. (2006) have pointed out that \u03b8 can be computed from p using the inclusion-exclusion principle in the log-linear model. We exploit this combinatorial property of the loglinear model using the Mo\u0308bius inversion formula on posets and extend the log-linear model from the power set 2V to any kind of posets (S,\u2264). Sugiyama et al. (2016) studied a relevant log-linear model, but the relationship with Mo\u0308bius inversion formula has not been analyzed yet."}, {"heading": "4.2. Dually Flat Riemannian Manifold", "text": "We theoretically analyze our log-linear model introduced in Equations (6), (7) and show that they form dual coordinate systems on a dually flat manifold, which has been mainly studied in the area of information geometry (Amari, 2001; Nakahara & Amari, 2002; Amari, 2014; 2016). Moreover, we show that the Riemannian metric and connection of our model can be analytically computed in closed forms.\nIn the following, we denote by \u03be the function \u03b8 or \u03b7 and by \u2207 the gradient operator with respect to S+ = S \\{\u22a5}, i.e., (\u2207f(\u03be))(x) = \u2202f/\u2202\u03be(x) for x \u2208 S+, and denote by S the set of probability distributions specified by probability vectors, which forms a statistical manifold. We use uppercase letters P,Q,R, . . . for points (distributions) in S and their lowercase letters p, q, r, . . . for the corresponding probability vectors treated as mappings. We write \u03b8P and \u03b7P if they are connected with p by Equations (6) and (7), respectively, and abbreviate subscripts if there is no ambiguity."}, {"heading": "4.2.1. DUALLY FLAT STRUCTURE", "text": "We show that S has the dually flat Riemannian structure induced by two functions \u03b8 and \u03b7 in Equation (6) and (7). We define \u03c8(\u03b8) as\n\u03c8(\u03b8) = \u2212\u03b8(\u22a5) = \u2212 log p(\u22a5), (10)\nwhich corresponds to the normalizer of p. It is a convex function since we have\n\u03c8(\u03b8) = log \u2211 x\u2208S exp  \u2211 \u22a5<s\u2264x \u03b8(s)  from log p(x) = \u2211 \u22a5<s\u2264x \u03b8(s)\u2212\u03c8(\u03b8). We apply the Legendre transformation to \u03c8(\u03b8) given as\n\u03c6(\u03b7) = max \u03b8\u2032\n( \u03b8\u2032\u03b7 \u2212 \u03c8(\u03b8\u2032) ) , \u03b8\u2032\u03b7 = \u2211 x\u2208S+ \u03b8\u2032(x)\u03b7(x). (11)\nThen \u03c6(\u03b7) coincides with the negative entropy.\nTheorem 1 (Legendre dual). \u03c6(\u03b7) = \u2211 x\u2208S p(x) log p(x).\nProof. From Equation (5), we have\n\u03b8\u2032\u03b7 = \u2211 x\u2208S+  \u2211 \u22a5<s\u2264x \u00b5(s, x) log p\u2032(s) \u2211 s\u2265x p(s)  = \u2211 x\u2208S+ p(x) ( log p\u2032(x)\u2212 log p\u2032(\u22a5) ) .\nThus it holds that \u03b8\u2032\u03b7 \u2212 \u03c8(\u03b8\u2032) = \u2211 x\u2208S p(x) log p\u2032(x). (12)\nHence it is maximized with p(x) = p\u2032(x).\nSince they are connected with each other by the Legendre transformation, they form a dual coordinate system \u2207\u03c8(\u03b8) and \u2207\u03c6(\u03b7) of S (Amari, 2016, Section 1.5), which coincides with \u03b8 and \u03b7 as follows.\nTheorem 2 (dual coordinate system).\n\u2207\u03c8(\u03b8) = \u03b7, \u2207\u03c6(\u03b7) = \u03b8. (13)\nProof. They can be directly derived from our definitions (Equations (6) and (11)) as\n\u2202\u03c8(\u03b8) \u2202\u03b8(x) =\n\u2211 y\u2265x exp (\u2211 \u22a5<s\u2264y \u03b8(s) ) \u2211\ny\u2208S exp (\u2211 \u22a5<s\u2264y \u03b8(s) ) =\u2211 s\u2265x p(s) = \u03b7(x),\n\u2202\u03c6(\u03b7) \u2202\u03b7(x) = \u2202 \u2202\u03b7(x)\n( \u03b8\u03b7 \u2212 \u03c8(\u03b8) ) = \u03b8(x).\nMoreover, we can confirm the orthogonality of \u03b8 and \u03b7 as\nE\n[ \u2202 log p(s)\n\u2202\u03b8(x)\n\u2202 log p(s)\n\u2202\u03b7(y) ] = \u2211 s\u2208S \u03b6(x, s)\u00b5(s, y) = \u03b4xy.\nThe last equation holds from Equation (5), hence the Mo\u0308bius inversion directly leads to the orthogonality.\nThe Bregman divergence is known to be the canonical divergence (Amari, 2016, Section 6.6) to measure the difference between two distributions P and Q on a dually flat manifold, which is defined as\nD [P,Q] = \u03c8(\u03b8P ) + \u03c6(\u03b7Q)\u2212 \u03b8P \u03b7Q. In our case, since we have \u03c6(\u03b7Q) = \u2211\nx\u2208S q(x) log q(x) and \u03b8P \u03b7Q\u2212\u03c8(\u03b8P ) = \u2211 x\u2208S q(x) log p(x) from Theorem 1 and Equation (12), it is given as\nD [P,Q] = \u2211 x\u2208S q(x) log q(x) p(x) ,\nwhich coincides with the Kullback\u2013Leibler divergence (KL divergence) from Q to P : D [P,Q] = DKL [Q,P ]."}, {"heading": "4.2.2. RIEMANNIAN STRUCTURE", "text": "Next we analyze the Riemannian structure on S and show that the Mo\u0308bius inversion formula enables us to compute the Riemannian metric of S. Theorem 3 (Riemannian metric). The manifold (S, g(\u03be)) is a Riemannian manifold with the Riemannian metric g(\u03be) such that for all x, y \u2208 S+\ngxy(\u03be) =  \u2211 s\u2208S [ \u03b6(x, s)\u03b6(y, s)p(s)\u2212 \u03b7(x)\u03b7(y) ] if \u03be = \u03b8,\n\u2211 s\u2208S \u00b5(s, x)\u00b5(s, y)p(s)\u22121 if \u03be = \u03b7.\nProof. Since the Riemannian metric is defined as\ng(\u03b8) = \u2207\u2207\u03c8(\u03b8), g(\u03b7) = \u2207\u2207\u03c6(\u03b7),\nwhen \u03be = \u03b8 we have\ngxy(\u03b8) = \u22022\n\u2202\u03b8(x)\u2202\u03b8(y) \u03c8(\u03b8) =\n\u2202\n\u2202\u03b8(x) \u03b7(y)\n= \u2202\n\u2202\u03b8(x) \u2211 s\u2208S \u03b6(y, s) exp  \u2211 \u22a5<u\u2264s \u03b8(u)\u2212 \u03c8(\u03b8) \n= \u2211 s\u2208S \u03b6(x, s)\u03b6(y, s)p(s)\u2212 |S|\u03b7(x)\u03b7(y).\nWhen \u03be = \u03b7, it follows that\ngxy(\u03b7) = \u22022\n\u2202\u03b7(x)\u2202\u03b7(y) \u03c6(\u03b7) =\n\u2202\n\u2202\u03b7(x) \u03b8(y)\n= \u2202\n\u2202\u03b7(x) \u2211 s\u2264y \u00b5(s, y) log \u2211 u\u2265s \u00b5(s, u)\u03b7(u)  = \u2211 s\u2208S \u00b5(s, x)\u00b5(s, y)p(s)\u22121.\nSince g(\u03be) coincides with the Fisher information matrix,\nE\n[ \u2202\n\u2202\u03b8(x) log p(s)\n\u2202\n\u2202\u03b8(y) log p(s)\n] = gxy(\u03b8),\nE\n[ \u2202\n\u2202\u03b7(x) log p(s)\n\u2202\n\u2202\u03b7(y) log p(s)\n] = gxy(\u03b7).\nThen the Riemannian (Levi\u2013Chivita) connection \u0393(\u03be) with respect to \u03be, which is defined as\n\u0393xyz(\u03be) = 1\n2\n( \u2202gyz(\u03be)\n\u2202\u03be(x) + \u2202gxz(\u03be) \u2202\u03be(y) \u2212 \u2202gxy(\u03be) \u2202\u03be(z) ) for all x, y, z \u2208 S+, can be analytically obtained. Theorem 4 (Riemannian connection). The Riemannian connection \u0393(\u03be) on the manifold (S, g(\u03be)) is given in the following for all x, y, z \u2208 S+,\n\u0393xyz(\u03be) =  1 2 \u2211 s\u2208S ( \u03b6(x, s)\u2212 \u03b7(x) )( \u03b6(y, s)\u2212 \u03b7(y) ) ( \u03b6(z, s)\u2212 \u03b7(z) ) p(s) if \u03be = \u03b8, \u22121 2 \u2211 s\u2208S \u00b5(s, x)\u00b5(s, y)\u00b5(s, z)p(s)\u22122 if \u03be = \u03b7.\nProof. Connections \u0393xyz(\u03b8) and \u0393xyz(\u03b7) can be obtained by directly computing \u2202gyz(\u03b8)/\u2202\u03b8(x) and \u2202gyz(\u03b7)/\u2202\u03b7(x), respectively."}, {"heading": "4.3. The Projection Algorithm", "text": "Projection of a distribution onto a submanifold is essential; several machine learning algorithms are known to be formulated as projection of a distribution empirically estimated from data onto a submanifold that is specified by the target model (Amari, 2016). Here we define projection of distributions on posets and show that Newton\u2019s method can be applied to perform projection as the Jacobian matrix can be analytically computed."}, {"heading": "4.3.1. DEFINITION", "text": "Let S(\u03b2) be a submanifold of S such that\nS(\u03b2) = {P \u2208 S | \u03b8P (x) = \u03b2(x), \u2200x \u2208 dom(\u03b2)} (14)\nspecified by a function \u03b2 with dom(\u03b2) \u2286 S+. Projection of P \u2208 S onto S(\u03b2), calledm-projection, which is defined as the distribution P\u03b2 \u2208 S(\u03b2) such that{\n\u03b8P\u03b2 (x) = \u03b2(x) if x \u2208 dom(\u03b2), \u03b7P\u03b2 (x) = \u03b7P (x) if x \u2208 S+ \\ dom(\u03b2),\nis the minimizer of the KL divergence from P to S(\u03b2):\nP\u03b2 = argmin Q\u2208S(\u03b2) DKL[P,Q].\nThe dually flat structure with the coordinate systems \u03b8 and \u03b7 guarantees that the projected distribution P\u03b2 always exists and is unique (Amari, 2009, Theorem 3). Moreover, the Pythagorean theorem holds in the dually flat manifold, that is, for any Q \u2208 S(\u03b2) we have\nDKL[P,Q] = DKL[P, P\u03b2 ] +DKL[P\u03b2 , Q].\nWe can switch \u03b7 and \u03b8 in the submanifold S(\u03b2) by changing DKL[P,Q] to DKL[Q,P ], where the projected distribution P\u03b2 of P is given as{\n\u03b8P\u03b2 (x) = \u03b8P (x) if x \u2208 S+ \\ dom(\u03b2), \u03b7P\u03b2 (x) = \u03b2(x) if x \u2208 dom(\u03b2),\nThis projection is called e-projection.\nExample 1 (Boltzmann machine). Given a Boltzmann machine represented as an undirected graph G = (V,E) with a vertex set V and an edge set E \u2286 {{i, j} | i, j \u2208 V }. The set of probability distributions that can be modeled by a Boltzmann machine G coincides with the submanifold\nSB = {P \u2208 S | \u03b8P (x) = 0 if |x| > 2 or x \u0338\u2208 E},\nwith S = 2V . Let P\u0302 be an empirical distribution estimated from a given dataset. The learned model is the mprojection of the empirical distribution P\u0302 onto SB, where the resulting distribution P\u03b2 is given as{\n\u03b8P\u03b2 (x) = 0 if |x| > 2 or x \u0338\u2208 E, \u03b7P\u03b2 (x) = \u03b7P\u0302 (x) if |x| = 1 or x \u2208 E."}, {"heading": "4.3.2. COMPUTATION", "text": "Here we show how to compute projection of a given probability distribution. We show that Newton\u2019s method can be used to efficiently compute the projected distribution P\u03b2 by iteratively updating P (0)\u03b2 = P as P (0) \u03b2 , P (1) \u03b2 , P (2) \u03b2 , . . . until converging to P\u03b2 .\nLet us start with the m-projection with initializing P (0)\u03b2 = P . In each iteration t, we update \u03b8(t)P\u03b2 (x) for all x \u2208 dom\u03b2 while fixing \u03b7(t)P\u03b2 (x) = \u03b7P (x) for all x \u2208 S\n+ \\ dom(\u03b2), which is possible from the orthogonality of \u03b8 and \u03b7. Using Newton\u2019s method, \u03b7(t+1)P\u03b2 (x) should satisfy( \u03b8 (t) P\u03b2 (x)\u2212 \u03b2(x) ) + \u2211\ny\u2208dom(\u03b2)\nJxy ( \u03b7 (t+1) P\u03b2 (y)\u2212 \u03b7(t)P\u03b2 (y) ) = 0,\nfor every x \u2208 dom(\u03b2), where Jxy is an entry of the |dom(\u03b2)| \u00d7 |dom(\u03b2)| Jacobian matrix J and given as\nJxy = \u2202\u03b8\n(t) P\u03b2 (x)\n\u2202\u03b7 (t) P\u03b2 (y) = \u2211 s\u2208S \u00b5(s, x)\u00b5(s, y)p (t) \u03b2 (s) \u22121\nfrom Theorem 3. Therefore, we have the update formula for all x \u2208 dom(\u03b2) as\n\u03b7 (t+1) P\u03b2 (x) = \u03b7 (t) P\u03b2\n(x)\u2212 \u2211\ny\u2208dom(\u03b2)\nJ\u22121xy ( \u03b8 (t) P\u03b2 (y)\u2212 \u03b2(y) ) .\nIn e-projection, update \u03b7(t)P\u03b2 (x) for x \u2208 dom(\u03b2) while fixing \u03b8(t)P\u03b2 (x) = \u03b8P (x) for all x \u2208 S\n+ \\ dom(\u03b2). To ensure \u03b7 (t) P\u03b2\n(\u22a5) = 1, we add \u22a5 to dom(\u03b2) and \u03b2(\u22a5) = 1. We update \u03b8(t)P\u03b2 (x) at each step t as\n\u03b8 (t+1) P\u03b2 (x) = \u03b8 (t) P\u03b2\n(x)\u2212 \u2211\ny\u2208dom(\u03b2)\nJ \u2032 \u22121 xy ( \u03b7 (t) P\u03b2 (y)\u2212 \u03b2(y) ) ,\nJ \u2032xy = \u2202\u03b7\n(t) P\u03b2 (x)\n\u2202\u03b8 (t) P\u03b2 (y) = \u2211 s\u2208S \u03b6(x, s)\u03b6(y, s)p (t) \u03b2 (s)\n\u2212 |S|\u03b7(t)P\u03b2 (x)\u03b7 (t) P\u03b2 (y).\nIn this case, we also need to update \u03b8(t)P\u03b2 (\u22a5) as it is not guaranteed to be fixed. Let us define\np \u2032(t+1) \u03b2 (x) = p (t) \u03b2 (x) \u220f s\u2208dom(\u03b2)\nexp ( \u03b8 (t+1) P\u03b2 (s) )\nexp ( \u03b8 (t) P\u03b2 (s) ) \u03b6(s, x).\nSince we have\np (t+1) \u03b2 (x) =\nexp ( \u03b8 (t+1) P\u03b2 (\u22a5) )\nexp ( \u03b8 (t) P\u03b2 (\u22a5) ) p\u2032(t+1)\u03b2 (x),\nit follows that\n\u03b8 (t+1) P\u03b2 (\u22a5)\u2212 \u03b8(t)P\u03b2 (\u22a5)\n= \u2212 log ( exp ( \u03b8 (t) P\u03b2 (\u22a5) ) + \u2211 x\u2208S+ p \u2032(t+1) \u03b2 (x) ) ,\nThe time complexity of each iteration is O(|dom(\u03b2)|3), which is required to compute the inverse of the Jacobian matrix.\nGlobal convergence of the projection algorithm is always guaranteed by the convexity of a submanifold S(\u03b2) defined in Equation (14). Since S(\u03b2) is always convex with respect to the \u03b8- and \u03b7-coordinates, it is straightforward to see that our e-projection is an instance of the Bregman algorithm onto a convex region, which is well known to always converge to the global solution (Censor & Lent, 1981)."}, {"heading": "5. Balancing Matrices and Tensors", "text": "Now we are ready to solve the problem of matrix and tensor balancing as projection on a dually flat manifold."}, {"heading": "5.1. Matrix Balancing", "text": "Recall that the task of matrix balancing is to find r, s \u2208 Rn that satisfy (RAS)1 = 1 and (RAS)T1 = 1 with R = diag(r) and S = diag(s) for a given nonnegative square matrix A = (aij) \u2208 Rn\u00d7n\u22650 .\nLet us define S as S = {(i, j) | i, j \u2208 [n] and aij \u0338= 0}, (15)\nwhere we remove zero entries from the outcome space S as our formulation cannot treat zero probability, and give each probability as p((i, j)) = aij/ \u2211 ij aij . The partial order \u2264 of S is naturally introduced as x = (i, j) \u2264 y = (k, l) \u21d4 i \u2264 j and k \u2264 l, (16)\nresulting in \u22a5 = (1, 1). In addition, we define \u03b9k,m for each k \u2208 [n] and m \u2208 {1, 2} such that\n\u03b9k,m = min{x = (i1, i2) \u2208 S | im = k }, where the minimum is with respect to the order \u2264. If \u03b9k,m does not exist, we just remove the entire kth row if m = 1 or kth column if m = 2 from A. Then we switch rows and columns of A so that the condition\n\u03b91,m \u2264 \u03b92,m \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03b9n,m (17) is satisfied for each m \u2208 {1, 2}, which is possible for any matrices. Since we have\n\u03b7(\u03b9k,m)\u2212 \u03b7(\u03b9k+1,m) = { \u2211n j=1 p((k, j)) if m = 1,\u2211n i=1 p((i, k)) if m = 2\nif the condition (17) is satisfied, the probability distribution is balanced if for all k \u2208 [n] and m \u2208 {1, 2}\n\u03b7(\u03b9k,m) = n\u2212k+1\nn .\nTherefore, we obtain the following result.\nMatrix balancing as e-projection: Given a matrix A \u2208 Rn\u00d7n with its normalized probability distribution P \u2208 S such that p((i, j)) = aij/ \u2211 ij aij . Define the poset (S,\u2264) by Equations (15) and (16) and let S(\u03b2) be the submanifold of S such that S(\u03b2) = {P \u2208 S | \u03b7P (x) = \u03b2(x) for all x \u2208 dom(\u03b2)},\nwhere the function \u03b2 is given as dom(\u03b2) = {\u03b9k,m \u2208 S | k \u2208 [n],m \u2208 {1, 2}},\n\u03b2(\u03b9k,m) = n\u2212k+1\nn .\nMatrix balancing is the e-projection of P onto the submanifold S(\u03b2), that is, the balanced matrix (RAS)/n is the distribution P\u03b2 such that{\n\u03b8P\u03b2 (x) = \u03b8P (x) if x \u2208 S+ \\ dom(\u03b2), \u03b7P\u03b2 (x) = \u03b2(x) if x \u2208 dom(\u03b2),\nwhich is unique and always exists in S, thanks to its dually flat structure. Moreover, two balancing vectors r and s are\nexp\n( i\u2211\nk=1\n\u03b8P\u03b2 (\u03b9k,m)\u2212 \u03b8P (\u03b9k,m)\n) = { ri if m = 1, ai if m = 2,\nfor every i \u2208 [n] and r = rn/ \u2211\nij aij . \u25a0"}, {"heading": "5.2. Tensor Balancing", "text": "Next, we generalize our approach from matrices to tensors. For anN th order tensorA = (ai1i2...iN ) \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nN and a vector b \u2208 Rnm , the m-mode product of A and b is defined as\n(A\u00d7m b)i1...im\u22121im+1...iN = nm\u2211\nim=1\nai1i2...iN bim .\nWe define tensor balancing as follows: Given a tensor A \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nN with n1 = \u00b7 \u00b7 \u00b7 = nN = n, find (N \u2212 1) order tensors R1, R2, . . . , RN such that\nA\u2032 \u00d7m 1 = 1 (\u2208 Rn1\u00d7\u00b7\u00b7\u00b7\u00d7nm\u22121\u00d7nm+1\u00d7\u00b7\u00b7\u00b7\u00d7nN ) (18) for all m \u2208 [N ], i.e., \u2211n\nim=1 a\u2032i1i2...iN = 1, where each\nentry a\u2032i1i2...iN of the balanced tensor A \u2032 is given as a\u2032i1i2...iN = ai1i2...iN \u220f\nm\u2208[N ]\nRmi1...im\u22121im+1...iN .\nA tensor A\u2032 that satisfies Equation (18) is called multistochastic (Cui et al., 2014). Note that this is exactly the same as the matrix balancing problem if N = 2.\nIt is straightforward to extend matrix balancing to tensor balancing as e-projection onto a submanifold. Given a tensor A \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nN with its normalized probability distribution P such that\np(x) = ai1i2...iN / \u2211 j1j2...jN aj1j2...jN (19)\nfor all x = (i1, i2, . . . , iN ). The objective is to obtain P\u03b2 such that \u2211n im=1 p\u03b2((i1, . . . , iN )) = 1/(n N\u22121) for all m \u2208 [N ] and i1, . . . , iN \u2208 [n]. In the same way as matrix balancing, we define S as\nS = { (i1, i2, . . . , iN ) \u2208 [n]N \u2223\u2223 ai1i2...iN \u0338= 0 } with removing zero entries and the partial order \u2264 as\nx = (i1 . . . iN ) \u2264 y = (j1 . . . jN ) \u21d4 \u2200m \u2208 [N ], im \u2264 jm.\nIn addition, we introduce \u03b9k,m as\n\u03b9k,m = min{x = (i1, i2, . . . , iN ) \u2208 S | im = k }.\nand require the condition in Equation (17).\nTensor balancing as e-projection: Given a tensor A \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nN with its normalized probability distribution P \u2208 S given in Equation (19). The submanifold S(\u03b2) of multistochastic tensors is given as\nS(\u03b2) = {P \u2208 S | \u03b7P (x) = \u03b2(x) for all x \u2208 dom(\u03b2)},\nwhere the domain of the function \u03b2 is given as\ndom(\u03b2) = { \u03b9k,m | k \u2208 [n],m \u2208 [N ] }\nand each value is described using the zeta function as \u03b2(\u03b9k,m) = \u2211 l\u2208[n] \u03b6(\u03b9k,m, \u03b9l,m) 1 nN\u22121 .\nTensor balancing is the e-projection of P onto the submanifold S(\u03b2), that is, the multistochastic tensor is the distribution P\u03b2 such that{\n\u03b8P\u03b2 (x) = \u03b8P (x) if x \u2208 S+ \\ dom(\u03b2), \u03b7P\u03b2 (x) = \u03b2(x) if x \u2208 dom(\u03b2),\nwhich is unique and always exists in S, thanks to its dually flat structure. Moreover, each balancing tensor Rm is\nRmi1...im\u22121im+1...iN\n= exp  \u2211 m\u2032 \u0338=m im\u2032\u2211 k=1 \u03b8P\u03b2 (\u03b9k,m\u2032)\u2212 \u03b8P (\u03b9k,m\u2032)  for every m \u2208 [N ] and R1 = R1nN\u22121/ \u2211 j1...jN\naj1...jN to recover a multistochastic tensor. \u25a0 Our result means that the e-projection algorithm based on Newton\u2019s method proposed in Section 4.3 converges to the unique balanced tensor whenever S(\u03b2) \u0338= \u2205 holds."}, {"heading": "6. Conclusion", "text": "In this paper, we have solved the open problem of tensor balancing and presented an efficient balancing algorithm using Newton\u2019s method. Our algorithm quadratically converges, while the popular Sinkhorn-Knopp algorithm linearly converges. We have examined the efficiency of our algorithm in numerical experiments on matrix balancing and showed that the proposed algorithm is several orders of magnitude faster than the existing approaches.\nWe have analyzed theories behind the algorithm, and proved that balancing is e-projection in a special type of a statistical manifold, in particular, a dually flat Riemannian manifold studied in information geometry. Our key finding is that the gradient of the manifold, equivalent to Riemannian metric or the Fisher information matrix, can be analytically obtained using the Mo\u0308bius inversion formula.\nOur information geometric formulation can model several machine learning applications such as statistical analysis on a DAG structure. Thus, we can perform efficient learning as projection using information of the gradient of manifolds by reformulating such models, which we will study in future work."}, {"heading": "Acknowledgements", "text": "The authors sincerely thank Marco Cuturi for his valuable comments. This work was supported by JSPS KAKENHI Grant Numbers JP16K16115, JP16H02870 (MS), JP26120732 and JP16H06570 (HN). The research of K.T. was supported by JST CREST JPMJCR1502, RIKEN PostK, KAKENHI Nanostructure and KAKENHI JP15H05711."}], "year": 2017, "references": [{"title": "Categorical data analysis", "authors": ["A. Agresti"], "venue": "Wiley, 3 edition,", "year": 2012}, {"title": "Polyhedral Cones of Magic Cubes and Squares, volume 25 of Algorithms and Combinatorics", "authors": ["M. Ahmed", "J. De Loera", "R. Hemmecke"], "year": 2003}, {"title": "Network models and biproportional rounding for fair seat allocations in the UK elections", "authors": ["K. Akartunal\u0131", "P.A. Knight"], "venue": "Annals of Operations Research,", "year": 2016}, {"title": "Information geometry on hierarchy of probability distributions", "authors": ["S. Amari"], "venue": "IEEE Transactions on Information Theory,", "year": 2001}, {"title": "Information geometry and its applications: Convex function and dually flat manifold", "authors": ["S. Amari"], "venue": "ETVC", "year": 2008}, {"title": "Information geometry of positive measures and positive-definite matrices: Decomposable dually flat structure. Entropy", "authors": ["S. Amari"], "year": 2014}, {"title": "Information Geometry and Its Applications. Springer, 2016", "authors": ["S. Amari"], "year": 2016}, {"title": "Fair majority voting (or how to eliminate gerrymandering)", "authors": ["M. Balinski"], "venue": "American Mathematical Monthly,", "year": 2008}, {"title": "An iterative row-action method for interval convex programming", "authors": ["Y. Censor", "A. Lent"], "venue": "Journal of Optimization Theory and Applications,", "year": 1981}, {"title": "Polytopes of stochastic tensors", "authors": ["H. Chang", "V.E. Paksoy", "F. Zhang"], "venue": "Annals of Functional Analysis,", "year": 2016}, {"title": "Birkhoff\u2013von Neumann theorem for multistochastic tensors", "authors": ["Cui", "L.-B", "W. Li", "M.K. Ng"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2014}, {"title": "Sinkhorn distances: Lightspeed computation of optimal transport", "authors": ["M. Cuturi"], "venue": "In Advances in Neural Information Processing Systems", "year": 2013}, {"title": "Learning with a Wasserstein loss", "authors": ["C. Frogner", "C. Zhang", "H. Mobahi", "M. Araya", "T.A. Poggio"], "venue": "In Advances in Neural Information Processing Systems", "year": 2015}, {"title": "Continuous Lattices and Domains", "authors": ["G. Gierz", "K.H. Hofmann", "K. Keimel", "J.D. Lawson", "M. Mislove", "D.S. Scott"], "year": 2003}, {"title": "A review of matrix scaling and sinkhorn\u2019s normal form for matrices and positive maps", "authors": ["M. Idel"], "year": 2016}, {"title": "ed.). Encyclopedic Dictionary of Mathematics", "authors": ["K. Ito"], "venue": "The MIT Press,", "year": 1993}, {"title": "The Sinkhorn\u2013Knopp algorithm: Convergence and applications", "authors": ["P.A. Knight"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2008}, {"title": "A fast algorithm for matrix balancing", "authors": ["P.A. Knight", "D. Ruiz"], "venue": "IMA Journal of Numerical Analysis,", "year": 2013}, {"title": "Biproportional techniques in input-output analysis: Table updating and structural analysis", "authors": ["M. Lahr", "L. de Mesnard"], "venue": "Economic Systems Research,", "year": 2004}, {"title": "Bregman\u2019s balancing method", "authors": ["B. Lamond", "N.F. Stewart"], "venue": "Transportation Research Part B: Methodological,", "year": 1981}, {"title": "Scaling by binormalization", "authors": ["O.E. Livne", "G.H. Golub"], "venue": "Numerical Algorithms,", "year": 2004}, {"title": "Scaling of matrices to achieve specified row and column sums", "authors": ["A.W. Marshall", "I. Olkin"], "venue": "Numerische Mathematik,", "year": 1968}, {"title": "Input-Output Analysis: Foundations and Extensions", "authors": ["R.E. Miller", "P.D. Blair"], "year": 2009}, {"title": "Sinkhorn solves sudoku", "authors": ["T.K. Moon", "J.H. Gunther", "J.J. Kupin"], "venue": "IEEE Transactions on Information Theory,", "year": 2009}, {"title": "Information-geometric measure for neural spikes", "authors": ["H. Nakahara", "S. Amari"], "venue": "Neural Computation,", "year": 2002}, {"title": "Gene interaction in DNA microarray data is decomposed by information geometric measure", "authors": ["H. Nakahara", "S. Nishimura", "M. Inoue", "G. Hori", "S. Amari"], "year": 2003}, {"title": "A comparison of descriptive models of a single spike train by information-geometric measure", "authors": ["H. Nakahara", "S. Amari", "B.J. Richmond"], "venue": "Neural computation,", "year": 2006}, {"title": "Forecasts of input-output matrices using the R.A.S", "authors": ["A. Parikh"], "venue": "method. The Review of Economics and Statistics,", "year": 1979}, {"title": "Methods for scaling to doubly stochastic form", "authors": ["B.N. Parlett", "T.L. Landis"], "venue": "Linear Algebra and its Applications,", "year": 1982}, {"title": "On the foundations of combinatorial theory I: Theory of M\u00f6bius functions", "authors": ["Rota", "G.-C"], "venue": "Z. Wahrseheinlichkeitstheorie,", "year": 1964}, {"title": "A relationship between arbitrary positive matrices and doubly stochastic matrices", "authors": ["R. Sinkhorn"], "venue": "The Annals of Mathematical Statistics, 35(2):876\u2013879,", "year": 1964}, {"title": "Concerning nonnegative matrices and doubly stochastic matrices", "authors": ["R. Sinkhorn", "P. Knopp"], "venue": "Pacific Journal of Mathematics,", "year": 1967}, {"title": "Convolutional Wasserstein distances: Efficient optimal transportation on geometric domains", "authors": ["J. Solomon", "F. de Goes", "G. Peyr\u00e9", "M. Cuturi", "A. Butscher", "A. Nguyen", "T. Du", "L. Guibas"], "venue": "ACM Transactions on Graphics,", "year": 2015}, {"title": "The rate of convergence of sinkhorn balancing", "authors": ["G.W. Soules"], "venue": "Linear Algebra and its Applications,", "year": 1991}, {"title": "Information decomposition on structured space", "authors": ["M. Sugiyama", "H. Nakahara", "K. Tsuda"], "venue": "IEEE International Symposium on Information Theory,", "year": 2016}, {"title": "A computational strategy to adjust for copy number in tumor", "authors": ["Wu", "H.-J", "F. Michor"], "venue": "Hi-C data. Bioinformatics,", "year": 2016}], "id": "SP:35703d3ec93f008ae1ba0da7dc044e4e208d0945", "authors": [{"name": "Mahito Sugiyama", "affiliations": []}, {"name": "Hiroyuki Nakahara", "affiliations": []}, {"name": "Koji Tsuda", "affiliations": []}], "abstractText": "We solve tensor balancing, rescaling an N th order nonnegative tensor by multiplying N tensors of order N \u22121 so that every fiber sums to one. This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to economics. We present an efficient balancing algorithm with quadratic convergence using Newton\u2019s method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones. To theoretically prove the correctness of the algorithm, we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold. The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton\u2019s method, can be analytically obtained using the M\u00f6bius inversion formula, the essential of combinatorial mathematics. Our model is not limited to tensor balancing, but has a wide applicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines.", "title": "Tensor Balancing on Statistical Manifold"}