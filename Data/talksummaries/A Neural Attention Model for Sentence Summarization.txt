2	21	Most successful summarization systems utilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version.
5	52	While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002).
8	47	We instead explore a fully data-driven approach for generating abstractive summaries.
9	33	Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder.
10	29	Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1).
11	196	Crucially both the encoder and the generation model are trained jointly on the sentence summarization task.
13	42	Our model also incorporates a beam-search decoder as well as additional features to model extractive elements; these aspects are discussed in Sections 4 and 5.
26	26	Furthermore define the notation x[i,j,k] to indicate the sub-sequence of elements i, j, k. A summarizer takes x as input and outputs a shortened sentence y of length N < M .
38	26	Here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network.
40	22	The core of our parameterization is a language model for estimating the contextual probability of the next word.
43	32	The parameters are θ = (E,U,V,W) where E ∈ RD×V is a word embedding matrix, U ∈ R(CD)×H , V ∈ RV×H , W ∈ RV×H are weight matrices,4 D is the size of the word embeddings, and h is a hidden layer of size H .
46	24	Note that without the encoder term this represents a standard language model.
61	24	Attention-Based Encoder While the convolutional encoder has richer capacity than bag-ofwords, it still is required to produce a single representation for the entire input sentence.
67	32	Figure 1 shows an example of this distribution p as a summary is generated.
89	23	Because there is no explicit constraint that each source word be used exactly once there is no need to maintain a bit set and we can simply move from left-to-right generating words.
101	36	These features correspond to indicators of unigram, bigram, and trigram match with the input as well as reordering of input words.
155	34	The headline vocabulary consists of 31 million tokens and 69K word types with the average title of length 8.3 words (note that this is significantly shorter than the DUC summaries).
167	21	This model uses the syntactic structure of the original sentence along with a language model trained on the headline data to produce a compressed output.
185	33	Additionally, as described in Section 5 we apply a MERT tuning step after training using the DUC2003 data.
187	91	We refer to the main model as ABS and the tuned model as ABS+.
189	110	We run experiments both using the DUC-2004 evaluation data set (500 sentences, 4 references, 75 bytes) with all systems and a randomly held-out Gigaword test set (2000 sentences, 1 reference).
192	46	Both ABS and MOSES+ perform better than TOPIARY, particularly on ROUGE-2 and ROUGE-L in DUC.
193	21	The full model ABS+ scores the best on these tasks, and is significantly better based on the default ROUGE confidence level than TOPIARY on all metrics, and MOSES+ on ROUGE-1 for DUC as well as ROUGE-1 and ROUGE-L for Gigaword.
194	24	Note that the additional extractive features bias the system towards retaining more input words, which is useful for the underlying metric.
207	38	Generally the models are good at picking out key words from the input, such as names and places.
208	27	However, both models will reorder words in syntactically incorrect ways, for instance in Sentence 7 both models have the wrong subject.
209	23	ABS often uses more interesting re-wording, for instance new nz pm after election in Sentence 4, but this can also lead to attachment mistakes such a russian oil giant chevron in Sentence 11.
211	27	We combine this probabilistic model with a generation algorithm which produces accurate abstractive summaries.
212	50	As a next step we would like to further improve the grammaticality of the summaries in a data-driven way, as well as scale this system to generate paragraph-level summaries.
213	22	Both pose additional challenges in terms of efficient alignment and consistency in generation.
