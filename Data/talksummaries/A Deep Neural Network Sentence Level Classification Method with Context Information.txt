0	38	Artificial neural networks (ANN) and especially Deep Neural Networks (DNN) give state-of-the art results for sentence classification tasks.
1	2	Usually, sentences are treated as separate instances for the task.
2	44	However, in many situations the sentence that is the focus of classification appears in a context that can provide additional information.
3	18	For example, in the below sentences from the IEMOCAP dataset, it is difficult to classify M02 as showing excitement, without the prior context: • M01: I got it.
5	29	Our work is motivated by sentence classification in the text of medical records, in which complex judgements may be made across several sentences, each adding weight and nuance to a point.
8	8	Previous work on using context for sentence classification used LSTM and CNN network layers to encode the surrounding context, giving an improvement in classification accuracy (Lee and Dernoncourt, 2016).
12	5	We therefore introduce a new method, Context-LSTM-CNN1, which is based on the computationally efficient FOFE (Fixed Size Ordinally Forgetting) method (Zhang et al., 2015), and an architecture that combines an LSTM and CNN for the focus sentence.
14	27	This paper makes three contributions: 1) a demonstration of the importance of context in some sentence classification tasks; 2) an adaptation of existing datasets for such sentence classification tasks, in order to support reproducibility of evaluations; 3) a neural architecture for sentence classification that outperforms previous methods, and can include context of arbitrary size without incurring a large computational cost.
26	5	The Context-LSTM-CNN model is shown in Figure 1.
27	5	It is based on the following components: 1.
28	17	Input layer using word embeddings to encode the words of the focus sentence.
30	9	CNN on the outputs of the LSTM.
34	27	In addition to processing the focus sentence, we also encode the full left and right contexts using an adaptation of FOFE applied to our embeddings.
36	8	The output of the FOFE layers are then each passed through separate fully connected layers, before being concatenated and connected to output layer.
37	12	In detail, the full network takes three inputs.
43	12	The first component of the inputs, derived from the focus sentence, is processed by a bi-directional LSTM with one layer, in order to capture longdistance dependencies within the sentence.
53	10	This method is fast and compactly encodes the words of a sentence in a single embedding vector.
54	8	For our use of FOFE, we encode all sentences in the document to left and right of the focus sentence, in two hierarchical steps.
55	7	First we encode each context sentence into a FOFE embedding zsent, with a slowly-decreasing αsent.
81	31	We examined the effect of the two context encoder hyperparameters: αcont (context level forgetting factor) and αw (sentence level forgetting factor) on classification performance over the IEMOCAP dataset.
82	2	We tested both in the range of 0.1 to 1 with an incremental step of 0.1.
85	4	This may be because context closest to the focus sentence is more important than distant context.
93	98	Both context based models (LLSTM-CNN and C-LSTM-CNN) perform better than non context based models, but note that LLSTM-CNN increases training time by approximately 1.5x, whereas C-LSTM-CNN shows only a marginal increase in training time, with a large increase in accuracy on the IEMOCAP corpus.
94	18	Table 2 shows the F1-measure for each class in the two datasets.
96	5	C-LSTM-CNN improves on average by 6.28 over L-LSTM-CNN, 10.16 over LSTMCNN, 11.4 over CNN and 13.29 over LSTM.
97	16	We conducted a t-test between L-LSTM-CNN and C-LSTM-CNN.
98	21	On ADE, C-LSTM-CNN is not significantly better than L-LSTM-CNN (p = 0.128).
99	2	This may because ADE sentences are less context dependent.
101	53	In this paper we introduced a new ANN model, Context-LSTM-CNN, that combines the strength of LSTM and CNN with the lightweight context encoding algorithm, FOFE.
102	43	Our model shows a consistent improvement over either a non-context based model and a LSTM context encoded model, for the sentence classification task.
