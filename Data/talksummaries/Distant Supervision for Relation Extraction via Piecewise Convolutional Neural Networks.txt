0	24	In relation extraction, one challenge that is faced when building a machine learning system is the generation of training examples.
2	23	Figure 1 shows an example of the auto- matic labeling of data through distant supervision.
3	46	In this example, Apple and Steve Jobs are two related entities in Freebase1.
13	39	Second, previous methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011) have typically applied supervised models to elaborately designed features when obtained the labeled data through distant supervision.
15	21	Since errors inevitably exist in NLP tools, the use of traditional features leads to error propagation or accumulation.
20	52	Therefore, when using traditional features, the problem of error propagation or accumulation will not only exist, it will grow more serious.
23	35	In multi-instance problem, the training set consists of many bags, and each contains many instances.
27	53	To address the second problem, we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by Zeng et al. (2014).
28	17	Our proposal is an extension of Zeng et al. (2014), in which a single max pooling operation is utilized to determine the most significant features.
29	42	Although this operation has been shown to be effective for textual feature representation (Collobert et al., 2011; Kim, 2014), it reduces the size of the hidden layers too rapidly and cannot capture the structural information between two entities (Graham, 2014).
30	83	For example, to identify the relation between Steve Jobs and Apple in Figure 1, we need to specify the entities and extract the structural features between them.
34	18	The internal context includes the characters inside the two entities, and the external context involves the characters around the two entities (Zhang et al., 2006).
43	26	• In the proposed network, we devise a piecewise max pooling layer, which aims to capture structural information between two entities.
87	18	In this paper, we use the Skip-gram model (Mikolov et al., 2013) to train word embeddings.
93	55	We then transform the relative distances into real valued vectors by looking up the position embedding matrixes.
95	42	In combined word embeddings and position embeddings, the vector representation part transforms an instance into a matrix S ∈ Rs×d, where s is the sentence length and d = dw + dp ∗ 2.
117	25	In addition, single max pooling is not sufficient to capture the structural information between two entities.
125	44	To compute the confidence of each relation, the feature vector g is fed into a softmax classifier.
146	39	Given all (T ) training bags (Mi, yi), we can define the objective function using cross-entropy at the bag level as follows: J (θ) = T∑ i=1 log p(yi|mji ; θ) (8) where j is constrained as follows: j∗ = arg max j p(yi|mji ; θ) 1 ≤ j ≤ qi (9) Using this defined objective function, we maximize J(θ) through stochastic gradient descent over shuffled mini-batches with the Adadelta (Zeiler, 2012) update rule.
157	58	This dataset was generated by aligning Freebase relations with the NYT corpus, with sentences from the years 2005-2006 used as the training corpus and sentences from 2007 used as the testing corpus.
163	35	To obtain the embeddings of the entities, we concatenate the tokens of a entity using the ## operator when the entity has multiple word tokens.
169	23	The batch size is fixed to 50.
176	34	To evaluate the proposed method, we select the following three traditional methods for comparison.
180	23	Figure 4 shows the precision-recall curves for each method, where PCNNs+MIL denotes our method, and demonstrates that PCNNs+MIL achieves higher precision over the entire range of recall.
186	29	Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction.
210	19	As expected, PCNNs+MIL obtains the best results because the advantages of both techniques are achieved simultaneously.
211	77	In this paper, we exploit Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning for distant supervised relation extraction.
212	59	In our method, features are automatically learned without complicated NLP preprocessing.
213	78	We also successfully devise a piecewise max pooling layer in the proposed network to capture structural information and incorporate multi-instance learning to address the wrong label problem.
214	44	Experimental results show that the proposed approach offers significant improvements over comparable methods.
