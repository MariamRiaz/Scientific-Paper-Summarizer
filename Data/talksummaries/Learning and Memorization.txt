0	45	Neural networks trained through stochastic gradient descent (SGD) can memorize their training data.
1	30	Although practitioners have long been aware of this phenomenon, Zhang et al. (2017) recently brought attention to it by showing that standard SGD-based training on AlexNet gets close to zero training error on a modification of the ImageNet dataset even when the labels are randomly permuted.
2	44	This leads to an interesting question: If neural nets have sufficient capacity to memorize random training sets why do they generalize on real data?
3	34	A natural hypothesis is that nets behave differently on real data than on random data.
4	14	Arpit et al. (2017) study this question experimentally and show that there are apparent differences in behavior.
6	37	But what if networks fundamentally do not behave differently on real data than on random data, and, in both cases, are simply memorizing?
10	39	), there is no easy way to tell when a network is memorizing real data as opposed to “learning”.
11	40	Second, and perhaps more importantly, it contradicts the intuitive notion—inherent in the preceding discussion—that memorization and generalization are at odds.
13	14	Is generalization even possible in this setting?
14	36	At first, generalization in such a setting of pure memorization may seem hopeless: the simplest way to memorize would be to build a lookup table from the training data.
16	15	One way to get around this limitation is to use k-Nearest Neighbors (k-NN) or any of its variants at test time.
19	11	Indeed some of the most interesting results from deep learning have been the discovery—through learning—of semantically meaningful distance functions (via embeddings).
22	38	We build a network of lookup tables (also called “luts”) where the luts are arranged in successive layers much like a neural network.
40	21	Since we want to learn by memorizing, we construct a lookup table with 2k rows (one for each possible bit pattern p ∈ Bk that can appear at the input) and two columns y0 and y1.
55	14	In particular, the training error is zero iff the training set has zero Bayes error.
64	21	To summarize, the procedure described to learn a single lookup table in this section is essentially memorization in the presence of Bayes error, where the idea is to simply remember the output that is most commonly associated with an input in the training set.
78	28	We train the lookup tables layer by layer, where the target of each lookup table is the final output.
89	14	Using the procedure in Section 2, the two lookup tables learned in the first layer (using y as the target) along with their corresponding functions f̂10 and f̂11 are: p x0x1 y0 y1 f̂10 00 1 3 1 01 0 0 1∗ 10 1 0 0 11 1 1 1∗ p x0x2 y0 y1 f̂11 00 1 2 1 01 0 1 1 10 2 1 0 11 0 0 1∗ Let the output of the luts in the first layer be w10 and w11, i.e., w10 = f̂10(x0x1) and w11 = f̂11(x0x2).
102	20	But what is surprising is that the network achieves an accuracy of 0.87 on a heldout set (the 10,000 test images in MNIST) which indicates generalization.
121	13	We now vary k to see if we can control the amount of memorization and to see the effect it has on generalization.
122	10	To avoid changing too much at once, we keep the number of layers and the number of luts per layer the same as in Experiment 1.
126	12	However, larger luts generalize less well, and the best test accuracy of 0.90 is achieved at k = 12 though with substantially good memorization of the training data (0.99).
131	32	This may be viewed as empirical evidence that the Rademacher complexity goes up with k. However, and this may be surprising for a pure memorization algorithm, memorizing random data turns out to be harder than memorizing real data (columns 2 and 3 of Table 2) in the sense that a larger k is required to get the same accuracy with random data than with real data.
132	21	For example, it takes until k = 12 to get comparable training accuracy on random data as k = 4 gets on real data.
134	20	But it also means that we cannot conclude that any such difference observed in neural networks is because they do not use brute force memorization on real data.
135	30	As this experiment shows, such differences can appear even with brute force memorization.
136	80	Finally, at k = 12 we have a network that is able to memorize random data (random training accuracy of 0.82) and yet generalizes to test data when trained on real data (real test accuracy of 0.90).
140	28	We have not specifically tuned the other methods since our goal is not to beat the state-of-the-art but to get a sense of how memorization alone does when compared to the standard methods.
145	11	We now consider the task of separating the i-th digit in MNIST from the j-th digit, which gives us( 10 2 ) = 45 binary classification tasks, which we collectively call Pairwise-MNIST.
149	104	The worst of these is 0.95 which is the best memorization can do for separating ‘4’ and ‘9’.
