0,1,label2,summary_sentences
"Dependency parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016).
",1 Introduction,[0],[0]
"Attractive for their linear time complexity and amenability to conventional classification methods, transition-based dependency parsers have sparked much research interest recently.",1 Introduction,[0],[0]
"A transition-based parser makes sequential predictions of transitions between states under the restrictions of a transition system (Nivre, 2003).",1 Introduction,[0],[0]
"Transition-based parsers have been shown to excel at parsing shorter-range dependency structures, as well as languages where non-projective parses are less pervasive (McDonald and Nivre, 2007).
",1 Introduction,[0],[0]
"However, the transition systems employed in state-of-the-art dependency parsers usually define very local transitions.",1 Introduction,[0],[0]
"At each step, only one or two words are affected, with very local attachments made.",1 Introduction,[0],[0]
"As a result, distant attachments require long and not immediately obvious transition sequences (e.g., ate→chopsticks in Figure 1, which requires two transitions).",1 Introduction,[0],[0]
"This is further aggravated by the usually local lexical information leveraged to make transition predictions (Chen and Manning, 2014; Andor et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we introduce a novel transition system, arc-swift, which defines non-local transitions that directly induce attachments of distance up to n (n = the number of tokens in the sentence).",1 Introduction,[0],[0]
"Such an approach is connected to graph-based dependency parsing, in that it leverages pairwise scores between tokens in making parsing decisions (McDonald et al., 2005).
",1 Introduction,[0],[0]
We make two main contributions in this paper.,1 Introduction,[0],[0]
"Firstly, we introduce a novel transition system for dependency parsing, which alleviates the difficulty of distant attachments in previous systems by allowing direct attachments anywhere in the stack.",1 Introduction,[0],[0]
"Secondly, we compare parsers by the number of mistakes they make in common linguistic con-
ar X
iv :1
70 5.
04 43
4v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 M
ay 2
01 7
structions.",1 Introduction,[0],[0]
We show that arc-swift parsers reduce errors in attaching prepositional phrases and conjunctions compared to parsers using existing transition systems.,1 Introduction,[0],[0]
Transition-based dependency parsing is performed by predicting transitions between states (see Figure 1 for an example).,2 Transition-based Dependency Parsing,[0],[0]
"Parser states are usually written as (σ|i, j|β,A), where σ|i denotes the stack with token i on the top, j|β denotes the buffer with token j at its leftmost, and A the set of dependency arcs.",2 Transition-based Dependency Parsing,[0],[0]
"Given a state, the goal of a dependency parser is to predict a transition to a new state that would lead to the correct parse.",2 Transition-based Dependency Parsing,[0],[0]
"A transition system defines a set of transitions that are sound and complete for parsers, that is, every transition sequence would derive a well-formed parse tree, and every possible parse tree can also be derived from some transition sequence.1
Arc-standard (Nivre, 2004) is one of the first transition systems proposed for dependency parsing.",2 Transition-based Dependency Parsing,[0],[0]
"It defines three transitions: shift, left arc (LArc), and right arc (RArc) (see Figure 2 for definitions, same for the following transition systems), where all arc-inducing transitions operate on the stack.",2 Transition-based Dependency Parsing,[0],[0]
"This system builds the parse bottom-up, i.e., a constituent is only attached to its head after it has received all of its dependents.",2 Transition-based Dependency Parsing,[0],[0]
"A potential drawback is that during parsing, it is difficult to predict if a constituent has consumed all of its right dependents.",2 Transition-based Dependency Parsing,[0],[0]
"Arc-eager (Nivre, 2003) remedies this drawback by defining arc-inducing transitions that operate between the stack and the buffer.",2 Transition-based Dependency Parsing,[0],[0]
"As a result, a constituent no longer needs to be complete
1We only focus on projective parses for the scope of this paper.
",2 Transition-based Dependency Parsing,[0],[0]
"before it can be attached to its head to the left, as a right arc doesn’t prevent the attached dependent from taking further dependents of its own.2 Kuhlmann et al. (2011) propose a hybrid system derived from a tabular parsing scheme, which they have shown both arc-standard and arc-eager can be derived from.",2 Transition-based Dependency Parsing,[0],[0]
"Arc-hybrid combines LArc from arc-eager and RArc from arc-standard to build dependencies bottom-up.
",2 Transition-based Dependency Parsing,[0],[0]
"3 Non-local Transitions with arc-swift
The traditional transition systems discussed in Section 2 only allow very local transitions affecting one or two words, which makes long-distance dependencies difficult to predict.",2 Transition-based Dependency Parsing,[0],[0]
"To illustrate the limitation of local transitions, consider parsing the following sentences:
I ate fish with ketchup.",2 Transition-based Dependency Parsing,[0],[0]
"I ate fish with chopsticks.
",2 Transition-based Dependency Parsing,[0],[0]
"The two sentences have almost identical structures, with the notable difference that the prepositional phrase is complementing the direct object in the first case, and the main verb in the second.
",2 Transition-based Dependency Parsing,[0],[0]
"For arc-standard and arc-hybrid, the parser would have to decide between Shift and RArc when the parser state is as shown in Figure 3a, where ? stands for either “ketchup” or “chopsticks”.3 Similarly, an arc-eager parser would deal with the state shown in Figure 3b.",2 Transition-based Dependency Parsing,[0],[0]
"Making the correct transition requires information about context words “ate” and “fish”, as well as “?”.
",2 Transition-based Dependency Parsing,[0],[0]
2A side-effect of arc-eager is that there is sometimes spurious ambiguity between Shift and Reduce transitions.,2 Transition-based Dependency Parsing,[0],[0]
"For the example in Figure 1, the first Reduce can be inserted before the third Shift without changing the correctness of the resulting parse, i.e., both are feasible at that time.
",2 Transition-based Dependency Parsing,[0],[0]
"3For this example, we assume that the sentence is being parsed into Universal Dependencies.
",2 Transition-based Dependency Parsing,[0],[0]
"Parsers employing traditional transition systems would usually incorporate more features about the context in the transition decision, or employ beam search during parsing (Chen and Manning, 2014; Andor et al., 2016).
",2 Transition-based Dependency Parsing,[0],[0]
"In contrast, inspired by graph-based parsers, we propose arc-swift, which defines non-local transitions as shown in Figure 2.",2 Transition-based Dependency Parsing,[0],[0]
"This allows direct comparison of different attachment points, and provides a direct solution to parsing the two example sentences.",2 Transition-based Dependency Parsing,[0],[0]
"When the arc-swift parser encounters a state identical to Figure 3b, it could directly compare transitions RArc[1] and RArc[2] instead of evaluating between local transitions.",2 Transition-based Dependency Parsing,[0],[0]
"This results in a direct attachment much like that in a graph-based parser, informed by lexical information about affinity of the pairs of words.
",2 Transition-based Dependency Parsing,[0],[0]
Arc-swift also bears much resemblance to arceager.,2 Transition-based Dependency Parsing,[0],[0]
"In fact, an LArc[k] transition can be viewed as k− 1 Reduce operations followed by one LArc in arc-eager, and similarly for RArc[k].",2 Transition-based Dependency Parsing,[0],[0]
"Reduce is no longer needed in arc-swift as it becomes part of LArc[k] and RArc[k], removing the ambiguity in derived transitions in arc-eager.",2 Transition-based Dependency Parsing,[0],[0]
"arc-swift is also equivalent to arc-eager in terms of soundness and completeness.4 A caveat is that the worst-case time complexity of arc-swift is O(n2) instead of O(n), which existing transition-based parsers enjoy.",2 Transition-based Dependency Parsing,[0],[0]
"However, in practice the runtime is nearly
4This is easy to show because in arc-eager, all Reduce transitions can be viewed as preparing for a later LArc or RArc transition.",2 Transition-based Dependency Parsing,[0],[0]
"We also note that similar to arc-eager transitions, arc-swift transitions must also satisfy certain pre-conditions.",2 Transition-based Dependency Parsing,[0],[0]
"Specifically, an RArc[k] transition requires that the top k − 1 elements in the stack are already attached; LArc[k] additionally requires that the k-th element is unattached, resulting in no more than one feasible LArc candidate for any parser state.
",2 Transition-based Dependency Parsing,[0],[0]
"linear, thanks to the usually small number of reducible tokens in the stack.",2 Transition-based Dependency Parsing,[0],[0]
"We use the Wall Street Journal portion of Penn Treebank with standard parsing splits (PTBSD), along with Universal Dependencies v1.3 (Nivre et al., 2016) (EN-UD).",4.1 Data and Model,[0],[0]
"PTB-SD is converted to Stanford Dependencies (De Marneffe and Manning, 2008) with CoreNLP 3.3.0 (Manning et al., 2014) following previous work.",4.1 Data and Model,[0],[0]
"We report labelled and unlabelled attachment scores (LAS/UAS), removing punctuation from all evaluations.
",4.1 Data and Model,[0],[0]
"Our model is very similar to that of (Kiperwasser and Goldberg, 2016), where features are extracted from tokens with bidirectional LSTMs, and concatenated for classification.",4.1 Data and Model,[0],[0]
"For the three traditional transition systems, features of the top 3 tokens on the stack and the leftmost token in the buffer are concatenated as classifier input.",4.1 Data and Model,[0],[0]
"For arc-swift, features of the head and dependent tokens for each arc-inducing transition are concatenated to compute scores for classification, and features of the leftmost buffer token is used for Shift.",4.1 Data and Model,[0],[0]
For other details we defer to Appendix A.,4.1 Data and Model,[0],[0]
The full specification of the model can also be found in our released code online at https://github.,4.1 Data and Model,[0],[0]
com/qipeng/arc-swift.,4.1 Data and Model,[0],[0]
"We use static oracles for all transition systems, and for arc-eager we implement oracles that always Shift/Reduce when ambiguity is present (arceager-S/R).",4.2 Results,[0],[0]
"We evaluate our parsers with greedy parsing (i.e., beam size 1).",4.2 Results,[0],[0]
"The results are shown in Table 1.5 Note that K&G 2016 is trained with a dynamic oracle (Goldberg and Nivre, 2012), Andor 2016 with a CRF-like loss, and both Andor 2016 and Weiss 2015 employed beam search (with sizes 32 and 8, respectively).
",4.2 Results,[0],[0]
"For each pair of the systems we implemented, we studied the statistical significance of their difference by performing a paired test with 10,000 bootstrap samples on PTB-SD.",4.2 Results,[0],[0]
"The resulting pvalues are analyzed with a 10-group BonferroniHolm test, with results shown in Table 2.",4.2 Results,[0],[0]
"We note
5In the interest of space, we abbreviate all transition systems (TS) as follows in tables: asw for arc-swift, asd for arcstandard, aeS/R for arc-eager-S/R, and ah for arc-hybrid.
",4.2 Results,[0],[0]
"that with almost the same implementation, arcswift parsers significantly outperform those using traditional transition systems.",4.2 Results,[0],[0]
We also analyzed the performance of parsers on attachments of different distances.,4.2 Results,[0],[0]
"As shown in Figure 4, arc-swift is equally accurate as existing systems for short dependencies, but is more robust for longer ones.
",4.2 Results,[0],[0]
"While arc-swift introduces direct long-distance transitions, it also shortens the overall sequence necessary to induce the same parse.",4.2 Results,[0],[0]
"A parser could potentially benefit from both factors: direct attachments could make an easier classification task, and shorter sequences limit the effect of error propagation.",4.2 Results,[0],[0]
"However, since the two effects are correlated in a transition system, precise attribution of the gain is out of the scope of this paper.
",4.2 Results,[0],[0]
Computational efficiency.,4.2 Results,[0],[0]
"We study the computational efficiency of the arc-swift parser by
6https://github.com/tensorflow/models/ blob/master/syntaxnet/g3doc/universal.md
comparing it to an arc-eager parser.",4.2 Results,[0],[0]
"On the PTBSD development set, the average transition sequence length per sentence of arc-swift is 77.5% of that of arc-eager.",4.2 Results,[0],[0]
"At each step of parsing, arc-swift needs to evaluate only about 1.24 times the number of transition candidates as arc-eager, which results in very similar runtime.",4.2 Results,[0],[0]
"In contrast, beam search with beam size 2 for arc-eager requires evaluating 4 times the number of transition candidates compared to greedy parsing, which results in a UAS 0.14% worse and LAS 0.22% worse for arc-eager compared to greedily decoded arcswift.",4.2 Results,[0],[0]
"We automatically extracted all labelled attachment errors by error type (incorrect attachment or relation), and categorized a few top parser errors by hand into linguistic constructions.",4.3 Linguistic Analysis,[0],[0]
"Results on PTB-SD are shown in Table 3.7 We note that the arc-swift parser improves accuracy on prepositional phrase (PP) and conjunction attachments, while it remains comparable to other parsers on other common errors.",4.3 Linguistic Analysis,[0],[0]
Analysis on EN-UD shows a similar trend.,4.3 Linguistic Analysis,[0],[0]
"As shown in the table, there are still many parser errors unaccounted for in our analysis.",4.3 Linguistic Analysis,[0],[0]
"We leave this to future work.
",4.3 Linguistic Analysis,[0],[0]
"7We notice that for some examples the parsers predicted a ccomp (complement clause) attachment to verbs “says” and “said”, where the CoreNLP output simply labelled the relation as dep (unspecified).",4.3 Linguistic Analysis,[0],[0]
For other examples the relation between the prepositions in “out of” is labelled as prep (preposition) instead of pcomp (prepositional complement).,4.3 Linguistic Analysis,[0],[0]
"We suspect this is due to the converter’s inability to handle certain corner cases, but further study is warranted.",4.3 Linguistic Analysis,[0],[0]
Previous work has also explored augmenting transition systems to facilitate longer-range attachments.,5 Related Work,[0],[0]
"Attardi (2006) extended the arcstandard system for non-projective parsing, with arc-inducing transitions that are very similar to those in arc-swift.",5 Related Work,[0],[0]
A notable difference is that their transitions retain tokens between the head and dependent.,5 Related Work,[0],[0]
"Fernández-González and GómezRodrı́guez (2012) augmented the arc-eager system with transitions that operate on the buffer, which shorten the transition sequence by reducing the number of Shift transitions needed.",5 Related Work,[0],[0]
"However, limited by the sparse feature-based classifiers used, both of these parsers just mentioned only allow direct attachments of distance up to 3 and 2, respectively.",5 Related Work,[0],[0]
"More recently, Sartorio et al. (2013) extended arc-standard with transitions that directly attach to left and right “spines” of the top two nodes in the stack.",5 Related Work,[0],[0]
"While this work shares very similar motivations as arc-swift, it requires additional data structures to keep track of the left and right spines of nodes.",5 Related Work,[0],[0]
"This transition system also introduces spurious ambiguity where multiple transition sequences could lead to the same correct parse, which necessitates easy-first training to achieve a more noticeable improvement over arcstandard.",5 Related Work,[0],[0]
"In contrast, arc-swift can be easily implemented given the parser state alone, and does not give rise to spurious ambiguity.
",5 Related Work,[0],[0]
"For a comprehensive study of transition systems for dependency parsing, we refer the reader to (Bohnet et al., 2016), which proposed a generalized framework that could derive all of the traditional transition systems we described by configuring the size of the active token set and the maximum arc length, among other control parameters.",5 Related Work,[0],[0]
"However, this framework does not cover
arc-swift in its original form, as the authors limit each of their transitions to reduce at most one token from the active token set (the buffer).",5 Related Work,[0],[0]
"On the other hand, the framework presented in (GómezRodrı́guez and Nivre, 2013) does not explicitly make this constraint, and therefore generalizes to arc-swift.",5 Related Work,[0],[0]
"However, we note that arc-swift still falls out of the scope of existing discussions in that work, by introducing multiple Reduces in a single transition.",5 Related Work,[0],[0]
"In this paper, we introduced arc-swift, a novel transition system for dependency parsing.",6 Conclusion,[0],[0]
We also performed linguistic analyses on parser outputs and showed arc-swift parsers reduce errors in conjunction and adverbial attachments compared to parsers using traditional transition systems.,6 Conclusion,[0],[0]
"We thank Timothy Dozat, Arun Chaganty, Danqi Chen, and the anonymous reviewers for helpful discussions.",Acknowledgments,[0],[0]
Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract,Acknowledgments,[0],[0]
No. FA8750-13-2-0040.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.",Acknowledgments,[0],[0]
"Our model setup is similar to that of (Kiperwasser and Goldberg, 2016)",A Model and Training Details,[0],[0]
(See Figure 5).,A Model and Training Details,[0],[0]
"We employ two blocks of bidirectional long short-term memory (BiLSTM) networks (Hochreiter and Schmidhuber, 1997) that share very similar structures, one for part-of-speech (POS) tagging, the other for parsing.",A Model and Training Details,[0],[0]
"Both BiLSTMs have 400 hidden units in each direction, and the output of both are concatenated and fed into a dense layer of rectified linear units (ReLU) before 32-dimensional representations are derived as classification features.",A Model and Training Details,[0],[0]
"As the input to the tagger BiLSTM, we represent words with 100-dimensional word embeddings, initialized with GloVe vectors (Pennington et al., 2014).8",A Model and Training Details,[0],[0]
"The output distribution of the tagger classifier is used to compute a weighted sum of 32- dimensional POS embeddings, which is then concatenated with the output of the tagger BiLSTM",A Model and Training Details,[0],[0]
(800-dimensional per token) as the input to the parser BiLSTM.,A Model and Training Details,[0],[0]
"For the parser BiLSTM, we use two separate sets of dense layers to derive a “head” and a “dependent” representation for each token.",A Model and Training Details,[0],[0]
"These representations are later merged according to the parser state to make transition predictions.
",A Model and Training Details,[0],[0]
"For traditional transition systems, we follow (Kiperwasser and Goldberg, 2016) by featurizing the top 3 tokens on the stack and the leftmost token in the buffer.",A Model and Training Details,[0],[0]
"To derive features for each token, we take its head representation vhead and dependent representation vdep, and perform the following biaffine combination
vfeat,i =",A Model and Training Details,[0],[0]
"[f(vhead, vdep)]i = ReLU ( v>headWivdep +",A Model and Training Details,[0],[0]
b,A Model and Training Details,[0],[0]
>,A Model and Training Details,[0],[0]
"i vhead
+ c",A Model and Training Details,[0],[0]
">i vdep + di ) (1)
where Wi ∈ R32×32, bi, ci ∈ R32, and di is a scalar for i = 1, . . .",A Model and Training Details,[0],[0]
", 32.",A Model and Training Details,[0],[0]
"The resulting 32- dimensional features are concatenated as the input
8We also kept the vectors of the top 400k words trained on Wikipedia and English Gigaword for a broader coverage of unseen words.
to a fixed-dimensional softmax classifier for transition decisions.
",A Model and Training Details,[0],[0]
"For arc-swift, we featurize for each arcinducing transition with the same composition function in Equation (1) with vhead of the head token and vdep of the dependent token of the arc to be induced.",A Model and Training Details,[0],[0]
"For Shift, we simply combine vhead and vdep of the leftmost token in the buffer with the biaffine combination, and obtain its score by computing the inner-product of the feature and a vector.",A Model and Training Details,[0],[0]
"At each step, the scores of all feasible transitions are normalized to a probability distribution by a softmax function.
",A Model and Training Details,[0],[0]
"In all of our experiments, the parsers are trained to maximize the log likelihood of the desired transition sequence, along with the tagger being trained to maximize the log likelihood of the correct POS tag for each token.
",A Model and Training Details,[0],[0]
"To train the parsers, we use the ADAM optimizer (Kingma and Ba, 2014), with β2 = 0.9, an initial learning rate of 0.001, and minibatches of size 32 sentences.",A Model and Training Details,[0],[0]
Parsers are trained for 10 passes through the dataset on PTB-SD.,A Model and Training Details,[0],[0]
We also find that annealing the learning rate by a factor of 0.5 for every pass after the 5th helped improve performance.,A Model and Training Details,[0],[0]
"For EN-UD, we train for 30 passes, and anneal the learning rate for every 3 passes after the 15th due to the smaller size of the dataset.",A Model and Training Details,[0],[0]
"For all of the biaffine combination layers and dense layers, we dropout their units with a small probability of 5%.",A Model and Training Details,[0],[0]
"Also during training time, we randomly replace 10% of the input words by an artificial 〈UNK〉 token, which is then used to replace
all unseen words in the development and test sets.",A Model and Training Details,[0],[0]
"Finally, we repeat each experiment with 3 independent random initializations, and use the average result for reporting and statistical significance tests.
",A Model and Training Details,[0],[0]
The code for the full specification of our models and aforementioned training details are available at https://github.com/qipeng/ arc-swift.,A Model and Training Details,[0],[0]
Transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments.,abstractText,[0],[0]
Correct individual decisions hence require global information about the sentence context and mistakes cause error propagation.,abstractText,[0],[0]
"This paper proposes a novel transition system, arc-swift, that enables direct attachments between tokens farther apart with a single transition.",abstractText,[0],[0]
This allows the parser to leverage lexical information more directly in transition decisions.,abstractText,[0],[0]
"Hence, arc-swift can achieve significantly better performance with a very small beam size.",abstractText,[0],[0]
Our parsers reduce error by 3.7–7.6% relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies.,abstractText,[0],[0]
Arc-swift: A Novel Transition System for Dependency Parsing,title,[0],[0]
"1 Are BLEU and Meaning Representation in Opposition?
One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.",text,[0],[0]
Deep learning has brought the possibility of automatically learning continuous representations of sentences.,1 Introduction,[0],[0]
"On the one hand, such representations can be geared towards particular tasks such as classifying the sentence in various aspects (e.g. sentiment, register, question type) or relating the sentence to other sentences (e.g. semantic similarity, paraphrasing, entailment).",1 Introduction,[0],[0]
"On the other hand, we can aim at “universal” sentence representations, that is representations performing reasonably well in a range of such tasks.
",1 Introduction,[0],[0]
"Regardless the evaluation criterion, the representations can be learned either in an unsupervised way (from simple, unannotated texts) or supervised, relying on manually constructed training sets of sentences equipped with annotations of the appropriate type.",1 Introduction,[0],[0]
"A different approach is to obtain sentence representations from training neural machine translation models (Hill et al., 2016).
",1 Introduction,[0],[0]
"Since Hill et al. (2016), NMT has seen substantial advances in translation quality and it is thus
natural to ask how these improvements affect the learned representations.
",1 Introduction,[0],[0]
"One of the key technological changes was the introduction of “attention” (Bahdanau et al., 2014), making it even the very central component in the network (Vaswani et al., 2017).",1 Introduction,[0],[0]
Attention allows the NMT system to dynamically choose which parts of the source are most important when deciding on the current output token.,1 Introduction,[0],[0]
"As a consequence, there is no longer a static vector representation of the sentence available in the system.
",1 Introduction,[0],[0]
"In this paper, we remove this limitation by proposing a novel encoder-decoder architecture with a structured fixed-size representation of the input that still allows the decoder to explicitly focus on different parts of the input.",1 Introduction,[0],[0]
"In other words, our NMT system has both the capacity to attend to various parts of the input and to produce static representations of input sentences.
",1 Introduction,[0],[0]
"We train this architecture on English-to-German and English-to-Czech translation and evaluate the learned representations of English on a wide range of tasks in order to assess its performance in learning “universal” meaning representations.
",1 Introduction,[0],[0]
"In Section 2, we briefly review recent efforts in obtaining sentence representations.",1 Introduction,[0],[0]
"In Section 3, we introduce a number of variants of our novel architecture.",1 Introduction,[0],[0]
Section 4 describes some standard and our own methods for evaluating sentence representations.,1 Introduction,[0],[0]
Section 5 then provides experimental results: translation and representation quality.,1 Introduction,[0],[0]
The relation between the two is discussed in Section 6.,1 Introduction,[0],[0]
The properties of continuous sentence representations have always been of interest to researchers working on neural machine translation.,2 Related Work,[0],[0]
"In the first works on RNN sequence-to-sequence models, Cho et al. (2014) and Sutskever et al. (2014)
ar X
iv :1
80 5.
06 53
6v 1
[ cs
.C",2 Related Work,[0],[0]
"L
] 1
6 M
ay 2
01 8
2
provided visualizations of the phrase and sentence embedding spaces and observed that they reflect semantic and syntactic structure to some extent.",2 Related Work,[0],[0]
"Hill et al. (2016) perform a systematic evaluation of sentence representation in different models, including NMT, by applying them to various sentence classification tasks and by relating semantic similarity to closeness in the representation space.",2 Related Work,[0],[0]
"Shi et al. (2016) investigate the syntactic properties of representations learned by NMT systems by predicting sentence- and word-level syntactic labels (e.g. tense, part of speech) and by generating syntax trees from these representations.",2 Related Work,[0],[0]
Schwenk and Douze (2017) aim to learn language-independent sentence representations using NMT systems with multiple source and target languages.,2 Related Work,[0],[0]
They do not consider the attention mechanism and evaluate primarily by similarity scores of the learned representations for similar sentences (within or across languages).,2 Related Work,[0],[0]
"Our proposed model architectures differ in (a) which encoder states are considered in subsequent processing, (b) how they are combined, and (c) how they are used in the decoder.",3 Model Architectures,[0],[0]
Table 1 summarizes all the examined configurations of RNN-based models.,3 Model Architectures,[0],[0]
"The first three (ATTN, FINAL, FINAL-CTX) correspond roughly to the standard sequence-to-sequence models, Bahdanau et al. (2014), Sutskever et al. (2014) and Cho et al. (2014), respectively.",3 Model Architectures,[0],[0]
"The last column (ATTN-ATTN) is our main proposed architecture: compound attention, described here in Section 3.1.",3 Model Architectures,[0],[0]
"In addition to RNN-based models, we experiment with the Transformer model, see Section 3.3.",3 Model Architectures,[0],[0]
Our compound attention model incorporates attention in both the encoder and the decoder.,3.1 Compound Attention,[0],[0]
Its architecture is shown in Fig. 1.,3.1 Compound Attention,[0],[0]
Encoder with inner attention.,3.1 Compound Attention,[0],[0]
"First, we process the input sequence x1, x2, . . .",3.1 Compound Attention,[0],[0]
", xT using a bidirectional recurrent network with gated recurrent units (GRU, Cho et al., 2014):",3.1 Compound Attention,[0],[0]
"−→ ht = −−→ GRU(xt, −−→ ht−1), ←− ht = ←−− GRU(xt, ←−− ht+1), ht =",3.1 Compound Attention,[0],[0]
"[ −→ ht , ←− ht ].
",3.1 Compound Attention,[0],[0]
3,3.1 Compound Attention,[0],[0]
"We denote by u the combined number of units in the two RNNs, i.e. the dimensionality of ht.",3.1 Compound Attention,[0],[0]
"Next, our goal is to combine the states (h1, h2, . . .",3.1 Compound Attention,[0],[0]
", hT )",3.1 Compound Attention,[0],[0]
= H of the encoder into a vector of fixed dimensionality that represents the entire sentence.,3.1 Compound Attention,[0],[0]
Traditional seq2seq models concatenate the final states of both encoder RNNs ( −→ hT and ←− h1) to obtain the sentence representation (denoted as FINAL in Table 1).,3.1 Compound Attention,[0],[0]
"Another option is to combine all encoder states using the average or maximum over time (Collobert and Weston, 2008; Schwenk and Douze, 2017) (AVGPOOL and MAXPOOL in Table 1 and following).",3.1 Compound Attention,[0],[0]
"We adopt an alternative approach, which is to use inner attention1 (Liu et al., 2016; Li et al., 2016) to compute several weighted averages of the encoder states (Lin et al., 2017).",3.1 Compound Attention,[0],[0]
"The main motivation for incorporating these multiple “views” of the state sequence is that it removes the need for the RNN cell to accumulate the representation of the whole sentence as it processes the input, and therefore it should have more capacity for modeling local dependencies.",3.1 Compound Attention,[0],[0]
"Specifically, we fix a number r, the number of attention heads, and compute an r×T matrixA of attention weights αjt, representing the importance of position t in the input for the jth attention head.",3.1 Compound Attention,[0],[0]
"We then use this matrix to compute r weighted sums of the encoder states, which become the rows of a new matrix M : M = AH.",3.1 Compound Attention,[0],[0]
(1) A vector representation of the source sentence (the “sentence embedding”) can be obtained by flattening the matrix M .,3.1 Compound Attention,[0],[0]
"In our experiments, we project the encoder states h1, h2, . . .",3.1 Compound Attention,[0],[0]
", hT down to a given dimensionality before applying Eq.",3.1 Compound Attention,[0],[0]
"(1), so that we can control the size of the representation.",3.1 Compound Attention,[0],[0]
"Following Lin et al. (2017), we compute the attention matrix by feeding the encoder states to a two-layer feed-forward network: A = softmax(U tanh(WH>)), (2) where W and U are weight matrices of dimensions d× u and r × d, respectively (d is the number of hidden units); the softmax function is applied along the second dimension, i.e. across the encoder states.",3.1 Compound Attention,[0],[0]
1Some papers call the same or similar approach selfattention or single-time attention.,3.1 Compound Attention,[0],[0]
Attentive decoder.,3.1 Compound Attention,[0],[0]
"In vanilla seq2seq models with a fixed-size sentence representation, the decoder is usually conditioned on this representation via the initial RNN state.",3.1 Compound Attention,[0],[0]
We propose to instead leverage the structured sentence embedding by applying attention to its components.,3.1 Compound Attention,[0],[0]
"This is no different from the classical attention mechanism used in NMT (Bahdanau et al., 2014), except that it acts on this fixed-size representation instead of the sequence of encoder states.",3.1 Compound Attention,[0],[0]
"In the ith decoding step, the attention mechanism computes a distribution {βij}rj=1 over the r components of the structured representation.",3.1 Compound Attention,[0],[0]
"This is then used to weight these components to obtain the context vector ci, which in turn is used to update the decoder state.",3.1 Compound Attention,[0],[0]
"Again, we can write this in matrix form as C = BM, (3) where B = (βij) T ′,r i=1,j=1 is the attention matrix and C = (ci, c2, . . .",3.1 Compound Attention,[0],[0]
", cT ′) are the context vectors.",3.1 Compound Attention,[0],[0]
Note that by combining Eqs.,3.1 Compound Attention,[0],[0]
"(1) and (3), we get C = (BA)H. (4) Hence, the composition of the encoder and decoder attentions (the “compound attention”) defines an implicit alignment between the source and the target sequence.",3.1 Compound Attention,[0],[0]
"From this viewpoint, our model can be regarded as a restriction of the conventional attention model.",3.1 Compound Attention,[0],[0]
"The decoder uses a conditional GRU cell (cGRUatt; Sennrich et al., 2017), which consists of two consecutively applied GRU blocks.",3.1 Compound Attention,[0],[0]
"The first block processes the previous target token yi−1, while the second block receives the context vector ci and predicts the next target token yi.",3.1 Compound Attention,[0],[0]
"Compared to the FINAL model, the compound attention architecture described in the previous section undoubtedly benefits from the fact that the decoder is presented with information from the encoder (i.e. the context vectors ci) in every decoding step.",3.2 Constant Context,[0],[0]
"To investigate this effect, we include baseline models where we replace all context vectors ci with the entire sentence embedding (indicated by the suffix “-CTX” in Table 1).",3.2 Constant Context,[0],[0]
"Specifically, we provide either the flattened matrixM (for models with inner attention; ATTN or ATTN-CTX), the final state of the encoder (FINAL-CTX), or the
4
result of mean- or max-pooling (*POOL-CTX) as a constant input to the decoder cell.",3.2 Constant Context,[0],[0]
"The Transformer (Vaswani et al., 2017) is a recently proposed model based entirely on feedforward layers and attention.",3.3 Transformer with Inner Attention,[0],[0]
"It consists of an encoder and a decoder, each with 6 layers, consisting of multi-head attention on the previous layer and a position-wise feed-forward network.",3.3 Transformer with Inner Attention,[0],[0]
"In order to introduce a fixed-size sentence representation into the model, we modify it by adding inner attention after the last encoder layer.",3.3 Transformer with Inner Attention,[0],[0]
The attention in the decoder then operates on the components of this representation (i.e. the rows of the matrix M ).,3.3 Transformer with Inner Attention,[0],[0]
This variation on the Transformer model corresponds to the ATTN-ATTN column in Table 1 and is therefore denoted TRF-ATTN-ATTN.,3.3 Transformer with Inner Attention,[0],[0]
"Continuous sentence representations can be evaluated in many ways, see e.g. Kiros et al. (2015), Conneau et al. (2017) or the RepEval workshops.2 We evaluate our learned representations with classification and similarity tasks from SentEval (Section 4.1) and by examining clusters of sentence paraphrase representations (Section 4.2).",4 Representation Evaluation,[0],[0]
"We perform evaluation on 10 classification and 7 similarity tasks using the SentEval3 (Conneau et al., 2017) evaluation tool.",4.1 SentEval,[0],[0]
This is a superset of the tasks from Kiros et al. (2015).,4.1 SentEval,[0],[0]
"2https://repeval2017.github.io/ 3https://github.com/facebookresearch/ SentEval/
Table 2 describes the classification tasks (number of classes, data size, task type and an example) and Table 3 lists the similarity tasks.",4.1 SentEval,[0],[0]
The similarity (relatedness) datasets contain pairs of sentences labeled with a real-valued similarity score.,4.1 SentEval,[0],[0]
"A given sentence representation model is evaluated either by learning to directly predict this score given the respective sentence embeddings (“regression”), or by computing the cosine similarity of the embeddings (“similarity”) without the need of any training.",4.1 SentEval,[0],[0]
"In both cases, Pearson and Spearman correlation of the predictions with the gold ratings is reported.",4.1 SentEval,[0],[0]
See Dolan et al. (2004) for details on MRPC and Hill et al. (2016) for the remaining tasks.,4.1 SentEval,[0],[0]
We also evaluate the representation of paraphrases.,4.2 Paraphrases,[0],[0]
We use two paraphrase sources for this purpose: COCO and HyTER Networks.,4.2 Paraphrases,[0],[0]
"COCO (Common Objects in Context; Lin et al., 2014) is an object recognition and image captioning dataset, containing 5 captions for each image.",4.2 Paraphrases,[0],[0]
We extracted the captions from its validation set to form a set of 5 × 5k = 25k sentences grouped by the source image.,4.2 Paraphrases,[0],[0]
The average sentence length is 11 tokens and the vocabulary size is 9k types.,4.2 Paraphrases,[0],[0]
"HyTER Networks (Dreyer and Marcu, 2014)
5 are large finite-state networks representing a subset of all possible English translations of 102 Arabic and 102 Chinese sentences.",4.2 Paraphrases,[0],[0]
"The networks were built by manually based on reference sentences in Arabic, Chinese and English.",4.2 Paraphrases,[0],[0]
Each network contains up to hundreds of thousands of possible translations of the given source sentence.,4.2 Paraphrases,[0],[0]
"We randomly generated 500 translations for each source sentence, obtaining a corpus of 102k sentences grouped into 204 clusters, each containing 500 paraphrases.",4.2 Paraphrases,[0],[0]
The average length of the 102k English sentences is 28 tokens and the vocabulary size is 11k token types.,4.2 Paraphrases,[0],[0]
"For every model, we encode each dataset to obtain a set of sentence embeddings with cluster labels.",4.2 Paraphrases,[0],[0]
We then compute the following metrics: Cluster classification accuracy (denoted “Cl”).,4.2 Paraphrases,[0],[0]
"We remove 1 point (COCO) or half of the points (HyTER) from each cluster, and fit an LDA classifier on the rest.",4.2 Paraphrases,[0],[0]
We then compute the accuracy of the classifier on the removed points.,4.2 Paraphrases,[0],[0]
Nearest-neighbor paraphrase retrieval accuracy (NN).,4.2 Paraphrases,[0],[0]
"For each point, we find its nearest neighbor according to cosine or L2 distance, and count how often the neighbor lies in the same cluster as the original point.",4.2 Paraphrases,[0],[0]
Inverse Davies-Bouldin index (iDB).,4.2 Paraphrases,[0],[0]
"The Davies-Bouldin index (Davies and Bouldin, 1979) measures cluster separation.",4.2 Paraphrases,[0],[0]
"For every pair of clusters, we compute the ratio Rij of their combined scatter (average L2 distance to the centroid) Si + Sj and the L2 distance of their centroids dij , then average the maximum values for all clusters: Rij = Si + Sj dij (5) DB = 1 N N∑ i=1",4.2 Paraphrases,[0],[0]
"max j 6=i Rij (6) The lower the DB index, the better the separation.",4.2 Paraphrases,[0],[0]
"To match with the rest of our metrics, we take its inverse: iDB = 1DB .",4.2 Paraphrases,[0],[0]
"We trained English-to-German and English-toCzech NMT models using Neural Monkey4 (Helcl and Libovický, 2017a).",5 Experimental Results,[0],[0]
"In the following, we distinguish these models using the code of the target language, i.e. de or cs. 4https://github.com/ufal/neuralmonkey",5 Experimental Results,[0],[0]
"The de models were trained on the Multi30K multilingual image caption dataset (Elliott et al., 2016), extended by Helcl and Libovický (2017b), who acquired additional parallel data using backtranslation (Sennrich et al., 2016) and perplexitybased selection (Yasuda et al., 2008).",5 Experimental Results,[0],[0]
"This extended dataset contains 410k sentence pairs, with the average sentence length of 12 ± 4 tokens in English.",5 Experimental Results,[0],[0]
We train each model for 20 epochs with the batch size of 32.,5 Experimental Results,[0],[0]
We truecased the training data as well as all data we evaluate on.,5 Experimental Results,[0],[0]
"For German, we employed Neural Monkey’s reversible pre-processing scheme, which expands contractions and performs morphological segmentation of determiners.",5 Experimental Results,[0],[0]
We used a vocabulary of at most 30k tokens for each language (no subword units).,5 Experimental Results,[0],[0]
"The cs models were trained on CzEng 1.7 (Bojar et al.,",5 Experimental Results,[0],[0]
"2016).5 We used byte-pair encoding (BPE) with a vocabulary of 30k sub-word units, shared for both languages.",5 Experimental Results,[0],[0]
"For English, the average sentence length is 15±19 BPE tokens and the original vocabulary size is 1.9M. We performed 1 training epoch with the batch size of 128 on the entire training section (57M sentence pairs).",5 Experimental Results,[0],[0]
"The datasets for both de and cs models come with their respective development and test sets of sentence pairs, which we use for the evaluation of translation quality.",5 Experimental Results,[0],[0]
(We use 1k randomly selected sentence pairs from CzEng 1.7 dtest as a development set.,5 Experimental Results,[0],[0]
"For evaluation, we use the entire etest.)",5 Experimental Results,[0],[0]
"We also evaluate the InferSent model6 (Conneau et al., 2017) as pre-trained on the natural language inference (NLI) task.",5 Experimental Results,[0],[0]
InferSent has been shown to achieve state-of-the-art results on the SentEval tasks.,5 Experimental Results,[0],[0]
"We also include a bag-ofwords baseline (GloVe-BOW) obtained by averaging GloVe7 word vectors (Pennington et al., 2014).",5 Experimental Results,[0],[0]
"We estimate translation quality of the various models using single-reference case-sensitive BLEU (Papineni et al., 2002) as implemented in Neural Monkey (the reference implementation is mteval-v13a.pl from NIST or Moses).",5.1 Translation Quality,[0],[0]
Tables 4 and 5 provide the results on the two datasets.,5.1 Translation Quality,[0],[0]
The cs dataset is much larger and the training takes much longer.,5.1 Translation Quality,[0],[0]
"We were thus able 5http://ufal.mff.cuni.cz/czeng/czeng17 6https://github.com/facebookresearch/ InferSent 7https://nlp.stanford.edu/projects/ glove/
6
to experiment with only a subset of the possible model configurations.",5.1 Translation Quality,[0],[0]
The columns “Size” and “Heads” specify the total size of sentence representation and the number of heads of encoder inner attention.,5.1 Translation Quality,[0],[0]
"In both cases, the best performing is the ATTN Bahdanau et al. model, followed by Transformer (de only) and our ATTN-ATTN (compound attention).",5.1 Translation Quality,[0],[0]
"The non-attentive FINAL Cho et al. is the worst, except cs-MAXPOOL.",5.1 Translation Quality,[0],[0]
"For 5 selected cs models, we also performed the WMT-style 5-way manual ranking on 200 sentence pairs.",5.1 Translation Quality,[0],[0]
The annotations are interpreted as simulated pairwise comparisons.,5.1 Translation Quality,[0],[0]
"For each model, the final score is the number of times the model was judged better than the other model in the pair.",5.1 Translation Quality,[0],[0]
Tied pairs are excluded.,5.1 Translation Quality,[0],[0]
"The results, shown in Table 5, confirm the automatic evaluation results.",5.1 Translation Quality,[0],[0]
"We also checked the relation between BLEU
and the number of heads and representation size.",5.1 Translation Quality,[0],[0]
"While there are many exceptions, the general tendency is that the larger the representation or the more heads, the higher the BLEU score.",5.1 Translation Quality,[0],[0]
The Pearson correlation between BLEU and the number of heads is 0.87 for cs and 0.31 for de.,5.1 Translation Quality,[0],[0]
"Due to the large number of SentEval tasks, we present the results abridged in two different ways: by reporting averages (Table 6) and by showing only the best models in comparison with other methods (Table 7).",5.2 SentEval,[0],[0]
The full results can be found in the supplementary material.,5.2 SentEval,[0],[0]
"Table 6 provides averages of the classification and similarity results, along with the results of selected tasks (SNLI, SICK-E).",5.2 SentEval,[0],[0]
"As the baseline for classifications tasks, we assign the most frequent class to all test examples.8",5.2 SentEval,[0],[0]
"The de models are generally worse, most likely due to the higher OOV rate and overall simplicity of the training sentences.",5.2 SentEval,[0],[0]
"On cs, we see a clear pattern that more heads hurt the performance.",5.2 SentEval,[0],[0]
The de set has more variations to consider but the results are less conclusive.,5.2 SentEval,[0],[0]
"For the similarity results, it is worth noting that cs-ATTN-ATTN performs very well with 1 attention head but fails miserably with more heads.",5.2 SentEval,[0],[0]
"Otherwise, the relation to the number of heads is less clear.",5.2 SentEval,[0],[0]
Table 7 compares our strongest models with other approaches on all tasks.,5.2 SentEval,[0],[0]
"Besides InferSent and GloVe-BOW, we include SkipThought as evaluated by Conneau et al. (2017), and the NMTbased embeddings by Hill et al. (2016) trained on the English-French WMT15 dataset (this is the best result reported by Hill et al. for NMT).",5.2 SentEval,[0],[0]
We see that the supervised InferSent clearly outperforms all other models in all tasks except for MRPC and TREC.,5.2 SentEval,[0],[0]
"Results by Hill et al. are always lower than our best setups, except MRPC.",5.2 SentEval,[0],[0]
"On classification tasks, our models are outperformed even by GloVe-BOW, except for the NLI tasks (SICK-E and SNLI) where cs-FINAL-CTX is better.",5.2 SentEval,[0],[0]
Table 6 also provides our measurements based on sentence paraphrases.,5.3 Paraphrase Scores,[0],[0]
"For paraphrase retrieval 8For MR, CR, SUBJ, and MPQA, where there is no distinct test set, the class is established on the whole collection.",5.3 Paraphrase Scores,[0],[0]
"For other tasks, the class is learned from the training set.
7
8
B L
E U
M R C R S U
B J
M P
Q A
S S
T 2
S S
T 5
T R
E C
M R
P C
S IC
K -E
S N
L I",5.3 Paraphrase Scores,[0],[0]
A v g,5.3 Paraphrase Scores,[0],[0]
A cc S IC K -R S T S B S T S 1 2 S T S 1 3 S T S 1 4 S T S 1 5 S T S 1 6 A v g S,5.3 Paraphrase Scores,[0],[0]
im H y -C l H y -N N H y -i D B C O -C,5.3 Paraphrase Scores,[0],[0]
l C O -N N C,5.3 Paraphrase Scores,[0],[0]
"O -i D B
BLEU MR CR
SUBJ MPQA
SST2 SST5 TREC MRPC
SICK-E SNLI AvgAcc SICK-R
STSB STS12 STS13",5.3 Paraphrase Scores,[0],[0]
"STS14 STS15 STS16 AvgSim Hy-Cl Hy-NN Hy-iDB CO-Cl CO-NN CO-iDB
−1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
Figure 2: Pearson correlations.",5.3 Paraphrase Scores,[0],[0]
"Upper triangle: de models, lower triangle: cs models.",5.3 Paraphrase Scores,[0],[0]
Positive values shown in shades of green.,5.3 Paraphrase Scores,[0],[0]
"For similarity tasks, only the Pearson (not Spearman) coefficient is represented.
",5.3 Paraphrase Scores,[0],[0]
"(NN), we found cosine distance to work better than L2 distance.",5.3 Paraphrase Scores,[0],[0]
"We therefore do not list or further consider L2-based results (except in the supplementary material).
",5.3 Paraphrase Scores,[0],[0]
"This evaluation seems less stable and discerning than the previous two, but we can again confirm the victory of InferSent followed by our nonattentive cs models.",5.3 Paraphrase Scores,[0],[0]
cs and de models are no longer clearly separated.,5.3 Paraphrase Scores,[0],[0]
"To assess the relation between the various measures of sentence representations and translation quality as estimated by BLEU, we plot a heatmap of Pearson correlations in Fig. 2.",6 Discussion,[0],[0]
"As one example, Fig. 3 details the cs models’ BLEU scores and AvgAcc (average of SentEval accuracies).
",6 Discussion,[0],[0]
"A good sign is that on the cs dataset, most metrics of representation are positively correlated (the pairwise Pearson correlation is 0.78± 0.32 on average), the outlier being TREC (−0.16±0.16 correlation with the other metrics on average)
",6 Discussion,[0],[0]
"On the other hand, most representation metrics correlate with BLEU negatively (−0.57±0.31) on cs.",6 Discussion,[0],[0]
"The pattern is less pronounced but still clear also on the de dataset.
",6 Discussion,[0],[0]
"A detailed understanding of what the learned
representations contain is difficult.",6 Discussion,[0],[0]
"We can only speculate that if the NMT model has some capability for following the source sentence superficially, it will use it and spend its capacity on closely matching the target sentences rather than on deriving some representation of meaning which would reflect e.g. semantic similarity.",6 Discussion,[0],[0]
We assume that this can be a direct consequence of NMT being trained for cross entropy: putting the exact word forms in exact positions as the target sentence requires.,6 Discussion,[0],[0]
"Performing well in single-reference BLEU is not an indication that the system understands the meaning but rather that it can maximize the chance of producing the n-grams required by the reference.
",6 Discussion,[0],[0]
"The negative correlation between the number of attention heads and the representation metrics from Fig. 3 (−0.81±0.12 for cs and−0.18±0.19 for de, on average) can be partly explained by the following observation.",6 Discussion,[0],[0]
We plotted the induced alignments (e.g. Fig. 4) and noticed that the heads tend to “divide” the sentence into segments.,6 Discussion,[0],[0]
"While one would hope that the segments correspond to some meaningful units of the sentence (e.g. subject, predicate, object), we failed to find any such interpretation for ATTN-ATTN and for cs models in general.",6 Discussion,[0],[0]
"Instead, the heads divide the source sentence more or less equidistantly, as documented by Fig. 5.",6 Discussion,[0],[0]
"Such a multi-headed sentence representation is then less fit for representing e.g. paraphrases where the subject and object swap their position due to passivization, because their representations are then accessed by different heads, and thus end up in different parts of the sentence embedding vector.
9
For de-ATTN-CTX models, we observed a much flatter distribution of attention weights for each head and, unlike in the other models, we were often able to identify a head focusing on the main verb.",6 Discussion,[0],[0]
"This difference between ATTN-ATTN and some ATTN-CTX models could be explained by the fact that in the former, the decoder is oblivious to the ordering of the heads (because of decoder attention), and hence it may not be useful for a given head to look for a specific syntactic or semantic role.",6 Discussion,[0],[0]
"We presented a novel variation of attentive NMT models (Bahdanau et al., 2014; Vaswani et al., 2017) that again provides a single meeting point with a continuous representation of the source sen-
tence.",7 Conclusion,[0],[0]
We evaluated these representations with a number of measures reflecting how well the meaning of the source sentence is captured.,7 Conclusion,[0],[0]
"While our proposed “compound attention” leads to translation quality not much worse than the fully attentive model, it generally does not perform well in the meaning representation.",7 Conclusion,[0],[0]
"Quite on the contrary, the better the BLEU score, the worse the meaning representation.",7 Conclusion,[0],[0]
"We believe that this observation is important for representation learning where bilingual MT now seems less likely to provide useful data, but perhaps more so for MT itself, where the struggle towards a high single-reference BLEU score (or even worse, cross entropy) leads to systems that refuse to consider the meaning of the sentence.",7 Conclusion,[0],[0]
"This work has been supported by the grants 18-24210S of the Czech Science Foundation, SVV 260 453 and “Progress” Q18+Q48 of Charles University, and using language resources distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (projects LM2015071 and OP VVV VI CZ.02.1.01/0.0/0.0/16 013/0001781).
",Acknowledgement,[0],[0]
10,Acknowledgement,[0],[0]
One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems.,abstractText,[0],[0]
The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted.,abstractText,[0],[0]
We propose several variations of the attentive NMT architecture bringing this meeting point back.,abstractText,[0],[0]
"Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.",abstractText,[0],[0]
Are BLEU and Meaning Representation in Opposition?,title,[0],[0]
"Argument mining consists of the automatic identification of argumentative structures in documents, a valuable task with applications in policy making, summarization, and education, among others.",1 Introduction,[0],[0]
The argument mining task includes the tightly-knit subproblems of classifying propositions into elementary unit types and detecting argumentative relations between the elementary units.,1 Introduction,[0],[0]
"The desired output is a document argumentation graph structure, such as the one in Figure 1, where propositions are denoted by letter subscripts, and the associated argumentation graph shows their types and support relations between them.
",1 Introduction,[0],[0]
"Most annotation and prediction efforts in argument mining have focused on tree or forest structures (Peldszus and Stede, 2015; Stab and Gurevych, 2016), constraining argument structures to form one or more trees.",1 Introduction,[0],[0]
"This makes the problem computationally easier by enabling the use of maximum spanning tree–style parsing ap-
proaches.",1 Introduction,[0],[0]
"However, argumentation in the wild can be less well-formed.",1 Introduction,[0],[0]
"The argument put forth in Figure 1, for instance, consists of two components: a simple tree structure and a more complex graph structure (c jointly supports b and d).",1 Introduction,[0],[0]
"In this work, we design a flexible and highly expressive structured prediction model for argument mining, jointly learning to classify elementary units (henceforth propositions) and to identify the argumentative relations between them (henceforth links).",1 Introduction,[0],[0]
"By formulating argument mining as inference in a factor graph (Kschischang et al., 2001), our model (described in Section 4) can account for correlations between the two tasks, can consider second order link structures (e.g., in Figure 1, c → b → a), and can impose arbitrary constraints (e.g., transitivity).
",1 Introduction,[0],[0]
"To parametrize our models, we evaluate two alternative directions: linear structured SVMs
1We describe proposition types (FACT, etc.)",1 Introduction,[0],[0]
"in Section 3.
ar X
iv :1
70 4.
06 86
9v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
3 A
pr 2
01 7
(Tsochantaridis et al., 2005), and recurrent neural networks with structured loss, extending (Kiperwasser and Goldberg, 2016).",1 Introduction,[0],[0]
"Interestingly, RNNs perform poorly when trained with classification losses, but become competitive with the featureengineered structured SVMs when trained within our proposed structured learning model.
",1 Introduction,[0],[0]
We evaluate our approach on two argument mining datasets.,1 Introduction,[0],[0]
"Firstly, on our new Cornell eRulemaking Corpus – CDCP,2 consisting of argument annotations on comments from an eRulemaking discussion forum, where links don’t always form trees (Figure 1 shows an abridged example comment, and Section 3 describes the dataset in more detail).",1 Introduction,[0],[0]
"Secondly, on the UKP argumentative essays v2 (henceforth UKP), where argument graphs are annotated strictly as multiple trees (Stab and Gurevych, 2016).",1 Introduction,[0],[0]
"In both cases, the results presented in Section 5 confirm that our models outperform unstructured baselines.",1 Introduction,[0],[0]
"On UKP, we improve link prediction over the best reported result in (Stab and Gurevych, 2016), which is based on integer linear programming postprocessing.",1 Introduction,[0],[0]
"For insight into the strengths and weaknesses of the proposed models, as well as into the differences between SVM and RNN parameterizations, we perform an error analysis in Section 5.1.",1 Introduction,[0],[0]
"To support argument mining research, we also release our Python implementation, Marseille.3",1 Introduction,[0],[0]
Our factor graph formulation draws from ideas previously used independently in parsing and argument mining.,2 Related work,[0],[0]
"In particular, maximum spanning tree (MST) methods for arc-factored dependency parsing have been successfully used by McDonald et al. (2005) and applied to argument mining with mixed results by Peldszus and Stede (2015).",2 Related work,[0],[0]
"As they are not designed for the task, MST parsers cannot directly handle proposition classification or model the correlation between proposition and link prediction—a limitation our model addresses.",2 Related work,[0],[0]
"Using RNN features in an MST parser with a structured loss was proposed by Kiperwasser and Goldberg (2016); their model can be seen as a particular case of our factor graph approach, limited to link prediction with a tree structure constraint.",2 Related work,[0],[0]
"Our models support multi-task learning for proposition classification, parameter-
2Dataset available at http://joonsuk.org.",2 Related work,[0],[0]
"3Available at https://github.com/vene/marseille.
",2 Related work,[0],[0]
"izing adjacent links with higher-order structures (e.g., c → b → a) and enforcing arbitrary constraints on the link structure, not limited to trees.",2 Related work,[0],[0]
"Such higher-order structures and logic constraints have been successfully used for dependency and semantic parsing by Martins et al. (2013) and Martins and Almeida (2014); to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural networks.",2 Related work,[0],[0]
"Stab and Gurevych (2016) used an integer linear program to combine the output of independent proposition and link classifiers using a hand-crafted scoring formula, an approach similar to our baseline.",2 Related work,[0],[0]
"Our factor graph method can combine the two tasks in a more principled way, as it fully learns the correlation between the two tasks without relying on hand-crafted scoring, and therefore can readily be applied to other argumentation datasets.",2 Related work,[0],[0]
"Furthermore, our model can enforce the tree structure constraint, required on the UKP dataset, using MST cycle constraints used by Stab and Gurevych (2016), thanks to the AD3 inference algorithm (Martins et al., 2015).
",2 Related work,[0],[0]
"Sequence tagging has been applied to the related structured tasks of proposition identification and classification (Stab and Gurevych, 2016; Habernal and Gurevych, 2016; Park et al., 2015b); integrating such models is an important next step.",2 Related work,[0],[0]
"Meanwhile, a new direction in argument mining explores pointer networks (Potash et al., 2016); a promising method, currently lacking support for tree structures and domain-specific constraints.",2 Related work,[0],[0]
"We release a new argument mining dataset consisting of user comments about rule proposals regarding Consumer Debt Collection Practices (CDCP) by the Consumer Financial Protection Bureau collected from an eRulemaking website, http:// regulationroom.org.
Argumentation structures found in web discussion forums, such as the eRulemaking one we use, can be more free-form than the ones encountered in controlled, elicited writing such as (Peldszus and Stede, 2015).",3 Data,[0],[0]
"For this reason, we adopt the model proposed by Park et al. (2015a), which does not constrain links to form tree structures, but unrestricted directed graphs.",3 Data,[0],[0]
"Indeed, over 20% of the comments in our dataset exhibit local structures that would not be allowable in a tree.",3 Data,[0],[0]
"Possible link types are reason and evidence, and propo-
sition types are split into five fine-grained categories: POLICY and VALUE contain subjective judgements/interpretations, where only the former specifies a specific course of action to be taken.",3 Data,[0],[0]
"On the other hand, TESTIMONY and FACT do not contain subjective expressions, the former being about personal experience, or “anecdotal.”",3 Data,[0],[0]
"Lastly, REFERENCE covers URLs and citations, which are used to point to objective evidence in an online setting.
",3 Data,[0],[0]
"In comparison, the UKP dataset (Stab and Gurevych, 2016) only makes the syntactic distinction between CLAIM, MAJOR CLAIM, and PREMISE types, but it also includes attack links.",3 Data,[0],[0]
"The permissible link structure is stricter in UKP, with links constrained in annotation to form one or more disjoint directed trees within each paragraph.",3 Data,[0],[0]
"Also, since web arguments are not necessarily fully developed, our dataset has many argumentative propositions that are not in any argumentation relations.",3 Data,[0],[0]
"In fact, it isn’t unusual for comments to have no argumentative links at all: 28% of CDCP comments have no links, unlike UKP, where all essays have complete argument structures.",3 Data,[0],[0]
"Such comments with no links make the problem harder, emphasizing the importance of capturing the lack of argumentative support, not only its presence.",3 Data,[0],[0]
"Each user comment was annotated by two annotators, who independently annotated the boundaries and types of propositions, as well as the links among them.4 To produce the final corpus, a third annotator manually resolved the conflicts,5 and two automatic preprocessing steps were applied: we take the link transitive closure, and we remove a small number of nested propositions.6 The resulting dataset contains 731 comments, consisting of about 3800 sentences (≈4700 propositions) and 88k words.",3.1 Annotation results,[0],[0]
"Out of the 43k possible pairs of propositions, links are present between only 1300 (roughly 3%).",3.1 Annotation results,[0],[0]
"In comparison, UKP has fewer documents (402), but they are longer, with a total of 7100 sentences (6100 propositions) and 147k
4The annotators used the GATE annotation tool (Cunningham et al., 2011).
",3.1 Annotation results,[0],[0]
"5Inter-annotator agreement is measured with Krippendorf’s α (Krippendorff, 1980) with respect to elementary unit type (α=64.8%) and links (α=44.1%).",3.1 Annotation results,[0],[0]
"A separate paper describing the dataset is under preparation.
",3.1 Annotation results,[0],[0]
"6When two propositions overlap, we keep the one that results in losing the fewest links.",3.1 Annotation results,[0],[0]
"For generality, we release the dataset without this preprocessing, and include code to reproduce it; we believe that handling nested argumentative units is an important direction for further research.
words.",3.1 Annotation results,[0],[0]
"Since UKP links only occur within the same paragraph and propositions not connected to the argument are removed in a preprocessing step, link prediction is less imbalanced in UKP, with 3800 pairs of propositions being linked out of a total of 22k (17%).",3.1 Annotation results,[0],[0]
"We reserve a test set of 150 documents (973 propositions, 272 links) from CDCP, and use the provided 80-document test split from UKP (1266 propositions, 809 links).",3.1 Annotation results,[0],[0]
for argument mining,4 Structured learning,[0],[0]
"Binary and multi-class classification have been applied with some success to proposition and link prediction separately, but we seek a way to jointly learn the argument mining problem at the document level, to better model contextual dependencies and constraints.",4.1 Preliminaries,[0],[0]
"We therefore turn to structured learning, a framework that provides the desired level of expressivity.
",4.1 Preliminaries,[0],[0]
"In general, learning from a dataset of documents xi ∈ X and their associated labels yi ∈ Y involves seeking model parameters w that can “pick out” the best label under a scoring function f :
ŷ",4.1 Preliminaries,[0],[0]
":= argmaxy∈Y f(x, y;w).",4.1 Preliminaries,[0],[0]
"(1)
Unlike classification or regression, where X is usually a feature space Rd and Y ⊆ R (e.g., we predict an integer class index or a probability), in structured learning, more complex inputs and outputs are allowed.",4.1 Preliminaries,[0],[0]
"This makes the argmax in Equation 1 impossible to evaluate by enumeration, so it is desirable to find models that decompose over smaller units and dependencies between them; for instance, as factor graphs.",4.1 Preliminaries,[0],[0]
"In this section, we give a factor graph description of our proposed structured model for argument mining.",4.1 Preliminaries,[0],[0]
An input document is a string of words with proposition offsets delimited.,4.2 Model description,[0],[0]
"We denote the propositions in a document by {a, b, c, ...} and the possible directed link between a and b as a → b.",4.2 Model description,[0],[0]
The argument structure we seek to predict consists of the type of each proposition ya ∈ P and a binary label for each link ya→b ∈,4.2 Model description,[0],[0]
"R = {on, off}.7
7For simplicity and comparability, we follow Stab and Gurevych (2016) in using binary link labels even if links could be of different types.",4.2 Model description,[0],[0]
"This can be addressed in our model by incorporating “labeled link” factors.
",4.2 Model description,[0],[0]
The possible proposition types P differ for the two datasets; such differences are documented in Table 1.,4.2 Model description,[0],[0]
"As we describe the variables and factors constituting a document’s factor graph, we shall refer to Figure 2 for illustration.
",4.2 Model description,[0],[0]
Unary potentials.,4.2 Model description,[0],[0]
Each proposition a and each link a → b has a corresponding random variable in the factor graph (the circles in Figure 2).,4.2 Model description,[0],[0]
"To encode the model’s belief in each possible value for these variables, we parametrize the unary factors (gray boxes in Figure 2) with unary potentials: φ(a) ∈ R|P| is a score of ya for each possible proposition type.",4.2 Model description,[0],[0]
"Similarly, link unary potentials φ(a → b) ∈ R|R| are scores for ya→b being on/off.",4.2 Model description,[0],[0]
"Without any other factors, this would amount to independent classifiers for each task.
",4.2 Model description,[0],[0]
Compatibility factors.,4.2 Model description,[0],[0]
"For every possible link a → b, the variables (a, b, a → b) are bound by a dense factor scoring their joint assignment (the black boxes in Figure 2).",4.2 Model description,[0],[0]
"Such a factor could automatically learn to encourage links from compatible types (e.g., from TESTIMONY to POLICY) or discourage links between less compatible ones (e.g., from FACT to TESTIMONY).",4.2 Model description,[0],[0]
"In the simplest form, this factor would be parametrized as a tensor T ∈ R|P|×|P|×|R|, with tijk retaining the score of a source proposition of type i to be (k = on) or not to be (k = off) in a link with a proposition of type j. For more flexibility, we parametrize this factor with compatibility features depending
only on simple structure: tijk becomes a vector, and the score of configuration (i, j, k) is given by v>abtijk where vab consists of three binary features:
• bias: a constant value of 1, allowing T to learn a base score for a label configuration (i, j, k), as in the simple form above,
• adjacency: when there are no other propositions between the source and the target,
• order: when the source precedes the target.
",4.2 Model description,[0],[0]
Second order factors.,4.2 Model description,[0],[0]
Local argumentation graph structures such as a → b → c might be modeled better together rather than through separate link factors for a → b and b → c.,4.2 Model description,[0],[0]
"As in higher-order structured models for semantic and dependency parsing (Martins et al., 2013; Martins and Almeida, 2014), we implement three types of second order factors: grandparent (a → b → c), sibling (a ← b → c), and co-parent (a → b ← c).",4.2 Model description,[0],[0]
"Not all of these types of factors make sense on all datasets: as sibling structures cannot exist in directed trees, we don’t use sibling factors on UKP.",4.2 Model description,[0],[0]
"On CDCP, by transitivity, every grandparent structure implies a corresponding sibling, so it is sufficient to parametrize siblings.",4.2 Model description,[0],[0]
"This difference between datasets is emphasized in Figure 2, where one example of each type of factor is pictured on the right side of the graphs (orange boxes with curved edges): on CDCP we illustrate a coparent factor (top right) and a sibling factor (bot-
tom right), while on UKP we show a co-parent factor (top right) and a grandparent factor (bottom right).",4.2 Model description,[0],[0]
"We call these factors second order because they involve two link variables, scoring the joint assignment of both links being on.
",4.2 Model description,[0],[0]
Valid link structure.,4.2 Model description,[0],[0]
The global structure of argument links can be further constrained using domain knowledge.,4.2 Model description,[0],[0]
We implement this using constraint factors; these have no parameters and are denoted by empty boxes in Figure 2.,4.2 Model description,[0],[0]
"In general, well-formed arguments should be cycle-free.",4.2 Model description,[0],[0]
"In the UKP dataset, links form a directed forest and can never cross paragraphs.",4.2 Model description,[0],[0]
"This particular constraint can be expressed as a series of tree factors,8 one for each paragraph (the factor connected to all link variables in Figure 2).",4.2 Model description,[0],[0]
"In CDCP, links do not form a tree, but we use logic constraints to enforce transitivity (top left factor in Figure 2) and to prevent symmetry (bottom left); the logic formulas implemented by these factors are described in Table 1.",4.2 Model description,[0],[0]
"Together, the two constraints have the desirable side effect of preventing cycles.
",4.2 Model description,[0],[0]
Strict constraints.,4.2 Model description,[0],[0]
"We may include further domain-specific constraints into the model, to express certain disallowed configurations.",4.2 Model description,[0],[0]
"For instance, proposition types that appear in CDCP data can be ordered by the level of objectivity (Park et al., 2015a), as shown in Table 1.",4.2 Model description,[0],[0]
"In a wellformed argument, we would want to see links from more objective to equally or less objective propositions: it’s fine to provide FACT as reason for VALUE, but not the other way around.",4.2 Model description,[0],[0]
"While the training data sometimes violates this constraint, enforcing it might provide a useful inductive bias.
Inference.",4.2 Model description,[0],[0]
"The argmax in Equation 1 is a MAP over a factor graph with cycles and many overlapping factors, including logic factors.",4.2 Model description,[0],[0]
"While exact inference methods are generally unavailable, our setting is perfectly suited for the Alternating Directions Dual Decomposition (AD3) algorithm: approximate inference on expressive factor graphs with overlapping factors, logic constraints, and generic factors (e.g., directed tree factors) defined through maximization oracles (Martins et al., 2015).",4.2 Model description,[0],[0]
"When AD3 returns an integral solution, it is globally optimal, but when solutions are frac-
8A tree factor regards each bound variable as an edge in a graph and assigns −∞ scores to configurations that are not valid trees.",4.2 Model description,[0],[0]
"For inference, we can use maximum spanning arborescence algorithms such as Chu-Liu/Edmonds.
tional, several options are available.",4.2 Model description,[0],[0]
"At test time, for analysis, we retrieve exact solutions using the branch-and-bound method.",4.2 Model description,[0],[0]
"At training time, however, fractional solutions can be used as-is; this makes better use of each iteration and actually increases the ratio of integral solutions in future iterations, as well as at test time, as proven by Meshi et al. (2016).",4.2 Model description,[0],[0]
"We also find that after around 15 training iterations with fractional solutions, over 99% of inference calls are integral.
Learning.",4.2 Model description,[0],[0]
"We train the models by minimizing the structured hinge loss (Taskar et al., 2004):∑ (x,",4.2 Model description,[0],[0]
"y)∈D max y′∈Y (f(x, y′;w)",4.2 Model description,[0],[0]
+,4.2 Model description,[0],[0]
"ρ(y, y′))− f(x, y;w) (2) where ρ is a configurable misclassification cost.",4.2 Model description,[0],[0]
"The max in Equation 2 is not the same as the one used for prediction, in Equation 1.",4.2 Model description,[0],[0]
"However, when the cost function ρ decomposes over the variables, cost-augmented inference amounts to regular inference after augmenting the potentials accordingly.",4.2 Model description,[0],[0]
"We use a weighted Hamming cost:
ρ(y, ŷ)",4.2 Model description,[0],[0]
":= ∑ v ρ(yv)I[yv = ŷv]
where v is summed over all variables in a document {a} ∪ {a → b}, and ρ(yv) is a misclassification cost.",4.2 Model description,[0],[0]
"We assign uniform costs ρ to 1 for all mistakes except false-negative links, where we use higher cost proportional to the class imbalance in the training split, effectively giving more weight to positive links during training.",4.2 Model description,[0],[0]
"One option for parameterizing the potentials of the unary and higher-order factors is with linear models, using proposition, link, and higher-order features.",4.3 Argument structure SVM,[0],[0]
"This gives birth to a linear structured SVM (Tsochantaridis et al., 2005), which, when using l2 regularization, can be trained efficiently in the dual using the online block-coordinate FrankWolfe algorithm of Lacoste-Julien et al. (2013), as implemented in the pystruct library (Müller and Behnke, 2014).",4.3 Argument structure SVM,[0],[0]
"This algorithm is more convenient than subgradient methods, as it does not require tuning a learning rate parameter.
",4.3 Argument structure SVM,[0],[0]
Features.,4.3 Argument structure SVM,[0],[0]
"For unary proposition and link features, we faithfully follow Stab and Gurevych (2016, Tables 9 and 10): proposition features are
lexical (unigrams and dependency tuples), structural (token statistics and proposition location), indicators (from hand-crafted lexicons), contextual, syntactic (subclauses, depth, tense, modal, and POS), probability, discourse (Lin et al., 2014), and average GloVe embeddings (Pennington et al., 2014).",4.3 Argument structure SVM,[0],[0]
"Link features are lexical (unigrams), syntactic (POS and productions), structural (token statistics, proposition statistics and location features), hand-crafted indicators, discourse triples, PMI, and shared noun counts.
",4.3 Argument structure SVM,[0],[0]
"Our proposed higher-order factors for grandparent, co-parent, and sibling structures require features extracted from a proposition triplet a, b, c. In dependency and semantic parsing, higher-order factors capture relationships between words, so sparse indicator features can be efficiently used.",4.3 Argument structure SVM,[0],[0]
"In our case, since propositions consist of many words, BOW features may be too noisy and too dense; so for simplicity we again take a cue from the link-specific features used by Stab and Gurevych (2016).",4.3 Argument structure SVM,[0],[0]
"Our higher-order factor features are: same sentence indicators (for all 3 and for each pair), proposition order (one for each of the 6 possible orderings), Jaccard similarity (between all 3 and between each pair), presence of any shared nouns (between all 3 and between each pair), and shared noun ratios: nouns shared by all 3 divided by total nouns in each proposition and each pair, and shared nouns between each pair with respect to each proposition.",4.3 Argument structure SVM,[0],[0]
"Up to vocabulary size difference, our total feature dimensionality is approximately 7000 for propositions and 2100 for links.",4.3 Argument structure SVM,[0],[0]
"The number of second order features is 35.
Hyperparameters.",4.3 Argument structure SVM,[0],[0]
"We pick the SVM regularization parameter C ∈ {0.001, 0.003, 0.01, 0.03, 0.1, 0.3} by k-fold cross validation at document level, optimizing for the average between link and proposition F1 scores.",4.3 Argument structure SVM,[0],[0]
Neural network methods have proven effective for natural language problems even with minimalto-no feature engineering.,4.4 Argument structure RNN,[0],[0]
"Inspired by the use of LSTMs (Hochreiter and Schmidhuber, 1997) for MST dependency parsing by Kiperwasser and Goldberg (2016), we parametrize the potentials in our factor graph with an LSTM-based neural network,9 replacing MST inference with the more general AD3 algorithm, and using relaxed solutions for training when inference is inexact.
",4.4 Argument structure RNN,[0],[0]
"We extract embeddings of all words with a corpus frequency > 1, initialized with GloVe word vectors.",4.4 Argument structure RNN,[0],[0]
"We use a deep bidirectional LSTM to encode contextual information, representing a proposition a as the average of the LSTM outputs of its words, henceforth denoted ↔ a.
Proposition potentials.",4.4 Argument structure RNN,[0],[0]
"We apply a multi-layer perceptron (MLP) with rectified linear activations to each proposition, with all layer dimensions equal except the final output layer, which has size |P| and is not passed through any nonlinearities.
",4.4 Argument structure RNN,[0],[0]
Link potentials.,4.4 Argument structure RNN,[0],[0]
"To score a dependency a → b, Kiperwasser and Goldberg (2016) pass the concatenation",4.4 Argument structure RNN,[0],[0]
[ ↔ a; ↔ b ] through an MLP.,4.4 Argument structure RNN,[0],[0]
"After trying this, we found slightly better performance by first passing each proposition through a slot-specific
dense layer ( a := σsrc( ↔ a), b := σtrg( ↔ b) )
followed by a bilinear transformation:
φon(a→ b) := a > Wb+w>srca+w > trgb+ w",4.4 Argument structure RNN,[0],[0]
"(on) 0 .
",4.4 Argument structure RNN,[0],[0]
"Since the bilinear expression returns a scalar, but the link potentials must have a value for both the on and off states, we set the full potential to φ(a → b) := [φon(a → b), w(off)0 ] where w (off) 0 is a learned scalar bias.",4.4 Argument structure RNN,[0],[0]
"We initialize W to the diagonal identity matrix.
",4.4 Argument structure RNN,[0],[0]
"9We use the dynet library (Neubig et al., 2017).
",4.4 Argument structure RNN,[0],[0]
Second order potentials.,4.4 Argument structure RNN,[0],[0]
"Grandparent potentials φ(a → b → c) score two adjacent directed edges, in other words three propositions.",4.4 Argument structure RNN,[0],[0]
We again first pass each proposition representation through a slot-specific dense layer.,4.4 Argument structure RNN,[0],[0]
"We implement a multilinear scorer analogously to the link potentials:
φ(a→ b→ c) := ∑ i,j,k aibjckwijk
where W = (w)ijk is a third-order cube tensor.",4.4 Argument structure RNN,[0],[0]
"To reduce the large numbers of parameters, we implicitly represent W as a rank r tensor: wijk = ∑r s=1",4.4 Argument structure RNN,[0],[0]
u (1) is u (2),4.4 Argument structure RNN,[0],[0]
js u,4.4 Argument structure RNN,[0],[0]
(3) ks .,4.4 Argument structure RNN,[0],[0]
"Notably, this model captures only third-order interactions between the representation of the three propositions.",4.4 Argument structure RNN,[0],[0]
"To capture first-order “bias” terms, we could include slotspecific linear terms, e.g., w>a a; but to further capture quadratic backoff effects (for instance, if two propositions carry a strong signal of being siblings regardless of their parent), we would require quadratically many parameters.",4.4 Argument structure RNN,[0],[0]
"Instead of explicit lower-order terms, we propose augmenting a, b, and c with a constant feature of 1, which has approximately the same effect, while benefiting from the parameter sharing in the low-rank factorization; an effect described by Blondel et al. (2016).",4.4 Argument structure RNN,[0],[0]
"Siblings and co-parents factors are similarly parametrized with their own tensors.
",4.4 Argument structure RNN,[0],[0]
Hyperparameters.,4.4 Argument structure RNN,[0],[0]
"We perform grid search using k-fold document-level cross-validation, tuning the dropout probability in the dense MLP layers over {0.05, 0.1, 0.15, 0.2, 0.25} and the optimal number of passes over the training data over {10, 25, 50, 75, 100}.",4.4 Argument structure RNN,[0],[0]
"We use 2 layers for the LSTM and the proposition classifier, 128 hidden units in all layers, and a multilinear decomposition with rank r = 16, after preliminary CV runs.",4.4 Argument structure RNN,[0],[0]
We compare our proposed models to equivalent independent unary classifiers.,4.5 Baseline models,[0],[0]
"The unary-only version of a structured SVM is an l2-regularized linear SVM.10 For the RNN, we compute unary potentials in the same way as in the structured model, but apply independent hinge losses at each variable, instead of the global structured hinge loss.",4.5 Baseline models,[0],[0]
"Since the RNN weights are shared, this is a form of multi-task learning.",4.5 Baseline models,[0],[0]
"The baseline predictions can
10We train our SVM using SAGA (Defazio et al., 2014) in lightning (Blondel and Pedregosa, 2016).
be interpreted as unary potentials, therefore we can simply round their output to the highest scoring labels, or we can, alternatively, perform testtime inference, imposing the desired structure.",4.5 Baseline models,[0],[0]
We evaluate our proposed models on both datasets.,5 Results,[0],[0]
"For model selection and development we used kfold cross-validation at document level: on CDCP we set k = 3 to avoid small validation folds, while on UKP we follow Stab and Gurevych (2016) setting k = 5.",5 Results,[0],[0]
We compare our proposed structured learning systems (the linear structured SVM and the structured RNN) to the corresponding baseline versions.,5 Results,[0],[0]
"We organize our experiments in three incremental variants of our factor graph: basic, full, and strict, each with the following components:11
component basic full strict (baseline)
unaries X X X X compat.",5 Results,[0],[0]
factors X X X compat.,5 Results,[0],[0]
"features X X higher-order X X link structure X X X strict constraints X X
Following Stab and Gurevych (2016), we compute F1 scores at proposition and link level, and also report their average as a summary of overall performance.12 The results of a single prediction run on the test set are displayed in Table 2.",5 Results,[0],[0]
"The overall trend is that training using a structured objective is better than the baseline models, even when structured inference is applied on the baseline predictions.",5 Results,[0],[0]
"On UKP, for link prediction, the linear baseline can reach good performance when using inference, similar to the approach of Stab and Gurevych (2016), but the improvement in proposition prediction leads to higher overall F1 for the structured models.",5 Results,[0],[0]
"Meanwhile, on the more difficult CDCP setting, performing inference on the baseline output is not competitive.",5 Results,[0],[0]
"While feature engineering still outperforms our RNN model, we find that RNNs shine on proposition classification, especially on UKP, and that structured training can make them competitive, reducing their observed lag on link prediction (Katiyar and Cardie, 2016), possibly through mitigating class imbalance.
11Components are described in Section 4.",5 Results,[0],[0]
"The baselines with inference support only unaries and factors with no parameters, as indicated in the last column.
",5 Results,[0],[0]
"12For link F1 scores, however, we find it more intuitive to only consider retrieval of positive links rather than macroaveraged two-class scores.",5 Results,[0],[0]
Contribution of compatibility features.,5.1 Discussion and analysis,[0],[0]
The compatibility factor in our model can be visualized as conditional odds ratios given the source and target proposition types.,5.1 Discussion and analysis,[0],[0]
"Since there are only four possible configurations of the compatibility features, we can plot all cases in Figure 3, alongside the basic model.",5.1 Discussion and analysis,[0],[0]
"Not using compatibility features, the basic model can only learn whether certain configurations are more likely than others (e.g. a REFERENCE supporting another REFERENCE is unlikely, while a REFERENCE supporting a FACT is more likely; essentially a soft version of our domain-specific strict constraints.",5.1 Discussion and analysis,[0],[0]
"The full model with compatibility features is finer grained, capturing, for example, that links from REFERENCE to FACT are more likely when the reference comes after, or that links from VALUE to POLICY are extremely likely only when the two are adjacent.
",5.1 Discussion and analysis,[0],[0]
Proposition errors.,5.1 Discussion and analysis,[0],[0]
The confusion matrices in Figure 4 reveal that the most common confusion is misclassifying FACT as VALUE.,5.1 Discussion and analysis,[0],[0]
The strongest difference between the various models tested is that the RNN-based models make this error less often.,5.1 Discussion and analysis,[0],[0]
"For instance, in the proposition:
And the single most frequently used excuse of any debtor is “I didn’t receive the letter/invoice/statement”
the pronouns in the nested quote may be mistaken for subjectivity, leading to the structured SVMs
predictions of VALUE or TESTIMONY, while the basic structured RNN correctly classifies it as FACT.
",5.1 Discussion and analysis,[0],[0]
Link errors.,5.1 Discussion and analysis,[0],[0]
"While structured inference certainly helps baselines by preventing invalid structures such as cycles, it still depends on local decisions, losing to fully structured training in cases where joint proposition and link decisions are needed.",5.1 Discussion and analysis,[0],[0]
"For instance, in the following conclusion of an UKP essay, the annotators found no links:
In short, [ the individual should finance his or her education ]a because [ it is a personal choice.",5.1 Discussion and analysis,[0],[0]
"]b Otherwise, [ it would cause too much cost from taxpayers and the government.",5.1 Discussion and analysis,[0],[0]
"]c
Indeed, no reasons are provided, but baseline are misled by the connectives: the SVM baseline outputs that b and c are PREMISEs supporting the CLAIM a.",5.1 Discussion and analysis,[0],[0]
"The full structured SVM combines the two tasks and correctly recognizes the link structure.
",5.1 Discussion and analysis,[0],[0]
"Linear SVMs are still a very good baseline, but they tend to overgenerate links due to class imbalance, even if we use class weights during training.",5.1 Discussion and analysis,[0],[0]
"Surprisingly, RNNs are at the opposite end, being extremely conservative, and getting the highest precision among the models.",5.1 Discussion and analysis,[0],[0]
"On CDCP, where the number of true links is 272, the linear baseline with strict inference predicts 796 links with a precision of only 16%, while the strict structured RNN only predicts 52 links, with 33% precision; the example in Figure 5 illustrates this.",5.1 Discussion and analysis,[0],[0]
"In terms of higher-order structures, we find that using higherorder factors increases precision, at a cost in recall.
",5.1 Discussion and analysis,[0],[0]
"This is most beneficial for the 856 co-parent structures in the UKP test set: the full structured SVM has 53% F1, while the basic structured SVM and the basic baseline get 47% and 45% respectively.",5.1 Discussion and analysis,[0],[0]
"On CDCP, while higher-order factors help, performance on siblings and co-parents is below 10% F1 score.",5.1 Discussion and analysis,[0],[0]
This is likely due to link sparsity and suggests plenty of room for further development.,5.1 Discussion and analysis,[0],[0]
"We introduce an argumentation parsing model based on AD3 relaxed inference in expressive factor graphs, experimenting with both linear struc-
tured SVMs and structured RNNs, parametrized with higher-order factors and link structure constraints.",6 Conclusions and future work,[0],[0]
We demonstrate our model on a new argumentation mining dataset with more permissive argument structure annotation.,6 Conclusions and future work,[0],[0]
"Our model also achieves state-of-the-art link prediction performance on the UKP essays dataset.
",6 Conclusions and future work,[0],[0]
Future work.,6 Conclusions and future work,[0],[0]
"Stab and Gurevych (2016) found polynomial kernels useful for modeling feature interactions, but kernel structured SVMs scale poorly, we intend to investigate alternate ways to capture feature interactions.",6 Conclusions and future work,[0],[0]
"While we focus on monological argumentation, our model could be extended to dialogs, for which argumentation theory thoroughly motivates non-tree structures (Afantenos and Asher, 2014).",6 Conclusions and future work,[0],[0]
We are grateful to André,Acknowledgements,[0],[0]
"Martins, Andreas Müller, Arzoo Katyiar, Chenhao Tan, Felix Wu, Jack Hessel, Justine Zhang, Mathieu Blondel, Tianze Shi, Tobias Schnabel, and the rest of the Cornell NLP seminar for extremely helpful discussions.",Acknowledgements,[0],[0]
We thank the anonymous reviewers for their thorough and well-argued feedback.,Acknowledgements,[0],[0]
"We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure.",abstractText,[0],[0]
(This is the case in over 20% of the web comments dataset we release.),abstractText,[0],[0]
Our model jointly learns elementary unit type classification and argumentative relation prediction.,abstractText,[0],[0]
"Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions.",abstractText,[0],[0]
Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.,abstractText,[0],[0]
Argument Mining with Structured SVMs and RNNs,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 217–226, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
Online forums are now one of the primary venues for public dialogue on current social and political issues.,1 Introduction,[0],[0]
"The related corpora are often huge, covering any topic imaginable, thus providing novel opportunities to address a number of open questions about the structure of dialogue.",1 Introduction,[0],[0]
Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic.,1 Introduction,[0],[0]
"We build a new dataset of 109,074 posts on the topics gay marriage, gun control, death penalty and evolution.",1 Introduction,[0],[0]
"We frame our problem as consisting of two separate tasks:
• Argument Extraction: How can we extract argument segments in dialogue that clearly express a particular argument facet?
",1 Introduction,[0],[0]
• Argument,1 Introduction,[0],[0]
"Facet Similarity: How can we recognize that two argument segments are semantically similar, i.e. about the same facet of the argument?
",1 Introduction,[0],[0]
Consider for example the sample posts and responses in Fig. 1.,1 Introduction,[0],[0]
"Argument segments that are good targets for argument extraction are indicated, in their dialogic context, in bold.",1 Introduction,[0],[0]
"Given extracted segments, the argument facet similarity module should recognize that R3 and R4 paraphrase the same argument facet, namely that there is a strong relationship between the availability of guns and the murder rate.",1 Introduction,[0],[0]
"This paper addresses only the argument extraction task, as an important first step towards producing argument summaries that reflect the range and type of arguments being made,
217
on a topic, over time, by citizens in public forums.",1 Introduction,[0],[0]
"Our approach to the argument extraction task is driven by a novel hypothesis, the IMPLICIT MARKUP hypothesis.",1 Introduction,[0],[0]
"We posit that the arguments that are good candidates for extraction will be marked by cues (implicit markups) provided by the dialog conversants themselves, i.e. their choices about the surface realization of their arguments.",1 Introduction,[0],[0]
"We examine a number of theoretically motivated cues for extraction, that we expect to be domain-independent.",1 Introduction,[0],[0]
"We describe how we use these cues to sample from the corpus in a way that lets us test the impact of the hypothesized cues.
",1 Introduction,[0],[0]
Both the argument extraction and facet similarity tasks have strong similarities to other work in natural language processing.,1 Introduction,[0],[0]
Argument extraction resembles the sentence extraction phase of multi-document summarization.,1 Introduction,[0],[0]
"Facet similarity resembles semantic textual similarity and paraphrase recognition (Misra et al., 2015; Boltuzic and Šnajder, 2014; Conrad et al., 2012; Han et al., 2013; Agirre et al., 2012).",1 Introduction,[0],[0]
"Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences (Barzilay, 2003; Gurevych and Strube, 2004; Misra et al., 2015).
",1 Introduction,[0],[0]
"Sec. 2 describes our corpus of arguments, and describes the hypothesized markers of highquality argument segments.",1 Introduction,[0],[0]
"We sample from the corpus using these markers, and then annotate the extracted argument segments for ARGUMENT QUALITY.",1 Introduction,[0],[0]
Sec. 3.2 describes experiments to test whether: (1) we can predict argument quality; (2) our hypothesized cues are good indicators of argument quality; and (3) an argument quality predictor trained on one topic or a set of topics can be used on unseen topics.,1 Introduction,[0],[0]
The results in Sec. 4 show that we can predict argument quality with RRSE values as low as .73 for some topics.,1 Introduction,[0],[0]
"Cross-domain training combined with domainadaptation yields RRSE values for several topics as low as .72, when trained on topic independent features, however some topics are much more difficult.",1 Introduction,[0],[0]
We provide a comparison of our work to previous research and sum up in Sec. 5.,1 Introduction,[0],[0]
"We created a large corpus consisting of 109,074 posts on the topics gay marriage (GM, 22425 posts), gun control (GC, 38102 posts), death penalty (DP, 5283 posts) and evolution (EV, 43624), by combining the Internet Argument Corpus (IAC) (Walker et al., 2012), with dialogues from http://www.createdebate.com/.
Our aim is to develop a method that can extract high quality arguments from a large corpus of argumentative dialogues, in a topic and domain-
independent way.",2 Corpus and Method,[0],[0]
It is important to note that arbitrarily selected utterances are unlikely to be high quality arguments.,2 Corpus and Method,[0],[0]
"Consider for example all the utterances in Fig. 1: many utterances are either not interpretable out of context, or fail to clearly frame an argument facet.",2 Corpus and Method,[0],[0]
Our IMPLICIT MARKUP hypothesis posits that arguments that are good candidates for extraction will be marked by cues from the surface realization of the arguments.,2 Corpus and Method,[0],[0]
We first describe different types of cues that we use to sample from the corpus in a way that lets us test their impact.,2 Corpus and Method,[0],[0]
"We then describe the MT HIT, and how we use our initial HIT results to refine our sampling process.",2 Corpus and Method,[0],[0]
"Table 2 presents the results of our sampling and annotation processes, which we will now explain in more detail.",2 Corpus and Method,[0],[0]
"The IMPLICIT MARKUP hypothesis is composed of several different sub-hypotheses as to how speakers in dialogue may mark argumentative structure.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Discourse Relation hypothesis suggests that the Arg1 and Arg2 of explicit SPECIFICATION, CONTRAST, CONCESSION and CONTINGENCY markers are more likely to contain good argumentative segments (Prasad et al., 2008).",2.1 Implicit Markup Hypothesis,[0],[0]
"In the case of explicit connectives, Arg2 is the argument to which the connective is syntactically bound, and Arg1 is the other argument.",2.1 Implicit Markup Hypothesis,[0],[0]
"For example, a CONTINGENCY relation is frequently marked by the lexical anchor If, as in R1 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
"A CONTRAST relation may mark a challenge to an opponent’s claim, what Ghosh et al. call callout-target argument pairs (Ghosh et al., 2014b; Maynard, 1985).",2.1 Implicit Markup Hypothesis,[0],[0]
"The CONTRAST relation is frequently marked by But, as in R3 and R4 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
"A SPECIFICATION relation may indicate a focused detailed argument, as marked by First in R2 in Fig. 1 (Li and Nenkova, 2015).",2.1 Implicit Markup Hypothesis,[0],[0]
"We decided to extract only the Arg2, where the discourse argument is syntactically bound to the connective, since Arg1’s are more difficult to locate, especially in dialogue.",2.1 Implicit Markup Hypothesis,[0],[0]
"We began by extracting the Arg2’s for the connectives most strongly associated with these discourse relations over the whole corpus, and then once we saw what the most frequent connectives were in our corpus, we refined this selection to include only but, if, so, and first.",2.1 Implicit Markup Hypothesis,[0],[0]
"We sampled a roughly even distribution of sentences from each category as well as sentences without any discourse connectives, i.e. None.",2.1 Implicit Markup Hypothesis,[0],[0]
See Table.,2.1 Implicit Markup Hypothesis,[0],[0]
"2.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Syntactic Properties hypothesis posits that syntactic properties of a clause may indicate good argument segments, such as being the main clause (Marcu, 1999), or the sentential complement of mental state or speech-act verbs, e.g. the SBAR
in you agree that SBAR as in P2 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
"Because these markers are not as frequent in our corpus, we do not test this with sampling: rather we test it as a feature as described in Sec. 3.2.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Dialogue Structure hypothesis suggests that position in the post or the relation to a verbatim quote could influence argument quality, e.g. being turn-initial in a response as exemplified by P2, R3 and R4 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
We indicate sampling by position in post with Starts:,2.1 Implicit Markup Hypothesis,[0],[0]
Yes/No in Table.,2.1 Implicit Markup Hypothesis,[0],[0]
2.,2.1 Implicit Markup Hypothesis,[0],[0]
Our corpora are drawn from websites that offer a “quoting affordance” in addition to a direct reply.,2.1 Implicit Markup Hypothesis,[0],[0]
"An example of a post from the IAC corpus utilizing this mechanism is shown in Table 1, where the quoted text is highlighted in blue and the response is directly below it.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Semantic Density hypothesis suggests that measures of rich content or SPECIFICITY will indicate good candidates for argument extraction (Louis and Nenkova, 2011).",2.1 Implicit Markup Hypothesis,[0],[0]
We initially posited that short sentences and sentences without any topic-specific words are less likely to be good.,2.1 Implicit Markup Hypothesis,[0],[0]
"For the topics gun control and gay marriage, we filtered sentences less than 4 words long, which removed about 8-9% of the sentences.",2.1 Implicit Markup Hypothesis,[0],[0]
"After collecting the argument quality annotations for these two topics and examining the distribution of scores (see Sec. 2.2 below), we developed an additional measure of semantic density that weights words in each candidate by its pointwise mutual information (PMI), and applied it to the evolution and death penalty.",2.1 Implicit Markup Hypothesis,[0],[0]
"Using the 26 topic annotations in the IAC, we calculate the PMI between every word in the corpus appearing more than 5 times and each topic.",2.1 Implicit Markup Hypothesis,[0],[0]
We only keep those sentences that have at least one word whose PMI is above our threshold of 0.1.,2.1 Implicit Markup Hypothesis,[0],[0]
"We determined this threshold by examining the values in gun control and gay marriage, such that at least 2/3 of the filtered sentences were in the bottom third of the argument quality score.",2.1 Implicit Markup Hypothesis,[0],[0]
"The PMI filter eliminates 39% of the sentences from death penalty (40% combined with the length filter) and 85% of the sentences from
evolution (87% combined with the length filter).",2.1 Implicit Markup Hypothesis,[0],[0]
Table 2 summarizes the results of our sampling procedure.,2.1 Implicit Markup Hypothesis,[0],[0]
"Overall our experiments are based on 5,374 sampled sentences, with roughly equal numbers over each topic, and equal numbers representing each of our hypotheses and their interactions.",2.1 Implicit Markup Hypothesis,[0],[0]
Table 8 in the Appendix provides example argument segments resulting from the sampling and annotation process.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"Sometimes arguments are completely self contained, e.g. S1 to S8 in Table 8.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"In other cases, e.g. S9 to S16 we can guess what the argument is based on using world knowledge of the domain, but it is not explicitly stated or requires several steps of inference.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"For example, we might be able to infer the argument in S14 in Table 8, and the context in which it arose, even though it is not explicitly stated.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"Finally, there are cases where the user is not making an argument or the argument cannot be reconstructed without significantly more context, e.g. S21 in Table 8.
","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We collect annotations for ARGUMENT QUALITY for all the sentences summarized in Table 2 on Amazon’s Mechanical Turk (AMT) platform.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
Figure 3 in the Appendix illustrates the basic layout of the HIT.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
Each HIT consisted of 20 sentences on one topic which is indicated on the page.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The annotator first checked a box if the sentence expressed an argument, and then rated the argument quality using a continuous slider ranging from hard (0.0) to easy to interpret (1.0).
","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We collected 7 annotations per sentence.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"All Turkers were required to pass our qualifier, have a HIT approval rating above 95%, and be located in the United States, Canada, Australia, or Great Britain.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The results of the sampling and annotation on the final annotated corpus are in Table 2.
","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"We measured the inter-annotator agreement (IAA) of the binary annotations using Krippendorff’s α (Krippendorff, 2013) and the continuous values using the intraclass correlation coefficient (ICC) for each topic.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We found that annotators could not distinguish between phrases that did not express an argument and hard sentences.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
See examples and definitions in Fig. 3.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"We therefore mapped unchecked sentences (i.e., non arguments) to zero argument quality.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"We then calculated the average pairwise ICC value for each rater between all Turkers with overlapping annotations, and removed the judgements of any Turker that did not have a positive ICC value.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
The ICC for each topic is shown in Table 2.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The mean rating across the remaining annotators for each sentence was used as the gold standard for argument quality, with means in the Argument Quality (AQ) column of Table 2.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The effect of the sampling on
argument quality can be seen in Table 2.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The differences between gun control and gay marriage, and the other two topics is due to effective use of the semantic density filter, which shifted the distribution of the annotated data towards higher quality arguments as we intended.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We can now briefly validate some of the IMPLICIT MARKUP hypothesis using an ANOVA testing the effect of a connective and its position in post on argument quality.,3.1 Implicit Markup Hypothesis Validation,[0],[0]
"Across all sentences in all topics, the presence of a connective is significant (p = 0.00).",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"Three connectives, if, but, and so, show significant differences in AQ from no-connective phrases (p = 0.00, 0.02, 0.00, respectively).",3.1 Implicit Markup Hypothesis Validation,[0],[0]
First does not show a significant effect.,3.1 Implicit Markup Hypothesis Validation,[0],[0]
"The mean AQ scores for sentences marked by if, but, and so differ from that of a no-connective sentence by 0.11, 0.04, and 0.04, respectively.",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"These numbers support our hypothesis that there are certain discourse connectives or cue words which can help to signal the existence of arguments, and they seem to suggest that the CONTINGENCY category may be most useful, but more research using more cue words is necessary to validate this suggestion.
",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"In addition to the presence of a connective, the dialogue structural position of being an initial sentence in a response post did not predict argument quality as we expected.",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"Response-initial sentences provide significantly lower quality arguments (p = 0.00), with response-initial sentences having an average AQ score 0.03 lower (0.40 vs. 0.43).",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"We use 3 regression algorithms from the Java Statistical Analysis Toolkit1: Linear Least Squared Error (LLS), Ordinary Kriging (OK) and Support Vector Machines using a radial basis function kernel (SVM).",3.2 Argument Quality Regression,[0],[0]
A random 75% of the sentences of each domain were put into training/development and 25% into the held out test.,3.2 Argument Quality Regression,[0],[0]
Training involved a grid search over the hyper-parameters of each model2 and a subset (23-29 and the complete set) of the top N features whose values correlate best with the argument quality dependent variable (using Pearson’s).,3.2 Argument Quality Regression,[0],[0]
"The combined set of parameters and features that achieved the best mean squared error over a 5-fold cross validation on the training data was used to train the complete model.
",3.2 Argument Quality Regression,[0],[0]
"We also compare hand-curated feature sets that are motivated by our hypotheses to this simple
1https://github.com/EdwardRaff/JSAT 2We used the default parameters for LLS and OK and only
searched hyper-parameters for the SVM model.
feature selection method, and the performance of in-domain, cross-domain, and domain-adaptation training using “the frustratingly easy” approach (Daumé III, 2007).
",3.2 Argument Quality Regression,[0],[0]
We use our training and development data to develop a set of feature templates.,3.2 Argument Quality Regression,[0],[0]
"The features are real-valued and normalized between 0 and 1, based on the min and max values in the training data for each domain.",3.2 Argument Quality Regression,[0],[0]
If not stated otherwise the presence of a feature was represented by 1.0 and its absence by 0.0.,3.2 Argument Quality Regression,[0],[0]
We describe all the handcurated feature sets below.,3.2 Argument Quality Regression,[0],[0]
Semantic Density Features: Deictic Pronouns (DEI): The presence of anaphoric references are likely to inhibit the interpretation of an utterance.,3.2 Argument Quality Regression,[0],[0]
"These features count the deictic pronouns in the sentence, such as this, that and it.
",3.2 Argument Quality Regression,[0],[0]
"Sentence Length (SLEN): Short sentences, particularly those under 5 words, are usually hard to interpret without context and complex linguistic processing, such as resolving long distance discourse anaphora.",3.2 Argument Quality Regression,[0],[0]
"We thus include a single aggregate feature whose value is the number of words.
",3.2 Argument Quality Regression,[0],[0]
Word Length (WLEN): Sentences that clearly articulate an argument should generally contain words with a high information content.,3.2 Argument Quality Regression,[0],[0]
"Several studies show that word length is a surprisingly good indicator that outperforms more complex measures, such as rarity (Piantadosi et al., 2011).",3.2 Argument Quality Regression,[0],[0]
"Thus we include features based on word length, including the min, max, mean and median.",3.2 Argument Quality Regression,[0],[0]
"We also create a feature whose value is the count of words of lengths 1 to 20 (or longer).
",3.2 Argument Quality Regression,[0],[0]
"Speciteller (SPTL): We add a single aggregate feature from the result of Speciteller, a tool that assesses the specificity of a sentence in the range of 0 (least specific) to 1 (most specific) (Li and Nenkova, 2015; Louis and Nenkova, 2011).",3.2 Argument Quality Regression,[0],[0]
"High specificity should correlate with argument quality.
",3.2 Argument Quality Regression,[0],[0]
Kullback-Leibler Divergence (KLDiv): We expect that sentences on one topic domain will have different content than sentences outside the domain.,3.2 Argument Quality Regression,[0],[0]
"We built two trigram language models using the Berkeley LM toolkit (Pauls and Klein, 2011).",3.2 Argument Quality Regression,[0],[0]
"One (P) built from all the sentences in the IAC within the domain, excluding all sentences from the annotated dataset, and one (Q) built from all sentences in IAC outside the domain.",3.2 Argument Quality Regression,[0],[0]
"The KL Divergence is then computed using the discrete n-gram probabilities in the sentence from each model as in equation (1).
",3.2 Argument Quality Regression,[0],[0]
DKL(P ||Q),3.2 Argument Quality Regression,[0],[0]
"= ∑
i
P (i) ln P (i) Q(i)
(1)
",3.2 Argument Quality Regression,[0],[0]
"Lexical N-Grams (LNG): N-Grams are a standard feature that are often a difficult baseline to
beat.",3.2 Argument Quality Regression,[0],[0]
However they are not domain independent.,3.2 Argument Quality Regression,[0],[0]
We created a feature for every unigram and bigram in the sentence.,3.2 Argument Quality Regression,[0],[0]
The feature value was the inverse document frequency of that n-gram over all posts in the entire combined IAC plus CreateDebate corpus.,3.2 Argument Quality Regression,[0],[0]
Any n-gram seen less than 5 times was not included.,3.2 Argument Quality Regression,[0],[0]
"In addition to the specific lexical features a set of aggregate features were also generated that only considered summary statistics of the lexical feature values, for example the min, max and mean IDF values in the sentence.",3.2 Argument Quality Regression,[0],[0]
"Discourse and Dialogue Features: We expect our features related to the discourse and dialogue hypotheses to be domain independent.
",3.2 Argument Quality Regression,[0],[0]
Discourse (DIS): We developed features based on discourse connectives found in the Penn Discourse Treebank as well as a set of additional connectives in our corpus that are related to dialogic discourse and not represented in the PDTB.,3.2 Argument Quality Regression,[0],[0]
We first determine if a discourse connective is present in the sentence.,3.2 Argument Quality Regression,[0],[0]
"If not, we create a NO CONNECTIVE feature with a value of 1.",3.2 Argument Quality Regression,[0],[0]
"Otherwise, we identify all connectives that are present.",3.2 Argument Quality Regression,[0],[0]
"For each of them, we derive a set of specific lexical features and a set of generic aggregate features.
",3.2 Argument Quality Regression,[0],[0]
The specific features make use of the lexical (String) and PDTB categories (Category) of the found connectives.,3.2 Argument Quality Regression,[0],[0]
We start by identifying the connective and whether it started the sentence or not (Location).,3.2 Argument Quality Regression,[0],[0]
"We then identify the connective’s most likely PDTB category based on the frequencies stated in the PDTB manual and all of its parent categories, for example but → CONTRAST → COMPARISON.",3.2 Argument Quality Regression,[0],[0]
The aggregate features only consider how many discourse connectives and if any of them started the sentence.,3.2 Argument Quality Regression,[0],[0]
"The templates are:
Specific:{Location}:{String} Specific:{Location}:{Category} Aggregate:{Location}:{Count}
",3.2 Argument Quality Regression,[0],[0]
"For example, the first sentence in Table 8 would generate the following features:
Specific:Starts:but Specific:Starts:Contrast
Specific:Starts:COMPARISON Aggregate:Starts:1 Aggregate:Any:1
Because our hypothesis about dialogue structure was disconfirmed by the results described in section 3.1, we did not develop a feature to independently test position in post.",3.2 Argument Quality Regression,[0],[0]
Rather the Discourse features only encode whether the discourse cue starts the post or not.,3.2 Argument Quality Regression,[0],[0]
"Syntactic Property Features: We also expect syntactic property features to generalize across domains.
",3.2 Argument Quality Regression,[0],[0]
Part-Of-Speech N-Grams (PNG): Lexical features require large amounts of training data and are likely to be topic-dependent.,3.2 Argument Quality Regression,[0],[0]
Part-of-speech tags are less sparse and and less likely to be topicspecific.,3.2 Argument Quality Regression,[0],[0]
"We created a feature for every unigram, bigram and trigram POS tag sequence in the sentence.",3.2 Argument Quality Regression,[0],[0]
"Each feature’s value was the relative frequency of the n-gram in the sentence.
",3.2 Argument Quality Regression,[0],[0]
Syntactic (SYN):,3.2 Argument Quality Regression,[0],[0]
"Certain syntactic structures may be used more frequently for expressing argumentative content, such as complex sentences with verbs that take clausal complements.",3.2 Argument Quality Regression,[0],[0]
"In CreateDebate, we found a number of phrases of the form",3.2 Argument Quality Regression,[0],[0]
"I <VERB> that <X>, such as I agree that, you said that, except that",3.2 Argument Quality Regression,[0],[0]
and I disagree because.,3.2 Argument Quality Regression,[0],[0]
"Thus we included two types of syntactic features: one for every internal node, excluding POS tags, of the parse tree (NODE) and another for each context free production rule (RULE) in the parse tree.",3.2 Argument Quality Regression,[0],[0]
"The feature value is the relative frequency of the node or rule within the sentence.
",3.2 Argument Quality Regression,[0],[0]
Meta Features: The 3 meta feature sets are: (1) all features except lexical n-grams (!LNG); (2) all features that use specific lexical or categorical information (SPFC); and (3) aggregate statistics (AGG) obtained from our feature extraction process.,3.2 Argument Quality Regression,[0],[0]
"The AGG set included features, such as sentence and word length, and summary statistics about the IDF values of lexical n-grams, but did not actually reference any lexical properties in the
feature name.",3.2 Argument Quality Regression,[0],[0]
We expect both !,3.2 Argument Quality Regression,[0],[0]
LNG and AGG to generalize across domains.,3.2 Argument Quality Regression,[0],[0]
"Sec. 4.1 presents the results of feature selection, which finds a large number of general features.",4 Results,[0],[0]
The results for argument quality prediction are in Secs.,4 Results,[0],[0]
4.2 and 4.3.,4 Results,[0],[0]
"Our standard training procedure (SEL) incorporates all the feature templates described in Sec. 3.2, which generates a total of 23,345 features.",4.1 Feature Selection,[0],[0]
It then performs a grid search over the model hyper-parameters and a subset of all the features using the simple feature selection technique described in section 3.2.,4.1 Feature Selection,[0],[0]
Table 3 shows the 10 features most correlated with the annotated quality value in the training data for the topics gun control and gay marriage.,4.1 Feature Selection,[0],[0]
"A few domain specific lexical items appear, but in general the top features tend to be non-lexical and relatively domain independent, such as part-of-speech tags and sentence specificity, as measured by Speciteller (Li and Nenkova, 2015; Louis and Nenkova, 2011).
",4.1 Feature Selection,[0],[0]
"Sentence length has the highest correlation with the target value in both topics, as does the node:root feature, inversely correlated with length.",4.1 Feature Selection,[0],[0]
"Therefore, in order to shift the quality distribution of the sample that we put out on MTurk for the death penalty or evolution topics, we applied a filter that removed all sentences shorter than 4 words.",4.1 Feature Selection,[0],[0]
"For these topics, domain specific features such as lexical n-grams are better predictors of argument quality.",4.1 Feature Selection,[0],[0]
"As discussed above, the PMI filter that was applied only to these two topics during sampling removed some shorter low quality sentences, which probably altered the predictive value of this feature in these domains.",4.1 Feature Selection,[0],[0]
"We first tested the performance of 3 regression algorithms using the training and testing data within each topic using 3 standard evaluation measures: R2, Root Mean Squared Error (RMSE) and Root
Relative Squared Error (RRSE).",4.2 In-Domain Training,[0],[0]
R2 estimates the amount of variability in the data that is explained by the model.,4.2 In-Domain Training,[0],[0]
Higher values indicate a better fit to the data.,4.2 In-Domain Training,[0],[0]
"The RMSE measures the average squared difference between predicted values and true values, which penalizes wrong answers more as the difference increases.",4.2 In-Domain Training,[0],[0]
"The RRSE is similar to RMSE, but is normalized by the squared error of a simple predictor that always guesses the mean target value in the test set.",4.2 In-Domain Training,[0],[0]
"Anything below a 1.0 indicates an improvement over the baseline.
",4.2 In-Domain Training,[0],[0]
"Table 4 shows that SVMs and OK perform the best, with better than baseline results for all topics.",4.2 In-Domain Training,[0],[0]
Performance for gun control and gay marriage are significantly better.,4.2 In-Domain Training,[0],[0]
See Fig. 2.,4.2 In-Domain Training,[0],[0]
"Since SVM was nearly always the best model, we only report SVM results in what follows.
",4.2 In-Domain Training,[0],[0]
We also test the impact of our theoretically motivated features and domain specific features.,4.2 In-Domain Training,[0],[0]
The top half of Table 5 shows the RRSE for each feature set with darker cells indicating better performance.,4.2 In-Domain Training,[0],[0]
The feature acronyms are described in Sec 3.2.,4.2 In-Domain Training,[0],[0]
"When training and testing on the same domain, using lexical features leads to the best performance for all topics (SEL, LEX, LNG and SPFC).",4.2 In-Domain Training,[0],[0]
"However, we can obtain good performance on all of the topics without using any lexical information at all (!LNG, WLEN, PNG, and AGG), sometimes close to our best results.",4.2 In-Domain Training,[0],[0]
"Despite the high correlation to the target value, sentence specificity as a single feature does not outperform any other feature sets.",4.2 In-Domain Training,[0],[0]
"In general, we do better for gun control and gay marriage than for death penalty and evolution.",4.2 In-Domain Training,[0],[0]
"Since the length and domain specific words are important features in the trained models, it seems likely that the filtering process made it harder to learn a good function.
",4.2 In-Domain Training,[0],[0]
"The bottom half of Table 5 shows the results using training data from all other topics, when testing on one topic.",4.2 In-Domain Training,[0],[0]
"The best results for GC are significantly better for several feature sets (SEL,
LEX, LNG), In general the performance remains similar to the in-domain training, with some minor improvements over the best performing models.",4.2 In-Domain Training,[0],[0]
"These results suggest that having more data outweighs any negative consequences of domain specific properties.
",4.2 In-Domain Training,[0],[0]
"●
●
●
● ●
● ● ● ●
●
0.8
0.9
1.0
250 500 750 1000 1250
Number of Training Instances
R oo
t R el
at iv
e S
qu ar
ed E
rr or
Domain ● Gun Control
Gay Marriage Evolution Death Penalty
Figure 2:",4.2 In-Domain Training,[0],[0]
"Learning curves for each of the 4 topics with 95% confidence intervals.
",4.2 In-Domain Training,[0],[0]
We also examine the effect of training set size on performance given the best performing feature sets.,4.2 In-Domain Training,[0],[0]
See Fig. 2.,4.2 In-Domain Training,[0],[0]
"We randomly divided our entire dataset into an 80/20 training/testing split and trained incrementally larger models from the 80% using the default training procedure, which were then applied to the 20% testing data.",4.2 In-Domain Training,[0],[0]
"The plotted points are the mean value of repeating this process 10 times, with the shaded region showing the 95% confidence interval.",4.2 In-Domain Training,[0],[0]
"Although most gains are achieved within 500-750 training examples, all models are still trending downward, suggesting that more training data would be useful.
",4.2 In-Domain Training,[0],[0]
"Finally, our results are actually even better than they appear.",4.2 In-Domain Training,[0],[0]
"Our primary application requires extracting arguments at the high end of the scale (e.g., those above 0.8 or 0.9), but the bulk of our data is closer to the middle of the scale, so our regressors are conservative in assigning high or low
values.",4.2 In-Domain Training,[0],[0]
To demonstrate this point we split the predicted values for each topic into 5 quantiles.,4.2 In-Domain Training,[0],[0]
The RMSE for each of the quantiles and domains in Table 6 demonstrates that the lowest RMSE is obtained in the top quantile.,4.2 In-Domain Training,[0],[0]
To investigate whether learned models generalize across domains we also evaluate the performance of training with data from one domain and testing on another.,4.3 Cross-Domain and Domain Adaptation,[0],[0]
The columns labeled CD in Table 7 summarize these results.,4.3 Cross-Domain and Domain Adaptation,[0],[0]
"Although cross domain training does not perform as well as in-domain training, we are able to achieve much better than baseline results between gun control and gay marriage for many of the feature sets and some other minor transferability for the other domains.",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"Although lexical features (e.g., lexical n-grams) perform best in-domain, the best performing features across domains are all non-lexical, i.e. !",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"LNG, PNG and AGG.
",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"We then applied Daume’s “frustratingly easy domain adaptation” technique (DA), by transforming the original features into a new augmented feature space where, each feature, is transformed into a general feature and a domain specific feature, source or target, depending on the input domain (Daumé III, 2007).",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"The training data from both the source and target domains are used to train
the model, unlike the cross-domain experiments where only the source data is used.",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"These results are given in the columns labeled DA in Table 7, which are on par with the best in-domain training results, with minor performance degradation on some gay marriage and gun control pairs, and slight improvements on the difficult death penalty and evolution topics.",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"This paper addresses the Argument Extraction task in a framework whose long-term aim is to first extract arguments from online dialogues, and then use them to produce a summary of the different facets of an issue.",5 Discussion and Conclusions,[0],[0]
We have shown that we can find sentences that express clear arguments with RRSE values of .72 for gay marriage and gun control (Table 6) and .93 for death penalty and evolution (Table 8 cross domain with adaptation).,5 Discussion and Conclusions,[0],[0]
"These results show that sometimes the best quality predictors can be trained in a domain-independent way.
",5 Discussion and Conclusions,[0],[0]
"The two step method that we propose is different than much of the other work on argument mining, either for more formal texts or for social media, primarily because the bulk of previous work takes a supervised approach on a labelled topicspecific dataset (Conrad et al., 2012; Boltuzic and Šnajder, 2014; Ghosh et al., 2014b).",5 Discussion and Conclusions,[0],[0]
Conrad & Wiebe developed a data set for supervised training of an argument mining system on weblogs and news about universal healthcare.,5 Discussion and Conclusions,[0],[0]
They separate the task into two components: one component identifies ARGUING SEGMENTS and the second component labels the segments with the relevant ARGUMENT TAGS.,5 Discussion and Conclusions,[0],[0]
Our argument extraction phase has the same goals as their first component.,5 Discussion and Conclusions,[0],[0]
"Boltuzic & Snajder also apply a supervised learning approach, producing arguments labelled with a concept similar to what we call FACETS.",5 Discussion and Conclusions,[0],[0]
"However they perform what we call argument extraction by hand, eliminating comments from com-
ment streams that they call “spam” (Boltuzic and Šnajder, 2014).",5 Discussion and Conclusions,[0],[0]
"Ghosh et al. also take a supervised approach, developing techniques for argument mining on online forums about technical topics and applying a theory of argument structure that is based on identifying TARGETS and CALLOUTS, where the callout attacks a target proposition in another speaker’s utterance (Ghosh et al., 2014b).",5 Discussion and Conclusions,[0],[0]
"However, their work does not attempt to discover high quality callouts and targets that can be understood out of context like we do.",5 Discussion and Conclusions,[0],[0]
"More recent work also attempts to do some aspects of argument mining in an unsupervised way (Boltuzic and Šnajder, 2015; Sobhani et al., 2015).",5 Discussion and Conclusions,[0],[0]
"However (Boltuzic and Šnajder, 2015) focus on the argument facet similarity task, using as input a corpus where the arguments have already been extracted.",5 Discussion and Conclusions,[0],[0]
"(Sobhani et al., 2015) present an architecture where arguments are first topic-labelled in a semi-supervised way, and then used for stance classification, however this approach treats the whole comment as the extracted argument, rather than attempting to pull out specific focused argument segments as we do here.
",5 Discussion and Conclusions,[0],[0]
A potential criticism of our approach is that we have no way to measure the recall of our argument extraction system.,5 Discussion and Conclusions,[0],[0]
However we do not think that this is a serious issue.,5 Discussion and Conclusions,[0],[0]
"Because we are only interested in determining the similarity between phrases that are high quality arguments and thus potential contributors to summaries of a specific facet for a specific topic, we believe that precision is more important than recall at this point in time.",5 Discussion and Conclusions,[0],[0]
"Also, given the redundancy of the arguments presented over thousands of posts on an issue it seems unlikely we would miss an important facet.",5 Discussion and Conclusions,[0],[0]
"Finally, a measure of recall applied to the facets of a topic may be irreconcilable with our notion that an argument does not have a limited, enumerable number of facets, and our belief that each facet is subject to judgements of granularity.",5 Discussion and Conclusions,[0],[0]
Fig. 3 shows how the Mechanical Turk hit was defined and the examples that were used in the qualification task.,6 Appendix,[0],[0]
"Table 8 illustrates the argument quality scale annotations collected from Mechanical Turk.
",6 Appendix,[0],[0]
We invite other researchers to improve upon our results.,6 Appendix,[0],[0]
Our corpus and the relevant annotated data is available at http://nldslab.soe.ucsc.edu/ arg-extraction/sigdial2015/.,6 Appendix,[0],[0]
This research is supported by National Science Foundation Grant CISE-IIS-RI #1302668.,7 Acknowledgements,[0],[0]
Online forums are now one of the primary venues for public dialogue on current social and political issues.,abstractText,[0],[0]
"The related corpora are often huge, covering any topic imaginable.",abstractText,[0],[0]
Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic.,abstractText,[0],[0]
We frame this goal as consisting of two tasks: argument extraction and argument facet similarity.,abstractText,[0],[0]
"We focus here on the argument extraction task, and show that we can train regressors to predict the quality of extracted arguments with RRSE values as low as .73 for some topics.",abstractText,[0],[0]
"A secondary goal is to develop regressors that are topic independent: we report results of cross-domain training and domain-adaptation with RRSE values for several topics as low as .72, when trained on topic independent features.",abstractText,[0],[0]
Argument Mining: Extracting Arguments from Online Dialogue,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1558–1572 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
In this work we introduce an unsupervised methodology for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role. By applying this framework to the setting of question sessions in the UK parliament, we show that the resulting typology encodes key aspects of the political discourse—such as the bifurcation in questioning behavior between government and opposition parties—and reveals new insights into the effects of a legislator’s tenure and political career ambitions.",text,[0],[0]
"“We’d now like to open the floor to shorter speeches disguised as questions...”
– Steve Macone, New Yorker cartoon caption
Why do we ask questions?",1 Introduction,[0],[0]
"Perhaps we are seeking factual information that others hold, or maybe we are requesting a favor.",1 Introduction,[0],[0]
"Alternatively we could be simply making a rhetorical point, perhaps at the start of an academic paper.
",1 Introduction,[0],[0]
"Questions play a prominent role in social interactions (Goffman, 1976), performing a multitude of rhetorical functions that go beyond mere factual
information gathering (Kearsley, 1976).",1 Introduction,[0],[0]
"While the informational component of questions has been well-studied in the context of question-answering applications, there is relatively little computational work addressing the rhetorical and social role of these basic dialogic units.
",1 Introduction,[0],[0]
One domain where questions have a particularly salient rhetorical role is politics.,1 Introduction,[0],[0]
"The ability to question the actions and intentions of governments is a crucial part of democracy (Pitkin, 1967), particularly in parliamentary systems.",1 Introduction,[0],[0]
"Consequently, scholars have studied parliamentary questions in detail, in terms of their origins (Chester and Bowring, 1962), their institutionalization (Eggers and Spirling, 2014) and their importance for oversight (Proksch and Slapin, 2011).",1 Introduction,[0],[0]
"In particular, the United Kingdom’s House of Commons, renowned for theatrical questions periods, has been studied in some depth.",1 Introduction,[0],[0]
"However, those accounts are largely qualitative in nature (Bull and Wells, 2012; Bates et al., 2014).",1 Introduction,[0],[0]
The present work: methodology.,1 Introduction,[0],[0]
"In order to approach these problems computationally, we introduce an unsupervised framework to structure the space of questions according to their rhetorical role.",1 Introduction,[0],[0]
"First, we identify common ways in which questions are phrased.",1 Introduction,[0],[0]
"To this end, we automatically extract these recurring surface forms, or motifs, based on the lexico-syntactic structure of the questions posed (Section 4).",1 Introduction,[0],[0]
"To capture rhetorical aspects we then group these motifs according to their role, relying on the intuition that this role is encoded in the type of answer a question receives.",1 Introduction,[0],[0]
To operationalize this intuition we construct a latent question-answer space in which question motifs triggering similar answers are mapped to the same region (Section 5).,1 Introduction,[0],[0]
The present work: application.,1 Introduction,[0],[0]
"We apply this general framework to the political discourse that occurs during parliamentary question sessions in
1558
the British House of Commons, a new dataset which we make publicly available (Section 3).",1 Introduction,[0],[0]
"Our framework extracts intuitive question types ranging from narrow factual queries to pointed criticisms disguised as questions (Section 5, Table 1).",1 Introduction,[0],[0]
We validate our framework by aligning these types with prior understandings of parliamentary proceedings from the political science literature (Section 6).,1 Introduction,[0],[0]
"In particular, previous work (Bates et al., 2014) has categorized questions asked in Parliament according to the intentions of the asker (e.g., to help the answerer, or to adversarially put them on the spot); we find a clear, predictive mapping between these expert-coded categories and the induced typology.",1 Introduction,[0],[0]
"We further show that the types of questions specific legislators tend to ask vary with whether they are part of the governing or opposition party, consistent with wellestablished accounts of partisan differences (Cowley, 2002; Spirling and McLean, 2007; Eggers and Spirling, 2014).",1 Introduction,[0],[0]
"Concretely, government legislators exhibit a preference for overtly friendly questions, while the opposition slants towards more aggressive question types.
",1 Introduction,[0],[0]
We then apply our methodology to provide new insights into how a legislator’s questioning behavior varies with their career trajectory.,1 Introduction,[0],[0]
"The pressures faced by legislators at various stages in their career are cross-cutting, and multiple possible hypotheses emerge.",1 Introduction,[0],[0]
"Younger, more enthusiastic legislators may be motivated to ask harderhitting questions, but risk being passed over for future promotion if they are too combative (Cowley, 2002).",1 Introduction,[0],[0]
"Older legislators, whose opportunities for promotion are largely behind them and hence have “less to lose”, may act more aggressively (Benedetto and Hix, 2007); or simply seek a quiet path to retirement.",1 Introduction,[0],[0]
"Viewing each group’s behavior through the questions they ask brings evidence for the latter hypothesis that more tenured legislators are more aggressive, even when questioning their own leaders.",1 Introduction,[0],[0]
"In this way, their presence in the House of Commons, and their refusal to simply ‘keep their heads down’, facilitates a core component of democracy.",1 Introduction,[0],[0]
Question-answering.,2 Related Work,[0],[0]
"Computationally, questions have received considerable attention in the context of question-answering (QA) systems—for a survey see Gupta and Gupta (2012)—with an em-
phasis on understanding their information need (Harabagiu, 2008).",2 Related Work,[0],[0]
"Techniques have been developed to categorize questions based on the nature of these information needs in the context of the TREC QA challenge (Harabagiu et al., 2000), and to identify questions asking for similar information (Shtok et al., 2012; Zhang et al., 2017; Jeon et al., 2005); questions have also been classified by topic (Cao et al., 2010) and quality (Treude et al., 2011; Ravi et al., 2014).",2 Related Work,[0],[0]
"In contrast, our work is not concerned with the information need central to QA applications, and instead focuses on the rhetorical aspect of questions.
",2 Related Work,[0],[0]
Question types.,2 Related Work,[0],[0]
"To facilitate retrieval of frequently asked questions, Lytinen and Tomuro (2002) manually developed a typology of surface question forms (e.g., ‘what’- and ‘why’-questions) starting from Lehnerts’ conceptual question categories (Lehnert, 1978).",2 Related Work,[0],[0]
"Question types were also hand annotated for dialog-act labeling, distinguishing between yes-no, wh-, open-ended and rhetorical questions (Dhillon et al., 2004).",2 Related Work,[0],[0]
"To complement this line of work, this paper introduces a completely unsupervised methodology to automatically build a domain-tailored question typology, bypassing the need for human annotation.
",2 Related Work,[0],[0]
Pragmatic dimensions.,2 Related Work,[0],[0]
"One important pragmatic dimension of questions that has been previously studied computationally is their level of politeness (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016); in the specific context of making requests, politeness was shown to correlate with the social status of the asker.",2 Related Work,[0],[0]
Sachdeva and Kumaraguru (2017) studied another rhetorical aspect by examining linguistic attributes distinguishing serviceable requests addressed to police on social media from general conversation.,2 Related Work,[0],[0]
"Previous research has also been directed at identifying rhetorical questions (Bhattasali et al., 2015) and understanding the motivations of their “askers” (Ranganath et al., 2016).",2 Related Work,[0],[0]
"Using the relationship between questions and answers, our work examines the rhetorical and social aspect of questions without predefining a pragmatic dimension and without relying on labeled data.",2 Related Work,[0],[0]
"We also complement these efforts in analyzing a broader range of situations in which questions may be posed without an information-seeking intent.
",2 Related Work,[0],[0]
Political discourse.,2 Related Work,[0],[0]
"Finally, our work contributes to a rapidly growing area of NLP applications to political domains (Monroe et al., 2008; Card et al.,
2016; Gonzalez-Bailon et al., 2010; Niculae et al., 2015; Grimmer and Stewart, 2013; Grimmer et al., 2012; Iyyer et al., 2014b, inter alia).",2 Related Work,[0],[0]
"Particularly relevant are applications to discourse in congressional and parliamentary settings (Thomas et al., 2006; Boydstun et al., 2014; Rheault et al., 2016).",2 Related Work,[0],[0]
"The bulk of our analysis focuses on the questions asked, and responses given during parliamentary question periods in the British House of Commons.",3 Data: Parliamentary Question Periods,[0],[0]
"Below, we provide a brief overview of key features of this political system in general, as well as a description of the question period setting.",3 Data: Parliamentary Question Periods,[0],[0]
Parliamentary systems.,3 Data: Parliamentary Question Periods,[0],[0]
"Legislators in the House of Commons (Members of Parliament, henceforth MPs or members) belong to two main voting and debating affiliations: a government party which controls the executive and holds a majority of the seats in the chamber, and a set of opposition parties.1",3 Data: Parliamentary Question Periods,[0],[0]
"The executive is headed by the Prime Minister (PM) and run by a cabinet of ministers, highranking government MPs responsible for various departments such as finance and education.",3 Data: Parliamentary Question Periods,[0],[0]
Question periods.,3 Data: Parliamentary Question Periods,[0],[0]
"The House of Commons holds weekly, moderated question periods, in which MPs of all affiliations take turns to ask questions to (and theoretically receive answers from) government ministers for each department regarding their specific domains.",3 Data: Parliamentary Question Periods,[0],[0]
Such events are a primary way in which legislators hold senior policy-makers responsible for their decisions.,3 Data: Parliamentary Question Periods,[0],[0]
"In practice, beyond narrow requests for information about specific policy points, MPs use their questions to critique or praise the government, or to self-promote; indeed, certain sessions, such as Questions to the Prime Minister, have gained renown for their partisan clashes, often fueled by the (mis)handling of a current crisis.",3 Data: Parliamentary Question Periods,[0],[0]
"The following question, asked to the Prime Minister by an opposition MP about contamination of the meat supply in 2013, encapsulates this odd mix of purposes:
“The Prime Minister is rightly shocked by the revelations that many food products contain 100% horse.",3 Data: Parliamentary Question Periods,[0],[0]
"Does he share my concern that, if tested, many of his answers may contain 100% bull?”2
1We use affiliation to refer broadly to the government and opposition roles, independent of the identity of the current government and opposition parties.",3 Data: Parliamentary Question Periods,[0],[0]
"In subsequent analysis we only consider the largest, “official” opposition party as the opposition.
",3 Data: Parliamentary Question Periods,[0],[0]
"2MPs almost always address each other in 3rd person.
",3 Data: Parliamentary Question Periods,[0],[0]
"The moderated, relatively rigid format of questions periods, along with the multifaceted array of underlying incentives and interpersonal relationships, yields a structurally controlled setting with a rich variety of social interactions, taking place in the realm of important policy discussions.",3 Data: Parliamentary Question Periods,[0],[0]
"This complexity makes question periods a particularly fruitful and consequential setting in which to study questions as social signals, and expand our understanding of their role beyond factual queries.",3 Data: Parliamentary Question Periods,[0],[0]
Dataset description.,3 Data: Parliamentary Question Periods,[0],[0]
"Our dataset covers question periods from May 1979 to December 2016, encompassing six different Prime Ministers.",3 Data: Parliamentary Question Periods,[0],[0]
"For each question period, we extract all questionanswer pairs, along with the identity of the asker and answerer.",3 Data: Parliamentary Question Periods,[0],[0]
"Because our focus here is on how questions are posed in a social setting, and not on the subsequent dialogue, we ignore questions which were tabled prior to the session, as well as any followup back-and-forth dialogue between the asker and answerer.
",3 Data: Parliamentary Question Periods,[0],[0]
"We augment this collection with metadata about each asker and answerer, including their political party, the time when they first took office, and whether they were serving as a minister at a given point in time.",3 Data: Parliamentary Question Periods,[0],[0]
"Such information is used to validate our methodology and interpret our results in light of the social context in which the questions were asked, described further in Sections 6 and 7.
",3 Data: Parliamentary Question Periods,[0],[0]
"In total there are 216,894 question-answer pairs in our data, occurring over 4,776 days and 6 prime-ministerships.",3 Data: Parliamentary Question Periods,[0],[0]
"The questions cover 1,975 different askers, 1,066 different answerers, and a variety of government departments with responsibilities ranging from defense to transport.",3 Data: Parliamentary Question Periods,[0],[0]
"We make this dataset publicly available, along with the code implementing our methodology, as part of the Cornell Conversational Analysis Toolkit.3",3 Data: Parliamentary Question Periods,[0],[0]
"The first component of our framework identifies lexico-syntactic phrasing patterns recurring in a collection of questions, which we call motifs.",4 Question Motifs,[0],[0]
"Intuitively, motifs constitute wordings commonly used to pose questions.",4 Question Motifs,[0],[0]
"To find motifs in a given collection, we first extract relevant fragments from each question.",4 Question Motifs,[0],[0]
"We then group sets of frequently co-occurring fragments into motifs.
",4 Question Motifs,[0],[0]
"3 https://github.com/CornellNLP/
Cornell-Conversational-Analysis-Toolkit
Question fragments.",4 Question Motifs,[0],[0]
Our goal is to find motifs which reflect functional characteristics of questions.,4 Question Motifs,[0],[0]
"Hence, we start by extracting the key fragments within a question which encapsulate its functional nature.",4 Question Motifs,[0],[0]
"Following the intuition that the bulk of this functional information is contained in the root of a question’s dependency parse along with its outgoing arcs (Iyyer et al., 2014a), we take the fragments of a question to be the root of its parse tree, along with each (root, child) pair.",4 Question Motifs,[0],[0]
"To capture cases when the operational word in the question is not connected to its root (such as “What...”), we also consider the initial unigram and bigram of a question as fragments.",4 Question Motifs,[0],[0]
"The following question has 5 fragments: what, what is, going→*, is←going and going→do.
",4 Question Motifs,[0],[0]
"(1) What is the minister going to do about ... ?
",4 Question Motifs,[0],[0]
"Because our goal is to capture topic-agnostic patterns, we ignore all fragments which contain a noun phrase (NP) or pronoun.",4 Question Motifs,[0],[0]
"NP subtrees are identified based on their outgoing dependencies to the root;4 in the event that an NP starts with a WHdeterminer (WDT), we consider (root, WDT) to be a fragment and drop the remainder of the NP.5
Finally, we note that some questions consist of multiple sub-questions (“What does the Minister think [...], and why [...]?”).",4 Question Motifs,[0],[0]
"For such questions, we recursively extract fragments from each child subtree in the same manner, starting from their roots.",4 Question Motifs,[0],[0]
From fragments to motifs.,4 Question Motifs,[0],[0]
We define motifs as sets of question fragments that frequently co-occur (in at least n questions).,4 Question Motifs,[0],[0]
"We find motifs by applying the apriori algorithm (Agrawal and Srikant, 1994) to find these common itemsets.",4 Question Motifs,[0],[0]
"This results in a collection of motifs M which correspond to different question phrasings.6 Examples of motifs are shown in Table 1.
",4 Question Motifs,[0],[0]
Motifs can identify phrasings to varying degrees of specificity.,4 Question Motifs,[0],[0]
"For example, the singleton motif {what is} corresponds to all questions starting with that bigram, while {what is, going→do} nar-
4We take as NPs subtrees connected to the root with the following: nsubj, nsubjpass, dobj, iobj, pobj, attr.
",4 Question Motifs,[0],[0]
"5In the particular case of the Parliament dataset, removing NPs also removes conventional, partisan address terms (e.g. “my hon. Friend”).
",4 Question Motifs,[0],[0]
"6In some cases, a pair of motifs almost always co-occurs in the same questions, making them redundant.",4 Question Motifs,[0],[0]
"We treat two motifs m1 and m2 as equivalent if, for some probability p, Pr(m1|m2) > p and Pr(m2|m1)",4 Question Motifs,[0],[0]
"> p; we keep the smaller of the two as the representative motif, or pick one of them arbitrarily if they are of equal sizes.
rows these down to questions also containing the fragment going→do.",4 Question Motifs,[0],[0]
"To model the specificity relation between motifs, we structure M as a directed acyclic graph where an edge points from a motif m1 to another motif m2 if the latter has exactly one more fragment in addition to those in m1, corresponding to a narrower set of phrasings.",4 Question Motifs,[0],[0]
Motif-representation of a question.,4 Question Motifs,[0],[0]
"Finally, a question q contains a motif if it includes all of the fragments comprising that motif.",4 Question Motifs,[0],[0]
"We can hence capture the phrasing of a given question q using the subset of motifs it contains, structured as the subgraphMq ⊂ M induced by this subset.",4 Question Motifs,[0],[0]
"This directed subgraph represents the question at multiple levels of specificity simultaneously; in particular, the set of sinks (i.e., nodes with outdegree 0; henceforth sink motifs) ofMq is the most finegrained way to specify the phrasing of q. For example {what is, is←going, going→do} is the only sink motif of the question in example (1); its entire subgraph is shown in Figure 3 in the appendix.",4 Question Motifs,[0],[0]
"The second component of our framework structures the space of questions according to their functional roles, thus going beyond the lexicosyntactic representation captured via motifs.",5 Latent Question Types,[0],[0]
The main intuition is that the nature of the answer that a question receives provides a good indication of its intention.,5 Latent Question Types,[0],[0]
"Therefore, if two questions are phrased differently but answered in similar ways, the parallels exhibited by their answers should reflect commonalities in the askers’ intentions.
",5 Latent Question Types,[0],[0]
"To operationalize this intuition, we first construct a latent space based on answers, and then map question motifs (Section 4) to the same space.",5 Latent Question Types,[0],[0]
"Using the resultant latent representations, we can then cluster questions with similar rhetorical functions, even if their surface forms are different.",5 Latent Question Types,[0],[0]
Constructing a space of answers.,5 Latent Question Types,[0],[0]
"In line with our focus on functional characterizations, we extract the fragments from each sentence of an answer, defined in the same way as question fragments.",5 Latent Question Types,[0],[0]
"We then construct a term-document matrix, where terms correspond to answer fragments, and documents correspond to individual answers in the corpus.",5 Latent Question Types,[0],[0]
"We filter out infrequent fragments occurring less than nA times, reweight the rows of this matrix with tf-idf reweighting, and scale to unit norm, producing a fragment-answer matrix A. We perform singular value decomposi-
tion on A and obtain a low-rank representation A ≈ Â = UASV TA , for some rank d, where rows of UA correspond to answer fragments and rows of VA correspond to answers.7
Latent projection of question motifs.",5 Latent Question Types,[0],[0]
We can draw a natural correspondence between a question motif m and answer term t if m occurs in a question whose answer contains t.,5 Latent Question Types,[0],[0]
"This enables us to compute representations of question motifs in the same space as Â. Concretely, we construct a motif-question matrix Q = (qij) where qij = 1 if motif i occurred in question j; we scale rows of Q to unit norm.",5 Latent Question Types,[0],[0]
"To represent Q in the latent answer space, we solve for Q̂ in Q = Q̂SV TA as Q̂ = QVAS−1, again scaling rows to unit norm.",5 Latent Question Types,[0],[0]
"Row i of Q̂ then gives a d-dimensional representation of motif i, denoted q̂i.
",5 Latent Question Types,[0],[0]
Grouping similar questions.,5 Latent Question Types,[0],[0]
"Finally, we identify question types—broad groups of similar motifs.",5 Latent Question Types,[0],[0]
"Intuitively, if two motifs mi and mj have vectors qi and qj which are close together, they elicit answers that are close in the latent space, so are functionally similar in this sense.",5 Latent Question Types,[0],[0]
"We use the KMeans algorithm (Pedregosa et al., 2011) to cluster motif vectors into k clusters; these clusters then constitute the desired set of question types.
",5 Latent Question Types,[0],[0]
"To determine the type of a particular question q∗, we transform it to a binary vector (q∗i ) where q∗i = 1 if motif i is a sink motif of q
∗; using only sink motifs at this stage allows us to characterize a question according to the most specific representation of its phrasing, thus avoiding spurious associations resulting from more general motifs.",5 Latent Question Types,[0],[0]
"We scale q∗, project it to the latent space as before, and assign the resultant projection q̂∗ to a cluster t, hence determining its type.
",5 Latent Question Types,[0],[0]
"Since question motifs and answer fragments have both been mapped to the same latent space (as rows of Q̂ and UA respectively), we can also assign each answer fragment to a question type.",5 Latent Question Types,[0],[0]
"This further facilitates interpretability through characterizing the answers commonly triggered by a particular type of question.
",5 Latent Question Types,[0],[0]
"7We experimented with grouping answer fragments into motifs as well, but found that most of the motifs produced were one fragment large.",5 Latent Question Types,[0],[0]
"While future work could focus more on understanding consistent phrasings of answers, we note that at least in our chosen corpus, answers are longer and encompass a much greater variation of possible phrasings.",5 Latent Question Types,[0],[0]
"We now apply our general framework to the particular setting of parliamentary question periods, structuring the space of questions posed within these sessions according to their rhetorical function.",6 Validation,[0],[0]
"To validate the induced typology, we quantitatively show that it recovers asker intentions in an expert-coded dataset, and qualitatively aligns with prior findings in the political science literature on parliamentary dynamics.",6 Validation,[0],[0]
Question types in Parliament.,6 Validation,[0],[0]
"We apply our motif extraction and question type induction pipeline to the questions in the parliamentary dataset.8 Over 90% of the questions in the dataset contain at least one of the resulting 2,817 motifs; in subsequent analyses we discard questions without a matching motif.",6 Validation,[0],[0]
"We apply our pipeline to the questions in the parliamentary dataset, and induce a typology of k = 8 question types to capture the rich array of questions represented in this space while preserving interpretability.
",6 Validation,[0],[0]
"Table 1 displays extracted types, along with example questions, answers, and motifs.9",6 Validation,[0],[0]
"The second author, a political scientist with domain expertise in the UK parliamentary setting, manually investigated each type and provided interpretable labels.",6 Validation,[0],[0]
"For example, in questions of type 4, the asker is aware that his main premise is supported by the minister, and thus will be met with a positive statement backing the thrust of the question; we call this the agreement cluster.",6 Validation,[0],[0]
"Types 6 and 7 are much more combative: in type 6 questions the asker explicitly attempts to force the minister to concede/accept a point that would undermine some government stance, while type 7 contains condemnatory questions that prompt the minister to justify a policy that is self-evidently bad in the eyes of the asker.",6 Validation,[0],[0]
"In contrast, type 2 constitutes tamer narrow queries that require the minister to simply report on non-partisan matters of policy.",6 Validation,[0],[0]
(Extended interpretations in the appendix.),6 Validation,[0],[0]
Quantitative validation.,6 Validation,[0],[0]
"We compare our output to a dataset of 1,256 questions asked to various Prime Ministers labeled by Bates et al. (2014)
8We consider questions to be sentences ending in question marks.",6 Validation,[0],[0]
"If an utterance consists of multiple questions, we extract fragments sets from each question separately, and take the motifs of the utterance to be the union of motifs of each component question.",6 Validation,[0],[0]
"We set n = 100, p = 0.9, nA = 100 and d = 25.",6 Validation,[0],[0]
"The choice of parameters was done via manual inspection of the dataset.
",6 Validation,[0],[0]
"9Each type contains a few hundred question motifs and answer fragments.
(also included in our data distribution).",6 Validation,[0],[0]
"Each question in this data is hand-coded by a domain expert with one of three labels indicating the rhetorical intention of the asker: compared to standard questions—denoting straightforward factual queries, helpful questions serve as prompts for the PM to talk favorably about their government, while unanswerable questions are effectively vehicles for delivering criticisms that the PM cannot respond to.",6 Validation,[0],[0]
Questions which are unanswered by the PM are also labeled.,6 Validation,[0],[0]
"If our framework captures meaningful rhetorical dimensions, we expect a given label to be over-represented in some of our induced types, and under-represented in others.
",6 Validation,[0],[0]
"Even though our clustering of questions is generated in an unsupervised fashion without any guidance from the coded rhetorical roles, we see that several of the types we discover closely align with these annotations.",6 Validation,[0],[0]
"In particular, helpful questions are highly associated with the agreement type (constituting 28% of questions of that type compared to 14% over the entire dataset; binomial test p < 0.01), reinforcing our interpretation
that this type captures MPs cheerleading their own government.",6 Validation,[0],[0]
"Conversely, unanswerable questions are frequently of the concede/accept type (20% in-type vs. 11% overall), while condemnatory questions are often unanswered (43% vs. 24% overall), suggesting that questions of these types have an increased tendency to be posed as aggressive criticisms packaged as questions.
",6 Validation,[0],[0]
"We also validate our framework in a prediction setting using these labels, in three binary classification tasks: distinguishing helpful vs. standard, unanswerable vs. standard, and unanswered vs. answered questions.",6 Validation,[0],[0]
"(In each task, we balance the two classes.)",6 Validation,[0],[0]
"To control for asker affiliation effects, we consider only questions asked by government MPs for the helpful task, and opposition questions in the unanswerable and unanswered tasks; we train on questions to Conservative PMs and evaluate on Labour PMs.10 For each setting, we train logistic regression classifiers; as
10These choices are motivated by the number of questions from each affiliation and party in the dataset (see appendix for further details on this dataset).
",6 Validation,[0],[0]
"features we compare the latent representation of each question to a unigram BOW baseline.11
In the unanswerable and unanswered tasks, we find that the BOW features do not perform significantly better than a random (50%) baseline.",6 Validation,[0],[0]
"However, the latent question features produced by our framework bring additional predictive signal and outperform the baseline when combined with BOW (binomial p < 0.05), achieving accuracies of 66% and 62% respectively (compared with 55% and 50% for BOW alone).",6 Validation,[0],[0]
"This suggests that our representation captures useful rhetorical information that, given our train-test split, generalizes across parties.",6 Validation,[0],[0]
"None of the models significantly outperform the random baseline on the helpful task, perhaps owing to the small data size.",6 Validation,[0],[0]
Qualitative validation: question partisanship.,6 Validation,[0],[0]
We additionally provide a qualitative validation of our framework by comparing the questionasking activity of government and oppositionaffiliated MPs—as viewed through the extracted question types—to well-established characterizations of these affiliations in the political science literature.,6 Validation,[0],[0]
"In particular, prior work has examined the bifurcation in behavior between government and opposition members, in their differing focus on various issues (Louwerse, 2012), and in settings such as roll call votes (Cowley, 2002;",6 Validation,[0],[0]
"Spirling and McLean, 2007; Eggers and Spirling, 2014).",6 Validation,[0],[0]
"Since government MPs are elected on the same party ticket and manifesto, they primarily act to sup-
11We used tf-idf reweighting and excluded unigrams occurring less than 5 times.
",6 Validation,[0],[0]
"port the government’s various policies and bolster the status of their cabinet, seldom airing disagreements publicly.",6 Validation,[0],[0]
"In contrast, opposition members tend to offer trenchant partisan criticism of government policies, seeking to destabilize the government’s relationship with its MPs and create negative press in the country at large.",6 Validation,[0],[0]
"In characterizing the question-asking activity of government and opposition MPs, this friendly vs. adversarial behavior should also be reflected in a rhetorical typology of questions.12
Concretely, to quantify the relationship between a particular question type t and asker affiliation P , we compute the log-odds ratio of type t questions asked by MPs in P , compared to MPs not in P .13
Figure 1A shows the resultant log-odds ratios of each question type for government and opposition members.",6 Validation,[0],[0]
"Notably, we see that agreement-type questions are significantly more likely to originate from government than from opposition MPs, while the opposite holds for concede/accept and condemnatory questions (binomial p < 10−4 for each, comparing within-type to overall proportions of questions from an affiliation).",6 Validation,[0],[0]
"No such
12While we induce the typology over our entire dataset, we perform all subsequent analyses on a filtered subset of 50,152 questions.",6 Validation,[0],[0]
"In particular, we omit utterances with multiple questions—i.e. multiple question marks—to ensure that we don’t confound effects arising from different co-occurring question types.",6 Validation,[0],[0]
Our filtering decisions are also determined by the availability of information about the asker and answerers’ roles in Parliament.,6 Validation,[0],[0]
"Further information about these filtering choices can be found in the appendix.
",6 Validation,[0],[0]
"13The log-odds values are not symmetric between government and opposition, because they includes questions asked by MPs not in the official opposition.
",6 Validation,[0],[0]
"slant is exhibited in the narrow factual type, further reinforcing the role of such questions as informational queries about relatively non-partisan issues.",6 Validation,[0],[0]
"These results strongly cohere with the “textbook” accounts of parliamentary activity in the literature, as well as our interpretation of these types as bolstering or antagonistic.
",6 Validation,[0],[0]
"Moreover, we find that the same MP shifts in her propensity for different question types as her affiliation changes.",6 Validation,[0],[0]
"When a new political party is elected into office, MPs who were previously in the opposition now belong to the government party, and vice versa.",6 Validation,[0],[0]
"Such a switch occurs within our data between the Major and Blair governments (Conservative to Labour, 1997), and between the Brown and Cameron governments (Labour to Conservative, 2010).",6 Validation,[0],[0]
"For both switches, we consider all MPs who asked at least 5 questions both before and after the switch, resulting in 88 members who became government MPs and 102 who became opposition MPs.",6 Validation,[0],[0]
"For an MP M we compute PM,t, their propensity for a question type t, as the proportion of questions they ask which are from t. Comparing PM,t before and after a switch, we replicate the key differences observed above—for instance, we find that former opposition MPs who become government MPs decrease in their propensity for condemnatory questions, while newly opposition MPs move in the other direction (Wilcoxon p < 0.001, Figure 1B).",6 Validation,[0],[0]
"This suggests that the general trends we observed before are driven by the shift in affiliation, and hence parliamentary role, of individual MPs.",6 Validation,[0],[0]
"We now apply our framework to gain further insights into the nature of political discourse in Parliament, focusing on how questioning behavior varies with a member’s tenure in the institution.",7 Career Trajectory Effects,[0],[0]
"As stated in the introduction, two alternative hypotheses arise: younger MPs may be more vigorously critical out of enthusiasm, but are potentially tempered by their stake in future promotion prospects compared to older members (Cowley, 2002, 2012).",7 Career Trajectory Effects,[0],[0]
"Alternatively, older MPs who have less at stake in terms of prospects of further promotion may ask more antagonistic questions.",7 Career Trajectory Effects,[0],[0]
"Throughout, young and old refer to tenure—i.e., how many years someone has served as an MP— rather than biological age.
",7 Career Trajectory Effects,[0],[0]
"In order to understand the extent to which young or old members contribute a specific type of question, for each question type t we compute the median tenure of askers of each question in t, and compare the median tenures of different question types, for each affiliation (Figure 2A).14 We see that among both affiliations, more aggressive questions tend to originate more from older members, reflected in significantly higher median tenures (for types 6 in both affiliations, and 7 in government MPs; Mann Whitney U test p < 0.001 comparing within-type median tenure with outside-type median tenure); whereas standard issue update questions tend to come from younger
14Median tenures for opposition members are generally higher; winning an election tends to result in more newlyelected and therefore younger MPs (Webb and Farrell, 1999).
",7 Career Trajectory Effects,[0],[0]
"members (p < 0.001, both affiliations).",7 Career Trajectory Effects,[0],[0]
"Notably, the disproportionate aggressiveness of older members manifests even among government MPs who direct these questions towards their own government.",7 Career Trajectory Effects,[0],[0]
"This supports the “less to lose” intuition, offering a rhetorical parallel to previous findings about the increased tendency to vote contrary to party lines from MPs with little chance of ministerial promotion (Benedetto and Hix, 2007).
",7 Career Trajectory Effects,[0],[0]
"Interestingly, we find that these differential preferences across member tenure also manifest at a finer granularity than simply less to more aggressive.",7 Career Trajectory Effects,[0],[0]
"For instance, younger opposition members tend to contribute more condemnatory questions compared to older members (Mann Whitney U test p < 0.01), who disproportionately favor concede/accept questions.",7 Career Trajectory Effects,[0],[0]
"While further work is needed to fully explain these differences, we speculate that they are potentially reflective of strategic attempts by younger MPs to signal traits that could facilitate future promotion, such as partisan loyalty (Kam, 2009).
",7 Career Trajectory Effects,[0],[0]
"To discount the possibility of these effects being solely driven by a few very prolific young or old MPs, we also consider a setting where type propensities are macroaveraged over MPs.",7 Career Trajectory Effects,[0],[0]
"For each affiliation we compare the cohort of younger MPs who are newly voted in at the 1997 and 2010 elections, with older MPs who have been in office prior to the election.15 We compute the type propensities of these two cohorts over the questions they asked during the subsequent parliamentary sitting, and replicate the tenure effects observed previously (Figure 2B).",7 Career Trajectory Effects,[0],[0]
"This suggests that these parliamentary career effects reflect behavioral changes at the level of individual MPs, whose incentives evolve over their tenure.",7 Career Trajectory Effects,[0],[0]
In this work we introduced an unsupervised framework for structuring the space of questions according to their rhetorical role.,8 Conclusion and Future Work,[0],[0]
"We instantiated and validated our approach in the domain of parliamentary question periods, and revealed new interactions between questioning behavior and career trajectories.
",8 Conclusion and Future Work,[0],[0]
We note that our methodology is not tied to a particular domain.,8 Conclusion and Future Work,[0],[0]
"It would be interesting to explore its potential in a variety of less structured do-
15This totals 272 new and 184 old government MPs, and 84 new and 179 old opposition MPs.
mains where questions likewise play a crucial role.",8 Conclusion and Future Work,[0],[0]
"For example, examining how interviewers in highprofile media settings (e.g., Frost on Nixon) can use their questions to elicit substantive responses from influential people would aid us in the broader normative goal of holding elites to account, by gaining a better understanding of what and how to ask, and what (not) to accept as an answer.
",8 Conclusion and Future Work,[0],[0]
"From a technical standpoint, future work could also augment the representation of questions and answers presently used in our framework, beyond our heuristic of using root arcs without noun phrases.",8 Conclusion and Future Work,[0],[0]
"Richer linguistic representations, as well as more judicious ways of weighting different fragments and motifs, could enable us to capture a wider range of possible surface and rhetorical forms, especially in settings where phrasings are potentially less structured by institutional conventions.",8 Conclusion and Future Work,[0],[0]
"Additionally, as with most unsupervised methods, our approach is limited by the need to hand-select parameters such as the number of clusters, and manually interpret the typology’s output.",8 Conclusion and Future Work,[0],[0]
"Having annotations of these corpora could better motivate the methodology and enable further evaluation and interpretation; we hope to encourage such annotation efforts by releasing the dataset.
",8 Conclusion and Future Work,[0],[0]
"Inevitably, drawing causal lessons from observational data is difficult.",8 Conclusion and Future Work,[0],[0]
"Moving forward, experimental tests of insights gathered through such explorations would enable us to establish causal effects of question-asking rhetoric, perhaps offering prescriptive insights into questioning strategies for objectives such as information-seeking (Dillman, 1978), request-making (Althoff et al., 2014; Mitra and Gilbert, 2014) and persuasion (Tan et al., 2016; Zhang et al., 2016; Wang et al., 2017).
",8 Conclusion and Future Work,[0],[0]
Acknowledgements.,8 Conclusion and Future Work,[0],[0]
"The first author thanks John Bercow, the Speaker of the House, for suggesting she “calm [her]self
down by taking up yoga” during the hectic deadline push
(https://youtu.be/AiAWdLAIj3c).",8 Conclusion and Future Work,[0],[0]
"The authors
thank the anonymous reviewers and Liye Fu for their com-
ments and for their helpful questions.",8 Conclusion and Future Work,[0],[0]
"We are grateful to the
organizers of the conference on New Directions in Text as
Data for fostering the inter-disciplinary collaboration that led
to this work, to Amber Boydstun and Philip Resnik for their
insights on questions in the political domain, and to Stephen
Bates, Peter Kerr and Christopher Byrne for sharing the la-
beled PMQ dataset.",8 Conclusion and Future Work,[0],[0]
"This research has been supported in part
by a Discovery and Innovation Research Seed Award from
the Office of the Vice Provost for Research at Cornell.",8 Conclusion and Future Work,[0],[0]
"Questions play a prominent role in social interactions, performing rhetorical functions that go beyond that of simple informational exchange.",abstractText,[0],[0]
"The surface form of a question can signal the intention and background of the person asking it, as well as the nature of their relation with the interlocutor.",abstractText,[0],[0]
"While the informational nature of questions has been extensively examined in the context of question-answering applications, their rhetorical aspects have been largely understudied.",abstractText,[0],[0]
"In this work we introduce an unsupervised methodology for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role.",abstractText,[0],[0]
"By applying this framework to the setting of question sessions in the UK parliament, we show that the resulting typology encodes key aspects of the political discourse—such as the bifurcation in questioning behavior between government and opposition parties—and reveals new insights into the effects of a legislator’s tenure and political career ambitions.",abstractText,[0],[0]
Asking too much? The rhetorical role of questions in political discourse,title,[0],[0]
"Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014).",1 Introduction,[0],[0]
"Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect.",1 Introduction,[0],[0]
"For example, in sentence “great food but the service was dreadful!”, the sentiment polarity of aspect “food” is positive while the polarity of aspect
∗ Corresponding author.
",1 Introduction,[0],[0]
“service” is negative.,1 Introduction,[0],[0]
Researchers typically use machine learning algorithms and build sentiment classifier in a supervised manner.,1 Introduction,[0],[0]
"Representative approaches in literature include feature based Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014) and neural network models (Dong et al., 2014; Lakkaraju et al., 2014; Vo and Zhang, 2015; Nguyen and Shirai, 2015; Tang et al., 2015a).",1 Introduction,[0],[0]
"Neural models are of growing interest for their capacity to learn text representation from data without careful engineering of features, and to capture semantic relations between aspect and context words in a more scalable way than feature based SVM.
",1 Introduction,[0],[0]
"Despite these advantages, conventional neural models like long short-term memory (LSTM) (Tang et al., 2015a) capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect.",1 Introduction,[0],[0]
We believe that only some subset of context words are needed to infer the sentiment towards an aspect.,1 Introduction,[0],[0]
"For example, in sentence “great food but the service was dreadful!”, “dreadful” is an important clue for the aspect “service” but “great” is not needed.",1 Introduction,[0],[0]
"Standard LSTM works in a sequential way and manipulates each context word with the same operation, so that it cannot explicitly reveal the importance of each context word.",1 Introduction,[0],[0]
A desirable solution should be capable of explicitly capturing the importance of context words and using that information to build up features for the sentence after given an aspect word.,1 Introduction,[0],[0]
"Furthermore, a human asked to do this task will selectively focus on parts of the contexts, and acquire information where it is needed to build up an internal representation towards an aspect in his/her mind.",1 Introduction,[0],[0]
"ar X iv :1 60 5.
",1 Introduction,[0],[0]
"08 90
0v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
4 Se
p 20
In pursuit of this goal, we develop deep memory network for aspect level sentiment classification, which is inspired by the recent success of computational models with attention mechanism and explicit memory (Graves et al., 2014; Bahdanau et al., 2015; Sukhbaatar et al., 2015).",1 Introduction,[0],[0]
"Our approach is data-driven, computationally efficient and does not rely on syntactic parser or sentiment lexicon.",1 Introduction,[0],[0]
The approach consists of multiple computational layers with shared parameters.,1 Introduction,[0],[0]
"Each layer is a content- and location- based attention model, which first learns the importance/weight of each context word and then utilizes this information to calculate continuous text representation.",1 Introduction,[0],[0]
The text representation in the last layer is regarded as the feature for sentiment classification.,1 Introduction,[0],[0]
"As every component is differentiable, the entire model could be efficiently trained end-toend with gradient descent, where the loss function is the cross-entropy error of sentiment classification.
",1 Introduction,[0],[0]
"We apply the proposed approach to laptop and restaurant datasets from SemEval 2014 (Pontiki et al., 2014).",1 Introduction,[0],[0]
"Experimental results show that our approach performs comparable to a top system using feature-based SVM (Kiritchenko et al., 2014).",1 Introduction,[0],[0]
"On both datasets, our approach outperforms both LSTM and attention-based LSTM models (Tang et al., 2015a) in terms of classification accuracy and running speed.",1 Introduction,[0],[0]
"Lastly, we show that using multiple computational layers over external memory could achieve improved performance.",1 Introduction,[0],[0]
"Our approach is inspired by the recent success of memory network in question answering (Weston et al., 2014; Sukhbaatar et al., 2015).",2 Background: Memory Network,[0],[0]
"We describe the background on memory network in this part.
",2 Background: Memory Network,[0],[0]
Memory network is a general machine learning framework introduced by Weston et al. (2014).,2 Background: Memory Network,[0],[0]
"Its central idea is inference with a long-term memory component, which could be read, written to, and jointly learned with the goal of using it for prediction.",2 Background: Memory Network,[0],[0]
"Formally, a memory network consists of a memory m and four components I , G, O and R, where m is an array of objects such as an array of vectors.",2 Background: Memory Network,[0],[0]
"Among these four components, I converts input to internal feature representation, G updates old memories with new input, O generates an out-
put representation given a new input and the current memory state, R outputs a response based on the output representation.
",2 Background: Memory Network,[0],[0]
Let us take question answering as an example to explain the work flow of memory network.,2 Background: Memory Network,[0],[0]
"Given a list of sentences and a question, the task aims to find evidences from these sentences and generate an answer, e.g. a word.",2 Background: Memory Network,[0],[0]
"During inference, I component reads one sentence si at a time and encodes it into a vector representation.",2 Background: Memory Network,[0],[0]
Then G component updates a piece of memory mi based on current sentence representation.,2 Background: Memory Network,[0],[0]
"After all sentences are processed, we get a memory matrix m which stores the semantics of these sentences, each row representing a sentence.",2 Background: Memory Network,[0],[0]
"Given a question q, memory network encodes it into vector representation eq, and then O component uses eq to select question related evidences from memory m and generates an output vector o. Finally, R component takes o as the input and outputs the final response.",2 Background: Memory Network,[0],[0]
It is worth noting that O component could consist of one or more computational layers (hops).,2 Background: Memory Network,[0],[0]
The intuition of utilizing multiple hops is that more abstractive evidences could be found based on previously extracted evidences.,2 Background: Memory Network,[0],[0]
"Sukhbaatar et al. (2015) demonstrate that multiple hops could uncover more abstractive evidences than single hop, and could yield improved results on question answering and language modeling.",2 Background: Memory Network,[0],[0]
"In this section, we describe the deep memory network approach for aspect level sentiment classification.",3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
We first give the task definition.,3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
"Afterwards, we describe an overview of the approach before presenting the content- and location- based attention models in each computational layer.",3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
"Lastly, we describe the use of this approach for aspect level sentiment classification.",3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
"Given a sentence s = {w1, w2, ..., wi, ...wn} consisting of n words and an aspect word wi 1 occur-
1In practice, an aspect might be a multi word expression such as “battery life”.",3.1 Task Definition and Notation,[0],[0]
"For simplicity we still consider aspect as a single word in this definition.
ring in sentence s, aspect level sentiment classification aims at determining the sentiment polarity of sentence s towards the aspect wi.",3.1 Task Definition and Notation,[0],[0]
"For example, the sentiment polarity of sentence “great food but the service was dreadful!”",3.1 Task Definition and Notation,[0],[0]
"towards aspect “food” is positive, while the polarity towards aspect “service” is negative.",3.1 Task Definition and Notation,[0],[0]
"When dealing with a text corpus, we map each word into a low dimensional, continuous and real-valued vector, also known as word embedding (Mikolov et al., 2013; Pennington et al., 2014).",3.1 Task Definition and Notation,[0],[0]
"All the word vectors are stacked in a word embedding matrix L ∈ Rd×|V |, where d is the dimension of word vector and |V",3.1 Task Definition and Notation,[0],[0]
| is vocabulary size.,3.1 Task Definition and Notation,[0],[0]
"The word embedding of wi is notated as ei ∈ Rd×1, which is a column in the embedding matrix L.",3.1 Task Definition and Notation,[0],[0]
"We present an overview of the deep memory network for aspect level sentiment classification.
",3.2 An Overview of the Approach,[0],[0]
"Given a sentence s = {w1, w2, ..., wi, ...wn} and the aspect word wi, we map each word into its embedding vector.",3.2 An Overview of the Approach,[0],[0]
"These word vectors are separated into two parts, aspect representation and context representation.",3.2 An Overview of the Approach,[0],[0]
"If aspect is a single word like “food” or “service”, aspect representation is the embedding of aspect word.",3.2 An Overview of the Approach,[0],[0]
"For the case where aspect is multi word expression like “battery life”, aspect representation is an average of its constituting word vectors (Sun et al., 2015).",3.2 An Overview of the Approach,[0],[0]
"To simplify the interpretation, we consider aspect as a single word wi.",3.2 An Overview of the Approach,[0],[0]
"Context word vectors {e1, e2 ...",3.2 An Overview of the Approach,[0],[0]
"ei−1, ei+1 ...",3.2 An Overview of the Approach,[0],[0]
"en} are stacked and regarded as the external memory m ∈ Rd×(n−1), where n is the sentence length.
",3.2 An Overview of the Approach,[0],[0]
"An illustration of our approach is given in Figure 1, which is inspired by the use of memory network in question answering (Sukhbaatar et al., 2015).",3.2 An Overview of the Approach,[0],[0]
"Our approach consists of multiple computational layers (hops), each of which contains an attention layer and a linear layer.",3.2 An Overview of the Approach,[0],[0]
"In the first computational layer (hop 1), we regard aspect vector as the input to adaptively select important evidences from memory m through attention layer.",3.2 An Overview of the Approach,[0],[0]
The output of attention layer and the linear transformation of aspect vector2 are summed and the result is considered as the input of next layer (hop 2).,3.2 An Overview of the Approach,[0],[0]
"In a similar way, we stack multiple hops and
2In preliminary experiments, we tried directly using aspect vector without a linear transformation, and found that adding a linear layer works slightly better.
run these steps multiple times, so that more abstractive evidences could be selected from the external memory m. The output vector in last hop is considered as the representation of sentence with regard to the aspect, and is further used as the feature for aspect level sentiment classification.
",3.2 An Overview of the Approach,[0],[0]
It is helpful to note that the parameters of attention and linear layers are shared in different hops.,3.2 An Overview of the Approach,[0],[0]
"Therefore, the model with one layer and the model with nine layers have the same number of parameters.",3.2 An Overview of the Approach,[0],[0]
We describe our attention model in this part.,3.3 Content Attention,[0],[0]
"The basic idea of attention mechanism is that it assigns a weight/importance to each lower position when computing an upper level representation (Bahdanau et al., 2015).",3.3 Content Attention,[0],[0]
"In this work, we use attention model to compute the representation of a sentence with regard to an aspect.",3.3 Content Attention,[0],[0]
The intuition is that context words do not contribute equally to the semantic meaning of a sentence.,3.3 Content Attention,[0],[0]
"Furthermore, the importance of a word should be different if we focus on different aspect.",3.3 Content Attention,[0],[0]
Let us again take the example of “great food but the service was dreadful!”.,3.3 Content Attention,[0],[0]
The context word “great” is more important than “dreadful” for aspect “food”.,3.3 Content Attention,[0],[0]
"On the contrary, “dreadful” is more important than “great” for aspect “service”.
",3.3 Content Attention,[0],[0]
"Taking an external memory m ∈ Rd×k and an aspect vector vaspect ∈ Rd×1 as input, the attention model outputs a continuous vector vec ∈ Rd×1.",3.3 Content Attention,[0],[0]
"The
output vector is computed as a weighted sum of each piece of memory in m, namely
vec = k∑
i=1
αimi (1)
where k is the memory size, αi ∈",3.3 Content Attention,[0],[0]
"[0, 1] is the weight of mi and ∑ i αi = 1.",3.3 Content Attention,[0],[0]
We implement a neural network based attention model.,3.3 Content Attention,[0],[0]
"For each piece of memory mi, we use a feed forward neural network to compute its semantic relatedness with the aspect.",3.3 Content Attention,[0],[0]
"The scoring function is calculated as follows, where Watt ∈ R1×2d and batt ∈ R1×1.
gi = tanh(Watt[mi; vaspect] + batt) (2)
After obtaining {g1, g2, ...",3.3 Content Attention,[0],[0]
"gk}, we feed them to a softmax function to calculate the final importance scores {α1, α2, ... αk}.
",3.3 Content Attention,[0],[0]
αi =,3.3 Content Attention,[0],[0]
"exp(gi)∑k j=1 exp(gj)
(3)
",3.3 Content Attention,[0],[0]
We believe that such an attention model has two advantages.,3.3 Content Attention,[0],[0]
One advantage is that this model could adaptively assign an importance score to each piece of memory mi according to its semantic relatedness with the aspect.,3.3 Content Attention,[0],[0]
"Another advantage is that this attention model is differentiable, so that it could be easily trained together with other components in an end-to-end fashion.",3.3 Content Attention,[0],[0]
We have described our neural attention framework and a content-based model in previous subsection.,3.4 Location Attention,[0],[0]
"However, the model mentioned above ignores the location information between context word and aspect.",3.4 Location Attention,[0],[0]
Such location information is helpful for an attention model because intuitively a context word closer to the aspect should be more important than a farther one.,3.4 Location Attention,[0],[0]
"In this work, we define the location of a context word as its absolute distance with the aspect in the original sentence sequence3.",3.4 Location Attention,[0],[0]
"On this basis, we study four strategies to encode the location information in the attention model.",3.4 Location Attention,[0],[0]
"The details are described below.
3The location of a context word could also be measured by its distance to the aspect along a syntactic path.",3.4 Location Attention,[0],[0]
"We leave this as a future work as we prefer to developing a purely data-driven approach without using external parsing results.
",3.4 Location Attention,[0],[0]
• Model 1.,3.4 Location Attention,[0],[0]
"Following Sukhbaatar et al. (2015), we calculate the memory vector mi with
mi = ei vi (4)
where means element-wise multiplication and vi ∈ Rd×1 is a location vector for word wi.",3.4 Location Attention,[0],[0]
"Every element in vi is calculated as follows,
vki = (1− li/n)− (k/d)(1− 2× li/n) (5)
where n is sentence length, k is the hop number and li is the location of wi. •",3.4 Location Attention,[0],[0]
Model 2.,3.4 Location Attention,[0],[0]
"This is a simplified version of Model 1, using the same location vector vi for wi in different hops.",3.4 Location Attention,[0],[0]
"Location vector vi is calculated as follows.
",3.4 Location Attention,[0],[0]
"vi = 1− li/n (6)
• Model 3.",3.4 Location Attention,[0],[0]
"We regard location vector vi as a parameter and compute a piece of memory with vector addition, namely
mi = ei + vi (7)
All the position vectors are stacked in a position embedding matrix, which is jointly learned with gradient descent.",3.4 Location Attention,[0],[0]
•,3.4 Location Attention,[0],[0]
Model 4.,3.4 Location Attention,[0],[0]
Location vectors are also regarded as parameters.,3.4 Location Attention,[0],[0]
"Different from Model 3, location representations are regarded as neural gates to control how many percent of word semantics is written into the memory.",3.4 Location Attention,[0],[0]
"We feed location vector vi to a sigmoid function σ, and calculatemi with element-wise multiplication:
mi = ei σ(vi) (8)",3.4 Location Attention,[0],[0]
"It is widely accepted that computational models that are composed of multiple processing layers have the ability to learn representations of data with multiple levels of abstraction (LeCun et al., 2015).",3.5 The Need for Multiple Hops,[0],[0]
"In this work, the attention layer in one layer is essentially a weighted average compositional function, which is not powerful enough to handle the sophisticated computationality like negation, intensification and contrary in language.",3.5 The Need for Multiple Hops,[0],[0]
Multiple computational layers allow the deep memory network to learn representations of text with multiple levels of abstraction.,3.5 The Need for Multiple Hops,[0],[0]
"Each layer/hop retrieves important context words,
and transforms the representation at previous level into a representation at a higher, slightly more abstract level.",3.5 The Need for Multiple Hops,[0],[0]
"With the composition of enough such transformations, very complex functions of sentence representation towards an aspect can be learned.",3.5 The Need for Multiple Hops,[0],[0]
"We regard the output vector in last hop as the feature, and feed it to a softmax layer for aspect level sentiment classification.",3.6 Aspect Level Sentiment Classification,[0],[0]
"The model is trained in a supervised manner by minimizing the cross entropy error of sentiment classification, whose loss function is given below, where T means all training instances, C is the collection of sentiment categories, (s, a) means a sentence-aspect pair.
",3.6 Aspect Level Sentiment Classification,[0],[0]
"loss = − ∑
(s,a)∈T ∑ c∈C P gc (s, a) · log(Pc(s, a))",3.6 Aspect Level Sentiment Classification,[0],[0]
"(9)
Pc(s, a) is the probability of predicting (s, a) as category c produced by our system.",3.6 Aspect Level Sentiment Classification,[0],[0]
"P gc (s, a) is 1 or 0, indicating whether the correct answer is c. We use back propagation to calculate the gradients of all the parameters, and update them with stochastic gradient descent.",3.6 Aspect Level Sentiment Classification,[0],[0]
"We clamp the word embeddings with 300-dimensional Glove vectors (Pennington et al., 2014), which is trained from web data and the vocabulary size is 1.9M4.",3.6 Aspect Level Sentiment Classification,[0],[0]
"We randomize other parameters with uniform distribution U(−0.01, 0.01), and set the learning rate as 0.01.",3.6 Aspect Level Sentiment Classification,[0],[0]
We describe experimental settings and report empirical results in this section.,4 Experiment,[0],[0]
"We conduct experiments on two datasets from SemEval 2014 (Pontiki et al., 2014), one from laptop domain and another from restaurant domain.",4.1 Experimental Setting,[0],[0]
Statistics of the datasets are given in Table 1.,4.1 Experimental Setting,[0],[0]
"It is worth noting that the original dataset contains the fourth category - conflict, which means that a sentence expresses both positive and negative opinion towards an aspect.",4.1 Experimental Setting,[0],[0]
"We remove conflict category as the number of instances is very tiny, incorporating which
4Available at: http://nlp.stanford.edu/projects/glove/.
will make the dataset extremely unbalanced.",4.1 Experimental Setting,[0],[0]
Evaluation metric is classification accuracy.,4.1 Experimental Setting,[0],[0]
"We compare with the following baseline methods on both datasets.
",4.2 Comparison to Other Methods,[0],[0]
"(1) Majority is a basic baseline method, which assigns the majority sentiment label in training set to each instance in the test set.
",4.2 Comparison to Other Methods,[0],[0]
(2) Feature-based SVM performs state-of-the-art on aspect level sentiment classification.,4.2 Comparison to Other Methods,[0],[0]
"We compare with a top system using ngram features, parse features and lexicon features (Kiritchenko et al., 2014).
",4.2 Comparison to Other Methods,[0],[0]
"(3) We compare with three LSTM models (Tang et al., 2015a)).",4.2 Comparison to Other Methods,[0],[0]
"In LSTM, a LSTM based recurrent model is applied from the start to the end of a sentence, and the last hidden vector is used as the sentence representation.",4.2 Comparison to Other Methods,[0],[0]
"TDLSTM extends LSTM by taking into account of the aspect, and uses two LSTM networks, a forward one and a backward one, towards the aspect.",4.2 Comparison to Other Methods,[0],[0]
"TDLSTM+ATT extends TDLSTM by incorporating an attention mechanism (Bahdanau et al., 2015) over the hidden vectors.",4.2 Comparison to Other Methods,[0],[0]
"We use the same Glove word vectors for fair comparison.
",4.2 Comparison to Other Methods,[0],[0]
"(4) We also implement ContextAVG, a simplistic version of our approach.",4.2 Comparison to Other Methods,[0],[0]
Context word vectors are averaged and the result is added to the aspect vector.,4.2 Comparison to Other Methods,[0],[0]
"The output is fed to a softmax function.
",4.2 Comparison to Other Methods,[0],[0]
Experimental results are given in Table 2.,4.2 Comparison to Other Methods,[0],[0]
"Our approach using only content attention is abbreviated to MemNet (k), where k is the number of hops.",4.2 Comparison to Other Methods,[0],[0]
"We can find that feature-based SVM is an extremely strong performer and substantially outperforms other baseline methods, which demonstrates the importance of a powerful feature representation for aspect level sentiment classification.",4.2 Comparison to Other Methods,[0],[0]
"Among three recurrent models, TDLSTM performs better than LSTM, which indicates that taking into account of the aspect information is helpful.",4.2 Comparison to Other Methods,[0],[0]
"This is reason-
able as the sentiment polarity of a sentence towards different aspects (e.g. “food” and “service”) might be different.",4.2 Comparison to Other Methods,[0],[0]
It is somewhat disappointing that incorporating attention model over TDLSTM does not bring any improvement.,4.2 Comparison to Other Methods,[0],[0]
We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position.,4.2 Comparison to Other Methods,[0],[0]
"Therefore, the model of TDLSTM+ATT actually selects such mixed semantics of word sequence, which is weird and not an intuitive way to selectively focus on parts of contexts.",4.2 Comparison to Other Methods,[0],[0]
"Different from TDLSTM+ATT, the proposed memory network approach removes the recurrent calculator over word sequence and directly apply attention mechanism on context word representations.
",4.2 Comparison to Other Methods,[0],[0]
"We can also find that the performance of ContextAVG is very poor, which means that assigning the same weight/importance to all the context words is not an effective way.",4.2 Comparison to Other Methods,[0],[0]
"Among all our models from single hop to nine hops, we can observe that using more computational layers could generally lead to better performance, especially when the number of hops is less than six.",4.2 Comparison to Other Methods,[0],[0]
"The best performances are achieved when the model contains seven and nine hops, respectively.",4.2 Comparison to Other Methods,[0],[0]
"On both datasets, the proposed approach could obtain comparable accuracy compared to the state-of-art feature-based SVM system.",4.2 Comparison to Other Methods,[0],[0]
We study the runtime of recurrent neural models and the proposed deep memory network approach with different hops.,4.3 Runtime Analysis,[0],[0]
"We implement all these approaches based on the same neural network infrastructure, use the same 300-dimensional Glove word vectors, and run them on the same CPU server.
",4.3 Runtime Analysis,[0],[0]
The training time of each iteration on the restaurant dataset is given in Table 3.,4.3 Runtime Analysis,[0],[0]
"We can find that LSTM based recurrent models are indeed computationally expensive, which is caused by the complex operations in each LSTM unit along the word sequence.",4.3 Runtime Analysis,[0],[0]
"Instead, the memory network approach is simpler and evidently faster because it does not need recurrent calculators of sequence length.",4.3 Runtime Analysis,[0],[0]
Our approach with nine hops is almost 15 times faster than the basic LSTM model.,4.3 Runtime Analysis,[0],[0]
"As described in Section 3.4, we explore four strategies to integrate location information into the attention model.",4.4 Effects of Location Attention,[0],[0]
We incorporate each of them separately into the basic content-based attention model.,4.4 Effects of Location Attention,[0],[0]
It is helpful to restate that the difference between four location-based attention models lies in the usage of location vectors for context words.,4.4 Effects of Location Attention,[0],[0]
"In Model 1 and Model 2, the values of location vectors are fixed and calculated in a heuristic way.",4.4 Effects of Location Attention,[0],[0]
"In Model 3 and Model 4, location vectors are also regarded as the parameters and jointly learned along with other parameters in the deep memory network.
",4.4 Effects of Location Attention,[0],[0]
Figure 2 shows the classification accuracy of each attention model on the restaurant dataset.,4.4 Effects of Location Attention,[0],[0]
We can find that using multiple computational layers could consistently improve the classification accuracy in all these models.,4.4 Effects of Location Attention,[0],[0]
All these models perform comparably when the number of hops is larger than five.,4.4 Effects of Location Attention,[0],[0]
"Among these four location-based models, we prefer Model 2 as it is intuitive and has less computation cost without loss of accuracy.",4.4 Effects of Location Attention,[0],[0]
"We also find
that Model 4 is very sensitive to the choice of neural gate.",4.4 Effects of Location Attention,[0],[0]
Its classification accuracy decreases by almost 5 percentage when the sigmoid operation over location vector is removed.,4.4 Effects of Location Attention,[0],[0]
We visualize the attention weight of each context word to get a better understanding of the deep memory network approach.,4.5 Visualize Attention Models,[0],[0]
"The results of context-based model and location-based model (Model 2) are given in Table 4 and Table 5, respectively.
From Table 4(a), we can find that in the first hop the context words “great”, “but” and “dreadful” contribute equally to the aspect “service”.",4.5 Visualize Attention Models,[0],[0]
"While after the second hop, the weight of “dreadful” increases and finally the model correctly predict the polarity towards “service” as negative.",4.5 Visualize Attention Models,[0],[0]
This case shows the effects of multiple hops.,4.5 Visualize Attention Models,[0],[0]
"However, in Table 4(b), the content-based model also gives a larger weight to “dreadful” when the target we focus on is “food”.",4.5 Visualize Attention Models,[0],[0]
"As a result, the model incorrectly predicts the polarity towards “food” as negative.",4.5 Visualize Attention Models,[0],[0]
This phenomenon might be caused by the neglect of location information.,4.5 Visualize Attention Models,[0],[0]
"From Table 5(b), we can find that the weight
of “great” is increased when the location of context word is considered.",4.5 Visualize Attention Models,[0],[0]
"Accordingly, Model 2 predicts the correct sentiment label towards “food”.",4.5 Visualize Attention Models,[0],[0]
We believe that location-enhanced model captures both content and location information.,4.5 Visualize Attention Models,[0],[0]
"For instance, in Table 5(a) the closest context words of the aspect “service” are “the” and “was”, while “dreadful” has the largest weight.",4.5 Visualize Attention Models,[0],[0]
"We carry out an error analysis of our location enhanced model (Model 2) on the restaurant dataset, and find that most of the errors could be summarized as follows.",4.6 Error Analysis,[0],[0]
The first factor is noncompositional sentiment expression.,4.6 Error Analysis,[0],[0]
This model regards single context word as the basic computational unit and cannot handle this situation.,4.6 Error Analysis,[0],[0]
"An example is “dessert was also to die for!”, where the aspect is underlined.",4.6 Error Analysis,[0],[0]
"The sentiment expression is “die for”, whose meaning could not be composed from its constituents “die” and “for”.",4.6 Error Analysis,[0],[0]
"The second factor is complex aspect expression consisting of many words, such as “ask for the round corner table next to the large window.”",4.6 Error Analysis,[0],[0]
"This model represents an aspect expression by averaging its constituting word vectors, which could not well handle this situation.",4.6 Error Analysis,[0],[0]
"The third factor is sentimental relation between context words such as negation, comparison and condition.",4.6 Error Analysis,[0],[0]
"An example is “but dinner here is never disappointing, even if the prices are a bit over the top”.",4.6 Error Analysis,[0],[0]
We believe that this is caused by the weakness of weighted average compositional function in each hop.,4.6 Error Analysis,[0],[0]
There are also cases when comparative opinions are expressed such as “i ’ve had better japanese food at a mall food court”.,4.6 Error Analysis,[0],[0]
This work is connected to three research areas in natural language processing.,5 Related Work,[0],[0]
We briefly describe related studies in each area.,5 Related Work,[0],[0]
"Aspect level sentiment classification is a finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014).",5.1 Aspect Level Sentiment Classification,[0],[0]
"Most existing works use machine learning algorithms, and build sentiment classifier from
sentences with manually annotated polarity labels.",5.1 Aspect Level Sentiment Classification,[0],[0]
One of the most successful approaches in literature is feature based SVM.,5.1 Aspect Level Sentiment Classification,[0],[0]
"Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014).",5.1 Aspect Level Sentiment Classification,[0],[0]
"In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data.",5.1 Aspect Level Sentiment Classification,[0],[0]
"However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect.",5.1 Aspect Level Sentiment Classification,[0],[0]
"Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect.",5.1 Aspect Level Sentiment Classification,[0],[0]
"It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015).",5.1 Aspect Level Sentiment Classification,[0],[0]
The aspect word in this work is given as a part of the input.,5.1 Aspect Level Sentiment Classification,[0],[0]
"In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892).",5.2 Compositionality in Vector Space,[0],[0]
Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector.,5.2 Compositionality in Vector Space,[0],[0]
Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases.,5.2 Compositionality in Vector Space,[0],[0]
"To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Schütze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015).",5.2 Compositionality in Vector Space,[0],[0]
"Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016).",5.2 Compositionality in Vector Space,[0],[0]
"Recently, there is a resurgence in computational models with attention mechanism and explicit mem-
ory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015).",5.3 Attention and Memory Networks,[0],[0]
"In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks.",5.3 Attention and Memory Networks,[0],[0]
"Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “where to look” by assigning a weight/importance to each lower position when computing an upper level representation.",5.3 Attention and Memory Networks,[0],[0]
"Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015).",5.3 Attention and Memory Networks,[0],[0]
We develop deep memory networks that capture importances of context words for aspect level sentiment classification.,6 Conclusion,[0],[0]
"Compared with recurrent neural models like LSTM, this approach is simpler and faster.",6 Conclusion,[0],[0]
"Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures.",6 Conclusion,[0],[0]
We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text representation.,6 Conclusion,[0],[0]
We also demonstrate that using multiple computational layers in memory network could obtain improved performance.,6 Conclusion,[0],[0]
Our potential future plans are incorporating sentence structure like parsing results into the deep memory network.,6 Conclusion,[0],[0]
We would especially want to thank Xiaodan Zhu for running their system on our setup.,Acknowledgments,[0],[0]
We greatly thank Yaming Sun for tremendously helpful discussions.,Acknowledgments,[0],[0]
We also thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
"This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61632011 and No.61273321).",Acknowledgments,[0],[0]
We introduce a deep memory network for aspect level sentiment classification.,abstractText,[0],[0]
"Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect.",abstractText,[0],[0]
"Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory.",abstractText,[0],[0]
"Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures.",abstractText,[0],[0]
On both datasets we show that multiple computational layers could improve the performance.,abstractText,[0],[0]
"Moreover, our approach is also fast.",abstractText,[0],[0]
The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.,abstractText,[0],[0]
Aspect Level Sentiment Classification with Deep Memory Network,title,[0],[0]
"We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.1",text,[0],[0]
Many NLP problems are naturally multitask classification problems.,1 Introduction,[0],[0]
"For instance, values extracted for different fields from the same document are often dependent as they share the same context.",1 Introduction,[0],[0]
Existing systems rely on this dependence (transfer across fields) to improve accuracy.,1 Introduction,[0],[0]
"In this paper, we consider a version of this problem where there is a clear dependence between two tasks but annotations are available only for the source task.",1 Introduction,[0],[0]
"For example,
1The code is available at https://github.com/ yuanzh/aspect_adversarial.
",1 Introduction,[0],[0]
the target goal may be to classify pathology reports (shown in Figure 1) for the presence of lymph invasion but training data are available only for carcinoma in the same reports.,1 Introduction,[0],[0]
"We call this problem aspect transfer as the objective is to learn to classify examples differently, focusing on different aspects, without access to target aspect labels.",1 Introduction,[0],[0]
"Clearly, such transfer learning is possible only with auxiliary information relating the tasks together.
",1 Introduction,[0],[0]
The key challenge is to articulate and incorporate commonalities across the tasks.,1 Introduction,[0],[0]
"For instance, in classifying reviews of different products, sentiment words (referred to as pivots) can be shared across the products.",1 Introduction,[0],[0]
"This commonality enables one to align feature spaces across multiple products, enabling useful transfer (Blitzer et al., 2006).",1 Introduction,[0],[0]
"Similar properties hold in other contexts and beyond senti-
ar X
iv :1
70 1.
00 18
8v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
5 Se
p 20
ment analysis.",1 Introduction,[0],[0]
"Figure 1 shows that certain words and phrases like “identified”, which indicates the presence of a histological property, are applicable to both carcinoma and lymph invasion.",1 Introduction,[0],[0]
"Our method learns and relies on such shared indicators, and utilizes them for effective transfer.
",1 Introduction,[0],[0]
"The unique feature of our transfer problem is that both the source and the target classifiers operate over the same domain, i.e., the same examples.",1 Introduction,[0],[0]
"In this setting, traditional transfer methods will always predict the same label for both aspects and thus leading to failure.",1 Introduction,[0],[0]
"Instead of supplying the target classifier with direct training labels, our approach builds on a secondary relationship between the tasks using aspect-relevance annotations of sentences.",1 Introduction,[0],[0]
"These relevance annotations indicate a possibility that the answer could be found in a sentence, not what the answer is.",1 Introduction,[0],[0]
"One can often write simple keyword rules that identify sentence relevance to a particular aspect through representative terms, e.g., specific hormonal markers in the context of pathology reports.",1 Introduction,[0],[0]
"Annotations of this kind can be readily provided by domain experts, or extracted from medical literature such as codex rules in pathology (Pantanowitz et al., 2008).",1 Introduction,[0],[0]
We assume a small number of relevance annotations (rules) pertaining to both source and target aspects as a form of weak supervision.,1 Introduction,[0],[0]
"We use this sentence-level aspect relevance to learn how to encode the examples (e.g., pathology reports) from the point of view of the desired aspect.",1 Introduction,[0],[0]
"In our approach, we construct different aspect-dependent encodings of the same document by softly selecting sentences relevant to the aspect of interest.",1 Introduction,[0],[0]
"The key to effective transfer is how these encodings are aligned.
",1 Introduction,[0],[0]
"This encoding mechanism brings the problem closer to the realm of standard domain adaptation, where the derived aspect-specific representations are considered as different domains.",1 Introduction,[0],[0]
"Given these representations, our method learns a label classifier shared between the two domains.",1 Introduction,[0],[0]
"To ensure that it can be adjusted only based on the source class labels, and that it also reasonably applies to the target encodings, we must align the two sets of encoded examples.2 Learning this alignment is pos-
2This alignment or invariance is enforced on the level of sets, not individual reports; aspect-driven encoding of any specific report should remain substantially different for the two tasks since the encoded examples are passed on to the same classifier.
sible because, as discussed above, some keywords are directly transferable and can serve as anchors for constructing this invariant space.",1 Introduction,[0],[0]
"To learn this invariant representation, we introduce an adversarial domain classifier analogous to the recent successful use of adversarial training in computer vision (Ganin and Lempitsky, 2014).",1 Introduction,[0],[0]
The role of the domain classifier (adversary) is to learn to distinguish between the two types of encodings.,1 Introduction,[0],[0]
During training we update the encoder with an adversarial objective to cause the classifier to fail.,1 Introduction,[0],[0]
"The encoder therefore learns to eliminate aspect-specific information so that encodings look invariant (as sets) to the classifier, thus establishing aspect-invariance encodings and enabling transfer.",1 Introduction,[0],[0]
"All three components in our approach, 1) aspect-driven encoding, 2) classification of source labels, and 3) domain adversary, are trained jointly (concurrently) to complement and balance each other.
",1 Introduction,[0],[0]
Adversarial training of domain and label classifiers can be challenging to stabilize.,1 Introduction,[0],[0]
"In our setting, sentences are encoded with a convolutional model.",1 Introduction,[0],[0]
Feedback from adversarial training can be an unstable guide for how the sentences should be encoded.,1 Introduction,[0],[0]
"To address this issue, we incorporate an additional word-level auto-encoder reconstruction loss to ground the convolutional processing of sentences.",1 Introduction,[0],[0]
"We empirically demonstrate that this additional objective yields richer and more diversified feature representations, improving transfer.
",1 Introduction,[0],[0]
We evaluate our approach on pathology reports (aspect transfer) as well as on a more standard review dataset (domain adaptation).,1 Introduction,[0],[0]
"On the pathology dataset, we explore cross-aspect transfer across different types of breast disease.",1 Introduction,[0],[0]
"Specifically, we test on six adaptation tasks, consistently outperforming all other baselines.",1 Introduction,[0],[0]
"Overall, our full model achieves 27% and 20.2% absolute improvement arising from aspect-driven encoding and adversarial training respectively.",1 Introduction,[0],[0]
"Moreover, our unsupervised adaptation method is only 5.7% behind the accuracy of a supervised target model.",1 Introduction,[0],[0]
"On the review dataset, we test adaptations from hotel to restaurant reviews.",1 Introduction,[0],[0]
"Our model outperforms the marginalized denoising autoencoder (Chen et al., 2012) by 5%.",1 Introduction,[0],[0]
"Finally, we examine and illustrate the impact of individual components on the resulting performance.",1 Introduction,[0],[0]
"Domain Adaptation for Deep Learning Existing approaches commonly induce abstract representations without pulling apart different aspects in the same example, and therefore are likely to fail on the aspect transfer problem.",2 Related Work,[0],[0]
"The majority of these prior methods first learn a task-independent representation, and then train a label predictor (e.g. SVM) on this representation in a separate step.",2 Related Work,[0],[0]
"For example, earlier researches employ a shared autoencoder (Glorot et al., 2011; Chopra et al., 2013) to learn a cross-domain representation.",2 Related Work,[0],[0]
Chen et al. (2012) further improve and stabilize the representation learning by utilizing marginalized denoising autoencoders.,2 Related Work,[0],[0]
"Later, Zhou et al. (2016) propose to minimize domain-shift of the autoencoder in a linear data combination manner.",2 Related Work,[0],[0]
Other researches have focused on learning transferable representations in an end-to-end fashion.,2 Related Work,[0],[0]
"Examples include using transduction learning for object recognition (Sener et al., 2016) and using residual transfer networks for image classification (Long et al., 2016).",2 Related Work,[0],[0]
"In contrast, we use adversarial training to encourage learning domaininvariant features in a more explicit way.",2 Related Work,[0],[0]
Our approach offers another two advantages over prior work.,2 Related Work,[0],[0]
"First, we jointly optimize features with the final classification task while many previous works only learn task-independent features using autoencoders.",2 Related Work,[0],[0]
"Second, our model can handle traditional domain transfer as well as aspect transfer, while previous methods can only handle the former.
",2 Related Work,[0],[0]
Adversarial Learning in Vision and NLP Our approach closely relates to the idea of domainadversarial training.,2 Related Work,[0],[0]
"Adversarial networks were originally developed for image generation (Goodfellow et al., 2014; Makhzani et al., 2015; Springenberg, 2015; Radford et al., 2015; Taigman et al., 2016), and were later applied to domain adaptation in computer vision (Ganin and Lempitsky, 2014; Ganin et al., 2015; Bousmalis et al., 2016; Tzeng et al., 2014) and speech recognition (Shinohara, 2016).",2 Related Work,[0],[0]
The core idea of these approaches is to promote the emergence of invariant image features by optimizing the feature extractor as an adversary against the domain classifier.,2 Related Work,[0],[0]
"While Ganin et al. (2015) also apply this idea to sentiment analysis, their practical gains have remained limited.
",2 Related Work,[0],[0]
Our approach presents two main departures.,2 Related Work,[0],[0]
"In computer vision, adversarial learning has been used for transferring across domains, while our method can also handle aspect transfer.",2 Related Work,[0],[0]
"In addition, we introduce a reconstruction loss which results in more robust adversarial training.",2 Related Work,[0],[0]
"We believe that this formulation will benefit other applications of adversarial training, beyond the ones described in this paper.
",2 Related Work,[0],[0]
"Semi-supervised Learning with Keywords In our work, we use a small set of keywords as a source of weak supervision for aspect-relevance scoring.",2 Related Work,[0],[0]
"This relates to prior work on utilizing prototypes and seed words in semi-supervised learning (Haghighi and Klein, 2006; Grenager et al., 2005; Chang et al., 2007; Mann and McCallum, 2008; Jagarlamudi et al., 2012; Li et al., 2012; Eisenstein, 2017).",2 Related Work,[0],[0]
All these prior approaches utilize prototype annotations primarily targeting model bootstrapping but not for learning representations.,2 Related Work,[0],[0]
"In contrast, our model uses provided keywords to learn aspect-driven encoding of input examples.
",2 Related Work,[0],[0]
"Attention Mechanism in NLP One may view our aspect-relevance scorer as a sentence-level “semi-supervised attention”, in which relevant sentences receive more attention during feature extraction.",2 Related Work,[0],[0]
"While traditional attention-based models typically induce attention in an unsupervised manner, they have to rely on a large amount of labeled data for the target task (Bahdanau et al., 2014; Rush et al., 2015; Chen et al., 2015; Cheng et al., 2016; Xu et al., 2015; Xu and Saenko, 2015; Yang et al., 2015; Martins and Astudillo, 2016; Lei et al., 2016).",2 Related Work,[0],[0]
"Unlike these methods, our approach assumes no label annotations in the target domain.",2 Related Work,[0],[0]
"Other researches have focused on utilizing human-provided rationales as “supervised attention” to improve prediction (Zaidan et al., 2007; Marshall et al., 2015; Zhang et al., 2016; Brun et al., 2016).",2 Related Work,[0],[0]
"In contrast, our model only assumes access to a small set of keywords as a source of weak supervision.",2 Related Work,[0],[0]
"Moreover, all these prior approaches focus on in-domain classification.",2 Related Work,[0],[0]
"In this paper, however, we study the task in the context of domain adaptation.
",2 Related Work,[0],[0]
Multitask Learning,2 Related Work,[0],[0]
Existing multitask learning methods focus on the case where supervision is available for all tasks.,2 Related Work,[0],[0]
"A typical architecture involves using a shared encoder with a separate clas-
sifier for each task.",2 Related Work,[0],[0]
"(Caruana, 1998; Pan and Yang, 2010; Collobert and Weston, 2008; Liu et al., 2015; Bordes et al., 2012).",2 Related Work,[0],[0]
"In contrast, our work assumes labeled data only for the source aspect.",2 Related Work,[0],[0]
We train a single classifier for both aspects by learning aspectinvariant representation that enables the transfer.,2 Related Work,[0],[0]
We begin by formalizing aspect transfer with the idea of differentiating it from standard domain adaptation.,3 Problem Formulation,[0],[0]
"In our setup, we have two classification tasks called the source and the target tasks.",3 Problem Formulation,[0],[0]
"In contrast to source and target tasks in domain adaptation, both of these tasks are defined over the same set of examples (here documents, e.g., pathology reports).",3 Problem Formulation,[0],[0]
What differentiates the two classification tasks is that they pertain to different aspects in the examples.,3 Problem Formulation,[0],[0]
"If each training document were annotated with both the source and the target aspect labels, the problem would reduce to multi-label classification.",3 Problem Formulation,[0],[0]
"However, in our setting training labels are available only for the source aspect so the goal is to solve the target task without any associated training label.
",3 Problem Formulation,[0],[0]
"To fix notation, let d = {si}|d|i=1 be a document that consists of a sequence of |d| sentences si.",3 Problem Formulation,[0],[0]
"Given a document d, and the aspect of interest, we wish to predict the corresponding aspect-dependent class label y (e.g., y ∈ {−1, 1}).",3 Problem Formulation,[0],[0]
We assume that the set of possible labels are the same across aspects.,3 Problem Formulation,[0],[0]
We use ysl;k to denote the k-th coordinate of a one-hot vector indicating the correct training source aspect label for document dl.,3 Problem Formulation,[0],[0]
"Target aspect labels are not available during training.
",3 Problem Formulation,[0],[0]
"Beyond labeled documents for the source aspect {dl, ysl }l∈L, and shared unlabeled documents for source and target aspects {dl}l∈U , we assume further that we have relevance scores pertaining to each aspect.",3 Problem Formulation,[0],[0]
"The relevance is given per sentence, for some subset of sentences across the documents, and indicates the possibility that the answer for that document would be found in the sentence but without indicating which way the answer goes.",3 Problem Formulation,[0],[0]
"Relevance is always aspect dependent yet often easy to provide in the form of simple keyword rules.
",3 Problem Formulation,[0],[0]
"We use rai ∈ {0, 1} to denote the given relevance label pertaining to aspect a for sentence si.",3 Problem Formulation,[0],[0]
"Only a small subset of sentences in the training set have as-
sociated relevance labels.",3 Problem Formulation,[0],[0]
"Let R = {(a, l, i)} denote the index set of relevance labels such that if (a, l, i) ∈ R then aspect a’s relevance label ral,i is available for the ith sentence in document dl.",3 Problem Formulation,[0],[0]
In our case relevance labels arise from aspect-dependent keyword matches.,3 Problem Formulation,[0],[0]
"rai = 1 when the sentence contains any keywords pertaining to aspect a and rai = 0 if it has any keywords of other aspects.3 Separate subsets of relevance labels are available for each aspect as the keywords differ.
",3 Problem Formulation,[0],[0]
The transfer that is sought here is between two tasks over the same set of examples rather than between two different types of examples for the same task as in standard domain adaptation.,3 Problem Formulation,[0],[0]
"However, the two formulations can be reconciled if full relevance annotations are assumed to be available during training and testing.",3 Problem Formulation,[0],[0]
"In this scenario, we could simply lift the sets of relevant sentences from each document as new types of documents.",3 Problem Formulation,[0],[0]
"The goal would be then to learn to classify documents of type T (consisting of sentences relevant to the target aspect) based on having labels only for type S (source) documents, a standard domain adaptation task.",3 Problem Formulation,[0],[0]
"Our problem is more challenging as the aspect-relevance of sentences must be learned from limited annotations.
",3 Problem Formulation,[0],[0]
"Finally, we note that the aspect transfer problem and the method we develop to solve it work the same even when source and target documents are a priori different, something we will demonstrate later.",3 Problem Formulation,[0],[0]
Our model consists of three key components as shown in Figure 2.,4.1 Overview of our approach,[0],[0]
"Each document is encoded in a relevance weighted, aspect-dependent manner (green, left part of Figure 2) and classified using the label predictor (blue, top-right).",4.1 Overview of our approach,[0],[0]
"During training, the encoded documents are also passed on to the domain classifier (orange, bottom-right).",4.1 Overview of our approach,[0],[0]
"The role of the domain classifier, as the adversary, is to ensure that the aspect-dependent encodings of documents are distributionally matched.",4.1 Overview of our approach,[0],[0]
"This matching justifies the use of the same end-classifier to provide the predicted label regardless of the task (aspect).
",4.1 Overview of our approach,[0],[0]
"3rai = 1 if the sentence contains keywords pertaining to both aspect a and other aspects.
",4.1 Overview of our approach,[0],[0]
"r = 1.0
r = 0.0r = 0.9
To encode a document, the model first maps each sentence into a vector and then passes the vector to a scoring network to determine whether the sentence is relevant for the chosen aspect.",4.1 Overview of our approach,[0],[0]
These predicted relevance scores are used to obtain document vectors by taking relevance-weighted sum of the associated sentence vectors.,4.1 Overview of our approach,[0],[0]
"Thus, the manner in which the document vector is constructed is always aspectdependent due to the chosen relevance weights.
",4.1 Overview of our approach,[0],[0]
"During training, the resulting adjusted document vectors are consumed by the two classifiers.",4.1 Overview of our approach,[0],[0]
"The primary label classifier aims to predict the source labels (when available), while the domain classifier determines whether the document vector pertains to the source or target aspect, which is the label that we know by construction.",4.1 Overview of our approach,[0],[0]
"Furthermore, we jointly update the document encoder with a reverse of the gradient from the domain classifier, so that the encoder learns to induce document representations that fool the domain classifier.",4.1 Overview of our approach,[0],[0]
"The resulting encoded representations will be aspect-invariant, facilitating transfer.
",4.1 Overview of our approach,[0],[0]
Our adversarial training scheme uses all the training losses concurrently to adjust the model parameters.,4.1 Overview of our approach,[0],[0]
"During testing, we simply encode each test document in a target-aspect dependent manner, and apply the same label predictor.",4.1 Overview of our approach,[0],[0]
"We expect that the same label classifier does well on the target task since it solves the source task, and operates on relevance-weighted representations that are matched across the tasks.",4.1 Overview of our approach,[0],[0]
"While our method is designed to work in the extreme setting that the examples for the two aspects are the same, this is by no means a re-
quirement.",4.1 Overview of our approach,[0],[0]
"Our method will also work fine in the more traditional domain adaptation setting, which we will demonstrate later.",4.1 Overview of our approach,[0],[0]
Sentence embedding We apply a convolutional model illustrated in Figure 3 to each sentence si to obtain sentence-level vector embeddings xseni .,4.2 Components in detail,[0],[0]
"The use of RNNs or bi-LSTMs would result in more flexible sentence embeddings but based on our initial experiments, we did not observe any significant gains over the simpler CNNs.
",4.2 Components in detail,[0],[0]
We further ground the resulting sentence embeddings by including an additional word-level reconstruction step in the convolutional model.,4.2 Components in detail,[0],[0]
The purpose of this reconstruction step is to balance adversarial training signals propagating back from the domain classifier.,4.2 Components in detail,[0],[0]
"Specifically, it forces the sentence encoder to keep rich word-level information in contrast to adversarial training that seeks to eliminate aspect specific features.",4.2 Components in detail,[0],[0]
"We provide an empirical analysis of the impact of this reconstruction in the
experiment section (Section 7).",4.2 Components in detail,[0],[0]
"More concretely, we reconstruct word embedding from the corresponding convolutional layer, as shown in Figure 3.4 We use xi,j to denote the embedding of the j-th word in sentence si.",4.2 Components in detail,[0],[0]
"Let hi,j be the convolutional output when xi,j is at the center of the window.",4.2 Components in detail,[0],[0]
"We reconstruct xi,j by
x̂i,j = tanh(W chi,j + b c) (1)
where Wc and bc are parameters of the reconstruction layer.",4.2 Components in detail,[0],[0]
"The loss associated with the reconstruction for document d is
Lrec(d)",4.2 Components in detail,[0],[0]
= 1 n ∑,4.2 Components in detail,[0],[0]
"i,j ||x̂i,j − tanh(xi,j)||22 (2)
where n is the number of tokens in the document and indexes",4.2 Components in detail,[0],[0]
"i, j identify the sentence and word, respectively.",4.2 Components in detail,[0],[0]
"The overall reconstruction loss Lrec is obtained by summing over all labeled/unlabeled documents.
",4.2 Components in detail,[0],[0]
"Relevance prediction We use a small set of keyword rules to generate binary relevance labels, both positive (r = 1) and negative (r = 0).",4.2 Components in detail,[0],[0]
These labels represent the only supervision available to predict relevance.,4.2 Components in detail,[0],[0]
The prediction is made on the basis of the sentence vector xseni passed through a feedforward network with a ReLU output unit.,4.2 Components in detail,[0],[0]
The network has a single shared hidden layer and a separate output layer for each aspect.,4.2 Components in detail,[0],[0]
"Note that our relevance prediction network is trained as a non-negative regression model even though the available labels are binary, as relevance varies more on a linear rather than binary scale.
",4.2 Components in detail,[0],[0]
"Given relevance labels indexed by R = {(a, l, i)}, we minimize
Lrel = ∑
(a,l,i)∈R
( ral,i − r̂al,i )2 (3) where r̂al,i is the predicted (non-negative) relevance score pertaining to aspect a for the ith sentence in document dl, as shown in the left part of Figure 2.",4.2 Components in detail,[0],[0]
"ral,i, defined earlier, is the given binary (0/1) relevance label.",4.2 Components in detail,[0],[0]
"We use a score in [0, 1] scale because it can be naturally used as a weight for vector combinations, as shown next.
4",4.2 Components in detail,[0],[0]
"This process is omitted in Figure 2 for brevity.
",4.2 Components in detail,[0],[0]
"Document encoding The initial vector representation for each document such as dl is obtained as a relevance weighted combination of the associated sentence vectors, i.e.,
xdoc,al =
∑ i r̂
a l,i · xsenl,i∑ i r̂ a l,i
(4)
The resulting vector selectively encodes information from the sentences based on relevance to the focal aspect.
",4.2 Components in detail,[0],[0]
Transformation layer The manner in which document vectors arise from sentence vectors means that they will retain aspect-specific information that will hinder transfer across aspects.,4.2 Components in detail,[0],[0]
"To help remove non-transferable information, we add a transformation layer to map the initial document vectors xdoc,al to their domain invariant (as a set) versions, as shown in Figure 2.",4.2 Components in detail,[0],[0]
"Specifically, the transformed representation is given by xtr,al = W
trxdoc,al . Meanwhile, the transformation has to be strongly regularized lest the gradient from the adversary would wipe out all the document signal.",4.2 Components in detail,[0],[0]
"We add the following regularization term
Ωtr = λtr||Wtr",4.2 Components in detail,[0],[0]
"− I||2F (5)
to discourage significant deviation away from identity I. λtr is a regularization parameter that has to be set separately based on validation performance.",4.2 Components in detail,[0],[0]
"We show an empirical analysis of the impact of this transformation layer in Section 7.
",4.2 Components in detail,[0],[0]
"Primary label classifier As shown in the topright part of Figure 2, the classifier takes in the adjusted document representation as an input and predicts a probability distribution over the possible class labels.",4.2 Components in detail,[0],[0]
The classifier is a feed-forward network with a single hidden layer using ReLU activations and a softmax output layer over the possible class labels.,4.2 Components in detail,[0],[0]
Note that we train only one label classifier that is shared by both aspects.,4.2 Components in detail,[0],[0]
The classifier operates the same regardless of the aspect to which the document was encoded.,4.2 Components in detail,[0],[0]
"It must therefore be cooperatively learned together with the encodings.
",4.2 Components in detail,[0],[0]
Let p̂l;k denote the predicted probability of class k for document dl when the document is encoded from the point of view of the source aspect.,4.2 Components in detail,[0],[0]
"Recall that [ysl;1, . . .",4.2 Components in detail,[0],[0]
", y s l;m] is a one-hot vector for the correct
(given) source class label for document dl, hence also a distribution.",4.2 Components in detail,[0],[0]
"We use the cross-entropy loss for the label classifier
Llab = ∑ l∈L
[ −
m∑ k=1 ysl;k log p̂l;k
] (6)
Domain classifier As shown in the bottomright part of Figure 2, the domain classifier functions as an adversary to ensure that the documents encoded with respect to the source and target aspects look the same as sets of examples.",4.2 Components in detail,[0],[0]
The invariance is achieved when the domain classifier (as the adversary) fails to distinguish between the two.,4.2 Components in detail,[0],[0]
"Structurally, the domain classifier is a feed-forward network with a single ReLU hidden layer and a softmax output layer over the two aspect labels.
",4.2 Components in detail,[0],[0]
Let ya =,4.2 Components in detail,[0],[0]
"[ya1 , y a 2 ] denote the one-hot domain label vector for aspect a ∈ {s, t}.",4.2 Components in detail,[0],[0]
"In other words, ys =",4.2 Components in detail,[0],[0]
"[1, 0] and yt =",4.2 Components in detail,[0],[0]
"[0, 1].",4.2 Components in detail,[0],[0]
"We use q̂k(x tr,a l ) as the predicted probability that the domain label is k when the domain classifier receives xtr,al as the input.",4.2 Components in detail,[0],[0]
"The domain classifier is trained to minimize
Ldom = ∑
l∈L∪U ∑ a∈{s,t}
[ −
2∑ k=1 yak log q̂k(x tr,a l ) ]",4.2 Components in detail,[0],[0]
(7),4.2 Components in detail,[0],[0]
"We combine the individual component losses pertaining to word reconstruction, relevance labels, transformation layer regularization, source class labels, and domain adversary into an overall objective function
Lall = Lrec + Lrel + Ωtr + Llab − ρLdom (8)
which is minimized with respect to the model parameters except for the adversary (domain classifier).",4.3 Joint learning,[0],[0]
The adversary is maximizing the same objective with respect to its own parameters.,4.3 Joint learning,[0],[0]
The last term −ρLdom corresponds to the objective of causing the domain classifier to fail.,4.3 Joint learning,[0],[0]
"The proportionality constant ρ controls the impact of gradients from the adversary on the document representation; the adversary itself is always directly minimizing Ldom.
",4.3 Joint learning,[0],[0]
All the parameters are optimized jointly using standard backpropagation (concurrent for the adversary).,4.3 Joint learning,[0],[0]
"Each mini-batch is balanced by aspect, half
coming from the source, the other half from the target.",4.3 Joint learning,[0],[0]
All the loss functions except Llab make use of both labeled and unlabeled documents.,4.3 Joint learning,[0],[0]
"Additionally, it would be straightforward to add a loss term for target labels if they are available.",4.3 Joint learning,[0],[0]
"Pathology dataset This dataset contains 96.6k breast pathology reports collected from three hospitals (Yala et al., 2016).",5 Experimental Setup,[0],[0]
"A portion of this dataset is manually annotated with 20 categorical values, representing various aspects of breast disease.",5 Experimental Setup,[0],[0]
"In our experiments, we focus on four aspects related to carcinomas and atypias:",5 Experimental Setup,[0],[0]
"Ductal Carcinoma InSitu (DCIS), Lobular Carcinoma In-Situ (LCIS), Invasive Ductal Carcinoma (IDC) and Atypical Lobular Hyperplasia (ALH).",5 Experimental Setup,[0],[0]
Each aspect is annotated using binary labels.,5 Experimental Setup,[0],[0]
"We use 500 held out reports as our test set and use the rest of the labeled data as our training set: 23.8k reports for DCIS, 10.7k for LCIS, 22.9k for IDC, and 9.2k for ALH.",5 Experimental Setup,[0],[0]
"Table 1 summarizes statistics of the dataset.
",5 Experimental Setup,[0],[0]
We explore the adaptation problem from one aspect to another.,5 Experimental Setup,[0],[0]
"For example, we want to train a model on annotations of DCIS and apply it on LCIS.",5 Experimental Setup,[0],[0]
"For each aspect, we use up to three common names
as a source of supervision for learning the relevance scorer, as illustrated in Table 2.",5 Experimental Setup,[0],[0]
Note that the provided list is by no means exhaustive.,5 Experimental Setup,[0],[0]
"In fact Buckley et al. (2012) provide example of 60 different verbalizations of LCIS, not counting negations.
",5 Experimental Setup,[0],[0]
Review dataset Our second experiment is based on a domain transfer of sentiment classification.,5 Experimental Setup,[0],[0]
"For the source domain, we use the hotel review dataset introduced in previous work (Wang et al., 2010; Wang et al., 2011), and for the target domain, we use the restaurant review dataset from Yelp.5 Both datasets have ratings on a scale of 1 to 5 stars.",5 Experimental Setup,[0],[0]
"Following previous work (Blitzer et al., 2007), we label reviews with ratings > 3 as positive and those with ratings < 3 as negative, discarding the rest.",5 Experimental Setup,[0],[0]
"The hotel dataset includes a total of around 200k reviews collected from TripAdvisor,6 so we split 100k as labeled and the other 100k as unlabeled data.",5 Experimental Setup,[0],[0]
We randomly select 200k restaurant reviews as the unlabeled data in the target domain.,5 Experimental Setup,[0],[0]
Our test set consists of 2k reviews.,5 Experimental Setup,[0],[0]
"Table 1 summarizes the statistics of the review dataset.
",5 Experimental Setup,[0],[0]
"The hotel reviews naturally have ratings for six aspects, including value, room quality, checkin service, room service, cleanliness and location.",5 Experimental Setup,[0],[0]
We use the first five aspects because the sixth aspect location has positive labels for over 95% of the reviews and thus the trained model will suffer from the lack of negative examples.,5 Experimental Setup,[0],[0]
"The restaurant reviews, however, only have single ratings for an overall impression.",5 Experimental Setup,[0],[0]
"Therefore, we explore the task of adaptation from each of the five hotel aspects to the restaurant domain.",5 Experimental Setup,[0],[0]
The hotel reviews dataset also provides a total of 280 keywords for different aspects that are generated by the bootstrapping method used in Wang et al. (2010).,5 Experimental Setup,[0],[0]
"We use those keywords as supervision for learning the relevance scorer.
",5 Experimental Setup,[0],[0]
Baselines and our method We first compare against a linear SVM trained on the raw bagof-words representation of labeled data in source.,5 Experimental Setup,[0],[0]
"Second, we compare against our SourceOnly model that assumes no target domain data or keywords.",5 Experimental Setup,[0],[0]
It thus has no adversarial training or target aspect-relevance scoring.,5 Experimental Setup,[0],[0]
"Next we compare
5The restaurant portion of https://www.yelp.com/ dataset_challenge.
6https://www.tripadvisor.com/
with marginalized Stacked Denoising Autoencoders (mSDA) (Chen et al., 2012), a domain adaptation algorithm that outperforms both prior deep learning and shallow learning approaches.7
In the rest part of the paper, we name our method and its variants as AAN (Aspect-augmented Adversarial Networks).",5 Experimental Setup,[0],[0]
We compare against AANNA and AAN-NR that are our model variants without adversarial training and without aspectrelevance scoring respectively.,5 Experimental Setup,[0],[0]
Finally we include supervised models trained on the full set of In-Domain annotations as the performance upper bound.,5 Experimental Setup,[0],[0]
Table 3 summarizes the usage of labeled and unlabeled data in each domain as well as keyword rules by our model (AAN-Full) and different baselines.,5 Experimental Setup,[0],[0]
"Note that our model assumes the same set of data as the AAN-NA, AAN-NR and mSDA methods.
",5 Experimental Setup,[0],[0]
Implementation details,5 Experimental Setup,[0],[0]
"Following prior work (Ganin and Lempitsky, 2014), we gradually increase the adversarial strength ρ and decay the learning rate during training.",5 Experimental Setup,[0],[0]
"We also apply batch normalization (Ioffe and Szegedy, 2015) on the sentence encoder and apply dropout with ratio 0.2 on word embeddings and each hidden layer activation.",5 Experimental Setup,[0],[0]
"We set the hidden layer size to 150 and pick the transformation regularization weight λt = 0.1 for the pathol-
7We use the publicly available implementation provided by the authors at http://www.cse.wustl.edu/˜mchen/ code/mSDA.tar.",5 Experimental Setup,[0],[0]
"We use the hyper-parameters from the authors and their models have more parameters than ours.
ogy dataset and λt = 10.0 for the review dataset.",5 Experimental Setup,[0],[0]
"Table 4 summarizes the classification accuracy of different methods on the pathology dataset, including the results of twelve adaptation tasks.",6 Main Results,[0],[0]
Our full model (AAN-Full) consistently achieves the best performance on each task compared with other baselines and model variants.,6 Main Results,[0],[0]
"It is not surprising that SVM and mSDA perform poorly on this dataset because they only predict labels based on an overall feature representation of the input, and do not utilize weak supervision provided by aspect-specific keywords.",6 Main Results,[0],[0]
"As a reference, we also provide a performance upper bound by training our model on the full labeled set in the target domain, denoted as InDomain in the last column of Table 4.",6 Main Results,[0],[0]
"On average, the accuracy of our model (AAN-Full) is only 5.7% behind this upper bound.
",6 Main Results,[0],[0]
"Table 5 shows the adaptation results from each aspect in the hotel reviews to the overall ratings of
restaurant reviews.",6 Main Results,[0],[0]
"AAN-Full and AAN-NR are the two best performing systems on this review dataset, attaining around 5% improvement over the mSDA baseline.",6 Main Results,[0],[0]
"Below, we summarize our findings when comparing the full model with the two model variants AAN-NA and AAN-NR.
Impact of adversarial training We first focus on comparisons between AAN-Full and AAN-NA.",6 Main Results,[0],[0]
The only difference between the two models is that AAN-NA has no adversarial training.,6 Main Results,[0],[0]
"On the pathology dataset, our model significantly outperforms AAN-NA, yielding a 20.2% absolute average gain (see Table 4).",6 Main Results,[0],[0]
"On the review dataset, our model obtains 2.5% average improvement over AAN-NA.",6 Main Results,[0],[0]
"As shown in Table 5, the gains are more significant when training on room and checkin aspects, reaching 6.9% and 4.5%, respectively.
",6 Main Results,[0],[0]
"Impact of relevance scoring As shown in Table 4, the relevance scoring component plays a crucial role in classification on the pathology dataset.
0.0
w/o reconstruction with reconstruction
Our model achieves more than 27% improvement over AAN-NR.",6 Main Results,[0],[0]
This is because in general aspects have zero correlations to each other in pathology reports.,6 Main Results,[0],[0]
"Therefore, it is essential for the model to have the capacity of distinguishing across different aspects in order to succeed in this task.
",6 Main Results,[0],[0]
"On the review dataset, however, we observe that relevance scoring has no significant impact on performance.",6 Main Results,[0],[0]
"On average, AAN-NR actually outperforms AAN-Full by 0.9%.",6 Main Results,[0],[0]
This observation can be explained by the fact that different aspects in hotel reviews are highly correlated to each other.,6 Main Results,[0],[0]
"For example, the correlation between room quality and cleanliness is 0.81, much higher than aspect correlations in the pathology dataset.",6 Main Results,[0],[0]
"In other words, the sentiment is typically consistent across all sentences in a review, so that selecting aspect-specific sentences becomes unnecessary.",6 Main Results,[0],[0]
"Moreover, our supervision for the relevance scorer is weak and noisy because the aspect keywords are obtained in a semiautomatic way.",6 Main Results,[0],[0]
"Therefore, it is not surprising that AAN-NR sometimes delivers a better classification
accuracy than AAN-Full.",6 Main Results,[0],[0]
Impact of the reconstruction loss Table 6 summarizes the impact of the reconstruction loss on the model performance.,7 Analysis,[0],[0]
"For our full model (AANFull), adding the reconstruction loss yields an average of 5.0% gain on the pathology dataset and 5.6% on the review dataset.
",7 Analysis,[0],[0]
Restaurant Reviews • the fries were undercooked and thrown haphazardly into the sauce holder .,7 Analysis,[0],[0]
the shrimp was over cooked and just deepfried .,7 Analysis,[0],[0]
… even the water tasted weird .,7 Analysis,[0],[0]
"…
• i had the shrimp boil and it was very underseasoned .",7 Analysis,[0],[0]
much closer to bland than anything .,7 Analysis,[0],[0]
"…
• the room was old .",7 Analysis,[0],[0]
… we did n’t like the night shows at all .,7 Analysis,[0],[0]
"…
• however , the decor was just fair .",7 Analysis,[0],[0]
"… the doorknob to our bathroom door fell off , as well as the handle on the toilet .",7 Analysis,[0],[0]
… in the second bedroom it literally rained water from above .,7 Analysis,[0],[0]
• the room decor was not entirely modern .,7 Analysis,[0],[0]
"… we just had the run of the mill hotel room without a view .
",7 Analysis,[0],[0]
"• stay away from fresh vegetable like lettuce , etc .",7 Analysis,[0],[0]
"…
• rest room in this restaurant is very dirty .",7 Analysis,[0],[0]
"… • the only problem i had was that … i was very ill with what was suspected to be food poison • probably the noisiest room he could have given us in the whole hotel .
",7 Analysis,[0],[0]
"Nearest Hotel Reviews by Ours-Full Nearest Hotel Reviews by Ours-NA
To analyze the reasons behind this difference, consider Figure 4 that shows the heat maps of the learned document representations on the review dataset.",7 Analysis,[0],[0]
The top half of the matrices corresponds to input documents from the source domain and the bottom half corresponds to the target domain.,7 Analysis,[0],[0]
"Unlike the first matrix, the other two matrices have no significant difference between the two halves, indicating that adversarial training helps learning of domain-invariant representations.",7 Analysis,[0],[0]
"However, adversarial training also removes a lot of information from representations, as the second matrix is much more sparse than the first one.",7 Analysis,[0],[0]
The third matrix shows that adding reconstruction loss effectively addresses this sparsity issue.,7 Analysis,[0],[0]
Almost 85% entries of the second matrix have small values (< 10−6) while the sparsity is only about 30% for the third one.,7 Analysis,[0],[0]
"Moreover, the standard deviation of the third matrix is also ten times higher than the second one.",7 Analysis,[0],[0]
These comparisons demonstrate that the reconstruction loss function improves both the richness and diversity of the learned representations.,7 Analysis,[0],[0]
"Note that in the case of no adversarial training (AAN-NA), adding the reconstruction component has no clear effect.",7 Analysis,[0],[0]
"This is expected because the main motivation of adding this component is to achieve a more robust adversarial training.
",7 Analysis,[0],[0]
"Regularization on the transformation layer Table 7 shows the averaged accuracy with differ-
ent regularization weights λt in Equation 5.",7 Analysis,[0],[0]
We change λt to reflect different model variants.,7 Analysis,[0],[0]
"First, λt = ∞ corresponds to the removal of the transformation layer because the transformation is always identity in this case.",7 Analysis,[0],[0]
"Our model performs better than this variant on both datasets, yielding an average improvement of 9.8% on the pathology dataset and 2.1% on the review dataset.",7 Analysis,[0],[0]
This result indicates the importance of adding the transformation layer.,7 Analysis,[0],[0]
"Second, using zero regularization (λt = 0) also consistently results in inferior performance, such as 13.8% loss on the pathology dataset.",7 Analysis,[0],[0]
We hypothesize that zero regularization will dilute the effect from reconstruction because there is too much flexibility in transformation.,7 Analysis,[0],[0]
"As a result, the transformed representation will become sparse due to the adversarial training, leading to a performance loss.
",7 Analysis,[0],[0]
"Examples of neighboring reviews Finally, we illustrate in Figure 5 a case study on the characteristics of learned abstract representations by different models.",7 Analysis,[0],[0]
The first column shows an example restaurant review.,7 Analysis,[0],[0]
"Sentiment phrases in this example are mostly food-specific, such as “undercooked” and “tasted weird”.",7 Analysis,[0],[0]
"In the other two columns, we show example hotel reviews that are nearest neighbors to the restaurant reviews, measured by cosine similarity between their representations.",7 Analysis,[0],[0]
"In column 2, many sentiment phrases are specific for room quality, such as “old” and “rained water from above”.",7 Analysis,[0],[0]
"In column 3, however, most sentiment phrases are either common sentiment expressions (e.g. dirty) or food-related (e.g. food poison), even though the focus of the reviews is based on the room quality of hotels.",7 Analysis,[0],[0]
"This observation indicates that adversarial training (AAN-Full) successfully learns to eliminate domain-specific information and to map those domain-specific words into similar domain-invariant
representations.",7 Analysis,[0],[0]
"In contrast, AAN-NA only captures domain-invariant features from phrases that commonly present in both domains.
",7 Analysis,[0],[0]
"Impact of keyword rules Finally, Figure 6 shows the accuracy of our full model (y-axis) when trained with various amount of keyword rules for relevance learning (x-axis).",7 Analysis,[0],[0]
"As expected, the transfer accuracy drops significantly when using fewer rules on the pathology dataset (LCIS as source and ALH as target).",7 Analysis,[0],[0]
"In contrary, the accuracy on the review dataset (hotel service as source and restaurant as target) is not sensitive to the amount of used relevance rules.",7 Analysis,[0],[0]
This can be explained by the observation from Table 5 that the model without relevance scoring performs equally well as the full model due to the tight dependence in aspect labels.,7 Analysis,[0],[0]
"In this paper, we propose a novel aspect-augmented adversarial network for cross-aspect and crossdomain adaptation tasks.",8 Conclusions,[0],[0]
"Experimental results demonstrate that our approach successfully learns invariant representation from aspect-relevant fragments, yielding significant improvement over the mSDA baseline and our model variants.",8 Conclusions,[0],[0]
"The effectiveness of our approach suggests the potential application of adversarial networks to a broader range of NLP tasks for improved representation learning, such as machine translation and language generation.",8 Conclusions,[0],[0]
The authors acknowledge the support of the U.S. Army Research Office under grant number W911NF-10-1-0533.,Acknowledgments,[0],[0]
We thank the MIT NLP group and the TACL reviewers for their comments.,Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.",Acknowledgments,[0],[0]
We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain.,abstractText,[0],[0]
"Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels.",abstractText,[0],[0]
Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner.,abstractText,[0],[0]
"A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents.",abstractText,[0],[0]
"We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant.",abstractText,[0],[0]
"Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.1",abstractText,[0],[0]
Aspect-augmented Adversarial Networks for Domain Adaptation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 115–124 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
115",text,[0],[0]
"Representing the meaning of a word is a prerequisite to solve many linguistic and non-linguistic problems, such as retrieving words with the same meaning, finding the most relevant images or sounds of a word and so on.",1 Introduction,[0],[0]
"In recent years we have seen a surge of interest in building computational models that represent word meanings from patterns of word co-occurrence in corpora (Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Clark, 2015; Wang et al., 2018b).",1 Introduction,[0],[0]
"However, word meaning is also tied to the physical world.",1 Introduction,[0],[0]
"Many behavioral studies suggest that human semantic representation is grounded in the external environment and sensorimotor experience (Landau et al., 1998; Barsalou, 2008).",1 Introduction,[0],[0]
"This has led to the development of multimodal representation models that utilize both textual and perceptual information (e.g., images, sounds).
",1 Introduction,[0],[0]
"As evidenced by a range of evaluations (Andrews et al., 2009; Bruni et al., 2014; Silberer
et al., 2016), multimodal models can learn better semantic word representations (a.k.a. embeddings) than text-based models.",1 Introduction,[0],[0]
"However, most existing models still have a number of drawbacks.",1 Introduction,[0],[0]
"First, they ignore the associations between modalities, and thus lack the ability of information transferring between modalities.",1 Introduction,[0],[0]
Consequently they cannot handle words without perceptual information.,1 Introduction,[0],[0]
"Second, they integrate textual and perceptual representations with simple concatenation, which is insufficient to effectively fuse information from various modalities.",1 Introduction,[0],[0]
"Third, they typically treat the representations from different modalities equally.",1 Introduction,[0],[0]
"This is inconsistent with many psychological findings that information from different modalities contributes differently to the meaning of words (Paivio, 1990; Anderson et al., 2017).
",1 Introduction,[0],[0]
"In this work, we introduce the associative multichannel autoencoder (AMA), a novel multimodal word representation model that addresses all the above issues.",1 Introduction,[0],[0]
"Our model is built upon the stacked autoencoder (Bengio et al., 2007) to learn semantic representations by integrating textual and perceptual inputs.",1 Introduction,[0],[0]
"Inspired by the re-constructive and associative nature of human memory, we propose two associative memory modules as extensions.",1 Introduction,[0],[0]
"One is to learn associations between modalities (e.g., associations between textual and visual features), so as to reconstruct corresponding perceptual information of concepts.",1 Introduction,[0],[0]
"The other is to learn associations between related concepts, by reconstructing embeddings of both target words and their associated words.",1 Introduction,[0],[0]
"Furthermore, we propose a gating mechanism to learn the importance weights of different modalities to each word.
",1 Introduction,[0],[0]
"To summarize, our main contributions in this work are two-fold:
• We present a novel associative multichannel autoencoder for multimodal word representation, which is capable of utilizing associations between different modalities and related
concepts, and assigning different importance weights to each modality according to different words.",1 Introduction,[0],[0]
"Results on six standard benchmarks demonstrate that our methods outperform strong unimodal baselines and state-ofthe-art multimodal models.
",1 Introduction,[0],[0]
"• Our model successfully integrates cognitive insights of the re-constructive and associative nature of semantic memory in humans, suggesting that rich information contained in human cognitive processing can be used to enhance NLP models.",1 Introduction,[0],[0]
"Furthermore, our results shed light on the fundamental questions of how to learn semantic representations, such as the plausibility of reconstructing perceptual information, associating related concepts and grounding word symbols to external environment.",1 Introduction,[0],[0]
"A large body of research evidences that human semantic memory is inherently re-constructive and associative (Collins and Loftus, 1975; Anderson and Bower, 2014).",2.1 Cognitive Grounding,[0],[0]
"That is, memories are not exact static copies of reality, but are rather reconstructed from their stimuli and associated concepts each time they are retrieved.",2.1 Cognitive Grounding,[0],[0]
"For example, when we see a dog, not only the concept itself, but also the corresponding perceptual information and associated words will be jointly activated and reconstructed.",2.1 Cognitive Grounding,[0],[0]
"Moreover, various theories state that the different sources of information contribute differently to the semantic representation of a concept (Wang et al., 2010; Ralph et al., 2017).",2.1 Cognitive Grounding,[0],[0]
"For instance, Dual Coding Theory (Hiscock, 1974) posits that concrete words are represented in the brain in terms of a perceptual and linguistic code, whereas abstract words are encoded only in the linguistic modality.
",2.1 Cognitive Grounding,[0],[0]
"In these respects, our method employs a retrieval and representation process analogous to that of humans, in which the retrieval of perceptual information and associated words is triggered and mediated by a linguistic input.",2.1 Cognitive Grounding,[0],[0]
The learned cross-modality mapping and reconstruction of associated words are inspired by the human mental model of associations between different modalities and related concepts.,2.1 Cognitive Grounding,[0],[0]
"Moreover, word meaning is tied to both linguistic and physical environment, and relies differently on each modality in-
puts (Wang et al., 2018a).",2.1 Cognitive Grounding,[0],[0]
These are also captured by our multimodal representation model.,2.1 Cognitive Grounding,[0],[0]
The existing multimodal representation models can be generally classified into two groups: 1) Jointly training models build multimodal representations with raw inputs of textual and perceptual resources.,2.2 Multimodal Models,[0],[0]
2),2.2 Multimodal Models,[0],[0]
Separate training models independently learn textual and perceptual representations and integrate them afterwards.,2.2 Multimodal Models,[0],[0]
"A class of models extends Latent Dirichlet Allocation (Blei et al., 2003) to jointly learn topic distributions from words and perceptual units (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013).",2.2.1 Jointly training models,[0],[0]
"Recently introduced work is an extension of the Skip-gram model (Mikolov et al., 2013).",2.2.1 Jointly training models,[0],[0]
"For instance, Hill and Korhonen (2014) propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model.",2.2.1 Jointly training models,[0],[0]
"Lazaridou et al. (2015) propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors.",2.2.1 Jointly training models,[0],[0]
"Kiela and Clark (2015) adopt the MMSkip to learn multimodal vectors with auditory perceptual inputs.
",2.2.1 Jointly training models,[0],[0]
These methods can implicitly propagate perceptual information to word representations and at the same time learn multimodal representations.,2.2.1 Jointly training models,[0],[0]
"However, they utilize raw text corpus in which words having perceptual information account for a small portion.",2.2.1 Jointly training models,[0],[0]
This weakens the effect of introducing perceptual information and consequently leads to the slight improvement of textual vectors.,2.2.1 Jointly training models,[0],[0]
The simplest approach is concatenation which fuses textual and visual vectors by concatenating them.,2.2.2 Separate training models,[0],[0]
"It has been proven to be effective in learning multimodal representations (Bruni et al., 2014; Hill et al., 2014; Collell et al., 2017).",2.2.2 Separate training models,[0],[0]
"Variations of this method employ transformation and dimension reduction on the concatenation result, including application of singular value decomposition (SVD) (Bruni et al., 2014) or canonical correlation analysis (CCA) (Hill et al., 2014).",2.2.2 Separate training models,[0],[0]
"There is also work using deep learning methods to project different modality inputs into a common
space, including restricted Boltzman machines (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012), autoencoders (Silberer and Lapata, 2014; Silberer et al., 2016), and recursive neural networks (Socher et al., 2013).",2.2.2 Separate training models,[0],[0]
"However, the above methods can only generate multimodal vectors of those words that have perceptual information, thus reducing multimodal vocabulary drastically.
",2.2.2 Separate training models,[0],[0]
An empirically superior model addresses this problem by predicting missing perceptual information firstly.,2.2.2 Separate training models,[0],[0]
"This includes Hill et al. (2014) who utilize the ridge regression method to learn a mapping matrix from textual modality to visual modality, and Collell et al. (2017) who employ a feedforward neural network to learn the mapping relation between textual vectors and visual vectors.",2.2.2 Separate training models,[0],[0]
"Applying the mapping function on textual representations, they obtain the predicted visual vectors for all words in textual vocabulary.",2.2.2 Separate training models,[0],[0]
Then they calculate multimodal representations by concatenating textual and predicted visual vectors.,2.2.2 Separate training models,[0],[0]
"However, the above methods learn separate mapping functions and fusion models, which are somewhat inelegant.",2.2.2 Separate training models,[0],[0]
"In this paper we employ a neural-network mapping function to integrate these two processes into a unified multimodal models.
",2.2.2 Separate training models,[0],[0]
"According to this classification, our method falls into the second group.",2.2.2 Separate training models,[0],[0]
"However, existing models ignore either the associative relations among modalities, associative relations among relative words, or the different contributions of each modality.",2.2.2 Separate training models,[0],[0]
This paper aims to integrate more perceptual information and the human-like associative memory into a unified multimodal model to learn better word representations.,2.2.2 Separate training models,[0],[0]
We first provide a brief description of the basic multichannel autoencoder for learning multimodal word representations (Figure 1).,3 Associative Multichannel Autoencoder,[0],[0]
Then we extend the model with two associative memory modules and a gating mechanism (Figure 2) in the next sections.,3 Associative Multichannel Autoencoder,[0],[0]
"An autoencoder is an unsupervised neural network which is trained to reconstruct a given input from its latent representation (Bengio, 2009).",3.1 Basic Mutichannel Autoencoder,[0],[0]
"In this work, we propose a variant of autoencoder called multichannel autoencoder, which maps multimodal inputs into a common space.
",3.1 Basic Mutichannel Autoencoder,[0],[0]
"Our model extends the unimodal and bimodal autoencoder (Ngiam et al., 2011; Silberer and Lapata, 2014) to induce semantic representations integrating textual, visual and auditory information.",3.1 Basic Mutichannel Autoencoder,[0],[0]
"As shown in Figure 1, our model first transforms input textual vector xt, visual vector xv and auditory vector xa to hidden representations:
ht = g(Wtxt +",3.1 Basic Mutichannel Autoencoder,[0],[0]
"bt)
hv",3.1 Basic Mutichannel Autoencoder,[0],[0]
"= g(Wvxv + bv)
",3.1 Basic Mutichannel Autoencoder,[0],[0]
"ha = g(Waxa + ba).
",3.1 Basic Mutichannel Autoencoder,[0],[0]
"(1)
Then the hidden representations are concatenated together and mapped to a common space:
hm = g(Wm[ht;hv;ha",3.1 Basic Mutichannel Autoencoder,[0],[0]
] + bm).,3.1 Basic Mutichannel Autoencoder,[0],[0]
"(2)
The model is trained to reconstruct the hidden representations of the three modalities from the multimodal representation hm:
[ĥt; ĥv; ĥa] = g(W ′mhm + bm̂), (3)
and finally to reconstruct the original embeddings of textual, visual and auditory inputs:
x̂t = g(W ′t ĥt + bt̂)",3.1 Basic Mutichannel Autoencoder,[0],[0]
x̂v,3.1 Basic Mutichannel Autoencoder,[0],[0]
= g(W ′vĥv + bv̂) x̂a,3.1 Basic Mutichannel Autoencoder,[0],[0]
"= g(W ′aĥa + bâ),
(4)
where x̂t, x̂v, x̂a are the reconstruction of input vectors xt, xv, xa, and ĥt, ĥv, ĥa
are the reconstruction of hidden representations ht, hv, ha.",3.1 Basic Mutichannel Autoencoder,[0],[0]
"The learning parameters {Wt,Wv,Wa,W ′t ,W ′v,W ′a,Wm,W ′m} are weight matrices, {bt, bv, ba, bt̂, bv̂, bâ, bm, bm̂} are bias vectors.",3.1 Basic Mutichannel Autoencoder,[0],[0]
"Here [· ; ·] denotes the vector concatenation, and g denotes the non-linear function which we use tanh(·).
",3.1 Basic Mutichannel Autoencoder,[0],[0]
Training a single-layer autoencoder corresponds to optimizing the learning parameters to minimize the overall loss between inputs and their reconstructions.,3.1 Basic Mutichannel Autoencoder,[0],[0]
"Following (Vincent et al., 2010), we use squared loss:
min θ1 n∑ i=1",3.1 Basic Mutichannel Autoencoder,[0],[0]
"(||xit− x̂it||2+ ||xiv− x̂iv||2+ ||xia− x̂ia||2), (5) where i denotes the ith word, and the model parameters are θ1 = {Wt,Wv,Wa,Wm,W ′t ,W ′v, W ′a,W ′",3.1 Basic Mutichannel Autoencoder,[0],[0]
"m, bt, bv, ba, bm, bt̂, bv̂, bâ, bm̂}.
",3.1 Basic Mutichannel Autoencoder,[0],[0]
Autoencoders can be stacked to create deep networks.,3.1 Basic Mutichannel Autoencoder,[0],[0]
"To enhance the quality of semantic representations, we employ a stacked multichannel autoencoder, which is composed of multiple hidden layers that are stacked together.",3.1 Basic Mutichannel Autoencoder,[0],[0]
"In reality, the words that have corresponding images or sounds are only a small subset of the textual vocabulary.",3.2 Integrating Modality Associations,[0],[0]
"To obtain the perceptual vectors for each word, we need associations between modalities (i.e., text-to-vision and text-to-audition mapping functions), that transform the textual vectors into visual and auditory ones.",3.2 Integrating Modality Associations,[0],[0]
"Previous methods learn separate mapping functions and fusion models, which are somewhat inelegant.",3.2 Integrating Modality Associations,[0],[0]
"Here we employ a neural-network mapping function to incorporate this modality association module into multimodal models.
",3.2 Integrating Modality Associations,[0],[0]
Take text-to-vision mapping as an example.,3.2 Integrating Modality Associations,[0],[0]
"Suppose that T ∈ Rmt×nt is the textual representation containing mt words, V ∈ Rmv×nv is the visual representation containing mv ( mt) words, where nt and nv are dimensions of the textual and visual representations respectively.",3.2 Integrating Modality Associations,[0],[0]
The textual and visual representations of the ith concept are denoted as Ti and Vi respectively.,3.2 Integrating Modality Associations,[0],[0]
Our goal is to learn a mapping function f : g(WpT + bp) from textual to visual space such that the prediction f(Ti) is similar to the actual visual vector Vi.,3.2 Integrating Modality Associations,[0],[0]
"The set of visual representations along with their corresponding textual representations
image2vec
...
word2vec sound2vec
...
...",3.2 Integrating Modality Associations,[0],[0]
"Multimodal representations
dog
... ...",3.2 Integrating Modality Associations,[0],[0]
"......
.........
...... ...",3.2 Integrating Modality Associations,[0],[0]
"...... ...
... ...",3.2 Integrating Modality Associations,[0],[0]
"...
",3.2 Integrating Modality Associations,[0],[0]
"In case you need, we've collected the cutest small dog breeds to lif t your
mood.
",3.2 Integrating Modality Associations,[0],[0]
"There's nothing that cheers you up quite as fast as a cute dog doing something peculiar.
are used to learn the mapping function.",3.2 Integrating Modality Associations,[0],[0]
"To train the model, we employ a square loss:
min θ2 mv∑ i=1 ||f(Ti)− Vi||2, (6)
where the training parameters are θ2 = {Wp, bp}.",3.2 Integrating Modality Associations,[0],[0]
We adopt the same method to learn the text-toaudition mapping function.,3.2 Integrating Modality Associations,[0],[0]
Word associations are a proxy for an aspect of human semantic memory that is not sufficiently captured by the usual training objectives of multimodal models.,3.3 Integrating Word Associations,[0],[0]
Therefore we assume that incorporating the objective of word associations helps to learn better semantic representations.,3.3 Integrating Word Associations,[0],[0]
"To achieve this, we propose to reconstruct the vector of associated word from the corresponding multimodal semantic representation.",3.3 Integrating Word Associations,[0],[0]
"Specifically, in the decoding process we change the equation (3) to:
[ĥt, ĥv, ĥa, ĥasc] = g(W ′mhm + bm̂), (7)
and equation (4) to:
x̂t = g(W ′t ĥt + bt̂) x̂v",3.3 Integrating Word Associations,[0],[0]
= g(W ′vĥv + bv̂) x̂a,3.3 Integrating Word Associations,[0],[0]
"= g(W ′aĥa + bâ)
",3.3 Integrating Word Associations,[0],[0]
x̂asc =,3.3 Integrating Word Associations,[0],[0]
"g(Wascĥasc + basc).
(8)
To train the model, we add an additional objective function, which is the mean square error
between the embeddings of the associated word y and their re-constructive embeddings",3.3 Integrating Word Associations,[0],[0]
"x̂asc:
min θ3 n∑ i=1",3.3 Integrating Word Associations,[0],[0]
"||yi − x̂iasc||2, (9)
where yi and xi are the embeddings of a pair of associated words.",3.3 Integrating Word Associations,[0],[0]
"Here, y is the concatenation of three unimodal vectors [yt; yv; ya].",3.3 Integrating Word Associations,[0],[0]
"The parameters of word association module are θ3 = {Wt,Wv,Wa,Wm, Ŵm,Wasc, bt, bv, ba, bm, bm̂, basc}.",3.3 Integrating Word Associations,[0],[0]
This additional criterion drives the learning towards a semantic representation capable of reconstructing its associated representation.,3.3 Integrating Word Associations,[0],[0]
"Considering that the meaning of each word has different dependencies on textual and perceptual information, we propose the sample-specific gate to assign different weights to each modality according to different words.",3.4 Integrating a Gating Mechanism,[0],[0]
"The weight parameters are calculated by the following feed-forward neural networks:
gt = g(Wgtxt + bgt)
gv = g(Wgvxv + bgv)
",3.4 Integrating a Gating Mechanism,[0],[0]
"ga = g(Wgaxa + bga),
(10)
where gt, gv and ga are value or vector gate of textual, visual and auditory representations respectively.",3.4 Integrating a Gating Mechanism,[0],[0]
"For the value gate, Wgt, Wgv and Wga are vectors, and bgt, bgv and bga are value parameters.",3.4 Integrating a Gating Mechanism,[0],[0]
"For the vector gate, the parameters Wgt, Wgv and Wga are matrices, bgt, bgv and bga are vectors.",3.4 Integrating a Gating Mechanism,[0],[0]
"The value gate controls the importance weights of different input representations as a whole, whereas the vector gate can adjust the importance weights of each dimension of input representations.
",3.4 Integrating a Gating Mechanism,[0],[0]
"Finally, we compute element-wise multiplication of the textual, visual and auditory representations with their corresponding gates:
xgt = xt gt xgv",3.4 Integrating a Gating Mechanism,[0],[0]
"= xv gv xga = xa ga.
(11)
",3.4 Integrating a Gating Mechanism,[0],[0]
"The xgt, xgv and xga can be seen as the weighted textual, visual and auditory representations.",3.4 Integrating a Gating Mechanism,[0],[0]
The parameters of our gating mechanism is trained together with that of the proposed model.,3.4 Integrating a Gating Mechanism,[0],[0]
"To train the AMA model, we use overall objective function of equation (5) + (6) + (9).",3.5 Model Training,[0],[0]
"In the training phase, model inputs are textual vectors, the corresponding visual vectors, auditory vectors, and association words (Figure 2).",3.5 Model Training,[0],[0]
"In the testing phase, we only need textual inputs to generate multimodal word representations.",3.5 Model Training,[0],[0]
Textual vectors.,4.1 Datasets,[0],[0]
"We use 300-dimensional GloVe vectors1 which are trained on the Common Crawl corpus consisting of 840B tokens and a vocabulary of 2.2M words2.
Visual vectors.",4.1 Datasets,[0],[0]
"Our source of visual vectors are collected from ImageNet (Russakovsky et al., 2015) which covers a total of 21,841 WordNet synsets (Fellbaum, 1998) that have 14,197,122 images.",4.1 Datasets,[0],[0]
"For our experiments, we delete words with fewer than 50 images or words not in the Glove vectors, and sample at most 100 images for each word.",4.1 Datasets,[0],[0]
"To generate a visual vector for each word, we use the forward pass of a pre-trained VGGnet model3 and extract the hidden representation of the last layer as the feature vector.",4.1 Datasets,[0],[0]
Then we use averaged feature vectors of the multiple images corresponding to the same word.,4.1 Datasets,[0],[0]
"Finally, we get 8,048 visual vectors of 128 dimensions.
",4.1 Datasets,[0],[0]
Auditory vectors.,4.1 Datasets,[0],[0]
"For auditory data, we gather audio files from Freesound4, in which we select words with more than 10 audio files and sample at most 50 sounds for one word.",4.1 Datasets,[0],[0]
"To extract auditory features, we use the VGG-net model which is pretrained on Audioset5.",4.1 Datasets,[0],[0]
"The final auditory vectors are averaged feature vectors of multiple audios of the same word, which contains 9,988 words of 128 dimensions6.
",4.1 Datasets,[0],[0]
Word associations.,4.1 Datasets,[0],[0]
"We use the word association data collected by (De Deyne et al., 2016), in which each word pair is generated by at least
1http://nlp.stanford.edu/projects/ glove
2We have tried skip-gram vectors and get the same conclusions.
3http://www.vlfeat.org/matconvnet/ 4http://www.freesound.org/ 5https://research.google.com/audioset 6We build auditory vectors with the released code at: https://github.com/tensorflow/models/ tree/master/research/audioset
one subject7.",4.1 Datasets,[0],[0]
"This dataset includes mostly words with similar meaning (e.g., occasionally & sometimes, adored & loved, supervisor & boss) and related words (e.g., eruption & volcano, cortex & brain, umbrella & rain).",4.1 Datasets,[0],[0]
We calculate the association score for each word pair (cue word + target word) as: the number of person who generated the word pair divided by the total number of people who were presented with the cue word.,4.1 Datasets,[0],[0]
"For training, we select pairs of associated words above a threshold of 0.15 and delete those that are not in the Glove vocabulary, which results in 7,674 word association data sets8.",4.1 Datasets,[0],[0]
"For the development set, we randomly sample 5,000 word association collections together with their association scores.",4.1 Datasets,[0],[0]
"Our models are implemented with PyTorch (Paszke et al., 2017), optimized with Adam (Kingma and Ba, 2014).",4.2 Model Settings,[0],[0]
"We set the initial learning rate to 0.05, and batch size to 64.",4.2 Model Settings,[0],[0]
"We tune the number of layers over 1, 2, 3, the size of multimodal vectors over 100, 200, 300, and the size of each layer in textual channel over 300, 250, 200, 150, 100 and in visual/auditory channel over 128, 120, 90, 60.",4.2 Model Settings,[0],[0]
We train the model for 500 epochs and select the best parameters on the development set.,4.2 Model Settings,[0],[0]
"All models are trained for 3 times and the average results are reported in Table 1.
",4.2 Model Settings,[0],[0]
"To test the effect of each module, we separately train the following models: multichannel autoencoder with modality association (AMAM), with modality and word associations (AMAMW), with modality and word associations plus value/vector gate (AMA-MW-Gval/vec).
",4.2 Model Settings,[0],[0]
"For AMA-M model, we initialize the text-tovision and text-to-audition mapping functions with pre-trained mapping matrices, which are parameters of one-layer feed-forward neural networks.",4.2 Model Settings,[0],[0]
"The network uses input of the textual vectors, output of visual or auditory vectors, and is trained with SGD for 100 epochs.",4.2 Model Settings,[0],[0]
"We initialize the network biases as zeros and network weights with He-initialisation (He et al., 2015).",4.2 Model Settings,[0],[0]
"The best parameters of AMA-M model are 2 hidden layers, with textual channel size of 300, 250 and 150, visual/auditory channel size of 128,
7The dataset can be found at: https:// simondedeyne.me/data.
8We have done experiments with Synonyms (which are extracted from WordNet and PPDB corpora), and the results are not as good as using word associations.
90, 60.",4.2 Model Settings,[0],[0]
"For AMA-MW model, we use the best AMA-M model parameters as initialization, and train the model with word association data.",4.2 Model Settings,[0],[0]
"The optimal parameter of association channel size is 300, 350, 556 (or 428 for bimodal inputs).",4.2 Model Settings,[0],[0]
"For AMA-MW-Gval and AMA-MW-Gvec, we adopt the same training strategy as AMA-MW model.",4.2 Model Settings,[0],[0]
The code for training and evaluation can be found at: https://github.com/wangshaonan/ Associative-multichannel-autoencoder.,4.2 Model Settings,[0],[0]
"We test the baseline and proposed models on six standard evaluation benchmarks, covering two different tasks: (i) Semantic relatedness: Men-3000 (Bruni et al., 2014) and Wordrel-252 (Agirre et al., 2009); (ii) Semantic similarity: Simlex-999 (Hill et al., 2016), Semsim-7576 (Silberer and Lapata, 2014), Wordsim-203 and Simverb-3500 (Gerz et al., 2016).",5.1 Evaluation Tasks,[0],[0]
"All test sets contain a list of word pairs along with their subject ratings.
",5.1 Evaluation Tasks,[0],[0]
We employ Spearman’s correlation method to evaluate the performance of our models.,5.1 Evaluation Tasks,[0],[0]
"This method calculates the correlation coefficients between model predictions and subject ratings, in which the model prediction is the cosine similarity between semantic representations of two words.",5.1 Evaluation Tasks,[0],[0]
Most of existing multimodal models only utilize textual and visual modalities.,5.2 Baseline Multimodal Models,[0],[0]
"For fair comparison, we re-implement several representative systems with our own textual and visual vectors.",5.2 Baseline Multimodal Models,[0],[0]
"The Concatenation (CONC) model (Kiela and Bottou, 2014) is simple concatenation of normalized textual and visual vectors.",5.2 Baseline Multimodal Models,[0],[0]
"The Mapping (Collell et al., 2017) and Ridge (Hill et al., 2014) models first learn a mapping matrix from textual to visual modality using feed-forward neural network and ridge regression respectively.",5.2 Baseline Multimodal Models,[0],[0]
"After applying the mapping function on the textual vectors, they obtain the predicted visual vectors for all words in textual vocabulary.",5.2 Baseline Multimodal Models,[0],[0]
Then they concatenate the normalized textual and predicted visual vectors to get multimodal word representations.,5.2 Baseline Multimodal Models,[0],[0]
"The SVD (Bruni et al., 2014) and CCA (Hill et al., 2014) models first concatenate normalized textual and visual vectors, and then conduct SVD or CCA transformations on the concatenated vectors.
",5.2 Baseline Multimodal Models,[0],[0]
"For multimodal models with textual, visual and
0.58
0.6
0.62
0.64
0.66
100% 80% 60% 40% 20% A ve ra ge S pe ar m an 's co rr el at io ns
Percentage of association data
AMA-M(TV)
AMA-M(TVA)
auditory inputs, we implement CONC and Ridge as baseline models.",5.2 Baseline Multimodal Models,[0],[0]
"The trimodal CONC model simply concatenates normalized textual, visual and auditory vectors.",5.2 Baseline Multimodal Models,[0],[0]
The trimodal Ridge model first learns text-to-vision and text-to-audition mapping matrices with ridge regression method.,5.2 Baseline Multimodal Models,[0],[0]
Then it applies the mapping functions on the textual vectors to get the predicted visual and auditory vectors.,5.2 Baseline Multimodal Models,[0],[0]
"Fi ally, the normalized textual, predictedvisual and predicted-auditory vectors are concatenated to get the multimodal representations.
",5.2 Baseline Multimodal Models,[0],[0]
All above baseline models are implemented with Sklearn9.,5.2 Baseline Multimodal Models,[0],[0]
"Same as the proposed AMA model,
9http://scikit-learn.org/
the hyper-parameters of baseline models are tuned on the development set using Spearman’s correlation method.",5.2 Baseline Multimodal Models,[0],[0]
"In Ridge model, the optimal regularization parameter is 0.6.",5.2 Baseline Multimodal Models,[0],[0]
"The Mapping model is trained with SGD for maximum 100 epochs with early stopping, and the optimal learning rate is 0.001.",5.2 Baseline Multimodal Models,[0],[0]
The output dimension of SVD and CCA models are 300.,5.2 Baseline Multimodal Models,[0],[0]
"As shown in Table 1, we divide all models into six groups: (1) existing multimodal models (with textual and visual inputs) in which results are reprinted from Collell et al. (2017).",5.3 Results and Discussion,[0],[0]
"(2) Unimodal models with textual, (predicted) visual or (pre-
dicted) auditory inputs.",5.3 Results and Discussion,[0],[0]
(3) Our re-implementation of baseline bimodal models with textual and visual inputs (TV).,5.3 Results and Discussion,[0],[0]
(4) Our AMA models with textual and visual inputs.,5.3 Results and Discussion,[0],[0]
"(5) Our implementation of trimodal baseline models with textual, visual and auditory inputs (TVA).",5.3 Results and Discussion,[0],[0]
"(6) Our AMA model with textual, visual and auditory inputs.
",5.3 Results and Discussion,[0],[0]
"Overall performance Our AMA models (in group 4 and 6) clearly outperform their baseline unimodal and multimodal models (in group 2, 3 and 5).",5.3 Results and Discussion,[0],[0]
We use Wilcoxon signed-rank test to check if significant difference exists between two models.,5.3 Results and Discussion,[0],[0]
"Results show that our multimodal models perform significantly better (p < 0.05) than all baseline models.
",5.3 Results and Discussion,[0],[0]
"As shown clearly, our bimodal and trimodal AMA models achieve better performance than baselines in both V/A (visual or auditory, the testing data that have associated visual or auditory vectors) and ZS (zero-shot, the testing data that do not have associated visual or auditory vectors) region.",5.3 Results and Discussion,[0],[0]
"In other words, our models outperform baseline models on words with or without perceptual information.",5.3 Results and Discussion,[0],[0]
"The good results in ZS region also indicate that our models have good generalization capacity.
",5.3 Results and Discussion,[0],[0]
"Unimodal baselines As shown in group 2, the Glove vectors are much better than CNNvisual and CNN-auditory vectors, in which CNNauditory has the worst performance on capturing concept similarities.",5.3 Results and Discussion,[0],[0]
"Comparing with visual and auditory vectors, the predicted visual and auditory vectors achieve much better performance.",5.3 Results and Discussion,[0],[0]
"This indicates that the predicted vectors contain richer information than purely perceptual representations and are more useful for building semantic representations.
",5.3 Results and Discussion,[0],[0]
"Multimodal baselines For bimodal models (group 3), the CONC model that combines Glove and visual vectors performs worse than Glove on four out of six datasets, suggesting that simple concatenation might be suboptimal.",5.3 Results and Discussion,[0],[0]
"The Mapping and Ridge models, which combine Glove and predicted visual vectors, improve over Glove on five out of six datasets in ALL regions.",5.3 Results and Discussion,[0],[0]
This reinforces the conclusion that the predicted visual vectors are more useful in building multimodal models.,5.3 Results and Discussion,[0],[0]
The SVD model gets similar results as Ridge model.,5.3 Results and Discussion,[0],[0]
"The CCA model maps different modality inputs into a common space, achieving better results on some datasets and worse results on the others.
",5.3 Results and Discussion,[0],[0]
"The improvement on three benchmark tests shows the potential of mapping multimodal inputs into a common space.
",5.3 Results and Discussion,[0],[0]
The above results can also be observed in the trimodal CONC and Ridge models (group 5).,5.3 Results and Discussion,[0],[0]
"Overall, the trimodal models, which utilize additional auditory inputs, get slightly worse performance than bimodal models.",5.3 Results and Discussion,[0],[0]
This is partly caused by the fusion method of concatenation.,5.3 Results and Discussion,[0],[0]
Note that our proposed AMA models are more effective with trimodal inputs as shown in group 6.,5.3 Results and Discussion,[0],[0]
"Our multimodal models With either bimodal or trimodal inputs, the proposed AMA-M model outperforms all baseline models by a large margin.",5.3 Results and Discussion,[0],[0]
Specifically our AMA-M model achieves an relative improvement of 4.1% on average (4.5% with trimodal inputs) over the state-of-the-art Ridge model.,5.3 Results and Discussion,[0],[0]
This illustrates that our AMA models can productively combine textual and perceptual representations.,5.3 Results and Discussion,[0],[0]
"Moreover, our AMA-MW model, which employs word associations, achieves an average improvement of 1.5% (2.7% with trimodal inputs) over the AMA-M model.",5.3 Results and Discussion,[0],[0]
"That is to say, the representation ability of multimodal models can be clearly improved by learning associative relations between words.",5.3 Results and Discussion,[0],[0]
"Furthermore, the AMAMW-Gval model improves the AMA-MW model by 1.3% (0.3% with trimodal inputs) on average, illustrating that the gating mechanism (especially the value gate) helps to learn better semantic representations.
",5.3 Results and Discussion,[0],[0]
"In addition, we explore the effect of word association data size.",5.3 Results and Discussion,[0],[0]
"We find that the decrease of association data has no discernible effect on model performance: when using 100%, 80%, 60%, 40%, 20% of the data, the average results are 0.6479, 0.6409, 0.6361, 0.6430, 0.6458 in bimodal model.",5.3 Results and Discussion,[0],[0]
The same trend is observed in trimodal models.,5.3 Results and Discussion,[0],[0]
We have proposed a cognitively-inspired multimodal model — associative multichannel autoencoder — which utilizes the associations between modalities and related words to learn multimodal word representations.,6 Conclusions and Future Work,[0],[0]
"Performance improvement on six benchmark tests shows that our models can efficiently fuse different modality inputs and build better semantic representations.
",6 Conclusions and Future Work,[0],[0]
"Ultimately, the present paper sheds light on the fundamental questions of how to learn word meanings, such as the plausibility of reconstructing per-
ceptual information, associating related concepts and grounding word symbols to external environment.",6 Conclusions and Future Work,[0],[0]
We believe that one of the promising future directions is to learn from how humans learn and store semantic word representations to build a more effective computational model.,6 Conclusions and Future Work,[0],[0]
The research work descried in this paper has been supported by the National Key Research and Development Program of China under Grant No. 2017YFB1002103 and also supported by the Natural Science Foundation of China under Grant No. 61333018.,Acknowledgement,[0],[0]
The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper.,Acknowledgement,[0],[0]
"In this paper we address the problem of learning multimodal word representations by integrating textual, visual and auditory inputs.",abstractText,[0],[0]
"Inspired by the re-constructive and associative nature of human memory, we propose a novel associative multichannel autoencoder (AMA).",abstractText,[0],[0]
"Our model first learns the associations between textual and perceptual modalities, so as to predict the missing perceptual information of concepts.",abstractText,[0],[0]
Then the textual and predicted perceptual representations are fused through reconstructing their original and associated embeddings.,abstractText,[0],[0]
Using a gating mechanism our model assigns different weights to each modality according to the different concepts.,abstractText,[0],[0]
Results on six benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models.,abstractText,[0],[0]
Associative Multichannel Autoencoder for Multimodal Word Representation,title,[0],[0]
"With the development of deep neural networks, including deep convolutional neural networks (CNN) (Krizhevsky et al., 2012), the ability to recognize images and languages has improved dramatically.",1. Inroduction,[0],[0]
Training deeplayered networks using a large number of labeled samples enables us to correctly categorize samples in diverse domains.,1. Inroduction,[0],[0]
"In addition, the transfer learning of a CNN has been utilized in many studies.",1. Inroduction,[0],[0]
"For object detection or segmentation, we can transfer the knowledge of a CNN trained using a large-scale dataset by fine-tuning it on a relatively small
1The University of Tokyo, Tokyo, Japan 2RIKEN, Japan.",1. Inroduction,[0],[0]
"Correspondence to: Kuniaki Saito <k-saito@mi.t.utokyo.ac.jp>, Yoshitaka Ushiku <ushiku@mi.t.u-tokyo.ac.jp>, Tatsuya Harada <harada@mi.t.u-tokyo.ac.jp>.
",1. Inroduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Inroduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Inroduction,[0],[0]
"dataset (Girshick et al., 2014; Long et al., 2015a).
",1. Inroduction,[0],[0]
"One of the problems inherent to neural networks is that, although such networks perform well on samples generated from the same distribution as the training samples, they may find it difficult to correctly recognize samples from different distributions at the test time.",1. Inroduction,[0],[0]
"An example of this is images collected from the Internet, which may come in abundance and are fully labeled.",1. Inroduction,[0],[0]
Such images have a distribution that differs from images taken from a camera.,1. Inroduction,[0],[0]
"Thus, a classifier that performs well on various domains is important for practical use.",1. Inroduction,[0],[0]
"To realize such a classifier, it is necessary to learn domain-invariantly discriminative representations.",1. Inroduction,[0],[0]
"However, acquiring such representations is not easy because it is often difficult to collect a large number of labeled samples, and because samples from different domains have domain-specific characteristics.
",1. Inroduction,[0],[0]
"In unsupervised domain adaptation, we try to train a classifier that works well on a target domain under the condition that we are provided labeled source samples and unlabeled target samples during training.",1. Inroduction,[0],[0]
Most of the previously developed deep domain adaptation methods operate mainly under the assumption that the adaptation can be realized by matching the distribution of features from different domains.,1. Inroduction,[0],[0]
"These methods have been aimed at obtaining domain-invariant features by minimizing the divergence between domains, as well as a category loss on the source domain (Ganin & Lempitsky, 2014; Long et al., 2015b; 2016).",1. Inroduction,[0],[0]
"However, as shown in (Ben-David et al., 2010), if a classifier that works well on both the source and the target domains does not exist, we theoretically cannot expect a discriminative classifier to be applicable to the target domain.",1. Inroduction,[0],[0]
"That is, even if the distributions are matched with the non-discriminative representations, the classifier may not work well on the target domain.",1. Inroduction,[0],[0]
"Because the direct learning discriminative representations for the target domain, in the absence of target labels, is considered very difficult, we propose assigning pseudo-labels to the target samples and training the target-specific networks as if they were true labels.
",1. Inroduction,[0],[0]
"Co-training and tri-training (Zhou & Li, 2005) leverage multiple classifiers to artificially label unlabeled samples and retrain the classifiers.",1. Inroduction,[0],[0]
"However, such methods do not assume labeling samples from different domains.",1. Inroduction,[0],[0]
"Because
our goal is to classify unlabeled target samples that have different characteristics from labeled source samples, we propose the use of asymmetric tri-training for unsupervised domain adaptation.",1. Inroduction,[0],[0]
"By asymmetric, we mean that we assign different roles to three different classifiers.
",1. Inroduction,[0],[0]
"In this paper, we propose a novel tri-training method for unsupervised domain adaptation, where we assign pseudolabels to unlabeled samples, and train the neural networks utilizing these samples.",1. Inroduction,[0],[0]
"As described in Fig. 1, two networks are used to label unlabeled target samples, and the remaining network is trained using the pseudo-labeled target samples.",1. Inroduction,[0],[0]
"We evaluated our method using digit classification tasks, traffic sign classification tasks, and sentiment analysis tasks using the Amazon Review dataset, and demonstrated its state-of-the-art performance for nearly all of the conducted experiments.",1. Inroduction,[0],[0]
"In particular, for the adaptation scenario, MNIST→SVHN, our method outperformed other methods by more than 10%.",1. Inroduction,[0],[0]
"A number of previous methods have attempted to realize adaptation by measuring the divergence between different domains (Ganin & Lempitsky, 2014; Long et al., 2015b; Li et al., 2016).",2. Related Work,[0],[0]
"Such methods are based on the theory proposed in (Ben-David et al., 2010), which states that the expected loss for a target domain is bounded by three terms: (i) the expected loss for the source domain, (ii) the domain divergence between the source and target, and (iii) the minimum value of a shared expected loss.",2. Related Work,[0],[0]
A shared expected loss indicates the sum of the loss on the source and target domains.,2. Related Work,[0],[0]
"Because the third term, which is usually considered to be very low, cannot be evaluated when labeled target samples are absent, most methods attempt to minimize the first and second terms.",2. Related Work,[0],[0]
"With regard to the training of deep architectures, the maximum mean discrepancy (MMD), or the loss of a domain classifier network, is utilized to measure the divergence corresponding to the second term (Gretton et al., 2012; Ganin & Lempitsky, 2014; Long et al., 2015b; 2016; Bousmalis et al., 2016).",2. Related Work,[0],[0]
"However, the third term is very important in training a CNN, which simultaneously extracts and recognizes the representations.",2. Related Work,[0],[0]
"The third term can easily become large when the representations are not discriminative for the target do-
main.",2. Related Work,[0],[0]
"Therefore, we focus on how to learn the targetdiscriminative representations to consider the third term.",2. Related Work,[0],[0]
"In (Long et al., 2016), the focus was on this point, and a target-specific classifier was constructed using a residual network structure.",2. Related Work,[0],[0]
"Differing from their method, we constructed a target-specific network by providing artificially labeled target samples.
",2. Related Work,[0],[0]
"Several transductive methods use a similarity of features to provide labels for unlabeled samples (Rohrbach et al., 2013; Khamis & Lampert, 2014).",2. Related Work,[0],[0]
"For unsupervised domain adaptation, in (Sener et al., 2016), a method was proposed to learn the labeling metrics by utilizing the k-nearest neighbors between unlabeled target samples and labeled source samples.",2. Related Work,[0],[0]
"In contrast to this method, our method explicitly and simply backpropagates the category loss for the target samples based on pseudo-labeled samples.
",2. Related Work,[0],[0]
"Many methods have proposed giving pseudo-labels to unlabeled samples by utilizing the predictions of a classifier and retraining it, including pseudo-labeled samples, a process called self-training.",2. Related Work,[0],[0]
"The underlying assumption of self-training is that one’s own high-confidence predictions are correct (Zhu, 2005).",2. Related Work,[0],[0]
"As the predictions are mostly correct, utilizing samples with high confidence will further improve the performance of the classifier.",2. Related Work,[0],[0]
"Co-training utilizes two classifiers, which have different views on one sample, to provide pseudo-labels (Blum & Mitchell, 1998; Tanha et al., 2011).",2. Related Work,[0],[0]
The unlabeled samples are then added to the training set if at least one classifier is confident regarding the predictions.,2. Related Work,[0],[0]
"The generalization capability of co-training is theoretically ensured (Balcan et al., 2004; Dasgupta et al., 2001) under certain assumptions, and applied to various tasks (Wan, 2009; Levin et al., 2003).",2. Related Work,[0],[0]
"In (Chen et al., 2011), the idea of co-training was incorporated into domain adaptation.",2. Related Work,[0],[0]
"Similar to co-training, tritraining uses the output of three different classifiers to provide pseudo-labels to unlabeled samples (Zhou & Li, 2005).",2. Related Work,[0],[0]
"Tri-training does not require partitioning features into different views; instead, tri-training initializes each classifier in a different manner.",2. Related Work,[0],[0]
"However, tri-training does not assume that the unlabeled samples follow different distributions from those the labeled ones are generated from.",2. Related Work,[0],[0]
"Hence, we developed a tri-training method for domain adaptation that utilizes three classifiers asymmetrically.
",2. Related Work,[0],[0]
"In (Lee, 2013), the effects of pseudo-labels on a neural network were investigated.",2. Related Work,[0],[0]
"The authors argued that the effect of training a classifier using pseudo-labels is equivalent to entropy regularization, thus leading to a low-density separation between classes.",2. Related Work,[0],[0]
"In our experiments, we observed that the target samples are separated in hidden features.",2. Related Work,[0],[0]
"In this section, we provide details of the proposed model for domain adaptation.",3. Method,[0],[0]
"We aim to construct a target-
specific network by utilizing pseudo-labeled target samples.",3. Method,[0],[0]
"Simultaneously, we expect two labeling networks to acquire target-discriminative representations and gradually increase the accuracy on the target domain.
",3. Method,[0],[0]
Our proposed network structure is shown in Fig. 2.,3. Method,[0],[0]
"Here, F denotes a network that outputs shared features from among three different networks, and F1 and F2 classify the features generated from F .",3. Method,[0],[0]
Their predictions are utilized to provide pseudo-labels.,3. Method,[0],[0]
"The classifier Ft classifies features generated from F , which is a target-specific network.",3. Method,[0],[0]
"Here, F1 and F2 learn from the source and pseudo-labeled target samples, and Ft learns only from the pseudo-labeled target samples.",3. Method,[0],[0]
"The shared network F learns from all gradients from F1, F2, and Ft.",3. Method,[0],[0]
"Without such a shared network, another option for the network architecture is training the three networks separately, although this is inefficient in terms of training and implementation.",3. Method,[0],[0]
"Furthermore, by building a shared network, F , F1, and F2 can also harness the target-discriminative representations learned through the feedback from Ft.
",3. Method,[0],[0]
"The set of source samples is defined as { (xi, yi) }ms i=1
∼ Xs, the unlabeled target set is { (xi) }mt i=1
∼ Xt, and the pseudo-labeled target set is { (xi, ŷi) }nt i=1",3. Method,[0],[0]
∼ Xtl.,3. Method,[0],[0]
"In existing studies (Chen et al., 2011) on co-training for domain adaptation, the given features are divided into separate parts, and considered to be different views.
",3.1. Loss for Multiview Features Network,[0],[0]
"Because we aim to label the target samples with high accuracy, we expect F1 and F2 to classify the samples based on different viewpoints.",3.1. Loss for Multiview Features Network,[0],[0]
"Therefore, we make a constraint for the weights of F1 and F2 to make their inputs different from each other.",3.1. Loss for Multiview Features Network,[0],[0]
"We add the term |W1TW2| to the cost function, where W1 and W2 denote fully connected layer
weights of F1 and F2, which are first applied to the feature F (xi).",3.1. Loss for Multiview Features Network,[0],[0]
"With this constraint, each network will learn from different features.",3.1. Loss for Multiview Features Network,[0],[0]
"The objective for the learning of F1 and F2 is defined as
E(θF , θF1 , θF2) = 1
n n∑ i=1",3.1. Loss for Multiview Features Network,[0],[0]
[ Ly(F1 ◦,3.1. Loss for Multiview Features Network,[0],[0]
"F (xi)), yi)
+ Ly(F2",3.1. Loss for Multiview Features Network,[0],[0]
◦,3.1. Loss for Multiview Features Network,[0],[0]
"(F (xi)), yi) ]",3.1. Loss for Multiview Features Network,[0],[0]
"+ λ|W1TW2|
(1)
where Ly denotes the standard softmax cross-entropy loss function.",3.1. Loss for Multiview Features Network,[0],[0]
We determined the trade-off parameter λ based on a validation split.,3.1. Loss for Multiview Features Network,[0],[0]
Pseudo-labeled target samples will provide targetdiscriminative information to the network.,3.2. Learning Procedure and Labeling Method,[0],[0]
"However, because they certainly contain false labels, we have to pick up reliable pseudo-labels, which our labeling and learning method is aimed at realizing.
",3.2. Learning Procedure and Labeling Method,[0],[0]
The entire training procedure of the network is shown in Algorithm 1.,3.2. Learning Procedure and Labeling Method,[0],[0]
"First, we train the entire network using the source training set Xs.",3.2. Learning Procedure and Labeling Method,[0],[0]
"Here, F1 and F2 are optimized through Eq.",3.2. Learning Procedure and Labeling Method,[0],[0]
"(1), and Ft is trained based on a standard category loss.",3.2. Learning Procedure and Labeling Method,[0],[0]
"After training on Xs, to provide pseudo-labels, we use the predictions of F1 and F2, namely, ŷ1, ŷ2 obtained from xk.",3.2. Learning Procedure and Labeling Method,[0],[0]
"When C1 and C2 denote the class that has the maximum predicted probability for ŷ1, ŷ2, we assign a pseudo-label to xk if the following two conditions are satisfied.",3.2. Learning Procedure and Labeling Method,[0],[0]
"First, we require C1 = C2 to provide pseudo-labels, which means the two different classifiers agree with the prediction.",3.2. Learning Procedure and Labeling Method,[0],[0]
"The second requirement is that the maximizing probability of ŷ1 or ŷ2 exceed the threshold parameter, which we set as 0.9 or 0.95 in the experiment.",3.2. Learning Procedure and Labeling Method,[0],[0]
"We suppose that unless one of the two classifiers is confident of the prediction, the prediction is not reliable.",3.2. Learning Procedure and Labeling Method,[0],[0]
"If the two requirements are satisfied, ( xk, yk = C1 = C2 ) is added to Xtl.",3.2. Learning Procedure and Labeling Method,[0],[0]
"To prevent an overfitting to the pseudo-labels, we resample the candidate for labeling the samples in each step.",3.2. Learning Procedure and Labeling Method,[0],[0]
"We set the number of initial candidates Ninit to 5,000.",3.2. Learning Procedure and Labeling Method,[0],[0]
"We gradually increase the number of candidates Nt = K/20 ∗ n, where n denotes the number of all target samples, and K denotes the number of steps; in addition, we set the maximum number of pseudo-labeled candidates to 40,000.",3.2. Learning Procedure and Labeling Method,[0],[0]
We set K to 30 in the experiments.,3.2. Learning Procedure and Labeling Method,[0],[0]
"After the pseudo-labeled training set Xtl is composed, F, F1, and F2 are updated based on the objective in Eq.",3.2. Learning Procedure and Labeling Method,[0],[0]
(1) for the labeled training set L = Xs ∪ Xtl.,3.2. Learning Procedure and Labeling Method,[0],[0]
"Then, F and Ft are simply optimized based on the category loss for Xtl.
Discriminative representations will be learned by constructing a target-specific network trained only on the target samples.",3.2. Learning Procedure and Labeling Method,[0],[0]
"However, if only noisy pseudo-labeled samples are used for the training, the network may not learn any
Algorithm 1 iter denotes the iteration of the training.",3.2. Learning Procedure and Labeling Method,[0],[0]
The function Labeling indicates the labeling method.,3.2. Learning Procedure and Labeling Method,[0],[0]
"We assign pseudo-labels to samples when the predictions of F1 and F2 agree, and at least one of them is confident of their predictions.
",3.2. Learning Procedure and Labeling Method,[0],[0]
"Input: data Xs = { (xi, ti) }m i=1 , Xt = { (xj) }n j=1",3.2. Learning Procedure and Labeling Method,[0],[0]
"Xtl = ∅ for j = 1 to iter do
Train F, F1, F2, Ft with a mini-batch from the training set S
end for Nt = Ninit Xtl = Labeling(F, F1, F2,Xt, Nt) L = Xs ∪Xtl for K steps do
for j = 1 to iter do Train F, F1, F2 with mini-batch from training set L Train F, Ft with mini-batch from training set Xtl end for Xtl = ∅, Nt = K/20 ∗ n",3.2. Learning Procedure and Labeling Method,[0],[0]
"Xtl = Labeling(F, F1, F2,Xt, Nt) L = Xs ∪Xtl
end for
useful representations.",3.2. Learning Procedure and Labeling Method,[0],[0]
"We then use both the source samples and pseudo-labeled samples for the training of F, F1, and F2 to ensure the accuracy.",3.2. Learning Procedure and Labeling Method,[0],[0]
"In addition, as the learning proceeds, F will learn target-discriminative representations, resulting in an improvement in accuracy for F1 and F2.",3.2. Learning Procedure and Labeling Method,[0],[0]
This cycle will gradually enhance the accuracy in the target domain.,3.2. Learning Procedure and Labeling Method,[0],[0]
"Batch normalization (BN) (Ioffe & Szegedy, 2015), which whitens the output of the hidden layer in a CNN, is an effective technique for accelerating the training speed and enhancing the accuracy of the model.",3.3. Batch Normalization for Domain Adaptation,[0],[0]
"In addition, in domain adaptation, whitening the output of the hidden layer is effective in improving the performance, and makes the distribution in different domains similar (Sun et al., 2016; Li et al., 2016).
",3.3. Batch Normalization for Domain Adaptation,[0],[0]
The input samples of F1 and F2 include both pseudolabeled target samples and source samples.,3.3. Batch Normalization for Domain Adaptation,[0],[0]
Introducing BN will be useful for matching the distribution and improving the performance.,3.3. Batch Normalization for Domain Adaptation,[0],[0]
"We add BN layers to F, F1 and F2, which we detail in our supplementary material.",3.3. Batch Normalization for Domain Adaptation,[0],[0]
"In this section, we provide a theoretical analysis to our approach.",4. Analysis,[0],[0]
"First, we provide insight into existing theory, and then introduce a simple expansion of the theory related to
our method.",4. Analysis,[0],[0]
"The distribution of the source samples is denoted as S; that of the target samples, as T ; and that of the pseudo-labeled target samples, as Tl.
",4. Analysis,[0],[0]
"In (Ben-David et al., 2010), an equation was introduced showing that the upper bound of the expected error in the target domain depends on three terms, which include the divergence between different domains and the error of an ideal joint hypothesis.",4. Analysis,[0],[0]
"The divergence between the source and target domains, H∆H-distance, is defined as follows:
dH∆H(S, T )
",4. Analysis,[0],[0]
"= 2 sup (h,h′)∈H2 ∣∣∣ E",4. Analysis,[0],[0]
x∼S,4. Analysis,[0],[0]
[h(x) ̸=,4. Analysis,[0],[0]
h′(x)]− E,4. Analysis,[0],[0]
x∼T,4. Analysis,[0],[0]
[h(x) ̸= h′(x)],4. Analysis,[0],[0]
"∣∣∣
This distance is frequently used to measure the adaptability between different domains.
",4. Analysis,[0],[0]
"The ideal joint hypothesis is defined as h∗ = arg min
h∈H
( RS(h) + RT (h) ) , and its corresponding error is
C = RS(h ∗)",4. Analysis,[0],[0]
"+ RT (h ∗), where R denotes the expected error for each hypothesis.",4. Analysis,[0],[0]
"The theorem is as follows.
",4. Analysis,[0],[0]
Theorem 1.,4. Analysis,[0],[0]
"(Ben-David et al., 2010) Let H be the hypothesis class.",4. Analysis,[0],[0]
"Given two different domains, S and T , we have ∀h ∈ H,RT (h) ≤ RS(h)",4. Analysis,[0],[0]
"+ 1
2 dH∆H(S, T ) + C (2)
",4. Analysis,[0],[0]
"This theorem indicates that the expected error on the target domain is upper bounded by three terms: the expected error on the source domain, the domain divergence measured by the disagreement of the hypothesis, and the error of the ideal joint hypothesis.",4. Analysis,[0],[0]
"In an existing work (Ganin & Lempitsky, 2014; Long et al., 2015b), C was disregarded because it was considered to be negligible.",4. Analysis,[0],[0]
"If we are provided with fixed features, we do not need to consider this term because it is also fixed.",4. Analysis,[0],[0]
"However, if we assume that xs ∼ S and xt ∼ T are obtained from the last fully connected layer of the deep models, we should note that C is determined based on the output of the layer, as well as the necessity of considering this term.
",4. Analysis,[0],[0]
We consider the pseudo-labeled target sample distributions Tl given false labels at a ratio of ρ.,4. Analysis,[0],[0]
"The shared error of h∗ on S, Tl is denoted as C ′. The following inequality then holds:
∀h ∈ H,RT (h) ≤ RS(h)",4. Analysis,[0],[0]
"+ 1
2 dH∆H(S, T ) +",4. Analysis,[0],[0]
"C
≤ RS(h) + 1
2 dH∆H(S, T ) +",4. Analysis,[0],[0]
C ′,4. Analysis,[0],[0]
"+ ρ
(3)
We show a simple derivation of the inequality in the Supplementary materials section.",4. Analysis,[0],[0]
"In Theorem 1, we cannot measure C in the absence of labeled target samples.",4. Analysis,[0],[0]
We can evaluate and minimize it approximately using pseudolabels.,4. Analysis,[0],[0]
"Furthermore, when we consider the second term on the right-hand side, our method is expected to reduce
this term.",4. Analysis,[0],[0]
This term intuitively denotes the discrepancy between different domains in the disagreement of two classifiers.,4. Analysis,[0],[0]
"If we regard h and h′ as F1 and F2, respectively, E
x∼S",4. Analysis,[0],[0]
[h(x) ̸= h′(x)] should be very low because the training is based on the same labeled samples.,4. Analysis,[0],[0]
"Moreover, for the same reason, E
x∼T [h(x) ̸= h′(x)] is expected to be low, al-
though we use the training set Xtl instead of the genuine labeled target samples.",4. Analysis,[0],[0]
"Thus, our method considers both the second and third terms in Theorem 1.",4. Analysis,[0],[0]
We conducted extensive evaluations of our method on image datasets and a sentiment analysis dataset.,5. Experiment and Evaluation,[0],[0]
"We evaluated the accuracy of the target-specific networks.
",5. Experiment and Evaluation,[0],[0]
"Visual Domain Adaptation For visual domain adaptation, we conducted our evaluation on the digit and traffic sign datasets.",5. Experiment and Evaluation,[0],[0]
"The digit datasets include MNIST (LeCun et al., 1998), MNIST-M (Ganin & Lempitsky, 2014), Street View House Numbers (SVHN) (Netzer et al., 2011), and Synthetic Digits (SYN DIGITS)",5. Experiment and Evaluation,[0],[0]
"(Ganin & Lempitsky, 2014).",5. Experiment and Evaluation,[0],[0]
We further evaluated our method on traffic sign datasets including Synthetic Traffic Signs (SYN SIGNS),5. Experiment and Evaluation,[0],[0]
"(Moiseev et al., 2013) and the German Traffic Sign Recognition Benchmark (Stallkamp et al., 2011) (GTSRB).",5. Experiment and Evaluation,[0],[0]
"In total, five adaptation scenarios were evaluated during this experiment.",5. Experiment and Evaluation,[0],[0]
"Because the datasets used for evaluation are varied in previous studies, we extensively evaluated our method using these five scenarios.
",5. Experiment and Evaluation,[0],[0]
Many previous studies have evaluated the fine-tuning of pretrained networks using ImageNet.,5. Experiment and Evaluation,[0],[0]
This protocol assumes the existence of another source domain.,5. Experiment and Evaluation,[0],[0]
"In our work, we want to evaluate a situation in which we have access to only a single source domain and a single target domain.
",5. Experiment and Evaluation,[0],[0]
"Adaptation in Amazon Reviews To investigate its behavior on the language datasets, we evaluated our method on the Amazon Review dataset (Blitzer et al., 2006) through the same preprocessing used by (Chen et al., 2011; Ganin et al., 2016).",5. Experiment and Evaluation,[0],[0]
"The dataset contains reviews on four types of products: books, DVDs, electronics, and kitchen appliances.",5. Experiment and Evaluation,[0],[0]
We evaluated our method under 12 domain adaptation scenarios.,5. Experiment and Evaluation,[0],[0]
"The results are shown in Table 1.
",5. Experiment and Evaluation,[0],[0]
"Baseline Methods We compared our method with five methods for unsupervised domain adaptation, including state-of-the art methods in visual domain adaptation: Maximum Mean Discrepancy (MMD) (Long et al., 2015b), Domain Adversarial Neural Network (DANN) (Ganin & Lempitsky, 2014), Deep Reconstruction Classification Network (DRCN) (Ghifary et al., 2016), Domain Separation Network (DSN) (Bousmalis et al., 2016), and k-Nearest Neighbor based adaptation (kNN-Ad) (Sener et al., 2016).",5. Experiment and Evaluation,[0],[0]
"We cited the results of MMD from (Bousmalis et al., 2016).",5. Experiment and Evaluation,[0],[0]
"In addition, we compared our
method with CNN trained only on the source samples.",5. Experiment and Evaluation,[0],[0]
"We compared our method with Variational Fair AutoEncoder (VFAE) (Louizos et al., 2015) and DANN (Ganin et al., 2016) in our experiment on the Amazon Review dataset.",5. Experiment and Evaluation,[0],[0]
"In our experiments on the image datasets, we employed the architecture of CNN used in (Ganin & Lempitsky, 2014).",5.1. Implementation Detail,[0],[0]
"For a fair comparison, we separated the network at the hidden layer from which (Ganin & Lempitsky, 2014) constructed discriminator networks.",5.1. Implementation Detail,[0],[0]
"Therefore, when considering a single classifier, for example, F1 ◦ F , the architecture is identical to a previous work.",5.1. Implementation Detail,[0],[0]
"We also followed (Ganin & Lempitsky, 2014) with the other protocols.",5.1. Implementation Detail,[0],[0]
"Based on a validation, we set the threshold value for the labeling method as 0.95 in MNIST↔SVHN.",5.1. Implementation Detail,[0],[0]
"In other scenarios, we set it as 0.9.",5.1. Implementation Detail,[0],[0]
"We used MomentumSGD for optimization, and set the momentum as 0.9, whereas the learning rate was set 0.01.",5.1. Implementation Detail,[0],[0]
λ was set to 0.01 for all scenarios based on our validation.,5.1. Implementation Detail,[0],[0]
"In the Supplementary materials section, we provide details of the network architecture and the hyper-parameters.
",5.1. Implementation Detail,[0],[0]
"For our experiments on the Amazon Review dataset, we used a similar architecture to that used in (Ganin et al., 2016): with the sigmoid activated, one dense hidden layer with 50 hidden units, and a softmax output.",5.1. Implementation Detail,[0],[0]
We extended its architecture to our method similarly to that of the CNN.,5.1. Implementation Detail,[0],[0]
λ was set to 0.001 based on a validation.,5.1. Implementation Detail,[0],[0]
"Because the input is sparse, we used Adagrad (Duchi et al., 2011) for optimization.",5.1. Implementation Detail,[0],[0]
"We repeated this evaluation ten times, and reported the mean accuracy.",5.1. Implementation Detail,[0],[0]
"In Tables 1 and 3, we show the main results of our experiments.",5.2. Experimental Result,[0],[0]
"When training only using source samples, the effect of the BN is not clear, as shown in the Tables 1.",5.2. Experimental Result,[0],[0]
"However, for most of the image recognition experiments, the effect of the BN with our method is clear; at the same time, the effect of our method is also clear when we do not use a BN in the network architecture compared to the Source Only method.",5.2. Experimental Result,[0],[0]
The effect of the weight constraint is not obvious in other than MNIST→SVHN.,5.2. Experimental Result,[0],[0]
"This result indicates that we can obtain sufficiently different classifiers when initializing the layer parameters differently.
MNIST→MNIST-M First, we evaluated the adaptation between the hand-written digit dataset, MNIST, and its transformed dataset, MNIST-M. MNIST-M was composed by merging clips of a background from the BSDS500 datasets (Arbelaez et al., 2011).",5.2. Experimental Result,[0],[0]
"A patch was randomly taken from the images in BSDS500, and merged with the MNIST digits.",5.2. Experimental Result,[0],[0]
"From 59,001 target training samples, we randomly selected 1,000 labeled target samples as a validation split and tuned the hyper-parameters.
",5.2. Experimental Result,[0],[0]
Our method outperformed the other existing method by about 7%.,5.2. Experimental Result,[0],[0]
Visualization of the features in the last pooling layer is shown in Fig. 3(a)(b).,5.2. Experimental Result,[0],[0]
We observed that the red target samples are more dispersed when adaptation is achieved.,5.2. Experimental Result,[0],[0]
A comparison of the accuracy between the actual labeling accuracy on the target samples during the training and the test accuracy is shown in Fig. 4.,5.2. Experimental Result,[0],[0]
"The test accuracy is very low initially, but as the steps increase, the accuracy becomes closer to that of the labeling accuracy.",5.2. Experimental Result,[0],[0]
"With this adaptation, we can clearly see that the actual labeling accuracy gradually improves with the accuracy of the network.",5.2. Experimental Result,[0],[0]
SVHN↔MNIST We increased the gap between distributions during this experiment.,5.2. Experimental Result,[0],[0]
"We evaluated the adaptation between SVHN (Netzer et al., 2011) and MNIST in a tenclass classification problem.",5.2. Experimental Result,[0],[0]
"SVHN and MNIST have distinct appearances, and thus this adaptation is a challenging scenario, particularly in MNIST→SVHN.",5.2. Experimental Result,[0],[0]
"The images in SVHN are colored, and some contain multiple digits.",5.2. Experimental Result,[0],[0]
"Therefore, a classifier trained on SVHN is expected to perform well on MNIST, but the reverse is not true.",5.2. Experimental Result,[0],[0]
"MNIST does not include any samples containing multiple digits,
and most of the samples are centered in the images, and thus adaptation from MNIST to SVHN is rather difficult.",5.2. Experimental Result,[0],[0]
"In both settings, we use 1,000 labeled target samples to find the optimal hyperparameters.
",5.2. Experimental Result,[0],[0]
We evaluated our method under both adaptation scenarios and achieved a state-of-the-art performance for both datasets.,5.2. Experimental Result,[0],[0]
"In particular, for the adaptation MNIST→SVHN, our method outperformed the other methods by more than 10%.",5.2. Experimental Result,[0],[0]
"In Fig. 3(c)(d), the representations in MNIST→SVHN are visualized.",5.2. Experimental Result,[0],[0]
"Although the distributions seem to be separated between domains, the red SVHN samples become more discriminative when using our method compared with non-adapted embedding.",5.2. Experimental Result,[0],[0]
A comparison between the actual labeling method accuracy and the testing accuracy is also shown in Fig. 4(b)(c).,5.2. Experimental Result,[0],[0]
"In this figure, it can be seen that the labeling accuracy rapidly decreases during the initial adaptation stage.",5.2. Experimental Result,[0],[0]
"On the other hand, the testing accuracy continues to improve, and finally exceeds the labeling accuracy.",5.2. Experimental Result,[0],[0]
There are two questions regarding this interesting phenomenon.,5.2. Experimental Result,[0],[0]
"The first is why does the labeling method continue to decrease despite the increase in the
0 5 10 15 20 25 30
Number of steps
0.4
0.5
0.6
0.7
0.8
0.9
1
A c c u",5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u m
b e r
o f s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(a) MNIST→MNIST-M
0 5 10 15 20 25 30
Number of steps
0.7
0.75
0.8
0.85
0.9
0.95
A",5.2. Experimental Result,[0],[0]
c c u,5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u m
b e r
o f s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(b) SVHN→MNIST
0 5 10 15 20 25 30
Number of steps
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
A",5.2. Experimental Result,[0],[0]
c c u,5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u m
b e r
o f s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(c) MNIST→SVHN
0 5 10 15 20 25 30
Number of steps
0.84
0.86
0.88
0.9
0.92
0.94
A",5.2. Experimental Result,[0],[0]
c c u,5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u
m b e
r o f
s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(d) SYNDIGITS→SVHN
0 5 10 15 20 25 30
Number of steps
0.75
0.8
0.85
0.9
0.95
1
A",5.2. Experimental Result,[0],[0]
c c u,5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u
m b e
r o f
s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(e) SYNSIGNS→GTSRB
0 5 10 15 20 25 30
Number of steps
0.65
0.7
0.75
0.8
0.85
",5.2. Experimental Result,[0],[0]
"0.9
A c c u",5.2. Experimental Result,[0],[0]
"ra
c y
Accuracy of Target Network Accuracy of Network1 Accuracy of Network2
(f) Comparision of accuracy of three networks on SVHN→MNIST (g) A-distance in MNIST→MNISTM
Figure 4.",5.2. Experimental Result,[0],[0]
(a) ∼ (e): Comparison of the actual accuracy of the pseudo-labels and the learned network accuracy during training.,5.2. Experimental Result,[0],[0]
"The blue curve indicates the pseudo-label accuracy, and the red curve is the learned network accuracy.",5.2. Experimental Result,[0],[0]
Note that the labeling accuracy is computed using (the number of correctly labeled samples)/(the number of labeled samples).,5.2. Experimental Result,[0],[0]
The green curve shows the number of labeled target samples in each step.,5.2. Experimental Result,[0],[0]
(f): Comparison of the accuracy of the three networks in our model.,5.2. Experimental Result,[0],[0]
The accuracy of the three networks improved almost simultaneously.,5.2. Experimental Result,[0],[0]
(g): Comparison of the A-distance of the different methods.,5.2. Experimental Result,[0],[0]
"Our model slightly reduced the divergence of the domain compared with the source-only trained CNN.
",5.2. Experimental Result,[0],[0]
testing accuracy?,5.2. Experimental Result,[0],[0]
"Target samples given pseudo-labels always include mistakenly labeled samples, whereas those given no labels are ignored in our method.",5.2. Experimental Result,[0],[0]
"Therefore, an error will be reinforced in the target samples included in the training set.",5.2. Experimental Result,[0],[0]
The second question is why does the test accuracy continue to increase despite the lower labeling accuracy?,5.2. Experimental Result,[0],[0]
"The assumed reason is that the network already acquires target discriminative representations during this phase, which can improve the accuracy when using source samples and correctly labeled target samples.
",5.2. Experimental Result,[0],[0]
"In Fig. 4(f), we show a comparison of the accuracy of the three networks F1, F2, and Ft in SVHN→MNIST.",5.2. Experimental Result,[0],[0]
The accuracy of these networks is nearly the same during every step.,5.2. Experimental Result,[0],[0]
The same situation was observed for the other scenarios.,5.2. Experimental Result,[0],[0]
"Based on this result, we can state that targetdiscriminative representations are shared in three networks.
SYN DIGITS→SVHN With this experiment, we aimed to address a common adaptation scenario from synthetic images to real images.",5.2. Experimental Result,[0],[0]
"The datasets of synthetic numbers (Ganin & Lempitsky, 2014) consist of 500,000 images generated from Windows fonts by varying the text, positioning, orientation, background and stroke colors, and the amount of blur.",5.2. Experimental Result,[0],[0]
"We used 479,400 source samples and 73,257 target samples for training, and 26,032 target samples for testing.",5.2. Experimental Result,[0],[0]
"In addition, we used 1,000 SVHN samples as the validation set.
",5.2. Experimental Result,[0],[0]
Our method also outperformed the other methods during this experiment.,5.2. Experimental Result,[0],[0]
"With this experiment, the effect of BN is not clear as compared with the other scenarios.",5.2. Experimental Result,[0],[0]
"The domain gap is considered small in this scenario, as the performance of the source-only classifier illustrates.",5.2. Experimental Result,[0],[0]
"In Fig. 4(d), although the labeling accuracy decreases, the accuracy of the learned network prediction improves, as in MNIST↔SVHN.
",5.2. Experimental Result,[0],[0]
SYN SIGNS→GTSRB,5.2. Experimental Result,[0],[0]
"This setting is similar to the previous one, adaptation from synthetic images to real images, but we have a larger number of classes, namely, 43 classes instead of ten.",5.2. Experimental Result,[0],[0]
"We used the SYN SIGNS dataset (Ganin & Lempitsky, 2014) for the source, and the GTSRB dataset (Stallkamp et al., 2011) for the target, which consist of real images of traffic signs.",5.2. Experimental Result,[0],[0]
"We randomly selected 31,367 samples for the target training samples and evaluated the accuracy on the remaining samples.",5.2. Experimental Result,[0],[0]
"A total of 3,000 labeled target samples were used for validation.
",5.2. Experimental Result,[0],[0]
"Under this scenario, our method outperformed the other methods, which indicates that our method is effective for the adaptation from synthesized images to real images with diverse classes.",5.2. Experimental Result,[0],[0]
"As shown in Fig. 4(e), the same tendency as in MNIST↔SVHN was observed for this adaptation scenario.
",5.2. Experimental Result,[0],[0]
Gradient Stop Experiment We evaluated the effects of a target-specific network using our method.,5.2. Experimental Result,[0],[0]
"We stopped the
gradient from the upper layer networks F1, F2, and Ft to examine the effect on Ft. Table 2 shows three scenarios, including the case in which we stopped the gradients from F1, F2, and Ft.
",5.2. Experimental Result,[0],[0]
"In the experiment on MNIST→MNIST-M, we assumed that only the backpropagation from F1 and F2 cannot construct discriminative representations for the target samples, and confirmed the effect of Ft.",5.2. Experimental Result,[0],[0]
"For the adaptation on MNIST→SVHN, the best performance was realized when F received all gradients from the upper networks.",5.2. Experimental Result,[0],[0]
Backwarding all gradients ensures both target-specific discriminative representations in difficult adaptations.,5.2. Experimental Result,[0],[0]
"In SYN SIGNS→GTSRB, backwarding only from Ft results in the worst performance because these domains are similar, and noisy pseudo-labeled samples worsen the performance.
",5.2. Experimental Result,[0],[0]
"A-distance Based on the theoretical results in (Ben-David et al., 2010), the A-distance is usually used as a measure of domain discrepancy.",5.2. Experimental Result,[0],[0]
The method of estimating the empirical A-distance is simple: We train a classifier to classify a domain from each domains’ feature.,5.2. Experimental Result,[0],[0]
"The approximate distance is then calculated as d̂A = 2(1 − 2ϵ), where ϵ is a generalization error of the classifier.",5.2. Experimental Result,[0],[0]
"We compared our method with the distribution matching methods, DANN and MMD.",5.2. Experimental Result,[0],[0]
We calculated the distance using the last pooling layer features.,5.2. Experimental Result,[0],[0]
"We followed the implementation of DANN (Ganin et al., 2016) for the training.",5.2. Experimental Result,[0],[0]
"For MMD training, we followed the implementation in (Bousmalis et al., 2016).",5.2. Experimental Result,[0],[0]
"In Fig. 4(g), the A-distance calculated from each CNN feature is shown.",5.2. Experimental Result,[0],[0]
We used a linear SVM to calculate the distance.,5.2. Experimental Result,[0],[0]
"From this graph, we can see that our method clearly reduces the A-distance compared with the CNN trained on only the source samples.",5.2. Experimental Result,[0],[0]
"In addition, when comparing the distribution matching methods against our own, although the former reduce the A-distance much more, our method shows a superior performance as shown in Table 1.
Semi-supervised domain adaptation We evaluated our model in a semi-supervised domain adaptation setting on MNIST→SVHN.",5.2. Experimental Result,[0],[0]
"We randomly selected the labeled target samples for each class, and reported the mean accuracy for
ten experiments.",5.2. Experimental Result,[0],[0]
The resulting accuracy was 58% on average when using ten labeled target samples per class.,5.2. Experimental Result,[0],[0]
We can see the effectiveness of our method in a semi-supervised setting.,5.2. Experimental Result,[0],[0]
"A detailed explanation of this is given in our Supplementary materials section.
",5.2. Experimental Result,[0],[0]
"Amazon Reviews The reviews were encoded in 5,000 dimensional vectors of bag-of-word unigrams and bigrams with binary labels.",5.2. Experimental Result,[0],[0]
Negative labels were attached to the samples if they were ranked with 1 to 3 stars.,5.2. Experimental Result,[0],[0]
Positive labels were attached if they were ranked with 4 or 5 stars.,5.2. Experimental Result,[0],[0]
"We used 2,000 labeled source samples and 2,000 unlabeled target samples for the training, and between 3,000 and 6,000 samples for the testing.",5.2. Experimental Result,[0],[0]
"We used 200 labeled target samples for validation.
",5.2. Experimental Result,[0],[0]
"Based on the results in Table 3, our method performed better than VFAE (Louizos et al., 2015) and DANN (Ganin et al., 2016) in nine out of twelve settings.",5.2. Experimental Result,[0],[0]
Our method was shown to be effective in learning a shallow network on different domains.,5.2. Experimental Result,[0],[0]
"In this paper, we proposed a novel asymmetric tri-training method for unsupervised domain adaptation, which is implemented in a simple manner.",6. Conclusion,[0],[0]
We aimed at learning discriminative representations by utilizing pseudo-labels assigned to unlabeled target samples.,6. Conclusion,[0],[0]
"We utilized three classifiers, two networks assigned pseudo-labels to unlabeled target samples, and the remaining network, which learned from them.",6. Conclusion,[0],[0]
"We evaluated our method regarding both domain adaptation for a visual recognition and a sentiment analysis, and the results show that we outperformed all other methods.",6. Conclusion,[0],[0]
"In particular, our method outperformed the other methods by more than 10% for MNIST→SVHN.",6. Conclusion,[0],[0]
"This work was partially funded by the ImPACT Program of the Council for Science, Technology, and Innovation (Cabinet Office, Government of Japan), and was partially supported by CREST, JST.",7. Acknowledgement,[0],[0]
It is important to apply models trained on a large number of labeled samples to different domains because collecting many labeled samples in various domains is expensive.,abstractText,[0],[0]
"To learn discriminative representations for the target domain, we assume that artificially labeling the target samples can result in a good representation.",abstractText,[0],[0]
"Tritraining leverages three classifiers equally to provide pseudo-labels to unlabeled samples; however, the method does not assume labeling samples generated from a different domain.",abstractText,[0],[0]
"In this paper, we propose the use of an asymmetric tritraining method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train the neural networks as if they are true labels.",abstractText,[0],[0]
"In our work, we use three networks asymmetrically, and by asymmetric, we mean that two networks are used to label unlabeled target samples, and one network is trained by the pseudo-labeled samples to obtain target-discriminative representations.",abstractText,[0],[0]
Our proposed method was shown to achieve a stateof-the-art performance on the benchmark digit recognition datasets for domain adaptation.,abstractText,[0],[0]
Asymmetric Tri-training for Unsupervised Domain Adaptation,title,[0],[0]
"Deep Neural Networks (DNN) have pushed the frontiers of many applications, such as speech recognition (Sak et al., 2014; Sercu et al., 2016), computer vision (Krizhevsky et al., 2012; He et al., 2016; Szegedy et al., 2016), and natural language processing (Mikolov et al., 2013; Bahdanau et al., 2014; Gehring et al., 2017).",1. Introduction,[0],[0]
"Part of the success of DNN should be attributed to the availability of big training data and powerful computational resources, which allow people to learn very deep and big DNN models in parallel
1University of Science and Technology of China 2School of Mathematical Sciences, Peking University 3Microsoft Research 4Academy of Mathematics and Systems Science, Chinese Academy of Sciences.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Taifeng Wang, Wei Chen <taifengw, wche@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"(Zhang et al., 2015; Chen & Huo, 2016; Chen et al., 2016).
",1. Introduction,[0],[0]
"Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks (Bottou, 2012; Dean et al., 2012; Kingma & Ba, 2014).",1. Introduction,[0],[0]
"As for the parallelization of SGD algorithms (suppose we use M machines for the parallelization), one can choose to do it in either a synchronous or asynchronous way.",1. Introduction,[0],[0]
"In synchronous SGD (SSGD), local workers compute the gradients over their own mini-batches of data, and then add the gradients to the global model.",1. Introduction,[0],[0]
"By using a barrier, these workers wait for each other, and will not continue their local training until the gradients from all the M workers have been added to the global model.",1. Introduction,[0],[0]
It is clear that the training speed will be dragged by the slowest worker1.,1. Introduction,[0],[0]
"To improve the training efficiency, asynchronous SGD (ASGD) (Dean et al., 2012) has been adopted, with which no barrier is imposed, and each local worker continues its training process right after its gradient is added to the global model.",1. Introduction,[0],[0]
"Although ASGD can achieve faster speed due to no waiting overhead, it suffers from another problem which we call delayed gradient.",1. Introduction,[0],[0]
"That is, before a worker wants to add its gradient g(wt) (calculated based on the model snapshot wt) to the global model, several other workers may have already added their gradients and the global model has been updated to wt+τ (here τ is called the delay factor).",1. Introduction,[0],[0]
"Adding gradient of model wt to another model wt+τ does not make a mathematical sense, and the training trajectory may suffer from unexpected turbulence.",1. Introduction,[0],[0]
"This problem has been well known, and some researchers have analyzed its negative effect on the convergence speed (Lian et al., 2015; Avron et al., 2015).
",1. Introduction,[0],[0]
"In this paper, we propose a novel method, called Delay Compensated ASGD (or DC-ASGD for short), to tackle the problem of delayed gradients.",1. Introduction,[0],[0]
"For this purpose, we study the Taylor expansion of the gradient function g(wt+τ ) at wt.",1. Introduction,[0],[0]
"We find that the delayed gradient g(wt) is just the zero-order approximator of the correct gradient g(wt+τ ), and we can leverage more items in the Taylor expansion to achieve more accurate approximation of g(wt+τ ).",1. Introduction,[0],[0]
"However, this straightforward idea is practically non-trivial, be-
1Recently, people proposed to use additional backup workers (Chen et al., 2016) to tackle this problem.",1. Introduction,[0],[0]
"However, this solution requires redundant computation resources and relies on the assumption that the majority of workers train almost equally fast.
",1. Introduction,[0],[0]
cause even including the first-order derivative of the gradient g(wt+τ ),1. Introduction,[0],[0]
"will require the computation of the secondorder derivative of the original loss function (i.e., the Hessian matrix), which will introduce high computation and space complexity.",1. Introduction,[0],[0]
"To overcome this challenge, we propose a cheap yet effective approximator of the Hessian matrix, which can achieve a good trade-off between bias and variance of approximation, only based on previously available gradients (without the necessity of directly computing the Hessian matrix).
",1. Introduction,[0],[0]
DC-ASGD is similar to ASGD in the sense that no worker needs to wait for others.,1. Introduction,[0],[0]
"It differs from ASGD in that it does not directly add the local gradient to the global model, but compensates the delay in the local gradient by using the approximate Taylor expansion.",1. Introduction,[0],[0]
"By doing so, it maintains almost the same efficiency as ASGD and achieves much higher accuracy.",1. Introduction,[0],[0]
"Theoretically, we proved that DC-ASGD can converge at a rate of the same order with sequential SGD for non-convex neural networks, if the delay is upper bounded; and it is more tolerant on the delay than ASGD2.",1. Introduction,[0],[0]
"Empirically, we conducted experiments on both CIFAR-10 and ImageNet datasets.",1. Introduction,[0],[0]
"The results show that (1) as compared to SSGD and ASGD, DC-ASGD accelerated the convergence of the training process; (2) the accuracy of the model obtained by DC-ASGD within the same time period is very close to the accuracy obtained by sequential SGD.",1. Introduction,[0],[0]
"In this section, we introduce DNN and its parallel training through ASGD.
",2. Problem Setting,[0],[0]
"Given a multi-class classification problem, we denote X = Rd as the input space, Y = {1, ...,K} as the output space, and P as the joint distribution over X × Y .",2. Problem Setting,[0],[0]
"Here d denotes the dimension of the input space, and K denotes the number of categories in the output space.
",2. Problem Setting,[0],[0]
"We have a training set {(x1, y1), ..., (xS , yS)}, whose elements are i.i.d. sampled from X × Y according to distribution",2. Problem Setting,[0],[0]
P. Our goal is to learn a neural network model O ∈,2. Problem Setting,[0],[0]
F : X × Y → R parameterized by w ∈,2. Problem Setting,[0],[0]
Rn based on the training set.,2. Problem Setting,[0],[0]
"Specifically, the neural network models have hierarchical structures, in which each node conducts linear combination and non-linear activation over its connected nodes in the lower layer.",2. Problem Setting,[0],[0]
The parameters are the weights on the edges between two layers.,2. Problem Setting,[0],[0]
"The neural network model produces an output vector, i.e., (O(x, k;w); k ∈ Y) for each input x ∈ X , indicating its likelihoods of belonging to different categories.",2. Problem Setting,[0],[0]
"Because the underlying distribution P is unknown, a common way of learning the model is to minimize the empirical loss function.",2. Problem Setting,[0],[0]
"A widely-used loss function for deep neural networks is the cross-entropy loss,
2We also obtained similar results for the convex cases.",2. Problem Setting,[0],[0]
"Due to space restrictions, we put the corresponding theorems and proofs in the appendix.
which is defined as follows,
f(x, y;w) =",2. Problem Setting,[0],[0]
"− K∑
k=1
(I[y=k] log σk(x;w)).",2. Problem Setting,[0],[0]
"(1)
Here σk(x;w) = e O(x,k;w)∑K
k′=1 e O(x,k′;w) is the Softmax operator.
",2. Problem Setting,[0],[0]
"The objective is to optimize the empirical risk, defined as below,
F (w) = 1 S S∑ s=1 fs(w)",2. Problem Setting,[0],[0]
":= 1 S S∑ s=1 f(xs, ys;w).",2. Problem Setting,[0],[0]
"(2)
As mentioned in the introduction, ASGD is a widely-used approach to perform parallel training of neural networks.",2. Problem Setting,[0],[0]
"Although ASGD is highly efficient, it is well known to suffer from the problem of delayed gradient.",2. Problem Setting,[0],[0]
"To better illustrate this problem, let us have a close look at the training process of ASGD as shown in Figure 1.",2. Problem Setting,[0],[0]
"According to the figure, local worker m starts from wt, the snapshot of the global model at time t, calculates the local gradient g(wt), and then add this gradient back to the global model3.",2. Problem Setting,[0],[0]
"However, before this happens, some other τ workers may have already added their local gradients to the global model, the global model has been updated τ times and becomes wt+τ .",2. Problem Setting,[0],[0]
"The ASGD algorithm is blind to this situation, and simply adds the gradient g(wt) to the global model wt+τ , as follows.
wt+τ+1 = wt+τ",2. Problem Setting,[0],[0]
"− ηg(wt), (3)
where η is the learning rate.
",2. Problem Setting,[0],[0]
It is clear that the above update rule of ASGD is problematic (and inequivalent to that of sequential SGD): one actually adds a “delayed” gradient g(wt) to the current global model wt+τ .,2. Problem Setting,[0],[0]
"In contrast, the correct way is to update the global model wt+τ based on the gradient w.r.t.",2. Problem Setting,[0],[0]
wt+τ .,2. Problem Setting,[0],[0]
"This
3Actually, the local gradient is also related to the randomly sampled data (xit , yit).",2. Problem Setting,[0],[0]
"For simplicity, when there is no confusion, we will omit xit , yit in the notations.
problem of delayed gradient has been well known (Agarwal & Duchi, 2011; Recht et al., 2011; Lian et al., 2015; Avron et al., 2015), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach accuracy parity of sequential SGD, especially when the number of workers is large (Dean et al., 2012; Ho et al., 2013; Zhang et al., 2015).",2. Problem Setting,[0],[0]
"Researchers have tried to improve ASGD from different perspectives (Ho et al., 2013; McMahan & Streeter, 2014; Zhang et al., 2015; Sra et al., 2015; Mitliagkas et al., 2016), however, to the best of our knowledge, there is still no solution that can compensate the delayed gradient while keeping the high efficiency of ASGD.",2. Problem Setting,[0],[0]
This is exactly the motivation of our paper.,2. Problem Setting,[0],[0]
"As explained in the previous sections, ideally, the optimization algorithm should add gradient g(wt+τ ) to the global model wt+τ , however, ASGD adds a delayed version g(wt).",3. Delay Compensation using Taylor Expansion and Hessian Approximation,[0],[0]
"In this section, we propose a novel method to bridge this gap by using Taylor expansion and Hessian approximation.",3. Delay Compensation using Taylor Expansion and Hessian Approximation,[0],[0]
"The Taylor expansion of the gradient function g(wt+τ ) at wt can be written as follows (Folland, 2005),
g(wt+τ )",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
= g(wt)+∇g(wt)(wt+τ −wt)+O((wt+τ,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"−wt)2)In, (4)
where ∇g denotes the matrix with the element gij = ∂2f",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
∂wi∂wj for i ∈,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
[n],3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
and j ∈,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"[n], (wt+τ − wt)2 =
(wt+τ,1−wt,1)α1 · · · (wt+τ,n−wt,n)αn with ∑n
i=1 αi = 2",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
and αi ∈ N,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"and In is a n-dimension vector with all the elements equal to 1.
",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
By comparing the above formula with Eqn.,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"(3), we can immediately find that ASGD actually uses the zero-order item in Taylor expansion as its approximation to g(wt+τ ), and totally ignores all the higher-order terms ∇g(wt)(wt+τ",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
− wt) + O((wt+τ,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
− wt)2)In.,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
This is exactly the root cause of the problem of delayed gradient.,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"With this insight, a straightforward and ideal method is to use the full Taylor expansion to compensate the delay.",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"However, this is practically intractable, since it involves the sum of an infinite number of items.",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"And even the simplest delay compensation, i.e., additionally keeping the first-order item in the Taylor expansion (which is shown below), is highly nontrivial,
g(wt+τ )",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
≈ g(wt),3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
+∇g(wt)(wt+τ,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
− wt).,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"(5)
This is because the first-order derivative of the gradient function g corresponds to the Hessian matrix of the original loss function f (e.g., cross entropy for neural net-
works), which is defined as Hf(w) =",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"[hij ]i,j=1,··· ,n where hij =
∂2f ∂wi∂wj (w).
",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"For a neural network model with millions of parameters (which is very common and may only be regarded as a medium-size network today), the corresponding Hessian matrix will contain trillions of elements.",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
It is clearly very computationally and spatially expensive to obtain such a large matrix4.,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"Fortunately, as shown in the next subsection, we find an easy-to-compute/store approximator to the Hessian matrix, which makes our proposal of delay compensation technically feasible.",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"Computing the exact Hessian matrix is computationally and spatially expensive, especially for large models.",3.2. Approximation of Hessian Matrix,[0],[0]
"Alternatively, we want to find some approximators that are theoretically close to the Hessian matrix, but can be easily stored and computed without introducing additional complexity (i.e., just using what we already have during the previous training process).
",3.2. Approximation of Hessian Matrix,[0],[0]
"First, we show that the outer product of the gradients is an asymptotically unbiased estimation of the Hessian matrix.",3.2. Approximation of Hessian Matrix,[0],[0]
"Let us use G(wt) to denote the outer product matrix of the gradient at wt, i.e.,
G(wt) =",3.2. Approximation of Hessian Matrix,[0],[0]
"( ∂
∂w f(x, y,wt)
)( ∂
∂w f(x, y,wt)
)T .",3.2. Approximation of Hessian Matrix,[0],[0]
"(6)
Because the cross entropy loss is a negative log-likelihood with respect to the Softmax distribution of the model, i.e., P(Y = k|x,wt) , σk(x;wt), it is not difficult to obtain that the outer product of the gradient is an asymptotically unbiased estimation of Hessian, according to the two equivalent methods to calculate the fisher information matrix (Friedman et al., 2001)5:
ϵt , E(y|x,w∗)||G(wt)−H(wt)|| → 0, t → ∞. (7)
",3.2. Approximation of Hessian Matrix,[0],[0]
The assumption behind the above equivalence is that the underlying distribution equals the model distribution with parameter w∗ (or there is no approximation error of the NN hypothesis space) and the training model wt gradually converges to the optimal model w∗ along with the training process.,3.2. Approximation of Hessian Matrix,[0],[0]
"This assumption is reasonable considering the universal approximation property of DNN (Hornik, 1991) and the recent results on the optimality of the local optima of DNN (Choromanska et al., 2015; Kawaguchi, 2016).
",3.2. Approximation of Hessian Matrix,[0],[0]
"Second, we show that by further introducing a welldesigned weight to the outer product of the gradients, we
4Although Hessian-free methods were used in some previous works (Martens, 2010), they double the computation and communication for each local worker and are therefore not very feasible in practice.
",3.2. Approximation of Hessian Matrix,[0],[0]
"5In this paper, the norm of the matrix is Frobenius norm.
can achieve a better trade-off between bias and variance for the approximation.
",3.2. Approximation of Hessian Matrix,[0],[0]
"Although the outer product of the gradients can achieve unbiased estimation to the Hessian matrix, it may induce high approximation error due to potentially large variance.",3.2. Approximation of Hessian Matrix,[0],[0]
"To further control the variance, we use mean square error (MSE) to measure the quality of an approximator, which is defined as follows,
mset(G) =",3.2. Approximation of Hessian Matrix,[0],[0]
"E(y|x,w∗)∥ ( G(wt)−H(wt) ) ||2.",3.2. Approximation of Hessian Matrix,[0],[0]
"(8)
We consider the following new approximator λG(wt) ∆ =",3.2. Approximation of Hessian Matrix,[0],[0]
"[ λgtij ] , and prove that with appropriately set λ, λG(wt) can lead to smaller MSE than G(wt), for arbitrary model wt during the training.
",3.2. Approximation of Hessian Matrix,[0],[0]
"Theorem 3.1 Assume that the loss function is L1-Lipschitz, and for arbitrary k ∈",3.2. Approximation of Hessian Matrix,[0],[0]
"[K], ∣∣∣ ∂σk∂wi ∣∣∣ ∈",3.2. Approximation of Hessian Matrix,[0],[0]
"[li, ui], |σk(x,w∗)σk(x,wt) | ∈",3.2. Approximation of Hessian Matrix,[0],[0]
"[α, β].",3.2. Approximation of Hessian Matrix,[0],[0]
If λ ∈,3.2. Approximation of Hessian Matrix,[0],[0]
"[0, 1] makes the following inequality holds,
K∑ k=1
1
σ3k(x,wt) ≥ 2C ( K∑ k=1
1
σk(x,wt)
)2 + 2L21ϵt  , (9) where C = maxi,j 11+λ ( uiujβ liljα )2, and the model wt converges
to the optimal model w∗, then mset(λG) ≤ mset(G).
",3.2. Approximation of Hessian Matrix,[0],[0]
"The following corollary gives simpler sufficient conditions for Theorem 3.1.
",3.2. Approximation of Hessian Matrix,[0],[0]
Corollary 3.2,3.2. Approximation of Hessian Matrix,[0],[0]
A sufficient condition for inequality (9) is ∃k0 ∈,3.2. Approximation of Hessian Matrix,[0],[0]
[K] such that σk0 ∈,3.2. Approximation of Hessian Matrix,[0],[0]
"[ 1− K−1 2C(K2+L21ϵt) , 1 ] .
",3.2. Approximation of Hessian Matrix,[0],[0]
"According to Corollary 3.2, we have the following discussions.",3.2. Approximation of Hessian Matrix,[0],[0]
"Please note that, if wt converges to w∗, ϵt is a decreasing term and approaches 0.",3.2. Approximation of Hessian Matrix,[0],[0]
"Thus, ϵt can be upper bounded by a very small constant for large t. Therefore, the condition on σk(x,wt) is more likely to be satisfied when σk(x,wt) (∃k ∈",3.2. Approximation of Hessian Matrix,[0],[0]
[K]) is close to 1.,3.2. Approximation of Hessian Matrix,[0],[0]
"Please note that this is not a strong condition, since if σk(x,wt) (∀k ∈",3.2. Approximation of Hessian Matrix,[0],[0]
"[K]) is very small, the classification power of the corresponding neural network model will be very weak and not useful in practice.
",3.2. Approximation of Hessian Matrix,[0],[0]
"Third, to reduce the storage of the approximator λG(w), we adopt a widely-used diagonalization trick (Becker et al., 1988), which has shown promising empirical results.",3.2. Approximation of Hessian Matrix,[0],[0]
"To be specific, we only store the diagonal elements of the approximator λG(w) and make all the other elements to be zero.",3.2. Approximation of Hessian Matrix,[0],[0]
"We denote the refined approximator as Diag(λG(w)) and assume that the diagonalization error is upper bounded by ϵD , i.e., ||Diag(H(wt))",3.2. Approximation of Hessian Matrix,[0],[0]
− H(wt)|| ≤ ϵD .,3.2. Approximation of Hessian Matrix,[0],[0]
"We give a uniform upper bound of its MSE in the supplementary materials, from which we can see that λ plays a role of trading off variance and Lipschitz6.
4.",3.2. Approximation of Hessian Matrix,[0],[0]
Delay Compensated ASGD:,3.2. Approximation of Hessian Matrix,[0],[0]
"Algorithm Description
In Section 3, we have shown that Diag(λG(w)) is a cheap approximator of the Hessian matrix, with guaranteed approxi-
6See Lemma 3.1 in Supplementary.
",3.2. Approximation of Hessian Matrix,[0],[0]
Algorithm 1 DC-ASGD:,3.2. Approximation of Hessian Matrix,[0],[0]
"worker m repeat
Pull wt from the parameter server.",3.2. Approximation of Hessian Matrix,[0],[0]
Compute gradient gm = ∇fm(wt).,3.2. Approximation of Hessian Matrix,[0],[0]
"Push gm to the parameter server.
until forever
Algorithm 2 DC-ASGD: parameter server Input: learning rate η, variance control parameter λt.",3.2. Approximation of Hessian Matrix,[0],[0]
"Initialize: t = 0, w0 is initialized randomly, wbak(m) = w0, m ∈ {1, 2, · · · ,M} repeat
if receive “gm"" then wt+1 ← wt−η· ( gm+λtgm⊙gm⊙(wt−wbak(m)) )",3.2. Approximation of Hessian Matrix,[0],[0]
t←,3.2. Approximation of Hessian Matrix,[0],[0]
"t+ 1
else if receive “pull request” then wbak(m)← wt Send wt back to worker m.
end if until forever
mation accuracy.",3.2. Approximation of Hessian Matrix,[0],[0]
"In this section, we will use this approximator to compensate the gradient delay, and call the corresponding algorithm Delay-Compensated ASGD (DC-ASGD).",3.2. Approximation of Hessian Matrix,[0],[0]
"Since Diag(λG(w)) = λg(wt) ⊙ g(wt), where ⊙ indicates the element-wise product, the update rule for DC-ASGD can be written as follows:
wt+τ+1 = wt+τ",3.2. Approximation of Hessian Matrix,[0],[0]
− η (g(wt) + λg(wt)⊙,3.2. Approximation of Hessian Matrix,[0],[0]
"g(wt)⊙ (wt+τ − wt)) , (10)
We call g(wt) + λg(wt) ⊙ g(wt) ⊙ (wt+τ − wt) the delaycompensated gradient for ease of reference.
",3.2. Approximation of Hessian Matrix,[0],[0]
The flow of DC-ASGD is shown in Algorithms 1 and 2.,3.2. Approximation of Hessian Matrix,[0],[0]
Here we assume that DC-ASGD is implemented by using the parameter server framework (although it can also be implemented in other frameworks).,3.2. Approximation of Hessian Matrix,[0],[0]
"According to Algorithm 1, local worker m pulls the latest global model wt from the parameter server, computes its gradient gm and sends it back to the server.",3.2. Approximation of Hessian Matrix,[0],[0]
"According to Algorithm 2, the parameter server will store a backup model wbak(m) when worker m pulls wt.",3.2. Approximation of Hessian Matrix,[0],[0]
"When the delayed gradient gm calculated by worker m is received at time t, the parameter server updates the global model according to Eqn (10).
",3.2. Approximation of Hessian Matrix,[0],[0]
"Please note that as compared to ASGD, DC-ASGD has no extra communication cost and no extra computational requirement on the local workers.",3.2. Approximation of Hessian Matrix,[0],[0]
And the additional computations regarding Eqn(10) only introduce a lightweight overhead to the parameter server.,3.2. Approximation of Hessian Matrix,[0],[0]
"As for the space requirement, for each worker m ∈ {1, 2, · · · ,M}, the parameter server needs to additionally store a backup model wbak(m).",3.2. Approximation of Hessian Matrix,[0],[0]
"This is not a critical issue since the parameter server is usually implemented in a distributed manner, and the parameters and its backup version are stored in CPU-side memory which is usually far beyond the total parameter size.",3.2. Approximation of Hessian Matrix,[0],[0]
"In this case, the cost of DC-ASGD is quite similar to ASGD, which is also reflected by our experiments.
",3.2. Approximation of Hessian Matrix,[0],[0]
The Delay Compensation is not only applicable to ASGD but SSGD.,3.2. Approximation of Hessian Matrix,[0],[0]
"Recently a study on SSGD(Goyal et al., 2017) assumes
g(wt+j) ≈ g(wt) for j < M to make the updates from small and large mini-batch SGD similar, which can be immediately improved by applying delay-compensated gradient.",3.2. Approximation of Hessian Matrix,[0],[0]
Please check the detailed discussion in Supplementary.,3.2. Approximation of Hessian Matrix,[0],[0]
"In this section, we prove the convergence rate of DC-ASGD.",5. Convergence Analysis,[0],[0]
"Due to space restrictions, we only give the results for the non-convex case, and leave the results for the convex case (which is much easier) to the supplementary.
",5. Convergence Analysis,[0],[0]
"In order to present our main theorem, we need to introduce the following mild assumptions.
",5. Convergence Analysis,[0],[0]
"Assumption 1 (Smoothness): (Lian et al., 2015)(Recht et al., 2011)",5. Convergence Analysis,[0],[0]
The loss function is smooth w.r.t.,5. Convergence Analysis,[0],[0]
"the model parameter, and we use L1, L2, L3 to denote the upper bounds of the first, second, and third-order derivatives of the loss function.",5. Convergence Analysis,[0],[0]
"The activation function σk(w) is L-Lipschitz continuous.
",5. Convergence Analysis,[0],[0]
"Assumption 2 (Non-convexity): (Lee et al., 2016)",5. Convergence Analysis,[0],[0]
"The loss function is µ-strongly convex in a ball centered at each local optimum which is denoted as d(wloc, r) with radius r, and twice differential about w.
We also introduce some notations to simplify the presentation of our results, i.e.,
M = max k,wloc
|P(Y = k|x,wloc)− P(Y = k|x,w∗)| ,
H = max k,x,w ∣∣∣∣∂2P(Y = k|x,w)∂2w × 1P(Y = k|x,w) ∣∣∣∣ ,
",5. Convergence Analysis,[0],[0]
∀k ∈,5. Convergence Analysis,[0],[0]
"[K], x, w.
Actually, the non-convexity error ϵnc = HKM , which is defined as the upper bound of the difference between the prediction outputs of the local optima and the global optimum (Please see Lemma 5.1 in the supplementary materials).",5. Convergence Analysis,[0],[0]
We assume that the DC-ASGD search in the set ∥w,5. Convergence Analysis,[0],[0]
"− w′∥22 ≤ π2, ∀w,w′ and denote D0 = F (w1)−F (w∗), C2λ = (L23π2/2+2((1−λ)L21+ ϵD)2+ 2ϵ2nc), C̃2λ = 4T0 maxs=1,··· ,T0 ϵs 2 + 4θ2 log (T − T0) where
T0 ≥ O(1/r4), θ = 2HKLV L2µ2 √ 1 µ ( 1 + L2+λL 2 1 L2 τ ) .
",5. Convergence Analysis,[0],[0]
"With all the above, we have the following theorem.
",5. Convergence Analysis,[0],[0]
Theorem 5.1 Assume that Assumptions 1-2 hold.,5. Convergence Analysis,[0],[0]
"Set the learning rate η = √ 2D0
bTL2V 2 ,where b is the mini-batch size, and V is
the upper bound of the variance of the delay-compensated gradient.",5. Convergence Analysis,[0],[0]
"If T ≥ max{O(1/r4), 2D0bL2/V 2} and delay τ is upperbounded as below,
τ ≤",5. Convergence Analysis,[0],[0]
"min { L2γ
Cλ , γ Cλ ,
√ Tγ
C̃ , L2Tγ 4C̃
} , (11)
",5. Convergence Analysis,[0],[0]
"where γ = √ L2TV 2
2D0b , then DC-ASGD has the following ergodic
convergence rate,
min t={1,··· ,T}
E(∥∇F (wt)∥2) ≤ V √
2D0L2 bT , (12)
where T is the number of iteration, the expectation is taken with respect to the random sampling in SGD and the data distribution P (Y |x,w∗).
",5. Convergence Analysis,[0],[0]
"Proof Sketch7:
Step 1: We denote the delay-compensated gradient as gdcm (wt) where m ∈ {1, · · · , b} is the index of instances in the mini-batch and ∇Fh(wt) = ∇F (wt) + EH(wt)(wt+τ",5. Convergence Analysis,[0],[0]
− wt).,5. Convergence Analysis,[0],[0]
"According to Assumption 1, we have
EF (wt+τ+1)− F (wt+τ )
≤",5. Convergence Analysis,[0],[0]
− bηt+τ 2 ∥∇F (wt+τ )∥2,5. Convergence Analysis,[0],[0]
"+ ∥∥∥∥∥ b∑
m=1
Egdcm (wt) ∥∥∥∥∥ 2 
+bηt+τ ∥∥∥∥∥∇F",5. Convergence Analysis,[0],[0]
(wt+τ ),5. Convergence Analysis,[0],[0]
"− b∑
m=1
∇Fh(wt) ∥∥∥∥∥ 2
+bηt+τ ∥∥∥∥∥ b∑
m=1
Egdcm (wt)− b∑
m=1
Fh(wt) ∥∥∥∥∥ 2
+ η2t+τL2
2 E ∥∥∥∥∥ b∑
m=1
gdcm (wt) ∥∥∥∥∥ 2  .",5. Convergence Analysis,[0],[0]
"(13)
The term ∥∥∥∑bm=1",5. Convergence Analysis,[0],[0]
"Egdcm (wt)−∑bm=1 Fh(wt)∥∥∥2, measured by the expectation with respect to P(Y |x,w∗), is bounded by C2λ · ∥wt+τ",5. Convergence Analysis,[0],[0]
− wt∥2.,5. Convergence Analysis,[0],[0]
"The term
∥∥∥∇F (wt+τ )",5. Convergence Analysis,[0],[0]
"−∑bm=1 ∇Fh(wt)∥∥∥2 can be bounded by L
2 3 4 ∥wt+τ −wt∥4, which will be smaller than
∥wt+τ",5. Convergence Analysis,[0],[0]
− wt∥2,5. Convergence Analysis,[0],[0]
when ∥wt+τ,5. Convergence Analysis,[0],[0]
− wt∥ is small.,5. Convergence Analysis,[0],[0]
"Other terms which are related to the gradients can be further upper bounded by the smoothness property of the loss function.
",5. Convergence Analysis,[0],[0]
"Step 2: We proved that, under the non-convexity assumption, if ∥λg(wt) ⊙ g(wt)∥ ≤",5. Convergence Analysis,[0],[0]
"λL21, then when t > O(1/r4), ϵt ≤ θ √
1 t−T0 + ϵnc, where T0 = O(1/r4).",5. Convergence Analysis,[0],[0]
"That is, we can find a weaker condition for the decreasing of ϵt than that for wt → w∗.
Step 3:",5. Convergence Analysis,[0],[0]
"By plugging in the decreasing rate of ϵt in Step 1 and following a similar proof of the convergence rate of ASGD (Lian et al., 2015), we can get the result in the theorem.
",5. Convergence Analysis,[0],[0]
"Discussions:
(1)",5. Convergence Analysis,[0],[0]
"The above theorem shows that the convergence rate of DCASGD is in the order of O( V√
T ).",5. Convergence Analysis,[0],[0]
"Recall that the convergence rate
of ASGD is O( V1√ T ), where V1 is the variance for the delayed gradient g(wt).",5. Convergence Analysis,[0],[0]
"By simple calculation, V can be upper bounded by V1 + λV2, where V2 is the extra moments of the noise introduced by the delay compensation term.",5. Convergence Analysis,[0],[0]
Thus if we set λ ∈,5. Convergence Analysis,[0],[0]
"[0, V1/V2], DC-ASGD and ASGD will converge at the same rate.",5. Convergence Analysis,[0],[0]
"As the training process goes on, g(w) will become smaller.",5. Convergence Analysis,[0],[0]
"Compared with V1, V2 (composed by variance of g ⊙ g) will not be the dominant order and can be gradually neglected.",5. Convergence Analysis,[0],[0]
"As a result, the feasible range for λ is actually very large.
",5. Convergence Analysis,[0],[0]
"(2) Although DC-ASGD converges at the same rate with ASGD, its tolerance on the delay is much better if T ≥ max{C̃2, 4C̃/L2} and Cλ < min{L2, 1}.",5. Convergence Analysis,[0],[0]
The intuition for the condition on T is that larger T induces smaller step size η.,5. Convergence Analysis,[0],[0]
A small step size means that wt and wt+τ are close to each other.,5. Convergence Analysis,[0],[0]
"According to the upper bound of Taylor expansion series (Folland, 2005), we can see that delay compensated gradient will be more
7Please check the complete proof in the supplementary material.
",5. Convergence Analysis,[0],[0]
accurate than the delayed gradient used in ASGD.,5. Convergence Analysis,[0],[0]
"Since Cλ is related to the diagonalization error ϵD and the non-convexity error ϵnc, smaller ϵD and ϵnc will lead to looser conditions for the convergence.",5. Convergence Analysis,[0],[0]
"If these two error are sufficiently small (which is usually the case according to (Choromanska et al., 2015; Kawaguchi, 2016; LeCun, 1987)), the condition L2 > Cλ can be simplified as L2 > (1 − λ)L21 +",5. Convergence Analysis,[0],[0]
"L3π, which is easy to be satisfied with a small 1−λ.",5. Convergence Analysis,[0],[0]
"Assume that L2−L3π > 0, which is easily to be satisfied if the gradient is small (e.g. at the later stage of the training progress).",5. Convergence Analysis,[0],[0]
"Accordingly, we can obtain the feasible range for λ as λ ∈",5. Convergence Analysis,[0],[0]
"[1 − (L2 − L3π)/2L21, 1].",5. Convergence Analysis,[0],[0]
"λ can be regarded as a trade-off between the extra variance introduced by the delay-compensate term λg(wt)⊙ g(wt) and the bias in Hessian approximation.
",5. Convergence Analysis,[0],[0]
"(3) Actually ASGD is an extreme case for DC-ASGD, with λ = 0.",5. Convergence Analysis,[0],[0]
Another extreme case is with λ = 1.,5. Convergence Analysis,[0],[0]
"DC-ASGD prefers larger T and smaller π, which can lead to a faster speed-up and larger tolerant for delay.
",5. Convergence Analysis,[0],[0]
"Based on the above discussions, we have the following corollary, which indicates that DC-ASGD is superior to ASGD in most cases.
",5. Convergence Analysis,[0],[0]
"Corollary 5.2 Let C0 = max{C̃2, 4C̃/L2}, which is a constant.",5. Convergence Analysis,[0],[0]
If we choose λ ∈,5. Convergence Analysis,[0],[0]
"[ 1− L2−L3π L21 , 1 ] ∩ [0, V1/V2] ∩",5. Convergence Analysis,[0],[0]
"[0, 1] and the number of total iterations T ≥ C0, DC-ASGD will outperform ASGD by a factor of T/C0.",5. Convergence Analysis,[0],[0]
"In this section, we evaluate our proposed DC-ASGD algorithm.",6. Experiments,[0],[0]
"We used two datasets: CIFAR-10 (Hinton, 2007) and ImageNet ILSVRC 2013 (Russakovsky et al., 2015).",6. Experiments,[0],[0]
The experiments were conducted on a GPU cluster interconnected with InfiniBand.,6. Experiments,[0],[0]
Each node has four K40 Tesla GPU processors.,6. Experiments,[0],[0]
We treat each GPU as a separate local worker.,6. Experiments,[0],[0]
"For the DNN algorithm running on
each worker, we chose ResNet (He et al., 2016) since it produces the state-of-the-art accuracy in many image related tasks and its implementation is available through open-source projects8.",6. Experiments,[0],[0]
"For the parallelization of ResNet across machines, we leveraged an open-source parameter server9.
",6. Experiments,[0],[0]
We implemented DC-ASGD on this experimental platform.,6. Experiments,[0],[0]
"We have two versions of implementations, one sets λt = λ0 as a constant, and the other adaptively tunes λt using a moving average method proposed by (Tieleman & Hinton, 2012).",6. Experiments,[0],[0]
"Specifically, we first define a quantity called MeanSquare as follows,
MeanSquare(t) = m·MeanSquare(t−1)+(1−m)·g(wt)2, (14) where m is a constant taking value from [0, 1).",6. Experiments,[0],[0]
And then we divide the initial λ0 by √ MeanSquare(t),6. Experiments,[0],[0]
"+ ϵ, where ϵ = 10−7 for all our experiments",6. Experiments,[0],[0]
.,6. Experiments,[0],[0]
This adaptive method is adopted to reduce the variance among coordinates with historical gradient values.,6. Experiments,[0],[0]
"For ease of reference, we denote the first implementation as DCASGD-c (constant) and the second as DC-ASGD-a (adaptive).
",6. Experiments,[0],[0]
"In addition to DC-ASGD, we also implemented ASGD and SSGD, which have been used in many previous works as baselines (Dean et al., 2012; Chen et al., 2016; Das et al., 2016).",6. Experiments,[0],[0]
"Furthermore, for the experiments on CIFAR-10, we used the sequential SGD algorithm as a reference model to examine the accuracy of parallel algorithms.",6. Experiments,[0],[0]
"However, for the experiments on ImageNet, we were not able to show this reference because it simply took too long time for a single machine to finish the training10.",6. Experiments,[0],[0]
"For sake of fairness, all experiments started from the same randomly initial-
8https://github.com/KaimingHe/ deep-residual-networks
9http://www.dmtk.io/ 10We also implemented the momentum variants of these algorithms.",6. Experiments,[0],[0]
"The corresponding comparisons are very similar to those without momentum.
ized model, and used the same strategy for learning rate scheduling.",6. Experiments,[0],[0]
The data were repartitioned randomly onto the local workers every epoch.,6. Experiments,[0],[0]
The CIFAR-10 dataset consists of a training set of 50k images and a test set of 10k images in 10 classes.,6.1. Experimental Results on CIFAR-10,[0],[0]
We trained a 20-layer ResNet model on this dataset (without data augmentation).,6.1. Experimental Results on CIFAR-10,[0],[0]
"For all the algorithms under investigation, we performed training for 160 epochs, with a mini-batch size of 128, and an initial learning rate which was reduced by ten times after 80 and 120 epochs following the practice in (He et al., 2016).",6.1. Experimental Results on CIFAR-10,[0],[0]
"We performed grid search for the hyper-parameter and the best test performances are obtained by choosing the initial learning rate η = 0.5, λ0 = 0.04 for DC-ASGD-c, and λ0 = 2, m = 0.95 for DC-ASGD-a. We tried different numbers of local workers in our experiments: M = {1, 4, 8}.
",6.1. Experimental Results on CIFAR-10,[0],[0]
"First, we investigate the learning curves with fixed number of effective passes as shown in Figure 2.",6.1. Experimental Results on CIFAR-10,[0],[0]
"From the figure, we have the following observations: (1) Sequential SGD achieves the best accuracy, and its final test error is 8.65%.",6.1. Experimental Results on CIFAR-10,[0],[0]
(2) The test errors of ASGD and SSGD increase with respect to the number of local workers.,6.1. Experimental Results on CIFAR-10,[0],[0]
"In particular, when M = 4, ASGD and SSGD achieve test errors of 9.27% and 9.17% respectively; and when M = 8, their test errors become 10.26% and 10.10% respectively.",6.1. Experimental Results on CIFAR-10,[0],[0]
"These results are reasonable: ASGD suffers from delayed gradients which becomes more serious for a larger number of workers; SSGD increases the effective mini-batch size by M times, and enlarged mini-batch size usually affects the training performances of DNN.",6.1. Experimental Results on CIFAR-10,[0],[0]
"(3) For DC-ASGD, no matter which λt is used, its performance is significantly better than ASGD and SSGD, and catches up with sequential SGD.",6.1. Experimental Results on CIFAR-10,[0],[0]
"For example, when M = 4, the test error of DC-ASGD-c is 8.67%, which is indistinguishable from sequential SGD, and the test error for DC-ASGD-a is 8.19%, which is even better than that achieved by sequential SGD.",6.1. Experimental Results on CIFAR-10,[0],[0]
It is not by design that DC-ASGD can beat sequential SGD.,6.1. Experimental Results on CIFAR-10,[0],[0]
The test performance lift might be attributed to the regularization effect brought by the variance introduced by parallel training.,6.1. Experimental Results on CIFAR-10,[0],[0]
"When M = 8, DC-ASGD-c can reduce the test error to 9.27%, which is nearly 1% better than ASGD and SSGD, meanwhile the test error is 8.57% for DC-ASGD-a, which again slightly better than sequential SGD.
",6.1. Experimental Results on CIFAR-10,[0],[0]
We further compared the convergence speeds of different algorithms as shown in Figure 3.,6.1. Experimental Results on CIFAR-10,[0],[0]
"From this figure, we have the following observations: (1)",6.1. Experimental Results on CIFAR-10,[0],[0]
"Although the convergent point is not very good, ASGD runs indeed very fast, and achieves almost linear speed-up as compared to sequential SGD in terms of throughput.",6.1. Experimental Results on CIFAR-10,[0],[0]
(2) SSGD also runs faster than sequential SGD.,6.1. Experimental Results on CIFAR-10,[0],[0]
"However, due to the synchronization barrier, it is significantly slower than ASGD.",6.1. Experimental Results on CIFAR-10,[0],[0]
(3) DC-ASGD achieves very good balance between accuracy and speed.,6.1. Experimental Results on CIFAR-10,[0],[0]
"On one hand, its converge speed is very similar to that of ASGD (although it involves a little more computational cost and
some memory cost when compensating the delay).",6.1. Experimental Results on CIFAR-10,[0],[0]
"On the other hand, its convergent point is as good as, or even better than that of sequential SGD.",6.1. Experimental Results on CIFAR-10,[0],[0]
The experiments results clearly demonstrate the effectiveness of our proposed delay compensation technologies11.,6.1. Experimental Results on CIFAR-10,[0],[0]
"In order to further verify our method on the large-scale setting, we conducted the experiment on the ImageNet dataset, which contains 1.28 million training images and 50k validation images in 1000 categories.",6.2. Experimental Results on ImageNet,[0],[0]
"We trained a 50-layer ResNet model (He et al., 2016) on this dataset.
",6.2. Experimental Results on ImageNet,[0],[0]
"According to the previous subsection, DC-ASGD-a seems to be better, therefore in this large-scale experiment, we only implemented DC-ASGD-a.",6.2. Experimental Results on ImageNet,[0],[0]
"For all algorithms in this experiment, we performed training for 120 epochs , with a mini-batch size of 32, and an initial learning rate reduced by ten times after every 30 epochs following the practice in (He et al., 2016).",6.2. Experimental Results on ImageNet,[0],[0]
"We did grid search for hyperparameter tuning and set the initial learning rate η = 0.1, λ0 = 2, m = 0.",6.2. Experimental Results on ImageNet,[0],[0]
"Since the training on the ImageNet dataset is very time consuming, we employed M = 16 GPU nodes in our experiments.",6.2. Experimental Results on ImageNet,[0],[0]
"The top-1 accuracies based on 1-crop testing of different algorithms are given in Figure 4.
",6.2. Experimental Results on ImageNet,[0],[0]
"11Please refer to the supplementary materials for the experiments on tuning the parameter λ.
",6.2. Experimental Results on ImageNet,[0],[0]
"According to the figure, we have the following observations: (1) After processing the same amount of training data, DC-ASGD always outperforms SSGD and ASGD.",6.2. Experimental Results on ImageNet,[0],[0]
"In particular, while the eventual test error achieved by ASGD and SSGD were 25.64% and 25.30% respectively, DC-ASGD achieved a lower error rate of 25.18%.",6.2. Experimental Results on ImageNet,[0],[0]
"Please note this time the accuracy of SSGD is quite good (which is consistent with a separate observation in (Chen et al., 2016)).",6.2. Experimental Results on ImageNet,[0],[0]
An explanation is that the training on ImageNet is less sensitive to the mini-batch size than that on CIFAR-10.,6.2. Experimental Results on ImageNet,[0],[0]
"(2) If we look at the learning curve with respect to wallclock time, SSGD is slowed down due to the synchronization barrier; ASGD and DC-ASGD have similar efficiency, once again indicating that the extra overhead for delay compensation introduced by DC-ASGD can almost be neglected in practice.",6.2. Experimental Results on ImageNet,[0],[0]
"Based on all our experiments, we can clearly see that DC-ASGD has outstanding performance in terms of both classification accuracy and convergence speed, which in return verifies the soundness of our proposed delay compensation technologies.",6.2. Experimental Results on ImageNet,[0],[0]
"In this paper, we have given a theoretical analysis on the problem of delayed gradients in the asynchronous parallelization of stochastic gradient descent (SGD) algorithms, and proposed a novel algorithm called Delay Compensated Asynchronous SGD (DC-ASGD) to tackle the problem.",7. Conclusion,[0],[0]
"We have evaluated DC-ASGD on CIFAR-10 and ImageNet datasets, and the results demonstrate that it can achieve better accuracy than both synchronous SGD and asynchronous SGD, and nearly approaches the performance of sequential SGD.",7. Conclusion,[0],[0]
"As for the future work, we plan to test DCASGD on larger computer clusters, where with the increasing number of local workers, the delay will become more serious.",7. Conclusion,[0],[0]
"Furthermore, we will investigate the economical approximation of higher-order items in the Taylor expansion to achieve more effective delay compensation.",7. Conclusion,[0],[0]
This work is partially supported by the National Natural Science Foundation of China (Grant No. 61371192).,Acknowledgments,[0],[0]
"With the fast development of deep learning, it has become common to learn big neural networks using massive training data.",abstractText,[0],[0]
"Asynchronous Stochastic Gradient Descent (ASGD) is widely adopted to fulfill this task for its efficiency, which is, however, known to suffer from the problem of delayed gradients.",abstractText,[0],[0]
"That is, when a local worker adds its gradient to the global model, the global model may have been updated by other workers and this gradient becomes “delayed”.",abstractText,[0],[0]
"We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD.",abstractText,[0],[0]
This is achieved by leveraging Taylor expansion of the gradient function and efficient approximation to the Hessian matrix of the loss function.,abstractText,[0],[0]
We call the new algorithm,abstractText,[0],[0]
Delay Compensated ASGD (DCASGD).,abstractText,[0],[0]
"We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and the experimental results demonstrate that DC-ASGD outperforms both synchronous SGD and asynchronous SGD, and nearly approaches the performance of sequential SGD.",abstractText,[0],[0]
Asynchronous Stochastic Gradient Descent with Delay Compensation,title,[0],[0]
"√ N) (N being the total
number of iterations) and it can achieve a linear speedup under certain conditions. We perform several experiments on both synthetic and real datasets. The results support our theory and show that the proposed algorithm provides a significant speedup over the recently proposed synchronous distributed L-BFGS algorithm.",text,[0],[0]
Quasi-Newton (QN) methods are powerful optimization techniques that are able to attain fast convergence rates by incorporating local geometric information through an approximation of the inverse of the Hessian matrix.,1. Introduction,[0],[0]
"The L-BFGS algorithm (Nocedal & Wright, 2006) is a wellknown limited-memory QN method that aims at solving the following optimization problem:
θ? = arg min θ∈Rd
{ U(θ) , NY∑ i=1",1. Introduction,[0],[0]
"Ui(θ) } , (1)
1LTCI, Télécom",1. Introduction,[0],[0]
"ParisTech, Université Paris-Saclay, 75013, Paris, France 2Department of Computer Science, Aalto University, Espoo, 02150, Finland 3Department of Computer Engineering, Boğaziçi",1. Introduction,[0],[0]
"University, 34342, Bebek, Istanbul, Turkey.",1. Introduction,[0],[0]
"Correspondence to: Umut Şimşekli <umut.simsekli@telecomparistech.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"whereU is a twice continuously differentiable function that can be convex or non-convex, and is often referred to as the empirical risk.",1. Introduction,[0],[0]
"In a typical machine learning context, a dataset Y with NY independent and identically distributed (i.i.d.)",1. Introduction,[0],[0]
"data points is considered, which renders the function U as a sum of NY different functions {Ui}NYi=1.
",1. Introduction,[0],[0]
"In large scale applications, the number of data points NY often becomes prohibitively large and therefore using a ‘batch’ L-BFGS algorithm becomes computationally infeasible.",1. Introduction,[0],[0]
"As a remedy, stochastic L-BGFS methods have been proposed (Byrd et al., 2016; Schraudolph et al., 2007; Moritz et al., 2016; Zhou et al., 2017; Yousefian et al., 2017; Zhao et al., 2017), which aim to reduce the computational requirements of L-BFGS by replacing ∇U (i.e. the full gradients that are required by L-BFGS) with some stochastic gradients that are computed on small subsets of the dataset.",1. Introduction,[0],[0]
"However, using stochastic gradients within LBFGS turns out to be a challenging task since it brings additional technical difficulties, which we will detail in Section 2.
",1. Introduction,[0],[0]
"In a very recent study, Berahas et al. (2016) proposed a parallel stochastic L-BFGS algorithm, called multi-batch L-BFGS (mb-L-BFGS), which is suitable for synchronous distributed architectures.",1. Introduction,[0],[0]
"This work illustrated that carrying out L-BFGS in a distributed setting introduces further theoretical and practical challenges; however, if these challenges are addressed, stochastic L-BFGS can be powerful in a distributed setting as well, and outperform conventional algorithms such as distributed stochastic gradient descent (SGD), as shown by their experimental results.
",1. Introduction,[0],[0]
"Despite the fact that synchronous parallel algorithms have clear advantages over serial optimization algorithms, the computational efficiency of synchronous algorithms is often limited by the overhead induced by the synchronization and coordination among the worker processes.",1. Introduction,[0],[0]
"Inspired by asynchronous parallel stochastic optimization techniques (Agarwal & Duchi, 2011; Lian et al., 2015; Zhang et al., 2015; Zhao & Li, 2015; Zheng et al., 2017), in this study, we propose an asynchronous parallel stochastic L-BFGS algorithm for large-scale non-convex optimization problems.",1. Introduction,[0],[0]
"The proposed approach aims at speeding up the synchronous algorithm presented in (Berahas et al., 2016) by allowing all the workers work independently from each
other and circumvent the inefficiencies caused by synchronization and coordination.
",1. Introduction,[0],[0]
Extending stochastic L-BFGS to asynchronous settings is a highly non-trivial task and brings several challenges.,1. Introduction,[0],[0]
"In our strategy, we first reformulate the optimization problem (1) as a sampling problem where the goal becomes drawing random samples from a distribution whose density is concentrated around θ?.",1. Introduction,[0],[0]
"We then build our algorithm upon the recent stochastic gradient Markov Chain Monte Carlo (SG-MCMC) techniques (Chen et al., 2015; 2016b) that have close connections with stochastic optimization techniques (Dalalyan, 2017; Raginsky et al., 2017; Zhang et al., 2017), and have proven successful in large-scale Bayesian machine learning.",1. Introduction,[0],[0]
We provide formal theoretical analysis and prove non-asymptotic guarantees for the proposed algorithm.,1. Introduction,[0],[0]
"Our theoretical results show that the proposed algorithm achieves an ergodic global convergence with rate O(1/ √ N), whereN denotes the total number of iterations.",1. Introduction,[0],[0]
"Our results further imply that the algorithm can achieve a linear speedup under ideal conditions.
",1. Introduction,[0],[0]
"For evaluating the proposed method, we conduct several experiments on synthetic and real datasets.",1. Introduction,[0],[0]
The experimental results support our theory: our experiments on a large-scale matrix factorization problem show that the proposed algorithm provides a significant speedup over the synchronous parallel L-BFGS algorithm.,1. Introduction,[0],[0]
"Preliminaries: As opposed to the classical optimization perspective, we look at the optimization problem (1) from a maximum a-posteriori (MAP) estimation point of view, where we consider θ as a random variable in Rd and θ?",2. Technical Background,[0],[0]
as the optimum of a Bayesian posterior whose density is given as p(θ|Y ) ∝ exp(−U(θ)),2. Technical Background,[0],[0]
", where Y ≡ {Y1, . . .",2. Technical Background,[0],[0]
", YNY } is a set of i.i.d. observed data points.",2. Technical Background,[0],[0]
"Within this context, U(θ) is often called the potential energy and defined as U(θ) = −[log p(θ) + ∑NY i=1",2. Technical Background,[0],[0]
"log p(Yi|θ)], where p(Yi|θ) is the likelihood function and p(θ) is the prior density.",2. Technical Background,[0],[0]
"In a classical optimization context, − log p(Yi|θ) would correspond to the data-loss and− log p(θ) would correspond to a regularization term.",2. Technical Background,[0],[0]
"Throughout this study, we will assume that the problem (1) has a unique solution in Rd.
",2. Technical Background,[0],[0]
"We define a stochastic gradient ∇Ũ(θ), that is an unbiased estimator of ∇U , as follows: ∇Ũ(θ) =",2. Technical Background,[0],[0]
−[∇ log p(θ) + NY,2. Technical Background,[0],[0]
"NΩ ∑ i∈Ω∇ log p(Yi|θ)], where Ω ⊂ {1, . . .",2. Technical Background,[0],[0]
", NY } denotes a random data subsample that is drawn with replacement, NΩ = |Ω| is the cardinality of Ω.",2. Technical Background,[0],[0]
"In the sequel, we will occasionally use the notation∇Ũn and∇ŨΩ to denote the stochastic gradient computed at iteration n of a given algorithm, or on a specific data subsample Ω, respectively.
",2. Technical Background,[0],[0]
The L-BFGS algorithm:,2. Technical Background,[0],[0]
"The L-BFGS algorithm itera-
tively applies the following equation in order to find the MAP estimate given in (1):
θn = θn−1 − hHn∇U(θn−1) (2)
where n denotes the iterations.",2. Technical Background,[0],[0]
"Here, Hn is an approximation to the inverse Hessian at θn−1 and is computed by using the M past values of the ‘iterate differences’ sn , θn − θn−1, and ‘gradient differences’ yn , ∇U(θn) − ∇U(θn−1).",2. Technical Background,[0],[0]
The collection of the iterate and gradient differences is called the L-BFGS memory.,2. Technical Background,[0],[0]
"The matrix-vector product Hn∇U(θn−1) is often implemented by using the two-loop recursion (Nocedal & Wright, 2006), which has linear time and space complexities O(Md).
",2. Technical Background,[0],[0]
"In order to achieve computational scalability, stochastic LBFGS algorithms replace ∇U with ∇Ũ .",2. Technical Background,[0],[0]
"This turns out to be problematic, since the gradient differences yn would be inconsistent, meaning that the stochastic gradients in different iterations will be computed on different data subsamples, i.e. Ωn−1 and Ωn.",2. Technical Background,[0],[0]
"On the other hand, in the presence of the stochastic gradients, L-BFGS is no longer guaranteed to produce positive definite approximations even in convex problems, therefore more considerations should be taken in order to make sure that Hn is positive definite.
",2. Technical Background,[0],[0]
Stochastic Gradient Markov Chain Monte Carlo:,2. Technical Background,[0],[0]
"Along with the recent advances in MCMC techniques, diffusion-based algorithms have become increasingly popular due to their applicability in large-scale machine learning applications.",2. Technical Background,[0],[0]
"These techniques, so called the Stochastic Gradient MCMC (SG-MCMC) algorithms, aim at generating samples from the posterior distribution p(θ|Y ) as opposed to finding the MAP estimate, and have strong connections with stochastic optimization techniques (Dalalyan, 2017).",2. Technical Background,[0],[0]
"In this line of work, Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011) is one of the pioneering algorithms and generates an approximate sample θn from p(θ|Y ) by iteratively applying the following update equation:
θn = θn−1 − h∇Ũn(θn−1) + √ 2h/βZn (3)
where h is the step-size and {Zn}Nn=1 is a collection of standard Gaussian random variables in Rd.",2. Technical Background,[0],[0]
"Here, β is called the inverse temperature: it is fixed to β = 1 in vanilla SGLD and when β 6= 1 the algorithm is called ‘tempered’.",2. Technical Background,[0],[0]
"In an algorithmic sense, SGLD is identical to SGD, except that it injects a Gaussian noise at each iteration and it coincides with SGD when β goes to infinity.
",2. Technical Background,[0],[0]
"SGLD has been extended in several directions (Ma et al., 2015; Chen et al., 2015; Şimşekli et al., 2016b; Şimşekli, 2017).",2. Technical Background,[0],[0]
"In (Şimşekli et al., 2016a), we proposed an LBFGS-based SGLD algorithm with O(M2d) computational complexity, which aimed to improve the convergence
speed of the vanilla SGLD.",2. Technical Background,[0],[0]
"We showed that a straightforward way of combining L-BFGS in SGLD would incur an undesired bias; however, the remedy to prevent this bias resulted in numerical instability, which would limit the applicability of the algorithm.",2. Technical Background,[0],[0]
"In other recent studies, SGLD has also been extended to synchronous (Ahn et al., 2014) and asynchronous (Chen et al., 2016b; Springenberg et al., 2016) distributed MCMC settings.
",2. Technical Background,[0],[0]
"SGLD can be seen as a discrete-time simulation of a continuous-time Markov process that is the solution of the following stochastic differential equation (SDE):
dθt = −∇U(θt)dt+ √ 2/βdWt, (4)
where Wt denotes the standard Brownian motion in Rd.",2. Technical Background,[0],[0]
"Under mild regularity conditions on U , the solution process (θt)t≥0 attains a unique stationary distribution with a density that is proportional to exp(−βU(θ))",2. Technical Background,[0],[0]
"(Roberts & Stramer, 2002).",2. Technical Background,[0],[0]
"An important property of this distribution is that, as β goes to infinity, this density concentrates around the global minimum of U(θ) (Hwang, 1980; Gelfand & Mitter, 1991).",2. Technical Background,[0],[0]
"Therefore, for large enough β, a random sample that is drawn for the stationary distribution of (θt)t≥0 would be close to θ?.",2. Technical Background,[0],[0]
"Due to this property, SG-MCMC methods have recently started drawing attention from the non-convex optimization community.",2. Technical Background,[0],[0]
Chen et al. (2016a) developed an annealed SG-MCMC algorithm for non-convex optimization and it was recently extended by Ye et al. (2017).,2. Technical Background,[0],[0]
"Raginsky et al. (2017) and Xu et al. (2017) provided finite-time guarantees for SGLD to find an ‘approximate’ global minimizer that is close to θ?, which imply that the additive Gaussian noise in SGLD can help the algorithm escape from poor local minima.",2. Technical Background,[0],[0]
"In a complementary study, Zhang et al. (2017) showed that SGLD enters a neighborhood of a local minimum of U(θ) in polynomial time, which shows that even if SGLD fails to find the global optimum, it will still find a point that is close to one of the local optima.",2. Technical Background,[0],[0]
"Even though these results showed that SG-MCMC is promising for optimization, it is still not clear how an asynchronous stochastic L-BFGS method could be developed within an SG-MCMC framework.",2. Technical Background,[0],[0]
"In this section, we propose a novel asynchronous L-BFGSbased (tempered) SG-MCMC algorithm that aims to provide an approximate optimum that is close to θ? by generating samples from a distribution that has a density that is proportional to exp(−βU(θ)).",3. Asynchronous Stochastic L-BFGS,[0],[0]
We call the proposed algorithm asynchronous parallel stochastic L-BFGS (as-LBFGS).,3. Asynchronous Stochastic L-BFGS,[0],[0]
Our method is suitable for both distributed and shared-memory settings.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"We will describe the algorithm only for the distributed setting; the shared-memory version is almost identical to the distributed version as long as the
updates are ensured to be atomic.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"We consider a classical asynchronous optimization architecture, which is composed of a master node, several worker nodes, and a data server.",3. Asynchronous Stochastic L-BFGS,[0],[0]
The main task of the master node is to maintain the newest iterate of the algorithm.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"At each iteration, the master node receives an additive update vector from a worker node, it adds this vector to the current iterate in order to obtain the next iterate, and then it sends the new iterate to the worker node which has sent the update vector.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"On the other hand, the worker nodes work in a completely asynchronous manner.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"A worker node receives the iterate from the master node, computes an update vector, and sends the update vector to the master node.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"However, since the iterate would be possibly modified by another worker node which runs asynchronously in the mean time, the update vector that is sent to the server will thus be computed on an old iterate, which causes both practical and theoretical challenges.",3. Asynchronous Stochastic L-BFGS,[0],[0]
Such updates are aptly called ‘delayed’ or ‘stale’.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"The full data is kept in the data server and we assume that all the workers have access to the data server.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"The proposed algorithm iteratively applies the following update equations in the master node:
un+1 = un + ∆un+1, θn+1 = θn + ∆θn+1, (5)
where n is the iteration index, un is called the momentum variable, and ∆un+1 and ∆θn+1 are the update vectors that are computed by the worker nodes.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"A worker node runs the following equations in order to compute the update vectors:
∆un+1 ,− h′Hn+1(θn−ln)∇Ũn+1(θn−ln)− γ′un−ln + √ 2h′γ′/βZn+1, (6)
∆θn+1 ,Hn+1(θn−ln)un−ln , (7)
where h′ is the step-size, γ′ > 0 is the friction parameter that determines the weight of the momentum, β is the inverse temperature, {Zn}n denotes standard Gaussian random variables, and Hn denotes the L-BFGS matrix at iteration n. Here,",3. Asynchronous Stochastic L-BFGS,[0],[0]
ln ≥ 0 denotes the ‘staleness’ of a particular update and measures the delay between the current update and the up-to-date iterate that is stored in the master node.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"We assume that the delays are bounded, i.e. maxn ln ≤",3. Asynchronous Stochastic L-BFGS,[0],[0]
"lmax < ∞. Note that the matrix-vector products have O(Md) time-space complexity.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Due to the asynchrony, the stochastic gradients and the L-BFGS matrices will be computed on the delayed variables θn−ln and un−ln .",3. Asynchronous Stochastic L-BFGS,[0],[0]
"As opposed to the asynchronous stochastic gradient algorithms, where the main difficulty stems from the delayed gradients, our algorithm faces further challenges since it is not straightforward to obtain the gradient and iterate differences that are required for the LBFGS computations in an asynchronously parallel setting.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
Algorithm 1: as-L-BFGS:,3. Asynchronous Stochastic L-BFGS,[0],[0]
"Master node 1 input: θ0, u0 // Global iteration index 2 n← 0 3",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Send (θ0, u0) to all the workers w = 1, . . .",3. Asynchronous Stochastic L-BFGS,[0],[0]
",W 4 while n < N",3. Asynchronous Stochastic L-BFGS,[0],[0]
"do 5 Receive (∆θn+1,∆un+1) from worker w
//",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Generate the new iterates
6 un+1 = un + ∆un+1, θn+1 = θn + ∆θn+1 7",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Send the iterates (θn+1, un+1) to worker w 8 Set n← n+ 1
We propose the following approach for the computation of the L-BFGS matrices.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"As opposed to the mb-L-BFGS algorithm, which uses a central L-BFGS memory (i.e. the collection of the gradient and iterate differences) that is stored in the master node, we let each worker have their own local L-BFGS memories since the master node would not be able to keep track of the gradient and iterate differences, which are received in an asynchronous manner.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"In our strategy, each worker updates its own L-BFGS memory right after sending the update vector to the master node.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"The overall algorithm is illustrated in Algorithms 1 and 2 (W denotes the number of workers).
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"In order to be able to have consistent gradient differences, each worker applies a multi-batch subsampling strategy that is similar to mb-L-BFGS.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"We divide the data subsample into two subsets, i.e. Ωn = {Sn, On} with NS , |Sn|, NO , |On|, and NΩ = NS +NO.",3. Asynchronous Stochastic L-BFGS,[0],[0]
Here the main idea is to chooseNS,3. Asynchronous Stochastic L-BFGS,[0],[0]
NO and useOn as an overlapping subset for the gradient differences.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"In this manner, in addition to the gradients that are computed on Sn andOn, we also perform an extra gradient computation on the previous overlapping subset, at the end of each iteration.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"As NO will be small, this extra cost will not be significant.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Finally, in order to ensure the L-BFGS matrices are positive definite, we use a ‘cautious’ update mechanism that is useful for non-convex settings (Li & Fukushima, 2001; Zhang & Sutton, 2011; Berahas et al., 2016) as shown in Algorithm 2.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Note that, in addition to asynchrony, the proposed algorithm also extends the current stochastic L-BFGS methods by introducing momentum.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"This brings two critical practical features: (i) without the existence of the momentum variables, the injected Gaussian noise must depend on the L-BFGS matrices, as shown in (Şimşekli et al., 2016a), which results in an algorithm withO(M2d) time complexity whereas our algorithm hasO(Md) time complexity, (ii) the use of the momentum significantly repairs the numerical instabilities caused by the asynchronous updates, since un inherently encapsulates a direction for θn, which provides additional information to the algorithm besides the gradients and L-BFGS computations.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Furthermore, in a
Algorithm 2: as-L-BFGS: Worker node (w) 1 input: M , γ, NS , NO (NΩ = NS +NO) //",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Local iteration index 2 i← 0 3 while the master node is running do 4 Receive (θn−ln , un−ln) from the master 5 Draw a subsample Ωn+1 = {Sn+1, On+1}
//",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Gradient computation
6 ∇Ũn+1(θn−ln) =",3. Asynchronous Stochastic L-BFGS,[0],[0]
"NO NΩ ∇ŨOn+1(θn−ln) + NSNΩ∇ŨSn+1(θn−ln)
7 Compute (∆θn+1,∆un+1) by (6) and (7) 8",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Send (∆θn+1,∆un+1) to the master
// Local variables for L-BFGS
9 θ̃i = θn−ln , Õi = On+1, g̃i = ∇ŨOn+1(θn−ln) 10 if i ≥ 1 then
// Compute the overlapping gradient
11 g′ = ∇ŨÕi−1(θ̃i) //",3. Asynchronous Stochastic L-BFGS,[0],[0]
Compute the L-BFGS variables 12 si = θ̃i,3. Asynchronous Stochastic L-BFGS,[0],[0]
"− θ̃i−1, yi = g′",3. Asynchronous Stochastic L-BFGS,[0],[0]
"− g̃i−1 // Cautious memory update 13 Add (si, yi) to the L-BFGS memory only if y>i si ≥ ‖si‖2 for some > 0",3. Asynchronous Stochastic L-BFGS,[0],[0]
14,3. Asynchronous Stochastic L-BFGS,[0],[0]
"Set i← i+ 1
very recent study (Loizou & Richtárik, 2017)",3. Asynchronous Stochastic L-BFGS,[0],[0]
the use of momentum variables has been shown to be useful in other second-order optimization methods.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"On the other hand, despite their advantages, the momentum variable also drifts apart the proposed algorithm from the original L-BFGS formulation.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"However, even such approximate approaches have proven useful in various scenarios (Zhang & Sutton, 2011; Fu et al., 2016).",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Also note that, when β → ∞, lmax = 0, and Hn(θ) = I for all n, the algorithm coincides with SGD with momentum.",3. Asynchronous Stochastic L-BFGS,[0],[0]
A more detailed illustration is given in the supplementary document.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"In this section, we will provide non-asymptotic guarantees for the proposed algorithm.",4. Theoretical Analysis,[0],[0]
Our analysis strategy is different from the conventional analysis approaches for stochastic optimization and makes use of tools from analysis of SDEs.,4. Theoretical Analysis,[0],[0]
"In particular, we will first develop a continuoustime Markov process whose marginal stationary measure admits a density that is proportional to exp(−βU(θ)).",4. Theoretical Analysis,[0],[0]
Then we will show that (5)-(7) form an approximate EulerMaruyama integrator that approximately simulates this continuous process in discrete-time.,4. Theoretical Analysis,[0],[0]
"Finally, we will analyze this approximate numerical scheme and provide a non-asymptotic error bound.",4. Theoretical Analysis,[0],[0]
"All the proofs are given in the supplementary document.
",4. Theoretical Analysis,[0],[0]
"We start by considering the following stochastic dynamical system:
dpt= [ 1 β Γt(θt)−Ht(θt)∇θU(θt)− γpt ]",4. Theoretical Analysis,[0],[0]
"dt+ √ 2γ β dWt
dθt =Ht(θt)ptdt (8)
where pt ∈ Rd is also called the momentum variable,Ht(·) denotes the L-BFGS matrix at time t and Γt(·) is a vector that is defined as follows:[
Γt(θ) ]",4. Theoretical Analysis,[0],[0]
"i , d∑ j=1 ∂[Ht(θ)]ij ∂[θ]j , (9)
where [v]i denotes the ith component of a vector v and similarly [M ]ij denotes a single element of a matrix M .
",4. Theoretical Analysis,[0],[0]
"In order to analyze the invariant measure of the SDE defined in (8), we need certain conditions to hold.",4. Theoretical Analysis,[0],[0]
"First, we have two regularity assumptions on U and Ht:
H1.",4. Theoretical Analysis,[0],[0]
"The gradient of the potential is Lipschitz continuous, i.e. ‖∇θU(θ)−∇θU(θ′)‖ ≤ L‖θ",4. Theoretical Analysis,[0],[0]
"− θ′‖, ∀θ, θ′ ∈ Rd.",4. Theoretical Analysis,[0],[0]
H 2.,4. Theoretical Analysis,[0],[0]
"The L-BFGS matrices have bounded second-order derivatives and they are Lipschitz continuous, i.e. ‖Ht(θ)− Ht(θ ′)‖ ≤ LH‖θ",4. Theoretical Analysis,[0],[0]
"− θ′‖, ∀θ, θ′ ∈ Rd, t ≥ 0.
",4. Theoretical Analysis,[0],[0]
"The assumptions H1 and H2 are standard conditions in analysis of SDEs (Duan, 2015) and similar assumptions have also been considered in stochastic gradient (Moulines & Bach, 2011) and stochastic L-BFGS algorithms (Zhou et al., 2017).",4. Theoretical Analysis,[0],[0]
"Besides, H2 provides a direct control on the partial derivatives of Ht, which will be useful for analyzing the overall numerical scheme.",4. Theoretical Analysis,[0],[0]
"We now present our first result that establishes the invariant measure of the SDE (8).
",4. Theoretical Analysis,[0],[0]
Proposition 1.,4. Theoretical Analysis,[0],[0]
Assume that the conditions H1 and 2 hold.,4. Theoretical Analysis,[0],[0]
Let,4. Theoretical Analysis,[0],[0]
Xt =,4. Theoretical Analysis,[0],[0]
"[θ>t , p > t ] > ∈ R2d and (Xt)t≥0 be a Markov process that is a solution of the SDE given in (8).",4. Theoretical Analysis,[0],[0]
Then (Xt)t≥0 has a unique invariant measure π that admits a density ρ(X) ∝,4. Theoretical Analysis,[0],[0]
exp(−E(X)),4. Theoretical Analysis,[0],[0]
"with respect to the Lebesgue measure, where E is an energy function on the extended state space and is defined as: E(X) , βU(θ) + β2 p >",4. Theoretical Analysis,[0],[0]
"p.
",4. Theoretical Analysis,[0],[0]
"This result shows that, if the SDE (8) could be exactly simulated, the marginal distribution of the samples θt would converge to a measure πθ which has a density that is proportional to exp(−βU(θ)).",4. Theoretical Analysis,[0],[0]
"Therefore, for large enough β and t, θt would be close to the global optimum θ?.
",4. Theoretical Analysis,[0],[0]
"We note that when β = 1, the SDE (8) shares similarities with the SDEs presented in (Fu et al., 2016; Ma et al., 2015).",4. Theoretical Analysis,[0],[0]
"While the main difference being the usage of the tempering scheme, (Fu et al., 2016) further differs from our approach as it directly discard the term Γt since is in a Metropolis-Hastings framework, which is not adequate for large-scale applications.",4. Theoretical Analysis,[0],[0]
"On the other hand, the stochastic
gradient Riemannian Hamiltonian Monte Carlo algorithm given in (Ma et al., 2015), chooses Ht as the Fisher information matrix; a quantity that requires O(d2) space-time complexity and is not analytically available in general.
",4. Theoretical Analysis,[0],[0]
We will now show that the proposed algorithm (5)-(7) form an approximate method for simulating (8) in discrete-time.,4. Theoretical Analysis,[0],[0]
"For illustration, we first consider the Euler-Maruyama integrator for (8), given as follows:
pn+1 =",4. Theoretical Analysis,[0],[0]
pn,4. Theoretical Analysis,[0],[0]
"− hHn(θn)∇θU(θn)− hγpn + h
β Γn(θn) +",4. Theoretical Analysis,[0],[0]
"√
2hγ/βZn+1, (10) θn+1 = θn + hHn(θn)pn.",4. Theoretical Analysis,[0],[0]
"(11)
Here, the term (1/β)Γn introduces an additional computational burden and its importance is very insignificant (i.e. its magnitude is of order O(1/β) due to H2).",4. Theoretical Analysis,[0],[0]
"Therefore, we discard Γn, define un , hpn, γ′ , hγ, h′ , h2, and use these quantities in (10) and (11).",4. Theoretical Analysis,[0],[0]
"We then obtain the following re-parametrized Euler integrator:
un+1=un−h′Hn(θn)∇θU(θn)−γ′un+ √ 2h′γ′/βZn+1
θn+1=θn+Hn(θn)un
The detailed derivation is given in the supplementary document.",4. Theoretical Analysis,[0],[0]
"Finally, we replace∇U with the stochastic gradients, replace the variables θn and un with stale variables θn−ln and pn−ln in the update vectors, and obtain the ultimate update equations, given in (5).",4. Theoretical Analysis,[0],[0]
"Note that, due to the negligence of Γn, the proposed approach would require a large β and would not be suitable for classical posterior sampling settings, where β = 1.
",4. Theoretical Analysis,[0],[0]
"In this section, we will analyze the ergodic error E[ÛN",4. Theoretical Analysis,[0],[0]
"− U?], where we define ÛN , (1/N) ∑N n=1 U(θn) and U? , U(θ?).",4. Theoretical Analysis,[0],[0]
"This error resembles the bias of a statistical estimator; however, as opposed to the bias, it directly measures the expected discrepancy to the global optimum.",4. Theoretical Analysis,[0],[0]
"Similar ergodic error notions have been considered in the analysis of non-convex optimization methods (Lian et al., 2015; Chen et al., 2016a; Berahas et al., 2016).
",4. Theoretical Analysis,[0],[0]
"In our proof strategy, we decompose the error into two terms: E[ÛN −U?] = A1 +A2, whereA1 , E[ÛN − Ūβ",4. Theoretical Analysis,[0],[0]
],4. Theoretical Analysis,[0],[0]
"A2 , [Ūβ−U?] ≥ 0, and Ūβ , ∫ Rd U(θ)πθ(dθ).",4. Theoretical Analysis,[0],[0]
"We then upper-bound these terms separately.
",4. Theoretical Analysis,[0],[0]
"The term A1 turns out to be the bias of a statistical estimator, which we can analyze by using ideas from recent SGMCMC studies.",4. Theoretical Analysis,[0],[0]
"However, existing tools cannot be directly used because of the additional difficulties introduced by the L-BFGS matrices.",4. Theoretical Analysis,[0],[0]
"In order to bound A1, we first require the following smoothness and boundedness condition.
H3.",4. Theoretical Analysis,[0],[0]
"Let ψ be a functional that is the unique solution of a
Poisson equation that is defined as follows:
Lnψ(Xn) = U(θn)− Ūβ , (12)
where Xn =",4. Theoretical Analysis,[0],[0]
"[θ>n , p > n ]",4. Theoretical Analysis,[0],[0]
">, Ln is the generator of (8) at t = nh and is formally defined in the supplementary document.",4. Theoretical Analysis,[0],[0]
"The functional ψ and its up to third-order derivatives Dkψ are bounded by a function V (X), such that ‖Dkψ‖ ≤",4. Theoretical Analysis,[0],[0]
"CkV rk for k = 0, 1, 2, 3 and Ck, rk > 0.",4. Theoretical Analysis,[0],[0]
"Furthermore, supnEV
r(Xn) <∞ and V is smooth such that sups∈(0,1) V
r(sX + (1 − s)X ′) ≤",4. Theoretical Analysis,[0],[0]
C(V r(X) + V r(X ′)),4. Theoretical Analysis,[0],[0]
"for all X,X ′ ∈ R2d, r ≤ max 2rk, and C > 0.
Assumption H3 is also standard in SDE analysis and SGMCMC (Mattingly et al., 2010; Teh et al., 2016; Chen et al., 2015; Durmus et al., 2016) and gives us control over the weak error of the numerical integrator.",4. Theoretical Analysis,[0],[0]
We further require the following regularity conditions in order to have control over the error induced by the delayed stochastic gradients.,4. Theoretical Analysis,[0],[0]
H4.,4. Theoretical Analysis,[0],[0]
"The variance of the stochastic gradients is bounded, i.e. E‖∇θU(θ)−∇θŨ(θ)‖2 ≤ σ for some 0 <",4. Theoretical Analysis,[0],[0]
σ,4. Theoretical Analysis,[0],[0]
<∞. H5.,4. Theoretical Analysis,[0],[0]
"For a smooth and bounded function f , the remainder rLn,f (·) in the following Taylor expansion is bounded:
ehLnf(X) = f(X) + hLnf(X)",4. Theoretical Analysis,[0],[0]
"+ h2rLn,f (X).",4. Theoretical Analysis,[0],[0]
"(13)
The following lemma presents an upper-bound for A1.",4. Theoretical Analysis,[0],[0]
Lemma 1.,4. Theoretical Analysis,[0],[0]
Assume the conditions H1-5 hold.,4. Theoretical Analysis,[0],[0]
"We have the following bound for the bias:∣∣E[ÛN − Ūβ ]∣∣ = O( 1
Nh + max(lmax, 1)h+
1
β
) .",4. Theoretical Analysis,[0],[0]
"(14)
Here, the term 1/β in (14) appears due to the negligence of Γn.",4. Theoretical Analysis,[0],[0]
"In order to bound the second term A2, we follow a similar strategy to (Raginsky et al., 2017), where we use H 1 and the following moment condition on πθ.",4. Theoretical Analysis,[0],[0]
H6.,4. Theoretical Analysis,[0],[0]
The second-order moments of πθ are bounded and satisfies the following inequality: ∫ Rd ‖θ‖2πθ(dθ) ≤,4. Theoretical Analysis,[0],[0]
"Cββ , for some Cβ > max(βd/(2πe), de/L).
",4. Theoretical Analysis,[0],[0]
This assumption is mild since πθ concentrates around θ?,4. Theoretical Analysis,[0],[0]
as β tends to infinity.,4. Theoretical Analysis,[0],[0]
"The order 1/β is arbitrary, hence the assumption can be further relaxed.",4. Theoretical Analysis,[0],[0]
The following lemma establishes an upper-bound for A2.,4. Theoretical Analysis,[0],[0]
Lemma 2.,4. Theoretical Analysis,[0],[0]
"Under assumptions H1 and 6, the following bound holds: Ūβ − U? =",4. Theoretical Analysis,[0],[0]
"O(1/β).
",4. Theoretical Analysis,[0],[0]
"We now present our main result, which can be easily proven by combining Lemmas 1 and 2.",4. Theoretical Analysis,[0],[0]
Theorem 1.,4. Theoretical Analysis,[0],[0]
Assume that the conditions H1-6 hold.,4. Theoretical Analysis,[0],[0]
Then the ergodic error of the proposed algorithm is bounded as follows:∣∣EÛN,4. Theoretical Analysis,[0],[0]
− U?∣∣,4. Theoretical Analysis,[0],[0]
"= O( 1
Nh + max(1, lmax)h+
1
β
) .",4. Theoretical Analysis,[0],[0]
"(15)
More explicit constants and a discussion on the relation of the theorem to other recent theoretical results are provided in the supplementary document.
",4. Theoretical Analysis,[0],[0]
Theorem 1 provides a non-asymptotic guarantee for convergence to a point that is close to the global optimizer θ?,4. Theoretical Analysis,[0],[0]
"even when U is non-convex, thanks to the additive Gaussian noise.",4. Theoretical Analysis,[0],[0]
"The bound suggests an optimal rate of convergence of O(1/ √ N), which is in line with the current rates of the non-convex asynchronous algorithms (Lian et al., 2015).",4. Theoretical Analysis,[0],[0]
"Furthermore, if we assume that the total number of iterations N is a linear function of the number of workers, e.g. N = NWW , where NW is the number of iterations executed by a single worker, Theorem 1 implies that, in the ideal case, the proposed algorithm can achieve a linear speedup with increasing W , provided that lmax = O(1/(Nh2)).
",4. Theoretical Analysis,[0],[0]
"Despite their nice theoretical properties, it is well-known that tempered sampling approaches also often get stuck near a local minimum.",4. Theoretical Analysis,[0],[0]
"In our case, this behavior would be mainly due to the hidden constant in (14), which can be exponential in dimension d, as illustrated in (Raginsky et al., 2017) for SGLD.",4. Theoretical Analysis,[0],[0]
"On the other hand, Theorem 1 does not guarantee that the proposed algorithm will converge to a neighborhood of a local minimum; however, we believe that we can also prove local convergence guarantees by using the techniques provided in (Zhang et al., 2017; Tzen et al., 2018), which we leave as a future work.",4. Theoretical Analysis,[0],[0]
"The performance of asynchronous stochastic gradient methods has been evaluated in several studies, where the advantages and limitations have been illustrated in various scenarios, to name a few (Dean et al., 2012; Zhang et al., 2015; Zheng et al., 2017).",5. Experiments,[0],[0]
"In this study, we will explore the advantages of using L-BFGS in an asynchronous environment.",5. Experiments,[0],[0]
"In order to illustrate the advantages of asynchrony, we will compare as-L-BFGS with mb-L-BFGS (Berahas et al., 2016); and in order to illustrate the advantages that are brought by using higher-order geometric information, we will compare as-L-BFGS to asynchronous SGD (aSGD)",5. Experiments,[0],[0]
"(Lian et al., 2015).",5. Experiments,[0],[0]
"We will also explore the speedup behavior of as-L-BFGS for increasing W .
",5. Experiments,[0],[0]
"We conduct experiments on both synthetic and real
datasets.",5. Experiments,[0],[0]
"For real data experiments, we have implemented all the three algorithms in C++ by using a low-level message passing protocol for parallelism, namely the OpenMPI library.",5. Experiments,[0],[0]
This code can be used both in a distributed environment or a single computer with multiprocessors.,5. Experiments,[0],[0]
"For the experiments on synthetic data, we have implemented the algorithms in MATLAB, by developing a realistic discreteevent simulator.",5. Experiments,[0],[0]
"This simulated environment is particularly useful for understanding the behaviors of the algorithms in detail since we can explicitly control the computation time that is spent at the master or worker nodes, and the communication time between the nodes.",5. Experiments,[0],[0]
"This simulation strategy also enables us to explicitly control the variation among the computational powers of the worker nodes; a feature that is much harder to control in real distributed environments.
",5. Experiments,[0],[0]
Linear Gaussian model: We conduct our first set of experiments on synthetic data where we consider a rather simple convex quadratic problem whose optimum is analytically available.,5. Experiments,[0],[0]
"The problem is formulated as finding the MAP estimate of the following linear Gaussian probabilistic model:
θ ∼ N (0, I), Yi|θ ∼ N (a>i θ, σ2x), ∀i = 1, . . .",5. Experiments,[0],[0]
", NY .
",5. Experiments,[0],[0]
We assume that {an}Nn=1 and σ2x are known and we aim at computing θ?.,5. Experiments,[0],[0]
"For these experiments, we develop a parametric discrete event simulator that aims to simulate the algorithms in a controllable yet realistic way.",5. Experiments,[0],[0]
"The simulator simulates a distributed optimization algorithm once it is provided four parameters: (i) µm: the average computational time spent by the master node at each iteration, (ii) µw: the average computational time spent by a single worker at each iteration, (iii) σw: the standard deviation of the computational time spent by a single worker per iteration, and (iv) τ",5. Experiments,[0],[0]
: the time spent for communications per iteration.,5. Experiments,[0],[0]
All these parameters are in a generic base time unit.,5. Experiments,[0],[0]
"Once these parameters are provided for one of the three algorithms, the simulator simulates the (a)synchronous distributed algorithm by drawing random computation times from a log-normal distribution whose mean and variance is specified by µw and σ2w.",5. Experiments,[0],[0]
"Figure 1 illustrates a typical outcome of the real and the simulated implementations of as-L-BFGS, where we observe that the simulator is able to provide realistic simulations that can even very well reflect
the fluctuations of the algorithm.
",5. Experiments,[0],[0]
"In our first experiment, we set d = 100, σ2x = 10, NY = 600, we randomly generate and fix the vectors {an}n in such a way that there will be a strong correlation in the posterior distribution, and we finally generate a true θ and the observations Y by using the generative model.
",5. Experiments,[0],[0]
"For each algorithm, we fix µm, µs, and τs to realistic values and investigate the effect of the variation among the workers by comparing the running time of the algorithms for achieving ε-accuracy (i.e., (U(θn) − U?)/U? ≤ ε) for different values of σ2w when W = 40.",5. Experiments,[0],[0]
We repeat each experiment 100 times.,5. Experiments,[0],[0]
"In all our experiments, we have tried several values for the hyper-parameters of each algorithm and we report the best results.",5. Experiments,[0],[0]
"All the hyper-parameters are provided in the supplementary document.
",5. Experiments,[0],[0]
Figure 2 visualizes the results for the first experiment.,5. Experiments,[0],[0]
"We can observe that, for smaller values σ2w as-L-BFGS and mbL-BFGS perform similarly, where a-SGD requires more computational time to achieve ε-accuracy.",5. Experiments,[0],[0]
"However, as we increase the value of σ2w, mb-L-BFGS requires more computational time in order to be able to collect sufficient amount of stochastic gradients.",5. Experiments,[0],[0]
"The results show that both asynchronous algorithms turn out to be more robust to the variability of the computational power of the workers, where as-L-BFGS shows a better performance.
",5. Experiments,[0],[0]
"In our second experiment, we investigate the speedup behavior of as-L-BFGS within the simulated setting.",5. Experiments,[0],[0]
"In this setting, we consider a highly varying set of workers and set σ2w = 200 and vary the number of workers W .",5. Experiments,[0],[0]
"As illustrated in Figure 3, as W increases, lmax increases as well and the algorithm hence requires more iterations in order to achieve ε-accuracy, since a smaller step-size needs to be used.",5. Experiments,[0],[0]
"However, this increment in the number of iterations is compensated by the increased number of workers, as we observe that the required computational time gracefully decreases with increasing W .",5. Experiments,[0],[0]
"We observe a similar behavior for different values of σ2w, where the speedup is more prominent for smaller σ2w.
",5. Experiments,[0],[0]
Large-scale matrix factorization:,5. Experiments,[0],[0]
"In our next set of experiments, we consider a large-scale matrix factorization problem (Gemulla et al., 2011; Şimşekli",5. Experiments,[0],[0]
"et al., 2015; Şimşekli et al., 2017), where the goal is to obtain the
MAP solution of the following probabilistic model: Frk ∼ N (0, 1), Gks ∼ N (0, 1), Yrs|F,G ∼ N (∑ k FrkGks, 1 ) .",5. Experiments,[0],[0]
"Here, Y ∈ RR×S is the data matrix, and F ∈ RR×K and G ∈ RK×S are the factor matrices to be estimated.
",5. Experiments,[0],[0]
"In this context, we evaluate the algorithms on three largescale movie ratings datasets, namely MovieLens 1Million (ML-1M), 10Million (ML-10M), and 20Million (ML20M) (grouplens.org).",5. Experiments,[0],[0]
"The ML-1M dataset contains 1 million non-zero entries, where R = 3883 (movies) and",5. Experiments,[0],[0]
S = 6040 (users).,5. Experiments,[0],[0]
"The ML-10M dataset contains 10 million non-zero entries, resulting in a 10681 × 71567 data matrix.",5. Experiments,[0],[0]
"Finally, the ML-20M dataset contains 20 million ratings, resulting in a 27278 × 138493 data matrix.",5. Experiments,[0],[0]
"We have conducted these experiments on a cluster of more than 500 interconnected computers, each of which is equipped with variable quality CPUs and memories.",5. Experiments,[0],[0]
"In these experiments, we have found that the numerical stability is improved when Hn is replaced with (Hn + ρI) for small ρ > 0.",5. Experiments,[0],[0]
This small modification does not violate our theoretical results.,5. Experiments,[0],[0]
"The hyper-parameters are provided in the supplementary document.
",5. Experiments,[0],[0]
"Figure 4 shows the performance of the three algorithms on the MovieLens datasets in terms of the root-mean-squarederror (RMSE), which is a standard metric for recommendation systems, and the norm of the gradients through iterations.",5. Experiments,[0],[0]
"In these experiments, we set K = 5 for all the three datasets and we set the number of workers toW = 10.",5. Experiments,[0],[0]
"The results show that, in all datasets, as-L-BFGS provides a significant speedup over mb-L-BFGS thanks to asynchrony.",5. Experiments,[0],[0]
We can observe that even when the speed of convergence of mb-L-BFGS is comparable to a-SGD and as-L-BFGS (cf.,5. Experiments,[0],[0]
"the plots showing the norm of the gradients), the final RMSE yielded by mb-L-BFGS is poorer than the two other
methods, which is an indicator that the asynchronous algorithms are able to find a better local minimum.",5. Experiments,[0],[0]
"On the other hand, the asynchrony causes more fluctuations in asL-BFGS when compared to a-SGD.
",5. Experiments,[0],[0]
"As opposed to the synthetic data experiments, in all the three MovieLens datasets, we observe that as-L-BFGS provides a slight improvement in the convergence speed when compared to a-SGD.",5. Experiments,[0],[0]
This indicates that a-SGD is able to achieve a comparable convergence speed by taking more steps while as-L-BFGS is computing the matrix-vector products.,5. Experiments,[0],[0]
"However, this gap can be made larger by considering a more efficient, yet more sophisticated implementation for L-BFGS computations (Chen et al., 2014).
",5. Experiments,[0],[0]
"In our last experiment, we investigate the speedup properties of as-L-BFGS in the real distributed setting.",5. Experiments,[0],[0]
"In this experiment, we only consider the ML-1M dataset and run the as-L-BFGS algorithm for different number of workers.",5. Experiments,[0],[0]
Figure 5 illustrates the results of this experiment.,5. Experiments,[0],[0]
"As we increase W from 1 to 10, we obtain a decent speedup that is close to a linear speedup.",5. Experiments,[0],[0]
"However, when we set W = 20 the algorithm becomes unstable, since the term lmaxh in (15) dominates.",5. Experiments,[0],[0]
"Therefore, for W = 20 we need to decrease the step-size h, which requires the algorithm to be run for a longer amount of time in order to achieve the same error as we achieved when W was smaller.",5. Experiments,[0],[0]
"On the other hand, the algorithm achieves a linear speedup in terms of iterations; however, the corresponding result is provided in the supplementary document due to the space constraints.",5. Experiments,[0],[0]
"In this study, we proposed an asynchronous parallel L-BFGS algorithm for non-convex optimization.",6. Conclusion,[0],[0]
"We developed the algorithm within the SG-MCMC framework, where we reformulated the problem as sampling from a concentrated probability distribution.",6. Conclusion,[0],[0]
We proved non-asymptotic guarantees and showed that as-LBFGS achieves an ergodic global convergence with rate O(1/ √ N) and it can achieve a linear speedup.,6. Conclusion,[0],[0]
Our experiments supported our theory and showed that the proposed algorithm provides a significant speedup over the synchronous parallel L-BFGS algorithm.,6. Conclusion,[0],[0]
The authors would like to thank to Murat A. Erdoğdu for fruitful discussions.,Acknowledgments,[0],[0]
"This work is partly supported by the French National Research Agency (ANR) as a part of the FBIMATRIX project (ANR-16-CE23-0014), by the Scientific and Technological Research Council of Turkey (TÜBİTAK) grant number 116E580, and by the industrial chair Machine Learning for Big Data from Télécom ParisTech.",Acknowledgments,[0],[0]
"Recent studies have illustrated that stochastic gradient Markov Chain Monte Carlo techniques have a strong potential in non-convex optimization, where local and global convergence guarantees can be shown under certain conditions.",abstractText,[0],[0]
"By building up on this recent theory, in this study, we develop an asynchronous-parallel stochastic L-BFGS algorithm for non-convex optimization.",abstractText,[0],[0]
The proposed algorithm is suitable for both distributed and shared-memory settings.,abstractText,[0],[0]
We provide formal theoretical analysis and show that the proposed method achieves an ergodic convergence rate of O(1/ √ N),abstractText,[0],[0]
(N being the total number of iterations) and it can achieve a linear speedup under certain conditions.,abstractText,[0],[0]
We perform several experiments on both synthetic and real datasets.,abstractText,[0],[0]
The results support our theory and show that the proposed algorithm provides a significant speedup over the recently proposed synchronous distributed L-BFGS algorithm.,abstractText,[0],[0]
Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 196–202 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2031",text,[0],[0]
"Sequence-to-sequence (S2S) learning with attention mechanism recently became the most successful paradigm with state-of-the-art results in machine translation (MT) (Bahdanau et al., 2014; Sennrich et al., 2016a), image captioning (Xu et al., 2015; Lu et al., 2016), text summarization (Rush et al., 2015) and other NLP tasks.
",1 Introduction,[0],[0]
All of the above applications of S2S learning make use of a single encoder.,1 Introduction,[0],[0]
"Depending on the modality, it can be either a recurrent neural network (RNN) for textual input data, or a convolutional network for images.
",1 Introduction,[0],[0]
"In this work, we focus on a special case of S2S learning with multiple input sequences of possibly different modalities and a single output-generating recurrent decoder.",1 Introduction,[0],[0]
"We explore various strategies the decoder can employ to attend to the hidden states of the individual encoders.
",1 Introduction,[0],[0]
"The existing approaches to this problem do not explicitly model different importance of the inputs to the decoder (Firat et al., 2016; Zoph and Knight,
2016).",1 Introduction,[0],[0]
"In multimodal MT (MMT), where an image and its caption are on the input, we might expect the caption to be the primary source of information, whereas the image itself would only play a role in output disambiguation.",1 Introduction,[0],[0]
"In automatic post-editing (APE), where a sentence in a source language and its automatically generated translation are on the input, we might want to attend to the source text only in case the model decides that there is an error in the translation.
",1 Introduction,[0],[0]
"We propose two interpretable attention strategies that take into account the roles of the individual source sequences explicitly—flat and hierarchical attention combination.
",1 Introduction,[0],[0]
"This paper is organized as follows: In Section 2, we review the attention mechanism in single-source S2S learning.",1 Introduction,[0],[0]
Section 3 introduces new attention combination strategies.,1 Introduction,[0],[0]
"In Section 4, we evaluate the proposed models on the MMT and APE tasks.",1 Introduction,[0],[0]
"We summarize the related work in Section 5, and conclude in Section 6.",1 Introduction,[0],[0]
The attention mechanism in S2S learning allows an RNN decoder to directly access information about the input each time before it emits a symbol.,2 Attentive S2S Learning,[0],[0]
"Inspired by content-based addressing in Neural Turing Machines (Graves et al., 2014), the attention mechanism estimates a probability distribution over the encoder hidden states in each decoding step.",2 Attentive S2S Learning,[0],[0]
"This distribution is used for computing the context vector—the weighted average of the encoder hidden states—as an additional input to the decoder.
",2 Attentive S2S Learning,[0],[0]
"The standard attention model as described by Bahdanau et al. (2014) defines the attention energies eij , attention distribution αij , and the con-
196
text vector ci in i-th decoder step as:
eij = v > a tanh(Wasi + Uahj), (1)
αij = exp(eij)∑Tx k=1 exp(eik) , (2)
ci =
Tx∑
j=1
αijhj .",2 Attentive S2S Learning,[0],[0]
"(3)
The trainable parameters Wa and Ua are projection matrices that transform the decoder and encoder states si and hj into a common vector space and va is a weight vector over the dimensions of this space.",2 Attentive S2S Learning,[0],[0]
Tx denotes the length of the input sequence.,2 Attentive S2S Learning,[0],[0]
"For the sake of clarity, bias terms (applied every time a vector is linearly projected using a weight matrix) are omitted.
",2 Attentive S2S Learning,[0],[0]
"Recently, Lu et al. (2016) introduced sentinel gate, an extension of the attentive RNN decoder with LSTM units (Hochreiter and Schmidhuber, 1997).",2 Attentive S2S Learning,[0],[0]
"We adapt the extension for gated recurrent units (GRU) (Cho et al., 2014), which we use in our experiments:
ψi = σ(Wyyi +Wssi−1) (4)
where Wy and Ws are trainable parameters, yi is the embedded decoder input, and si−1 is the previous decoder state.
",2 Attentive S2S Learning,[0],[0]
"Analogically to Equation 1, we compute a scalar energy term for the sentinel:
eψi = v > a tanh ( Wasi + U (ψ) a (ψi si) )",2 Attentive S2S Learning,[0],[0]
"(5)
where Wa, U (ψ) a are the projection matrices, va is the weight vector, and ψi si is the sentinel vector.",2 Attentive S2S Learning,[0],[0]
Note that the sentinel energy term does not depend on any hidden state of any encoder.,2 Attentive S2S Learning,[0],[0]
The sentinel vector is projected to the same vector space as the encoder state hj in Equation 1.,2 Attentive S2S Learning,[0],[0]
"The term eψi is added as an extra attention energy term to Equation 2 and the sentinel vector ψi si is used as the corresponding vector in the summation in Equation 3.
",2 Attentive S2S Learning,[0],[0]
This technique should allow the decoder to choose whether to attend to the encoder or to focus on its own state and act more like a language model.,2 Attentive S2S Learning,[0],[0]
This can be beneficial if the encoder does not contain much relevant information for the current decoding step.,2 Attentive S2S Learning,[0],[0]
"In S2S models with multiple encoders, the decoder needs to be able to combine the attention information collected from the encoders.
",3 Attention Combination,[0],[0]
"A widely adopted technique for combining multiple attention models in a decoder is concatenation of the context vectors c(1)i , . . .",3 Attention Combination,[0],[0]
", c (N)",3 Attention Combination,[0],[0]
"i (Zoph and Knight, 2016; Firat et al., 2016).",3 Attention Combination,[0],[0]
"As mentioned in Section 1, this setting forces the model to attend to each encoder independently and lets the attention combination to be resolved implicitly in the subsequent network layers.
",3 Attention Combination,[0],[0]
"In this section, we propose two alternative strategies of combining attentions from multiple encoders.",3 Attention Combination,[0],[0]
"We either let the decoder learn the αi distribution jointly over all encoder hidden states (flat attention combination) or factorize the distribution over individual encoders (hierarchical combination).
",3 Attention Combination,[0],[0]
Both of the alternatives allow us to explicitly compute distribution over the encoders and thus interpret how much attention is paid to each encoder at every decoding step.,3 Attention Combination,[0],[0]
Flat attention combination projects the hidden states of all encoders into a shared space and then computes an arbitrary distribution over the projections.,3.1 Flat Attention Combination,[0],[0]
"The difference between the concatenation of the context vectors and the flat attention combination is that the αi coefficients are computed jointly for all encoders:
α (k) ij =
exp(e (k) ij )
",3.1 Flat Attention Combination,[0],[0]
"∑N n=1 ∑T (n)x m=1 exp ( e (n) im ) (6)
",3.1 Flat Attention Combination,[0],[0]
where T (n)x is the length of the input sequence of the n-th encoder and e(k)ij is the attention energy of the j-th state of the k-th encoder in the i-th decoding step.,3.1 Flat Attention Combination,[0],[0]
These attention energies are computed as in Equation 1.,3.1 Flat Attention Combination,[0],[0]
"The parameters va andWa are shared among the encoders, and Ua is different for each encoder and serves as an encoder-specific projection of hidden states into a common vector space.
",3.1 Flat Attention Combination,[0],[0]
"The states of the individual encoders occupy different vector spaces and can have a different dimensionality, therefore the context vector cannot be computed as their weighted sum.",3.1 Flat Attention Combination,[0],[0]
"We project
them into a single space using linear projections:
ci = N∑
k=1
T (k) x∑
j=1
α (k) ij U (k) c h (k) j (7)
",3.1 Flat Attention Combination,[0],[0]
where U (k)c are additional trainable parameters.,3.1 Flat Attention Combination,[0],[0]
The matrices U (k)c project the hidden states into a common vector space.,3.1 Flat Attention Combination,[0],[0]
"This raises a question whether this space can be the same as the one that is projected into in the energy computation using matrices U (k)a in Equation 1, i.e., whether U (k) c",3.1 Flat Attention Combination,[0],[0]
= U (k) a .,3.1 Flat Attention Combination,[0],[0]
"In our experiments, we explore both options.",3.1 Flat Attention Combination,[0],[0]
We also try both adding and not adding the sentinel α(ψ)i,3.1 Flat Attention Combination,[0],[0]
U (ψ) c,3.1 Flat Attention Combination,[0],[0]
(ψi si) to the context vector.,3.1 Flat Attention Combination,[0],[0]
"The hierarchical attention combination model computes every context vector independently, similarly to the concatenation approach.",3.2 Hierarchical Attention Combination,[0],[0]
"Instead of concatenation, a second attention mechanism is constructed over the context vectors.
",3.2 Hierarchical Attention Combination,[0],[0]
"We divide the computation of the attention distribution into two steps: First, we compute the context vector for each encoder independently using Equation 3.",3.2 Hierarchical Attention Combination,[0],[0]
"Second, we project the context vectors (and optionally the sentinel) into a common space (Equation 8), we compute another distribution over the projected context vectors (Equation 9) and their corresponding weighted average (Equation 10):
e (k) i = v > b tanh(Wbsi + U (k) b c (k) i ), (8)
β (k) i =",3.2 Hierarchical Attention Combination,[0],[0]
exp(e (k) i ),3.2 Hierarchical Attention Combination,[0],[0]
"∑N
n=1 exp(e (n) i )
, (9)
ci = N∑
k=1
β (k)",3.2 Hierarchical Attention Combination,[0],[0]
i U (k) c c,3.2 Hierarchical Attention Combination,[0],[0]
"(k) i (10)
where c(k)i is the context vector of the k-th encoder, additional trainable parameters vb and Wb are shared for all encoders, and U (k)b and U (k) c are encoder-specific projection matrices, that can be set equal and shared, similarly to the case of flat attention combination.",3.2 Hierarchical Attention Combination,[0],[0]
"We evaluate the attention combination strategies presented in Section 3 on the tasks of multimodal translation (Section 4.1) and automatic post-editing (Section 4.2).
",4 Experiments,[0],[0]
"The models were implemented using the Neural Monkey sequence-to-sequence learning toolkit (Helcl and Libovický, 2017).12 In both setups, we process the textual input with bidirectional GRU network (Cho et al., 2014) with 300 units in the hidden state in each direction and 300 units in embeddings.",4 Experiments,[0],[0]
"For the attention projection space, we use 500 hidden units.",4 Experiments,[0],[0]
"We optimize the network to minimize the output cross-entropy using the Adam algorithm (Kingma and Ba, 2014) with learning rate 10−4.",4 Experiments,[0],[0]
"The goal of multimodal translation (Specia et al., 2016) is to generate target-language image captions given both the image and its caption in the source language.
",4.1 Multimodal Translation,[0],[0]
"We train and evaluate the model on the Multi30k dataset (Elliott et al., 2016).",4.1 Multimodal Translation,[0],[0]
"It consists of 29,000 training instances (images together with English captions and their German translations), 1,014 validation instances, and 1,000 test instances.",4.1 Multimodal Translation,[0],[0]
"The results are evaluated using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011).
",4.1 Multimodal Translation,[0],[0]
"In our model, the visual input is processed with a pre-trained VGG 16 network (Simonyan and Zisserman, 2014) without further fine-tuning.",4.1 Multimodal Translation,[0],[0]
"Atten-
",4.1 Multimodal Translation,[0],[0]
"1http://github.com/ufal/neuralmonkey 2The trained models can be downloaded from
http://ufallab.ms.mff.cuni.cz/ ˜libovicky/acl2017_att_models/
tion distribution over the visual input is computed from the last convolutional layer of the network.",4.1 Multimodal Translation,[0],[0]
"The decoder is an RNN with 500 conditional GRU units (Firat and Cho, 2016) in the recurrent layer.",4.1 Multimodal Translation,[0],[0]
"We use byte-pair encoding (Sennrich et al., 2016b) with a vocabulary of 20,000 subword units shared between the textual encoder and the decoder.
",4.1 Multimodal Translation,[0],[0]
The results of our experiments in multimodal MT are shown in Table 1.,4.1 Multimodal Translation,[0],[0]
"We achieved the best results using the hierarchical attention combination without the sentinel mechanism, which also showed the fastest convergence.",4.1 Multimodal Translation,[0],[0]
The flat combination strategy achieves similar results eventually.,4.1 Multimodal Translation,[0],[0]
Sharing the projections for energy and context vector computation does not improve over the concatenation baseline and slows the training almost prohibitively.,4.1 Multimodal Translation,[0],[0]
"Multimodal models were not able to surpass the textual baseline (BLEU 33.0).
",4.1 Multimodal Translation,[0],[0]
"Using the conditional GRU units brought an improvement of about 1.5 BLEU points on average, with the exception of the concatenation scenario where the performance dropped by almost 5 BLEU points.",4.1 Multimodal Translation,[0],[0]
"We hypothesize this is caused by the fact the model has to learn the implicit attention combination on multiple places – once in the output projection and three times inside the conditional GRU unit (Firat and Cho, 2016, Equations 10-12).",4.1 Multimodal Translation,[0],[0]
We thus report the scores of the introduced attention combination techniques trained with conditional GRU units and compare them with the concatenation baseline trained with plain GRU units.,4.1 Multimodal Translation,[0],[0]
"Automatic post-editing is a task of improving an automatically generated translation given the source sentence where the translation system is treated as a black box.
",4.2 Automatic MT Post-editing,[0],[0]
"We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain.",4.2 Automatic MT Post-editing,[0],[0]
"Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference.",4.2 Automatic MT Post-editing,[0],[0]
"In case of this dataset, the MT outputs are almost perfect in and only little effort was required to post-edit the sentences.",4.2 Automatic MT Post-editing,[0],[0]
"The results are evaluated using the humantargeted error rate (HTER) (Snover et al., 2006) and BLEU score (Papineni et al., 2002).
",4.2 Automatic MT Post-editing,[0],[0]
"Following Libovický et al. (2016), we encode the target sentence as a sequence of edit operations transforming the MT output into the reference.",4.2 Automatic MT Post-editing,[0],[0]
"By this technique, we prevent the model from paraphrasing the input sentences.",4.2 Automatic MT Post-editing,[0],[0]
The decoder is a GRU network with 300 hidden units.,4.2 Automatic MT Post-editing,[0],[0]
"Unlike in the MMT setup (Section 4.1), we do not use the conditional GRU because it is prone to overfitting on the small dataset we work with.
",4.2 Automatic MT Post-editing,[0],[0]
"The models were able to slightly, but significantly improve over the baseline – leaving the MT output as is (HTER 24.8).",4.2 Automatic MT Post-editing,[0],[0]
The differences between the attention combination strategies are not significant.,4.2 Automatic MT Post-editing,[0],[0]
"Attempts to use S2S models for APE are relatively rare (Bojar et al., 2016).",5 Related Work,[0],[0]
"Niehues et al. (2016) concatenate both inputs into one long sequence, which forces the encoder to be able to work with both source and target language.",5 Related Work,[0],[0]
"Their attention is then similar to our flat combination strategy; however, it can only be used for sequential data.
",5 Related Work,[0],[0]
"The best system from the WMT’16 competition (Junczys-Dowmunt and Grundkiewicz, 2016) trains two separate S2S models,",5 Related Work,[0],[0]
one translating from MT output to post-edited targets and the second one from source sentences to post-edited targets.,5 Related Work,[0],[0]
The decoders average their output distributions similarly to decoder ensembling.,5 Related Work,[0],[0]
"The biggest source of improvement in this state-of-theart posteditor came from additional training data generation, rather than from changes in the network architecture.
",5 Related Work,[0],[0]
Source: a man sleeping in a green room on a couch .,5 Related Work,[0],[0]
Reference:,5 Related Work,[0],[0]
ein Mann schläft in einem grünen Raum auf einem Sofa .,5 Related Work,[0],[0]
"Output with attention:
Caglayan et al. (2016) used an architecture very similar to ours for multimodal translation.",5 Related Work,[0],[0]
They made a strong assumption that the network can be trained in such a way that the hidden states of the encoder and the convolutional network occupy the same vector space and thus sum the context vectors from both modalities.,5 Related Work,[0],[0]
"In this way, their multimodal MT system (BLEU 27.82) remained far bellow the text-only setup (BLEU 32.50).
",5 Related Work,[0],[0]
New state-of-the-art results on the Multi30k dataset were achieved very recently by Calixto et al. (2017).,5 Related Work,[0],[0]
"The best-performing architecture uses the last fully-connected layer of VGG-19 network (Simonyan and Zisserman, 2014) as decoder initialization and only attends to the text encoder hidden states.",5 Related Work,[0],[0]
"With a stronger monomodal baseline (BLEU 33.7), their multimodal model achieved a BLEU score of 37.1.",5 Related Work,[0],[0]
"Similarly to Niehues et al. (2016) in the APE task, even further improvement was achieved by synthetically extending the dataset.",5 Related Work,[0],[0]
We introduced two new strategies of combining attention in a multi-source sequence-to-sequence setup.,6 Conclusions,[0],[0]
"Both methods are based on computing a joint distribution over hidden states of all encoders.
",6 Conclusions,[0],[0]
"We conducted experiments with the proposed strategies on multimodal translation and automatic post-editing tasks, and we showed that the flat and hierarchical attention combination can be applied to these tasks with maintaining competitive score to previously used techniques.
",6 Conclusions,[0],[0]
"Unlike the simple context vector concatenation, the introduced combination strategies can be used with the conditional GRU units in the decoder.",6 Conclusions,[0],[0]
"On top of that, the hierarchical combination strategy exhibits faster learning than than the other strategies.",6 Conclusions,[0],[0]
"We would like to thank Ondřej Dušek, Rudolf Rosa, Pavel Pecina, and Ondřej Bojar for a fruitful discussions and comments on the draft of the paper.
",Acknowledgments,[0],[0]
"This research has been funded by the Czech Science Foundation grant no. P103/12/G084, the EU grant no.",Acknowledgments,[0],[0]
"H2020-ICT-2014-1-645452 (QT21), and Charles University grant no. 52315/2014 and SVV project",Acknowledgments,[0],[0]
no.,Acknowledgments,[0],[0]
260 453.,Acknowledgments,[0],[0]
This work has been using language resources developed and/or stored and/or distributed by the LINDAT-Clarin project of the Ministry of Education of the Czech Republic (project LM2010013).,Acknowledgments,[0],[0]
"Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities.",abstractText,[0],[0]
"We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical.",abstractText,[0],[0]
We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks.,abstractText,[0],[0]
We show that the proposed methods achieve competitive results on both tasks.,abstractText,[0],[0]
Attention Strategies for Multi-Source Sequence-to-Sequence Learning,title,[0],[0]
In typical machine learning problems like image classification it is assumed that an image clearly represents a category (a class).,1. Introduction,[0],[0]
"However, in many real-life applications multiple instances are observed and only a general statement of the category is given.",1. Introduction,[0],[0]
"This scenario is called multiple instance learning (MIL) (Dietterich et al., 1997; Maron & Lozano-Pérez, 1998) or, learning from weakly annotated data (Oquab et al., 2014).",1. Introduction,[0],[0]
"The problem of weakly annotated data is especially apparent in medical imaging (Quellec et al., 2017) (e.g., computational pathology, mammography or CT lung screening) where an image is typically described by a single label (benign/malignant) or a Region Of Interest (ROI) is roughly given.
",1. Introduction,[0],[0]
MIL deals with a bag of instances for which a single class label is assigned.,1. Introduction,[0],[0]
"Hence, the main goal of MIL is to learn a
*Equal contribution 1University of Amsterdam, the Netherlands.",1. Introduction,[0],[0]
"Correspondence to: Maximilian Ilse <m.ilse@uva.nl>, Jakub M. Tomczak <j.m.tomczak@uva.nl>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
model that predicts a bag label, e.g., a medical diagnosis.",1. Introduction,[0],[0]
"An additional challenge is to discover key instances (Liu et al., 2012), i.e., the instances that trigger the bag label.",1. Introduction,[0],[0]
In the medical domain the latter task is of great interest because of legal issues1 and its usefulness in clinical practice.,1. Introduction,[0],[0]
"In order to solve the primary task of a bag classification different methods are proposed, such as utilizing similarities among bags (Cheplygina et al., 2015b), embedding instances to a compact low-dimensional representation that is further fed to a bag-level classifier (Andrews et al., 2003; Chen et al., 2006), and combining responses of an instance-level classifier (Ramon & De Raedt, 2000; Raykar et al., 2008; Zhang et al., 2006).",1. Introduction,[0],[0]
Only the last approach is capable of providing interpretable results.,1. Introduction,[0],[0]
"However, it was shown that the instance level accuracy of such methods is low (Kandemir & Hamprecht, 2015) and in general there is a disagreement among MIL methods at the instance level (Cheplygina et al., 2015a).",1. Introduction,[0],[0]
"These issues call into question the usability of current MIL models for interpreting the final decision.
",1. Introduction,[0],[0]
"In this paper, we propose a new method that aims at incorporating interpretability to the MIL approach and increasing its flexibility.",1. Introduction,[0],[0]
We formulate the MIL model using the Bernoulli distribution for the bag label and train it by optimizing the log-likelihood function.,1. Introduction,[0],[0]
"We show that the application of the Fundamental Theorem of Symmetric Functions provides a general procedure for modeling the bag label probability (the bag score function) that consists of three steps: (i) a transformation of instances to a low-dimensional embedding, (ii) a permutation-invariant (symmetric) aggregation function, and (iii) a final transformation to the bag probability.",1. Introduction,[0],[0]
"We propose to parameterize all transformations using neural networks (i.e., a combination of convolutional and fully-connected layers), which increases the flexibility of the approach and allows to train the model in an endto-end manner by optimizing an unconstrained objective function.",1. Introduction,[0],[0]
"Last but not least, we propose to replace widelyused permutation-invariant operators such as the maximum operator max and the mean operator mean by a trainable weighted average where weights are given by a two-layered neural network.",1. Introduction,[0],[0]
"The two-layered neural network corre-
1According to the European Union General Data Protection Regulation (taking effect 2018), a user should have the right to obtain an explanation of the decision reached.
sponds to the attention mechanism (Bahdanau et al., 2014; Raffel & Ellis, 2015).",1. Introduction,[0],[0]
"Notably, the attention weights allow us to find key instances, which could be further used to highlight possible ROIs.",1. Introduction,[0],[0]
"In the experiments we show that our model is on a par with the best classical MIL methods on common benchmark MIL datasets, and that it outperforms other methods on a MNIST-based MIL problem as well as two real-life histopathology image datasets.",1. Introduction,[0],[0]
"Moreover, in the image datasets we provide empirical evidence that our model can indicate key instances.",1. Introduction,[0],[0]
"Problem formulation In the classical (binary) supervised learning problem one aims at finding a model that predicts a value of a target variable, y ∈ {0, 1}, for a given instance, x ∈ RD.",2.1. Multiple instance learning (MIL),[0],[0]
"In the case of the MIL problem, however, instead of a single instance there is a bag of instances, X = {x1, . . .",2.1. Multiple instance learning (MIL),[0],[0]
",xK}, that exhibit neither dependency nor ordering among each other.",2.1. Multiple instance learning (MIL),[0],[0]
We assume that K could vary for different bags.,2.1. Multiple instance learning (MIL),[0],[0]
There is also a single binary label Y associated with the bag.,2.1. Multiple instance learning (MIL),[0],[0]
"Furthermore, we assume that individual labels exist for the instances within a bag, i.e., y1, . . .",2.1. Multiple instance learning (MIL),[0],[0]
", yK and yk ∈ {0, 1}, for k = 1, . . .",2.1. Multiple instance learning (MIL),[0],[0]
",K, however, there is no access to those labels and they remain unknown during training.",2.1. Multiple instance learning (MIL),[0],[0]
"We can re-write the assumptions of the MIL problem in the following form:
Y =
{ 0, iff ∑ k yk = 0,
1, otherwise.",2.1. Multiple instance learning (MIL),[0],[0]
"(1)
These assumptions imply that a MIL model must be permutation-invariant.",2.1. Multiple instance learning (MIL),[0],[0]
"Further, the two statements could be re-formulated in a compact form using the maximum operator:
Y = max k {yk}.",2.1. Multiple instance learning (MIL),[0],[0]
"(2)
Learning a model that tries to optimize an objective based on the maximum over instance labels would be problematic at least for two reasons.",2.1. Multiple instance learning (MIL),[0],[0]
"First, all gradient-based learning methods would encounter issues with vanishing gradients.",2.1. Multiple instance learning (MIL),[0],[0]
"Second, this formulation is suitable only when an instancelevel classifier is used.
",2.1. Multiple instance learning (MIL),[0],[0]
"In order to make the learning problem easier, we propose to train a MIL model by optimizing the log-likelihood function where the bag label is distributed according to the Bernoulli distribution with the parameter θ(X) ∈",2.1. Multiple instance learning (MIL),[0],[0]
"[0, 1], i.e., the probability of Y = 1 given the bag of instances X .
MIL approaches In the MIL setting the bag probability θ(X) must be permutation-invariant since we assume neither ordering nor dependency of instances within a bag.",2.1. Multiple instance learning (MIL),[0],[0]
"Therefore, the MIL problem can be considered in terms of
a specific form of the Fundamental Theorem of Symmetric Functions with monomials given by the following theorem (Zaheer et al., 2017):
Theorem 1.",2.1. Multiple instance learning (MIL),[0],[0]
"A scoring function for a set of instances X , S(X) ∈ R, is a symmetric function (i.e., permutationinvariant to the elements in X), if and only if it can be decomposed in the following form:
S(X)",2.1. Multiple instance learning (MIL),[0],[0]
"= g ( ∑ x∈X f(x) ) , (3)
where f and g are suitable transformations.
",2.1. Multiple instance learning (MIL),[0],[0]
This theorem provides a general strategy for modeling the bag probability using the decomposition given in (3).,2.1. Multiple instance learning (MIL),[0],[0]
"A similar decomposition with max instead of sum is given by the following theorem (Qi et al., 2017):
Theorem 2.",2.1. Multiple instance learning (MIL),[0],[0]
"For any ε > 0, a Hausdorff continuous symmetric function S(X) ∈ R can be arbitrarily approximated by a function in the form g ( maxx∈X f(x) ) , where max is the element-wise vector maximum operator and f and g are continuous functions, that is:
|S(X)− g ( max x∈X f(x) )",2.1. Multiple instance learning (MIL),[0],[0]
| < ε.,2.1. Multiple instance learning (MIL),[0],[0]
"(4)
The difference between Theorems 1 and 2 is that the former is a universal decomposition while the latter provides an arbitrary approximation.",2.1. Multiple instance learning (MIL),[0],[0]
"Nonetheless, they both formulate a general three-step approach for classifying a bag of instances: (i) a transformation of instances using the function f , (ii) a combination of transformed instances using a symmetric (permutation-invariant) function σ, (iii) a transformation of combined instances transformed by f using a function g. Finally, the expressiveness of the score function relies on the choice of classes of functions for f and g.
In the MIL problem formulation the score function in both theorems is the probability θ(X) and the permutationinvariant function σ is referred to as the MIL pooling.",2.1. Multiple instance learning (MIL),[0],[0]
"The choice of functions f , g and σ determines a specific approach to modeling the label probability.",2.1. Multiple instance learning (MIL),[0],[0]
"For a given MIL operator there are two main MIL approaches:
(i)",2.1. Multiple instance learning (MIL),[0],[0]
The instance-level approach: The transformation f is an instance-level classifier that returns scores for each instance.,2.1. Multiple instance learning (MIL),[0],[0]
Then individual scores are aggregated by MIL pooling to obtain θ(X).,2.1. Multiple instance learning (MIL),[0],[0]
"The function g is the identity function.
",2.1. Multiple instance learning (MIL),[0],[0]
(ii),2.1. Multiple instance learning (MIL),[0],[0]
The embedding-level approach: The function f maps instances to a low-dimensional embedding.,2.1. Multiple instance learning (MIL),[0],[0]
MIL pooling is used to obtain a bag representation that is independent of the number of instances in the bag.,2.1. Multiple instance learning (MIL),[0],[0]
"The bag representation is further processed by a bag-level classifier to provide θ(X).
",2.1. Multiple instance learning (MIL),[0],[0]
"It is advocated in (Wang et al., 2016) that the latter approach is preferable in terms of the bag level classification performance.",2.1. Multiple instance learning (MIL),[0],[0]
"Since the individual labels are unknown, there is a threat that the instance-level classifier might be trained insufficiently and it introduces additional error to the final prediction.",2.1. Multiple instance learning (MIL),[0],[0]
The embedding-level approach determines a joint representation of a bag and therefore it does not introduce additional bias to the bag-level classifier.,2.1. Multiple instance learning (MIL),[0],[0]
"On the other hand, the instance-level approach provides a score that can be used to find key instances i.e., the instances that trigger the bag label.",2.1. Multiple instance learning (MIL),[0],[0]
Liu et al. (2012) were able to show that a model that is successfully detecting key instances is more likely to achieve better bag label predictions.,2.1. Multiple instance learning (MIL),[0],[0]
We will show how to modify the embedding-level approach to be interpretable by using a new MIL pooling.,2.1. Multiple instance learning (MIL),[0],[0]
"In classical MIL problems it is assumed that instances are represented by features that do not require further processing, i.e., f is the identity.",2.2. MIL with Neural Networks,[0],[0]
"However, for some tasks like image or text analysis additional steps of feature extraction are necessary.",2.2. MIL with Neural Networks,[0],[0]
"Additionally, Theorem 1 and 2 indicate that for a flexible enough class of functions we can model any permutation-invariant score function.",2.2. MIL with Neural Networks,[0],[0]
"Therefore, we consider a class of transformations that are parameterized by neural networks fψ(·) with parameters ψ that transform the k-th instance into a low-dimensional embedding, hk = fψ(xk), where hk ∈ H such that H =",2.2. MIL with Neural Networks,[0],[0]
"[0, 1] for the instance-based approach andH = RM for the embeddingbased approach.
",2.2. MIL with Neural Networks,[0],[0]
"Eventually, the parameter θ(X) is determined by a transformation",2.2. MIL with Neural Networks,[0],[0]
gφ :,2.2. MIL with Neural Networks,[0],[0]
HK,2.2. MIL with Neural Networks,[0],[0]
"→ [0, 1].",2.2. MIL with Neural Networks,[0],[0]
"In the instance-based approach the transformation gφ is simply the identity, while in the embedding-based approach it could be also parameterized by a neural network with parameters φ.",2.2. MIL with Neural Networks,[0],[0]
"The former approach is depicted in Figure 6(a) and the latter in Figure 6(b) in the Appendix.
",2.2. MIL with Neural Networks,[0],[0]
The idea of parameterizing all transformations using neural networks is very appealing because the whole approach can be arbitrarily flexible and it can be trained end-to-end by backpropagation.,2.2. MIL with Neural Networks,[0],[0]
The only restriction is that the MIL pooling must be differentiable.,2.2. MIL with Neural Networks,[0],[0]
The formulation of the MIL problem requires the MIL pooling σ to be permutation-invariant.,2.3. MIL pooling,[0],[0]
"As shown in Theorem 1 and 2, there are two MIL pooling operators that ensure the score function (i.e., the bag probability) to be a symmetric function, namely, the maximum operator:
∀m=1,...,M : zm = max k=1,...,K {hkm}, (5)
and the mean operator:2
z = 1
K K∑ k=1 hk.",2.3. MIL pooling,[0],[0]
"(6)
In fact, other operators could be used such as, the convex maximum operator (i.e., log-sum-exp) (Ramon & De Raedt, 2000), Integrated Segmentation and Recognition (Keeler et al., 1991), noisy-or (Maron & Lozano-Pérez, 1998) and noisy-and (Kraus et al., 2016).",2.3. MIL pooling,[0],[0]
"These MIL pooling operators could replace max in Theorem 2 and proofs would follow in a similar manner (see Supplementary in (Qi et al., 2017) for a detailed proof for the maximum operator).",2.3. MIL pooling,[0],[0]
"All of these operators are differentiable, hence, they could be easily used as a MIL pooling layer in a deep neural network architecture.",2.3. MIL pooling,[0],[0]
"All MIL pooling operators mentioned in the previous section have a clear disadvantage, namely, they are pre-defined and non-trainable.",2.4. Attention-based MIL pooling,[0],[0]
"For instance, the max-operator could be a good choice in the instance-based approach but it might be inappropriate for the embedding-based approach.",2.4. Attention-based MIL pooling,[0],[0]
"Similarly, the mean operator is definitely a bad MIL pooling to aggregate instance scores, although, it could succeed in calculating the bag representation.",2.4. Attention-based MIL pooling,[0],[0]
"Therefore, a flexible and adaptive MIL pooling could potentially achieve better results by adjusting to a task and data.",2.4. Attention-based MIL pooling,[0],[0]
"Ideally, such MIL pooling should also be interpretable, a trait that is missing in all operators mentioned in Section 2.3.
",2.4. Attention-based MIL pooling,[0],[0]
Attention mechanism We propose to use a weighted average of instances (low-dimensional embeddings) where weights are determined by a neural network.,2.4. Attention-based MIL pooling,[0],[0]
"Additionally, the weights must sum to 1 to be invariant to the size of a bag.",2.4. Attention-based MIL pooling,[0],[0]
The weighted average fulfills the requirements of the Theorem 1 where the weights together with the embeddings are part of the f function.,2.4. Attention-based MIL pooling,[0],[0]
"Let H = {h1, . . .",2.4. Attention-based MIL pooling,[0],[0]
",hK} be a bag of K embeddings, then we propose the following MIL pooling:
z = K∑ k=1 akhk, (7)
where:
ak = exp{w> tanh
( Vh>k ) }
K∑ j=1 exp{w> tanh",2.4. Attention-based MIL pooling,[0],[0]
"( Vh>j ) } , (8)
where w ∈ RL×1 and V ∈ RL×M are parameters.",2.4. Attention-based MIL pooling,[0],[0]
"Moreover, we utilize the hyperbolic tangent tanh(·) element-wise non-linearity to include both negative and positive values for proper gradient flow.",2.4. Attention-based MIL pooling,[0],[0]
"The proposed construction allows to discover (dis)similarities among instances.
",2.4. Attention-based MIL pooling,[0],[0]
"2Notice that the weight 1 K can be seen as a part of the f function.
",2.4. Attention-based MIL pooling,[0],[0]
"Interestingly, the proposed MIL pooling corresponds to a version of the attention mechanism (Lin et al., 2017; Raffel & Ellis, 2015).",2.4. Attention-based MIL pooling,[0],[0]
The main difference is that typically in the attention mechanism all instances are sequentially dependent while here we assume that all instances are independent.,2.4. Attention-based MIL pooling,[0],[0]
"Therefore, a naturally arising question is whether the attention mechanism could work without sequential dependencies among instances, and if it will not learn the mean operator.",2.4. Attention-based MIL pooling,[0],[0]
"We will address this issue in the experiments.
",2.4. Attention-based MIL pooling,[0],[0]
Gated attention mechanism,2.4. Attention-based MIL pooling,[0],[0]
"Furthermore, we notice that the tanh(·) non-linearity could be inefficient to learn complex relations.",2.4. Attention-based MIL pooling,[0],[0]
Our concern follows from the fact that tanh(x) is approximately linear for x ∈,2.4. Attention-based MIL pooling,[0],[0]
"[−1, 1], which could limit the final expressiveness of learned relations among instances.",2.4. Attention-based MIL pooling,[0],[0]
"Therefore, we propose to additionally use the gating mechanism (Dauphin et al., 2016) together with tanh(·) non-linearity that yields:
ak = exp{w>
( tanh ( Vh>k ) sigm",2.4. Attention-based MIL pooling,[0],[0]
"( Uh>k )) }
K∑ j=1 exp{w> ( tanh ( Vh>j ) sigm",2.4. Attention-based MIL pooling,[0],[0]
"( Uh>j )) } , (9)
where U ∈ RL×M are parameters, is an element-wise multiplication and sigm(·) is the sigmoid non-linearity.",2.4. Attention-based MIL pooling,[0],[0]
"The gating mechanism introduces a learnable non-linearity that potentially removes the troublesome linearity in tanh(·).
Flexibility In principle, the proposed attention-based MIL pooling allows to assign different weights to instances within a bag and hence the final representation of the bag could be highly informative for the bag-level classifier.",2.4. Attention-based MIL pooling,[0],[0]
"In other words, it should be able to find key instances.",2.4. Attention-based MIL pooling,[0],[0]
"Moreover, application of the attention-based MIL pooling together with the transformations f and g parameterized by neural networks makes the whole model fully differentiable and adaptive.",2.4. Attention-based MIL pooling,[0],[0]
These two facts make the proposed MIL pooling a potentially very flexible operator that could model an arbitrary permutation-invariant score function.,2.4. Attention-based MIL pooling,[0],[0]
"The proposed attention mechanism together with a deep MIL model is depicted in Figure 6(c) in the Appendix.
",2.4. Attention-based MIL pooling,[0],[0]
"Interpretability Ideally, in the case of a positive label (Y = 1), high attention weights should be assigned to instances that are likely to have label yk = 1 (key instances).",2.4. Attention-based MIL pooling,[0],[0]
"Namely, the attention mechanism allows to easily interpret the provided decision in terms of instance-level labels.",2.4. Attention-based MIL pooling,[0],[0]
"In fact, the attention network does not provide scores as the instance-based classifier does but it can be considered as a proxy to that.",2.4. Attention-based MIL pooling,[0],[0]
"The attention-based MIL pooling bridges the instance-level approach and the embedding-level approach.
",2.4. Attention-based MIL pooling,[0],[0]
"From the practical point of view, e.g., in the computational pathology, it is desirable to provide ROIs together with the final diagnosis to a doctor.",2.4. Attention-based MIL pooling,[0],[0]
"Therefore, the attention mechanism is potentially of great interest in practical applications.",2.4. Attention-based MIL pooling,[0],[0]
"MIL pooling Typically, MIL approaches utilize either the mean pooling or the max pooling, while the latter is mostly used (Feng & Zhou, 2017; Pinheiro & Collobert, 2015; Zhu et al., 2017).",3. Related work,[0],[0]
Both operators are non-trainable which potentially limits their applicability.,3. Related work,[0],[0]
"There are MIL pooling operators that contain global adaptive parameters, such as noisy-and (Kraus et al., 2016), however, their flexibility is restricted.",3. Related work,[0],[0]
"We propose a fully trainable MIL pooling that adapts to new instances.
",3. Related work,[0],[0]
MIL with neural networks In the classical work on MIL it is assumed that instances are represented by precomputed features and there is very little need to apply additional feature extraction.,3. Related work,[0],[0]
"Nevertheless, recent work on utilizing fully-connected neural networks in MIL shows that it could still be beneficial (Wang et al., 2016).",3. Related work,[0],[0]
"Similarly, in computer vision the idea of MIL combined with deep learning significantly improves final accuracy (Oquab et al., 2014).",3. Related work,[0],[0]
"In this paper, we follow this line of research since it allows to apply a flexible class of transformations that can be trained end-to-end by backpropagation.
MIL and attention The attention mechanism is widely used in deep learning for image captioning (Xu et al., 2015) or text analysis (Bahdanau et al., 2014; Lin et al., 2017).",3. Related work,[0],[0]
In the context of the MIL problem it has rarely been used and only in a very limited form.,3. Related work,[0],[0]
"In (Pappas & Popescu-Belis, 2014) an attention-based MIL was proposed but attention weights were trained as parameters of an auxiliary linear regression model.",3. Related work,[0],[0]
"This idea was further expanded and the linear regression model was replaced by a one-layer neural network with single output (Pappas & Popescu-Belis, 2017).",3. Related work,[0],[0]
"The attention-based MIL operator was used very recently in (Qi et al., 2017), however, the attention was calculated using the dot product and it performed worse than the max operator.",3. Related work,[0],[0]
"Here, we propose to use a two-layered neural network to learn the MIL operator and we show that it outperforms commonly used MIL pooling operators.
",3. Related work,[0],[0]
MIL for medical imaging The MIL seems to perfectly fit medical imaging where processing a whole image consisting of billions of pixels is computationally infeasible.,3. Related work,[0],[0]
"Moreover, in the medical domain it is very difficult to obtain pixel-level annotations, that drastically reduces number of available data.",3. Related work,[0],[0]
"Therefore, it is tempting to divide a medical image into smaller patches that could be further considered as a bag with a single label (Quellec et al., 2017).",3. Related work,[0],[0]
"This idea attracts a great interest in the computational histopathology where patches could correspond to cells that are believed to indicate malignant changes (Sirinukunwattana et al., 2016).",3. Related work,[0],[0]
"Different MIL approaches were used for histopathology data, such as, Gaussian processes (Kandemir et al., 2014; 2016) or a two-stage approach with neural networks and EM algorithm to determine instance classes (Hou et al., 2016).
",3. Related work,[0],[0]
"Other applications of MIL methods in medical imaging are mammography (nodule) classification (Zhu et al., 2017) and microscopy cell detection (Kraus et al., 2016).",3. Related work,[0],[0]
"In this paper, we show that the proposed attention-based deep MIL approach can be used not only to provide the final diagnosis but also to indicate ROIs in a histopathology slide.",3. Related work,[0],[0]
In the experiments we aim at evaluating the proposed approach: a MIL model parameterized with neural networks and a (gated) attention-based pooling layer (’Attention’ and ’Gated-Attention’).,4. Experiments,[0],[0]
"We evaluate our approach on a number of different MIL datasets: five MIL benchmark datasets (MUSK1, MUSK2, FOX, TIGER, ELEPHANT), an MNIST-based image dataset (MNIST-BAGS) and two reallife histopathology datasets (BREAST CANCER, COLON CANCER).",4. Experiments,[0],[0]
"We want to verify two research questions in the experiments: (i) whether our approach achieves the best performance or is comparable to the best performing method, (ii) if our method can provide interpretable results by using the attention weights that indicate key instances or ROIs.
",4. Experiments,[0],[0]
"In order to obtain a fair comparison we use a common evaluation methodology, i.e., 10-fold-cross-validation, and five repetitions per experiment.",4. Experiments,[0],[0]
In the case of MNIST-BAGS we use a fixed division into training and test set.,4. Experiments,[0],[0]
In order to create test bags we solely sampled images from the MNIST test set.,4. Experiments,[0],[0]
During training we only used images from the MNIST training set.,4. Experiments,[0],[0]
"For all experiments we use modified versions of models that have shown high classification performance on the individual datasets (Wang et al., 2016; LeCun et al., 1998; Sirinukunwattana et al., 2016).",4. Experiments,[0],[0]
The MIL pooling layers are either located before the last layer of the model (the embedded-based approach) or after last layer of the model (the instance-based approach).,4. Experiments,[0],[0]
If an attention-based MIL pooling layer is used the number of parameters in V was determined using a validation set.,4. Experiments,[0],[0]
"We tested the following dimensions (L): 64, 128 and 256.",4. Experiments,[0],[0]
The different dimensions only resulted in minor changes of the model’s performance.,4. Experiments,[0],[0]
For layers using the gated attention mechanism V and U have the same number of parameters.,4. Experiments,[0],[0]
"Finally, all layers were initialized according to Glorot & Bengio (2010) and biases were set to zero.
",4. Experiments,[0],[0]
We compare our approach to various MIL methods on MIL benchmark datasets.,4. Experiments,[0],[0]
On the image datasets our method is compared with instance-level and embedding-level neural networks and commonly used MIL pooling layers (max and mean).,4. Experiments,[0],[0]
"In the following, we are using ’Instance+max/mean’ and ’Embedding+max/mean’ to indicate networks that are build from convolutional layers and fully-connected layers.",4. Experiments,[0],[0]
"In contrast to networks purely build from fully-connected layers, referred to as ’mi-Net’ and ’MI-Net’ (Wang et al., 2016).
",4. Experiments,[0],[0]
"On MNIST-BAGS we include a SVM-based MIL model, called (MI-SVM).",4. Experiments,[0],[0]
We do not present results of MI-SVM on the histopathology datasets since we could not train (including hyperparameter search and five times 10-fold-crossvalidation procedure) the model in a reasonable amount of time.3,4. Experiments,[0],[0]
"In order to compare the bag level performance we use the following metrics: the classification accuracy, precision, recall, F-score, and the area under the receiver operating characteristic curve (AUC).",4. Experiments,[0],[0]
Details In the first experiment we aim at verifying whether our approach can compete with the best MIL methods on historically important benchmark datasets.,4.1. Classical MIL datasets,[0],[0]
"Since all five datasets contain precomputed features and only a small number of instances and bags, neural networks are most likely not well suited.",4.1. Classical MIL datasets,[0],[0]
First we predict drug activity (MUSK1 and MUSK2).,4.1. Classical MIL datasets,[0],[0]
A molecule has the desired drug effect if and only if one or more of its conformations bind to the target binding site.,4.1. Classical MIL datasets,[0],[0]
"Since molecules can adopt multiple shapes, a bag is made up of shapes belonging to the same molecule (Dietterich et al., 1997).",4.1. Classical MIL datasets,[0],[0]
"The three remaining datasets, ELEPHANT, FOX and TIGER, contain features extracted from images.",4.1. Classical MIL datasets,[0],[0]
Each bag consists of a set of segments of an image.,4.1. Classical MIL datasets,[0],[0]
"For each category, positive bags are images that contain the animal of interest, and negative bags are images that contain other animals (Andrews et al., 2003).",4.1. Classical MIL datasets,[0],[0]
"For detailed information on the number of bags, instances and features in each dataset see Section 6.3 in the Appendix.
",4.1. Classical MIL datasets,[0],[0]
"In our experiments we use the same architecture, optimizer and hyperparameters as in the MI-Net model (Wang et al., 2016).
Results and discussion The results of the experiment are 3Learning a single MI-SVM took approximately one week due to the large number of patches.
presented in Table 1.",4.1. Classical MIL datasets,[0],[0]
Our approaches (Attention and GatedAttention) are comparable with the best performing classical MIL methods (notice the standard error of the mean).,4.1. Classical MIL datasets,[0],[0]
Details The main disadvantage of the classical MIL benchmark datasets is that instances are represented by precomputed features.,4.2. MNIST-bags,[0],[0]
"In order to consider a more challenging scenario, we propose to investigate a dataset that is created using the well-known MNIST image dataset.",4.2. MNIST-bags,[0],[0]
A bag is made up of a random number of 28× 28 grayscale images taken from the MNIST dataset.,4.2. MNIST-bags,[0],[0]
The number of images in a bag is Gaussian-distributed and the closest integer value is taken.,4.2. MNIST-bags,[0],[0]
A bag is given a positive label if it contains one or more images with the label ’9’.,4.2. MNIST-bags,[0],[0]
We chose ’9’ since it can be easily mistaken with ’7’ or ’4’.,4.2. MNIST-bags,[0],[0]
We investigate the influence of the number of bags in the training set as well as the average number of instances per bag on the prediction performance.,4.2. MNIST-bags,[0],[0]
During evaluation we use a fixed number of 1000 test bags.,4.2. MNIST-bags,[0],[0]
"For all experiments a LeNet5 model is used (LeCun et al., 1998), see Table 8 and 9 in the Appendix.",4.2. MNIST-bags,[0],[0]
"The models are trained with the Adam optimization algorithm (Kingma & Ba, 2014).",4.2. MNIST-bags,[0],[0]
"We keep the default parameters for β1 and β2, see Table 10 in the Appendix.",4.2. MNIST-bags,[0],[0]
"In addition, we compare our method with a SVM-based MIL method (MI-SVM) (Andrews et al., 2003) that uses a Gaussian kernel on raw pixel features4.
",4.2. MNIST-bags,[0],[0]
"In the experiments we use different numbers of the mean bag size, namely, 10, 50 and 100, and the variance 2, 10, 20, respectively.",4.2. MNIST-bags,[0],[0]
"Moreover, we use varying numbers of training bags, i.e., 50, 100, 150, 200, 300, 400, 500.",4.2. MNIST-bags,[0],[0]
These different settings allow us to verify how different number of training bags and different number of instances influence MIL models.,4.2. MNIST-bags,[0],[0]
We compare instance-based and embedding-based approaches parameterized with a neural network (LeNet5) with mean and max MIL pooling.,4.2. MNIST-bags,[0],[0]
"We use AUC as the evaluation metric.
Results and discussion The results of AUC for the mean bag sizes equal to 10, 50 and 100 are presented in Figure 1, 2 and 3, respectively, and detailed results are given in the Appendix.",4.2. MNIST-bags,[0],[0]
"The findings of the experiment are the following: First, the proposed attention-based deep MIL approach performs much better than other methods in the small sample size regime.",4.2. MNIST-bags,[0],[0]
"Moreover, when there is a small effective size of the training set that corresponds to 50-150 bags for around 10 instances per bag (see Figure 1) or 50-100 bags in the case of on average 50 instances in a bag (see Figure 2), our method still achieves significantly higher AUC than all other methods.",4.2. MNIST-bags,[0],[0]
"Second, we notice that our approach is more flexible and obtained better results than the SVM-
4We use code provided with (Doran & Ray, 2014): https: //github.com/garydoranjr/misvm
based approach in all cases except large effective sample sizes (see Figure 3).",4.2. MNIST-bags,[0],[0]
"Third, the embedding-based models performed better than the instance-based models.",4.2. MNIST-bags,[0],[0]
"However, for a sufficient number of training images (number of training bags and training instances per bag) all models achieve very similar results.",4.2. MNIST-bags,[0],[0]
"Fourth, the mean operator performs significantly worse than the max operator.",4.2. MNIST-bags,[0],[0]
"However, the embedding-based model with the mean operator converged eventually to the best value but always later than the one with max.",4.2. MNIST-bags,[0],[0]
"See Section 6.4 in the Appendix for details.
",4.2. MNIST-bags,[0],[0]
The results of this experiment indicate that for a smallsample size regime our approach is preferable to others.,4.2. MNIST-bags,[0],[0]
"Since attention serves as a gradient update filter during backpropagation (Wang et al., 2017), instances with higher weights will contribute more to learning the encoder network of instances.",4.2. MNIST-bags,[0],[0]
This is especially important since medical imaging problems contain only a small number of cases.,4.2. MNIST-bags,[0],[0]
"In general, the more instances are in a bag the easier the MIL task becomes, since the MIL assumption states that every instance in a negative bag is negative.",4.2. MNIST-bags,[0],[0]
"For example, a negative bag of size 100 from the MNIST-bags dataset will include about 11 negative examples per class.
",4.2. MNIST-bags,[0],[0]
"Finally, we present an exemplary result of the attention mechanism in Figure 4.",4.2. MNIST-bags,[0],[0]
In this example a bag consists of 13 images.,4.2. MNIST-bags,[0],[0]
For each digit the corresponding attention weight is given by the trained network.,4.2. MNIST-bags,[0],[0]
The bag is properly predicted as positive and all nines are correctly highlighted.,4.2. MNIST-bags,[0],[0]
"Hence, the attention mechanism works as expected.",4.2. MNIST-bags,[0],[0]
More examples are given in the Appendix.,4.2. MNIST-bags,[0],[0]
Details An automatic detection of cancerous regions in hematoxylin and eosin (H&E) stained whole-slide images is a task with high clinical relevance.,4.3. Histopathology datasets,[0],[0]
"Current supervised approaches utilize pixel-level annotations (Litjens et al., 2017).",4.3. Histopathology datasets,[0],[0]
"However, data preparation requires large amount of time from pathologists which highly interferes with their daily routines.",4.3. Histopathology datasets,[0],[0]
"Hence, a successful solution working with weak labels would hold a great promise to reduce the workload of the pathologists.",4.3. Histopathology datasets,[0],[0]
"In the following, we perform two experiments on classifying weakly-labeled real-life histopathol-
ogy images of the breast cancer dataset (BREAST CANCER) (Gelasca et al., 2008) and the colon cancer dataset (COLON CANCER) (Sirinukunwattana et al., 2016).
",4.3. Histopathology datasets,[0],[0]
BREAST CANCER consists of 58 weakly labeled 896× 768 H&E images.,4.3. Histopathology datasets,[0],[0]
"An image is labeled malignant if it contains breast cancer cells, otherwise it is benign.",4.3. Histopathology datasets,[0],[0]
We divide every image into 32 × 32 patches.,4.3. Histopathology datasets,[0],[0]
This results in 672 patches per bag.,4.3. Histopathology datasets,[0],[0]
"A patch is discarded if it contains 75% or more of white pixels.
",4.3. Histopathology datasets,[0],[0]
COLON CANCER comprises 100 H&E images.,4.3. Histopathology datasets,[0],[0]
The images originate from a variety of tissue appearance from both normal and malignant regions.,4.3. Histopathology datasets,[0],[0]
For every image the majority of nuclei of each cell were marked.,4.3. Histopathology datasets,[0],[0]
"In total there are 22,444 nuclei with associated class label, i.e. epithelial, inflammatory, fibroblast, and miscellaneous.",4.3. Histopathology datasets,[0],[0]
A bag is composed of 27×27 patches.,4.3. Histopathology datasets,[0],[0]
"Furthermore, a bag is given a positive label if it contains one or more nuclei from the epithelial class.",4.3. Histopathology datasets,[0],[0]
"Tagging epithelial cells is highly relevant from a clinical point of view, since colon cancer originates from epithelial cells (Ricci-Vitiani et al., 2007).
",4.3. Histopathology datasets,[0],[0]
"For both datasets we use the model proposed in (Sirinukunwattana et al., 2016) for the transformation f .",4.3. Histopathology datasets,[0],[0]
"All models are trained with the Adam optimization algorithm (Kingma & Ba, 2014).",4.3. Histopathology datasets,[0],[0]
Due to the limited amount of data samples in both datasets we performed data augmentation to prevent overfitting.,4.3. Histopathology datasets,[0],[0]
"See the Appendix for further details.
Results and discussion We present results in Table 2 and 3 for BREAST CANCER and COLON CANCER, respectively.",4.3. Histopathology datasets,[0],[0]
"First, we notice that the obtained results confirm our findings in MNIST-BAGS experiment that our approach outperforms all other methods.",4.3. Histopathology datasets,[0],[0]
A trend that is especially visible in the small-sample size regime of the MNIST-BAGS.,4.3. Histopathology datasets,[0],[0]
"Surprisingly, the embedding-based method with the max pooling failed almost completely on BREAST CANCER but in general this dataset is difficult due to high variability of slides and small number of cases.",4.3. Histopathology datasets,[0],[0]
The proposed method is not only most accurate but it also received the highest recall.,4.3. Histopathology datasets,[0],[0]
High recall is especially important in the medical domain since false negatives could lead to severe consequences including patient fatality.,4.3. Histopathology datasets,[0],[0]
"We also notice that the gated-attention mechanism performs better than the plain attention mechanism on BREAST CANCER while these two behave similarly on COLON CANCER.
",4.3. Histopathology datasets,[0],[0]
"Eventually, we present the usefulness of the attention mechanism in providing ROIs.",4.3. Histopathology datasets,[0],[0]
In Figure 5 we show a histopathology image divided into patches containing (mostly) single cells.,4.3. Histopathology datasets,[0],[0]
We create a heatmap by multiplying patches by its corresponding attention weight.,4.3. Histopathology datasets,[0],[0]
"Although only image-level annotations are used during training, there is a substantial matching between the heatmap in Figure 5(d) and the ground truth in Figure 5(c).",4.3. Histopathology datasets,[0],[0]
"Additionally, we notice that
the instance-based classifier tends to select only a small subset of positive patches (see Figure 10(e) in Appendix) that confirms low instance accuracy of the instance-based approach discussed in (Kandemir & Hamprecht, 2015).",4.3. Histopathology datasets,[0],[0]
"For
more examples please see the Appendix.
",4.3. Histopathology datasets,[0],[0]
The obtained results again confirm that the proposed approach attains high predictive performance and allows to properly highlight ROIs.,4.3. Histopathology datasets,[0],[0]
"Moreover, the attention weights can be used to create a reliable heatmap.",4.3. Histopathology datasets,[0],[0]
"In this paper, we proposed a flexible and interpretable MIL approach that is fully parameterized by neural networks.",5. Conclusion,[0],[0]
We outlined the usefulness of deep learning for modeling a permutation-invariant bag score function in terms of the Fundamental Theorem of Symmetric Functions.,5. Conclusion,[0],[0]
"Moreover, we presented a trainable MIL pooling based on the (gated) attention mechanism.",5. Conclusion,[0],[0]
"We showed empirically on five MIL datasets, one image corpora and two real-life histopathology datasets that our method is on a par with the best performing methods or performs the best in terms of different evaluation metrics.",5. Conclusion,[0],[0]
"Additionally, we showed that our approach provides an interpretation of the decision by presenting ROIs, which is extremely important in many practical applications.
",5. Conclusion,[0],[0]
We strongly believe that the presented line of research is worth pursuing further.,5. Conclusion,[0],[0]
"Here we focused on a binary MIL problem, however, the multi-class MIL is more interesting and challenging (Feng & Zhou, 2017).",5. Conclusion,[0],[0]
"Moreover, in some applications it is worth to consider repulsion points (Scott et al., 2005), i.e., instances for which a bag is always negative, or assume dependencies among instances within a bag (Zhou et al., 2009).",5. Conclusion,[0],[0]
We leave investigating these issues for future research.,5. Conclusion,[0],[0]
"The authors are very grateful to Rianne van den Berg for insightful remarks and discussions.
",Acknowledgements,[0],[0]
Maximilian Ilse was funded by the Nederlandse Organisatie voor Wetenschappelijk Onderzoek (Grant DLMedIa:,Acknowledgements,[0],[0]
"Deep Learning for Medical Image Analysis).
",Acknowledgements,[0],[0]
"Jakub Tomczak was funded by the European Commission within the Marie Skodowska-Curie Individual Fellowship (Grant No. 702666, ”Deep learning and Bayesian inference for medical imaging”).",Acknowledgements,[0],[0]
Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances.,abstractText,[0],[0]
"In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks.",abstractText,[0],[0]
"Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism.",abstractText,[0],[0]
"Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label.",abstractText,[0],[0]
We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.,abstractText,[0],[0]
Attention-based Deep Multiple Instance Learning,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 247–256, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Most of the sentiment analysis research focuses on sentiment classification which aims to determine whether the users attitude is positive, neutral or negative.",1 Introduction,[0],[0]
"There are two classes of mainstreaming sentiment classification algorithms: unsupervised methods which usually require a sentiment lexicon
(Taboada et al., 2011) and supervised methods (Pang et al., 2002) which require manually labeled data.",1 Introduction,[0],[0]
"However, both of these sentiment resources are unbalanced in different languages.",1 Introduction,[0],[0]
The sentiment lexicon or labeled data are rich in several languages such as English and are poor in others.,1 Introduction,[0],[0]
Manually building these resources for all the languages will be expensive and time-consuming.,1 Introduction,[0],[0]
Cross-lingual sentiment classification tackles the problem by trying to adapt the resources in one language to other languages.,1 Introduction,[0],[0]
"It can also be regarded as a special kind of cross-lingual text classification task.
",1 Introduction,[0],[0]
"Recently, there have been several bilingual representation learning methods such as (Hermann and Blunsom, 2014; Gouws et al., 2014) for cross-lingual sentiment or text classification which achieve promising results.",1 Introduction,[0],[0]
They try to learn a joint embedding space for different languages such that the training data in the source language can be directly applied to the test data in the target language.,1 Introduction,[0],[0]
"However, most of the studies only use simple functions, e.g. arithmetic average, to synthesize representations for larger text sequences.",1 Introduction,[0],[0]
"Some of them use more complicated compositional models such as the bi-gram non-linearity model in (Hermann and Blunsom, 2014) which also fail to capture the long distance dependencies in texts.
",1 Introduction,[0],[0]
"In this study, we propose an attention-based bilingual LSTM network for cross-lingual sentiment classification.",1 Introduction,[0],[0]
LSTMs have been proved to be very effective to model word sequences and are powerful to learn on data with long range temporal dependencies.,1 Introduction,[0],[0]
"After translating the training data into the target language using machine translation
247
tools, we use the bidirectional LSTM network to model the documents in both of the source and the target languages.",1 Introduction,[0],[0]
"The LSTMs show strong ability to capture the compositional semantics for the bilingual texts in our experiments.
",1 Introduction,[0],[0]
"For the traditional LSTM network, each word in the input document is treated with equal importance, which is reasonable for traditional text classification tasks.",1 Introduction,[0],[0]
"In this paper, we propose a hierarchical attention mechanism which enables our model to focus on certain part of the input document.",1 Introduction,[0],[0]
The motivation mainly comes from the following three observations: 1) the machine translation tool that we use to translate the documents will always introduce much noise for sentiment classification.,1 Introduction,[0],[0]
We hope that the attention mechanism can help to filter out these noises.,1 Introduction,[0],[0]
2),1 Introduction,[0],[0]
"In each individual language, the sentiment of a document is usually decided by a relative small part of it.",1 Introduction,[0],[0]
"In a long review document, the user might discuss both the advantages and disadvantages of a product.",1 Introduction,[0],[0]
The sentiment will be confusing if we consider each sentence of the same contribution.,1 Introduction,[0],[0]
"For example, in the first review of Table 1, the first sentence reveals a negative sentiment towards the movie but the second one reveals a positive sentiment.",1 Introduction,[0],[0]
"As human readers, we can understand that the review is expressing a positive overall sentiment but it is hard for the sequence modeling algorithms including LSTM to capture.",1 Introduction,[0],[0]
3),1 Introduction,[0],[0]
"At the sentence level, it is important to focus on the sentiment signals such as the sentiment words.",1 Introduction,[0],[0]
"They are usually very decisive to determine the polarity even for a very long sentence, e.g. “easy” and “nice” in the second example of Table 1.
",1 Introduction,[0],[0]
"In sum, the main contributions of this study are summarized as follows:
1) We propose a bilingual LSTM network for
cross-lingual sentiment classification.",1 Introduction,[0],[0]
"Compared to the previous methods which only use weighted or arithmetic average of word embeddings to represent the document, LSTMs have obvious advantage to model the compositional semantics and to capture the long distance dependencies between words for bilingual texts.
",1 Introduction,[0],[0]
2),1 Introduction,[0],[0]
We propose a hierarchical bilingual attention mechanism for our model.,1 Introduction,[0],[0]
"To the best of our knowledge, this is the first attention-based model designed for cross-lingual sentiment analysis.
",1 Introduction,[0],[0]
3),1 Introduction,[0],[0]
The proposed framework achieves good results on a benchmark dataset from a cross-language sentiment classification evaluation.,1 Introduction,[0],[0]
It outperforms the best team in the evaluation as well as several strong baseline methods.,1 Introduction,[0],[0]
"Sentiment analysis is the field of studying and analyzing peoples opinions, sentiments, evaluations, appraisals, attitudes, and emotions (Liu, 2012).",2 Related Work,[0],[0]
The most common task of sentiment analysis is polarity classification which arises with the emergence of customer reviews on the Internet.,2 Related Work,[0],[0]
Pang et al. (2002) used supervised learning methods and achieved promising results with simple unigram and bi-gram features.,2 Related Work,[0],[0]
"In subsequent research, more features and learning algorithms were tried for sentiment classification by a large number of researchers.",2 Related Work,[0],[0]
"Recently, the emerging of deep learning has also shed light on this area.",2 Related Work,[0],[0]
"Lots of representation learning methods has been proposed to address the sentiment classification task and many of them achieve the state-of-the-art performance on several benchmark datasets, such as the recursive neural tensor network (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), multi-channel convolutional neural networks (Kim, 2012), dynamic convolutional neural network (Blunsom et al., 2014) and tree structure LSTM (Tai et al., 2015).",2 Related Work,[0],[0]
"Very recently, Yang et al. (2016) proposed a similar hierarchical attention network based on GRU in the monolingual setting.",2 Related Work,[0],[0]
"Note that our work is independent with theirs and their study was released online after we submitted this study.
",2 Related Work,[0],[0]
"Cross-lingual sentiment classification is also a popular research topic in the sentiment analysis
community which aims to solve the sentiment classification task from a cross-language view.",2 Related Work,[0],[0]
It is of great importance since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language.,2 Related Work,[0],[0]
Cross-lingual sentiment classification has been extensively studied in the very recent years.,2 Related Work,[0],[0]
Mihalcea et al. (2007) translated English subjectivity words and phrases into the target language to build a lexicon-based classifier.,2 Related Work,[0],[0]
Wan (2009) translated both the training data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages.,2 Related Work,[0],[0]
"Chen et al. (2015) proposed a knowledge validation method and incorporated it into a boosting model to transfer credible information between the two languages during training.
",2 Related Work,[0],[0]
There have also been several studies addressing the task via multi-lingual text representation learning.,2 Related Work,[0],[0]
Xiao and Guo (2013) learned different representations for words in different languages.,2 Related Work,[0],[0]
Part of the word vector is shared among different languages and the rest is language-dependent.,2 Related Work,[0],[0]
"Klementiev et al. (2012) treated the task as a multi-task learning problem where each task corresponds to a single word, and the task relatedness is derived from cooccurrence statistics in bilingual parallel corpora.",2 Related Work,[0],[0]
Chandar A P et al. (2014) and Zhou et al. (2015) used the autoencoders to model the connections between bilingual sentences.,2 Related Work,[0],[0]
It aims to minimize the reconstruction error between the bag-of-words representations of two parallel sentences.,2 Related Work,[0],[0]
Pham et al. (2015) extended the paragraph model into bilingual setting.,2 Related Work,[0],[0]
"Each pair of parallel sentences shares the same paragraph vector.
",2 Related Work,[0],[0]
"Compared to the existing studies, we propose to use the bilingual LSTM network to learn the document representations of reviews in each individual language.",2 Related Work,[0],[0]
It has obvious advantage to model the compositional semantics and to capture the long distance dependencies between words.,2 Related Work,[0],[0]
"Besides, we propose a hierarchical neural attention mechanism to capture the sentiment attention in each document.",2 Related Work,[0],[0]
The attention model helps to filter out the noise which is irrelevant to the overall sentiment.,2 Related Work,[0],[0]
Cross-language sentiment classification aims to use the training data in the source language to build a model which is adaptable for the test data in the target language.,3.1 Problem Definition,[0],[0]
"In our setting, we have labeled training data in English LEN = {xi, yi}Ni=1 , where xi is the review text and yi is the sentiment label vector.",3.1 Problem Definition,[0],[0]
"yi = (1, 0) represents the positive sentiment and yi = (0, 1) represents the negative sentiment.",3.1 Problem Definition,[0],[0]
"In the target language Chinese, we have the test data TCN = {xi}Ti=1 and unlabeled data UCN = {xi}Mi=1.",3.1 Problem Definition,[0],[0]
"The task is to use LEN and UCN to learn a model and classify the sentiment polarity for the review texts in TCN .
",3.1 Problem Definition,[0],[0]
"In our method, the labeled, unlabeled and test data are all translated into the other language using an online machine translation tool.",3.1 Problem Definition,[0],[0]
"In the subsequent part of the paper, we refer to a document and its corresponding translation in the other language as a pair of parallel documents.",3.1 Problem Definition,[0],[0]
"Recurrent neural network (RNN) (Rumelhart et al., 1988) is a special kind of feed-forward neural network which is useful for modeling time-sensitive sequences.",3.2 RNN and LSTM,[0],[0]
"At each time t, the model receives input from the current example and also from the hidden layer of the network’s previous state.",3.2 RNN and LSTM,[0],[0]
The output is calculated given the hidden state at that time stamp.,3.2 RNN and LSTM,[0],[0]
The recurrent connection makes the output at each time associated with all the previous inputs.,3.2 RNN and LSTM,[0],[0]
The vanilla RNN model has been considered to be difficult to train due to the well-known problem of vanishing and exploding gradients.,3.2 RNN and LSTM,[0],[0]
"The LSTM (Hochreiter and Schmidhuber, 1997) addresses the problem by re-parameterizing the RNN model.",3.2 RNN and LSTM,[0],[0]
The core idea of LSTM is introducing the “gates” to control the data flow in the recurrent neural unit.,3.2 RNN and LSTM,[0],[0]
The LSTM structure ensures that the gradient of the long-term dependencies cannot vanish.,3.2 RNN and LSTM,[0],[0]
The detailed architecture that we use in shown in Figure 1.,3.2 RNN and LSTM,[0],[0]
"In this study, we try to model the bilingual texts through the attention based LSTM network.",4 Framework,[0],[0]
"We first
describe the general architecture of the model and then describe the attention mechanism used in it.",4 Framework,[0],[0]
The general architecture of our approach is shown in Figure 2.,4.1 Architecture,[0],[0]
"For a pair of parallel documents xcn and xen, each of them is sent into the attention based
LSTM network.",4.1 Architecture,[0],[0]
The English-side and Chineseside architectures are the same but have different parameters.,4.1 Architecture,[0],[0]
We only show the Chinese-side network in the figure due to space limit.,4.1 Architecture,[0],[0]
The whole model is divided into four layers.,4.1 Architecture,[0],[0]
"In the input layer, the documents are represented as a word sequence where each position corresponds to a word vector from pre-trained word embeddings.",4.1 Architecture,[0],[0]
"In the LSTM layer, we get the high-level representation from a bidirectional LSTM network.",4.1 Architecture,[0],[0]
We use the hidden units from both the forward and backward LSTMs.,4.1 Architecture,[0],[0]
"In the document representation layer, we incorporate the attention model into the network and derive the final document representation.",4.1 Architecture,[0],[0]
"At the output layer, we concatenate the representations of the English and Chinese documents and use the softmax function to predict the sentiment label.
",4.1 Architecture,[0],[0]
Input Layer:,4.1 Architecture,[0],[0]
The input layer of the network is the word sequences in a document x which can be either Chinese or English.,4.1 Architecture,[0],[0]
"The document x contains several sentences {si}|x|i=1 and each sentence is composed of several words si = {wi,j}|si|j=1 .",4.1 Architecture,[0],[0]
"We represent each word in the document as a fixed-size vector from pre-trained word embeddings.
",4.1 Architecture,[0],[0]
LSTM Layer:,4.1 Architecture,[0],[0]
"In each individual language, we use bi-directional LSTMs to model the input sequences.",4.1 Architecture,[0],[0]
"In the bidirectional architecture, there are two layers of hidden nodes from two separate LSTMs.",4.1 Architecture,[0],[0]
The two LSTMs capture the dependencies in different directions.,4.1 Architecture,[0],[0]
"The first hidden layers have recurrent connections from the past words while second one’s direction of recurrent of connections is flipped, passing activation backwards in the texts.",4.1 Architecture,[0],[0]
"Therefore, in the LSTM layer, we can get the forward hidden state ~hi,j from the forward LSTM network and the backward hidden state ~hi,j from the backward LSTM network.",4.1 Architecture,[0],[0]
"We represent the final state at position (i, j), i.e. the j-th word in the i-th sentence of the document, with the concatenation of ~hi,j and ~hi,j .
hi,j = ~hi,j ‖ ~hi,j
It captures the compositional semantics in both directions of the word sequences.
",4.1 Architecture,[0],[0]
"Document Representation Layer:As described above, different parts of the document usually have different importance for the overall sentiment.",4.1 Architecture,[0],[0]
"Some
sentences or words can be decisive while the others are irrelevant.",4.1 Architecture,[0],[0]
"In this study, we use a hierarchical attention mechanism which assigns a real value score for each word and a real value score for each sentence.",4.1 Architecture,[0],[0]
"The detailed strategy of our attention model will be described in the next subsection.
",4.1 Architecture,[0],[0]
Suppose we have the sentence attention score Ai for each sentence,4.1 Architecture,[0],[0]
"si ∈ x, and the word attention score ai,j for each word wi,j ∈ si, both of the scores are normalized which satisfy the following equations,
∑
i
Ai = 1 and ∑
j
ai,j = 1
The sentence attention measures which sentence is more important for the overall sentiment while the word attention captures sentiment signals such as sentiment words in each sentence.",4.1 Architecture,[0],[0]
"Therefore, the document representation r for document x is calculated as follows,
r = ∑
i
[Ai · ∑
j
(ai,j · hi,j)]
Note that many LSTM based models represent the word sequences only using the hidden layer at the final node.",4.1 Architecture,[0],[0]
"In this study, the hidden states at all the positions are considered with different attention weights.",4.1 Architecture,[0],[0]
"We believe that, for document sentiment classification, focusing on some certain parts of the document will be effective to filter out the sentimentirrelevant noise.
",4.1 Architecture,[0],[0]
Output Layer:,4.1 Architecture,[0],[0]
"At the output layer, we need to predict the overall sentiment of the document.",4.1 Architecture,[0],[0]
"For each English document xen and its corresponding translation xcn, suppose the document representations of them are obtained in previous steps as ren and rcn, we simply concatenate them as the feature vector and use the softmax function to predict the final sentiment.
",4.1 Architecture,[0],[0]
ŷ = softmax(rcn ‖ ren),4.1 Architecture,[0],[0]
"For document-level sentiment classification task, we have shown that capturing both the sentence and word level attention is important.",4.2 Hierarchical Attention Mechanism,[0],[0]
"The general idea is inspired by previous works such as Bahdanau et
al. (2014) and Hermann et al. (2015) which have successfully applied the attention model to machine translation and question answering.",4.2 Hierarchical Attention Mechanism,[0],[0]
Bahdanau et al. (2014) incorporated the attention model into the sequence to sequence learning framework.,4.2 Hierarchical Attention Mechanism,[0],[0]
"During the decoding phase of the machine translation task, the attention model helps to find which input word should be “aligned” to the current output.",4.2 Hierarchical Attention Mechanism,[0],[0]
"In our case, the output of the model is not a sequence but only one sentiment vector.",4.2 Hierarchical Attention Mechanism,[0],[0]
"We hope to find the important units in the input sequence which are influential for the output.
",4.2 Hierarchical Attention Mechanism,[0],[0]
We propose to learn a hierarchical attention model jointly with the bilingual LSTM network.,4.2 Hierarchical Attention Mechanism,[0],[0]
The first level is the sentence attention model which measures which sentences are more important for the overall sentiment of a document.,4.2 Hierarchical Attention Mechanism,[0],[0]
"For each sentence si = {wi,j}|si|j=1 in the document, we represent the sentence via the final hidden state of the forward LSTM and the backward LSTM, i.e.
si = ~hi,|si| ‖ ~hi,1
",4.2 Hierarchical Attention Mechanism,[0],[0]
"We use a two-layer feed-forward neural network to predict the attention score of si
Âi = f(si; θs)
Ai = exp(Âi)∑ j exp(Âj)
where f denotes the two-layer feed-forward neural network and θs denotes the parameters in it.
",4.2 Hierarchical Attention Mechanism,[0],[0]
"At the word level, we represent each word wi,j using its word embedding and the hidden state of the bidirectional LSTM layer, i.e. hi,j .",4.2 Hierarchical Attention Mechanism,[0],[0]
"Similarly, we use a two-layer feed forward neural network to predict the attention score of wi,j ,
ei,j = wi,j ‖ ~hi,j ‖ ~hi,j
âi,j = f(ei,j ; θw)
ai,j = exp(âi,j)∑ j exp(âi,j)
where θw denotes the parameters for predicting word attention.",4.2 Hierarchical Attention Mechanism,[0],[0]
The proposed model is trained in a semi-supervised manner.,4.3 Training of the Proposed Model,[0],[0]
"In the supervised part, we use the cross entropy loss to minimize the sentiment prediction error between the output results and the gold standard labels,
L1 = ∑
(xen,xcn)
∑
i
−yi log(ŷi)
where xen and xcn are a pair of parallel documents in the training data, y is the gold-standard sentiment vector and ŷ is the predicted vector from our model.
",4.3 Training of the Proposed Model,[0],[0]
The unsupervised part tries to minimize the document representations between the parallel data.,4.3 Training of the Proposed Model,[0],[0]
"Following previous research, we simply measure the distance of two parallel documents via the Euclidean Distance,
L2 = ∑
(xen,xcn)
‖ren",4.3 Training of the Proposed Model,[0],[0]
"− rcn‖2
where xen and xcn are a pair of parallel documents from both the labeled and unlabeled data.
",4.3 Training of the Proposed Model,[0],[0]
"The final objective function is a weighted sum of L1 and L2,
L = L1 + α · L2 where α is the hyper-parameter controlling the weight.",4.3 Training of the Proposed Model,[0],[0]
"We use Adadelta (Zeiler, 2012) to update the parameters during training.",4.3 Training of the Proposed Model,[0],[0]
"It can dynamically adapt over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent.
",4.3 Training of the Proposed Model,[0],[0]
"In the test phase, the test document in TCN is sent into our model along with the corresponding machine translated text in TEN .",4.3 Training of the Proposed Model,[0],[0]
The final sentiment is predicted via a softmax function over the concatenated representation of the bilingual texts as described above.,4.3 Training of the Proposed Model,[0],[0]
"We use the dataset from the cross-language sentiment classification evaluation of NLP&CC 2013.1
1The dataset can be found at http://tcci.ccf.org.cn/conference/2013/index.html.",5.1 Dataset,[0],[0]
"NLP&CC is an annual conference specialized in the fields of Natural
The dataset contains reviews in three domains including book, DVD and music.",5.1 Dataset,[0],[0]
"In each domain, it has 2000 positive reviews and 2000 negative reviews in English for training and 4000 Chinese reviews for test.",5.1 Dataset,[0],[0]
"It also contains 44113, 17815 and 29678 unlabeled reviews for book, DVD and music respectively.",5.1 Dataset,[0],[0]
We use Google Translate2 to translate the labeled data to Chinese and translate the unlabeled data and test data to English.,5.2 Implementation Detail,[0],[0]
"All the texts are tokenized and converted into lower case.
",5.2 Implementation Detail,[0],[0]
"In the proposed framework, the dimensions of the word vectors and the hidden layers of LSTMs are set as 50.",5.2 Implementation Detail,[0],[0]
The initial word embeddings are trained on both the unlabeled and labeled reviews using word2vec in each individual language.,5.2 Implementation Detail,[0],[0]
The word vectors are fine-tuned during the training procedure.,5.2 Implementation Detail,[0],[0]
The hyper-parameter a is set to 0.2.,5.2 Implementation Detail,[0],[0]
The dropout rate is set to 0.5 to prevent overfitting.,5.2 Implementation Detail,[0],[0]
Ten percent of the training data are randomly selected as validation set.,5.2 Implementation Detail,[0],[0]
The training procedure is stopped when the prediction accuracy does not improve for 10 iterations.,5.2 Implementation Detail,[0],[0]
"We implement the framework based on theano (Bastien et al., 2012) and use a GTX 980TI graphic card for training.",5.2 Implementation Detail,[0],[0]
"To evaluate the performance of our model, we compared it with the following baseline methods:
LR and SVM:",5.3 Baselines and Results,[0],[0]
We use logistic regression and SVM to learn different classifiers based on the translated Chinese training data.,5.3 Baselines and Results,[0],[0]
"We simply use unigram features.
",5.3 Baselines and Results,[0],[0]
"MT-PV: Paragraph vector (Le and Mikolov, 2014) is considered as one of the state-of-the-art monolingual document modeling methods.",5.3 Baselines and Results,[0],[0]
We translate all the training data into Chinese and use paragraph vector to learn a vector representation for the training and test data.,5.3 Baselines and Results,[0],[0]
"A logistic regression classifier is used to predict the sentiment polarity.
",5.3 Baselines and Results,[0],[0]
Bi-PV: Pham et al. (2015) is one the state-ofthe-art bilingual document modeling methods.,5.3 Baselines and Results,[0],[0]
"It extends the paragraph vector into bilingual setting.
",5.3 Baselines and Results,[0],[0]
"Language Processing (NLP) and Chinese Computing (CC) organized by Chinese Computer Federation (CCF).
",5.3 Baselines and Results,[0],[0]
"2http://translate.google.com/
Each pair of parallel sentences in the training data shares the same vector representation.
",5.3 Baselines and Results,[0],[0]
BSWE:,5.3 Baselines and Results,[0],[0]
Zhou et al. (2015) proposed the bilingual sentiment word embedding algorithm based on denoising autoencoders.,5.3 Baselines and Results,[0],[0]
It learns the vector representations for 2000 sentiment words.,5.3 Baselines and Results,[0],[0]
"Each document is then represented by the sentiment words and the corresponding negation words in it.
",5.3 Baselines and Results,[0],[0]
H-Eval: Gui et al. (2013) got the highest performance in the NLP&CC 2013 cross-lingual sentiment classification evaluation.,5.3 Baselines and Results,[0],[0]
"It uses a mixed CLSC model by combining co-training and transfer learning strategies.
",5.3 Baselines and Results,[0],[0]
A-Eval:,5.3 Baselines and Results,[0],[0]
"This is the average performance of all the teams in the NLP&CC 2013 cross-lingual sentiment classification evaluation.
",5.3 Baselines and Results,[0],[0]
"The attention-based models EN-Attention, CNAttention and BI-Attention: Bi-Attention is the model described in the above sections which concatenate the document representations of the English side and the Chinese side texts.",5.3 Baselines and Results,[0],[0]
"EN-Attention only translates the Chinese test data into English and uses English-side attention model while CN-Attention only uses the Chinese side attention model.
",5.3 Baselines and Results,[0],[0]
Table 2 shows the cross-lingual sentiment classification accuracy of all the approaches.,5.3 Baselines and Results,[0],[0]
The first kind baseline algorithms are based on traditional bag-of-word features.,5.3 Baselines and Results,[0],[0]
SVM performs better than LR on book and DVD but gets much worse result on music.,5.3 Baselines and Results,[0],[0]
"The second kind baseline algorithms are based on deep learning methods which learn the vector representations for words or documents.
",5.3 Baselines and Results,[0],[0]
MT-PV achieves similar results with LR.,5.3 Baselines and Results,[0],[0]
Bi-PV improves the accuracy by about 0.03 using both the bilingual documents.,5.3 Baselines and Results,[0],[0]
"While MT-PV and BiPV directly learn document representations, BSWE learns the embedding for the words in a bilingual sentiment lexicon.",5.3 Baselines and Results,[0],[0]
"It gets higher accuracy than both Bi-PV and MT-PV which shows that the sentiment words are very important for this task.
",5.3 Baselines and Results,[0],[0]
Our attention based models achieve the highest prediction accuracy among all the approaches.,5.3 Baselines and Results,[0],[0]
The results show that CN-Attention always outperforms EN-Attention.,5.3 Baselines and Results,[0],[0]
The combination of the English-side and Chinese-side model brings improvement to both the book and music domains and yields the highest average prediction accuracy.,5.3 Baselines and Results,[0],[0]
The attention-based models outperform the algorithms using traditional features as well as the existing deep learning based methods.,5.3 Baselines and Results,[0],[0]
"Compared to the highest performance in the NLP&CC evaluation, we improve the average accuracy by about 0.05.",5.3 Baselines and Results,[0],[0]
"In this study, we propose a hierarchical attention mechanism to capture the sentiment-related information of each document.",5.4 Influence of the Attention Mechanism,[0],[0]
"In table 3, we show the results of models with different attention mechanisms.",5.4 Influence of the Attention Mechanism,[0],[0]
All the models are based on the bilingual bi-directional LSTM network as shown in Figure 2.,5.4 Influence of the Attention Mechanism,[0],[0]
LSTM is the basic bilingual bi-directional LSTM network.,5.4 Influence of the Attention Mechanism,[0],[0]
LSTM+SA considers only sentence-level attention while LSTM+WA considers only wordlevel attention.,5.4 Influence of the Attention Mechanism,[0],[0]
LSTM+HA combines both wordlevel and sentence-level attentions.,5.4 Influence of the Attention Mechanism,[0],[0]
"From the results, we can observe that LSTM+HA outperforms the other three methods, which proves the effectiveness of the hierarchical attention mechanism.",5.4 Influence of the Attention Mechanism,[0],[0]
"Besides, the word-level attention shows better performance than the sentence-level attention.
",5.4 Influence of the Attention Mechanism,[0],[0]
We also conduct a case study using the examples in Table 1.,5.4 Influence of the Attention Mechanism,[0],[0]
"We show the visualized word attention
using a heat map in Figure 3 by drawing the attention of each word in it.",5.4 Influence of the Attention Mechanism,[0],[0]
The darker color reveals higher attention scores while the lighter part has little importance.,5.4 Influence of the Attention Mechanism,[0],[0]
We can observe that our model successfully identifies the important units of the sentence.,5.4 Influence of the Attention Mechanism,[0],[0]
The sentiment word “easy” gets much higher attention score than the other words.,5.4 Influence of the Attention Mechanism,[0],[0]
The word “nice” gets the third highest score in the sentence right after the two “easy”.,5.4 Influence of the Attention Mechanism,[0],[0]
Note that our attention mechanism considers both the word embedding vector and the hidden state vectors.,5.4 Influence of the Attention Mechanism,[0],[0]
"Therefore, the same word “easy” gets different scores in different positions.",5.4 Influence of the Attention Mechanism,[0],[0]
"For the deep learning based methods, the initial word embeddings used as the inputs for the network usually play an important role.",5.5 Influence of the Word Embeddings,[0],[0]
"We study four different settings called rand, static, fine-tuned and multi-channel, respectively.",5.5 Influence of the Word Embeddings,[0],[0]
"In rand setting, the word embeddings are randomly initialized.",5.5 Influence of the Word Embeddings,[0],[0]
The static setting keeps initial embedding fixed while the fine-tuned setting learns a refined embedding during the training procedure.,5.5 Influence of the Word Embeddings,[0],[0]
Multi-channel is the combination of static and fine-tuned.,5.5 Influence of the Word Embeddings,[0],[0]
Two same word vectors are concatenated to represent each word.,5.5 Influence of the Word Embeddings,[0],[0]
"During the training procedure, half of it is fine-tuned while the rest is fixed.",5.5 Influence of the Word Embeddings,[0],[0]
"Note that finetuned is the embedding setting that we use in our model.
",5.5 Influence of the Word Embeddings,[0],[0]
Table 4 shows the performance of our model in these settings.,5.5 Influence of the Word Embeddings,[0],[0]
"Rand gets the lowest accuracy among
them.",5.5 Influence of the Word Embeddings,[0],[0]
"The fine-tuned word embeddings perform better than static which fits the results in previous study (Kim, 2012).",5.5 Influence of the Word Embeddings,[0],[0]
Multi-channel gets similar results with fine-tuned on DVD and music but is a bit lower on book.,5.5 Influence of the Word Embeddings,[0],[0]
We also find that using pre-trained word embeddings helps the model to converge much faster than random initialization.,5.5 Influence of the Word Embeddings,[0],[0]
"In our experiment, we set the size of the hidden layers in both the forward and backward LSTMs the same as the size of the input word vectors.",5.6 Influence of Vector Sizes,[0],[0]
"Therefore, the dimension of the document representation is twice of the word vector size.",5.6 Influence of Vector Sizes,[0],[0]
"In Figure 4, we show the performance of our model with different input vector sizes.",5.6 Influence of Vector Sizes,[0],[0]
"We use the vector size in the following set {10, 25, 50, 100, 150, 200}.",5.6 Influence of Vector Sizes,[0],[0]
"Note that the dimensions of all the units in the model also change with that.
",5.6 Influence of Vector Sizes,[0],[0]
We can observe from Figure 4 that the prediction accuracy for the book domain keeps steady when the vector size changes.,5.6 Influence of Vector Sizes,[0],[0]
"For DVD and music, the performance increases at the beginning and becomes stable after the vector size grows larger than 50.",5.6 Influence of Vector Sizes,[0],[0]
It shows that our model is robust to a wide range of vector sizes.,5.6 Influence of Vector Sizes,[0],[0]
"In this paper, we propose an attention based LSTM network for cross-language sentiment classification.",6 Conclusion,[0],[0]
We use the bilingual bi-directional LSTMs to model the word sequences in the source and target languages.,6 Conclusion,[0],[0]
"Based on the special characteristics of the sentiment classification task, we propose a hierarchical attention model which is jointly trained with the LSTM network.",6 Conclusion,[0],[0]
"The sentence level attention
enables us to find the key sentences in a document and the word level attention helps to capture the sentiment signals.",6 Conclusion,[0],[0]
The proposed model achieves promising results on a benchmark dataset using Chinese as the source language and English as the target language.,6 Conclusion,[0],[0]
It outperforms the best results in the NLPC&CC cross-language sentiment classification evaluation as well as several strong baselines.,6 Conclusion,[0],[0]
"In future work, we will evaluate the performance of our model on more datasets and more language pairs.",6 Conclusion,[0],[0]
The sentiment lexicon is also another kind of useful resource for classification.,6 Conclusion,[0],[0]
We will explore how to make full usages of these resources in the proposed framework.,6 Conclusion,[0],[0]
"The work was supported by National Natural Science Foundation of China (61331011), National HiTech Research and Development Program (863 Program) of China (2015AA015403, 2014AA015102) and IBM Global Faculty Award Program.",Acknowledgments,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
Xiaojun Wan is the corresponding author.,Acknowledgments,[0],[0]
Most of the state-of-the-art sentiment classification methods are based on supervised learning algorithms which require large amounts of manually labeled data.,abstractText,[0],[0]
"However, the labeled resources are usually imbalanced in different languages.",abstractText,[0],[0]
Cross-lingual sentiment classification tackles the problem by adapting the sentiment resources in a resource-rich language to resource-poor languages.,abstractText,[0],[0]
"In this study, we propose an attention-based bilingual representation learning model which learns the distributed semantics of the documents in both the source and the target languages.",abstractText,[0],[0]
"In each language, we use Long Short Term Memory (LSTM) network to model the documents, which has been proved to be very effective for word sequences.",abstractText,[0],[0]
"Meanwhile, we propose a hierarchical attention mechanism for the bilingual LSTM network.",abstractText,[0],[0]
The sentence-level attention model learns which sentences of a document are more important for determining the overall sentiment while the word-level attention model learns which words in each sentence are decisive.,abstractText,[0],[0]
The proposed model achieves good results on a benchmark dataset using English as the source language and Chinese as the target language.,abstractText,[0],[0]
Attention-based LSTM Network for Cross-Lingual Sentiment Classification,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 127–136, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
One major application of embodied spoken dialogue systems is to improve life for elderly people by providing companionship and social interaction.,1 Introduction,[0],[0]
"Several conversational robots have been designed for this specific purpose (Heerink et al., 2008; Sabelli et al., 2011; Iwamura et al., 2011).",1 Introduction,[0],[0]
A necessary feature of such a system is that it be an attentive listener.,1 Introduction,[0],[0]
This means providing feedback to the user as they are talking so that they feel some sort of rapport and engagement with the system.,1 Introduction,[0],[0]
"Humans can interact with attentive listeners
at any time, making them a useful tool for people such as the elderly.
",1 Introduction,[0],[0]
Our motivation is to create a robot which can function as an attentive listener.,1 Introduction,[0],[0]
"Towards this goal, we use the autonomous android named Erica.",1 Introduction,[0],[0]
Our long-term goal is for Erica to be able to participate in a conversation with a human user while displaying human-like speech and gesture.,1 Introduction,[0],[0]
"In this work we focus on integrating an attentive listener function into Erica and describe a new approach for this application.
",1 Introduction,[0],[0]
"The approaches to these kind of dialogue systems have focused mainly on backchanneling behavior and have been implemented in large-scale projects such as SimSensei (DeVault et al., 2014), Sensitive Artificial Listeners (Bevacqua et al., 2012) and active listening robots (Johansson et al., 2016).",1 Introduction,[0],[0]
"These systems are multimodal in nature, using human-like non-verbal behaviors to give feedback to the user.",1 Introduction,[0],[0]
"However, the backchannels are usually generated after the end of utterance and they do not necessarily create synchrony in the conversation (Kawahara et al., 2015).",1 Introduction,[0],[0]
"Moreover, the dialogue systems are still based on handcrafted keyword matching.",1 Introduction,[0],[0]
"This means that new lines of dialogue or extensions to new topics must be handcrafted, which becomes impractical.
",1 Introduction,[0],[0]
In this paper we present an approach to attentive listening which integrates continuous backchannels with responsive dialogue to user statements to maintain the flow of conversation.,1 Introduction,[0],[0]
We create a continuous prediction model which is perceived as being better than a model which predicts only after an IPU (inter-pausal unit) has been received from the automatic speech recognition (ASR) system.,1 Introduction,[0],[0]
"Meanwhile, the statement response system detects focus words of the user’s utterance and uses them to generate responses as a wh-question or by repeating it back to the user.",1 Introduction,[0],[0]
"We also introduce a novel approach to turn-taking which uses
127
backchannels and fillers to indicate confidence in taking the speaking turn.
",1 Introduction,[0],[0]
Our approach is not limited by the topic of conversation and no prior parameters about the conversation are required so it can be applied to open domain conversation.,1 Introduction,[0],[0]
"We also do not require perfect speech recognition accuracy, which has been identified as a limitation in other attentive listening systems (Bevacqua et al., 2012).",1 Introduction,[0],[0]
"Our system runs efficiently in real-time and can be flexibly integrated into a larger architecture, which we will also demonstrate through a conversational robot.
",1 Introduction,[0],[0]
The next section outlines the architecture of our attentive listener.,1 Introduction,[0],[0]
In Section 3 we describe in detail the major components of the attentive listener including results of evaluation experiments.,1 Introduction,[0],[0]
"We then implement this system into Erica as a proofof-concept in Section 4, before the conclusion of the paper.",1 Introduction,[0],[0]
"Our system is in Japanese, but English translations are used in the paper for clarity.",1 Introduction,[0],[0]
Figure 1 summarizes the components of attentive listening and the general system architecture.,2 System architecture,[0],[0]
"Inputs to the system are prosodic features, which is calculated continuously, and ASR results from the Japanese speech recognition system Julius (Lee et al., 2001).
",2 System architecture,[0],[0]
"We implement a dialogue act tagger which classifies an utterance into questions, statements or others such as greetings.",2 System architecture,[0],[0]
This is currently based on a support vector machine and is moving to a recurrent neural network.,2 System architecture,[0],[0]
Questions and others are handled by a separate module which will not be explained in this paper.,2 System architecture,[0],[0]
Statements are handled by a statement response component.,2 System architecture,[0],[0]
"The other two components in the attentive listener are a backchannel generator and a turn-taking model.
",2 System architecture,[0],[0]
"Backchannels are generated by one component, while the statement response component can generate different types of dialogue depending on the utterance of the user.",2 System architecture,[0],[0]
"As part of our NLP functionalities we have a focus word extractor trained by a conditional random field (Yoshino and Kawahara, 2015) which identifies the focus of an utterance.",2 System architecture,[0],[0]
"For example, the statement “Yesterday I ate curry.”",2 System architecture,[0],[0]
would produce a focus word of “curry”.,2 System architecture,[0],[0]
We then send this information to the statement response component which generates a question response “What kind of curry?”.,2 System architecture,[0],[0]
"Further details of the technical implementation are described in the
next section.",2 System architecture,[0],[0]
The process flow of the system is as follows.,2 System architecture,[0],[0]
The system performs continuous backchanneling behavior while listening to the speaker.,2 System architecture,[0],[0]
"At the same time, ASR results of the user are received.",2 System architecture,[0],[0]
"When the utterance unit is detected and its dialogue act is tagged as a statement, then a response is generated and then stored.",2 System architecture,[0],[0]
"However, a response is only actually output when the system predicts an appropriate time to take the turn.",2 System architecture,[0],[0]
This is because the user may wish to keep talking and the system should not interrupt.,2 System architecture,[0],[0]
"Thus, we can manage turn-taking more flexibly.
",2 System architecture,[0],[0]
"In summary, the three major components required for attentive listening are backchanneling, statement response and turn-taking.",2 System architecture,[0],[0]
In this section we describe the three major components of attentive listening.,3 Attentive listening components,[0],[0]
We evaluate each of these components individually.,3 Attentive listening components,[0],[0]
"Our goal is to increase rapport (Huang et al., 2011) with the user by showing that the system is interested in the content of the user’s speech.",3.1 Continuous backchannel generation,[0],[0]
"There have been many works on automatic backchannel generation, with most using prosodic features for either rule-based models (Ward and Tsukahara, 2000; Truong et al., 2010) or machine learning methods (Morency et al., 2008; Ozkan et al., 2010; Kawahara et al., 2015).
",3.1 Continuous backchannel generation,[0],[0]
"In this work we use a model in which backchanneling behavior occurs continuously during the speaker’s turn, not only at the end of an utterance.",3.1 Continuous backchannel generation,[0],[0]
We take a machine learning approach by implementing a logistic regression model to predict if a backchannel would occur 500ms into the future.,3.1 Continuous backchannel generation,[0],[0]
"We predict into the future rather than at the current time point, because in the real-time system Erica requires processing time to generate nodding and mouth movements that synchronize with her utterance.",3.1 Continuous backchannel generation,[0],[0]
We trained the model using a counseling corpus.,3.1 Continuous backchannel generation,[0],[0]
"This corpus consisted of eight one-to-one counseling sessions between a counselor and a student and were transcribed according to the guidelines of the Corpus of Spontaneous Japanese (CSJ) (Maekawa, 2003).
",3.1 Continuous backchannel generation,[0],[0]
"The model makes a prediction every 100ms by using windows of prosodic features of sizes 100, 200, 500, 1000 and 2000 milliseconds.",3.1 Continuous backchannel generation,[0],[0]
"For a win-
dow size s, feature extraction is conducted within windows every s milliseconds before the current time point, up to a maximum of 4s milliseconds.",3.1 Continuous backchannel generation,[0],[0]
"For example, for a time window of 100ms, prosodic features are calculated inside windows starting at 400, 300, 200 and 100 milliseconds before the current time point.",3.1 Continuous backchannel generation,[0],[0]
"The prosodic features are the mean, maximum, minimum, range and slope of the pitch and intensity.",3.1 Continuous backchannel generation,[0],[0]
"Finally, we add the durations of silence, voice activity, and overlap of the speaker and listener.
",3.1 Continuous backchannel generation,[0],[0]
We conducted two evaluations of the backchannel timing model.,3.1 Continuous backchannel generation,[0],[0]
The first is an objective evaluation of the precision and recall.,3.1 Continuous backchannel generation,[0],[0]
We used 8-fold cross validation and tested on individual sessions.,3.1 Continuous backchannel generation,[0],[0]
We compared against a baseline model which generated a backchannel after every IPU (Fixed) and an IPU-based model based on logistic regression which also predicted after every IPU using additional linguistic features (IPU-based).,3.1 Continuous backchannel generation,[0],[0]
"Our model showed that the most influential prosodic feature was the range and maximum intensity of the speech, with larger windows located just before the prediction point generally being more influential than other windows.",3.1 Continuous backchannel generation,[0],[0]
"Although we have no quantitative evidence, we propose that a reduction in the intensity of the speech provides an opportunity for the listener to produce a backchannel.",3.1 Continuous backchannel generation,[0],[0]
"The results are displayed in Table 1.
",3.1 Continuous backchannel generation,[0],[0]
We see that the time-based model performs better than the baseline and the IPU-based model with a high AUC and recall.,3.1 Continuous backchannel generation,[0],[0]
"The precision is fairly low, due to predicting a large number backchannels even though none in the corpus are found.
",3.1 Continuous backchannel generation,[0],[0]
We also conducted a subjective evaluation of this model by comparing against the same models as the objective evaluation.,3.1 Continuous backchannel generation,[0],[0]
"We also included an additional counselor condition, in which backchannels in the real corpus were substituted with the same recorded pattern.
",3.1 Continuous backchannel generation,[0],[0]
"Participants in the experiment listened to recorded segments from the counseling corpus, lasting around 30-40 seconds each.",3.1 Continuous backchannel generation,[0],[0]
We chose segments where the counselor acted as an attentive listener by only responding through the backchannels used in our model.,3.1 Continuous backchannel generation,[0],[0]
The counselor’s voice for backchannels was generated using a recorded pattern by a female voice actress.,3.1 Continuous backchannel generation,[0],[0]
We created the different conditions for each recording by applying our model directly to the audio signal of the speaker.,3.1 Continuous backchannel generation,[0],[0]
The audio channel of the counselor’s voice was separated and so could be removed.,3.1 Continuous backchannel generation,[0],[0]
"When the model determined that a backchannel should be generated at a timepoint, we manually inserted the backchannel pattern into the speaker’s channel using audio editing software, effectively replacing the counselor’s voice.
",3.1 Continuous backchannel generation,[0],[0]
Each condition was listened to twice by each participant through different recordings selected at random.,3.1 Continuous backchannel generation,[0],[0]
"Subjects rated each recording over five measures - naturalness and tempo of backchannels (Q1 and Q2), empathy and understanding (Q3 and Q4) and if the participant would like to talk with the counselor in the recording (Q5).",3.1 Continuous backchannel generation,[0],[0]
"Each measure was rated using a 7-point Likert scale.
",3.1 Continuous backchannel generation,[0],[0]
For analysis we conducted a repeated measures ANOVA with Bonferroni corrections.,3.1 Continuous backchannel generation,[0],[0]
"Results are
shown in Table 2.",3.1 Continuous backchannel generation,[0],[0]
"Our proposed model outperformed the baseline models and was comparable to the counselor condition.
",3.1 Continuous backchannel generation,[0],[0]
The results of both evaluations show the need for backchannel timing to be done continuously and not just at the end of utterances.,3.1 Continuous backchannel generation,[0],[0]
The statement response component is triggered for statements and outputs when the system takes a turn.,3.2 Statement response,[0],[0]
The purpose is to encourage the user to expand on what they have just said and extend the thread of the conversation.,3.2 Statement response,[0],[0]
The statement response tries to use a question phrase which repeats a word that the user has previously said.,3.2 Statement response,[0],[0]
"For example, if the user says “I will go to the beach.”, the statement response should generate a question such as “Which beach?”.",3.2 Statement response,[0],[0]
"It may also repeat the focus of the utterance back to the user to encourage elaboration, such as “The beach?”.
",3.2 Statement response,[0],[0]
Our approach uses wh-questions as a means to continue the conversation.,3.2 Statement response,[0],[0]
"From a linguistic perspective, they are described in question taxonomies by Graesser et al. (1994) and Nielsen et al. (2008) as concept completions (who, what, when, where) or feature specifications (what properties does X have?).",3.2 Statement response,[0],[0]
"We observe that listeners in everyday conversations use such phrases to get the speaker to provide more information.
",3.2 Statement response,[0],[0]
"From a technical perspective, there are two processes for the system.",3.2 Statement response,[0],[0]
The first process is to detect the focus word of the utterance.,3.2 Statement response,[0],[0]
The second is to correctly pair this with an appropriate whquestion word to form a meaningful question.,3.2 Statement response,[0],[0]
"The basic wh-question words are similar for both English and Japanese.
",3.2 Statement response,[0],[0]
"To detect the focus word we use a conditional random field classifier in previous work which uses part-of-speech tags and a phrase-level depen-
dency tree (Yoshino and Kawahara, 2015).",3.2 Statement response,[0],[0]
The model was trained with utterances from users interacting with two different dialogue systems.,3.2 Statement response,[0],[0]
"This corpus was then annotated to identify the focus phrases of sentences.
",3.2 Statement response,[0],[0]
We use a decision tree in Figure 2 to decide from one of four response types.,3.2 Statement response,[0],[0]
"If a focus phrase can be detected, we take each noun in the phrase, match them to a wh-question and select the pair with the maximum likelihood.",3.2 Statement response,[0],[0]
We used an ngram language model to compute the joint probability of the focus noun being associated with each question word.,3.2 Statement response,[0],[0]
"The corpus used is the Balanced Corpus of Contemporary Written Japanese, which contains 100 million words from written documents.",3.2 Statement response,[0],[0]
We then consider the maximum joint probability of this noun and a question word.,3.2 Statement response,[0],[0]
"If this is over a threshold Tf , then a question on the focus word is generated.",3.2 Statement response,[0],[0]
"If no question is generated, the focus noun is repeated with a rising tone.
",3.2 Statement response,[0],[0]
If no focus phrase is found we match the predicate of the utterance to a question word using the same method as above.,3.2 Statement response,[0],[0]
"If this is above a threshold Tp, then the response is a question on the predicate, otherwise a formulaic expression is generated as a fallback response.",3.2 Statement response,[0],[0]
"We provide examples of each of the response types in Table 3.
",3.2 Statement response,[0],[0]
We evaluated this component in two different ways.,3.2 Statement response,[0],[0]
"Firstly, we extracted dialogue from an existing chatting corpus created for Project Next’s NLP task1.",3.2 Statement response,[0],[0]
We selected 200 user statements from this corpus as a test set and applied the statement response system to them.,3.2 Statement response,[0],[0]
Two annotators then checked if the generated responses were appropriate.,3.2 Statement response,[0],[0]
"The results are shown in Table 4.
",3.2 Statement response,[0],[0]
The results showed that the algorithm could classify the statements reasonably well.,3.2 Statement response,[0],[0]
"However, in the case of a focus word being unable to be
1https://sites.google.com/ site/dialoguebreakdowndetection/ chat-dialogue-corpus
found correctly identifying a question word for a predicate is a challenge.
",3.2 Statement response,[0],[0]
"Next, we evaluated our statement response system by testing if it could reduce the number of fallback responses used by the system.",3.2 Statement response,[0],[0]
"We conducted this experiment with 22 participants, and gathered data on their utterances during a first-time meeting with Erica.",3.2 Statement response,[0],[0]
"In most cases the participants asked questions that could be answered by the system, but sometimes the users said statements for which the question-answering system could not formulate a response.",3.2 Statement response,[0],[0]
"In these cases a generic fallback response was generated.
",3.2 Statement response,[0],[0]
From the data we found that 39 out of 226 (17.2%) user utterances produced fallback responses.,3.2 Statement response,[0],[0]
We processed all these utterances offline through the statement response component.,3.2 Statement response,[0],[0]
"From these 39 statements, 19 (47.7%) result in a statement which could be categorized into either a question on focus, partial repeat, or a question on predicate.",3.2 Statement response,[0],[0]
"Furthermore, the generated responses were deemed to be coherent with the correct focus and question words being applied.",3.2 Statement response,[0],[0]
This would have continued the flow of conversation.,3.2 Statement response,[0],[0]
The goal of turn-taking is to manage the floor of the conversation.,3.3 Flexible turn-taking,[0],[0]
The system decides when it should take the turn using a decision model.,3.3 Flexible turn-taking,[0],[0]
One simple approach is to wait for a fixed duration of silence from the user before starting the speaking turn.,3.3 Flexible turn-taking,[0],[0]
"However, we have found this is highly user-dependent and very challenging when the user continues talking.",3.3 Flexible turn-taking,[0],[0]
"The major problem is that if the user has not finished their turn and the system begins speaking, they must then wait for the system’s utterance to finish.",3.3 Flexible turn-taking,[0],[0]
This disrupts the flow of the conversation and makes the user frustrated.,3.3 Flexible turn-taking,[0],[0]
"Solving this problem is not trivial so several works have attempted to develop a robust model for turn-taking (Raux and Eskenazi, 2009; Selfridge and Heeman, 2010; Ward et al., 2010).
",3.3 Flexible turn-taking,[0],[0]
"Figure 3 displays our approach towards turntaking behavior, rather than having to make a binary decision about whether or not to take the turn.",3.3 Flexible turn-taking,[0],[0]
"When the user has the floor and the system receives an ASR result, our model outputs a likelihood score between 0 and 1 that the system should take the turn.",3.3 Flexible turn-taking,[0],[0]
The actual likelihood score determines the system’s response.,3.3 Flexible turn-taking,[0],[0]
"The system has four possible responses - silence, generate a backchannel, generate a filler or take the turn by speaking.
",3.3 Flexible turn-taking,[0],[0]
The novelty of our approach is that we do not have to immediately take a turn based on a hard threshold.,3.3 Flexible turn-taking,[0],[0]
Backchannels encourage the user to continue speaking and signal that the system will not take the turn.,3.3 Flexible turn-taking,[0],[0]
"Fillers are known to indicate a willingness to take the turn (Clark and Tree, 2002; Ishi et al., 2006) and so are used to grab the turn from the user.",3.3 Flexible turn-taking,[0],[0]
"However, the user may still wish to continue speaking and if they do the system won’t grab the turn and so doesn’t interrupt the flow of
conversation.",3.3 Flexible turn-taking,[0],[0]
"To guarantee that Erica will eventually take the turn, we set a threshold for the user’s silence time and automatically take the turn once it elapses.
",3.3 Flexible turn-taking,[0],[0]
"To implement this system, we used a logistic regression model with the same features as our backchanneling model.",3.3 Flexible turn-taking,[0],[0]
We train using the same counseling corpus and features that were used for the backchanneling model.,3.3 Flexible turn-taking,[0],[0]
"We found 25% of the outputs within the corpus to be turn changes.
",3.3 Flexible turn-taking,[0],[0]
Our proposed model requires two likelihood score thresholds (T1 and T2) to decide whether or not to be silent (≤ T1) or take the turn (≥ T2).,3.3 Flexible turn-taking,[0],[0]
We set a threshold for deciding between backchannels and fillers to 0.5.,3.3 Flexible turn-taking,[0],[0]
"We determined T1 to be 0.45 and T2 to be 0.85 based on Figure 4, which displays the distributions of likelihood score for the two classes.
",3.3 Flexible turn-taking,[0],[0]
The performance of this model is shown in Table 5.,3.3 Flexible turn-taking,[0],[0]
We compared the proposed model to a logistic regression model with a single threshold at 0.5.,3.3 Flexible turn-taking,[0],[0]
"Results are shown in Table 5.
",3.3 Flexible turn-taking,[0],[0]
These two thresholds degrade the recall of turntaking ground-truth actions because the cases in between them are discarded.,3.3 Flexible turn-taking,[0],[0]
"However we improve the precision of taking the turn, which is critical in spoken dialogue systems, from 0.428 to 0.624.",3.3 Flexible turn-taking,[0],[0]
"The cases discarded in this stage will be recovered by uttering fillers or backchannels.
",3.3 Flexible turn-taking,[0],[0]
"Moreover, the ground-truth labels are based on actual turn-taking actions made by the human listener, and there should be more Transition Relevance Places (Sacks et al., 1974), where turntaking would be allowed.",3.3 Flexible turn-taking,[0],[0]
This should be addressed in future work.,3.3 Flexible turn-taking,[0],[0]
In this section we describe the overall system with the attentive listener being integrated into the conversational android Erica.,4 System,[0],[0]
Erica is an android robot that takes the appearance of a young woman.,4.1 ERICA,[0],[0]
Her purpose is to use conversation to play a variety of social roles.,4.1 ERICA,[0],[0]
"The physical realism of Erica necessitates that her conver-
sational behaviors are also human-like.",4.1 ERICA,[0],[0]
"Therefore our objective is not only to undertake natural language processing, but to also address a variety of conversational phenomena.
",4.1 ERICA,[0],[0]
The environment we create for Erica reduces the need to use a physical interface such as a handheld microphone or headset to have a conversation.,4.1 ERICA,[0],[0]
Instead we use a spherical microphone array placed on a table between Erica and the user.,4.1 ERICA,[0],[0]
"A photo of this environment is shown in Figure 5.
",4.1 ERICA,[0],[0]
"Based on the microphone array and the Kinect sensor, we are able to reliably determine the source of speech.",4.1 ERICA,[0],[0]
Erica only considers speech from a particular user and ignores unrelated noises such as ambient sounds and her own voice.,4.1 ERICA,[0],[0]
We conducted an initial evaluation of our system as a pilot study to demonstrate its appropriateness for attentive listening.,4.2 Pilot study,[0],[0]
We have observed from previous demonstrations that users often do not speak with Erica as if she is an attentive listener.,4.2 Pilot study,[0],[0]
"Rather, they simply ask Erica questions and wait for her answers.",4.2 Pilot study,[0],[0]
"To overcome this issue in order to evaluate the statement response system, we first provided the subjects with dialogue prompts in the form of scripts.",4.2 Pilot study,[0],[0]
This allowed users familiarize themselves with Erica for free conversation.,4.2 Pilot study,[0],[0]
"Two male graduate students were subjects in the experiment and interacted with Erica in these two different tasks.
",4.2 Pilot study,[0],[0]
The first task was to read from four conversational scripts of 3 to 5 turns each.,4.2 Pilot study,[0],[0]
"These scripts were not hand-crafted, but taken from a corpus of real attentive listening conversations with a Wizard-of-Oz controlled robot.",4.2 Pilot study,[0],[0]
Subjects were instructed to pause after each sentence in the script to wait for a statement response.,4.2 Pilot study,[0],[0]
"If Erica replied with a question they could answer it before con-
tinuing the scripted conversation.",4.2 Pilot study,[0],[0]
The second task was to speak with Erica freely while she did attentive listening.,4.2 Pilot study,[0],[0]
In this scenario the subjects talked freely on the subject of their favorite travel memories.,4.2 Pilot study,[0],[0]
They could end the conversation whenever they wished.,4.2 Pilot study,[0],[0]
"Statistics of the subjects’ turns are shown in Table 6.
",4.2 Pilot study,[0],[0]
We find that the subjects reading from the script had longer turns but the speaking rate was lower than for free talk.,4.2 Pilot study,[0],[0]
"In other words, script reading was slower and longer.",4.2 Pilot study,[0],[0]
"We also analyzed the distribution of response types generated from the system as shown in Table 7.
Backchannels were generated most frequently, while both questions on focus and formulaic expressions were the most common response types, with questions on focus words having the highest frequency in free conversation.",4.2 Pilot study,[0],[0]
Partial repeats had a much higher frequency in the scripts than in free conversation.,4.2 Pilot study,[0],[0]
"This is because the script readings were taken from conversations which used more complex sentences than the free talk, and focus nouns for which a suitable question word could not be reliably matched.",4.2 Pilot study,[0],[0]
We evaluated the system by asking 8 evaluators to listen to the recording of both the scripts and free conversation.,4.3 Subjective ratings,[0],[0]
"Each evaluator was assigned
one random script and both free conversations to evaluate.",4.3 Subjective ratings,[0],[0]
"The evaluators rated each of Erica’s backchannels and statement responses in terms of coherence (coherent, somewhat coherent, or incoherent) and timing (fast, appropriate, or slow).",4.3 Subjective ratings,[0],[0]
We used a majority vote to determine the overall rating of each speech act.,4.3 Subjective ratings,[0],[0]
"The ratings on the coherence of each statement are shown in Figure 6.
",4.3 Subjective ratings,[0],[0]
We see that the results are similar to the previous evaluation of the statement response system.,4.3 Subjective ratings,[0],[0]
"More than half of questions on focus words were coherent, although most of these were in response to the scripts.",4.3 Subjective ratings,[0],[0]
"Formulaic expressions were mostly coherent even though they were selected at random.
",4.3 Subjective ratings,[0],[0]
"Similarly, we categorized system utterances into backchannels or statements and analyzed timing.",4.3 Subjective ratings,[0],[0]
"The results are shown in Figure 7.
",4.3 Subjective ratings,[0],[0]
"We can see that while most backchannels have suitable timing, statement responses are slow due to the processing of the utterance that is required.",4.3 Subjective ratings,[0],[0]
Table 8 shows dialogue from a free talk conversation.,4.4 Generated dialogue,[0],[0]
"User utterances were punctuated by backchannels and the system is able to extract a focus noun or predicate and produce a coherent response.
",4.4 Generated dialogue,[0],[0]
"We also found that the system could produce a coherent response even in the case of ASR errors.
",4.4 Generated dialogue,[0],[0]
In one case the subject said “sakana tsuri wo shimashita (I went fishing.).”.,4.4 Generated dialogue,[0],[0]
"The ASR system generated “sakana wo sore wo sumashita”, which is nonsensical.",4.4 Generated dialogue,[0],[0]
"In this case, the word “fish” was successfully detected as the focus noun and a coherent response could be generated.",4.4 Generated dialogue,[0],[0]
We also examined 17 utterances determined to be incoherent (excluding backchannels and formulaic expressions) and analyzed the reasons for these.,4.5 Analysis of incoherent statements,[0],[0]
"Table 9 shows the sources of errors in the statement response with their associated frequencies.
",4.5 Analysis of incoherent statements,[0],[0]
Incorrect question word matching was found several times.,4.5 Analysis of incoherent statements,[0],[0]
"For example, the user said “Tokyo ni ryokou ni ittekimashita (I went on a trip to Tokyo)”, generating the reply “Donna Tokyo desu ka?",4.5 Analysis of incoherent statements,[0],[0]
(What kind of Tokyo?)”,4.5 Analysis of incoherent statements,[0],[0]
which does not make sense.,4.5 Analysis of incoherent statements,[0],[0]
Another source of error was the system detecting a focus noun or predicate which did not make sense.,4.5 Analysis of incoherent statements,[0],[0]
Repeated statements were also found.,4.5 Analysis of incoherent statements,[0],[0]
The subject had already explained something during the conversation but the system asked a question on it.,4.5 Analysis of incoherent statements,[0],[0]
This can be addressed by keeping a history of the dialogue.,4.5 Analysis of incoherent statements,[0],[0]
"The ASR word error rate was approximately 10% for both script reading and free talk, so was not a major issue.",4.5 Analysis of incoherent statements,[0],[0]
"In most cases, incorrect ASR results cannot be parsed and so a formulaic expression is produced.",4.5 Analysis of incoherent statements,[0],[0]
Our pilot study showed that our system is feasible with no technical failures.,4.6 Lessons from pilot study,[0],[0]
Backchannels can be generated at appropriate times.,4.6 Lessons from pilot study,[0],[0]
Coherent responses could be generated by the system and errors in Erica’s dialog can be addressed.,4.6 Lessons from pilot study,[0],[0]
"We chose third-party evaluations for this experiment due to the small sample size and also because the subjects could not evaluate specific utterances while they were using the system.
",4.6 Lessons from pilot study,[0],[0]
However we intend to conduct a more comprehensive study where the subjects evaluate their own interaction with Erica.,4.6 Lessons from pilot study,[0],[0]
"Subjects should engage in free talk, but we have found that motivating them to do so is not trivial.",4.6 Lessons from pilot study,[0],[0]
A reasonable metric for a full experiment is the subject’s willingness to continue the interaction with with Erica which indicates engagement with the system.,4.6 Lessons from pilot study,[0],[0]
We can also use more objective metrics such as the number and length of turns taken by the user.,4.6 Lessons from pilot study,[0],[0]
Our strategy of using fillers and backchannels to regulate turn-taking should also be evaluated.,4.6 Lessons from pilot study,[0],[0]
In this paper we described our approach towards creating an attentive listening system which is integrated inside the android Erica.,5 Conclusion and future work,[0],[0]
"The major components are backchannel generation, statement response system, and a turn-taking model.",5 Conclusion and future work,[0],[0]
We presented individual evaluations of each of these components and how they work together to form the attentive listening system.,5 Conclusion and future work,[0],[0]
We also conducted a pilot study to demonstrate the feasibility of the attentive listener.,5 Conclusion and future work,[0],[0]
We intend to conduct a full experiment with the system to discover if it is comparable to human conversational behavior.,5 Conclusion and future work,[0],[0]
"Our aim is for this system to be used in a practical setting, particularly with elderly people.",5 Conclusion and future work,[0],[0]
"This work was supported by JST ERATO Ishiguro Symbiotic Human-Robot Interaction program (Grant Number JPMJER1401), Japan.",Acknowledgements,[0],[0]
"Attentive listening systems are designed to let people, especially senior people, keep talking to maintain communication ability and mental health.",abstractText,[0],[0]
This paper addresses key components of an attentive listening system which encourages users to talk smoothly.,abstractText,[0],[0]
"First, we introduce continuous prediction of end-of-utterances and generation of backchannels, rather than generating backchannels after end-point detection of utterances.",abstractText,[0],[0]
This improves subjective evaluations of backchannels.,abstractText,[0],[0]
"Second, we propose an effective statement response mechanism which detects focus words and responds in the form of a question or partial repeat.",abstractText,[0],[0]
This can be applied to any statement.,abstractText,[0],[0]
"Moreover, a flexible turn-taking mechanism is designed which uses backchannels or fillers when the turnswitch is ambiguous.",abstractText,[0],[0]
These techniques are integrated into a humanoid robot to conduct attentive listening.,abstractText,[0],[0]
We test the feasibility of the system in a pilot experiment and show that it can produce coherent dialogues during conversation.,abstractText,[0],[0]
"Attentive listening system with backchanneling, response generation and flexible turn-taking",title,[0],[0]
Categorical distributions are fundamental to many areas of machine learning.,1. Introduction,[0],[0]
"Examples include classification (Gupta et al., 2014), language models (Bengio et al., 2006), recommendation systems (Marlin & Zemel, 2004), reinforcement learning (Sutton & Barto, 1998), and neural attention models (Bahdanau et al., 2015).",1. Introduction,[0],[0]
"They also play an important role in discrete choice models (McFadden, 1978).
",1. Introduction,[0],[0]
"A categorical is a die with K sides, a discrete random variable that takes on one of K unordered outcomes; a categorical distribution gives the probability of each possible outcome.",1. Introduction,[0],[0]
Categorical variables are challenging to use when there are many possible outcomes.,1. Introduction,[0],[0]
"Such large categoricals appear in common applications such as image classification
1University of Cambridge.",1. Introduction,[0],[0]
2Columbia University.,1. Introduction,[0],[0]
3Athens University of Economics and Business..,1. Introduction,[0],[0]
"Correspondence to: Francisco J. R. Ruiz <f.ruiz@eng.cam.ac.uk, f.ruiz@columbia.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
with many classes, recommendation systems with many items, and language models over large vocabularies.",1. Introduction,[0],[0]
"In this paper, we develop a new method for fitting and using large categorical distributions.
",1. Introduction,[0],[0]
"The most common way to form a categorical is through the softmax transformation, which maps a K-vector of reals to a distribution of K outcomes.",1. Introduction,[0],[0]
Let ψ be a real-valued K-vector.,1. Introduction,[0],[0]
"The softmax transformation is
p(y",1. Introduction,[0],[0]
= k |ψ) = exp {ψk}∑ k′ exp {ψk′} .,1. Introduction,[0],[0]
"(1)
Note the softmax is not the only way to map real vectors to categorical distributions; for example, the multinomial probit (Albert & Chib, 1993) is an alternative.",1. Introduction,[0],[0]
"Also note that in many applications, such as in multiclass classification, the parameter ψk is a function of per-sample features",1. Introduction,[0],[0]
x.,1. Introduction,[0],[0]
"For example, a linear classifier forms a categorical over classes through a linear combination, ψk = w>k",1. Introduction,[0],[0]
"x.
We usually fit a categorical with maximum likelihood estimation or any other closely related strategy.",1. Introduction,[0],[0]
"Given a dataset y1:N of categorical data—each yn is one of K values—we aim to maximize the log likelihood,
Llog likelihood = N∑ n=1 log p(yn |ψ).",1. Introduction,[0],[0]
"(2)
Fitting this objective requires evaluating both the log probability and its gradient.
",1. Introduction,[0],[0]
Eqs.,1. Introduction,[0],[0]
1 and 2 reveal the challenge to using large categoricals.,1. Introduction,[0],[0]
Evaluating the log probability and evaluating its gradient are both O(K) operations.,1. Introduction,[0],[0]
"But this is not OK: most algorithms for fitting categoricals—for example, stochastic gradient ascent—require repeated evaluations of both gradients and probabilities.",1. Introduction,[0],[0]
"When K is large, these algorithms are prohibitively expensive.
",1. Introduction,[0],[0]
"Here we develop a method for fitting large categorical distributions, including the softmax but also more generally.",1. Introduction,[0],[0]
It is called augment and reduce (A&R).,1. Introduction,[0],[0]
"A&R rewrites the categorical distribution with an auxiliary variable ε,
p(y |ψ) = ∫ p(y, ε |ψ)dε.",1. Introduction,[0],[0]
"(3)
A&R then replaces the expensive log probability with a variational bound on the integral in Eq. 3.",1. Introduction,[0],[0]
"Using stochastic
variational methods (Hoffman et al., 2013), the cost to evaluate the bound (or its gradient) is far below O(K).
",1. Introduction,[0],[0]
"Because it relies on variational methods, A&R provides a lower bound on the marginal likelihood of the data.",1. Introduction,[0],[0]
"With this bound, we can embed A&R in a larger algorithm for fitting a categorical, e.g., a (stochastic) variational expectation maximization (VEM) algorithm (Beal, 2003).",1. Introduction,[0],[0]
"Though we focus on maximum likelihood, we can also use A&R in other algorithms that require log p(y |ψ) or its gradient, e.g., fully Bayesian approaches (Gelman et al., 2003) or the REINFORCE algorithm (Williams, 1992).
",1. Introduction,[0],[0]
We study A&R on linear classification tasks with up to 104 classes.,1. Introduction,[0],[0]
"On simulated and real data, we find that it provides accurate estimates of the categorical probabilities and gives better performance than existing approaches.
",1. Introduction,[0],[0]
Related work.,1. Introduction,[0],[0]
"There are many methods to reduce the cost of large categorical distributions, particularly under the softmax transformation.",1. Introduction,[0],[0]
"These include methods that approximate the exact computations (Gopal & Yang, 2013; Vijayanarasimhan et al., 2014), those that rely on sampling (Bengio & Sénécal, 2003; Mikolov et al., 2013; Devlin et al., 2014; Ji et al., 2016; Botev et al., 2017), those that use approximations and distributed computing (Grave et al., 2017), double-sum formulations (Raman et al., 2017; Fagan & Iyengar, 2018), and those that avail themselves of other techniques such as noise contrastive estimation (Smith & Jason, 2005; Gutmann & Hyvärinen, 2010) or random nearest neighbor search (Mussmann et al., 2017).
",1. Introduction,[0],[0]
Other methods change the model.,1. Introduction,[0],[0]
"They might replace the softmax transformation with a hierarchical or stickbreaking model (Kurzynski, 1988; Morin & Bengio, 2005; Tsoumakas et al., 2008; Beygelzimer et al., 2009; Dembczyński et al., 2010; Khan et al., 2012).",1. Introduction,[0],[0]
"These approaches can be successful, but the structure of the hierarchy may influence the learned probabilities.",1. Introduction,[0],[0]
"Other methods replace the softmax with a scalable spherical family of losses (Vincent et al., 2015; de Brébisson & Vincent, 2016).
",1. Introduction,[0],[0]
A&R is different from all of these techniques.,1. Introduction,[0],[0]
"Unlike many of them, it provides a lower bound on the log probability rather than an approximation.",1. Introduction,[0],[0]
The bound is useful because it can naturally be embedded in algorithms like stochastic VEM.,1. Introduction,[0],[0]
"Further, the A&R methodology applies to transformations beyond the softmax.",1. Introduction,[0],[0]
"In this paper, we study large categoricals via softmax, multinomial probit, and multinomial logistic.",1. Introduction,[0],[0]
A&R is the first scalable approach for the two latter models.,1. Introduction,[0],[0]
"It accelerates any transformation that can be recast as an additive noise model (e.g., Gumbel, 1954; Albert & Chib, 1993).
",1. Introduction,[0],[0]
"The approach that most closely relates to A&R is the one-vseach (OVE) bound of Titsias (2016), which is a lower bound of the softmax.",1. Introduction,[0],[0]
"Like the other related methods, it is narrower
than A&R in that it does not apply to transformations beyond the softmax.",1. Introduction,[0],[0]
We also empirically compare A&R to OVE in Section 4.,1. Introduction,[0],[0]
A&R provides a tighter lower bound and yields better predictive performance.,1. Introduction,[0],[0]
"We develop augment and reduce (A&R), a method for computing with large categorical random variables.
",2. Augment and Reduce,[0],[0]
The utility perspective.,2. Augment and Reduce,[0],[0]
"A&R uses the additive noise model perspective on the categorical, which we refer to as the utility perspective.",2. Augment and Reduce,[0],[0]
Define a mean utility ψk for each possible outcome k ∈,2. Augment and Reduce,[0],[0]
"{1, . . .",2. Augment and Reduce,[0],[0]
",K}.",2. Augment and Reduce,[0],[0]
"To draw a variable y from a categorical, we draw a zero-mean noise term εk for each possible outcome and then choose the value that maximizes the realized utility ψk + εk.",2. Augment and Reduce,[0],[0]
"This corresponds to the following process,
εk ∼ φ(·), k ∈ {1, . . .",2. Augment and Reduce,[0],[0]
",K}, y = argmax
k",2. Augment and Reduce,[0],[0]
"(ψk + εk) .
",2. Augment and Reduce,[0],[0]
"(4)
Note the errors εk are drawn fresh each time we draw a variable y.",2. Augment and Reduce,[0],[0]
"We assume that the errors are independent of each other, independent of the mean utility ψk, and identically distributed according to some distribution φ(·).
",2. Augment and Reduce,[0],[0]
Now consider the model where we marginalize the errors from Eq. 4.,2. Augment and Reduce,[0],[0]
"This results in a distribution p(y |ψ), a categorical that transforms ψ to the simplex.",2. Augment and Reduce,[0],[0]
"Depending on the distribution of the errors, this induces different transformations.",2. Augment and Reduce,[0],[0]
"For example, a standard Gumbel distribution recovers the softmax transformation; a standard Gaussian recovers the multinomial probit transformation; a standard logistic recovers the multinomial logistic transformation.
",2. Augment and Reduce,[0],[0]
"Typically, the mean utility ψk is a function of observed features x, e.g., ψk = x>wk in linear models or ψk = fwk(x) in non-linear settings.",2. Augment and Reduce,[0],[0]
"In both cases, wk are model parameters, relating the features to mean utilities.
",2. Augment and Reduce,[0],[0]
Let us focus momentarily on a linear classification problem under the softmax model.,2. Augment and Reduce,[0],[0]
"For each observation n, the mean utilities are ψnk = x>nwk and the random errors εnk are Gumbel distributed.",2. Augment and Reduce,[0],[0]
"After marginalizing out the errors, the probability that observation n is in class k is given by Eq. 1, p(yn = k |xn, w) ∝",2. Augment and Reduce,[0],[0]
exp{x>nwk}.,2. Augment and Reduce,[0],[0]
Fitting the classifier involves learning the weights wk that parameterize ψ.,2. Augment and Reduce,[0],[0]
"For example, maximum likelihood uses gradient ascent to maximize ∑ n log p(yn |xn, w) with respect to w.
Large categoricals.",2. Augment and Reduce,[0],[0]
"When the number of outcomes K is large, the normalizing constant of the softmax is a computational burden; it is O(K).",2. Augment and Reduce,[0],[0]
"Consequently, it is burdensome to calculate useful quantities like log p(yn |xn, w) and its gradient ∇w log p(yn |xn, w).",2. Augment and Reduce,[0],[0]
"As an ultimate consequence, maximum likelihood estimation is slow—it needs to evalu-
ate the gradient for each n at each iteration.
",2. Augment and Reduce,[0],[0]
Its difficulty scaling is not unique to the softmax.,2. Augment and Reduce,[0],[0]
Similar issues arise for the multinomial probit and multinomial logistic.,2. Augment and Reduce,[0],[0]
"With these transformations as well, evaluating likelihoods and related quantities is O(K).",2. Augment and Reduce,[0],[0]
We introduce A&R to relieve this burden.,2.1. Augment and reduce,[0],[0]
"A&R accelerates training in models with categorical distributions and a large number of outcomes.
",2.1. Augment and reduce,[0],[0]
"Rather than operating directly on the marginal p(y |ψ), A&R augments the model with one of the error terms and forms a joint p(y, ε |ψ).",2.1. Augment and reduce,[0],[0]
(We drop the subscript n to avoid cluttered notation.),2.1. Augment and reduce,[0],[0]
This augmented model has a desirable property: its log-joint is a sum over all the possible outcomes.,2.1. Augment and reduce,[0],[0]
A&R then reduces—it subsamples a subset of outcomes to construct estimates of the log-joint and its gradient.,2.1. Augment and reduce,[0],[0]
"As a result, its complexity relates to the size of the subsample, not the total number of outcomes K.
The augmented model.",2.1. Augment and reduce,[0],[0]
"Let φ(ε) be the distribution over the error terms, and Φ(ε) = ∫ ε −∞ φ(τ)dτ the corresponding cumulative distribution function (CDF).",2.1. Augment and reduce,[0],[0]
"The marginal probability of outcome k is the probability that its realized utility (ψk + εk) is greater than all others,
p(y",2.1. Augment and reduce,[0],[0]
"= k |ψ) = Pr (ψk + εk ≥ ψk′ + εk′ ∀k′ 6= k) .
",2.1. Augment and reduce,[0],[0]
"We write this probability as an integral over the kth error εk using the CDF of the other errors, p(y = k |ψ) = ∫",2.1. Augment and reduce,[0],[0]
"+∞ −∞ φ(εk) (∏ k′ 6=k ∫ εk+ψk−ψk′ −∞ φ(εk′)dεk′ ) dεk
= ∫",2.1. Augment and reduce,[0],[0]
+∞ −∞ φ(ε) (∏ k′ 6=k Φ(ε+ ψk,2.1. Augment and reduce,[0],[0]
− ψk′) ),2.1. Augment and reduce,[0],[0]
"dε. (5)
(We renamed the dummy variable εk as ε to avoid clutter.)",2.1. Augment and reduce,[0],[0]
"Eq. 5 is the same as found by Girolami & Rogers (2006) for the multinomial probit model, although we do not assume a Gaussian density φ(ε).",2.1. Augment and reduce,[0],[0]
"Rather, we only assume that we can evaluate both φ(ε) and Φ(ε).
",2.1. Augment and reduce,[0],[0]
"We derived Eq. 5 from the utility perspective, which encompasses many common models.",2.1. Augment and reduce,[0],[0]
"We obtain the softmax by choosing a standard Gumbel distribution for φ(ε), in which case Eqs. 1 and 5 are equivalent.",2.1. Augment and reduce,[0],[0]
"We obtain the multinomial probit by choosing a standard Gaussian distribution over the errors, and in this case the integral in Eq.",2.1. Augment and reduce,[0],[0]
5 does not have a closed form.,2.1. Augment and reduce,[0],[0]
"Similarly, we obtain the multinomial logistic by choosing a standard logistic distribution φ(ε).",2.1. Augment and reduce,[0],[0]
"What is important is that regardless of the model, the cost to compute the marginal probability p(y = k |ψ) is O(K).
",2.1. Augment and reduce,[0],[0]
"We now augment the model with the auxiliary latent variable ε to form the joint distribution p(y, ε |ψ),
p(y = k, ε |ψ) = φ(ε) ∏ k′ 6=k Φ(ε+ ψk",2.1. Augment and reduce,[0],[0]
− ψk′).,2.1. Augment and reduce,[0],[0]
"(6)
This is a model that includes the kth error term from Eq. 4 but marginalizes out all the other errors.",2.1. Augment and reduce,[0],[0]
"By construction, marginalizing ε from Eq. 6 recovers the original model p(y |ψ) in Eq. 5.",2.1. Augment and reduce,[0],[0]
"Figure 1 illustrates this idea.
",2.1. Augment and reduce,[0],[0]
Riihimäki et al. (2013) used Eq. 6 in the nested expectation propagation for Gaussian process classification.,2.1. Augment and reduce,[0],[0]
"We use it to scale learning with categorical distributions.
",2.1. Augment and reduce,[0],[0]
The variational bound.,2.1. Augment and reduce,[0],[0]
The augmented model in Eq. 6 involves one latent variable ε.,2.1. Augment and reduce,[0],[0]
But our goal is to calculate the marginal log p(y |ψ) and its gradient.,2.1. Augment and reduce,[0],[0]
A&R derives a variational lower bound on log p(y |ψ) using the joint in Eq. 6.,2.1. Augment and reduce,[0],[0]
Define q(ε) to be a variational distribution on the auxiliary variable.,2.1. Augment and reduce,[0],[0]
"The bound is log p(y |ψ) ≥ L, where
L = Eq(ε) [ log p(y = k, ε |ψ)− log q(ε) ]",2.1. Augment and reduce,[0],[0]
"(7)
= Eq(ε) [ log φ(ε) + ∑ k′ 6=k logΦ(ε+ ψk",2.1. Augment and reduce,[0],[0]
"− ψk′)− log q(ε) ] .
",2.1. Augment and reduce,[0],[0]
"In Eq. 7, L is the evidence lower bound (ELBO); it is tight when q(ε) is equal to the posterior of ε given y, p(ε | y, ψ)",2.1. Augment and reduce,[0],[0]
"(Jordan et al., 1999; Blei et al., 2017).
",2.1. Augment and reduce,[0],[0]
The ELBO contains a summation over the outcomes k′ 6=,2.1. Augment and reduce,[0],[0]
"k. A&R exploits this property to reduce complexity, as we describe below.",2.1. Augment and reduce,[0],[0]
"Next we show how to use the bound in a variational expectation maximization (VEM) procedure and we describe the reduce step of A&R.
Variational expectation maximization.",2.1. Augment and reduce,[0],[0]
"Consider again a linear classification task, where we have a dataset of features xn and labels yn ∈ {1, . . .",2.1. Augment and reduce,[0],[0]
",K} for n = 1, . . .",2.1. Augment and reduce,[0],[0]
", N .",2.1. Augment and reduce,[0],[0]
The mean utility for each observation n is ψnk =,2.1. Augment and reduce,[0],[0]
"w>k xn, and the goal is to learn the weights wk by maximizing the log likelihood ∑ n log p(yn |xn, w).
",2.1. Augment and reduce,[0],[0]
A&R replaces each term in the data log likelihood with its bound using Eq. 7.,2.1. Augment and reduce,[0],[0]
The objective becomes ∑ n L(n).,2.1. Augment and reduce,[0],[0]
Maximizing this objective requires an iterative process with two steps.,2.1. Augment and reduce,[0],[0]
"In one step, A&R optimizes the objective with respect to w.",2.1. Augment and reduce,[0],[0]
"In the other step, A&R optimizes each L(n) with respect to the variational distribution.",2.1. Augment and reduce,[0],[0]
"The resulting procedure takes the form of a VEM algorithm (Beal, 2003).
",2.1. Augment and reduce,[0],[0]
The VEM algorithm requires optimizing the ELBO with respect to w and the variational distributions.1,2.1. Augment and reduce,[0],[0]
This is challenging for two reasons.,2.1. Augment and reduce,[0],[0]
"First, the expectations in Eq. 7 might not be tractable.",2.1. Augment and reduce,[0],[0]
"Second, the cost to compute the gradients of Eq. 7 is still O(K).
",2.1. Augment and reduce,[0],[0]
Section 3 addresses these issues.,2.1. Augment and reduce,[0],[0]
"To sidestep the intractable expectations, A&R forms unbiased Monte Carlo estimates of the gradient of the ELBO.",2.1. Augment and reduce,[0],[0]
"To alleviate the computational complexity, A&R uses stochastic optimization, subsampling a set of outcomes k′.
Reduce by subsampling.",2.1. Augment and reduce,[0],[0]
The subsampling step in the VEM procedure is one of the key ideas behind A&R.,2.1. Augment and reduce,[0],[0]
Since Eq. 7 contains a summation over the outcomes k′,2.1. Augment and reduce,[0],[0]
6=,2.1. Augment and reduce,[0],[0]
"k, we can apply stochastic optimization techniques to obtain unbiased estimates of the ELBO and its gradient.
",2.1. Augment and reduce,[0],[0]
"More specifically, consider the gradient of the ELBO in Eq. 7 with respect to w (the parameters of ψ).",2.1. Augment and reduce,[0],[0]
"It is
∇wL = ∑ k′ 6=k Eq(ε) [ ∇w logΦ(ε+ ψk",2.1. Augment and reduce,[0],[0]
− ψk′),2.1. Augment and reduce,[0],[0]
"] .
",2.1. Augment and reduce,[0],[0]
"A&R estimates this by first randomly sampling a subset of outcomes S ⊆ {1, . . .",2.1. Augment and reduce,[0],[0]
",K} {k} of size |S|.",2.1. Augment and reduce,[0],[0]
"A&R then uses the outcomes in S to approximate the gradient,
∇̃wL = K − 1 |S| ∑ k′∈S Eq(ε) [ ∇w logΦ(ε+ ψk",2.1. Augment and reduce,[0],[0]
− ψk′),2.1. Augment and reduce,[0],[0]
"] .
",2.1. Augment and reduce,[0],[0]
"This is an unbiased estimator2 of the gradient ∇wL. Crucially, A&R only needs to iterate over |S| outcomes to obtain it, reducing the complexity to O(|S|).
",2.1. Augment and reduce,[0],[0]
The reduce step is also applicable to optimize the ELBO with respect to q(ε).,2.1. Augment and reduce,[0],[0]
Section 3 gives further details about the stochastic VEM procedure in different settings.,2.1. Augment and reduce,[0],[0]
"Here we provide the details to run the variational expectation maximization (VEM) algorithm for the softmax model (Sec-
1Note that maximizing the ELBO in Eq. 7 with respect to the distribution q(ε) is equivalent to minimizing the Kullback-Leibler divergence from q(ε) to the posterior p(ε | y, ψ).
",3. Algorithm Description,[0],[0]
2This is not the only way to construct an unbiased estimator.,3. Algorithm Description,[0],[0]
"Alternatively, we can draw the outcomes k′ using importance sampling, taking into account the frequency of each class.",3. Algorithm Description,[0],[0]
"We leave this for future work.
tion 3.1) and for more general models including the multinomial probit and multinomial logistic (Section 3.2).",3. Algorithm Description,[0],[0]
"These models only differ in the prior over the errors φ(ε).
",3. Algorithm Description,[0],[0]
Augment and reduce (A&R) is not limited to point-mass estimation of the parameters,3. Algorithm Description,[0],[0]
w.,3. Algorithm Description,[0],[0]
"It is straightforward to extend the algorithm to perform posterior inference on w via stochastic variational inference, but for simplicity we describe maximum likelihood estimation.",3. Algorithm Description,[0],[0]
"In the softmax model, the distribution over the error terms is a standard Gumbel (Gumbel, 1954),
φsoftmax(ε) = exp{−ε− e−ε}, Φsoftmax(ε) = exp{−e−ε}.
",3.1. Augment and Reduce for Softmax,[0],[0]
"In this model, the optimal distribution q?(ε), which achieves equality in the bound, has closed-form expression:
q?softmax(ε) = Gumbel(ε ; log η ?, 1), with η? = 1 + ∑ k′ 6=k e
ψk′−ψk .",3.1. Augment and Reduce for Softmax,[0],[0]
"However, even though q?softmax(ε) has an analytic form, its parameter η
? is computationally expensive to obtain because it involves a summation over K − 1 classes.",3.1. Augment and Reduce for Softmax,[0],[0]
"Instead, we set
qsoftmax(ε ; η) = Gumbel(ε ; log η, 1).
",3.1. Augment and Reduce for Softmax,[0],[0]
"Substituting this choice for qsoftmax(ε ; η) into Eq. 7 gives the following evidence lower bound (ELBO):
Lsoftmax = 1− log(η)− 1
η 1 + ∑ k′ 6=k eψk′−ψk  .",3.1. Augment and Reduce for Softmax,[0],[0]
"(8) Eq. 8 coincides with the log-concavity bound (Bouchard, 2007; Blei & Lafferty, 2007), although we have derived it from a completely different perspective.",3.1. Augment and Reduce for Softmax,[0],[0]
"This derivation allows us to optimize η efficiently, as we describe next.
",3.1. Augment and Reduce for Softmax,[0],[0]
"The Gumbel(ε ; log η, 1) is an exponential family distribution whose natural parameter is η.",3.1. Augment and Reduce for Softmax,[0],[0]
This allows us to use natural gradients in the stochastic inference procedure.,3.1. Augment and Reduce for Softmax,[0],[0]
"A&R iterates between a local step, in which we update η, and a global step, in which we update the parameters ψ.
",3.1. Augment and Reduce for Softmax,[0],[0]
"In the local step (E step), we optimize η by taking a step in the direction of the noisy natural gradient, yielding ηnew = (1 − α)ηold + αη̃.",3.1. Augment and Reduce for Softmax,[0],[0]
"Here, η̃ is an estimate of the optimal natural parameter, which we obtain using a random set of outcomes, i.e., η̃ = 1 + K−1|S| ∑ k′∈S e
ψk′−ψk , where S ⊆ {1, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
",K} {k}.",3.1. Augment and Reduce for Softmax,[0],[0]
"The parameter α is the step size; it must satisfy the Robbins-Monro conditions (Robbins & Monro, 1951; Hoffman et al., 2013).
",3.1. Augment and Reduce for Softmax,[0],[0]
"In the global step (M step), we take a gradient step with respect to w (the parameters of ψ), holding η fixed.",3.1. Augment and Reduce for Softmax,[0],[0]
"Similarly, we can estimate the gradient of Eq. 8 with complexity O(|S|) by leveraging stochastic optimization.
",3.1. Augment and Reduce for Softmax,[0],[0]
"Algorithm 1 Softmax A&R for classification Input: data (xn, yn), minibatch sizes |B| and |S| Output: weights w = {wk}Kk=1 Initialize all weights and natural parameters for iteration t = 1, 2, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
", do # Sample minibatches: Sample a minibatch of data, B ⊆ {1, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
", N} for n ∈ B do
Sample a set of labels, Sn ⊆ {1, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
",K} {yn} end for # Local step (E step): for n ∈ B do
Compute η̃n = 1 + K−1|S| ∑ k′∈Sn e ψnk′−ψnyn
Update natural param., ηn ← (1−α(t))ηn+α(t)η̃n end for # Global step (M step):",3.1. Augment and Reduce for Softmax,[0],[0]
"Set g = − N|B| K−1 |S| ∑ n∈B 1 ηn ∑ k′∈Sn∇we ψnk′−ψnyn
Gradient step on the weights, w ← w + ρ(t)g end for
Algorithm 1 summarizes the procedure for a classification task.",3.1. Augment and Reduce for Softmax,[0],[0]
"In this example, the dataset consists of N datapoints (xn, yn), where xn is a feature vector and yn ∈ {1, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
",K} is the class label.",3.1. Augment and Reduce for Softmax,[0],[0]
"Each observation is associated with its parameters ψnk; e.g., ψnk = x>nwk.",3.1. Augment and Reduce for Softmax,[0],[0]
"We posit a softmax likelihood, and we wish to infer the weights via maximum likelihood using A&R.",3.1. Augment and Reduce for Softmax,[0],[0]
"Thus, the objective function is∑ n L (n) softmax.",3.1. Augment and Reduce for Softmax,[0],[0]
(It is straightforward to obtain the maximum a posteriori solution by adding a regularizer.),3.1. Augment and Reduce for Softmax,[0],[0]
"At each iteration, we process a random subset of observations as well as a random subset of classes for each one.
",3.1. Augment and Reduce for Softmax,[0],[0]
"Finally, note that we can perform posterior inference on the parameters w (instead of maximum likelihood) using A&R.",3.1. Augment and Reduce for Softmax,[0],[0]
"One way is to consider a variational distribution q(w) and take gradient steps with respect to the variational parameters of q(w) in the global step, using the reparameterization trick (Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014; Kingma & Welling, 2014) to approximate that gradient.",3.1. Augment and Reduce for Softmax,[0],[0]
"In the local step, we only need to evaluate the moment generating function, estimating the optimal natural parameter as η̃ = 1 + K−1|S| ∑ k′∈S Eq(w)",3.1. Augment and Reduce for Softmax,[0],[0]
[ eψk′−ψk ] .,3.1. Augment and Reduce for Softmax,[0],[0]
"For most models, the expectations of the ELBO in Eq. 7 are intractable, and there is no closed-form solution for the optimal variational distribution q?(ε).",3.2. Augment and Reduce for Other Models,[0],[0]
"Fortunately, we can apply A&R, using the reparameterization trick to build Monte Carlo estimates of the gradient of the ELBO with respect to the variational parameters (Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014; Kingma & Welling, 2014).
",3.2. Augment and Reduce for Other Models,[0],[0]
"More in detail, consider the variational distribution q(ε ; ν),
Algorithm 2 General A&R for classification Input: data (xn, yn), minibatch sizes |B| and |S| Output: weights w = {wk}Kk=1 Initialize all weights and local variational parameters for iteration t = 1, 2, . . .",3.2. Augment and Reduce for Other Models,[0],[0]
", do # Sample minibatches: Sample a minibatch of data, B ⊆ {1, . . .",3.2. Augment and Reduce for Other Models,[0],[0]
", N} for n ∈ B do
Sample a set of labels, Sn ⊆ {1, . . .",3.2. Augment and Reduce for Other Models,[0],[0]
",K} {yn} end for # Local step (E step): for n ∈ B do
Sample auxiliary variable un ∼ q(rep)(un) Transform auxiliary variable, εn = T (un ; νn) Estimate the gradient ∇̃νnL(n)",3.2. Augment and Reduce for Other Models,[0],[0]
"(Eq. 9) Update variational param., νn ← νn+α(t)∇̃νnL(n)
end for # Global step (M step):",3.2. Augment and Reduce for Other Models,[0],[0]
"Sample εn ∼ q(εn ; νn) for all n ∈ B Set g= N|B| K−1 |S| ∑ n∈B ∑ k′∈Sn ∇wlogΦ(εn+ψnyn−ψnk′)
",3.2. Augment and Reduce for Other Models,[0],[0]
"Gradient step on the weights, w ← w + ρ(t)g end for
parameterized by some variational parameters ν.",3.2. Augment and Reduce for Other Models,[0],[0]
"We assume that this distribution is reparameterizable, i.e., we can sample from q(ε ; ν) by first sampling an auxiliary variable u ∼ q(rep)(u) and then setting ε = T (u ; ν).
",3.2. Augment and Reduce for Other Models,[0],[0]
"In the local step, we fit q(ε ; ν) by taking a gradient step of the ELBO with respect to the variational parameters ν.",3.2. Augment and Reduce for Other Models,[0],[0]
"Since the expectations in Eq. 7 are not tractable, we obtain Monte Carlo estimates by sampling ε from the variational distribution.",3.2. Augment and Reduce for Other Models,[0],[0]
"To sample ε, we sample u ∼ q(rep)(u) and set ε = T (u ; ν).",3.2. Augment and Reduce for Other Models,[0],[0]
"To alleviate the computational complexity, we apply the reduce step, sampling a random subset S ⊆ {1, . . .",3.2. Augment and Reduce for Other Models,[0],[0]
",K} {k} of outcomes.",3.2. Augment and Reduce for Other Models,[0],[0]
"We thus form a one-sample gradient estimator as
∇̃νL = ∇ε log p̃(y, ε |ψ)∇νT (u ; ν) +∇νH[q(ε ; ν)], (9) where H[q(ε ; ν)] is the entropy of the variational distribution,3 and log p̃(y, ε |ψ) is a log joint estimate,
log p̃(y, ε |ψ) = log φ(ε)+K",3.2. Augment and Reduce for Other Models,[0],[0]
"− 1 |S| ∑ k′∈S logΦ(ε+ψk−ψk′).
",3.2. Augment and Reduce for Other Models,[0],[0]
"In the global step, we estimate the gradient of the ELBO with respect to w. Following a similar approach, we obtain an unbiased one-sample gradient estimator as ∇̃wL = K−1 |S| ∑ k′∈S ∇w logΦ(ε+ ψk",3.2. Augment and Reduce for Other Models,[0],[0]
"− ψk′).
",3.2. Augment and Reduce for Other Models,[0],[0]
"Algorithm 2 summarizes the procedure to efficiently run
3We can estimate the gradient of the entropy when it is not available analytically.",3.2. Augment and Reduce for Other Models,[0],[0]
"Even when it is, the Monte Carlo estimator may have lower variance (Roeder et al., 2017).
maximum likelihood on a classification problem.",3.2. Augment and Reduce for Other Models,[0],[0]
"We subsample observations and classes at each iteration.
",3.2. Augment and Reduce for Other Models,[0],[0]
"Finally, note that we can perform posterior inference on the parameters w by positing a variational distribution q(w) and taking gradient steps with respect to the variational parameters of q(w) in the global step.",3.2. Augment and Reduce for Other Models,[0],[0]
"In this case, the reparameterization trick is needed in both the local and global step to obtain Monte Carlo estimates of the gradient.
",3.2. Augment and Reduce for Other Models,[0],[0]
"We now particularize A&R for the multinomial probit and multinomial logistic models.
",3.2. Augment and Reduce for Other Models,[0],[0]
A&R for multinomial probit.,3.2. Augment and Reduce for Other Models,[0],[0]
"Consider a standard Gaussian distribution over the error terms,
φprobit(ε) = 1√ 2π e− 1 2 ε 2 , Φprobit(ε) = ∫ ε",3.2. Augment and Reduce for Other Models,[0],[0]
"−∞ φprobit(τ)dτ.
",3.2. Augment and Reduce for Other Models,[0],[0]
"A&R chooses a Gaussian variational distribution qprobit(ε ; ν) = N (ε ; µ, σ2) and fits the variational parameters ν",3.2. Augment and Reduce for Other Models,[0],[0]
=,3.2. Augment and Reduce for Other Models,[0],[0]
"[µ, σ]>.",3.2. Augment and Reduce for Other Models,[0],[0]
"The Gaussian is reparameterizable in terms of a standard Gaussian, i.e., q(rep)probit(u) = N (u ; 0, 1).",3.2. Augment and Reduce for Other Models,[0],[0]
The transformation is ε = T (u ; ν) = µ + σu.,3.2. Augment and Reduce for Other Models,[0],[0]
"Thus, the gradients in Eq. 9 are ∇νT (u ; ν) =",3.2. Augment and Reduce for Other Models,[0],[0]
"[1, u]> and ∇νH[qprobit(ε ; ν)] = [0, 1/σ]>.
",3.2. Augment and Reduce for Other Models,[0],[0]
A&R for multinomial logistic.,3.2. Augment and Reduce for Other Models,[0],[0]
"Consider now a standard logistic distribution over the errors,
φlogistic(ε) = σ(ε)σ(−ε), Φlogistic(ε) = σ(ε),
where σ(ε) = 11+e−ε is the sigmoid function.",3.2. Augment and Reduce for Other Models,[0],[0]
(The logistic distribution has heavier tails than the Gaussian.),3.2. Augment and Reduce for Other Models,[0],[0]
"Under this model, the ELBO in Eq. 7 takes the form
Llogistic=Eq(ε) [ log
σ(ε)σ(−ε) q(ε) + ∑ k′ 6=k log σ(ε+ψk−ψk′) ] .
",3.2. Augment and Reduce for Other Models,[0],[0]
"Note the close resemblance between this expression and the one-vs-each (OVE) bound of Titsias (2016),
LOVE = ∑ k′ 6=k log σ(ψk − ψk′).",3.2. Augment and Reduce for Other Models,[0],[0]
"(10)
However, while the former is a bound on the multinomial logistic model, the OVE is a bound on the softmax.
",3.2. Augment and Reduce for Other Models,[0],[0]
A&R sets qlogistic(ε ; ν) = 1βσ ( ε−µ β ) σ,3.2. Augment and Reduce for Other Models,[0],[0]
"( − ε−µβ ) , a logistic distribution.",3.2. Augment and Reduce for Other Models,[0],[0]
The variational parameters are ν =,3.2. Augment and Reduce for Other Models,[0],[0]
"[µ, β]>.",3.2. Augment and Reduce for Other Models,[0],[0]
"The logistic distribution is reparameterizable, with q(rep)logistic(u) = σ(u)σ(−u) and transformation ε = T (u ; ν) = µ+ βu.",3.2. Augment and Reduce for Other Models,[0],[0]
The gradient of the entropy in Eq. 9 is ∇νH[qlogistic(ε ; ν)] =,3.2. Augment and Reduce for Other Models,[0],[0]
"[0, 1/β]>.",3.2. Augment and Reduce for Other Models,[0],[0]
We showcase augment and reduce (A&R) on a linear classification task.,4. Experiments,[0],[0]
"Our goal is to assess the predictive performance
of A&R in this classification task, to assess the quality of the marginal bound of the data, and to compare its complexity4 with existing approaches.
",4. Experiments,[0],[0]
"We run A&R for three different models of categorical distributions (softmax, multinomial probit, and multinomial logistic).5 For the softmax model, we compare A&R against the one-vs-each (OVE) bound (Titsias, 2016).",4. Experiments,[0],[0]
"Just like A&R, OVE is a rigorous lower bound on the marginal likelihood.",4. Experiments,[0],[0]
"It can also run on a single machine,6 and it has been shown to outperform other approaches.
",4. Experiments,[0],[0]
"For softmax, A&R runs nearly as fast as OVE but has better predictive performance and provides a tighter bound on the marginal likelihood than OVE.",4. Experiments,[0],[0]
"On two small datasets, the A&R bound closely reaches the marginal likelihood of exact softmax maximum likelihood estimation.
",4. Experiments,[0],[0]
We now describe the experimental settings.,4. Experiments,[0],[0]
"In Section 4.1, we analyze synthetic data and K = 104 classes.",4. Experiments,[0],[0]
"In Section 4.2, we analyze five real datasets.
",4. Experiments,[0],[0]
Experimental setup.,4. Experiments,[0],[0]
"We consider linear classification, where the mean utilities are ψnk =",4. Experiments,[0],[0]
w>k xn,4. Experiments,[0],[0]
+ w (0) k .,4. Experiments,[0],[0]
"We fit the model parameters (weights and biases) via maximum likelihood estimation, using stochastic gradient ascent.",4. Experiments,[0],[0]
"We initialize the weights and biases randomly, drawing from a Gaussian distribution with zero mean and standard deviation 0.1 (0.001 for the biases).",4. Experiments,[0],[0]
"For each experiment, we use the same initialization across all methods.
",4. Experiments,[0],[0]
Algorithms 1 and 2 require setting a step size schedule for ρ(t).,4. Experiments,[0],[0]
"We use the adaptive step size sequence proposed by Kucukelbir et al. (2017), which combines RMSPROP (Tieleman & Hinton, 2012) and Adagrad (Duchi et al., 2011).",4. Experiments,[0],[0]
"We set the step size using the default parameters, i.e.,
ρ(t) = ρ0 × t−1/2+10 −16",4. Experiments,[0],[0]
×,4. Experiments,[0],[0]
( 1 + √ s(t) ),4. Experiments,[0],[0]
"−1 ,
s(t)",4. Experiments,[0],[0]
"= 0.1(g(t))2 + 0.9s(t−1).
",4. Experiments,[0],[0]
We set ρ0 = 0.02 and we additionally decrease ρ0 by a factor of 0.9 every 2000 iterations.,4. Experiments,[0],[0]
"We use the same step size sequence for OVE.
",4. Experiments,[0],[0]
"We set the step size α(t) in Algorithm 1 as α(t) = (1+t)−0.9, the default values suggested by Hoffman et al. (2013).",4. Experiments,[0],[0]
"For the step size α(t) in Algorithm 2, we set α(t) = 0.01(1 + t)−0.9.",4. Experiments,[0],[0]
"For the multinomial logit and multinomial probit A&R, we parameterize the variational distributions in terms of their means µ and their unconstrained scale parameter γ, such that the scale parameter is log(1 + exp(γ)).
",4. Experiments,[0],[0]
4We focus on runtime cost.,4. Experiments,[0],[0]
"A&R requires O(N) memory storage capacity due to the local variational parameters.
",4. Experiments,[0],[0]
"5Code for A&R is available at https://github.com/ franrruiz/augment-reduce.
",4. Experiments,[0],[0]
"6A&R is amenable to an embarrassingly parallel algorithm, but we focus on single-core procedures.",4. Experiments,[0],[0]
We mimic the toy experiment of Titsias (2016) to assess how well A&R estimates the categorical probabilities.,4.1. Synthetic Dataset,[0],[0]
"We generate a dataset with 104 classes andN = 3×105 observations, each assigned label k with probability pk ∝ p̃2k, where each p̃k is randomly generated from a uniform distribution in [0, 1].",4.1. Synthetic Dataset,[0],[0]
"After generating the data, we have K = 9,035 effective classes (thus we use this value for K).",4.1. Synthetic Dataset,[0],[0]
"In this simple setting, there are no observed covariates xn.
",4.1. Synthetic Dataset,[0],[0]
We estimate the probabilities pk via maximum likelihood on the biases w(0)k .,4.1. Synthetic Dataset,[0],[0]
"We posit a softmax model, and we apply both the variational expectation maximization (VEM) in Section 3.1 and the OVE bound.",4.1. Synthetic Dataset,[0],[0]
"For both approaches, we choose a minibatch size of |B| = 500 observations and |S| = 100 classes, and we run 5× 105 iterations.
",4.1. Synthetic Dataset,[0],[0]
We run each approach on one CPU core.,4.1. Synthetic Dataset,[0],[0]
"On average, the wall-clock time per epoch (one epoch takes N/|B| = 600 iterations) is 0.196 minutes for softmax A&R and 0.189 minutes for OVE.",4.1. Synthetic Dataset,[0],[0]
"A&R is slightly slower because of the local step that OVE does not require; however, the bound on the marginal log likelihood is tighter (by orders of magnitude) for A&R than for OVE (−2.62×106 and−1.40×109, respectively).",4.1. Synthetic Dataset,[0],[0]
The estimated probabilities are similar for both methods: the average absolute error is 3.00 × 10−6 for A&R and 3.65 × 10−6 for OVE; the difference is not statistically significant.,4.1. Synthetic Dataset,[0],[0]
We now turn to real datasets.,4.2. Real Datasets,[0],[0]
"We consider MNIST and Bibtex (Katakis et al., 2008; Prabhu & Varma, 2014), where we can compare against the exact softmax.",4.2. Real Datasets,[0],[0]
"We also analyze Omniglot (Lake et al., 2015), EURLex-4K (Mencia & Furnkranz, 2008; Bhatia et al., 2015), and AmazonCat-13K (McAuley & Leskovec, 2013).7 Table 1 gives information about the structure of these datasets.
",4.2. Real Datasets,[0],[0]
We run each method for a fixed number of iterations.,4.2. Real Datasets,[0],[0]
We set the minibatch sizes |B| and |S| beforehand.,4.2. Real Datasets,[0],[0]
"The specific values for each dataset are also in Table 1.
Data preprocessing.",4.2. Real Datasets,[0],[0]
"For MNIST, we divide the pixel values by 255 so that the maximum value is one.",4.2. Real Datasets,[0],[0]
"For Omniglot, following other works in the literature (e.g., Burda et al., 2016), we resize the images to 28 × 28 pixels.",4.2. Real Datasets,[0],[0]
"For EURLex-4K and AmazonCat-13K, we normalize the covariates dividing by their maximum value.
",4.2. Real Datasets,[0],[0]
"Bibtex, EURLex-4K, and AmazonCat-13K are multi-class datasets, i.e., each observation may be assigned more than one label.",4.2. Real Datasets,[0],[0]
"Following Titsias (2016), we keep only the first non-zero label for each data point.",4.2. Real Datasets,[0],[0]
"See Table 1 for the resulting number of classes in each case.
",4.2. Real Datasets,[0],[0]
Evaluation.,4.2. Real Datasets,[0],[0]
"For the softmax, we compare A&R against the OVE bound.8",4.2. Real Datasets,[0],[0]
"We also compare against the exact softmax on MNIST and Bibtex, where the number of classes is small.",4.2. Real Datasets,[0],[0]
"For the multinomial probit and multinomial logistic models, we also report the predictive performance of A&R.
We evaluate performance with test log likelihood and accuracy.",4.2. Real Datasets,[0],[0]
"The accuracy is the fraction of correctly classified instances, assuming that we assign the most likely label (i.e., the one with the highest mean utility).",4.2. Real Datasets,[0],[0]
"To compute the test log likelihood, we use Eq. 1 for the softmax and Eq. 5 for the multinomial probit and multinomial logistic models.",4.2. Real Datasets,[0],[0]
"We approximate the integral in Eq. 5 with 1,000 samples using importance sampling (we use a Gaussian distribution with mean 5 and standard deviation 5 as a proposal).
",4.2. Real Datasets,[0],[0]
Results.,4.2. Real Datasets,[0],[0]
Table 2 shows the wall-clock time per epoch for each method and dataset.,4.2. Real Datasets,[0],[0]
"In general, softmax A&R is almost as fast as OVE because the extra local step can be performed efficiently without additional expensive operations.",4.2. Real Datasets,[0],[0]
It requires to evaluate exponential functions that can be reused in the global step.,4.2. Real Datasets,[0],[0]
"Multinomial probit A&R and multinomial
7MNIST is available at http://yann.lecun.com/ exdb/mnist.",4.2. Real Datasets,[0],[0]
Omniglot can be found at https://github.,4.2. Real Datasets,[0],[0]
com/brendenlake/omniglot.,4.2. Real Datasets,[0],[0]
"Bibtex, EURLex-4K, and AmazonCat-13K are available at http://manikvarma.org/ downloads/XC/XMLRepository.html.
",4.2. Real Datasets,[0],[0]
"8We also implemented the approach of Botev et al. (2017), but we do not report the results because it did not outperform OVE in terms of test log-likelihood on four out of the five considered datasets.",4.2. Real Datasets,[0],[0]
"On the fifth dataset, softmax A&R was still superior.
",4.2. Real Datasets,[0],[0]
Table 1.,4.2. Real Datasets,[0],[0]
Statistics and experimental settings of the considered datasets.,4.2. Real Datasets,[0],[0]
Ntrain and Ntest are the number of training and test data points.,4.2. Real Datasets,[0],[0]
The number of classes is the resulting value after the preprocessing step (see text).,4.2. Real Datasets,[0],[0]
"The minibatch sizes correspond to |B| and |S|, respectively.
",4.2. Real Datasets,[0],[0]
"dataset
MNIST Bibtex
Omniglot EURLex-4K
AmazonCat-13K
Ntrain Ntest covariates classes
60, 000 10, 000 784 10 4, 880 2, 413 1, 836 148 25, 968 6, 492 784 1, 623 15, 539 3, 809 5, 000 896 1, 186, 239 306, 782 203, 882 2,",4.2. Real Datasets,[0],[0]
"919
minibatch (obs.) minibatch (classes) iterations
500 1 35, 000 488 20 5, 000 541 50 45, 000 379 50 100, 000 1, 987 60 5, 970
Table 2.",4.2. Real Datasets,[0],[0]
Average time per epoch for each method and dataset.,4.2. Real Datasets,[0],[0]
Softmax A&R (Section 3.1) is almost as fast as OVE.,4.2. Real Datasets,[0],[0]
"The A&R approaches in Section 3.2 take longer because they require some additional computations, but they are still competitive.
",4.2. Real Datasets,[0],[0]
"dataset
MNIST Bibtex
Omniglot EURLex-4K
AmazonCat-13K
OVE (Titsias, 2016)
0.336 s 0.181 s 4.47 s 5.54 s 2.80 h
A&R [this paper] softmax multi. probit multi. logistic
0.337 s 0.431 s 0.511 s 0.188 s 0.244 s 0.246 s 4.65 s 5.63 s 5.57 s 5.65 s 6.46 s 6.23 s 2.80 h 2.82 h 2.91 h
logistic A&R are slightly slower because of the local step, but they are still competitive.
",4.2. Real Datasets,[0],[0]
"For the five datasets, Figure 2 shows the evolution of the evidence lower bound (ELBO) as a function of wall-clock time for the softmax A&R (Eq. 8), compared to the OVE (Eq. 10).",4.2. Real Datasets,[0],[0]
"For easier visualization, we plot a smoothed version of the bounds after applying a moving average window of size 100.",4.2. Real Datasets,[0],[0]
"(For AmazonCat-13K, we only compute the ELBO every 50 iterations and we use a window of size 5.)",4.2. Real Datasets,[0],[0]
"Softmax A&R provides a significantly tighter bound for most datasets (except for Bibtex, where the ELBO of A&R is close to the OVE bound).",4.2. Real Datasets,[0],[0]
"For MNIST and Bibtex, we also plot the marginal likelihood obtained after running maximum likelihood estimation on the exact softmax model.",4.2. Real Datasets,[0],[0]
"The ELBO of A&R nearly achieves this value.
",4.2. Real Datasets,[0],[0]
"Finally, Table 3 shows the predictive performance for all methods across all datasets.",4.2. Real Datasets,[0],[0]
We report test log likelihood and accuracy.,4.2. Real Datasets,[0],[0]
Softmax A&R outperforms OVE in both metrics on all but one dataset (except EURLex-4K).,4.2. Real Datasets,[0],[0]
"Although our goal is not to compare performance across different models, for completeness Table 3 also shows the predictive performance of multinomial probit A&R and multinomial logistic A&R.",4.2. Real Datasets,[0],[0]
"In general, softmax A&R provides the highest
test log likelihood, but multinomial probit A&R outperforms all other methods in EURLex-4K and AmazonCat-13K.",4.2. Real Datasets,[0],[0]
"Additionally, multinomial logistic A&R presents better predictive performance than OVE on Omniglot and Bibtex.",4.2. Real Datasets,[0],[0]
"We have introduced augment and reduce (A&R), a scalable method to fit models involving categorical distributions.",5. Conclusion,[0],[0]
"A&R is general and applicable to many models, including the softmax and the multinomial probit.",5. Conclusion,[0],[0]
"On classification tasks, we found that A&R outperforms state-of-the art algorithms with little extra computational cost.",5. Conclusion,[0],[0]
"This work was supported by ONR N00014-15-1-2209, ONR 133691-5102004, NIH 5100481-5500001084, NSF CCF1740833, the Alfred P. Sloan Foundation, the John Simon Guggenheim Foundation, Facebook, Amazon, and IBM.",Acknowledgements,[0],[0]
"Francisco J. R. Ruiz is supported by the EU Horizon 2020 programme (Marie Skłodowska-Curie Individual Fellowship, grant agreement 706760).",Acknowledgements,[0],[0]
We also thank Victor Elvira and Pablo Moreno for their comments and help.,Acknowledgements,[0],[0]
"Categorical distributions are ubiquitous in machine learning, e.g., in classification, language models, and recommendation systems.",abstractText,[0],[0]
"However, when the number of possible outcomes is very large, using categorical distributions becomes computationally expensive, as the complexity scales linearly with the number of outcomes.",abstractText,[0],[0]
"To address this problem, we propose augment and reduce (A&R), a method to alleviate the computational complexity.",abstractText,[0],[0]
A&R uses two ideas: latent variable augmentation and stochastic variational inference.,abstractText,[0],[0]
It maximizes a lower bound on the marginal likelihood of the data.,abstractText,[0],[0]
"Unlike existing methods which are specific to softmax, A&R is more general and is amenable to other categorical models, such as multinomial probit.",abstractText,[0],[0]
"On several large-scale classification problems, we show that A&R provides a tighter bound on the marginal likelihood and has better predictive performance than existing approaches.",abstractText,[0],[0]
Augment and Reduce: Stochastic Inference for Large Categorical Distributions,title,[0],[0]
"The problem of learning mappings between domains from unpaired data has recently received increasing attention, especially in the context of image-to-image translation (Zhu et al., 2017a; Kim et al., 2017; Liu et al., 2017).",1. Introduction,[0],[0]
"This problem is important because, in some cases, paired information may be scarce or otherwise difficult to obtain.",1. Introduction,[0],[0]
"For example, consider tasks like face transfiguration (male to female), where obtaining explicit pairs would be difficult as it would require artistic authoring.",1. Introduction,[0],[0]
"An effective unsupervised model may help when learning from relatively few paired examples, as compared to training strictly from the paired examples.",1. Introduction,[0],[0]
"Intuitively, forcing inter-domain mappings to be (approximately) invertible by a model of limited capacity acts as a strong regularizer.
",1. Introduction,[0],[0]
"Motivated by the success of Generative Adversarial Networks (GANs) in image generation (Goodfellow et al., 2014; Radford et al., 2015), existing unsupervised mapping meth-
1Montreal Institute for Learning Algorithms (MILA), Canada.",1. Introduction,[0],[0]
"2Microsoft Research Montreal, Canada.",1. Introduction,[0],[0]
3CIFAR Fellow.,1. Introduction,[0],[0]
†Work,1. Introduction,[0],[0]
partly done at MSR Montreal.,1. Introduction,[0],[0]
Correspondence to: Amjad Almahairi,1. Introduction,[0],[0]
"<amjad.almahairi@umontreal.ca>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"ods such as CycleGAN (Zhu et al., 2017a) learn a generator which produces images in one domain given images from the other.",1. Introduction,[0],[0]
"Without the use of pairing information, there are many possible mappings that could be inferred.",1. Introduction,[0],[0]
"To reduce the space of the possible mappings, these models are typically trained with a cycle-consistency constraint which enforces a strong connection across domains, by requiring that mapping an image from the source domain to the target domain and then back to source will result in the same starting image.",1. Introduction,[0],[0]
"This framework has been shown to learn convincing mappings across image domains and proved successful in a variety of related applications (Tung et al., 2017; Wolf et al., 2017; Hoffman et al., 2017).
",1. Introduction,[0],[0]
"One major limitation of CycleGAN is that it only learns one-to-one mappings, i.e. the model associates each input image with a single output image.",1. Introduction,[0],[0]
"We believe that most relationships across domains are more complex, and better characterized as many-to-many.",1. Introduction,[0],[0]
"For example, consider mapping silhouettes of shoes to images of shoes.",1. Introduction,[0],[0]
"While the mapping that CycleGAN learns can be superficially convincing (e.g. it produces a single reasonable shoe with a particular style), we would like to learn a mapping that can capture diversity of the output (e.g. produces multiple shoes with different styles).",1. Introduction,[0],[0]
The limits of one-to-one mappings are more dramatic when the source domain and target domain substantially differ.,1. Introduction,[0],[0]
"For instance, it would be difficult to learn a CycleGAN model when the two domains are descriptive facial attributes and images of faces.
",1. Introduction,[0],[0]
We propose a model for learning many-to-many mappings between domains from unpaired data.,1. Introduction,[1.0],['We propose a model for learning many-to-many mappings between domains from unpaired data.']
"Specifically, we “augment” each domain with auxiliary latent variables and extend CycleGAN’s training procedure to the augmented spaces.",1. Introduction,[0],[0]
"The mappings in our model take as input a sample from the source domain and a latent variable, and output both a sample in the target domain and a latent variable (Fig. 1b).",1. Introduction,[0],[0]
"The learned mappings are one-to-one in the augmented space, but many-to-many in the original domains after marginalizing over the latent variables.
",1. Introduction,[0],[0]
Our contributions are as follows.,1. Introduction,[0],[0]
(i) We introduce the Augmented CycleGAN model for learning many-to-many mappings across domains in an unsupervised way.,1. Introduction,[0],[0]
(ii) We show that our model can learn mappings which produce a diverse set of outputs for each input.,1. Introduction,[0],[0]
"(iii) We show that our model can learn mappings across substantially different domains, and we apply it in a semi-supervised setting for mapping between faces and attributes with competitive results.",1. Introduction,[1.0],"['(iii) We show that our model can learn mappings across substantially different domains, and we apply it in a semi-supervised setting for mapping between faces and attributes with competitive results.']"
"Given two domains A and B, we assume there exists a mapping, potentially many-to-many, between their elements.",2.1. Problem Setting,[0],[0]
The objective is to recover this mapping using unpaired samples from distributions pd(a) and pd(b) in each domain.,2.1. Problem Setting,[0],[0]
This can be formulated as a conditional generative modeling task where we try to estimate the true conditionals p(a|b) and p(b|a) using samples from the true marginals.,2.1. Problem Setting,[0],[0]
"An important assumption here is that elements in domains A and B are highly dependent; otherwise, it is unlikely that the model would uncover a meaningful relationship without any pairing information.",2.1. Problem Setting,[0],[0]
"The CycleGAN model (Zhu et al., 2017a) estimates these conditionals using two mappings GAB : A 7→ B and GBA : B 7→ A, parameterized by neural networks, which satisfy the following constraints:
1.",2.2. CycleGAN Model,[0.9999999777807282],"['The CycleGAN model (Zhu et al., 2017a) estimates these conditionals using two mappings GAB : A 7→ B and GBA : B 7→ A, parameterized by neural networks, which satisfy the following constraints: 1.']"
"Marginal matching: The output of each mapping should match the empirical distribution of the target domain, when marginalized over the source domain.
2.",2.2. CycleGAN Model,[0],[0]
"Cycle-consistency: Mapping an element from one domain to the other, and then back, should produce a sample close to the original element.
",2.2. CycleGAN Model,[0.9999998964788713],"['Cycle-consistency: Mapping an element from one domain to the other, and then back, should produce a sample close to the original element.']"
"Marginal matching in CycleGAN is achieved using the generative adversarial networks framework (GAN) (Goodfellow et al., 2014).",2.2. CycleGAN Model,[0],[0]
"Mappings GAB and GBA are given by neural networks trained to fool adversarial discriminators DB and
DA, respectively.",2.2. CycleGAN Model,[0],[0]
"Enforcing marginal matching on target domain B, marginalized over source domain A, involves minimizing an adversarial objective with respect to GAB :
LBGAN(GAB , DB) = E b∼pd(b)
[ logDB(b) ]",2.2. CycleGAN Model,[0],[0]
"+
E a∼pd(a)
[ log(1−DB(GAB(a)))",2.2. CycleGAN Model,[0],[0]
"] ,
(1)",2.2. CycleGAN Model,[0],[0]
while the discriminatorDB is trained to maximize it.,2.2. CycleGAN Model,[0],[0]
"A similar adversarial loss LAGAN(GBA, DA) is defined for marginal matching in the reverse direction.
",2.2. CycleGAN Model,[0.9999999641301675],"['A similar adversarial loss LAGAN(GBA, DA) is defined for marginal matching in the reverse direction.']"
"Cycle-consistency enforces that, when starting from a sample a from A, the reconstruction a′ = GBA(GAB(a)) remains close to the original a. For image domains, closeness between a and a′ is typically measured with L1 or L2 norms.",2.2. CycleGAN Model,[0],[0]
"When using the L1 norm, cycle-consistency starting from A can be formulated as:
LACYC(GAB , GBA) = E a∼pd(a) ∥∥GBA(GAB(a))− a∥∥1.",2.2. CycleGAN Model,[0],[0]
"(2) And similarly for cycle-consistency starting from B. The full CycleGAN objective is given by:
LAGAN(GBA, DA) + LBGAN(GAB , DB) + γLACYC(GAB , GBA) + γLBCYC(GAB , GBA),
(3)
where γ is a hyper-parameter that balances between marginal matching and cycle-consistency.
",2.2. CycleGAN Model,[0],[0]
The success of CycleGAN can be attributed to the complementary roles of marginal matching and cycle-consistency in its objective.,2.2. CycleGAN Model,[0],[0]
Marginal matching encourages generating realistic samples in each domain.,2.2. CycleGAN Model,[0],[0]
Cycle-consistency encourages a tight relationship between domains.,2.2. CycleGAN Model,[0],[0]
"It may also help prevent multiple items from one domain mapping to a single item from the other, analogous to the troublesome mode collapse in adversarial generators (Li et al., 2017).",2.2. CycleGAN Model,[0],[0]
A fundamental weakness of the CycleGAN model is that it learns deterministic mappings.,2.3. Limitations of CycleGAN,[0],[0]
"In CycleGAN, and in other similar models (Kim et al., 2017; Yi et al., 2017), the conditionals between domains correspond to delta functions: p̂(a|b) = δ(GBA(b)) and p̂(b|a) = δ(GAB(a)), and cycleconsistency forces the learned mappings to be inverses of each other.",2.3. Limitations of CycleGAN,[1.0],"['In CycleGAN, and in other similar models (Kim et al., 2017; Yi et al., 2017), the conditionals between domains correspond to delta functions: p̂(a|b) = δ(GBA(b)) and p̂(b|a) = δ(GAB(a)), and cycleconsistency forces the learned mappings to be inverses of each other.']"
"When faced with complex cross-domain relationships, this results in CycleGAN learning an arbitrary one-toone mapping instead of capturing the true, structured conditional distribution more faithfully.",2.3. Limitations of CycleGAN,[1.0],"['When faced with complex cross-domain relationships, this results in CycleGAN learning an arbitrary one-toone mapping instead of capturing the true, structured conditional distribution more faithfully.']"
"Deterministic mappings are also an obstacle to optimizing cycle-consistency when the domains differ substantially in complexity, in which case mapping from one domain (e.g. class labels) to the other (e.g. real images) is generally one-to-many.",2.3. Limitations of CycleGAN,[1.0],"['Deterministic mappings are also an obstacle to optimizing cycle-consistency when the domains differ substantially in complexity, in which case mapping from one domain (e.g. class labels) to the other (e.g. real images) is generally one-to-many.']"
"Next, we dis-
cuss how to extend CycleGAN to capture more expressive relationships across domains.",2.3. Limitations of CycleGAN,[0],[0]
A straightforward approach for extending CycleGAN to model many-to-many relationships is to equip it with stochastic mappings between A and B. Let Z be a latent space with a standard Gaussian prior p(z) over its elements.,2.4. CycleGAN with Stochastic Mappings,[0],[0]
We define mappings GAB : A × Z 7→ B and GBA : B × Z 7→ A1.,2.4. CycleGAN with Stochastic Mappings,[0],[0]
"Each mapping takes as input a vector of auxiliary noise and a sample from the source domain, and generates a sample in the target domain.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"Therefore, by sampling different z ∼ p(z), we could in principle generate multiple b’s conditioned on the same a and vice-versa.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"We can write the marginal matching loss on domain B as:
LBGAN(GAB , DB) = E b∼pd(b)
[ logDB(b) ]",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"+
E a∼pd(a) z∼p(z)
",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"[ log(1−DB(GAB(a, z))) ] .
(4) Cycle-consistency starting from A is now given by:
LACYC(GAB , GBA) = E a∼pd(a)
z1,z2∼p(z) ∥∥GBA(GAB(a, z1), z2)− a∥∥1 (5)
",2.4. CycleGAN with Stochastic Mappings,[0],[0]
The full training loss is similar to the objective in Eqn. 3.,2.4. CycleGAN with Stochastic Mappings,[0],[0]
"We refer to this model as Stochastic CycleGAN.
",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"In principle, stochastic mappings can model multi-modal conditionals, and hence generate a richer set of outputs than deterministic mappings.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"However, Stochastic CycleGAN suffers from a fundamental flaw: the cycle-consistency in Eq. 5 encourages the mappings to ignore the latent z. Specifically, the unimodality assumption implicit in the reconstruction error from Eq. 5 forces the mapping GBA to be manyto-one when cycling A→ B → A′, since any b generated for a given a must map to a′ = GBA(b, z)",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"≈ a, for all z.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"For the cycle B → A → B′, GAB is similarly forced to be many-to-one.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
The only way for to GBA and GAB to be both many-to-one and mutual inverses is if they collapse to being (roughly) one-to-one.,2.4. CycleGAN with Stochastic Mappings,[1.0],['The only way for to GBA and GAB to be both many-to-one and mutual inverses is if they collapse to being (roughly) one-to-one.']
We could possibly mitigate this degeneracy by introducing a VAE-like encoder and exchanging the L1 error in Eq. 5 for a more complex variational bound on conditional log-likelihood.,2.4. CycleGAN with Stochastic Mappings,[0],[0]
"In the next section, we discuss an alternative approach to learning complex, stochastic mappings between domains.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"In order to learn many-to-many mappings across domains, we propose to learn to map between pairs of items (a, zb) ∈
1To avoid clutter in notation, we reuse the same symbols of deterministic mappings.
",3. Approach,[0],[0]
"A× Zb and (b, za) ∈ B × Za, where Za and Zb are latent spaces that capture any missing information when transforming an element from A to B, and vice-versa.",3. Approach,[0],[0]
"For example, when generating a female face (b ∈ B) which resembles a male face (a ∈ A), the latent code zb ∈",3. Approach,[0],[0]
"Zb can capture female face variations (e.g. hair length or style) independent from a. Similarly, za ∈ Za captures variations in a generated male face independent from the given female face.",3. Approach,[0],[0]
"This approach can be described as learning mappings between augmented spaces A× Zb and B × Za (Figure 1b); hence, we call it Augmented CycleGAN.",3. Approach,[0],[0]
"By learning to map a pair (a, zb) ∈",3. Approach,[0],[0]
"A × Zb to (b, za) ∈ B × Za, we can (i) learn a stochastic mapping from a to multiple items in B by sampling different zb ∈",3. Approach,[0],[0]
"Zb, and (ii) infer latent codes za containing information about a not captured in the generated b, which allows for doing proper reconstruction of a. As a result, we are able to optimize both marginal matching and cycle consistency while using stochastic mappings.",3. Approach,[0],[0]
We present details of our approach in the next sections.,3. Approach,[0],[0]
2,3. Approach,[0],[0]
Our proposed model has four components.,3.1. Augmented CycleGAN,[0],[0]
"First, the two mappings GAB : A × Zb 7→ B and GBA : B × Za 7→",3.1. Augmented CycleGAN,[0],[0]
"A, which are the conditional generators of items in each domain.",3.1. Augmented CycleGAN,[0],[0]
These models are similar to those used in Stochastic CycleGAN.,3.1. Augmented CycleGAN,[0],[0]
We also have two encoders EA :,3.1. Augmented CycleGAN,[0],[0]
A×B,3.1. Augmented CycleGAN,[0],[0]
"7→ Za and EB : A × B 7→ Zb, which enable optimization of cycle-consistency with stochastic, structured mappings.",3.1. Augmented CycleGAN,[0],[0]
All components are parameterized with neural networks – see Fig. 2.,3.1. Augmented CycleGAN,[0],[0]
We define mappings over augmented spaces in our model as follows.,3.1. Augmented CycleGAN,[0],[0]
"Let p(za) and p(zb) be standard Gaussian priors over Za and Zb, which are independent from pd(b) and pd(a).",3.1. Augmented CycleGAN,[0],[0]
"Given a pair (a, zb) ∼ pd(a)p(zb), we generate a pair (b̃, z̃a) as follows:
b̃ = GAB(a, zb), z̃a = EA(a, b̃).",3.1. Augmented CycleGAN,[0],[0]
"(6)
That is, we first generate a sample in domain B, then we use it along with a to generate latent code z̃a.",3.1. Augmented CycleGAN,[0],[0]
"Note here that by sampling different zb ∼ p(zb), we can generate multiple b̃’s conditioned on the same a. In addition, given the pair (a, b̃), we can recover information about a which is not captured in b̃, via z̃a.",3.1. Augmented CycleGAN,[0],[0]
"Similarly, given a pair (b, za) ∼ pd(b)p(za), we generate a pair (ã, z̃b) as follows:
ã = GBA(b, za), z̃b = EB(b, ã).",3.1. Augmented CycleGAN,[1.0000000199507575],"['Similarly, given a pair (b, za) ∼ pd(b)p(za), we generate a pair (ã, z̃b) as follows: ã = GBA(b, za), z̃b = EB(b, ã).']"
"(7)
Learning in Augmented CycleGAN follows a similar approach to CycleGAN – optimizing both marginal matching and cycle-consistency losses, albeit over augmented spaces.
",3.1. Augmented CycleGAN,[0],[0]
"2Our model captures many-to-many relationships because it captures both one-to-many and many-to-one: one item in A maps to many items in B, and many items in B map to one item in A (cycle).",3.1. Augmented CycleGAN,[0],[0]
"The same is true in the other direction.
",3.1. Augmented CycleGAN,[0],[0]
"Marginal Matching Loss We adopt an adversarial approach for marginal matching over B × Za where we use two independent discriminators DB and DZa to match generated pairs to real samples from the independent priors pd(b) and p(za), respectively.",3.1. Augmented CycleGAN,[0],[0]
Marginal matching loss over B is defined as in Eqn 4.,3.1. Augmented CycleGAN,[0],[0]
"Marginal matching over Za is given by:
LZaGAN(EA, GAB , DZa)",3.1. Augmented CycleGAN,[0],[0]
"= E za∼p(za)
",3.1. Augmented CycleGAN,[0],[0]
[ logDZa(za) ],3.1. Augmented CycleGAN,[0],[0]
"+
E a∼pd(a) zb∼p(zb)
",3.1. Augmented CycleGAN,[0],[0]
"[ log(1−DZa(z̃a)) ] ,
(8)
where z̃a is defined by Eqn 6.",3.1. Augmented CycleGAN,[0],[0]
"As in CycleGAN, the goal of marginal matching over B is to insure that generated samples b̃ are realistic.",3.1. Augmented CycleGAN,[0],[0]
"For latent codes z̃a, marginal matching acts as a regularizer for the encoder, encouraging the marginalized encoding distribution to match a simple prior p(za).",3.1. Augmented CycleGAN,[0],[0]
"This is similar to adversarial regularization of latent codes in adversarial autoencoders (Makhzani et al., 2016).",3.1. Augmented CycleGAN,[0],[0]
"We define similar losses LAGAN(GBA, DA) and LZbGAN(EB , GBA, DZb) for marginal matching over A×Zb.
",3.1. Augmented CycleGAN,[0],[0]
"Cycle Consistency Loss We define two cycle-consistency constraints in Augmented CycleGAN starting from each of the two augmented spaces, as shown in Fig. 2.",3.1. Augmented CycleGAN,[0],[0]
"In cycleconsistency starting from A × Zb, we ensure that given a pair (a, zb) ∼ pd(a)p(zb), the model is able to produce a faithful reconstruction of it after being mapped to (b̃, z̃a).",3.1. Augmented CycleGAN,[0],[0]
"This is achieved with two losses; first for reconstructing a ∼ pd(a):
LACYC(GAB , GBA, EA) = E a∼pd(a) zb∼p(zb)
",3.1. Augmented CycleGAN,[0],[0]
"∥∥a′ − a∥∥ 1 ,
b̃ = GAB(a, zb), z̃a = EA(a, b̃), a ′ = GBA(b̃, z̃a).",3.1. Augmented CycleGAN,[0],[0]
"(9)
The second is for reconstructing zb ∼ p(zb):
LZbCYC(GAB , EB) = E a∼pd(a) zb∼p(zb)",3.1. Augmented CycleGAN,[0],[0]
∥∥z′b,3.1. Augmented CycleGAN,[0],[0]
"− zb∥∥1, z′b = EB(a, b̃), b̃ = GAB(a, zb).",3.1. Augmented CycleGAN,[0],[0]
"(10)
These reconstruction costs represent an autoregressive decomposition of the basic CycleGAN cycle-consistency cost from Eq. 2, after extending it to the augmented domains.",3.1. Augmented CycleGAN,[0],[0]
"Specifically, we decompose the required reconstruction distribution p(b, za|a, zb) into the conditionals p(b|a, zb) and p(za|a, zb, b).
",3.1. Augmented CycleGAN,[0],[0]
"Just like in CycleGAN, the cycle loss in Eqn. 9 enforces the dependency of generated samples in B on samples of A. Thanks to the encoder EA, the model is able to reconstruct a because it can recover information loss in generated b̃ through z̃a.",3.1. Augmented CycleGAN,[0],[0]
"On the other hand, the cycle loss in Eqn. 10 enforces the dependency of a generated sample b̃ on the given latent code zb.",3.1. Augmented CycleGAN,[0],[0]
"In effect, it increases the mutual information between zb and b conditioned on a, i.e. I(b, zb|a) (Chen et al., 2016; Li et al., 2017).
",3.1. Augmented CycleGAN,[0],[0]
"Training Augmented CycleGAN in the direction A× Zb to B × Za is done by optimizing:
LBGAN(DB , GAB)",3.1. Augmented CycleGAN,[0],[0]
+,3.1. Augmented CycleGAN,[0],[0]
"L za GAN(DZa , EA, GAB) + γ1LACYC(GAB , GBA, EA) + γ2L zb CYC(GAB , EB), (11)
where γ1 and γ2 are a hyper-parameters used to balance objectives.",3.1. Augmented CycleGAN,[0],[0]
"We define a similar objective for the direction going from B × Za to A× Zb, and train the model on both objectives simultaneously.",3.1. Augmented CycleGAN,[0],[0]
"In cases where we have access to paired data, we can leverage it to train our model in a semi-supervised setting (Fig. 3).",3.2. Semi-supervised Learning with Augmented CycleGAN,[0],[0]
"Given pairs sampled from the true joint,
i.e. (a, b) ∼ pd(a, b), we can define a supervision cost for the mapping GAB as follows:
LASUP(GBA, EA) = E (a,b)∼pd(a,b) ∥∥GBA(b, z̃a)− a∥∥1, (12)
where z̃a = EA(a, b) infers a latent code which can produce a given b via GBA(b, z̃a).",3.2. Semi-supervised Learning with Augmented CycleGAN,[0],[0]
"We also apply an adversarial regularization cost on the encoder, in the form of Eqn. 8.",3.2. Semi-supervised Learning with Augmented CycleGAN,[0],[0]
"Similar supervision and regularization costs can be defined for GBA and EB , respectively.",3.2. Semi-supervised Learning with Augmented CycleGAN,[0],[0]
We note here some design choices that we found important for training our stochastic mappings.,3.3. Modeling Stochastic Mappings,[0],[0]
We discuss architectural and training details further in Sec. 5.,3.3. Modeling Stochastic Mappings,[0],[0]
"In order to allow the latent codes to capture diversity in generated samples, we found it important to inject latent codes to layers of the network which are closer to the inputs.",3.3. Modeling Stochastic Mappings,[0],[0]
"This allows the injected codes to be processed with a larger number of remaining layers and therefore capture high-level variations of the output, as opposed to small pixel-level variations.",3.3. Modeling Stochastic Mappings,[0],[0]
"We also found that Conditional Normalization (CN) (Dumoulin et al.; Perez et al., 2017) for conditioning layers can be more effective than concatenation, which is more commonly used (Radford et al., 2015; Zhu et al., 2017b).",3.3. Modeling Stochastic Mappings,[0],[0]
"The basic idea of CN is to replace parameters of affine transformations in normalization layers (Ioffe & Szegedy, 2015) of a neural network with a learned function of the conditioning information.",3.3. Modeling Stochastic Mappings,[0],[0]
"We apply CN by learning two linear functions f and g which take a latent code z as input and output scale and shift parameters of normalization layers in intermediate layers, i.e. γ = f(z) and β = g(z).",3.3. Modeling Stochastic Mappings,[0],[0]
"When activations are normalized over spatial dimensions only, we get Conditional Instance Normalization (CIN), and when they are also normalized over batch dimension, we get Conditional Batch Normalization (CBN).",3.3. Modeling Stochastic Mappings,[0],[0]
"There has been a surge of interest recently in unsupervised learning of cross-domain mappings, especially for image translation tasks.",4. Related Work,[0],[0]
Previous attempts for image-to-image translation have unanimously relied on GANs to learn mappings that produce compelling images.,4. Related Work,[0],[0]
"In order to constrain learned mappings, some methods have relied on cycleconsistency based constraints similar to CycleGAN (Kim et al., 2017; Yi et al., 2017; Royer et al., 2017), while others relied on weight sharing constraints (Liu & Tuzel, 2016; Liu et al., 2017).",4. Related Work,[0],[0]
"However, the focus in all of these methods was on learning conditional image generators that produce single output images given the input image.",4. Related Work,[0],[0]
"Notably, Liu et al. (2015) propose to map inputs from both domains into a
shared latent space.",4. Related Work,[0],[0]
"This approach may constrain too much the space of learnable mappings, for example in cases where the domains differ substantially (class labels and images).
",4. Related Work,[0],[0]
"Unsupervised learning of mappings have also been addressed recently in language translation, especially for machine translation (Lample et al., 2017) and text style transfer (Shen et al., 2017).",4. Related Work,[0],[0]
These methods also rely on some notion of cycle-consistency over domains in order to constrain the learned mappings.,4. Related Work,[0],[0]
They rely heavily on the power of the RNN-based decoders to capture complex relationships across domains while we propose to use auxiliary latent variables.,4. Related Work,[0],[0]
"The two approaches may be synergistic, as it was recently suggested in (Gulrajani et al., 2016).
",4. Related Work,[0],[0]
"Recently, Zhu et al. (2017b) proposed the BiCycleGAN model for learning multi-modal mappings but in fully supervised setting.",4. Related Work,[0],[0]
"This model extends the pix2pix framework in (Isola et al., 2017) by learning a stochastic mapping from the source to the target, and shows interesting diversity in the generated samples.",4. Related Work,[0],[0]
"Several modeling choices in BiCycleGAN resemble our proposed model, including the use of stochastic mappings and an encoder to handle multi-modal targets.",4. Related Work,[0],[0]
"However, our approach focuses on unsupervised many-to-many mappings, which allows it to handle domains with no or very little paired data.",4. Related Work,[0],[0]
"We first study a one-to-many image translation task between edges (domain A) and photos of shoes (domain B).3 Training data is composed of almost 50K shoe images with corresponding edges (Yu & Grauman, 2014; Zhu et al., 2016; Isola et al., 2017), but as in previous approaches (e.g. (Kim et al., 2017)), we assume no pairing information while training unsupervised models.",5.1. Edges-to-Photos,[0],[0]
"Stochastic mappings in our Augmented CycleGAN (AugCGAN) model are based on ResNet conditional image generators of (Zhu et al., 2017a), where we inject noise with CIN to all intermediate layers.",5.1. Edges-to-Photos,[0],[0]
"As baselines, we train: CycleGAN, Stochastic CycleGAN (StochCGAN) and Triangle-GAN (∆-GAN) of (Gan et al., 2017) which share the same architectures and training procedure for fair comparison.",5.1. Edges-to-Photos,[0],[0]
"4
Quantitative Results First, we evaluate conditionals learned by each model by measuring the ability of the model of generating a specific edge-shoe pair from a test set.",5.1. Edges-to-Photos,[0],[0]
"We follow the same evaluation methodology adopted in (Metz et al., 2016; Xiang & Li, 2017), which opt for an
3 Public code available at: https://github.com/ aalmah/augmented_cyclegan
4∆-GAN architecture differs only in the two discriminators, which match conditionals/joints instead of marginals.
inference-via-optimization approach to estimate the reconstruction error of a specific shoe given an edge.",5.1. Edges-to-Photos,[0],[0]
"Specifically, given a trained model with mapping GAB and an edgeshoe pair (a, b) in the test set, we solve the optimization task z∗b = arg minzb ‖GAB(a, zb)−b‖1 and compute reconstruction error ‖GAB(a, z∗b )",5.1. Edges-to-Photos,[0.9982682081990465],"['Specifically, given a trained model with mapping GAB and an edgeshoe pair (a, b) in the test set, we solve the optimization task z∗b = arg minzb ‖GAB(a, zb)−b‖1 and compute reconstruction error ‖GAB(a, z∗b ) − b‖1.']"
− b‖1.,5.1. Edges-to-Photos,[0],[0]
"Optimization is done with RMSProp as in (Xiang & Li, 2017).",5.1. Edges-to-Photos,[1.0],"['Optimization is done with RMSProp as in (Xiang & Li, 2017).']"
"We show the average errors over a predefined test set of 200 samples in Table 1 for: AugCGAN (unsupervised and semi-supervised with 10% paired data), unsupervised CycleGAN and StochCGAN, and a semi-supervised ∆-GAN, all sharing the same architecture.",5.1. Edges-to-Photos,[0],[0]
"Our unsupervised AugCGAN model outperforms all baselines including semi-supervised ∆-GAN, which indicates that reconstruction-based cycle-consistency is more effective in learning conditionals than the adversarial approach of ∆-GAN.",5.1. Edges-to-Photos,[0],[0]
"As expected, adding 10% supervision to AugCGAN improves shoe predictions further.",5.1. Edges-to-Photos,[0],[0]
"In addition, we evaluate edge predictions given real shoes from test set as well.",5.1. Edges-to-Photos,[0],[0]
"We report mean squared error (MSE) similar to (Gan et al., 2017), where we normalize over all edge pixels.",5.1. Edges-to-Photos,[0],[0]
"The ∆-GAN model with our architecture outperforms
the one reported in (Gan et al., 2017), but is outperformed by our unsupervised AugCGAN model.",5.1. Edges-to-Photos,[0],[0]
"Again, adding 10% supervision to AugCGAN reduces MSE even further.
",5.1. Edges-to-Photos,[0],[0]
Qualitative Results We qualitatively compare the mappings learned by our model AugCGAN and StochCGAN.,5.1. Edges-to-Photos,[0],[0]
"Fig. 6 shows generated images of shoes given an edge a ∼ pd(a) (row) and zb ∼ p(zb) (column) from both model, and",5.1. Edges-to-Photos,[0],[0]
Fig. 5 shows cycles starting from edges and shoes.,5.1. Edges-to-Photos,[0],[0]
Note that here the edges are sampled from the data distribution and not produced by the learnt stochastic mapping GBA.,5.1. Edges-to-Photos,[0],[0]
"In this case, both models can (i) generate diverse set of shoes with color variations mostly defined by zb, and (ii) perform reconstructions of both edges and shoes.
",5.1. Edges-to-Photos,[0],[0]
"While we expect our model to achieve these results, the fact that StochCGAN can reconstruct shoes perfectly without an inference model may seem at first surprising.",5.1. Edges-to-Photos,[0],[0]
"However, this can be explained by the “steganography” behavior of CycleGAN (Chu et al., 2017):",5.1. Edges-to-Photos,[0],[0]
"the model hides in the generated edge ã imperceptible information about a given shoe b (e.g. its color), in order to satisfy cycle-consistency without being
penalized by the discriminator on A. A good model of the true conditionals p(b|a), p(a|b) should reproduce the hidden joint distribution and consequently the marginals by alternatively sampling from conditionals.",5.1. Edges-to-Photos,[0],[0]
"Therefore, we examine the behavior of the models when edges are generated from the model itself (instead of the empirical data distribution).",5.1. Edges-to-Photos,[0],[0]
"In Fig. 7, we plot multiple generated shoes given an edge generated by the model, i.e. ã, and 5 different zb sampled from p(zb).",5.1. Edges-to-Photos,[0],[0]
"In StochCGAN, the mapping GBA(ã, zb) collapses to a deterministic function generating a single shoe for every zb.",5.1. Edges-to-Photos,[1.0],"['In StochCGAN, the mapping GBA(ã, zb) collapses to a deterministic function generating a single shoe for every zb.']"
"This distinction between behaviour on real and synthetic data is undesirable, e.g. regularization benefits of using unpaired data may be reduced if the model slips into this regime switching style.",5.1. Edges-to-Photos,[0],[0]
"In AugCGAN, on the other hand, the mapping seem to closely capture the diversity in the conditional distribution of shoes given edges.",5.1. Edges-to-Photos,[1.0],"['In AugCGAN, on the other hand, the mapping seem to closely capture the diversity in the conditional distribution of shoes given edges.']"
"Furthermore, in Fig. 8, we run a Markov chain by generating from the learned mappings multiple times, starting from a real shoe.",5.1. Edges-to-Photos,[0],[0]
"Again AugCGAN produces diverse samples while StochCGAN seems to collapse to a single mode.
",5.1. Edges-to-Photos,[0],[0]
"We investigate “steganography” behavior in both AugCGAN and StochCGAN using a similar approach to (Chu et al., 2017), where we corrupt generated edges with noise sampled fromN (0, 2), and compute reconstruction error of shoes.",5.1. Edges-to-Photos,[0],[0]
Fig. 4 shows L1 reconstruction error as we increase .,5.1. Edges-to-Photos,[0],[0]
"AugCGAN seems more robust to corruption of edges than in StochCGAN, which confirms that information is being stored in the latent codes instead of being completely hidden in generated edges.",5.1. Edges-to-Photos,[0],[0]
We study another image translation task of translating between male and female faces.,5.2. Male-to-Female,[1.0],['We study another image translation task of translating between male and female faces.']
"Data is based on CelebA dataset (Liu et al., 2015) where we split it into two separate domains using provided attributes.",5.2. Male-to-Female,[0],[0]
"Several key features distinguish this task from other image-translation tasks: (i) there is no predefined correspondence in real data of each domain, (ii) the relationship is many-to-many between domains, as we can map a male to female face, and vice-versa, in many possible ways, and (iii) capturing realistic variations in generated faces requires transformations that go beyond simple color and texture changes.",5.2. Male-to-Female,[1.0],"['Several key features distinguish this task from other image-translation tasks: (i) there is no predefined correspondence in real data of each domain, (ii) the relationship is many-to-many between domains, as we can map a male to female face, and vice-versa, in many possible ways, and (iii) capturing realistic variations in generated faces requires transformations that go beyond simple color and texture changes.']"
"The architecture of stochastic mappings are based on U-NET conditional image generators of (Isola et al., 2017), and again with noise injected to all intermediate layers.",5.2. Male-to-Female,[0],[0]
Fig. 9 shows results of applying our model to this task on 128 × 128 resolution CelebA images.,5.2. Male-to-Female,[0.9923694300701785],['9 shows results of applying our model to this task on 128 × 128 resolution CelebA images.']
We can see that our model depicts meaningful variations in generated faces without compromising their realistic appearance.,5.2. Male-to-Female,[0],[0]
"In Fig. 10 we show 64 × 64 generated samples in both domains from our model ((a) and (b)), and compare them to both: (c) our model but with noise injected noise only in last 3 layers of the GAB’s
network, and (d) StochCGAN with the same architecture.",5.2. Male-to-Female,[0.9964536651368726],"['10 we show 64 × 64 generated samples in both domains from our model ((a) and (b)), and compare them to both: (c) our model but with noise injected noise only in last 3 layers of the GAB’s network, and (d) StochCGAN with the same architecture.']"
"We can see that in Fig. 10-(c) variations are very limited, which highlights the importance of processing latent code with multiple layers.",5.2. Male-to-Female,[0],[0]
"StochCGAN in this task produces almost no variations at all, which highlights the importance of proper optimization of cycle-consistency for capturing meaningful variations.",5.2. Male-to-Female,[0],[0]
"We verify these results quantitatively using LPIPS distance (Zhang et al., 2018), where we average distance between 1000 pairs of generated female faces (10 random pairs from 100 male faces).",5.2. Male-to-Female,[0],[0]
"AugCGAN (Fig. 10-(b)) achieves highest LPIPS diversity score with 0.108 ± 0.003, while AugCGAN with z in low-level layers (Fig. 10-(c)) gets 0.059 +/- 0.001, and finally StochCGAN (Fig. 10-(d)) gets 0.008 +/- 0.000, i.e. severe mode collapse.",5.2. Male-to-Female,[0],[0]
"In this task, we make use of the CelebA dataset in order map from descriptive facial attributes A to images of faces B and vice-versa.",5.3. Attributes-to-Faces,[1.0],"['In this task, we make use of the CelebA dataset in order map from descriptive facial attributes A to images of faces B and vice-versa.']"
We report both quantitative and qualitative results.,5.3. Attributes-to-Faces,[0],[0]
"For the quantitative results, we follow (Gan et al.,
2017) and test our models in a semi-supervised attribute prediction setting.",5.3. Attributes-to-Faces,[0],[0]
We let the model train on all the available data without the pairing information and only train with a small amount of paired data as described in Sec. 3.2.,5.3. Attributes-to-Faces,[0],[0]
We report Precision (P) and normalized Discounted Cumulative Gain (nDCG) as the two metrics for multi-label classification problems.,5.3. Attributes-to-Faces,[0],[0]
"As an additional baseline, we also train a supervised classifier (which has the same architecture as GBA) on the paired subset.",5.3. Attributes-to-Faces,[0],[0]
The results are reported in Table 3.,5.3. Attributes-to-Faces,[0],[0]
"In Fig. 11, we show some generation obtained from the model in the direction attributes to faces.",5.3. Attributes-to-Faces,[0],[0]
We can see that the model generates reasonable diverse faces for the same set of attributes.,5.3. Attributes-to-Faces,[0],[0]
In this paper we have introduced the Augmented CycleGAN model for learning many-to-many cross-domain mappings in unsupervised fashion.,6. Conclusion,[0],[0]
This model can learn stochastic mappings which leverage auxiliary noise to capture multimodal conditionals.,6. Conclusion,[1.0],['This model can learn stochastic mappings which leverage auxiliary noise to capture multimodal conditionals.']
Our experimental results verify quantitatively and qualitatively the effectiveness of our approach in image translation tasks.,6. Conclusion,[1.0],['Our experimental results verify quantitatively and qualitatively the effectiveness of our approach in image translation tasks.']
"Furthermore, we apply our model in a challenging task of learning to map across attributes and faces, and show that it can be used effectively in a semi-supervised learning setting.",6. Conclusion,[1.0],"['Furthermore, we apply our model in a challenging task of learning to map across attributes and faces, and show that it can be used effectively in a semi-supervised learning setting.']"
Authors would like to thank Zihang Dai for valuable discussions and feedback.,Acknowledgements,[0],[0]
We are also grateful for ICML anonymous reviewers for their comments.,Acknowledgements,[0],[0]
"Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data.",abstractText,[0],[0]
"CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one.",abstractText,[0],[0]
"This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings.",abstractText,[0],[0]
"We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains.",abstractText,[0],[0]
We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.,abstractText,[0],[0]
Augmented CycleGAN: Learning Many-to-Many Mappings  from Unpaired Data,title,[0],[0]
