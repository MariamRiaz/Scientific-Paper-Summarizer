19	16	This line of work (Liberty, 2013; Ghashami & Phillips, 2014; Ghashami et al., 2016; Ghashami et al., 2016) is the notable exception in numerical linear algebra, as it provides deterministic methods, although all such methods are specific to the 2-norm.
22	22	The core of the summary is to find rows of the original matrix which have high leverage scores.
31	26	In particular, we use it to solve the `∞-regression problem with additive error in a stream.
34	35	Given the relationship between the streaming model and the distributed model that we later define, this could be seen in the context of having data stored over multiple machines who could send ‘important’ rows of their data to a central coordinator in order to compute the approximation.
35	45	All our algorithms are deterministic polynomial time, and use significantly sublinear memory or communication in streaming and distributed models, respectively.
39	51	The key insight here is that rows of high norm in the full wellconditioned basis cannot have their norm decrease too much in a well-conditioned basis associated with a subblock; in fact they remain large up to a multiplicative poly(d) factor.
41	19	The space is nγ to obtain dO(1/γ) distortion, for γ ∈ (0, 1) a small constant.
45	38	Section 6 describes an algorithm for computing an additiveerror solution to the `∞-regression problem, and shows a corresponding lower bound, showing that relative error solutions in this norm are not possible in sublinear space, even for randomized algorithms.
54	23	Subspace embedding, regression and `1 low-rank approximation: various approaches using row-sampling (Cohen & Peng, 2015; Dasgupta et al., 2008), and data oblivious methods such as low-distortion embeddings can solve regression in time proportional to the sparsity of the input matrix (Clarkson et al., 2013; Meng & Mahoney, 2013; Song et al., 2017; Woodruff & Zhang, 2013).
79	48	In both models we measure the summary size (storage), the update time which is the time taken to find the local summary, and the query time which is the time taken to compute an approximation to P using the summary.
82	38	Further rows are then appended and the process is repeated until the full matrix has been read.
84	41	The Distributed Summary Model: Given a small constant γ ∈ (0, 1), the input in the form of matrix A ∈ Rn×d is partitioned into blocks among distributed compute nodes so that no block exceeds nγ rows.
92	29	As the methods require only light synchronization (compute summary and return to coordinator), we do not model implementation issues relating to synchronization.
99	48	Let R be a change of basis matrix such that AR is a well-conditioned basis for the column space of A.
100	15	The (full) `p-leverage scores are defined as wi = ‖eTi AR‖pp.
111	115	Define the local `p-leverage scores of Y with respect to X to be the leverage scores of rows Y found by computing a well-conditioned basis for Y rather than the whole matrix X .
112	178	A key technical insight to proving Theorem 3.3 below is that rows of high leverage globally can be found by repeatedly finding rows of local high leverage.
114	32	This is because leverage scores are calculated from a well-conditioned basis for a matrix which need not be a well-conditioned basis for a block.
158	40	Then there exists a deterministic distributed and streaming algorithm (namely Algorithm 5 in Appendix C) which can output a solution to the `1-Low Rank Approximation Problem with relative error poly(k) approximation factor, update time poly(n, d), space bounded by nγpoly(d), and query time poly(n, d).
213	13	Despite the simplicity of uniform sampling to keep a summary, the succeeding sections discuss the increased time and space costs of using such a sample and show that doing so is not favourable.
220	21	In contrast, if m is the bound on the summary size, then uniform sampling always returns a summary of size exactly m. However, we see that this is not optimal as both conditioning methods can return a set of rows which are pruned at every iteration to roughly half the size and contains only the most important rows in that block.
221	40	Both conditioning methods exhibit similar behavior and are bounded between both Sample and Identity methods.
222	38	Therefore, both of the conditioning methods respect the theoretical bound and, crucially, return a summary which is sublinear in the space constraint and hence a significantly smaller fraction of the input size.
231	21	Hence, an approximation can be obtained which is highly accurate, and in total time faster than the brute force solver.
233	64	Although a simple sample of randomly chosen rows can be easily maintained, this appears less useful due to the increased time costs associated with larger summaries when conditioning methods output a similar estimate in less time over the entire stream.
239	97	All of these factors make the conditioning method fast in practice to both find the important rows in the data and then compute the reduced regression problem with high accuracy.
240	20	Due to the problems in constructing summaries which can be used to solve regression quickly and accurately when using random sampling or no transformation, our methods are shown to be efficient and accurate alternatives.
241	46	Our approach is vindicated both theoretically and practically: this is most clear in the U.S. Census dataset where small error can be achieved using a summary roughly 2% the size of the data.
242	68	This also results in an overall speedup as solving the optimization on the reduced set is much faster than solving on the full problem.
243	35	Such significant savings show that this general approach can be useful in large-scale applications.
