4	38	In order to compute the gradient of the loss with respect to the weights of a module, one performs backpropagation (Williams & Hinton, 1986) – sequentially applying the chain rule to compute the exact gradient of the loss with respect to a module.
5	15	However, this scheme has many potential drawbacks, as well as lacking biological plausibility (Marblestone et al., 2016; Bengio et al., 2015).
6	14	In particular, backpropagation results in locking – the weights of a network module can only be updated after a full forwards propagation of the data through the network, followed by loss evaluation, then finally after waiting for the backpropagation of error gradients.
10	19	These models of error gradients are local to the network modules they are predicting the error gradient for, so that an update to the module can be computed by using the predicted, synthetic gradients, thus bypassing the need for subsequent forward execution, loss evaluation, and backpropagation.
22	13	In addition, in Section 6 we look at formalising the connection between SGs and other forms of approximate error propagation such as Feedback Alignment (Lillicrap et al., 2016), Direct Feedback Alignment (Nøkland, 2016; Baldi et al., 2016), and Kickback (Balduzzi et al., 2014), and show that all these error approximation schemes can be captured in a unified framework, but crucially only using synthetic gradients can one achieve unlocked training.
38	13	This model is shown in Figure 1 (b).
39	23	We also focus on Understanding Synthetic Gradients and DNIs SG modules which take the point’s true label/value as conditioning SG(h, y) as opposed to SG(h).
50	14	Since the SG module is a linear model of ∂L/∂h, the approximation of the true loss that Fh is being optimised for will be a quadratic function of h and y.
51	15	Note that this is not what a second order method does when a function is locally approximated with a quadratic and used for optimisation – here we are approximating the current loss, which is a function of parameters θ with a quadratic which is a function of h. Three appealing properties of an approximation based on h is that h already encapsulates a lot of non-linearities due to the processing of Fh, h is usually vastly lower dimensional than θ<h which makes learning more tractable, and the error only depends on quantities (h) which are local to this part of the network rather than θ which requires knowledge of the entire network.
80	26	However, SG can also introduce new critical points, leading to premature convergence and spurious additional solutions.
87	36	For MSE and both shallow and deep linear architectures the SG-based model converges to the global optimum (exact numerical results provided in Supplementary Material Table 2).
99	13	This is a consequence of the fact that close to the end of training, the highly nonlinear model has quite complex derivatives which cannot be well represented in a space of linear functions.
100	18	It is worth 3∂L/∂p = exp(xW + b)/(1 + exp(xW + b))− y noting, that in these experiments, the quality of the loss approximation dropped significantly when the true loss was around 0.001, thus it created good approximations for the majority of the learning process.
101	53	There is also an empirical confirmation of the previous claim, that with log loss and data that can be separated, linear SGs will have problems modeling this relation close to the end of training (Figure 2 (b) left), while there is no such problem for MSE loss (Figure 2 (a) left).
102	110	It is trivial to note that if a SG module used is globally convergent to the true gradient, and we only use its output after it converges, then the whole model behaves like the one trained with regular backprop.
104	93	We now discuss some of the consequences of this, and begin by showing that as long as a synthetic gradient produced is close enough to the true one we still get convergence to the true critical points.
105	143	Namely, only if the error introduced by SG, backpropagated to all the parameters, is consistently smaller than the norm of true gradient multiplied by some positive constant smaller than one, the whole system converges.
106	27	Thus, we essentially need the SG error to vanish around critical points.
108	20	If ‖∂h/∂θ<h‖ is upper bounded by some K and there exists a constant δ ∈ (0, 1) such that in every iteration K ≤ ‖∂L/∂θ<h‖ 1−δ1+δ , then the whole training process converges to the solution of the original problem.
110	51	Details are provided in the Supplementary Materials Section C. As a consequence of Proposition 2 we can show that with specifically chosen learning rates (not merely ones that are small enough) we obtain convergence for deep linear models.
111	12	For a deep linear model minimising MSE, trained with a linear SG module attached between two of its hidden layers, there exist learning rates in each iteration such that it converges to the critical point of the original problem.
113	14	Full proof is given in Supplementary Materials Section C. For a shallow model we can guarantee convergence to the global solution provided we have a small enough learning rate, which is the main theoretical result of this paper.
119	17	We now shift our attention to more realistic data.
120	30	We train deep relu networks of varied depth (up to 50 hidden layers) with batch-normalisation and with two different activation functions on MNIST and compare models trained with full backpropagation to variants that employ a SG module in the middle of the hidden stack.
125	79	Since the use of synthetic gradients can alter learning dynamics and introduce new critical points, they might converge to different types of models.
130	23	This representation is permutation invariant, and thus the emergence of a block-diagonal correlation matrix means that at a given layer, points from the same class already have very correlated representations.
132	46	In particular, in the MNIST model with 20 hidden layers trained with standard backpropagation we see that the representation covariance after 9 layers is nearly the same as the final layer’s representation.
133	77	However, by contrast, if we consider the same architecture but with a SG module in the middle we see that the layers before the SG module develop a qualitatively different style of representation.
135	13	To confirm this, we also introduced linear classifier probes (Alain & Bengio, 2016) and observed that, as with the pure backpropagation trained model, such probes can achieve 100% training accuracy after the first two hidden-layers of the SGbased model, as shown in Supplementary Material’s Figure 8.
136	52	With 20 SG modules (one between every pair of Understanding Synthetic Gradients and DNIs layers), the representation is scattered even more broadly: we see rather different learning dynamics, with each layer contributing a small amount to the final solution, and there is no longer a point in the progression of layers where the representation is more or less static in terms of correlation structure (see Figure 3).
