12	43	To the best of our knowledge, this is the first work that exploits cross-lingual embeddings for logical representations for semantic parsing.
27	18	In this work, we build our model and conduct experiments on top of the discriminative hybrid tree semantic parser (Lu, 2014, 2015).
28	20	The parser was designed based on the hybrid tree representation (HT-G) originally introduced in (Lu et al., 2008).
29	109	The hybrid tree is a joint representation encoding both sentence and semantics that aims to capture the interactions between words and semantic units.
31	44	Such a model allows us to incorporate rich features and long-range dependencies.
32	4	Recently, Susanto and Lu (2017b) extended HT-D by attaching neural architectures, resulting in their neural hybrid tree (HT-D (NN)).
34	10	Formally, for each sentence n with its semantic representation m from the training set, we assume the joint representation (a hybrid tree) is h. Now we can define a discriminative log-linear model as follows: PΛ(m|n) = ∑ h∈H(n,m) PΛ(m,h|n) = ∑ h∈H(n,m) e FΛ(n,m,h)∑ m′,h′∈H(n,m′) e FΛ(n,m′,h′)) (1) FΛ(n,m,h)) = Λ · Φ(n,m,h)) (2) whereH(n,m) returns the set of all possible joint representations that contain both n and m exactly, and F is a scoring function that is calculated as a dot product between a feature function Φ defined over tuple (m, n, h) and a weight vector Λ.
35	5	To incorporate neural features, HT-D (NN) defines the following scoring function: FΛ,Θ(n,m,h)) = Λ · Φ(n,m,h) + GΘ(n,m,h) (3) where Θ is the set of parameters of the neural networks and G is the neural scoring function over the (n,m,h) tuple (Susanto and Lu, 2017b).
37	39	Following the work (Susanto and Lu, 2017b), we denote the window size as J ∈ {0, 1, 2}.
38	29	A multilingual dataset used in semantic parsing comes with instances consisting of logical forms annotated with sentences from multiple different languages.
39	35	In this work, we aim to learn one monolingual semantic parser for each language, while leveraging useful information that can be extracted from other languages.
40	27	Each time, we train the parser for a target language and regard the other languages as auxiliary languages.
42	39	Next, for each target language, we construct a semantics-word co-occurrence matrix M ∈ Rm×n (where m is the number of unique semantic units, n is the number of unique words in the combined dataset).
45	6	To do so, we first apply singular value decomposition (SVD) to this matrix, leading to: M = UΣV∗ (4) where U ∈ Rm×m and V ∈ Rn×m are unitary matrices, V∗ is the conjugate transpose of V, and Σ ∈ Rm×m is a diagonal matrix.
46	18	We truncate the diagonal matrix Σ and left multiply it with U: UΣ̃ (5) where Σ̃ ∈ Rm×d is a matrix that consists of only the left d columns of Σ, containing the d largest singular values.
48	18	Each row in the above matrix is a ddimensional vector, giving a low-dimensional representation for one semantic unit.
49	17	Such distributed output representations can be readily used as continuous features in Φ(n,m,h).
72	9	It was noted in Kwiatkowski et al. (2010); Lu (2014) that evaluations based on these two versions are not directly comparable – the version that uses tree-shaped representations appears to be more challenging.
75	5	We can observe that when distributed logical representations are included, both HT-D and HT-D (NN) can lead to competitive results.
76	6	Specifically, when such features are included, evaluation results for 5 out of 8 languages get improved.
81	9	Differently, we train monolingual semantic parsers augmented with cross-lingual distributed semantic information.
85	5	Output is the prediction from HT-D (NN) and Output (+O) is the parsing result given by HT-D (NN+O) where the learned cross-lingual representations of semantics are included.
89	52	We note, however, previously it was also reported in the literature that the behavior of the performance associated with this language is different than other languages in the presence of additional features (Lu, 2014).
90	28	To qualitatively understand how good the learned distributed representations are, we also visualize the learned distributed representations for semantic units.
93	7	In general, we found that semantic units expressing similar meanings tend to appear to- gether.
95	13	However, we also found that occasionally semantic units conveying opposite meanings are also grouped together.
96	9	This reveals the limitations associated with such a simple cooccurrence based approach for learning distributed representations for logical expressions.
97	13	In this paper, we empirically show that the distributed representations of logical expressions learned from multilingual datasets for semantic parsing can be exploited to improve the performance of a monolingual semantic parser.
98	27	Our approach is simple, relying on an SVD over semantics-word co-occurrence matrix for finding such distributed representations for semantic units.
99	21	Future directions include investigating better ways of learning such distributed representations as well as learning such distributed representations and semantic parsers in a joint manner.
