0	52	Multi-modal models that learn semantic concept representations from both linguistic and perceptual input were originally motivated by parallels with human concept acquisition, and evidence that many concepts are grounded in the perceptual system (Barsalou et al., 2003).
1	24	Such models extract information about the perceptible characteristics of words from data collected in property norming experiments (Roller and Schulte im Walde, 2013; Silberer and Lapata, 2012) or directly from ‘raw’ data sources such as images (Feng and Lapata, 2010; Bruni et al., 2012).
2	13	This input is combined with information from linguistic corpora to produce enhanced representations of concept meaning.
4	12	Despite these results, the advantage of multimodal over linguistic-only models has only been demonstrated on concrete concepts, such as chocolate or cheeseburger, as opposed to abstract concepts such as such as guilt or obesity.
5	18	Indeed, experiments indicate that while the addition of perceptual input is generally beneficial for representations of concrete concepts (Hill et al., 2013a; Bruni et al., 2014), it can in fact be detrimental to representations of abstract concepts (Hill et al., 2013a).
6	16	Further, while the theoretical importance of the perceptual modalities to concrete representations is well known, evidence suggests this is not the case for more abstract concepts (Paivio, 1990; Hill et al., 2013b).
9	14	Since perceptual data sources typically contain information about both abstract and concrete concepts, such information is included for both concept types.
21	17	Other potential sources, such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (Von Ahn and Dabbish, 2004), either do not contain images for abstract concepts or do not contain sufficient images for the concepts in our evaluation sets.
22	41	Following the motivation outlined in Section 1, we aim to distinguish visual input corresponding to concrete concepts from visual input corresponding to abstract concepts.
24	40	Specifically, we anticipate greater congruence or similarity among a set of images for, say, elephant than among images for happiness.
26	23	Formally, we propose a measure, image dispersion d of a concept word w, defined as the average pairwise cosine distance between all the image representations { ~w1 .
36	37	Generating Linguistic Representations We extract continuous vector representations (also of 50 dimensions) for concepts using the continuous log-linear skipgram model of Mikolov et al. (2013a), trained on the 100M word British National Corpus (Leech et al., 1994).
51	51	We apply image dispersion-based filtering as follows: if both concepts in an evaluation pair have an image dispersion below a given threshold, both the linguistic and the visual representations are included.
52	15	If not, in accordance with the Dual Coding Theory of human concept processing (Paivio, 1990), only the linguistic representation is used.
53	24	For both datasets, we set the threshold as the median image dispersion, although performance could in principle be improved by adjusting this parameter.
54	23	We compare dispersion filtered representations with linguistic, perceptual and standard multi-modal representations (concatenated linguistic and perceptual representations).
61	14	Since research has demonstrated the applicability of concreteness to a range of other NLP tasks (Turney et al., 2011; Kwong, 2008), it is important to examine the connection between image dispersion and concreteness in more detail.
62	35	To evaluate the effectiveness of image dispersion as a proxy for concreteness we evaluated our algorithm on a binary classification task based on the set of 100 concrete and 100 abstract concepts A∪C introduced in Section 2.
63	27	By classifying con- cepts with image dispersion below the median as concrete and concepts above this threshold as abstract we achieved an abstract-concrete prediction accuracy of 81%.
66	42	According to the Dual Coding Theory, however, concrete concepts are precisely those with a salient perceptual representation.
71	38	On this more diverse sample, which reflects the range of concepts typically found in linguistic corpora, image dispersion is a particularly useful diagnostic for identifying the very abstract or very concrete concepts.
73	52	It should be noted that all previous approaches to the automatic measurement of concreteness rely on annotator ratings, dictionaries or manuallyconstructed resources.
84	14	This is a notable improvement on the largest-class baseline of 55%.
85	31	We presented a novel method, image dispersionbased filtering, that improves multi-modal representations by approximating conceptual concreteness from images and filtering model input.
86	15	The results clearly show that including more perceptual input in multi-modal models is not always better.
87	20	Motivated by this fact, our approach provides an intuitive and straightforward metric to determine whether or not to include such information.
89	40	To our knowledge, our algorithm constitutes the first unsupervised method for quantifying conceptual concreteness as applied to NLP, although it does, of course, rely on the Google Images retrieval algorithm.
91	13	It is striking that this apparently linguistic problem can be addressed solely using the raw data encoded in images.
92	50	In future work, we will investigate the precise quantity of perceptual information to be included for best performance, as well as the optimal filtering threshold.
93	15	In addition, we will explore whether the application of image data, and the interaction between images and language, can yield improvements on other tasks in semantic processing and representation.
