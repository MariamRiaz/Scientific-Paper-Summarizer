{"sections": [{"text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 950\u2013962 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1088"}, {"heading": "1 Introduction", "text": "Knowledge bases (KB), such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and DBpedia (Lehmann et al., 2015), are useful resources for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al., 2009). However, knowledge bases suffer from incompleteness despite their formidable sizes (Socher et al., 2013; West et al., 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al., 2015) or link prediction.\nThe fundamental motivation behind these studies is that there exist some statistical regularities under the intertwined facts stored in the multirelational knowledge base. By discovering gener-\nalizable regularities in known facts, missing ones may be recovered in a faithful way. Due to its excellent generalization capability, distributed representations, a.k.a. embeddings, have been popularized to address the KBC task (Nickel et al., 2011; Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016b).\nAs a seminal work, Bordes et al. (2013) proposes the TransE, which models the statistical regularities with linear translations between entity embeddings operated by a relation embedding. Implicitly, TransE assumes both entity embeddings and relation embeddings dwell in the same vector space, posing an unnecessarily strong prior. To relax this requirement, a variety of models first project the entity embeddings to a relationdependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space. Typically, these relation-dependent spaces are characterized by the projection matrices unique to each relation. As a benefit, different aspects of the same entity can be temporarily emphasized or depressed as an effect of the projection. For instance, STransE (Nguyen et al., 2016b) utilizes two projection matrices per relation, one for the head entity and the other for the tail entity.\nDespite the superior performance of STransE compared to TransE, it is more prone to the data sparsity problem. Concretely, since the projection spaces are unique to each relation, projection matrices associated with rare relations can only be exposed to very few facts during training, resulting in poor generalization. For common relations, a similar issue exists. Without any restrictions on the number of projection matrices, logically related or conceptually similar relations may have distinct projection spaces, hindering the discovery, sharing, and generalization of statistical regularities.\n950\nPreviously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem. In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garc\u0131\u0301a-Dura\u0301n et al., 2015; Lin et al., 2015a; Shen et al., 2016). Since the number of paths grows exponentially with its length, as a side effect, path-based models enjoy much more training cases, suffering less from the problem.\nIn this paper, we propose an interpretable knowledge transfer model (ITransF), which encourages the sharing of statistic regularities between the projection matrices of relations and alleviates the data sparsity problem. At the core of ITransF is a sparse attention mechanism, which learns to compose shared concept matrices into relation-specific projection matrices, leading to a better generalization property. Without any external resources, ITransF improves mean rank and Hits@10 on two benchmark datasets, over all previous approaches of the same kind. In addition, the parameter sharing is clearly indicated by the learned sparse attention vectors, enabling us to interpret how knowledge transfer is carried out. To induce the desired sparsity during optimization, we further introduce a block iterative optimization algorithm.\nIn summary, the contributions of this work are: (i) proposing a novel knowledge embedding model which enables knowledge transfer by learning to discover shared regularities; (ii) introducing a learning algorithm to directly optimize a sparse representation from which the knowledge transferring procedure is interpretable; (iii) showing the effectiveness of our model by outperforming baselines on two benchmark datasets for knowledge base completion task."}, {"heading": "2 Notation and Previous Models", "text": "Let E denote the set of entities and R denote the set of relations. In knowledge base completion, given a training set P of triples (h, r, t) where h, t \u2208 E are the head and tail entities having a relation r \u2208 R, e.g., (Steve Jobs, FounderOf, Apple), we want to predict missing facts such as (Steve Jobs, Profession, Businessperson).\nMost of the embedding models for knowledge base completion define an energy function fr(h, t)\naccording to the fact\u2019s plausibility (Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Yang et al., 2015; Guu et al., 2015; Nguyen et al., 2016b). The models are learned to minimize energy fr(h, t) of a plausible triple (h, r, t) and to maximize energy fr(h\u2032, t\u2032) of an implausible triple (h\u2032, r, t\u2032).\nMotivated by the linear translation phenomenon observed in well trained word embeddings (Mikolov et al., 2013), TransE (Bordes et al., 2013) represents the head entity h, the relation r and the tail entity t with vectors h, r and t \u2208 Rn respectively, which were trained so that h+r \u2248 t. They define the energy function as\nfr(h, t) = \u2016h+ r\u2212 t\u2016` where ` = 1 or 2, which means either the `1 or the `2 norm of the vector h + r \u2212 t will be used depending on the performance on the validation set.\nTo better model relation-specific aspects of the same entity, TransR (Lin et al., 2015b) uses projection matrices and projects the head entity and the tail entity to a relation-dependent space. STransE (Nguyen et al., 2016b) extends TransR by employing different matrices for mapping the head and the tail entity. The energy function is\nfr(h, t) = \u2016Wr,1h+ r\u2212Wr,2t\u2016` However, not all relations have abundant data to estimate the relation specific matrices as most of the training samples are associated with only a few relations, leading to the data sparsity problem for rare relations."}, {"heading": "3 Interpretable Knowledge Transfer", "text": ""}, {"heading": "3.1 Model", "text": "As discussed above, a fundamental weakness in TransR and STransE is that they equip each relation with a set of unique projection matrices, which not only introduces more parameters but also hinders knowledge sharing. Intuitively, many relations share some concepts with each other, although they are stored as independent symbols in KB. For example, the relation \u201c(somebody) won award for (some work)\u201d and \u201c(somebody) was nominated for (some work)\u201d both describe a person\u2019s high-quality work which wins an award or a nomination respectively. This phenomenon suggests that one relation actually represents a collection of real-world concepts, and one concept\ncan be shared by several relations. Inspired by the existence of such lower-level concepts, instead of defining a unique set of projection matrices for every relation, we can alternatively define a small set of concept projection matrices and then compose them into customized projection matrices. Effectively, the relation-dependent translation space is then reduced to the smaller concept spaces.\nHowever, in general, we do not have prior knowledge about what concepts exist out there and how they are composed to form relations. Therefore, in ITransF, we propose to learn this information simultaneously from data, together with all knowledge embeddings. Following this idea, we first present the model details, then discuss the optimization techniques for training.\nEnergy function Specifically, we stack all the concept projection matrices to a 3-dimensional tensor D \u2208 Rm\u00d7n\u00d7n, wherem is the pre-specified number of concept projection matrices and n is the dimensionality of entity embeddings and relation embeddings. We let each relation select the most useful projection matrices from the tensor, where the selection is represented by an attention vector. The energy function of ITransF is defined as:\nfr(h, t) = \u2016\u03b1Hr \u00b7D \u00b7 h+ r\u2212\u03b1Tr \u00b7D \u00b7 t\u2016` (1)\nwhere \u03b1Hr ,\u03b1 T r \u2208 [0, 1]m, satisfying \u2211 i\u03b1\nH r,i =\u2211\ni\u03b1 T r,i = 1, are normalized attention vectors used to compose all concept projection matrices in D by a convex combination. It is obvious that STransE can be expressed as a special case of our model when we use m = 2|R| concept matrices and set attention vectors to disjoint one-hot vectors. Hence our model space is a generalization of STransE. Note that we can safely use fewer concept matrices in ITransF and obtain better performance (see section 4.3), though STransE always requires 2|R| projection matrices.\nWe follow previous work to minimize the following hinge loss function:\nL = \u2211\n(h,r,t)\u223cP, (h\u2032,r,t\u2032)\u223cN\n[ \u03b3 + fr(h, t)\u2212 fr(h\u2032, t\u2032) ] + (2)\nwhere P is the training set consisting of correct triples, N is the distribution of corrupted triples defined in section 3.3, and [\u00b7]+ = max(\u00b7, 0). Note that we have omitted the dependence of N on (h, r, t) to avoid clutter. We normalize the entity vectors h, t, and the projected entity vectors\n\u03b1Hr \u00b7D \u00b7 h and \u03b1Tr \u00b7D \u00b7 t to have unit length after each update, which is an effective regularization method that benefits all models.\nSparse attention vectors In Eq. (1), we have defined \u03b1Hr ,\u03b1 T r to be some normalized vectors used for composition. With a dense attention vector, it is computationally expensive to perform the convex combination of m matrices in each iteration. Moreover, a relation usually does not consist of all existing concepts in practice. Furthermore, when the attention vectors are sparse, it is often easier to interpret their behaviors and understand how concepts are shared by different relations.\nMotivated by these potential benefits, we further hope to learn sparse attention vectors in ITransF. However, directly posing `1 regularization (Tibshirani, 1996) on the attention vectors fails to produce sparse representations in our preliminary experiment, which motivates us to enforce `0 constraints on \u03b1Tr ,\u03b1 H r .\nIn order to satisfy both the normalization condition and the `0 constraints, we reparameterize the attention vectors in the following way:\n\u03b1Hr = SparseSoftmax(v H r , I H r ) \u03b1Tr = SparseSoftmax(v T r , I T r )\nwhere vHr ,v T r \u2208 Rm are the pre-softmax scores, IHr , I T r \u2208 {0, 1}m are the sparse assignment vectors, indicating the non-zero entries of attention vectors, and the SparseSoftmax is defined as\nSparseSoftmax(v, I)i = exp(vi/\u03c4)Ii\u2211 j exp(vj/\u03c4)Ij\nwith \u03c4 being the temperature of Softmax. With this reparameterization, vHr ,v T r and IHr , I T r replace \u03b1 T r ,\u03b1 H r to become the real parameters of the model. Also, note that it is equivalent to pose the `0 constraints on IHr , I T r instead of \u03b1Tr ,\u03b1 H r . Putting these modifications together, we can rewrite the optimization problem as\nminimize L subject to \u2016IHr \u20160 \u2264 k, \u2016ITr \u20160 \u2264 k\n(3)\nwhere L is the loss function defined in Eq. (2)."}, {"heading": "3.2 Block Iterative Optimization", "text": "Though sparseness is favorable in practice, it is generally NP-hard to find the optimal solution under `0 constraints. Thus, we resort to an approximated algorithm in this work.\nFor convenience, we refer to the parameters with and without the sparse constraints as the sparse partition and the dense partition, respectively. Based on this notion, the high-level idea of the approximated algorithm is to iteratively optimize one of the two partitions while holding the other one fixed. Since all parameters in the dense partition, including the embeddings, the projection matrices, and the pre-softmax scores, are fully differentiable with the sparse partition fixed, we can simply utilize SGD to optimize the dense partition. Then, the core difficulty lies in the step of optimizing the sparse partition (i.e. the sparse assignment vectors), during which we want the following two properties to hold\n1. the sparsity required by the `0 constaint is maintained, and\n2. the cost define by Eq. (2) is decreased.\nSatisfying the two criterion seems to highly resemble the original problem defined in Eq. (3). However, the dramatic difference here is that with parameters in the dense partition regarded as constant, the cost function is decoupled w.r.t. each relation r. In other words, the optimal choice of IHr , I T r is independent of I H r\u2032 , I T r\u2032 for any r\n\u2032 6= r. Therefore, we only need to consider the optimization for a single relation r, which is essentially an assignment problem. Note that, however, IHr and ITr are still coupled, without which we basically reach the situation in a backpack problem. In principle, one can explore combinatorial optimization techniques to optimize IHr\u2032 , I T r\u2032 jointly, which usually involve some iterative procedure. To avoid adding another inner loop to our algorithm, we turn to a simple but fast approximation method based on the following single-matrix cost.\nSpecifically, for each relation r, we consider the induced cost LHr,i where only a single projection matrix i is used for the head entity:\nLHr,i = \u2211\n(h,r,t)\u223cPr, (h\u2032,r,t\u2032)\u223cNr\n[ \u03b3 + fHr,i(h, t)\u2212 fHr,i(h\u2032, t\u2032) ] +\nwhere fHr,i(h, t) = \u2016Di \u00b7 h + r \u2212 \u03b1Tr \u00b7 D \u00b7 t\u2016 is the corresponding energy function, and the subscript in Pr and Nr denotes the subsets with relation r. Intuitively, LHr,i measures, given the current tail attention vector \u03b1Tr , if only one project matrix could be chosen for the head entity, how implausible Di would be. Hence, i\u2217 = argmini LHr,i gives\nus the best single projection matrix on the head side given \u03b1Tr .\nNow, in order to choose the best k matrices, we basically ignore the interaction among projection matrices, and update IHr in the following way:\nIHr,i \u2190 { 1, i \u2208 argpartitioni(LHr,i, k) 0, otherwise\nwhere the function argpartitioni(xi, k) produces the index set of the lowest-k values of xi.\nAnalogously, we can define the single-matrix cost LTr,i and the energy function fTr,i(h, t) on the tail side in a symmetric way. Then, the update rule for IHr follows the same derivation. Admittedly, the approximation described here is relatively crude. But as we will show in section 4, the proposed algorithm yields good performance empirically. We leave the further improvement of the optimization method as future work."}, {"heading": "3.3 Corrupted Sample Generating Method", "text": "Recall that we need to sample a negative triple (h\u2032, r, t\u2032) to compute hinge loss shown in Eq. 2, given a positive triple (h, r, t) \u2208 P . The distribution of negative triple is denoted by N(h, r, t). Previous work (Bordes et al., 2013; Lin et al., 2015b; Yang et al., 2015; Nguyen et al., 2016b) generally constructs a set of corrupted triples by replacing the head entity or tail entity with a random entity uniformly sampled from the KB.\nHowever, uniformly sampling corrupted entities may not be optimal. Often, the head and tail entities associated a relation can only belong to a specific domain. When the corrupted entity comes from other domains, it is very easy for the model to induce a large energy gap between true triple and corrupted one. As the energy gap exceeds \u03b3, there will be no training signal from this corrupted triple. In comparison, if the corrupted entity comes from the same domain, the task becomes harder for the model, leading to more consistent training signal.\nMotivated by this observation, we propose to sample corrupted head or tail from entities in the same domain with a probability pr and from the whole entity set with probability 1 \u2212 pr. The choice of relation-dependent probability pr is specified in Appendix A.1. In the rest of the paper, we refer to the new proposed sampling method as \u201ddomain sampling\u201d."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Setup", "text": "To evaluate link prediction, we conduct experiments on the WN18 (WordNet) and FB15k (Freebase) introduced by Bordes et al. (2013) and use the same training/validation/test split as in (Bordes et al., 2013). The information of the two datasets is given in Table 1.\nIn knowledge base completion task, we evaluate model\u2019s performance of predicting the head entity or the tail entity given the relation and the other entity. For example, to predict head given relation r and tail t in triple (h, r, t), we compute the energy function fr(h\u2032, t) for each entity h\u2032 in the knowledge base and rank all the entities according to the energy. We follow Bordes et al. (2013) to report the filter results, i.e., removing all other correct candidates h\u2032 in ranking. The rank of the correct entity is then obtained and we report the mean rank (mean of the predicted ranks) and Hits@10 (top 10 accuracy). Lower mean rank or higher Hits@10 mean better performance."}, {"heading": "4.2 Implementation Details", "text": "We initialize the projection matrices with identity matrices added with a small noise sampled from normal distribution N (0, 0.0052). The entity and relation vectors of ITransF are initialized by TransE (Bordes et al., 2013), following Lin et al. (2015b); Ji et al. (2015); Garc\u0131\u0301a-Dura\u0301n et al. (2016, 2015); Lin et al. (2015a). We ran minibatch SGD until convergence. We employ the \u201cBernoulli\u201d sampling method to generate incorrect triples as used in Wang et al. (2014), Lin et al. (2015b), He et al. (2015), Ji et al. (2015) and Lin et al. (2015a).\nSTransE (Nguyen et al., 2016b) is the most similar knowledge embedding model to ours except that they use distinct projection matrices for each relation. We use the same hyperparameters as used in STransE and no significant improvement is ob-\nserved when we alter hyperparameters. We set the margin \u03b3 to 5 and dimension of embedding n to 50 for WN18, and \u03b3 = 1, n = 100 for FB15k. We set the batch size to 20 for WN18 and 1000 for FB15k. The learning rate is 0.01 on WN18 and 0.1 on FB15k. We use 30 matrices on WN18 and 300 matrices on FB15k. All the models are implemented with Theano (Bergstra et al., 2010). The Softmax temperature is set to 1/4."}, {"heading": "4.3 Results & Analysis", "text": "The overall link prediction results1 are reported in Table 2. Our model consistently outperforms previous models without external information on both the metrics of WN18 and FB15k. On WN18, we even achieve a much better mean rank with comparable Hits@10 than current state-of-the-art model IRN employing external information.\nWe can see that path information is very helpful on FB15k and models taking advantage of path information outperform intrinsic models by a significant margin. Indeed, a lot of facts are easier to recover with the help of multi-step inference. For example, if we know Barack Obama is born in Honolulu, a city in the United States, then we easily know the nationality of Obama is the United States. An straightforward way of extending our proposed model to k-step path P = {ri}ki=1 is to define a path energy function \u2016\u03b1HP \u00b7 D \u00b7 h +\u2211\nri\u2208P ri \u2212 \u03b1TP \u00b7 D \u00b7 t\u2016`, \u03b1HP is a concept association related to the path. We plan to extend our model to multi-step path in the future.\nTo provide a detailed understanding why the proposed model achieves better performance, we present some further analysis in the sequel.\nPerformance on Rare Relations In the proposed ITransF, we design an attention mechanism to encourage knowledge sharing across different relations. Naturally, facts associated with rare relations should benefit most from such sharing, boosting the overall performance. To verify this hypothesis, we investigate our model\u2019s performance on relations with different frequency.\nThe overall distribution of relation frequencies resembles that of word frequencies, subject to the zipf\u2019s law. Since the frequencies of relations approximately follow a power distribution, their log\n1Note that although IRN (Shen et al., 2016) does not explicitly exploit path information, it performs multi-step inference through the multiple usages of external memory. When IRN is allowed to access memory once for each prediction, its Hits@10 is 80.7, similar to models without path information.\nfrequencies are linear. The statistics of relations on FB15k and WN18 are shown in Figure 1. We can clearly see that the distributions exhibit long tails, just like the Zipf\u2019s law for word frequency.\nIn order to study the performance of relations with different frequencies, we sort all relations by their frequency in the training set, and split them into 3 buckets evenly so that each bucket has a similar interval length of log frequency.\nWithin each bucket, we compare our model with STransE, as shown in Figure 2.2 As we can see, on WN18, ITransF outperforms STransE by a significant margin on rare relations. In particular, in the last bin (rarest relations), the average Hits@10 increases from 55.2 to 93.8, showing the great benefits of transferring statistical strength from common relations to rare ones. The comparison on each relation is shown in Appendix A.2. On FB15k, we can also observe a similar pattern, although the degree of improvement is less significant. We conjecture the difference roots in the fact that many rare relations on FB15k have disjoint domains, knowledge transfer through common concepts is harder.\nInterpretability In addition to the quantitative evidence supporting the effectiveness of knowledge sharing, we provide some intuitive examples to show how knowledge is shared in our model. As\n2Domain sampling is not employed.\nwe mentioned earlier, the sparse attention vectors fully capture the association between relations and concepts and hence the knowledge transfer among relations. Thus, we visualize the attention vectors for several relations on both WN18 and FB15K in Figure 3.\nFor WN18, the words \u201chyponym\u201d and \u201chypernym\u201d refer to words with more specific or general meaning respectively. For example, PhD is a hyponym of student and student is a hypernym of PhD. As we can see, concepts associated with the head entities in one relation are also associated with the tail entities in its reverse relation. Further, \u201cinstance hypernym\u201d is a special hypernym with the head entity being an instance, and the tail entity being an abstract notion. A typical example is (New York,instance hypernym, city). This connection has also been discovered by our model, indicated by the fact that \u201cinstance hypernym(T)\u201d and \u201chypernym(T)\u201d share a common concept matrix. Finally, for symmetric relations like \u201csimilar to\u201d, we see the head attention is identical to the tail attention, which well matches our intuition.\nOn FB15k, we also see the sharing between reverse relations, as in \u201c(somebody) won award for (some work)\u201d and \u201c(some work) award winning work (somebody)\u201d. What\u2019s more, although relation \u201cwon award for\u201d and \u201cwas nominated for\u201d share the same concepts,\ntheir attention distributions are different, suggesting distinct emphasis. Finally, symmetric relations like spouse behave similarly as mentioned before.\nModel Compression A byproduct of parameter sharing mechanism employed by ITransF is a much more compact model with equal performance. Figure 5 plots the average performance of ITransF against the number of projection matrices m, together with two baseline models. On FB15k, when we reduce the number of matrices from 2200 to 30 (\u223c 90\u00d7 compression), our model performance decreases by only 0.09% on Hits@10, still outperforming STransE. Similarly, on WN18, ITransF continues to achieve the best performance when we reduce the number of concept project matrices to 18."}, {"heading": "5 Analysis on Sparseness", "text": "Sparseness is desirable since it contribute to interpretability and computational efficiency of our model. We investigate whether enforcing sparseness would deteriorate the model performance and compare our method with another sparse encoding methods in this section.\nDense Attention w/o `1 regularization Although `0 constrained model usually enjoys many practical advantages, it may deteriorate the model performance when applied improperly. Here, we show that our model employing sparse attention can achieve similar results with dense attention with a significantly less computational burden. We also compare dense attention with `1 regularization. We set the `1 coefficient to 0.001 in our experiments and does not apply Softmax since the `1 of a vector after Softmax is always 1. We compare models in a setting where the computation time of\ndense attention model is acceptable3. We use 22 weight matrices on WN18 and 15 weight matrices on FB15k and train both the models for 2000 epochs.\nThe results are reported in Table 3. Generally, ITransF with sparse attention has slightly better or comparable performance comparing to dense attention. Further, we show the attention vectors of\n3With 300 projection matrices, it takes 1h1m to run one epoch for a model with dense attention.\nmodel with `1 regularized dense attention in Figure 4. We see that `1 regularization does not produce a sparse attention, especially on FB15k.\nNonnegative Sparse Encoding In the proposed model, we induce the sparsity by a carefully designed iterative optimization procedure. Apart from this approach, one may utilize sparse encoding techniques to obtain sparseness based on the pretrained projection matrices from STransE. Concretely, stacking |2R| pretrained projection\nmatrices into a 3-dimensional tensor X \u2208 R2|R|\u00d7n\u00d7n, similar sparsity can be induced by solving an `1-regularized tensor completion problem minA,D ||X \u2212 DA||22 + \u03bb\u2016A\u2016`1 . Basically, A plays the same role as the attention vectors in our model. For more details, we refer readers to (Faruqui et al., 2015).\nFor completeness, we compare our model with the aforementioned approach4. The comparison is summarized in table 4. On both benchmarks, ITransF achieves significant improvement against sparse encoding on pretrained model. This performance gap should be expected since the objective function of sparse encoding methods is to minimize the reconstruction loss rather than optimize the criterion for link prediction."}, {"heading": "6 Related Work", "text": "In KBC, CTransR (Lin et al., 2015b) enables relation embedding sharing across similar relations, but they cluster relations before training rather than learning it in a principled way. Further, they do not solve the data sparsity problem because there is no sharing of projection matrices which have a lot more parameters. Learning the association between semantic relations has been used in related problems such as relational similarity measurement (Turney, 2012) and relation adaptation (Bollegala et al., 2015).\nData sparsity is a common problem in many fields. Transfer learning (Pan and Yang, 2010) has been shown to be promising to transfer knowl-\n4We use the toolkit provided by (Faruqui et al., 2015).\nedge and statistical strengths across similar models or languages. For example, Bharadwaj et al. (2016) transfers models on resource-rich languages to low resource languages by parameter sharing through common phonological features in name entity recognition. Zoph et al. (2016) initialize from models trained by resource-rich languages to translate low-resource languages.\nSeveral works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping theK largest values. However, the sorting operation in these works is not GPU-friendly.\nThe block iterative optimization algorithm in our work is inspired by LightRNN (Li et al., 2016). They allocate every word in the vocabulary in a table. A word is represented by a row vector and a column vector depending on its position in the table. They iteratively optimize embeddings and allocation of words in tables."}, {"heading": "7 Conclusion and Future Work", "text": "In summary, we propose a knowledge embedding model which can discover shared hidden concepts, and design a learning algorithm to induce the interpretable sparse representation. Empirically, we show our model can improve the performance on two benchmark datasets without external resources, over all previous models of the same kind.\nIn the future, we plan to enable ITransF to perform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters. In addition, our framework can also be applied to multi-task learning, promoting a finer sharing among different tasks."}, {"heading": "Acknowledgments", "text": "We thank anonymous reviewers and Graham Neubig for valuable comments. We thank Yulun Du, Paul Mitchell, Abhilasha Ravichander, Pengcheng Yin and Chunting Zhou for suggestions on the draft. We are also appreciative for the great working environment provided by staff in LTI.\nThis research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program."}, {"heading": "A Appendix", "text": "A.1 Domain Sampling Probability In this section, we define the probability pr to generate a negative sample from the same domain mentioned in Section 3.3. The probability cannot be too high to avoid generating negative samples that are actually correct, since there are generally a lot of facts missing in KBs.\nSpecifically, let MHr = {h | \u2203t(h, r, t) \u2208 P} and MTr = {t | \u2203h(h, r, t) \u2208 P} denote the head or tail domain of relation r. Suppose Nr = {(h, r, t) \u2208 P} is the induced set of edges with relation r. We define the probability pr as\npr = min( \u03bb|MTr ||MHr | |Nr| , 0.5) (4)\nOur motivation of such a formulation is as follows: Suppose Or is the set that contains all truthful fact triples on relation r, i.e., all triples in training set and all other missing correct triples. If we assume all fact triples within the domain has uniform probability of being true, the probability of a random triple being correct is Pr((h, r, t) \u2208 Or | h \u2208 MHr , t \u2208 MTr ) = |Or||MHr ||MTr |\nAssume that all facts are missing with a probability \u03bb, then |Nr| = \u03bb|Or| and the above probability can be approximated by |Nr|\n\u03bb|MHr ||MTr | . We\nwant the probability of generating a negative sample from the domain to be inversely proportional to the probability of the sample being true, so we define the probability as Eq. 4. The results in section 4 are obtained with \u03bb set to 0.001.\nWe compare how different value of \u03bb would influence our model\u2019s performance in Table. 5. With large \u03bb and higher domain sampling probability, our model\u2019s Hits@10 increases while mean rank also increases. The rise of mean rank is due to higher probability of generating a valid triple as a negative sample causing the energy of a valid triple to increase, which leads to a higher overall rank of a correct entity. However, the reasoning capability is boosted with higher Hits@10 as shown in the table.\nA.2 Performance on individual relations of WN18\nWe plot the performance of ITransF and STransE on each relation. We see that the improvement is greater on rare relations."}], "year": 2017, "references": [{"title": "Semantic parsing on Freebase from question-answer pairs", "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "year": 2013}, {"title": "Theano: a cpu and gpu math expression compiler", "authors": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proceedings of the Python", "year": 2010}, {"title": "Phonologically aware neural model for named entity recognition in low resource transfer settings", "authors": ["Akash Bharadwaj", "David Mortensen", "Chris Dyer", "Jaime Carbonell."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-", "year": 2016}, {"title": "Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge", "authors": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD International Conference on Man-", "year": 2008}, {"title": "Embedding semantic relations into word representations", "authors": ["Danushka Bollegala", "Takanori Maehara", "Ken-ichi Kawarabayashi."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence.", "year": 2015}, {"title": "A Semantic Matching Energy Function for Learning with Multi-relational Data", "authors": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio."], "venue": "Machine Learning 94(2):233\u2013259.", "year": 2014}, {"title": "Translating Embeddings for Modeling Multirelational Data", "authors": ["Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko."], "venue": "Advances in Neural Information Processing Systems 26, pages 2787\u20132795.", "year": 2013}, {"title": "Learning Structured Embeddings of Knowledge Bases", "authors": ["Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio."], "venue": "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence. pages 301\u2013306.", "year": 2011}, {"title": "Cfo: Conditional focused neural question answering with largescale knowledge bases", "authors": ["Zihang Dai", "Lei Li", "Wei Xu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Asso-", "year": 2016}, {"title": "Sparse overcomplete word vector representations", "authors": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International", "year": 2015}, {"title": "WordNet: An Electronic Lexical Database", "authors": ["Christiane D. Fellbaum."], "venue": "MIT Press.", "year": 1998}, {"title": "Composing Relationships with Translations", "authors": ["Alberto Garc\u0131\u0301a-Dur\u00e1n", "Antoine Bordes", "Nicolas Usunier"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "year": 2015}, {"title": "Combining Two and Three-Way Embedding Models for Link Prediction in Knowledge Bases", "authors": ["Alberto Garc\u0131\u0301a-Dur\u00e1n", "Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet"], "venue": "Journal of Artificial Intelligence Research", "year": 2016}, {"title": "Traversing Knowledge Graphs in Vector Space", "authors": ["Kelvin Guu", "John Miller", "Percy Liang."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 318\u2013327.", "year": 2015}, {"title": "Learning to Represent Knowledge Graphs with Gaussian Embedding", "authors": ["Shizhu He", "Kang Liu", "Guoliang Ji", "Jun Zhao."], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. pages 623\u2013632.", "year": 2015}, {"title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "authors": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "year": 2015}, {"title": "DBpedia - A Large-scale, Multilingual Knowledge Base", "authors": ["Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo N. Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "S\u00f6ren Auer", "Christian Bizer"], "year": 2015}, {"title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks", "authors": ["Xiang Li", "Tao Qin", "Jian Yang", "Tieyan Liu."], "venue": "Advances in Neural Information Processing Systems 29.", "year": 2016}, {"title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "authors": ["Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language", "year": 2015}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "authors": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence Learning, pages", "year": 2015}, {"title": "K-sparse autoencoders", "authors": ["Alireza Makhzani", "Brendan Frey."], "venue": "Proceedings of the International Conference on Learning Representations.", "year": 2014}, {"title": "From softmax to sparsemax: A sparse model of attention and multi-label classification", "authors": ["Andr\u00e9 FT Martins", "Ram\u00f3n Fernandez Astudillo."], "venue": "Proceedings of the 33th International Conference on Machine Learning.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "authors": ["Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on", "year": 2009}, {"title": "Neighborhood mixture model for knowledge base completion", "authors": ["Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson."], "venue": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL). Association for Com-", "year": 2016}, {"title": "STransE: a novel embedding model of entities and relationships in knowledge bases", "authors": ["Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for", "year": 2016}, {"title": "A Review of Relational Machine Learning for Knowledge Graphs", "authors": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich."], "venue": "Proceedings of the IEEE, to appear .", "year": 2015}, {"title": "A Three-Way Model for Collective Learning on Multi-Relational Data", "authors": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel."], "venue": "Proceedings of the 28th International Conference on Machine Learning. pages 809\u2013816.", "year": 2011}, {"title": "A survey on transfer learning", "authors": ["Sinno Jialin Pan", "Qiang Yang."], "venue": "IEEE Transactions on knowledge and data engineering 22(10):1345\u20131359.", "year": 2010}, {"title": "Outrageously large neural networks", "authors": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "year": 2017}, {"title": "Implicit reasonet: Modeling large-scale structured relationships with shared memory", "authors": ["Yelong Shen", "Po-Sen Huang", "Ming-Wei Chang", "Jianfeng Gao."], "venue": "arXiv preprint arXiv:1611.04642 .", "year": 2016}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "authors": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng."], "venue": "Advances in Neural Information Processing Systems 26, pages 926\u2013934.", "year": 2013}, {"title": "YAGO: A Core of Semantic Knowledge", "authors": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum."], "venue": "Proceedings of the 16th International Conference on World Wide Web. pages 697\u2013706.", "year": 2007}, {"title": "Regression shrinkage and selection via the lasso", "authors": ["Robert Tibshirani."], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) pages 267\u2013288.", "year": 1996}, {"title": "Observed Versus Latent Features for Knowledge Base and Text Inference", "authors": ["Kristina Toutanova", "Danqi Chen."], "venue": "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. pages 57\u201366.", "year": 2015}, {"title": "Representing Text for Joint Embedding of Text and Knowledge Bases", "authors": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural", "year": 2015}, {"title": "Domain and function: A dualspace model of semantic relations and compositions", "authors": ["Peter D Turney."], "venue": "Journal of Artificial Intelligence Research 44:533\u2013 585.", "year": 2012}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "authors": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pages 1112\u20131119.", "year": 2014}, {"title": "Mining inference formulas by goal-directed random walks", "authors": ["Zhuoyu Wei", "Jun Zhao", "Kang Liu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas,", "year": 2016}, {"title": "Knowledge Base Completion via Searchbased Question Answering", "authors": ["Robert West", "Evgeniy Gabrilovich", "Kevin Murphy", "Shaohua Sun", "Rahul Gupta", "Dekang Lin."], "venue": "Proceedings of the 23rd International Conference on World Wide Web.", "year": 2014}, {"title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "authors": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "Proceedings of the International Conference on Learning Representations.", "year": 2015}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "authors": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "year": 2015}, {"title": "Transfer learning for low-resource neural machine translation", "authors": ["Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "year": 2016}], "id": "SP:585059ffd433f2b6ba6ebfe4052551ade7b46910", "authors": [{"name": "Qizhe Xie", "affiliations": []}, {"name": "Xuezhe Ma", "affiliations": []}, {"name": "Zihang Dai", "affiliations": []}, {"name": "Eduard Hovy", "affiliations": []}], "abstractText": "Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets\u2014 WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.", "title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion"}