1	23	Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy (Eriguchi et al., 2016b, 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016).
9	22	The latent parser can be independently pre-trained with human-annotated treebanks and is then adapted to the translation task.
20	35	This approach is similar to that of graph-based dependency parsing (McDonald et al., 2005) in that a sentence is represented with a set of weighted arcs between the words.
21	119	To obtain the latent graph representation of the sentence, we use a dependency parsing model based on multi-task learning proposed by Hashimoto et al. (2017).
26	26	h(1)i is then fed into a softmax classifier to predict a probability distribution p(1)i ∈ RC (1) for word-level tags, where C(1) is the number of POS classes.
31	27	While the models of Hashimoto et al. (2017), Zhang et al. (2017), and Dozat and Manning (2017) learn the model parameters of their parsing models only by humanannotated data, we allow the model parameters to be learned by the translation task.
34	35	The latent graph representation described in Section 2 can be used for any sentence-level tasks, and here we apply it to an Attention-based NMT (ANMT) model (Luong et al., 2015).
36	46	The ANMT model first encodes the information about the input sentence and then generates a sentence in another language.
42	31	In our model, we explicitly incorporate such relationships into the encoder by defining a dependency composition function: dep(wi) = tanh(Wdep[henci ; h(Hwi); p(`wi |wi)]), (3) where h(Hwi) = ∑ j 6=i p(Hwi = wj |wi)h(enc)j is the weighted average of the hidden states of the parent nodes.
50	70	In addition, like the attention mechanism over constituency tree nodes (Eriguchi et al., 2016b), our model uses attention to the dependency composition vectors: s′(i, t) = exp (h (dec) t ·dep(wi))∑N j=1 exp (h (dec) t ·dep(wj)) , (6) a′t = ∑N i=1 s ′(i, t)dep(wi), (7) To predict the target word, a hidden state h̃(dec)t ∈ Rd3 is then computed as follows: h̃ (dec) t = tanh(W̃ [h (dec) t ; at; a ′ t]), (8) where W̃ ∈ Rd3×3d3 is a weight matrix.
54	42	To speed up the training, we use BlackOut sampling (Ji et al., 2016).
56	24	Implementation Tips Inspired by Zoph et al. (2016), we further speed up BlackOut sampling by sharing noise samples across words in the same sentences.
60	73	Translation At test time, we use a novel beam search algorithm which combines statistics of sentence lengths (Eriguchi et al., 2016b) and length normalization (Cho et al., 2014).
94	28	The latent graph parser in our model can be optionally pre-trained by using human annotations for dependency parsing.
95	30	In this paper we used the widely-used Wall Street Journal (WSJ) training data to jointly train the POS tagging and dependency parsing components.
96	28	We used the standard training split (Section 0-18) for POS tagging.
104	34	SEQ is constructed by removing the dependency composition in Equation (3), forming a sequential NMT model with the multi-layer encoder.
110	34	We first show our translation results using the small and medium training datasets.
116	18	We can also see that DEP performs the worst.
125	19	Table 3 shows the results of using the medium training dataset.
137	17	Again, we see that the translation scores of our model can be further improved by pre-training the model.
140	22	Notice also that our implementation of the sequential model (SEQ) provides a very strong baseline, the performance of which is already comparable to the previous state of the art, even without using ensemble techniques.
145	63	Figure 2 shows two translation examples7 to see how the proposed model works and what is missing in the state-of-the-art sequential NMT model, SEQ.
159	27	By contrast, the pre-training strategy effectively embeds the information about the POS tags and the dependency relations into our model.
173	22	We see that dependencies between words in distant positions, such as subject-verb-object relations, can be captured.
174	17	With Pre-Training We also inspected the pretrained latent graphs.
183	62	These results suggest that human-annotated treebanks can provide useful prior knowledge to guide the overall model training by pre-training, but the resulting sentence structures adapted to the target task do not need to highly correlate with the treebanks.
206	47	By pre-training our model using treebank annotations, our model significantly outperforms both a pipelined syntax-based model and a state-of-the-art sequential model.
207	47	On English-to-Japanese translation, our model outperforms the previous best models by a large margin.
208	95	In future work, we investigate the effectiveness of our approach in different types of target tasks.
