0,1,label2,summary_sentences
"Given a dataset, similarity relationship between examples can be represented by a graph in which each example is represented by a vertex.",1. Introduction,[0],[0]
"While pairwise relationship between two vertices can be represented by an edge in a normal graph, a higher order relationship involving multiple vertices can be captured by a hyperedge, which means that all the corresponding examples are similar to one another.",1. Introduction,[0],[0]
"Hypergraphs have been used in several learning applications such as clustering of categorical data (Gibson et al., 1998), multi-label classification (Sun et al., 2008), Laplacian sparse coding (Gao et al., 2013), image classification (Yu et al., 2012), image retrieval (Huang et al., 2010), mapping users across different social networks (Tan et al., 2014) and predicting edge labels in hypernode graphs (Ricatte et al., 2014).
",1. Introduction,[0],[0]
"*Equal contribution 1University of Hong Kong, Hong Kong.",1. Introduction,[0],[0]
2This research was partially supported by the Hong Kong RGC under the grant 17200214.,1. Introduction,[0],[0]
"Correspondence to: T-H. Hubert Chan <hubert@cs.hku.hk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we consider semi-supervised learning on an edge-weighted hypergraph H = (V,E,w), with a set L of labeled vertices, whose labels are given by f∗L ∈ {−1,+1}L. The task is to predict the labels of the unlabeled vertices N , with the working principle that vertices contained in a hyperedge e ∈ E are more similar to one another if the edge weight we is larger.",1. Introduction,[0],[0]
"This problem is also known as transductive inference and has been studied in (Zhou et al., 2006) and (Hein et al., 2013).
",1. Introduction,[0],[0]
"However, the methods in (Zhou et al., 2006) have been criticized by (Agarwal et al., 2006), because essentially a hypergraph is converted into a normal graph.",1. Introduction,[0],[0]
"For instance, given a hyperedge e containing vertices S, either (i) a clique is added between the vertices in S, or (ii) a star is formed by adding a new vertex ve connecting every vertex in S to ve.",1. Introduction,[0],[0]
"Then, a standard convex program using a regularization potential function for normal graphs can be applied (Zhu et al., 2003).",1. Introduction,[0],[0]
"By choosing appropriate edge weights, it was shown in (Agarwal et al., 2006) that the two approaches are equivalent to the following convex program relaxation:
min Φold(f) := 1
2 ∑ e∈E we ∑
{u,v}∈(e2)
(fu − fv)2
subject to fu ∈",1. Introduction,[0],[0]
"[−1, 1], ∀u ∈ V fu = f ∗ u , ∀u ∈",1. Introduction,[0],[0]
"L.
On the other hand, it was proposed in (Hein et al., 2013) that the following regularization function is more suitable to capture hyperedge expansion:
Φnew(f) := 1
2 ∑ e∈E we · (max u∈e fu −min v∈e fv) 2.
",1. Introduction,[0],[0]
"Indeed, it was shown in (Hein et al., 2013) that their approach outperforms (Zhou et al., 2006) on several datasets from the UCI Machine Learning Repository (Lichman, 2013).
",1. Introduction,[0],[0]
Loss Function.,1. Introduction,[0],[0]
"In (Hein et al., 2013), a squared loss function was added by considering the convex program with objective function Φnew(f) + µ ‖f − f∗‖22 on f ∈",1. Introduction,[0],[0]
"[−1, 1]V , where µ > 0 is a parameter to be tuned, f∗L is given by the labeled vertices L, and for the unlabeled vertices f∗N = 0.
",1. Introduction,[0],[0]
"The loss function allows errors in the labeled vertices, and also ensures that the minimizer is unique.",1. Introduction,[0],[0]
"However, as a result, unlabeled vertices have a tendency to acquire f values close to 0.",1. Introduction,[0],[0]
"This might remove useful information as illustrated in the following example.
",1. Introduction,[0],[0]
Example.,1. Introduction,[0],[0]
"In Figure 1.1, vertices a, b ∈ L are labeled as +1 and c ∈ L is labeled as −1.",1. Introduction,[0],[0]
"Vertices x, y ∈ N are unlabeled.",1. Introduction,[0],[0]
"There are three (undirected) edges: {a, x}, {b, x} and {x, y, c}, each with unit weight.
",1. Introduction,[0],[0]
"By choosing µ = 12 for squared loss function, the unique minimizer gives fx = 15 and fy = 0.",1. Introduction,[0],[0]
"Hence, this solution gives no useful information regarding the label for vertex y.
On the other hand, if we just use the objective function Φnew(f) with the constraints fL = f∗L, then in an optimal solution, fx = 13 , but fy could be anywhere in the confidence interval",1. Introduction,[0],[0]
"[−1, 13 ].",1. Introduction,[0],[0]
"Hence, in this case, we could use the average value − 13 to predict −1 for vertex y.
Our Contributions.",1. Introduction,[0],[0]
"In this paper, we revisit the approach used in (Hein et al., 2013) and consider several extensions and simplifications.",1. Introduction,[0],[0]
"We summarize our results and give an outline of the paper as follows.
1.",1. Introduction,[0],[0]
Unified Framework for Directed Hypergraphs.,1. Introduction,[0],[0]
"Inspired also from the recent result on Laplacians for directed normal graphs (Yoshida, 2016), we introduce a semisupervised learning framework using directed hypergraphs that can capture higher order causal relationships.",1. Introduction,[0],[0]
"This notion of directed hypergraph was first introduced in (Gallo et al., 1993), who considered applications in propositional logic, analyzing dependency in relational database, and traffic analysis.",1. Introduction,[0.9674458839335729],"['For example, the study reported in (Collobert et al., 2011) demonstrated significant accuracy gains in tagging, named entity recognition, and semantic role labeling when using vector space word representations learned from large corpora.']"
"On a high level, a directed hyperedge e consists of a tail set Te pointing to a head set He such that a vertex in Te labeled +1 implies that a vertex in He is more likely to be labeled +1.",1. Introduction,[0],[0]
"(Equivalently in terms of its contrapositive, a vertex in He labeled −1 implies that a vertex in Te is more likely to be labeled −1.)",1. Introduction,[0],[0]
"In Section 2, we formally define the model and the corresponding potential function Φ. An additional advantage of our potential function is that there is no need to tune any parameters.
2.",1. Introduction,[0],[0]
Confidence Interval for Unlabeled Vertices.,1. Introduction,[0],[0]
Observe that the minimizer for our convex program might not be unique.,1. Introduction,[0],[0]
"In Section 3, we introduce the concept of confidence interval for each unlabeled vertex that can be useful for predicting its label.",1. Introduction,[0],[0]
"Furthermore, we provide an algorithm to calculate the confidence interval given an optimal solution.
3.",1. Introduction,[0],[0]
Simpler Subgradient Method.,1. Introduction,[0],[0]
"Since the new potential function is not everywhere differentiable but still convex, we use the subgradient method (Shor et al., 1985) to obtain an estimated minimizer for label prediction.",1. Introduction,[0],[0]
"Inspired by the diffusion processes used for defining Laplacians in hypergraphs (Louis, 2015) and directed graphs (Yoshida, 2016), in Section 4, we define a simple Markov operator that returns a subgradient for Φ, which is used to solve the underlying convex program.",1. Introduction,[0],[0]
"We remark that our framework is very easy to understand, because it is a variation on the well-known gradient descent.
",1. Introduction,[0],[0]
"In contrast, the primal-dual approach in (Hein et al., 2013) considers the convex conjugate of the primal objective and involves complicated update operations on the primal and dual variables.",1. Introduction,[0],[0]
"The subgradient used in our approach gives the update direction, and we can actually solve exactly the same convex program with a much simpler method.",1. Introduction,[0],[0]
"Section 5, we revisit some datasets in the UCI Machine Learning Repository (Lichman, 2013), and experiments confirm that our prediction model based on confidence interval gives better accuracy than that in (Hein et al., 2013).",4. Experimental Results on Real-World Datasets. In,[0],[0]
"Our simpler subgradient method takes more iterations than the primal-dual method (Hein et al., 2013), but each iteration is much faster.",4. Experimental Results on Real-World Datasets. In,[0],[0]
"Experiments show that overall both methods have similar running times, and the subgradient method has an advantage when the number of vertices is much larger than the number of edges.
",4. Experimental Results on Real-World Datasets. In,[0],[0]
"Moreover, using the DBLP dataset (Ley, 2009), our experiments also support that using directed hypergraphs to capture causal relationships can improve the prediction accuracy.",4. Experimental Results on Real-World Datasets. In,[0],[0]
The experiments for directed hypergraphs are described in the full version.,4. Experimental Results on Real-World Datasets. In,[0],[0]
"We consider an edge-weighted directed hypergraph H = (V,E,w) with vertex set V (with n = |V |), edge set E and weight function",2. Preliminaries,[0],[0]
w : E → R+.,2. Preliminaries,[0],[0]
Each hyperedge e ∈ E consists of a tail set Te ⊆ V and a head set He ⊆ V (which are not necessarily disjoint); we use the convention that the direction is from tail to head.,2. Preliminaries,[0],[0]
"For x ∈ R, we denote [x]+ := max{x, 0}.
",2. Preliminaries,[0],[0]
"In our application, each vertex v ∈ V is supposed to have a label in {−1,+1}.",2. Preliminaries,[0],[0]
"Intuitively, the directed hypergraph attempts to capture the rule that for each edge e ∈ E, if there is a vertex in Te having label +1, then it is more likely for vertices in He to receive label +1.",2. Preliminaries,[0],[0]
"In terms of its contrapositive, if there is a vertex in He having label −1, then it is more likely for vertices in Te to receive label −1.
",2. Preliminaries,[0],[0]
"We use f ∈ RV to denote a vector, where the coordi-
nates are labeled by vertices in V .",2. Preliminaries,[0],[0]
"For U ⊆ V , we use fU ∈ RU to denote the vector restricting f to coordinates inU .",2. Preliminaries,[0],[0]
"In semi-supervised learning, we consider a setL ⊆ V of labeled vertices, which have labels f∗L ∈",2. Preliminaries,[0],[0]
"{−1,+1}L. Typically, |L| |V | and the task is to assign a label in {−1,+1} to each unlabeled vertex in N := V \ L, using information from the directed hypergraph H .
",2. Preliminaries,[0],[0]
"By relaxing labels to be in the interval [−1, 1], we consider the following regularization potential function Φ : RV → R:
Φ(f)",2. Preliminaries,[0],[0]
"= 1
2 ∑ e∈E we · ([∆e(f)]+)2,
where ∆e(f) := max(u,v)∈Te×He(fu − fv) = maxu∈Te fu −minv∈He fv .
",2. Preliminaries,[0],[0]
"In particular, there is a penalty due to edge e only if some vertex in Te receives a label larger than that of some vertex in He.",2. Preliminaries,[0],[0]
"The convexity of Φ is proved in the full version.
",2. Preliminaries,[0],[0]
Our approach is to consider the following convex program to obtain an estimated minimizer f ∈,2. Preliminaries,[0],[0]
"[−1, 1]V , which can be rounded to an integer solution for labeling all vertices.
min Φ(f) (CP1) subject to fu ∈",2. Preliminaries,[0],[0]
"[−1, 1], ∀u ∈ V
fu = f ∗ u , ∀u",2. Preliminaries,[0],[0]
"∈ L
Since the f values for the labeled vertices L are fixed in (CP1), we also view Φ : RN → R as a function on the f values of unlabeled vertices N .",2. Preliminaries,[0],[0]
"We use OPT ⊂ RV to denote the set of optimal solutions to (CP1).
",2. Preliminaries,[0],[0]
Trivial Edges.,2. Preliminaries,[0],[0]
An edge e ∈ E is trivial if there exist vertices u ∈ Te ∩ L and v ∈,2. Preliminaries,[0],[0]
He ∩ L such that f∗u = +1 and f∗v = −1.,2. Preliminaries,[0],[0]
"As trivial edges contribute constant towards the objective function Φ, we shall assume that there are no trivial edges in the convex program (CP1).
",2. Preliminaries,[0],[0]
Special Cases.,2. Preliminaries,[0],[0]
"Our directed hypergraph model can capture other graph models as follows.
1.",2. Preliminaries,[0],[0]
Undirected Hypergraphs.,2. Preliminaries,[0],[0]
"For each hyperedge e, we can set Te = He to the corresponding subset of vertices.",2. Preliminaries,[0],[0]
2.,2. Preliminaries,[0],[0]
Undirected Normal Graphs.,2. Preliminaries,[0],[0]
"For each edge e = {u, v}, we can set Te = He = e. Observe that in this case, the potential function becomes Φ(f) =∑
(u,v)∈E wuv(fu− fv)2, which is differentiable, and hence, (CP1) can be solved by standard techniques like gradient descent.
",2. Preliminaries,[0],[0]
Soft Constraints.,2. Preliminaries,[0],[0]
"In (Hein et al., 2013), each labeled vertex u ∈ L can also have some weight µu ∈ R+, which can, for instance, indicate how trustworthy the label
f∗u ∈ {−1,+1} is.",2. Preliminaries,[0],[0]
"The following relaxation is considered.
",2. Preliminaries,[0],[0]
"min Φ̂(f) := Φ(f) + 1
2 ∑ u∈L µu(fu",2. Preliminaries,[0],[0]
"− f∗u)2 (CP2)
subject to fu ∈",2. Preliminaries,[0],[0]
"[−1, 1],∀u ∈ V.
Observe that (CP2) can also be expressed in the framework of (CP1).",2. Preliminaries,[0],[0]
"We simply consider an augmented hypergraph Ĥ such that all vertices V are treated as unlabeled, and for each u ∈ L, we add a new vertex û with label f∗u and a new undirected edge {u, û} with weight µu.",2. Preliminaries,[0],[0]
"Then, it follows that the convex program (CP1) for the augmented instance for Ĥ is exactly the same as (CP2).
",2. Preliminaries,[0],[0]
Challenges Ahead.,2. Preliminaries,[0],[0]
"We next outline how we resolve the encountered challenges when we use (CP1) for semisupervised learning.
",2. Preliminaries,[0],[0]
"• Unlike the case for normal graphs, the set OPT can contain more than one optimal solution for (CP1).",2. Preliminaries,[0],[0]
"In Section 3, we prove some structural properties of the convex program, and illustrate that each u ∈ N has some confidence interval from which we can predict its label.",2. Preliminaries,[0],[0]
• The function Φ is not everywhere differentiable.,2. Preliminaries,[0],[0]
"Hence, we use the subgradient method (Shor et al., 1985).",2. Preliminaries,[0],[0]
"In Section 4, we give a method to generate a subgradient, which is inspired by the continuous diffusion processes for hypergraphs (Louis, 2015) and directed graphs (Yoshida, 2016), and our method can in fact be viewed as a discretized version.",2. Preliminaries,[0],[0]
"In general, a minimizer for (CP1) might not be unique.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Hence, we introduce the concept of confidence interval.
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
Definition 3.1 (Confidence Interval),3. Confidence Interval for Semi-supervised Learning,[0],[0]
"For each u ∈ V , we define its confidence interval to be [mu,Mu], where mu := minf∈OPT fu and Mu := maxf∈OPT fu.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"The confidence intervals induce the lower and the upper confidence vectors, ~m and ~M ∈ RV , respectively.
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"In Section 3.1, we give the proof of the following lemma, which states that the confidence vectors ~m and ~M are optimal solutions, and so are their convex combinations.
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
Lemma 3.1 (Confidence Vectors Give Optimal Solutions),3. Confidence Interval for Semi-supervised Learning,[0],[0]
For any λ ∈,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"[0, 1], the convex combination λ~m + (1− λ) ~M",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"∈ OPT is optimal for (CP1).
",3. Confidence Interval for Semi-supervised Learning,[0],[0]
Semi-supervised Learning via Confidence Interval.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
Lemma 3.1 suggests what one can do when (CP1) has more than one optimal solution.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Specifically, in Algorithm 1, the
average vector 12 (~m + ~M) ∈ OPT can be used for label prediction.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"We show that the confidence vectors ~m and ~M can be recovered from any optimal solution f ∈ OPT, which in turn can be estimated by the subgradient method described in Section 4.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Algorithm 1 Semi-Supervised Learning
1: Input: Directed hypergraph H = (V,E,w), labels f∗L for labeled vertices L 2: Compute (estimated) confidence vectors (~m, ~M) ∈",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"RN × RN , either by Algorithm 2 or 3.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
3: Compute average vector fN ← 12 (~m+ ~M).,3. Confidence Interval for Semi-supervised Learning,[0],[0]
4: Compute threshold θ ← 1|N | ∑ u∈N fu.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"5: for each u ∈ N do 6: if fu ≥ θ then 7: f̂u ← +1; 8: else 9: f̂u ← −1;
10: end if 11: end for 12: return f̂N
Fine-Tuning Parameters.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"In view of Lemma 3.1, one could further optimize the choice of λ ∈",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"[0, 1] in defining fN ← λ~m+ (1−λ) ~M in Line 3.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"Similarly, one could pick the threshold θ to be the ϑ-percentile of the sorted coordinates of fN , for some choice of ϑ ∈",3. Confidence Interval for Semi-supervised Learning,[0],[0]
"[0, 1].",3. Confidence Interval for Semi-supervised Learning,[0],[0]
The parameters λ and ϑ can be tuned using standard techniques like cross-validation.,3. Confidence Interval for Semi-supervised Learning,[0],[0]
"However, to illustrate our concepts, we keep the description simple without introducing too many free parameters.",3. Confidence Interval for Semi-supervised Learning,[0],[0]
We derive some properties of the confidence vectors to prove Lemma 3.1.,3.1. Properties of Confidence Vectors,[0],[0]
"The full proofs of Lemma 3.2 and 3.3 are given in the full version.
",3.1. Properties of Confidence Vectors,[0],[0]
"Given a feasible solution f ∈ RV to (CP1), we define the following:
1. Se(f) := arg maxu∈Te fu ⊆ Te and Ie(f) := arg minv∈He fv ⊆ He.",3.1. Properties of Confidence Vectors,[0],[0]
2. f(Se),3.1. Properties of Confidence Vectors,[0],[0]
:= maxu∈Te fu and f(Ie) := minv∈He fv .,3.1. Properties of Confidence Vectors,[0],[0]
"Hence, we have ∆e(f) = f(Se)− f(Ie).",3.1. Properties of Confidence Vectors,[0],[0]
3.,3.1. Properties of Confidence Vectors,[0],[0]
"The set of active edges with respect to f is E(f) := {e ∈ E : ∆e(f) > 0}.
",3.1. Properties of Confidence Vectors,[0],[0]
"The following lemma states even though a minimizer for (CP1) might not be unique, there are still some structural properties for any optimal solution.
",3.1. Properties of Confidence Vectors,[0],[0]
Lemma 3.2 (Active Edges in an Optimal Solution) Suppose f and g are optimal solutions to (CP1).,3.1. Properties of Confidence Vectors,[0],[0]
"Then, for all e ∈ E, [∆e(f)]+ = [∆e(g)]+.",3.1. Properties of Confidence Vectors,[0],[0]
"In particular, this implies that the set of active edges E∗",3.1. Properties of Confidence Vectors,[0],[0]
":= E(f) = E(g) in any op-
timal solution is uniquely determined.",3.1. Properties of Confidence Vectors,[0],[0]
"Hence, for e ∈ E∗, we can define the corresponding ∆∗e = ∆e(f).
",3.1. Properties of Confidence Vectors,[0],[0]
"Definition 3.2 (Pinned Vertex) An unlabeled vertex u is pinned in a solution f ∈ RV if there exist active edges e and e′ ∈ E(f) such that u ∈ Se(f)∩ Ie′(f), in which case we say that the edges e and e′ pin the vertex u under f .
",3.1. Properties of Confidence Vectors,[0],[0]
Lemma 3.3 (Extending an Active Edge),3.1. Properties of Confidence Vectors,[0],[0]
Suppose edge e ∈ E(f) is active in an optimal solution f .,3.1. Properties of Confidence Vectors,[0],[0]
"If He does not contain a vertex labeled with −1, then there exist u ∈ Ie(f) and another active edge e′ ∈ E(f) such that the following holds.
",3.1. Properties of Confidence Vectors,[0],[0]
(a) The edges e and e′,3.1. Properties of Confidence Vectors,[0],[0]
"pin u under f , i.e., u ∈ Se′(f).",3.1. Properties of Confidence Vectors,[0],[0]
(b),3.1. Properties of Confidence Vectors,[0],[0]
"If g is an optimal solution, then Ie(f) ∩ Se′(f) =
Ie(g) ∩ Se′(g) and fu = gu.",3.1. Properties of Confidence Vectors,[0],[0]
vertex labeled with +1.,An analogous result holds when Te does not contain any,[0],[0]
∗(Ie),"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
":= minu∈He fu are uniquely determined by any optimal solution f .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Corollary 3.1 (Pinned Vertices),"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"In any optimal solution, the set of pinned vertices is uniquely determined.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
We use L∗ to denote the set of labeled or pinned vertices in an optimal solution.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Then, for each u ∈ L∗, its value f∗u in any optimal solution is also uniquely determined.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"From Corollary 3.1, the confidence interval for any u ∈ L∗ contains exactly one value, namely the unique value f∗u in any optimal solution.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The following lemma gives a characterization of an optimal solution.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Lemma 3.4 Characterization of Optimal Solutions,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"A solution f to (CP1) is optimal iff the following conditions hold.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"(a) For each u ∈ L∗, fu = f∗u .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"(b) For each active edge e ∈ E∗, both the maximum
maxu∈Te fu and the minimum minv∈He fv are attained by vertices in L∗. (c) For each inactive edge e /∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"E∗,","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
for all u ∈ Te and v ∈,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He, fu ≤ fv .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Proof: We first observe that Corollary 3.1 states that the values of the vertices in L∗ are uniquely determined in any optimal solution.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, any optimal solution must satisfy the three conditions.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"We next show that the three conditions implies that the objective value is optimal.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Once the values for vertices in L∗ are fixed, Lemma 3.3 and condition (b) implies that the contribution of all active edges E∗ are determined and are the same as any optimal solution.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Finally, condition (c) implies that edges not in E∗ do not have any contribution towards the objective function.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, any solution satisfying the three conditions must be optimal.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Deriving Confidence Vectors.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"To prove Lemma 3.5, we define a procedure that returns a vector ~m ∈ V R such that for any optimal f ∈ OPT, we have f ≥ ~m.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, we shall show that ~m ∈ OPT and hence ~m is the lower confidence vector.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
The argument for the upper confidence vector ~M is similar.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For the special case of undirected hypergraphs, the procedure can be simplified to Algorithm 2 in Section 3.2.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Lemma 3.5 (Confidence Vectors are Optimal: Proof of Lemma 3.1),"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
The confidence vectors ~m and ~M defined in Definition 3.1 are optimal solutions to (CP1).,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"This implies that any of their convex combination is also optimal.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Proof: We give a procedure that returns a vector ~m such that at any moment during the procedure, the following invariant is maintained: for any f ∈ OPT, f ≥ ~m.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The following steps correspond to maintaining the conditions in Lemma 3.4.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(a) Initialization.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For v ∈ L∗, set mv := f∗v ; for v /∈ L∗, set mv := −1.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"This satisfies the invariant, because for any f ∈ OPT and any v ∈ L∗, fv = f∗v .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(b) Preserving Active Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For each v /∈ L∗, set mv ← max{mv,maxe∈E∗:v∈He f∗(Ie)}.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Observe that Lemma 3.4(b) implies that for any optimal f ∈ OPT, any e ∈ E∗ and any v ∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He, fv ≥ f∗(Ie).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, the invariant is maintained.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(c) Preserving Inactive Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
While there is an inactive edge e /∈ E∗,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"such that u ∈ Te, v ∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He and mu > mv , set mv ← mu.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
We argue why each such update preserves the invariant.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Consider any optimal f ∈ OPT.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Before this update, the invariant holds.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, we have mu ≤ fu.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, Lemma 3.4 implies that fu ≤ fv .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Therefore, after setting mv ← mu, we still have mv ≤ fv .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Finally, observe that after step (b), the coordinates of ~m can take at most n distinct values.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, after each update in step (c), one coordinate of ~m must increase strictly.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, this procedure will terminate.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"We next argue that ~m is an optimal solution by checking that it satisfies the conditions in Lemma 3.4.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Condition (a).,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Observe that for each v ∈ L∗, mv is initialized to f∗v .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Afterwards the value mv could only be increased.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"However, because the invariant holds when the procedure terminates, it must be the case that mv = f∗v at the end.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
Condition (b).,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The procedure makes sure that at the end of
step (b), for every active edge e ∈ E∗, minv∈He mv can be attained by some vertex in L∗. Since only mv for v /∈ L∗ can be increased in step (c), it follows that in the end, the minimum can still be attained by some vertex in L∗.
Next, consider u ∈ Te, where e ∈ E∗. For any optimal solution f , Lemma 3.3 implies that fu ≤ f∗(Se).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Hence, the invariant implies thatmu ≤ fu ≤ f∗(Se).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Since condition (a) holds, this means that maxv∈Te mv can be attained by some vertex in L∗.
Condition (c).","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"This is clearly satisfied because of the while-termination condition.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Therefore, we have ~m ∈ OPT, as required.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
The proof for the upper confidence vector ~M is similar.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"We omit the detailed proof and just give the corresponding procedure to return ~M .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(a) Initialization.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For v ∈ L∗, set Mv := f∗v ; for v /∈ L∗, set Mv := +1.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(b) Preserving Active Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"For each v /∈ L∗, set Mv ← min{Mv,mine∈E∗:v∈Te f∗(Se)}.
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
(c) Preserving Inactive Edges.,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
While there is an inactive edge e /∈ E∗,"In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"such that u ∈ Te, v ∈","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"He and Mu > Mv , set Mu ←Mv .
","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"The same argument can show that for any optimal f ∈ OPT, we have f ≤ ~M .","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"Moreover, we also have ~M ∈ OPT.","In particular, for any active edge e ∈ E∗, the extremal values f∗(Se) := maxu∈Te fu and f",[0],[0]
"As mentioned before, the proof of Lemma 3.5 implicitly gives a procedure to compute the confidence vectors from any optimal solution.",3.2. Computing the Confidence Interval,[0],[0]
"For the special case of undirected hypergraphs, a simplified version of the procedure is given in Algorithm 2.
",3.2. Computing the Confidence Interval,[0],[0]
"Alternatively, we can try to solve the convex program (CP1), for example using Algorithm 5 in Section 4, from two initial feasible solutions to heuristically estimate the confidence vectors.",3.2. Computing the Confidence Interval,[0],[0]
"In Algorithm 3, one instance approaches an optimal solution from high f values and the other from low f values.",3.2. Computing the Confidence Interval,[0],[0]
Resolving Ties.,4. Subgradient Method via Markov Operator,[0],[0]
Observe that Φ : RN → R is differentiable at fN ∈ RN that has distinct coordinates.,4. Subgradient Method via Markov Operator,[0],[0]
"For the purpose of computing a subgradient, we assume that there is some global ordering π on V to resolve ties among coordinates with the same value.",4. Subgradient Method via Markov Operator,[0],[0]
"In particular, the vertices in L having label +1 are the highest, and those in L labeled −1 are the lowest.",4. Subgradient Method via Markov Operator,[0],[0]
"Hence, in this section, we may assume that any arg max or arg min operator over a subset of vertices
Algorithm 2 Confidence Intervals for Undirected Hypergraphs
1: Input: Undirected hypergraph H = (V,E,w), label vector f∗L and tolerance ≥ 0.",4. Subgradient Method via Markov Operator,[0],[0]
"2: Let f be a solution of (CP1), either by Algorithm 5 or by PDHG method (Hein et al., 2013) 3: For all v ∈ V , set p(v)← v, mv ← −1, Mv ← +1.",4. Subgradient Method via Markov Operator,[0],[0]
"4: Ê := {e ∈ E : ∆e(f) ≤ } 5: while ∃e1 6= e2 ∈ Ê, e1 ∩ e2 6= ∅",4. Subgradient Method via Markov Operator,[0],[0]
"do 6: Ê ← (Ê \ {e1, e2}) ∪ {e1 ∪ e2} 7: end while 8: for each e ∈ Ê do 9: x← an arbitrary vertex in e
10: for each vertex v ∈ e do 11: p(v)← p(x) 12: end for 13: end for 14: for each vertex v ∈ L do 15: mp(v) ← f∗v , Mp(v) ← f∗v 16: end for 17: for each edge e ∈ E such that ∆e(f) >",4. Subgradient Method via Markov Operator,[0],[0]
do 18: for each vertex v ∈,4. Subgradient Method via Markov Operator,[0],[0]
e,4. Subgradient Method via Markov Operator,[0],[0]
"do 19: mp(v) ← max{mp(v), f(Ie)} 20: Mp(v) ← min{Mp(v), f(Se)} 21: end for 22: end for 23: for each vertex v ∈ V do 24: mv ← mp(v), Mv ←Mp(v) 25: end for 26: return vectors (~m, ~M)
will return a unique vertex.
",4. Subgradient Method via Markov Operator,[0],[0]
"We next define a Markov operator that is inspired from the diffusion processes on hypergraphs (Louis, 2015) and directed graphs (Yoshida, 2016) in the context of defining Laplacians.",4. Subgradient Method via Markov Operator,[0],[0]
"We denote the projection operator ΠN : RV → RN that takes f ∈ RV and returns the restricted vector fN ∈ RN .
",4. Subgradient Method via Markov Operator,[0],[0]
Lemma 4.1 For f ∈,4. Subgradient Method via Markov Operator,[0],[0]
"[−1, 1]V that is feasible in (CP1), the Markov operator Mf given in Algorithm 4 returns a subgradient of Φ : RN → R at fN .
",4. Subgradient Method via Markov Operator,[0],[0]
"Proof: (Sketch) Observe that if fN ∈ RN has distinct coordinates, then Φ is differentiable at fN , and Mf gives exactly the gradient (which is the only possible subgradient in this case).",4. Subgradient Method via Markov Operator,[0],[0]
"Observe that in our subgradient method application, we could imagine that at every iteration, infinitesimal perturbation is performed on the current solution to ensure that all coordinates are distinct, and ties are resolved according to our global ordering π.
",4. Subgradient Method via Markov Operator,[0],[0]
"Algorithm 3 Estimate confidence interval 1: Input: Directed hypergraph H = (V,E,w), labels f∗L
for labeled vertices L 2: Construct feasible f (0,+)N ← +1 ∈ RN with all entries
being +1; 3: Construct feasible f (0,−)N ← −1 ∈",4. Subgradient Method via Markov Operator,[0],[0]
"RN with all entries
being −1; 4: ~M ← SGM(f (0,+)N ); 5: ~m← SGM(f (0,−)N ); 6: return the vectors (~m, ~M)
",4. Subgradient Method via Markov Operator,[0],[0]
"Algorithm 4 Markov Operator M : RV → RN
1: Input: Directed hypergraph H = (V,E,w), feasible f ∈ RV for (CP1) 2: Construct symmetric matrix A ∈ RV×V ; set A← 0.",4. Subgradient Method via Markov Operator,[0],[0]
3: for each e ∈ E such that ∆e(f) > 0,4. Subgradient Method via Markov Operator,[0],[0]
do 4: u← arg maxu∈Te fu; 5: v ← arg minv∈He fv; 6: Auv ← Auv + we; 7: (The same is done forAvu becauseA is symmetric.),4. Subgradient Method via Markov Operator,[0],[0]
"8: end for 9: Construct diagonal matrix W ∈ RN×N ; set W ← 0.
10: for each u ∈ N",4. Subgradient Method via Markov Operator,[0],[0]
do 11:,4. Subgradient Method via Markov Operator,[0],[0]
"Wuu ← ∑ v∈V Auv; 12: end for 13: return (WΠN −ΠNA)f
Hence, as the magnitude of the perturbation tends to zero, if the global ordering π is preserved, then the gradient remains the same, which implies that the gradient is also the subgradient when the perturbation reaches 0.
",4. Subgradient Method via Markov Operator,[0],[0]
"Using the Markov operator M as a subroutine to generate a subgradient, we have the following subgradient method (SGM) (Shor et al., 1985).
",4. Subgradient Method via Markov Operator,[0],[0]
"Algorithm 5 Subgradient Method SGM(f (0)N ∈ RN ) 1: Input: Directed hypergraph H = (V,E,w) with la-
bels f∗L for labeled vertices L, initial feasible solution f (0) N ∈",4. Subgradient Method via Markov Operator,[0],[0]
"RN , step size {ηt := 1 t }t≥1
2: t← 1; 3: (Throughout the algorithm, f (t)L = f ∗ L is given by the
labeled vertices.)",4. Subgradient Method via Markov Operator,[0],[0]
4: while Solution f (t)N has not “stabilized” do 5: g(t)N ← Mf (t−1) ∈ RN ; 6: f (t)N = f (t−1) N,4. Subgradient Method via Markov Operator,[0],[0]
"− ηt ·
g (t)",4. Subgradient Method via Markov Operator,[0],[0]
"N∥∥∥g(t)N ∥∥∥
2
;
7: t← t+ 1; 8: end while 9: return f (t)
",4. Subgradient Method via Markov Operator,[0],[0]
Stabilizing Condition.,4. Subgradient Method via Markov Operator,[0],[0]
"Our experiments in Section 5 suggest that it suffices to run the solver for a short time, after which a better feasible solution f does not improve the prediction accuracy.",4. Subgradient Method via Markov Operator,[0],[0]
Our experiments are run on a standard PC.,5. Experimental Results,[0],[0]
"In our graphs, each point refers to a sample mean, and the height of the vertical bar is the standard error of the mean.",5. Experimental Results,[0],[0]
"We show that our treatment of hypergraphs performs better than the previously best method in (Hein et al., 2013).
",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Hypergraph Model.,5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"We use three datasets from the UCI Machine Learning Repository (Lichman, 2013): mushroom, covertype45 and covertype67.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"As in (Hein et al., 2013), each dataset fits into the hypergraph learning model in the following way.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Each entry in the dataset corresponds to a vertex, which is labeled either +1 or −1.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Moreover, each entry has some categorical attributes.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"For each attribute and each realized value for that attribute, we form a unit-weight hyperedge containing all the vertices corresponding to entries having that attribute value.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"To summarize, below are the properties of the resulting hypergraphs.
",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Dataset mushroom covertype45 covertype67
n = |V",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"| 8124 12240 37877 m = |E| 112 104 123 k =∑
e∈E |e| m
1523 1412 3695
Semi-supervised Learning Framework.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"We compare our semi-supervised learning framework with that in (Hein et al., 2013), which was previously the best (compared to (Zhou et al., 2006), for instance).",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Specifically, we compare the prediction accuracy of the following two prediction algorithms.
1.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Confidence Interval (CI).,5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"We use hard constraints (CP1) and confidence intervals for prediction, as described in Algorithm 1 in Section 3. 2.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Hein et al. We implement the method described in (Hein et al., 2013), which uses soft constraints (regularized version), plus 5-fold cross validation to determine the regularization parameter.
",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Testing Methodology.,5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Since we focus on prediction accuracy, using either subgradient method or PDHG (Hein et al., 2013) for solving the underlying convex programs in each algorithm produces the same results.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"For each algorithm candidate, we try different sizes of labeled vertices L, where l = |L| ranges from 20 to 200.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"For each size l
of labeled vertices, we randomly pick l vertices from the dataset to form the set L and treat the rest as unlabeled vertices; we re-sample if only one label (+1 or −1) appears in L. For each size",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"l, we perform 100 trials to report the average error rate together with its standard error.
Results.",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"Our experiment can recover the results reported in (Hein et al., 2013).",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
"The test error for the two algorithms on the three datasets is presented in Figure 5.1, which shows that our CI method consistently has lower test error than the one in (Hein et al., 2013).",5.1. Undirected Hypergraph: Comparing Accuracy of Prediction Methods,[0],[0]
Different Solvers.,5.2. Comparing Running Times of Solvers,[0],[0]
"We compare the running times of the following two convex program solvers:
• Subgradient Method (SG), proposed by us.",5.2. Comparing Running Times of Solvers,[0],[0]
"Empirically, the step size ηt := 1
(t+1) min( 0.16t 105 ,1)
gives good
performance.",5.2. Comparing Running Times of Solvers,[0],[0]
"For large t, ηt grows like 1t and so the method converges; however, for small t, we would like a larger step size to speed up convergence.",5.2. Comparing Running Times of Solvers,[0],[0]
"• Primal-Dual Hybrid Gradient (PDHG), proposed in (Hein et al., 2013).",5.2. Comparing Running Times of Solvers,[0],[0]
"We choose σ = τ = 1√
1+d ,
where d is the maximum degree.
",5.2. Comparing Running Times of Solvers,[0],[0]
Theoretical Analysis.,5.2. Comparing Running Times of Solvers,[0],[0]
"Given a hypergraph with n vertices and m edges, where the average size of an edge is k, each vertex on average appears in mkn edges.",5.2. Comparing Running Times of Solvers,[0],[0]
"For SG, we use a heap-based data structure to maintain the vertices within a hyperedge.",5.2. Comparing Running Times of Solvers,[0],[0]
"Vertices attaining the maximum and the minimum value within a hyperedge can be retrieved in O(1) time, and a value update takes O(log k) time.",5.2. Comparing Running Times of Solvers,[0],[0]
"In each iteration, at most 2m vertices will have their values updated.",5.2. Comparing Running Times of Solvers,[0],[0]
"Hence, in each iteration, SG takes time 2m·mkn ·O(log k) = O(m
2k n log k).",5.2. Comparing Running Times of Solvers,[0],[0]
"In the description of PDHG in (Hein et al., 2013), each iteration takesO(mk log k) time.",5.2. Comparing Running Times of Solvers,[0],[0]
"Hence, when n m, each iteration of SG will be significantly faster, although in general, the number of iterations required by the subgradient method can be larger than that for PDHG.
",5.2. Comparing Running Times of Solvers,[0],[0]
Testing Methodology.,5.2. Comparing Running Times of Solvers,[0],[0]
"In each experiment, we consider the hypergraph from one of the above three datasets.",5.2. Comparing Running Times of Solvers,[0],[0]
"We pick l = 160 vertices at random as the labeled vertices L, and form the corresponding convex program (CP1) for the two solvers, where the initial values for unlabeled vertices are chosen independently to be uniformly at random from [−1, 1].",5.2. Comparing Running Times of Solvers,[0],[0]
"To compare the performance, we run the two solvers on the same convex program, and record each trajectory of the objective value versus the time duration.",5.2. Comparing Running Times of Solvers,[0],[0]
"According to experience, 100 seconds is good enough for either solver to reach an almost optimal solution, and we use the minimum value achieved by the two solvers after 100 seconds as an estimate for the true optimal value OPT.",5.2. Comparing Running Times of Solvers,[0],[0]
"Then, we scan each trajectory, and for each relative gap
∈ {10−i : i = 1, 2, . . .",5.2. Comparing Running Times of Solvers,[0],[0]
", 6}, we find the smallest time T ( ) after which the objective value is at most OPT away from the estimate OPT.",5.2. Comparing Running Times of Solvers,[0],[0]
Each instance of the experiment is repeated 100 times (with different sets of labeled vertices) to obtain an average of those T ( )’s and their standard error.,5.2. Comparing Running Times of Solvers,[0],[0]
"For each relative gap , we also report the test error for using a feasible solution that is OPT away from the presumed optimal value OPT.
Results.",5.2. Comparing Running Times of Solvers,[0],[0]
Both solvers have similar performance.,5.2. Comparing Running Times of Solvers,[0],[0]
"As predicted by our theoretical analysis, we see in Figure 5.2 that SG has an advantage when the number n of vertices is much larger than the number m of edges, which is the case for the the last dataset covertype67.",5.2. Comparing Running Times of Solvers,[0],[0]
"Moreover, in Figure 5.3, we see that achieving a relative gap smaller than 10−4 has almost no effect on improving the prediction accuracy.",5.2. Comparing Running Times of Solvers,[0],[0]
"Hence, we can conclude that for either solver, it takes roughly 10 to 20 seconds to produce a solution for the underlying convex program that can give good predic-
tion accuracy.",5.2. Comparing Running Times of Solvers,[0],[0]
DBLP Dataset.,5.3. Directed Hypergraph: More Powerful,[0],[0]
"We use the DBLP (Ley, 2009) dataset.",5.3. Directed Hypergraph: More Powerful,[0],[0]
Each paper is represented by a vertex.,5.3. Directed Hypergraph: More Powerful,[0],[0]
"We include papers from year 2000 to 2015 from conferences belonging to the following research areas to conduct our experiments:
• 7049 papers from machine learning (ML): NIPS, ICML • 2539 papers from theoretical computer science (TCS): STOC, FOCS • 3374 papers from database (DB): VLDB, SIGMOD
We perform the following prediction tasks: (a) ML (+1) vs TCS (-1), and (b) ML (+1) vs DB (-1).
",5.3. Directed Hypergraph: More Powerful,[0],[0]
The details of the experiment setup and the results are given in the full version.,5.3. Directed Hypergraph: More Powerful,[0],[0]
We revisit semi-supervised learning on hypergraphs.,abstractText,[0],[0]
"Same as previous approaches, our method uses a convex program whose objective function is not everywhere differentiable.",abstractText,[0],[0]
"We exploit the non-uniqueness of the optimal solutions, and consider confidence intervals which give the exact ranges that unlabeled vertices take in any optimal solution.",abstractText,[0],[0]
"Moreover, we give a much simpler approach for solving the convex program based on the subgradient method.",abstractText,[0],[0]
"Our experiments on real-world datasets confirm that our confidence interval approach on hypergraphs outperforms existing methods, and our sub-gradient method gives faster running times when the number of vertices is much larger than the number of edges.",abstractText,[0],[0]
Re-revisiting Learning on Hypergraphs:  Confidence Interval and Subgradient Method,title,[0],[0]
"The Fisher Information Metric (FIM) I(Θ) = (Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients Iij = Ep [ ∂l ∂Θi ∂l ∂Θj ] , where l(Θ) denotes the log-density function log p(x |Θ).",1. Fisher Information Metric,[0],[0]
"Under light regularity conditions, FIM can be rewritten equivalently as
Iij = −Ep",1. Fisher Information Metric,[0],[0]
"[ ∂2l
∂Θi∂Θj
] = 4 ∫ ∂ √ p(x |Θ) ∂Θi ∂ √ p(x |Θ) ∂Θj dx.
",1. Fisher Information Metric,[0],[0]
"As its empirical counterpart, the observed FIM (Efron & Hinkley, 1978) with respect to (wrt) a sample set Xn = {xk}nk=1 is Î(Θ |Xn) = −∇2l(Θ",1. Fisher Information Metric,[0],[0]
"|Xn), which is often evaluated at the maximum likelihood estimate Θ = Θ̂(Xn).",1. Fisher Information Metric,[0],[0]
"By the law of large numbers, Î(Θ) converges to the (expected) FIM I(Θ) as n→∞.
1King Abdullah University of Science and Technology (KAUST), Saudi Arabia 2École Polytechnique, France 3Sony Computer Science Laboratories Inc., Japan.",1. Fisher Information Metric,[0],[0]
"Correspondence to: Ke Sun <sunk@ieee.org>, Frank Nielsen <Frank.Nielsen@acm.org>.
",1. Fisher Information Metric,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Fisher Information Metric,[0],[0]
"Copyright 2017 by the author(s).
",1. Fisher Information Metric,[0],[0]
The FIM is not invariant and depends on the parameterization.,1. Fisher Information Metric,[0],[0]
We can optionally write I(Θ) as IΘ(Θ) to emphasize the coordinate system.,1. Fisher Information Metric,[0],[0]
"By definition, IΘ(Θ) = JᵀIΛ(Λ)J where J = (Jij), Jij = ∂Λi∂Θj is the Jacobian matrix.",1. Fisher Information Metric,[0],[0]
"For example, the FIM of regular natural exponential families (NEFs) l(Θ) = Θᵀt(x)",1. Fisher Information Metric,[0],[0]
− F (Θ) (loglinear models with sufficient statistics t(x)) is I(Θ),1. Fisher Information Metric,[0],[0]
"= ∇2F (Θ) 0, the Hessian of the log-normalizer function F (Θ).",1. Fisher Information Metric,[0],[0]
"Although exponential families can approximate arbitrarily any smooth density (Cobb et al., 1983), the lognormalizer function may not be available in closed-form nor computationally tractable (Montanari, 2015).
",1. Fisher Information Metric,[0],[0]
The FIM is an important concept for statistical machine learning.,1. Fisher Information Metric,[0],[0]
"It gives a Riemannian metric (Hotelling, 1929; Rao, 1945) of the learning parameter space which is unique (Čencov, 1982; Dowty, 2017).",1. Fisher Information Metric,[0],[0]
"Hence any learning is in a space that is intrinsically curved based on the FIM, regardless of the choice of the coordinate system.",1. Fisher Information Metric,[0],[0]
"It also gives a bound (Fréchet, 1943; Cramér, 1946; Nielsen, 2013) of learning efficiency saying that the variance of any unbiased learning of Θ is at least I−1(Θ)/n, where n is the i.i.d. sample size.",1. Fisher Information Metric,[0],[0]
"The FIM is applied to neural network optimization (Amari, 1997), metric learning (Lebanon, 2005), reinforcement learning (Thomas, 2014) and manifold learning (Sun & Marchand-Maillet, 2014).
",1. Fisher Information Metric,[0],[0]
However computing the FIM is expensive.,1. Fisher Information Metric,[0],[0]
"Besides the fact that learning machines have often singularities (Watanabe, 2009) (|I(Θ)| = 0, not full rank) characterized by plateaux in gradient learning, computing/estimating the FIM of a large neuron system (e.g. one with millions of parameters, Szegedy, Christian et al. 2015) is very challenging due to the finiteness of data, and the huge number D(D+1)2 of matrix coefficients to evaluate.",1. Fisher Information Metric,[0],[0]
"Furthermore, gradient descent techniques require inverting this large matrix and tuning the learning rate.
",1. Fisher Information Metric,[0],[0]
"To tackle this problem, past works mainly focus on how to approximate the FIM with a block diagonal form (Kurita, 1994; Le Roux et al., 2008; Martens, 2010; Pascanu & Bengio, 2014; Martens & Grosse, 2015) or quasi-diagonal form (Ollivier, 2013; Marceau-Caron & Ollivier, 2016).",1. Fisher Information Metric,[0],[0]
"This global approach faces increasing approximation error and increasing computational cost as the system scales up
and as complex and dynamic structures (Looks et al., 2017) emerge.
",1. Fisher Information Metric,[0],[0]
This work aims at a different local approach.,1. Fisher Information Metric,[0],[0]
"The idea is to accurately describe the information geometry (IG) in a subsystem of the large learning system, which is invariant to the scaling up and structural change of the global system, so that the local machinery, including optimization, can be discussed regardless of the other parts.
",1. Fisher Information Metric,[0.9595197568160803],"['In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks.']"
"For this purpose, a novel concept, the Relative Fisher Information Metric (RFIM), is defined.",1. Fisher Information Metric,[0],[0]
"Unlike the traditional geometric view of a high-dimensional parameter manifold, RFIMs defines multiple projected low-dimensional geometries of subsystems.",1. Fisher Information Metric,[0],[0]
This geometry is correlated to the parameters beyond the subsystem and is therefore considered dynamic.,1. Fisher Information Metric,[0],[0]
It can be used to characterize the efficiency of a local learning process.,1. Fisher Information Metric,[0],[0]
Taking this stance has potential in deep learning because a deep neural network can be decomposed into many local components such as neurons or layers.,1. Fisher Information Metric,[0],[0]
The RFIM is well suited to the compositional block structures of neural networks.,1. Fisher Information Metric,[0],[0]
"The RFIM can be used for out-of-core learning.
",1. Fisher Information Metric,[0],[0]
The paper is organized as follows.,1. Fisher Information Metric,[0],[0]
Sec. 2 reviews natural gradient within the context of Multi-Layer Perceptrons (MLPs).,1. Fisher Information Metric,[0],[0]
"Sec. 3 formally defines the RFIM, and gives a table of RFIMs of several commonly used subsystems.",1. Fisher Information Metric,[0],[0]
Sec. 4 discusses the advantages of using the RFIM as compared to the FIM. Sec. 5 gives an algorithmic framework and proof-of-concept experiments on neural network optimization.,1. Fisher Information Metric,[0],[0]
Sec. 6 presents related works on parameter diagonalization.,1. Fisher Information Metric,[0],[0]
Sec. 7 concludes this work and further hints at perspectives.,1. Fisher Information Metric,[0],[0]
"Consider a MLP x θ1−→ h1 · · ·hL−1 θL−−→ y, whose statistical model is the following conditional distribution
p(y |x,Θ) = ∑
h1,··· ,hL−1
p(h1 |x,θ1) · · · p(y |hL−1,θL).
",2. Natural Gradient: Review and Insights,[0],[0]
"The often intractable sum over h1, · · · ,hL−1 can be get rid off by deteriorating p(h1 |x,θ1), · · · , p(hL−1 |hL−2,θL−1) to Dirac’s deltas δ, and letting merely the last layer p(y |hL−1,θL) be stochastic.",2. Natural Gradient: Review and Insights,[0],[0]
"Other models such as restricted Boltzmann machines (Nair & Hinton, 2010; Montavon & Müller, 2012), deep belief networks (Hinton et al., 2006), dropout (Wager et al., 2013), and variational autoencoders (Kingma & Welling, 2014) do consider the hi’s to be stochastic.
",2. Natural Gradient: Review and Insights,[0],[0]
"The tensor metric of the neuromanifold (Amari, 1995) M, consisting of all MLPs with the same architecture but different parameter values, is locally defined by the FIM.",2. Natural Gradient: Review and Insights,[0],[0]
"Because a MLP corresponds to a con-
ditional distribution, its FIM is a function of the input x. By taking an empirical average over the input samples {xk}nk=1, the FIM of a MLP can be expressed as IΘ(Θ) = 1n",2. Natural Gradient: Review and Insights,[0],[0]
"∑n k=1Ep(y |xk,Θ) [ ∂lk ∂Θ ∂lk ∂Θᵀ ] , where lk(Θ) = log p(y |xk, Θ) denotes the conditional log-likelihood function wrt xk.
",2. Natural Gradient: Review and Insights,[0],[0]
"To understand the meaning of the Riemannian metric IΘ(Θ), it measures the intrinsic difference between two nearby neural networks around Θ ∈ M. A learning step can be regarded as a tiny displacement δΘ",2. Natural Gradient: Review and Insights,[0],[0]
onM.,2. Natural Gradient: Review and Insights,[0],[0]
"According to the FIM, the infinitesimal square distance
〈δΘ, δΘ〉IΘ(Θ) = 1
n n∑ k=1 Ep(y |xk,Θ)
[( δΘᵀ
∂lk ∂Θ )",2. Natural Gradient: Review and Insights,[0],[0]
"2] (1)
measures how much δΘ",2. Natural Gradient: Review and Insights,[0],[0]
"(with a radius constraint) is statistically along ∂l∂Θ , or equivalently how much δΘ affects intrinsically the conditional distribution p(y |x, Θ).
",2. Natural Gradient: Review and Insights,[0],[0]
Consider the negative log-likelihood function L(Θ) =,2. Natural Gradient: Review and Insights,[0],[0]
"− ∑n k=1 log p(yk |xk,Θ) wrt the observed pairs {(xk,yk)}nk=1, we try to minimize the loss while maintaining a small learning step size 〈δΘ, δΘ〉IΘ(Θ) on M. At Θt ∈ M, the target is to minimize wrt δΘ",2. Natural Gradient: Review and Insights,[0],[0]
"the Lagrange function
L(Θt + δΘ)",2. Natural Gradient: Review and Insights,[0],[0]
"+ 1
2γ 〈δΘ, δΘ〉IΘ(Θt)
",2. Natural Gradient: Review and Insights,[0],[0]
"≈ L(Θt) + δΘᵀ 5Θ L(Θt) + 1
2γ δΘᵀIΘ(Θt)δΘ,
where γ > 0 is a learning rate.",2. Natural Gradient: Review and Insights,[0],[0]
"The optimal solution of the above quadratic optimization gives a learning step
δΘt = −γI−1Θ (Θt)5Θ L(Θt).
",2. Natural Gradient: Review and Insights,[0],[0]
"In this update procedure, ∇̃ΘL(Θ) = I−1Θ (Θ)5Θ L(Θ) replaces the role of the usual gradient ∇ΘL(Θ) and is called the natural gradient (Amari, 1997).
",2. Natural Gradient: Review and Insights,[0],[0]
"Although the FIM depends on the chosen parameterization, the natural gradient is invariant to reparameterization.",2. Natural Gradient: Review and Insights,[0],[0]
Let Λ be another coordinate system and J be the Jacobian matrix of the mapping,2. Natural Gradient: Review and Insights,[0],[0]
"Θ→ Λ. Then we have
I−1Θ (Θ)5Θ L(Θ) =",2. Natural Gradient: Review and Insights,[0],[0]
"(J ᵀIΛ(Λ)J)−1 Jᵀ 5Λ L(Λ)
",2. Natural Gradient: Review and Insights,[0],[0]
"= J−1I−1Λ (Λ)5Λ L(Λ),
showing that ∇̃ΘL(Θ) and ∇̃ΛL(Λ) are the same dynamic up to coordinate transformation.",2. Natural Gradient: Review and Insights,[0],[0]
"As the learning rate γ is not infinitesimal in practice, natural gradient descent actually depends on the coordinate system (see e.g. Martens 2014).",2. Natural Gradient: Review and Insights,[0],[0]
"Other intriguing properties of natural gradient optimization lie in being free from getting trapped in plateaux of the error surface, and attaining Fisher efficiency in online learning (see Sec. 4 Amari 1998).
",2. Natural Gradient: Review and Insights,[0],[0]
"MΘ
Θ yx
Mθ1
x
x+ ∆x
θ1x
Mθ2h1
h1 + ∆h1
θ2h1
Mθ3
h2
h2 + ∆h2
θ3h2",2. Natural Gradient: Review and Insights,[0],[0]
"y
Model:
Manifold:
Computational graph:
Metric:
Θ
Θ I(Θ)
θ3 h2
θ3
h2
gy(θ3)
",2. Natural Gradient: Review and Insights,[0],[0]
"θ2 h1
θ2
h1
gh2(θ2)
θ1
θ1 gh1(θ1)
p(y |Θ,x) =",2. Natural Gradient: Review and Insights,[0],[0]
"∑ h1 ∑ h2 p(h1 |θ1,x) p(h2 |θ2,h1) p(y |θ3,h2)
",2. Natural Gradient: Review and Insights,[0],[0]
Figure 1.,2. Natural Gradient: Review and Insights,[0],[0]
(left),2. Natural Gradient: Review and Insights,[0],[0]
The traditional global geometry of a MLP; (right) information geometry of subsystems.,2. Natural Gradient: Review and Insights,[0],[0]
The gray and blue meshes show that the subsystem geometry is dynamic when the reference variable makes a tiny move.,2. Natural Gradient: Review and Insights,[0],[0]
"The square under the (sub-)system means the (R-)FIM is computed by (i) computing the FIM in the traditional way wrt all free parameters that affect the system output; (ii) choosing a sub-block that contains only the internal parameters of the (sub-)system and regarding the remaining variables as the reference.
",2. Natural Gradient: Review and Insights,[0],[0]
"For the sake of simplicity, we do not discuss singular FIMs with a subset of parameters having zero metric.",2. Natural Gradient: Review and Insights,[0],[0]
"This set of parameters forms an analytic variety (Watanabe, 2009), and technically the MLP as a statistical model is said to be non-regular (and the parameter Θ is not identifiable).",2. Natural Gradient: Review and Insights,[0],[0]
"The natural gradient has been extended (Thomas, 2014) to cope with singular FIMs having positive semi-definite matrices by taking the Moore-Penrose pseudo-inverse (that coincides with the inverse matrix for full rank matrices).
",2. Natural Gradient: Review and Insights,[0],[0]
"In the family of 2nd-order optimization methods, a fuzzy line can be drawn from the natural gradient and alternative methods such as the Hessian-free optimization (Martens, 2010).",2. Natural Gradient: Review and Insights,[0],[0]
"By definition, the FIM is a property of the parameter space which is independent or weakly dependent on the input samples.",2. Natural Gradient: Review and Insights,[0],[0]
"For example, the FIM of a MLP is independent of {yi}.",2. Natural Gradient: Review and Insights,[0],[0]
"In contrast, the Hessian (or related concepts such as the Gauss-Newton matrix, Martens 2014) is a property of the learning cost function wrt the input samples.
",2. Natural Gradient: Review and Insights,[0],[0]
"Bonnabel (Bonnabel, 2013) proposed to use the Riemannian exponential map to define a gradient descent step, thus ensuring to stay on the manifold for any chosen learning rate.",2. Natural Gradient: Review and Insights,[0],[0]
Convergence is proven for Hadamard manifolds (of negative curvatures).,2. Natural Gradient: Review and Insights,[0],[0]
"However, it is not mathematically tractable to express the exponential map of hierarchical model manifolds like the neuromanifold.",2. Natural Gradient: Review and Insights,[0],[0]
"In general, for large parametric systems, it is impossible to diagonalize or decorrelate all the parameters, so that we split instead all random variables into three parts θf , θ and h.",3. RFIM: Definition and Expressions,[0],[0]
We examine their intuitive meanings before giving the formal definition.,3. RFIM: Definition and Expressions,[0],[0]
"The reference, θf , consists of the majority of the random variables that are considered fixed (therefore allowing us to simplify the analysis).",3. RFIM: Definition and Expressions,[0],[0]
This is in analogy to the notion of a reference frame in physics.,3. RFIM: Definition and Expressions,[0],[0]
"θ is the
subsystem parameters, resembling the long-term memory adapting slowly to the observations (e.g. neural network weights).",3. RFIM: Definition and Expressions,[0],[0]
The response h is a random variable that reacts to the variations of θ.,3. RFIM: Definition and Expressions,[0],[0]
"Usually, h is the output of the subsystem that is connected to neighbour subsystems (e.g. hidden layer outputs).",3. RFIM: Definition and Expressions,[0],[0]
"Formally, a subsystem which factorizes the learning machine is characterized by the conditional distribution p(h |θ,θf ), where θ can be estimated based on h and θf .",3. RFIM: Definition and Expressions,[0],[0]
We make the following definition.,3. RFIM: Definition and Expressions,[0],[0]
Definition 1 (RFIM).,3. RFIM: Definition and Expressions,[0],[0]
"Given θf , the RFIM 1 of θ wrt h is
gh (θ |θf )",3. RFIM: Definition and Expressions,[0],[0]
"def = Ep(h | θ, θf ) [ ∂
∂θ log p(h |θ, θf )
∂
∂θᵀ log p(h |θ, θf )
] ,
or simply gh (θ), corresponding to the estimation of θ based on observations of h given θf .
",3. RFIM: Definition and Expressions,[0],[0]
"For example, consider a MLP.",3. RFIM: Definition and Expressions,[0],[0]
"If we choose θf to be the input features x, choose h to be the final output y, and choose θ to be all the network weights Θ, then the RFIM becomes the FIM: I(Θ) = gy(Θ |x).
",3. RFIM: Definition and Expressions,[0],[0]
"More generally, we can choose the response h to be other than the observables to compute the Fisher information of subsystems, especially dynamically during the learning of the global machine.",3. RFIM: Definition and Expressions,[0],[0]
"To see the meaning of the RFIM, similar to eq.",3. RFIM: Definition and Expressions,[0],[0]
"(1), the infinitesimal square distance 〈δθ, δθ〉gh(θ) =",3. RFIM: Definition and Expressions,[0],[0]
"Ep(h | θ, θf ) [( δθᵀ ∂∂θ log p(h |θ, θf )
)2] measures how much δθ impacts intrinsically the stochastic mapping θ → h which features the subsystem.",3. RFIM: Definition and Expressions,[0],[0]
"We have the following proposition following definition 1.
",3. RFIM: Definition and Expressions,[0],[0]
Proposition 2 (Relative Geometry Consistency).,3. RFIM: Definition and Expressions,[0],[0]
"If θ1 consists of a subset of θ2 so that θ2 = (θ1, θ̃1), then ∀θ̃1, Mθ1 with the metric gh(θ1 | θ̃1) has exactly the same Rie-
1We use the same term “relative FIM” (Zegers, 2015) with a different definition.
",3. RFIM: Definition and Expressions,[0],[0]
"mannian metric with the sub-manifold {θ2 ∈ Mθ2 : θ̃1 is fixed} induced by the ambient metric gh (θ2).
",3. RFIM: Definition and Expressions,[0],[0]
"When the response h is chosen, then different splits of (θ,θf ) are consistent with the same ambient geometry.
",3. RFIM: Definition and Expressions,[0],[0]
"Figure 1 shows the traditional global geometry of a learning system, where the curvature is defined by the learner’s parameter sensitivity to the external environment (x and y), as compared to the information geometry of subsystems, where the curvature is defined by the parameter sensitivity wrt hidden interface variables h.",3. RFIM: Definition and Expressions,[0],[0]
"The two-colored meshes show that the geometry structure is dynamic and varies with the reference variable θf .
",3. RFIM: Definition and Expressions,[0],[0]
"One should not confuse the RFIM with the diagonal blocks of the FIM (Kurita, 1994).",3. RFIM: Definition and Expressions,[0],[0]
Both their meanings and expressions are different.,3. RFIM: Definition and Expressions,[0],[0]
The RFIM is computed by integrating out the hidden response variables h.,3. RFIM: Definition and Expressions,[0],[0]
The FIM is always computed by integrating out the observables x and y.,3. RFIM: Definition and Expressions,[0],[0]
Hence the RFIM is a more general concept and includes the FIM as a special case.,3. RFIM: Definition and Expressions,[0],[0]
"This highlights a main difference with the backpropagated metric (Ollivier, 2013), which essentially considers parameter sensitivity wrt the final output.",3. RFIM: Definition and Expressions,[0],[0]
"Despite the fact that the FIMs of small parametric structures such as single neurons was studied (Amari, 1997), we are not looking at a small single-component system but a component embedded in a large system, targeting at improving the large system.
",3. RFIM: Definition and Expressions,[0],[0]
"In the following we provide a short table of commonly used RFIMs for future reference (the RFIMs listed are mostly straightforward from definition 1, with detailed derivations given in the supplementary material).",3. RFIM: Definition and Expressions,[0],[0]
This is meaningful since the RFIM is a new concept.,3. RFIM: Definition and Expressions,[0],[0]
We also want to demonstrate these simple closed form expressions without any approximations.,3. RFIM: Definition and Expressions,[0],[0]
We start from the RFIM of single neuron models.,3.1. RFIMs of One Neuron,[0],[0]
"Consider a stochastic neuron with input x and weights w. After a nonlinear activation function f , the output y is randomized surrounding the mean f(wᵀx̃) with a variance.",3.1. RFIMs of One Neuron,[0],[0]
"Throughout this paper x̃ = (xᵀ, 1)ᵀ denotes the augmented vector of x (homogeneous coordinates) so that wᵀx̃ contains a bias term, and a general linear transformation can be written simply asAx̃.
Using x as the reference, the RFIM of w with respect to y has a common form gy(w |x)",3.1. RFIMs of One Neuron,[0],[0]
"= νf (w,x)x̃x̃ᵀ, where νf (w,x) is a positive coefficient with large values in the linear region, or the effective learning zone of the neuron.",3.1. RFIMs of One Neuron,[0],[0]
"This agrees with early studies on single neuron FIMs (Amari, 1997; Kurita, 1994).
",3.1. RFIMs of One Neuron,[0],[0]
"If f(t) = tanh(t) is the hyperbolic tangent func-
tion, then νf (w,x) = sech2(wᵀx̃), where sech(t) = 2 exp(t)+exp(−t) is the hyperbolic secant function.",3.1. RFIMs of One Neuron,[0],[0]
"Similarly, if f(t) = sigm(t) is the sigmoid function, then νf (w,x) = sigm (w ᵀx̃)",3.1. RFIMs of One Neuron,[0],[0]
"[ 1− sigm (wᵀx̃) ] .
",3.1. RFIMs of One Neuron,[0],[0]
"If f is defined by Parametric Rectified Linear Unit (PReLU) (He et al., 2015), which includes Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) as a special case, so that f(t) = t (t ≥ 0), f(t) = ιt (t < 0), 0 ≤ ι < 1, then under certain approximations (see supplementary material)
",3.1. RFIMs of One Neuron,[0],[0]
"νf (w,x) =
[ ι+ (1− ι)sigm",3.1. RFIMs of One Neuron,[0],[0]
"( 1− ι ω wᵀx̃ )]2 ,
where ω > 0 is a hyper-parameter (e.g. ω = 1).
",3.1. RFIMs of One Neuron,[0],[0]
"For the exponential linear unit (ELU) (Clevert et al., 2015), f(t) = t (t ≥ 0), f(t) = α (exp(t)− 1) (t < 0), where α > 0 is a hyper-parameter.",3.1. RFIMs of One Neuron,[0],[0]
"We get
νf (w,x) =",3.1. RFIMs of One Neuron,[0],[0]
{ 1 if wᵀx̃ ≥ 0 α2 exp (2wᵀx̃),3.1. RFIMs of One Neuron,[0],[0]
if wᵀx̃ < 0.,3.1. RFIMs of One Neuron,[0],[0]
Let D denote the dimensionality of the corresponding variable.,3.2. RFIM of One Layer,[0],[0]
"A linear layer with input x, connection weights W =",3.2. RFIM of One Layer,[0],[0]
"[ w1, · · · ,wDy ] , and stochastic output y can be represented by y ∼ G(W ᵀx̃, σ2I), where I is the identity matrix, and σ is the scale of the observation noise, and G(µ,Σ) is a multivariate Gaussian distribution with mean µ and covariance matrix Σ. We vectorize W by stacking its columns {wi}.",3.2. RFIM of One Layer,[0],[0]
"Then gy(W |x) is a tensor of size (Dx + 1)Dy× (Dx + 1)Dy , given by gy(W |x)",3.2. RFIM of One Layer,[0],[0]
"= diag [x̃x̃ᵀ, · · · , x̃x̃ᵀ], where diag(·) means the (block) diagonal matrix constructed by the given matrix entries.
",3.2. RFIM of One Layer,[0],[0]
"A nonlinear layer increments a linear layer by adding an element-wise activation function applied on W ᵀx̃, and then randomized wrt the choice of the neuron.",3.2. RFIM of One Layer,[0],[0]
"By definition 1, its RFIM is given by
gy (",3.2. RFIM of One Layer,[0],[0]
W |x) =,3.2. RFIM of One Layer,[0],[0]
"diag [ νf (w1,x)x̃x̃ ᵀ, · · · , νf (wm,x)x̃x̃ᵀ ] , (2)
where νf (wi,x) is given in Subsec.",3.2. RFIM of One Layer,[0],[0]
"3.1.
",3.2. RFIM of One Layer,[0],[0]
"A softmax layer, which often appears as the last layer of a MLP, is given by y ∈ {1, . . .",3.2. RFIM of One Layer,[0],[0]
",m}, where p(y) = ηy = exp(wyx̃)∑m i=1 exp(wix̃) .",3.2. RFIM of One Layer,[0],[0]
"Its RFIM is a dense matrix given by
gy(W )",3.2. RFIM of One Layer,[0],[0]
=  (η1 − η21)x̃x̃ᵀ · · · −η1ηmx̃x̃ᵀ −η2η1x̃x̃ᵀ · · · −η2ηmx̃x̃ᵀ ... . .,3.2. RFIM of One Layer,[0],[0]
".
...",3.2. RFIM of One Layer,[0],[0]
−ηmη1x̃x̃ᵀ · · · (ηm − η2m)x̃x̃ᵀ  .,3.2. RFIM of One Layer,[0],[0]
Notice that its i’th diagonal block (ηi − η2i ),3.2. RFIM of One Layer,[0],[0]
x̃x̃ᵀ resembles the RFIM of a single sigm neuron.,3.2. RFIM of One Layer,[0],[0]
"By eq. (2), the one-layer RFIM is a product metric (Jost, 2011) and does not consider the inter-neuron correlations, which must be obtained by looking at a larger subsystem.",3.3. RFIM of Two Layers,[0],[0]
"Consider a two-layer model with stochastic output y around the mean vector f(Cᵀh̃), where h = f (W ᵀx̃).",3.3. RFIM of Two Layers,[0],[0]
"For simplicity, we ignore inter-layer correlations between the first layer and the second layer and focus on the interneuron correlations within the first layer.",3.3. RFIM of Two Layers,[0],[0]
"To do this, both x and C are considered as references to compute the RFIM of W .",3.3. RFIM of Two Layers,[0],[0]
"By definition 1, gy(W |x,C) =",3.3. RFIM of Two Layers,[0],[0]
"[Gij ]Dh×Dh and each block has the form
Gij = Dy∑ l=1 cilcjlνf (cl,h)νf (wi,x)νf (wj ,x)x̃x̃ ᵀ.
Now that we have the one-layer and two-layer RFIMs, we can either split a given feed-forward neural network into one-layer subsystems or into two-layer subsystems.",3.3. RFIM of Two Layers,[0],[0]
"A trade-off is that using a larger subsystem entails greater analytical and computational difficulty, although it could more accurately model the global system dynamics.",3.3. RFIM of Two Layers,[0],[0]
"In the extreme case, the FIM is obtained if the whole system is considered as one single subsystem.",3.3. RFIM of Two Layers,[0],[0]
This section discusses the theoretical advantages of the RFIM over the FIM.,4. RFIM: Key Advantages,[0],[0]
"Consider wlog a MLP with Bernoulli outputs y ∈ {0, 1}m, whose mean µ is a deterministic function depending on the input x and the network parameters Θ. By Sec. 2, the FIM of the MLP can be computed as (see supplementary for proof)
I(Θ)",4. RFIM: Key Advantages,[0],[0]
= 1 n n∑ i=1,4. RFIM: Key Advantages,[0],[0]
"m∑ j=1
1 µj(xi)(1− µj(xi))",4. RFIM: Key Advantages,[0],[0]
"∂µj(xi) ∂Θ ∂µj(xi) ∂Θᵀ .
",4. RFIM: Key Advantages,[0],[0]
(3) Therefore rank(I(Θ)),4. RFIM: Key Advantages,[0],[0]
≤,4. RFIM: Key Advantages,[0],[0]
nm.,4. RFIM: Key Advantages,[0],[0]
The rank of a diagonal block of I(Θ) corresponding to one layer is even smaller.,4. RFIM: Key Advantages,[0],[0]
"In a deep neural network (e.g. Szegedy, Christian et al. 2015), if the sample size n < dim(Θ)/m, then I(Θ) is doomed to be singular.",4. RFIM: Key Advantages,[0],[0]
All methods trying to approximate the FIM suffer from this problem and therefore rely on proper regularizations.,4. RFIM: Key Advantages,[0],[0]
"If the network is decomposed into layers, the RFIM of each subsystem (layer) is given by eq.",4. RFIM: Key Advantages,[0],[0]
(2).,4. RFIM: Key Advantages,[0],[0]
Each sample can contribute maximally 1 to the rank of the neuron-RFIM and can contribute maximally Dy to the rank of the layer-RFIM.,4. RFIM: Key Advantages,[0],[0]
"It only requires maxi{dim(wi)} (the maximum layer width) observations to have a full rank RFIM, where wi is the weight vector of the i’th neuron.",4. RFIM: Key Advantages,[0],[0]
The RFIM is expected to have a much higher rank than the FIM.,4. RFIM: Key Advantages,[0],[0]
Higher rank means less singularity and more information is captured.,4. RFIM: Key Advantages,[0],[0]
"Models that can
be distinguished by the RFIM may be identical in the sense of the FIM.",4. RFIM: Key Advantages,[0],[0]
"Essentially, the RFIM integrates the internal randomness (Bengio, 2013) of the neural system by considering the output of each layer as a random variable.",4. RFIM: Key Advantages,[0],[0]
"In theory, the FIM should also consider stochastic neurons.",4. RFIM: Key Advantages,[0],[0]
"However it requires marginalizing the joint distribution of h1, h2, · · · , y. This makes the already infeasible computation even more challenging.
",4. RFIM: Key Advantages,[0],[0]
"The RFIM is not an approximation of the FIM but is an accurate metric, defining the geometry of θ wrt to its direct response h in the system, or adjacent nodes in a graphical model.",4. RFIM: Key Advantages,[0],[0]
By the example in fig.,4. RFIM: Key Advantages,[0],[0]
"1, gy(θL) of the last layer is exactly the corresponding block in I(Θ): they both characterize how θL affects the mapping hL−1",4. RFIM: Key Advantages,[0],[0]
→ y. They start to diverge from the second to last layer.,4. RFIM: Key Advantages,[0],[0]
"To compute the geometry of θL−1, the RFIM looks at how θL−1 affects the local mapping hL−2 → hL−1, which can be measured reliably regardless of the rest of the system (think of a “debugging” process to separate and measure a single component).",4. RFIM: Key Advantages,[0],[0]
"In contrast, the FIM examines how θL−1 affects the non-local mapping hL−2 → y.",4. RFIM: Key Advantages,[0],[0]
This is a difficult task because it must consider the correlation between different layers.,4. RFIM: Key Advantages,[0],[0]
"As an approximation, the block diagonalized version of the FIM ignores such correlations and therefore faces the loss of accuracy.
",4. RFIM: Key Advantages,[0],[0]
The RFIM makes it possible to maintain global system stability so that the intrinsic variations of different subsystems are balanced during learning.,4. RFIM: Key Advantages,[0],[0]
Consider a set of interconnected subsystems with internal parameters {θl} and the corresponding response variables {hl}.,4. RFIM: Key Advantages,[0],[0]
The RFIM ghl(θl) measures how much the likelihood surface of hl is curved wrt a small learning step δθl.,4. RFIM: Key Advantages,[0],[0]
"By constraining the squared Riemannian distance δθᵀl g
hl(θl)δθl having similar scales, different subsystems will present similar variations during learning.",4. RFIM: Key Advantages,[0],[0]
"Within one subsystem, the learning along sensitive parameter directions is penalized.",4. RFIM: Key Advantages,[0],[0]
"Among different subsystems, the learning of sensitive subsystems is penalized.",4. RFIM: Key Advantages,[0],[0]
"Globally, the inter-subsystem stochastic connections have similar variance, maintaining a stable reference system and achieving efficient learning.",4. RFIM: Key Advantages,[0],[0]
"This is similar to the idea of batch normalization (BN) (Ioffe & Szegedy, 2015) but has a deeper theoretical foundation.
",4. RFIM: Key Advantages,[0],[0]
"Formally, we have the following theorem.
",4. RFIM: Key Advantages,[0],[0]
Theorem 3.,4. RFIM: Key Advantages,[0],[0]
"Consider a learning system represented by a joint distribution p(x,h) of x (observables) and h (hidden variables which connect subsystems).",4. RFIM: Key Advantages,[0],[0]
"The joint FIM J (Θ) = Ep ( log p(x,h |Θ) ∂Θ",4. RFIM: Key Advantages,[0],[0]
"log p(x,h |Θ) ∂Θᵀ ) has a block diagonal form.",4. RFIM: Key Advantages,[0],[0]
"Each block isEp(gh(θ)), where θ is the parameters within a subsystem and h is its response variables to neighour subsystems.
",4. RFIM: Key Advantages,[0],[0]
"The global correspondence of the local RFIM is the joint
FIM.",4. RFIM: Key Advantages,[0],[0]
"By theorem 3, the square distance dΘᵀJ (Θ)dΘ = Ep( ∑ l dθ ᵀ l g hl(θl)dθl) measures the system variance, including both the observables x and the hidden variables h.",4. RFIM: Key Advantages,[0],[0]
An intrinsic trade-off between the RFIM and the FIM is learning system stability versus efficiency.,4. RFIM: Key Advantages,[0],[0]
"Normalizing the FIM is more efficient because it helps to achieve Fisher efficiency (Amari, 1998).",4. RFIM: Key Advantages,[0],[0]
"Normalizing the RFIM is more stable since the hidden variations are bounded, which only guarantees subsystem Fisher efficiency characterized by the Cramér-Rao lower bound of local parameters.",4. RFIM: Key Advantages,[0],[0]
The traditional non-parametric way of applying natural gradient requires re-calculating the FIM and solving a large linear system in each learning step.,5. Relative Natural Gradient Descent,[0],[0]
"Besides the huge computational cost, it has a large approximation error.",5. Relative Natural Gradient Descent,[0],[0]
"For example during online learning, a mini-batch of samples cannot faithfully reflect the “true” geometry, which has to integrate the risk of sample variations.",5. Relative Natural Gradient Descent,[0],[0]
"That is, the FIM of a mini-batch is likely to be singular or poorly conditioned.
",5. Relative Natural Gradient Descent,[0],[0]
"A recent series of efforts (Montavon & Müller, 2012; Raiko et al., 2012; Desjardins et al., 2015) are gearing towards a parametric approach to applying natural gradient, which memorizes and learns a geometry.",5. Relative Natural Gradient Descent,[0],[0]
"For example, natural neural networks (Desjardins et al., 2015) augment each layer with a redundant linear layer, and let these linear layers parametrize the geometry of the neural manifold.
",5. Relative Natural Gradient Descent,[0],[0]
"By dividing the learning system into subsystems, the RFIM potentially gives a systematical implementation of parametric natural gradient descent.",5. Relative Natural Gradient Descent,[0],[0]
"The memory complexity of storing the Riemannian metric has been reduced from O(D2) to O( ∑ iD 2 i ), where Di = dim(wi) is the size of the i’th neuron.",5. Relative Natural Gradient Descent,[0],[0]
"Consider there are M neurons in total, then the memory cost is reduced by a factor of M .",5. Relative Natural Gradient Descent,[0],[0]
"The computational complexity has been reduced from O(D%) (% ≈ 2.373, Williams 2012) to O( ∑ iD % i ).",5. Relative Natural Gradient Descent,[0],[0]
"Optimization based on RFIM is called Relative Natural Gradient Descent (RNGD).
",5. Relative Natural Gradient Descent,[0],[0]
"The good performance of batch normalization (Ioffe & Szegedy, 2015) provides an empirical support for the RFIM.",5. Relative Natural Gradient Descent,[0],[0]
"Basically, BN uses an inter-sample normalization layer to transform the layer input x to z with zero mean and unit variance and thus reduces “internal covariate shift”.",5. Relative Natural Gradient Descent,[0],[0]
"In a typical case, above this normalization layer is a linear layer given by y = W ᵀz̃.",5. Relative Natural Gradient Descent,[0],[0]
"If each dimension of z is normalized, then the diagonal blocks of the linear layer RFIM gy(W )",5. Relative Natural Gradient Descent,[0],[0]
"= diag[z̃z̃ᵀ, · · · , z̃z̃ᵀ] become a covariance matrix with identity diagonal entries (after taking an empirical average).",5. Relative Natural Gradient Descent,[0],[0]
"This gives the coordinate system W a well conditioned RFIM for efficient learning.
5.1.",5. Relative Natural Gradient Descent,[0],[0]
"RNGD with a relu MLP
",5. Relative Natural Gradient Descent,[0],[0]
This subsection builds a proof-of-concept experiment on MLP optimization.,5. Relative Natural Gradient Descent,[0],[0]
We partition the MLP into layers (one layer consists of a linear layer plus an element-wise nonlinear activation function) as the subsystems.,5. Relative Natural Gradient Descent,[0],[0]
"By eq. (2), the RFIM of layer l (l = 1, · · · , L) with input hl−1 (h0 = x) and weights {wl1, · · · ,wlml} is
diag [ νf (wl1,hl−1)h̃l−1h̃ ᵀ l−1, · · · , νf (wlml ,hl−l)h̃l−1h̃ ᵀ l−1 ] .
",5. Relative Natural Gradient Descent,[0],[0]
The subsystem stability during one learning step δw can be measured geometrically by∑L l=1 ∑ml i=1,5. Relative Natural Gradient Descent,[0],[0]
"νf (wli,hl−1)(δw ᵀ lih̃l−1)
2.",5. Relative Natural Gradient Descent,[0],[0]
"Using this term as the geometric cost (the Lagrange term) in the trust region approach in Sec. 2, we get the following RNGD method.",5. Relative Natural Gradient Descent,[0],[0]
"In a stochastic gradient descent scenario, each neuron i in layer l is updated by
wnewli ← woldli −G−1li ∂E
∂wli ,
where E is the cost function and Gli is a learned metric.",5. Relative Natural Gradient Descent,[0],[0]
"The consideration is that a mini-batch of samples do not contain enough information to compute the RFIM, which should be averaged over all training samples.",5. Relative Natural Gradient Descent,[0],[0]
"Therefore, for the i’th neuron in layer l, Gli is initialized to identity, and is updated based on
Gnewli ← (1− λ)Goldli + λνf (wli,hl−1)h̃l−1h̃ ᵀ",5. Relative Natural Gradient Descent,[0],[0]
l−1,5. Relative Natural Gradient Descent,[0],[0]
+,5. Relative Natural Gradient Descent,[0],[0]
"I,
where > 0 is a hyper-parameter to avoid singularity caused by small sample size, and the average is taken over all samples in a mini-batch, and λ is a learning rate.",5. Relative Natural Gradient Descent,[0],[0]
"In theory, λ should be gradually reduced to zero to guarantee the convergence of this geometry learning.",5. Relative Natural Gradient Descent,[0],[0]
"To avoid solving a linear system in each iteration, every T iterations we recompute and store G−1li based on the most updated Gli.",5. Relative Natural Gradient Descent,[0],[0]
"In the next T iterations, this G−1li will be used as an approximation of the inverse RFIM.",5. Relative Natural Gradient Descent,[0],[0]
"For the input layer which scales with the number of input features, and the final softmax layer, we apply instead the RFIM of the corresponding linear layer to improve the computational efficiency.
",5. Relative Natural Gradient Descent,[0],[0]
We compare different optimizers on classifying MNIST digits.,5. Relative Natural Gradient Descent,[0],[0]
"The network has shape 784-80-80-80-10, with relu activation units, a final soft-max layer, and uses the persample average cross-entropy with L2-regularization as the learning cost function.",5. Relative Natural Gradient Descent,[0],[0]
"We experiment on two different architectures: one is a plain MLP (PLAIN); the other has a batch normalization layer after each hidden layer (BNA), where a rescaling parameter is applied to ensure enough flexibility of the parametric structure (Ioffe & Szegedy, 2015).",5. Relative Natural Gradient Descent,[0],[0]
"For simplicity, the architecture, mini-batch size (50), and L2 regularization strength (10−3) are fixed to be the same for all compared methods.",5. Relative Natural Gradient Descent,[0],[0]
"The observations are consistent when these configurations vary.
",5. Relative Natural Gradient Descent,[0],[0]
Figure 2 shows the learning curves of different methods.,5. Relative Natural Gradient Descent,[0],[0]
SGD is stochastic gradient descent.,5. Relative Natural Gradient Descent,[0],[0]
"ADAM is the Adam optimizer (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999 and = 10−8.",5. Relative Natural Gradient Descent,[0],[0]
"Our RNGD is implemented by modifying TensorFlow’s (Abadi, Martı́n",5. Relative Natural Gradient Descent,[0],[0]
"et al., 2015) SGD optimizer.",5. Relative Natural Gradient Descent,[0],[0]
"We set empirically T = 100, λ = 0.005 and ω = 1.
RNGD presents a sharper learning curve and better generalization, especially when it is combined with BN.",5. Relative Natural Gradient Descent,[0],[0]
"In this case, the final tranining error of RNGD is slightly larger than ADAM because by validation it favors a larger learning rate, which is applied on the neural network weights (based on RNGD) and BN parameters (based on SGD).",5. Relative Natural Gradient Descent,[0],[0]
"For the ReLU activation, νf (wi,x) is approximately binary, emphasizing such informative samples with wᵀi x̃ > 0, which are the ones contributing to the learning of wi with non-zero gradient values.",5. Relative Natural Gradient Descent,[0],[0]
Each output neuron has a different subset of informative samples.,5. Relative Natural Gradient Descent,[0],[0]
"RNGD normalizes x differently wrt different output neurons, so that the in-
formative samples for each output neuron are centered and decorrelated.
",5. Relative Natural Gradient Descent,[0],[0]
"In the above experiment, RNGD’s computational time per each epoch is roughly 4 ∼ 10 times more than SGD and ADAM on a modern graphic card.",5. Relative Natural Gradient Descent,[0],[0]
Therefore in terms of wall clock time RNGD does not show advantages.,5. Relative Natural Gradient Descent,[0],[0]
This can be improved by more efficient implementations with low rank approximation techniques and early stopping.,5. Relative Natural Gradient Descent,[0],[0]
Our RNGD prototype hints at a promising direction to develop scalable 2nd-order deep learning optimizers based on the RFIM.,5. Relative Natural Gradient Descent,[0],[0]
One may ponder whether we can always find a suitable parameterization that yields a diagonal FIM that is straightforward to invert.,6. Related Works on FIM Diagonalization,[0],[0]
This fundamental problem of parameter orthogonalization was first investigated by Jeffreys (1998) for decorrelating the parameters of interest from the nuisance parameters.,6. Related Works on FIM Diagonalization,[0],[0]
"Fisher diagonalization yields parameter orthogonalization (Cox & Reid, 1987), and is proved useful when estimating Θ̂ using a maximum likelihood estimator (MLE) that is asymptotically normally distributed, Θ̂n ∼ G(Θ, I−1(Θ)/n), and efficient since the variance of the estimator matches the Cramér-Rao lower bound.",6. Related Works on FIM Diagonalization,[0],[0]
"Using the chain rule, this amounts to find a suitable parameterization Ω = Ω(Θ) satisfying∑
",6. Related Works on FIM Diagonalization,[0],[0]
"i,j
E
[ ∂2l
∂Θi∂Θj ] ∂Θi",6. Related Works on FIM Diagonalization,[0],[0]
"∂Ωk ∂Θj ∂Ωl = 0, ∀k 6=",6. Related Works on FIM Diagonalization,[0],[0]
"l.
Thus in general, we end up with ( D 2 ) = D(D−1)2 (nonlinear) partial differential equations to satisfy (Huzurbazar, 1950).",6. Related Works on FIM Diagonalization,[0],[0]
"Therefore, in general there is no solution when( D 2 )",6. Related Works on FIM Diagonalization,[0],[0]
"> D, that is when D > 3.",6. Related Works on FIM Diagonalization,[0],[0]
"When D = 2, the single differential equation is usually solvable and tractable, and the solution may not be unique: For example, Huzurbazar (1950) reports two orthogonalization schemes for the location-scale families { 1σp0( x−µ σ )} that include the Gaussian family and the Cauchy family.",6. Related Works on FIM Diagonalization,[0],[0]
"Sometimes, the structure of the differential equation system yields a solution: For example, Jeffreys (1998) reported a parameter orthogonalization for Pearson’s distributions of type I which is of orderD = 4.",6. Related Works on FIM Diagonalization,[0],[0]
"Cox and Reid (1987) further investigated this topic with application to conditional inference, and provide examples (including the Weibull distribution).
",6. Related Works on FIM Diagonalization,[0],[0]
"From the viewpoint of geometry, the FIM induces a Riemannian manifold with metric tensor g(Θ) = I(Θ).",6. Related Works on FIM Diagonalization,[0],[0]
"When the FIM may be degenerate, this yields a pseudoRiemannian manifold (Thomas, 2014).",6. Related Works on FIM Diagonalization,[0],[0]
"In differential geometry, orthogonalization amounts to transforming the square length infinitesimal element gijdΘiΘj of a Riemannian geometry into an orthogonal system ω with match-
ing square length infinitesimal element ΩiidΩ2i .",6. Related Works on FIM Diagonalization,[0],[0]
"However, such a global orthogonal metric does not exist (Huzurbazar, 1950)",6. Related Works on FIM Diagonalization,[0],[0]
"when D > 3 for an arbitrary metric tensor, although interesting Riemannian parameterization structures may be derived in Riemannian 4D geometry (Grant & Vickers, 2009).
",6. Related Works on FIM Diagonalization,[0],[0]
"For NEFs, the FIM can be made block-diagonal easily by using the mixed coordinate system (Amari, 2016) (Θ1:k,Hk+1:D), where H = Ep[t(x)] = ∇F (Θ) is the moment parameter, for any k ∈ {1, ..., D − 1}, where vb:e denotes the subvector (vb, ..., ve)ᵀ of v. The geometry of NEFs is a dually flat structure (Amari, 2016) induced by the convex mgf, the potential function.",6. Related Works on FIM Diagonalization,[0],[0]
"It defines a dual affine coordinate systems ei = ∂i = ∂∂Hi and ej = ∂
j = ∂∂Θj that are orthogonal: 〈ei, ej〉 = δij , where δij = 1 iff i = j and δij = 0 otherwise.",6. Related Works on FIM Diagonalization,[0],[0]
Hence the FIM has two diagonal blocks.,6. Related Works on FIM Diagonalization,[0],[0]
"Those dual affine coordinate systems are defined up to an affine invertible transformation: Θ̃ = AΘ + b, H̃ = A−1H + c.",6. Related Works on FIM Diagonalization,[0],[0]
"In particular, for any order-2 NEF (D = 2), we can always obtain two mixed parameterizations (Θ1, H2) or (H1,Θ2).
",6. Related Works on FIM Diagonalization,[0],[0]
The RFIM contributes another line of thought in parameter diagonalization.,6. Related Works on FIM Diagonalization,[0],[0]
"We investigate the Fisher information of hidden variables, or internal interfaces in the learning machine.",6. Related Works on FIM Diagonalization,[0],[0]
"This is novel since the majority of previous works concentrate on the FIM of the observables, or the external interface of the machine.",6. Related Works on FIM Diagonalization,[0],[0]
"From a causality perspective, we factor out the main cause (parameters within the subsystem) of the response variable with a direct action-reaction relationship, and regard the remaining parameters as a reference that can be easily estimated by the empirical distribution.",6. Related Works on FIM Diagonalization,[0],[0]
"This simplification may lead to broader applications of Fisher information in machine learning.
",6. Related Works on FIM Diagonalization,[0],[0]
"The particular case of a mixed coordinate system (that is not an affine coordinate system) induces in information geometry (Amari, 2016) a dual pair of orthogonal e- and morthogonal foliations.",6. Related Works on FIM Diagonalization,[0],[0]
"Our splits in RFIMs consider general non-orthogonal foliations that provide the factorization decompositions of the whole manifold into submanifolds, that are the leaves of the foliation (see section 3.7 of Amari & Nagaoka 2000).",6. Related Works on FIM Diagonalization,[0],[0]
We investigate local structures of large learning systems using the new concept of Relative Fisher Information Metric.,7. Conclusion and Discussions,[0],[0]
The key advantage of this approach is that the local learning dynamics can be analyzed in an accurate way without approximation.,7. Conclusion and Discussions,[0],[0]
"We present a core list of such local structures in neural networks, and give their corresponding RFIMs.",7. Conclusion and Discussions,[0],[0]
"This list of recipes can be used to provide guiding principles to design new optimizers for deep learning.
",7. Conclusion and Discussions,[0],[0]
"Our work applies to mirror descent as well since natural gradient is related to mirror descent (Raskutti & Mukherjee, 2015) as follows:",7. Conclusion and Discussions,[0],[0]
"In mirror descent to minimize a cost function E(Θ), given a strictly convex distance function D(·, ·) in the first argument (playing the role of the proximity function), we express the gradient descent step as:
Θt+1 = arg min Θ
{ Θ>∇E(Θt) + 1
γ D(Θ,Θt)
} .
",7. Conclusion and Discussions,[0],[0]
"When D(Θ,Θ′) is chosen as a Bregman divergence BF (Θ,Θ
′) = F (Θ)− F (Θ′)− (Θ−Θ′)>∇F (Θ′) wrt to a convex function F , it has been proved that the mirror descent on the Θ-parameterization is equivalent (Raskutti & Mukherjee, 2015) to the natural gradient optimization on the induced Riemannian manifold with metric tensor (∇2F (Θ)) parameterized by the dual coordinate system H = ∇F (Θ).
",7. Conclusion and Discussions,[0],[0]
"In general, to perform a Riemannian gradient descent for minimizing a real-valued function f(Θ) on the manifold, one needs to choose a proper metric tensor given in matrix form G(Θ).",7. Conclusion and Discussions,[0],[0]
Thomas (2014) constructed a toy example showing that the natural gradient may diverge while the ordinary gradient (for G = I) converges.,7. Conclusion and Discussions,[0],[0]
"Recently, Thomas et al. (2016) proposed a new kind of descent method based on what they called the Energetic Natural Gradient that generalizes the natural gradient.",7. Conclusion and Discussions,[0],[0]
"The energy distance DE(p(Θ1), p(Θ2))2 = E[2dp(Θ1)(X,Y )",7. Conclusion and Discussions,[0],[0]
"− dp(Θ1)(X,X
′)",7. Conclusion and Discussions,[0],[0]
"− dp(Θ1)(Y, Y ′)] where X,X ′ ∼ p(Θ1) and Y, Y ′ ∼ p(Θ2), where dp(Θ1)(·, ·) is a distance metric over the support.",7. Conclusion and Discussions,[0],[0]
"Using a Taylor’s expansion on their energy distance, they get the Energy Information Matrix (in a way similar to recovering the FIM from a Taylor’s expansion of any f -divergence like the Kullback-Leibler divergence).",7. Conclusion and Discussions,[0],[0]
Their idea is to incorporate prior knowledge on the structure of the support (observation space) to define energy distance.,7. Conclusion and Discussions,[0],[0]
"Twisting the geometry of the support (say, Wasserstein’s optimal transport) with the geometry of the parametric distributions (Fisher-Rao geodesic distances) is indeed important (Chizat et al., 2015).",7. Conclusion and Discussions,[0],[0]
"In information geometry, invariance on the support is provided by a Markov morphism that is a probabilistic mapping of the support to itself (Čencov, 1982).",7. Conclusion and Discussions,[0],[0]
There is no neighbourhood structure on the support in IG.,7. Conclusion and Discussions,[0],[0]
Markov morphism includes deterministic transformation of a random variable by a statistic.,7. Conclusion and Discussions,[0],[0]
It is well-known that IT (Θ) IX(Θ) with equality iff.,7. Conclusion and Discussions,[0],[0]
T = T (X) is a sufficient statistic of X .,7. Conclusion and Discussions,[0],[0]
"Thus to get the same invariance for the energy distance (Thomas et al., 2016), one shall further require dp(Θ)(T (X), T (Y ))",7. Conclusion and Discussions,[0],[0]
"= dp(Θ)(X,Y ).
",7. Conclusion and Discussions,[0],[0]
We believe that RFIMs will provide a sound methodology to build further efficient systems for deep learning.,7. Conclusion and Discussions,[0],[0]
The full source codes to reproduce the experimental results are available at https://www.lix.polytechnique.,7. Conclusion and Discussions,[0],[0]
fr/˜nielsen/RFIM.,7. Conclusion and Discussions,[0],[0]
The authors would like to thank the anonymous reviewers and Yann Ollivier for the helpful comments.,Acknowledgements,[0],[0]
This work was mainly conducted when the first author was a postdoctoral researcher at École Polytechnique.,Acknowledgements,[0],[0]
Fisher information and natural gradient provided deep insights and powerful tools to artificial neural networks.,abstractText,[0],[0]
However related analysis becomes more and more difficult as the learner’s structure turns large and complex.,abstractText,[0],[0]
This paper makes a preliminary step towards a new direction.,abstractText,[0],[0]
"We extract a local component from a large neural system, and define its relative Fisher information metric that describes accurately this small component, and is invariant to the other parts of the system.",abstractText,[0],[0]
This concept is important because the geometry structure is much simplified and it can be easily applied to guide the learning of neural networks.,abstractText,[0],[0]
"We provide an analysis on a list of commonly used components, and demonstrate how to use this concept to further improve optimization.",abstractText,[0],[0]
1.,abstractText,[0],[0]
Fisher Information Metric The Fisher Information Metric (FIM) I(Θ) =,abstractText,[0],[0]
"(Iij) of a statistical parametric model p(x |Θ) of order D is defined by a D×D positive semidefinite (psd) matrix (I(Θ) 0) with coefficients Iij = Ep [ ∂l ∂Θi ∂l ∂Θj ] , where l(Θ) denotes the log-density function log p(x |Θ).",abstractText,[0],[0]
"Under light regularity conditions, FIM can be rewritten equivalently as Iij = −Ep [ ∂l ∂Θi∂Θj ]",abstractText,[0],[0]
Relative Fisher Information and Natural Gradient for Learning Large Modular Models,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 44–54, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality. We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese.",text,[0],[0]
"Preordering (Collins et al., 2005) aims at permuting the words of a source sentence s into a new order ś, hopefully close to a plausible target word order.",1 Introduction,[0],[0]
"Preordering is often used to bridge long distance reorderings (e.g., in Japanese- or GermanEnglish), before applying phrase-based models (Koehn et al., 2007).",1 Introduction,[0],[0]
"Preordering is often broken down into two steps: finding a suitable tree structure, and then finding a transduction function over it.",1 Introduction,[0],[0]
"A common approach is to use monolingual syntactic trees and focus on finding a transduction function of the sibling subtrees under the nodes (Lerner and Petrov, 2013; Xia and Mccord, 2004).",1 Introduction,[0],[0]
"The (direct correspondence) assumption
underlying this approach is that permuting the siblings of nodes in a source syntactic tree can produce a plausible target order.",1 Introduction,[0],[0]
"An alternative approach creates reordering rules manually and then learns the right structure for applying these rules (Katz-Brown et al., 2011).",1 Introduction,[0],[0]
"Others attempt learning the transduction structure and the transduction function in two separate, consecutive steps (DeNero and Uszkoreit, 2011).",1 Introduction,[0],[0]
"Here we address the challenge of learning both the trees and the transduction functions jointly, in one fell swoop, from word-aligned parallel corpora.
",1 Introduction,[0],[0]
Learning both trees and transductions jointly raises two questions.,1 Introduction,[0],[0]
How to obtain suitable trees for the source sentence and how to learn a distribution over random variables specifically aimed at reordering in a hierarchical model?,1 Introduction,[0],[0]
"In this work we solve both challenges by using the factorizations of permutations into Permutation Trees (PETs) (Zhang and Gildea, 2007).",1 Introduction,[0],[0]
"As we explain next, PETs can be crucial for exposing the hierarchical reordering patterns found in wordalignments.
",1 Introduction,[0],[0]
We obtain permutations in the training data by segmenting every word-aligned source-target pair into minimal phrase pairs; the resulting alignment between minimal phrases is written as a permutation (1:1 and onto) on the source side.,1 Introduction,[0],[0]
"Every permutation can be factorized into a forest of PETs (over the source sentences) which we use as a latent treebank for training a Probabilistic ContextFree Grammar (PCFG) tailor made for preordering as we explain next.
",1 Introduction,[0],[0]
Figure 1 shows two alternative PETs for the same permutation over minimal phrases.,1 Introduction,[0],[0]
"The nodes have labels (like P3142) which stand for local permutations (called prime permutation) over the child nodes; for example, the root label P3142 stands for prime permutation 〈3, 1, 4, 2〉, which says that the first child of the root becomes 3rd on the target side, the second becomes 1st, the third
44
becomes 4th and the fourth becomes 2nd.",1 Introduction,[0],[0]
"The prime permutations are non-factorizable permutations like 〈1, 2〉, 〈2, 1〉 and 〈2, 4, 1, 3〉.
",1 Introduction,[0],[0]
We think PETs are suitable for learning preordering for two reasons.,1 Introduction,[0],[0]
"Firstly, PETs specify exactly the phrase pairs defined by the permutation.",1 Introduction,[0],[0]
"Secondly, every permutation is factorizable into prime permutations only (Albert and Atkinson, 2005).",1 Introduction,[0],[0]
"Therefore, PETs expose maximal sharing between different permutations in terms of both phrases and their reordering.",1 Introduction,[0],[0]
"We expect this to be advantageous for learning hierarchical reordering.
",1 Introduction,[0],[0]
"For learning preordering, we first extract an initial PCFG from the latent treebank of PETs over the source sentences only.",1 Introduction,[0],[0]
We initialize the nonterminal set of this PCFG to the prime permutations decorating the PET nodes.,1 Introduction,[0],[0]
"Subsequently we split these coarse labels in the same way as latent variable splitting is learned for treebank parsing (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Saluja et al., 2014).",1 Introduction,[0],[0]
"Unlike treebank parsing, however, our training treebank is latent because it consists of a whole forest of PETs per training instance (s).
",1 Introduction,[0],[0]
"Learning the splits on a latent treebank of PETs results in a Reordering PCFG which we use to parse input source sentences into split-decorated trees, i.e., the labels are the splits of prime permutations.",1 Introduction,[0],[0]
"After parsing s, we map the splits back on their initial prime permutations, and then retrieve a reordered version ś of s.",1 Introduction,[0],[0]
"In this sense, our latent splits are dedicated to reordering.
",1 Introduction,[0],[0]
We face two technical difficulties alien to work on latent PCFGs in treebank parsing.,1 Introduction,[0],[0]
"Firstly, as mentioned above, permutations may factorize into more than one PET (a forest) leading to a latent training treebank.1",1 Introduction,[0],[0]
"And secondly, after we parse a source string s, we are interested in ś, the permuted version of s, not in the best derivation/PET.",1 Introduction,[0],[0]
"Exact computation is a known NP-Complete problem (Sima’an, 2002).",1 Introduction,[0],[0]
"We solve this by a new Minimum-Bayes Risk decoding approach using Kendall reordering score as loss function, which is an efficient measure over permutations (Birch and Osborne, 2011; Isozaki et al., 2010a).
",1 Introduction,[0],[0]
"In summary, this paper contributes: • A novel latent hierarchical source reordering
model working over all derivations of PETs
1All PETs for the same permutation share the same set of prime permutations but differ only in bracketing structure (Zhang and Gildea, 2007).
•",1 Introduction,[0],[0]
"A label splitting approach based on PCFGs over minimal phrases as terminals, learned from an ambiguous treebank, where the label splits start out from prime permutations.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"A fast Minimum Bayes Risk decoding over
Kendall τ reordering score for selecting ś. We report results for extensive experiments on English-Japanese showing that our Reordering PCFG gives substantial improvements when used as preordering for phrase-based models, outperforming two existing baselines for this task.",1 Introduction,[0],[0]
"We aim at learning a PCFG which we will use for parsing source sentences s into synchronous trees, from which we can obtain a reordered source version ś. Since PCFGs are non-synchronous grammars, we will use the nonterminal labels to encode reordering transductions, i.e., this PCFG is implicitly an SCFG.",2 PETs and the Hidden Treebank,[0],[0]
"We can do this because s and ś are over the same alphabet.
",2 PETs and the Hidden Treebank,[0],[0]
"Here, we have access only to a word-aligned parallel corpus, not a treebank.",2 PETs and the Hidden Treebank,[0],[0]
"The following steps summarize our approach for acquiring a latent treebank and how it is used for learning a Reordering PCFG:
1.",2 PETs and the Hidden Treebank,[0],[0]
Obtain a permutation over minimal phrases from every word-alignment.,2 PETs and the Hidden Treebank,[0],[0]
2.,2 PETs and the Hidden Treebank,[0],[0]
Obtain a latent treebank of PETs by factorizing the permutations.,2 PETs and the Hidden Treebank,[0],[0]
3. Extract a PCFG from the PETs with initial nonterminals taken from the PETs.,2 PETs and the Hidden Treebank,[0],[0]
4.,2 PETs and the Hidden Treebank,[0],[0]
"Learn to split the initial nonterminals and estimate rule probabilities.
",2 PETs and the Hidden Treebank,[0],[0]
"These steps are detailed in the next section, but we will start out with an intuitive exposition of PETs, the latent treebank and the Reordering Grammar.
",2 PETs and the Hidden Treebank,[0],[0]
"Figure 1 shows examples of how PETs look like – see (Zhang and Gildea, 2007) for algorithmic details.",2 PETs and the Hidden Treebank,[0],[0]
Here we label the nodes with nonterminals which stand for prime permutations from the operators on the PETs.,2 PETs and the Hidden Treebank,[0],[0]
"For example, nonterminals P12, P21 and P3142 correspond respectively to reordering transducers 〈1, 2〉, 〈2, 1〉 and 〈3, 1, 4, 2〉.",2 PETs and the Hidden Treebank,[0],[0]
"A prime permutation on a source node µ is a transduction dictating how the children of µ are reordered at the target side, e.g., P21 inverts the child order.",2 PETs and the Hidden Treebank,[0],[0]
"We must stress that any similarity with ITG (Wu, 1997) is restricted to the fact that the straight and inverted operators of ITG are the binary case of prime permutations
in PETs (P12 and P21).",2 PETs and the Hidden Treebank,[0],[0]
"ITGs recognize only the binarizable permutations, which is a major restriction when used on the data: there are many nonbinarizable permutations in actual data (Wellington et al., 2006).",2 PETs and the Hidden Treebank,[0],[0]
"In contrast, our PETs are obtained by factorizing permutations obtained from the data, i.e., they exactly fit the range of prime permutations in the parallel corpus.",2 PETs and the Hidden Treebank,[0],[0]
"In practice we limit them to maximum arity 5.
",2 PETs and the Hidden Treebank,[0],[0]
"We can extract PCFG rules from the PETs, e.g., P21 → P12 P2413.",2 PETs and the Hidden Treebank,[0],[0]
"However, these rules are decorated with too coarse labels.",2 PETs and the Hidden Treebank,[0],[0]
"A similar problem was encountered in non-lexicalized monolingual parsing, and one solution was to lexicalize the productions (Collins, 2003) using head words.",2 PETs and the Hidden Treebank,[0],[0]
"But linguistic heads do not make sense for PETs, so we opt for the alternative approach (Matsuzaki et al., 2005), which splits the nonterminals and softly percolates the splits through the trees gradually fitting them to the training data.",2 PETs and the Hidden Treebank,[0],[0]
"Splitting has a shadow side, however, because it leads to combinatorial explosion in grammar size.
",2 PETs and the Hidden Treebank,[0],[0]
Suppose for example node P21 could split into P211 and P212 and similarly P2413 splits into P24131 and 24132.,2 PETs and the Hidden Treebank,[0],[0]
"This means that rule P21 → P12 P2413 will form eight new rules:
P211 → P121 P24131",2 PETs and the Hidden Treebank,[0],[0]
P211 → P121 P24132 P211,2 PETs and the Hidden Treebank,[0],[0]
→ P122 P24131,2 PETs and the Hidden Treebank,[0],[0]
P211 → P122 P24132 P212 → P121 P24131,2 PETs and the Hidden Treebank,[0],[0]
P212 → P121 P24132 P212 → P122 P24131,2 PETs and the Hidden Treebank,[0],[0]
"P212 → P122 P24132
Should we want to split each nonterminal into 30 subcategories, then an n-ary rule will split into 30n+1 new rules, which is prohibitively large.",2 PETs and the Hidden Treebank,[0],[0]
Here we use the “unary trick” as in Figure 2.,2 PETs and the Hidden Treebank,[0],[0]
The superscript on the nonterminals denotes the child position from left to right.,2 PETs and the Hidden Treebank,[0],[0]
"For example P2121 means that this node is a second child, and the
mother nonterminal label is P211.",2 PETs and the Hidden Treebank,[0],[0]
"For the running example rule, this gives the following rules:
P211 → P2111 P2121 P212",2 PETs and the Hidden Treebank,[0],[0]
→ P2112 P2122,2 PETs and the Hidden Treebank,[0],[0]
P2111,2 PETs and the Hidden Treebank,[0],[0]
→ P121 P2121 → P24131,2 PETs and the Hidden Treebank,[0],[0]
P2111,2 PETs and the Hidden Treebank,[0],[0]
→ P122 P2121 → P24132 P2112 → P121 P2122 → P24131,2 PETs and the Hidden Treebank,[0],[0]
P2112 → P122 P2122,2 PETs and the Hidden Treebank,[0],[0]
"→ P24132
",2 PETs and the Hidden Treebank,[0],[0]
"The unary trick leads to substantial reduction in grammar size, e.g., for arity 5 rules and 30 splits we could have had 306 = 729000000 split-rules, but with the unary trick we only have 30+302∗5 = 4530 split rules.",2 PETs and the Hidden Treebank,[0],[0]
"The unary trick was used in early lexicalized parsing work (Carroll and Rooth, 1998).2 This split PCFG constitutes a latent PCFG because the splits cannot be read of a treebank.",2 PETs and the Hidden Treebank,[0],[0]
"It must be learned from the latent treebank of PETs, as described next.",2 PETs and the Hidden Treebank,[0],[0]
"Obtaining permutations Given a source sentence s and its alignment a to a target sentence
2After applying the unary trick, we add a constraint on splitting: all nonterminals on an n-ary branching rule must be split simultaneously.
",3 Details of Latent Reordering PCFG,[0],[0]
"t in the training corpus, we segment 〈s,a, t〉 into a sequence of minimal phrases sm (maximal sequence) such that the reordering between these minimal phrases constitutes a permutation πm.",3 Details of Latent Reordering PCFG,[0],[0]
"We do not extract non-contiguous or non-minimal phrases because reordering them often involves complicated transductions which could hamper the performance of our learning algorithm.3
Unaligned words Next we describe the use of the factorization of permutations into PET forests for training a PCFG model.",3 Details of Latent Reordering PCFG,[0],[0]
But first we need to extend the PETs to allow for unaligned words.,3 Details of Latent Reordering PCFG,[0],[0]
"An unaligned word is joined with a neighboring phrase to the left or the right, depending on the source language properties (e.g., whether the language is head-initial or -final (Chomsky, 1970)).",3 Details of Latent Reordering PCFG,[0],[0]
"Our experiments use English as source language (head-initial), so the unaligned words are joined to phrases to their right.",3 Details of Latent Reordering PCFG,[0],[0]
This modifies a PET by adding a new binary branching node µ (dominating the unaligned word and the phrase it is joined to) which is labeled with a dedicated nonterminal: P01 if the unaligned word joins to the right and P10 if it joins to the left.,3 Details of Latent Reordering PCFG,[0],[0]
"We decompose the permutation πm into a forest of permutation trees PEF (πm) in O(n3), following algorithms in (Zhang et al., 2008; Zhang and Gildea, 2007) with trivial modifications.",3.1 Probability model,[0],[0]
Each PET ∆ ∈ PEF (πm) is a different bracketing (differing in binary branching structure only).,3.1 Probability model,[0],[0]
"We consider the bracketing hidden in the latent treebank, and apply unsupervised learning to induce a distribution over possible bracketings.",3.1 Probability model,[0],[0]
Our probability model starts from the joint probability of a sequence of minimal phrases sm and a permutation πm over it.,3.1 Probability model,[0],[0]
"This demands summing over all PETs ∆ in the forest PEF (πm), and for every PET also over all its label splits, which are given by the grammar derivations",3.1 Probability model,[0],[0]
"d:
P (sm, πm) = ∑
∆∈PEF (πm) ∑ d∈∆ P (d, sm) (1)
",3.1 Probability model,[0],[0]
"The probability of a derivation d is a product of probabilities of all the rules r that build it:
P (sm, πm) = ∑
∆∈PEF (πm) ∑ d∈∆ ∏ r∈d P (r) (2)
3Which differs from (Quirk and Menezes, 2006).
",3.1 Probability model,[0],[0]
"As usual, the parameters of this model are the PCFG rule probabilities which are estimated from the latent treebank using EM as explained next.",3.1 Probability model,[0],[0]
"For training the latent PCFG over the latent treebank, we resort to EM (Dempster et al., 1977) which estimates PCFG rule probabilities to maximize the likelihood of the parallel corpus instances.",3.2 Learning Splits on Latent Treebank,[0],[0]
"Computing expectations for EM is done efficiently using Inside-Outside (Lari and Young, 1990).",3.2 Learning Splits on Latent Treebank,[0],[0]
"As in other state splitting models (Matsuzaki et al., 2005), after splitting the nonterminals, we distribute the probability uniformly over the new rules, and we add to each new rule some random noise to break the symmetry.",3.2 Learning Splits on Latent Treebank,[0],[0]
"We split the non-terminals only once as in (Matsuzaki et al., 2005) (unlike (Petrov et al., 2006)).",3.2 Learning Splits on Latent Treebank,[0],[0]
For estimating the distribution for unknown words we replace all words that appear ≤ 3 times with the “UNKNOWN” token.,3.2 Learning Splits on Latent Treebank,[0],[0]
"We use CKY+ (Chappelier and Rajman, 1998) to parse a source sentence s into a forest using the learned split PCFG.",3.3 Inference,[0],[0]
"Unfortunately, computing the most-likely permutation (or alternatively ś) as in
argmax π∈Π ∑ ∆∈PEF (π) ∑ d∈∆ P (d, πm)
from a lattice of permutations Π using a PCFG is NP-complete (Sima’an, 2002).",3.3 Inference,[0],[0]
"Existing techniques, like variational decoding or MinimumBayes Risk (MBR), used for minimizing loss over trees as in (Petrov and Klein, 2007), are not directly applicable here.",3.3 Inference,[0],[0]
"Hence, we opt for minimizing the risk of making an error under a loss function over permutations using the MBR decision rule (Kumar and Byrne, 2004):
π̂ = argmin π ∑ πr Loss(π, πr)P (πr) (3)
",3.3 Inference,[0],[0]
The loss function we minimize is Kendall τ,3.3 Inference,[0],[0]
"(Birch and Osborne, 2011; Isozaki et al., 2010a) which is a ratio of wrongly ordered pairs of words (including gapped pairs) to the total number of pairs.",3.3 Inference,[0],[0]
We do Monte Carlo sampling of 10000 derivations from the chart of the s and then find the least risky permutation in terms of this loss.,3.3 Inference,[0],[0]
"We sample from the true distribution by sampling edges recursively
using their inside probabilities.",3.3 Inference,[0],[0]
"An empirical distribution over permutations P (π) is given by the relative frequency of π in the sample.
",3.3 Inference,[0],[0]
With large samples it is hard to efficiently compute expected Kendall τ loss for each sampled hypothesis.,3.3 Inference,[0],[0]
For sentence of length k and sample of size n the complexity of a naive algorithm is O(n2k2).,3.3 Inference,[0],[0]
Computing Kendall τ alone takes O(k2).,3.3 Inference,[0],[0]
"We use the fact that Kendall τ decomposes as a linear function over all skip-bigrams b that could be built for any permutation of length k:
Kendall(π, πr) = ∑ b 1− δ(π, b) k(k−1) 2 δ(πr, b) (4)
",3.3 Inference,[0],[0]
"Here δ returns 1 if permutation π contains the skip bigram b, otherwise it returns 0.",3.3 Inference,[0],[0]
"With this decomposition we can use the method from (DeNero et al., 2009) to efficiently compute the MBR hypothesis.",3.3 Inference,[0],[0]
"Combining Equations 3 and 4 we get:
π̂ = argmin π ∑ πr ∑ b 1− δ(π, b) k(k−1) 2 δ(πr, b)P (πr) (5)
",3.3 Inference,[0],[0]
"We can move the summation inside and reformulate the expected Kendall τ loss as expectation over the skip-bigrams of the permutation.
",3.3 Inference,[0],[0]
= argmin π ∑ b,3.3 Inference,[0],[0]
"(1− δ(π, b))",3.3 Inference,[0],[0]
"[∑ πr δ(πr, b)P (πr) ] (6)
= argmin π ∑ b (1− δ(π, b))EP (πr)δ(πr, b) (7)
= argmax π ∑ b δ(π, b)EP (πr)δ(πr, b) (8)
This means we need to pass through the sampled list only twice: (1) to compute expectations over skip bigrams and (2) to compute expected loss of each sampled permutation.",3.3 Inference,[0],[0]
The time complexity is O(nk2) which is quite fast in practice.,3.3 Inference,[0],[0]
We conduct experiments with three baselines:,4 Experiments,[0],[0]
• Baseline A: No preordering.,4 Experiments,[0],[0]
"• Baseline B: Rule based preordering (Isozaki
et al., 2010b), which first obtains an HPSG parse tree using Enju parser 4 and after that swaps the children by moving the syntactic head to the final position to account for different head orientation in English and Japanese.
",4 Experiments,[0],[0]
"4http://www.nactem.ac.uk/enju/
• Baseline C: LADER (Neubig et al., 2012): latent variable preordering that is based on ITG and large-margin training with latent variables.",4 Experiments,[0],[0]
"We used LADER in standard settings without any linguistic features (POS tags or syntactic trees).
",4 Experiments,[0],[0]
And we test four variants of our model:,4 Experiments,[0],[0]
• RGleft - only canonical left branching PET •,4 Experiments,[0],[0]
"RGright - only canonical right branching PET • RGITG-forest - all PETs that are binary (ITG) • RGPET-forest - all PETs.
",4 Experiments,[0],[0]
We test these models on English-Japanese NTCIR-8 Patent Translation (PATMT) Task.,4 Experiments,[0],[0]
For tuning we use all NTCIR-7 dev sets and for testing the test set from NTCIR-9 from both directions.,4 Experiments,[0],[0]
All used data was tokenized (English with Moses tokenizer and Japanese with KyTea 5) and filtered for sentences between 4 and 50 words.,4 Experiments,[0],[0]
"A subset of this data is used for training the Reordering Grammar, obtained by filtering out sentences that have prime permutations of arity > 5, and for the ITG version arity > 2.",4 Experiments,[0],[0]
Baseline C was trained on 600 sentences because training is prohibitively slow.,4 Experiments,[0],[0]
"Table 1 shows the sizes of data used.
",4 Experiments,[0],[0]
The Reordering Grammar was trained for 10 iterations of EM on train RG data.,4 Experiments,[0],[0]
We use 30 splits for binary non-terminals and 3 for non-binary.,4 Experiments,[0],[0]
Training on this dataset takes 2 days and parsing tuning and testing set without any pruning takes 11 and 18 hours respectively.,4 Experiments,[0],[0]
We test how well our model predicts gold reorderings before translation by training the alignment model using MGIZA++ 6 on the training corpus and using it to align the test corpus.,4.1 Intrinsic evaluation,[0],[0]
"Gold reorderings for the test corpus are obtained by sorting words by their average target position and (unaligned words follow their right neighboring
5http://www.phontron.com/kytea/ 6http://www.kyloo.net/software/doku.php/mgiza:overview
word).",4.1 Intrinsic evaluation,[0],[0]
"We use Kendall τ score for evaluation (note the difference with Section 3.3 where we defined it as a loss function).
",4.1 Intrinsic evaluation,[0],[0]
Table 2 shows that our models outperform all baselines on this task.,4.1 Intrinsic evaluation,[0],[0]
"The only strange result here is that rule-based preordering obtains a lower score than no preordering, which might be an artifact of the Enju parser changing the tokenization of its input, so the Kendall τ of this system might not really reflect the real quality of the preordering.",4.1 Intrinsic evaluation,[0],[0]
All other systems use the same tokenization.,4.1 Intrinsic evaluation,[0],[0]
"The reordered output of all the mentioned baselines and versions of our model are translated with phrase-based MT system (Koehn et al., 2007) (distortion limit set to 6 with distance based reordering model) that is trained on gold preordering of the training data 7 ś − t.",4.2 Extrinsic evaluation in MT,[0],[0]
"The only exception is Baseline A which is trained on original s− t.
We use a 5-gram language model trained with KenLM 8, tune 3 times with kb-mira (Cherry and Foster, 2012) to account for tuner instability and evaluated using Multeval 9 for statistical significance on 3 metrics: BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and TER (Snover et al., 2006).",4.2 Extrinsic evaluation in MT,[0],[0]
"We additionally report RIBES score (Isozaki et al., 2010a) that concentrates on word order more than other metrics.
",4.2 Extrinsic evaluation in MT,[0],[0]
Single or all PETs?,4.2 Extrinsic evaluation in MT,[0],[0]
In Table 3 we see that using all PETs during training makes a big impact on performance.,4.2 Extrinsic evaluation in MT,[0],[0]
"Only the all PETs variants
7Earlier work on preordering applies the preordering model to the training data to obtain a parallel corpus of guessed ś − t pairs, which are the word re-aligned and then used for training the back-end MT system (Khalilov and Sima’an, 2011).",4.2 Extrinsic evaluation in MT,[0],[0]
"We skip this, we take the risk of mismatch between the preordering and the back-end system, but this simplifies training and saves a good amount of training time.
8http://kheafield.com/code/kenlm/ 9https://github.com/jhclark/multeval
(RGITG-forest and RGPET-forest) significantly outperform all baselines.",4.2 Extrinsic evaluation in MT,[0],[0]
"If we are to choose a single PET per training instance, then learning RG from only left-branching PETs (the one usually chosen in other work, e.g. (Saluja et al., 2014)) performs slightly worse than the right-branching PET.",4.2 Extrinsic evaluation in MT,[0],[0]
This is possibly because English is mostly rightbranching.,4.2 Extrinsic evaluation in MT,[0],[0]
"So even though both PETs describe the same reordering, RGright captures reordering over English input better than RGleft.
",4.2 Extrinsic evaluation in MT,[0],[0]
All PETs or binary only?,4.2 Extrinsic evaluation in MT,[0],[0]
RGPET-forest performs significantly better than RGITG-forest (p < 0.05).,4.2 Extrinsic evaluation in MT,[0],[0]
"Non-ITG reordering operators are predicted rarely (in only 99 sentences of the test set), but they make a difference, because these operators often appear high in the predicted PET.",4.2 Extrinsic evaluation in MT,[0],[0]
"Furthermore, having these operators during training might allow for better fit to the data.
",4.2 Extrinsic evaluation in MT,[0],[0]
How much reordering is resolved by the Reordering Grammar?,4.2 Extrinsic evaluation in MT,[0],[0]
"Obviously, completely factorizing out the reordering from the translation process is impossible because reordering depends to a certain degree on target lexical choice.",4.2 Extrinsic evaluation in MT,[0],[0]
"To quantify the contribution of Reordering Grammar, we tested decoding with different distortion limit values in the SMT system.",4.2 Extrinsic evaluation in MT,[0],[0]
"We compare the phrase-based (PB) system with distance based cost function for reordering (Koehn et al., 2007) with and without preordering.
",4.2 Extrinsic evaluation in MT,[0],[0]
Figure 3 shows that Reordering Grammar gives substantial performance improvements at all distortion limits (both BLEU and RIBES).,4.2 Extrinsic evaluation in MT,[0],[0]
RGPET-forest is less sensitive to changes in decoder distortion limit than standard PBSMT.,4.2 Extrinsic evaluation in MT,[0],[0]
"The perfor-
mance of RGPET-forest varies only by 1.1 BLEU points while standard PBSMT by 4.3 BLEU points.",4.2 Extrinsic evaluation in MT,[0],[0]
Some local reordering in the decoder seems to help RGPET-forest but large distortion limits seem to degrade the preordering choice.,4.2 Extrinsic evaluation in MT,[0],[0]
"This shows also that the improved performance of RGPET-forest is not only a result of efficiently exploring the full space of permutations, but also a result of improved scoring of permutations.
",4.2 Extrinsic evaluation in MT,[0],[0]
Does the improvement remain for a decoder with MSD reordering model?,4.2 Extrinsic evaluation in MT,[0],[0]
"We compare the RGPET-forest preordered model against a decoder that uses the strong MSD model (Tillmann, 2004; Koehn et al., 2007).",4.2 Extrinsic evaluation in MT,[0],[0]
Table 4 shows that using Reordering Grammar as front-end to MSD reordering (full Moses) improves performance by 2.8 BLEU points.,4.2 Extrinsic evaluation in MT,[0],[0]
"The improvement is confirmed by METEOR, TER and RIBES.",4.2 Extrinsic evaluation in MT,[0],[0]
"Our preordering model and MSD are complementary – the Reordering Grammar captures long distance reordering, while MSD possibly does better local reorderings, especially reorderings conditioned on the lexical part of translation units.
",4.2 Extrinsic evaluation in MT,[0],[0]
"Interestingly, the MSD model (BLEU 29.6) improves over distance-based reordering (BLEU 27.8) by (BLEU 1.8), whereas the difference between these systems as back-ends to Reordering Grammar (respectively BLEU 32.4 and 32.0) is
far smaller (0.4 BLEU).",4.2 Extrinsic evaluation in MT,[0],[0]
This suggests that a major share of reorderings can be handled well by preordering without conditioning on target lexical choice.,4.2 Extrinsic evaluation in MT,[0],[0]
"Furthermore, this shows that RGPET-forest preordering is not very sensitive to the decoder’s reordering model.
",4.2 Extrinsic evaluation in MT,[0],[0]
Comparison to a Hierarchical model (Hiero).,4.2 Extrinsic evaluation in MT,[0],[0]
"Hierarchical preordering is not intended for a hierarchical model as Hiero (Chiang, 2005).",4.2 Extrinsic evaluation in MT,[0],[0]
"Yet, here we compare our preordering system (PB MSD+RG) to Hiero for completeness, while we should keep in mind that Hiero’s reordering model has access to much richer training data.",4.2 Extrinsic evaluation in MT,[0],[0]
"We will discuss these differences shortly.
",4.2 Extrinsic evaluation in MT,[0],[0]
"Table 4 shows that the difference in BLEU is not statistically significant, but there is more difference in METEOR and TER. RIBES, which concentrates more on reordering, prefers Reordering Grammar over Hiero.",4.2 Extrinsic evaluation in MT,[0],[0]
It is somewhat surprising that a preordering model combined with a phrase-based model succeeds to rival Hiero’s performance on English-Japanese.,4.2 Extrinsic evaluation in MT,[0],[0]
"Especially when looking at the differences between the two:
1.",4.2 Extrinsic evaluation in MT,[0],[0]
"Reordering Grammar uses only minimal phrases, while Hiero uses composite (longer) phrases which encapsulate internal reorderings, but also non-contiguous phrases.",4.2 Extrinsic evaluation in MT,[0],[0]
2.,4.2 Extrinsic evaluation in MT,[0],[0]
"Hiero conditions its reordering on the lexical target side, whereas the Reordering Grammar does not (by definition).",4.2 Extrinsic evaluation in MT,[0],[0]
3.,4.2 Extrinsic evaluation in MT,[0],[0]
"Hiero uses a range of features, e.g., a language model, while Reordering Grammar is a mere generative PCFG.",4.2 Extrinsic evaluation in MT,[0],[0]
"The advantages of Hiero can be brought to bear upon Reordering Grammar by reformulating it as a discriminative model.
",4.2 Extrinsic evaluation in MT,[0],[0]
Which structure is learned?,4.2 Extrinsic evaluation in MT,[0],[0]
"Figure 4 shows an example PET output showing how our model learns: (1) that the article “the” has no equivalent in Japanese, (2) that verbs go after their object, (3) to use postpositions instead of prepositions, and (4) to correctly group certain syntactic units, e.g. NPs and VPs.",4.2 Extrinsic evaluation in MT,[0],[0]
"The majority of work on preordering is based on syntactic parse trees, e.g., (Lerner and Petrov, 2013; Khalilov and Sima’an, 2011; Xia and Mccord, 2004).",5 Related work,[0],[0]
Here we concentrate on work that has common aspects with this work.,5 Related work,[0],[0]
"Neubig et
al (2012) trains a latent non-probabilistic discriminative model for preordering as an ITG-like grammar limited to binarizable permutations.",5 Related work,[0],[0]
Tromble and Eisner (2009) use ITG but do not train the grammar.,5 Related work,[0],[0]
They only use it to constrain the local search.,5 Related work,[0],[0]
DeNero and Uszkoreit (2011) present two separate consecutive steps for unsupervised induction of hierarchical structure (ITG) and the induction of a reordering function over it.,5 Related work,[0],[0]
"In contrast, here we learn both the structure and the reordering function simultaneously.",5 Related work,[0],[0]
"Furthermore, at test time, our inference with MBR over a measure of permutation (Kendall) allows exploiting both structure and reordering weights for inference, whereas test-time inference in (DeNero and Uszkoreit, 2011) is also a two step process – the parser forwards to the next stage the best parse.
",5 Related work,[0],[0]
Dyer and Resnik (2010) treat reordering as a latent variable and try to sum over all derivations that lead not only to the same reordering but also to the same translation.,5 Related work,[0],[0]
"In their work they consider all permutations allowed by a given syntactic tree.
",5 Related work,[0],[0]
"Saers et al (2012) induce synchronous grammar for translation by splitting the non-terminals, but unlike our approach they split generic nonterminals and not operators.",5 Related work,[0],[0]
Their most expressive grammar covers only binarizable permutations.,5 Related work,[0],[0]
The decoder that uses this model does not try to sum over many derivations that have the same yield.,5 Related work,[0],[0]
They do not make independence assumption like our “unary trick” which is probably the reason they do not split more than 8 times.,5 Related work,[0],[0]
"They do not compare their results to any other SMT system and test on a very small dataset.
",5 Related work,[0],[0]
"Saluja et al (2014) attempts inducing a refined Hiero grammar (latent synchronous CFG) from Normalized Decomposition Trees (NDT) (Zhang et al., 2008).",5 Related work,[0],[0]
"While there are similarities with
the present work, there are major differences.",5 Related work,[0],[0]
"On the similarity side, NDTs are decomposing alignments in ways similar to PETs, and both Saluja’s and our models refine the labels on the nodes of these decompositions.",5 Related work,[0],[0]
"However, there are major differences between the two:
• Our model is completely monolingual and unlexicalized (does not condition its reordering on the translation) in contrast with the Latent SCFG used in (Saluja et al., 2014), • Our Latent PCFG label splits are defined
as refinements of prime permutations, i.e., specifically designed for learning reordering, whereas (Saluja et al., 2014) aims at learning label splitting that helps predicting NDTs from source sentences, • Our model exploits all PETs and all deriva-
tions, both during training (latent treebank) and during inferences.",5 Related work,[0.9500865936076183],"['Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data.']"
"In (Saluja et al., 2014) only left branching NDT derivations are used for learning the model.",5 Related work,[0],[0]
•,5 Related work,[0],[0]
"The training data used by (Saluja et al., 2014)
is about 60 times smaller in number of words than the data used here; the test set of (Saluja et al., 2014) also consists of far shorter sentences where reordering could be less crucial.
",5 Related work,[0],[0]
"A related work with a similar intuition is presented in (Maillette de Buy Wenniger and Sima’an, 2014), where nodes of a tree structure similar to PETs are labeled with reordering patterns obtained by factorizing word alignments into Hierarchical Alignment Trees.",5 Related work,[0],[0]
These patterns are used for labeling the standard Hiero grammar.,5 Related work,[0],[0]
"Unlike this work, the labels extracted by (Maillette de Buy Wenniger and Sima’an, 2014) are clustered manually into less than a dozen labels without the possibility of fitting the labels to the training data.",5 Related work,[0],[0]
We present a generative Reordering PCFG model learned from latent treebanks over PETs obtained by factorizing permutations over minimal phrase pairs.,6 Conclusion,[0],[0]
Our Reordering PCFG handles non-ITG reordering patterns (up to 5-ary branching) and it works with all PETs that factorize a permutation (rather than a single PET).,6 Conclusion,[0],[0]
To the best of our knowledge this is the first time both extensions are shown to improve performance.,6 Conclusion,[0],[0]
"The empirical results on English-Japanese show that (1) when used for preordering, the Reordering PCFG helps particularly with relieving the phrase-based model from long range reorderings, (2) combined with a state-of-the-art phrase model, Reordering PCFG shows performance not too different from Hiero, supporting the common wisdom of factorizing long range reordering outside the decoder, (3) Reordering PCFG generates derivations that seem to coincide well with linguistically-motivated reordering patterns for English-Japanese.",6 Conclusion,[0],[0]
"There are various direction we would like to explore, the most obvious of which are integrating the learned reordering with other feature functions in a discriminative setting, and extending the model to deal with non-contiguous minimal phrases.",6 Conclusion,[0],[0]
This work is supported by STW grant nr. 12271 and NWO VICI grant nr. 277-89-002.,Acknowledgments,[0],[0]
We thank Wilker Aziz for comments on earlier version of the paper and discussions about MBR and sampling.,Acknowledgments,[0],[0]
"We present a novel approach for unsupervised induction of a Reordering Grammar using a modified form of permutation trees (Zhang and Gildea, 2007), which we apply to preordering in phrase-based machine translation.",abstractText,[0],[0]
"Unlike previous approaches, we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora.",abstractText,[0],[0]
"Furthermore, our model (1) handles non-ITG reordering patterns (up to 5-ary branching), (2) is learned from all derivations by treating not only labeling but also bracketing as latent variable, (3) is entirely unlexicalized at the level of reordering rules, and (4) requires no linguistic annotation.",abstractText,[0],[0]
"Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality.",abstractText,[0],[0]
"We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese.",abstractText,[0],[0]
Reordering Grammar Induction,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2401–2410 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Deep neural models are known to be computationally expensive to train even with fast hardware (Sutskever et al., 2014; Wu et al., 2016).",1 Introduction,[0],[0]
"For example, it takes three weeks to train a deep neural machine translation system on 100 Graphics Processing Units (GPUs) (Wu et al., 2016).",1 Introduction,[0],[0]
"Furthermore, a large amount of data is usually required to train effective neural models (Goodfellow et al., 2016; Hirschberg and Manning, 2015).
",1 Introduction,[0],[0]
Bengio et al. (2009) and Kumar et al. (2010) developed training paradigms which are inspired by the learning principle that humans can learn more effectively when training starts with easier concepts and gradually proceeds with more difficult concepts.,1 Introduction,[0],[0]
"Since these approaches are motivated by
1Our code is available at scholar.harvard.edu/ hadi/RbF/
a “starting small” strategy they are called curriculum or self-paced learning.
",1 Introduction,[0],[0]
"In this paper, we present a novel training paradigm which is inspired by the broad evidence in psychology that shows human ability to retain information improves with repeated exposure and exponentially decays with delay since last exposure (Cepeda et al., 2006; Averell and Heathcote, 2011).",1 Introduction,[0],[0]
"Spaced repetition was presented in psychology (Dempster, 1989) and forms the building block of many educational devices, including flashcards, in which small pieces of information are repeatedly presented to a learner on a schedule determined by a spaced repetition algorithm.",1 Introduction,[0],[0]
"Such algorithms show that human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (Dempster, 1989; Novikoff et al., 2012).
",1 Introduction,[0],[0]
"We investigate the analogy between training neural models and findings in psychology about human memory model and develop a spaced repetition algorithm (named Repeat before Forgetting, RbF) to efficiently and effectively train neural models.",1 Introduction,[0],[0]
The core part of our algorithm is a scheduler that ensures a given neural network spends more time working on difficult training instances and less time on easier ones.,1 Introduction,[0],[0]
"Our scheduler is inspired by factors that affect human memory retention, namely, difficulty of learning materials, delay since their last review, and strength of memory.",1 Introduction,[0],[0]
The scheduler uses these factors to lengthen or shorten review intervals with respect to individual learners and training instances.,1 Introduction,[0],[0]
"We evaluate schedulers based on their scheduling accuracy, i.e., accuracy in estimating network memory retention with respect to previously-seen instances, as well as their effect on the efficiency and effectiveness of downstream neural networks.2
2 In this paper, we use the terms memory retention, recall, and learning interchangeably.
2401
The contributions of this paper are: (1) we show that memory retention in neural networks is affected by the same (known) factors that affect memory retention in humans, (2) we present a novel training paradigm for neural networks based on spaced repetition, and (3) our approach can be applied without modification to any neural network.
",1 Introduction,[0],[0]
"Our best RbF algorithm uses 34-50% of training data per epoch while producing similar results to state-of-the-art systems on three tasks, namely sentiment classification, image categorization, and arithmetic addition.3",1 Introduction,[0],[0]
"It also runs 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines.",1 Introduction,[0],[0]
"Research in psychology describes the following memory model for human learning: the probability that a human recalls a previously-seen item (e.g., the Korean translation of a given English word) depends on the difficulty of the item, delay since last review of the item, and the strength of the human memory.",2 Neural and Brain Memory Models,[0],[0]
"The relation between these indicators and memory retention has the following functional form (Reddy et al., 2016; Ebbinghaus, 1913):
Pr(recall) = exp(−difficulty × delay strength ).",2 Neural and Brain Memory Models,[0],[0]
"(1)
An accurate memory model enables estimating the time by which an item might be forgotten by a learner so that a review can be scheduled for the learner before that time.
",2 Neural and Brain Memory Models,[0],[0]
We investigate the analogy between the above memory model and memory model of artificial neural networks.,2 Neural and Brain Memory Models,[0],[0]
"Our intuition is that if the probability that a network recalls an item (e.g., correctly predicts its category) depends on the same factors (difficulty of the item, delay since last review of the item, or strength of the network), then we can develop spaced repetition algorithms to efficiently and effectively train neural networks.",2 Neural and Brain Memory Models,[0],[0]
We design a set of preliminarily experiments to directly evaluate the effect of the aforementioned factors (recall indicators) on memory retention in neural networks.,2.1 Recall Indicators,[0],[0]
"For this purpose, we use a set of training instances that are partially made available to the network during training.",2.1 Recall Indicators,[0],[0]
"This scheme
3We obtained similar results on QA tasks (Weston et al., 2016) but they are excluded due to space limit.
will allow us to intrinsically examine the effect of recall indicators on memory retention in isolation from external effects such as size of training data, number of training epochs, etc.
",2.1 Recall Indicators,[0],[0]
"We first define the following concepts to ease understanding the experiments (see Figure 1):
• First and Last review points (fRev and lRev) of a training instance are the first and last epochs in which the instance is used to train the network respectively,
• Recall point (Rec) is the epoch in which network retention is computed against some training instances; network retention is the probability that a neural network recalls (i.e. correctly classifies) a previously-seen training instance, and
• Delay since last review of a training instance is the difference between the recall point and the last review point of the training instance.
",2.1 Recall Indicators,[0],[0]
"Given training data and a neural network, we uniformly at random divide the data into three disjoint sets: a base set A, a review set B, and a replacement set C that respectively contain 80%, 10%, and 10% of the data.",2.1 Recall Indicators,[0],[0]
"As depicted in Figure 1, instances of A are used for training at every epoch, while those in B and C are partially used for training.",2.1 Recall Indicators,[0],[0]
The network initially starts to train with {A ∪ C} instances.,2.1 Recall Indicators,[0],[0]
"Then, starting from the first review point, we inject the review set B and remove C, training with {A ∪ B} instances at every epoch until the last review point.",2.1 Recall Indicators,[0],[0]
The network will then continue training with {A ∪ C} instances until the recall point.,2.1 Recall Indicators,[0],[0]
"At this point, network retention is computed against set B instances, with delay defined as the number of epochs since last review point.",2.1 Recall Indicators,[0],[0]
"The intuition behind using review and replacement sets, B and C respectively, is to avoid external effects (e.g.
size of data or network generalization and learning capability) for our intrinsic evaluation purpose.
",2.1 Recall Indicators,[0],[0]
"To conduct these experiments, we identify different neural models designed for different tasks.4 For each network, we fix the recall point to either the epoch in which the network is fully trained (i.e., obtains its best performance based on standard or “rote” training in which all instances are used for training at every iteration), or partially trained (i.e., obtains half of its best performance based on rote training).",2.1 Recall Indicators,[0],[0]
We report average results across these networks for each experiment.,2.1 Recall Indicators,[0],[0]
"As aforementioned, delay since last review of a training instance is the difference between the recall point (Rec) and the last review point (lRev) of the training instance.",2.1.1 Delay since Last Review,[0],[0]
We evaluate the effect of delay on network retention (against set B instances) by keeping the recall point fixed while moving the sliding window in Figure 1.,2.1.1 Delay since Last Review,[0],[0]
Figures 2(a) and 2(b) show average network retention across networks for the fully and partially trained recall points respectively.,2.1.1 Delay since Last Review,[0],[0]
The results show an inverse relationship between network retention and delay since last review in neural networks.,2.1.1 Delay since Last Review,[0],[0]
We define difficulty of training instances by the loss values generated by a network for the instances.,2.1.2 Item Difficulty,[0],[0]
Figure 2(c) shows the difficulty of set B instances at the last review point against average network retention on these instances at recall point.,2.1.2 Item Difficulty,[0],[0]
"We normalize loss values to unit vectors (to make them com-
4See section 4, we use Addition and CIFAR10 datasets and their corresponding neural networks for these experiments.
",2.1.2 Item Difficulty,[0],[0]
parable across networks) and then average them across networks for both fully and partially trained recall points.,2.1.2 Item Difficulty,[0],[0]
"As the results show, network retention decreases as item difficulty increases.",2.1.2 Item Difficulty,[0],[0]
We define strength of a network by its performance on validation data.,2.1.3 Network Strength,[0],[0]
"To understand the effect of network strength on its retention, we use the same experimental setup as before except that we keep the delay (difference between recall point and last review point) fixed while gradually increasing the recall point; this will make the networks stronger by training them for more epochs.",2.1.3 Network Strength,[0],[0]
"Then, at every recall point, we record network retention on set B instances and network accuracy on validation data.",2.1.3 Network Strength,[0],[0]
Average results across networks for two sets of 10 consecutive recall points (before fully and partially trained recall points) are shown in Figure 2(d).,2.1.3 Network Strength,[0],[0]
"As the results show, network retention increases as memory strength increases.
",2.1.3 Network Strength,[0],[0]
"The above experiments show that memory retention in neural networks is affected by the same factors that affect memory retention in humans: (a) neural networks forget training examples after a certain period of intervening training data (b): the period of recall is shorter for more difficult examples, and (c): recall improves as networks achieve better overall performance.",2.1.3 Network Strength,[0],[0]
"We conclude that delay since last review, item difficulty (loss values of training instances), and memory strength (network performance on validation data) are key indicators that affect network retention and propose to design spaced repetition algorithms that take such indicators into account in training neural networks.",2.1.3 Network Strength,[0],[0]
"We present two spaced repetition-based algorithms: a modified version of the Leitner system developed in (Reddy et al., 2016) and our Repeat before Forgetting (RbF) model respectively.",3 Spaced Repetition,[0],[0]
"Suppose we have n queues {q0, q1, . . .",3.1 Leitner System,[0],[0]
", qn−1}.",3.1 Leitner System,[0],[0]
"The Leitner system initially places all training instances in the first queue, q0.",3.1 Leitner System,[0],[0]
"As Algorithm 1 shows, at each training iteration, the Leitner scheduler chooses some queues to train a downstream neural network.",3.1 Leitner System,[0],[0]
Only instances in the selected queues will be used for training the network.,3.1 Leitner System,[0],[0]
"During training, if an instance from qi is recalled (e.g. correctly classified) by the network, the instance will be “promoted” to qi+1, otherwise it will be “demoted” to the first queue, q0.5
The Leitner scheduler reviews instances of qi at every 2i iterations.",3.1 Leitner System,[0],[0]
"Therefore, instance in lower queues (difficult/forgotten instances) are reviewed more frequently than those in higher queues (easy/recalled ones).",3.1 Leitner System,[0],[0]
Figure 3 (bottom) provides examples of queues and their processing epochs.,3.1 Leitner System,[0],[0]
"Note that the overhead imposed on training by
5 Note that in (Reddy et al., 2016) demoted instances are moved to qi−1.",3.1 Leitner System,[0],[0]
"We observed significant improvement in Leitner system by moving such instances to q0 instead of qi−1.
the Leitner system is O(|current batch|) at every epoch for moving instances between queues.",3.1 Leitner System,[0],[0]
The challenge in developing memory models is to estimate the time by which a training instance should be reviewed before it is forgotten by the network.,3.2.1 RbF Memory Models,[0],[0]
Accurate estimation of the review time leads to efficient and effective training.,3.2.1 RbF Memory Models,[0],[0]
"However, a heuristic scheduler such as Leitner system is suboptimal as its hard review schedules (i.e. only 2iiteration delays) may lead to early or late reviews.
",3.2.1 RbF Memory Models,[0],[0]
We develop flexible schedulers that take recall indicators into account in the scheduling process.,3.2.1 RbF Memory Models,[0],[0]
Our schedulers lengthen or shorten inter-repetition intervals with respect to individual training instances.,3.2.1 RbF Memory Models,[0],[0]
"In particular, we propose using density kernel functions to estimate the latest epoch in which a given training instance can be recalled.",3.2.1 RbF Memory Models,[0],[0]
"We aim to investigate how much improvement (in terms of efficiency and effectiveness) can be achieved using more flexible schedulers that utilize the recall indicators.
",3.2.1 RbF Memory Models,[0],[0]
"We propose considering density kernels as schedulers that favor (i.e., more confidently delay) less difficult training instances in stronger networks.",3.2.1 RbF Memory Models,[0],[0]
"As a kernel we can use any non-increasing function of the following quantity:
xi = di × ti se , (2)
where di indicates the loss of network for a training instance hi ∈ H, ti indicates the number of epochs to next review of hi, and se indicates the performance of network— on validation data— at epoch e. We investigate the Gaussian, Laplace, Linear, Cosine, Quadratic, and Secant kernels as described below respectively:
fgau(x, τ) = exp(−τx2), (3) flap(x, τ) = exp(−τx), (4)
flin(x, τ) = { 1− τx x < 1τ 0",3.2.1 RbF Memory Models,[0],[0]
"otherwise , (5)
fcos(x, τ) =
{ 1 2 cos(τπx)",3.2.1 RbF Memory Models,[0],[0]
"+ 1 x < 1 τ
0 otherwise ,
(6)
fqua(x, τ) = { 1− τx2",3.2.1 RbF Memory Models,[0],[0]
x2 < 1τ 0,3.2.1 RbF Memory Models,[0],[0]
"otherwise , (7)
fsec(x, τ) = 2
exp(−τx2) + exp(τx2) , (8)
where τ is a learning parameter.",3.2.1 RbF Memory Models,[0],[0]
Figure 4 depicts these kernels with τ = 1.,3.2.1 RbF Memory Models,[0],[0]
"As we will discuss in the next section, we use these kernels to optimize delay with respect to item difficulty and network strength for each training instance.",3.2.1 RbF Memory Models,[0],[0]
"Our Repeat before Forgetting (RbF) model is a spaced repetition algorithm that takes into account the previously validated recall indicators to train neural networks, see Algorithm 2.",3.2.2 RbF Algorithm,[0],[0]
RbF divides training instances into current and delayed batches based on their delay values at each iteration.,3.2.2 RbF Algorithm,[0],[0]
Instances in the current batch are those that RbF is less confident about their recall and therefore are reviewed (used to re-train the network) at current iteration.,3.2.2 RbF Algorithm,[0],[0]
"On the other hand, instances in the delayed batch are those that are likely to be recalled by the network in the future and therefore are not reviewed at current epoch.",3.2.2 RbF Algorithm,[0],[0]
"At each iteration, the RbF scheduler estimates the optimum delay (number of epochs to next review) for each training instance in the current batch.",3.2.2 RbF Algorithm,[0],[0]
"RbF makes such item-specific estimations as follows:
Given the difficulty of a training instance di, the memory strength of the neural network at epoch e, se, and an RbF memory model f (see section 3.2.1), RbF scheduler estimates the maximum delay t̂i for the instance such that it can be recalled with a confidence greater than the given threshold η ∈",3.2.2 RbF Algorithm,[0],[0]
"(0, 1) at time e+ t̂i.",3.2.2 RbF Algorithm,[0],[0]
"As described before, di and se can be represented by the current loss of the network for the instance and the current performance of the network on validation data respectively.",3.2.2 RbF Algorithm,[0],[0]
"Therefore, the maximum delay between the current (epoch e) and next reviews of the instance can be estimated as follows:
t̂i = arg min ti
( f(xi, τ̂)− η )2 , (9)
",3.2.2 RbF Algorithm,[0],[0]
s.t 1 ≤,3.2.2 RbF Algorithm,[0],[0]
ti ≤ k,3.2.2 RbF Algorithm,[0],[0]
"− e
where τ̂ is the optimum value for the learning parameter obtained from validation data, see Equation (10).",3.2.2 RbF Algorithm,[0],[0]
"In principle, reviewing instances could be delayed for any number of epochs; in practice however, delay is bounded both below and above (e.g., by queues in the Leitner system).",3.2.2 RbF Algorithm,[0],[0]
"Thus, we assume that, at each epoch e, instances could be delayed for at least one iteration and at most k − e iterations where k is the total number of training epochs.",3.2.2 RbF Algorithm,[0],[0]
"We also note that ti is a lower bound of the maximum delay as se is expected to increase and di is expected to decrease as the network trains in next iterations.
",3.2.2 RbF Algorithm,[0],[0]
Algorithm 2 shows the outline of the proposed RbF model.,3.2.2 RbF Algorithm,[0],[0]
We estimate the optimum value of τ (line 5 of Algorithm 2) for RbF memory models using validation data.,3.2.2 RbF Algorithm,[0],[0]
"In particular, RbF uses the loss values of validation instances and strength of the network obtained at the previous epoch to estimate network retention for validation instances at the current epoch (therefore ti = 1 for every validation instance).",3.2.2 RbF Algorithm,[0],[0]
"The parameter τ for each memory model is computed as follows:
τ̂ = arg min τ
( f(xj , τ)− aj )2 ,∀hj ∈ V, aj ≥ η,
(10) where aj ∈ (0, 1) is the current accuracy of the model for the validation instance hj .",3.2.2 RbF Algorithm,[0.9550559771133048],"['For query classification of class Ct, we use the cross-entropy loss as the objective: −{yt lnP (Ct|Q)+(1−yt) ln(1−P (Ct|Q))} (5) where yt = {0, 1} is the label and the loss is summed over all samples in the mini-batch (1024 samples in experiments).']"
RbF then predicts the delay for current batch instances and reduces the delay for those in the delayed batch by one epoch.,3.2.2 RbF Algorithm,[0],[0]
The overhead of RbF is O(|H|) to compute delays and O(|V|) to compute τ̂ .,3.2.2 RbF Algorithm,[0],[0]
Note that (9) and (10) have closed form solutions.,3.2.2 RbF Algorithm,[0],[0]
"Table 1 describes the tasks, datasets, and models that we consider in our experiments.",4 Experiments,[0],[0]
It also reports the training epochs for which the models produce their best performance on validation data (based on rote training).,4 Experiments,[0],[0]
"We note that the Addition dataset is randomly generated and contains numbers with at most 4 digits.6
We consider three schedulers as baselines: a slightly modified version of the Leitner scheduler (Lit) developed in Reddy et al. (2016) for human learners (see Footnote 5), curriculum learning (CL) in which training instances are scheduled with respect to their easiness (Jiang et al., 2015), and the uniform scheduler of rote training (Rote) in which all instances are used for training at every epoch.",4 Experiments,[0],[0]
"For Lit, we experimented with different queue lengths, n = {3, 5, 7}, and set n = 5 in the experiments as this value led to the best performance of this scheduler across all datasets.
",4 Experiments,[0],[0]
Curriculum learning starts training with easy instances and gradually introduces more complex instances for training.,4 Experiments,[0],[0]
"Since easiness information is not readily available in most datasets, previous approaches have used heuristic techniques (Spitkovsky et al., 2010; Basu and Christensen, 2013) or optimization algorithms (Jiang et al., 2015, 2014) to quantify easiness of training instances.",4 Experiments,[0],[0]
These approaches consider an instance as easy if its loss is smaller than a threshold (λ).,4 Experiments,[0],[0]
"We adopt this technique as follows: at each iteration e, we divide the entire training data into easy and hard sets using iteration-specific λe and the loss values of instances, obtained from the current partially-trained network.",4 Experiments,[0],[0]
All easy instances in conjunction with αe ∈,4 Experiments,[0],[0]
"[0, 1] fraction of easiest hard instances (those with smallest loss values greater than λe) are used for training at",4 Experiments,[0],[0]
"iteration e. We set
6https://github.com/fchollet/keras/ blob/master/examples/addition_rnn.py
each λe to the average loss of training instances that are correctly classified by the current partiallytrained network.",4 Experiments,[0],[0]
"Furthermore, at each iteration e, we set αe = e/k to gradually introduce complex instances at every new iteration.7 Note that we treat all instances as easy at e = 0.
",4 Experiments,[0],[0]
Performance values reported in experiments are averaged over 10 runs of systems and the confidence parameter η is always set to 0.5 unless otherwise stated.,4 Experiments,[0],[0]
"In these experiments, we evaluate memory schedulers with respect to their accuracy in predicting network retention for delayed instances.",4.1 Evaluation of Memory Models,[0],[0]
"Since curriculum learning does not estimate delay for training instances, we only consider Leitner and RbF schedulers in these experiments.
",4.1 Evaluation of Memory Models,[0],[0]
"For this evaluation, if a scheduler predicts a delay t for a training instance h at epoch e, we evaluate network retention with respect to h at epoch e+ t. If the network recalls (correctly classifies) the instance at epoch e+ t, the scheduler has correctly predicted network retention for h, and otherwise, it has made a wrong prediction.",4.1 Evaluation of Memory Models,[0],[0]
We use this binary outcome to evaluate the accuracy of each scheduler.,4.1 Evaluation of Memory Models,[0],[0]
Note that the performance of schedulers on instances that have not been delayed is not a major concern.,4.1 Evaluation of Memory Models,[0],[0]
"Although failing to delay an item inversely affects efficiency, it makes the network stronger by providing more instances to train from.",4.1 Evaluation of Memory Models,[0],[0]
"Therefore, we consider a good scheduler as the one that accurately delays more items.
",4.1 Evaluation of Memory Models,[0],[0]
Figure 6 depicts the average accuracy of schedulers in predicting networks’ retention versus the average fraction of training instances that they delayed per epoch.,4.1 Evaluation of Memory Models,[0],[0]
"As the results show, all schedulers
7k is the total number of iterations.
",4.1 Evaluation of Memory Models,[0],[0]
delay substantial amount of instances per epoch.,4.1 Evaluation of Memory Models,[0],[0]
"In particular, Cos and Qua outperform Lit in both predicting network retention and delaying items, delaying around 50% of training instances per epoch.",4.1 Evaluation of Memory Models,[0],[0]
This is while Gau and Sec show comparable accuracy to Lit but delay more instances.,4.1 Evaluation of Memory Models,[0],[0]
"On the other hand, Lap, which has been found effective in Psychology, and Lin are less accurate in predicting network retention.",4.1 Evaluation of Memory Models,[0],[0]
This is because of the tradeoff between delaying more instances and creating stronger networks.,4.1 Evaluation of Memory Models,[0],[0]
"Since these schedulers are more flexible in delaying greater amount of instances, they might not provide networks with enough data to fully train.
",4.1 Evaluation of Memory Models,[0],[0]
"Figure 7 shows the performance of RbF schedulers with respect to the recall confidence parameter η, see Equation (9).",4.1 Evaluation of Memory Models,[0],[0]
"As the results show, schedulers have poor performance with smaller values of η.",4.1 Evaluation of Memory Models,[0],[0]
This is because smaller values of η make schedulers very flexible in delaying instances.,4.1 Evaluation of Memory Models,[0],[0]
"However, the performance of schedulers are not dramatically low even with very small ηs.",4.1 Evaluation of Memory Models,[0],[0]
"Our further analyses on the delay patterns show that although a smaller η leads to more delayed instances, the delays are significantly shorter.",4.1 Evaluation of Memory Models,[0],[0]
"Therefore, most delayed instances will be “reviewed” shortly in next epochs.",4.1 Evaluation of Memory Models,[0],[0]
"These bulk reviews make the network stronger and help it to recall most delayed instance in future iterations.
",4.1 Evaluation of Memory Models,[0],[0]
"On the other hand, greater ηs lead to more accurate schedulers at the cost of using more training data.",4.1 Evaluation of Memory Models,[0],[0]
"In fact, we found that larger ηs do not delay most training instances in the first few iterations.",4.1 Evaluation of Memory Models,[0],[0]
"However, once the network obtains a reasonably high performance, schedulers start delaying instances for longer durations.",4.1 Evaluation of Memory Models,[0],[0]
We will further study this effect in the next section.,4.1 Evaluation of Memory Models,[0],[0]
We compare RbF against Leitner and curriculum learning in terms of efficiency of training and effectiveness of trained models.,4.2 Efficiency and Effectiveness,[0],[0]
"We define effectiveness as the accuracy of a trained network on balanced test data, and efficiency as (a): fraction of instances used for training per epoch, and (b): required time for training the networks.",4.2 Efficiency and Effectiveness,[0],[0]
"For RbF schedulers, we set η to 0.5 and consider the best performing kernel Cosine with η = 0.9 based on results in Figure 7.
",4.2 Efficiency and Effectiveness,[0],[0]
The results in Table 2 show that all training paradigms have comparable effectiveness (Accuracy) to that of rote training (Rote).,4.2 Efficiency and Effectiveness,[0],[0]
Our RbF schedulers use less data per epoch (34-50% of data) and run considerably faster than Rote (2.90-4.78 times faster for η = 0.5).,4.2 Efficiency and Effectiveness,[0],[0]
"The results also show that Lit is slightly less accurate but runs 2.87 time faster than Rote; note that, as a scheduler, Lit is less accurate than RbF models, see Figures 6 and 7.
",4.2 Efficiency and Effectiveness,[0],[0]
"In addition, CL leads to comparable performance to RbF but is considerably slower than other schedulers.",4.2 Efficiency and Effectiveness,[0],[0]
This is because this scheduler has to identify easier instances and sort the harder ones to sample training data at each iteration.,4.2 Efficiency and Effectiveness,[0],[0]
"Overall, the performance of Lit, CL, Cos η = .5 and Cos η = .9 are only 2.76, 1.90, 1.88, and 0.67 absolute values lower than that of Rote respectively.",4.2 Efficiency and Effectiveness,[0],[0]
"Considering the achieved efficiency, these differences are negligible (see the overall gain in Table 2).
",4.2 Efficiency and Effectiveness,[0],[0]
Figure 8 reports detailed efficiency and effectiveness results across datasets and networks.,4.2 Efficiency and Effectiveness,[0],[0]
"For clear illustration, we report accuracy at iterations 2i ∀i in which Lit is trained on the entire data, and consider Cos η = .5",4.2 Efficiency and Effectiveness,[0],[0]
as RbF scheduler.,4.2 Efficiency and Effectiveness,[0],[0]
"In terms of efficiency (first row of Figure 8), CL starts with (small set of)
easier instances and gradually increases the amount of training data by adding slightly harder instances into its training set.",4.2 Efficiency and Effectiveness,[0],[0]
"On the other hand, Lit and RbF start big and gradually delay reviewing (easy) instances that the networks have learned.",4.2 Efficiency and Effectiveness,[0],[0]
"The difference between these two training paradigms is apparent in Figures 8(a)-8(c).
",4.2 Efficiency and Effectiveness,[0],[0]
The results also show that the efficiency of a training paradigm depends on the initial effectiveness of the downstream neural network.,4.2 Efficiency and Effectiveness,[0],[0]
"For CL to be efficient, the neural network need to initially have low performance (accuracy) so that the scheduler works on smaller set of easy instances.",4.2 Efficiency and Effectiveness,[0],[0]
"For example, in case of Addition, Figures 8(b) and 8(e), the initial network accuracy is only 35%, therefore most instances are expected to be initially treated as hard instances and don’t be used for training.",4.2 Efficiency and Effectiveness,[0],[0]
"On the other hand, CL shows a considerably lower efficiency for networks with slightly high initial accuracy, e.g. in case of IMDb or CIFAR10 where the initial network accuracy is above 56%, see Figures 8(a) and 8(d), and 8(c) and 8(f) respectively.
",4.2 Efficiency and Effectiveness,[0],[0]
"In contrast to CL, Lit and RbF are more efficient when the network has a relatively higher initial performance.",4.2 Efficiency and Effectiveness,[0],[0]
"A higher initial performance helps the
schedulers to more confidently delay “reviewing” most instances and therefore train with a much smaller set of instances.",4.2 Efficiency and Effectiveness,[0],[0]
"For example, since the initial network accuracy in IMDb or CIFAR10 is above 56%, Lit and RbF are considerably more efficient from the beginning of the training process.",4.2 Efficiency and Effectiveness,[0],[0]
"However, in case of low initial performance, Lit and RbF tend to avoid delaying instances at lower iterations which leads to poor efficiency at the beginning.",4.2 Efficiency and Effectiveness,[0],[0]
"This is the case for the Addition dataset in which instances are gradually delayed by these two schedulers even at epoch 8 when the performance of the network reaches above 65%, see Figures 8(e) and 8(b).",4.2 Efficiency and Effectiveness,[0],[0]
"However, Lit gains its true efficiency after iteration 12, see Figure 8(b), while RbF still gradually improves the efficiency.",4.2 Efficiency and Effectiveness,[0],[0]
"This might be because of the lower bound delays that RbF estimates, see Equation (9).
",4.2 Efficiency and Effectiveness,[0],[0]
"Furthermore, the effectiveness results in Figure 8 (bottom) show that all schedulers produce comparable accuracy to the Rote scheduler throughout the training process, not just at specific iterations.",4.2 Efficiency and Effectiveness,[0],[0]
"This indicates that these training paradigms can much faster achieve the same generalizability as standard training, see Figures 8(b) and 8(e).",4.2 Efficiency and Effectiveness,[0],[0]
We investigate the effect of spaced repetition on overtraining.,4.3 Robustness against Overtraining,[0],[0]
The optimal number of training epochs required to train fastText on the IMDb dataset is 8 epochs (see Table 1).,4.3 Robustness against Overtraining,[0],[0]
"In this experiment, we run fastText on IMDb for greater number of iterations to investigate the robustness of different schedulers against overtraining.",4.3 Robustness against Overtraining,[0],[0]
The results in Figure 9 show that Lit and RbF (Cos η = 0.5) are more robust against overtraining.,4.3 Robustness against Overtraining,[0],[0]
"In fact, the performance of Lit and RbF further improve at epoch 16 while CL and Rote overfit at epoch 16 (note that CL and Rote also require considerably more amount of time to reach to higher iterations).",4.3 Robustness against Overtraining,[0],[0]
We attribute the robustness of Lit and RbF to the scheduling mechanism which helps the networks to avoid retraining with easy instances.,4.3 Robustness against Overtraining,[0],[0]
"On the other hand, overtraining affects Lit and RbF at higher training iterations, compare performance of each scheduler at epochs 8 and 32.",4.3 Robustness against Overtraining,[0],[0]
This might be because these training paradigms overfit the network by paying too much training attention to very hard instances which might introduce noise to the model.,4.3 Robustness against Overtraining,[0],[0]
"Ebbinghaus (1913, 2013), and recently Murre and Dros (2015), studied the hypothesis of the exponential nature of forgetting, i.e. how information is lost over time when there is no attempt to retain it.",5 Related Work,[0],[0]
"Previous research identified three critical indicators that affect the probability of recall: repeated exposure to learning materials, elapsed time since their last review (Ebbinghaus, 1913; Wixted, 1990; Dempster, 1989), and more recently item difficulty (Reddy et al., 2016).",5 Related Work,[0],[0]
We based our investigation on these findings and validated that these indicators indeed affect memory retention in neural networks.,5 Related Work,[0],[0]
"We then developed training paradigms that utilize the above indicators to train networks.
",5 Related Work,[0],[0]
Bengio et al. (2009) and Kumar et al. (2010) also developed cognitively-motivated training paradigms which are inspired by the principle that learning can be more effective when training starts with easier concepts and gradually proceeds with more difficult ones.,5 Related Work,[0],[0]
"Our idea is motivated by the spaced repetition principle which indicates learning improves with repeated exposure and decays with delay since last exposure (Ebbinghaus, 1913; Dempster, 1989).",5 Related Work,[0],[0]
"Based on this principle, we developed schedulers that space the reviews of training instances over time for efficient and effective training of neural networks.",5 Related Work,[0],[0]
We developed a cognitively-motivated training paradigm (scheduler) that space instances over time for efficient and effective training of neural networks.,6 Conclusion and Future Work,[0],[0]
Our scheduler only uses a small fraction of training data per epoch but still effectively train neural networks.,6 Conclusion and Future Work,[0],[0]
It achieves this by estimating the time (number of epochs) by which training could be delayed for each instance.,6 Conclusion and Future Work,[0],[0]
"Our work was inspired by three recall indicators that affect memory retention in humans, namely difficulty of learning materials, delay since their last review, and memory strength of the learner, which we validated in the context of neural networks.
",6 Conclusion and Future Work,[0],[0]
There are several avenues for future work including the extent to which our RbF model and its kernels could be combined with curriculum learning or Leitner system to either predict easiness of novel training instances to inform curriculum learning or incorporate Leitner’s queueing mechanism to the RbF model.,6 Conclusion and Future Work,[0],[0]
"Other directions include extending RbF to dynamically learn the recall confidence parameter with respect to network behavior, or developing more flexible delay functions with theoretical analysis on their lower and upper bounds.",6 Conclusion and Future Work,[0],[0]
We thank Mitra Mohtarami for her constructive feedback during the development of this paper and anonymous reviewers for their thoughtful comments.,Acknowledgments,[0],[0]
This work was supported by National Institutes of Health (NIH) grant R01GM114355 from the National Institute of General Medical Sciences (NIGMS).,Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.,Acknowledgments,[0],[0]
We present a novel approach for training artificial neural networks.,abstractText,[0],[0]
Our approach is inspired by broad evidence in psychology that shows human learners can learn efficiently and effectively by increasing intervals of time between subsequent reviews of previously learned materials (spaced repetition).,abstractText,[0],[0]
We investigate the analogy between training neural models and findings in psychology about human memory model and develop an efficient and effective algorithm to train neural models.,abstractText,[0],[0]
The core part of our algorithm is a cognitively-motivated scheduler according to which training instances and their “reviews” are spaced over time.,abstractText,[0],[0]
"Our algorithm uses only 34-50% of data per epoch, is 2.9-4.8 times faster than standard training, and outperforms competing state-of-the-art baselines.1",abstractText,[0],[0]
Repeat before Forgetting: Spaced Repetition for Efficient and Effective Training of Neural Networks,title,[0],[0]
"With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups. However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions. In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks. We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction. 1",text,[0],[0]
The field of Natural Language Processing (NLP) is going through the data revolution.,1 Introduction,[0],[0]
"With the persistent increase of the heterogeneous web, for the first time in human history, written language from multiple languages, domains, and genres is now abundant.",1 Introduction,[0],[0]
"Naturally, the expectations from NLP algorithms also grow and evaluating a new algorithm on as many languages, domains, and genres as possible is becoming a de-facto standard.
",1 Introduction,[0],[0]
"1Our code is at: https://github.com/rtmdrr/replicabilityanalysis-NLP .
",1 Introduction,[0],[0]
"For example, the phrase structure parsers of Charniak (2000) and Collins (2003) were mostly evaluated on the Wall Street Journal Penn Treebank (Marcus et al., 1993), consisting of written, edited English text of economic news.",1 Introduction,[0],[0]
"In contrast, modern dependency parsers are expected to excel on the 19 languages of the CoNLL 2006-2007 shared tasks on multilingual dependency parsing (Buchholz and Marsi, 2006; Nilsson et al., 2007), and additional challenges, such as the shared task on parsing multiple English Web domains (Petrov and McDonald, 2012), are continuously proposed.
",1 Introduction,[0],[0]
"Despite the growing number of evaluation tasks, the analysis toolbox employed by NLP researchers has remained quite stable.",1 Introduction,[0],[0]
"Indeed, in most experimental NLP papers, several algorithms are compared on a number of datasets where the performance of each algorithm is reported together with per-dataset statistical significance figures.",1 Introduction,[0],[0]
"However, with the growing number of evaluation datasets, it becomes more challenging to draw comprehensive conclusions from such comparisons.",1 Introduction,[0],[0]
"This is because although the probability of drawing an erroneous conclusion from a single comparison is small, with multiple comparisons the probability of making one or more false claims may be very high.
",1 Introduction,[0],[0]
"The goal of this paper is to provide the NLP community with a statistical analysis framework, which we term Replicability Analysis, which will allow us to draw statistically sound conclusions in evaluation setups that involve multiple comparisons.",1 Introduction,[0],[0]
"The classical goal of replicability analysis is to examine the consistency of findings across studies in order to address the basic dogma of science, that a find-
471
Transactions of the Association for Computational Linguistics, vol. 5, pp.",1 Introduction,[0],[0]
"471–486, 2017.",1 Introduction,[0],[0]
Action Editor: Brian Roark.,1 Introduction,[0],[0]
"Submission batch: 3/2017; Revision batch: 7/2017; Published 11/2017.
",1 Introduction,[0],[0]
c©2017 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
"ing is more convincingly true if it is replicated in at least one more study (Heller et al., 2014; Patil et al., 2016).",1 Introduction,[0],[0]
"We adapt this goal to NLP, where we wish to ascertain the superiority of one algorithm over another across multiple datasets, which may come from different languages, domains, and genres.",1 Introduction,[0],[0]
"Finding that one algorithm outperforms another across domains gives a sense of consistency to the results and positive evidence that the better performance is not specific to a selected setup.2
In this work we address two questions: (1) Counting: For how many datasets does a given algorithm outperform another?",1 Introduction,[0],[0]
"and (2) Identification: What are these datasets?
",1 Introduction,[0],[0]
"When comparing two algorithms on multiple datasets, NLP papers often answer informally the questions we address in this work.",1 Introduction,[0],[0]
"In some cases this is done without any statistical analysis, by simply declaring better performance of a given algorithm for datasets where its performance measure is better than that of another algorithm, and counting these datasets.",1 Introduction,[0],[0]
In other cases answers are based on the p-values from statistical tests performed for each dataset: declaring better performance for datasets with p-value below the significance level (e.g. 0.05) and counting these datasets.,1 Introduction,[0],[0]
"While it is clear that the first approach is not statistically valid, it seems that our community is not aware of the fact that the second approach, which may seem statistically sound, is not valid as well.",1 Introduction,[0],[0]
"This may lead to erroneous conclusions, which result in adopting new (and probably complicated) algorithms, while they are not better than previous (probably more simple) ones.
",1 Introduction,[0],[0]
"In this work, we demonstrate this problem and show that it becomes more severe as the number of evaluation sets grows, which seems to be the current trend in NLP.",1 Introduction,[0],[0]
"We adopt a known general statistical methodology for addressing the counting (question (1)) and identification (question (2)) problems, by choosing the tests and procedures which are valid for
2“Replicability” is sometimes referred to as “reproducibility”.",1 Introduction,[0],[0]
"In recent NLP work the term reproducibility was used when trying to get identical results on the same data (Névéol et al., 2016; Marrese-Taylor and Matsuo, 2017).",1 Introduction,[0],[0]
"In this paper, we adopt the meaning of “replicability” and its distinction from “reproducibility” from Peng (2011) and Leek and Peng (2015) and refer to replicability analysis as the effort to show that a finding is consistent over different datasets from different domains or languages, and is not idiosyncratic to a specific scenario.
situations encountered in NLP problems, and giving specific recommendations for such situations.
",1 Introduction,[0],[0]
"Particularly, we first demonstrate (Section 3) that the current prominent approach in the NLP literature, identifying the datasets for which the difference between the performance of the algorithms reaches a predefined significance level according to some statistical significance test, does not guarantee to bound the probability to make at least one erroneous claim.",1 Introduction,[0],[0]
Hence this approach is error-prone when the number of participating datasets is large.,1 Introduction,[0],[0]
We thus propose an alternative approach (Section 4).,1 Introduction,[0],[0]
"For question (1), we adopt the approach of Benjamini et al. (2009) to replicability analysis of multiple studies, based on the partial conjunction framework of Benjamini and Heller (2008).",1 Introduction,[0],[0]
This analysis comes with a guarantee that the probability of overestimating the true number of datasets with effect is upper bounded by a predefined constant.,1 Introduction,[0],[0]
"For question (2), we motivate a multiple testing procedure which guarantees that the probability of making at least one erroneous claim on the superiority of one algorithm over another is upper bounded by a predefined constant.
",1 Introduction,[0],[0]
"In Sections 5 and 6 we demonstrate how to apply the proposed frameworks to two synthetic data toy examples and four NLP applications: multidomain dependency parsing, multilingual POS tagging, cross-domain sentiment classification, and word similarity prediction with word embedding models.",1 Introduction,[0],[0]
"Our results demonstrate that the current practice in NLP for addressing our questions is error-prone, and illustrate the differences between it and the proposed statistically sound approach.
",1 Introduction,[0],[0]
"We hope that this work will encourage our community to increase the number of standard evaluation setups per task when appropriate (e.g. including additional languages and domains), possibly paving the way to hundreds of comparisons per study.",1 Introduction,[0],[0]
This is due to two main reasons.,1 Introduction,[0],[0]
"First, replicability analysis is a statistically sound framework that allows a researcher to safely draw valid conclusions with well defined statistical guarantees.",1 Introduction,[0],[0]
"Moreover, this framework provides a means of summarizing a large number of experiments with a handful of easily interpretable numbers (e.g., see Table 1).",1 Introduction,[0],[0]
"This allows researchers to report results over a large number of comparisons in a concise manner, delving into details of particular comparisons when necessary.",1 Introduction,[0],[0]
"Our work recognizes the current trend in the NLP community where, for many tasks and applications, the number of evaluation datasets constantly increases.",2 Previous Work,[0],[0]
We believe this trend is inherent to language processing technology due to the multiplicity of languages and of linguistic genres and domains.,2 Previous Work,[0],[0]
"In order to extend the reach of NLP algorithms, they have to be designed so that they can deal with many languages and with the various domains of each.",2 Previous Work,[0],[0]
"Having a sound statistical framework that can deal with multiple comparisons is hence crucial for the field.
",2 Previous Work,[0],[0]
This section is hence divided into two.,2 Previous Work,[0],[0]
"We start by discussing representative examples for multiple comparisons in NLP, focusing on evaluations across multiple languages and multiple domains.",2 Previous Work,[0],[0]
"We then discuss existing analysis frameworks for multiple comparisons, both in the NLP and in the machine learning literatures, pointing to the need for establishing new standards for our community.
",2 Previous Work,[0],[0]
"Multiple Comparisons in NLP Multiple comparisons of algorithms over datasets from different languages, domains and genres have become a de-facto standard in many areas of NLP.",2 Previous Work,[0],[0]
Here we survey a number of representative examples.,2 Previous Work,[0],[0]
"A full list of NLP tasks is beyond the scope of this paper.
",2 Previous Work,[0],[0]
"A common multilingual example is, naturally, machine translation, where it is customary to compare algorithms across a large number of sourcetarget language pairs.",2 Previous Work,[0],[0]
"This is done, for example, with the Europarl corpus consisting of 21 European languages (Koehn, 2005; Koehn and Schroeder, 2007) and with the datasets of the WMT workshop series with its multiple domains (e.g. news and biomedical in 2017), each consisting of several language pairs (7 and 14, respectively, in 2017).
",2 Previous Work,[0],[0]
Multiple dataset comparisons are also abundant in domain adaptation work.,2 Previous Work,[0],[0]
"Representative tasks include named entity recognition (Guo et al., 2009), POS tagging (Daumé III, 2007), dependency parsing (Petrov and McDonald, 2012), word sense disambiguation (Chan and Ng, 2007) and sentiment classification (Blitzer et al., 2006; Blitzer et al., 2007).
",2 Previous Work,[0],[0]
"More recently, with the emergence of crowdsourcing that makes data collection cheap and fast (Snow et al., 2008), an ever growing number of datasets is being created.",2 Previous Work,[0],[0]
"This is particularly notice-
able in lexical semantics tasks that have become central in NLP research due to the prominence of neural networks.",2 Previous Work,[0],[0]
"For example, it is customary to compare word embedding models (Mikolov et al., 2013; Pennington et al., 2014; Ó",2 Previous Work,[0],[0]
"Séaghdha and Korhonen, 2014; Levy and Goldberg, 2014; Schwartz et al., 2015) on multiple datasets where word pairs are scored according to the degree to which different semantic relations, such as similarity and association, hold between the members of the pair (Finkelstein et al., 2001a; Bruni et al., 2014; Silberer and Lapata, 2014; Hill et al., 2015).",2 Previous Work,[0],[0]
"In some works (e.g., Baroni et al. (2014))",2 Previous Work,[0],[0]
"these embedding models are compared across a large number of simple tasks.
",2 Previous Work,[0],[0]
"As discussed in Section 1, the outcomes of such comparisons are often summarized in a table that presents numerical performance values, usually accompanied by statistical significance figures and sometimes also with cross-comparison statistics such as average performance figures.",2 Previous Work,[0],[0]
"Here, we analyze the conclusions that can be drawn from this information and suggest that with the growing number of comparisons, a more intricate analysis is required.
",2 Previous Work,[0],[0]
"Existing Analysis Frameworks Machine learning work on multiple dataset comparisons dates back to Dietterich (1998) who raised the question: “given two learning algorithms and datasets from several domains, which algorithm will produce more accurate classifiers when trained on examples from new domains?”.",2 Previous Work,[0],[0]
The seminal work that proposed practical means for this problem is that of Demšar (2006).,2 Previous Work,[0],[0]
"Given performance measures for two algorithms on multiple datasets, the authors test whether there is at least one dataset on which the difference between the algorithms is statistically significant.",2 Previous Work,[0],[0]
"For this goal they propose methods such as a paired t-test, a nonparametric sign-rank test and a wins/losses/ties count, all computed across the results collected from all participating datasets.",2 Previous Work,[0],[0]
"In contrast, our goal is to count and identify the datasets for which one algorithm significantly outperforms the other, which provides more intricate information, especially when the datasets come from different sources.
",2 Previous Work,[0],[0]
"In NLP, several studies addressed the problem of measuring the statistical significance of results on a single dataset (e.g., Berg-Kirkpatrick et al. (2012); Søgaard (2013); Søgaard et al. (2014)).",2 Previous Work,[0],[0]
"Søgaard
(2013) is, to the best of our knowledge, the only work that addressed the statistical properties of evaluation with multiple datasets.",2 Previous Work,[0],[0]
"For this aim he modified the statistical tests proposed in Demšar (2006) to use a Gumbel distribution assumption on the test statistics, which he considered to suit NLP better than the original Gaussian assumption.",2 Previous Work,[0],[0]
"However, while this procedure aims to estimate the effect size across datasets, it answers neither the counting nor the identification question of Section 1.
",2 Previous Work,[0],[0]
In the next section we provide the preliminary knowledge from the field of statistics that forms the basis for the proposed framework and then proceed with its description.,2 Previous Work,[0],[0]
We start by formulating a general hypothesis testing framework for a comparison between two algorithms.,3 Preliminaries,[0],[0]
"This is a common type of hypothesis testing framework applied in NLP, its detailed formulation will help us develop our ideas.",3 Preliminaries,[0],[0]
"We wish to compare between two algorithms, A and B. Let X be a collection of datasets X = {X1, X2, . . .",3.1 Hypothesis Testing,[0],[0]
", XN}, where for all i ∈ {1, . . .",3.1 Hypothesis Testing,[0],[0]
", N}, Xi = {xi,1, . . .",3.1 Hypothesis Testing,[0],[0]
", xi,ni} .",3.1 Hypothesis Testing,[0],[0]
Each dataset Xi can be of a different language or a different domain.,3.1 Hypothesis Testing,[0],[0]
"We denote by xi,k the granular unit on which results are being measured, that, in most NLP tasks, is a word or a sequence of words.",3.1 Hypothesis Testing,[0],[0]
"The difference in performance between the two algorithms is measured using one or more of the evaluation measures in the setM = {M1, . . .",3.1 Hypothesis Testing,[0],[0]
",Mm}.3
Let us denoteMj(ALG,Xi) as the value of the measureMj when algorithmALG is applied on the dataset Xi.",3.1 Hypothesis Testing,[0],[0]
"Without loss of generality, we assume that higher values of the measure are better.",3.1 Hypothesis Testing,[0],[0]
"We define the difference in performance between two algorithms, A and B, according to the measure",3.1 Hypothesis Testing,[0],[0]
"Mj on the dataset Xi as:
δj(X i) =Mj(A,Xi)−Mj(B,Xi).
",3.1 Hypothesis Testing,[0],[0]
"3To keep the discussion concise, throughout this paper we assume that only one evaluation measure is used.",3.1 Hypothesis Testing,[0],[0]
"Our framework can be easily extended to deal with multiple measures.
",3.1 Hypothesis Testing,[0],[0]
"Finally, using this notation we formulate the following statistical hypothesis testing problem:
H0i(j) :δj(X i) ≤ 0
H1i(j) :δj(X i) > 0.
(1)
The null hypothesis, stating that there is no difference between the performance of algorithm A and algorithmB, or thatB performs better, is tested versus the alternative statement thatA is superior.",3.1 Hypothesis Testing,[0],[0]
"If the statistical test results in rejecting the null hypothesis, one concludes that A outperforms B in this setup.",3.1 Hypothesis Testing,[0],[0]
"Otherwise, there is not enough evidence in the data to make this conclusion.
",3.1 Hypothesis Testing,[0],[0]
"Rejection of the null hypothesis when it is true is termed type I error, and non-rejection of the null hypothesis when the alternative is true is termed type II error.",3.1 Hypothesis Testing,[0],[0]
"The classical approach to hypothesis testing is to find a test that guarantees that the probability of making a type I error is upper bounded by a predefined constant α, the test significance level, while achieving as low probability of type II error as possible, a.k.a “achieving as high power as possible”.
",3.1 Hypothesis Testing,[0],[0]
We next turn to the case where the difference between two algorithms is tested across multiple datasets.,3.1 Hypothesis Testing,[0],[0]
Equation 1 defines a multiple hypothesis testing problem when considering the formulation for all N datasets.,3.2 The Multiplicity Problem,[0],[0]
"If N is large, testing each hypothesis separately at the nominal significance level may result in a high number of erroneously rejected null hypotheses.",3.2 The Multiplicity Problem,[0],[0]
"In our context, when the performance of algorithm A is compared to that of algorithm B across multiple datasets, and for each dataset algorithm A is declared as superior, based on a statistical test at the nominal significance level α, the expected number of erroneous claims may grow as N grows.
",3.2 The Multiplicity Problem,[0],[0]
"For example, if a single test is performed with a significance level of α = 0.05, there is only a 5% chance of incorrectly rejecting the null hypothesis.",3.2 The Multiplicity Problem,[0],[0]
"On the other hand, for 100 tests where all null hypotheses are true, the expected number of incorrect rejections is 100 · 0.05 = 5.",3.2 The Multiplicity Problem,[0],[0]
"Denoting the total number of type I errors as V , we can see below that if the test statistics are independent then the probability of
making at least one incorrect rejection is 0.994:
P(V > 0)",3.2 The Multiplicity Problem,[0],[0]
"= 1− P(V = 0) =
1− 100∏
i=1
P(no type I error in i)",3.2 The Multiplicity Problem,[0],[0]
"=1− (1− 0.05)100.
",3.2 The Multiplicity Problem,[0],[0]
This demonstrates that the naive method of counting the datasets for which significance was reached at the nominal level is error-prone.,3.2 The Multiplicity Problem,[0],[0]
"Similar examples can be constructed for situations where some of the null hypotheses are false.
",3.2 The Multiplicity Problem,[0],[0]
"The multiple testing literature proposes various procedures for bounding the probability of making at least one type I error, as well as other, less restrictive error criteria (see a survey in Farcomeni (2007)).",3.2 The Multiplicity Problem,[0],[0]
"In this paper, we address the questions of counting and identifying the datasets for which algorithm A outperforms B, with certain statistical guarantees regarding erroneous claims.",3.2 The Multiplicity Problem,[0],[0]
"While identifying the datasets gives more information when compared to just declaring their number, we consider these two questions separately.",3.2 The Multiplicity Problem,[0],[0]
"As our experiments show, according to the statistical analysis we propose the estimated number of datasets with effect (question 1) may be higher than the number of identified datasets (question 2).",3.2 The Multiplicity Problem,[0],[0]
We next present the fundamentals of the partial conjunction framework which is at the heart of our proposed methods.,3.2 The Multiplicity Problem,[0],[0]
We start by reformulating the set of hypothesis testing problems of Equation 1 as a unified hypothesis testing problem.,3.3 Partial Conjunction Hypotheses,[0],[0]
This problem aims to identify whether algorithm A is superior to B across all datasets.,3.3 Partial Conjunction Hypotheses,[0],[0]
"The notation for the null hypothesis in this problem is HN/N0 since we test if N out of N alternative hypotheses are true:
H N/N 0",3.3 Partial Conjunction Hypotheses,[0],[0]
":
N⋃
i=1
H0i is true vs. H N/N 1 :
N⋂
i=1
H1i is true.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"Requiring the rejection of the disjunction of all null hypotheses is often too restrictive for it involves observing a significant effect on all datasets, i ∈ {1, . . .",3.3 Partial Conjunction Hypotheses,[0],[0]
", N}.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Instead, one can require a rejection of the global null hypothesis stating that all individual null hypotheses are true, i.e., evidence that
at least one alternative hypothesis is true.",3.3 Partial Conjunction Hypotheses,[0],[0]
"This hypothesis testing problem is formulated as follows:
H 1/N 0",3.3 Partial Conjunction Hypotheses,[0],[0]
":
N⋂
i=1
H0i is true vs. H 1/N 1 :
N⋃
i=1
H1i is true.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"Obviously, rejecting the global null may not provide enough information: it only indicates that algorithm A outperforms B on at least one dataset.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Hence, this claim does not give any evidence for the consistency of the results across multiple datasets.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"A natural compromise between the above two formulations is to test the partial conjunction null, which states that the number of false null hypotheses is lower than u, where 1 ≤ u ≤ N is a pre-specified integer constant.",3.3 Partial Conjunction Hypotheses,[0],[0]
"The partial conjunction test contrasts this statement with the alternative statement that at least u out of the N null hypotheses are false.
",3.3 Partial Conjunction Hypotheses,[0],[0]
Definition 1 (Benjamini and Heller (2008)).,3.3 Partial Conjunction Hypotheses,[0],[0]
"Consider N ≥ 2 null hypotheses: H01, H02, . . .",3.3 Partial Conjunction Hypotheses,[0],[0]
",H0N , and let p1, . . .",3.3 Partial Conjunction Hypotheses,[0],[0]
", pN be their associated p−values.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Let k be the true unknown number of false null hypotheses, then our question “Are at least u out of N null hypotheses false?” can be formulated as follows:
H u/N 0",3.3 Partial Conjunction Hypotheses,[0],[0]
:,3.3 Partial Conjunction Hypotheses,[0],[0]
"k < u vs. H u/N 1 : k ≥ u.
",3.3 Partial Conjunction Hypotheses,[0],[0]
"In our context, k is the number of datasets where algorithm A is truly better, and the partial conjunction test examines whether algorithmA outperforms algorithm B in at least u of N cases.
",3.3 Partial Conjunction Hypotheses,[0],[0]
Benjamini and Heller (2008) developed a general method for testing the above hypothesis for a given u. They also showed how to extend their method in order to answer our counting question.,3.3 Partial Conjunction Hypotheses,[0],[0]
"We next describe their framework and advocate a different, yet related method for dataset identification.",3.3 Partial Conjunction Hypotheses,[0],[0]
"Referred to as the cornerstone of science (Moonesinghe et al., 2007), replicability analysis is of predominant importance in many scientific fields including psychology (Collaboration, 2012), genomics (Heller et al., 2014), economics (Herndon et al., 2014) and medicine (Begley and Ellis, 2012), among others.",4 Replicability Analysis for NLP,[0],[0]
"Findings are usually considered as replicated if they are obtained in two or more
studies that differ from each other in some aspects (e.g. language, domain or genre in NLP).
",4 Replicability Analysis for NLP,[0],[0]
"The replicability analysis framework we employ (Benjamini and Heller, 2008; Benjamini et al., 2009) is based on partial conjunction testing.",4 Replicability Analysis for NLP,[0],[0]
"Particularly, these authors have shown that a lower bound on the number of false null hypotheses with a confidence level of 1 − α can be obtained by finding the largest u for which we can reject the partial conjunction null hypothesis Hu/N0 along with H
1/N 0 , . . .",4 Replicability Analysis for NLP,[0],[0]
",H (u−1)/N 0 at a significance levelα.",4 Replicability Analysis for NLP,[0],[0]
"Since rejecting Hu/N0 means that we see evidence in at least u out of N datasets, algorithm",4 Replicability Analysis for NLP,[0],[0]
"A is superior to B. This lower bound on k is taken as our answer to the Counting question of Section 1.
",4 Replicability Analysis for NLP,[0],[0]
"In line with the hypothesis testing framework of Section 3, the partial conjunction null, Hu/N0 , is rejected at level α if pu/N ≤ α, where pu/N is the partial conjunction p-value.",4 Replicability Analysis for NLP,[0],[0]
"Based on the known methods for testing the global null hypothesis (see, e.g., Loughin (2004)), Benjamini and Heller (2008) proposed methods for combining the p−values p1, . . .",4 Replicability Analysis for NLP,[0],[0]
", pN of H01, H02, . . .",4 Replicability Analysis for NLP,[0],[0]
",H0N in order to obtain pu/N .",4 Replicability Analysis for NLP,[0],[0]
"Below, we describe two such methods and their properties.",4 Replicability Analysis for NLP,[0],[0]
"The methods we focus on were developed by Benjamini and Heller (2008), and are based on Fisher’s and Bonferroni’s methods for testing the global null hypothesis.",4.1 The Partial Conjunction p−value,[0],[0]
"For brevity, we name them Bonferroni and Fisher.",4.1 The Partial Conjunction p−value,[0],[0]
"We choose them because they are valid in different setups that are frequently encountered in NLP (Section 6): Bonferroni for dependent datasets and both Fisher and Bonferroni for independent datasets.4
Bonferroni’s method does not make any assumptions about the dependencies between the participating datasets and it is hence applicable in NLP tasks, since in NLP it is most often hard to determine the type of dependence between the datasets.",4.1 The Partial Conjunction p−value,[0],[0]
"Fisher’s method, while assuming independence across the
4For simplicity we refer to dependent/independent datasets as those for which the test statistics are dependent/independent.",4.1 The Partial Conjunction p−value,[0],[0]
"We assume the test statistics are independent if the corresponding datasets do not have mutual samples, and one dataset is not a transformation of the other.
participating datasets, is often more powerful than Bonferroni’s method (see Loughin (2004) and Benjamini and Heller (2008) for other methods and a comparison between them).",4.1 The Partial Conjunction p−value,[0],[0]
"Our recommendation is hence to use the Bonferroni’s method when the datasets are dependent and to use the more powerful Fisher’s method when the datasets are independent.
",4.1 The Partial Conjunction p−value,[0],[0]
"Let p(i) be the i-th smallest p−value among p1, . . .",4.1 The Partial Conjunction p−value,[0],[0]
", pN .",4.1 The Partial Conjunction p−value,[0],[0]
"The partial conjunction p−values are:
p u/N Bonferroni = (N − u+ 1)p(u) (2)
p u/N Fisher = P ( χ22(N−u+1) ≥ −2",4.1 The Partial Conjunction p−value,[0],[0]
"N∑
i=u
ln p(i)
) (3)
where χ22(N−u+1) denotes a chi-squared random variable with 2(N − u+ 1) degrees of freedom.
",4.1 The Partial Conjunction p−value,[0],[0]
"To understand the reasoning behind these methods, let us consider first the above p−values for testing the global null, i.e., for the case of u = 1.",4.1 The Partial Conjunction p−value,[0],[0]
Rejecting the global null hypothesis requires evidence that at least one null hypothesis is false.,4.1 The Partial Conjunction p−value,[0],[0]
"Intuitively, we would like to see one or more small p−values.
",4.1 The Partial Conjunction p−value,[0],[0]
Both of the methods above agree with this intuition.,4.1 The Partial Conjunction p−value,[0],[0]
"Bonferroni’s method rejects the global null if p(1) ≤ α/N , i.e. if the minimum p−value is small enough, where the threshold guarantees that the significance level of the test is α for any dependency among the p−values p1, . . .",4.1 The Partial Conjunction p−value,[0],[0]
", pN .",4.1 The Partial Conjunction p−value,[0],[0]
"Fisher’s method rejects the global null for large values of −2∑Ni=1 ln p(i), or equivalently for small values of∏N i=1",4.1 The Partial Conjunction p−value,[0],[0]
pi.,4.1 The Partial Conjunction p−value,[0],[0]
"That is, while both these methods are intuitive, they are different.",4.1 The Partial Conjunction p−value,[0],[0]
Fisher’s method requires a small enough product of p−values as evidence that at least one null hypothesis is false.,4.1 The Partial Conjunction p−value,[0],[0]
"Bonferroni’s method, on the other hand, requires as evidence at least one small enough p−value.
",4.1 The Partial Conjunction p−value,[0],[0]
"For the case u = N , i.e., when the alternative states that all null hypotheses are false, both methods require that the maximal p−value is small enough for rejection of HN/N0 .",4.1 The Partial Conjunction p−value,[0],[0]
This is also intuitive because we expect that all the p−values will be small when all the null hypotheses are false.,4.1 The Partial Conjunction p−value,[0],[0]
"For other cases, where 1 < u < N , the reasoning is more complicated and is beyond the scope of this paper.
",4.1 The Partial Conjunction p−value,[0],[0]
The partial conjunction test for a specific u answers the question “Does algorithm A perform better than B on at least u datasets?”,4.1 The Partial Conjunction p−value,[0],[0]
"The next step is
the estimation of the number of datasets for which algorithm A performs better than B.",4.1 The Partial Conjunction p−value,[0],[0]
Recall that the number of datasets where algorithm A outperforms algorithm B (denoted with k in Definition 1) is the true number of false null hypotheses in our problem.,4.2 Dataset Counting (Question 1),[0],[0]
"Benjamini and Heller (2008) proposed to estimate k to be the largest u for which H u/N 0 , along with H 1/N 0 , . . .",4.2 Dataset Counting (Question 1),[0],[0]
",H (u−1)/N 0 is rejected.",4.2 Dataset Counting (Question 1),[0],[0]
"Specifically, the estimator k̂ is defined as follows:
k̂ = max{u : pu/N∗ ≤",4.2 Dataset Counting (Question 1),[0],[0]
"α}, (4)
where pu/N∗ = max{p(u−1)/N∗ , pu/N}, p1/N = p1/N∗",4.2 Dataset Counting (Question 1),[0],[0]
and α is the desired upper bound on the probability to overestimate the true k.,4.2 Dataset Counting (Question 1),[0],[0]
"It is guaranteed that P(k̂ > k) ≤ α as long as the p−value combination method used for constructing pu/N is valid for the given dependency across the test statistics.5 When k̂ is based on pu/NBonferroni it is denoted with k̂Bonferroni; when it is based on p u/N Fisher, it is denoted with k̂Fisher.",4.2 Dataset Counting (Question 1),[0],[0]
"A crucial practical consideration, when choosing between k̂Bonferroni and k̂Fisher, is the assumed dependency between the datasets.",4.2 Dataset Counting (Question 1),[0],[0]
"As discussed in Section 4.1, pu/NFisher is recommended when the participating datasets are assumed to be independent; when this assumption cannot be made, only pu/NBonferroni is appropriate.",4.2 Dataset Counting (Question 1),[0],[0]
"As the k̂ estimators are based on the respective pu/N s, the same considerations hold when choosing between them.
",4.2 Dataset Counting (Question 1),[0],[0]
"With the k̂ estimators, one can answer the counting question of Section 1, reporting that algorithm",4.2 Dataset Counting (Question 1),[0],[0]
A is better than algorithm B in at least k̂ out of N datasets with a confidence level of 1 − α.,4.2 Dataset Counting (Question 1),[0],[0]
"Regarding the identification question, a natural approach would be to declare the k̂ datasets with the smallest p−values as those for which the effect holds.",4.2 Dataset Counting (Question 1),[0],[0]
"However, with k̂Fisher this approach does not guarantee control over type I errors.",4.2 Dataset Counting (Question 1),[0],[0]
"In contrast, for k̂Bonferroni, the above approach comes with such guarantees, as described in the next section.
",4.2 Dataset Counting (Question 1),[0],[0]
5This result is a special case of Theorem 4 in Benjamini and Heller (2008).,4.2 Dataset Counting (Question 1),[0],[0]
"As demonstrated in Section 3.2, identifying the datasets with p−value below the nominal significance level and declaring them as those where algorithm A is better than B may lead to a very high number of erroneous claims.",4.3 Dataset Identification (Question 2),[0],[0]
A variety of methods exist for addressing this problem.,4.3 Dataset Identification (Question 2),[0],[0]
"A classical and very simple method for addressing this problem is named the Bonferroni’s procedure, which compensates for the increased probability of making at least one type I error by testing each individual hypothesis at a significance level of α′ = α/N , where α is the predefined bound on this probability and N is the number of hypotheses tested.6 While Bonferroni’s procedure is valid for any dependency among the p−values, the probability of detecting a true effect using this procedure is often very low, because of its strict p−value threshold.
",4.3 Dataset Identification (Question 2),[0],[0]
"Many other procedures controlling the above or other error criteria, and having less strict p−value thresholds, have been proposed.",4.3 Dataset Identification (Question 2),[0],[0]
"Below we advocate one of these methods: the Holm procedure (Holm, 1979).",4.3 Dataset Identification (Question 2),[0],[0]
This is a simple p−value based procedure that is concordant with the partial conjunction analysis when pu/NBonferroni is used in that analysis.,4.3 Dataset Identification (Question 2),[0],[0]
"Importantly for NLP applications, Holm controls the probability of making at least one type I error for any type of dependency between the participating datasets (see a demonstration in Section 6).
",4.3 Dataset Identification (Question 2),[0],[0]
"Let α be the desired upper bound on the probability that at least one false rejection occurs, let p(1) ≤",4.3 Dataset Identification (Question 2),[0],[0]
p(2) ≤ . . .,4.3 Dataset Identification (Question 2),[0],[0]
≤,4.3 Dataset Identification (Question 2),[0],[0]
p(N) be the ordered p−values and let the associated hypotheses be H(1) . . .,4.3 Dataset Identification (Question 2),[0],[0]
H(N).,4.3 Dataset Identification (Question 2),[0],[0]
"The Holm procedure for identifying the datasets with a significant effect is given below.
",4.3 Dataset Identification (Question 2),[0],[0]
Procedure Holm 1),4.3 Dataset Identification (Question 2),[0],[0]
"Let k be the minimal index such that
p(k)",4.3 Dataset Identification (Question 2),[0],[0]
> α N+1−k . 2) Reject the null hypotheses H(1) . . .,4.3 Dataset Identification (Question 2),[0],[0]
H(k−1),4.3 Dataset Identification (Question 2),[0],[0]
"and
do not reject H(k) . . .",4.3 Dataset Identification (Question 2),[0],[0]
H(N).,4.3 Dataset Identification (Question 2),[0],[0]
"If no such k exists, then reject all null hypotheses.
",4.3 Dataset Identification (Question 2),[0],[0]
The output of the Holm procedure is a rejection 6Bonferroni’s correction is based on similar considerations as pu/NBonferroni for u = 1 (Eq. 2).,4.3 Dataset Identification (Question 2),[0],[0]
"The partial conjunction framework (Sec. 4.1) extends this idea for other values of u.
list of null hypotheses; the corresponding datasets are those we return in response to the identification question of Section 1.",4.3 Dataset Identification (Question 2),[0],[0]
Note that the Holm procedure rejects a subset of hypotheses with p-value below α.,4.3 Dataset Identification (Question 2),[0],[0]
Each p-value is compared to a threshold which is smaller or equal to α and depends on the number of evaluation datasets N.,4.3 Dataset Identification (Question 2),[0],[0]
"The dependence of the thresholds on N can be intuitively explained as follows: the probability of making one or more erroneous claims may increase with N, as demonstrated in Section 3.2.",4.3 Dataset Identification (Question 2),[0],[0]
"Therefore, in order to bound this probability by a pre-specified level α, the thresholds for p-values should depend on N.
It can be shown that the Holm procedure at level α always rejects the k̂Bonferroni hypotheses with the smallest p−values, where k̂Bonferroni is the lower bound for k with a confidence level of 1 − α.",4.3 Dataset Identification (Question 2),[0],[0]
"Therefore, k̂Bonferroni corresponding to a confidence level of 1 − α is always smaller or equal to the number of datasets for which the difference between the compared algorithms is significant at level α.",4.3 Dataset Identification (Question 2),[0],[0]
"This is not surprising in view of the fact that, without making any assumptions on the dependencies among the datasets, k̂Bonferroni guarantees that the probability of making a too optimistic claim (k̂ > k) is bounded by α, when simply counting the number of datasets with p-value below α, the probability of making a too optimistic claim may be close to 1, as demonstrated in Section 5.
",4.3 Dataset Identification (Question 2),[0],[0]
Framework Summary Following Section 4.2 we answer the counting question of Section 1 by reporting either k̂Fisher (when all datasets can be assumed to be independent) or k̂Bonferroni (when such an independence assumption cannot be made).,4.3 Dataset Identification (Question 2),[0],[0]
"Based on Section 4.3 we suggest to answer the identification question of Section 1 by reporting the rejection list returned by the Holm procedure.
",4.3 Dataset Identification (Question 2),[0],[0]
Our proposed framework is based on certain assumptions regarding the experiments conducted in NLP setups.,4.3 Dataset Identification (Question 2),[0],[0]
The most prominent of these assumptions states that for dependent datasets the type of dependency cannot be determined.,4.3 Dataset Identification (Question 2),[0],[0]
"Indeed, to the best of our knowledge, the nature of the dependency between dependent test sets in NLP work has not been analyzed before.",4.3 Dataset Identification (Question 2),[0],[0]
In Section 7 we revisit our assumptions and point to alternative methods for answering our questions.,4.3 Dataset Identification (Question 2),[0],[0]
"These methods may be ap-
propriate under other assumptions that may become relevant in future.
",4.3 Dataset Identification (Question 2),[0],[0]
We next demonstrate the value of the proposed replicability analysis through toy examples with synthetic data (Section 5) as well as analysis of state-of-the-art algorithms for four major NLP applications (Section 6).,4.3 Dataset Identification (Question 2),[0],[0]
"Our point of reference is the standard, yet statistically unjustified, counting method that sets its estimator, k̂count, to the number of datasets for which the difference between the compared algorithms is significant with p−value ≤ α (i.e. k̂count = #{i : pi ≤ α}).7",4.3 Dataset Identification (Question 2),[0],[0]
"For the examples of this section we synthesize p−values to emulate a test with N = 100 hypotheses (domains), and set α to 0.05.",5 Toy Examples,[0],[0]
"We start with a simulation of a scenario where algorithmA is equivalent to B for each domain, and the datasets representing these domains are independent.",5 Toy Examples,[0],[0]
"We sample the 100 p−values from a standard uniform distribution, which is the p−value distribution under the null hypothesis, repeating the simulation 1,000 times.
",5 Toy Examples,[0],[0]
"Since all the null hypotheses are true then k, the number of false null hypotheses, is 0.",5 Toy Examples,[0],[0]
"Figure 1 presents the histogram of k̂ values from all 1,000 iterations according to k̂Bonferroni, k̂Fisher and k̂count.
",5 Toy Examples,[0],[0]
The figure clearly demonstrates that k̂count provides an overestimation of k while k̂Bonferroni and k̂Fisher do much better.,5 Toy Examples,[0],[0]
"Indeed, the histogram yields the following probability estimates: P̂ (k̂count >
7We use α in two different contexts: the significance level of an individual test and the bound on the probability to overestimate k.",5 Toy Examples,[0],[0]
"This is the standard notation in the statistical literature.
",5 Toy Examples,[0],[0]
"k) = 0.963, P̂ (k̂Bonferroni > k) = 0.001 and P̂ (k̂Fisher > k) = 0.021 (only the latter two are lower than 0.05).",5 Toy Examples,[0],[0]
"This simulation strongly supports the theoretical results of Section 4.2.
",5 Toy Examples,[0],[0]
"To consider a scenario where a dependency between the participating datasets does exist, we consider a second toy example.",5 Toy Examples,[0],[0]
"In this example we generate N = 100 p−values corresponding to 34 independent normal test statistics, and two other groups of 33 positively correlated normal test statistics with ρ = 0.2 and ρ = 0.5, respectively.",5 Toy Examples,[0],[0]
"We again assume that all null hypotheses are true and thus all the p−values are distributed uniformly, repeating the simulation 1,000 times.",5 Toy Examples,[0],[0]
"To generate positively dependent p−values, we followed the process described in Section 6.1 of Benjamini et al. (2006).
",5 Toy Examples,[0],[0]
We estimate the probability that k̂ > k,5 Toy Examples,[0],[0]
= 0,5 Toy Examples,[0],[0]
"for the three k̂ estimators based on the 1000 repetitions and get the values of: P̂ (k̂count > k) = 0.943, P̂ (k̂Bonferroni > k) = 0.046 and P̂ (k̂Fisher > k) = 0.234.",5 Toy Examples,[0],[0]
"This simulation demonstrates the importance of using Bonferroni’s method rather than Fisher’s method when the datasets are dependent, even if some of the datasets are independent.",5 Toy Examples,[0],[0]
In this section we demonstrate the potential impact of replicability analysis on the way experimental results are analyzed in NLP setups.,6 NLP Applications,[0],[0]
We explore four NLP applications: (a) two where the datasets are independent: multi-domain dependency parsing and multilingual POS tagging; and (b) two where dependency between the datasets does exist: cross-domain sentiment classification and word similarity prediction with word embedding models.,6 NLP Applications,[0],[0]
"Dependency Parsing We consider a multidomain setup, analyzing the results reported in Choi et al. (2015).",6.1 Data,[0],[0]
"The authors compared ten state-of-the-art parsers from which we pick three: (a) Mate (Bohnet, 2010)8 that performed best on the majority of datasets; (b) Redshift (Honnibal et al., 2013)9 which demonstrated comparable, still somewhat lower, performance compared to Mate;
8code.google.com/p/mate-tools.",6.1 Data,[0],[0]
"9github.com/syllog1sm/Redshift.
",6.1 Data,[0],[0]
"and (c) SpaCy (Honnibal and Johnson, 2015) that was substantially outperformed by Mate.
",6.1 Data,[0],[0]
"All parsers were trained and tested on the English portion of the OntoNotes 5 corpus (Weischedel et al., 2011; Pradhan et al., 2013), a large multigenre corpus consisting of the following 7 genres: broadcasting conversations (BC), broadcasting news (BN), news magazine (MZ), newswire (NW), pivot text (PT), telephone conversations (TC) and web text (WB).",6.1 Data,[0],[0]
"Train and test set size (in sentences) range from 6672 to 34,492 and from 280 to 2327, respectively (see Table 1 of Choi et al. (2015)).",6.1 Data,[0],[0]
"We copy the test set UAS results of Choi et al. (2015) and compute p−values using the data downloaded from http://amandastent.com/dependable/.
POS Tagging We consider a multilingual setup, analyzing the results reported in (Pinter et al., 2017).",6.1 Data,[0],[0]
"The authors compare their MIMICK model with the model of Ling et al. (2015), denoted with CHAR→TAG.",6.1 Data,[0],[0]
"Evaluation is performed on 23 of the 44 languages shared by the Polyglot word embedding dataset (Al-Rfou et al., 2013) and the universal dependencies (UD) dataset (De Marneffe et al., 2014).",6.1 Data,[0],[0]
"Pinter et al. (2017) choose their languages so that they reflect a variety of typological, and particularly morphological, properties.",6.1 Data,[0],[0]
The training/test split is the standard UD split.,6.1 Data,[0],[0]
"We copy the word level accuracy figures of Pinter et al. (2017) for the low resource training set setup, the focus setup of that paper.",6.1 Data,[0],[0]
"The authors kindly sent us their p-values.
",6.1 Data,[0],[0]
"Sentiment Classification In this task, an algorithm is trained on reviews from one domain and should classify the sentiment of reviews from another domain to the positive and negative classes.",6.1 Data,[0],[0]
For replicability analysis we explore the results of Ziser and Reichart (2017) for the cross-domain sentiment classification task of Blitzer et al. (2007).,6.1 Data,[0],[0]
"The data in this task consists of Amazon product reviews from 4 domains: books (B), DVDs (D), electronic items (E), and kitchen appliances (K), for the total of 12 domain pairs, each domain having a 2000 review test set.10 Ziser and Reichart (2017) compared the accuracy of their AE-SCL-SR model to MSDA (Chen et al., 2011), a well known domain adaptation
10http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment
method, and kindly sent us the required p-values.
",6.1 Data,[0],[0]
Word Similarity We compare two state-of-the-art word embedding collections: (a) word2vec,6.1 Data,[0],[0]
"CBOW (Mikolov et al., 2013) vectors, generated by the model titled the best “predict” model in Baroni et al. (2014);11 and (b) GloVe (Pennington et al., 2014) vectors generated by a model trained on a 42B token common web crawl.12 We employed the demo of Faruqui and Dyer (2014) to perform a Spearman correlation evaluation of these vector collections on 12 English word pair datasets: WS-353 (Finkelstein et al., 2001b), WS-353-SIM (Agirre et al., 2009), WS-353-REL (Agirre et al., 2009), MC-30 (Miller and Charles, 1991), RG-65 (Rubenstein and Goodenough, 1965), Rare-Word (Luong et al., 2013), MEN (Bruni et al., 2012), MTurk-287 (Radinsky et al., 2011), MTurk-771",6.1 Data,[0],[0]
"(Halawi et al., 2012), YP-130 (Yang and Powers, ), SimLex-999 (Hill et al., 2016), and Verb-143 (Baker et al., 2014).",6.1 Data,[0],[0]
"We first calculate the p−values for each task and dataset according to the principals of p−values computation for NLP as discussed in Yeh (2000), BergKirkpatrick et al. (2012) and Søgaard et al. (2014).
",6.2 Statistical Significance Tests,[0],[0]
"For dependency parsing, we employ the aparametric paired bootstrap test (Efron and Tibshirani, 1994) that does not assume any distribution on the test statistics.",6.2 Statistical Significance Tests,[0],[0]
We chose this test because the distribution of the values for the measures commonly applied in this task is unknown.,6.2 Statistical Significance Tests,[0],[0]
"We implemented the test as in (Berg-Kirkpatrick et al., 2012) with a bootstrap size of 500 and with 105 repetitions.
",6.2 Statistical Significance Tests,[0],[0]
"For multilingual POS tagging, we employ the Wilcoxon signed-rank test (Wilcoxon, 1945) on the differences of the sentence level accuracy scores of the two compared models.",6.2 Statistical Significance Tests,[0],[0]
"This test is a nonparametric test for differences in measure, testing the null hypothesis that the difference has a symmetric distribution around zero.",6.2 Statistical Significance Tests,[0],[0]
"It is appropriate for tasks with paired continuous measures for each observation, which is the case when comparing sentence level accuracies.
",6.2 Statistical Significance Tests,[0],[0]
11http://clic.cimec.unitn.it/composes/ semantic-vectors.html.,6.2 Statistical Significance Tests,[0],[0]
"Parameters: 5-word context window, 10 negative samples, subsampling, 400 dimensions.
",6.2 Statistical Significance Tests,[0],[0]
"12http://nlp.stanford.edu/projects/glove/. 300 dimensions.
",6.2 Statistical Significance Tests,[0],[0]
"For sentiment classification we employ the McNemar test for paired nominal data (McNemar, 1947).",6.2 Statistical Significance Tests,[0],[0]
"This test is appropriate for binary classification tasks and since we compare the results of the algorithms when applied on the same datasets, we employ its paired version.",6.2 Statistical Significance Tests,[0],[0]
"Finally, for word similarity with its Spearman correlation evaluation, we choose the Steiger test (Steiger, 1980) for comparing elements in a correlation matrix.
",6.2 Statistical Significance Tests,[0],[0]
We consider the case of α = 0.05 for all four applications.,6.2 Statistical Significance Tests,[0],[0]
"For the dependent datasets experiments (sentiment classification and word similarity prediction) with their generally lower p−values (see below), we also consider the case where α = 0.01.",6.2 Statistical Significance Tests,[0],[0]
"Table 1 summarizes the replicability analysis results while Table 2 – 5 present task specific performance measures and p−values.
",6.3 Results,[0],[0]
"Independent Datasets Dependency parsing (Table 2) and multilingual POS tagging (Table 3) are our example tasks for this setup, where k̂Fisher is our recommended valid estimator for the number of cases where one algorithm outperforms another.
",6.3 Results,[0],[0]
"For dependency parsing, we compare two scenarios: (a) where in most domains the differences between the compared algorithms are quite large and the p−values are small (Mate vs. SpaCy); and (b)
where in most domains the differences between the compared algorithms are smaller and the p−values are higher (Mate vs. Redshift).",6.3 Results,[0],[0]
"Our multilingual POS tagging scenario (MIMICK vs. Char→Tag) is more similar to scenario (b) in terms of the differences between the participating algorithms.
",6.3 Results,[0],[0]
Table 1 demonstrates the k̂ estimators for the various tasks and scenarios.,6.3 Results,[0],[0]
"For dependency parsing, as expected, in scenario (a) where all the p−values are small, all estimators, even the error-prone k̂count, provide the same information.",6.3 Results,[0],[0]
"In case (b) of dependency parsing, however, k̂Fisher estimates the number of domains where Mate outperforms Redshift to be 5, while k̂count estimates this number to be 2.",6.3 Results,[0],[0]
This is a substantial difference given that the number of domains is 7.,6.3 Results,[0],[0]
"The k̂Bonferroni estimator, that is valid under arbitrary dependencies, is even more conservative than k̂count and its estimation is only 1.
",6.3 Results,[0],[0]
"Perhaps not surprisingly, the multilingual POS
tagging results are similar to case (b) of dependency parsing.",6.3 Results,[0],[0]
"Here, again, k̂count is too conservative, estimating the number of languages with effect to be 11 (out of 23) while k̂Fisher estimates this number to be 16 (an increase of 5/23 in the estimated number of languages with effect).",6.3 Results,[0],[0]
"k̂Bonferroni is again more conservative, estimating the number of languages with effect to be only 6, which is not very surprising given that it does not exploit the independence between the datasets.",6.3 Results,[0],[0]
"These two examples of case (b) demonstrate that when the differences between the algorithms are quite small, k̂Fisher may be more sensitive than the current practice in NLP for discovering the number of datasets with effect.
",6.3 Results,[0],[0]
"To complete the analysis, we would like to name the datasets with effect.",6.3 Results,[0],[0]
"As discussed in Section 4.2, while this can be straightforwardly done by naming the datasets with the k̂ smallest p−values, in general, this approach does not control the probability of identifying at least one dataset erroneously.",6.3 Results,[0],[0]
"We thus employ the Holm procedure for the identification task, noticing that the number of datasets it identifies should be equal to the value of the k̂Bonferroni estimator (Section 4.3).
",6.3 Results,[0],[0]
"Indeed, for dependency parsing in case (a), the Holm procedure identifies all seven domains as cases where Mate outperforms SpaCy, while in case (b) it identifies only the MZ domain as a case where Mate outperforms Redshift.",6.3 Results,[0],[0]
"For multilingual POS
tagging the Holm procedure identifies Tamil, Hungarian, Basque, Indonesian, Chinese and Czech as languages where MIMICK outperforms Char→Tag.",6.3 Results,[0],[0]
"This analysis demonstrates that when the performance gap between two algorithms becomes narrower, inquiring for more information (i.e. identifying the domains with effect rather than just estimating their number), may result in weaker results.13
Dependent Datasets In cross-domain sentiment classification (Table 4) and word similarity prediction (Table 5), the involved datasets manifest mutual dependence.",6.3 Results,[0],[0]
"Particularly, each sentiment setup shares its test dataset with 2 other setups, while in word similarity WS-353 is the union of WS-353REL and WS-353-SIM.",6.3 Results,[0],[0]
"As discussed in Section 4, k̂Bonferroni is the appropriate estimator of the number of cases one algorithm outperforms another.
",6.3 Results,[0],[0]
"The results in Table 1 manifest the phenomenon demonstrated by the second toy example in Section 5, which shows that when the datasets are dependent, k̂Fisher as well as the error-prone k̂count may be too optimistic regarding the number of datasets with effect.",6.3 Results,[0],[0]
"This stands in contrast to k̂Bonferroni which controls the probability to overestimate the number of such datasets.
",6.3 Results,[0],[0]
"Indeed, k̂Bonferroni is much more conservative, yielding values of 6 (α = 0.05) and 2 (α = 0.01) for sentiment, and of 6 (α = 0.05) and 4 (α = 0.01) for word similarity.",6.3 Results,[0],[0]
The differences from the conclusions that might have been drawn by k̂count are again quite substantial.,6.3 Results,[0],[0]
"The difference between k̂Bonferroni and k̂count in sentiment classification is 4, which accounts to 1/3 of the 12 test setups.",6.3 Results,[0],[0]
"Even for word similarity, the difference between the two methods, which account to 2 for both α values, represents 1/6 of the 12 test setups.",6.3 Results,[0],[0]
"The domains identified by the Holm procedure are marked in the tables.
",6.3 Results,[0],[0]
"Results Overview Our goal in this section is to demonstrate that the approach of simply looking at the number of datasets for which the difference between the performance of the algorithms reaches a predefined significance level, gives different results
13For completeness, we also performed the analysis for the independent dataset setups with α = 0.01.",6.3 Results,[0],[0]
"The results are (k̂count, k̂Bonferroni, k̂Fisher): Mate vs. SpaCy: (7,7,7); Mate vs. Redshift (1,0,2); MIMICK vs. Char→Tag: (7,5,13).",6.3 Results,[0],[0]
"The patterns are very similar to those discussed in the text.
from our suggested statistically sound analysis.",6.3 Results,[0],[0]
This approach is denoted here with k̂count and shown to be statistically not valid in Sections 3.2 and 5.,6.3 Results,[0],[0]
We observe that this happens especially in evaluation setups where the differences between the algorithms are small for most datasets.,6.3 Results,[0],[0]
"In some cases, when the datasets are independent, our analysis has the power to declare a larger number of datasets with effect than the number of individual significant test values (k̂count).",6.3 Results,[0],[0]
"In other cases, when the datasets are interdependent, k̂count is much too optimistic.
",6.3 Results,[0],[0]
Our proposed analysis changes the observations that might have been made based on the papers where the results analyzed here were originally reported.,6.3 Results,[0],[0]
"For example, for the Mate-Redshift comparison (independent evaluation sets), we show that there is evidence that the number of datasets with effect is much higher than one would assume based on counting the significant sets (5 vs. 2 out of 7 evaluation sets), giving a stronger claim regarding the superiority of Mate.",6.3 Results,[0],[0]
"In multilingual POS tagging (again, a setup of independent evaluation sets) our analysis shows evidence for 16 sets with effect compared to only 11 of the erroneous count method - a difference in 5 out of 23 evaluation sets (21.7%).",6.3 Results,[0],[0]
"Finally, in the cross-domain sentiment classification and the word similarity judgment tasks (dependent evaluation sets), the unjustified counting method may be too optimistic (e.g. 10 vs. 6 out of 12 evaluation sets, for α = 0.05 in the sentiment task), in favor of the new algorithms.",6.3 Results,[0],[0]
We proposed a statistically sound replicability analysis framework for cases where algorithms are compared across multiple datasets.,7 Discussion and Future Directions,[0],[0]
"Our main contributions are: (a) analyzing the limitations of the current practice in NLP work; and (b) proposing a framework that addresses both the estimation of the number of datasets with effect and their identification.
",7 Discussion and Future Directions,[0],[0]
The framework we propose addresses two different situations encountered in NLP: independent and dependent datasets.,7 Discussion and Future Directions,[0],[0]
"For dependent datasets, we assumed that the type of dependency cannot be determined.",7 Discussion and Future Directions,[0],[0]
One could use more powerful methods if certain assumptions on the dependency between the test statistics could be made.,7 Discussion and Future Directions,[0],[0]
"For example, one could use
the partial conjunction p-value based on Simes test for the global null hypothesis (Simes, 1986), which was proposed by Benjamini and Heller (2008) for the case where the test statistics satisfy certain positive dependency properties (see Theorem 1 in (Benjamini and Heller, 2008)).",7 Discussion and Future Directions,[0],[0]
"Using this partial conjunction p-value rather than the one based on Bonferroni, one may obtain higher values of k̂ with the same statistical guarantee.",7 Discussion and Future Directions,[0],[0]
"Similarly, for the identification question, if certain positive dependency properties hold, Holm’s procedure could be replaced by Hochberg’s or Hommel’s procedures (Hochberg, 1988; Hommel, 1988) which are more powerful.
",7 Discussion and Future Directions,[0],[0]
"An alternative, more powerful multiple testing procedure for identification of datasets with effect, is the method in Benjamini and Hochberg (1995), that controls the false discovery rate (FDR), a less strict error criterion than the one considered here.",7 Discussion and Future Directions,[0],[0]
"This method is more appropriate in cases where one may tolerate some errors as long as the proportion of errors among all the claims made is small, as expected to happen when the number of datasets grows.
",7 Discussion and Future Directions,[0],[0]
We note that the increase in the number of evaluation datasets may have positive and negative aspects.,7 Discussion and Future Directions,[0],[0]
"As noted in Section 2, we believe that multiple comparisons are integral to NLP research when aiming to develop algorithms that perform well across languages and domains.",7 Discussion and Future Directions,[0],[0]
"On the other hand, experimenting with multiple evaluation sets that reflect very similar linguistic phenomena may only complicate the comparison between alternative algorithms.
",7 Discussion and Future Directions,[0],[0]
"In fact, our analysis is useful mostly where the datasets are heterogeneous, coming from different languages or domains.",7 Discussion and Future Directions,[0],[0]
"When they are just technically different but could potentially be just combined into a one big dataset, then we believe the question of Demšar (2006), whether at least one dataset shows evidence for effect, is more appropriate.",7 Discussion and Future Directions,[0],[0]
The research of M. Bogomolov was supported by the Israel Science Foundation grant No. 1112/14.,Acknowledgement,[0],[0]
We thank Yuval Pinter for his great help with the multilingual experiments and for his useful feedback.,Acknowledgement,[0],[0]
"We also thank Ruth Heller, Marten van Schijndel, Oren Tsur, Or Zuk and the ie@technion NLP group members for their useful comments.",Acknowledgement,[0],[0]
"With the ever growing amount of textual data from a large variety of languages, domains, and genres, it has become standard to evaluate NLP algorithms on multiple datasets in order to ensure a consistent performance across heterogeneous setups.",abstractText,[0],[0]
"However, such multiple comparisons pose significant challenges to traditional statistical analysis methods in NLP and can lead to erroneous conclusions.",abstractText,[0],[0]
In this paper we propose a Replicability Analysis framework for a statistically sound analysis of multiple comparisons between algorithms for NLP tasks.,abstractText,[0],[0]
"We discuss the theoretical advantages of this framework over the current, statistically unjustified, practice in the NLP literature, and demonstrate its empirical value across four applications: multi-domain dependency parsing, multilingual POS tagging, cross-domain sentiment classification and word similarity prediction.",abstractText,[0],[0]
1,abstractText,[0],[0]
Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets,title,[0],[0]
Graphs are a ubiquitous structure that widely occurs in data analysis problems.,1. Introduction,[0],[0]
"Real-world graphs such as social networks, financial networks, biological networks and citation networks represent important rich information which is not seen from the individual entities alone, for example, the communities a person is in, the functional role of a molecule, and the sensitivity of the assets of an enterprise to external shocks.",1. Introduction,[0],[0]
"Therefore, representation learning of nodes in graphs aims to extract high-level features from a node as well as its neighborhood, and has proved extremely useful for many applications, such as node classification, clustering, and link prediction (Perozzi et al., 2014; Monti et al.,
1Massachusetts Institute of Technology (MIT) 2National Institute of Informatics, Tokyo.",1. Introduction,[0],[0]
Correspondence to: Keyulu Xu,1. Introduction,[0],[0]
"<keyulu@mit.edu>, Stefanie Jegelka <stefje@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
2017; Grover & Leskovec, 2016; Tang et al., 2015).
",1. Introduction,[0],[0]
Recent works focus on deep learning approaches to node representation.,1. Introduction,[0],[0]
"Many of these approaches broadly follow a neighborhood aggregation (or “message passing” scheme), and those have been very promising (Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Veličković",1. Introduction,[0],[0]
"et al., 2018; Kearnes et al., 2016).",1. Introduction,[0],[0]
"These models learn to iteratively aggregate the hidden features of every node in the graph with its adjacent nodes’ as its new hidden features, where an iteration is parametrized by a layer of the neural network.",1. Introduction,[0],[0]
"Theoretically, an aggregation process of k iterations makes use of the subtree structures of height k rooted at every node.",1. Introduction,[0],[0]
"Such schemes have been shown to generalize the Weisfeiler-Lehman graph isomorphism test (Weisfeiler & Lehman, 1968) enabling to simultaneously learn the topology as well as the distribution of node features in the neighborhood (Shervashidze et al., 2011; Kipf & Welling, 2017; Hamilton et al., 2017).
",1. Introduction,[0],[0]
"Yet, such aggregation schemes sometimes lead to surprises.",1. Introduction,[0],[0]
"For example, it has been observed that the best performance with one of the state-of-the-art models, Graph Convolutional Networks (GCN), is achieved with a 2-layer model.",1. Introduction,[0],[0]
"Deeper versions of the model that, in principle, have access to more information, perform worse (Kipf & Welling, 2017).",1. Introduction,[0],[0]
"A similar degradation of learning for computer vision problems is resolved by residual connections (He et al., 2016a) that greatly aid the training of deep models.",1. Introduction,[0],[0]
"But, even with residual connections, GCNs with more layers do not perform as well as the 2-layer GCN on many datasets, e.g. citation networks.
",1. Introduction,[0],[0]
"Motivated by observations like the above, in this paper, we address two questions.",1. Introduction,[0],[0]
"First, we study properties and resulting limitations of neighborhood aggregation schemes.",1. Introduction,[0],[0]
"Second, based on this analysis, we propose an architecture that, as opposed to existing models, enables adaptive, structure-aware representations.",1. Introduction,[0],[0]
"Such representations are particularly interesting for representation learning on large complex graphs with diverse subgraph structures.
",1. Introduction,[0],[0]
Model analysis.,1. Introduction,[0],[0]
"To better understand the behavior of different neighborhood aggregation schemes, we analyze the effective range of nodes that any given node’s representation draws from.",1. Introduction,[0],[0]
"We summarize this sensitivity analysis by what
we name the influence distribution of a node.",1. Introduction,[0],[0]
This effective range implicitly encodes prior assumptions on what are the “nearest neighbors” that a node should draw information from.,1. Introduction,[0],[0]
"In particular, we will see that this influence is heavily affected by the graph structure, raising the question whether “one size fits all”, in particular in graphs whose subgraphs have varying properties (such as more tree-like or more expander-like).
",1. Introduction,[0],[0]
"In particular, our more formal analysis connects influence distributions with the spread of a random walk at a given node, a well-understood phenomenon as a function of the graph structure and eigenvalues (Lovász, 1993).",1. Introduction,[0],[0]
"For instance, in some cases and applications, a 2-step random walk influence that focuses on local neighborhoods can be more informative than higher-order features where some of the information may be “washed out” via averaging.
",1. Introduction,[0],[0]
Changing locality.,1. Introduction,[0],[0]
"To illustrate the effect and importance of graph structure, recall that many real-world graphs possess locally strongly varying structure.",1. Introduction,[0],[0]
"In biological and citation networks, the majority of the nodes have few connections, whereas some nodes (hubs) are connected to many other nodes.",1. Introduction,[0],[0]
"Social and web networks usually consist of an expander-like core part and an almost-tree (bounded treewidth) part, which represent well-connected entities and the small communities respectively (Leskovec et al., 2009; Maehara et al., 2014; Tsonis et al., 2006).
",1. Introduction,[0],[0]
"Besides node features, this subgraph structure has great impact on the result of neighborhood aggregation.",1. Introduction,[0],[0]
"The speed of expansion or, equivalently, growth of the influence radius, is characterized by the random walk’s mixing time, which changes dramatically on subgraphs with different structures (Lovász, 1993).",1. Introduction,[0],[0]
"Thus, the same number of iterations (layers) can lead to influence distributions of very different locality.",1. Introduction,[0],[0]
"As an example, consider the social network in Figure 1 from GooglePlus (Leskovec & Mcauley, 2012).",1. Introduction,[0],[0]
The figure illustrates the expansions of a random walk starting at the square node.,1. Introduction,[0],[0]
The walk (a) from a node within the core rapidly includes almost the entire graph.,1. Introduction,[0],[0]
"In contrast, the walk (b) starting at a node in the tree part includes only a very small fraction of all nodes.",1. Introduction,[0],[0]
"After 5 steps, the same walk has reached the core and, suddenly, spreads quickly.",1. Introduction,[0],[0]
"Translated to graph representation models, these spreads become the influence distributions or, in other words, the averaged features yield the new feature of the walk’s starting node.",1. Introduction,[0],[0]
"This shows that in the same graph, the same number of steps can lead to very different effects.",1. Introduction,[0],[0]
"Depending on the application, wide-range or smallrange feature combinations may be more desirable.",1. Introduction,[0],[0]
"A too rapid expansion may average too broadly and thereby lose information, while in other parts of the graph, a sufficient neighborhood may be needed for stabilizing predictions.
",1. Introduction,[0],[0]
JK networks.,1. Introduction,[0],[0]
"The above observations raise the question
whether it is possible to adaptively adjust (i.e., learn) the influence radii for each node and task.",1. Introduction,[0],[0]
"To achieve this, we explore an architecture that learns to selectively exploit information from neighborhoods of differing locality.",1. Introduction,[0],[0]
"This architecture selectively combines different aggregations at the last layer, i.e., the representations “jump” to the last layer.",1. Introduction,[0],[0]
"Hence, we name the resulting networks Jumping Knowledge Networks (JK-Nets).",1. Introduction,[0],[0]
"We will see that empirically, when adaptation is an option, the networks indeed learn representations of different orders for different graph substructures.",1. Introduction,[0],[0]
"Moreover, in Section 6, we show that applying our framework to various state-of-the-art neighborhood-aggregation models consistently improves their performance.",1. Introduction,[0],[0]
"We begin by summarizing some of the most common neighborhood aggregation schemes and, along the way, introduce our notation.",2. Background and Neighborhood aggregation schemes,[0],[0]
"Let G = (V,E) be a simple graph with node features Xv ∈",2. Background and Neighborhood aggregation schemes,[0],[0]
Rdi for v ∈ V .,2. Background and Neighborhood aggregation schemes,[0],[0]
Let G̃ be the graph obtained by adding a self-loop to every v ∈ V .,2. Background and Neighborhood aggregation schemes,[0],[0]
The hidden feature of node v learned by the l-th layer of the model is denoted by h (l) v ∈,2. Background and Neighborhood aggregation schemes,[0],[0]
Rdh .,2. Background and Neighborhood aggregation schemes,[0],[0]
"Here, di is the dimension of the input features and dh is the dimension of the hidden features, which, for simplicity of exposition, we assume to be the same across layers.",2. Background and Neighborhood aggregation schemes,[0],[0]
We also use h(0)v = Xv for the node feature.,2. Background and Neighborhood aggregation schemes,[0],[0]
"The neighborhood N(v) = {u ∈ V | (v, u) ∈ E} of node v is the set of adjacent nodes of v. The analogous neighborhood Ñ(v) = {v} ∪ {u ∈ V | (v, u) ∈ E} on G̃ includes v.
A typical neighborhood aggregation scheme can generically be written as follows: for a k-layer model, the l-th layer (l = 1..k) updates h(l)v for every v ∈ V simultaneously as
h(l)v = σ",2. Background and Neighborhood aggregation schemes,[0],[0]
"( Wl · AGGREGATE ({ h(l−1)u ,∀u ∈ Ñ(v) }))",2. Background and Neighborhood aggregation schemes,[0],[0]
"(1)
where AGGREGATE is an aggregation function defined by the specific model, Wl is a trainable weight matrix on the lth layer shared by all nodes, and σ is a non-linear activation function, e.g. a ReLU.
",2. Background and Neighborhood aggregation schemes,[0],[0]
Graph Convolutional Networks (GCN).,2. Background and Neighborhood aggregation schemes,[0],[0]
"Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), initially motivated by spectral graph convolutions (Hammond et al., 2011; Defferrard et al., 2016), are a specific instantiation of this framework (Gilmer et al., 2017), of the form
h(l)v = ReLU ( Wl · ∑ u∈Ñ(v) (deg(v)deg(u))−1/2 h(l−1)u ) (2)
where deg(v) is the degree of node v in G. Hamilton et al. (2017) derived a variant of GCN that also works in inductive settings (previously unseen nodes), by using a different normalization to average:
h(l)v",2. Background and Neighborhood aggregation schemes,[0],[0]
"= ReLU ( Wl · 1
d̃eg(v) ∑ u∈Ñ(v) h(l−1)u ) (3)
where d̃eg(v) is the degree of node v in G̃.
Neighborhood Aggregation with Skip Connections.",2. Background and Neighborhood aggregation schemes,[0],[0]
Instead of aggregating a node and its neighbors at the same time as in Eqn.,2. Background and Neighborhood aggregation schemes,[0],[0]
"(1), a number of recent approaches aggregate the neighbors first and then combine the resulting neighborhood representation with the node’s representation from the last iteration.",2. Background and Neighborhood aggregation schemes,[0],[0]
"More formally, each node is updated as
h (l) N(v) = σ",2. Background and Neighborhood aggregation schemes,[0],[0]
"( Wl · AGGREGATEN ( {h(l−1)u ,∀u ∈ N(v)} )) h(l)v =",2. Background and Neighborhood aggregation schemes,[0],[0]
"COMBINE ( h(l−1)v , h (l) N(v)
) where AGGREGATEN and COMBINE are defined by the specific model.",2. Background and Neighborhood aggregation schemes,[0],[0]
The COMBINE step is key to this paradigm and can be viewed as a form of a ”skip connection” between different layers.,2. Background and Neighborhood aggregation schemes,[0],[0]
"For COMBINE, GraphSAGE (Hamilton et al., 2017) uses concatenation after a feature transform.",2. Background and Neighborhood aggregation schemes,[0],[0]
"Column Networks (Pham et al., 2017) interpolate the neighborhood representation and the node’s previous representation, and Gated GNN (Li et al., 2016) uses the Gated Recurrent Unit (GRU) (Cho et al., 2014).",2. Background and Neighborhood aggregation schemes,[0],[0]
"Another wellknown variant of skip connections, residual connections, use the identity mapping to help signals propagate (He et al., 2016a;b).
",2. Background and Neighborhood aggregation schemes,[0],[0]
"These skip connections are input- but not output-unit specific: If we ”skip” a layer for h(l)v (do not aggregate) or use a certain COMBINE, all subsequent units using this representation will be using this skip implicitly.",2. Background and Neighborhood aggregation schemes,[0],[0]
It is impossible that a certain higher-up representation h(l+j)u uses the skip and another one does not.,2. Background and Neighborhood aggregation schemes,[0],[0]
"As a result, skip connections cannot adaptively adjust the neighborhood sizes of the final-layer representations independently.
",2. Background and Neighborhood aggregation schemes,[0],[0]
Neighborhood Aggregation with Directional Biases.,2. Background and Neighborhood aggregation schemes,[0],[0]
"Some recent models, rather than treating the features of
adjacent nodes equally, weigh “important” neighbors more.",2. Background and Neighborhood aggregation schemes,[0],[0]
"This paradigm can be viewed as neighborhood-aggregation with directional biases because a node will be influenced by some directions of expansion more than the others.
",2. Background and Neighborhood aggregation schemes,[0],[0]
"Graph Attention Networks (GAT) (Veličković et al., 2018) and VAIN (Hoshen, 2017) learn to select the important neighbors via an attention mechanism.",2. Background and Neighborhood aggregation schemes,[0],[0]
"The max-pooling operation in GraphSAGE (Hamilton et al., 2017) implicitly selects the important nodes.",2. Background and Neighborhood aggregation schemes,[0],[0]
"This line of work is orthogonal to ours, because it modifies the direction of expansion whereas our model operates on the locality of expansion.",2. Background and Neighborhood aggregation schemes,[0],[0]
Our model can be combined with these models to add representational power.,2. Background and Neighborhood aggregation schemes,[0],[0]
"In Section 6, we demonstrate that our framework works with not only simple neighborhood-aggregation models (GCN), but also with skip connections (GraphSAGE) and directional biases (GAT).",2. Background and Neighborhood aggregation schemes,[0],[0]
"Next, we explore some important properties of the above aggregation schemes.",3. Influence Distribution and Random Walks,[0],[0]
"Related to ideas of sensitivity analysis and influence functions in statistics (Koh & Liang, 2017) that measure the influence of a training point on parameters, we study the range of nodes whose features affect a given node’s representation.",3. Influence Distribution and Random Walks,[0],[0]
"This range gives insight into how large a neighborhood a node is drawing information from.
",3. Influence Distribution and Random Walks,[0],[0]
"We measure the sensitivity of node x to node y, or the influence of y on x, by measuring how much a change in the input feature of y affects the representation of x in the last layer.",3. Influence Distribution and Random Walks,[0],[0]
"For any node x, the influence distribution captures the relative influences of all other nodes.
",3. Influence Distribution and Random Walks,[0],[0]
Definition 3.1 (Influence score and distribution).,3. Influence Distribution and Random Walks,[0],[0]
"For a simple graph G = (V,E), let h(0)x be the input feature and h
(k) x be the learned hidden feature of node x ∈ V at the k-th (last) layer of the model.",3. Influence Distribution and Random Walks,[0],[0]
"The influence score I(x, y) of node x by any node y ∈ V is the sum of the absolute values
of the entries of the Jacobian matrix [ ∂h(k)x ∂h (0) y ] .",3. Influence Distribution and Random Walks,[0],[0]
"We define the influence distribution Ix of x ∈ V by normalizing the influence scores: Ix(y) = I(x, y)/ ∑ z I(x, z), or
Ix(y) = e T
[ ∂h (k) x
∂h (0) y
] e /(∑
z∈V eT
[ ∂h (k) x
∂h (0) z
] e )
where e is the all-ones vector.
",3. Influence Distribution and Random Walks,[0],[0]
"Later, we will see connections of influence distributions with random walks.",3. Influence Distribution and Random Walks,[0],[0]
"For completeness, we also define random walk distributions.
",3. Influence Distribution and Random Walks,[0],[0]
Definition 3.2.,3. Influence Distribution and Random Walks,[0],[0]
"Consider a random walk on G̃ starting at a node v0; if at the t-th step we are at a node vt, we move to any neighbor of vt (including vt) with equal probability.
",3. Influence Distribution and Random Walks,[0],[0]
"The t-step random walk distribution Pt of v0 is
Pt (i) =",3. Influence Distribution and Random Walks,[0],[0]
Prob (vt = i) .,3. Influence Distribution and Random Walks,[0],[0]
"(4)
Analogous definitions apply for random walks with nonuniform transition probabilities.
",3. Influence Distribution and Random Walks,[0],[0]
An important property of the random walk distribution is that it becomes more spread out as t increases and converges to the limit distribution if the graph is non-bipartite.,3. Influence Distribution and Random Walks,[0],[0]
"The rate of convergence depends on the structure of the subgraph and can be bounded by the spectral gap (or the conductance) of the random walk’s transition matrix (Lovász, 1993).",3. Influence Distribution and Random Walks,[0],[0]
The influence distribution for different aggregation models and nodes can give insights into the information captured by the respective representations.,3.1. Model Analysis,[0],[0]
The following results show that the influence distributions of common aggregation schemes are closely connected to random walk distributions.,3.1. Model Analysis,[0],[0]
"This observation hints at specific implications – strengths and weaknesses – that we will discuss.
",3.1. Model Analysis,[0],[0]
"With a randomization assumption of the ReLU activations similar to that in (Kawaguchi, 2016; Choromanska et al., 2015), we can draw connections between GCNs and random walks:
Theorem 1.",3.1. Model Analysis,[0],[0]
"Given a k-layer GCN with averaging as in Equation (3), assume that all paths in the computation graph of the model are activated with the same probability of success ρ.",3.1. Model Analysis,[0],[0]
"Then the influence distribution Ix for any node
x ∈ V is equivalent, in expectation, to the k-step random walk distribution on G̃ starting at node x.
We prove Theorem 1 in the appendix.
",3.1. Model Analysis,[0],[0]
It is straightforward to modify the proof of Theorem 1 to show a nearly equivalent result for the version of GCN in Equation (2).,3.1. Model Analysis,[0],[0]
"The only difference is that each random walk path v0p, v 1 p, ..., v k p from node x (v 0 p) to y (v k p), in-
stead of probability ρ ∏k l=1 1
d̃eg(vlp) , now has probability
ρ Q ∏k−1 l=1 1
d̃eg(vlp) · (d̃eg(x)d̃eg(y))−1/2, where Q is a nor-
malizing factor.",3.1. Model Analysis,[0],[0]
"Thus, the difference in probability is small, especially when the degree of x and y are close.
",3.1. Model Analysis,[0],[0]
"Similarly, we can show that neighborhood aggregation schemes with directional biases resemble biased random walk distributions.",3.1. Model Analysis,[0],[0]
"This follows by substituting the corresponding probabilities into the proof of Theorem 1.
",3.1. Model Analysis,[0],[0]
"Empirically, we observe that, despite somewhat simplifying assumptions, our theory is close to what happens in practice.",3.1. Model Analysis,[0],[0]
"We visualize the heat maps of the influence distributions for a node (labeled square) for trained GCNs, and compare with the random walk distributions starting at the same node.",3.1. Model Analysis,[0],[0]
Figure 2 shows example results.,3.1. Model Analysis,[0],[0]
Darker colors correspond to higher influence probabilities.,3.1. Model Analysis,[0],[0]
"To show the effect of skip connections, Figure 3 visualizes the analogous heat maps for one example—GCN with residual connections.",3.1. Model Analysis,[0],[0]
"Indeed, we observe that the influence distributions of networks with residual connections approximately correspond to lazy random walks: each step has a higher probability of staying at
the current node.",3.1. Model Analysis,[0],[0]
Local information is retained with similar probabilities for all nodes in each iteration; this cannot adapt to diverse needs of specific upper-layer nodes.,3.1. Model Analysis,[0],[0]
"Further visualizations may be found in the appendix.
",3.1. Model Analysis,[0],[0]
Fast Collapse on Expanders.,3.1. Model Analysis,[0],[0]
"To better understand the implication of Theorem 1 and the limitations of the corresponding neighborhood aggregation algorithms, we revisit the scenario of learning on a social network shown in Figure 1.",3.1. Model Analysis,[0],[0]
"Random walks starting inside an expander converge rapidly in O(log |V |) steps to an almost-uniform distribution (Hoory et al., 2006).",3.1. Model Analysis,[0],[0]
"After O(log |V |) iterations of neighborhood aggregation, by Theorem 1 the representation of every node is influenced almost equally by any other node in the expander.",3.1. Model Analysis,[0],[0]
"Thus, the node representations will be representative of the global graph and carry limited information about individual nodes.",3.1. Model Analysis,[0],[0]
"In contrast, random walks starting at the bounded tree-width (almost-tree) part converge slowly, i.e., the features retain more local information.",3.1. Model Analysis,[0],[0]
"Models that impose a fixed random walk distribution inherit these discrepancies in the speed of expansion and influence neighborhoods, which may not lead to the best representations for all nodes.",3.1. Model Analysis,[0],[0]
"The above observations raise the question whether the fixed but structure-dependent influence radius size induced by
common aggregation schemes really achieves the best representations for all nodes and tasks.",4. Jumping Knowledge Networks,[0],[0]
"Large radii may lead to too much averaging, while small radii may lead to instabilities or insufficient information aggregation.",4. Jumping Knowledge Networks,[0],[0]
"Hence, we propose two simple yet powerful architectural changes – jump connections and a subsequent selective but adaptive aggregation mechanism.
",4. Jumping Knowledge Networks,[0],[0]
"Figure 4 illustrates the main idea: as in common neighborhood aggregation networks, each layer increases the size of the influence distribution by aggregating neighborhoods from the previous layer.",4. Jumping Knowledge Networks,[0],[0]
"At the last layer, for each node, we carefully select from all of those itermediate representations (which “jump” to the last layer), potentially combining a few.",4. Jumping Knowledge Networks,[0],[0]
"If this is done independently for each node, then the model can adapt the effective neighborhood size for each node as needed, resulting in exactly the desired adaptivity.
",4. Jumping Knowledge Networks,[0],[0]
Our model permits general layer-aggregation mechanisms.,4. Jumping Knowledge Networks,[0],[0]
We explore three approaches; others are possible too.,4. Jumping Knowledge Networks,[0],[0]
"Let h (1) v , ..., h (k) v be the jumping representations of node v (from k layers) that are to be aggregated.
",4. Jumping Knowledge Networks,[0],[0]
Concatenation.,4. Jumping Knowledge Networks,[0],[0]
"A concatenation [ h (1) v , ..., h (k) v ] is the
most straightforward way to combine the layers, after which we may perform a linear transformation.",4. Jumping Knowledge Networks,[0],[0]
"If the transformation weights are shared across graph nodes, this approach is not node-adaptive.",4. Jumping Knowledge Networks,[0],[0]
"Instead, it optimizes the weights to combine the subgraph features in a way that works best for the dataset overall.",4. Jumping Knowledge Networks,[0],[0]
"One may expect concatenation to be suitable for small graphs and graphs with regular structure that require less adaptivity; also because weight-sharing helps reduce overfitting.
",4. Jumping Knowledge Networks,[0],[0]
Max-pooling.,4. Jumping Knowledge Networks,[0],[0]
"An element-wise max ( h (1) v , ..., h (k) v ) selects the most informative layer for each feature coordinate.",4. Jumping Knowledge Networks,[0],[0]
"For example, feature coordinates that represent more local properties can use the feature coordinates learned from the close neighbors and those representing global status would favor features from the higher-up layers.",4. Jumping Knowledge Networks,[0],[0]
"Max-pooling is adaptive and has the advantage that it does not introduce any additional parameters to learn.
",4. Jumping Knowledge Networks,[0],[0]
LSTM-attention.,4. Jumping Knowledge Networks,[0],[0]
"An attention mechanism identifies the most useful neighborhood ranges for each node v by computing an attention score s(l)v for each layer l (∑ l s (l) v = 1 ) , which represents the importance of the feature learned on the l-th layer for node v. The aggregated representation for node v is a weighted average of the layer features∑ l s (l) v · h(l)v .",4. Jumping Knowledge Networks,[0],[0]
"For LSTM attention, we input h(1)v , ..., h(k)v into a bi-directional LSTM (Hochreiter & Schmidhuber, 1997) and generate the forward-LSTM and backward-LSTM",4. Jumping Knowledge Networks,[0],[0]
hidden features f (l)v and b (l) v for each layer l.,4. Jumping Knowledge Networks,[0],[0]
A linear mapping of the concatenated features [f (l)v ||b(l)v ] yields the scalar importance score s(l)v .,4. Jumping Knowledge Networks,[0],[0]
"A Softmax layer applied to {s(l)v }kl=1
yields the attention of node v on its neighborhood in different ranges.",4. Jumping Knowledge Networks,[0],[0]
Finally we take the sum of [f (l)v ||b(l)v ] weighted by SoftMax({s(l)v }kl=1) to get the final layer representation.,4. Jumping Knowledge Networks,[0],[0]
Another possible implementation combines LSTM with max-pooling.,4. Jumping Knowledge Networks,[0],[0]
LSTM-attention is node adaptive because the attention scores are different for each node.,4. Jumping Knowledge Networks,[0],[0]
"We shall see that the this approach shines on large complex graphs, although it may overfit on small graphs (fewer training nodes) due to its relatively higher complexity.",4. Jumping Knowledge Networks,[0],[0]
"The key idea for the design of layer-aggregation functions is to determine the importance of a node’s subgraph features at different ranges after looking at the learned features on all layers, rather than to optimize and fix the same weights for all nodes.",4.1. JK-Net Learns to Adapt,[0],[0]
"Under the same assumption on the ReLU activation distribution as in Theorem 1, we show below that layer-wise max-pooling implicitly learns the influence locality adaptively for different nodes.",4.1. JK-Net Learns to Adapt,[0],[0]
"The proof for layerwise attention follows similarly.
",4.1. JK-Net Learns to Adapt,[0],[0]
Proposition 1.,4.1. JK-Net Learns to Adapt,[0],[0]
Assume that paths of the same length in the computation graph are activated with the same probability.,4.1. JK-Net Learns to Adapt,[0],[0]
"The influence score I(x, y) for any x, y ∈ V under a k-layer JK-Net with layer-wise max-pooling is equivalent in expectation to a mixture of 0, .., k-step random walk distributions on G̃ at y starting at x, the coefficients of which depend on the values of the layer features h(l)x .
",4.1. JK-Net Learns to Adapt,[0],[0]
We prove Proposition 1 in the appendix.,4.1. JK-Net Learns to Adapt,[0],[0]
"Contrasting this result with the influence distributions of other aggregation mechanisms, we see that JK-networks indeed differ in their node-wise adaptivity of neighborhood ranges.
",4.1. JK-Net Learns to Adapt,[0],[0]
Figure 5 illustrates how a 6-layer JK-Net with max-pooling aggregation learns to adapt to different subgraph structures on a citation network.,4.1. JK-Net Learns to Adapt,[0],[0]
"Within a tree-like structure, the influence stays in the “small community” the node belongs to.",4.1. JK-Net Learns to Adapt,[0],[0]
"In contrast, 6-layer models whose influence distributions follow random walks, e.g. GCNs, would reach out too far into irrelevant parts of the graph, and models with few layers may not be able to cover the entire “community”, as illustrated in Figure 1, and Figures 7, 8 in the appendix.",4.1. JK-Net Learns to Adapt,[0],[0]
"For
a node affiliated to a “hub”, which presumably plays the role of connecting different types of nodes, JK-Net learns to put most influence on the node itself and otherwise spreads out the influence.",4.1. JK-Net Learns to Adapt,[0],[0]
"GCNs, however, would not capture the importance of the node’s own features in such a structure because the probability at an affiliate node is small after a few random walk steps.",4.1. JK-Net Learns to Adapt,[0],[0]
"For hubs, JK-Net spreads out the influence across the neighboring nodes in a reasonable range, which makes sense because the nodes connected to the hubs are presumably as informative as the hubs’ own features.",4.1. JK-Net Learns to Adapt,[0],[0]
"For comparison, Table 6 in the appendix includes more visualizations of how models with random walk priors behave.",4.1. JK-Net Learns to Adapt,[0],[0]
"Looking at Figure 4, one may wonder whether the same inter-layer connections could be drawn between all layers.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"The resulting architecture is approximately a graph correspondent of DenseNets, which were introduced for computer vision problems (Huang et al., 2017), if the layer-wise concatenation aggregation is applied.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"This version, however, would require many more features to learn.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Viewing the DenseNet setting (images) from a graph-theoretic perspective, images correspond to regular, in fact, near-planar graphs.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Such graphs are far from being expanders, and do not pose the challenges of graphs with varying subgraph structures.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Indeed, as we shall see, models with concatenation aggregation perform well on graphs with more regular structures such as images and well-structured communities.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"As a more general framework, JK-Net admits general layerwise aggregation models and enables better structure-aware representations on graphs with complex structures.",4.2. Intermediate Layer Aggregation and Structures,[0],[0]
"Spectral graph convolutional neural networks apply convolution on graphs by using the graph Laplacian eigenvectors as the Fourier atoms (Bruna et al., 2014; Shuman et al., 2013; Defferrard et al., 2016).",5. Other Related Work,[0],[0]
"A major drawback of the spectral methods, compared to spatial approaches like neighborhoodaggregation, is that the graph Laplacian needs to be known in advance.",5. Other Related Work,[0],[0]
"Hence, they cannot generalize to unseen graphs.",5. Other Related Work,[0],[0]
We evaluate JK-Nets on four benchmark datasets.,6. Experiments,[0],[0]
(I),6. Experiments,[0],[0]
"The task on citation networks (Citeseer, Cora) (Sen et al., 2008) is to classify academic papers into different subjects.",6. Experiments,[0],[0]
The dataset contains bag-of-words features for each document (node) and citation links (edges) between documents.,6. Experiments,[0],[0]
"(II) On Reddit (Hamilton et al., 2017), the task is to predict the community to which different Reddit posts belong.",6. Experiments,[0],[0]
Reddit is an online discussion forum where users comment in different topical communities.,6. Experiments,[0],[0]
Two posts (nodes) are connected if some user commented on both posts.,6. Experiments,[0],[0]
The dataset contains word vectors as node features.,6. Experiments,[0],[0]
"(III) For protein-protein interaction networks (PPI) (Hamilton et al., 2017), the task is to classify protein functions.",6. Experiments,[0],[0]
"PPI consists of 24 graphs, each corresponds to a human tissue.",6. Experiments,[0],[0]
"Each node has positional gene sets, motif gene sets and immunological signatures as features and gene ontology sets as labels.",6. Experiments,[0],[0]
"20 graphs are used for training, 2 graphs are used for validation and the rest for testing.",6. Experiments,[0],[0]
"Statistics of the datasets are summarized in Table 1.
",6. Experiments,[0],[0]
Settings.,6. Experiments,[0],[0]
"In the transductive setting, we are only allowed to access a subset of nodes in one graph as training data, and validate/test on others.",6. Experiments,[0],[0]
"Our experiments on Citeseer, Cora and Reddit are transductive.",6. Experiments,[0],[0]
"In the inductive setting, we use a number of full graphs as training data and use other completely unseen graphs as validation/testing data.",6. Experiments,[0],[0]
"Our experiments on PPI are inductive.
",6. Experiments,[0],[0]
"We compare against three baselines: Graph Convolutional Networks (GCN) (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017) and Graph Attention Networks (GAT) (Veličković et al., 2018).",6. Experiments,[0],[0]
"For experiments on Citeseer and Cora, we choose GCN as the base model since on our data split, it is outperforming GAT.",6.1. Citeseer & Cora,[0],[0]
"We construct JK-Nets by choosing MaxPooling (JKMaxPool), Concatenation (JK-Concat), or LSTM-attention (JK-LSTM) as final aggregation layer.",6.1. Citeseer & Cora,[0],[0]
"When taking the final aggregation, besides normal graph convolutional layers, we also take the first linear-transformed representation into account.",6.1. Citeseer & Cora,[0],[0]
The final prediction is done via a fully connected layer on top of the final aggregated representation.,6.1. Citeseer & Cora,[0],[0]
"We split nodes in each graph into 60%, 20% and 20% for training, validation and testing.",6.1. Citeseer & Cora,[0],[0]
"We vary the number of layers from 1
to 6 for each model and choose the best performing model with respect to the validation set.",6.1. Citeseer & Cora,[0],[0]
"Throughout the experiments, we use the Adam optimizer (Kingma & Ba, 2014) with learning rate 0.005.",6.1. Citeseer & Cora,[0],[0]
"We fix the dropout rate to be 0.5, the dimension of hidden features to be within {16, 32}, and add an L2 regularization of 0.0005 on model parameters.",6.1. Citeseer & Cora,[0],[0]
"The results are shown in Table 2.
Results.",6.1. Citeseer & Cora,[0],[0]
We observe in Table 2 that JK-Nets outperform both GCN and GAT baselines in terms of prediction accuracy.,6.1. Citeseer & Cora,[0],[0]
"Though JK-Nets perform well in general, there is no consistent winner and performance varies slightly across datasets.
",6.1. Citeseer & Cora,[0],[0]
"Taking a closer look at results on Cora, both GCN and GAT achieve their best accuracies with only 2 or 3 layers, suggesting that local information is a stronger signal for classification than global ones.",6.1. Citeseer & Cora,[0],[0]
"However, the fact that JKNets achieve the best performance with 6 layers indicates that global together with local information will help boost performance.",6.1. Citeseer & Cora,[0],[0]
This is where models like JK-Nets can be particularly beneficial.,6.1. Citeseer & Cora,[0],[0]
LSTM-attention may not be suitable for such small graphs because of its relatively high complexity.,6.1. Citeseer & Cora,[0],[0]
The Reddit data is too large to be handled well by current implementations of GCN or GAT.,6.2. Reddit,[0],[0]
"Hence, we use the more scalable GraphSAGE as the base model for JK-Net.",6.2. Reddit,[0],[0]
It has skip connections and different modes of node aggregation.,6.2. Reddit,[0],[0]
"We experiment with Mean and MaxPool node aggregators, which take mean and max-pooling of a linear transformation of representations of the sampled neighbors.",6.2. Reddit,[0],[0]
"Combining each of GraphSAGE modes with MaxPooling, Concatenation or LSTM-attention as the last aggregation layer gives 6 JK-Net variants.",6.2. Reddit,[0],[0]
"We follow exactly the same setting of GraphSAGE as in the original paper (Hamilton et al., 2017), where the model consists of 2 hidden layers, each with 128 hidden units and is trained with Adam with learning rate of 0.01 and no weight decay.",6.2. Reddit,[0],[0]
"Results are shown in Table 3.
Results.",6.2. Reddit,[0],[0]
"With MaxPool as node aggregator and Concat as layer aggregator, JK-Net achieves the best Micro-F1 score
among GarphSAGE and JK-Net variants.",6.2. Reddit,[0],[0]
Note that the original GraphSAGE already performs fairly well with a Micro-F1 of 0.95.,6.2. Reddit,[0],[0]
JK-Net reduces the error by 30%.,6.2. Reddit,[0],[0]
"The communities in the Reddit dataset were explicitly chosen from the well-behaved middle-sized communities to avoid the noisy cores and tree-like small communities (Hamilton et al., 2017).",6.2. Reddit,[0],[0]
"As a result, this graph is more regular than the original Reddit data, and hence not exhibit the problems of varying subgraph structures.",6.2. Reddit,[0],[0]
"In such a case, the added flexibility of the node-specific neighborhood choices may not be as relevant, and the stabilizing properties of concatenation instead come into play.",6.2. Reddit,[0],[0]
"We demonstrate the power of adaptive JK-Nets, e.g., JKLSTM, with experiments on the PPI data, where the subgraphs have more diverse and complex structures than those in the Reddit community detection dataset.",6.3. PPI,[0],[0]
We use both GraphSAGE and GAT as base models for JK-Net.,6.3. PPI,[0],[0]
"The implementation of GraphSAGE and GAT are quite different: GraphSAGE is sample-based, where neighbors of a node are sampled to be a fixed number, while GAT considers all neighbors.",6.3. PPI,[0],[0]
Such differences cause large gaps in terms of both scalability and performances.,6.3. PPI,[0],[0]
"Given that GraphSAGE scales to much larger graphs, it appears particularly valuable to evaluate how much JK-Net can improve upon GraphSAGE.
",6.3. PPI,[0],[0]
"For GraphSAGE we follow the setup as in the Reddit experiments, except that we use 3 layers when possible, and compare the performance after 10 and 30 epochs of training.",6.3. PPI,[0],[0]
The results are shown in Table 4.,6.3. PPI,[0],[0]
"For GAT and its JK-Net variants we stack two hidden layers with 4 attention heads computing 256 features (for a total of 1024 features), and a final prediction layer with 6 attention heads computing 121 features each.",6.3. PPI,[0],[0]
They are further averaged and input into sigmoid activations.,6.3. PPI,[0],[0]
We employ skip connections across intermediate attentional layers.,6.3. PPI,[0],[0]
These models are trained with Batch-size 2 and Adam optimizer with learning rate of 0.005.,6.3. PPI,[0],[0]
"The results are shown in Table 5.
Results.",6.3. PPI,[0],[0]
"JK-Nets with the LSTM-attention aggregators outperform the non-adaptive models GraphSAGE, GAT and JK-Nets with concatenation aggregators.",6.3. PPI,[0],[0]
"In particular, JKLSTM outperforms GraphSAGE by 0.128 in terms of micro-
F1 score after 30 epochs of training.",6.3. PPI,[0],[0]
Structure-aware node adaptive models are especially beneficial on such complex graphs with diverse structures.,6.3. PPI,[0],[0]
"Motivated by observations that reveal great differences in neighborhood information ranges for graph node embeddings, we propose a new aggregation scheme for node representation learning that can adapt neigborhood ranges to nodes individually.",7. Conclusion,[0],[0]
"This JK-network can improve representations in particular for graphs that have subgraphs of diverse local structure, and may hence not be well captured by fixed numbers of neighborhood aggregations.",7. Conclusion,[0],[0]
Interesting directions for future work include exploring other layer aggregators and studying the effect of the combination of various layer-wise and node-wise aggregators on different types of graph structures.,7. Conclusion,[0],[0]
"This research was supported by NSF CAREER award 1553284, and JST ERATO Kawarabayashi Large Graph Project, Grant Number JPMJER1201, Japan.",Acknowledgements,[0],[0]
Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure.,abstractText,[0],[0]
"We analyze some important properties of these models, and propose a strategy to overcome those.",abstractText,[0],[0]
"In particular, the range of “neighboring” nodes that a node’s representation draws from strongly depends on the graph structure, analogous to the spread of a random walk.",abstractText,[0],[0]
"To adapt to local neighborhood properties and tasks, we explore an architecture – jumping knowledge (JK) networks – that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation.",abstractText,[0],[0]
"In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance.",abstractText,[0],[0]
"Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models’ performance.",abstractText,[0],[0]
Representation Learning on Graphs with Jumping Knowledge Networks ,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 912–921, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.",text,[0.9667389801102214],"['Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks.']"
"Recent advances in deep neural networks (DNNs) have demonstrated the importance of learning vector-space representations of text, e.g., words and sentences, for a number of natural language processing tasks.",1 Introduction,[0],[0]
"For example, the study reported in (Collobert et al., 2011) demonstrated significant accuracy gains in tagging, named entity recognition, and semantic role labeling when using vector space word
∗This research was conducted during the author’s internship at Microsoft Research.
representations learned from large corpora.",1 Introduction,[0],[0]
"Further, since these representations are usually in a lowdimensional vector space, they result in more compact models than those built from surface-form features.",1 Introduction,[0],[0]
"A recent successful example is the parser by (Chen and Manning, 2014), which is not only accurate but also fast.
",1 Introduction,[0],[0]
"However, existing vector-space representation learning methods are far from optimal.",1 Introduction,[0],[0]
"Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014).",1 Introduction,[0],[0]
"Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data.",1 Introduction,[0],[0]
"Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks.",1 Introduction,[0],[0]
"In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks.
",1 Introduction,[0],[0]
"Our contributions are of two-folds: First, we propose a multi-task deep neural network for representation learning, in particular focusing on semantic classification (query classification) and semantic information retrieval (ranking for web search) tasks.",1 Introduction,[1.0],"['Our contributions are of two-folds: First, we propose a multi-task deep neural network for representation learning, in particular focusing on semantic classification (query classification) and semantic information retrieval (ranking for web search) tasks.']"
Our model learns to map arbitrary text queries and documents into semantic vector representations in a low dimensional latent space.,1 Introduction,[0],[0]
"While the general concept of multi-task neural nets is not new, our model is novel in that it successfully combines tasks as disparate as operations necessary for classifica-
912
tion or ranking.",1 Introduction,[0],[0]
"Second, we demonstrate strong results on query classification and web search.",1 Introduction,[0],[0]
Our multi-task representation learning consistently outperforms stateof-the-art baselines.,1 Introduction,[0],[0]
"Meanwhile, we show that our model is not only compact but it also enables agile deployment into new domains.",1 Introduction,[0],[0]
This is because the learned representations allow domain adaptation with substantially fewer in-domain labels.,1 Introduction,[0],[0]
Our multi-task model combines classification and ranking tasks.,2.1 Preliminaries,[1.0],['Our multi-task model combines classification and ranking tasks.']
"For concreteness, throughout this paper we will use query classification as the classification task and web search as the ranking task.",2.1 Preliminaries,[0],[0]
"These are important tasks in commercial search engines:
Query Classification:",2.1 Preliminaries,[0],[0]
"Given a search query Q, the model classifies in the binary fashion as to whether it belongs to one of the domains of interest.",2.1 Preliminaries,[0],[0]
"For example, if the query Q is “Denver sushi”, the classifier should decide that it belongs to the “Restaurant” domain.",2.1 Preliminaries,[0],[0]
"Accurate query classification enables a richer personalized user experience, since the search engine can tailor the interface and results.",2.1 Preliminaries,[0],[0]
"It is however challenging because queries tend to be short (Shen et al., 2006).",2.1 Preliminaries,[0],[0]
"Surface-form word features that are common in traditional document classification problems tend to be too sparse for query classification, so representation learning is a promising solution.",2.1 Preliminaries,[1.0],"['Surface-form word features that are common in traditional document classification problems tend to be too sparse for query classification, so representation learning is a promising solution.']"
"In this study, we classify queries into four domains of interest: (“Restaurant”, “Hotel”, “Flight”, “Nightlife”).",2.1 Preliminaries,[0],[0]
Note that one query can belong to multiple domains.,2.1 Preliminaries,[1.0],['Note that one query can belong to multiple domains.']
"Therefore, a set of binary classifiers are built, one for each domain, to perform the classification.",2.1 Preliminaries,[0],[0]
We frame the problem as four binary classification tasks.,2.1 Preliminaries,[0],[0]
"Thus, for domain Ct, our goal is binary classification based on P (Ct| Q) (Ct = {0, 1} ).",2.1 Preliminaries,[0],[0]
"For each domain t, we assume supervised data (Q, yt = {0, 1} with yt as binary labels.1
Web Search:",2.1 Preliminaries,[0],[0]
"Given a search queryQ and a document list L, the model ranks documents in the order
1One could frame the problem as a a single multi-class classification task, but our formulation is more practical as it allows adding new domains without retraining existing classifiers.",2.1 Preliminaries,[0],[0]
"This will be relevant in domain adaptation (§3.3).
of relevance.",2.1 Preliminaries,[0],[0]
"For example, if the queryQ is ”Denver sushi”, model returns a list of documents that satisfies such information need.",2.1 Preliminaries,[0],[0]
"Formally, we estimate P (D1|Q), P (D2|Q), . . .",2.1 Preliminaries,[0],[0]
for each document Dn and rank according to these probabilities.,2.1 Preliminaries,[0],[0]
"We assume that supervised data exist; I.e., there is at least one relevant document Dn for each query Q.",2.1 Preliminaries,[0],[0]
"Briefly, our proposed model maps any arbitrary queries Q or documents D into fixed lowdimensional vector representations using DNNs.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
These vectors can then be used to perform query classification or web search.,2.2 The Proposed Multi-Task DNN Model,[1.0],['These vectors can then be used to perform query classification or web search.']
"In contrast to existing representation learning methods which employ either unsupervised or single-task supervised objectives, our model learns these representations using multi-task objectives.
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
The architecture of our multi-task DNN model is shown in Figure 1.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"The lower layers are shared across different tasks, whereas the top layers represent task-specific outputs.",2.2 The Proposed Multi-Task DNN Model,[1.0],"['The lower layers are shared across different tasks, whereas the top layers represent task-specific outputs.']"
"Importantly, the input X (either a query or document), initially represented as a bag of words, is mapped to a vector (l2) of dimension 300.",2.2 The Proposed Multi-Task DNN Model,[1.0],"['Importantly, the input X (either a query or document), initially represented as a bag of words, is mapped to a vector (l2) of dimension 300.']"
This is the shared semantic representation that is trained by our multi-task objectives.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"In the following, we elaborate the model in detail:
Word Hash Layer (l1): Traditionally, each word is represented by a one-hot word vector, where the dimensionality of the vector is the vocabulary size.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"However, due to the large size of vocabulary in realworld tasks, it is very expensive to learn such kind of models.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"To alleviate this problem, we adopt the word hashing method (Huang et al., 2013).",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"We map a one-hot word vector, with an extremely high dimensionality, into a limited letter-trigram space (e.g., with the dimensionality as low as 50k).",2.2 The Proposed Multi-Task DNN Model,[1.0],"['We map a one-hot word vector, with an extremely high dimensionality, into a limited letter-trigram space (e.g., with the dimensionality as low as 50k).']"
"For example, word cat is hashed as the bag of letter trigram {#-c-a, c-a-t, a-t-#}, where # is a boundary symbol.",2.2 The Proposed Multi-Task DNN Model,[1.0],"['For example, word cat is hashed as the bag of letter trigram {#-c-a, c-a-t, a-t-#}, where # is a boundary symbol.']"
"Word hashing complements the one-hot vector representation in two aspects: 1) out of vocabulary words can be represented by letter-trigram vectors; 2) spelling variations of the same word can be mapped to the points that are close to each other in the letter-trigram space.
",2.2 The Proposed Multi-Task DNN Model,[0.9999999289364018],['Word hashing complements the one-hot vector representation in two aspects: 1) out of vocabulary words can be represented by letter-trigram vectors; 2) spelling variations of the same word can be mapped to the points that are close to each other in the letter-trigram space.']
Semantic-Representation Layer (l2):,2.2 The Proposed Multi-Task DNN Model,[0],[0]
This is a shared representation learned across different tasks.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"this layer maps the letter-trigram inputs into a 300-
1
dimensional vector by
l2 = f(W1 · l1) (1)
where f(·) is the tanh nonlinear activation f(z) = 1−e−2z 1+e−2z .",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"This 50k-by-300 matrix W1 is responsible for generating the cross-task semantic representation for arbitrary text inputs (e.g., Q or D).
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Task-Specific Representation (l3): For each task, a nonlinear transformation maps the 300- dimension semantic representation l2 into the 128- dimension task-specific representation by
l3 = f(Wt2 · l2) (2)
where, t denotes different tasks (query classification or web search).
",2.2 The Proposed Multi-Task DNN Model,[0.9531788490961434],"['The probability that Q belongs to class C1 is predicted by a logistic regression, with sigmoid g(z) = 1 1+e−z : P (C1|Q) = g(Wt=C13 ·QC1) (3) Web Search Output: For the web search task, both the query Q and the document D are mapped into 128-dimension task-specific representations QSq and DSd .']"
Query Classification Output: Suppose QC1 ≡ l3 = f(Wt=C12 · l2) is the 128-dimension taskspecific representation for a query Q.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"The probability that Q belongs to class C1 is predicted by a logistic regression, with sigmoid g(z)",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"= 1
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"1+e−z :
",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"P (C1|Q) = g(Wt=C13 ·QC1) (3)
Web Search Output: For the web search task, both the query Q and the document D are mapped into 128-dimension task-specific representations QSq and DSd .",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Then, the relevance score is
Algorithm 1: Training a Multi-task DNN Initialize model Θ : {W1,Wt2,Wt3} randomly for iteration in 0...∞ do
1.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Pick a task t randomly 2.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Pick sample(s) from task t
(Q, yt = {0, 1}) for query classification (Q,L) for web search
3.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
Compute loss: L(Θ) L(Θ)=Eq.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
5 for query classification L(Θ)=Eq.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
6 for web search 4.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
Compute gradient: ∇(Θ) 5.,2.2 The Proposed Multi-Task DNN Model,[0],[0]
"Update model: Θ = Θ− ∇(Θ)
end The task t is one of the query classification tasks or web search task, as shown in Figure 1.",2.2 The Proposed Multi-Task DNN Model,[0],[0]
"For query classification, each training sample includes one query and its category label.",2.2 The Proposed Multi-Task DNN Model,[1.0],"['For query classification, each training sample includes one query and its category label.']"
"For web search, each training sample includes query and document list.
computed by cosine similarity as:
R(Q,D) = cos(QSq , DSd)",2.2 The Proposed Multi-Task DNN Model,[0],[0]
= QSq ·DSd ||QSq,2.2 The Proposed Multi-Task DNN Model,[0],[0]
||||DSd || (4),2.2 The Proposed Multi-Task DNN Model,[0],[0]
"In order to learn the parameters of our model, we use mini-batch-based stochastic gradient descent (SGD) as shown in Algorithm 1.",2.3 The Training Procedure,[0],[0]
"In each iteration, a task t is selected randomly, and the model is updated ac-
cording to the task-specific objective.",2.3 The Training Procedure,[0],[0]
This approximately optimizes the sum of all multi-task objectives.,2.3 The Training Procedure,[1.0],['This approximately optimizes the sum of all multi-task objectives.']
"For query classification of class Ct, we use the cross-entropy loss as the objective: −{yt lnP (Ct|Q)+(1−yt) ln(1−P (Ct|Q))}",2.3 The Training Procedure,[0],[0]
"(5)
where yt = {0, 1} is the label and the loss is summed over all samples in the mini-batch (1024 samples in experiments).
",2.3 The Training Procedure,[0],[0]
"The objective for web search used in this paper follows the pair-wise learning-to-rank paradigm outlined in (Burges et al., 2005).",2.3 The Training Procedure,[0],[0]
"Given a query Q, we obtain a list of documents L that includes a clicked document D+ (positive sample), and J randomlysampled non-clicked documents {D−j }j=1,.,J .",2.3 The Training Procedure,[1.0],"['Given a query Q, we obtain a list of documents L that includes a clicked document D+ (positive sample), and J randomlysampled non-clicked documents {D−j }j=1,.,J .']"
"We then minimize the negative log likelihood of the clicked document (defined in Eq. 7) given queries across the training data
− log ∏
(Q,D+)
P (D+|Q) (6)
where the probability of a given document D+ is computed
P (D+|Q) =",2.3 The Training Procedure,[0],[0]
"exp(γR(Q,D +))",2.3 The Training Procedure,[0],[0]
"∑
D′∈L exp(γR(Q,D′)) (7)
",2.3 The Training Procedure,[0],[0]
"here, γ is a tuning factor determined on held-out data.",2.3 The Training Procedure,[0],[0]
"Additional training details: (1) Model parameters are initialized with uniform distribution in the range (−√6/(fanin + fanout),√6/(fanin + fanout))",2.3 The Training Procedure,[0],[0]
"(Montavon et al., 2012).",2.3 The Training Procedure,[0],[0]
"Empirically, we have not observed better performance by initialization with layer-wise pre-training.",2.3 The Training Procedure,[0],[0]
"(2) Moment methods and AdaGrad training (Duchi et al., 2011) speed up the convergence speed but gave similar results as plain SGD.",2.3 The Training Procedure,[0],[0]
The SGD learning rate is fixed at = 0.1/1024.,2.3 The Training Procedure,[0],[0]
"(3) We run Algorithm 1 for 800K iterations, taking 13 hours on an NVidia K20 GPU.",2.3 The Training Procedure,[0],[0]
"Our proposed multi-task DNN (Figure 1) can be viewed as a combination of a standard DNN for classification and a Deep Structured Semantic Model (DSSM) for ranking, shown in Figure 2.",2.4 An Alternative View of the Multi-Task Model,[0],[0]
Other ways to merge the models are possible.,2.4 An Alternative View of the Multi-Task Model,[0],[0]
"Figure 3 shows an alternative multi-task architecture, where only the query part is shared among all tasks and the DSSM
retains independent parameters for computing the document representations.",2.4 An Alternative View of the Multi-Task Model,[0],[0]
This is more similar to the original DSSM.,2.4 An Alternative View of the Multi-Task Model,[0],[0]
"We have attempted training this model using Algorithm 1, but it achieves good results on query classification at the expense of web search.",2.4 An Alternative View of the Multi-Task Model,[0],[0]
"This is likely due to unbalanced updates (i.e. parameters for queries are updated more often than that of documents), and implying that the amount of sharing is an important design choice in multi-task models.
",2.4 An Alternative View of the Multi-Task Model,[0],[0]
3,2.4 An Alternative View of the Multi-Task Model,[0],[0]
"We employ large-scale, real data sets in our evaluation.",3.1 Data Sets and Evaluation Metrics,[0],[0]
See Table 1 for statistics.,3.1 Data Sets and Evaluation Metrics,[0],[0]
The test data for query classification were sampled from one-year log files of a commercial search engine with labels (yes or no) judged by humans.,3.1 Data Sets and Evaluation Metrics,[0],[0]
"The test data for web search contains 12,071 English queries, where each query-document pair has a relevance label manually annotated on a 5-level relevance scale: bad, fair,
good, excellent and perfect.",3.1 Data Sets and Evaluation Metrics,[0],[0]
"The evaluation metric for query classification is the Area under of Receiver Operating Characteristic (ROC) curve (AUC) score (Bradley, 1997).",3.1 Data Sets and Evaluation Metrics,[1.0],"['The evaluation metric for query classification is the Area under of Receiver Operating Characteristic (ROC) curve (AUC) score (Bradley, 1997).']"
"For web search, we employ the Normalized Discounted Cumulative Gain (NDCG) (Järvelin and Kekäläinen, 2000).",3.1 Data Sets and Evaluation Metrics,[0],[0]
"First, we evaluate whether our model can robustly improve performance, measured as accuracy across multiple tasks.
",3.2 Results on Accuracy,[0],[0]
"Table 2 summarizes the AUC scores for query classification, comparing the following classifiers: • SVM-Word: a SVM model2 with unigram, bi-
gram and trigram surface-form word features.
",3.2 Results on Accuracy,[0],[0]
• SVM-Letter: a SVM model with letter trigram features (i.e. l1 in Figure 1 as input to SVM).,3.2 Results on Accuracy,[1.0],['• SVM-Letter: a SVM model with letter trigram features (i.e. l1 in Figure 1 as input to SVM).']
• DNN:,3.2 Results on Accuracy,[0],[0]
single-task deep neural net (Figure 2).,3.2 Results on Accuracy,[0],[0]
• MT-DNN: our multi-task proposal (Figure 1).,3.2 Results on Accuracy,[0],[0]
The results show that the proposed MT-DNN performs best in all four domains.,3.2 Results on Accuracy,[0],[0]
"Further, we observe:
1.",3.2 Results on Accuracy,[0],[0]
"MT-DNN outperforms DNN, indicating the usefulness of the multi-task objective (that includes web search) over the single-task objective of query classification.
2.",3.2 Results on Accuracy,[0],[0]
"Both DNN and MT-DNN outperform SVMLetter, which initially uses the same input features (l1).",3.2 Results on Accuracy,[0],[0]
"This indicates the importance of learning a semantic representation l2 on top of these letter trigrams.
3.",3.2 Results on Accuracy,[0],[0]
"Both DNN and MT-DNN outperform a strong SVM-Word baseline, which has a large feature set that consists of 3 billion features.
",3.2 Results on Accuracy,[0],[0]
"Table 3 summarizes the NDCG results on web search, comparing the following models:",3.2 Results on Accuracy,[0],[0]
"2In this paper, we use the liblinear to build SVM classifiers and optimize the corresponding parameter C by using 5-fold cross-validation in training data.",3.2 Results on Accuracy,[0],[0]
"http://www.csie.ntu.edu.tw/ cjlin/liblinear/
• Popular baselines in the web search literature, e.g. BM25, Language Model, PLSA
• DSSM: single-task ranking model (Figure 2) • MT-DNN:",3.2 Results on Accuracy,[0.9715240432406947],"['http://www.csie.ntu.edu.tw/ cjlin/liblinear/ • Popular baselines in the web search literature, e.g. BM25, Language Model, PLSA • DSSM: single-task ranking model (Figure 2) • MT-DNN: our multi-task proposal (Figure 1) Again, we observe that MT-DNN performs best.']"
"our multi-task proposal (Figure 1)
",3.2 Results on Accuracy,[0],[0]
"Again, we observe that MT-DNN performs best.",3.2 Results on Accuracy,[0],[0]
"For example, MT-DNN achieves NDCG@1=0.334, outperforming the current state-of-the-art single-task DSSM (0.327) and the classic methods like PLSA (0.308) and BM25 (0.305).",3.2 Results on Accuracy,[0],[0]
"This is a statistically significant improvement (p < 0.05) over DSSM and other baselines.
",3.2 Results on Accuracy,[0],[0]
"To recap, our MT-DNN robustly outperforms strong baselines across all web search and query classification tasks.",3.2 Results on Accuracy,[0],[0]
"Further, due to the use of larger training data (from different domains) and the regularization effort as we discussed in Section 1, we confirm the advantage of multi-task models over than single-task ones.3",3.2 Results on Accuracy,[0],[0]
Important criteria for building practical systems are agility of deployment and small memory footprint and fast run-time.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Our model satisfies both with 3We have also trained SVM using Word2Vec (Mikolov et al., 2013b; Mikolov et al., 2013a) features.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Unfortunately, the results are poor at 60-70 AUC, indicating the sub-optimality of unsupervised representation learning objectives for actual prediction tasks.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"We optimized the Word2Vec features in the SVM baseline by scaling and normalizing as well, but did not observe much improvement.
high model compactness.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
The key to the compactness is the aggressive compression from the 500kdimensional bag-of-words input to 300-dimensional semantic representation l2.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
This significantly reduces the memory/run-time requirements compared to systems that rely on surface-form features.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"The most expensive portion of the model is storage of the 50k-by-300 W1 and its matrix multiplication with l1, which is sparse: this is trivial on modern hardware.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Our multi-task DNN takes < 150KB in memory whereas e.g. SVM-Word takes about 200MB.
",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Compactness is particularly important for query classification, since one may desire to add new domains after discovering new needs from the query logs of an operational system.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"On the other hand, it is prohibitively expensive to collect labeled training data for new domains.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Very often, we only have very small training data or even no training data.
",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"To evaluate the models using the above criteria, we perform domain adaptation experiments on query classification using the following procedure: (1) Select one query classification task t∗.",3.3 Results on Model Compactness and Domain Adaptation,[1.0],"['To evaluate the models using the above criteria, we perform domain adaptation experiments on query classification using the following procedure: (1) Select one query classification task t∗.']"
"Train MTDNN on the remaining tasks (including Web Search
task) to obtain a semantic representation (l2); (2) Given a fixed l2, train an SVM on the training data t∗, using varying amounts of labels; (3) Evaluate the AUC on the test data of t∗
We compare three SVM classifiers trained using different feature representations: (1) SemanticRepresentation uses the l2 features generated according to the above procedure.",3.3 Results on Model Compactness and Domain Adaptation,[1.000000054821894],"['Train MTDNN on the remaining tasks (including Web Search task) to obtain a semantic representation (l2); (2) Given a fixed l2, train an SVM on the training data t∗, using varying amounts of labels; (3) Evaluate the AUC on the test data of t∗ We compare three SVM classifiers trained using different feature representations: (1) SemanticRepresentation uses the l2 features generated according to the above procedure.']"
"(2) Word3gram uses unigram, bigram and trigram word features.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
(3) Letter3gram uses letter-trigrams.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Note that Word3gram and Letter3gram correspond to SVMWord and SVM-Letter respectively in Table 2.
",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
The AUC results for different amounts of t∗ training data are shown in Figure 4.,3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"In the Hotel, Flight and Restaurant domains, we observe that our semantic representation dominated the other two feature representations (Word3gram and Letter3gram) in all cases except the extremely large-data regime (more than 1 million training samples in domain t∗).",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Given sufficient labels, SVM is able to train well on Word3gram sparse features, but for most cases Se-
manticRepresentation is recommended.4
In a further experiment, we compare the following two DNNs using the same domain adaptation procedure: (1) DNN1: DNN where W1 is randomly initialized and parameters W1,W2,Wt ∗ 3 are trained on varying amounts of data in t∗;",3.3 Results on Model Compactness and Domain Adaptation,[0.9804077035804493],"['Given sufficient labels, SVM is able to train well on Word3gram sparse features, but for most cases Se- manticRepresentation is recommended.4 In a further experiment, we compare the following two DNNs using the same domain adaptation procedure: (1) DNN1: DNN where W1 is randomly initialized and parameters W1,W2,Wt ∗ 3 are trained on varying amounts of data in t∗; (2) DNN2: DNN where W1 is obtained from other tasks (i.e. SemanticRepresentation) and fixed, while parameters W2,Wt ∗ 3 are trained on varying amounts of data in t∗.']"
"(2) DNN2: DNN where W1 is obtained from other tasks (i.e. SemanticRepresentation) and fixed, while parameters W2,Wt ∗ 3 are trained on varying amounts of data in t∗.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
The purpose is to see whether shared semantic representation is useful even under a DNN architecture.,3.3 Results on Model Compactness and Domain Adaptation,[1.0],['The purpose is to see whether shared semantic representation is useful even under a DNN architecture.']
"Figure 5 show the AUC results of DNN1 vs. DNN2 (the results SVM denotes the same system as SemanticRepresentation in Figure 4, plotted here for reference).",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"We observe that when the training data is extremely large (millions of samples), one does best by training all parameters from scratch (DNN1).",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"Otherwise, one is better off using a shared semantic representation trained by multitask objectives.",3.3 Results on Model Compactness and Domain Adaptation,[1.0],"['Otherwise, one is better off using a shared semantic representation trained by multitask objectives.']"
"Comparing DNN2 and SVM with SemanticRepresentation, we note that SVM works best for training data of several thousand samples; DNN2 works best in the medium data range.",3.3 Results on Model Compactness and Domain Adaptation,[0],[0]
"There is a large body of work on representation learning for natural language processing, sometimes using different terminologies for similar concepts; e.g., feature generation, dimensionality reduction, and vector space models.",4 Related Work,[0],[0]
"The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics.",4 Related Work,[0],[0]
"Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Schölkopf et al., 1998).",4 Related Work,[0],[0]
"Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014).
",4 Related Work,[0],[0]
"Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec-
4The trends differ slightly in the Nightlife domain.",4 Related Work,[0],[0]
"We believe this may be due to data bias on test data (only 298 samples).
tives of predicting words or word frequencies from raw text.",4 Related Work,[0],[0]
"End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy).",4 Related Work,[0],[0]
"A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a).
",4 Related Work,[0],[0]
"Our model takes queries and documents as input, so it learns sentence/document representations.",4 Related Work,[0],[0]
"This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013).",4 Related Work,[0],[0]
"While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 2014b), parse tree structure (Irsoy and Cardie, 2014), and run-time inference (Le and Mikolov, 2014).
",4 Related Work,[0],[0]
"The synergy between multi-task learning and neural nets is quite natural; the general idea dates back to (Caruana, 1997).",4 Related Work,[0],[0]
The main challenge is in designing the tasks and the network structure.,4 Related Work,[0],[0]
"For example, (Collobert et al., 2011) defined part-of-speech tagging, chunking, and named entity recognition as multiple tasks in a single sequence labeler; (Bordes et al., 2012) defined multiple data sources as tasks in their relation extraction system.",4 Related Work,[0],[0]
"While conceptually similar, our model is novel in that it combines tasks as disparate as classification and ranking.",4 Related Work,[0],[0]
"Further, considering that multi-task models often exhibit mixed results (i.e. gains in some tasks but degradation in others), our accuracy improvements across all tasks is a very satisfactory result.",4 Related Work,[0],[0]
"In this work, we propose a robust and practical representation learning algorithm based on multi-task objectives.",5 Conclusion,[0],[0]
"Our multi-task DNN model successfully combines tasks as disparate as classification and ranking, and the experimental results demon-
strate that the model consistently outperforms strong baselines in various query classification and web search tasks.",5 Conclusion,[0],[0]
"Meanwhile, we demonstrated compactness of the model and the utility of the learned query/document representation for domain adaptation.
",5 Conclusion,[0],[0]
Our model can be viewed as a general method for learning semantic representations beyond the word level.,5 Conclusion,[0],[0]
"Beyond query classification and web search, we believe there are many other knowledge sources (e.g. sentiment, paraphrase) that can be incorporated either as classification or ranking tasks.",5 Conclusion,[1.0],"['Beyond query classification and web search, we believe there are many other knowledge sources (e.g. sentiment, paraphrase) that can be incorporated either as classification or ranking tasks.']"
A comprehensive exploration will be pursued as future work.,5 Conclusion,[1.0],['A comprehensive exploration will be pursued as future work.']
"We thank Xiaolong Li, Yelong Shen, Xinying Song, Jianshu Chen, Byungki Byun, Bin Cao and the anonymous reviewers for valuable discussions and comments.",Acknowledgments,[0],[0]
Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks.,abstractText,[0],[0]
"However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data.",abstractText,[0],[0]
"We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains.",abstractText,[0],[0]
"Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation.",abstractText,[0],[0]
Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval,title,[0],[0]
