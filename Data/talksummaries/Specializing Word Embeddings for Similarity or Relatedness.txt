1	29	Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al., 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al., 2013b), sentiment analysis (Socher et al., 2013) and named entity recognition (Turian et al., 2010; Guo et al., 2014).
2	64	Arguably, one of the reasons behind the popularity of word embeddings is that they are “general purpose”: they can be used in a variety of tasks without modification.
4	32	For example, when classifying documents by topic, we are particularly interested in related words rather than similar ones: knowing that dog is associated with cat is much more informative of the topic than knowing that it is a synonym of canine.
5	23	Conversely, if our embeddings indicate that table is closely related to chair, that does not mean we should translate table into French as chaise.
6	23	This distinction between “genuine” similarity and associative similarity (i.e., relatedness) is well-known in cognitive science (Tversky, 1977).
7	21	In NLP, however, semantic spaces are generally evaluated on how well they capture both similarity and relatedness, even though, for many word combinations (such as car and petrol), these two objectives are mutually incompatible (Hill et al., 2014b).
8	18	In part, this oversight stems from the distributional hypothesis itself: car and petrol do not have the same, or even very similar, meanings, but these two words may well occur in similar contexts.
9	22	Corpus-driven approaches based on the distributional hypothesis therefore generally learn embeddings that capture both similarity and relatedness reasonably well, but neither perfectly.
10	15	In this work we demonstrate the advantage of specializing semantic spaces for either similarity or relatedness.
15	11	The underlying assumption of our approach is that, during training, word embeddings can be “nudged” in a particular direction by including information from an additional semantic data 2044 source.
16	20	For directing embeddings towards genuine similarity, we use the MyThes thesaurus developed by the OpenOffice.org project1.
18	17	For directing embeddings towards relatedness, we use the University of South Florida (USF) free association norms (Nelson et al., 2004).
19	27	This dataset contains scores for free association (an experimental measure of cognitive association) of over 10,000 concept words.
21	19	For instrinsic comparisons with human judgements, we evaluate on SimLex (Hill et al., 2014b) (999 pairwise comparisons), which explicitly measures similarity, and MEN (Bruni et al., 2014) (3000 comparisons), which explicitly measures relatedness.
24	22	For a more extrinsic evaluation, we use a document classification task based on the Reuters Corpus Volume 1 (RCV1) (Lewis et al., 2004).
26	35	The standard skip-gram training objective for a sequence of training wordsw1, w2, ..., wT and a context size c is the log-likelihood criterion: 1 T T∑ t=1 Jθ(wt) = 1 T T∑ t=1 ∑ −c≤j≤c log p(wt+j |wt) where p(wt+j |wt) is obtained via the softmax: p(wt+j |wt) = exp u>wt+j vwt∑ w′ exp u> w′vwt where uw and vw are the context and target vector representations for word w, respectively, and w′ ranges over the full vocabulary (Mikolov et al., 2013a).
27	9	For our joint learning approach, we supplement the skip-gram objective with additional contexts (synonyms or free-associates) from an external data source.
28	17	In the sampling condition, for target word wt, we modify the objective to include an additional context wa sampled uniformly from the set of additional contexts Awt : 1 T T∑ t=1 ( Jθ(wt) + [wa ∼ UAwt ] log p(wa|wt) ) In the all condition, all additional contexts for a target word are added at each occurrence: 1 T T∑ t=1 Jθ(wt) + ∑ wa∈Awt log p(wa|wt)  The set of additional contexts Awt contains the relevant contexts for a word wt; e.g., for the word dog, Adog for the thesaurus is the set of all synonyms of dog in the thesaurus.
32	51	In other words, we first train a standard skip-gram model, and then learn from the additional contexts in a second training stage as if they form a separate corpus: 1 T T∑ t=1 ∑ wa∈Awt log p(wa|wt) We call this approach skip-gram retrofitting.
36	45	As shown in Table 1, embeddings that were specialized for similarity using a thesaurus perform better on SimLex-999, and those specialized for relatedness using association data perform better on MEN.
37	35	Fitting, or learning only from the additional semantic resource without access to raw text, does not perform well.
39	25	There is an interesting difference between the two joint learning approaches: while sampling a single free associate as additional context works best for relatedness, presenting all additional contexts (synonyms) works best for similarity.
51	27	The fact that joint learning works better when supplementing raw text input with free associates, but skip-gram retrofitting works better with additional thesaurus information, could be due to curriculum learning effects (Bengio et al., 2009).
52	23	Unlike the USF norms, many of the words from the thesaurus are unusual and have low frequency.
55	24	However, with retrofitting the model first acquires good representations for frequent words from the raw text, after which it can better understand, and learn from, the information in the thesaurus.
59	10	As Table 2 shows, similarity-specialized embeddings perform much better than standard embeddings and relatedness-specialized embeddings.
69	26	In doing so, we compared two retrofitting methods and a joint learning approach.
70	48	Specialized embeddings outperform standard embeddings by a large margin on instrinsic similarity and relatedness evaluations.
71	115	We showed that the difference in how embeddings are specialized carries to downstream NLP tasks, demonstrating that similarity embeddings are better at the TOEFL synonym selection task and relatedness embeddings at a document topic classification task.
72	144	Lastly, we varied the number of iterations that we use for retrofitting, showing that performance could be improved even further by going over several iterations of the semantic resource.
