14	61	Second, dividing a corpus into separate time bins may lead to training sets that are too small to train a word embedding model.
15	33	Hence, one runs the risk of overfitting to few data whenever the required temporal resolution is fine-grained, as we show in the experimental section.
30	15	We show the ten words whose meaning changed most drastically in terms of cosine distance over the last 150 years.
31	11	We thereby automatically discover words such as “computer” or “radio” whose meaning changed due to technological advances, but also words like “peer” and “notably” whose semantic shift is less obvious.
59	15	Their approach uses a non-Bayesian treatment of the latent embedding trajectories, which makes the approach less robust to noise when the data per time step is small.
60	19	trained end-to-end and scales to massive data by means of approximate Bayesian inference.
67	12	For each pair of words i, j in the vocabulary, the model assigns probabilities that word i appears in the context of word j.
82	41	Defining n± = (n+, n−) the combination of both positive and negative examples, the resulting log likelihood is log p(n±|U, V ) = L∑ i,j=1 ( n+ij log σ(u > i vj) + n − ij log σ(−u > i vj) ) .
85	18	Barkan (2017) gives an approximate Bayesian treatment of the model with Gaussian priors on the embeddings.
89	16	, T} the sufficient statistics of word-context pairs are encoded in the L×L matrices n+t , n−t of positive and negative counts with matrix elements n+ij,t and n − ij,t, respectively.
92	17	The graphical model is shown in Figure 2b).
95	18	At every time step t, we add an additional Gaussian prior with zero mean and variance σ20 which prevents the embedding vectors from growing very large, thus p(Ut+1|Ut) ∝ N (Ut, σ2t )N (0, σ20).
98	31	This is also called Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930).
118	12	We thereby use a variational distribution that factorizes across all times, q(U, V ) =∏T t=1 q(Ut, Vt) and we update the variational factor at a given time t based on the evidence at time t and the approximate posterior of the previous time step.
132	17	Thus, the second contribution in Eq.
136	19	This approach results in smoother trajectories and typically higher likelihoods than with filtering, because evidence is used from both future and past observations.
137	12	Besides the new inference scheme, we also use a different variational distribution.
138	25	As the model is fitted jointly to all time steps, we are no longer restricted to a variational distribution that factorizes in time.
139	20	For simplicity we focus here on the variational distribution for the word embeddings U ; the context embeddings V are treated identically.
143	14	This factor is a multivariate Gaussian distribution in the time domain with tridiagonal precision matrix Λ, q(u1:T ) = N (µ,Λ−1) (15) Both the means µ = µ1:T and the entries of the tridiagonal precision matrix Λ ∈ RT×T are variational parameters.
147	38	We then switch to the full batch to reduce the sampling noise.
149	20	We also derive an efficient algorithm that allows us to estimate the reparametrization gradient using Θ(T ) time and memory, while a naive implementation of black-box variational inference with our structured variational distribution would require Θ(T 2) of both resources.
155	37	This allows us to track semantic changes of individual words by following nearest-neighbor relations over time.
190	10	We can automatically detect words that undergo significant semantic changes over time.
216	11	• For DSG-S, we held out 10%, 10% and 20% of the documents from the Google books, SoU, and Twitter corpora for testing, respectively.
225	11	The improvement of our approach over the static model is particularly pronounced in the SoU and Twitter corpora, which are much smaller than the massive Google books corpus.
228	23	We presented the dynamic skip-gram model: a Bayesian probabilistic model that combines word2vec with a latent continuous time series.
229	30	We showed experimentally that both dynamic skip-gram filtering (which conditions only on past observations) and dynamic skip-gram smoothing (which uses all data) lead to smoothly changing embedding vectors that are better at predicting word-context statistics at held-out time steps.
230	142	The benefits are most drastic when the data at individual time steps is small, such that fitting a static word embedding model is hard.
231	66	Our approach may be used as a data mining and anomaly detection tool when streaming text on social media, as well as a tool for historians and social scientists interested in the evolution of language.
