1	11	A number of acquisition functions (e.g., probability of improvement or expected improvement (EI) over the currently found maximum (Brochu et al., 2010), entropybased (Villemonteix et al., 2009; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014), and upper confidence bound (UCB) (Srinivas et al., 2010)) have been devised to perform BO: They repeatedly select an input for evaluating/querying the black-box function (i.e., until the budget is depleted) that intuitively trades off between sampling where the maximum is likely to be given the current, possibly imprecise belief of the function modeled by a Gaussian process (GP) (i.e., exploitation) vs. improving the GP belief of the function over the entire input domain (i.e., exploration) to guarantee finding the global maximum.
3	128	Such batch/parallel BO algorithms can be classified into two types: On one extreme, batch BO algorithms like multi-points EI (q-EI) (Chevalier & Ginsbourger, 2013), parallel predictive entropy search (PPES) (Shah & Ghahramani, 2015), and the parallel knowledge gradient method (q-KG) (Wu & Frazier, 2016) jointly optimize the batch of inputs and hence scale poorly in the batch size.
4	20	On the other extreme, greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016) boost the scalability by selecting the inputs of the batch one at a time.
5	21	We argue that such a highly suboptimal approach to gain scalability is an overkill: In practice, each function evaluation is often much more computationally and/or economically costly (e.g., hyperparameter tuning for deep learning, drug testing on human subjects), which justifies dedicating more time to obtain better BO performance.
7	42	To achieve this, we first observe that, interestingly, batch BO can be perceived as a cooperative multi-agent decision making problem whereby each agent optimizes a separate input of the batch while coordinating with the other agents doing likewise.
9	21	In particular, if batch BO can be framed as some known class of multi-agent decision making problems, then it can be solved efficiently and scalably by the latter’s state-of-the-art solvers.
10	21	The key technical challenge would therefore be to investigate how batch BO can be cast as one of such to exploit its advantage of scalability in the number of agents (hence, batch size) while at the same time theoretically guaranteeing the resulting BO performance.
17	12	We consider the domain to be discrete as it is known how to generalize results to a continuous, compact domain via suitable discretizations (Srinivas et al., 2010).
34	33	Specifically, to avoid selecting the same input multiple times within a batch (hence reducing to GP-UCB), they update the posterior variance (but not the posterior mean) after adding each input to the batch, which can be performed prior to evaluating its corresponding f since the posterior variance is independent of the observed outputs (1).
40	50	A DCOP can be defined as a tuple (X ,V,A, h,W) that comprises a set X of input random vectors, a set V of |X | corresponding finite domains (i.e., a separate domain for each random vector), a set A of agents, a function h : X → A assigning each input random vector to an agent responsible for optimizing it, and a setW , {wn}n=1,...,N of non-negative payoff functions such that each function wn defines a constraint over only a subset Xn ⊆ X of input random vectors and represents the joint payoff that the corresponding agents An , {h(x)|x ∈ Xn} ⊆ A achieve.
45	21	A straightforward generalization of GP-UCB (Srinivas et al., 2010) to jointly optimize a batch of inputs is to simply consider summing the GP-UCB acquisition function over all inputs of the batch.
49	15	So, in each iteration t, our proposed batch GP-UCB algorithm (2) selects a batch Dt ⊂ D of inputs for evaluating/querying f that trades off between sampling close to expected maxima (i.e., with a large sum of posterior means 1>µDt = ∑ x∈Dt µ{x}) given the current GP belief of f (i.e., exploitation) vs. that yielding a large information gain I[fD;yDt |yD1:t-1 ] on f over D to improve its GP belief (i.e., exploration).
52	32	However, we will show in this section that our batch variant of GP-UCB is, interestingly, amenable to a Markov approximation, which can then be naturally formulated as a multi-agent DCOP in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size.
53	31	The key idea is to design the structure of a matrix ΨDtDt whose log-determinant can closely approximate that of ΨDtDt , I + σ −2 n ΣDtDt residing in the I[fD;yDt |yD1:t-1 ] term in (2) and at the same time be decomposed into a sum of log-determinant terms, each of which is defined by submatrices of ΨDtDt that all depend on only a subset of the batch.
58	13	To address this issue, we significantly relax this assumption and show that it is in fact possible to construct a more refined, dense matrix approximation ΨDtDt by exploiting a Markov assumption, which consequently correlates the outputs between all its constituent blocks and is, perhaps surprisingly, still amenable to the decomposition to achieve scalability in the batch size.
60	25	,DtN and ΨDtDt (ΨDtDt ) into N ×N square blocks, i.e., ΨDtDt , [ΨDtnDtn′ ]n,n′=1,...,N (ΨDtDt , [ΨDtnDtn′ ]n,n′=1,...,N ).
61	17	Our first result below derives a decomposition of the logdeterminant of any symmetric positive definite block matrix ΨDtDt into a sum of log-determinant terms, each of which is defined by a separate diagonal block of the Cholesky factor of Ψ −1 DtDt : Proposition 1.
70	14	Then, Ψ −1 DtDt is B-block-banded (see Fig.
73	39	1 that (a) though Ψ −1 DtDt is a sparse B-block-banded matrix, ΨDtDt is a dense matrix approximation for B = 1, .
74	29	, N − 1; (b) when B = N − 1 or N = 1, ΨDtDt = ΨDtDt ; and (c) the blocks within the B-block band of ΨDtDt (i.e., |n − n′| ≤ B) coincide with that of ΨDtDt while each block outside the Bblock band of ΨDtDt (i.e., |n − n′| > B) is fully specified by the blocks within the B-block band of ΨDtDt (i.e., |n − n′| ≤ B) due to its recursive series of |n − n′| − B reduced-rank approximations (Fig.
93	23	It can also be observed that (5) is naturally formulated as a multi-agent DCOP (Section 2) whereby every agent an ∈ A is responsible for optimizing a disjoint subset Dtn of the batch Dt for n = 1, .
94	24	, N and each function wn defines a constraint over only the subset Dtn ∪ DBtn = ⋃η n′=nDtn′ of the batch Dt and represents the joint payoff that the corresponding agents An , {an′}ηn′=n ⊆ A achieve.
95	38	As a result, (5) can be efficiently and scalably solved by the state-of-the-art DCOP algorithms (Chapman et al., 2011; Leite et al., 2014).
96	94	For example, the time complexity of an iterative message-passing algorithm called max-sum (Farinelli et al., 2008) scales exponentially in only the largest arity maxn∈{1,...,N} |Dtn ∪ DBtn| = (B+1)|Dt|/N of the functionsw1, .
97	39	Given a limited time budget, a practitioner can set a maximum arity of ω for any function wn, after which the number N of functions is adjusted to d(B + 1)|Dt|/ωe so that the time incurred by max-sum to solve the DCOP in (5) is O(|D|ωω3B|Dt|)6 per iteration (i.e., linear in the batch size |Dt| by assuming ω and the Markov orderB to be constants).
98	48	In contrast, our batch variant of GP-UCB (2) incurs exponential time in the batch size |Dt|.
99	33	The max-sum algorithm is also amenable to a distributed implementation on a cluster of parallel machines to boost scalability further.
100	37	If a solution quality guarantee is desired, then a variant of maxsum called bounded max-sum (Rogers et al., 2011) can be used7.
101	38	Finally, the Markov order B can be varied to trade off between the approximation quality of ΨDtDt (4) and the time efficiency of max-sum in solving the DCOP in (5).
103	27	Let δ ∈ (0, 1) be given, C1 , 4/ log(1 + σ−2n ), γT , maxD1:T⊂D I[fD;yD1:T ], αt , C1|Dt| exp(2C) log(|D|t2π2/(6δ)), and ν̄T , ∑T t=1 νt.
