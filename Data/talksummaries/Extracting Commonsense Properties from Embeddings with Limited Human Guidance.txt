0	53	Automatically extracting common sense from text is a long-standing challenge in natural language processing (Schubert, 2002; Van Durme and Schubert, 2008; Vanderwende, 2005).
1	62	As argued by Forbes and Yejin (2017), typical language use may reflect common sense, but the commonsense knowledge itself is not often explicitly stated, due to reporting bias (Gordon and Van Durme, 2013).
2	6	Thus, additional human knowledge or annotated training data are often used to help systems learn common sense.
4	28	Specifically, we focus on learning relative comparisons of (one-dimensional) object properties, such as the fact that a cantaloupe is more round than a hammer.
5	19	Methods for learning this kind of common sense have been developed previously (e.g. Forbes and Choi, 2017), but the best-performing methods in that previous work requires dozens of manually-annotated frames for each comparison property, to connect the property to how it is indirectly reflected in text—e.g., if text asserts that “x carries y,” this implies that x is probably larger than y.
7	23	It takes the form of a neural network that compares a projection of embeddings for each of two objects (e.g. “elephant” and “tiger”) to the embeddings for the two poles of the target dimension of comparison (e.g., “big” and ”small” for the size property).
10	25	Further, because our architecture takes the property (pole) labels as arguments, it can extend to the zero-shot setting in which we evaluate on properties not seen in training.
13	25	We show that synthesizing AL queries can be effective using an approach that explicitly models which comparison questions are nonsensical (e.g., is Batman taller than Democracy?).
23	40	For example, for the property size, we pick ”big” and ”small”.
29	16	We also experiment with generating label representations from just a single adjective (property) embedding R<, namely R≈ = σ(R<W2), R> = σ(R<W3).
30	6	We refer to this simpler method as PCE(one-pole).
34	33	The key distinction of our method is that it learns a projection from the object word embedding space to the label embedding space.
35	10	This allows the model to leverage the property label embeddings to perform zero-shot prediction on properties not observed in training.
36	43	For example, from a training example ”dogs are smaller than elephants”, the model will learn a projection that puts ”dogs” relatively closer to ”small,” and far from ”big” and ”similar.” Doing so may also result in projecting ”dog” to be closer to ”light” than to ”heavy,” such that the model is able to predict ”dogs are lighter than elephants” despite never being trained on any weight comparison examples.
49	30	We test our three-way model on the VERB PHYSICS data set from (Forbes and Choi, 2017).
50	18	As there are only 5 properties in VERB PHYSICS, we also develop a new data set we call PROPERTY COMMON SENSE.
51	8	We select 32 commonsense properties to form our property set (e.g., value, roundness, deliciousness, intelligence, etc.).
67	15	In Table 1, we compare the performance of the three-way PCE model against the existing state of the art on the VERB PHYSICS data set.
68	27	The use of LSTM embeddings in PCE yields the best accuracy for all properties.
69	6	Across all embedding choices, PCE performs as well or better than F&C, despite the fact that PCE does not use the annotated frames that F&C requires (approximately 188 labels per property).
70	18	Thus, our approach matches or exceeds the performance of previous work using significantly less annotated knowledge.
72	45	Table 2 evaluates our models on properties not seen in training (zero-shot learning).
74	22	PCE outperforms the baselines.
77	15	Here, the LSTM embeddings perform similarly to the Word2vec embeddings, perhaps because the PROPERTY COMMON SENSE vocabulary consists of less frequent nouns than in VERB PHYSICS.
78	34	Thus, the Word2vec embeddings are able to catch up due to their larger vocabulary and much larger training corpus.
80	68	The synthesis approach performs best, especially later in training when the training pool for the pool-based methods has only uninformative examples remaining.
82	11	As noted above, we found a “good” level of agreement (Cohen’s Kappa of 0.64) for our PROPERTY COMMON SENSE data, which is lower than one might expect for task aimed at common sense.
83	42	We analyzed the disagreements and found that they stem from two sources of subjectivity in the task.
84	46	The first is that different labelers may have different thresholds for what counts as similar—a spider and an ant might be marked similar in size for one labeler, but not for another labeler.
85	15	In our data, 58% of the disagreements are cases in which one annotator marks similar while the other says not similar.
