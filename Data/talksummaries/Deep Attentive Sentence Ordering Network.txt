0	25	Modeling a coherent text is one of the key problems in natural language processing.
3	20	It aims to organize a set of sentences into a coherent text with a logically consistent order and has wide applications in natural language generation such as concept-to-text generation (Konstas and Lapata, 2012a,b, 2013), retrieval-based question answering (Yu et al., 2018; Verberne, 2011), and extractive multi-document summarization (Barzilay and Elhadad, 2002; Galanis et al., 2012; Nallapati et al., 2017), where the improper ordering of sentences would introduce ambiguity and degrade readability.
5	13	Traditional methods developed for this task employ handcrafted linguistic features to model the document structure such as Entity Grid (Barzilay and Lapata, 2008), Content Model (Barzilay and Lee, 2004), and Probabilistic Model (Lapata, 2003).
7	15	Inspired by the success of deep learning, datadriven approaches based on neural networks have been proposed including Pairwise Ranking Model (Chen et al., 2016) which learns the relative order of sentence pairs to predict the pairwise ordering of sentences, and Window network (Li and Hovy, 2014) sliding a window over the text to evaluate the coherence.
9	16	Such methods exploit LSTMs based paragraph encoder to compute a context representation for the whole sequential sentences and then adopt a pointer network (Vinyals et al., 2015) as the decoder to predict their order.
10	48	However, since LSTM works sequentially, paragraph encoder only based on LSTMs suffers from the incorrect input sentence order and has difficulty in capturing a logically reliable representation through the recurrent connections, which makes trouble for the decoder to find the correct order.
11	18	To overcome the above limitation, in this work, we develop a novel deep attentive sentence ordering network (referred as ATTOrderNet) by inte- grating self-attention mechanism (Vaswani et al., 2017) with LSTMs to learn a relatively reliable paragraph representation for subsequent sentence ordering.
24	11	Specifically, a set of n sentences with the order o = [o1, o2, · · · , on] can be described as s = [so1, so2, · · · , son ].
25	27	The goal is to find the correct order o∗ for them, o∗ = [o∗1, o∗2, · · · , o∗n], with which the whole sentences have the highest coherence probability: P(o∗ |s) > P(o|s), ∀o ∈ ψ (1) where o indicates any order of these sentences and ψ denotes the set of all possible orders.
35	14	For a sentence, we first apply word embedding matrix to translate the raw words in the sentence into distributional representations, and then adopt bidirectional LSTMs to learn a sentence-level representation for summarizing its high level semantic concepts.
46	17	The mathematical formulation is shown below: Mi = Attention(QWQi ,KWKi ,VWVi ) (4) MH(Q,K,V) = Concat(M1, · · · ,Mh)W (5) where WQi ,W K i ,W V i ∈ Rd×da with da = d/h are the projection matrices for the i-th head and W ∈ Rhda×d.
47	24	Self-attention (Vaswani et al., 2017; Tan et al., 2017; Shen et al., 2017) is a special case of attention mechanism that only requires a single sequence to compute its representation where queries, keys, and values are all from the same place.
48	29	The paragraph encoder is composed of multiple self-attention layers followed by an average pooling layer.
54	25	This allows each sentence to build links with all other sentences in the text, which enables the encoder to exploit latent dependency relationships among sentences without regarding to their input order.
55	37	Then, attention mechanism uses weighted sum operation to establish a higher level representation for the entire sentence set.
56	30	As we see, there is no order information used in the encoding process which prevents the model from being affected by the incorrect sentence order.
61	19	Following the previous approaches (Gong et al., 2016; Logeswaran et al., 2018), the coherence probability of given sentences s with the order o is formalized as: P(o|s) = n∏ i=1 P(oi |oi−1, · · · , o1, s) (10) The higher the probability, the more coherent sentences assignment is.
71	16	And P(oi |oi−1, · · · , o1, s) can be interpreted as the coherence probability for the current output sequence when soi being the sentence choice at position i conditioned on the previous sentences assignment.
73	24	For each ordered document, we use one random permutation of sentences as the input sample at each epoch during the training and testing process.
82	29	arXiv abstract, SIND caption: we further consider two datasets used in (Gong et al., 2016).
95	23	We first evaluate our model on the sentence ordering task, as proposed by Barzilay and Lapata (2008).
114	13	Accuracy (Acc): We follow (Logeswaran et al., 2018) in employing Accuracy to measure how often the absolute position of a sentence was correctly predicted.
126	24	This performance clearly demonstrates the adaptability and flexibility of the proposed model.
129	14	Among the traditional ordering approaches, Content Model (Barzilay and Lee, 2004) representing topics as states and capturing possible orderings for global coherence performs better than other methods with the tau score of 0.81 on Earthquake dataset, which also demonstrates that global context is important to sentence ordering.
142	13	Since the first and the last sentences of the text are more special to discern (Chen et al., 2016; Gong et al., 2016), we also evaluate the ratio of correctly predicting the first and the last sentences.
164	15	Models are evaluated with Pairwise Accuracy: the ratio of correctly identifying the original document with higher coherence probability (defined in Equation 10) than the probability of its permutation.
175	98	This could also be blamed for their paragraph encoder.
177	61	Compared to the result in the sentence ordering task, Entity Grid (Barzilay and Lapata, 2008) achieves a good performance in this task and even outperforms Recurrent neural networks and Recursive neural networks (Li and Hovy, 2014) on Accident dataset.
182	11	ATTOrderNet is evaluated on Sentence Ordering and Order Discrimination tasks.
183	22	The experimental results demonstrate its effectiveness and show promising improvements over existing models across most datasets.
