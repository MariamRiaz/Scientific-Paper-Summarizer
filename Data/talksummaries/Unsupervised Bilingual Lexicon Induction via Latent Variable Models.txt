0	27	Learning the representations of languages is a fundamental problem in natural language processing and most existing methods exploit the hypothesis that words occurring in similar contexts tend to have similar meanings (Pennington et al., 2014; Bojanowski et al., 2017), which could lead word vectors to capture semantic information.
1	35	Mikolov et al. (2013) first point out that word embeddings learned on separate monolingual corpora exhibit similar structures.
2	7	Based on this finding, they suggest it is possible to learn a linear mapping from a source to a target embedding space and then generate bilingual dictionaries.
3	59	This simple yet effective approach has led researchers to investigate on improving cross-lingual word embeddings with the help of bilingual word lexicons (Faruqui and Dyer, 2014; Xing et al., 2015).
4	18	For low-resource languages and domains, crosslingual signal would be hard and expensive to obtain, and thus it is necessary to reduce the need for bilingual supervision.
5	10	Artetxe et al. (2017) successfully learn bilingual word embeddings with only a parallel vocabulary of aligned digits.
7	42	However, their performance is still significantly worse than supervised methods.
8	15	By combining the merits of several previous works, Conneau et al. (2018) introduce a model that reaches and even outperforms supervised state-of-the-art methods with no parallel data.
10	11	Both Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) and Variational Autoencoders (VAEs) (Kingma and Welling, 2014) are prominent ones.
13	5	We also utilize adversarial training for our model and require no form of supervision.
39	30	Basically, our model assumes that the source word embedding {xn} and the target word embedding {yn} could be drawn from a same latent variable space {zn}, where {zn} is capable of capturing semantic meaning of words.
40	13	In contrast to the standard VAE prior that assumes each latent embedding zn to be drawn from the same latent Gaussian, our model just requires the distribution of latent variables for source and target word embeddings to be equal.
42	67	As in adversarial training, we have networks φs and φt for both source and target space, striving to map words into the same latent space, while the discriminator D is a binary classifier which tries to distinguish between the two languages.
43	31	We also have reconstruction networks θs and θt as in VAEs.
44	1	The objective function for the discriminator D could be formulated as LD = Ezy∼qφt (z|y)[logD(zy)] + Ezx∼qφs (z|x)[log(1−D(zx))].
53	5	Our model has two generators φs and φt, and we have found that training them jointly would be extremely unstable.
54	10	In this paper, we propose an iterative method to train our models.
55	14	Basically, we first initialize Wµt to be identity matrix and train φs and θs on the source side.
56	9	After convergence, we freeze Wµs , and then train φt and θt in the target side.
60	18	Our experiments could be divided into two parts.
63	3	In this section, our experiments focus on smallscale datasets and our main baseline model is adversarial autoencoder (Zhang et al., 2017).
72	6	The baseline models are MonoGiza system (Dou et al., 2015), translation matrix (TM) (Mikolov et al., 2013), isometric alignment (IA) (Zhang et al., 2016b) and adversarial training approach (Zhang et al., 2017).
75	16	As we can see from the table, our model could achieve superior performance compared with other baseline models.
77	3	We also conduct experiments on Spanish-English and Italian-English language pairs.
82	10	Because Spanish, Italian and English are closely related languages, the accuracy would be higher than the Chinese-English dataset.
90	7	As seen, our model could consistently achieve better performance compared with adversarial training.
91	5	After refinement, our model could further achieve competitive and even superior results compared with state-of-the-art unsupervised methods.
93	23	Based on the assumption that word vectors in different languages could be drawn from a same latent variable space, we propose a novel approach which builds cross-lingual dictionaries via latent variable models and adversarial training with no parallel corpora.
95	27	We hope our method could be beneficial to other areas such as unsupervised machine translation (Lample et al., 2018).
96	24	Future directions include validate our model on more realistic scenarios (Dinu et al., 2015) as well as combine our algorithms with more sophiscated adversarial networks (Arjovsky et al., 2017; Gulrajani et al., 2017).
