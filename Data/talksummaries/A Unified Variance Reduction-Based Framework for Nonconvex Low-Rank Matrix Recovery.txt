9	7	Recently, Jin et al. (2016) and Zhang et al. (2017b) proposed stochastic gradient descent algorithms for noiseless matrix completion and matrix sensing, respectively.
10	27	Although these algorithms achieve linear rate of convergence and improved computational complexity over aforementioned deterministic optimization based approaches, they are limited to specific low-rank matrix recovery problems, and unable to be extended to more general problems and settings.
11	40	In this paper, inspired by the idea of variance reduction for stochastic gradient (Schmidt et al., 2013; Konečnỳ & Richtárik, 2013; Johnson & Zhang, 2013; Defazio et al., 2014a;b; Mairal, 2014; Xiao & Zhang, 2014; Konečnỳ et al., 2014; Reddi et al., 2016; Allen-Zhu & Hazan, 2016; Chen & Gu, 2016; Zhang & Gu, 2016), we propose a unified stochastic gradient descent framework with variance reduction for low-rank matrix recovery, which integrates both optimization-theoretic and statistical analyses.
12	19	To the best of our knowledge, this is the first unified accelerated stochastic gradient descent framework for low-rank matrix recovery with strong convergence guarantees.
15	93	We develop a generic stochastic variance-reduced gradient descent algorithm for low-rank matrix recovery, which can be applied to various low rank-matrix estimation problems, including matrix sensing, noisy matrix completion and one-bit matrix completion.
16	67	In particular, for noisy matrix sensing, it is guaranteed to linearly converge to the unknown low-rank matrix up to the minimax statistical precision (Negahban & Wainwright, 2011; Wang et al., 2016); while for noiseless matrix sensing, our algorithm achieves the optimal sample complexity (Recht et al., 2010; Tu et al., 2015; Wang et al., 2016), and attains a linear rate of convergence.
17	17	Besides, for noisy matrix completion, it achieves the best-known sample complexity required by nonconvex matrix factorization (Zheng & Lafferty, 2016).
18	99	At the core of our algorithm, we construct a novel semi-stochastic gradient term, which is substantially different from the one if following the original stochastic variance-reduced gradient using chain rule (Johnson & Zhang, 2013).
19	25	This uniquely constructed semi-stochastic gradient has not appeared in the literature, and is essential for deriving the minimax optimal statistical rate.
20	22	Our unified framework is built upon the mild restricted strong convexity and smoothness conditions (Negahban et al., 2009; Negahban & Wainwright, 2011) regarding the objective function.
21	7	Based on the above mentioned conditions, we derive an innovative projected notion of the restricted Lipschitz continuous gradient property, which we believe is of independent interest for other nonconvex problems to prove sharp statistical rates.
23	20	Besides, for each specific examples, we verify that the conditions required in the generic setting are satisfied with high probability, which demonstrates the applicability of our framework.
24	17	Our algorithm has a lower computational complexity compared with existing approaches (Jain et al., 2013a; Zhao et al., 2015; Chen & Wainwright, 2015; Zheng & Lafferty, 2015; 2016; Tu et al., 2015; Bhojanapalli et al., 2015; Park et al., 2016b; Wang et al., 2016).
25	66	More specifically, to achieve ✏ precision, the gradient complexity1 of our algorithm is O (N + 2b) log(1/✏) .
26	37	Here N denotes the total number of observations, d denotes the dimensionality of the unknown low-rank matrix X⇤, b denotes the batch size, and  denotes the condition number of X⇤ (see Section 2 for a detailed definition).
27	8	In particular, if the condition number satisfies   N/b, our algorithm is computationally more efficient than the state-of-the-art generic algorithm in Wang et al. (2016).
28	17	, d} and d⇥d identity matrix respectively.
29	8	We write A>A = I d2 , if A 2 Rd1⇥d2 is orthonormal.
30	60	For any matrix A 2 Rd1⇥d2 , we use A i,⇤ and A⇤,j to denote the i-th row and j-th column of A, respectively.
31	6	In addition, we use A ij to denote the (i, j)-th element of A. Denote the row space and column space of A by row(A) and col(A) respectively.
32	34	Let d = max{d 1 , d 2 }, and ` (A) be the `-th largest singular value of A.
35	12	We use kAk1,1 = maxi,j |Aij | to denote the element-wise infinity norm of A, and we use kAk 2,1 to represent the largest `2-norm of its rows.
38	26	In this section, we present our generic stochastic gradient descent algorithm with variance reduction as well as several illustrative examples.
45	43	R be the sample loss function, which evaluates the fitness of any matrix X associated with the total N observations.
46	20	Then the low-rank matrix recovery problem can be formulated as follows: minX2Rd1⇥d2 LN (X) := 1 N P N i=1 ` i (X), subject to X 2 C, rank(X)  r, (2.1) where ` i (X) measures the fitness of X associated with the i-th observation.
49	24	In order to deal with such identifiability issue, following Tu et al. (2015); Zheng & Lafferty (2016); Park et al. (2016b), we consider the following regularized optimization problem: minU2C1,V2C2 FN (U,V) := LN (UV>) +R(U,V), where the regularization term is defined as R(U,V) = kU>U V>Vk2 F /8.
50	58	We further decompose the objective function F N (U,V) into n components to apply stochastic variance-reduced gradient descent: F N (U,V) := 1 n P n i=1 F i (U,V), (2.3) where we assume N = nb, and b denotes batch size, i.e., the number of observations associated with each F i .
51	30	More specifically, we have F i (U,V) = L i (UV>) +R(U,V), L i (UV>) = 1 b P b j=1 ` ij (UV > ).
53	30	As will be seen in later theoretical analysis, the variance of the proposed stochastic gradient indeed decreases as the iteration number increases, which leads to a faster convergence rate.
55	13	Note that our proposed Algorithm 1 is different from the standard stochastic variance-reduced gradient algorithm (Johnson & Zhang, 2013) in several aspects.
