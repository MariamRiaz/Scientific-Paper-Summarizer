0	115	We consider learning a sparse linear regressor β minimizing the population objective: β arg min β EX,Y D r`pY, xX,βyqs , (1) where pX, Y q P X Y Rp Y are drawn from an unknown distribution D and `p , q is a convex loss function, based on N i.i.d.
1	62	samples txi, yiuNi 1 drawn from D, and when the support S : supportpβ q tj P rps | β j 0u of β is small, |S| ¤ s. In a standard single-machine setting, a common empirical approach is to minimize the `1 regularized empirical loss (see, e.g., (2) below).
2	118	Here we consider a setting where data are distributed across m machines, and, for simplicity, assume1 that N nm, so that each machine j has access to n i.i.d.
3	10	observations (from the same source D) txji, yjiuni 1 (equivalently, that N nm samples are randomly partitioned across machines).
4	24	The main contribution of the paper is a novel algorithm for estimating β in a distributed setting.
5	43	Our estimator is able to achieve the performance of a centralized procedure that has access to all data, while keeping computation and communication costs low.
6	72	Compared to the existing oneshot estimation approach (Lee et al., 2015b), our method can achieve the same statistical performance without performing the expensive debiasing step.
7	64	As the number of communication rounds increases, the estimation accuracy improves until matching the performance of a centralized procedure, which happens after the logarithm of the total number of machines rounds.
8	18	Furthermore, our results can be achieved under weak assumptions on the data generating procedure.
10	7	In each round, machines exchange messages with the master machine.
11	7	Between two rounds, each machine only computes based on its local information, which includes local data and previous messages (Zhang et al., 2013b; Shamir & Srebro, 2014; Arjevani & Shamir, 2015).
12	61	In a nondistributed setting, efficient estimation procedures need to balance statistical efficiency with computation efficiency (runtime).
13	19	In a distributed setting, the situation is more complicated and we need to balance two resources, local runtime and number of rounds of communication, with the statistical error.
14	19	The local runtime refers to the amount of work each machine needs to do.
15	8	The number of rounds of communication refers to how often do local machines need to exchange messages with the master machine.
16	72	We compare our procedure to other algorithm using the aforementioned metrics.
17	23	We consider the following two baseline estimators of β : the local estimator uses data available only on the master (first) machine and ignores data available on other machines.
19	23	The local procedure is efficient in both communication and computation, however, the resulting estimation error is large compared to an estimator that uses all of the available data.
20	10	The other idealized baseline is the centralized estimator β̂centralize arg min β 1 mn m̧ j 1 ņ i 1 `pyji, xxji,βyq λ||β||1.
22	4	In a related setting, Lee et al. (2015b) studied a one-shot approach to learning β , called Avg-Debias, that is based on averaging the debiased lasso estimators (Zhang & Zhang, 2013).
23	17	Under strong assumptions on the data generating procedure, their approach matches the centralized error bound after one round of communication.
