1	26	Recently, RNNs were applied to task of generating sentences from an explicit semantic representation (Wen et al., 2015a).
3	25	Although it is now clear that LSTMbased conditional LMs can generate plausible natural language, less effort has been put in comparing the different model architectures.
5	145	In this work, we study the use of conditional LSTMs in the generation component of neural network (NN)-based dialogue systems which depend on multiple conditioning sources and optimising multiple metrics.
6	72	Neural conversational agents (Vinyals and Le, 2015; Shang et al., 2015) are direct extensions of the sequence-to-sequence model (Sutskever et al., 2014) in which a conversation is cast as a source to target transduction problem.
7	60	However, these models are still far from real world applications because they lack any capability for supporting domain specific tasks, for example, being able to interact with databases (Sukhbaatar et al., 2015; Yin et al., 2016) and aggregate useful information into their responses.
8	21	Recent work by Wen et al. (2016a), however, proposed an end-to-end trainable neural dialogue system that can assist users to complete specific tasks.
41	31	The testbed for this work is a neural network-based task-oriented dialogue system proposed by Wen et al. (2016a).
43	27	The model consists of both encoder and decoder modules.
47	62	Intent Network The intent network takes a sequence of tokens1 and converts it into a sentence embedding representing the user intent using an LSTM network.
49	33	As mentioned in Wen et al. (2016a), this representation can be viewed as a distributed version of the speech act (Traum, 1999) used in traditional systems.
51	35	By taking each user input as new evidence, the task of a belief tracker is to maintain a multinomial distribution p over values v ∈ Vs for each informable slot2 s, and a binary distribution for each requestable slot2.
53	30	The belief states pst , together with the intent vector zt, can be viewed as the system’s comprehension of the user requests up to turn t. Database Operator Based on the belief states pst , a DB query is formed by taking the union of the maximum values of each informable slot.
57	21	Policy Network Based on the vectors zt, pst , and xt from the above three modules, the policy network combines them into a single action vector mt by a three-way matrix transformation, mt = tanh(Wzmzt + Wxmxt + ∑ s∈G W s pmp s t ) (1) where matrices Wzm, Wspm, and Wxm are parameters and G is the domain ontology.
59	26	The final system response can then be formed by substituting the actual values of the database entries into the skeletal sentence structure.
60	109	In this paper we study and analyse three different variants of LSTM-based conditional generation architectures: Language Model Type The most straightforward way to condition the LSTM network on additional source information is to concatenate the conditioning vector mt together with the input word embedding wj and previous hidden layer hj−1,   ij fj oj ĉj   =   sigmoid sigmoid sigmoid tanh  W4n,3n   mt wj hj−1   cj = fj cj−1 + ij ĉj hj = oj tanh(cj) where index j is the generation step, n is the hidden layer size, ij , fj ,oj ∈ [0, 1]n are input, forget, and output gates respectively, ĉj and cj are proposed cell value and true cell value at step j, and W4n,3n are the model parameters.
62	66	Since it does not differ significantly from the original LSTM, we call it the language model type (lm) conditional generation network.
63	32	Memory Type The memory type (mem) conditional generation network was introduced by Wen et al. (2015b), shown in Figure 1b, in which the conditioning vector mt is governed by a standalone reading gate rj .
64	27	This reading gate decides how much information should be read from the conditioning vector and directly writes it into the memory cell cj ,   ij fj oj rj   =   sigmoid sigmoid sigmoid sigmoid  W4n,3n   mt wj hj−1   ĉj = tanh ( Wc(wj ⊕ hj−1) ) cj = fj cj−1 + ij ĉj + rj mt hj = oj tanh(cj) where Wc is another weight matrix to learn.
66	59	Hybrid Type Continuing with the same idea as the memory type network, a complete separation of conditioning vector and LM (except for the gate controlling the signals) is provided by the hybrid type network shown in Figure 1c,   ij fj oj rj   =   sigmoid sigmoid sigmoid sigmoid  W4n,3n   mt wj hj−1   ĉj = tanh ( Wc(wj ⊕ hj−1) ) cj = fj cj−1 + ij ĉj hj = oj tanh(cj) + rj mt This model was motivated by the fact that long-term dependency is not needed for the conditioning vector because we apply this information at every step j anyway.
69	59	Like Wen et al. (2016a), we explore the use of an attention mechanism to combine the tracker belief states in which the policy network in Equation 1 is modified as mjt = tanh(Wzmzt + Wxmxt + ∑ s∈G α j sWspmp s t ) where the attention weights αjs are calculated by, αjs = softmax ( rᵀ tanh ( Wr · (vt ⊕ pst ⊕wtj ⊕ htj−1) )) where vt = zt + xt and matrix Wr and vector r are parameters to learn.
73	197	Learning conditional generation models from sequential supervision signals can be difficult, because it requires the model to learn both long-term word dependencies and potentially distant source encoding functions.
74	85	To mitigate this difficulty, we introduce a novel method called snapshot learning to create a vector of binary labels Υjt ∈ [0, 1]d, d < dim(mjt ) as the snapshot of the remaining part of the output sentence Tt,j:|Tt| from generation step j.
75	24	Each element of the snapshot vector is an indicator function of a certain event that will happen in the future, which can be obtained either from the system response or dialogue context at training time.
78	27	The indicator functions we use in this work have two forms: (1) whether a particular slot value (e.g., [v.food]1) is going to occur, and (2) whether the system has offered a venue3, as shown in Figure 2.
107	44	Results Table 1 shows the evaluation results.
108	70	The numbers to the left and right of each table cell are the same model trained w/o and w/ snapshot learning.
110	26	This is especially true for BLEU scores.
111	251	We think this may be attributed to the more discriminative conditioning vector learned through the snapshot method, which makes the learning of the conditional LM easier.
113	70	As can be seen, using a succinct representation is better (summary>full) because the identity of each categorical value in the belief state does not help when the generation decisions are done in skeletal form.
116	30	This result shows that the language model type (lm) and memory type (mem) networks perform better in terms of BLEU score and slot matching rate, while the hybrid type (hybrid) networks achieve higher task success.
