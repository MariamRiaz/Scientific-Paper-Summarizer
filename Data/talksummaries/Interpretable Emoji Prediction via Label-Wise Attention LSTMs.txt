0	22	Communication in social media differs from more standard linguistic interactions across a wide range of dimensions.
1	18	Immediacy, short text length, the use of pseudowords like #hashtags or @mentions, and even metadata such as user information or geolocalization are essential components of social media messages.
2	27	In addition, the use of emojis, small ideograms depicting objects, people and scenes (Cappallo et al., 2015), are becoming increasingly important for fully modeling the underlying semantics of a social media message, be it a product review, a tweet or an Instagram post.
3	10	Emojis are the evolution of characterbased emoticons (Pavalanathan and Eisenstein, 2015), and are extensively used, not only as sentiment carriers or boosters, but more importantly, to express ideas about a myriad of topics, e.g., mood ( ), food ( ), sports ( ) or scenery ( ).
4	6	Emoji modeling and prediction is, therefore, an important problem towards the end goal of properly capturing the intended meaning of a so- cial media message.
5	22	In fact, emoji prediction, i.e., given a (usually short) message, predict its most likely associated emoji(s), may help to improve different NLP tasks (Novak et al., 2015), such as information retrieval, generation of emojienriched social media content or suggestion of emojis when writing text messages or sharing pictures online.
15	3	While emoji prediction has predominantly been treated as a multi-class classification problem in the literature, it would be more informative to analyze which text fragments are considered important for each individual emoji.
16	12	With this motivation in mind, in this paper we put forward a label-wise mechanism that operates over each label during training.
19	14	First, we use the proposed label-wise mechanism to analyze the behavior of neural emoji classifiers, exploiting the attention weights to uncover and interpret emoji usages.
20	15	Second, we experimentally compare the effect of the label-wise mechanism on the performance of an emoji classifier.
28	8	The weight vector wa ∈ Rd and bias term ba ∈ R map this hidden representation to a value that reflects the importance of this state for the considered classification problem.
29	44	The values z1, ..., zn are then normalized using a softmax function, yielding the attention weights αi.
30	76	The sentence representation s is defined as a weighted average of the vectors hi.
35	15	Specifically, we apply the same type of attention, but repeating it |L| (number of labels) times, where each attention module is reserved for a specific label l: zi,l = wa,lhi + ba,l αi,l = ezi,l∑N j=1 e zj,l sl = N∑ j=1 αj,lhj βl = wf,lsl + bf,l pl = eβl∑L r=1 e βr
38	37	Given a tweet, the task consists of predicting an associated emoji from a predefined set of 20 emoji labels.
40	31	We also show results from additional experiments in which the label space ranged from 20 to 200 emojis.
42	25	In order to put our proposed labelwise attention mechanism in context, we compare its performance with a set of baselines: (1) FastText (Joulin et al., 2017) (FT), which was the official baseline in the SemEval task; (2) 2 stacked Bi-LSTMs (2-BiLSTMs) without attention; and (3) 2 stacked Bi-LSTMs with standard attention (2-BiLSTMsa) (Felbo et al., 2017).
45	12	The evaluation metrics used are: F1, Accuracy@k (A@k, where k ∈ {1, 5}), and Coverage Error (CE1) (Tsoumakas et al., 2009).
48	4	The results show that our proposed 2-BiLSTMsl method outperforms all baselines for F1 in three out of four settings, and for CE in all of them.
50	32	By inspecting the predictions of our model, we found that the label-wise attention mechanism tends to be less heavily biased towards the most frequent emojis.
51	14	This is reflected in the lower coverage error results in all settings, and becomes more noticeable as the number of labels grows.
52	14	We verified this by computing the average difference between ranked predictions of the two attentive models in the 200-label setting (Figure 2).
53	30	We can observe a sudden switch at more or less the median emoji, after which the label-wise attention model becomes increasingly accurate (relative to the standard attention model).
56	19	Gold: For the above example2, the predictions of the single attention model were all linked to the general meaning of the message, that is love and friendship, leading it to predict associated emojis ( , and ), failing to capture the most relevant bit of information.
63	6	Here, the emoji was predicted with rank 1, and we see it being strongly associated with the ordinal second, suggesting that the model assumed this was some kind of “ticked enumeration” of completed tasks, which is indeed regular practice in Twitter.
66	19	One interesting example is the emoji, which shows two clear usage patterns: one literal (a tree) and one figurative (christmas and holidays).
70	18	In this paper we have presented a neural architecture for emoji prediction based on a label-wise attention mechanism, which, in addition to improving performance, provides a degree of interpretability about how different features are used for predictions, a topic of increasing interest in NLP (Linzen et al., 2016; Palangi et al., 2017).
71	6	As we experimented with sets of emoji labels of different sizes, our proposed label-wise attention architecture proved especially well-suited for emojis which were infrequent in the training data, making the system less biased towards the most frequent.
72	186	We see this as a first step to improve the robustness of recurrent neural networks in datasets with unbalanced distributions, as they were shown not to perform better than well-tuned SVMs on the emoji predicion task (Çöltekin and Rama, 2018).
73	31	As for future work, we plan to apply our labelwise attention mechanism to understand other interesting linguistic properties of human-generated text in social media, and other multi-class or multilabel classification problems.
