0	9	Metaphors are pervasive in natural language, and detecting them requires challenging contextual reasoning about whether specific situations can actually happen.
1	34	For example, in Table 1, “examining” is metaphorical because it is impossible to literally use a “microscope” to examine an entire country.
2	8	In this paper, we present end-to-end neural models for metaphor detection, which can learn rich contextual word representations that are crucial for accurate interpretation of figurative language.
3	21	In contrast, most previous approaches focused on limited forms of linguistic context, for example by only providing SVO triples such as (car, drink, gasoline) to the model (Shutova et al., 2016; Tsvetkov et al., 2013; Rei et al., 2017; Bulat et al., 2017).
4	11	While the verbal arguments provide strong cues, providing the full sentential context supports more accurate prediction, as seen in Table 1.
5	47	Even in the few cases when the full sentence is used (Köper and im Walde, 2017; Turney et al., 2011; Jang et al., 2016) existing models have used unigram-based features with limited expressivity.
6	70	We investigate two common task formulations: (1) given a target verb in a sentence, classifying whether it is metaphorical or not, and (2) given a sentence, detecting all of the metaphorical words (independent of their POS tags).
7	8	We find that relatively standard architectures based on bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) augmented with contextualized word embeddings (Peters et al., 2018) perform surprisingly well on both tasks, even with modest amount of training data.
9	28	Our code is publicly available at https://github.com/gao-g/ metaphor-in-context.
17	10	In addition, as will be shown in Section 5, we find that given accurate annotations for all words in a sentence, the sequence labeling model outperforms the classification model even when the evaluation is set up as a classification task.
20	32	To further encode contextual information, we also concatenate ELMo (Embeddings from Language Models) vectors ei from Peters et al. (2018).
22	13	Figure 1 shows the model architecture.
27	33	We concatenate an index embedding ni, which indicates whether xi is the target verb.
28	7	We use [wi; ei;ni] as an input to a bidirectional LSTM, producing a contextualized representation hi.
30	38	ai = SoftMaxi(Wahi + ba) c = n∑ i=1 aihi Finally, we feed c to a feedforward network to compute the label scores for target verb.
31	11	We evaluate performance on a number of benchmark datasets, including two for classification (TroFi and MOH\MOH-X) and one for tagging (VUA).1 Table 2 shows statistics for the verb classification datasets.
37	11	Sequence Labeling Experiment Setup The VUA dataset contains annotations for all words in each sentence.
38	16	We divide the data into training, development, and test set following the same split for the VUA verb classification task.
39	11	While the label classes are less balanced (only 11% metaphors at the token level), this dataset is much bigger.
42	50	For the VUA dataset, we also report macro-averaged F1 score across four genres (conversation, academic writing, fiction and news).
43	41	Comparison Systems We propose a simple yet effective lexical baseline.
44	87	It assigns the metaphor label if the word is annotated metaphorically more frequently than as literally in the training set, and the literal label otherwise.
45	119	We also compare our models to previously published work, including: (1) a logistic regression classifier with features that indicate verb lemmas and the verbs’ semantic class from WordNet (Klebanov et al., 2016), (2) a neural similarity network with skip-gram word embeddings (Rei et al., 2017), (3) a balanced logistic regression classifier on target verb lemma that uses a set of features based on multisense abstractness rating (Köper and im Walde, 2017), and (4) a CNN-LSTM ensemble model with weighted-softmax classifier which incorporates pre-trained word2vec, POS tags, and word cluster features (Wu et al., 2018).2 We experiment with both sequence labeling model (SEQ) and classification model (CLS) for the verb classification task, and the sequence labeling model (SEQ) for the sequence labeling task.
51	9	We used SGD to optimize the CLS model and Adam (Kingma and Ba, 2013) for the SEQ model.
53	25	Sequence Labeling Results Performance on the sequence labeling task is reported in Table 4.
55	78	Our sequence labeling model mainly improves recall.
56	32	Table 5 reports the breakdown of performance by POS tags.
57	33	Not surprisingly, tags with more data are easier to classify.
59	18	On the other hand, particles are challenging to identify, since they are often associated with multi-word expressions, such as “put down the disturbances”.
63	27	This result shows that predicting metaphor labels of context words helps to predict the target verb.
