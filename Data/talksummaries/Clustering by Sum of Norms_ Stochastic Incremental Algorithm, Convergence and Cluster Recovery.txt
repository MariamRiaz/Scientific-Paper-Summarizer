3	67	For example, the formulation of K-means is NP-hard and the typical way to solve it is the Lloyds method, which requires randomly initializing the clusters.
4	92	However, one needs to know the number of clusters in advance and different initializations may lead to significantly different final cluster results.
5	131	Lindsten et al. (2011) and Hocking et al. (2011) proposed the following convex optimization procedure for clustering, called SON (“Sum of norms” clustering) by the former and Clusterpath by the latter; min {ui∈Rm} 1 2 n∑ i=1 ‖xi − ui‖22 + λ ∑ i<j ‖ui − uj‖2 (1) The main idea of the formulation is that if input data points xi and xj belong to the same cluster, then their corresponding centroids ui and uj should be forced to be the same.
7	137	From another point of view, the regularization term can be seen as an `1,2 norm, i.e., the sum of `2 norms.
39	25	• We give experimental results giving evidence that our algorithm produces clusters of comparable quality to previous methods but scales much better to large scale problems.
52	24	, xn} ⊂ Rm and its partitioning V = {V1, V2, .
61	26	,WL} of X a coarsening of V if each partition Wl is obtained by taking the union of a number of partitions Vk.
97	160	Recently, there have been a number of theoretical results of the form that if we have data points generated by a mixture of K probability distributions, then one can cluster the data points into the K clusters, one corresponding to each component, provided the means of the different components are well–separated.
99	25	These results generally make heavy use of the generative model and particular properties of the distributions (Indeed, many of them specialize to Gaussians or independent Bernoulli trials).
104	101	Suppose we have a mixture of K Gaussians in d dimensions with mixture weights w1, · · · , wK , let w := mini wi and let µ1, · · · , µK denote their means respectively.
105	44	If we have n = Ω(poly(d/w)) points sampled from this mixture distribution, then with high probability, the center separation condition is satisfied if: ‖µr − µs‖ ≥ cKσ√ w polylog(d/w).
108	179	In the planted partition model of McSherry, a set of n points is implicitly partitioned into K groups.
110	22	We are given a graph G on these n points, where an edge between two vertices from groups r and s is present with probability Pr,s.
111	34	We can consider these n points x1, · · · , xn ∈ Rn where coordinate j in xi is 1 if (i, j) ∈ G and 0 otherwise.
112	148	The center µr of cluster r has in coordinate j the value Pr,ψ(j), where ψ(j) is the cluster vertex j belongs to.
113	32	Kumar and Kannan show that the center separation condition holds with probability at least 1− δ if: ‖µr − µs‖ ≥ cσ2K( 1 w + log n δ ) where c is a large constant, w is such that every group has size at least w · n and σ2 := maxr,s Pr,s.
114	41	Our center separation condition (7) is satisfied if: ‖µr − µs‖ ≥ c σ2K w √ n
115	41	Besides the stochastic models, we take a closer look at the result in A. Kumar (2010) and identify deterministic cases where the SON has better performance than the proved bounds for K-means.
116	78	These cases essentially guarantee that the term ‖A−C‖ in (5) remains large and the bound therein becomes highly restrictive: Definition 4.
117	359	We say that a partition V = {V1, V2, .
118	40	We say that a set X = {x1, x2, .
125	68	The proximal-based incremental method is a variant of the stochastic gradient technique, in problems where many terms in the objective function are not differentiable, and the local gradient steps are replaced by local proximal operators.
129	54	Our algorithm simply consists in iteratively applying randomly selected proximal operators.
131	58	Algorithm 1 Stochastic Splitting Algorithm Input: The data vectors {xk}nk=1 and step sizes {µk}∞k=1 Initialization: Set u1, u2, .
132	22	, un arbitrarily (we use u1 = u2 = .
169	26	The data is generated from Gaussian mixture models with two components in R2 where the means are separated by d = √ 2 and the variance σ2 is varied.
174	29	We report the adjusted Rand index (Rand, 1971) as measure of cluster quality, and would like to emphasize that this does not rely on identifying the number of clusters beforehand.
176	70	We see in Figure 1 that the quality of the clustering produced by the stochastic splitting algorithm is comparable to that of the exact algorithm.
183	32	While the stochastic splitting algorithm could in principle be implemented in time constant in the number of samples, and instead determined by the number of iterations, the adaptive stepsize used to improve performance requires evaluation of the objective value which scales with the number of samples.
188	30	The cluster recovery conditions can also be extended to cover almost perfect recovery i.e. correctly clustering all except a small fraction of points.
