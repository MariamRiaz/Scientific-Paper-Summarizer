0	36	In this paper, we study reinforcement learning (RL) problems where the agent receives rich sensory observations from the environment, forms complex contexts from sensorimotor streams, uses function approximation to generalize to unseen contexts, and must perform systematic exploration to learn efficiently.
1	25	Such problems are at the core of empirical RL research (e.g., Mnih et al., 2015; Bellemare et al., 2016), yet no existing theory provides rigorous and satisfactory guarantees in a general setting.
8	36	As our first major contribution, we define a notion of Bellman factorization (Definition 5) in Section 3, and focus on problems with low Bellman rank.
9	18	At a high level, Bellman rank is an algebraic dimension capturing the interplay between the CDP and the valuefunction approximator that we show is small for many previously-studied settings.
39	19	In this section, we introduce a new model, called a Contextual Decision Process, as a unified framework for reinforcement learning with rich observations.
42	24	A (finite-horizon) CDP is defined as a tuple (X ,A, H, P ), where X is the context space, A is the action space, and H is the horizon of the problem.
43	23	P = (P∅, P+) is the system descriptor, where P∅ ∈ ∆(X ) is a distribution over initial contexts, that is x1 ∼ P∅, and P+ : (X × A × R)∗ × X × A → ∆(R × X ) elicits the next reward and context from the interactions so far x1, a1, r1, .
53	17	Consider a finite-horizon MDP (S,A, H,Γ1,Γ, R), where S is the state space, A is the action space, H is the horizon, Γ1 ∈ ∆(S) is the initial state distribution, Γ : S × A → ∆(S) is the state transition function, R : S × A → ∆([0, 1]) is the reward function, and an episode takes the form of (s1, a1, r1, .
74	33	A CDP makes no assumptions on the cardinality of the context space, which makes it critical to generalize across contexts, since an agent might not observe the same context twice.
93	32	Given an MDP and a space of functions F : S × [H]×A → [0, 1], if Q?
107	18	Formally, the sample complexity of learning CDPs in the worst-case is Ω(KH) when K = |A|, even when the complexity of the function class, measured by log |F|, is small.
108	25	The result is due to Krishnamurthy et al. (2016) and is included in Appendix F.1 for completeness.
109	44	Of course the lower bound instances are quite pathological and devoid of any structure that is often present in real problems.
110	51	To capture these realistic scenarios, we propose a new complexity measure and restrict our attention to settings where this measure is low.
111	48	As we will see, this measure is naturally small for many existing models, and, when it is small, efficient reinforcement learning is possible.
114	89	However, observe that the Bellman equations are structured in tabular MDPs: the average Bellman error under any roll-in policy is a stochastic combination of the single-state errors, and checking the single-state errors (which is tractable) is sufficient to guarantee validity.
116	68	This intuition motivates a new complexity measure that we call the Bellman rank.
117	34	Define the Bellman error matrices, one for each h, to be |F|× |F| matrices where the (f, f ′)th entry is the Bellman error E(f, πf ′ , h).
146	24	Similar to the case of POMDPs, we can bound the Bellman rank in terms of the rank of the PSR6 when the candidate value functions are reactive.
147	14	Proposition 4 (Bellman rank in PSRs, informally).
151	53	We show that the Bellman rank in LQRs is bounded by the dimension of the state space.
159	14	To aid presentation and help convey the main ideas, we make three simplifying assumptions.
175	28	At iteration t, we choose the roll-in policy πt optimistically, by choosing ft that predicts the highest value at the starting context distribution and setting πt = πft .
185	20	The first important fact is that the algorithm never eliminates a valid function, since the learning step in Line 14 only eliminates a function f if we can find a distribution on which it has a large average Bellman error.
192	28	In the analysis, we incorporate sampling effects to derive robust versions of these facts so the algorithm always outputs a policy that is at most -suboptimal.
200	25	Thus the Bellman rank (times H) upper-bounds the number of iterations.
222	15	To our knowledge, this is the most general polynomial sample complexity bound for RL with rich observations and function approximation, as many popular models are shown to admit small Bellman rank (see Section 3, Table 1).
225	14	The most closely related result is the recent work of Krishnamurthy et al. (2016), who also consider episodic RL with infinite observation spaces and function approximation.
243	85	We leave as future work the question of optimal sample complexity for learning CDPs with low Bellman rank.
245	73	Handling infinite function classes with dependence on VC-dimension like quantities.
