41	27	However the switching relies on a heuristic based on a combination of previous values of the acquisition function and the number of steps without improvement of the incumbent.
42	57	We would prefer to make use of a Bayesian criterion for any changes in behaviour, to avoid the need for additional parameters which must be manually chosen by an expert.
44	13	However, they do not show the final value achieved by their method in most experiments, and the use of fixed pre-trained hyperparameter samples makes their implementation unsuitable for an online setting.
50	15	We shall now separate this concept into two distinct components which we treat separately.
53	14	Both components are non-negative by definition, and we expect both to decrease as we learn about the objective.
58	16	To address the issues identified above, we split our optimization into four distinct modes, intending to use the most effective at each iteration.
59	26	The modes are; Random Initialization, Bayesian Optimization, Global Regret Reduction and Local Optimization.
60	18	Random Initialization is, as usual, only required for the first few iterations.
61	19	Bayesian Optimization using Predictive Entropy Search is our default acquisition function if no relevant conditions to change behaviour have been satisfied: PES provides the usual balance between exploration and exploitation.
63	52	By making this change, we avoid the inefficient convergence of exploitation due to poor conditioning in Gaussian Processes model when used for Bayesian Optimization.
66	15	In this work, we assume that we have access to noiseless evaluations of the objective functions so that we can employ a quasi-Newton local optimization routine, such as BFGS (Nocedal & Wright, 2006), which delivers super-linear convergence to the local minimum and is free of the numerical conditioning problems present in Gaussian Processes.
78	16	This is a second order algorithm which updates an estimate of the Hessian at each step.
83	17	Once we have started this process we are no longer performing Bayesian Optimization and can continue using BFGS until convergence.
87	25	In Figure 2 we show the decision making process used.
92	23	We have defined the global regret as the difference between the objective value at the local minimizer in some region and the true global minimum.
107	15	We can make use of this behaviour to provide a test for positive definiteness returning a binary result: D(X) : R d(d+1) 2 → {0, 1} where d is the dimensionality of the problem.
118	27	We now wish to use this function to find a convex region centred around the posterior minimum (again we exclude any axes on the boundary of the search domain).
119	11	We choose to find the hypersphere centred at xmin with the greatest possible radius Rmax.
131	13	To evaluate this expression we can drawN samples from our GP model and find the value of yo in each case.
134	25	To evaluate µi and σi we can use the same set of draws, considering this time only points within the convex region, to obtain a sequence of samples of yi which can be used for a maximum likelihood estimate of the mean and variance of a normal distribution.
135	32	We compare BLOSSOM to Expected improvement with the PI stopping criteria of Lorenz et al. (2015), and to PES using the acquisition function value as the stopping criterion.
139	11	We will always achieve the global minimum, and the target regret only alters the number of steps required to terminate.
144	14	This shape is extremely dissimilar to draws from the Matérn 52 kernel used by our GP model, so yields very poor results.
146	19	Neither the number of steps taken nor the regret achieved is alone a useful metric for the effectiveness of a stopping condition (few steps with high regret are obviously undesirable, but also a small decrease in regret may not be worth a much increased number of steps), so in Table 1 we have also shown the mean product of steps and regret, E[nR].
160	58	Optimizing model hyperparameters is a common problem in machine learning.
162	18	As shown in Figure 6 we are able to obtain the best absolute result while terminating within a reasonable number of iterations, avoiding taking unnecessary further evaluations once the optimum has been achieved.
163	20	1www2.nationalgrid.com/UK/Industryinformation/Electricity-transmission-operational-data/Dataexplorer
166	11	We are further able to halt optimization once a specified value of global regret has been achieved.
167	20	This has the potential to save considerable computation in comparison to manual specification of the number of iterations to perform.
168	31	We have shown that BLOSSOM is able to achieve an improvement in the final result of several orders of magnitude compared to existing methods.
