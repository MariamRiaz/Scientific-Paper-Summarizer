1	32	Creative applications where such research exists include the composition of music (Humphrey et al., 2013; Sturm et al., 2016; Choi et al., 2016), the design of sculptures (Lehman et al., 2016), and automatic choreography (Crnkovic-Friis and Crnkovic-Friis, 2016).
3	34	A distinguishing feature of poetry is its aesthetic forms, e.g. rhyme and rhythm/meter.1 In this work, we treat the task of poem generation as a constrained language modelling task, such that lines of a given poem rhyme, and each line follows a canonical meter and has a fixed number Shall I compare thee to a summer’s day?
5	34	Specifically, we focus on sonnets and generate quatrains in iambic pentameter (e.g. see Figure 1), based on an unsupervised model of language, rhyme and meter trained on a novel corpus of sonnets.
17	31	A sonnet line obeys an alternating stress pattern, called the iambic pentameter, e.g.: S− S+ S− S+ S− S+ S− S+ S− S+ Shall I compare thee to a summer’s day?
21	21	We build our sonnet dataset from the latest image of Project Gutenberg.4 We first create a (generic) poetry document collection using the GutenTag tool (Brooke et al., 2015), based on its inbuilt poetry classifier and rule-based structural tagging of individual poems.
22	20	Given the poems, we use word and character statistics derived from Shakespeare’s 154 sonnets to filter out all non-sonnet poems (to form the “BACKGROUND” dataset), leaving the sonnet corpus (“SONNET”).5 Based on a small-scale manual analysis of SONNET, we find that the approach is sufficient for extracting sonnets with high precision.
23	20	BACKGROUND serves as a large corpus (34M words) for pre-training word embeddings, and SONNET is further partitioned into training, development and testing sets.
25	38	We propose modelling both content and forms jointly with a neural architecture, composed of 3 components: (1) a language model; (2) a pentameter model for capturing iambic pentameter; and (3) a rhyme model for learning rhyming words.
30	24	The language model is a variant of an LSTM encoder–decoder model with attention (Bahdanau et al., 2015), where the encoder encodes the preceding context (i.e. all sonnet lines before the current line) and the decoder decodes one word at a time for the current line, while attending to the preceding context.
45	27	Given a sonnet line, the pentameter model learns to attend to the appropriate characters to predict the 10 binary stress symbols sequentially.11 As punctuation is not pronounced, we preprocess each sonnet line to remove all punctuation, leaving only spaces and letters.
47	26	In the encoder, we embed the characters using the shared embedding matrix Wchr and feed them to the shared bidirectional character-level LSTM (Equation (1)) to produce the character encodings for the sentence: uj = [~uj ; ~uj ].
48	69	In the decoder, it attends to the characters to predict the stresses sequentially with an LSTM: gt = LSTM(u∗t−1,gt−1) where u∗t−1 is the weighted sum of character encodings from the previous time step, produced by an attention network which we describe next,12 and gt is fed to a linear layer with softmax activation to compute the stress distribution.
49	18	The attention network is designed to focus on stress-producing characters, whose positions are monotonically increasing (as stress is predicted sequentially).
61	38	Two reasons motivate us to learn rhyme in an unsupervised manner: (1) we intend to extend the current model to poetry in other languages (which may not have pronunciation dictionaries); and (2) the language in our SONNET data is not Modern English, and so contemporary dictionaries may not accurately reflect the rhyme of the data.
62	23	Exploiting the fact that rhyme exists in a quatrain, we feed sentence-ending word pairs of a quatrain as input to the rhyme model and train it to learn how to separate rhyming word pairs from non-rhyming ones.
68	39	We represent the encoding of the whole word by taking the last state u = uL, where L is the character length of the word.
70	25	Intuitively, the model is trained to learn a sufficient margin (defined by δ) that separates the best pair with all others, with the second-best being used to quantify all others.
72	19	With this network we can estimate whether two words rhyme by computing the cosine similarity score during generation, and resample words as necessary to enforce rhyme.
85	27	We apply this inversion trick at the word level (character order of a word is not modified) and only to the language model; the pentameter model receives the original word order as input.
86	70	We assess our sonnet model in two ways: (1) component evaluation of the language, pentameter and rhyme models; and (2) poetry generation evaluation, by crowd workers and an English literature expert.
137	26	Based on the suspicion that workers were using rhyme to judge the poems, we tested a second model, LM∗∗+RM, which is the full model without the pentameter component.
140	80	To better understand the qualitative aspects of our generated quatrains, we asked an English literature expert (a Professor of English literature at a major English-speaking university; the last author of this paper) to directly rate 4 aspects: meter, rhyme, readability and emotion (i.e. amount of emotion the poem evokes).
145	94	We found that our full model has the highest ratings for both rhyme and meter, even higher than human poets.
148	201	In particular, there is evidence here that our focus on form actually hurts the readability of the resulting poems, relative even to the simpler language models.
149	136	Another surprise is how well simple language models do in terms of their grasp of meter: in this expert evaluation, we see only marginal benefit as we increase the sophistication of the model.
150	70	Taken as a whole, this evaluation suggests that future research should look beyond forms, towards the substance of good poetry.
151	57	We propose a joint model of language, meter and rhyme that captures language and form for modelling sonnets.
152	33	We provide quantitative analyses for each component, and assess the quality of generated poems using judgements from crowdworkers and a literature expert.
153	107	Our research reveals that vanilla LSTM language model captures meter implicitly, and our proposed rhyme model performs exceptionally well.
154	128	Machine-generated generated poems, however, still underperform in terms of readability and emotion.
