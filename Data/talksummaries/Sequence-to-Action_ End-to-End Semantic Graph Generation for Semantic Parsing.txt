0	12	Semantic parsing aims to map natural language sentences to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2013).
1	26	For example, the sentence “Which states border Texas?” will be mapped to answer (A, (state (A), next to (A, stateid ( texas )))).
2	38	A semantic parser needs two functions, one for structure prediction and the other for semantic grounding.
4	8	These parsers compose structure using manually designed grammars, use lexicons for semantic grounding, and exploit fea- tures for candidate logical forms ranking.
8	8	Semantic graph-based methods (Reddy et al., 2014, 2016; Bast and Haussmann, 2015; Yih et al., 2015) represent the meaning of a sentence as a semantic graph (i.e., a sub-graph of a knowledge base, see example in Figure 1) and treat semantic parsing as a semantic graph matching/generation process.
9	6	Compared with logical forms, semantic graphs have a tight-coupling with knowledge bases (Yih et al., 2015), and share many commonalities with syntactic structures (Reddy et al., 2014).
12	5	Currently, semantic graphs are either constructed by matching with patterns (Bast and Haussmann, 2015), transforming from dependency tree (Reddy et al., 2014, 2016), or via a staged heuristic search algorithm (Yih et al., 2015).
18	48	In this paper, we propose a new neural semantic parsing framework – Sequence-to-Action, which can simultaneously leverage the advantages of semantic graph representation and the strong prediction ability of Seq2Seq models.
19	13	Specifically, we model semantic parsing as an end-to-end semantic graph generation process.
20	46	For example in Figure 1, our model will parse the sentence “Which states border Texas” by generating a sequence of actions [add variable:A, add type:state, ...].
21	212	To achieve the above goal, we first design an action set which can encode the generation process of semantic graph (including node actions such as add variable, add entity, add type, edge actions such as add edge, and operation actions such as argmin, argmax, count, sum, etc.).
22	284	And then we design a RNN model which can generate the action sequence for constructing the semantic graph of a sentence.
23	49	Finally we further enhance parsing by incorporating both structure and semantic constraints during decoding.
26	113	Compared with the previous Seq2Seq semantic parsing methods, our sequence-to-action model predicts a sequence of semantic graph generation actions, rather than linearized logical forms.
27	110	We find that the action sequence encoding can better capture structure and semantic information, and is more compact.
28	32	And the parsing can be enhanced by exploiting structure and semantic constraints.
29	102	For example, in GEO dataset, the action add edge:next to must subject to the semantic constraint that its arguments must be of type state and state, and the structure constraint that the edge next to must connect two nodes to form a valid graph.
30	62	We evaluate our approach on three standard datasets: GEO (Zelle and Mooney, 1996), ATIS (He and Young, 2005) and OVERNIGHT (Wang et al., 2015b).
31	32	The results show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets.
32	13	The main contributions of this paper are summarized as follows: • We propose a new semantic parsing framework – Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process.
33	8	This new framework can synthesize the advantages of semantic graph representation and the prediction ability of Seq2Seq models.
34	155	• We design a sequence-to-action model, including an action set encoding for semantic graph generation and a Seq2Seq RNN model for action sequence prediction.
35	4	We further enhance the parsing by exploiting structure and semantic constraints during decoding.
36	9	Experiments validate the effectiveness of our method.
37	95	Given a sentence X = x1, ..., x|X|, our sequenceto-action model generates a sequence of actions Y = y1, ..., y|Y | for constructing the correct semantic graph.
38	11	The conditional probability P (Y |X) used in our model is decomposed as follows: P (Y |X) = |Y |∏ t=1 P (yt|y<t, X) (1) where y<t = y1, ..., yt−1.
