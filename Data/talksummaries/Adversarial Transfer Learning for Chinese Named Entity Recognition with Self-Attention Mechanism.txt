0	52	The task of named entity recognition (NER) is to recognize the named entities in given text.
2	12	In recent years, numerous methods have been carefully studied for NER task, including Hidden Markov Models (HMMs) (Bikel et al., 1997), Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001).
3	27	Currently, with the development of deep learning, neural networks (Lample et al., 2016; Peng and Dredze, 2016; Luo and Yang, 2016) have been introduced to NER task.
4	27	All these methods need to determine entities boundaries and classify them into pre-defined categories.
5	11	Although great improvements have been achieved by these methods on Chinese NER task, some issues still have not been well addressed.
7	26	Weibo NER dataset (Peng and Dredze, 2015; He and Sun, 2017a) and Sighan2006 NER dataset (Levow, 2006) are two widely used datasets for Chinese NER task, containing 1.3k and 45k training examples, respectively.
8	19	On the two datasets, the highest F1 scores are 48.41% and 89.21%, respectively.
12	48	As shown in Figure 1, given a sentence “ » ¯ : : (Hilton leaves Houston Airport)”, the two tasks have the same boundaries for some words such as “ (Hilton)” and “» (leaves)”, while Chinese NER has more coarse-grained boundaries than CWS task for certain word such as “ ¯ :: (Houston Airport)” in the example of Figure 1, which we call task-specific information.
15	25	For example, the CWS task splits “ ¯ :: (Houston Airport)” into “ ¯ (Houston)” and “:: (Airport)”, while the NER task takes “ ¯ :: (Houston Airport)” as a whole entity.
16	21	Thus, how to exploit task-shared information and prevent the noise brought by CWS task to Chinese NER task is a challenging problem.
17	15	Another issue is that most proposed models cannot explicitly model long range dependencies when predicting entity type.
20	14	However, when the model explicitly captures the dependencies between “ (Hilton)” and “» (leaves)”, it is easy to classify “ (Hilton)” into “person” category.
57	12	The architecture of our proposed model is illustrated in Figure 2.
59	22	In the following sections, we will describe each part of our proposed model in detail.
60	15	Similar to other neural network models, the first step of our proposed model is to map discrete characters into the distributed representations.
68	33	As shown in Figure 2, we propose a sharedprivate feature extractor, which assigns a private BiLSTM layer and shared BiLSTM layer for task k ∈ {NER,CWS}.
71	21	Inspired by the self-attention applied to machine translation (Vaswani et al., 2017) and semantic role labelling (Tan et al., 2017), we exploit selfattention to explicitly learn the dependencies between any two characters in sentence and capture the inner structure information of sentence.
100	23	We propose a task discriminator to estimate which task the sentence comes from.
116	14	We use Adam (Kingma and Ba, 2014) algorithm to optimize the final loss function.
117	17	Since Chinese NER task and CWS task may have different convergence rate, we repeat the above iterations until early stopping according to the Chinese NER task performance.
118	16	To evaluate our proposed model on Chinese NER, we experiment on two different widely used datasets, including Weibo NER dataset (WeiboNER) (Peng and Dredze, 2015; He and Sun, 2017a) and SIGHAN2006 NER dataset (SighanNER) (Levow, 2006).
119	24	We use the MSR dataset (from SIGHAN2005) for CWS task.
125	13	Table 1 gives the details of the three datasets.
143	11	In the second block of Table 2, we report the performance of the main model and baselines proposed by Peng and Dredze (2016).
175	24	From the experimental results of Table 5, we have following observations: • Effectiveness of transfer learning.
177	18	• Effectiveness of adversarial training.
194	53	While the greater improvement on WeiboNER dataset proves that our method is helpful to solve the problem.
195	20	In this paper, we propose a novel adversarial transfer learning framework for Chinese NER task, which can exploit task-shared word boundaries features and prevent the specific information of CWS task.
196	26	Besides, we introduce self-attention mechanism to capture the dependencies of arbitrary two characters and learn the inner structure information of sentence.
197	32	Experiments on two different widely used datasets demonstrate that our method significantly and consistently outperforms previous state-of-the-art models.
