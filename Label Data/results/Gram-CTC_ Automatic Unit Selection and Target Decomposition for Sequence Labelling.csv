0,1,label2,summary_sentences
"ar X
iv :1
80 2.
06 09
3v 4
[ cs
.L G",text,[0],[0]
"Residual networks (He et al., 2016) are deep neural networks in which, roughly, subnetworks determine how a feature transformation should differ from the identity, rather than how it should differ from zero.",1 Introduction,[0],[0]
"After enabling the winning entry in the ILSVRC 2015 classification task, they have become established as a central idea in deep networks.
",1 Introduction,[0],[0]
Hardt & Ma (2017) provided a theoretical analysis that shed light on residual networks.,1 Introduction,[0],[0]
"They showed that (a) any linear transformation with a positive determinant and a bounded condition number can be approximated by a “deep linear network” of the form f(x) = ΘLΘL−1...Θ1x, where,
for large L, each layer Θi is close to the identity, and (b) for networks that compose near-identity transformations this way, if the excess loss is large, then the gradient is steep.",1 Introduction,[0],[0]
"Bartlett et al. (2018) extended both results to the nonlinear case, showing that any smooth, bi-Lipschitz map can be represented as a composition of near-identity functions, and that a suboptimal loss in a composition of near-identity functions implies that the functional gradient of the loss with respect to a function in the composition cannot be small.",1 Introduction,[0],[0]
"These results are interesting because they suggest that, in many cases, this non-convex objective may be efficiently optimized through gradient descent if the layers stay close to the identity, possibly with the help of a regularizer.
",1 Introduction,[0],[0]
"This paper describes and analyzes such algorithms for linear regression with d input variables and d response variables with respect to the quadratic loss, the same setting analyzed by Hardt and Ma.",1 Introduction,[0],[0]
We abstract away sampling issues by analyzing an algorithm that performs gradient descent with respect to the population loss.,1 Introduction,[0],[0]
We focus on the case that the distribution on the input patterns is isotropic.,1 Introduction,[0],[0]
(The data may be transformed through a preprocessing step to satisfy this constraint.),1 Introduction,[0],[0]
"
",1 Introduction,[0],[0]
"The traditional analysis of convex optimization algorithms (see Boyd & Vandenberghe, 2004) provides a bound in terms of the quality of the initial solution, together with bounds on the eigenvalues of the Hessian of the loss.",1 Introduction,[0],[0]
"For the non-convex problem of this paper, we show that if gradient descent starts at the identity in each layer, and if the excess loss of that initial solution is bounded by a constant, then the Hessian remains well-conditioned enough throughout training for successful learning.",1 Introduction,[0],[0]
"Specifically, there is a constant c0 such that, if the excess loss of the identity (over the least squares linear map) is at most c0, then back-propagation initialized at the identity in each layer achieves loss within at most ǫ of optimal in time polynomial in log(1/ǫ), d, and L (Section 3).",1 Introduction,[0],[0]
"On the other hand, we show that there is a constant c1 and a least squares matrix Φ such that the identity has excess loss c1 with respect to Φ, but backpropagation with identity initialization fails to learn Φ (Section 6).
",1 Introduction,[0],[0]
"We also show that if the least squares matrix Φ is symmetric positive definite then gradient descent with identity initialization achieves excess loss at most ǫ in a number of steps bounded by a polynomial in log(d/ǫ), L and the condition number of Φ (Section 4).
",1 Introduction,[0],[0]
"In contrast, for any least squares matrix Φ that is symmetric but has a negative eigenvalue, we show that no such guarantee is possible for a wide variety of algorithms of this type: the excess loss is forever bounded below by the square of this negative eigenvalue.",1 Introduction,[0],[0]
"This holds for step-and-project algorithms, and also algorithms that initialize to the identity and regularize by early stopping or penalizing ∑
i ||Θi − I|| 2 F (Section 6).",1 Introduction,[0],[0]
"Both this and the previous impossibility result can be
proved using a least squares matrix Φ with a positive determinant and a good condition number.",1 Introduction,[0],[0]
"Recall that such Φ were proved by Hardt and Ma to have a good approximation as a product of near-identity matrices – we prove that gradient descent cannot learn them, even with the help of regularizers that reward near-identity representations.
",1 Introduction,[0],[0]
"In Section 5 we provide a convergence guarantee for a least squares matrix Φ that may not be symmetric, but satisfies the positivity condition u⊤Φu",1 Introduction,[0],[0]
> γ for some γ > 0,1 Introduction,[0],[0]
that appears in the bounds.,1 Introduction,[0],[0]
We call such matrices γ-positive.,1 Introduction,[0],[0]
Such Φ include rotations by acute angles.,1 Introduction,[0],[0]
"In this case, we consider an algorithm that regularizes in addition to a near-identity initialization.",1 Introduction,[0],[0]
"After the gradient update, the algorithm performs what we call power projection, projecting its hypothesis ΘLΘL−1...Θ1 onto the set of γ-positive matrices.",1 Introduction,[0],[0]
"Second, it “balances” Θ1, ...,ΘL so that, informally, they contribute equally to ΘLΘL−1...Θ1.",1 Introduction,[0],[0]
(See Section 5 for the details.),1 Introduction,[0],[0]
"We view this
regularizer as a theoretically tractable proxy for regularizers that promote positivity and balance between layers by adding penalties.
",1 Introduction,[0],[0]
"While, in practice, deep networks are non-linear, analysis of the linear case can provide a tractable way to gain insight through rigorous theoretical analysis (Saxe et al., 2013; Kawaguchi, 2016; Hardt & Ma, 2017).",1 Introduction,[0],[0]
We might view back-propagation in the non-linear case as an approximation to a procedure that locally modifies the function computed by each layer in a manner that reduces the loss as fast as possible.,1 Introduction,[0],[0]
"If a non-linear network is obtained by composing transformations, each of which is chosen from a Hilbert space of functions (as in Daniely et al. (2016)), then a step in “function space” corresponds to a step in an (infinite-dimensional) linear space of functions.
",1 Introduction,[0],[0]
Related work.,1 Introduction,[0],[0]
The motivation for this work comes from the papers of Hardt & Ma (2017) and Bartlett et al. (2018).,1 Introduction,[0],[0]
Saxe et al. (2013) studied the dynamics of a continuous-time process obtained by taking the step size of backpropagation applied to deep linear neural networks to zero.,1 Introduction,[0],[0]
Kawaguchi (2016) showed that deep linear neural networks have no suboptimal local minima.,1 Introduction,[0],[0]
"In the case that L = 2, the problem studied here has a similar structure as problems arising from low-rank approximation of matrices, especially as regards algorithms that approximate a matrix A by iteratively improving an approximation of the form UV .",1 Introduction,[0],[0]
"For an interesting survey on the rich literature on these algorithms, please see Ge et al. (2017a); successful algorithms have included a regularizer that promotes balance in the sizes of U and V .",1 Introduction,[0],[0]
"Taghvaei et al. (2017) studied the properties of critical points on the loss when learning deep linear neural networks in the presence of a weight decay regularizer; they studied networks that transform the input to the output through a process indexed by a continuous variable, instead of through discrete layers.",1 Introduction,[0],[0]
"Lee et al. (2016) showed that, given regularity conditions, for a random initialization, gradient descent converges to a local minimizer almost surely; while their paper yields useful insights, their regularity condition does not hold for our problem.",1 Introduction,[0],[0]
Many papers have analyzed learning of neural networks with non-linearities.,1 Introduction,[0],[0]
The papers most closely related to this work analyze algorithms based on gradient descent.,1 Introduction,[0],[0]
"Some of these (Andoni et al., 2014; Brutzkus & Globerson, 2017; Ge et al., 2017b; Li & Yuan, 2017; Zhong et al., 2017; Zhang et al., 2018; Brutzkus et al., 2018; Ge et al., 2018) analyze constant-depth networks.",1 Introduction,[0],[0]
Daniely (2017) showed that stochastic gradient descent learns a subclass of functions computed by log-depth networks in polynomial time; this class includes constant-degree polynomials with polynomially bounded coefficients.,1 Introduction,[0],[0]
"Other theoretical treatments of neural network learning algorithms include Lee et al. (1996); Arora et al. (2014); Livni et al. (2014); Janzamin et al. (2015); Safran & Shamir (2016); Zhang et al. (2016); Nguyen & Hein (2017); Zhang et al. (2017); Orhan & Pitkow (2018), although these are less closely related.
",1 Introduction,[0],[0]
Our three upper bound analyses combine a new upper bound on the operator norm of the Hessian of a deep linear network with the result of Hardt and Ma that gradients are lower bounded in terms of the loss for near-identity matrices.,1 Introduction,[0],[0]
They otherwise have different outlines.,1 Introduction,[0],[0]
The bound in terms of the loss of the initial solution proceeds by showing that the distance from each layer to the identity grows slowly enough that the loss is reduced before the layers stray far enough to harm the conditioning of the Hessian.,1 Introduction,[0],[0]
"The bound for symmetric positive definite matrices proceeds by showing that, in this case, all of the layers are the same, and each of their eigenvalues converges to the Lth root of a corresponding eigenvalue of Φ. As mentioned above, the bound for γ-positive matrices Φ is for an algorithm that achieves favorable conditioning through regularization.
",1 Introduction,[0],[0]
"We expect that the theoretical analysis reported here will inform the design of practical algorithms
for learning non-linear deep networks.",1 Introduction,[0],[0]
One potential avenue for this arises from the fact that the leverage provided by regularizing toward the identity appears to already be provided by a weaker policy of promoting the property that the composition of layers is (potentially asymmetric) positive definite.,1 Introduction,[0],[0]
"Also, balancing singular values of the layers of the network aided our analysis; an analogous balancing of Jacobians associated with various layers may improve conditioning in practice in the non-linear case.",1 Introduction,[0],[0]
"For a joint distribution P with support contained in ℜd × ℜd and g : ℜd → ℜd, define ℓP (g) = E(X,Y )∼P (||g(X)",2.1 Setting,[0],[0]
− Y || 2/2).,2.1 Setting,[0],[0]
"We focus on the case that, for (X,Y ) drawn from P , the marginal on X is isotropic, with",2.1 Setting,[0],[0]
EXX⊤ = Id.,2.1 Setting,[0],[0]
"For convenience, we assume that Y = ΦX for Φ ∈ ℜd×d.",2.1 Setting,[0],[0]
This assumption is without loss of generality: if Φ is the least squares matrix (so that f defined by f(X) = ΦX minimizes ℓP,2.1 Setting,[0],[0]
"(f) among linear functions), for any linear g we have
ℓP (g) =",2.1 Setting,[0],[0]
E‖g(X),2.1 Setting,[0],[0]
− f(X)‖ 2/2 + E‖f(X)−,2.1 Setting,[0],[0]
"Y ‖2/2
+ E",2.1 Setting,[0],[0]
((g(X),2.1 Setting,[0],[0]
− f(X))(f(X),2.1 Setting,[0],[0]
"− Y ))
= E‖g(X)",2.1 Setting,[0],[0]
− f(X)‖2/2 +,2.1 Setting,[0],[0]
"E‖f(X)− Y ‖2/2
= E‖g(X)",2.1 Setting,[0],[0]
− ΦX)‖2/2 + E‖ΦX,2.1 Setting,[0],[0]
"− Y ‖2/2,
since f is the projection of Y onto the set of linear functions ofX.",2.1 Setting,[0],[0]
"So assuming Y = ΦX corresponds to setting Φ as the least squares matrix and replacing the loss ℓP (g) by the excess loss
E‖g(X)",2.1 Setting,[0],[0]
− ΦX‖2/2 = E‖g(X),2.1 Setting,[0],[0]
− Y ‖2/2−,2.1 Setting,[0],[0]
E‖ΦX,2.1 Setting,[0],[0]
"− Y ‖2/2.
",2.1 Setting,[0],[0]
We study algorithms that learn linear mappings parameterized by deep networks.,2.1 Setting,[0],[0]
"The network with L layers and parameters Θ = (Θ1, . . .",2.1 Setting,[0],[0]
",ΘL) computes the parameterized function fΘ(x) = ΘLΘL−1 · · ·Θ1x, where",2.1 Setting,[0],[0]
"x ∈ ℜd and Θi ∈ ℜd×d.
",2.1 Setting,[0],[0]
"We use the notation Θi:j = ΘjΘj−1 · · ·Θi for i ≤ j, so that we can write fΘ(x) = Θ1:Lx = Θi+1:LΘiΘ1:i−1x.
",2.1 Setting,[0],[0]
"When there is no possibility of confusion, we will sometimes refer to loss ℓ(fΘ) simply as ℓ(Θ).",2.1 Setting,[0],[0]
"Because the distribution of X is isotropic, ℓ(Θ) = 12 ||Θ1:L − Φ|| 2 F with respect to least squares matrix Φ. When Θ is produced by an iterative algorithm, will we also refer to loss of the tth iterate by ℓ(t).
",2.1 Setting,[0],[0]
Definition 1.,2.1 Setting,[0],[0]
"For γ > 0, a matrix A ∈ ℜd×d is γ-positive if, for all unit length u, we have u⊤Au > γ.",2.1 Setting,[0],[0]
"We use ||A||F for the Frobenius norm of matrix A, ||A||2 for its operator norm, and σmin(A) for its least singular value.",2.2 Tools and background,[0],[0]
"For vector v, we use ||v|| for its Euclidian norm.
",2.2 Tools and background,[0],[0]
"For a matrix A and a matrix-valued function B, define DAB(A) to be the matrix with
(DAB(A))i,j = ∂vec(B(A))i ∂vec(A)j ,
where vec(A) is the column vector constructed by stacking the columns of A. We use Td,d to denote the d2 × d2 permutation matrix mapping vec(A) to vec(A⊤) for A ∈ ℜd×d.",2.2 Tools and background,[0],[0]
"For A ∈ ℜn×m and B ∈ ℜp×q, A⊗B denotes the Kronecker product, that is, the np×mq matrix of n×m blocks, with the i, jth block given by AijB.
We will need the gradient and Hessian of ℓ. (The gradient, which can be computed using backprop, is of course well known.)",2.2 Tools and background,[0],[0]
"The proof is in Appendix A.
Lemma 1.
DΘiℓ (fΘ)=(vec(Id))",2.2 Tools and background,[0],[0]
"⊤ (( Θ⊤1:i−1 ⊗ (Θ1:L−Φ) ⊤Θi+1:L ))
",2.2 Tools and background,[0],[0]
"= vec(G)⊤,
where G is the d× d matrix given by
G def = Θ⊤i+1:",2.2 Tools and background,[0],[0]
"L (Θ1:L − Φ)Θ ⊤ 1:i−1. (1)
",2.2 Tools and background,[0],[0]
"For i < j,
DΘjDΘiℓ (fΘ) =",2.2 Tools and background,[0],[0]
(Id2 ⊗ (vec(Id)) ⊤),2.2 Tools and background,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 )
",2.2 Tools and background,[0],[0]
"((Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1)Td,d + (Θ ⊤ i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1:L)).
DΘiDΘiℓ (fΘ)",2.2 Tools and background,[0],[0]
=,2.2 Tools and background,[0],[0]
(Id2 ⊗ (vec(Id)) ⊤),2.2 Tools and background,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 )
(
Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1
)
",2.2 Tools and background,[0],[0]
"Td,d.",2.2 Tools and background,[0],[0]
"In this section, we prove an upper bound for gradient descent in terms of the loss of the initial solution.",3 Targets near the identity,[0],[0]
"First, set Θ(0) =",3.1 Procedure and upper bound,[0],[0]
"(I, I, ..., I), and then iteratively update
Θ (t+1)",3.1 Procedure and upper bound,[0],[0]
i = Θ,3.1 Procedure and upper bound,[0],[0]
(t),3.1 Procedure and upper bound,[0],[0]
"i − η(Θ (t) i+1:L)
⊤ (
Θ (t) 1:",3.1 Procedure and upper bound,[0],[0]
"L − Φ
)
",3.1 Procedure and upper bound,[0],[0]
"(Θ (t) 1:i−1) ⊤.
Theorem 1.",3.1 Procedure and upper bound,[0],[0]
"There are positive constants c1 and c2 and polynomials p1 and p2 such that, if ℓ(Θ (0) 1:L) ≤ c1, L ≥ c2, and η ≤ 1 p1(L,d,||Φ||2) , then the above gradient descent procedure achieves
ℓ(fΘ(t)) ≤ ǫ within t = p2
(
1 η
)",3.1 Procedure and upper bound,[0],[0]
"ln (
ℓ(0) ǫ
)
iterations.",3.1 Procedure and upper bound,[0],[0]
"The following lemma, which is implicit in the proof of Theorem 2.2 in Hardt & Ma (2017), shows that the gradient is steep if the loss is large and the singular values of the layers are not too small.
",3.2 Proof of Theorem 1,[0],[0]
Lemma 2 (Hardt & Ma 2017).,3.2 Proof of Theorem 1,[0],[0]
Let ∇Θℓ(Θ) be the gradient of ℓ(Θ) with respect to any flattening of Θ.,3.2 Proof of Theorem 1,[0],[0]
"If, for all layers i, σmin(Θi) ≥ 1− a, then ||∇Θℓ(Θ)|| 2 ≥ 4ℓ(Θ)L(1− a)2L.
Next, we show that, if Θ(t) and Θ(t+1) are both close to the identity, then the gradient is not changing very fast between them, so that rapid progress continues to be made.",3.2 Proof of Theorem 1,[0],[0]
"We prove this through an upper bound on the operator norm of the Hessian that holds uniformly over members of a ball around the identity, which in turn can be obtained through a bound on the Frobenius norm.",3.2 Proof of Theorem 1,[0],[0]
"The proof is in Appendix B.
Lemma 3.",3.2 Proof of Theorem 1,[0],[0]
"Choose an arbitrary Θ with ||Θi||2 ≤ 1 + z for all i, and least squares matrix Φ with ||Φ||2 ≤ (1 + z)
L.",3.2 Proof of Theorem 1,[0],[0]
"Let ∇2 be the Hessian of ℓ(fΘ) with respect to an arbitrary flattening of the parameters of Θ. We have
||∇2||F ≤ 3Ld 5(1 + z)2L.
Armed with Lemmas 2 and 3, let us now analyze gradient descent.",3.2 Proof of Theorem 1,[0],[0]
"Very roughly, our strategy will be to show that the distance from the identity to the various layers grows slowly enough for the leverage from Lemmas 2 and 3 to enable successful learning.",3.2 Proof of Theorem 1,[0],[0]
Let R(Θ) = maxi ||Θi − I||2.,3.2 Proof of Theorem 1,[0],[0]
"From the update, we have
||Θ (t+1)",3.2 Proof of Theorem 1,[0],[0]
i − I||2 ≤ ||Θ (t) i,3.2 Proof of Theorem 1,[0],[0]
"− I||2 + η||(Θ (t) i+1:L)
⊤ (
Θ (t) 1:L − Φ
)
(Θ (t) 1:i−1) ⊤||2
≤ ||Θ (t) i",3.2 Proof of Theorem 1,[0],[0]
− I||2,3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
+R(Θ (t)))L||Θ (t) 1:L − Φ||2 ≤ ||Θ,3.2 Proof of Theorem 1,[0],[0]
(t),3.2 Proof of Theorem 1,[0],[0]
i,3.2 Proof of Theorem 1,[0],[0]
− I||2,3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
"+R(Θ (t)))L||Θ (t) 1:L − Φ||F .
",3.2 Proof of Theorem 1,[0],[0]
If R(t) = maxs≤tR(Θ(s)),3.2 Proof of Theorem 1,[0],[0]
(so R(0) = 0) and ℓ(t) = 12 ||Θ (t) 1:,3.2 Proof of Theorem 1,[0],[0]
"L − Φ|| 2 F , this implies
R(t+ 1) ≤",3.2 Proof of Theorem 1,[0],[0]
R(t),3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
+R(t))L √ 2ℓ(t).,3.2 Proof of Theorem 1,[0],[0]
"(2)
By Lemma 3, for all Θ on the line segment from Θ(t) to Θ(t+1), we have
||∇2Θ||2 ≤ ||∇ 2 Θ||F ≤",3.2 Proof of Theorem 1,[0],[0]
"3Ld 5 max{(1 +R(t+ 1))2L, ||Φ||22},
so that
ℓ(t+ 1) ≤ ℓ(t)− η||∇Θ(t)",3.2 Proof of Theorem 1,[0],[0]
||,3.2 Proof of Theorem 1,[0],[0]
"2 +
3 2",3.2 Proof of Theorem 1,[0],[0]
η2Ld5 max{(1,3.2 Proof of Theorem 1,[0],[0]
"+R(t+ 1))2L, ||Φ||22}||∇Θ(t) || 2.
",3.2 Proof of Theorem 1,[0],[0]
"Thus, if we ensure
η ≤ 1
3Ld5 max{(1 +R(t+ 1))2L, ||Φ||22} , (3)
we have ℓ(t+ 1) ≤ ℓ(t)− (η/2)||∇Θ(t) || 2, which, using Lemma 2, gives
ℓ(t+ 1) ≤ ( 1− 2ηL(1 −R(t))2L ) ℓ(t).",3.2 Proof of Theorem 1,[0],[0]
"(4)
Pick any c ≥ 1.",3.2 Proof of Theorem 1,[0],[0]
"Assume that L ≥ (4/3) ln c = c2, ℓ(Θ (0) 1:L) ≤
ln(c)2
8c10 = c1 and η ≤ 1
3Ld5 max{c4,||Φ||22} .
",3.2 Proof of Theorem 1,[0],[0]
"We claim that, for all t ≥ 0,
1. R(t) ≤ ηc √ 2ℓ(0) ∑ 0≤s<t exp ( − sηLc4 )
2.",3.2 Proof of Theorem 1,[0],[0]
"ℓ(t) ≤ ( exp (
−2tηL c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"The base case holds as R(0) = 0 and ℓ(0) = ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"Before starting the inductive step, notice that for any t ≥ 0,
ηc √ 2ℓ(0) ∑
0≤s<t exp
(
− sηL
c4
)
≤ ηc √ 2ℓ(0)× 1
1− exp (
−ηL c4
)
≤ ηc √ 2ℓ(0)× 2c4
ηL (since ηLc4 ≤ 1)
= 2c5 √ 2ℓ(0)
L ≤
ln c
L ≤ 3/4
where the last two inequalities follow from the constraints on ℓ(0) and L.
Using (2),
R(t+ 1) ≤ R(t) + η(1",3.2 Proof of Theorem 1,[0],[0]
"+R(t))L √ 2ℓ(t)
≤",3.2 Proof of Theorem 1,[0],[0]
R(t),3.2 Proof of Theorem 1,[0],[0]
"+ η
(
1 + ln c
L
)L √
2ℓ(t)
≤ R(t) + ηc √ 2ℓ(t)
≤ R(t) + ηc √ 2ℓ(0) exp
(
− tηL
c4
)
≤ ηc √ 2ℓ(0)",3.2 Proof of Theorem 1,[0],[0]
"∑
0≤s<t+1 exp
(
− sηL
c4
)
.
",3.2 Proof of Theorem 1,[0],[0]
"Since R(t+ 1) ≤ ln cL , the choice of η satisfies (3), so
ℓ(t+ 1) ≤ ( 1− 2ηL(1 −R(t))2L ) ℓ(t).
",3.2 Proof of Theorem 1,[0],[0]
"Now consider (1−R(t))2L:
ln ( (1−R(t))2L )",3.2 Proof of Theorem 1,[0],[0]
"= 2L ln(1−R(t))
",3.2 Proof of Theorem 1,[0],[0]
≥ 2L(−2R(t)),3.2 Proof of Theorem 1,[0],[0]
since R(t) ∈,3.2 Proof of Theorem 1,[0],[0]
"[0, 3/4]
≥ 2L
(
−2 ln c
L
)
since R(t) ≤",3.2 Proof of Theorem 1,[0],[0]
ln,3.2 Proof of Theorem 1,[0],[0]
"c
L
(1−R(t))2L ≥ 1/c4.
",3.2 Proof of Theorem 1,[0],[0]
"Using this in the bound on ℓ(t+ 1):
ℓ(t+ 1) ≤",3.2 Proof of Theorem 1,[0],[0]
"( 1− 2ηL(1 −R(t))2L ) ℓ(t)
≤
(
1− 2ηL
c4
)
ℓ(t)
≤
(
exp
(
− 2ηL
c4
))",3.2 Proof of Theorem 1,[0],[0]
"(
exp
(
− 2tηL
c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0)
",3.2 Proof of Theorem 1,[0],[0]
"=
(
exp
(
− 2(t+ 1)ηL
c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"Solving ℓ(0) exp (
−2tηL c4
)
≤ ǫ for t and recalling that η < 1/c4 completes the proof of the theorem.",3.2 Proof of Theorem 1,[0],[0]
"In this section, we analyze the procedure of Section 3.1 when the least squares matrix Φ is symmetric and positive definite.
Theorem 2.",4 Symmetric positive definite targets,[0],[0]
"There is an absolute positive constant c3 such that, if Φ is symmetric and γ-positive with 0 <",4 Symmetric positive definite targets,[0],[0]
"γ < 1, and L ≥ c3 ln (||Φ||2/γ), then for all η ≤
1 L(1+||Φ||22) , gradient descent achieves
ℓ(fΘ(t))",4 Symmetric positive definite targets,[0],[0]
"≤ ǫ in poly(L, ||Φ||2/γ, 1/η) log(d/ǫ) iterations.
",4 Symmetric positive definite targets,[0],[0]
Note that a symmetric matrix is γ-positive when its minimum eigenvalue is at least γ.,4 Symmetric positive definite targets,[0],[0]
"Let Φ be a symmetric, real, γ-positive matrix with γ > 0, and let Θ(0),Θ(1), ... be the iterates of gradient descent with a step size 0 < η ≤ 1
L(1+||Φ||22) .
",4.1 Proof of Theorem 2,[0],[0]
Definition 2.,4.1 Proof of Theorem 2,[0],[0]
"Symmetric matrices A ⊆ ℜd×d are commuting normal matrices if there is a single unitary matrix U such that for all A ∈ A, U⊤AU is diagonal.
",4.1 Proof of Theorem 2,[0],[0]
"We will use the following well-known facts about commuting normal matrices.
",4.1 Proof of Theorem 2,[0],[0]
Lemma 4 (Horn & Johnson 2013),4.1 Proof of Theorem 2,[0],[0]
.,4.1 Proof of Theorem 2,[0],[0]
"If A ⊆ ℜd×d is a set of symmetric commuting normal matrices and A,B ∈ A, the following hold:
• AB = BA;
",4.1 Proof of Theorem 2,[0],[0]
"• for all scalars α and β, A ∪ {αA+ βB,AB} are commuting normal;
• there is a unitary matrix U such that U⊤AU and U⊤BU are real and diagonal;
• the multiset of singular values of A is the same as the multiset of magnitudes of its eigenvalues;
• ||A− I||2 is the largest value of |z",4.1 Proof of Theorem 2,[0],[0]
"− 1| for an eigenvalue z of A.
Lemma 5.",4.1 Proof of Theorem 2,[0],[0]
"The matrices {Φ} ∪ {Θ (t) i : i ∈ {1, ..., L}, t ∈ Z +} are commuting normal.",4.1 Proof of Theorem 2,[0],[0]
"For all t, Θ (t) 1 = ... = Θ (t) L .
Proof.",4.1 Proof of Theorem 2,[0],[0]
The proof is by induction.,4.1 Proof of Theorem 2,[0],[0]
"The base case follows from the fact that Φ and I are commuting normal.
",4.1 Proof of Theorem 2,[0],[0]
"For the induction step, the fact that
{Φ} ∪ {
Θ (s)",4.1 Proof of Theorem 2,[0],[0]
i :,4.1 Proof of Theorem 2,[0],[0]
"i ∈ {1, ..., L}, s ≤ t
} ∪ {
Θ (s+1)",4.1 Proof of Theorem 2,[0],[0]
i :,4.1 Proof of Theorem 2,[0],[0]
"i ∈ {1, ..., L}, s ≤ t
}
are commuting normal follows from Lemma 4.",4.1 Proof of Theorem 2,[0],[0]
"The update formula now reveals that Θ (t+1) 1 = ... = Θ (t+1) L .
",4.1 Proof of Theorem 2,[0],[0]
Now we are ready to analyze the dynamics of the learning process.,4.1 Proof of Theorem 2,[0],[0]
"Let Φ = U⊤DLU be a diagonalization of Φ. Let Γ = max{1, ||Φ||2}.",4.1 Proof of Theorem 2,[0],[0]
"We next describe a sense in which gradient descent learns each eigenvalue independently.
",4.1 Proof of Theorem 2,[0],[0]
Lemma 6.,4.1 Proof of Theorem 2,[0],[0]
"For each t, there is a real diagonal matrix D̂(t) such that, for all i, Θ (t)",4.1 Proof of Theorem 2,[0],[0]
"i = U ⊤D̂(t)U and
D̂(t+1) = D̂(t)",4.1 Proof of Theorem 2,[0],[0]
− η(D̂(t))L−1((D̂(t))L −DL).,4.1 Proof of Theorem 2,[0],[0]
"(5)
Proof.",4.1 Proof of Theorem 2,[0],[0]
Lemma 5 implies that there is a single real U such that Θ (t),4.1 Proof of Theorem 2,[0],[0]
"i = U ⊤D̂(t)U for all i. Applying Lemma 1, recalling that Θ (t) 1 = ...",4.1 Proof of Theorem 2,[0],[0]
"= Θ (t) L , and applying the fact that Θ (t) i and Φ commute, we get
Θ (t+1)",4.1 Proof of Theorem 2,[0],[0]
i = Θ,4.1 Proof of Theorem 2,[0],[0]
(t),4.1 Proof of Theorem 2,[0],[0]
"i − η(Θ (t) i )
",4.1 Proof of Theorem 2,[0],[0]
"L−1 (
(Θ (t) i )
",4.1 Proof of Theorem 2,[0],[0]
"L − Φ ) .
",4.1 Proof of Theorem 2,[0],[0]
"Replacing each matrix by its diagonalization, we get
U⊤D̂(t+1)U = U⊤D̂(t)U",4.1 Proof of Theorem 2,[0],[0]
"− η(U⊤(D̂(t))L−1U) ( U⊤(D̂(t))LU − U⊤DLU )
",4.1 Proof of Theorem 2,[0],[0]
"= U⊤D̂(t)U − ηU⊤(D̂(t))L−1 ( (D̂(t))L −DL ) U,
and left-multiplying by U and right-multiplying by U⊤ gives (5).
",4.1 Proof of Theorem 2,[0],[0]
We will now analyze the convergence of each D̂ (t) kk to Dkk separately.,4.1 Proof of Theorem 2,[0],[0]
"Let us focus for now on an arbitrary single index k, let λ = Dkk and λ̂",4.1 Proof of Theorem 2,[0],[0]
(t) =,4.1 Proof of Theorem 2,[0],[0]
"D̂ (t) kk .
",4.1 Proof of Theorem 2,[0],[0]
Recalling that ||Φ||2 ≤,4.1 Proof of Theorem 2,[0],[0]
"Γ, we have γ 1/L ≤ λ ≤",4.1 Proof of Theorem 2,[0],[0]
"Γ1/L. Also, Γ1/L = e 1 L ln Γ ≤ e1/a ≤ 1+2/a whenever a ≥ 1 and L ≥ a ln Γ. Similarly, γ1/L ≥ 1 − a whenever L ≥ a ln(1/γ).",4.1 Proof of Theorem 2,[0],[0]
"Thus, there are absolute constants c3 and c4 such that",4.1 Proof of Theorem 2,[0],[0]
|1−,4.1 Proof of Theorem 2,[0],[0]
"λ| ≤ c4 ln(Γ/γ)
L < 1 for all L ≥ c3 ln(Γ/γ).
",4.1 Proof of Theorem 2,[0],[0]
"We claim that, for all t, λ̂(t) lies between 1 and λ inclusive, so that |λ̂(t)",4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ c4 ln(Γ/γ)L .,4.1 Proof of Theorem 2,[0],[0]
The base case holds because λ̂(t) = 1 and |1,4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ c4 ln(Γ/γ)L .,4.1 Proof of Theorem 2,[0],[0]
Now let us work on the induction step.,4.1 Proof of Theorem 2,[0],[0]
"Applying (5) together with Lemma 1, we get
λ̂(t+1) = λ̂(t) +",4.1 Proof of Theorem 2,[0],[0]
η(λ̂(t))L−1(λL,4.1 Proof of Theorem 2,[0],[0]
− (λ̂(t))L).,4.1 Proof of Theorem 2,[0],[0]
"(6)
By the induction hypothesis, we just need to show that sign(λ̂(t+1)",4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)),4.1 Proof of Theorem 2,[0],[0]
= sign(λ − λ̂(t)) and |λ̂(t+1),4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)| ≤ |λ,4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)| (i.e., the step is in the correct direction, and does not “overshoot”).",4.1 Proof of Theorem 2,[0],[0]
"First, to see that the step is in the right direction, note that λL ≥ (λ̂(t))L if and only if λ ≥ (λ̂(t)), and the inductive hypothesis implies that λ̂(t), and therefore (λ̂(t))L−1, is non-negative.",4.1 Proof of Theorem 2,[0],[0]
To show that |λ̂(t+1),4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)| ≤ |λ,4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)|, it suffices to show that η(λ̂(t))L−1 ∣ ∣ ∣ λL − (λ̂(t))L) ∣ ∣ ∣ ≤ |λ",4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)|,
which, in turn would be implied by η ≤
∣ ∣ ∣ ∣ 1 (λ̂(t))L−1( ∑L−1 i=0 (λ̂ (t))iλL−1−i) ∣ ∣ ∣ ∣ (since λL − (λ̂(t))L = (λ −
λ̂(t))",4.1 Proof of Theorem 2,[0],[0]
"∑L−1 i=0 (λ̂ (t))iλL−1−i), which follows from the inductive hypothesis and η ≤ 1 LΓ2 .",4.1 Proof of Theorem 2,[0],[0]
"We have proved that each λ̂(t) lies between λ and 1, so that |1−",4.1 Proof of Theorem 2,[0],[0]
λ̂(t)| ≤ |1−,4.1 Proof of Theorem 2,[0],[0]
"λ| ≤ c4 ln(Γ/γ).
",4.1 Proof of Theorem 2,[0],[0]
"Now, since the step is in the right direction, and does not overshoot,
|λ̂(t+1)",4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ |λ̂(t) − λ| − η(λ̂(t))L−1|λL,4.1 Proof of Theorem 2,[0],[0]
"− (λ̂(t))L|
≤ |λ̂(t) − λ|
( 1− η(λ̂(t))L−1 ( L−1 ∑
i=0
(λ̂(t))iλL−1−i ))
≤ |λ̂(t)",4.1 Proof of Theorem 2,[0],[0]
"− λ| ( 1− ηLγ2 ) ,
since the fact that λ̂(t) lies between 1 and λ implies that λ̂(t) ≥ γ1/L. Thus, |λ̂(t) − λ| ≤ (
1− ηLγ2 )t c4",4.1 Proof of Theorem 2,[0],[0]
ln(Γ/γ).,4.1 Proof of Theorem 2,[0],[0]
"This implies that, for any ǫ ∈ (0, 1), for any absolute constant c5, there is a
constant c6 such that, after c6 1 ηLγ2 ln ( dL lnΓ γǫ )
steps, we have |λ̂(t)−λ| ≤ c5γ √ ǫ
LΓ √ d .Writing",4.1 Proof of Theorem 2,[0],[0]
"r = λ̂(t)−λ,
this implies, if c5 is small enough, that
((λ̂(t))L − λL)2 = ((λ+r)L−λL)2
≤ Γ2",4.1 Proof of Theorem 2,[0],[0]
"( ( 1+ r
λ
)L −1
)2
≤ Γ2 ( 2c5rL
λ
)2
≤ Γ2 ( 2c5rL
γ
)2
≤ ǫ
d .
",4.1 Proof of Theorem 2,[0],[0]
"Thus, after O (
1 ηLγ2
ln (
dL lnΓ γǫ
))
steps, (Dkk − D̂ (t) kk ) 2 ≤ ǫ/d for all k, and therefore ℓ(Θ(t))",4.1 Proof of Theorem 2,[0],[0]
"≤ ǫ,
completing the proof.",4.1 Proof of Theorem 2,[0],[0]
"We have seen that if the least squares matrix is symmetric, γ-positivity is sufficient for convergence of gradient descent.",5 Asymmetric positive definite matrices,[0],[0]
We shall see in Section 6 that positivity is also necessary for a broad family of gradient-based algorithms to converge to the optimal solution when the least squares matrix is symmetric.,5 Asymmetric positive definite matrices,[0],[0]
"Thus, in the symmetric case, positivity characterizes the success of gradient methods.
",5 Asymmetric positive definite matrices,[0],[0]
"In this section, we show that positivity suffices for the convergence of a gradient method even without the assumption that the least squares matrix is symmetric.
",5 Asymmetric positive definite matrices,[0],[0]
Note that the set of γ-positive (but not necessarily symmetric) matrices includes both rotations by an acute angle and “partial reflections” of the form ax + b refl(x) where refl(·) is a lengthpreserving reflection and 0 ≤,5 Asymmetric positive definite matrices,[0],[0]
|b| < a.,5 Asymmetric positive definite matrices,[0],[0]
"Since ( u⊤Au )⊤
= u⊤A⊤u, a matrix A is γ-positive if",5 Asymmetric positive definite matrices,[0],[0]
"and only if u⊤(A+A⊤)u ≥ 2γ for all unit length u, i.e. A+A⊤ is positive definite with eigenvalues at least 2γ.",5 Asymmetric positive definite matrices,[0],[0]
"The algorithm analyzed in this section uses a construction that is new, as far as we know, that we call a balanced factorization.",5.1 Balanced factorizations,[0],[0]
"This factorization may be of independent interest.
",5.1 Balanced factorizations,[0],[0]
Recall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP .,5.1 Balanced factorizations,[0],[0]
The principal Lth root of a complex number whose expression in polar coordinates is reθi is r1/Leθi/L.,5.1 Balanced factorizations,[0],[0]
"The principal Lth root of a matrix A is the matrix B such that BL = A, and each eigenvalue of B is the principal Lth root of the corresponding eigenvalue of A.
Definition 3.",5.1 Balanced factorizations,[0],[0]
"If A be a matrix with polar decomposition RP , then A has the balanced factorization A = A1, ..., AL where for each i,
Ai = R 1/LPi, with Pi = R (L−i)/LP 1/LR−(L−i)/L,
and each of the Lth roots is the principal Lth root.
",5.1 Balanced factorizations,[0],[0]
The motivation for balanced factorization is as follows.,5.1 Balanced factorizations,[0],[0]
"We want each factor to do a 1/L fraction of the total amount of rotation, and a 1/L fraction of the total amount of scaling.",5.1 Balanced factorizations,[0],[0]
"However, the scaling done by the ith factor should be done in directions that take account of the partial rotations done by the other factors.",5.1 Balanced factorizations,[0],[0]
"The following is the key property of the balanced factorization; its proof is in Appendix C.
Lemma 7.",5.1 Balanced factorizations,[0],[0]
"If σ1, ..., σd are the singular values of A, and A1, ..., AL is a balanced factorization of A, then the following hold: (a) A = ∏L
i=1Ai; (b) for each i ∈ {1, ..., L}, σ 1/L 1 , ..., σ 1/L d are the singular
values of Ai.",5.1 Balanced factorizations,[0],[0]
The following is the power projection algorithm.,5.2 Procedure and upper bound,[0],[0]
"It has a positivity parameter γ > 0, and uses H = {A : ∀u s.t. ||u||",5.2 Procedure and upper bound,[0],[0]
"= 1, u⊤Au ≥ γ} as its “hypothesis space”.",5.2 Procedure and upper bound,[0],[0]
"First, it initializes Θ(0)i = γ 1/LI for all i ∈ {1, ..., L}.",5.2 Procedure and upper bound,[0],[0]
"Then, for each t, it does the following.
",5.2 Procedure and upper bound,[0],[0]
• Gradient Step.,5.2 Procedure and upper bound,[0],[0]
"For each i ∈ {1, ..., L}, update:
Θ (t+1/2)",5.2 Procedure and upper bound,[0],[0]
i = Θ,5.2 Procedure and upper bound,[0],[0]
(t),5.2 Procedure and upper bound,[0],[0]
"i − η(Θ (t) i+1:L)
⊤ (
Θ (t) 1:",5.2 Procedure and upper bound,[0],[0]
"L − Φ
)
",5.2 Procedure and upper bound,[0],[0]
"(Θ (t) 1:i−1) ⊤.
• Power Project.",5.2 Procedure and upper bound,[0],[0]
Compute the projection Ψ(t+1/2) (w.r.t.,5.2 Procedure and upper bound,[0],[0]
"the Frobenius norm) of Θ (t+1/2) 1:L
onto H.
• Factor.",5.2 Procedure and upper bound,[0],[0]
"Let Θ (t+1) 1 , ...,Θ (t+1) L be the balanced factorization of Ψ (t+1/2), so that Ψ(t+1/2) =
Θ (t+1) 1:L .
",5.2 Procedure and upper bound,[0],[0]
Theorem 3.,5.2 Procedure and upper bound,[0],[0]
For any Φ such that u⊤Φu >,5.2 Procedure and upper bound,[0],[0]
"γ for all unit-length u, the power projection algorithm produces Θ(t) with ℓ(Θ(t))",5.2 Procedure and upper bound,[0],[0]
"≤ ǫ in poly(d, ||Φ||F , 1 γ ) log(1/ǫ) iterations.",5.2 Procedure and upper bound,[0],[0]
Lemma 8.,5.3 Proof of Theorem 3,[0],[0]
"For all t, Θ (t) 1:L ∈ H.
Proof.",5.3 Proof of Theorem 3,[0],[0]
"Θ (0) 1:L = γI ∈ H, and, for all t, Ψ (t+1/2) is obtained by projection onto H, and Θ (t+1) 1:L = Ψ(t+1/2).
",5.3 Proof of Theorem 3,[0],[0]
Definition 4.,5.3 Proof of Theorem 3,[0],[0]
The exponential of a matrix A is exp(A),5.3 Proof of Theorem 3,[0],[0]
"def = ∑∞
k=0 1 k!A k, and B is a logarithm of A if A = exp(B).
",5.3 Proof of Theorem 3,[0],[0]
Lemma 9 (Culver 1966).,5.3 Proof of Theorem 3,[0],[0]
"A real matrix has a real logarithm if and only if it is invertible and each Jordan block belonging to a negative eigenvalue occurs an even number of times.
",5.3 Proof of Theorem 3,[0],[0]
Lemma 10.,5.3 Proof of Theorem 3,[0],[0]
"For all t, Θ (t) 1:L has a real Lth root.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
Since Θ (t) 1:L ∈ H implies u,5.3 Proof of Theorem 3,[0],[0]
⊤Θ(t)1,5.3 Proof of Theorem 3,[0],[0]
:Lu > 0,5.3 Proof of Theorem 3,[0],[0]
"for all u, Θ (t) 1:L does not have a negative eigenvalue and is invertible.",5.3 Proof of Theorem 3,[0],[0]
"By Lemma 9, Θ (t) 1:L has a real logarithm.",5.3 Proof of Theorem 3,[0],[0]
"Thus, its real Lth root can be constructed via exp(log(Θ (t) 1:L)/L).
",5.3 Proof of Theorem 3,[0],[0]
"The preceding lemma implies that the algorithm is well-defined, since all of the required roots can be calculated.
",5.3 Proof of Theorem 3,[0],[0]
Lemma 11.,5.3 Proof of Theorem 3,[0],[0]
"H is convex.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
"Suppose A and B are in H and λ ∈ (0, 1).",5.3 Proof of Theorem 3,[0],[0]
"We have
u⊤(λA+ (1− λ)B)u = λu⊤Au+ (1− λ)u⊤Bu ≥ γ.
Lemma 12.",5.3 Proof of Theorem 3,[0],[0]
"For all A ∈ H, σmin(A) ≥ γ.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
"Let u and v be singular vectors such that u⊤Av = σmin(A).
",5.3 Proof of Theorem 3,[0],[0]
"γ ≤ v⊤Av = σmin(A)v ⊤u ≤ σmin(A).
",5.3 Proof of Theorem 3,[0],[0]
Lemma 13.,5.3 Proof of Theorem 3,[0],[0]
"For all t, σmin(Θ (t) i )",5.3 Proof of Theorem 3,[0],[0]
"≥ γ 1/L.
Proof.",5.3 Proof of Theorem 3,[0],[0]
"First, σmin(Θ (0) i ) =",5.3 Proof of Theorem 3,[0],[0]
γ 1/L ≥ γ1/L. Now consider t > 0.,5.3 Proof of Theorem 3,[0],[0]
"Since Ψ(t−1/2) was projected into H, we have σmin(Ψ(t−1/2))",5.3 Proof of Theorem 3,[0],[0]
≥ γ.,5.3 Proof of Theorem 3,[0],[0]
"Lemma 7 then completes the proof.
",5.3 Proof of Theorem 3,[0],[0]
"Define U(t) = max {
maxs≤tmaxi ||Θ (s) i ||2, ||Φ|| 1/L 2
}
, B(t) = mins≤tmini σmin(Θ (s) i ), and recall that
ℓ(t) = ||Θ (t) 1:L − Φ|| 2 F .
",5.3 Proof of Theorem 3,[0],[0]
"Arguing as in the initial portion of Section 3.2, as long as
η ≤ 1
3Ld5U(t)2L (7)
we have ℓ(t + 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
( 1− ηLB(t)2L ) ℓ(t) (see Equation 4).,5.3 Proof of Theorem 3,[0],[0]
"Lemma 13 gives B(t) ≥ γ1/L, so ℓ(t+ 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
"( 1− ηLγ2 )
ℓ(t).",5.3 Proof of Theorem 3,[0],[0]
"Since Ψ(t+1/2) is the projection of Θ (t+1/2) 1:L onto a convex set H that
contains Φ, and Θ (t+1) 1:L = Ψ (t+1/2), (7) implies
ℓ(t+ 1) ≤ ℓ(t+ 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
( 1− ηLγ2 ) ℓ(t).,5.3 Proof of Theorem 3,[0],[0]
"(8)
Next, we prove an upper bound on U .
",5.3 Proof of Theorem 3,[0],[0]
Lemma 14.,5.3 Proof of Theorem 3,[0],[0]
"For all t, U(t) ≤ ( √
ℓ(t) + ||Φ||F
)1/L .
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
Recall that ℓ(t) = ||Θ (t) 1:L−Φ|| 2 F .,5.3 Proof of Theorem 3,[0],[0]
"By the triangle inequality, ||Θ (t) 1:L||F ≤",5.3 Proof of Theorem 3,[0],[0]
√ ℓ(t)+ ||Φ||F .,5.3 Proof of Theorem 3,[0],[0]
Thus ||Θ (t) 1:L||2 ≤ √ ℓ(t) + ||Φ||F .,5.3 Proof of Theorem 3,[0],[0]
"By Lemma 7, for all i, we have ||Θ (t)",5.3 Proof of Theorem 3,[0],[0]
i ||2 ≤ ( √ ℓ(t) + ||Φ||F )1/L .,5.3 Proof of Theorem 3,[0],[0]
"Since ||Φ||2 ≤ ||Φ||F , this completes the proof.
",5.3 Proof of Theorem 3,[0],[0]
Note that the triangle inequality implies that ℓ(0) ≤ ||Θ (0) 1:L|| 2 F + ||Φ|| 2 F ≤,5.3 Proof of Theorem 3,[0],[0]
γ 2d,5.3 Proof of Theorem 3,[0],[0]
+,5.3 Proof of Theorem 3,[0],[0]
||Φ||2F .,5.3 Proof of Theorem 3,[0],[0]
Since σmin(Φ) ≥,5.3 Proof of Theorem 3,[0],[0]
"γ, we have ||Φ|| 2 F ≥ γ 2d, so ℓ(t) ≤ 2||Φ||2F and U(t) ≤ (3||Φ||2) 1/L.",5.3 Proof of Theorem 3,[0],[0]
"Now, if we set η = 1 cLd5||Φ||2
F , for a large enough absolute constant c, then (7) is satisfied, so that (8) gives ℓ(t+1) ≤ (
1− γ 2
cd5||Φ||2 F
)
ℓ(t) and the power projection algorithm achieves ℓ(t+ 1) ≤ ǫ after
O
(
d5||Φ||2F γ2 log
(
ℓ(0)
ǫ
))
",5.3 Proof of Theorem 3,[0],[0]
"=O
(
d5||Φ||2F γ2 log
(
||Φ||2F ǫ
))
updates.",5.3 Proof of Theorem 3,[0],[0]
"In this section, we show that positive definite Φ are necessary for several gradient descent algorithms with different kinds of regularization to minimize the loss.",6 Failure,[0],[0]
"One family of algorithms that we will
analyze is parameterized by a function ψ mapping the number of inputs d and the number of layers L to a radius ψ(d, L), step sizes ηt and initialization parameter γ ≥ 0.",6 Failure,[0],[0]
"In particular, a ψ-step-and-project algorithm is any instantiation of the following algorithmic template.
",6 Failure,[0],[0]
Initialize each Θ (0),6 Failure,[0],[0]
"i = γ 1/LI for some γ ≥ 0 and iterate:
• Gradient Step.",6 Failure,[0],[0]
"For each i ∈ {1, ..., L}, update:
Θ (t+1/2)",6 Failure,[0],[0]
i = Θ,6 Failure,[0],[0]
(t),6 Failure,[0],[0]
"i − ηt(Θ (t) i+1:L)
⊤ (
Θ (t) 1:L − Φ
)
(Θ (t) 1:i−1) ⊤.
• Project.",6 Failure,[0],[0]
Set each Θt+1i,6 Failure,[0],[0]
"to the projection of Θ t+1/2 i onto {A : ||A− I||2 ≤ ψ(d, L)}.
",6 Failure,[0],[0]
We will also show that Penalty Regularized Gradient Descent which uses gradient descent with any step sizes ηt on the regularized objective ℓ(Θ) + κ 2 ∑ i ||I,6 Failure,[0],[0]
"−Θ|| 2 F also fails to minimize the loss.
",6 Failure,[0],[0]
"Both results use the simple observation that when Θ1:L and Φ are mutually diagonalizable then
||Θ1:L − Φ|| 2 F = ||U ⊤D̂U",6 Failure,[0],[0]
"− U⊤DU ||2F = d ∑
j=1
(D̂jj −Djj) 2,
where the Dii are the eigenvalues of Φ.
Theorem 4.",6 Failure,[0],[0]
If the least squares matrix Φ is symmetric then Penalty Regularized Gradient Descent produces hypotheses Θ (t) 1:L that are commuting normal with Φ.,6 Failure,[0],[0]
"In addition, if Φ has a negative eigenvalue −λ and L is even, then ℓ(Θ(t))",6 Failure,[0],[0]
"≥ λ2/2 for all t.
Proof.",6 Failure,[0],[0]
"For all t, Penalty Regularized Gradient Descent produces Θ (t+1) i = (1 − κ)Θ (t) i + κI",6 Failure,[0],[0]
− ηt(Θ (t) i+1:L) ⊤ ( Θ (t) 1:L −Φ ) (Θ (t) 1:i−1) ⊤.,6 Failure,[0],[0]
"Thus, by induction, the Θ(t)i are matrix polynomials of Φ, and therefore they are all commuting normal.",6 Failure,[0],[0]
As in Lemmas 5 and 6 each Θ (t) i is the same U ⊤D̃(t)U and Θ (t) 1,6 Failure,[0],[0]
:L = U ⊤(D̃(t))LU .,6 Failure,[0],[0]
"Since L is even, each (D̃(t))Ljj ≥ 0, so ℓ(Θ (t))",6 Failure,[0],[0]
"= 12 ||Θ (t) 1:L−Φ|| 2 F ≥ λ 2/2.
To analyze step-and-project algorithms, it is helpful to first characterize the project step (see also (Lefkimmiatis et al., 2013)).
",6 Failure,[0],[0]
Lemma 15.,6 Failure,[0],[0]
"Let X be a symmetric matrix and let U⊤DU be its diagonalization.
",6 Failure,[0],[0]
"For a > 0, let Y be the Frobenius norm projection of X onto Ba = {A : A is symmetric psd and ||A−I||2 ≤ a}.",6 Failure,[0],[0]
"Then Y = U
⊤D̃U where D̃ is obtained from D by projecting all of its diagonal elements onto [1− a, 1 + a].
",6 Failure,[0],[0]
"Thus {X,Y } are symmetric commuting normal matrices.
",6 Failure,[0],[0]
Proof.,6 Failure,[0],[0]
"First, if X ∈ Ba, then Y = X and we are done.",6 Failure,[0],[0]
Assume X 6∈ Ba.,6 Failure,[0],[0]
"Clearly U ⊤D̃U ∈ Ba, so we just need to show that any member of Ba is at least as far from X as U⊤D̃U is.",6 Failure,[0],[0]
"Let Λ be the multiset of eigenvalues of X (with repetitions) that are not in [1 − a, 1 + a], and for each λ ∈ Λ, let eλ be the adjustment to λ necessary to bring it to [1− a, 1 + a]; i.e., so that λ+ eλ is the projection of λ onto [1− a, 1 + a].
",6 Failure,[0],[0]
"If uλ is the eigenvector associated with λ, we have U ⊤D̃U −X = ∑ λ∈Λ eλuλu ⊤ λ , so that ||U ⊤D̃U",6 Failure,[0],[0]
− X||2F = ∑,6 Failure,[0],[0]
λ∈Λ,6 Failure,[0],[0]
e 2 λ.,6 Failure,[0],[0]
Let Z be an arbitrary member of Ba.,6 Failure,[0],[0]
We would like to show that ||Z,6 Failure,[0],[0]
− X|| 2 F ≥ ∑ λ∈Λ e 2 λ.,6 Failure,[0],[0]
"Since Z ∈ Ba, we have ||Z − I||2 ≤ a. ||Z",6 Failure,[0],[0]
− I||2 is the largest singular value of Z,6 Failure,[0],[0]
"− I so, for any unit length vector, in particular some uλ for λ ∈ Λ, |u ⊤ λ (Z",6 Failure,[0],[0]
− I)uλ| = |u ⊤ λ,6 Failure,[0],[0]
"Zuλ − 1| ≤ a, which implies u⊤λZuλ ∈",6 Failure,[0],[0]
"[1 − a, 1 + a].",6 Failure,[0],[0]
Since U is unitary U ⊤(X,6 Failure,[0],[0]
"− Z)U has the same eigenvalues as X − Z, and, since the Frobenius norm is a function of the eigenvalues, ||U⊤(X − Z)U ||F = ||X − Z||F .",6 Failure,[0],[0]
But since u⊤λZuλ ∈,6 Failure,[0],[0]
"[1 − a, 1 + a] for all λ ∈ Λ, just summing over the diagonal elements, we get ||U⊤(X − Z)U",6 Failure,[0],[0]
"||2F ≥ ∑ λ∈Λ e 2 λ, completing the proof.
",6 Failure,[0],[0]
Theorem 5.,6 Failure,[0],[0]
If the least squares matrix Φ is symmetric then ψ-step-and-project algorithms produce hypotheses Θ (t) 1:L that are commuting normal with Φ.,6 Failure,[0],[0]
"In addition, if Φ has a negative eigenvalue −λ and either L is even or ψ(L, d) ≤ 1, then ℓ(Θ(t))",6 Failure,[0],[0]
"≥ λ2/2 for all t.
Proof.",6 Failure,[0],[0]
"As in Lemmas 5 and 6, the Θ (t+1/2)",6 Failure,[0],[0]
i are identical and mutually diagonalizable with Φ. Lemma 15 shows that this is preserved by the projection step.,6 Failure,[0],[0]
Thus there is a real diagonal D̃(t) such that each Θ (t) i = U ⊤D(t)i,6 Failure,[0],[0]
"U , so Θ (t) 1:L = U ⊤(D̃(t))LU .",6 Failure,[0],[0]
"When L is even, each (D̃(t))L)j,j ≥ 0.",6 Failure,[0],[0]
"When ψ(d, L) ≤ 1",6 Failure,[0],[0]
"then the projection ensures that the elements of D̃(t) are non-negative, and thus each (D̃(t))L)j,j ≥ 0.",6 Failure,[0],[0]
"In either case, ℓ(Θ (t))",6 Failure,[0],[0]
"= 12 ||Θ (t) 1:L− Φ||2F ≥ λ 2/2.
",6 Failure,[0],[0]
"One choice of Φ that satisfies the requirements of Theorems 4 and 5 is Φ = diag(−λ, 1, 1, ..., 1).",6 Failure,[0],[0]
"For constant λ, the loss of Θ(0) = (I, I, ..., I) is a constant for this target.",6 Failure,[0],[0]
"Another choice is Φ = diag(−λ,−λ, 1, 1, ..., 1), which has a positive determinant.
",6 Failure,[0],[0]
Our proof of failure to minimize the loss exploits the fact that the layers are initialized to multiples of the identity.,6 Failure,[0],[0]
"Since the training process is a continuous function of the initial solution, this implies that any convergence to a good solution will be very slow if the initializations are sufficiently close to the identity.",6 Failure,[0],[0]
"We thank Yair Carmon, Nigel Duffy, Matt Feiszli, Roy Frostig, Vineet Gupta, Moritz Hardt, Tomer Koren, Antoine Saliou, Hanie Sedghi, Yoram Singer and Kunal Talwar for valuable conversations.
",Acknowledgements,[0],[0]
Peter Bartlett gratefully acknowledges the support of the NSF through grant IIS-1619362 and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS).,Acknowledgements,[0],[0]
"We rely on the following facts (Horn, 1986; Harville, 1997).
",A Proof of Lemma 1,[0],[0]
Lemma 16.,A Proof of Lemma 1,[0],[0]
"For compatible matrices (and, where m,n, p, q, r, s are mentioned, A ∈ ℜm×n, B ∈ ℜp×q, X ∈ ℜr×s):
A⊗ (B ⊗ E) =",A Proof of Lemma 1,[0],[0]
"(A⊗B)⊗E,
AC ⊗BD = (A⊗B)(C ⊗D),
(A⊗B)⊤ = A⊤ ⊗B⊤,
vec(AXB) = (B⊤ ⊗A)vec(X),
Tm,nvec(A) def = vec(A⊤),
Tn,mTm,n = Imn,
Tm,n = T ⊤ n,m,
T1,n = Tn,1 =",A Proof of Lemma 1,[0],[0]
"In,
DX(A(B(X)))",A Proof of Lemma 1,[0],[0]
"= DB(A(B(X)))DX (B(X)),
DX(A(X)B(X))",A Proof of Lemma 1,[0],[0]
= (B(X) ⊤ ⊗ Im)DXA(X),A Proof of Lemma 1,[0],[0]
+,A Proof of Lemma 1,[0],[0]
"(Iq ⊗A(X))DXB(X),
DX(A(X) T ) = Tn,mDX(A(X)),
DX(AXB)",A Proof of Lemma 1,[0],[0]
"= B ⊤ ⊗A,
DA(A⊗B) =",A Proof of Lemma 1,[0],[0]
"(In ⊗ Tq,m ⊗ Ip)(Imn ⊗ vec(B))
=",A Proof of Lemma 1,[0],[0]
"(Inq ⊗ Tm,p)(In ⊗ vec(B)⊗",A Proof of Lemma 1,[0],[0]
"Im),
DB(A⊗B) =",A Proof of Lemma 1,[0],[0]
"(In ⊗ Tq,m ⊗ Ip)(vec(A)⊗ Ipq)
= (Tp,q ⊗ Imn)(Iq ⊗ vec(A)⊗",A Proof of Lemma 1,[0],[0]
"Ip).
",A Proof of Lemma 1,[0],[0]
"Armed with Lemma 16, we now prove Lemma 1.",A Proof of Lemma 1,[0],[0]
"We have
DΘifΘ(x) = DΘi (Θi+1:LΘiΘ1:i−1x) =",A Proof of Lemma 1,[0],[0]
"(Θ1:i−1x) ⊤ ⊗Θi+1:L.
Again, from Lemma 16
DΘi ( DΘjfΘ(x) )",A Proof of Lemma 1,[0],[0]
"= DΘi
(
(Θ1:j−1x) ⊤ ⊗Θj+1:L
)
= DΘ1:j−1x
(
(Θ1:j−1x) ⊤ ⊗Θj+1:L
)
DΘi (Θ1:j−1x)
(by the chain rule, since i < j)
= DΘ1:j−1x
(
(
(Θ1:j−1x)⊗Θ ⊤ j+1:L
)⊤ ) (
(Θ1:i−1x) ⊤",A Proof of Lemma 1,[0],[0]
"⊗Θi+1:j−1
)
.",A Proof of Lemma 1,[0],[0]
"(9)
Define P = Θ1:j−1x and Q = Θj+1:L, so that P ∈ ℜd×1 and Q ∈ ℜd×d.",A Proof of Lemma 1,[0],[0]
"We have
DP
(
( P ⊗Q⊤ )⊤ )
= Td2,dDP
( P ⊗Q⊤ )
",A Proof of Lemma 1,[0],[0]
"= Td2,d(I1 ⊗ Td,d ⊗ Id)(Id ⊗ vec(Q T ))",A Proof of Lemma 1,[0],[0]
"= Td2,d(Td,d ⊗ Id)(Id ⊗ vec(Q ⊤)).
",A Proof of Lemma 1,[0],[0]
"Substituting back into (9), we get
DΘi ( DΘjfΘ(x) )",A Proof of Lemma 1,[0],[0]
"= Td2,d(Td,d ⊗ Id)(Id ⊗ vec(Θ ⊤ j+1:L))
",A Proof of Lemma 1,[0],[0]
"(
(Θ1:i−1x) ⊤",A Proof of Lemma 1,[0],[0]
"⊗Θi+1:j−1
)
.
",A Proof of Lemma 1,[0],[0]
"The product rule in Lemma 16 gives, for each i,
DΘiℓ (fΘ)",A Proof of Lemma 1,[0],[0]
"= E(DΘi(ℓ(fΘ(X)))
= E(DΘi( 1
2 (fΘ(X)− ΦX)
⊤(fΘ(X)",A Proof of Lemma 1,[0],[0]
"−ΦX)))
",A Proof of Lemma 1,[0],[0]
= E(((Θ1:L − Φ)X) ⊤DΘifΘ(X)),A Proof of Lemma 1,[0],[0]
"= E ( ((Θ1:L − Φ)X) ⊤ ( (Θ1:i−1X) ⊤ ⊗Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= E,A Proof of Lemma 1,[0],[0]
( (I1 ⊗,A Proof of Lemma 1,[0],[0]
"((Θ1:L − Φ)X) ⊤) ( (Θ1:i−1X) ⊤ ⊗Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
"= E (( (Θ1:i−1X) ⊤ ⊗ ((Θ1:L −Φ)X) ⊤Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= E,A Proof of Lemma 1,[0],[0]
(( X⊤Θ⊤1:i−1 ) ⊗,A Proof of Lemma 1,[0],[0]
( X⊤(Θ1:L −Φ) ⊤Θi+1:,A Proof of Lemma 1,[0],[0]
L )),A Proof of Lemma 1,[0],[0]
"= E ( (X⊤ ⊗X⊤) (
Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L
))
= E ((X ⊗X)vec(1))⊤ ( Θ⊤1:i−1 ⊗",A Proof of Lemma 1,[0],[0]
"(Θ1:L − Φ) ⊤Θi+1:L )
= E ( vec(XX⊤) )",A Proof of Lemma 1,[0],[0]
"⊤ (
Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L
)
= (vec(Id))",A Proof of Lemma 1,[0],[0]
"T ( Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L ) .
",A Proof of Lemma 1,[0],[0]
"Hence,
(DΘiℓ (fΘ))",A Proof of Lemma 1,[0],[0]
"⊤ =
(
Θ1:i−1 ⊗Θ ⊤ i+1:L(Θ1:L − Φ)
)
(vec(Id))
",A Proof of Lemma 1,[0],[0]
"= vec ( Θ⊤i+1:L(Θ1:L − Φ)IdΘ ⊤ 1:i−1 ) .
",A Proof of Lemma 1,[0],[0]
"Also, recalling that i < j, we have
DΘjDΘiℓ (fΘ)",A Proof of Lemma 1,[0],[0]
"= DΘj
( (vec(Id))",A Proof of Lemma 1,[0],[0]
T ( Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L ))
",A Proof of Lemma 1,[0],[0]
= (Id2 ⊗ (vec(Id)),A Proof of Lemma 1,[0],[0]
"T )DΘj
(
Θ⊤1:i−1 ⊗",A Proof of Lemma 1,[0],[0]
"(Θ1:L − Φ) ⊤Θi+1:L
)
= (Id2 ⊗ (vec(Id))",A Proof of Lemma 1,[0],[0]
"T ) (Id ⊗ Td,d ⊗",A Proof of Lemma 1,[0],[0]
"Id)
( vec(Θ⊤1:i−1)⊗ Id2 ) DΘj",A Proof of Lemma 1,[0],[0]
( (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L ) .
",A Proof of Lemma 1,[0],[0]
"Continuing with the subproblem,
DΘj
(
(Θ1:L −Φ) ⊤Θi+1:",A Proof of Lemma 1,[0],[0]
"L
)
",A Proof of Lemma 1,[0],[0]
"= (Θ⊤i+1:L ⊗ Id)DΘj
( (Θ1:L − Φ) ⊤ )
",A Proof of Lemma 1,[0],[0]
+ (Id ⊗ (Θ1:,A Proof of Lemma 1,[0],[0]
L − Φ) ⊤)DΘj,A Proof of Lemma 1,[0],[0]
"(Θi+1:L)
= (Θ⊤i+1:L ⊗ Id)DΘj
( Θ⊤1:L )
+",A Proof of Lemma 1,[0],[0]
(Id ⊗ (Θ1:L − Φ) ⊤)DΘj,A Proof of Lemma 1,[0],[0]
"(Θi+1:L)
= (Θ⊤i+1:L ⊗ Id) ( Θj+1:L ⊗Θ ⊤ 1:j−1 ) DΘj (Θ ⊤ j )
+",A Proof of Lemma 1,[0],[0]
"(Id ⊗ (Θ1:L − Φ) ⊤) ( Θ⊤i+1:j−1 ⊗Θj+1:L )
= (Θ⊤i+1:L ⊗ Id) ( Θj+1:L ⊗Θ ⊤ 1:j−1 )",A Proof of Lemma 1,[0],[0]
"Td,d
+ (Id ⊗ (Θ1:L − Φ) ⊤)",A Proof of Lemma 1,[0],[0]
"( Θ⊤i+1:j−1 ⊗Θj+1:L )
=",A Proof of Lemma 1,[0],[0]
( Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1 ),A Proof of Lemma 1,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1:L ) .
",A Proof of Lemma 1,[0],[0]
"Finally,
DΘiDΘiℓ (fΘ) = DΘi
( (vec(Id)) T ( Θ⊤1:i−1 ⊗ (Θ1:",A Proof of Lemma 1,[0],[0]
"L − Φ) ⊤Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= (Id2 ⊗ (vec(Id)),A Proof of Lemma 1,[0],[0]
"T )DΘi
(
Θ⊤1:i−1 ⊗ (Θ1:",A Proof of Lemma 1,[0],[0]
L − Φ) ⊤Θi+1:,A Proof of Lemma 1,[0],[0]
"L
)
= (Id2 ⊗ (vec(Id)) T )",A Proof of Lemma 1,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 ) DΘi",A Proof of Lemma 1,[0],[0]
( (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L )
and
DΘi
(
(Θ1:L − Φ) ⊤Θi+1:L
)
= (Θ⊤i+1:L ⊗ Id)DΘi
( (Θ1:L − Φ) ⊤ )
= (Θ⊤i+1:L ⊗ Id)DΘi
( Θ⊤1:L )
= (Θ⊤i+1:L ⊗ Id) ( Θi+1:L ⊗Θ ⊤ 1:i−1 ) DΘi(Θ ⊤ i ) =",A Proof of Lemma 1,[0],[0]
(Θ⊤i+1:L ⊗ Id) ( Θi+1:L ⊗Θ ⊤ 1:i−1 ),A Proof of Lemma 1,[0],[0]
"Td,d = (
Θ⊤i+1:LΘi+1:",A Proof of Lemma 1,[0],[0]
"L ⊗Θ ⊤ 1:i−1
)
",A Proof of Lemma 1,[0],[0]
"Td,d.",A Proof of Lemma 1,[0],[0]
"We have ||∇2||2F = 2 ∑
i<j
||DΘjDΘiℓ(fΘ)|| 2 F +
∑
i
||DΘiDΘiℓ(fΘ)|| 2 F .",B Proof of Lemma 3,[0],[0]
"(10)
Let’s start with the easier term.",B Proof of Lemma 3,[0],[0]
Choose Θ such that ||Θi − I||2 ≤ z,B Proof of Lemma 3,[0],[0]
"for all i. We have
||DΘiDΘiℓ (fΘ) ||F = ∣ ∣ ∣ ∣(Id2⊗(vec(Id)) ⊤) (Id⊗Td,d⊗Id)
( vec(Θ⊤1:i−1)⊗Id2 )
(
Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣
F
≤ ∣ ∣
∣
∣ ∣ ∣ (Id2 ⊗ (vec(Id)) ⊤) (Id ⊗ Td,d ⊗ Id) ∣ ∣ ∣ ∣ ∣ ∣
F
× ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗Id2 )( Θ⊤i+1:LΘi+1:L⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d3/2 ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗ Id2 )
(
Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
"L ⊗Θ ⊤ 1:i−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
∣ ∣ ∣ ∣ ∣ ∣
F
≤ d3/2 ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗ Id2 ) ∣ ∣ ∣ ∣ ∣ ∣
F
× ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
L ⊗Θ ⊤ 1:i−1 ),B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d7/2 ∣ ∣
∣
∣ ∣
∣ vec(Θ⊤1:i−1)
∣ ∣ ∣ ∣ ∣ ∣
F
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d7/2 ||Θ1:i−1||F
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
L ⊗Θ ⊤ 1:i−1 ),B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
≤ d4 ||Θ1:i−1||2
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1 ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d4(1 + z)i−1 ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 ) ∣ ∣ ∣ ∣ ∣ ∣
F
= d4(1 + z)i−1 ∣ ∣
∣
∣ ∣
∣ Θ⊤i+1:LΘi+1:L
∣ ∣ ∣ ∣ ∣ ∣ F × ∣ ∣ ∣ ∣ ∣ ∣ Θ⊤1:i−1 ∣ ∣ ∣ ∣ ∣ ∣ F
≤",B Proof of Lemma 3,[0],[0]
"d5(1 + z)i−1 ∣ ∣
∣
∣ ∣
∣ Θ⊤i+1:LΘi+1:L
∣ ∣ ∣ ∣ ∣ ∣ 2 × ∣ ∣ ∣ ∣ ∣ ∣ Θ⊤1:i−1 ∣ ∣ ∣ ∣ ∣ ∣ 2
≤ d5(1 + z)2(L−1).
",B Proof of Lemma 3,[0],[0]
"Similarly,
||DΘjDΘiℓ (fΘ) ||F = ∣ ∣ ∣ ∣(Id2⊗(vec(I)) ⊤) (Id⊗Td,d⊗Id)
( vec(Θ⊤1:i−1)⊗Id2 )
(
(
Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L −Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )
)
∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1 ∣ ∣ ∣ ∣
(
Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L −Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L ) ∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"(∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
+ ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )∣ ∣ ∣ ∣ ∣ ∣
F
)
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"( d(1 + z)2L−1−i
+ ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )∣ ∣ ∣ ∣ ∣ ∣
F
)
= d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"( d(1 + z)2L−1−i
+ ||Θi+1:j−1||F × ∣ ∣ ∣ ∣ ∣ ∣ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L ∣ ∣ ∣ ∣ ∣ ∣
F
)
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
( d(1 + z)2L−1−i + 2d(1 + z)2L−1−i ),B Proof of Lemma 3,[0],[0]
"= 3d5(1 + z)2L−2.
",B Proof of Lemma 3,[0],[0]
"Putting these together with (10), we get ||∇2||2F ≤",B Proof of Lemma 3,[0],[0]
"L 29d10(1 + z)4L, so that
||∇2||F ≤ 3Ld 5(1 + z)2L.",B Proof of Lemma 3,[0],[0]
"Recall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP .
",C Proof of Lemma 7,[0],[0]
"Lemma 17 ((Horn & Johnson, 2013)).",C Proof of Lemma 7,[0],[0]
"A is a unitary matrix if and only if all of the (complex) eigenvalues z of A have magnitude 1.
",C Proof of Lemma 7,[0],[0]
"Lemma 18 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A is unitary then A is normal.
",C Proof of Lemma 7,[0],[0]
"Lemma 19 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A is normal with eigenvalues λ1, ..., λd, the singular values of A are |λ1|, ..., |λd|.
Lemma 20.",C Proof of Lemma 7,[0],[0]
"If A is unitary, then A1/L is unitary, and thus Ai/L is unitary for any non-negative integer i.
Lemma 21.",C Proof of Lemma 7,[0],[0]
"If A is invertible and normal with singular values σ1, ..., σd, then, for any positive integer L, the singular values of A1/L are σ 1/L 1 , ..., σ 1/L d .
",C Proof of Lemma 7,[0],[0]
Proof.,C Proof of Lemma 7,[0],[0]
"Follows from Lemma 19 together with the fact that raising a non-singular matrix to a power results in raising its eigenvalues to the same power.
",C Proof of Lemma 7,[0],[0]
"Lemma 22 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A = RP is the polar decomposition of A, then the singular values of A are the same as the singular values of P .
",C Proof of Lemma 7,[0],[0]
Lemma 23.,C Proof of Lemma 7,[0],[0]
"If σ1, ..., σd are the principal components of A, and A = ∏L i=1Ai is a balanced factorization of A, then then σ 1/L 1 , ..., σ 1/L d are the principal components of Ai, for each i ∈ {1, ..., L}.
",C Proof of Lemma 7,[0],[0]
Proof.,C Proof of Lemma 7,[0],[0]
"The singular values of Ai = RiPi are the same as the singular values of Pi, which is similar to P 1/L, whose singular values are the Lth roots of the singular values of P , which are the same as the singular values of A.
Lemma 24.",C Proof of Lemma 7,[0],[0]
"If A1, ..., AL is a balanced factorization of A, then
A = L ∏
i=1
Ai.
Proof.",C Proof of Lemma 7,[0],[0]
"We have
A = RP
= R1/LR1−1/LP 1/LP 1−1/L = R1/LR1−1/LP 1/LR−(1−1/L)R1−1/LP 1−1/L = R1P1R 1−1/LP 1−1/L = A1R 1−1/LP 1−1/L = A1R 1/LR1−2/LP 1/LP 1−2/L
and so on.",C Proof of Lemma 7,[0],[0]
"We analyze algorithms for approximating a function f(x) = Φxmapping R to R using deep linear neural networks, i.e. that learn a function h parameterized by matrices Θ1, ...,ΘL and defined by h(x)",abstractText,[0],[0]
= ΘLΘL−1...Θ1x.,abstractText,[0],[0]
We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic.,abstractText,[0],[0]
"We provide polynomial bounds on the number of iterations for gradient descent to approximate the least squares matrix Φ, in the case where the initial hypothesis Θ1 = ...",abstractText,[0],[0]
= ΘL = I has excess loss bounded by a small enough constant.,abstractText,[0],[0]
"On the other hand, we show that gradient descent fails to converge for Φ whose distance from the identity is a larger constant, and we show that some forms of regularization toward the identity in each layer do not help.",abstractText,[0],[0]
"If Φ is symmetric positive definite, we show that an algorithm that initializes Θi = I learns an ǫ-approximation of f using a number of updates polynomial in L, the condition number of Φ, and log(d/ǫ).",abstractText,[0],[0]
"In contrast, we show that if the least squares matrix Φ is symmetric and has a negative eigenvalue, then all members of a class of algorithms that perform gradient descent with identity initialization, and optionally regularize toward the identity in each layer, fail to converge.",abstractText,[0],[0]
"We analyze an algorithm for the case that Φ satisfies uΦu > 0 for all u, but may not be symmetric.",abstractText,[0],[0]
This algorithm uses two regularizers: one that maintains the invariant u⊤ΘLΘL−1...,abstractText,[0],[0]
Θ1u > 0,abstractText,[0],[0]
"for all u, and another that “balances” Θ1, ...,ΘL so that they have the same singular values.",abstractText,[0],[0]
"Single-task learning in computer vision has enjoyed much success in deep learning, with many single-task models now performing at or beyond human accuracies for a wide array of tasks.",1. Introduction,[0],[0]
"However, an ultimate visual system for full scene understanding must be able to perform many diverse perceptual tasks simultaneously and efficiently, especially within the limited compute environments of embedded systems
1Magic Leap, Inc. Correspondence to: Zhao Chen <zchen@magicleap.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
such as smartphones, wearable devices, and robots/drones.",1. Introduction,[0],[0]
"Such a system can be enabled by multitask learning, where one model shares weights across multiple tasks and makes multiple inferences in one forward pass.",1. Introduction,[0],[0]
"Such networks are not only scalable, but the shared features within these networks can induce more robust regularization and boost performance as a result.",1. Introduction,[0],[0]
"In the ideal limit, we can thus have the best of both worlds with multitask networks: more efficiency and higher performance.
",1. Introduction,[0],[0]
"In general, multitask networks are difficult to train; different tasks need to be properly balanced so network parameters converge to robust shared features that are useful across all tasks.",1. Introduction,[0],[0]
"Methods in multitask learning thus far have largely tried to find this balance by manipulating the forward pass of the network (e.g. through constructing explicit statistical relationships between features (Long & Wang, 2015) or optimizing multitask network architectures (Misra et al., 2016), etc.), but such methods ignore a key insight: task imbalances impede proper training because they manifest as imbalances between backpropagated gradients.",1. Introduction,[0],[0]
"A task that is too dominant during training, for example, will necessarily express that dominance by inducing gradients which have relatively large magnitudes.",1. Introduction,[0],[0]
"We aim to mitigate such issues at their root by directly modifying gradient magnitudes through tuning of the multitask loss function.
",1. Introduction,[0],[0]
"In practice, the multitask loss function is often assumed to be linear in the single task losses Li, L = ∑ i wiLi, where the sum runs over all T tasks.",1. Introduction,[0],[0]
"In our case, we propose an adaptive method, and so wi can vary at each training step t: wi = wi(t).",1. Introduction,[0],[0]
"This linear form of the loss function is convenient for implementing gradient balancing, as wi very directly and linearly couples to the backpropagated gradient magnitudes from each task.",1. Introduction,[0],[0]
The challenge is then to find the best value for each wi at each training step t that balances the contribution of each task for optimal model training.,1. Introduction,[0],[0]
"To optimize the weights wi(t) for gradient balancing, we propose a simple algorithm that penalizes the network when backpropagated gradients from any task are too large or too small.",1. Introduction,[0],[0]
"The correct balance is struck when tasks are training at similar rates; if task i is training relatively quickly, then its weight wi(t) should decrease relative to other task weights wj(t)|j 6=i to allow other tasks more influence on
training.",1. Introduction,[0],[0]
"Our algorithm is similar to batch normalization (Ioffe & Szegedy, 2015) with two main differences: (1) we normalize across tasks instead of across data batches, and (2) we use rate balancing as a desired objective to inform our normalization.",1. Introduction,[0],[0]
"We will show that such gradient normalization (hereafter referred to as GradNorm) boosts network performance while significantly curtailing overfitting.
",1. Introduction,[0],[0]
"Our main contributions to multitask learning are as follows:
1.",1. Introduction,[0],[0]
"An efficient algorithm for multitask loss balancing which directly tunes gradient magnitudes.
2.",1. Introduction,[0],[0]
"A method which matches or surpasses the performance of very expensive exhaustive grid search procedures, but which only requires tuning a single hyperparameter.
3.",1. Introduction,[0],[0]
A demonstration that direct gradient interaction provides a powerful way of controlling multitask learning.,1. Introduction,[0],[0]
"Multitask learning was introduced well before the advent of deep learning (Caruana, 1998; Bakker & Heskes, 2003), but the robust learned features within deep networks and their excellent single-task performance have spurned renewed interest.",2. Related Work,[0],[0]
"Although our primary application area is computer vision, multitask learning has applications in multiple other fields, from natural language processing (Collobert & Weston, 2008; Hashimoto et al., 2016; Søgaard & Goldberg, 2016) to speech synthesis (Seltzer & Droppo, 2013; Wu et al., 2015), from very domain-specific applications such as traffic prediction (Huang et al., 2014) to very general cross-domain work (Bilen & Vedaldi, 2017).",2. Related Work,[0],[0]
"Multitask learning has also been explored in the context of curriculum learning (Graves et al., 2017), where subsets of tasks are subsequently trained based on local rewards; we here explore the opposite approach, where tasks are jointly trained based on global rewards such as total loss decrease.
",2. Related Work,[0],[0]
"Multitask learning is very well-suited to the field of computer vision, where making multiple robust predictions is crucial for complete scene understanding.",2. Related Work,[0],[0]
"Deep networks have been used to solve various subsets of multiple vision tasks, from 3-task networks (Eigen & Fergus, 2015; Teichmann et al., 2016) to much larger subsets as in UberNet (Kokkinos, 2016).",2. Related Work,[0],[0]
"Often, single computer vision problems can even be framed as multitask problems, such as in Mask R-CNN for instance segmentation (He et al., 2017) or YOLO-9000 for object detection (Redmon & Farhadi, 2016).",2. Related Work,[0],[0]
Particularly of note is the rich and significant body of work on finding explicit ways to exploit task relationships within a multitask model.,2. Related Work,[0],[0]
"Clustering methods have shown success beyond deep models (Jacob et al., 2009; Kang et al., 2011), while constructs such as deep relationship networks (Long & Wang, 2015) and cross-stich networks (Misra et al., 2016)
give deep networks the capacity to search for meaningful relationships between tasks and to learn which features to share between them.",2. Related Work,[0],[0]
"Work in (Warde-Farley et al., 2014) and (Lu et al., 2016) use groupings amongst labels to search through possible architectures for learning.",2. Related Work,[0],[0]
"Perhaps the most relevant to the current work, (Kendall et al., 2017) uses a joint likelihood formulation to derive task weights based on the intrinsic uncertainty in each task.",2. Related Work,[0],[0]
"For a multitask loss function L(t) = ∑ wi(t)Li(t), we aim to learn the functions wi(t) with the following goals: (1) to place gradient norms for different tasks on a common scale through which we can reason about their relative magnitudes, and (2) to dynamically adjust gradient norms so different tasks train at similar rates.",3.1. Definitions and Preliminaries,[0],[0]
"To this end, we first define the relevant quantities, first with respect to the gradients we will be manipulating.
",3.1. Definitions and Preliminaries,[0],[0]
•,3.1. Definitions and Preliminaries,[0],[0]
W : The subset of the full network weights W ⊂ W where we actually apply GradNorm.,3.1. Definitions and Preliminaries,[0],[0]
"W is generally chosen as the last shared layer of weights to save on compute costs1.
",3.1. Definitions and Preliminaries,[0],[0]
• G(i)W (t) = ||∇Wwi(t)Li(t)||2: the L2 norm of the gradient of the weighted single-task loss wi(t)Li(t),3.1. Definitions and Preliminaries,[0],[0]
"with respect to the chosen weights W .
• GW (t) = Etask[G(i)W (t)]: the average gradient norm across all tasks at training time t.
We also define various training rates for each task i:
• L̃i(t) = Li(t)/Li(0): the loss ratio for task",3.1. Definitions and Preliminaries,[0],[0]
"i at time t. L̃i(t) is a measure of the inverse training rate of task i (i.e. lower values of L̃i(t) correspond to a faster training rate for task i)2.
• ri(t) = L̃i(t)/Etask[L̃i(t)]",3.1. Definitions and Preliminaries,[0],[0]
": the relative inverse training rate of task i.
With the above definitions in place, we now complete our description of the GradNorm algorithm.",3.1. Definitions and Preliminaries,[0],[0]
"As stated in Section 3.1, GradNorm should establish a common scale for gradient magnitudes, and also should balance
1In our experiments this choice of W causes GradNorm to increase training time by only ∼ 5% on NYUv2.
2Networks in this paper all had stable initializations and Li(0) could be used directly.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When Li(0) is sharply dependent on initialization, we can use a theoretical initial loss instead.",3.2. Balancing Gradients with GradNorm,[0],[0]
"E.g. for Li the CE loss across C classes, we can use Li(0) = log(C).
training rates of different tasks.",3.2. Balancing Gradients with GradNorm,[0],[0]
"The common scale for gradients is most naturally the average gradient norm, GW (t), which establishes a baseline at each timestep t by which we can determine relative gradient sizes.",3.2. Balancing Gradients with GradNorm,[0],[0]
"The relative inverse training rate of task i, ri(t), can be used to rate balance our gradients.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Concretely, the higher the value of ri(t), the higher the gradient magnitudes should be for task i in order to encourage the task to train more quickly.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Therefore, our desired gradient norm for each task i is simply:
G (i) W (t) 7→ GW (t)× [ri(t)]",3.2. Balancing Gradients with GradNorm,[0],[0]
"α, (1)
where α is an additional hyperparameter.",3.2. Balancing Gradients with GradNorm,[0],[0]
α sets the strength of the restoring force which pulls tasks back to a common training rate.,3.2. Balancing Gradients with GradNorm,[0],[0]
"In cases where tasks are very different in their complexity, leading to dramatically different learning dynamics between tasks, a higher value of α should be used to enforce stronger training rate balancing.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When tasks are more symmetric (e.g. the synthetic examples in Section 4), a lower value of α is appropriate.",3.2. Balancing Gradients with GradNorm,[0],[0]
Note that α = 0 will always try to pin the norms of backpropagated gradients from each task to be equal at W .,3.2. Balancing Gradients with GradNorm,[0],[0]
"See Section 5.4 for more details on the effects of tuning α.
",3.2. Balancing Gradients with GradNorm,[0],[0]
"Equation 1 gives a target for each task i’s gradient norms, and we update our loss weights wi(t) to move gradient
norms towards this target for each task.",3.2. Balancing Gradients with GradNorm,[0],[0]
"GradNorm is then implemented as an L1 loss function Lgrad between the actual and target gradient norms at each timestep for each task, summed over all tasks:
Lgrad(t;wi(t))",3.2. Balancing Gradients with GradNorm,[0],[0]
"= ∑ i ∣∣∣∣G(i)W (t)−GW (t)× [ri(t)]α∣∣∣∣ 1 (2)
where the summation runs through all T tasks.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When differentiating this loss Lgrad, we treat the target gradient norm GW (t)× [ri(t)]α as a fixed constant to prevent loss weights wi(t) from spuriously drifting towards zero.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Lgrad is then differentiated only with respect to the wi, as the wi(t) directly control gradient magnitudes per task.",3.2. Balancing Gradients with GradNorm,[0],[0]
The computed gradients ∇wiLgrad are then applied via standard update rules to update each wi (as shown in Figure 1).,3.2. Balancing Gradients with GradNorm,[0],[0]
The full GradNorm algorithm is summarized in Algorithm 1.,3.2. Balancing Gradients with GradNorm,[0],[0]
"Note that after every update step, we also renormalize the weights wi(t) so that ∑ i wi(t) = T in order to decouple gradient normalization from the global learning rate.",3.2. Balancing Gradients with GradNorm,[0],[0]
"To illustrate GradNorm on a simple, interpretable system, we construct a common scenario for multitask networks: training tasks which have similar loss functions but different loss scales.",4. A Toy Example,[0],[0]
"In such situations, if we naı̈vely pick wi(t) = 1
Algorithm 1 Training with GradNorm Initialize wi(0) = 1 ∀i Initialize network weightsW Pick value for α > 0 and pick the weightsW (usually the
final layer of weights which are shared between tasks) for t = 0",4. A Toy Example,[0],[0]
"to max train steps do
Input batch xi to compute Li(t) ∀i and L(t) = ∑ i wi(t)Li(t)",4. A Toy Example,[0],[0]
"[standard forward pass] Compute G(i)W (t) and ri(t) ∀i Compute GW (t) by averaging the G (i) W (t)
Compute Lgrad = ∑ i|G (i) W (t)−GW (t)× [ri(t)]α|1 Compute GradNorm gradients∇wiLgrad, keeping targets GW (t)× [ri(t)]α constant Compute standard gradients∇WL(t) Update wi(t) 7→ wi(t+ 1) using ∇wiLgrad UpdateW(t) 7→ W(t+ 1) using∇WL(t)",4. A Toy Example,[0],[0]
"[standard
backward pass] Renormalize wi(t+ 1) so that ∑ i wi(t+ 1) = T
end for
for all loss weights wi(t), the network training will be dominated by tasks with larger loss scales that backpropagate larger gradients.",4. A Toy Example,[0],[0]
"We will demonstrate that GradNorm overcomes this issue.
",4. A Toy Example,[0],[0]
"Consider T regression tasks trained using standard squared loss onto the functions
fi(x) =",4. A Toy Example,[0],[0]
"σi tanh((B + i)x), (3)
where tanh(·) acts element-wise.",4. A Toy Example,[0],[0]
"Inputs are dimension 250 and outputs dimension 100, while B and i are constant matrices with their elements generated IID from N (0, 10) and N (0, 3.5), respectively.",4. A Toy Example,[0],[0]
Each task therefore shares information in B but also contains task-specific information i.,4. A Toy Example,[0],[0]
The σi are the key parameters within this problem; they are fixed scalars which set the scales of the outputs fi.,4. A Toy Example,[0],[0]
A higher scale for fi induces a higher expected value of squared loss for that task.,4. A Toy Example,[0],[0]
"Such tasks are harder to learn due to the higher variances in their response values, but they also backpropagate larger gradients.",4. A Toy Example,[0],[0]
"This scenario generally leads to suboptimal training dynamics when the higher σi tasks dominate the training across all tasks.
",4. A Toy Example,[0],[0]
"To train our toy models, we use a 4-layer fully-connected ReLU-activated network with 100 neurons per layer as a common trunk.",4. A Toy Example,[0],[0]
A final affine transformation layer gives T final predictions (corresponding to T different tasks).,4. A Toy Example,[0],[0]
"To ensure valid analysis, we only compare models initialized to the same random values and fed data generated from the same fixed random seed.",4. A Toy Example,[0],[0]
"The asymmetry α is set low to 0.12 for these experiments, as the output functions fi are all of the same functional form and thus we expect the asymmetry between tasks to be minimal.
",4. A Toy Example,[0],[0]
"In these toy problems, we measure the task-normalized testtime loss to judge test-time performance, which is the sum of the test loss ratios for each task, ∑ i Li(t)/Li(0).",4. A Toy Example,[0],[0]
We do this because a simple sum of losses is an inadequate performance metric for multitask networks when different loss scales exist; higher loss scale tasks will factor disproportionately highly in the loss.,4. A Toy Example,[0],[0]
"There unfortunately exists no general single scalar which gives a meaningful measure of multitask performance in all scenarios, but our toy problem was specifically designed with tasks which are statistically identical except for their loss scales σi.",4. A Toy Example,[0],[0]
"There is therefore a clear measure of overall network performance, which is the sum of losses normalized by each task’s variance σ2i - equivalent (up to a scaling factor) to the sum of loss ratios.
",4. A Toy Example,[0],[0]
"For T = 2, we choose the values (σ0, σ1)",4. A Toy Example,[0],[0]
"= (1.0, 100.0) and show the results of training in the top panels of Figure 2.",4. A Toy Example,[0],[0]
"If we train with equal weightswi = 1, task 1 suppresses task 0 from learning due to task 1’s higher loss scale.",4. A Toy Example,[0],[0]
"However, gradient normalization increases w0(t) to counteract the larger gradients coming from T1, and the improved task balance results in better test-time performance.
",4. A Toy Example,[0],[0]
The possible benefits of gradient normalization become even clearer when the number of tasks increases.,4. A Toy Example,[0],[0]
"For T = 10, we sample the σi from a wide normal distribution and plot the results in the bottom panels of Figure 2.",4. A Toy Example,[0],[0]
GradNorm significantly improves test time performance over naı̈vely weighting each task the same.,4. A Toy Example,[0],[0]
"Similarly to the T = 2 case, for T = 10 the wi(t) grow larger for smaller σi tasks.
",4. A Toy Example,[0],[0]
"For both T = 2 and T = 10, GradNorm is more stable and outperforms the uncertainty weighting proposed by (Kendall et al., 2017).",4. A Toy Example,[0],[0]
"Uncertainty weighting, which enforces that wi(t) ∼ 1/Li(t), tends to grow the weights wi(t) too large and too quickly as the loss for each task drops.",4. A Toy Example,[0],[0]
"Although such networks train quickly at the onset, the training soon deteriorates.",4. A Toy Example,[0],[0]
"This issue is largely caused by the fact that uncertainty weighting allows wi(t) to change without constraint (compared to GradNorm which ensures∑ wi(t) = T always), which pushes the global learning rate up rapidly as the network trains.
",4. A Toy Example,[0],[0]
The traces for each wi(t) during a single GradNorm run are observed to be stable and convergent.,4. A Toy Example,[0],[0]
"In Section 5.3 we will see how the time-averaged weightsEt[wi(t)] lie close to the optimal static weights, suggesting GradNorm can greatly simplify the tedious grid search procedure.",4. A Toy Example,[0],[0]
"We use two variants of NYUv2 (Nathan Silberman & Fergus, 2012) as our main datasets.",5. Application to a Large Real-World Dataset,[0],[0]
"Please refer to the Supplementary Materials for additional results on a 9-task facial landmark dataset found in (Zhang et al., 2014).",5. Application to a Large Real-World Dataset,[0],[0]
"The standard NYUv2 dataset carries depth, surface normals, and semantic
segmentation labels (clustered into 13 distinct classes) for a variety of indoor scenes in different room types (bathrooms, living rooms, studies, etc.).",5. Application to a Large Real-World Dataset,[0],[0]
"NYUv2 is relatively small (795 training, 654 test images), but contains both regression and classification labels, making it a good choice to test the robustness of GradNorm across various tasks.
",5. Application to a Large Real-World Dataset,[0],[0]
"We augment the standard NYUv2 depth dataset with flips and additional frames from each video, resulting in 90,000 images complete with pixel-wise depth, surface normals, and room keypoint labels (segmentation labels are, unfortunately, not available for these additional frames).",5. Application to a Large Real-World Dataset,[0],[0]
"Keypoint labels are professionally annotated by humans, while surface normals are generated algorithmically.",5. Application to a Large Real-World Dataset,[0],[0]
The full dataset is then split by scene for a 90/10 train/test split.,5. Application to a Large Real-World Dataset,[0],[0]
See Figure 6 for examples.,5. Application to a Large Real-World Dataset,[0],[0]
"We will generally refer to these two datasets as NYUv2+seg and NYUv2+kpts, respectively.
",5. Application to a Large Real-World Dataset,[0],[0]
All inputs are downsampled to 320 x 320 pixels and outputs to 80 x 80 pixels.,5. Application to a Large Real-World Dataset,[0],[0]
"We use these resolutions following (Lee et al., 2017), which represents the state-of-the-art in room keypoint prediction and from which we also derive our VGG-style model architecture.",5. Application to a Large Real-World Dataset,[0],[0]
These resolutions also allow us to keep models relatively slim while not compromising semantic complexity in the ground truth output maps.,5. Application to a Large Real-World Dataset,[0],[0]
"We try two different models: (1) a SegNet (Badrinarayanan et al., 2015; Lee et al., 2017) network with a symmetric VGG16 (Simonyan & Zisserman, 2014) encoder/decoder,
and (2) an FCN (Long et al., 2015) network with a modified ResNet-50",5.1. Model and General Training Characteristics,[0],[0]
"(He et al., 2016) encoder and shallow ResNet decoder.",5.1. Model and General Training Characteristics,[0],[0]
"The VGG SegNet reuses maxpool indices to perform upsampling, while the ResNet FCN learns all upsampling filters.",5.1. Model and General Training Characteristics,[0],[0]
"The ResNet architecture is further thinned (both in its filters and activations) to contrast with the heavier, more complex VGG SegNet:",5.1. Model and General Training Characteristics,[0],[0]
stride-2 layers are moved earlier and all 2048-filter layers are replaced by 1024-filter layers.,5.1. Model and General Training Characteristics,[0],[0]
"Ultimately, the VGG SegNet has 29M parameters versus 15M for the thin ResNet.",5.1. Model and General Training Characteristics,[0],[0]
All model parameters are shared amongst all tasks until the final layer.,5.1. Model and General Training Characteristics,[0],[0]
"Although we will focus on the VGG SegNet in our more in-depth analysis, by designing and testing on two extremely different network topologies we will further demonstrate GradNorm’s robustness to the choice of base architecture.
",5.1. Model and General Training Characteristics,[0],[0]
"We use standard pixel-wise loss functions for each task: cross entropy for segmentation, squared loss for depth, and cosine similarity for normals.",5.1. Model and General Training Characteristics,[0],[0]
"As in (Lee et al., 2017), for room layout we generate Gaussian heatmaps for each of 48 room keypoint types and predict these heatmaps with a pixel-wise squared loss.",5.1. Model and General Training Characteristics,[0],[0]
"Note that all regression tasks are quadratic losses (our surface normal prediction uses a cosine loss which is quadratic to leading order), allowing us to use ri(t) for each task i as a direct proxy for each task’s relative inverse training rate.
",5.1. Model and General Training Characteristics,[0],[0]
All runs are trained at a batch size of 24 across 4 Titan X GTX 12GB GPUs and run at 30fps on a single GPU at inference.,5.1. Model and General Training Characteristics,[0],[0]
All NYUv2 runs begin with a learning rate of 2e5.,5.1. Model and General Training Characteristics,[0],[0]
"NYUv2+kpts runs last 80000 steps with a learning rate
decay of 0.2 every 25000 steps.",5.1. Model and General Training Characteristics,[0],[0]
NYUv2+seg runs last 20000 steps with a learning rate decay of 0.2 every 6000 steps.,5.1. Model and General Training Characteristics,[0],[0]
"Updating wi(t) is performed at a learning rate of 0.025 for both GradNorm and the uncertainty weighting ((Kendall et al., 2017)) baseline.",5.1. Model and General Training Characteristics,[0],[0]
"All optimizers are Adam, although we find that GradNorm is insensitive to the optimizer chosen.",5.1. Model and General Training Characteristics,[0],[0]
We implement GradNorm using TensorFlow v1.2.1.,5.1. Model and General Training Characteristics,[0],[0]
In Table 1 we display the performance of GradNorm on the NYUv2+seg dataset.,5.2. Main Results on NYUv2,[0],[0]
"We see that GradNorm α = 1.5 improves the performance of all three tasks with respect to the equal-weights baseline (where wi(t) = 1 for all t,i), and either surpasses or matches (within statistical noise) the best performance of single networks for each task.",5.2. Main Results on NYUv2,[0],[0]
"The GradNorm Static network uses static weights derived from a GradNorm network by calculating the time-averaged weights Et[wi(t)] for each task during a GradNorm training run, and retraining a network with weights fixed to those values.",5.2. Main Results on NYUv2,[0],[0]
GradNorm thus can also be used to extract good values for static weights.,5.2. Main Results on NYUv2,[0],[0]
"We pursue this idea further in Section 5.3 and show that these weights lie very close to the optimal weights extracted from exhaustive grid search.
",5.2. Main Results on NYUv2,[0],[0]
"To show how GradNorm can perform in the presence of a larger dataset, we also perform extensive experiments on the NYUv2+kpts dataset, which is augmented to a factor of 50x more data.",5.2. Main Results on NYUv2,[0],[0]
The results are shown in Table 2.,5.2. Main Results on NYUv2,[0],[0]
"As with the NYUv2+seg runs, GradNorm networks outperform other multitask methods, and either matches (within noise) or surpasses the performance of single-task networks.
",5.2. Main Results on NYUv2,[0],[0]
Figure 3 shows test and training loss curves for GradNorm (α = 1.5) and baselines on the larger NYUv2+kpts dataset for our VGG SegNet models.,5.2. Main Results on NYUv2,[0],[0]
"GradNorm improves test-time depth error by ∼ 5%, despite converging to a much higher training loss.",5.2. Main Results on NYUv2,[0],[0]
"GradNorm achieves this by aggressively rate balancing the network (enforced by a high asymmetry α = 1.5), and ultimately suppresses the depth weight wdepth(t) to lower than 0.10 (see Section 5.4 for more details).",5.2. Main Results on NYUv2,[0],[0]
"The same
trend exists for keypoint regression, and is a clear signal of network regularization.",5.2. Main Results on NYUv2,[0],[0]
"In contrast, uncertainty weighting (Kendall et al., 2017) always moves test and training error in the same direction, and thus is not a good regularizer.",5.2. Main Results on NYUv2,[0],[0]
"Only results for the VGG SegNet are shown here, but the Thin ResNet FCN produces consistent results.",5.2. Main Results on NYUv2,[0],[0]
"For our VGG SegNet, we train 100 networks from scratch with random task weights on NYUv2+kpts.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
Weights are sampled from a uniform distribution and renormalized to sum to T = 3.,5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"For computational efficiency, we only train for 15000 iterations out of the normal 80000, and then compare the performance of that network to our GradNorm
α = 1.5 VGG SegNet network at the same 15000 steps.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"The results are shown in Figure 4.
",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"Even after 100 networks trained, grid search still falls short of our GradNorm network.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"Even more remarkably, there is a strong, negative correlation between network performance and task weight distance to our time-averaged GradNorm weights Et[wi(t)].",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"At an L2 distance of ∼ 3, grid search networks on average have almost double the errors per task compared to our GradNorm network.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
GradNorm has therefore found the optimal grid search weights in one single training run.,5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
The only hyperparameter in our algorithm is the asymmetry α.,5.4. Effects of tuning the asymmetry α,[0],[0]
"The optimal value of α for NYUv2 lies near α = 1.5, while in the highly symmetric toy example in Section 4 we used α = 0.12.",5.4. Effects of tuning the asymmetry α,[0],[0]
"This observation reinforces our characterization of α as an asymmetry parameter.
",5.4. Effects of tuning the asymmetry α,[0],[0]
"Tuning α leads to performance gains, but we found that for NYUv2, almost any value of 0",5.4. Effects of tuning the asymmetry α,[0],[0]
< α < 3 will improve network performance over an equal weights baseline (see Supplementary for details).,5.4. Effects of tuning the asymmetry α,[0],[0]
"Figure 5 shows that higher values of α tend to push the weights wi(t) further apart, which more aggressively reduces the influence of tasks which overfit or learn too quickly (in our case, depth).",5.4. Effects of tuning the asymmetry α,[0],[0]
"Remarkably, at α = 1.75 (not shown) wdepth(t) is suppressed to below 0.02 at no detriment to network performance on the depth task.",5.4. Effects of tuning the asymmetry α,[0],[0]
"Figure 6 shows visualizations of the VGG SegNet outputs on test set images along with the ground truth, for both the NYUv2+seg and NYUv2+kpts datasets.",5.5. Qualitative Results,[0],[0]
"Ground truth labels are juxtaposed with outputs from the equal weights network, 3 single networks, and our best GradNorm network.",5.5. Qualitative Results,[0],[0]
"Some
improvements are incremental, but GradNorm produces superior visual results in tasks for which there are significant quantitative improvements in Tables 1 and 2.",5.5. Qualitative Results,[0],[0]
"We introduced GradNorm, an efficient algorithm for tuning loss weights in a multi-task learning setting based on balancing the training rates of different tasks.",6. Conclusions,[0],[0]
"We demonstrated on both synthetic and real datasets that GradNorm improves multitask test-time performance in a variety of scenarios, and can accommodate various levels of asymmetry amongst the different tasks through the hyperparameter α.",6. Conclusions,[0],[0]
"Our empirical results indicate that GradNorm offers su-
perior performance over state-of-the-art multitask adaptive weighting methods and can match or surpass the performance of exhaustive grid search while being significantly less time-intensive.
",6. Conclusions,[0],[0]
"Looking ahead, algorithms such as GradNorm may have applications beyond multitask learning.",6. Conclusions,[0],[0]
"We hope to extend the GradNorm approach to work with class-balancing and sequence-to-sequence models, all situations where problems with conflicting gradient signals can degrade model performance.",6. Conclusions,[0],[0]
"We thus believe that our work not only provides a robust new algorithm for multitask learning, but also reinforces the powerful idea that gradient tuning is fundamental for training large, effective models on complex tasks.",6. Conclusions,[0],[0]
"Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly.",abstractText,[0],[0]
We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes.,abstractText,[0],[0]
"We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques.",abstractText,[0],[0]
"GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter α.",abstractText,[0],[0]
"Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks.",abstractText,[0],[0]
"Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.",abstractText,[0],[0]
GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,title,[0],[0]
"Deep neural networks have become the state-of-the-art systems for image recognition (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Qiao et al., 2018; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Wang et al., 2017; Zeiler & Fergus, 2013) as well as other vision tasks (Chen et al., 2015; Girshick et al., 2014; Long et al., 2015; Qiao et al., 2017; Ren et al., 2015; Shen et al., 2015; Xie & Tu, 2015).",1. Introduction,[0],[0]
"The architectures keep going deeper, e.g., from five convolutional layers (Krizhevsky et al., 2012) to 1001 layers (He et al., 2016b).",1. Introduction,[0],[0]
"The benefit of deep architectures is their strong learning capacities because each new layer can potentially introduce more non-linearities and typically uses larger receptive fields (Simonyan & Zisserman, 2014).",1. Introduction,[0],[0]
"In addition, adding certain types of layers (e.g. (He et al., 2016b)) will not harm the performance theoretically since they can just learn identity mapping.",1. Introduction,[0],[0]
"This makes stacking up layers more appealing in the network designs.
",1. Introduction,[0],[0]
1Johns Hopkins University 2Shanghai University 3Hikvision Research.,1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Siyuan Qiao <siyuan.qiao@jhu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Although deeper architectures usually lead to stronger learning capacities, cascading convolutional layers (e.g. VGG (Simonyan & Zisserman, 2014)) or blocks (e.g. ResNet (He et al., 2016a)) is not necessarily the only method to achieve this goal.",1. Introduction,[0],[0]
"In this paper, we present a new way to increase the depth of the networks as an alternative to stacking up convolutional layers or blocks.",1. Introduction,[0],[0]
Figure 2 provides an illustration that compares our proposed convolutional network that gradually updates the feature representations against the traditional convolutional network that computes its output simultaneously.,1. Introduction,[0],[0]
"By only adding an ordering to the channels without any additional computation, the later computed channels become deeper than the corresponding ones in the traditional convolutional network.",1. Introduction,[0],[0]
We refer to the neural networks with the proposed computation orderings on the channels as Gradually Updated Neural Networks (GUNN).,1. Introduction,[0],[0]
Figure 1 provides two examples of architecture designs based on cascading building blocks and GUNN.,1. Introduction,[0],[0]
"Without repeating the building blocks, GUNN increases the depths of the networks as well as their learning capacities.
",1. Introduction,[0],[0]
It is clear that converting plain networks to GUNN increases the depths of the networks without any additional computations.,1. Introduction,[0],[0]
"What is less obvious is that GUNN in fact eliminates the overlap singularities inherent in the loss landscapes of the cascading-based convolutional networks, which have been shown to adversely affect the training of deep neural networks as well as their performances (Wei et al., 2008;
Orhan & Pitkow, 2018).",1. Introduction,[0],[0]
"Overlap singularity is when internal neurons collapse into each other, i.e. they are unidentifiable by their activations.",1. Introduction,[0],[0]
"It happens in the networks, increases the training difficulties and degrades the performances (Orhan & Pitkow, 2018).",1. Introduction,[0],[0]
"However, if a plain network is converted to GUNN, the added computation orderings will break the symmetry between the neurons.",1. Introduction,[0],[0]
We prove that the internal neurons in GUNN are impossible to collapse into each other.,1. Introduction,[0],[0]
"As a result, the effective dimensionality can be kept during training and the model will be free from the degeneracy caused by collapsed neurons.",1. Introduction,[0],[0]
"Reflected in the training dynamics and the performances, this means that converting to GUNN will make the plain networks easier to train and perform better.",1. Introduction,[0],[0]
"Figure 3 compares the training dynamics of a 15-layer plain network on CIFAR-10 dataset (Krizhevsky & Hinton, 2009) before and after converted to GUNN.
",1. Introduction,[0],[0]
"In this paper, we test our proposed GUNN on highly competitive benchmark datasets, i.e. CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015).",1. Introduction,[0],[0]
Experimental results demonstrate that our proposed GUNNbased networks achieve the state-of-the-art performances compared with the previous cascading-based architectures.,1. Introduction,[0],[0]
"The research focuses of image recognition have moved from feature designs (Dalal & Triggs, 2005; Lowe, 2004) to architecture designs (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Sermanet et al., 2014; Simonyan
& Zisserman, 2014; Szegedy et al., 2015; Xie et al., 2017; Zeiler & Fergus, 2013) due to the recent success of the deep neural networks.",2. Related Work,[0],[0]
"Highway Networks (Srivastava et al., 2015) proposed architectures that can be trained end-to-end with more than 100 layers.",2. Related Work,[0],[0]
The main idea of Highway Networks is to use bypassing paths.,2. Related Work,[0],[0]
"This idea was further investigated in ResNet (He et al., 2016a), which simplifies the bypassing paths by using only identity mappings.",2. Related Work,[0],[0]
"As learning ultra-deep networks became possible, the depths of the models have increased tremendously.",2. Related Work,[0],[0]
"ResNet with pre-activation (He et al., 2016b) and ResNet with stochastic depth (Huang et al., 2016) even managed to train neural networks with more than 1000 layers.",2. Related Work,[0],[0]
"FractalNet (Larsson et al., 2016) argued that in addition to summation, concatenation also helps train a deep architecture.",2. Related Work,[0],[0]
"More recently, ResNeXt (Xie et al., 2017) used group convolutions in ResNet and outperformed the original ResNet.",2. Related Work,[0],[0]
"DenseNet (Huang et al., 2017b) proposed an architecture with dense connections by feature concatenation.",2. Related Work,[0],[0]
"Dual Path Net (Chen et al., 2017) finds a middle point between ResNet and DenseNet by concatenating them in two paths.",2. Related Work,[0],[0]
"Unlike the above cascading-based methods, GUNN eliminates the overlap singularities caused by the architecture symmetry.",2. Related Work,[0],[0]
"The detailed analyses can be found in Section 4.3.
",2. Related Work,[0],[0]
"Alternative to increasing the depth of the neural networks, another trend is to increase the widths of the networks.",2. Related Work,[0],[0]
"GoogleNet (Szegedy et al., 2015; 2016) proposed an Inception module to concatenate feature maps produced by different filters.",2. Related Work,[0],[0]
"Following ResNet (He et al., 2016a), the WideResNet (Zagoruyko & Komodakis, 2016) argued that compared with increasing the depth, increasing the width of the networks can be more effective in improving the performances.",2. Related Work,[0],[0]
"Besides varying the width and the depth, there are also other design strategies for deep neural networks (Hariharan et al., 2015; Kontschieder et al., 2015; Pezeshki et al., 2016; Rasmus et al., 2015; Yang & Ramanan, 2015).",2. Related Work,[0],[0]
"Deeply-Supervised Nets (Lee et al., 2014) used auxiliary classifiers to provide direct supervisions for the internal layers.",2. Related Work,[0],[0]
"Network in Network (Lin et al., 2013) adds micro perceptrons to the convolutional layers.",2. Related Work,[0],[0]
"We consider a feature transformation F : Rm×n → Rm×n, where n denotes the channel of the features and m denotes the feature location on the 2-D feature map.",3.1. Feature Update,[0],[0]
"For example, F can be a convolutional layer with n channels for both the input and the output.",3.1. Feature Update,[0],[0]
Let x ∈,3.1. Feature Update,[0],[0]
Rm×n be the input and y ∈,3.1. Feature Update,[0],[0]
"Rm×n be the output, we have
y = F(x) (1)
Suppose that F can be decomposed into channel-wise transformation Fc(·) that are independent with eath other, then for any location k and channel c we have
ykc = Fc(xr(k))",3.1. Feature Update,[0],[0]
"(2)
where xr(k) denotes the receptive field of the location k",3.1. Feature Update,[0],[0]
"and Fc denotes the transformation on channel c.
Let UC denote a feature update on channel set C, i.e.,
UC(x) :",3.1. Feature Update,[0],[0]
"y k c = Fc(xr(k)),∀c ∈ C, k ykc",3.1. Feature Update,[0],[0]
"= x k c ,∀c",3.1. Feature Update,[0],[0]
"∈ C, k
(3)
",3.1. Feature Update,[0],[0]
"Then, UC = F when C = {1, ..., n}.",3.1. Feature Update,[0],[0]
"By defining the feature update UC on channel set C, the commonly used one-layer CNN is a special case of feature updates where every channel is updated simultaneously.",3.2. Gradually Updated Neural Networks,[0],[0]
"However, we can also update the channels gradually.",3.2. Gradually Updated Neural Networks,[0],[0]
"For example, the proposed GUNN can be formulated by
GUNN(x) =",3.2. Gradually Updated Neural Networks,[0],[0]
(Ucl ◦ Uc(l−1),3.2. Gradually Updated Neural Networks,[0],[0]
"◦ ... ◦ Uc2 ◦ Uc1)(x)
",3.2. Gradually Updated Neural Networks,[0],[0]
"where l⋃
i=1
ci = {1, 2, ..., n} and ci ∩ cj = Φ, ∀i 6= j
(4) When l = 1, GUNN is equivalent to F .
",3.2. Gradually Updated Neural Networks,[0],[0]
"Note that the number of parameters and computation of GUNN are the same as those of the corresponding F for any partitions c1, ..., cl of {1, ..., n}.",3.2. Gradually Updated Neural Networks,[0],[0]
"However, by decomposing F into channel-wise transformations and sequentially applying them, the later computed channels are deeper than the previous ones.",3.2. Gradually Updated Neural Networks,[0],[0]
"As a result, the depth of the network can be increased, as well as the network’s learning capacity.",3.2. Gradually Updated Neural Networks,[0],[0]
"We consider the residual learning proposed by ResNet (He et al., 2016a) in our model.",3.3. Channel-wise Update by Residual Learning,[0],[0]
"Specifically, we consider the channel-wise transformation Fc : Rm×n → Rm×1 to be
Fc(x) = Gc(x) + xc (5)
",3.3. Channel-wise Update by Residual Learning,[0],[0]
Algorithm 1 Back-propagation for GUNN Input :U(·) =,3.3. Channel-wise Update by Residual Learning,[0],[0]
(Ucl ◦ Uc(l−1),3.3. Channel-wise Update by Residual Learning,[0],[0]
"◦ ... ◦ Uc1)(·), input x,
output y = U(x), gradients ∂L/∂y, and parameters Θ for U .
",3.3. Channel-wise Update by Residual Learning,[0],[0]
"Output :∂L/∂Θ, ∂L/∂x ∂L/∂x← ∂L/∂y for i←",3.3. Channel-wise Update by Residual Learning,[0],[0]
"l to 1 do
yc ←",3.3. Channel-wise Update by Residual Learning,[0],[0]
"xc, ∀c",3.3. Channel-wise Update by Residual Learning,[0],[0]
"∈ ci ∂L/∂y, ∂L/∂Θci ← BP(y, ∂L/∂x, Uci ,Θci) (∂L/∂x)c ← (∂L/∂y)c, ∀c ∈ ci (∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c",3.3. Channel-wise Update by Residual Learning,[0],[0]
"6∈ ci
end
where Gc is a convolutional neural network Gc : Rm×n → Rm×1.",3.3. Channel-wise Update by Residual Learning,[0],[0]
"The motivation of expressing F in a residual learning manner is to reduce overlap singularities (Orhan & Pitkow, 2018), which will be discussed in Section 4.",3.3. Channel-wise Update by Residual Learning,[0],[0]
Here we show the backpropagation algorithm for learning the parameters in GUNN that uses the same amount of computations and memory as in F .,3.4. Learning GUNN by Backpropagation,[0],[0]
"In Eq. 4, let the feature update Uci be parameterized by Θci .",3.4. Learning GUNN by Backpropagation,[0],[0]
"Let BP(x, ∂L/∂y, f,Θ) be the back-propagation algorithm for differentiable function y = f(x; Θ) with the loss L and the parameters Θ. Algorithm 1 presents the back-propagation algorithm for GUNN.",3.4. Learning GUNN by Backpropagation,[0],[0]
"Since Uci has the residual structures (He et al., 2016a), the last two steps can be merged into
(∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c (6)
which further simplifies the implementation.",3.4. Learning GUNN by Backpropagation,[0],[0]
It is easy to see that converting networks to GUNN-based does not increase the memory usage in feed-forwarding.,3.4. Learning GUNN by Backpropagation,[0],[0]
"Given Algorithm 1, converting networks to GUNN will not affect the memory in both the training and the evaluation.",3.4. Learning GUNN by Backpropagation,[0],[0]
Overlap singularities are inherent in the loss landscapes of some network architectures which are caused by the nonidentifiability of subsets of the neurons.,4. GUNN Eliminates Overlap Singularities,[0],[0]
"They are identified and discussed in previous work (Wei et al., 2008; Anandkumar & Ge, 2016; Orhan & Pitkow, 2018), and are shown to be harmful for the performances of deep networks.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Intuitively, overlap singularities exist in architectures where the internal neurons collapse into each other.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"As a result, the models are degenerate and the effective dimensionality is reduced.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"(Orhan & Pitkow, 2018) demonstrated through experiments that residual learning (see Eq. 5) helps to reduce the overlap singularities in deep networks, which partly explains the exceptional performances of ResNet (He et al., 2016a) compared with plain networks.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"In the following, we first use linear transformation as an example to demonstrate
how GUNN-based networks break the overlap singularities.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Then, we generalize the results to ReLU DNN.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Finally, we compare GUNN with the previous state-of-the-art network architectures from the perspective of singularity elimination.",4. GUNN Eliminates Overlap Singularities,[0],[0]
Consider a linear function y = f(x) :,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Rn → Rn such that
yi = n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (7)
Suppose that there exists a pair of collapsed neurons yp and yq (p < q).",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Then, for ∀x, yp = yq, and the equality holds after any number of gradient descents, i.e. ∆yp = ∆yq .
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
Eq. 7 describes a plain network.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"The solution for the existence of yp and yq is that ωp,j = ωq,j ,∀j.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"This is the case that is mostly discussed previously, which happens in the networks and degrades the performances.
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"When we add the residual learning, Eq. 7 becomes
yi = xi + n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (8)
Collapsed neurons require that ωp,p + 1 = ωq,p, ωq,q + 1 = ωp,q.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"This will make the collapse of yp and yq very hard when ω is initialized from a normal distribution N (0, √ 2/n) as in ResNet, but still possible.
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Next, we convert Eq. 8 to GUNN, i.e.,
yi = xi + i−1∑ j=1 ωi,jyj + n∑ j=i ωi,jxj , ∀i ∈ {1, .., n} (9)
Suppose that yp and yq (p < q) collapse.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Consider ∆y, the value difference at x after one step of gradient descent on ω with input x, ∂L/∂y and learning rate .",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"When → 0,
∆yi = ∂L
∂yi ( i−1∑ j=1 y2j + n∑ j=i x2j )",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"+ i−1∑ j=1 ωi,j∆yj (10)
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"As ∆yp = ∆yq,∀x, we have ωq,j = 0, ∀j : p",4.1. Overlap Singularities in Linear Transformations,[0],[0]
< j < q.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"But this condition will be broken in the next update; thus, q = p + 1.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Then, we derive that yp = yq = 0.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
But these will also be broken in the next step of gradient descent optimization.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Hence, yp and yq cannot collapse into each other.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
The complete proof can be found in the appendix.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"In practice, architectures are usually composed of several linear layers and non-linearity layers.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Analyzing all the possible architectures is beyond our scope.,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Here, we discuss the commonly used ReLU DNN, in which only linear transformations and ReLUs are used by simple layer cascading.
",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Following the notations in §3, we use y = G(x) + x, in which G(x) is a ReLU DNN.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Note that G is continuous piecewise linear (PWL) function (Arora et al., 2018), which means that there exists a finite set of polyhedra whose union is Rn, and G is affine linear over each polyhedron.
",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Suppose that we convert G(x)+x to GUNN and there exists a pair of collapsed neurons yp and yq (p < q).,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Then, the set of polyhedra for yp is the same as for yq.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Let P be a polyhedron for yp and yq defined above.,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Then, ∀x,P, i,
yi = xi + i−1∑ j=1 ωi,j(P)yj + n∑ j=i ωi,j(P)xj (11)
where ω(P) denotes the parameters for polyhedron P. Note that on each P, y is a function of x in the form of Eq. 9; hence, yp and yq cannot collapse into each other.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Since the union of all polyhedra is Rn, we conclude that GUNN eliminates the overlap singularities in ReLU DNN.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"The previous two subsections consider the GUNN conversion where |ci| = 1,∀i (see Eq. 4).",4.3. Discussions and Comparisons,[0],[0]
But this will slow down the computation on GPU due to the data dependency.,4.3. Discussions and Comparisons,[0],[0]
"Without specialized hardware or library support, we decide to increase |ci| to > 10.",4.3. Discussions and Comparisons,[0],[0]
"The resulted models run at the speed between ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b).",4.3. Discussions and Comparisons,[0],[0]
But this change introduces singularities into the channels from the same set ci.,4.3. Discussions and Comparisons,[0],[0]
"Then, the residual learning helps GUNN to reduce the singularities within the same set ci since we initialize the parameters from a normal distributionN (0, √ 2/n).",4.3. Discussions and Comparisons,[0],[0]
"We will compare the results of GUNN with and without residual learning in the experiments.
",4.3. Discussions and Comparisons,[0],[0]
We compare GUNN with the state-of-the-art architectures from the perspective of overlap singularities.,4.3. Discussions and Comparisons,[0],[0]
"ResNet (He et al., 2016a) and its variants use residual learning, which reduces but cannot eliminate the singularities.",4.3. Discussions and Comparisons,[0],[0]
"ResNeXt (Xie et al., 2017) uses group convolutions to break the symmetry between groups, which further helps to avoid neuron collapses.",4.3. Discussions and Comparisons,[0],[0]
"DenseNet (Huang et al., 2017b) concatenates the outputs of layers as the input to the next layer.",4.3. Discussions and Comparisons,[0],[0]
"DenseNet and GUNN both create dense connections, while DenseNet reuses the outputs by concatenating and GUNN by adding them back to the inputs.",4.3. Discussions and Comparisons,[0],[0]
But the channels within the same layer of DenseNet are still possible to collapse into each other since they are symmetric.,4.3. Discussions and Comparisons,[0],[0]
"In contrast, adding back makes residual learning possible in GUNN.",4.3. Discussions and Comparisons,[0],[0]
This makes residual learning indispensable in GUNN-based networks.,4.3. Discussions and Comparisons,[0],[0]
"In this section, we will present the details of our architectures for the CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015) datasets.",5. Network Architectures,[0],[0]
"Since the proposed GUNN is a method for increasing the depths of the convolutional networks, specifying the architectures to be converted is equivalent to specifying the GUNN-based architectures.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The architectures before conversion, the Simultaneously Updated Neural Networks (SUNN), become natural baselines for our proposed GUNN networks.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We first study what baseline architectures can be converted.
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"There are two assumptions about the feature transformation F (see Eq. 1): (1) the input and the output sizes are the same, and (2) F is channel-wise decomposable.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To satisfy the first assumption, we will first use a convolutional layer with Batch Normalization (Ioffe & Szegedy, 2015) and ReLU (Nair & Hinton, 2010) to transform the feature space to a new space where the number of the channels is wanted.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To satisfy the second assumption, instead of directly specifying the transform F , we focus on designing Fci , where ci is a subset of the channels (see Eq. 4).",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To be consistent with the term update used in GUNN and SUNN, we refer to Fci as the update units for channels ci.
Bottleneck Update Units In the architectures proposed in this paper, we adopt bottleneck neural networks as shown in Figure 4 for the update units for both the SUNN and GUNN.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Suppose that the update unit maps the input features of channel size nin to the output features of size nout.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Each unit contains three convolutional layers.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The first convolutional layer transforms the input features to K × nout using a 1× 1 convolutional layer.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The second convolutional layer is of kernel size 3× 3, stride 1, and padding 1, outputting the features of size K × nout.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The third layer computes the features of size nout using a 1 × 1 convolutional layer.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The output is then added back to the input, following the residual architecture proposed in ResNet (He et al., 2016a).",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We add batch normalization layer (Ioffe & Szegedy, 2015) and ReLU layer (Nair & Hinton, 2010) after the first and the second convolutional layers, while only adding batch
normalization layer after the third layer.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Stacking up M update units also generates a new one.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"In total, we have two hyperparameters for designing an update unit: the expansion rate K and the number of the 3-layer update units M .
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"One Resolution, One Representation Our architectures will have only one representation at one resolution besides the pooling layers and the convolutional layers that initialize the needed numbers of channels.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Take the architecture in Table 1 as an example.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
There are two processes for each resolution.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The first one is the transition process, which computes the initial features with the dimensions of the next resolution, then down samples it to 1/4 using a 2×2 average pooling.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
A convolutional operation is needed here because F is assumed to have the same input and output sizes.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The next process is using GUNN to update this feature space gradually.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Each channel will only be updated once, and all channels will be updated after this process.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Unlike most of the previous networks, after this two processes, the feature transformations at this resolution are complete.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"There will be no more convolutional layers or blocks following this feature representation, i.e., one resolution, one representation.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Then, the network will compute the initial features for the next resolution, or compute the final vector representation of the entire image by a global average pooling.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"By designing networks in this way, SUNN networks usually have about 20 layers before converting to GUNN-based networks.
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Channel Partitions With the clearly defined update units, we can easily build SUNN and GUNN layers by using the units to update the representations following Eq. 4.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The hyperparameters for the SUNN/GUNN layer are the number of the channels N and the partition over those channels.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"In our proposed architectures, we evenly partition the channels into P segments.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Then, we can useN and P to represent the configuration of a layer.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Together with the hyperparameters in the update units, we have four hyperparameters to tune for one SUNN/GUNN layer, i.e. {N,P,K,M}.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We have implemented two neural networks based on GUNN to compete with the previous state-of-the-art methods on CIFAR datasets, i.e., GUNN-15 and GUNN-24.",5.2. Architectures for CIFAR,[0],[0]
Table 1 shows the big picture of GUNN-15.,5.2. Architectures for CIFAR,[0],[0]
"Here, we present the details of the hyperparameter settings for GUNN-15 and GUNN-24.",5.2. Architectures for CIFAR,[0],[0]
"For GUNN-15, we have three GUNN layers, Conv2, Conv3 and Conv4.",5.2. Architectures for CIFAR,[0],[0]
"The configuration for Conv2 is {N = 240, P = 20,K = 2,M = 1}, the configuration for Conv3 is {N = 300, P = 25,K = 2,M = 1}, and the configuration for Conv4 is {N = 360, P = 30,K = 2,M = 1}.",5.2. Architectures for CIFAR,[0],[0]
"For GUNN-24, based on GUNN-15, we change the number of output channels of Conv1 to 720, Trans1 to 900, Trans2 to 1080, and Trans3 to 1080.",5.2. Architectures for CIFAR,[0],[0]
"The hyperparameters are {N = 720, P = 20,K = 3,M = 2} for
Conv2, {N = 900, P = 25,K = 3,M = 2} for Conv3, and {N = 1080, P = 30,K = 3,M = 2} for Conv3.",5.2. Architectures for CIFAR,[0],[0]
The number of parameters of GUNN-15 is 1585746 for CIFAR10 and 1618236 for CIFAR-100.,5.2. Architectures for CIFAR,[0],[0]
The number of parameters of GUNN-24 is 29534106 for CIFAR-10 and 29631396 for CIFAR-100.,5.2. Architectures for CIFAR,[0],[0]
"The GUNN-15 is aimed to compete with the methods published in an early stage by using a much smaller model, while GUNN-24 is targeted at comparing with ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b) to get the state-of-the-art performance.",5.2. Architectures for CIFAR,[0],[0]
We implement a neural network GUNN-18 to compete with the state-of-the-art neural networks on ImageNet with a similar number of parameters.,5.3. Architectures for ImageNet,[0],[0]
Table 2 shows the big picture of the neural network architecture of GUNN-18.,5.3. Architectures for ImageNet,[0],[0]
"Here, we present the detailed hyperparameters for the GUNN layers in GUNN-18.",5.3. Architectures for ImageNet,[0],[0]
"The GUNN layers include Conv2, Conv3, Conv4 and Conv5.",5.3. Architectures for ImageNet,[0],[0]
"The hyperparameters are {N = 400, P = 10,K = 2,M = 1} for Conv2,
{N = 800, P = 20,K = 2,M = 1} for Conv3, {N = 1600, P = 40,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5.",5.3. Architectures for ImageNet,[0],[0]
The number of parameters is 28909736.,5.3. Architectures for ImageNet,[0],[0]
"The GUNN-18 is targeted at competing with the previous state-of-the-art methods that have similar numbers of parameters, e.g., ResNet50 (Xie et al., 2017), ResNeXt-50 (Xie et al., 2017) and DenseNet-264 (Huang et al., 2017b).
",5.3. Architectures for ImageNet,[0],[0]
We also implement a wider GUNN-based neural networks Wide-GUNN-18 for better capacities.,5.3. Architectures for ImageNet,[0],[0]
"The hyperparameters are {N = 1200, P = 30,K = 2,M = 1} for Conv2, {N = 1600, P = 40,K = 2,M = 1} for Conv3, {N = 2000, P = 50,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5.",5.3. Architectures for ImageNet,[0],[0]
The number of parameters is 45624936.,5.3. Architectures for ImageNet,[0],[0]
"The Wide-GUNN-18 is targeted at competing with ResNet-101, ResNext-101, DPN (Chen et al., 2017) and SENet (Hu et al., 2017).",5.3. Architectures for ImageNet,[0],[0]
"In this section, we demonstrate the effectiveness of the proposed GUNN on several benchmark datasets.",6. Experiments,[0],[0]
"CIFAR CIFAR (Krizhevsky & Hinton, 2009) has two color image datasets: CIFAR-10 (C10) and CIFAR-100 (C100).",6.1. Benchmark Datasets,[0],[0]
Both datasets consist of natural images with the size of 32× 32 pixels.,6.1. Benchmark Datasets,[0],[0]
"The CIFAR-10 dataset has 10 categories, while the CIFAR-100 dataset has 100 categories.",6.1. Benchmark Datasets,[0],[0]
"For both of the datasets, the training and test set contain 50, 000 and 10, 000 images, respectively.",6.1. Benchmark Datasets,[0],[0]
"To fairly compare our method with the state-of-the-arts (He et al., 2016a; Huang et al., 2017b; 2016; Larsson et al., 2016; Lee et al., 2014; Lin et al., 2013; Romero et al., 2014; Springenberg et al., 2014; Srivastava et al., 2015; Xie et al., 2017), we use the same training and testing strategies, as well as the data processing methods.",6.1. Benchmark Datasets,[0],[0]
"Specifically, we adopt a commonly used data augmentation scheme, i.e., mirroring and shifting, for these two datasets.",6.1. Benchmark Datasets,[0],[0]
"We use channel means and standard derivations to normalize the images for data pre-processing.
",6.1. Benchmark Datasets,[0],[0]
"ImageNet The ImageNet dataset (Russakovsky et al., 2015) contains about 1.28 million color images for training and 50, 000 for validation.",6.1. Benchmark Datasets,[0],[0]
The dataset has 1000 categories.,6.1. Benchmark Datasets,[0],[0]
"We adopt the same data augmentation methods as in the state-of-the-art architectures (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017) for training.",6.1. Benchmark Datasets,[0],[0]
"For testing, we use single-crop at the size of 224× 224.",6.1. Benchmark Datasets,[0],[0]
"Following the
state-of-the-arts (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017), we report the validation error rates.",6.1. Benchmark Datasets,[0],[0]
We train all of our networks using stochastic gradient descents.,6.2. Training Details,[0],[0]
"On CIFAR-10/100 (Krizhevsky & Hinton, 2009), the initial learning rate is set to 0.1, the weight decay is set to 1e−4, and the momentum is set to 0.9 without dampening.",6.2. Training Details,[0],[0]
We train the models for 300 epochs.,6.2. Training Details,[0],[0]
The learning rate is divided by 10 at 150th epoch and 225th epoch.,6.2. Training Details,[0],[0]
"We set the batch size to 64, following (Huang et al., 2017b).",6.2. Training Details,[0],[0]
"All the results reported for CIFAR, regardless of the detailed configurations, were trained using 4 NVIDIA Titan X GPUs with the data parallelism.",6.2. Training Details,[0],[0]
"On ImageNet (Russakovsky et al., 2015), the learning rate is also set to 0.1 initially, and decreases following the schedule in DenseNet (Huang et al., 2017b).",6.2. Training Details,[0],[0]
The batch size is set to 256.,6.2. Training Details,[0],[0]
"The network parameters are also initialized following (He et al., 2016a).",6.2. Training Details,[0],[0]
We use 8 Tesla V100 GPUs with the data parallelism to get the reported results.,6.2. Training Details,[0],[0]
"Our results are directly comparable with ResNet, WideResNet, ResNeXt and DenseNet.",6.2. Training Details,[0],[0]
We train two models GUNN-15 and GUNN-24 for the CIFAR-10/100 dataset.,6.3. Results on CIFAR,[0],[0]
Table 3 shows the comparisons between our method and the previous state-of-the-art methods.,6.3. Results on CIFAR,[0],[0]
Our method GUNN achieves the best results in the test of both the single model and the ensemble test.,6.3. Results on CIFAR,[0],[0]
"Here, we use Snapshot Ensemble (Huang et al., 2017a).
",6.3. Results on CIFAR,[0],[0]
Baseline Methods,6.3. Results on CIFAR,[0],[0]
Here we present the details of baseline methods in Table 3.,6.3. Results on CIFAR,[0],[0]
"The performances of ResNet (He et al., 2016a) are reported in Stochastic Depth (Huang et al., 2016) for both C10 and C100.",6.3. Results on CIFAR,[0],[0]
"The WideResNet (Zagoruyko & Komodakis, 2016) WRN-40-10 is reported in their official code repository on GitHub.",6.3. Results on CIFAR,[0],[0]
"The ResNeXt in the third group is of configuration 16 × 64d, which has the best result reported in the paper (Xie et al., 2017).",6.3. Results on CIFAR,[0],[0]
"The DenseNet is of configuration DenseNet-BC (k = 40), which achieves the best performances on CIFAR-10/100.",6.3. Results on CIFAR,[0],[0]
"The Snapshot Ensemble (Huang et al., 2017a) uses 6 DenseNet-100 to ensemble during inference.",6.3. Results on CIFAR,[0],[0]
"We do not compare with methods that use more data augmentation (e.g. (Zhang et al., 2017)) or stronger regularizations (e.g. (Gastaldi, 2017)) for the fairness of comparison.
",6.3. Results on CIFAR,[0],[0]
"Ablation Study For ablation study, we compare GUNN with SUNN, i.e., the networks before the conversion.",6.3. Results on CIFAR,[0],[0]
"Table 5 shows the comparison results, which demonstrate the effectiveness of GUNN.",6.3. Results on CIFAR,[0],[0]
We also compare the performances of GUNN with and without residual learning.,6.3. Results on CIFAR,[0],[0]
"We evaluate the GUNN on the ImageNet classification task, and compare our performances with the state-of-the-art methods.",6.4. Results on ImageNet,[0],[0]
"These methods include VGGNet (Simonyan & Zisserman, 2014), ResNet (He et al., 2016a), ResNeXt (Xie
et al., 2017), DenseNet (Huang et al., 2017b), DPN (Chen et al., 2017) and SENet (Hu et al., 2017).",6.4. Results on ImageNet,[0],[0]
The comparisons are shown in Table 4.,6.4. Results on ImageNet,[0],[0]
"The results of ours, ResNeXt, and DenseNet are directly comparable as these methods use the same framework for training and testing networks.",6.4. Results on ImageNet,[0],[0]
"Table 4 groups the methods by their numbers of parameters, except VGGNet which has 1.38× 108 parameters.
",6.4. Results on ImageNet,[0],[0]
"The results presented in Table 4 demonstrate that with the similar number of parameters, GUNN can achieve comparable performances with the previous state-of-the-art methods.",6.4. Results on ImageNet,[0],[0]
"For GUNN-18, we also conduct an ablation experiment by comparing the corresponding SUNN with GUNN of the same configuration.",6.4. Results on ImageNet,[0],[0]
"Consistent with the experimental results on the CIFAR-10/100 dataset, the proposed GUNN improves the accuracy on ImageNet dataset.",6.4. Results on ImageNet,[0],[0]
"In this paper, we propose Gradually Updated Neural Network (GUNN), a novel, simple yet effective method to increase the depths of neural networks as an alternative to cascading layers.",7. Conclusions,[0],[0]
"GUNN is based on Convolutional Neural Networks (CNNs), but differs from CNNs in the way of computing outputs.",7. Conclusions,[0],[0]
The outputs of GUNN are computed gradually rather than simultaneously as in CNNs in order to increase the depth.,7. Conclusions,[0],[0]
"Essentially, GUNN assumes the input and the output are of the same size and adds a computation ordering to the channels.",7. Conclusions,[0],[0]
The added ordering increases the receptive fields and non-linearities of the later computed channels.,7. Conclusions,[0],[0]
"Moreover, it eliminates the overlap singularities inherent in the traditional convolutional networks.",7. Conclusions,[0],[0]
We test GUNN on the task of image recognition.,7. Conclusions,[0],[0]
"The evaluations are done in three highly competitive benchmarks, CIFAR10, CIFAR-100 and ImageNet.",7. Conclusions,[0],[0]
The experimental results demonstrate the effectiveness of the proposed GUNN on image recognition.,7. Conclusions,[0],[0]
"In the future, since the proposed GUNN can be used to replace CNNs in other neural networks, we will study the applications of GUNN in other visual tasks, such as object detection and semantic segmentation.",7. Conclusions,[0],[0]
"We thank Wanyu Huang, Huiyu Wang and Chenxi Liu for their insightful comments and suggestions.",Acknowledgments,[0],[0]
We gratefully acknowledge funding supports from NSF award CCF-1317376 and ONR N00014-15-1-2356.,Acknowledgments,[0],[0]
This work was also supported in part by the National Natural Science Foundation of China under Grant 61672336.,Acknowledgments,[0],[0]
Depth is one of the keys that make neural networks succeed in the task of large-scale image recognition.,abstractText,[0],[0]
The state-of-the-art network architectures usually increase the depths by cascading convolutional layers or building blocks.,abstractText,[0],[0]
"In this paper, we present an alternative method to increase the depth.",abstractText,[0],[0]
"Our method is by introducing computation orderings to the channels within convolutional layers or blocks, based on which we gradually compute the outputs in a channel-wise manner.",abstractText,[0],[0]
"The added orderings not only increase the depths and the learning capacities of the networks without any additional computation costs, but also eliminate the overlap singularities so that the networks are able to converge faster and perform better.",abstractText,[0],[0]
Experiments show that the networks based on our method achieve the state-of-the-art performances on CIFAR and ImageNet datasets.,abstractText,[0],[0]
Gradually Updated Neural Networks for Large-Scale Image Recognition,title,[0],[0]
"In recent years, there has been an explosion of interest in sequence labelling tasks.",1. Introduction,[0],[0]
"Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) and Sequenceto-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) present powerful approaches to multiple applications, such as Automatic Speech Recognition (ASR) (Chan et al., 2016a; Hannun et al., 2014; Bahdanau et al.,
*Equal contribution 1Baidu Silicon Valley AI Lab, 1195 Bordeaux Dr, Sunnyvale, CA 94089, USA.",1. Introduction,[0],[0]
"Correspondence to: Hairong Liu <liuhairong@baidu.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
2016), machine translation (Sébastien et al., 2015), and parsing (Vinyals et al., 2015).",1. Introduction,[0],[0]
"These methods are based on 1) a fixed and carefully chosen set of basic units, such as words (Sutskever et al., 2014), phonemes (Chorowski et al., 2015) or characters (Chan et al., 2016a), and 2) a fixed and pre-determined decomposition of target sequences into these basic units.",1. Introduction,[0],[0]
"While these two preconditions greatly simplify the problems, especially the training processes, they are also strict and unnecessary constraints, which usually lead to suboptimal solutions.",1. Introduction,[0],[0]
"CTC models are especially harmed by fixed basic units in target space, because they build on the independence assumption between successive outputs in that space - an assumption which is often violated in practice.
",1. Introduction,[0],[0]
"The problem with fixed set of basic units is obvious: it is really hard, if not impossible, to determine the optimal set of basic units beforehand.",1. Introduction,[0],[0]
"For example, in English ASR, if we use words as basic units, we will need to deal with the large vocabulary-sized softmax, as well as rare words and data sparsity problem.",1. Introduction,[0],[0]
"On the other hand, if we use characters as basic units, the model is forced to learn the complex rules of English spelling and pronunciation.",1. Introduction,[0],[0]
"For example, the ""oh"" sound can be spelled in any of following ways, depending on the word it occurs in - { o, oa, oe, ow, ough, eau, oo, ew }.",1. Introduction,[0],[0]
"While CTC can easily model commonly co-occuring grams together, it is impossible to give roughly equal probability to many possible spellings when transcribing unseen words.",1. Introduction,[0],[0]
"Most speech recognition systems model phonemes, sub-phoneme units and senones e.g, (Xiong et al., 2016a) to get around these problems.",1. Introduction,[0],[0]
"Similarly, state-of-the-art neural machine translation systems use pre-segmented word pieces e.g, (Wu et al., 2016a) aiming to find the best of both worlds.
",1. Introduction,[0],[0]
"In reality, groups of characters are typically cohesive units for many tasks.",1. Introduction,[0],[0]
"For the ASR task, words can be decomposed into groups of characters that can be associated with sound (such as ‘tion’ and ‘eaux’).",1. Introduction,[0],[0]
"For the machine translation task, there may be values in decomposing words as root words and extensions (so that meaning may be shared explicitly between ‘paternal’ and ‘paternity’).",1. Introduction,[0],[0]
"Since this information is already available in the training data, it is perhaps, better to let the model figure it out by itself.",1. Introduction,[0],[0]
"At the same time, it raises another import question: how to de-
compose a target sequence into basic units?",1. Introduction,[0],[0]
"This is coupled with the problem of automatic selection of basic units, thus also better to let the model determine.",1. Introduction,[0],[0]
"Recently, there are some interesting attempts in these directions in the seq2seq framework.",1. Introduction,[0],[0]
"For example, Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decomposition to decompose target sequences with variable length units as a function of both input sequence and the output sequence.
",1. Introduction,[0],[0]
"In this work, we propose Gram-CTC - a strictly more general version of CTC - to automatically seek the best set of basic units from the training data, called grams, and automatically decompose target sequences into sequences of grams.",1. Introduction,[0],[0]
"Just as sequence prediction with cross entropy training can be seen as special case of the CTC loss with a fixed alignment, CTC can be seen as a special case of Gram-CTC with a fixed decomposition of target sequences.",1. Introduction,[1.0],"['Just as sequence prediction with cross entropy training can be seen as special case of the CTC loss with a fixed alignment, CTC can be seen as a special case of Gram-CTC with a fixed decomposition of target sequences.']"
"Since it is a loss function, it can be applied to many seq2seq tasks to enable automatic selection of grams and decomposition of target sequences without modifying the underlying networks.",1. Introduction,[0],[0]
"Extensive experiments on multiple scales of data validate that Gram-CTC can improve CTC in terms of both performance and efficiency, and that using Gram-CTC the models outperform state-of-the-arts on standard speech benchmarks.",1. Introduction,[1.0],"['Extensive experiments on multiple scales of data validate that Gram-CTC can improve CTC in terms of both performance and efficiency, and that using Gram-CTC the models outperform state-of-the-arts on standard speech benchmarks.']"
"The basic text units that previous works utilized for text prediction tasks (e.g,, automatic speech recognition, handwriting recognition, machine translation, and image captioning) can be generally divided into two categories: handcrafted ones and learning-based ones.
",2. Related Work,[0],[0]
Hand-crafted Basic Units.,2. Related Work,[0],[0]
"Fixed sets of characters (graphemes) (Graves et al., 2006; Amodei et al., 2015), word-pieces (Wu et al., 2016b; Collobert et al., 2016; Zweig et al., 2016a), words (Soltau et al., 2016; Sébastien et al., 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al., 2016b) have been widely used as basic units for text prediction, but all of them have drawbacks.",2. Related Work,[0],[0]
"Using these fixed deterministic decompositions of text sequences defines a prior, which is not necessarily optimal for end-to-end learning.
",2. Related Work,[0],[0]
• Word-segmented models remove the component of learning to spell and thus enable direct optimization towards reducing Word Error Rate (WER).,2. Related Work,[0],[0]
"However, these models suffer from having to handle a large vocabulary (1.7 million in (Soltau et al., 2016)), out-of-vocabulary words (Soltau et al., 2016; Sébastien et al., 2015) and data sparsity problems (Soltau et al., 2016).
",2. Related Work,[0],[0]
"• Using characters results in much smaller vocabularies (e.g, 26 for English and thousands for Chinese), but it requires much longer contexts compared to using words or word-pieces and poses the challenge of composing characters to words (Graves et al., 2006; Chan et al., 2015),
which is very noisy for languages like English.
",2. Related Work,[0],[0]
"• Word-pieces lie at the middle-ground of words and characters, providing a good trade-off between vocabulary size and context size, while the performance of using word pieces is sensitive to the choice of the word-piece set and its decomposition.
",2. Related Work,[0],[0]
"• For the ASR task, the use of phonemes was popular in the past few decades as it eases acoustic modeling (Lee and Hon, 1988) and good results were reported with phonemic models (Xiong et al., 2016b; Sercu and Goel, 2016).",2. Related Work,[0],[0]
"However, it introduces the uncertainties of mapping phonemes to words during decoding (Doss et al., 2003), which becomes less robust especially for accented speech data.
",2. Related Work,[0],[0]
Learning-based Basic Units.,2. Related Work,[0],[0]
"More recently, attempts have been made to learn basic unit sets automatically.",2. Related Work,[0],[0]
"(Luong and Manning, 2016) proposed a hybrid WordCharacter model which translates mostly at the word level and consults the character components for rare words.",2. Related Work,[0],[0]
"Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decompositions framework to decomposes target sequences with variable length-ed basic units as a function of both input sequence and the output sequence.
",2. Related Work,[0],[0]
"There exist some earlier works on the “unit discovery” task (Cartwright and Brent, 1994; Goldwater et al., 2006).",2. Related Work,[0],[0]
"A standard problem with MLE solutions to this task is that there are degenerate solutions, i.e., predicting the full corpus with probability 1 at the start.",2. Related Work,[0],[0]
Often Bayesian priors or “minimum description length” constraints are used to remedy this.,2. Related Work,[0],[0]
"CTC (Graves et al., 2006) is a very popular method in seq2seq learning since it does not require the alignment information between inputs and outputs, which is usually expensive, if not impossible, to obtain.
",3.1. CTC,[0],[0]
"Since there is no alignment information, CTC marginalizes over all possible alignments.",3.1. CTC,[1.0],"['Since there is no alignment information, CTC marginalizes over all possible alignments.']"
"That is, it tries to maximize p(l|x) =",3.1. CTC,[0],[0]
"∑ π p(π|x), where x is input, and π represent a valid alignment.",3.1. CTC,[0],[0]
"For example, if the size of input is 3, and the output is ‘hi’, whose length is 2, there are three possible alignments, ‘-hi’, ‘h-i’ and ‘hi-’, where ‘-’ represents blank.",3.1. CTC,[1.0],"['For example, if the size of input is 3, and the output is ‘hi’, whose length is 2, there are three possible alignments, ‘-hi’, ‘h-i’ and ‘hi-’, where ‘-’ represents blank.']"
"For the details, please refer to the original paper (Graves et al., 2006).",3.1. CTC,[1.0],"['For the details, please refer to the original paper (Graves et al., 2006).']"
"In CTC, the basic units are fixed, which is not desirable in some applications.",3.2. From CTC to Gram-CTC,[0],[0]
"Here we generalize CTC by considering a sequence of basic units, called gram, as a whole, which is usually more reasonable in many applications.
",3.2. From CTC to Gram-CTC,[0],[0]
"Let G be a set of n-grams of the set of basic units C of the target sequence, and τ be the length of the longest gram in G. A Gram-CTC network has a softmax output layer with |G|+1 units, that is, the probability over all grams inG and one additional symbol, blank.",3.2. From CTC to Gram-CTC,[0],[0]
"To simplify the problem, we also assume C ⊆ G. 1
For an input sequence x of length T , let y = Nw(x) be the sequence of network outputs, and denote by ytk as the probability of the k-th gram at time t, where k is the index of grams in G′ = G ∪ {blank}, then we have
p(π|x)",3.2. From CTC to Gram-CTC,[0],[0]
"= T∏ t=1 ytπt ,∀π ∈ G ′T (1)
Just as in the case of CTC, here we refer to the elements of G′T as paths, and denote them by π, which represents a possible alignment between input and output.",3.2. From CTC to Gram-CTC,[0],[0]
"The difference is that for each word in the target sequence, it may be decomposed into different sequences of grams.",3.2. From CTC to Gram-CTC,[0],[0]
"For example, the word ‘hello’ can only be decomposed into the sequence [‘h’, ‘e’, ‘l’, ‘l’, ‘o’] for CTC (assume uni-gram CTC here), but it also can be decomposed into the sequence [‘he’, ‘ll’, ‘o’] if ‘he’ and ‘ll’ are in G.
For each π, we map it into a target sequence in the same way as CTC using the collapsing function that 1) removes all repeated labels from the path and then 2) removes all blanks.",3.2. From CTC to Gram-CTC,[0],[0]
"Note that essentially it is these rules which de-
1This is because there may be no valid decompositions for some target sequences if C 6⊆ G. Since Gram-CTC will figure out the ideal decomposition of target sequences into grams during training, this condition guarantees that there is at least one valid decomposition for every target sequence.
termine the transitions between the states of adjacent time steps in Figure 1.",3.2. From CTC to Gram-CTC,[0],[0]
This is a many-to-one mapping and we denote it by B. Note that other rules can be adopted here and the general idea presented in this paper does not depend on these specific rules.,3.2. From CTC to Gram-CTC,[0],[0]
"For a target sequence l, B−1(l) represents all paths mapped to l. Then, we have
p(l|x) = ∑
π∈B−1(l)
p(π|x) (2)
",3.2. From CTC to Gram-CTC,[0],[0]
"This equation allows for training sequence labeling models without any alignment information using CTC loss, because it marginalizes over all possible alignments during training.",3.2. From CTC to Gram-CTC,[0],[0]
"Gram-CTC uses the same effect to enable the model to marginalize over not only alignments, but also decompositions of the target sequence.
",3.2. From CTC to Gram-CTC,[0.9999999144175656],"['Gram-CTC uses the same effect to enable the model to marginalize over not only alignments, but also decompositions of the target sequence.']"
"Note that for each target sequence l, the set B−1(l) has O(τ2) more paths than it does in CTC.",3.2. From CTC to Gram-CTC,[0],[0]
"This is because there are O(τ) times more valid states per time step, and each state may have a valid transition from O(τ) states in the previous time step.",3.2. From CTC to Gram-CTC,[1.0],"['This is because there are O(τ) times more valid states per time step, and each state may have a valid transition from O(τ) states in the previous time step.']"
"The original CTC method is thus, a special case of Gram-CTC when G = C and τ = 1.",3.2. From CTC to Gram-CTC,[0],[0]
"While the quadratic increase in the complexity of the algorithm is non trivial, we assert that it is a trivial increase in the overall training time of typical neural networks, where the computation time is dominated by the neural networks themselves.",3.2. From CTC to Gram-CTC,[1.0],"['While the quadratic increase in the complexity of the algorithm is non trivial, we assert that it is a trivial increase in the overall training time of typical neural networks, where the computation time is dominated by the neural networks themselves.']"
"Additionally, the algorithm extends generally to any arbitrary G and need not have all possible n-grams up to length τ .",3.2. From CTC to Gram-CTC,[1.0],"['Additionally, the algorithm extends generally to any arbitrary G and need not have all possible n-grams up to length τ .']"
"To efficiently compute p(l|x), we also adopt the dynamic programming algorithm.",3.3. The Forward-Backward Algorithm,[1.0],"['To efficiently compute p(l|x), we also adopt the dynamic programming algorithm.']"
"The essence here is identifying
the states of the problem, so that we may solve future states by reusing solutions to earlier states.",3.3. The Forward-Backward Algorithm,[0],[0]
"In our case, the state must contain all the information required to identify all valid extensions of an incomplete path π such that the collapsing function will eventually collapse the complete π back to l. For Gram-CTC, this can be done by collapsing all but the last element of the path π.",3.3. The Forward-Backward Algorithm,[1.0],"['In our case, the state must contain all the information required to identify all valid extensions of an incomplete path π such that the collapsing function will eventually collapse the complete π back to l. For Gram-CTC, this can be done by collapsing all but the last element of the path π.']"
"Therefore, the state is a tuple (l1:i, j), where the first item is a collapsed path, representing a prefix of the target label sequence, and j ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0.996614041020234],"['Therefore, the state is a tuple (l1:i, j), where the first item is a collapsed path, representing a prefix of the target label sequence, and j ∈ {0, .']"
", τ} is the length of the last gram (li−j+1:i) used for making the prefix.",3.3. The Forward-Backward Algorithm,[1.0],"[', τ} is the length of the last gram (li−j+1:i) used for making the prefix.']"
j = 0 is valid and means that blank was used.,3.3. The Forward-Backward Algorithm,[0],[0]
"We denote the gram (li−j+1:i) by g j i (l), and the state (l1:i, j) as s j i (l).",3.3. The Forward-Backward Algorithm,[1.0],"['We denote the gram (li−j+1:i) by g j i (l), and the state (l1:i, j) as s j i (l).']"
"For readability, we will further shorten sji (l) to s j i and g j",3.3. The Forward-Backward Algorithm,[0],[0]
i (l) to g j i .,3.3. The Forward-Backward Algorithm,[0],[0]
"For a state s, its corresponding gram is denoted by sg , and the positions of the first character and last character of sg are denoted by b(s) and e(s), respectively.",3.3. The Forward-Backward Algorithm,[1.0],"['For a state s, its corresponding gram is denoted by sg , and the positions of the first character and last character of sg are denoted by b(s) and e(s), respectively.']"
"During dynamic programming, we are dealing with sequence of states, for a state sequence ζ, its corresponding gram sequences is unique, denoted by ζg .
",3.3. The Forward-Backward Algorithm,[0],[0]
Figure 1 illustrates partially the dynamic programming process for the target sequence ‘CAT’.,3.3. The Forward-Backward Algorithm,[0],[0]
Here we suppose G contains all possible uni-grams and bi-grams.,3.3. The Forward-Backward Algorithm,[0],[0]
"Thus, for each character in ‘CAT’, there are three possible states associated with it: 1) the current character, 2) the bi-gram ending in current character, and 3) the blank after current character.",3.3. The Forward-Backward Algorithm,[0],[0]
There is also one blank at beginning.,3.3. The Forward-Backward Algorithm,[0],[0]
"In total we have 10 states.
",3.3. The Forward-Backward Algorithm,[0],[0]
"Supposing the maximum length of grams inG is τ , we first scan l to get the set S of all possible states, such that for all sji ∈ S, its corresponding g j i ∈",3.3. The Forward-Backward Algorithm,[0],[0]
"G′. i ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", |l|}",3.3. The Forward-Backward Algorithm,[0],[0]
"and j ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ}.",3.3. The Forward-Backward Algorithm,[0],[0]
"For a target sequence l, define the forward variable αt(s) for any s ∈ S to the total probability of all valid paths prefixes that end at state s at time t.
αt(s) def = ∑ ζ|B(ζg)=l1:e(s),ζt=s t∏ t′=1 yt ′ ζt′g (3)
",3.3. The Forward-Backward Algorithm,[0],[0]
"Following this definition, we have the following rules for initialization
α1(s) =  y1b s = s 0 0 y1 gii
s = sii ∀i ∈ {1, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ} 0 otherwise
(4)
and recursion
αt(s) =  α̂it−1 ∗ ytb when s = s0i , [α̂i−jt−1 + αt−1(s)] ∗ ytgji when s = sji and g j",3.3. The Forward-Backward Algorithm,[0],[0]
i 6=,3.3. The Forward-Backward Algorithm,[0],[0]
"g j i−j , [α̂i−jt−1 + αt−1(s)− αt−1(s j i−j)] ∗ ytgji
when s = sji and g j i = g j i−j
(5)
where α̂it = ∑τ j=0",3.3. The Forward-Backward Algorithm,[0],[0]
αt(s j i ) and y t b is the probability of blank at time,3.3. The Forward-Backward Algorithm,[0],[0]
"t.
The total probability of the target sequence l is then expressed in the following way:
p(l|x) = τ∑ j=0",3.3. The Forward-Backward Algorithm,[0],[0]
"αT (s j |l|) (6)
similarly, we can define the backward variable βt(s) as:
βt(s) def = ∑ ζ|B(ζg)=lb(s):l,ζt=s T∏ t′=t yt ′ ζt′g (7)
",3.3. The Forward-Backward Algorithm,[0],[0]
"For the initialization and recursion of βt(s), we have
βT",3.3. The Forward-Backward Algorithm,[0],[0]
"(s) =  yTb s = s 0 T yT giT
s = siT ∀i ∈ {1, . .",3.3. The Forward-Backward Algorithm,[0],[0]
.,3.3. The Forward-Backward Algorithm,[0],[0]
", τ} 0 otherwise
(8)
and
βt(s) =  β̂it+1 ∗ ytb when s = s0i , [β̂i+jt+1 + βt+1(s)] ∗ ytgji when s = sji and g j i 6=",3.3. The Forward-Backward Algorithm,[0],[0]
"g j i+j , [β̂i+jt+1 + βt+1(s)− βt+1(s j i+j)] ∗ ytgji
when s = sji and g j i = g j i+j
(9) where β̂it = ∑τ j=0 βt(s j i+j)",3.3. The Forward-Backward Algorithm,[0],[0]
"Similar to CTC, we have the following expression:
p(l|x) = ∑ s∈S αt(s)βt(s)",3.4. BackPropagation,[0],[0]
"ytsg ∀t ∈ {1, . . .",3.4. BackPropagation,[0],[0]
",T} (10)
",3.4. BackPropagation,[0],[0]
"The derivative with regards to ytk is:
∂p(l|x) ∂ytk",3.4. BackPropagation,[0],[0]
"= 1 ytk 2 ∑ s∈lab(l,k) αt(s)βt(s) (11)
where lab(l, k) is the set of states in S whose corresponding gram is",3.4. BackPropagation,[0],[0]
k.,3.4. BackPropagation,[0],[0]
"This is because there may be multiple states corresponding to the same gram.
",3.4. BackPropagation,[0],[0]
"For the backpropagation, the most important formula is the partial derivative of loss with regard to the unnormalized output utk.
∂ ln p(l|x) ∂utk = ytk",3.4. BackPropagation,[0],[0]
"− 1 ytkZt ∑ s∈lab(l,k) αt(s)βt(s) (12)
where Zt def = ∑ s∈S
αt(s)βt(s)",3.4. BackPropagation,[0.9923831958995274],"['∂ ln p(l|x) ∂utk = ytk − 1 ytkZt ∑ s∈lab(l,k) αt(s)βt(s) (12) where Zt def = ∑ s∈S αt(s)βt(s) ytsg .']"
"ytsg .
",3.4. BackPropagation,[0],[0]
(a) Training curves before (blue) and after (orange) auto-refinement of grams.,3.4. BackPropagation,[0],[0]
"(b) Training curves without (blue) and with (orange) joint-training
Gram-CTC C _",3.4. BackPropagation,[0],[0]
"AT
CTC _",3.4. BackPropagation,[0],[0]
C _,3.4. BackPropagation,[0],[0]
"A T _
Gram-CTC C - AT
CTC - C - A T -
(c) Joint-training Architecture
Figure 2.",3.4. BackPropagation,[0],[0]
(Figure 2a) compares the training curves before (blue) and after (orange) auto-refinement of grams.,3.4. BackPropagation,[0],[0]
"They look very similar, although the number of grams is greatly reduced after refinement, which makes training faster and potentially more robust due to less gram sparsity.",3.4. BackPropagation,[1.0],"['They look very similar, although the number of grams is greatly reduced after refinement, which makes training faster and potentially more robust due to less gram sparsity.']"
Figure (2b) Training curve of model with and without joint-training.,3.4. BackPropagation,[0],[0]
"The model corresponding to the orange training curve is jointly trained together with vanilla CTC, such models are often more stable during training.",3.4. BackPropagation,[0],[0]
Figure (2c) Typical joint-training model architecture - vanilla CTC loss is best applied a few levels lower than the Gram-CTC loss.,3.4. BackPropagation,[0],[0]
Here we describe additional techniques we found useful in practice to enable the Gram-CTC to work efficiently as well as effectively.,4. Methodology,[1.0],['Here we describe additional techniques we found useful in practice to enable the Gram-CTC to work efficiently as well as effectively.']
"Although Gram-CTC can automatically select useful grams, it is challenging to train with a large G.",4.1. Iterative Gram Selection,[0],[0]
The total number of possible grams is usually huge.,4.1. Iterative Gram Selection,[0],[0]
"For example, in English, we have 26 characters, then the total number of bi-grams is 262 = 676, the total number of tri-grams are 263 = 17576, . . .",4.1. Iterative Gram Selection,[0.9956126846000837],"['For example, in English, we have 26 characters, then the total number of bi-grams is 262 = 676, the total number of tri-grams are 263 = 17576, .']"
", which grows exponentially and quickly becomes intractable.",4.1. Iterative Gram Selection,[0],[0]
"However, it is unnecessary to consider many grams, such as ‘aaaa’, which are obviously useless.
",4.1. Iterative Gram Selection,[1.0000000754282445],"['However, it is unnecessary to consider many grams, such as ‘aaaa’, which are obviously useless.']"
"In our experiments, we first eliminate most of useless grams from the statistics of a huge corpus, that is, we count the frequency of each gram in the corpus and drop these grams with rare frequencies.",4.1. Iterative Gram Selection,[1.0],"['In our experiments, we first eliminate most of useless grams from the statistics of a huge corpus, that is, we count the frequency of each gram in the corpus and drop these grams with rare frequencies.']"
"Then, we train a model with Gram-CTC on all the remaining grams.",4.1. Iterative Gram Selection,[1.0],"['Then, we train a model with Gram-CTC on all the remaining grams.']"
"By applying (decoding) the trained model on a large speech dataset, we get the real statistics of gram’s usage.",4.1. Iterative Gram Selection,[1.0],"['By applying (decoding) the trained model on a large speech dataset, we get the real statistics of gram’s usage.']"
"Ultimately, we choose high frequency grams together with all uni-grams as our final gram set G. Table 1 shows the impact of iterative gram selection on WSJ (without LM).",4.1. Iterative Gram Selection,[0],[0]
Figure 2a shows its corresponding training curve.,4.1. Iterative Gram Selection,[0],[0]
"For details, please refer to Section 5.2.",4.1. Iterative Gram Selection,[1.0],"['For details, please refer to Section 5.2.']"
"Gram-CTC needs to solve both decomposition and alignment tasks, which is a harder task for a model to learn than CTC.",4.2. Joint Training with Vanilla CTC,[1.0],"['Gram-CTC needs to solve both decomposition and alignment tasks, which is a harder task for a model to learn than CTC.']"
"This is often manifested in unstable training curves, forcing us to lower the learning rate which in turn results
in models converging to a worse optima.",4.2. Joint Training with Vanilla CTC,[0.9999999276425754],"['This is often manifested in unstable training curves, forcing us to lower the learning rate which in turn results in models converging to a worse optima.']"
"To overcome this difficulty, we found it beneficial to train a model with both the Gram-CTC, as well as the vanilla CTC loss (similar to joint-training CTC together with CE loss as mentioned in (Sak et al., 2015)).",4.2. Joint Training with Vanilla CTC,[0],[0]
"Joint training of multiple objectives for sequence labelling has also been explored in previous works (Kim et al., 2016; Kim and Rush, 2016).
",4.2. Joint Training with Vanilla CTC,[0],[0]
"A typical joint-training model looks like Figure 2c, and the corresponding training curve is shown in Figure 2b.",4.2. Joint Training with Vanilla CTC,[0],[0]
The effect of joint-training are shown in Table 4 and Table 5 in the experiments.,4.2. Joint Training with Vanilla CTC,[0],[0]
"We test the Gram-CTC loss on the ASR task, while both CTC and the introduced Gram-CTC are generic techniques for other sequence labelling tasks.",5. Experiments,[0],[0]
"For all of the experiments, the model specification and training procedure are the same as in (Amodei et al., 2015) -",5. Experiments,[0],[0]
"The model is a recurrent neural network (RNN) with 2 two-dimensional convolutional input layers, followed by K forward (Fwd) or bidirectional (Bidi)",5. Experiments,[0],[0]
"Gated Recurrent layers, N cells each, and one fully connected layer before a softmax layer.",5. Experiments,[0],[0]
"In short hand, such a model is written as ‘2x2D Conv - KxN GRU’.",5. Experiments,[0],[0]
"The network is trained end-to-end with the CTC, GramCTC or a weighted combination of both.",5. Experiments,[0],[0]
"This combination is described in the earlier section.
",5. Experiments,[0],[0]
"In all experiments, audio data is is sampled at 16kHz.",5. Experiments,[0],[0]
"Linear FFT features are extracted with a hop size of 10ms and window size of 20ms, and are normalized so that each input feature has zero mean and unit variance.",5. Experiments,[0],[0]
The network inputs are thus spectral magnitude maps ranging from 0-8kHz with 161 features per 10ms frame.,5. Experiments,[0],[0]
"At each epoch, 40% of the utterances are randomly selected to add
background noise to.",5. Experiments,[0],[0]
The optimization method we use is stochastic gradient descent with Nesterov momentum.,5. Experiments,[0],[0]
"Learning hyperparameters (batch-size, learning-rate, momentum, and etc.) vary across different datasets and are tuned for each model by optimizing a hold-out set.",5. Experiments,[0],[0]
Typical values are a learning rate of 10−3 and momentum of 0.99.,5. Experiments,[0],[0]
Wall Street Journal (WSJ).,5.1. Data and Setup,[0],[0]
"This corpora consists primarily of read speech with texts drawn from a machinereadable corpus of Wall Street Journal news text, and contains about 80 hours speech data.",5.1. Data and Setup,[0],[0]
"We used the standard configuration of train si284 dataset for training, dev93 for validation and eval92 for testing.",5.1. Data and Setup,[0],[0]
"This is a relatively ‘clean’ task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).
",5.1. Data and Setup,[0],[0]
Fisher-Switchboard.,5.1. Data and Setup,[0],[0]
"This is a commonly used English conversational telephone speech (CTS) corpora, which contains 2300 hours CTS data.",5.1. Data and Setup,[0],[0]
"Following the previous works (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu and Goel, 2016), evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets.
10K Speech Dataset.",5.1. Data and Setup,[0],[0]
"We conduct large scale ASR experiments on a noisy internal dataset of 10,000 hours.",5.1. Data and Setup,[0],[0]
"This dataset contains speech collected from various scenarios, such as different background noises, far-field, different accents, and so on.",5.1. Data and Setup,[0],[0]
"Due to its inherent complexities, it is a very challenging task, and can thus validate the effectiveness of the proposed method for real-world application.",5.1. Data and Setup,[0],[0]
"We employ the WSJ dataset for demonstrating different strategies of selecting grams for Gram-CTC, since it is a widely used dataset and also small enough for rapid idea verification.",5.2. Gram Selection,[0],[0]
"However, because it is small, we cannot use large grams here due to data sparsity problem.",5.2. Gram Selection,[0],[0]
"Thus, the auto-refined gram set on WSJ is not optimal for other larger datasets, where larger grams could be effectively used, but the procedure of refinement is the same for them.
",5.2. Gram Selection,[0],[0]
"We first train a model using all uni-grams and bi-grams (29
uni-grams and 262 = 676 bi-grams, in total 705 grams), and then do decoding with the obtained model on another speech dataset to get the statistics of the usage of grams.",5.2. Gram Selection,[0],[0]
Top 100 bi-grams together with all 29 uni-grams (autorefined grams) are used for the second round of training.,5.2. Gram Selection,[0],[0]
"For comparison, we also present the result of the best handpicked grams, as well as the results on uni-grams.",5.2. Gram Selection,[0],[0]
"All the results are shown in Table 1.
",5.2. Gram Selection,[0],[0]
Some interesting observations can be found in Table 1.,5.2. Gram Selection,[0],[0]
"First, the performance of auto-refined grams is only slightly better than the combination of all uni-grams and all bigrams.",5.2. Gram Selection,[0],[0]
This is probably because WSJ is so small that gram learning suffers from the data sparsity problem here (similar to word-segmented models).,5.2. Gram Selection,[0],[0]
"The auto-refined gram set contains only a small subset of bi-grams, thus more robust.",5.2. Gram Selection,[0],[0]
"This is also why we only try bi-grams, not including higher-order grams.",5.2. Gram Selection,[0],[0]
"Second, the performance of best handpicked grams is worse than auto-refined grams.",5.2. Gram Selection,[0],[0]
This is desirable.,5.2. Gram Selection,[0],[0]
"It is time-consuming to handpick grams, especially when you consider high-order grams.",5.2. Gram Selection,[0],[0]
"The method of iterative gram selection is not only fast, but usually better.",5.2. Gram Selection,[0],[0]
"Third, the performance of Gram-CTC on auto-refined grams is only slightly better than CTC on uni-grams.",5.2. Gram Selection,[0],[0]
"This is because Gram-CTC is inherently difficult to train, since it needs to learn both decomposition and alignment.",5.2. Gram Selection,[0],[0]
WSJ is too small to provide enough data to train Gram-CTC.,5.2. Gram Selection,[0],[0]
"Using a large time stride for sequence labelling with RNNs can greatly boost the overall computation efficiency, since it effectively reduces the time steps for recurrent computation, thus speeds up the process of both forward inference and backward propagation.",5.3. Sequence Labelling in Large Stride,[0],[0]
"However, the largest stride that can be used is limited by the gram set we use.",5.3. Sequence Labelling in Large Stride,[0],[0]
The (unigram) CTC has to work in a high time resolution (small stride) in order to have enough number of frames to output every character.,5.3. Sequence Labelling in Large Stride,[0],[0]
"This is very inefficient as we know the same acoustic feature could correspond to several grams of different lengths (e.g., {‘i’, ‘igh’, ‘eye’}) .",5.3. Sequence Labelling in Large Stride,[0],[0]
"The larger the grams are, the larger stride we are potentially able to use.
",5.3. Sequence Labelling in Large Stride,[0],[0]
"DS2 (Amodei et al., 2015) employed non-overlapping bigram outputs to allow for a larger stride.",5.3. Sequence Labelling in Large Stride,[0],[0]
"This imposes an artificial constraint forcing the model to learn, not only the spelling of each word, but also how to split words into bigrams.",5.3. Sequence Labelling in Large Stride,[0],[0]
"For example, part is split as [pa, rt] but the word
apart is forced to be decomposed as [ap, ar, t].",5.3. Sequence Labelling in Large Stride,[0],[0]
GramCTC removes this constraint by allowing the model to decompose words into larger units into the most convenient or sensible decomposition.,5.3. Sequence Labelling in Large Stride,[0],[0]
"Comparison results show this change enables Gram-CTC to work much better than bigram CTC, as in Table 2.
",5.3. Sequence Labelling in Large Stride,[0],[0]
"In Table 2, we compare the performance of trained model and training efficiency on two strides, 2 and 4.",5.3. Sequence Labelling in Large Stride,[0],[0]
"For GramCTC, we use the auto-refined gram set from previous section.",5.3. Sequence Labelling in Large Stride,[0],[0]
"As expected, using stride 4 almost cuts the training time per epoch into half, compared to stride 2.",5.3. Sequence Labelling in Large Stride,[0],[0]
"From stride 2 to stride 4, the performance of uni-gram CTC drops quickly.",5.3. Sequence Labelling in Large Stride,[0],[0]
This is because small grams inherently need higher time resolutions.,5.3. Sequence Labelling in Large Stride,[0],[0]
"As for Gram-CTC, from stride 2 to stride 4, its performance decreases a little bit, while in experiments on the other datasets, Gram-CTC constantly works better in stride 4.",5.3. Sequence Labelling in Large Stride,[0],[0]
One possible explanation is that WSJ is too small for Gram-CTC to learn large grams well.,5.3. Sequence Labelling in Large Stride,[0],[0]
"In contrast, the performance of bi-gram CTC is not as good as that of Gram-CTC in either stride.",5.3. Sequence Labelling in Large Stride,[0],[0]
Figure 3 illustrates the max-decoding results of both CTC and Gram-CTC on nine utterances.,5.4. Decoding Examples,[0],[0]
"Here the label set for CTC is the set of all characters, and the label set for GramCTC is an auto-refined gram set containing all uni-grams and some high-frequency high-order grams.",5.4. Decoding Examples,[0],[0]
"Here Gram-
CTC uses stride 4 while CTC uses stride 2.
",5.4. Decoding Examples,[0],[0]
"From Figure 3, we can find that: 1) Gram-CTC does automatically find many intuitive and meaningful grams, such as ‘the’, ‘ng’, and ‘are’.",5.4. Decoding Examples,[0],[0]
2) It also decomposes the sentences into segments which are meaningful in term of pronunciation.,5.4. Decoding Examples,[0],[0]
"This decomposition resembles the phonetic decomposition, but in larger granularity and arguably more natural.",5.4. Decoding Examples,[0],[0]
"3) Since Gram-CTC predicts a chunk of characters (a gram) each time, each prediction utilizes larger context and these characters in the same predicted chunk are dependent, thus potentially more robust.",5.4. Decoding Examples,[0],[0]
"One example is the word ‘will’ in the last sentence in Figure 3. 4) Since the output of network is the probability over all grams, the decoding process is almost the same as CTC, still end-toend.",5.4. Decoding Examples,[0],[0]
This makes such decomposition superior to phonetic decomposition.,5.4. Decoding Examples,[0],[0]
"In summary, Gram-CTC combines the advantages of both CTC on characters and CTC on phonemes.",5.4. Decoding Examples,[0],[0]
"The model used here is [2x2D conv, 3x1280 Bidi GRU] with a CTC or Gram-CTC loss.",5.5.1. WSJ DATASET,[0],[0]
The results are shown in Table 3.,5.5.1. WSJ DATASET,[0],[0]
"For all models we trained, language model can greatly improve their performances, in term of WER.",5.5.1. WSJ DATASET,[0],[0]
"Though this dataset contains very limited amount of text data for learning gram selection and decomposition, Gram-
CTC can still improve the vanilla CTC notably.",5.5.1. WSJ DATASET,[0],[0]
The acoustic model trained here is composed of two 2D convolutions and six bi-directional GRU layer in 2048 dimension.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"The corresponding labels are used for training N-gram language models.
",5.5.2. FISHER-SWITCHBOARD,[0],[0]
• Switchboard,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"English speech 97S62 • Fisher English speech Part 1 - 2004S13, 2004T19 • Fisher English speech Part 2 - 2005S13, 2005T19
We use a sample of the Switchboard-1 portion of the NIST 2002 dataset (2004S11 RT-02) for tuning language model hyper-parameters.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
The evaluation is done on the NIST 2000 set.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
This configuration forms a standard benchmark for evaluating ASR models.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Results are in Table 4.
",5.5.2. FISHER-SWITCHBOARD,[0],[0]
We compare our model against best published results on in-domain data.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"These results can often be improved using out-of-domain data for training the language model, and sometimes the acoustic model as well.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Together these techniques allow (Xiong et al., 2016b) to reach a WER of 5.9 on the SWBD set.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Finally, we experiment on a large noisy dataset collected by ourself for building large-vocabulary Continuous Speech Recognition (LVCSR) systems.",5.5.3. 10K SPEECH DATASET,[0],[0]
"This dataset contains about 10000 hours speech in a diversity of scenarios, such as farfield, background noises, accents.",5.5.3. 10K SPEECH DATASET,[0],[0]
"In all cases, the model is [2x2D Conv, 3x2560 Fwd GRU, LA Conv] with only a change in the loss function.",5.5.3. 10K SPEECH DATASET,[0],[0]
"‘LA Conv’ refers to a look ahead convolution layer as seen in (Amodei et al., 2015) which works together with forward-only RNNs for deployment purpose.
",5.5.3. 10K SPEECH DATASET,[0],[0]
"As with the Fisher-Switchboard dataset, the optimal stride is 4 for Gram-CTC and 2 for vanilla CTC on this dataset.",5.5.3. 10K SPEECH DATASET,[0],[0]
"Thus, in both experiments, both Gram-CTC and vanilla
CTC + Gram-CTC are trained mush faster than vanilla CTC itself.",5.5.3. 10K SPEECH DATASET,[0],[0]
The result is shown in Table 5.,5.5.3. 10K SPEECH DATASET,[0],[0]
Gram-CTC performs better than CTC.,5.5.3. 10K SPEECH DATASET,[0],[0]
"After joint-training with vanilla CTC and alignment information through a CE loss, its performance is further boosted, which verifies joint-training helps training.",5.5.3. 10K SPEECH DATASET,[0],[0]
"In fact, with only a small additional cost of time, it effectively reduces the WER from 27.56% to 25.59% (without language model).",5.5.3. 10K SPEECH DATASET,[0],[0]
"In this paper, we have proposed the Gram-CTC loss to enable automatic decomposition of target sequences into learned grams.",6. Conclusions and Future Work,[0],[0]
We also present techniques to train the Gram-CTC in a clean and stable way.,6. Conclusions and Future Work,[0],[0]
"Our extensive experiments demonstrate the proposed Gram-CTC enables the models to run more efficiently than the vanilla CTC, by using larger stride, while obtaining better performance of sequence labelling.",6. Conclusions and Future Work,[0],[0]
"Comparison experiments on multiplescale datasets show the proposed Gram-CTC obtains stateof-the-art results on various ASR tasks.
",6. Conclusions and Future Work,[0],[0]
"An interesting observation is that the learning of GramCTC implicitly avoids the “degenerated solution” that occurring in the traditional “unit discovery” task, without involving any Bayesian priors or the “minimum description length” constraint.",6. Conclusions and Future Work,[0],[0]
"Using a small gram set that contains only short (up to 5 in our experiments) as well as highfrequency grams may explain the success here.
",6. Conclusions and Future Work,[0],[0]
"We will continue investigating techniques of improving the optimization of Gram-CTC loss, as well as the applications of Gram-CTC for other sequence labelling tasks.",6. Conclusions and Future Work,[0],[0]
Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units.,abstractText,[0],[0]
"These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed.",abstractText,[0],[0]
These drawbacks usually result in sub-optimal performance of modeling sequences.,abstractText,[0],[0]
"In this paper, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC.",abstractText,[0],[0]
"While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of target sequences.",abstractText,[0],[0]
"Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency.",abstractText,[0],[0]
"We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.",abstractText,[0],[0]
Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling,title,[0],[0]
