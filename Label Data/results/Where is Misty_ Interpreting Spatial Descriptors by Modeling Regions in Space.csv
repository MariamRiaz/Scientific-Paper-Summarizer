0,1,label2,summary_sentences
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1214–1223, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"People engage in argumentation in various contexts, both online and in the real life.",1 Introduction,[0],[0]
"Existing definitions of argumentation do not solely focus on giving reasons and laying out a logical framework of premises and conclusions, but also highlight its social purpose which is to convince or to persuade (O’Keefe,
2011; van Eemeren et al., 2014; Blair, 2011).",1 Introduction,[0],[0]
Assessing the quality and strength of perceived arguments therefore plays an inherent role in argumentative discourse.,1 Introduction,[0],[0]
"Despite strong theoretical foundations and plethora of normative theories, such as Walton’s schemes and their critical questions (Walton, 1989), an ideal model of critical discussion in the pragma-dialectic view (Van Eemeren and Grootendorst, 1987), or research into fallacies (Boudry et al., 2015), assessing qualitative criteria of everyday argumentation represents a challenge for argumentation scholars and practitioners (Weltzer-Ward et al., 2009; Swanson et al., 2015; Rosenfeld and Kraus, 2015).
",1 Introduction,[0],[0]
Addressing qualitative aspects of arguments has recently started gaining attention in the field of computational argumentation.,1 Introduction,[0],[0]
"Scoring strength of persuasive essays (Farra et al., 2015; Persing and Ng, 2015), exploring interaction in persuasive dialogues on Reddit (Tan et al., 2016), or detecting convincing arguments (Habernal and Gurevych, 2016) are among recent attempts to tackle the quality of argumentation.",1 Introduction,[0],[0]
"However, these approaches are holistic and do not necessarily explain why a given argument is strong or convincing.
",1 Introduction,[0],[0]
We asked the following research questions.,1 Introduction,[0],[0]
"First, can we assess what makes an argument convincing in a purely empirical fashion as opposite to theoretical normative approaches?",1 Introduction,[0],[0]
"Second, to what extent can the problem be tackled by computational models?",1 Introduction,[0],[0]
"To address these questions, we exploit our recently introduced UKPConvArg1 corpus (Habernal and Gurevych, 2016).",1 Introduction,[0],[0]
"This data set consists of 11,650 argument pairs – two arguments with the
1214
Prompt: Should physical education be mandatory in schools?",1 Introduction,[0],[0]
Stance:,1 Introduction,[0],[0]
"Yes!
Argument 1 Argument 2 PE should be compulsory because it keeps us constantly fit and healthy.",1 Introduction,[0],[0]
"If you really dislike sports, then you can quit it when you’re an adult.",1 Introduction,[0],[0]
"But when you’re a kid, the best thing for you to do is study, play and exercise.",1 Introduction,[0],[0]
If you prefer to be lazy and lie on the couch all day then you are most likely to get sick and unfit.,1 Introduction,[0],[0]
"Besides, PE helps kids be better at teamwork.",1 Introduction,[0],[0]
"physical education should be mandatory cuhz 112,000 people have died in the year 2011 so far and it’s because of the lack of physical activity and people are becoming obese!!!!
",1 Introduction,[0],[0]
"A1 is more convincing than A2, because: • “A1 is more intelligently written and makes
same standpoint to the given topic, annotated with a binary relation describing which argument from the pair is more convincing.",1 Introduction,[0],[0]
Each pair also contains several reasons written in natural language explaining which properties of the arguments influence their convincingness.,1 Introduction,[0],[0]
"An example of such an argument pair is shown in Figure 1.
",1 Introduction,[0],[0]
We use these natural language reasons as a proxy to assess qualitative properties of the arguments in each argument pair.,1 Introduction,[0],[0]
Our main contributions are: (1) We propose empirically inspired labels of quality properties of Web arguments and design a hierarchical annotation scheme.,1 Introduction,[0],[0]
"(2) We create a new large crowd-sourced benchmark data set containing 9,111 argument pairs multi-labeled with 17 categories which is improved by local and global filtering techniques.",1 Introduction,[0],[0]
"(3) We experiment with several computational models, both traditional and neu-
ral network-based, and evaluate their performance quantitatively and qualitatively.
",1 Introduction,[0],[0]
The newly created data set UKPConvArg2 is available under CC-BY-SA license along with the experimental software for full reproducibility at GitHub.1,1 Introduction,[0],[0]
"The growing field of computational argumentation has been traditionally devoted to structural tasks, such as argument component detection and classification (Habernal and Gurevych, 2017; Habernal and Gurevych, 2015), argument structure parsing (Peldszus and Stede, 2015; Stab and Gurevych, 2014), or argument schema classification (Lawrence and Reed, 2015), leaving the issues of argument evaluation or quality assessment as an open future work.
",2 Related Work,[0],[0]
"There are only few attempts to tackle the qualitative aspects of arguments, especially in the Web discourse.",2 Related Work,[0],[0]
Park and Cardie (2014) classified propositions in Web arguments into four classes with respect to their level of verifiability.,2 Related Work,[0],[0]
"Focusing on convincingness of Web arguments, Habernal and Gurevych (2016) annotated 16k pairs of arguments with a binary relation “is more convincing” and also elicited explanation for the annotators’ decisions.
",2 Related Work,[0],[0]
"Recently, research in persuasive essay scoring has started combining holistic approaches based on rubrics for several dimensions typical to this genre with explicit argument detection.",2 Related Work,[0],[0]
"Persing and Ng (2015) manually labeled 1,000 student persuasive essays with a single score on the 1–4 scale and trained a regression predictor with a rich feature set using LIBSVM.",2 Related Work,[0],[0]
"Among traditional features (such as POS or semantic frames), an argument structure parser by Stab and Gurevych (2014) was employed.",2 Related Work,[0],[0]
"Farra et al. (2015) also deal with essay scoring but rather then tackling the argument structure, they focus on methods for detecting opinion expressions.",2 Related Work,[0],[0]
"Persuasive essays however represent a genre with a rather strict qualitative and formal requirements (as taught in curricula) and substantially differ from online argumentation.
",2 Related Work,[0],[0]
"Argument evaluation belongs to the central research topics among argumentation scholars (Toul-
1https://github.com/UKPLab/ emnlp2016-empirical-convincingness
min, 2003; Walton et al., 2008; Van Eemeren and Grootendorst, 1987).",2 Related Work,[0],[0]
"Yet treatment of assessing argumentation quality, persuasiveness, or convincingness is traditionally based on evaluating relevance, sufficiency or acceptability of premises (Govier, 2010; Johnson and Blair, 2006) or categorizing fallacies (Hamblin, 1970; Tindale, 2007).",2 Related Work,[0],[0]
"However, the nature of these normative approaches causes a gap between the ‘ideal’ models and empirically encountered real-world arguments, such as those on the Web (van Eemeren et al., 2014; Walton, 2012).
",2 Related Work,[0],[0]
"Regarding the methodology utilized later in this paper, deep (recursive) neural networks have gained extreme popularity in NLP in recent years.",2 Related Work,[0],[0]
"Long Short-Term Memory networks (LSTM) with Attention mechanism have been applied on textual entailment (Rocktäschel et al., 2016), QuestionAnswering (Golub and He, 2016), or source-code summarization (Allamanis et al., 2016).",2 Related Work,[0],[0]
"As our source data set, we took the publicly available UKPConvArg1 corpus.2 It is based on arguments originated from 16 debates from Web debate platforms createdebate.com and convinceme.net, each debate has two sides (usually pro and con).",3 Data,[0],[0]
"Arguments from each of the 32 debate sides are connected into a set of argument pairs, and each argument pair is annotated with a binary relation (argument A is more/less convincing than argument B), resulting in total into 11,650 argument pairs.",3 Data,[0],[0]
"Annotations performed by Habernal and Gurevych (2016) also contain several reasons written by crowd-workers that explain why a particular argument is more or less convincing; see an example in Figure 1.
",3 Data,[0],[0]
"As these reasons were written in an uncontrolled setting, they naturally reflect the main properties of argument quality in a downstream task, which is to decide which argument from a pair is more convincing.",3 Data,[0],[0]
"It differs from scoring arguments in isolation, which is inherently harder not only due to subjectivity in argument “strength” decision but also because of possible annotator’s prior bias (Habernal and Gurevych, 2016).",3 Data,[0],[0]
"Assessing an argument
2https://github.com/UKPLab/ acl2016-convincing-arguments
in context helps to emphasize its main flaws or strengths.",3 Data,[0],[0]
This approach is also known as knowledge elicitation – acquiring appropriate information from experts by asking ”why?”,3 Data,[0],[0]
"(Reed and Rowe, 2004).
",3 Data,[0],[0]
We therefore used the reasons as a proxy for developing a scheme for labeling argument quality attributes.,3 Data,[0],[0]
"This was done in a purely bottom-up empirical manner, as opposed to using ‘standard’ evaluation criteria known from argumentation literature (Johnson and Blair, 2006; Schiappa and Nordin, 2013).",3 Data,[0],[0]
"In particular, we split all reasons into several reason units by simple preprocessing (splitting using Stanford CoreNLP",3 Data,[0],[0]
"(Manning et al., 2014), segmentation into Elementary Discourse Units by RST tools (Surdeanu et al., 2015)) and identified the referenced arguments (A1 or A2) by pattern matching and dependency parsing.",3 Data,[0],[0]
"For example, each reason from Figure 1 would be transformed into two reason units.3 Overall, we obtained about 70k reason units from the entire UKPConvArg1 corpus.",3 Data,[0],[0]
"In order to develop a code book for assigning a label to each reason unit, we ran several pilot expert annotation studies (each with 200-300 reason units).",3.1 Annotation scheme,[0],[0]
"Having a set of ≈ 25 distinct labels, we ran two larger studies on Amazon Mechanical Turk (AMT), each with 500 reason units and 10 workers.",3.1 Annotation scheme,[0],[0]
"The workers were split into two groups; we then estimated gold labels for each group using MACE (Hovy et al., 2013) and compared both groups’ results in order to find systematic discrepancies.",3.1 Annotation scheme,[0],[0]
"Finally, we ended up with a set of 19 distinct labels (classes).",3.1 Annotation scheme,[0],[0]
"As the number of classes is too big for non-expert crowd workers, we developed a hierarchical annotation process guided by questions that narrow down the final class decision.",3.1 Annotation scheme,[0],[0]
"The scheme is depicted in Figure 2.4 Workers were shown only the reason units without seeing the original arguments.
",3.1 Annotation scheme,[0],[0]
"3We picked this example for its simplicity, in reality the texts are much more fuzzy.
",3.1 Annotation scheme,[0],[0]
"4It might seem that some labels are missing, such as C8-2 and C8-3; these belong to those removed during the pilot studies.",3.1 Annotation scheme,[0],[0]
"We sampled 26,000 unique reason units ordered by the original author competence provided as part of the UKPConvArg corpus.",3.2 Annotation,[0],[0]
We expected that workers with higher competence tend to write better reasons for their explanations.,3.2 Annotation,[0],[0]
"Using the previously introduced scheme, 776 AMT workers annotated the batch during two weeks; we required assignments from 5 workers for a single item.",3.2 Annotation,[0],[0]
"We employed MACE (Hovy et al., 2013) for gold label and worker competence estimation with 95% threshold to ignore the less confident labels.",3.2 Annotation,[0],[0]
"Several workers were rejected based on their low computed competence and other criteria, such as too short submission times.",3.2 Annotation,[0],[0]
"We performed several cleaning procedures to increase quality and consistency of the annotated data (apart from initial MACE filtering already explained above).
",3.3 Data cleaning,[0],[0]
"Local cleaning First, we removed 3,859 reason units annotated either with C1-2 (”not an explanation”) and C8-6 (”too topic-specific”, which usually paraphrases some details from the related argument and is not general enough).",3.3 Data cleaning,[0],[0]
"In the next step, we removed reason units with wrong polarity.",3.3 Data cleaning,[0],[0]
"In particular, all reason units labeled with C8-* or C9-* should refer to the more convincing argument in the argument pair (as they describe positive properties), whereas all reasons with labels C5-*, C6-*, and C7-* should refer to the less convincing argument.",3.3 Data cleaning,[0],[0]
"The target arguments for reason units were known from the heuristic preprocessing (see above); in this step 2,455 units were removed.
",3.3 Data cleaning,[0],[0]
"Global cleaning Since the argument pairs from one debate can be projected into an argument graph (Habernal and Gurevych, 2016), we utilized this ‘global’ context for further consistency cleaning.
",3.3 Data cleaning,[0],[0]
"Suppose we have two argument pairs, P1(A → B) and P2(B → C) (where→ means “is more convincing than”).",3.3 Data cleaning,[0],[0]
"Let P1(RB) be reason unit targeting
B in argument pair P1 and similarly P2(RB) reason unit targeting B in argument pair P2.",3.3 Data cleaning,[0],[0]
"In other words, two reason units target the same argument in two different argument pairs (in one of them the argument is more convincing while in the other pair it is less convincing).",3.3 Data cleaning,[0],[0]
There might then exist contradicting combination of classes for P1(RB) and P2(RB).,3.3 Data cleaning,[0],[0]
"For example classes C9-2 and C7-3 are contradicting, as the same argument cannot be both ”on the topic” and ”off-topic” at the same time.
",3.3 Data cleaning,[0],[0]
"When such a conflict between two reason units occurred, we selected the reason with a higher score using the following formula:
wW ∗ σ   ∑
A=G
wA",3.3 Data cleaning,[0],[0]
"− λ ∑
A 6=G wA
  (1)
where wW is the competence of the original author of the reason unit (originated from the UKPConvArg corpus), A = G are crowdsourced assignments for a single reason unit that match the final predicted gold label, A 6= G are assignments that differ from the final predicted gold label, wA is the competence of worker for assignment A, λ is a penalty for non-gold labels, and σ is the sigmoid function to squeeze the score between 0 and 1.
",3.3 Data cleaning,[0],[0]
"We found 25 types of global contradictions between labels for reason units and used them for cleaning the data; in total 3,790 reason units were removed in this step.",3.3 Data cleaning,[0],[0]
"After all cleaning procedures, annotations from reason units were mapped back to argument pairs, resulting into a multi-label annotation of one or both arguments from the given pair.",3.3 Data cleaning,[0],[0]
"In total 9,111 pairs from the UKPConvArg corpus were annotated.
",3.3 Data cleaning,[0],[0]
"For example, the final annotations of argument pair shown in Figure 1 contain four labels – C8-1 (as the more convincing argument “has more details, information, facts, or examples / more reasons / better reasoning / goes deeper / is more specific”), C9-3 (as the more convincing argument “has provoking question / makes you think”), C5-2 (as the less convincing argument “has language issues / bad grammar /...”), and C6-1 (as the less convincing argument “provides not enough support / ...” ).",3.3 Data cleaning,[0],[0]
"Only four of six reason units for this argument pair were annotated because of the competence score of their authors.
",3.3 Data cleaning,[0],[0]
Table 1 shows number of labels per argument pairs; about a half of the argument pairs have only one label.,3.3 Data cleaning,[0],[0]
Figure 3 shows distribution of label in the entire data set which is heavily skewed towards C8-1 label.,3.3 Data cleaning,[0],[0]
"This is not surprising, as this label was used for reason units pointing out that the more convincing argument provided more reasons, details, information or better reasoning – a feature inherent to argumentation seen as giving reasons (Freeley and Steinberg, 2008).",3.3 Data cleaning,[0],[0]
"Since the qualitative attributes of arguments were annotated indirectly by labeling their corresponding reason units without seeing the original arguments, we wanted to validate correctness of this approach.",3.4 Data validation,[0],[0]
"We designed a validation study, in which workers were shown the original argument pair and two sets of labels.",3.4 Data validation,[0],[0]
"The first set contained the true labels as annotated previously, while we randomly replaced few labels in the second set.",3.4 Data validation,[0],[0]
"The goal was then to decide which set of labels better explains that argument A is
more convincing than argument B.",3.4 Data validation,[0],[0]
"For example, for the argument pair from Figure 1, one set of shown labels would be {C8-1, C9-3, C5-2, C6-1} (the correct set) while the other ‘distracting’ set would be {C8-1, C9-3, C5-1, C7-3} .
",3.4 Data validation,[0],[0]
We randomly sampled 500 argument pairs and collected 9 assignments per pair on AMT; we again used MACE with 95% threshold.,3.4 Data validation,[0],[0]
Accuracy of workers on 235 argument pairs achieved 82%.,3.4 Data validation,[0],[0]
We can thus conclude that workers tend to prefer explanations based on labels from the reason units and using the annotation process presented in this section is reliable.,3.4 Data validation,[0],[0]
"Total costs of the annotations including pilot studies, bonuses, and data validation were USD 3,300.",3.4 Data validation,[0],[0]
"We propose two experiments, both performed in 16- fold cross-domain validation.",4 Experiments,[0],[0]
"In each fold, argument pairs from 15 debates are used and the remaining one is used for testing.",4 Experiments,[0],[0]
"In both experiments, it is assumed that the more convincing argument in a pair is known and we concatenate (using a particular delimiter) both arguments such that the more convincing argument comes first.",4 Experiments,[0],[0]
This experiment is a multi-label classification.,4.1 Predicting full multi-label distribution,[0],[0]
"Given an argument pair annotated with several labels, the goal is to predict all these labels.
",4.1 Predicting full multi-label distribution,[0],[0]
We use two deep learning models.,4.1 Predicting full multi-label distribution,[0],[0]
"Our first model, Bidirectional Long Short-Term Memory (BLSTM) network contains two LSTM blocks (forward and backward), each with 64 hidden units on the output.",4.1 Predicting full multi-label distribution,[0],[0]
The output is concatenated into a single vector and pushed through sigmoid layer with 17 output units (corresponding to 17 labels).,4.1 Predicting full multi-label distribution,[0],[0]
"We use cross entropy loss function in order to minimize distance of label distributions in training and test data (Nam et al., 2014).",4.1 Predicting full multi-label distribution,[0],[0]
"In the input layer, we rely on pre-trained word embeddings from Glove (Pennington et al., 2014) whose weights are updated during training the network.
",4.1 Predicting full multi-label distribution,[0],[0]
"The second models is BLSTM extended with an attention mechanism (Rocktäschel et al., 2016; Golub and He, 2016) combined with convolution layers over the input.",4.1 Predicting full multi-label distribution,[0],[0]
"In particular, the input em-
bedding layer is convoluted using 4 different convolution sizes (2, 3, 5, 7), each with 1,000 randomly initialized weight vectors.",4.1 Predicting full multi-label distribution,[0],[0]
Then we perform maxover-time pooling and concatenate the output into a single vector.,4.1 Predicting full multi-label distribution,[0],[0]
"This vector is used as the attention module in BLSTM.
",4.1 Predicting full multi-label distribution,[0],[0]
We evaluate the system using two widely used metrics in multi-label classification.,4.1 Predicting full multi-label distribution,[0],[0]
"First, Hamming loss is the average per-item per-class total error; the smaller the better (Zhang and Zhou, 2007).",4.1 Predicting full multi-label distribution,[0],[0]
"Second, we report One-error (Sokolova and Lapalme, 2009) which corresponds to the error of the predicted label with highest probability; the smaller the better.",4.1 Predicting full multi-label distribution,[0.9509194571723126],"['Finally, our model aggregates localizations across all words in the sentence, combining the information provided by the phrases to the right of the table and just under the torch (Figure 3e).']"
"We do not report other metrics (such as Area Under PRC-curves, MAP, or cover) as they require tuning a threshold parameter, see a survey by Zhang and Zhou (2014).
",4.1 Predicting full multi-label distribution,[0],[0]
Results from Table 2 do not show significant differences between the two models.,4.1 Predicting full multi-label distribution,[0],[0]
"Putting the oneerror numbers into human performance context can be done only indirectly, as the data validation pre-
sented in Section 3.4 had a different set-up.",4.1 Predicting full multi-label distribution,[0],[0]
"Here we can see that the error rate of the most confident predicted label is about 30%, while human performed similarly by choosing from a two different label sets in a binary settings, so their task was inherently harder.
",4.1 Predicting full multi-label distribution,[0],[0]
Error analysis and discussion We examined outputs from the label distribution prediction for BLSTM/ATT/CNN.,4.1 Predicting full multi-label distribution,[0],[0]
"It turns out that the output layer leans toward predicting the dominant label C8-1, while prediction of other labels is seldom.",4.1 Predicting full multi-label distribution,[0],[0]
"We suspect two causes, first, the highly skewed distribution of labels (see Figure 3) and, second, insufficient training data sizes where 13 classes have less than 1k training examples (while Goodfellow et al. (2016) recommend at least 5k instances per class).
",4.1 Predicting full multi-label distribution,[0],[0]
"Although multi-label classification may be viewed as a set of binary classification tasks that decides for each label independently (and thus allows for employing other ‘standard’ classifiers such as SVM), this so-called binary relevance approach ignores dependencies between the labels.",4.1 Predicting full multi-label distribution,[0],[0]
"That is why we focused directly on deep-learning methods, as they are capable of learning and predicting a full label distribution (Nam et al., 2014).",4.1 Predicting full multi-label distribution,[0],[0]
"In the second experiment, we focus on predicting flaws in arguments using coarse-grained labels.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"While this task makes several simplifications in the labeling, it still provides meaningful insights into argument quality assessment.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"For this purpose, we use only argument pairs where the less convincing argument is labeled with a single label (no multi-label classification).",4.2 Predicting flaws in less convincing arguments,[0],[0]
"Second, we merged all labels from categories C5-",4.2 Predicting flaws in less convincing arguments,[0],[0]
*,4.2 Predicting flaws in less convincing arguments,[0],[0]
C6-* C7-* into three classes corresponding to their parent nodes in the annotation decision schema from Figure 2.,4.2 Predicting flaws in less convincing arguments,[0],[0]
Table 3 shows distribution of the gold data for this task with explanation of the labels.,4.2 Predicting flaws in less convincing arguments,[0],[0]
"It is worth noting that predicting flaws in the less convincing argument is still contextdependent and requires the entire argument pair because some of the quality labels are relative to the more convincing argument (such as “less reasoning” or “not enough support”).
",4.2 Predicting flaws in less convincing arguments,[0],[0]
"For this experiment, we modified the output layer
of the neural models from the previous experiment.",4.2 Predicting flaws in less convincing arguments,[0],[0]
The non-linear output function is softmax and we train the networks using categorical cross-entropy loss.,4.2 Predicting flaws in less convincing arguments,[0],[0]
"We also add another baseline model that employs SVM with RBF kernel5 and a rich set of linguistically motivated features, similarly to (Habernal and Gurevych, 2016).",4.2 Predicting flaws in less convincing arguments,[0],[0]
"The feature set includes uni- and bi-gram presence, ratio of adjective and adverb endings that may signalize neuroticism (Corney et al., 2002), contextuality measure (Heylighen and Dewaele, 2002), dependency tree depth, ratio of exclamation or quotation marks, ratio of modal verbs, counts of several named entity types, ratio of past vs. future tense verbs, POS n-grams, presence of dependency tree production rules, seven different readability measures (e.g., Ari (Senter and Smith, 1967), Coleman-Liau (Coleman and Liau, 1975), Flesch (Flesch, 1948), and others), five sentiment scores (from very negative to very positive) (Socher et al., 2013), spell-checking using standard Unix words, ratio of superlatives, and some surface features such as sentence lengths, longer words count, etc.6 It results into a sparse 60k-dimensional feature vector space.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
Results in Table 4 suggest that the SVM-RBF baseline system performs poorly and its results are on par with a majority class baseline (not reported in detail).,4.2 Predicting flaws in less convincing arguments,[0],[0]
"Both deep learning models significantly outperform the baseline, yielding Macro-F1 score about 0.35.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"The attention-based model performs better than simple BLSTM in two classes (C5 and C6), but the overall Macro-F1 score is not significantly better.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
"5We used LISBVM (Chang and Lin, 2011) with the default hyper-parameters.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"As Fernández-Delgado et al. (2014) show, SVM with gaussian kernels is a reasonable best choice on average.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
"6Detailed explanation of the features can be found directly in the attached source codes.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
Error analysis We manually examined several dozens of predictions where the BLSTM model failed but the BLSTM/ATT/CNN model was correct in order to reveal some phenomena that the system is capable to cope with.,4.2 Predicting flaws in less convincing arguments,[0],[0]
"First, the BLSTM/ATT/CNN model started catching some purely abusive, sarcastic, and attacking arguments.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"Also, the language/grammar issues were revealed in many cases, as well as using slang in arguments.
",4.2 Predicting flaws in less convincing arguments,[0],[0]
Examining predictions in which both systems failed reveal some fundamental limitations of the current purely data-driven computational approach.,4.2 Predicting flaws in less convincing arguments,[0],[0]
"While the problem of not catching off-topic arguments can be probably modeled by incorporating the debate description or some sort of debate topic model into the attention vector, the more common issue of non-sense arguments or fallacious arguments (which seem like actual arguments on the first view) needs much deeper understanding of realworld knowledge, logic, and reasoning.",4.2 Predicting flaws in less convincing arguments,[0],[0]
"This paper presented a novel task in the field of computational argumentation, namely empirical assessment of reasons for argument convincingness.",5 Conclusion,[0],[0]
We created a new large benchmark data set by utilizing a new annotation scheme and several filtering strategies for crowdsourced data.,5 Conclusion,[0],[0]
"Then we tackled two challenging tasks, namely multi-label classification of argument pairs in order to reveal qualitative properties of the arguments, and predicting flaws in the less convincing argument from the given argument pair.",5 Conclusion,[0],[0]
We performed all evaluations in a cross-domain scenario and experimented with feature-rich SVM and two state-of-the-art neural network models.,5 Conclusion,[0],[0]
The results are promising but show that the task is inherently complex as it requires deep reasoning about the presented arguments that goes beyond capabilities of the current computational models.,5 Conclusion,[0],[0]
"By releasing the
UKPConvArg2 data and code to the community, we believe more progress can be made in this direction in the near future.",5 Conclusion,[0],[0]
"This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No I/82806, by the German Institute for Educational Research (DIPF), by the German Research Foundation (DFG) via the GermanIsraeli Project Cooperation (DIP, grant DA 1600/1- 1), by the GRK 1994/1",Acknowledgments,[0],[0]
"AIPHES (DFG), by the ArguAna Project GU 798/20-1 (DFG), and by Amazon Web Services in Education Grant award.",Acknowledgments,[0],[0]
"Lastly, we would like to thank the anonymous reviewers for their valuable feedback.",Acknowledgments,[0],[0]
This article tackles a new challenging task in computational argumentation.,abstractText,[0],[0]
"Given a pair of two arguments to a certain controversial topic, we aim to directly assess qualitative properties of the arguments in order to explain why one argument is more convincing than the other one.",abstractText,[0],[0]
We approach this task in a fully empirical manner by annotating 26k explanations written in natural language.,abstractText,[0],[0]
"These explanations describe convincingness of arguments in the given argument pair, such as their strengths or flaws.",abstractText,[0],[0]
"We create a new crowd-sourced corpus containing 9,111 argument pairs, multilabeled with 17 classes, which was cleaned and curated by employing several strict quality measures.",abstractText,[0],[0]
"We propose two tasks on this data set, namely (1) predicting the full label distribution and (2) classifying types of flaws in less convincing arguments.",abstractText,[0],[0]
Our experiments with feature-rich SVM learners and Bidirectional LSTM neural networks with convolution and attention mechanism reveal that such a novel fine-grained analysis of Web argument convincingness is a very challenging task.,abstractText,[0],[0]
We release the new corpus UKPConvArg2 and the accompanying software under permissive licenses to the research community.,abstractText,[0],[0]
What makes a convincing argument? Empirical analysis and detecting attributes of convincingness in Web argumentation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208–4219 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4208",text,[0],[0]
"Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014).",1 Introduction,[0],[0]
"One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999).",1 Introduction,[0],[0]
"Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017).
",1 Introduction,[0],[0]
"In this study, we examine MRC datasets and discuss what is needed to create datasets suit-
able for the detailed testing of NLU.",1 Introduction,[0],[0]
"Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017).
",1 Introduction,[0],[0]
We conjecture that a situation similar to this occurs in MRC datasets.,1 Introduction,[0],[0]
"Consider the question shown in Figure 1, for example.",1 Introduction,[0],[0]
"Although the question, starting with when, requires an answer that is expressed as a moment in time, there is only one such expression (i.e., November 2014) in the given text (we refer to the text as the context).",1 Introduction,[0],[0]
"In other words, the question has only a single candidate answer.",1 Introduction,[0],[0]
The system can solve it merely by recognizing the entity type required by when.,1 Introduction,[0],[0]
"In addition to this, even if another expression of time appears in other sentences, only one sentence (i.e., s1) appears to be related to the question; thus, the system can easily determine the correct answer by attention, that is, by matching the words appearing both in the context and the ques-
tion.",1 Introduction,[0.9543987071895917],"['High scores correspond to high compatibility; for any given word, we can visualize the set s(xt, ·) of scores assigned to different voxels by interpreting it as logits that encode a probability distribution over blocks in the scene.']"
"Therefore, this kind of question does not require a complex understanding of language—e.g., multiple-sentence reasoning, which is known as a more challenging task (Richardson et al., 2013).
",1 Introduction,[0],[0]
"In Section 3, we define two heuristics, namely entity-type recognition and attention.",1 Introduction,[0],[0]
We specifically analyze the differences in the performance of baseline systems for the following two configurations: (i) questions answerable or unanswerable with the first k tokens; and (ii) questions whose correct answer appears or does not appear in the context sentence that is most similar to the question (henceforth referred to as the most similar sentence).,1 Introduction,[0],[0]
"Although similar heuristics are proposed by Weissenborn et al. (2017), ours are utilized for question filtering, rather than system development; Using these simple heuristics, we split each dataset into easy and hard subsets for further investigation of the baseline performance.
",1 Introduction,[0],[0]
"After conducting the experiments, we analyze the following two points in Section 4.",1 Introduction,[0],[0]
"First, we consider which questions are valid for testing, i.e., reasonably solvable.",1 Introduction,[0],[0]
"Second, we consider what reasoning skills are required and whether this exposes any differences among the subsets.",1 Introduction,[0],[0]
"To investigate these two concerns, we manually annotate sample questions from each subset in terms of validity and required reasoning skills, such as word matching, knowledge inference, and multiple sentence reasoning.
",1 Introduction,[0],[0]
"We examine 12 recently proposed MRC datasets (Table 1), which include answer extraction, description, and multiple-choice styles.",1 Introduction,[0],[0]
We also observe differences based on these styles.,1 Introduction,[0],[0]
"For our baselines, we use two neural-based systems, namely, the Bidirectional Attention Flow (Seo et al., 2017) and the Gated-Attention Reader (Dhingra et al., 2017).
",1 Introduction,[0],[0]
"In Section 5, we describe the advantages and disadvantages of different question styles with regard to evaluating NLU systems.",1 Introduction,[0],[0]
"We also interpret our heuristics for constructing realistic MRC datasets.
",1 Introduction,[0],[0]
"Our contributions are as follows:
•",1 Introduction,[0],[0]
"This study is the first large-scale investigation across recent 12 MRC datasets with three question styles.
",1 Introduction,[0],[0]
"• We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets.
",1 Introduction,[0],[0]
"• We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions.
",1 Introduction,[0],[0]
"We observed the following:
•",1 Introduction,[0],[0]
"The baseline performances for the hard subsets remarkably degrade compared to those of entire datasets.
",1 Introduction,[0],[0]
"• Our annotation study shows that hard questions require knowledge inference and multiplesentence reasoning in comparison with easy questions.
",1 Introduction,[0],[0]
"• Compared to questions with answer extraction and description styles, multiple-choice questions tend to require a broader range of reasoning skills while exhibiting answerability, multiple answer candidates, and unambiguity.
",1 Introduction,[0],[0]
These findings suggest that one might overestimate recent advances in MRC systems.,1 Introduction,[0],[0]
"They also emphasize the importance of considering simple answer-seeking heuristics when sourcing questions, in that a dataset could be easily biased unless such heuristics are employed.1",1 Introduction,[0],[0]
"We analyzed 12 MRC datasets with three question styles: answer extraction, description, and
1All scripts used in this study, along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics.
multiple choice (Table 1).",2.1 Datasets,[0],[0]
"Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015),",2.1 Datasets,[0],[0]
"WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018).",2.1 Datasets,[0],[0]
"We employed the following two widely used baselines.
",2.2 Baseline Systems,[0],[0]
"Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets.",2.2 Baseline Systems,[0],[0]
BiDAF models bi-directional attention between the context and question.,2.2 Baseline Systems,[0],[0]
"It achieved state-of-the-art performance on the SQuAD dataset.
",2.2 Baseline Systems,[0],[0]
"Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets.",2.2 Baseline Systems,[0],[0]
GA has a multi-hop architecture with an attention mechanism.,2.2 Baseline Systems,[0],[0]
"It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets.
",2.2 Baseline Systems,[0],[0]
"Why we used different baseline systems: The multiple-choice style can be transformed to answer extraction, as mentioned in Clark et al. (2018).",2.2 Baseline Systems,[0],[0]
"However, in some datasets, many questions have no textual overlap to determine the correct answer span in the context.",2.2 Baseline Systems,[0],[0]
"Therefore, in order to avoid underestimating the baseline performance of those datasets, we used the GA system which is applicable to multiple choice questions.
",2.2 Baseline Systems,[0.9521812043590642],"['Throughout this section, we will use the example description Misty is to the right of the table and just under the torch.']"
"We scored the performance using exact match (EM)/F1 (Rajpurkar et al., 2016), Rouge-L (Lin, 2004), and accuracy for the answer extraction, description, and multiple-choice datasets, respectively (henceforth, we refer to these collectively as the score, for simplicity).",2.2 Baseline Systems,[0],[0]
"For the description datasets, we determined in advance the answer span of the context that gives the highest Rouge-L score to the human-generated gold answer.",2.2 Baseline Systems,[0],[0]
"We computed the Rouge-L score between
2The ARC Easy and Challenge were collected using different methods; hence, we treated them as different datasets (see Clark et al. (2018) for further details).
",2.2 Baseline Systems,[0],[0]
"the predicted span and the gold answer.3
Reproduction of the baseline performance: We used the same architecture as the official baseline systems unless specified otherwise.",2.2 Baseline Systems,[0],[0]
All systems were trained on the training set and tested on the development/test set of each dataset.,2.2 Baseline Systems,[0],[0]
We also used different hyperparameters for each dataset according to characteristics such as context length (see Appendix A for details).,2.2 Baseline Systems,[0],[0]
We show the baseline performance of both the official results and those from our implementations in Tables 2 and 3.,2.2 Baseline Systems,[0],[0]
Our implementations outperformed or showed comparable performance to the official baseline on most datasets.,2.2 Baseline Systems,[0],[0]
"However, in TriviaQA, MCTest, RACE, and ARC-E, our baseline performance did not reach that of the official baseline, due to differences in architecture or the absence of reported hyperparameters in the literature.",2.2 Baseline Systems,[0],[0]
The first goal of this paper is to determine whether there are unintended biases of the kind exposed in Figure 1 in MRC datasets.,3 Two Filtering Heuristics,[0],[0]
We examined the influence of the two filtering heuristics: (i) entity type recognition (Section 3.1) and (ii) attention (Section 3.2).,3 Two Filtering Heuristics,[0],[0]
We then investigated the performance of the baseline systems on the questions filtered by the defined heuristics (Section 3.3).,3 Two Filtering Heuristics,[0],[0]
"The aim of this heuristic was to detect questions that can be solved based on (i) the existence of a single candidate answer that is restricted by expressions such as “wh-” and “how many,” and (ii) lexical patterns that appear around the correct answer.",3.1 Entity Type-based Heuristic,[0],[0]
"Because the query styles are not uniform across datasets (e.g., MARCO uses search engine queries), we could not directly use interrogatives.",3.1 Entity Type-based Heuristic,[0],[0]
"Instead, we simply provided the first k tokens of questions to the baseline systems.",3.1 Entity Type-based Heuristic,[0],[0]
We chose smaller values for k than the (macro) average of the question length across the datasets (= 12.2 tokens).,3.1 Entity Type-based Heuristic,[0],[0]
"For example, for k = 4 of the question will I qualify for OSAP if I’m new in Canada (excerpted from MARCO), we use will I qualify for.",3.1 Entity Type-based Heuristic,[0],[0]
"Even if the tokens do not have an interrogative, the system may recognize lexical patterns around the correct answer.",3.1 Entity Type-based Heuristic,[0],[0]
"Questions that can be solved
3We used the official evaluation scripts of SQuAD and MS MARCO to compute the EM/F1 and Rouge-L, respectively.
by examining these patterns were also of interest when filtering.
",3.1 Entity Type-based Heuristic,[0],[0]
"Results: Tables 2 and 3 present the results for k = 1, 2, 4.",3.1 Entity Type-based Heuristic,[0],[0]
"In addition, to know the exact ratio of the questions that are solved rather than the scores for the answer extraction and description styles, we counted questions with k = 2 that achieved the score ≥ 0.5.4",3.1 Entity Type-based Heuristic,[0],[0]
"As k decreased, so too did the baseline performance on all datasets in Table 2 except QAngaroo.",3.1 Entity Type-based Heuristic,[0],[0]
"By contrast, in QAngaroo and the multiple-choice datasets, the performance did not degrade so strongly.",3.1 Entity Type-based Heuristic,[0],[0]
"In particular, the difference between the scores on the full and k = 1 questions in QAngaroo was 1.8.",3.1 Entity Type-based Heuristic,[0],[0]
"Because the questions in QAngaroo are not complete sentences, but rather knowledge-base entries that have a blank, such as country of citizenship Henry VI of England, this result implies that the baseline system can infer the answer merely by the first token of questions, i.e., the type of knowledge-base entry.
",3.1 Entity Type-based Heuristic,[0.9502916097371501],"['In the example, the word table would ideally be matched to the uniform reference distribution over blocks that are part of a table, and similarly for the word torch (Figure 3a).']"
"In most multiple-choice datasets, the k = 1 scores were significantly higher than randomchoice scores.",3.1 Entity Type-based Heuristic,[0],[0]
"Given that multiple-choice ques-
4We considered that this threshold is sufficient to judge that the system attends to the correct span because of the potential ambiguity of these styles (see Section 4).
tions offer multiple options that are of valid entity/event types, this gap was not necessarily caused by the limited number of candidate answers, as in the case with the answer extraction datasets.",3.1 Entity Type-based Heuristic,[0.9526119659434338],"['Our offset filters ot are much smaller than our voxel grid, which means that convolving any offset filter with a uniform reference distribution over the voxel grid will also result in a uniform localization distribution (edge effects are immaterial given the small filter size and the fact that Misty is generally not at the immediate edges of the scene).']"
"Therefore, we inferred that in the solved questions, incorrect options appeared less than the correct option did or did not appear at all in the context (such questions were regarded as solvable exclusively using the word match skill, which we analyzed in Section 4).",3.1 Entity Type-based Heuristic,[0],[0]
"Remarkably, although we failed to achieve a higher baseline performance, the score for the complete questions in MCTest was lower than that of the k = 1 questions.",3.1 Entity Type-based Heuristic,[0],[0]
This result showed that the MCTest questions were sufficiently difficult such that it was not especially useful for the baseline system to consider the entire question statement.,3.1 Entity Type-based Heuristic,[0],[0]
"Next, we examined in each dataset (i) how many questions have their correct answers in the most similar sentence and (ii) whether a performance gap exists for such questions (i.e., whether such questions are easier than the others).
",3.2 Attention-based Heuristic,[0],[0]
"We used uni-gram overlap as a similarity mea-
sure.5 We counted how many times question words appeared in each sentence, where question words were stemmed and stopwords were dropped.",3.2 Attention-based Heuristic,[0],[0]
We then checked whether the correct answer appeared in the most similar sentence.,3.2 Attention-based Heuristic,[0],[0]
"For the multiple-choice datasets, we selected the text span that provided the highest Rouge-L score with the correct option as the correct answer.
",3.2 Attention-based Heuristic,[0],[0]
Results: Tables 2 and 3 show the results.,3.2 Attention-based Heuristic,[0],[0]
"Considering the average number of context sentences, most datasets contained a significantly high proportion of questions whose answers were in the most similar sentence.
",3.2 Attention-based Heuristic,[0],[0]
"In the answer extraction and description datasets, except QAngaroo, the baseline performance improved when the correct answer appeared in the most similar sentence, and gaps were found between the performances on these questions and the others.",3.2 Attention-based Heuristic,[0],[0]
These gaps indicated that the dataset may lack balance for testing NLU.,3.2 Attention-based Heuristic,[0],[0]
"If these questions tend to require the word matching skill exclusively, attending the other portion is useful in studying a more realistic NLU, e.g., common-sense reasoning and discourse understanding.",3.2 Attention-based Heuristic,[0],[0]
"Therefore, we investigated whether
5Although there are other similarity measures, we used this basic measure to obtain an intuitive result.
",3.2 Attention-based Heuristic,[0],[0]
"these questions merely require word matching (see Section 4).
",3.2 Attention-based Heuristic,[0],[0]
"Meanwhile, in the first three multiple-choice datasets, the performance differences were marginal or inversed, implying that although the baseline performance was not especially high, the difficulty of these questions for the baseline system was not affected by whether their correct answers appeared in the most similar sentence.
",3.2 Attention-based Heuristic,[0],[0]
We further analyzed the baseline performance after removing the context and leaving only the most similar sentence.,3.2 Attention-based Heuristic,[0],[0]
"In AddSent and QAngaroo, the scores remarkably improved (>20 F1).",3.2 Attention-based Heuristic,[0],[0]
"From this result, we can infer that on these datasets the baseline systems were distracted by other sentences in the context.",3.2 Attention-based Heuristic,[0],[0]
"This observation was supported by the results from the AddSent dataset (Jia and Liang, 2017), which contains manually injected distracting sentences (i.e., adversarial examples).
",3.2 Attention-based Heuristic,[0],[0]
"3.3 Performance on Hard Subsets
In the previous two sections, we observed that in the examined datasets (i) some questions were solved by the baseline systems merely with the first k tokens and/or (ii) the baseline performances increased for questions whose answers were in the most similar sentence.",3.2 Attention-based Heuristic,[0],[0]
"We were concerned that these two will become dominant factors in measuring the baseline performance using the datasets; Hence, we split each development/test set into easy and hard subsets for further investigation.
",3.2 Attention-based Heuristic,[0],[0]
Hard subsets: A hard subset comprised questions (i) whose score is not positive when k = 2 and (ii) whose correct answer does not appear in the most similar sentence.,3.2 Attention-based Heuristic,[0],[0]
The easy subsets comprised the remaining questions.,3.2 Attention-based Heuristic,[0],[0]
We aimed to investigate the gap of the performance values between the easy and hard subsets.,3.2 Attention-based Heuristic,[0],[0]
"If the gap is large, the dataset may be strongly biased toward questions that are solved by recognizing entity types or lexical patterns and may not be suitable for measuring the system’s ability for complex reasoning.
",3.2 Attention-based Heuristic,[0],[0]
Results and clarification: The bottom row of Tables 2 and 3 shows that the baseline performances on the hard subset remarkably decreased in almost all examined datasets.,3.2 Attention-based Heuristic,[0],[0]
These results revealed that we may overestimate the ability of the baseline systems previously perceived.,3.2 Attention-based Heuristic,[0],[0]
"How-
ever, we clarify that our intention is not to remove the questions solved or mitigated by our defined heuristics to create a new hard subset because this may generate new biases as indicated in Gururangan et al. (2018).",3.2 Attention-based Heuristic,[0],[0]
"Rather, we would like to emphasize the importance of the defined heuristics when sourcing questions.",3.2 Attention-based Heuristic,[0],[0]
"Indeed, ill attention to these heuristics can lead to unintended biases.",3.2 Attention-based Heuristic,[0],[0]
"Objectives: To complement the observations in the previous sections, we annotated sampled questions from each subset of the datasets.",4.1 Annotation Specifications,[0],[0]
Our motivation can be summarized as follows: (i) How many questions are valid in each dataset?,4.1 Annotation Specifications,[0],[0]
"That is, the hard questions may not in fact be hard, but just unsolvable, as indicated in Chen et al. (2016).",4.1 Annotation Specifications,[0],[0]
(ii) What kinds of reasoning skills explain the easy/hard questions?,4.1 Annotation Specifications,[0],[0]
"(iii) Are there any differences among the datasets and the question styles?
",4.1 Annotation Specifications,[0],[0]
We annotated the minimum skills required to choose the correct answer among other candidates.,4.1 Annotation Specifications,[0],[0]
"We assumed that the solver knows what type of entity or event is entailed by the question.
",4.1 Annotation Specifications,[0],[0]
Annotation labels:,4.1 Annotation Specifications,[0],[0]
"Our annotation labels (Table 4) were inspired by previous works such as Chen et al. (2016), Trischler et al. (2017), and Lai et al. (2017).",4.1 Annotation Specifications,[0],[0]
"The major modifications were twofold: (i) detailed question validity, including a number of reasonable candidate answers and answer ambiguity, and (ii) posing multiple-sentence reasoning as a skill compatible with other skills.
",4.1 Annotation Specifications,[0],[0]
Reasoning types indeed have other classifications.,4.1 Annotation Specifications,[0],[0]
"For instance, Lai et al. (2017) defined five reasoning types, including attitude analysis and whole-picture reasoning.",4.1 Annotation Specifications,[0],[0]
We incorporated them into the knowledge and meta/whole classes.,4.1 Annotation Specifications,[0],[0]
"Clark et al. (2018) proposed detailed knowledge and reasoning types, but these were specific to science exams and, thus, omitted from our study.
",4.1 Annotation Specifications,[0],[0]
"Independent of the abovementioned reasoning types, we checked whether the question required multiple-sentence reasoning to answer the questions.",4.1 Annotation Specifications,[0],[0]
"As another modification, we extended the notion of “sentence” in our annotation and considered a subordinate clause as a sentence.",4.1 Annotation Specifications,[0],[0]
"This modification was intended to deal with the internal complexity of a sentence with multiple clauses, which can also render a question difficult.
",4.1 Annotation Specifications,[0],[0]
"Settings: For each subset of the datasets, 30 questions were annotated.",4.1 Annotation Specifications,[0],[0]
Therefore we obtained annotations for 30× 2× 12 = 720 questions.,4.1 Annotation Specifications,[0],[0]
The annotation was performed by the authors.,4.1 Annotation Specifications,[0],[0]
"The annotator was given the context, question, and candidate answers for multiple-choice questions along with the correct answer.",4.1 Annotation Specifications,[0],[0]
"To reduce bias, the annotator did not know which easy or hard subset the questions were in, and was not told the predictions and scores of the respective baseline systems.",4.1 Annotation Specifications,[0],[0]
Tables 5 and 6 show the annotation results.,4.2 Annotation Results,[0],[0]
"Validity: TriviaQA, QAngaroo, and ARCs revealed a relatively high unsolvability, which seemed to be caused by the unrelatedness between the questions and their context.",4.2 Annotation Results,[0],[0]
"For example, QAngaroo’s context was gathered from Wikipedia articles that were not necessarily related to the questions.6",4.2 Annotation Results,[0],[0]
"The context passages in ARCs were
6Nonetheless, it is remarkable that even though the dataset was automatically constructed, the remaining valid hard
curated from textbooks that may not provide sufficient information to answer the questions.7 Note
questions were difficult for the baseline system.",4.2 Annotation Results,[0],[0]
"7Our analysis was not intended to undermine the quality
that it is possible for unsolvable questions to be permitted, and that the system must indicate them in some datasets, such as QA4MRE, NewsQA, MARCO, and SQuAD (v2.0).
",4.2 Annotation Results,[0],[0]
"However, for single candidate, we found that few questions had only single-candidate answers.",4.2 Annotation Results,[0],[0]
"Furthermore, there were even fewer singlecandidate answers in AddSent than in SQuAD.",4.2 Annotation Results,[0],[0]
"This result supported the claim that the adversarial examples augmented the number of possible candidate answers, thereby degrading the baseline performance.
",4.2 Annotation Results,[0],[0]
"In our annotation, ambiguous questions were
of these questions.",4.2 Annotation Results,[0],[0]
"We refer readers to Clark et al. (2018).
found to be those with multiple correct spans.",4.2 Annotation Results,[0],[0]
Figure 2 shows an example.,4.2 Annotation Results,[0],[0]
"In this case, several answers aside from “93” were correct.",4.2 Annotation Results,[0],[0]
"Ambiguity is an important feature insofar because it can lead to unstable scoring in EM/F1.
",4.2 Annotation Results,[0],[0]
"The multiple-choice datasets mostly comprised valid questions, with the exception of the unsolvable questions in the ARC datasets.
",4.2 Annotation Results,[0],[0]
"Reasoning skills: We can see that word matching was more important in the easy subsets, and knowledge was more pertinent to the hard subsets in 10 of the 12 datasets.",4.2 Annotation Results,[0],[0]
These results confirmed that the manner by which we split the subsets was successful at filtering questions that were relatively easy in terms of reasoning skills.,4.2 Annotation Results,[0],[0]
"However, we did not observe this trend with paraphrasing, which seemed difficult to distinguish from word matching and knowledge.",4.2 Annotation Results,[0],[0]
"With regard to meta/whole and math/logic, we can see that these skills were needed less in the answer extraction and description datasets.",4.2 Annotation Results,[0],[0]
"They were more pertinent to the multiple-choice datasets.
",4.2 Annotation Results,[0],[0]
Multiple-sentence reasoning:,4.2 Annotation Results,[0],[0]
Multiplesentence reasoning was more correlated with the hard subsets in 10 of the 12 datasets.,4.2 Annotation Results,[0],[0]
"Although NewsQA showed the inverse tendency for word matching, knowledge, and multiple-sentence reasoning, we suspect that this was caused by annotation variance and filtering a large portion of ambiguous questions.",4.2 Annotation Results,[0],[0]
"For relational types, we did not see a significant trend in any particular type.
",4.2 Annotation Results,[0],[0]
"Correlation of labels and baseline scores: Across all examined datasets, we analyzed the correlations between the annotation labels and the scores of each baseline system in Table 7.",4.2 Annotation Results,[0],[0]
"In spite of the small size of the annotated samples, we derived statistically significant correlations for six labels.",4.2 Annotation Results,[0],[0]
These results confirmed that BiDAF performed well for the word matching questions and relatively poorly with the knowledge questions.,4.2 Annotation Results,[0],[0]
"By contrast, we did not observe this trend in GA.",4.2 Annotation Results,[0],[0]
"In this section, we discuss the advantages and disadvantages of the question styles.",5 Discussion,[0.9513497806827144],"['In Figure 5c, we show the effects of replacing the phrase right above with the words in front of.']"
"We also interpret the defined heuristics in terms of constructing more realistic MRC datasets.
",5 Discussion,[0],[0]
"Differences among the question styles: The biggest advantage to the answer extraction style is its ease in generating questions, which enables us to produce large-scale datasets.",5 Discussion,[0],[0]
"In contrast, a disadvantage to this style is that it rarely demands meta/whole and math/logic skills, which can require answers not contained in the context.",5 Discussion,[0],[0]
"Moreover, as observed in Section 4, it seems difficult to guarantee that all possible answer spans are given as the correct answers.",5 Discussion,[0],[0]
"By contrast, the description and multiple-choice styles have the advantage of having no such restrictions on the appearance of candidate answers (Kočiský et al., 2018; Khashabi et al., 2018).",5 Discussion,[0],[0]
"Nonetheless, the description style is difficult to evaluate because the Rouge-L and BLEU scores are insufficient for testing NLU.",5 Discussion,[0],[0]
"Whereas it is easy to evaluate the performance on multiple-choice questions, generating multiple reasonable options requires considerable effort.
",5 Discussion,[0],[0]
"Interpretation of our heuristics: When we regard the MRC task as recognizing textual entailment (RTE) (Dagan et al., 2006), the task requires the reader to construct one or more premises from the context and form the most reasonable hypothesis from the question and candidate answer (Sachan et al., 2015).",5 Discussion,[0],[0]
"Thus, easier questions are those (i) where the reader needs to generate only one hypothesis, and (ii) where the premises directly describe the correct hypothesis.",5 Discussion,[0],[0]
Our two heuristics can also be seen as the formalizations of these criteria.,5 Discussion,[0],[0]
"Therefore, to make questions more realistic, we need to create multiple hypotheses that require complex reasoning to be distinguished.",5 Discussion,[0],[0]
"Moreover, the integration of premises should be complemented by external knowledge to provide sufficient information to verify the correct hypothesis.",5 Discussion,[0],[0]
"Our heuristics and annotation were motivated by unintended biases (Levesque, 2014) and evaluation overfitting (Whiteson et al., 2011), respectively.
",6 Related Work,[0],[0]
"Unintended biases: The MRC task tests a reading process that involves retrieving stored information and performing inferences (Sutcliffe et al.,
2013).",6 Related Work,[0],[0]
"However, constructing datasets that comprehensively require those skills is difficult.",6 Related Work,[0],[0]
"As Levesque (2014) discussed as a desideratum for testing AI, we should avoid creating questions that can be solved by matching patterns, using unintended biases, and selectional restrictions.",6 Related Work,[0],[0]
"For the unintended biases, one suggestive example is the Story Cloze Test (Mostafazadeh et al., 2016), in which a system chooses a sentence among candidates to conclude a given paragraph of the story.",6 Related Work,[0],[0]
"A recent attempt at this task showed that recognizing superficial features in the correct candidate is critical to achieve the state of the art (Schwartz et al., 2017).
",6 Related Work,[0],[0]
"Similarly, in MRC, Weissenborn et al. (2017) proposed context/type matching heuristic to develop a simple neural system.",6 Related Work,[0],[0]
"Min et al. (2018) observed that, in SQuAD, 92% of answerable questions can be answered only using a single context sentence.",6 Related Work,[0],[0]
"In visual question answering, Agrawal et al. (2016) analyzed the behavior of models with the variable length of the first question words.",6 Related Work,[0],[0]
"Khashabi et al. (2018) more recently proposed a dataset with questions for multisentence reasoning.
",6 Related Work,[0],[0]
"Evaluation overfitting: The theory behind evaluating AI distinguishes between taskand skill-oriented approaches (Hernández-Orallo, 2017).",6 Related Work,[0],[0]
"In the task-oriented approach, we usually develop a system and test it on a specific dataset.",6 Related Work,[0],[0]
The developed system sometimes lacks generality but achieves the state of the art for that specific dataset.,6 Related Work,[0],[0]
"Further, it becomes difficult to verify and explain the solution to tasks.",6 Related Work,[0],[0]
"The situation in which we are biased to the specific tasks is called evaluation overfitting (Whiteson et al., 2011).",6 Related Work,[0],[0]
"By contrast, with the skill-oriented approach, we aim to interpret the relationships between tasks and skills.",6 Related Work,[0],[0]
"This orientation can encourage the development of more realistic NLU systems.
",6 Related Work,[0],[0]
"As One of our goals was to investigate whether easy questions are dominant in recent datasets, it did not necessarily require a detailed classification of reasoning types.",6 Related Work,[0],[0]
"Nonetheless, we recognize there are more fine-grained classifications of the required skills for NLU.",6 Related Work,[0],[0]
"For example, Weston et al. (2015) defined 20 skills as a set of toy tasks.",6 Related Work,[0],[0]
Sugawara et al. (2017) also organized 10 prerequisite skills for MRC.,6 Related Work,[0],[0]
LoBue and Yates (2011) and Sammons et al. (2010) analyzed entailment phenomena using detailed classifications in RTE.,6 Related Work,[0],[0]
"For
the ARC dataset, Boratko et al. (2018) proposed knowledge and reasoning types.",6 Related Work,[0],[0]
This study examined MRC questions from 12 datasets to determine what makes such questions easier to answer.,7 Conclusion,[0],[0]
We defined two heuristics that limit candidate answers and thereby mitigate the difficulty of questions.,7 Conclusion,[0],[0]
"Using these heuristics, the datasets were split into easy and hard subsets.",7 Conclusion,[0],[0]
We further annotated the questions with their validity and the reasoning skills needed to answer them.,7 Conclusion,[0],[0]
"Our experiments revealed that the baseline performance degraded with the hard questions, which required knowledge inference and multiple-sentence reasoning compared to easy questions.",7 Conclusion,[0],[0]
These results suggest that one might overestimate the ability of the baseline systems.,7 Conclusion,[0],[0]
They also emphasize the importance of analyzing and reporting the properties of new datasets when released.,7 Conclusion,[0],[0]
One limitation of this work was the heavy cost of the annotation.,7 Conclusion,[0],[0]
"In future research, we plan to explore a method for automatically classifying reasoning types.",7 Conclusion,[0],[0]
This will enable us to evaluate systems through a detailed organization of the datasets.,7 Conclusion,[0],[0]
"We would like to thank Rajarshi Das, Shehzaad Dhuliawala, and anonymous reviewers for their insightful comments.",Acknowledgments,[0],[0]
This work was supported by JSPS KAKENHI Grant Numbers 18H03297 and 18J12960.,Acknowledgments,[0],[0]
"We used different hyperparameters for each dataset because of the different characteristics of the datasets, e.g., the context length.",A Hyperparameters of the Baseline Systems,[0],[0]
Tables 8 and 9 show the hyperparameters.,A Hyperparameters of the Baseline Systems,[0],[0]
A challenge in creating a dataset for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues.,abstractText,[0],[0]
"In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice).",abstractText,[0],[0]
We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets.,abstractText,[0],[0]
We then manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions.,abstractText,[0],[0]
"From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiplechoice questions tend to require a broader range of reasoning skills than answer extraction and description questions.",abstractText,[0],[0]
These results suggest that one might overestimate recent advances in MRC.,abstractText,[0],[0]
What Makes Reading Comprehension Questions Easier?,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 25–32 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,[0],[0]
"Many commercial applications of artificial agents require task-oriented conversational agents that help customers achieve a specific goal, such as making or cancelling a payment or reservation (Zue et al., 2000; Bennacef et al., 1996).",1 Introduction,[0],[0]
"These chatbots must extract relevant information from the user, provide relevant knowledge to her, and issue appropriate system calls to achieve the goal.
",1 Introduction,[0],[0]
"Supervised approaches such as seq2seq models (Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Sordoni et al., 2015b), have recently gained attention in non-task oriented dialog, due to their ability to perform end-to-end learning from expert dialogues1, removing the need for many of the independent modules in traditional systems such as, natural language understanding, dialog state tracker and natural language generator.
",1 Introduction,[0],[0]
Seq2Seq models have also shown promising results on small domain or synthetic task-oriented dialog datasets.,1 Introduction,[0],[0]
"However, performance was much worse when we applied these models to real world
1We refer to an entire session of text exchanges between an agent and a customer as a dialogue.
datasets.",1 Introduction,[0],[0]
"This is in part because end-to-end methods, in general, require large amounts of data before they are able to generate fluent textual responses.",1 Introduction,[0],[0]
"In real world settings, words chosen by human users and agents are not constrained to a fixed vocabulary, and hence we see many lexical variations even among semantically similar dialogs.
",1 Introduction,[0],[0]
"To ensure that information is both conveyed and understood, we want responses to be fluent as well as coherent.",1 Introduction,[0],[0]
We say a response is coherent if it is a sensible response in the dialogue context.,1 Introduction,[0],[0]
"Table 1 shows responses generated by a variant of the seq2seq model, when trained on real customeragent chat transcripts.",1 Introduction,[0],[0]
"The response of the chatbot during the fourth turn2 in Table 1, accepting the customer’s expression of gratitude, is coherent and fluent.",1 Introduction,[0],[0]
Coherence of a response does not necessarily guarantee fluency.,1 Introduction,[0],[0]
"The generated response during the second turn is coherent but not fluent.
",1 Introduction,[0],[0]
"On our customer support dataset, seq2seq models performed well with salutations, but performed poorly both in terms of fluency and coherency on intermediate responses.",1 Introduction,[0],[0]
"The reason being, salutations contain minimal lexical variations across dialogs and occur more frequently when compared to other utterances.",1 Introduction,[0],[0]
"(Koehn and Knowles, 2017) use beam search decoding in Neural Machine Translation to mitigate fluency issues on larger translation datasets.",1 Introduction,[0],[0]
"Typically increasing the beam size improves translation quality, however, increasing beam sizes in Neural MT has shown to produce poor translations (Koehn and Knowles, 2017).
",1 Introduction,[0],[0]
We propose nearest neighbor based approaches that can directly use and replay available expert utterances.,1 Introduction,[0],[0]
"This removes the need for the models to learn the grammar of the language, and allows the models to focus on learning what to say, rather than how to say it.",1 Introduction,[0],[0]
"The nearest neighbor-based
2We define a turn as a pair of text exchanges between the customer and the agent.
25
methods we propose naturally generate more fluent responses, since they use actual agent responses.",1 Introduction,[0],[0]
"However, our results in Table 3 show that they perform poorly in predicting external actions and at ensuring dialogue level coherency.",1 Introduction,[0],[0]
"In contrast, the skip-connection seq2seq models we propose here, learn when to produce external actions and produce more coherent dialogues.",1 Introduction,[0],[0]
"We propose a hybrid model that brings together the strengths of both the approaches.
",1 Introduction,[0],[0]
"The contributions of this paper are as follows:
• We propose skip-connections to handle multiturn dialogue that outperforms previous models.
",1 Introduction,[0],[0]
• We propose a hybrid model where nearest neighbor-based models generate fluent responses and skip-connection models generate accurate responses and external actions.,1 Introduction,[0],[0]
"We show the effectiveness of the belief state representations obtained from the skip-connection model by comparing against previous approaches.
",1 Introduction,[0],[0]
"• To the best of our knowledge, our paper makes the first attempt at evaluating state of the art models on a large real world task with human users.",1 Introduction,[0],[0]
"We show that methods that achieve state of the art performance on synthetic datasets, perform poorly in real world dialog tasks.",1 Introduction,[0],[0]
"Comparing Tables 2 and 3, we see the impact of moving from synthetic to real world datasets, and as a result, find issues with previously proposed models that may have been obscured by the simplicity and regularity of synthetic datasets.",1 Introduction,[0],[0]
"Although seq2seq models have been applied in taskoriented settings (Wen et al., 2017; Williams and
Zweig, 2016; Bordes and Weston, 2016; Zhao and Eskénazi, 2016), they have only been evaluated on small domain or synthetic datasets.
",2 Related Work,[0],[0]
More recent work has focused on representation learning for multi-turn dialogue.,2 Related Work,[0],[0]
Sordoni et al. (2015b) use a single bag-of-words representation of the entire dialog history.,2 Related Work,[0],[0]
"Such a representation ignores the order of responses, which is crucial to ensure that utterances are coherent across turns.",2 Related Work,[0.9589218036786041],['The first component of our model is responsible for associating words with the voxels that they refer to.']
An alternative approach is to use a hierarchical encoder-decoder network (HRED),2 Related Work,[0],[0]
"(Sordoni et al., 2015a) which uses a complex three layered RNN network, a query level encoder, a session level encoder and a decoder.",2 Related Work,[0],[0]
"Attentional networks (Bordes and Weston, 2016; Dodge et al., 2015) use a weighted combination of all the context vectors upto the current turn.",2 Related Work,[0],[0]
Attentional networks proved to be a stronger baseline over HRED during our evaluation.,2 Related Work,[0],[0]
"We propose models that learn fixed size representations of the history using simpler skip-connection models showing comparable performance with attentional networks (Bordes and Weston, 2016; Dodge et al., 2015).
",2 Related Work,[0],[0]
Our work is closely related to retrieval-based chatbots.,2 Related Work,[0],[0]
"Williams and Zweig (2016), select a response from a small set of templates.",2 Related Work,[0],[0]
"Zhou et al. (2016); Yan et al. (2016) perform multi-turn dialogue by treating the dialogue history as the query, and perform classification with the number of classes equal to the number of possible responses.",2 Related Work,[0],[0]
"They evaluate precision@K, from a restricted list, but do not indicate how this list is obtained in practice.",2 Related Work,[0],[0]
"In our real world dataset, the number of possible responses grows with the dataset size.",2 Related Work,[0],[0]
"In addition, responses are unevenly distributed with salutations occurring frequently.",2 Related Work,[0],[0]
"As a
result, the classification based approach performed poorly, with most of the outputs being salutations.",2 Related Work,[0],[0]
Complete automation of customer service is still not possible as chatbots are not perfect yet.,3 Proposed Approach,[0],[0]
"However, automation where possible in the workflow could still result in considerable savings.",3 Proposed Approach,[0],[0]
"In order to ensure that the end user experience is not substandard, in live user testing, we ask a human agent to play intermediary role between the chatbot and the user.",3 Proposed Approach,[0],[0]
A user initiates a chat by entering an initial query or an issue that requires resolution (Figure 1).,3 Proposed Approach,[0],[0]
The chatbot responds with 5 diverse responses.,3 Proposed Approach,[0],[0]
"The agent selects the most relevant response, and may choose to modify it.",3 Proposed Approach,[0],[0]
"If the response is not relevant, she may type a different response.",3 Proposed Approach,[0],[0]
"During offline testing, the chatbot returns only one response and no human agent is used.",3 Proposed Approach,[0],[0]
The following section describes our skip connection seq2seq model for representation learning and our nearest neighbor approach for response selection.,3 Proposed Approach,[0],[0]
First we describe the datasets and metrics we use.,3 Proposed Approach,[0],[0]
"We use data from bAbI (Task1 and Task2) (Bordes and Weston, 2016) to evaluate our models.",3.1 Dataset and Metrics,[0],[0]
"Other dialog tasks in bAbI require the model to mimic a knowledge base i.e., memorize it.",3.1 Dataset and Metrics,[0],[0]
"This is not a suitable strategy for our application, since in practice knowledge bases undergo frequent changes, making this infeasible.",3.1 Dataset and Metrics,[0],[0]
"In the bAbI task, the user interacts with an agent in a simulated restaurant reservation application, by providing her constraints, such as place, cuisine, number of people or price range.",3.1 Dataset and Metrics,[0],[0]
"The agent or chatbot performs external actions or SQL-like queries (api call) to retrieve information
from the knowledge base of restaurants.",3.1 Dataset and Metrics,[0],[0]
"We used 80% of the data for training (of which 10% was used for validation) and the remaining 20% for testing.
",3.1 Dataset and Metrics,[0],[0]
We also evaluate our models on an internal customer support dataset of 160k chat transcripts containing 3 million interactions.,3.1 Dataset and Metrics,[0],[0]
We limit the number of turns to 20.,3.1 Dataset and Metrics,[0],[0]
We will refer to this dataset as CS large.,3.1 Dataset and Metrics,[0],[0]
"We perform spell correction, deidentification to remove customer sensitive information, lexical normalization particularly of lingo words such as, lol and ty.",3.1 Dataset and Metrics,[0],[0]
Generalizing such entities reduces the amount of training data required.,3.1 Dataset and Metrics,[0],[0]
"The values must be reinserted, currently by a human in the loop.",3.1 Dataset and Metrics,[0],[0]
"We have also masked product and the organization name in the examples.
",3.1 Dataset and Metrics,[0],[0]
"The use of MT evaluation metrics to evaluate dialogue fluency with just one reference has been debated (Liu et al., 2016).",3.1 Dataset and Metrics,[0],[0]
"There is still no good alternative to evaluate dialog systems, and so we continue to report fluency using BLEU (BiLingual Evaluation Understudy (Papineni et al., 2002)), in addition to other metrics and human evaluations.",3.1 Dataset and Metrics,[0],[0]
"Coherency also requires measuring correctness of the external actions which we measure using a metric we call, Exact Query Match (EQM), which represents the fraction of times the api call matched the ground truth query issued by the human agent.",3.1 Dataset and Metrics,[0],[0]
We do not assign any credit to partial matches.,3.1 Dataset and Metrics,[0],[0]
"In addition, we report the precision (P), recall (R) and accuracy (Acc) achieved by the models in predicting whether to make an api call (positive) or not (negative).",3.1 Dataset and Metrics,[0],[0]
Obtaining and aligning api calls with the chat transcripts is often complex as such information is typically stored in multiple confidential logs.,3.1 Dataset and Metrics,[0],[0]
"In order to measure coherency with respect to api calls, we randomly sampled 1000 chat tran-
scripts and asked human agents to hand annotate the api calls wherever appropriate.",3.1 Dataset and Metrics,[0],[0]
We will refer to this labeled dataset as CS small.,3.1 Dataset and Metrics,[0],[0]
"Seq2seq models are an application of Long ShortTerm Memory (Hochreiter and Schmidhuber, 1997) architecture where inputs and outputs are variable length sequences.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
We unroll the basic seq2seq model and make one copy for each turn.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
This is illustrated in Figure 2.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"Input words are one hot encoded, and projected using a linear layer to obtain xtk for the input word at position k in turn t, resulting in a sequence Xt = {xt1, xt2, ...xtL}.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"The output sequence to be generated is represented by Yt = {yt1, yt2, ...ytL′}.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"The encoder at turn t receives the user’s projected input, as well as the context vectors from the final hidden units of the encoder and the decoder at turn t − 1, forming a skip connection.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
This ensures that a fixed size vector is used to represent the dialogue history at every turn.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
Orange-solid-square boxes in Figure 2 represent LSTM cells of the encoder.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"htL,enc is the context vector which is sent to every LSTM cell in the decoder (dec) at any turn t (Cho et al., 2014).
",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
Green-dashed-square cells in the decoder represent the LSTM and dense layers with a softmax non-linearity.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
These are trained to predict each word in the agent’s utterance.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
Each of the seq2seq copies share the same parameters.,3.1.1 Skip Connection Seq2Seq Model,[0],[0]
"Once the training is complete, we use only one copy of the seq2seq model to make predictions.",3.1.1 Skip Connection Seq2Seq Model,[0],[0]
The results obtained with the vanilla seq2seq model on the bAbI dataset is shown in the first row (Model 1) of Table 2.,3.1.2 Results with Skip-Connections,[0],[0]
"The EQM is 0%, even though the BLEU scores look reasonable.",3.1.2 Results with Skip-Connections,[0],[0]
"Model 2 is the skip-connection seq2seq model, where only the output of the hidden states from the decoder at turn t− 1 is appended to the input at time t, i.e., ht−1L,enc from the encoder history is not explicitly presented to turn t.
Model 3 extends Model 1 by adding an attentional layer.",3.1.2 Results with Skip-Connections,[0],[0]
Model 3 is a variant of Bordes and Weston (2016); Dodge et al. (2015) where the output of the attentional layer is sent to the decoder for generating the responses rather than classifying as one of the known responses.,3.1.2 Results with Skip-Connections,[0],[0]
This variant performed better on the customer support data compared to a direct implementation of Bordes and Weston (2016).,3.1.2 Results with Skip-Connections,[0],[0]
"The reason being, salutations occurred more frequently in the customer support data and hence, the classification based approach originally proposed by Bordes and Weston (2016) classified most of the outputs as salutations.",3.1.2 Results with Skip-Connections,[0],[0]
"Finally, Model 4 extends Model 2 by providing ht−1L,enc to turn t.
We see that explicitly adding skip-connections substantially improves performance in EQM, from 0 or 6% to 55%, and has a positive effect on BLEU.",3.1.2 Results with Skip-Connections,[0],[0]
The models show similar behavior on CS small.,3.1.2 Results with Skip-Connections,[0],[0]
"In this case, when an api call is executed, the result is treated as a response and sent as input to the next turn.",3.1.2 Results with Skip-Connections,[0],[0]
"Although Model 4 performed the best
on CS small and CS large, our analysis showed that the generated responses were most often incoherent and not fluent, a phenomenon that did not arise in the synthetic dataset.",3.1.2 Results with Skip-Connections,[0],[0]
"We now proceed to explain the nearest neighbor based approach, which we show is able to produce reasonable responses that are more fluent.",3.1.2 Results with Skip-Connections,[0],[0]
"In our nearest neighbor approach, an agent’s response is chosen from human generated transcripts or the training data - ensuring fluency.",3.2 Nearest Neighbor-based approach,[0],[0]
"However, this does not necessarily ensure that the responses are coherent in the context of the dialogue.",3.2 Nearest Neighbor-based approach,[0],[0]
"The nearest neighbor approach starts with a representation of the entire dialogue history bst,i for turn t and dialogue i. Together with at,i, the action the agent took while in this state i.e., the natural language response or api call query issued by the agent, this results in a tuple < bst,i, at,i >.",3.2 Nearest Neighbor-based approach,[0],[0]
"The entire training data is converted into a set of tuples S, that contains pairwise relationships between dialog state representations and agent actions.
",3.2 Nearest Neighbor-based approach,[0],[0]
"In the online or test phase, given an embedding of the dialogue so far, testV ec, we find the nearest neighbor bstestV ec in S. We return the nearest neighbor’s corresponding response, atestV ec, as the predicted agent’s response.",3.2 Nearest Neighbor-based approach,[0],[0]
"We use ball trees (Kibriya and Frank, 2007) to perform efficient nearest neighbor search.",3.2 Nearest Neighbor-based approach,[0],[0]
"Since we want to provide more flexibility to the human agent in choosing the most
appropriate response, we extended this approach to find k = 100 responses and then used a diversitybased ranking approach (Zhu et al., 2007) to return 5 diverse responses.",3.2 Nearest Neighbor-based approach,[0],[0]
"To construct the adjacency matrix for diversity ranking, we use word overlap between responses after stop word removal.
",3.2 Nearest Neighbor-based approach,[0],[0]
"Numerous techniques have been proposed for representating text including word2vec and sent2vec (Mikolov et al., 2013b,a; Pagliardini et al., 2017; Pennington et al., 2014).",3.2 Nearest Neighbor-based approach,[0],[0]
"In the following sections, we compare these approaches against our proposed representations using skip connections.",3.2 Nearest Neighbor-based approach,[0.9519736725783657],"['In practice, however, we find that the intersective interpretation suffices for many of the descriptions that occur in our dataset.']"
"In our first baseline, Model 6, for a dialogue, i, the user’s response at turn t, usert, is concatenated with his/her responses in previous turns (useri,1:t−1) and the agent’s responses upto turn t − 1 (agenti,1:t−1), to obtain, pi,t = (useri,1:t, agenti,1:t−1).",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"We obtain a belief state vector representation as the average of the word2vec (Mikolov et al., 2013b) representations of words in pi,t. We then apply the nearest neighbor approach described in Section 3.2.",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"Results obtained with this approach on CS small are in Table 3.
",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
We emphasize a subtle but important oracle advantage that we give this baseline algorithm.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"When we obtain the embeddings of a test dialogue, we use the true utterances of the expert agent so far,
which would not be available in practice.",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"However, we will show that our proposed representation, described in Section 3.3, performs better, even without access to this information.
",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
Pagliardini et al. (2017) recently described a method that leads to better sentence-level representations.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
We use their approach as another baseline.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
bst is represented by the average of the sentence embeddings of all agent’s responses upto turn t− 1 and user’s responses upto turn t. We also explore geometric discounting to give higher importance to recent responses.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
We use a similar process to obtain representations for the user’s responses during the test phase.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"As done with word-embeddings, we provide true agent responses upto turn t− 1 for predicting the agent’s response at turn t. Results obtained on CS small by averaging (Model 7) and discounted averaging (Model 8) are given in Table 3.",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
Model 8 performs better than Model 7 across all measures.,3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"A comparison between Model 6, 7 and 8 with Model 4 in Table 3, would not be a fair one as Model 4 does not use previous true agent responses to predict the agent’s next response.",3.2.1 Dialogue Embeddings from Word/Sentence Embeddings,[0],[0]
"We suggest using the outputs of the hidden units in the decoder of our skip connection seq2seq model, as suitable representations for the belief states.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
The seq2seq model for handling multi-turn dialogue is trained as before (Section 3.1.1).,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Once the parameters have been learned, we proceed to generate representations for all turns in the training data.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"The output of the last hidden unit of the encoder or the decoder before turn t is used to represent the belief state vector at turn t. As before, we obtain a set S consisting of pairs of belief state vectors and next actions taken by the agent.
",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"We test the models as done in Section 3.1.1, except now we select responses using the nearest neighbor approach (Figure 2).",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Results obtained are in Table 3 (Models 9 and 10).,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Model 9 uses the output of the last hidden unit of the encoder.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Model 10 uses previous turn’s decoder’s last hidden unit.
",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Both the models show a significant improvement in BLEU when compared to generating the agent’s response (Model 4).,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Although Model 10 was not exposed to the past true agent responses, it still achieved comparable performance to that of Model 8.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Appending both the encoder and the decoder outputs did not have significant impact.
",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
The results also show that the seq2seq model achieved a better EQM when compared to the nearest neighbor approach.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"The final hybrid model, we propose (Model 11) combines both strategies.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
We run both the Models 4 and 10 in parallel.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"When Model 4 predicts an API response, we use the output generated by Model 4 as the agent’s response, otherwise we use the output of Model 10 as the predicted agent’s response.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"This model achieved the best results among all models we study, both in terms of fluency (BLEU) as well as correctness of external actions (EQM).",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"The hybrid model achieves a 78% relative improvement (from 9.91 to 17.67) in fluency scores, and 200% improvement in EQM over previous approaches (from 0.10 to 0.30).
",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Table 4 shows results obtained on CS large (column 3) using models that performed the best on the other datasets.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
Another obvious baseline is to use traditional retrieval approaches.,3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"(query, agent response) pairs are created for each agent response, with a query constructed by concatenating all the agent’s responses upto turn t− 1 and user’s responses upto turn t, for an agent response at time t. For a given dialogue history query, the corresponding agent response is retrieved using Lucene3.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"Since CS large did not contain labeled api calls, we report results using Model 10.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"As seen, Model 10 provides a substantial boost in performance.",3.3 Hybrid model: Nearest Neighbor with Seq2Seq Embeddings,[0],[0]
"One caveat to the above evaluations is that they are based on customer responses to the actual human agent interactions, and are not fully indicative of how customers would react to the real automated system in practice.",3.4 Manual Online Evaluation,[0],[0]
"Another disadvantage of using
3https://lucene.apache.org/
automated evaluation with just one reference, is that the score (BLEU) penalizes valid responses that may be lexically different from the available agent response.",3.4 Manual Online Evaluation,[0],[0]
"To overcome this issue, we conducted online experiments with human agents.
",3.4 Manual Online Evaluation,[0],[0]
We used 5 human users and 2 agents.,3.4 Manual Online Evaluation,[0],[0]
On average each user interacted with an agent on 10 different issues that needed resolution.,3.4 Manual Online Evaluation,[0],[0]
"To compare against our baseline, each user interacted with the Model 4, 5 and 10 using the same issues.",3.4 Manual Online Evaluation,[0],[0]
This resulted in ≈ 50 dialogues from each of the models.,3.4 Manual Online Evaluation,[0],[0]
"After every response from the user, the human agent was allowed to select one of the top five responses the system selected.",3.4 Manual Online Evaluation,[0],[0]
"We refer to the selected response as A. The human agent was asked to make minimal modifications to the selected response, resulting in a response A′. If the responses suggested were completely irrelevant, the human agent was allowed to type in the most suitable response.
",3.4 Manual Online Evaluation,[0],[0]
"We then computed the BLEU between the system generated responses (As) and human generated responses (A′s), referred to as Online-BLEU in Table 4.",3.4 Manual Online Evaluation,[0],[0]
"Since the human agent only made minimal changes where appropriate, we believe the BLEU score would now be more correlated to human judgments.",3.4 Manual Online Evaluation,[0],[0]
"Since CS large did not contain any api calls, we only report BLEU scores.",3.4 Manual Online Evaluation,[0],[0]
"The results obtained with models 4, 5 and 10 on CS large are shown in Table 4 (column 4).",3.4 Manual Online Evaluation,[0],[0]
Model 10 performs better than Models 4 and 5.,3.4 Manual Online Evaluation,[0],[0]
"We do not measure inter-annotator agreement as each human user can take a different dialog trajectory.
",3.4 Manual Online Evaluation,[0],[0]
We noticed that the approach mimics certain interesting human behavior.,3.4 Manual Online Evaluation,[0],[0]
"For example, in Table 5, the chatbot detects that the user is frustrated and responds with smileys and even makes exceptions on the return policy.",3.4 Manual Online Evaluation,[0],[0]
We demonstrated limitations of previous end-end dialog approaches and proposed variants to make them suitable for real world settings.,4 Conclusion and Future Work,[0],[0]
"In ongoing work, we explore reinforcement learning tech-
niques to reach the goal state quicker thereby reducing the number of interactions.",4 Conclusion and Future Work,[0],[0]
"In task-oriented dialog, agents need to generate both fluent natural language responses and correct external actions like database queries and updates.",abstractText,[0],[0]
"We show that methods that achieve state of the art performance on synthetic datasets, perform poorly in real world dialog tasks.",abstractText,[0],[0]
"We propose a hybrid model, where nearest neighbor is used to generate fluent responses and Sequence-to-Sequence (Seq2Seq) type models ensure dialogue coherency and generate accurate external actions.",abstractText,[0],[0]
"The hybrid model on an internal customer support dataset achieves a 78% relative improvement in fluency, and a 200% improvement in external call accuracy.",abstractText,[0],[0]
What we need to learn if we want to do and not just talk,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2126–2136 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2126",text,[0],[0]
"Despite Ray Mooney’s quip that you cannot cram the meaning of a whole %&!$# sentence into a single $&!#* vector, sentence embedding methods have achieved impressive results in tasks ranging from machine translation (Sutskever et al., 2014; Cho et al., 2014) to entailment detection (Williams et al., 2018), spurring the quest for “universal embeddings” trained once and used in a variety of applications (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018).",1 Introduction,[0],[0]
Positive results on concrete problems suggest that embeddings capture important linguistic properties of sentences.,1 Introduction,[0],[0]
"However, real-life “downstream” tasks require complex forms of inference, making it difficult to pinpoint the information a model is relying upon.",1 Introduction,[0],[0]
"Impressive as it might be that a system can tell that the sentence “A movie that doesn’t aim too high, but it doesn’t need to” (Pang and Lee, 2004) expresses a subjective viewpoint, it is
hard to tell how the system (or even a human) comes to this conclusion.",1 Introduction,[0],[0]
"Complex tasks can also carry hidden biases that models might lock onto (Jabri et al., 2016).",1 Introduction,[0],[0]
"For example, Lai and Hockenmaier (2014) show that the simple heuristic of checking for explicit negation words leads to good accuracy in the SICK sentence entailment task.
",1 Introduction,[0],[0]
Model introspection techniques have been applied to sentence encoders in order to gain a better understanding of which properties of the input sentences their embeddings retain (see Section 5).,1 Introduction,[0],[0]
"However, these techniques often depend on the specifics of an encoder architecture, and consequently cannot be used to compare different methods.",1 Introduction,[0],[0]
"Shi et al. (2016) and Adi et al. (2017) introduced a more general approach, relying on the notion of what we will call probing tasks.",1 Introduction,[0],[0]
A probing task is a classification problem that focuses on simple linguistic properties of sentences.,1 Introduction,[0],[0]
"For example, one such task might require to categorize sentences by the tense of their main verb.",1 Introduction,[0],[0]
"Given an encoder (e.g., an LSTM) pre-trained on a certain task (e.g., machine translation), we use the sentence embeddings it produces to train the tense classifier (without further embedding tuning).",1 Introduction,[0],[0]
"If the classifier succeeds, it means that the pre-trained encoder is storing readable tense information into the embeddings it creates.",1 Introduction,[0],[0]
Note that: (i),1 Introduction,[0],[0]
"The probing task asks a simple question, minimizing interpretability problems.",1 Introduction,[0],[0]
"(ii) Because of their simplicity, it is easier to control for biases in probing tasks than in downstream tasks.",1 Introduction,[0],[0]
"(iii) The probing task methodology is agnostic with respect to the encoder architecture, as long as it produces a vector representation of sentences.
",1 Introduction,[0],[0]
We greatly extend earlier work on probing tasks as follows.,1 Introduction,[0],[0]
"First, we introduce a larger set of probing tasks (10 in total), organized by the type of linguistic properties they probe.",1 Introduction,[0],[0]
"Second, we systematize the probing task methodology, controlling for
a number of possible nuisance factors, and framing all tasks so that they only require single sentence representations as input, for maximum generality and to ease result interpretation.",1 Introduction,[0],[0]
"Third, we use our probing tasks to explore a wide range of state-of-the-art encoding architectures and training methods, and further relate probing and downstream task performance.",1 Introduction,[0],[0]
"Finally, we are publicly releasing our probing data sets and tools, hoping they will become a standard way to study the linguistic properties of sentence embeddings.1",1 Introduction,[0],[0]
"In constructing our probing benchmarks, we adopted the following criteria.",2 Probing tasks,[0],[0]
"First, for generality and interpretability, the task classification problem should only require single sentence embeddings as input (as opposed to, e.g., sentence and word embeddings, or multiple sentence representations).",2 Probing tasks,[0],[0]
"Second, it should be possible to construct large training sets in order to train parameter-rich multi-layer classifiers, in case the relevant properties are non-linearly encoded in the sentence vectors.",2 Probing tasks,[0],[0]
"Third, nuisance variables such as lexical cues or sentence length should be controlled for.",2 Probing tasks,[0],[0]
"Finally, and most importantly, we want tasks that address an interesting set of linguistic properties.",2 Probing tasks,[0],[0]
"We thus strove to come up with a set of tasks that, while respecting the previous constraints, probe a wide range of phenomena, from superficial properties of sentences such as which words they contain to their hierarchical structure to subtle facets of semantic acceptability.",2 Probing tasks,[0],[0]
"We think the current task set is reasonably representative of different linguistic domains, but we are not claiming that it is exhaustive.",2 Probing tasks,[0],[0]
"We expect future work to extend it.
",2 Probing tasks,[0],[0]
"The sentences for all our tasks are extracted from the Toronto Book Corpus (Zhu et al., 2015), more specifically from the random pre-processed portion made available by Paperno et al. (2016).",2 Probing tasks,[0],[0]
We only sample sentences in the 5-to-28 word range.,2 Probing tasks,[0],[0]
"We parse them with the Stanford Parser (2017-06-09 version), using the pre-trained PCFG model (Klein and Manning, 2003), and we rely on the part-of-speech, constituency and dependency parsing information provided by this tool where needed.",2 Probing tasks,[0],[0]
"For each task, we construct training sets containing 100k sentences, and 10k-sentence val-
1https://github.com/facebookresearch/ SentEval/tree/master/data/probing
idation and test sets.",2 Probing tasks,[0],[0]
"All sets are balanced, having an equal number of instances of each target class.
",2 Probing tasks,[0],[0]
Surface information These tasks test the extent to which sentence embeddings are preserving surface properties of the sentences they encode.,2 Probing tasks,[0],[0]
One can solve the surface tasks by simply looking at tokens in the input sentences: no linguistic knowledge is called for.,2 Probing tasks,[0],[0]
The first task is to predict the length of sentences in terms of number of words (SentLen).,2 Probing tasks,[0],[0]
"Following Adi et al. (2017), we group sentences into 6 equal-width bins by length, and treat SentLen as a 6-way classification task.",2 Probing tasks,[0],[0]
The word content (WC) task tests whether it is possible to recover information about the original words in the sentence from its embedding.,2 Probing tasks,[0],[0]
"We picked 1000 mid-frequency words from the source corpus vocabulary (the words with ranks between 2k and 3k when sorted by frequency), and sampled equal numbers of sentences that contain one and only one of these words.",2 Probing tasks,[0],[0]
The task is to tell which of the 1k words a sentence contains (1k-way classification).,2 Probing tasks,[0],[0]
"This setup allows us to probe a sentence embedding for word content without requiring an auxiliary word embedding (as in the setup of Adi and colleagues).
",2 Probing tasks,[0],[0]
Syntactic information The next batch of tasks test whether sentence embeddings are sensitive to syntactic properties of the sentences they encode.,2 Probing tasks,[0],[0]
The bigram shift (BShift) task tests whether an encoder is sensitive to legal word orders.,2 Probing tasks,[0],[0]
"In this binary classification problem, models must distinguish intact sentences sampled from the corpus from sentences where we inverted two random adjacent words (“What you are doing out there?”).
",2 Probing tasks,[0],[0]
"The tree depth (TreeDepth) task checks whether an encoder infers the hierarchical structure of sentences, and in particular whether it can group sentences by the depth of the longest path from root to any leaf.",2 Probing tasks,[0],[0]
"Since tree depth is naturally correlated with sentence length, we de-correlate these variables through a structured sampling procedure.",2 Probing tasks,[0],[0]
"In the resulting data set, tree depth values range from 5 to 12, and the task is to categorize sentences into the class corresponding to their depth (8 classes).",2 Probing tasks,[0],[0]
"As an example, the following is a long (22 tokens) but shallow (max depth: 5) sentence: “[1 [2 But right now, for the time being, my past, my fears, and my thoughts [3 were [4 my [5business]]].]",2 Probing tasks,[0],[0]
"]” (the outermost brackets correspond to the ROOT and S nodes in the parse).
",2 Probing tasks,[0],[0]
"In the top constituent task (TopConst), sentences must be classified in terms of the sequence of top constituents immediately below the sentence (S) node.",2 Probing tasks,[0],[0]
"An encoder that successfully addresses this challenge is not only capturing latent syntactic structures, but clustering them by constituent types.",2 Probing tasks,[0],[0]
TopConst was introduced by Shi et al. (2016).,2 Probing tasks,[0],[0]
"Following them, we frame it as a 20-way classification problem: 19 classes for the most frequent top constructions, and one for all other constructions.",2 Probing tasks,[0],[0]
"As an example, “[Then] [very dark gray letters on a black screen]",2 Probing tasks,[0],[0]
[appeared],2 Probing tasks,[0],[0]
"[.]” has top constituent sequence: “ADVP NP VP .”.
",2 Probing tasks,[0],[0]
"Note that, while we would not expect an untrained human subject to be explicitly aware of tree depth or top constituency, similar information must be implicitly computed to correctly parse sentences, and there is suggestive evidence that the brain tracks something akin to tree depth during sentence processing (Nelson et al., 2017).
",2 Probing tasks,[0],[0]
"Semantic information These tasks also rely on syntactic structure, but they further require some understanding of what a sentence denotes.",2 Probing tasks,[0],[0]
"The Tense task asks for the tense of the main-clause verb (VBP/VBZ forms are labeled as present, VBD as past).",2 Probing tasks,[0],[0]
"No target form occurs across the train/dev/test split, so that classifiers cannot rely on specific words (it is not clear that Shi and colleagues, who introduced this task, controlled for this factor).",2 Probing tasks,[0],[0]
The subject number (SubjNum) task focuses on the number of the subject of the main clause (number in English is more often explicitly marked on nouns than verbs).,2 Probing tasks,[0],[0]
"Again, there is no target overlap across partitions.",2 Probing tasks,[0],[0]
"Similarly, object number (ObjNum) tests for the number of the direct object of the main clause (again, avoiding lexical overlap).",2 Probing tasks,[0],[0]
"To solve the previous tasks correctly, an encoder must not only capture tense and number, but also extract structural information (about the main clause and its arguments).",2 Probing tasks,[0],[0]
"We grouped Tense, SubjNum and ObjNum with the semantic tasks, since, at least for models that treat words as unanalyzed input units (without access to morphology), they must rely on what a sentence denotes (e.g., whether the described event took place in the past), rather than on structural/syntactic information.",2 Probing tasks,[0],[0]
"We recognize, however, that the boundary between syntactic and semantic tasks is somewhat arbitrary.
",2 Probing tasks,[0],[0]
"In the semantic odd man out (SOMO) task, we modified sentences by replacing a random noun
or verb o with another noun or verb r. To make the task more challenging, the bigrams formed by the replacement with the previous and following words in the sentence have frequencies that are comparable (on a log-scale) with those of the original bigrams.",2 Probing tasks,[0],[0]
"That is, if the original sentence contains bigrams wn−1o and own+1, the corresponding bigrams",2 Probing tasks,[0],[0]
wn−1r and rwn+1 in the modified sentence will have comparable corpus frequencies.,2 Probing tasks,[0],[0]
"No sentence is included in both original and modified format, and no replacement is repeated across train/dev/test sets.",2 Probing tasks,[0],[0]
The task of the classifier is to tell whether a sentence has been modified or not.,2 Probing tasks,[0],[0]
An example modified sentence is: “ No one could see this Hayes,2 Probing tasks,[0],[0]
and I wanted to know if it was real or a spoonful (orig.: ploy).”,2 Probing tasks,[0],[0]
"Note that judging plausibility of a syntactically well-formed sentence of this sort will often require grasping rather subtle semantic factors, ranging from selectional preference to topical coherence.
",2 Probing tasks,[0],[0]
The coordination inversion (CoordInv) benchmark contains sentences made of two coordinate clauses.,2 Probing tasks,[0],[0]
"In half of the sentences, we inverted the order of the clauses.",2 Probing tasks,[0],[0]
The task is to tell whether a sentence is intact or modified.,2 Probing tasks,[0],[0]
"Sentences are balanced in terms of clause length, and no sentence appears in both original and inverted versions.",2 Probing tasks,[0],[0]
"As an example, original “They might be only memories, but I can still feel each one” becomes: “I can still feel each one, but they might be only memories.”",2 Probing tasks,[0],[0]
"Often, addressing CoordInv requires an understanding of broad discourse and pragmatic factors.
",2 Probing tasks,[0],[0]
Row Hum.,2 Probing tasks,[0],[0]
Eval.,2 Probing tasks,[0],[0]
"of Table 2 reports humanvalidated “reasonable” upper bounds for all the tasks, estimated in different ways, depending on the tasks.",2 Probing tasks,[0],[0]
"For the surface ones, there is always a straightforward correct answer that a human annotator with enough time and patience could find.",2 Probing tasks,[0],[0]
The upper bound is thus estimated at 100%.,2 Probing tasks,[0],[0]
"The TreeDepth, TopConst, Tense, SubjNum and ObjNum tasks depend on automated PoS and parsing annotation.",2 Probing tasks,[0],[0]
"In these cases, the upper bound is given by the proportion of sentences correctly annotated by the automated procedure.",2 Probing tasks,[0],[0]
"To estimate this quantity, one linguistically-trained author checked the annotation of 200 randomly sampled test sentences from each task.",2 Probing tasks,[0],[0]
"Finally, the BShift, SOMO and CoordInv manipulations can accidentally generate acceptable sentences.",2 Probing tasks,[0],[0]
"For
example, one modified SOMO sentence is: “He pulled out the large round onion (orig.: cork) and saw the amber balm inside.”",2 Probing tasks,[0],[0]
", that is arguably not more anomalous than the original.",2 Probing tasks,[0],[0]
"For these tasks, we ran Amazon Mechanical Turk experiments in which subjects were asked to judge whether 1k randomly sampled test sentences were acceptable or not.",2 Probing tasks,[0],[0]
Reported human accuracies are based on majority voting.,2 Probing tasks,[0],[0]
See Appendix for details.,2 Probing tasks,[0],[0]
"In this section, we present the three sentence encoders that we consider and the seven tasks on which we train them.",3 Sentence embedding models,[0],[0]
A wide variety of neural networks encoding sentences into fixed-size representations exist.,3.1 Sentence encoder architectures,[0],[0]
"We focus here on three that have been shown to perform well on standard NLP tasks.
",3.1 Sentence encoder architectures,[0],[0]
BiLSTM-last/max,3.1 Sentence encoder architectures,[0],[0]
"For a sequence of T words {wt}t=1,...,T , a bidirectional LSTM computes a set of T vectors {ht}t.",3.1 Sentence encoder architectures,[0],[0]
For t ∈,3.1 Sentence encoder architectures,[0],[0]
"[1, . . .",3.1 Sentence encoder architectures,[0],[0]
", T ], ht is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions.",3.1 Sentence encoder architectures,[0.9518010595659524],"['This is accomplished by running a bidirectional LSTM over the embeddings wt of the words in the sentence, and using its output to compute offset probabilities: [z0, z1, .']"
"We experiment with two ways of combining the varying number of (h1, . . .",3.1 Sentence encoder architectures,[0],[0]
", hT ) to form a fixed-size vector, either by selecting the last hidden state of hT or by selecting the maximum value over each dimension of the hidden units.",3.1 Sentence encoder architectures,[0],[0]
"The choice of these models are motivated by their demonstrated efficiency in seq2seq (Sutskever et al., 2014) and universal sentence representation learning (Conneau et al., 2017), respectively.2
Gated ConvNet We also consider the nonrecurrent convolutional equivalent of LSTMs, based on stacked gated temporal convolutions.",3.1 Sentence encoder architectures,[0],[0]
"Gated convolutional networks were shown to perform well as neural machine translation encoders (Gehring et al., 2017) and language modeling decoders (Dauphin et al., 2017).",3.1 Sentence encoder architectures,[0],[0]
"The encoder is composed of an input word embedding table that is augmented with positional encodings (Sukhbaatar et al., 2015), followed by a stack of temporal convolutions with small kernel size.",3.1 Sentence encoder architectures,[0],[0]
"The output of each convolutional layer is filtered by a gating mechanism, similar to the one of LSTMs.",3.1 Sentence encoder architectures,[0.9563122863801277],['The function f consists of the first two layers of a convolutional neural network that is pretrained on the task of predicting a voxel’s identity given the 5x5x5 neighborhood around it.']
"Finally,
2We also experimented with a unidirectional LSTM, with consistently poorer results.
",3.1 Sentence encoder architectures,[0],[0]
"max-pooling along the temporal dimension is performed on the output feature maps of the last convolution (Collobert and Weston, 2008).",3.1 Sentence encoder architectures,[0],[0]
"Seq2seq systems have shown strong results in machine translation (Zhou et al., 2016).",3.2 Training tasks,[0],[0]
"They consist of an encoder that encodes a source sentence into a fixed-size representation, and a decoder which acts as a conditional language model and that generates the target sentence.",3.2 Training tasks,[0],[0]
"We train Neural Machine Translation systems on three language pairs using about 2M sentences from the Europarl corpora (Koehn, 2005).",3.2 Training tasks,[0],[0]
"We pick English-French, which involves two similar languages, English-German, involving larger syntactic differences, and English-Finnish, a distant pair.",3.2 Training tasks,[0],[0]
"We also train with an AutoEncoder objective (Socher et al., 2011) on Europarl source English sentences.",3.2 Training tasks,[0],[0]
"Following Vinyals et al. (2015), we train a seq2seq architecture to generate linearized grammatical parse trees (see Table 1) from source sentences (Seq2Tree).",3.2 Training tasks,[0],[0]
We use the Stanford parser to generate trees for Europarl source English sentences.,3.2 Training tasks,[0],[0]
"We train SkipThought vectors (Kiros et al., 2015) by predicting the next sentence given the current one (Tang et al., 2017), on 30M sentences from the Toronto Book Corpus, excluding those in the probing sets.",3.2 Training tasks,[0],[0]
"Finally, following Conneau et al. (2017), we train sentence encoders on Natural Language Inference using the concatenation of the SNLI (Bowman et al., 2015) and MultiNLI (Bowman et al., 2015) data sets (about 1M sentence pairs).",3.2 Training tasks,[0],[0]
"In this task, a sentence encoder is trained to encode two sentences, which are fed to a classifier and whose role is to distinguish whether the sentences are contradictory, neutral or entailed.",3.2 Training tasks,[0],[0]
"Finally, as in Conneau et al. (2017), we also include Untrained encoders with random weights, which act as random projections of pre-trained word embeddings.",3.2 Training tasks,[0],[0]
"BiLSTM encoders use 2 layers of 512 hidden units (∼4M parameters), Gated ConvNet has 8 convolutional layers of 512 hidden units, kernel size 3 (∼12M parameters).",3.3 Training details,[0],[0]
"We use pre-trained fastText word embeddings of size 300 (Mikolov et al., 2018) without fine-tuning, to isolate the impact of encoder architectures and to handle words outside the training sets.",3.3 Training details,[0],[0]
Training task performance and further details are in Appendix.,3.3 Training details,[0],[0]
Baselines Baseline and human-bound performance are reported in the top block of Table 2.,4 Probing task experiments,[0],[0]
Length is a linear classifier with sentence length as sole feature.,4 Probing task experiments,[0],[0]
"NB-uni-tfidf is a Naive Bayes classifier using words’ tfidf scores as features, NBbi-tfidf its extension to bigrams.",4 Probing task experiments,[0],[0]
"Finally, BoVfastText derives sentence representations by averaging the fastText embeddings of the words they contain (same embeddings used as input to the encoders).3
Except, trivially, for Length on SentLen and the NB baselines on WC, there is a healthy gap between top baseline performance and human upper bounds.",4 Probing task experiments,[0.9515240309229643],"['Offset scores ot are assigned based on the context the word xt occurs in, which allows the model to incorporate information from words such as right or under in its decisions.']"
NB-uni-tfidf evaluates to what extent our tasks can be addressed solely based on knowledge about the distribution of words in the training sentences.,4 Probing task experiments,[0],[0]
"Words are of course to some extent informative for most tasks, leading to relatively high performance in Tense, SubjNum and ObjNum.",4 Probing task experiments,[0],[0]
"Recall that the words containing the probed features are disjoint between train and test partitions, so we are not observing a confound here, but rather the effect of the redundancies one expects in natural language data.",4 Probing task experiments,[0],[0]
"For example, for Tense, since sentences often contain more than one verb in the same tense, NB-uni-tfidf can exploit nontarget verbs as cues: the NB features most associated to the past class are verbs in the past tense (e.g “sensed”, “lied”, “announced”), and similarly for present (e.g “uses”, “chuckles”, “frowns”).",4 Probing task experiments,[0],[0]
"Using bigram features (NB-bi-tfidf) brings in general little or no improvement with respect to the unigram baseline, except, trivially, for the BShift
3Similar results are obtained summing embeddings, and using GloVe embeddings (Pennington et al., 2014).
task, where NB-bi-tfidf can easily detect unlikely bigrams.",4 Probing task experiments,[0],[0]
"NB-bi-tfidf has below-random performance on SOMO, confirming that the semantic intruder is not given away by superficial bigram cues.
",4 Probing task experiments,[0],[0]
"Our first striking result is the good overall performance of Bag-of-Vectors, confirming early insights that aggregated word embeddings capture surprising amounts of sentence information (Pham et al., 2015; Arora et al., 2017; Adi et al., 2017).",4 Probing task experiments,[0],[0]
BoV’s good WC and SentLen performance was already established by Adi et al. (2017).,4 Probing task experiments,[0],[0]
"Not surprisingly, word-order-unaware BoV performs randomly in BShift and in the more sophisticated semantic tasks SOMO and CoordInv.",4 Probing task experiments,[0],[0]
"More interestingly, BoV is very good at the Tense, SubjNum, ObjNum, and TopConst tasks (much better than the word-based baselines), and well above chance in TreeDepth.",4 Probing task experiments,[0],[0]
"The good performance on Tense, SubjNum and ObjNum has a straightforward explanation we have already hinted at above.",4 Probing task experiments,[0],[0]
"Many sentences are naturally “redundant”, in the sense that most tensed verbs in a sentence are in the same tense, and similarly for number in nouns.",4 Probing task experiments,[0],[0]
"In 95.2% Tense, 75.9% SubjNum and 78.7% ObjNum test sentences, the target tense/number feature is also the majority one for the whole sentence.",4 Probing task experiments,[0],[0]
"Word embeddings capture features such as number and tense (Mikolov et al., 2013), so aggregated word embeddings will naturally track these features’ majority values in a sentence.",4 Probing task experiments,[0],[0]
BoV’s TopConst and TreeDepth performance is more surprising.,4 Probing task experiments,[0],[0]
"Accuracy is well above NB, showing that BoV is exploiting cues beyond specific words strongly associated to the target classes.",4 Probing task experiments,[0],[0]
"We conjecture that more abstract word features captured
Task SentLen WC TreeDepth TopConst",4 Probing task experiments,[0],[0]
BShift,4 Probing task experiments,[0],[0]
Tense SubjNum ObjNum SOMO CoordInv Baseline representations Majority vote 20.0 0.5 17.9 5.0 50.0 50.0 50.0 50.0 50.0 50.0 Hum.,4 Probing task experiments,[0],[0]
Eval.,4 Probing task experiments,[0],[0]
"100 100 84.0 84.0 98.0 85.0 88.0 86.5 81.2 85.0 Length 100 0.2 18.1 9.3 50.6 56.5 50.3 50.1 50.2 50.0 NB-uni-tfidf 22.7 97.8 24.1 41.9 49.5 77.7 68.9 64.0 38.0 50.5 NB-bi-tfidf 23.0 95.0 24.6 53.0 63.8 75.9 69.1 65.4 39.9 55.7 BoV-fastText 66.6 91.6 37.1 68.1 50.8 89.1 82.1 79.8 54.2 54.8
BiLSTM-last encoder Untrained 36.7 43.8 28.5 76.3 49.8 84.9 84.7 74.7 51.1 64.3 AutoEncoder 99.3 23.3 35.6 78.2 62.0 84.3 84.7 82.1 49.9 65.1 NMT En-Fr 83.5 55.6 42.4 81.6 62.3 88.1 89.7 89.5 52.0 71.2 NMT En-De 83.8 53.1 42.1 81.8 60.6 88.6 89.3 87.3 51.5 71.3 NMT En-Fi 82.4 52.6 40.8 81.3 58.8 88.4 86.8 85.3 52.1 71.0 Seq2Tree 94.0 14.0 59.6 89.4 78.6 89.9 94.4 94.7 49.6 67.8 SkipThought 68.1 35.9 33.5 75.4 60.1 89.1 80.5 77.1 55.6 67.7 NLI 75.9 47.3 32.7 70.5 54.5 79.7 79.3 71.3 53.3 66.5
BiLSTM-max encoder Untrained 73.3 88.8 46.2 71.8 70.6 89.2 85.8 81.9 73.3 68.3 AutoEncoder 99.1 17.5 45.5 74.9 71.9 86.4 87.0 83.5 73.4 71.7 NMT En-Fr 80.1 58.3 51.7 81.9 73.7 89.5 90.3 89.1 73.2 75.4 NMT En-De 79.9 56.0 52.3 82.2 72.1 90.5 90.9 89.5 73.4 76.2 NMT En-Fi 78.5 58.3 50.9 82.5 71.7 90.0 90.3 88.0 73.2 75.4 Seq2Tree 93.3 10.3 63.8 89.6 82.1 90.9 95.1 95.1 73.2 71.9 SkipThought 66.0 35.7 44.6 72.5 73.8 90.3 85.0 80.6 73.6 71.0 NLI 71.7 87.3 41.6 70.5 65.1 86.7 80.7 80.3 62.1 66.8
GatedConvNet encoder Untrained 90.3 17.1 30.3 47.5 62.0 78.2 72.2 70.9 61.4 59.6 AutoEncoder 99.4 16.8 46.3 75.2 71.9 87.7 88.5 86.5 73.5 72.4 NMT En-Fr 84.8 41.3 44.6 77.6 67.9 87.9 88.8 86.6 66.1 72.0 NMT En-De 89.6 49.0 50.5 81.7 72.3 90.4 91.4 89.7 72.8 75.1 NMT En-Fi 89.3 51.5 49.6 81.8 70.9 90.4 90.9 89.4 72.4 75.1 Seq2Tree 96.5 8.7 62.0 88.9 83.6 91.5 94.5 94.3 73.5 73.8 SkipThought 79.1 48.4 45.7 79.2 73.4 90.7 86.6 81.7 72.4 72.3 NLI 73.8 29.2 43.2 63.9 70.7 81.3 77.5 74.4 73.3 71.0
Table 2: Probing task accuracies.",4 Probing task experiments,[0],[0]
"Classification performed by a MLP with sigmoid nonlinearity, taking pre-learned sentence embeddings as input (see Appendix for details and logistic regression results).
",4 Probing task experiments,[0],[0]
by the embeddings (such as the part of speech of a word) might signal different syntactic structures.,4 Probing task experiments,[0],[0]
"For example, sentences in the “WHNP SQ .”",4 Probing task experiments,[0],[0]
"top constituent class (e.g., “How long before you leave us again?”) must contain a wh word, and will often feature an auxiliary or modal verb.",4 Probing task experiments,[0],[0]
"BoV can rely on this information to noisily predict the correct class.
",4 Probing task experiments,[0],[0]
"Encoding architectures Comfortingly, proper encoding architectures clearly outperform BoV.",4 Probing task experiments,[0],[0]
"An interesting observation in Table 2 is that different encoder architectures trained with the same objective, and achieving similar performance on the training task,4 can lead to linguistically different embeddings, as indicated by the probing tasks.",4 Probing task experiments,[0],[0]
"Coherently with the findings of Conneau et al. (2017) for the downstream tasks, this sug-
4See Appendix for details on training task performance.
",4 Probing task experiments,[0],[0]
gests that the prior imposed by the encoder architecture strongly preconditions the nature of the embeddings.,4 Probing task experiments,[0],[0]
"Complementing recent evidence that convolutional architectures are on a par with recurrent ones in seq2seq tasks (Gehring et al., 2017), we find that Gated ConvNet’s overall probing task performance is comparable to that of the best LSTM architecture (although, as shown in Appendix, the LSTM has a slight edge on downstream tasks).",4 Probing task experiments,[0],[0]
We also replicate the finding of Conneau et al. (2017) that BiLSTM-max outperforms BiLSTM-last both in the downstream tasks (see Appendix) and in the probing tasks (Table 2).,4 Probing task experiments,[0],[0]
"Interestingly, the latter only outperforms the former in SentLen, a task that captures a superficial aspect of sentences (how many words they contain), that could get in the way of inducing more useful linguistic knowledge.
",4 Probing task experiments,[0],[0]
"Training tasks We focus next on how different training tasks affect BiLSTM-max, but the patterns are generally representative across architectures.",4 Probing task experiments,[0],[0]
"NMT training leads to encoders that are more linguistically aware than those trained on the NLI data set, despite the fact that we confirm the finding of Conneau and colleagues that NLI is best for downstream tasks (Appendix).",4 Probing task experiments,[0],[0]
"Perhaps, NMT captures richer linguistic features useful for the probing tasks, whereas shallower or more adhoc features might help more in our current downstream tasks.",4 Probing task experiments,[0],[0]
"Suggestively, the one task where NLI clearly outperforms NMT is WC.",4 Probing task experiments,[0],[0]
"Thus, NLI training is better at preserving shallower word features that might be more useful in downstream tasks (cf.",4 Probing task experiments,[0],[0]
"Figure 2 and discussion there).
",4 Probing task experiments,[0],[0]
"Unsupervised training (SkipThought and AutoEncoder) is not on a par with supervised tasks, but still effective.",4 Probing task experiments,[0],[0]
"AutoEncoder training leads, unsurprisingly, to a model excelling at SentLen, but it attains low performance in the WC prediction task.",4 Probing task experiments,[0],[0]
"This curious result might indicate that the latter information is stored in the embeddings in a complex way, not easily readable by our MLP.",4 Probing task experiments,[0],[0]
"At the other end, Seq2Tree is trained to predict annotation from the same parser we used to create some of the probing tasks.",4 Probing task experiments,[0],[0]
"Thus, its high performance on TopConst, Tense, SubjNum, ObjNum and TreeDepth is probably an artifact.",4 Probing task experiments,[0],[0]
"Indeed, for most of these tasks, Seq2Tree performance is above the human bound, that is, Seq2Tree learned to mimic the parser errors in our benchmarks.",4 Probing task experiments,[0],[0]
"For the more challenging SOMO and CoordInv tasks, that only indirectly rely on tagging/parsing information, Seq2Tree is comparable to NMT, that does not use explicit syntactic information.
",4 Probing task experiments,[0],[0]
"Perhaps most interestingly, BiLSTM-max already achieves very good performance without any training (Untrained row in Table 2).",4 Probing task experiments,[0],[0]
Untrained BiLSTM-max also performs quite well in the downstream tasks (Appendix).,4 Probing task experiments,[0],[0]
This architecture must encode priors that are intrinsically good for sentence representations.,4 Probing task experiments,[0],[0]
"Untrained BiLSTM-max exploits the input fastText embeddings, and multiplying the latter by a random recurrent matrix provides a form of positional encoding.",4 Probing task experiments,[0],[0]
"However, good performance in a task such as SOMO, where BoV fails and positional information alone should not help (the intruder is randomly distributed across the sentence), suggests that other architectural biases are at work.",4 Probing task experiments,[0],[0]
"In-
triguingly, a preliminary comparison of untrained BiLSTM-max and human subjects on the SOMO sentences evaluated by both reveals that, whereas humans have a bias towards finding sentences acceptable (62% sentences are rated as untampered with, vs. 48% ground-truth proportion), the model has a strong bias in the opposite direction (it rates 83% of the sentences as modified).",4 Probing task experiments,[0],[0]
"A cursory look at contrasting errors confirms, unsurprisingly, that those made by humans are perfectly justified, while model errors are opaque.",4 Probing task experiments,[0],[0]
"For example, the sentence “I didn’t come here to reunite",4 Probing task experiments,[0],[0]
"(orig. undermine) you” seems perfectly acceptable in its modified form, and indeed subjects judged it as such, whereas untrained BiLSTM-max “correctly” rated it as a modified item.",4 Probing task experiments,[0],[0]
"Conversely, it is difficult to see any clear reason for the latter tendency to rate perfectly acceptable originals as modified.",4 Probing task experiments,[0],[0]
We leave a more thorough investigation to further work.,4 Probing task experiments,[0],[0]
"See similar observations on the effectiveness of untrained ConvNets in vision by Ulyanov et al. (2017).
",4 Probing task experiments,[0],[0]
"Probing task comparison A good encoder, such as NMT-trained BiLSTM-max, shows generally good performance across probing tasks.",4 Probing task experiments,[0],[0]
"At one extreme, performance is not particularly high on the surface tasks, which might be an indirect sign of the encoder extracting “deeper” linguistic properties.",4 Probing task experiments,[0],[0]
"At the other end, performance is still far from the human bounds on TreeDepth, BShift, SOMO and CoordInv.",4 Probing task experiments,[0],[0]
The last 3 tasks ask if a sentence is syntactically or semantically anomalous.,4 Probing task experiments,[0],[0]
"This is a daunting job for an encoder that has not been explicitly trained on acceptability, and it is interesting that the best models are, at least to a certain extent, able to produce reasonable anomaly judgments.",4 Probing task experiments,[0],[0]
The asymmetry between the difficult TreeDepth and easier TopConst is also interesting.,4 Probing task experiments,[0],[0]
"Intuitively, TreeDepth requires more nuanced syntactic information (down to the deepest leaf of the tree) than TopConst, that only requires identifying broad chunks.
",4 Probing task experiments,[0],[0]
Figure 1 reports how probing task accuracy changes in function of encoder training epochs.,4 Probing task experiments,[0],[0]
"The figure shows that NMT probing performance is largely independent of target language, with strikingly similar development patterns across French, German and Finnish.",4 Probing task experiments,[0],[0]
"Note in particular the similar probing accuracy curves in French and Finnish, while the corresponding BLEU scores (in lavender) are consistently higher in the former lan-
guage.",4 Probing task experiments,[0.9534854565304823],"['Localizations are then summed across all words in the sentence, resulting in a single score for each voxel in the scene (Figure 3e).']"
"For both NMT and SkipThought, WC performance keeps increasing with epochs.",4 Probing task experiments,[0],[0]
"For the other tasks, we observe instead an early flattening of the NMT probing curves, while BLEU performance keeps increasing.",4 Probing task experiments,[0],[0]
"Most strikingly, SentLen performance is actually decreasing, suggesting again that, as a model captures deeper linguistic properties, it will tend to forget about this superficial feature.",4 Probing task experiments,[0],[0]
"Finally, for the challenging SOMO task, the curves are mostly flat, suggesting that what BiLSTM-max is able to capture about this task is already encoded in its architecture, and further training doesn’t help much.
",4 Probing task experiments,[0],[0]
"Probing vs. downstream tasks Figure 2 reports correlation between performance on our probing tasks and the downstream tasks available in the SentEval5 suite, which consists of classification (MR, CR, SUBJ, MPQA, SST2, SST5, TREC), natural language inference (SICK-E), semantic relatedness (SICK-R, STSB), paraphrase detection (MRPC) and semantic textual similarity (STS 2012 to 2017) tasks.",4 Probing task experiments,[0],[0]
"Strikingly, WC is significantly positively correlated with all downstream tasks.",4 Probing task experiments,[0],[0]
"This suggests that, at least for current models, the latter do not require extracting particularly abstract knowledge from the data.",4 Probing task experiments,[0],[0]
"Just relying on the words contained in the input sentences
5https://github.com/facebookresearch/ SentEval
can get you a long way.",4 Probing task experiments,[0],[0]
"Conversely, there is a significant negative correlation between SentLen and most downstream tasks.",4 Probing task experiments,[0],[0]
The number of words in a sentence is not informative about its linguistic contents.,4 Probing task experiments,[0],[0]
"The more models abstract away from such information, the more likely it is they will use their capacity to capture more interesting features, as the decrease of the SentLen curve along training (see Figure 1) also suggests.",4 Probing task experiments,[0],[0]
"CoordInv and, especially, SOMO, the tasks requiring the most sophisticated semantic knowledge, are those that positively correlate with the largest number of downstream tasks after WC.",4 Probing task experiments,[0],[0]
"We observe intriguing asymmetries: SOMO correlates with the SICK-E sentence entailment test, but not with SICK-R, which is about modeling sentence relatedness intuitions.",4 Probing task experiments,[0],[0]
"Indeed, logical entailment requires deeper semantic analysis than modeling similarity judgments.",4 Probing task experiments,[0],[0]
"TopConst and the number tasks negatively correlate with various similarity and sentiment data sets (SST, STS, SICK-R).",4 Probing task experiments,[0],[0]
"This might expose biases in these tasks: SICK-R, for example, deliberately contains sentence pairs with opposite voice, that will have different constituent structure but equal meaning (Marelli et al., 2014).",4 Probing task experiments,[0],[0]
"It might also mirrors genuine factors affecting similarity judgments (e.g., two sentences differing only in object number are very similar).",4 Probing task experiments,[0],[0]
"Remarkably, TREC question type classification is the downstream task correlating with most probing tasks.",4 Probing task experiments,[0],[0]
"Question classification is certainly an outlier among our downstream tasks, but we must leave a full understanding of this behaviour to future work (this is exactly the sort of analysis our probing tasks should stimulate).",4 Probing task experiments,[0],[0]
"Adi et al. (2017) introduced SentLen, WC and a word order test, focusing on a bag-of-vectors baseline, an autoencoder and skip-thought (all trained on the same data used for the probing tasks).",5 Related work,[0],[0]
"We recast their tasks so that they only require a sentence embedding as input (two of their tasks also require word embeddings, polluting sentencelevel evaluation), we extend the evaluation to more tasks, encoders and training objectives, and we relate performance on the probing tasks with that on downstream tasks.",5 Related work,[0],[0]
"Shi et al. (2016) also use 3 probing tasks, including Tense and TopConst.",5 Related work,[0],[0]
"It is not clear that they controlled for the same factors we considered (in particular, lexical overlap and
sentence length), and they use much smaller training sets, limiting classifier-based evaluation to logistic regression.",5 Related work,[0],[0]
"Moreover, they test a smaller set of models, focusing on machine translation.
",5 Related work,[0],[0]
"Belinkov et al. (2017a), Belinkov et al. (2017b) and Dalvi et al. (2017) are also interested in understanding the type of linguistic knowledge encoded in sentence and word embeddings, but their focus is on word-level morphosyntax and lexical semantics, and specifically on NMT encoders and decoders.",5 Related work,[0],[0]
"Sennrich (2017) also focuses on NMT systems, and proposes a contrastive test to assess how they handle various linguistic phenomena.",5 Related work,[0],[0]
"Other work explores the linguistic behaviour of recurrent networks and related models by using visualization, input/hidden representation deletion techniques or by looking at the word-by-word behaviour of the network (e.g., Nagamine et al., 2015; Hupkes et al., 2017; Li et al., 2016; Linzen et al., 2016; Kàdàr et al., 2017; Li et al., 2017).",5 Related work,[0],[0]
"These methods, complementary to ours, are not agnostic to encoder architecture, and cannot be used for general-purpose cross-model evaluation.
",5 Related work,[0],[0]
"Finally, Conneau et al. (2017) propose a largescale, multi-task evaluation of sentence embeddings, focusing entirely on downstream tasks.",5 Related work,[0],[0]
We introduced a set of tasks probing the linguistic knowledge of sentence embedding methods.,6 Conclusion,[0],[0]
"Their purpose is not to encourage the development of ad-hoc models that attain top performance on them, but to help exploring what information is
captured by different pre-trained encoders.",6 Conclusion,[0],[0]
We performed an extensive linguistic evaluation of modern sentence encoders.,6 Conclusion,[0],[0]
"Our results suggest that the encoders are capturing a wide range of properties, well above those captured by a set of strong baselines.",6 Conclusion,[0],[0]
"We further uncovered interesting patterns of correlation between the probing tasks and more complex “downstream” tasks, and presented a set of intriguing findings about the linguistic properties of various embedding methods.",6 Conclusion,[0],[0]
"For example, we found that Bag-of-Vectors is surprisingly good at capturing sentence-level properties, thanks to redundancies in natural linguistic input.",6 Conclusion,[0],[0]
"We showed that different encoder architectures trained with the same objective with similar performance can result in different embeddings, pointing out the importance of the architecture prior for sentence embeddings.",6 Conclusion,[0],[0]
"In particular, we found that BiLSTM-max embeddings are already capturing interesting linguistic knowledge before training, and that, after training, they detect semantic acceptability without having been exposed to anomalous sentences before.",6 Conclusion,[0],[0]
"We hope that our publicly available probing task set will become a standard benchmarking tool of the linguistic properties of new encoders, and that it will stir research towards a better understanding of what they learn.
",6 Conclusion,[0],[0]
"In future work, we would like to extend the probing tasks to other languages (which should be relatively easy, given that they are automatically generated), investigate how multi-task training affects probing task performance and leverage our probing tasks to find more linguistically-aware universal encoders.",6 Conclusion,[0],[0]
"We thank David Lopez-Paz, Holger Schwenk, Hervé Jégou, Marc’Aurelio Ranzato and Douwe Kiela for useful comments and discussions.",Acknowledgments,[0],[0]
"Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing.",abstractText,[0],[0]
"“Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations.",abstractText,[0],[0]
The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations.,abstractText,[0],[0]
"We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.",abstractText,[0],[0]
What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2032–2037, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
There is much interest in automatic recognition of demographic information of Internet users to improve the quality of online interactions.,1 Introduction,[0],[0]
"Researchers have looked into identifying a variety of factors about users, including age, gender, language, religious beliefs and political views.",1 Introduction,[0],[0]
"Most work leverages multiple sources of information, such as search query history, Twitter feeds, Facebook likes, social network links, and user profiles.",1 Introduction,[0],[0]
"However, in many situations, little of this information is available.",1 Introduction,[0],[0]
"Conversely, usernames are almost always available.
",1 Introduction,[0],[0]
"In this work, we look specifically at classifying gender and language based only on the username.",1 Introduction,[0],[0]
"Prior work by sociologists has established a link between usernames and gender (Cornetto and Nowak, 2006), and studies have linked usernames to other attributes, such as individual beliefs (Crabill, 2007; Hassa, 2012) and shown how usernames shape perceptions of gender and ethnicity in the absence of common nonverbal cues (Pelletier, 2014).",1 Introduction,[0],[0]
"The connections to ethnicity motivate the exploration of language identification.
",1 Introduction,[0],[0]
"Gender identification based on given names is very effective for English (Liu and Ruths, 2013), since many names are strongly associated with a
particular gender, like “Emily” or “Mark”.",1 Introduction,[0],[0]
"Unfortunately, the requirement that each username be unique precludes use of given names alone.",1 Introduction,[0],[0]
"Instead, usernames are typically a combination of component words, names and numbers.",1 Introduction,[0],[0]
"For example, the Twitter name @taylorswift13 might decompose into “taylor”, “swift” and “13”.",1 Introduction,[0],[0]
"The sub-units carry meaning and, importantly, they are shared with many other individuals.",1 Introduction,[0],[0]
"Thus, our approach is to leverage automatic decomposition of usernames into sub-units for use in classification.
",1 Introduction,[0],[0]
"We use the Morfessor algorithm (Creutz and Lagus, 2006; Virpioja et al., 2013) for unsupervised morphology induction to learn the decomposition of the usernames into sub-units.",1 Introduction,[0],[0]
"Morfessor has been used successfully in a variety of language modeling frameworks applied to a number of languages, particularly for learning concatenative morphological structure.",1 Introduction,[0],[0]
"The usernames that we analyze are a good match to the Morfessor framework, which allows us to push the boundary of how much can be done with only a username.
",1 Introduction,[0],[0]
"The classifier design is described in the next section, followed by a description of experiments on gender and language recognition that demonstrate the utility of morph-based features compared to character n-gram features.",1 Introduction,[0],[0]
The paper closes with a discussion of related work and a summary of key findings.,1 Introduction,[0],[0]
"In linguistics, a morpheme is the “minimal linguistic unit with lexical or grammatical meaning” (Booij, 2012).",2.1 Unsupervised Morphology Learning,[0],[0]
Morphemes are combined in various ways to create longer words.,2.1 Unsupervised Morphology Learning,[0],[0]
"Similarly, usernames are frequently made up of a concatenated sequence of smaller units.",2.1 Unsupervised Morphology Learning,[0],[0]
"These sub-units will be referred to as u-morphs to highlight the fact that they play an analogous role to morphemes but for
2032
purposes of encoding usernames rather than standard words in a language.",2.1 Unsupervised Morphology Learning,[0],[0]
"The u-morphs are subunits that are small enough to be shared across different usernames but retain some meaning.
",2.1 Unsupervised Morphology Learning,[0],[0]
"Unsupervised morphology induction using Morfessor (Creutz and Lagus, 2006) is based on a minimum description length (MDL) objective, which balances two competing goals: maximizing both the likelihood of the data and of the model.",2.1 Unsupervised Morphology Learning,[0],[0]
The likelihood of the data is maximized by longer tokens and a bigger lexicon whereas the likelihood of the model is maximized by a smaller lexicon with shorter tokens.,2.1 Unsupervised Morphology Learning,[0],[0]
"A parameter controls the trade-off between the two parts of the objective function, which alters the average u-morph length.",2.1 Unsupervised Morphology Learning,[0],[0]
"We tune this parameter on held-out data to optimize the classification performance of the demographic tasks.
",2.1 Unsupervised Morphology Learning,[0],[0]
Maximizing the Morfessor objective exactly is computationally intractable.,2.1 Unsupervised Morphology Learning,[0],[0]
The Morfessor algorithm searches for the optimal lexicon using an iterative approach.,2.1 Unsupervised Morphology Learning,[0],[0]
"First, the highest probability decomposition for each training token is found given the current model.",2.1 Unsupervised Morphology Learning,[0],[0]
"Then, the model is updated with the counts of the u-morphs.",2.1 Unsupervised Morphology Learning,[0],[0]
"A umorph is added to the lexicon when it increases the weighted likelihood of the data by more than the cost of increasing the size of the lexicon.
",2.1 Unsupervised Morphology Learning,[0],[0]
"Usernames can be mixed-case, e.g. “JohnDoe”.",2.1 Unsupervised Morphology Learning,[0],[0]
"The case change gives information about a likely u-morph boundary, but at the cost of doubling the size of the character set.",2.1 Unsupervised Morphology Learning,[0],[0]
"To more effectively leverage this cue, all characters are made lowercase but each change from lower to uppercase is marked with a special token, e.g. “john$doe”.",2.1 Unsupervised Morphology Learning,[0],[0]
"Using this encoding reduces the u-morph inventory size, and we found it to give slightly better results in language identification.
",2.1 Unsupervised Morphology Learning,[0],[0]
Character 3-grams and 4-grams are used as baseline features.,2.1 Unsupervised Morphology Learning,[0],[0]
Before extracting the n-grams a “#” token is placed at the start and end of each username.,2.1 Unsupervised Morphology Learning,[0],[0]
The n-grams are overlapping to give them the best chance of finding a semantically meaningful sub-unit.,2.1 Unsupervised Morphology Learning,[0],[0]
"Given a decomposition of the username into a sequence of u-morphs (or character n-grams), we represent the relationship between the observed features and each class with a unigram language model.",2.2 Classifier Design,[0],[0]
"If a username u has decomposition
m1, . . .",2.2 Classifier Design,[0],[0]
",mn then it is assigned to the class ci for which the unigram model gives it the highest posterior probability, or equivalently:
argmaxi pC(ci) n∏
k=1
p(mk|ci),
where pC(ci) is the class prior and p(mk|ci) is the class-dependent unigram.1
For some demographics, the class prior can be very skewed, as in the case of language detection where English is the dominant language.",2.2 Classifier Design,[0],[0]
"The choice of smoothing algorithm can be important in such cases, since minority classes have much less training data for estimating the language model and benefit from having more probability mass assigned to unseen words.",2.2 Classifier Design,[0],[0]
"Here, we follow the approach proposed in (Frank and Bouckaert, 2006) that normalizes the token count vectors for each class to have the same L1 norm, specifically:
p(mk|ci) = 1 Z
( 1 +
β · n(mk, ci) n(ci)
) ,
where n(·) indicates counts and β controls the strength of the smoothing.",2.2 Classifier Design,[0],[0]
Setting β equal to the number of training examples approximately matches the strength of the smoothing to the addone-smoothing algorithm.,2.2 Classifier Design,[0],[0]
"Z = β + |M | is a constant to make the probabilities sum to one.
",2.2 Classifier Design,[0],[0]
Only a small portion of usernames on the Internet come with gender labels.,2.2 Classifier Design,[0],[0]
"In these situations, semi-supervised learning algorithms can use the unlabeled data to improve the performance of the classifier.",2.2 Classifier Design,[0],[0]
"We use a self-training expectationmaximization (EM) algorithm similar to that described in (Nigam et al., 2000).",2.2 Classifier Design,[0],[0]
The algorithm first learns a classifier on the labeled data.,2.2 Classifier Design,[0],[0]
"In the E-step, the classifier assigns probabilistic labels to the unlabeled data.",2.2 Classifier Design,[0],[0]
"In the M-step, the labeled data and the probabilistic labels are combined to learn a new classifier.",2.2 Classifier Design,[0],[0]
"These steps are iterated until convergence, which usually requires three iterations for our tasks.
",2.2 Classifier Design,[0],[0]
"1Note that the unigram model used here, which considers only the observed u-morphs or n-grams, is not the same as using a Naive Bayes (NB) classifier based on a vector of u-morph counts.",2.2 Classifier Design,[0],[0]
"In the former, unobserved u-morphs do not impact the class-dependent probability, whereas the zero counts do impact the probability for the NB classifier.",2.2 Classifier Design,[0],[0]
"Since the vast majority of possible u-morphs are unobserved in a username, it is better to base the decision only on the observed u-morphs.",2.2 Classifier Design,[0],[0]
"The n-gram model is actually a unigram with an n-gram “vocabulary” rather than an n-gram language model.
",2.2 Classifier Design,[0],[0]
Nigam et al. (2000) call their method EM-λ because it uses a parameter λ to reduce the weight of the unlabeled examples relative to the labeled data.,2.2 Classifier Design,[0],[0]
This is important because the independence assumptions of the unigram model lead to overconfident predictions.,2.2 Classifier Design,[0],[0]
We used another method that directly corrects the estimated posterior probabilities.,2.2 Classifier Design,[0],[0]
"Using a small validation set, we binned the probability estimates and calculated the true class probability for each bin.",2.2 Classifier Design,[0],[0]
The EM algorithm used the corrected probabilities for each bin for the unlabeled data during the maximization step.,2.2 Classifier Design,[0],[0]
Samples with a prediction confidence of less than 60% are not used for training.,2.2 Classifier Design,[0],[0]
"Data was collected from the OkCupid dating site by downloading up to 1,000 profiles from 27 cities in the United States, first for men seeking women and again for women seeking men to obtain a balanced set of 44,000 usernames.",3.1 Gender Identification,[0],[0]
The data is partitioned into three sets with 80% assigned to training and 10% each to validation and test.,3.1 Gender Identification,[0],[0]
"We also use 3.5M usernames from the photo messaging app Snapchat (McCormick, 2014): 1.5M are used for u-morph learning and 2M are for self-training.",3.1 Gender Identification,[0],[0]
"All names in this task used only lower case, due to the nature of the available data.
",3.1 Gender Identification,[0],[0]
The top features ranked by likelihood ratios are given in Table 1.,3.1 Gender Identification,[0],[0]
"The u-morphs clearly carry semantic meaning, and the trigram features appear to be substrings of the top u-morph features.",3.1 Gender Identification,[0],[0]
The trigram features have an advantage when the u-morphs are under-segmented such as if the u-morph “niceguy” or “thatguy” is included in the lexicon.,3.1 Gender Identification,[0],[0]
"Conversely, the n-grams can suffer from over-segmentation.",3.1 Gender Identification,[0],[0]
"For example, the trigram “guy” is inside the surname “Nguyen” even though it is better to ignore that substring in this context.",3.1 Gender Identification,[0],[0]
"Many other tokens suffer from this problem, e.g. “miss” is in “mission”.
",3.1 Gender Identification,[0],[0]
The variable-length u-morphs are longer on average than the character n-grams (4.9 characters).,3.1 Gender Identification,[0],[0]
"The u-morph inventory size is similar to that for 3-grams but 5-10 times smaller than the 4-gram inventory, depending on the amount of data used since the inventory is expanded in semi-supervised training.",3.1 Gender Identification,[0],[0]
"By using the MDL criterion in unsupervised morphology learning, the u-morphs provide a more efficient representation of usernames than n-grams and make it easier to control the tradeoff between vocabulary size and average segment length.",3.1 Gender Identification,[0],[0]
"The smaller inventory is less sensitive to sparse data in language model training.
",3.1 Gender Identification,[0],[0]
The experiment results are presented in Table 2.,3.1 Gender Identification,[0],[0]
"For the supervised learning method, the character 3-gram and 4-gram features give equivalent performance, and the u-morph features give the lowest error rate by a small amount (3% relative).",3.1 Gender Identification,[0],[0]
"More significantly, the character n-gram systems do not benefit from semi-supervised learning, but the u-morph features do.",3.1 Gender Identification,[0],[0]
"The semi-supervised u-morph features obtain an error rate of 25.8%, which represents a 10% relative reduction over the baseline character n-gram results.",3.1 Gender Identification,[0],[0]
This experiment takes usernames from the Twitter streaming API.,3.2 Language Identification on Twitter,[0],[0]
"Each username is associated with a tweet, for which the Twitter API identifies a language.",3.2 Language Identification on Twitter,[0],[0]
"The language labels are noisy, so we remove approximately 35% of the tweets where the Twitter API does not agree with the langid.py classifier (Lui and Baldwin, 2012).",3.2 Language Identification on Twitter,[0],[0]
Both training and test sets are restricted to the nine languages that comprise at least 1% of the training set.,3.2 Language Identification on Twitter,[0],[0]
These languages cover 96% of the observed tweets (see Table 4).,3.2 Language Identification on Twitter,[0],[0]
"About 110,000 usernames were reserved for testing and 430,000 were used for training both u-morphs and the classifier.",3.2 Language Identification on Twitter,[0],[0]
Semi-supervised methods are not used because of the abundant labeled data.,3.2 Language Identification on Twitter,[0],[0]
"For each language, we train a one-vs.all classifier.",3.2 Language Identification on Twitter,[0],[0]
"The mixed case encoding technique (see sec. 2.1) gives a small increase (0.5%) in the
accuracy of the model and reduces the u-morph model size by 5%.
",3.2 Language Identification on Twitter,[0],[0]
"The results in Tables 3 and 4 contrast systems using 4-grams, u-morphs, and a combination model, showing precision-recall trade-offs for all users together and F1 scores broken down by specific languages, respectively.",3.2 Language Identification on Twitter,[0],[0]
The combination system simply uses the average of the posterior log-probabilities for each class giving equal weight to each model.,3.2 Language Identification on Twitter,[0],[0]
"While the overall F1 scores are similar for the 4-gram and u-morph systems, their precision and recall trade-offs are quite different, making them effective in combination.",3.2 Language Identification on Twitter,[0],[0]
"The 4-gram system has higher recall, and the u-morph system has higher precision.",3.2 Language Identification on Twitter,[0],[0]
"With the combination, we obtain a substantial gain in precision over the 4-gram system with a modest loss in recall, resulting in a 3% absolute improvement in average F1 score.
",3.2 Language Identification on Twitter,[0],[0]
"Looking at performance on the different languages, we find that the F1 score for the combination model is higher than the 4-gram for every language, with precision always improving.",3.2 Language Identification on Twitter,[0],[0]
"For the dominant languages, the difference in recall is negligible.",3.2 Language Identification on Twitter,[0],[0]
"The infrequent languages have a 4- 8% drop in recall, but the gains in precision are substantial for these languages, ranging from 50- 100% relative.",3.2 Language Identification on Twitter,[0],[0]
"The greatest contrast between the 4-gram and the combination system can be seen for the least frequent languages, i.e. the languages with the least amount of training data.",3.2 Language Identification on Twitter,[0],[0]
"In particular, for French, the precision of the combination system (0.36) is double that of the 4-gram model (0.18) with only a 34% loss in recall (0.24 to 0.16).
",3.2 Language Identification on Twitter,[0],[0]
Looking at the most important features from the classifier highlights the ability of the morphemes to capture relevant meaning.,3.2 Language Identification on Twitter,[0],[0]
"The presence of the morpheme “juan”, “jose” or “flor” increase the probability of a Spanish language tweet by five times.",3.2 Language Identification on Twitter,[0],[0]
The same is true for Portuguese and the morpheme “bieber”.,3.2 Language Identification on Twitter,[0],[0]
"The morpheme “q8”
increases the odds of an Arabic language tweet by thirteen times due to its phonetic similarity to the name of the Arabic speaking country Kuwait.",3.2 Language Identification on Twitter,[0],[0]
Other features may simply reflect cultural norms.,3.2 Language Identification on Twitter,[0],[0]
"For example, having an underscore in the username makes it five percent less likely to observe an English tweet.",3.2 Language Identification on Twitter,[0],[0]
These highly discriminative morphemes are both long and short.,3.2 Language Identification on Twitter,[0],[0]
It is hard for the fixed-length n-grams to capture this information as well as the morphemes do.,3.2 Language Identification on Twitter,[0],[0]
"Of the many studies on automatic classification of online user demographics, few have leveraged names or usernames at all, and the few that do mainly explore their use in combination with other features.",4 Related Work,[0],[0]
"The work presented here differs in its use of usernames alone, but more importantly in the introduction of morphological analysis to handle a large number of usernames.
",4 Related Work,[0],[0]
Two studies on gender recognition are particularly relevant.,4 Related Work,[0],[0]
"Burger et al. (2011) use the Twitter username (or screen name) in combination with other profile and text features to predict gender, but they also look at the use of username features alone.",4 Related Work,[0],[0]
"The results are not directly comparable to ours, because of differences in the data set used (150k Twitter users) and the classifier framework (Winnow), but the character n-gram performance is similar to ours (21-22% different from the majority baseline).",4 Related Work,[0],[0]
The study uses over 400k character n-grams (n=1-5) for screen names alone; our study indicatess that the u-morphs can reduce this number by a factor of 10.,4 Related Work,[0],[0]
"Burger et al. (2011) used the same strategy with the self-identified full
name of the user as entered into their profile, obtaining 89% gender recognition (vs. 77% for screen names).",4 Related Work,[0],[0]
"Later, Liu and Ruths (2013) use the full first name from a user’s profile for gender detection, finding that for the names that are highly predictive of gender, performance improves by relying on this feature alone.",4 Related Work,[0],[0]
"However, more than half of the users have a name that has an unknown gender association.",4 Related Work,[0],[0]
"Manual inspection of these cases indicated that the majority includes strings formed like usernames, nicknames or other types of word concatenations.",4 Related Work,[0],[0]
"These examples are precisely what the u-morph approach tries to address.
",4 Related Work,[0],[0]
"Language identification is an active area of research (Bergsma et al., 2012; Zubiaga et al., 2014), but the username has not been used as a feature.",4 Related Work,[0],[0]
"Again, results are difficult to compare due to the lack of a common test set, but it is notable that the average F1 score for the combination model approaches the scores obtained on a similar Twitter language identification task where the algorithm has access to the full text of the tweet (Lui and Baldwin, 2014): 73% vs. 77% .
",4 Related Work,[0],[0]
"A study that is potentially relevant to our work is automatic classification of ethnicity of Twitter users, specifically whether a user is AfricanAmerican (Pennacchiotti and Popescu, 2011).",4 Related Work,[0],[0]
"Again, a variety of content, profile and behavioral features are used.",4 Related Work,[0],[0]
"Orthographic features of the username are used (e.g. length, number of numeric/alpha characters), and names of users that a person retweets or replies to.",4 Related Work,[0],[0]
"The profile name features do not appear to be useful, but examples of related usernames point to the utility of our approach for analysis of names in other fields.",4 Related Work,[0],[0]
"In summary, this paper has introduced the use of unsupervised morphological analysis of usernames to extract features (u-morphs) for identifying user demographics, particularly gender and language.",5 Conclusions,[0],[0]
"The experimental results demonstrate that usernames contain useful personal information, and that the u-morphs provide a more efficient and complementary representation than character n-grams.2 The result for language identification is particularly remarkable because it comes close to matching the performance achieved by us-
2In order to allow the replicability of the experiments, software and data for building and evaluating our classifiers using pre-trained Morfessor models is available at http: //github.com/ajaech/username_analytics.
ing the full text of a tweet.",5 Conclusions,[0],[0]
"The work is complementary to other demographic studies in that the username prediction can be used together with other features, both for the user and members of his/her social network.
",5 Conclusions,[0],[0]
The methods proposed here could be extended in different directions.,5 Conclusions,[0],[0]
The unsupervised morphology learning algorithm could incorporate priors related to capitalization and non-alphabetic characters to better model these phenomena than our simple text normalization approach.,5 Conclusions,[0],[0]
"More sophisticated classifiers could also be used, such as variable-length n-grams or neural-network-based n-gram language models, as opposed to the unigram model used here.",5 Conclusions,[0],[0]
"Of course the sophistication of the classifier will be limited by the amount of training data available.
",5 Conclusions,[0],[0]
A large amount of data is not necessary to build a high precision username classifier.,5 Conclusions,[0],[0]
"For example, less than 7,000 training examples were available for Turkish in the language identification experiment and the classifier had a precision of 76%.",5 Conclusions,[0],[0]
"Since little data is required, there may be many more applications of this type of model.
",5 Conclusions,[0],[0]
Prior work on unsupervised morphological induction focused on applying the algorithm to natural language input.,5 Conclusions,[0],[0]
"By using those techniques with a new type of input, this paper shows that there are other applications of morphology learning.",5 Conclusions,[0],[0]
"Usernames are ubiquitous on the Internet, and they are often suggestive of user demographics.",abstractText,[0],[0]
This work looks at the degree to which gender and language can be inferred from a username alone by making use of unsupervised morphology induction to decompose usernames into sub-units.,abstractText,[0],[0]
Experimental results on the two tasks demonstrate the effectiveness of the proposed morphological features compared to a character n-gram baseline.,abstractText,[0],[0]
What Your Username Says About You,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper, we benchmark recursive neural models against sequential recurrent neural models, enforcing applesto-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answerphrases; (3) discourse parsing; (4) semantic relation extraction.
Our goal is to understand better when, and why, recursive models can outperform simpler models. We find that recursive models help mainly on tasks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models.",text,[0],[0]
"Deep learning based methods learn lowdimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text.
",1 Introduction,[0],[0]
"For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks.",1 Introduction,[0],[0]
"Models for achieving this usually fall into two categories: recurrent models and recursive models:
Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012).",1 Introduction,[0],[0]
"They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings.",1 Introduction,[0],[0]
Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left.,1 Introduction,[0],[0]
"These models generally consider no linguistic structure aside from word order.
",1 Introduction,[0],[0]
"Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees.",1 Introduction,[0],[0]
"Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached.",1 Introduction,[0],[0]
"For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) )",1 Introduction,[0],[0]
rather than the sequential order (((the food) is) delicious),1 Introduction,[0],[0]
.,1 Introduction,[0],[0]
"Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014).
2304
",1 Introduction,[0],[0]
"One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence.",1 Introduction,[0],[0]
"For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013).",1 Introduction,[0],[0]
"However we do not know if this advantage is truly important, and if so for which tasks, or whether other issues are at play.",1 Introduction,[0],[0]
"Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful.
",1 Introduction,[0],[0]
"On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed.",1 Introduction,[0],[0]
"Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset.
",1 Introduction,[0],[0]
"Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages.",1 Introduction,[0],[0]
"We investigate four tasks with different properties.
",1 Introduction,[0],[0]
"• Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios such as different lengths of inputs and whether or not supervision is comprehensive.
",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Phrase Matching on the UMD-QA dataset (Iyyer et al., 2014) can help see the difference between outputs from intermediate components from different models, i.e., representations for intermediate parse tree nodes and outputs from recurrent models at different time steps.",1 Introduction,[0],[0]
"It also helps see whether pars-
ing is useful for finding similarities between question sentences and target phrases.
",1 Introduction,[0],[0]
"• Semantic Relation Classification on the SemEval-2010 (Hendrickx et al., 2009)",1 Introduction,[0],[0]
"data can help understand whether parsing is helpful in dealing with long-term dependencies, such as relations between two words that are far apart in the sequence.
",1 Introduction,[0],[0]
• Discourse parsing (RST dataset) is useful for measuring the extent to which parsing improves discourse tasks that need to combine meanings of larger text units.,1 Introduction,[0],[0]
"Discourse parsing treats elementary discourse units (EDUs) as basic units to operate on, which are usually short clauses.",1 Introduction,[0],[0]
"The task also sheds light on the extent to which syntactic structures help acquire shot text representations.
",1 Introduction,[0],[0]
"The principal motivation for this paper is to understand better when, and why, recursive models are needed to outperform simpler models by enforcing apples-to-apples comparison as much as possible.",1 Introduction,[0],[0]
"This paper applies existing models to existing tasks, barely offering novel algorithms or tasks.",1 Introduction,[0],[0]
"Our goal is rather an analytic one, to investigate different versions of recursive and recurrent models.",1 Introduction,[0],[0]
"This work helps understand the limitations of both classes of models, and suggest directions for improving recurrent models.
",1 Introduction,[0],[0]
"The rest of this paper organized as follows: We detail versions of recursive/recurrent models in Section 2, present the tasks and results in Section 3, and conclude with discussions in Section 4.",1 Introduction,[0],[0]
"We assume that the text unit S, which could be a phrase, a sentence or a document, is comprised of a sequence of tokens/words: S = {w1, w2, ..., wNS}, where Ns denotes the number of tokens in S. Each word w is associated with a K-dimensional vector embedding ew = {e1w, e2w, ..., eKw }.",2.1 Notations,[0],[0]
"The goal of recursive and recurrent models is to map the sequence to a Kdimensional eS , based on its tokens and their correspondent embeddings.
",2.1 Notations,[0],[0]
"Standard Recurrent/Sequence Models successively take word wt at step t, combines its vector representation et with the previously built hidden vector ht−1 from time t− 1, calculates the re-
sulting current embedding ht, and passes it to the next step.",2.1 Notations,[0],[0]
"The embedding ht for the current time t is thus:
ht = f(W · ht−1 + V · et) (1)
whereW and V denote compositional matrices.",2.1 Notations,[0],[0]
"If Ns denotes the length of the sequence, hNs represents the whole sequence S.
Standard recursive/Tree models work in a similar way, but processing neighboring words by parse tree order rather than sequence order.",2.1 Notations,[0],[0]
It computes a representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree.,2.1 Notations,[0],[0]
"For a given node η in the tree and its left child ηleft (with representation eleft) and right child ηright (with representation eright), the standard recursive network calculates eη as follows:
eη = f(W · eηleft + V · eηright) (2)
Bidirectional Models (Schuster and Paliwal, 1997) add bidirectionality to the recurrent framework where embeddings for each time are calculated both forwardly and backwardly:
h→t = f(W → · h→t−1 + V→ · et) h←t = f(W ← · h←t+1 + V← · et)
(3)
Normally, final representations for sentences can be achieved either by concatenating vectors calculated from both directions",2.1 Notations,[0],[0]
"[e←1 , e→NS ] or using further compositional operation to preserve vector dimensionality
ht = f(WL ·",2.1 Notations,[0],[0]
"[h←t , h→t ]) (4)
where WL denotes a K×2K dimensional matrix.",2.1 Notations,[0],[0]
"Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX}, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot.",2.1 Notations,[0],[0]
"We notationally disambiguate e and h: et denotes the vector for individual text units (e.g., word or sentence) at time step t, while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1.",2.1 Notations,[0],[0]
σ denotes the sigmoid function.,2.1 Notations,[0],[0]
"The vector representation ht for each time-step t is given by:
 it ft ot lt  =  σ σ σ",2.1 Notations,[0],[0]
"tanh W · [ ht−1et ]
(5)
ct = ft · ct−1 + it · lt (6) hst = ot · ct (7)
where W ∈ R4K×2K .",2.1 Notations,[0],[0]
"Labels at the phrase/sentence level are predicted representations outputted from the last time step.
",2.1 Notations,[0],[0]
"Tree LSTMs Recent research has extended the LSTM idea to tree-based structures (Zhu et al., 2015; Tai et al., 2015) that associate memory and forget gates to nodes of the parse trees.
",2.1 Notations,[0],[0]
Bi-directional LSTMs These combine bidirectional models and LSTMs.,2.1 Notations,[0],[0]
"In this section, we detail our experimental settings and results.",3 Experiments,[0],[0]
"We consider the following tasks, each representative of a different class of NLP tasks.
",3 Experiments,[0],[0]
• Binary sentiment classification on the Pang et al. (2002) dataset.,3 Experiments,[0],[0]
"This addresses the issues where supervision only appears globally after a long sequence of operations.
",3 Experiments,[0],[0]
"• Sentiment Classification on the Stanford Sentiment Treebank (Socher et al., 2013): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned.
",3 Experiments,[0],[0]
"• Sentence-Target Matching on the UMDQA dataset (Iyyer et al., 2014):",3 Experiments,[0],[0]
"Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models.
•",3 Experiments,[0],[0]
"Semantic Relation Classification on the SemEval-2010 task (Hendrickx et al., 2009).",3 Experiments,[0],[0]
"Learns long-distance relationships between two words that may be far apart sequentially.
",3 Experiments,[0],[0]
"• Discourse Parsing (Li et al., 2014; Hernault et al., 2010): Learns sentence-to-sentence relations based on calculated representations.
",3 Experiments,[0],[0]
In each case we followed the protocols described in the original papers.,3 Experiments,[0],[0]
"We first group the algorithm variants into two groups as follows:
• Standard tree models vs standard sequence models vs standard bi-directional sequence models
• LSTM tree models, LSTM sequence models vs LSTM bi-directional sequence models.
",3 Experiments,[0],[0]
"We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad (Duchi et al., 2011) with minibatches (Cotter et al., 2011).",3 Experiments,[0],[0]
Parameters are tuned using the development dataset if available in the original datasets or from crossvalidation if not.,3 Experiments,[0],[0]
"Derivatives are calculated from standard back-propagation (Goller and Kuchler, 1996).",3 Experiments,[0],[0]
"Parameters to tune include size of mini batches, learning rate, and parameters for L2 penalizations.",3 Experiments,[0],[0]
"The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated.
",3 Experiments,[0],[0]
"For settings where no repeated experiments are performed, the bootstrap test is adopted for statistical significance testing (Efron and Tibshirani, 1994).",3 Experiments,[0],[0]
Test scores that achieve significance level of 0.05 are marked by an asterisk (*).,3 Experiments,[0],[0]
"Task Description We start with the Stanford Sentiment TreeBank (Socher et al., 2013).",3.1 Stanford Sentiment TreeBank,[0],[0]
"This dataset contains gold-standard labels for every parse tree constituent, from the sentence to phrases to individual words.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"Of course, any conclusions drawn from implementing sequence models on a dataset that was based on parse trees may have to be weakened, since sequence models may still benefit from the way that the dataset was collected.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Nevertheless we add an evaluation on this dataset because it has been a widely used benchmark dataset for neural model evaluations.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"For recursive models, we followed the protocols in Socher et al. (2013) where node embeddings in the parse trees are obtained from recursive models and then fed to a softmax classifier.",3.1 Stanford Sentiment TreeBank,[0],[0]
We transformed the dataset for recurrent model use as illustrated in Figure 1.,3.1 Stanford Sentiment TreeBank,[0],[0]
Each phrase is reconstructed from parse tree nodes and treated as a separate data point.,3.1 Stanford Sentiment TreeBank,[0],[0]
"As the treebank contains 11,855
sentences with 215,154 phrases, the reconstructed dataset for recurrent models comprises 215,154 examples.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Models are evaluated at both the phrase level (82,600 instances) and the sentence root level (2,210 instances).
",3.1 Stanford Sentiment TreeBank,[0],[0]
Results are shown in Table 1 and 21.,3.1 Stanford Sentiment TreeBank,[0],[0]
"When comparing the standard version of tree models to sequence models, we find it helps a bit at root level identification (for sequences but not bisequences), but yields no significant improvement at the phrase level.
",3.1 Stanford Sentiment TreeBank,[0],[0]
LSTM Tai et al. (2015) discovered that LSTM tree models generate better performances in terms of sentence root level evaluation than sequence models.,3.1 Stanford Sentiment TreeBank,[0],[0]
We explore this task a bit more by training deeper and more sophisticated models.,3.1 Stanford Sentiment TreeBank,[0],[0]
"We examine the following three models:
1.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Tree-structured LSTM models (Tai et al., 2015)2.
2.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Deep Bi-LSTM sequence models (denoted as Sequence) that treat the whole sentence as just one sequence.
3.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Deep Bi-LSTM hierarchical sequence models (denoted as Hierarchical Sequence) that first slice the sentence into a sequence of subsentences by using a look-up table of punctuations (i.e., comma, period, question mark
1The performance of our implementations of recursive models is not exactly identical to that reported in Socher et al. (2013), but the relative difference is around 1% to 2%.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"2Tai et al.. achieved 0.510 accuracy in terms of finegrained evaluation at the root level as reported in (Tai et al., 2015), similar to results from our implementations (0.504).
and exclamation mark).",3.1 Stanford Sentiment TreeBank,[0],[0]
"The representation for each sub-sentence is first computed separately, and another level of sequence LSTM (one-directional) is then used to join the subsentences.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Illustrations are shown in Figure2.
",3.1 Stanford Sentiment TreeBank,[0],[0]
We consider the third model because the dataset used in Tai et al. (2015) contains long sentences and the evaluation is performed only at the sentence root level.,3.1 Stanford Sentiment TreeBank,[0],[0]
"Since a parsing algorithm will naturally break long sentences into sub-sentences, we would like to know whether any performance boost is introduced by the intra-clause parse tree structure or just by this broader segmentation of a sentence into clause-like units; this latter advantage could be approximated by using punctuationbased approximations to clause boundaries.
",3.1 Stanford Sentiment TreeBank,[0],[0]
We run 15 iterations for each algorithm.,3.1 Stanford Sentiment TreeBank,[0],[0]
Parameters are harvested at the end of each iteration; those performing best on the development set are used on the test set.,3.1 Stanford Sentiment TreeBank,[0],[0]
The whole process takes roughly 15-20 minutes on a single GPU machine3.,3.1 Stanford Sentiment TreeBank,[0],[0]
"For a more convincing comparison, we did not use the bootstrap test where parallel examples are generated from one same dataset.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Instead, we repeated the aforementioned procedure for each algorithm 20 times and report accuracies
3Tesla K40m, 2880 Cuda cores.
with standard deviation in Table 3.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"Tree LSTMs are equivalent or marginally better than standard bi-directional sequence model (two-tailed p-value equals 0.041*, and only at the root level, with p-value for the phrase level at 0.376).",3.1 Stanford Sentiment TreeBank,[0],[0]
"The hierarchical sequence model achieves the same performance with a p-value of 0.198.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"Discussion The results above suggest that clausal segmentation of long sentences offers a slight performance boost, a result also supported by the fact that very little difference exists between the three models for phrase-level sentiment evaluation.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Clausal segmentation of long sentences thus provides a simple approximation to parse-tree based models.
",3.1 Stanford Sentiment TreeBank,[0],[0]
"We suggest a few reasons for this slightly better performances introduced by clausal segmentation:
1.",3.1 Stanford Sentiment TreeBank,[0],[0]
"Treating clauses as basic units (to the extent that punctuation approximates clauses) preserves the semantic structure of text.
2.",3.1 Stanford Sentiment TreeBank,[0],[0]
Semantic compositions such as negations or conjunctions usually appear at the clause level.,3.1 Stanford Sentiment TreeBank,[0],[0]
"Working on clauses individually and then combining them model inter-clause compositions.
",3.1 Stanford Sentiment TreeBank,[0],[0]
3.,3.1 Stanford Sentiment TreeBank,[0],[0]
Errors are back-propagated to individual tokens using fewer steps in hierarchical models than in standard models.,3.1 Stanford Sentiment TreeBank,[0],[0]
"Consider a movie
review “simple as the plot was , i still like it a lot”.",3.1 Stanford Sentiment TreeBank,[0],[0]
"With standard recurrent models it takes 12 steps before the prediction error gets back to the first token “simple”:
error→lot→a→it→like→still→i→,→was →plot→",3.1 Stanford Sentiment TreeBank,[0],[0]
"the→as→simple In a hierarchical model, the second clause is compacted into one component, and the error propagation is thus given by:
error→",3.1 Stanford Sentiment TreeBank,[0],[0]
second-clause → first-clause → was→plot→the→as→simple.,3.1 Stanford Sentiment TreeBank,[0],[0]
Propagation with clause segmentation consists of only 8 operations.,3.1 Stanford Sentiment TreeBank,[0],[0]
"Such a procedure thus tends to attenuate the gradient vanishing problem, potentially yielding better performance.",3.1 Stanford Sentiment TreeBank,[0],[0]
Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence.,3.2 Binary Sentiment Classification (Pang),[0],[0]
We divide the original dataset into training(8101)/dev(500)/testing(2000).,3.2 Binary Sentiment Classification (Pang),[0],[0]
No pretraining procedure as described in Socher et al. (2011b) is employed.,3.2 Binary Sentiment Classification (Pang),[0],[0]
Word embeddings are initialized using skip-grams and kept fixed in the learning procedure.,3.2 Binary Sentiment Classification (Pang),[0],[0]
We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package4.,3.2 Binary Sentiment Classification (Pang),[0],[0]
Sentence level embeddings are fed into a sigmoid classifier.,3.2 Binary Sentiment Classification (Pang),[0],[0]
"Performances for 50 dimensional vectors are given in the table below:
Discussion Why don’t parse trees help on this task?",3.2 Binary Sentiment Classification (Pang),[0],[0]
"One possible explanation is the distance
4https://code.google.com/p/word2vec/
of the supervision signal from the local compositional structure.",3.2 Binary Sentiment Classification (Pang),[0],[0]
"The Pang et al. dataset has an average sentence length of 22.5 words, which means it takes multiple steps before sentiment related evidence comes up to the surface.",3.2 Binary Sentiment Classification (Pang),[0],[0]
"It is therefore unclear whether local compositional operators (such as negation) can be learned; there is only a small amount of training data (around 8,000 examples) and the sentiment supervision only at the level of the sentence may not be easy to propagate down to deeply buried local phrases.",3.2 Binary Sentiment Classification (Pang),[0],[0]
Task Description:,3.3 Question-Answer Matching,[0],[0]
"In the question-answering dataset QANTA5, each answer is a token or short phrase.",3.3 Question-Answer Matching,[0],[0]
"The task is different from standard generation focused QA task but formalized as a multiclass classification task that matches a source question with a candidates phrase from a predefined pool of candidate phrases We give an illustrative example here:
Question: He left unfinished a novel whose title character forges his father’s signature to get out of school and avoids the draft by feigning desire to join.",3.3 Question-Answer Matching,[0],[0]
"Name this German author of The Magic Mountain and Death in Venice.
",3.3 Question-Answer Matching,[0],[0]
Answer: Thomas Mann from the pool of phrases.,3.3 Question-Answer Matching,[0],[0]
"Other candidates might include George Washington, Charlie Chaplin, etc.
",3.3 Question-Answer Matching,[0],[0]
The model of Iyyer et al. (2014) minimizes the distances between answer embeddings and node embeddings along the parse tree of the question.,3.3 Question-Answer Matching,[0],[0]
"Concretely, let c denote the correct answer to question S, with embedding ~c, and z denoting any random wrong answer.",3.3 Question-Answer Matching,[0],[0]
"The objective function sums over the dot product between representation for every node η along the question parse trees and the answer representations:
L = ∑
η∈[parse tree] ∑ z max(0, 1−~c ·eη+~z ·eη) (8)
5http://cs.umd.edu/˜miyyer/qblearn/. Because the publicly released dataset is smaller than the version used in (Iyyer et al., 2014) due to privacy issues, our numbers are not comparable to those in (Iyyer et al., 2014).
",3.3 Question-Answer Matching,[0],[0]
where eη denotes the embedding for parse tree node calculated from the recursive neural model.,3.3 Question-Answer Matching,[0],[0]
"Here the parse trees are dependency parses following (Iyyer et al., 2014).
",3.3 Question-Answer Matching,[0],[0]
"By adjusting the framework to recurrent models, we minimize the distance between the answer embedding and the embeddings calculated from each timestep t of the sequence:
L = ∑
t∈[1,Ns]
∑ z max(0, 1− ~c · et + ~z · et) (9)
",3.3 Question-Answer Matching,[0],[0]
"At test time, the model chooses the answer (from the set of candidates) that gives the lowest loss score.",3.3 Question-Answer Matching,[0],[0]
"As can be seen from results presented in Table 5, the difference is only significant for the LSTM setting between the tree model and the sequence model; no significant difference is observed for other settings.
",3.3 Question-Answer Matching,[0],[0]
"Discussion The UMD-QA task represents a group of situations where because we have insufficient supervision about matching (it’s hard to know which node in the parse tree or which timestep provides the most direct evidence for the answer), decisions have to be made by looking at and iterating over all subunits (all nodes in parse trees or timesteps).",3.3 Question-Answer Matching,[0],[0]
"Similar ideas can be found in pooling structures (e.g. Socher et al. (2011a)).
",3.3 Question-Answer Matching,[0],[0]
"The results above illustrate that for tasks where we try to align the target with different source components (i.e., parse tree nodes for tree models and different time steps for sequence models), components from sequence models are able to embed important information, despite the fact that sequence model components are just sentence fragments and hence usually not linguistically meaningful components in the way that parse tree constituents are.",3.3 Question-Answer Matching,[0],[0]
"Task Description: SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2”
classifying the relation between [apartment] and",3.4 Semantic Relationship Classification,[0],[0]
[kitchen] as component-whole.,3.4 Semantic Relationship Classification,[0],[0]
"The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009; Socher et al. (2012) for details.
",3.4 Semantic Relationship Classification,[0],[0]
"For the recursive implementations, we follow the neural framework defined in Socher et al. (2012).",3.4 Semantic Relationship Classification,[0],[0]
"The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier6.",3.4 Semantic Relationship Classification,[0.9578874922969792],"['The offset vector distribution at the word platform, shown in Figure 5b, shows that the model assigns high proba- Misty is between the wall and the flowers that are close to the corner.']"
"Retrieved paths are transformed for the recurrent models as shown in Figure 5.
",3.4 Semantic Relationship Classification,[0],[0]
"Discussion Unlike for earlier tasks, here recursive models yield much better performance than the corresponding recurrent versions for all versions (e.g., standard tree vs. standard sequence, p = 0.004).",3.4 Semantic Relationship Classification,[0],[0]
These results suggest that it is the need to integrate structures far apart in the sentence that characterizes the tasks where recursive models surpass recurrent models.,3.4 Semantic Relationship Classification,[0],[0]
"In parse-based models, the two target words are drawn together much earlier in the decision process than in recurrent models, which must remember one target until the other one appears.",3.4 Semantic Relationship Classification,[0],[0]
"Task Description: Our final task, discourse parsing based on the RST-DT corpus (Carlson et
6(Socher et al., 2012) achieve state-of-art performance by combining a sophisticated model, MV-RNN, in which each word is presented with both a matrix and a vector with human-feature engineering.",3.5 Discourse Parsing,[0],[0]
"Again, because MV-RNN is difficult to adapt to a recurrent version, we do not employ this state-of-the-art model, adhering only to the general versions of recursive models described in Section 2, since our main goal is to compare equivalent recursive and recurrent models rather than implement the state of the art.
",3.5 Discourse Parsing,[0],[0]
"al., 2003), is to build a discourse tree for a document, based on assigning Rhetorical Structure Theory (RST) relations between elementary discourse units (EDUs).",3.5 Discourse Parsing,[0],[0]
"Because discourse relations express the coherence structure of discourse, they presumably express different aspects of compositional meaning than sentiment or nominal relations.",3.5 Discourse Parsing,[0],[0]
"See Hernault et al. (2010) for more details on discourse parsing and the RST-DT corpus.
",3.5 Discourse Parsing,[0],[0]
"Representations for adjacent EDUs are fed into binary classification (whether two EDUs are related) and multi-class relation classification models, as defined in Li et al. (2014).",3.5 Discourse Parsing,[0],[0]
"Related EDUs are then merged into a new EDU, the representation of which is obtained through an operation of neural composition based on the previous two related EDUs.",3.5 Discourse Parsing,[0],[0]
"This step is repeated until all units are merged.
",3.5 Discourse Parsing,[0],[0]
"Discourse parsing takes EDUs as the basic units to operate on; EDUs are short clauses, not full sentences, with an average length of 7.2 words.",3.5 Discourse Parsing,[0],[0]
Recursive and recurrent models are applied on EDUs to create embeddings to be used as inputs for discourse parsing.,3.5 Discourse Parsing,[0],[0]
We use this task for two reasons: (1) to illustrate whether syntactic parse trees are useful for acquiring representations for short clauses.,3.5 Discourse Parsing,[0],[0]
"(2) to measure the extent to which pars-
ing improves discourse tasks that need to combine the meanings of larger text units.
",3.5 Discourse Parsing,[0],[0]
"Models are traditionally evaluated in terms of three metrics, i.e., spans7, nuclearity8, and identifying the rhetorical relation between two clauses.",3.5 Discourse Parsing,[0],[0]
"Due to space limits, we only focus the last one, rhetorical relation identification, because (1) relation labels are treated as correct only if spans and nuclearity are correctly labeled (2) relation identification between clauses offer more insights about model’s abilities to represent sentence semantics.",3.5 Discourse Parsing,[0],[0]
"In order to perform a plain comparison, no additional human-developed features are added.
",3.5 Discourse Parsing,[0],[0]
Discussion We see no large differences between equivalent recurrent and recursive models.,3.5 Discourse Parsing,[0],[0]
We suggest two possible explanations.,3.5 Discourse Parsing,[0],[0]
"(1) EDUs tend to be short; thus for some clauses, parsing might not change the order of operations on words.",3.5 Discourse Parsing,[0],[0]
"Even for those whose orders are changed by parse trees, the influence of short phrases on the final representation may not be great enough.",3.5 Discourse Parsing,[0],[0]
"(2) Unlike earlier tasks, where text representations are immediately used as inputs into classifiers, the algorithm presented here adopts additional levels of neural composition during the process of EDU merging.",3.5 Discourse Parsing,[0],[0]
"We suspect that neural layers may act as information filters, separating the informational chaff from the wheat, which in turn makes the model a bit more immune to the initial inputs.",3.5 Discourse Parsing,[0],[0]
"We compared recursive and recurrent neural models for representation learning on 5 distinct NLP tasks in 4 areas for which recursive neural models are known to achieve good performance (Socher et al., 2012; Socher et al., 2013; Li et al., 2014; Iyyer et al., 2014).
",4 Discussions and Conclusions,[0],[0]
"As with any comparison between models, our results come with some caveats: First, we explore the most general or basic forms of recur-
7on blank tree structures.",4 Discussions and Conclusions,[0],[0]
"8on tree structures with nuclearity indication.
",4 Discussions and Conclusions,[0],[0]
sive/recurrent models rather than various sophisticated algorithm variants.,4 Discussions and Conclusions,[0],[0]
"This is because fair comparison becomes more and more difficult as models get complex (e.g., the number of layers, number of hidden units within each layer, etc.).",4 Discussions and Conclusions,[0],[0]
Thus most neural models employed in this work are comprised of only one layer of neural compositions—despite the fact that deep neural models with multiple layers give better results.,4 Discussions and Conclusions,[0],[0]
"Our conclusions might thus be limited to the algorithms employed in this paper, and it is unclear whether they can be extended to other variants or to the latest state-of-the-art.",4 Discussions and Conclusions,[0.9555676780125008],"['For example, if probability mass is placed on the “oneblock-to-the-right” offset vector, this corresponds to predicting that Misty will be one block to the right of the voxels that a word refers to.']"
"Second, in order to compare models “fairly”, we force every model to be trained exactly in the same way:",4 Discussions and Conclusions,[0],[0]
"AdaGrad with minibatches, same set of initializations, etc.",4 Discussions and Conclusions,[0],[0]
"However, this may not necessarily be the optimal way to train every model; different training strategies tailored for specific models may improve their performances.",4 Discussions and Conclusions,[0],[0]
"In that sense, our attempts to be “fair” in this paper may nevertheless be unfair.
",4 Discussions and Conclusions,[0],[0]
"Pace these caveats, our conclusions can be summarized as follows:
•",4 Discussions and Conclusions,[0],[0]
"In tasks like semantic relation extraction, in which single headwords need to be associated across a long distance, recursive models shine.",4 Discussions and Conclusions,[0],[0]
"This suggests that for the many other kinds of tasks in which long-distance semantic dependencies play a role (e.g., translation between languages with significant reordering like Chinese-English translation), syntactic structures from recursive models may offer useful power.
",4 Discussions and Conclusions,[0],[0]
"• Tree models tend to help more on long sequences than shorter ones with sufficient supervision: tree models slightly help root level identification on the Stanford Sentiment Treebank, but do not help much at the phrase level.",4 Discussions and Conclusions,[0],[0]
"Adopting bi-directional versions of recurrent models seem to largely bridge this gap, producing equivalent or sometimes better results.
",4 Discussions and Conclusions,[0],[0]
"• On long sequences where supervision is not sufficient, e.g., in Pang at al.,’s dataset (supervision only exists on top of long sequences), no significant difference is observed between tree based and sequence based models.
",4 Discussions and Conclusions,[0],[0]
"• In cases where tree-based models do well, a simple approximation to tree-based models
seems to improve recurrent models to equivalent or almost equivalent performance: (1) break long sentences (on punctuation) into a series of clause-like units, (2) work on these clauses separately, and (3) join them together.",4 Discussions and Conclusions,[0],[0]
"This model sometimes works as well as tree models for the sentiment task, suggesting that one of the reasons tree models help is by breaking down long sentences into more manageable units.
",4 Discussions and Conclusions,[0],[0]
"• Despite that the fact that components (outputs from different time steps) in recurrent models are not linguistically meaningful, they may do as well as linguistically meaningful phrases (represented by parse tree nodes) in embedding informative evidence, as demonstrated in UMD-QA task.",4 Discussions and Conclusions,[0],[0]
"Indeed, recent work in parallel with ours (Bowman et al., 2015) has shown that recurrent models like LSTMs can discover implicit recursive compositional structure.",4 Discussions and Conclusions,[0],[0]
"We would especially like to thank Richard Socher and Kai-Sheng Tai for insightful comments, advice, and suggestions.",5 Acknowledgments,[0],[0]
"We would also like to thank Sam Bowman, Ignacio Cases, Jon Gauthier, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as the anonymous reviewers for their helpful advice on various aspects of this work.",5 Acknowledgments,[0],[0]
"We acknowledge the support of NVIDIA Corporation with the donation of Tesla K40 GPUs We gratefully acknowledge support from an Enlight Foundation Graduate Fellowship, a gift from Bloomberg L.P., the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.",5 Acknowledgments,[0],[0]
"FA8750-13-2-0040, and the NSF via award IIS-1514268.",5 Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of Bloomberg L.P., DARPA, AFRL, NSF, or the US government.",5 Acknowledgments,[0],[0]
"Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture.",abstractText,[0],[0]
However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate.,abstractText,[0],[0]
"In this paper, we benchmark recursive neural models against sequential recurrent neural models, enforcing applesto-apples comparison as much as possible.",abstractText,[0],[0]
We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answerphrases; (3) discourse parsing; (4) semantic relation extraction.,abstractText,[0],[0]
"Our goal is to understand better when, and why, recursive models can outperform simpler models.",abstractText,[0],[0]
"We find that recursive models help mainly on tasks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences.",abstractText,[0],[0]
We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining.,abstractText,[0],[0]
"Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models.",abstractText,[0],[0]
When Are Tree Structures Necessary for Deep Learning of Representations?,title,[0],[0]
"In the last two decades, statistical machine learning algorithms for processing massive datasets have been intensively studied for a wide-range of applications in computer vision, biology, chemistry and healthcare (Murdoch & Detsky, 2013; Tarca et al., 2007).",1. Introduction,[0],[0]
"While the challenges posed by large scale datasets are compelling, one is often faced with a fairly distinct set of technical issues for studies in biological and health sciences.",1. Introduction,[0],[0]
"For instance, a sizable portion
1University of Wisconsin-Madison 2William S. Middleton Memorial Veteran’s Affairs Hospital.",1. Introduction,[0],[0]
"Correspondence to: Hao Zhou <hzhou@stat.wisc.edu>, Vikas Singh <",1. Introduction,[0],[0]
"vsingh@biostat.wisc.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"of scientific research is carried out by small or mediumsized groups (Fortin & Currie, 2013) supported by modest budgets (Lauer, 2016).",1. Introduction,[0],[0]
"Hence, there are logistic/financial constraints on the number of experiments and/or number of participants within a trial, leading to small size datasets.",1. Introduction,[0],[0]
"While the analysis may be sufficiently powered to evaluate the primary hypothesis of the study/experiment, interesting follow-up scientific questions (often more nuanced), come up during the course of the project.",1. Introduction,[0],[0]
These tasks may be underpowered for the sample sizes available.,1. Introduction,[0],[0]
"This necessitates efforts to identify similar datasets elsewhere so that the combined sample size of the “pooled” dataset is enough to determine significant associations between a response and a set of predictors, e.g., within linear regression.
",1. Introduction,[0],[0]
Motivating Application.,1. Introduction,[0],[0]
"In genomics, funding agencies have invested effort into standardizing/curating data collection across large international projects (ENCODE Project Consortium, 2004).",1. Introduction,[0],[0]
"In other disciplines, such as in the study of neurological disorders, heterogeneity in disease etiology, variations in scanners and/or acquisition tools make standardization more difficult.",1. Introduction,[0],[0]
"For Alzheimer’s disease (AD), a motivation of this work, efforts such as ADNI (Weiner et al., 2015) provide a variety of clinical, imaging and cognitive tests data for 800+ older adults.",1. Introduction,[0],[0]
"However, the research focus has now moved to the early stages of disease – as early as late middle age – where treatments are expected to be more effective.",1. Introduction,[0],[0]
"But (a) the statistical signal at this stage is weak and difficult to demonstrate without large sample sizes and (b) such “preclinical” participants are not well represented, even in large ADNI sized studies.",1. Introduction,[0],[0]
"Hence, there is a concerted effort in general for smaller standalone projects (focused on a specific disease stage), that can be retrospectively pooled for analysis towards addressing a challenging scientific hypothesis (Jahanshad et al., 2013).",1. Introduction,[0],[0]
"Unfortunately, acquisition protocols for various measures across sites are usually different and data are heterogeneous.",1. Introduction,[0],[0]
These issues raise a fundamental technical question.,1. Introduction,[0],[0]
"When is it meaningful to pool datasets for estimating a simple statistical model (e.g., linear regression)?",1. Introduction,[0],[0]
"When can we guarantee improvements in statistical power, and when are such pooling efforts not worth it?",1. Introduction,[0],[0]
Can we give a hypothesis test and obtain p-values to inform our policies/decisions?,1. Introduction,[0],[0]
"While related problems have been stud-
ied in machine learning from an algorithm design perspective, even simple hypothesis tests which can be deployed by a researcher in practice, are currently unavailable.",1. Introduction,[0],[0]
"Our goal is to remove this significant limitation.
",1. Introduction,[0],[0]
Putting our development in context.,1. Introduction,[0],[0]
The realization that “similar” datasets from multiple sites can be pooled to potentially improve statistical power is not new.,1. Introduction,[0],[0]
"With varying empirical success, models tailored to perform regression in multi-site studies (Group, 2002), (Haase et al., 2009), (Klunk et al., 2015) have been proposed, where due to operational reasons, recruitment and data acquisition are distributed over multiple sites, or even countries.",1. Introduction,[0],[0]
"When the pooling is being performed retrospectively (i.e., after the data has been collected), resolving site-specific confounds, such as distributional shifts or biases in measurements, is essential before estimation/inference of a statistical model.",1. Introduction,[0],[0]
"We will not develop new algorithms for estimating the distributional mismatch or for performing multi-site regression — rather, our primary goal is to identify the regimes (and give easily computable checks) where this regression task on a pooled dataset is statistically meaningful, assuming that good pre-processing schemes are available.",1. Introduction,[0],[0]
"We will present a rigorous yet simple to implement hypothesis test, analyze its behavior, and show extensive experimental evidence (for an important scientific problem).",1. Introduction,[0],[0]
"The practitioner is free to use his/her preferred procedure for the “before step” (estimating the distributional shifts).
",1. Introduction,[0],[0]
Contributions: a),1. Introduction,[0],[0]
Our main result is a hypothesis test to evaluate whether pooling data across multiple sites for regression (before or after correcting for site-specific distributional shifts) can improve the estimation (mean squared error) of the relevant coefficients (while permitting an influence from a set of confounding variables).,1. Introduction,[0],[0]
b),1. Introduction,[0],[0]
We derive analogous results in the high-dimensional setting by leveraging a different set of analysis techniques.,1. Introduction,[0],[0]
"Using an existing sparse multi-task Lasso model, we show how the utility of pooling can be evaluated even when the support set of the features (predictors) is not exactly the same across sites using ideas broadly related to high dimensional simultaneous inference (Dezeure et al., 2015).",1. Introduction,[0],[0]
"We show `2-consistency rate, which supports the use of sparse multitask Lasso when sparsity patterns are not totally identical.",1. Introduction,[0],[0]
c),1. Introduction,[0],[0]
"On an important scientific problem of analyzing early Alzheimer’s disease (AD) individuals, we provide compelling experimental results showing consistent acceptance rate and statistical power.",1. Introduction,[0],[0]
"Via a publicly available software package, this will facilitate many multi-site regression analysis efforts in the short to medium term future.",1. Introduction,[0],[0]
Meta-analysis approaches.,1.1. Related Work,[0],[0]
"If datasets at multiple different sites cannot be shared or pooled, the task of deriving
meaningful scientific conclusions from results of multiple independently conducted analyses generally falls under the umbrella term of “meta analysis”.",1.1. Related Work,[0],[0]
The literature provides various strategies to cumulate the general findings from analyses on different datasets.,1.1. Related Work,[0],[0]
"But even experts believe that, minor violations of assumptions can lead to misleading scientific conclusions (Greco et al., 2013), and substantial personal judgment (and expertise) is needed to conduct them.",1.1. Related Work,[0],[0]
"It is widely accepted that when the ability to pool the data is an option, simpler schemes may perform better.
",1.1. Related Work,[0],[0]
Domain adaptation/shift.,1.1. Related Work,[0],[0]
"Separately, the idea of addressing “shift” within datasets has been rigorously studied within statistical machine learning, see (Patel et al., 2015; Li, 2012).",1.1. Related Work,[0],[0]
"For example, domain adaptation, including dataset and covariate shift, seeks to align (the distributions of) multiple datasets to enable follow-up processing (Ben-David & Schuller, 2003).",1.1. Related Work,[0],[0]
"Typically, such algorithms assume a bias in the sampling process, and adopt reweighting as the solution (Huang et al., 2007; Gong et al., 2013).",1.1. Related Work,[0],[0]
"Alternatively, a family of such methods assume that sites (or datasets) differ due to feature distortions (e.g., calibration error), which are resolved, in general, by minimizing some distance measure between appropriate distributions (Baktashmotlagh et al., 2013; Pan et al., 2011; Long et al., 2015; Ganin et al., 2016).",1.1. Related Work,[0],[0]
"In general, these approaches have nice theoretical properties (Ben-David et al., 2010; Cortes & Mohri, 2011; Zhou et al., 2016).",1.1. Related Work,[0],[0]
"However, it is important to note that the domain adaptation literature focuses on the algorithm itself – to resolve the distributional site-wise differences.",1.1. Related Work,[0],[0]
"It does not address the issue of whether pooling the datasets, after applying the calculated adaptation (i.e., transformation), is beneficial.",1.1. Related Work,[0],[0]
Our goal in this work is to assess whether multiple datasets can be pooled — either before or usually after applying the best domain adaptation methods — for improving our estimation of the relevant coefficients within linear regression.,1.1. Related Work,[0],[0]
"We propose a hypothesis test to directly address this question.
",1.1. Related Work,[0],[0]
The high-dimensional case.,1.1. Related Work,[0],[0]
"High dimensional scenarios, in general, involve predicting a response (e.g., cognitive score) from high dimensional predictors such as image scans (or derived features) and genetic data, which in general, entails Lasso-type formulations unlike the classical regression models.",1.1. Related Work,[0],[0]
"Putting multi-task representation learning (Maurer et al., 2016; Ando & Zhang, 2005; Maurer et al., 2013) together with a sparsity regularizer, we get the multitask Lasso model (Liu et al., 2009; Kim & Xing, 2010).",1.1. Related Work,[0],[0]
"Although this seems like a suitable model (Chen et al., 2012), it assumes that the multiple tasks (sites here) have an identical active set of predictors.",1.1. Related Work,[0],[0]
"Instead, we find that the sparse multi-task Lasso (Lee et al., 2010), roughly, a multitask version of sparse group Lasso (Simon et al., 2013; Lee et al., 2010) is a better starting point.",1.1. Related Work,[0],[0]
"There is no theoretical analysis in (Simon et al., 2013); although a `2-consistency
for sparse group lasso is derived in (Chatterjee et al., 2012) using a general proof procedure for M-estimators, it does not take into account the specific sparse group Lasso properties.",1.1. Related Work,[0],[0]
"This makes the result non-informative for sparse group Lasso (much less, sparse multi-task Lasso).",1.1. Related Work,[0],[0]
"Specifically, as we will see shortly, in sparse multi-task Lasso, the joint effects of two penalties induces a special type of asymmetric structure.",1.1. Related Work,[0],[0]
"We show a new result, in the style of Lasso (Meinshausen & Yu, 2009; Liu & Zhang, 2009b), for `2 convergence rate for this model.",1.1. Related Work,[0],[0]
"It matches known results for Lasso/group Lasso, and identifies regimes where the sparse multi-task (multi-site) setting is advantageous.
",1.1. Related Work,[0],[0]
Simultaneous High dimensional Inference.,1.1. Related Work,[0],[0]
"Simultaneous high dimensional inference models such as multi sample-splitting and de-biased Lasso is an active research topic in statistics (Dezeure et al., 2015).",1.1. Related Work,[0],[0]
Multi samplesplitting use half of the dataset for variable selection and the rest for calculating p-values.,1.1. Related Work,[0],[0]
De-biased Lasso chooses one feature as a response and the others as predictors to estimate a Lasso model; this procedure is repeated for each feature.,1.1. Related Work,[0],[0]
"Estimators from De-biased Lasso asymptotically follow the multi-normal distribution (Dezeure et al., 2016), and using Bonferroni-Holm adjustment produces simultaneous p-values.",1.1. Related Work,[0],[0]
"Such ideas together with the `2- convergence results for sparse multitask Lasso, will help extend our analysis to the high dimensional setting.",1.1. Related Work,[0],[0]
We first describe a simple setting where one seeks to apply standard linear regression to data pooled from multiple sites.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"For presentation purposes, we will deal with variable selection issues later.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Within this setup, we will introduce our main result — a hypothesis test to evaluate statistical power improvements (e.g., mean squared error) when running a regression model on a pooled dataset.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"We will see that the proposed test is transparent to the use of adaptation algorithms, if any, to pre-process the multi-site data (more details in appendix).",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"In later sections, we will present convergence analysis and extensions to the large p setting.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
Matrices (vectors/scalars) are upper case (and lower case).,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"‖.‖∗ is the nuclear norm.
",2. Hypothesis Test for Multi-Site Regression,[0],[0]
We first introduce the single-site regression model.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
Let X ∈ Rn×p and y ∈ Rn×1 denote the feature matrix of predictors and the response vector respectively.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"If β corresponds to the coefficient vector (i.e., predictor weights), then the regression model is
min β
1 n ‖y",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"−Xβ‖22 (1)
where y = Xβ∗ +",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"and ∼ N(0, σ2) I.I.D. if β∗ is the true coefficient vector from which y is generated.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"The mean-squared error (MSE) and `2-consistency of regres-
sion is well-known.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
The mean-squared error (MSE) of (1) is E‖β̂ − β∗‖22 = tr ( (XTX)−1 ) σ2.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"If k denotes the number of sites, then one may first apply a domain adaptation scheme to account for the distributional shifts between the k different predictors {Xi}ki=1, and then run a regression model.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"If the underlying “concept” (i.e., predictors and responses relationship) can be assumed to be the same across the different sites, then it is reasonable to impose the same β for all sites.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"For instance, the influence of CSF protein measurements on cognitive scores of an individual may be invariant to demographics.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Nonetheless, if the distributional mismatch correction is imperfect, we may define ∆βi = βi − β∗ where i ∈ {1, . . .",2. Hypothesis Test for Multi-Site Regression,[0],[0]
", k} as the residual difference between the site-specific coefficients and the true shared coefficient vector (in the ideal case, we have ∆βi = 0).",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"In the multi-site setting, we can write
min β k∑ i=1",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"τ2i ‖yi −Xiβ‖22 (2)
where for each site i we have yi = Xiβ∗+Xi∆βi+ i",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"and i ∼ N (0, σ2i )",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"I.I.D. Here, τi is a weighting parameter for each site, if such information is available.
",2. Hypothesis Test for Multi-Site Regression,[0],[0]
Our main goal is to test if the combined regression improves the estimation for a single site.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
We can pose this question in terms of improvements in the mean squared error (MSE).,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Hence, W.L.O.G. using site 1 as the reference, we set τ1 = 1 in (2) and consider β∗ = β1,
min β ‖y1 −X1β‖22 + k∑ i=2",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"τ2i ‖yi −Xiβ‖22 (3)
β1 β2
β̂
Figure 1.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
β1 and β2 are 1st and 2nd site coefficients.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"After combination, β1’s bias increases but variance reduces, resulting in a smaller MSE.
",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Clearly, when the sample size is not large enough, the multi-site formulation in (3) may reduce variance significantly, because of the averaging effect in the objective function, while increasing the bias by a little.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"This reduces the Mean Squared Error (MSE), see Figure 1.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Note that while traditionally, the unbiasedness property was desirable, an extensive body of literature on ridge regression suggests that the quantity of interest should really be E ‖β̂",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"−
β∗‖22 (Hoerl & Kennard, 1970; James & Stein, 1961).",2. Hypothesis Test for Multi-Site Regression,[0],[0]
These ideas are nicely studied within papers devoted to the “bias-variance” trade-off.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Similar to these results, we will focus on the mean squared error because the asymptotic consistency properties that come with an unbiased estimator are not meaningful here anyway — the key reason we want to pool datasets in the first place is because of small sample sizes.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"We now provide a result showing how the tuning parameters τ2, . . .",2. Hypothesis Test for Multi-Site Regression,[0],[0]
", τk can be chosen.
",2. Hypothesis Test for Multi-Site Regression,[0],[0]
"Theorem 2.1 τi = σ1σi achieves the smallest variance in β̂.
Remarks: This result follows from observing that the each site’s contribution is inversely proportional to site-specific noise level, σi.",2. Hypothesis Test for Multi-Site Regression,[0],[0]
We will show that this choice of τis also leads to a simple mechanism to setup a hypothesis test.,2. Hypothesis Test for Multi-Site Regression,[0],[0]
"In the specification above, the estimates of βi across all k sites are restricted to be the same.",2.1. Sharing all βs,[0],[0]
"Without this constraint, (3) is equivalent to fitting a regression separately on each site.",2.1. Sharing all βs,[0],[0]
"So, a natural question is whether this constraint improves estimation.",2.1. Sharing all βs,[0],[0]
"To evaluate whether MSE is reduced, we first need to quantify the change in the bias and variance of (3) compared to (1).",2.1. Sharing all βs,[0],[0]
"To do so, we introduce a few notations.",2.1. Sharing all βs,[0],[0]
"Let ni be the sample size of site i, and let β̂i denote the regression estimate from a specific site i. We have ∆β̂i = β̂i",2.1. Sharing all βs,[0],[0]
− β̂1.,2.1. Sharing all βs,[0],[0]
"We define the length kp vector ∆βT as ∆βT = (∆βT2 , ...,∆β T k ) (similarly for ∆β̂
T ).",2.1. Sharing all βs,[0],[0]
"We use Σ̂i for the sample covariance matrix of the data (predictors) from the site i and G ∈ R(k−1)p×(k−1)p for the covariance matrix of ∆β̂, where Gii = (n1Σ̂1)−1 + (niτ2i Σ̂i)
−1 and Gij = (n1Σ̂1) −1",2.1. Sharing all βs,[0],[0]
whenever i 6=,2.1. Sharing all βs,[0],[0]
"j.
Let the difference in bias and variance between the single site model in (1) and the multi-site model in (3) be Biasβ and V arβ respectively.",2.1. Sharing all βs,[0],[0]
Let Σ̂k2 = ∑k i=2,2.1. Sharing all βs,[0],[0]
niτ 2,2.1. Sharing all βs,[0],[0]
i Σ̂i and Σ̂ k 1 = n1Σ̂1 +,2.1. Sharing all βs,[0],[0]
Σ̂,2.1. Sharing all βs,[0],[0]
k 2 .,2.1. Sharing all βs,[0],[0]
"We have,
Lemma 2.2 For model (3), we have
‖Biasβ‖22 ‖G−1/2∆β‖22 ≤ ‖(Σ̂k1)−2(Σ̂k2(n1Σ̂1)−1Σ̂k2",2.1. Sharing all βs,[0],[0]
"+ Σ̂k2)‖∗, (4)
V arβ = σ 2 1 ∥∥∥(n1Σ̂1)−1",2.1. Sharing all βs,[0],[0]
− (n1Σ̂1 + Σ̂k2)−1∥∥∥ ∗ .,2.1. Sharing all βs,[0],[0]
"(5)
Remarks: The above result bounds the increase in bias and the reduction in variance (see discussion of Figure 1).",2.1. Sharing all βs,[0],[0]
"Since our goal is to test MSE reduction — in principle, we can use bootstrapping to calculate MSE approximately.",2.1. Sharing all βs,[0],[0]
This procedure has a significant computational footprint.,2.1. Sharing all βs,[0],[0]
"Instead, (4) (from a one-step Cauchy-Schwartz inequality), gives a sufficient condition for MSE reduction as shown below.
",2.1. Sharing all βs,[0],[0]
"Theorem 2.3 a) Model (3) has smaller MSE of β̂ than model (1) whenever
H0 : ‖G−1/2∆β‖22 ≤ σ21 .",2.1. Sharing all βs,[0],[0]
"(6)
b)",2.1. Sharing all βs,[0],[0]
"Further, we have the following test statistic,∥∥∥∥∥G−1/2∆β̂σ1 ∥∥∥∥∥ 2
2
∼ χ2(k−1)∗p (∥∥∥∥G−1/2∆βσ1 ∥∥∥∥2 2 ) , (7)
where ‖G−1/2∆β/σ1‖2 is called a “condition value”.
",2.1. Sharing all βs,[0],[0]
Remarks:,2.1. Sharing all βs,[0],[0]
This is our main test result.,2.1. Sharing all βs,[0],[0]
"Although σi is typically unknown, it can be easily replaced using its sitespecific estimation.",2.1. Sharing all βs,[0],[0]
Theorem 2.3 implies that we can conduct a non-central χ2 distribution test based on the statistic.,2.1. Sharing all βs,[0],[0]
"Also, (6) shows that the non-central χ2 distribution, which the test statistics will follow, has a non-central parameter smaller than 1 when the sufficient condition H0 holds.",2.1. Sharing all βs,[0],[0]
"Meanwhile, in obtaining the (surprisingly simple) sufficient condition H0, no other arbitrary assumption is needed except the application of Cauchy-Schwartz inequality.",2.1. Sharing all βs,[0],[0]
"From a practical perspective, Theorem 2.3 implies that the sites, in fact, do not even need to share the full dataset to assess whether pooling will be useful.",2.1. Sharing all βs,[0],[0]
"Instead, the test only requires very high-level information such as β̂i, Σ̂i, σi and ni for all participating sites – which can be transferred very cheaply with no additional cost of data storage, or privacy implications.",2.1. Sharing all βs,[0],[0]
"The following result deals with the special case where we have two participating sites.
",2.1. Sharing all βs,[0],[0]
"Corollary 2.4 For the case where we have two participating sites, the condition (6) from Theorem 2.3 reduces to
H0 : ∆β T ((n1Σ̂1) −1",2.1. Sharing all βs,[0],[0]
+ (n2τ 2 2 Σ̂2) −1)−1∆β ≤ σ21 .,2.1. Sharing all βs,[0],[0]
"(8)
Remarks: The left side above relates to the Mahalanobis distance between β1, β2 with covariance (n1Σ̂1)−1 + (n2τ 2 2 Σ̂2)
−1, implying that the test statistic is a type of a normalized metric between the two regression models.",2.1. Sharing all βs,[0],[0]
"In numerous pooling scenarios, we are faced with certain systemic differences in the way predictors and responses associate across sites.",2.2. Sharing a subset of βs,[0],[0]
"For example, socio-economic status may (or may not) have a significant association with a health outcome (response) depending on the country of the study (e.g., due to insurance coverage policies).",2.2. Sharing a subset of βs,[0],[0]
"Unlike in Section 2.1, we now relax the restriction that all coefficients are the same across sites, see Fig. 2.",2.2. Sharing a subset of βs,[0],[0]
"The model in (3) will now include another design matrix of predictors Z ∈ Rn×q and corresponding coefficients γi for each site i,
min β,γ k∑ i=1",2.2. Sharing a subset of βs,[0],[0]
"τ2i ‖yi −Xiβ − Ziγi‖22 (9)
yi = Xiβ ∗",2.2. Sharing a subset of βs,[0],[0]
+Xi∆βi +,2.2. Sharing a subset of βs,[0],[0]
"Ziγ ∗ i + i, τ1 = 1 (10)
",2.2. Sharing a subset of βs,[0],[0]
Our goal is still to evaluate whether the MSE of β reduces.,2.2. Sharing a subset of βs,[0],[0]
We do not take into account the MSE change in γ because they correspond to site-specific variables.,2.2. Sharing a subset of βs,[0],[0]
"For estimation, β̂ can first be computed from (9).",2.2. Sharing a subset of βs,[0],[0]
"Treating it as a fixed entity now, γ̂i can be computed using yi and Zi on each site independently.",2.2. Sharing a subset of βs,[0],[0]
"Clearly, if β̂ is close to the “true” β∗, it will also enable a better estimation of site-specific variables.",2.2. Sharing a subset of βs,[0],[0]
"It turns out that, if Σ̂is are replaced by the conditional covariance, the analysis from Section 2.1 still holds for this case.
",2.2. Sharing a subset of βs,[0],[0]
"Specifically, let Σ̂abi be the sample covariance matrix between features a and b from some site i.",2.2. Sharing a subset of βs,[0],[0]
"We have,
Theorem 2.5 Analysis in Section 2.1 holds for β in (9) by replacing Σ̂i with Σ̃i = Σ̂xxi − Σ̂xzi(Σ̂zzi)−1Σ̂zxi
Remarks: The test now allows evaluating power improvements focused only on the subset of coefficients that is shared and permits site-specific confounds.",2.2. Sharing a subset of βs,[0],[0]
"For example, we can test which subset of parameters might benefit from parameter estimation on pooled data from multiple sites.",2.2. Sharing a subset of βs,[0],[0]
We now describe our analysis of pooling multi-site data in the high-dimensional setting where p n.,3. Pooling in High Dimensional Regression,[0],[0]
The challenge here is that variable section has to be a first order concern.,3. Pooling in High Dimensional Regression,[0],[0]
"In classical regression, `2 consistency properties are well known and so our focus in Section 2 was devoted to deriving sufficient conditions for the hypothesis test.",3. Pooling in High Dimensional Regression,[0],[0]
"In other words, imposing the same β across sites works in (3) because we understand its consistency.",3. Pooling in High Dimensional Regression,[0],[0]
"In contrast, here, one cannot enforce a shared β for all sites before the active set of predictors within each site are selected — directly imposing the same β leads to a loss of `2-consistency, making follow-up analysis problematic.",3. Pooling in High Dimensional Regression,[0],[0]
"Therefore, once a suitable model for high-dimensional multi-site regression is chosen, the first requirement is to characterize its consistency.
",3. Pooling in High Dimensional Regression,[0],[0]
"We start with the multi-task Lasso (a special case of group Lasso) (Liu et al., 2009), where the authors show that the strategy selects better explanatory features compared to separately fitting Lasso on each site.",3. Pooling in High Dimensional Regression,[0],[0]
"But this algorithm underperforms when the sparsity pattern of the predictors is not identical across sites, so we use a recent variant called sparse multi-task Lasso (Lee et al., 2010) – essentially substituting “sites” for “tasks”.",3. Pooling in High Dimensional Regression,[0],[0]
"The sparse multi-site Lasso in p n setting (p is the number of predictors) is given as
B̂λ = arg min β k∑ i=1",3. Pooling in High Dimensional Regression,[0],[0]
‖yi,3. Pooling in High Dimensional Regression,[0],[0]
"−Xiβi‖22 + λΛ(B) (11)
Λ(B)",3. Pooling in High Dimensional Regression,[0],[0]
= α p∑ j=1,3. Pooling in High Dimensional Regression,[0],[0]
"‖βj‖1 + (1− α) √ k p∑ j=1 ‖βj‖2, (12)
where λ is the Lasso regularization parameter.",3. Pooling in High Dimensional Regression,[0],[0]
"Here, B ∈
Rk×p is a matrix where the ith row gives the coefficients from ith site (k sites in total).",3. Pooling in High Dimensional Regression,[0],[0]
"Also, βi with subscript denotes the ith row (site) of B, we use βj with superscript to give the j-th column (coefficients) of B. The hyperparameter α ∈",3. Pooling in High Dimensional Regression,[0],[0]
"[0, 1] balances the two penalties (and will be used shortly); a larger α weighs the `1 penalty more and a smaller α puts more weight on the grouping.",3. Pooling in High Dimensional Regression,[0],[0]
"Similar to a Lasso-based regularization parameter, λ here will produce a solution path (to select coefficients) for a given α.",3. Pooling in High Dimensional Regression,[0],[0]
"We first address the consistency behavior of the sparse multisite Lasso in (11), which was not known in the literature.",3. Pooling in High Dimensional Regression,[0],[0]
"Our analysis of (11) is related to known results for Lasso (Meinshausen & Yu, 2009) and the group Lasso (Liu & Zhang, 2009a).",3.1. `2 consistency,[0],[0]
"Recall that X1, . . .",3.1. `2 consistency,[0],[0]
", Xk are the data matrices from k sites.",3.1. `2 consistency,[0],[0]
"We define n̄ = maxki=1{ni} and C = n̄−1DIAG(XT1 X1, ..., X T k Xk) where DIAG(A,B) corresponds to constructing a block-diagonal matrix with A and B as blocks on the diagonal.",3.1. `2 consistency,[0],[0]
"We require the following useful properties of C (‖·‖0 denotes `0-norm).
",3.1. `2 consistency,[0],[0]
"Definition 3.1 The m-sparse minimal and maximal eigenvalues of C, denoted by φmin(m) and φmax(m), are
min ν:‖ν‖0≤dme
νTCν
νT ν and max ν:‖ν‖0≤dme
νTCν
νT ν",3.1. `2 consistency,[0],[0]
"(13)
We call a feature “active” if its coefficient is non-zero.",3.1. `2 consistency,[0],[0]
"Now, each site may have different active features: let sh ≤ kp be the sum of the number of active features over all sites.",3.1. `2 consistency,[0],[0]
"Similarly, sp is the cardinality of the union of features that are active in at least one site (sh ≤ ksp, sp ≤ p).",3.1. `2 consistency,[0],[0]
"Recall that when α 6= 0, we add the Lasso penalty to the multi-site Lasso penalty.",3.1. `2 consistency,[0],[0]
"When the sparsity patterns are assumed to be similar across all sites, α is small.",3.1. `2 consistency,[0],[0]
"In contrast, to encourage site-specific sparsity patterns, we may set α to be large.",3.1. `2 consistency,[0],[0]
"We now analyze these cases independently.
",3.1. `2 consistency,[0],[0]
Theorem 3.2 Let 0 ≤ α ≤ 0.4.,3.1. `2 consistency,[0],[0]
Assume there exist constants 0,3.1. `2 consistency,[0],[0]
≤ ρmin ≤,3.1. `2 consistency,[0],[0]
"ρmax ≤ ∞ such that
lim inf n→∞ φmin
( sp ( 1 + 2α
1− 2α
)2)",3.1. `2 consistency,[0],[0]
"≥ ρmin
lim sup n→∞",3.1. `2 consistency,[0],[0]
φmax(sp + min{ k∑ i=1,3.1. `2 consistency,[0],[0]
"ni, kp}) ≤",3.1. `2 consistency,[0],[0]
"ρmax.
(14)
",3.1. `2 consistency,[0],[0]
"Then, for λ ∝ σ √ n̄ log(kp), there exists a constant ω > 0
such that, with probability converging to 1 for n→∞,
1 k ‖B̂λ −B∗‖2F ≤ ωσ2 s̄ log(kp) n̄ , (15)
where s̄ = {(1−α)√sp+α √ sh/k}2, σ is the noise level.
",3.1. `2 consistency,[0],[0]
"Remarks: The above result agrees with known results for multi-task Lasso (Liu et al., 2009; Liu & Zhang, 2009b) when the sparsity patterns are the same across sites.",3.1. `2 consistency,[0],[0]
"The simplest way to interpret Theorem 3.2 is via the ratio r = sh sp
.",3.1. `2 consistency,[0],[0]
"Here, r = k when the sparsity patterns are the same across sites.",3.1. `2 consistency,[0],[0]
"As r decreases, the sparsity patterns across sites start to differ, in turn, the sparse multi-site Lasso from (11) will provide stronger consistency compared to the multi-site Lasso (which corresponds to α = 0).",3.1. `2 consistency,[0],[0]
"In other words, whenever we expect site-specific active features, the `2 consistency of (11) will improve as one includes an additional `1-penalty together with multi-site Lasso.
",3.1. `2 consistency,[0],[0]
"Observe that for the non-sparse βj , we can verify that ‖βj‖1 and √ k‖βj‖2 have the same scale.",3.1. `2 consistency,[0],[0]
"On the other hand, for sparse βj , ‖βj‖1 has the same scale as ‖βj‖2, i.e., with no √ k penalization (see appendix).",3.1. `2 consistency,[0],[0]
"Unlike Theorem 3.2 where the sparsity patterns across sites are similar, due to this scaling issue, the parameters α and λ need to be ‘corrected’ for the setting where sparsity patterns have little overlap.",3.1. `2 consistency,[0],[0]
"We denote these corrected versions by α̃ = α
(1−α) √ k+α
and λ̃ = ((1− α) √ k + α)λ.
",3.1. `2 consistency,[0],[0]
Theorem 3.3 Let 0.4 ≤ α̃ ≤ 1.,3.1. `2 consistency,[0],[0]
Assume there exist constants 0,3.1. `2 consistency,[0],[0]
≤ ρmin ≤,3.1. `2 consistency,[0],[0]
"ρmax ≤ ∞ such that
lim inf",3.1. `2 consistency,[0],[0]
"n→∞ φmin
( sh ( 1 +
(1− α̃) α̃
)2) ≥ ρmin
lim sup n→∞ φmax(sh + min{ k∑ i=1 ni, kp}) ≤",3.1. `2 consistency,[0],[0]
"ρmax.
(16)
",3.1. `2 consistency,[0],[0]
"Then, for λ̃ ∝ σ",3.1. `2 consistency,[0],[0]
"√ n̄ log(kp), there exists ω > 0",3.1. `2 consistency,[0],[0]
"such that,
with probability converging to 1 for n → ∞, we have (15) with s̃ = {(1− α̃) √ sp/k + α̃ √ sh/k}2 instead of s̄.
Remarks: This result agrees with known results for Lasso (Meinshausen & Yu, 2009) when the sparsity patterns are completely different across sites.",3.1. `2 consistency,[0],[0]
"In this case (i.e., α is large), the sparse multi-site Lasso has stronger consistency compared to Lasso (α = 1).",3.1. `2 consistency,[0],[0]
The sparse multi-site Lasso is preferable as r = shsp increases.,3.1. `2 consistency,[0],[0]
"Note that although α̃ and λ̃ are used for the results instead of α and λ, in practice, one can simply scale the chosen αs appropriately, e.g., with k = 100, we see that α ≈ 0.99 corresponds to α̃ = 0.95.
",3.1. `2 consistency,[0],[0]
Performing hypothesis tests: Theorems 3.2 and 3.3 show consistency of sparse multi-site Lasso estimation.,3.1. `2 consistency,[0],[0]
"Hence, if the hyper-parameters α and λ are known, we can estimate the coefficients B∗. This variable selection phase can be followed by a hypothesis test, similar to Theorem 2.3 from Section 2.",3.1. `2 consistency,[0],[0]
The only remaining issue is the choice of α.,3.1. `2 consistency,[0],[0]
"The existing methods show that joint crossvalidation for α and λ performs unsatisfactorily and instead use a heuristic: set it to 0.05 when it is known that sparsity
patterns are similar across sites and 0.95 otherwise (Simon et al., 2013).",3.1. `2 consistency,[0],[0]
"Below, instead of a fixed α, we provide a data-driven alternative that works well in practice.
",3.1. `2 consistency,[0],[0]
Choosing α using simultaneous inference: Our results in Thm. 3.2 (and Thm. 3.3 resp.) seem to suggest that increasing (and decreasing resp.),3.1. `2 consistency,[0],[0]
"α will always improve consistency; however, this ends up requiring stronger msparsity conditions.",3.1. `2 consistency,[0],[0]
We now describe a procedure to choose α.,3.1. `2 consistency,[0],[0]
"First, recall that an active feature corresponds to a variable with non-zero coefficient.",3.1. `2 consistency,[0],[0]
"We call a feature “siteactive” if it is active at a site, an “always-active” feature is active at all k sites.",3.1. `2 consistency,[0],[0]
The proposed solution involves three steps.,3.1. `2 consistency,[0],[0]
"(1) First, we apply simultaneous inference (like multi sample-splitting or de-biased Lasso) using all features at each of the k sites with FWER control.",3.1. `2 consistency,[0],[0]
"This step yields “site-active” features for each site, and therefore, gives the set of always-active features and the sparsity patterns.",3.1. `2 consistency,[0],[0]
"(2) Then, each site runs a Lasso and chooses a λi based on cross-validation.",3.1. `2 consistency,[0],[0]
We then set λmulti−site to be the minimum among the best λ’s from each site.,3.1. `2 consistency,[0],[0]
"Using λmulti−site, we can vary α to fit various sparse multi-site Lasso models – each run will select some number of always-active features.",3.1. `2 consistency,[0],[0]
We plot α versus the number of always-active features.,3.1. `2 consistency,[0],[0]
"(3) Finally, based on the sparsity patterns from the site-active set, we estimate whether the sparsity patterns across sites are similar or different (i.e., share few active features).",3.1. `2 consistency,[0],[0]
"Then, based on the plot from step (2), if the sparsity patterns from the site-active sets are different (similar) across sites, then the smallest (largest) value of α that selects the minimum (maximum) number of always-active features is chosen.",3.1. `2 consistency,[0],[0]
The appendix includes details.,3.1. `2 consistency,[0],[0]
Our experiments are two-fold.,4. Experiments,[0],[0]
First we perform simulations evaluating the hypothesis test from §2 and sparse multi-site Lasso from §3.,4. Experiments,[0],[0]
"We then evaluate pooling two Alzheimer’s disease (AD) datasets from different studies to evaluate improvements in power, and checking whether the proposed tests provide insights into the regimes when pooling is beneficial for regression, and will yield tangible statistical benefits in investigating scientific hypotheses.
",4. Experiments,[0],[0]
Power and Type I Error of Theorem 2.3:,4. Experiments,[0],[0]
The first set of simulations evaluate the setting from Section 2.1 where the coefficients are same across two different sites.,4. Experiments,[0],[0]
"The inputs for the two sites are set as X1, X2(∈ Rn×3) ∼ N (0,Σ) with Σ = 0.5(I + E) (where I is identity and E is a 3 × 3 matrix of 1s).",4. Experiments,[0],[0]
"The true coefficients are given by β1 ∼ U(0, 4I) and β2 = β1+0.1 (where U(·) is multivariate uniform), and the noise corresponds to 1 ∼ N (0, 3I) and 2 ∼ N (0, 0.5I) for the two sites respectively.",4. Experiments,[0],[0]
"With this design, the responses are set as y1 = X1β1 + 1 and y2 = X2β2 + 2.",4. Experiments,[0],[0]
"Using {X1, y1} and {X2, y2}, the shared
β̂ are estimated.",4. Experiments,[0],[0]
"The simulation is repeated 100 times with 9 different sample sizes (n = 2b with b = 4, . . .",4. Experiments,[0],[0]
", 12) for each repetition.",4. Experiments,[0],[0]
Fig. 3(a) shows the MSE of two-site (blue bars) and a baseline single-site (red bars) model computed using the corresponding β̂s on site 1.,4. Experiments,[0],[0]
"Although both MSEs decrease as n increases, the two-sites model consistently produces smaller MSE – with large gains for small sample sizes (left-end of Fig. 3(a)).",4. Experiments,[0],[0]
Fig. 3(d) shows the acceptance rates of our proposed hypothesis test (from (6) and (8)) with 0.05 significance level.,4. Experiments,[0],[0]
"The purple solid line is the sufficient condition from Theorem 2.3, while the dotted line is where the MSE of the baseline single-site model starts to decrease below that of the two-site model.",4. Experiments,[0],[0]
"The trend in Fig. 3(d) implies that as n increases, the test tends to reject pooling the multi-site data with power→ 1.",4. Experiments,[0],[0]
"Further, the type I error is well-controlled to the left of the solid line, and is low between the two lines.",4. Experiments,[0],[0]
"See appendix for additional details about Figs. 3(a,d).
",4. Experiments,[0],[0]
Power and Type I Error of Theorem 2.5:,4. Experiments,[0],[0]
The second set of simulations evaluate the confounding variables setup from Section 2.2.,4. Experiments,[0],[0]
"Similar to Section 4, here we have (X1, Z1), (X2, Z2) ∼ N (0,Σ) with Σ =(
0.5I3×3 + 0.5E3×3, 0.2E3×5 0.2E5×3, 0.8I5×5 + 0.2E5×5
) .",4. Experiments,[0],[0]
"β1 and
β2 are the same as before.",4. Experiments,[0],[0]
"γ1 = (1, 1, 2, 2, 2)T and γ2 = (2, 2, 2, 1, 1)
T are the coefficients for Z1 and Z2 respectively.",4. Experiments,[0],[0]
The new responses y1 and y2 will have the extra terms Z1γ1 and Z2γ2 respectively.,4. Experiments,[0],[0]
"Fig. 3(b,e) shows the results.",4. Experiments,[0],[0]
"All the observations from Fig. 3(a,d) hold here as well.",4. Experiments,[0],[0]
"For small n, MSE of two-site model is much smaller than baseline, and as sample size increases this difference reduces.",4. Experiments,[0],[0]
"The test accepts with high probability for small n, and as sample size increases it rejects with high power.",4. Experiments,[0],[0]
"The regimes of low type I error and high power in Fig. 3(e) are similar to those from Fig. 3(d).
4.1.",4. Experiments,[0],[0]
"Sparse multi-sites Lasso `2-consistency
We now use 4 sites with n = 150 samples each and p = 400 features to test the sparse multi-site model from §3.",4. Experiments,[0],[0]
"We set the design matrices Xi (i = 1, . . .",4. Experiments,[0],[0]
", 4) ∼ N (0,Σ) with Σ = 0.8Ip×p + 0.2Ep×p.",4. Experiments,[0],[0]
"We consider the two cases (sparsity patterns shared/not shared) separately.
",4. Experiments,[0],[0]
Few sparsity patterns shared: 6 shared features and 14 site-specific features (out of the 400) are set to be active in 4 sites.,4. Experiments,[0],[0]
"Each shared feature is sampled from U(0, 4) for the first two sites and U(0, 0.5) for the rest.",4. Experiments,[0],[0]
"All the sitespecific features are ∼ U(0, 4).",4. Experiments,[0],[0]
"The noise i ∼ N (0, 1), and the responses are yi = Xiβi + i. Fig. 3(c) shows the 10-fold cross validation error as λ changes (i.e., solution path) for different α settings, including the value from our proposed selection procedure (from Section 3.1), Lasso (α = 1), group Lasso (α = 0) and arbitrary values α = 0.05, 0.95 (as suggested by (Simon et al., 2013)).
",4. Experiments,[0],[0]
"Our chosen α = 0.97 (the blue curve in Fig. 3(c)) has the smallest error, across all λs, thereby implying a better `2 consistency.",4. Experiments,[0],[0]
"Table 1 in the appendix includes more details, including α = 0.97 discovers more always-active features, while preserving the ratio of correctly discovered active features to all the discovered ones.
",4. Experiments,[0],[0]
"Most sparsity patterns shared: Unlike the earlier case, here we set 16 shared and 4 site-specific features (both ∼ U(0, 4)) to be active among all 400 features.",4. Experiments,[0],[0]
"The result, shown in Fig. 3(f), is similar to Fig. 3(c).",4. Experiments,[0],[0]
The proposed choice of α = 0.25 competes favorably with alternate choices while preserving the correctly discovered number of always-active features.,4. Experiments,[0],[0]
"Unlike the previous case, the ratio of correctly discovered active features to all discovered ones increases here (see appendix).",4. Experiments,[0],[0]
We now evaluate whether two AD datasets acquired at different sites – an Alzheimer’s Disease Neuroimage Initiative (ADNI) dataset and a local dataset from Wisconsin,4.2. Combining AD datasets from multiple sites,[0],[0]
ADRC (ADlocal) can be combined (appendix has dataset details).,4.2. Combining AD datasets from multiple sites,[0],[0]
The sample sizes are 318 and 156 respectively.,4.2. Combining AD datasets from multiple sites,[0],[0]
"Cerebrospinal fluid (CSF) protein levels are the inputs, and the response is hippocampus volume.",4.2. Combining AD datasets from multiple sites,[0],[0]
"Using 81 age-matched samples from each dataset, we first perform domain adaptation (using a maximum mean discrepancy objective as a measure of distance between the two marginals), and then transform CSF proteins from ADlocal to match with ADNI.",4.2. Combining AD datasets from multiple sites,[0],[0]
The transformed data is then used to evaluate whether adding ADlocal data to ADNI will improve the regression performed on the ADNI data.,4.2. Combining AD datasets from multiple sites,[0],[0]
"This is done by training a regression model on the ‘transformed’ ADlocal and a subset of ADNI data, and then testing the resulting model on the remaining ADNI samples.",4.2. Combining AD datasets from multiple sites,[0],[0]
"We use two baseline models each of which are trained using – ADNI data alone; and non-transformed ADlocal (with ADNI subset).
",4.2. Combining AD datasets from multiple sites,[0],[0]
"Fig. 4(a,b) show the resulting mean prediction error (MPE) scaled by the estimated noise level in ADNI responses, and the corresponding acceptance rate (with significance level 0.05) respectively.",4.2. Combining AD datasets from multiple sites,[0],[0]
"The x-axis in Fig. 4(a,b) represents the size of ADNI subset used for training.",4.2. Combining AD datasets from multiple sites,[0],[0]
"As expected, the MPE reduces as this subset size increases.",4.2. Combining AD datasets from multiple sites,[0],[0]
"Most importantly, pooling after transformation (green bars) seems to be the most beneficial in terms of MPE reduction.",4.2. Combining AD datasets from multiple sites,[0],[0]
"As shown in Fig. 4(a), to the left of purple line where the subset size is smaller than ADlocal datasize, pooling the datasets improves estimation.",4.2. Combining AD datasets from multiple sites,[0],[0]
This is the small sample size regime which necessitates pooling efforts in general.,4.2. Combining AD datasets from multiple sites,[0],[0]
"As the dataset size increases (to the right of x-axis in Fig. 4(a)) the resulting MPE for the pooled model is close to what we will achieve using the ADNI data by itself.
",4.2. Combining AD datasets from multiple sites,[0],[0]
"Since pooling after transformation is at least as good as us-
ing ADNI data alone, our hypothesis test accepts the combination with high rate (≈ 95%), see Fig. 4(b).",4.2. Combining AD datasets from multiple sites,[0],[0]
"The test rejects the pooling strategy with high power for combining before domain adaptation (see Fig. 4(b)), as one would expect.",4.2. Combining AD datasets from multiple sites,[0],[0]
"This rejection power increases rapidly as sample size increases, see red curve in Fig. 4(b).",4.2. Combining AD datasets from multiple sites,[0],[0]
"The results in Fig. 4(c,d) show the setting where one cannot change the dataset sizes at the sites i.e., the training set uses an equal number of labeled samples from both the ADNI and ADlocal (x-axis in Fig. 4(c)), and the testing set always corresponds to 20% of ADNI data.",4.2. Combining AD datasets from multiple sites,[0],[0]
"This is a more interesting scenario for a practitioner compared to Fig. 4(a,b), because in Fig. 4(c,d) we use same sample sizes for both datasets.",4.2. Combining AD datasets from multiple sites,[0],[0]
"The trends in Fig. 4(c,d) are the same as Fig. 4(a,b).",4.2. Combining AD datasets from multiple sites,[0],[0]
"We present a hypothesis test to answer whether pooling multiple datasets acquired from different sites is guaran-
teed to increase statistical power for regression models.",5. Conclusions,[0],[0]
"For both standard and high dimensional linear regression, we identify regimes where such pooling is sensible, and show how such policy decisions can be made via simple checks executable on each site before any data transfer ever happens.",5. Conclusions,[0],[0]
"We also show empirical results by combining two Alzheimer’s disease datasets in the context of different regimes proposed by our analysis, and see that the regression fit improves as suggested by the theory.",5. Conclusions,[0],[0]
"The code is available at https://github.com/hzhoustat/ICML2017.
Acknowledgments: This work is supported by NIH grants R01 AG040396, R01 EB022883, UW CPCP AI117924, R01 AG021155, and NSF awards DMS 1308877, CAREER 1252725 and CCF 1320755.",5. Conclusions,[0],[0]
"The authors are grateful for partial support from UW ADRC AG033514, UW ICTR 1UL1RR025011 and funding from a UW-Madison/DZNE collaboration initiative.",5. Conclusions,[0],[0]
Many studies in biomedical and health sciences involve small sample sizes due to logistic or financial constraints.,abstractText,[0],[0]
"Often, identifying weak (but scientifically interesting) associations between a set of predictors and a response necessitates pooling datasets from multiple diverse labs or groups.",abstractText,[0],[0]
"While there is a rich literature in statistical machine learning to address distributional shifts and inference in multi-site datasets, it is less clear when such pooling is guaranteed to help (and when it does not) – independent of the inference algorithms we use.",abstractText,[0],[0]
"In this paper, we present a hypothesis test to answer this question, both for classical and high dimensional linear regression.",abstractText,[0],[0]
"We precisely identify regimes where pooling datasets across multiple sites is sensible, and how such policy decisions can be made via simple checks executable on each site before any data transfer ever happens.",abstractText,[0],[0]
"With a focus on Alzheimer’s disease studies, we present empirical results showing that in regimes suggested by our analysis, pooling a local dataset with data from an international study improves power.",abstractText,[0],[0]
"When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests, 2-consistency and Neuroscience Applications",title,[0],[0]
"Proceedings of the SIGDIAL 2016 Conference, pages 360–369, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Studies about laughter in interaction have been mainly focused on the acoustic or perceptual features, and often observations of the events preceding to it have been the base for claims concerning what laughter is about.",1 Introduction,[0],[0]
"(Provine, 1993) made a claim that has been subsequently adopted in much of the literature: laughter is, for the most part, not related to humour, because it is found to most frequently follow banal comments.",1 Introduction,[0],[0]
Similar reasoning has been adopted by several other studies on the kind of situations that elicit laughter.,1 Introduction,[0],[0]
The deduction process in these studies rely on an important yet untested assumption: what laughter follows is what it is about.,1 Introduction,[0],[0]
Our paper investigates this assumption.,1 Introduction,[0],[0]
We first briefly discuss previous studies on laughter in interaction; we then argue for a semantic/pragmatic account in which we treat laughter as a gestural event anaphora referring to a laughable.,1 Introduction,[0],[0]
We present a corpus study of laughables and evaluate our results against previous proposals.,1 Introduction,[0],[0]
"In (Provine, 1993), the researcher observed natural conversations, and “when an observer heard laughter, she recorded in a notebook the comment immediately preceding the laughter and if the speaker and/or the audience laughed, the gender, and the estimated age of the speaker and the audience",1.1 Studies on what laughter is about,[0],[0]
[...].,1.1 Studies on what laughter is about,[0],[0]
A laugh episode was defined as the occurrence of audible laughter and included any laughter by speaker or audience that followed within an estimated 1 s of the initial laugh event.,1.1 Studies on what laughter is about,[0],[0]
The laugh episode included the last comment by a speaker if it occurred within an estimated 1 s preceding the onset of the initial laughter.,1.1 Studies on what laughter is about,[0],[0]
"A laugh episode was terminated if an estimated 1 s passed without speaker or audience laughter, or if either the speaker or the audience spoke.”.",1.1 Studies on what laughter is about,[0],[0]
"They found that “Only about 10-20% of episodes were estimated by the observers to be humorous” (Provine, 1993), and thus derived the conclusion which is now widely adopted in the literature: laughter is, for the most part, not related to humour but about social interaction.",1.1 Studies on what laughter is about,[0],[0]
"An additional conclusion based on this study is that laughter never interrupts speech but “punctuates” it occurring exclusively at phrase boundaries.
",1.1 Studies on what laughter is about,[0],[0]
"Similarly, (Vettin and Todt, 2004) used exclusively timing parameters – i.e., what precedes and what follows the laugh (within a threshold of 3s) – to distinguish 6 different contexts (see table 1) for laughter occurrence to support claims about situations that elicit laughter.",1.1 Studies on what laughter is about,[0],[0]
"In (Provine, 1993), the author assumed that laughter always immediately follows the laughable.",1.2 Weaknesses,[0],[0]
"Not only do the methods described above provide imprecise data (timing information was estimated during observation), it prevents the possibility of recording any data where laughter does not follow the laughable.",1.2 Weaknesses,[0],[0]
"In addition, even when the com-
360
ment that immediately precedes laughter is the actual trigger for a laugh, and it is not “amusing” in itself (i.e. it is a “banal comment”), it doesn’t necessarily entail that the laughable is not humourous.",1.2 Weaknesses,[0],[0]
"The funniness might arise from the “banal comment” in relation to the previous utterance, the context of the interaction, shared experiences between the speakers, world knowledge and cultural conventions.",1.2 Weaknesses,[0],[0]
"For example, in (1) “what’s funny” resides in the implicit content that the utterance refers to.",1.2 Weaknesses,[0],[0]
"In (2), the preceding utterance is funny only in relation to the context.
",1.2 Weaknesses,[0],[0]
(1) A: Do you remember that time?,1.2 Weaknesses,[0],[0]
B and A: < laughter/,1.2 Weaknesses,[0],[0]
>.,1.2 Weaknesses,[0],[0]
"Laughable= the enriched denotation of ‘that time’.
",1.2 Weaknesses,[0],[0]
"(2) (Context: the speakers are discussing the plan of an imagined shared apartment, and they have already planned two bathrooms).",1.2 Weaknesses,[0],[0]
A: I want another bathroom.,1.2 Weaknesses,[0],[0]
B:,1.2 Weaknesses,[0],[0]
"< laughter/ > Laughable= “I want another bathroom”
(Vettin and Todt, 2004) is methodologically more precise than (Provine, 1993), and they allow for the possibility that in addition to laughter occurring after the laughable, a laughter may precede an utterance, or occur during an exophoric situation.",1.2 Weaknesses,[0],[0]
"However, this analysis excludes laughters that occur in the middle of or overlaps with an utterance, and it uses exclusively timing parameters to determine what laughter is about (as illustrated in figure 1).",1.2 Weaknesses,[0],[0]
"For example, whether a laugh is considered to be about the preceding utterance or about the following utterance is decided purely on the difference in the length of gaps with the two utterances.",1.2 Weaknesses,[0],[0]
"Crucially, the conclusion is also drawn assuming an adjacency relationship between laughter and laughable.",1.2 Weaknesses,[0],[0]
"We argue that previous studies have ignored analysing the laughable because they did not attempt to integrate their account with an explicit
semantic/pragmatic module on the basis of which content is computed.1 The sole recent exception to this, as far as we are aware, is the account of (Ginzburg et al., 2015), which sketches an information state–based account of the meaning and use of laughter in dialogue.
",2 Laughter as an event anaphor,[0],[0]
"Taking this as a starting point, we argue that laughter is a gestural event anaphor, whose meaning contains two dimensions: one dimension about the arousal and the other about the trigger or the laughable.",2 Laughter as an event anaphor,[0],[0]
"In line with (Morreall, 1983)",2 Laughter as an event anaphor,[0],[0]
"we think that laughter effects a “positive psychological shift”, and the “arousal” dimension signals the amplitude in the shift.2.",2 Laughter as an event anaphor,[0],[0]
"The positive psychological shift is triggered by an appraisal of an event - the laughable l, and the second dimension communicates the type of the appraisal.",2 Laughter as an event anaphor,[0],[0]
"(Ginzburg et al., 2015) propose two basic types of meaning in the laughable dimension: the person laughing may express her perception of the laughable l as being incongruous, or just that l is enjoyable (playful).",2 Laughter as an event anaphor,[0],[0]
"We propose that in addition, certain uses of laughter in dialogue may suggest the need for a third possible type: expressing that l is a socially close ingroup situation.",2 Laughter as an event anaphor,[0],[0]
Here we sketch a formal semantic and pragmatic treatment of laughter.,2.1 Formal treatment of laughter,[0],[0]
"On the approach developed in KoS (Ginzburg, 2012), information states comprise a private part and the dialogue gameboard that represents information arising from publicized interactions.",2.1 Formal treatment of laughter,[0],[0]
"In addition to tracking shared assumptions/visual space, Moves, and QUD, the dialogue gameboard also tracks topoi and enthymemes that conversational participants exploit during an interaction (e.g., in reasoning about rhetorical relations.).",2.1 Formal treatment of laughter,[0],[0]
"Here topoi represent general inferential patterns (e.g., given two routes choose
1This is not the case for some theories of humour, e.g., that due to (Raskin, 1985), who offers a reasonably explicit account of incongruity emanating from verbal content without, however, attempting to offer a theory of laughter in conversation.
2The amplitudes in the shift depend on both the trigger itself and on the individual current information/emotional state.",2.1 Formal treatment of laughter,[0],[0]
"It is important to point out that laughter does not signal that the speaker’s current emotional state is positive, merely that there was a shift which was positive.",2.1 Formal treatment of laughter,[0],[0]
The speaker could have a very negative baseline emotional state (being very sad or angry) but the recognition of the incongruity in the laughable or its enjoyment can provoke a positive shift (which could be very minor),2.1 Formal treatment of laughter,[0],[0]
"The distinction between the overall emotional state and the direction of the shift explains why laughter can be produced when one is sad or angry.
the shortest one) represented as functions from records to record types, and enthymemes are instances of topoi (e.g., given that the route via Walnut street is shorter than the route via Alma choose Walnut street).",2.1 Formal treatment of laughter,[0],[0]
"An enthymeme belongs to a topos if its domain type is a subtype of the domain type of the topos.
",2.1 Formal treatment of laughter,[0],[0]
"(Ginzburg et al., 2015) posit distinct, though quite similar lexical entries for enjoyment and incongruous laughter.",2.1 Formal treatment of laughter,[0],[0]
For reasons of space in (3) we exhibit a unified entry with two distinct contents.,2.1 Formal treatment of laughter,[0],[0]
"(3) associates an enjoyment laugh with the laugher’s judgement of a proposition whose situational component l is active as enjoyable; for incongruity, a laugh marks a proposition whose situational component l is active as incongruous, relative to the currently maximal enthymeme under discussion.",2.1 Formal treatment of laughter,[0],[0]
(3) makes appeal to a notion of an active situation.,2.1 Formal treatment of laughter,[0],[0]
"This pertains to the accessible situational antecedents of a laughter act, given that (Ginzburg et al., 2015) proposed viewing laughter as an event anaphor.",2.1 Formal treatment of laughter,[0],[0]
"However, given the existence of a significant amount of speech laughter, as we discuss below, this notion apparently needs to be rethought somewhat, viewing laughter in gestural terms.",2.1 Formal treatment of laughter,[0],[0]
"This requires interfacing the two channels, a problem we will not address here, though see (Rieser, 2015) for a recent discussion in the context of manual gesture.
",2.1 Formal treatment of laughter,[0],[0]
(3) phon :,2.1 Formal treatment of laughter,[0],[0]
laughterphontype dgb-params :  spkr : Ind addr : Ind t : TIME c1 :,2.1 Formal treatment of laughter,[0],[0]
"addressing(spkr,addr,t) MaxEud = e : (Rec)RecType p = [ sit = l sit-type = L ] : prop c2 : ActiveSit(l)  contentenjoyment = Enjoy(spkr,p) : RecType contentincongruity = Incongr(p,e,τ ) :",2.1 Formal treatment of laughter,[0],[0]
"RecType  The dialogue gameboard parameters utilised in the account of (Ginzburg et al., 2015) are all ‘informational’ or utterance related ones.",2.1 Formal treatment of laughter,[0],[0]
"However, in order to deal with notions such as arousal and psychological shift, one needs to introduce also parameters that track appraisal (see e.g.,
(Scherer, 2009)).",2.1 Formal treatment of laughter,[0],[0]
"For current purposes, we mention merely one such parameter we dub pleasantness that relates to the appraisal issue—in Scherer’s formulation—Is the event intrinsically pleasant or unpleasant?.",2.1 Formal treatment of laughter,[0],[0]
"We assume that this parameter is scalar in value, with positive and negative values corresponding to varying degrees of pleasantness or unpleasantness.
",2.1 Formal treatment of laughter,[0],[0]
"This enables us to formulate conversational rules of the form ‘if A laughs and pleasantness is set to k, then reset pleasantness to k + θ(α)’, where α is a parameter corresponding to arousal.",2.1 Formal treatment of laughter,[0],[0]
The study is part of a broader project where we analyse laughter using a multi-layered scheme and propose a semantic/ pragmatic account of the meaning and effects of laughter.,2.2 Research questions,[0],[0]
"The focus of the current study is the positioning of laughter in relation to its laughable.
",2.2 Research questions,[0],[0]
Our account suggests that resolving the laughable is crucial for deriving the content of a laughter event.,2.2 Research questions,[0],[0]
We hypothesize that laughter is not always adjacent to its laughable.,2.2 Research questions,[0],[0]
"Rather, the sequential distribution between laughter and laughable is somewhat free, illustrated in Figure 2.",2.2 Research questions,[0],[0]
"We hypothesize that laughter can occur before, during and after the laughable, and that it is possible for intervening materials to occur between a laughter event and its laughable.
",2.2 Research questions,[0],[0]
"In more detail, we make the following hypotheses in relation to our research questions:
Q1: Does laughter always follow its laughable?
–If not, does laughter-laughable alignment differ among different types of laughters?
",2.2 Research questions,[0],[0]
"We hypothesize that laughter can occur before, during or after the laughable; laughter and laughable should not have a one-to-one relationship: one laughable can be the referent of several laughter events.
–More specifically, laughter-laughable alignment may vary depending on at least the source of the laughable (self or partner) and whether it is speech laugh or laughter bouts.
",2.2 Research questions,[0],[0]
"Q2: Does laughter interrupt speech?
",2.2 Research questions,[0],[0]
"We hypothesize that laughter can occur both at utterance boundaries and at utterance-medial position.
",2.2 Research questions,[0],[0]
"Q3: Is laughter-laughable alignment pattern language specific?
",2.2 Research questions,[0],[0]
"We hypothesize that language/culture influence alignment and thus predict to find differences between, in this case, French and Chinese.",2.2 Research questions,[0],[0]
"We analyzed a portion of the DUEL corpus (Hough et al., 2016a)",3.1 Corpus,[0],[0]
"The corpus consists of 30 dyads (10 per language)/ 24 hours of natural, faceto-face, loosely task-directed dialogue in French, Mandarin Chinese and German.",3.1 Corpus,[0],[0]
Each dyad conversed in three tasks which in total lasted around 45 minutes.,3.1 Corpus,[0],[0]
"The three tasks used were:
1.",3.1 Corpus,[0],[0]
"Dream Apartment: the participants are told that they are to share a large open-plan apartment, and will receive a large amount of money to furnish and decorate it.",3.1 Corpus,[0],[0]
"They discuss the layout, furnishing and decoration decisions;
2.",3.1 Corpus,[0],[0]
Film Script:,3.1 Corpus,[0],[0]
"The participants spend 15 minutes creating a scene for a film in which something embarrassing happens to the main character;
3.",3.1 Corpus,[0],[0]
"Border control: one participant plays the role of a traveller attempting to pass through the border control of an imagined country, and is interviewed by an officer.",3.1 Corpus,[0],[0]
The traveller has a personal situation that disfavours him/her in this interview.,3.1 Corpus,[0],[0]
The officer asks questions that are general as well as specific.,3.1 Corpus,[0],[0]
"In addition, the traveller happens to be a parent-in-law of the officer.
",3.1 Corpus,[0],[0]
The corpus is transcribed in the target language and glossed in English.,3.1 Corpus,[0],[0]
"Disfluency, laughter, and exclamations are annotated.",3.1 Corpus,[0],[0]
The current paper presents analysis of laughter in two dyads in French and Chinese (3 tasks x 2 pairs x 2 languages).,3.1 Corpus,[0],[0]
"Coding was conducted by the first and second authors and by 2 trained, but naı̈ve to the aim of the study, masters students: each video was observed until a laugh occurred.",3.2 Audio-video coding of laughter,[0],[0]
"The coder detected the exact onset and offset in Praat (Boersma and others, 2002), and conducted a multi-layer analysis as explained shortly.",3.2 Audio-video coding of laughter,[0],[0]
"A laugh was identified referring to the same criteria used in (Nwokah et al., 1994), based on the facial expression and vocalization descriptions of laughter elaborated by (Apte, 1985) and (Ekman and Friesen, 1975).",3.2 Audio-video coding of laughter,[0],[0]
"Following (Urbain and Dutoit, 2011)",3.2 Audio-video coding of laughter,[0],[0]
"we counted laughter offset (final laughter in-breath inhalation) as part of the laughter event itself, thus resulting in laughter timings longer than other authors (Bachorowski and Owren, 2001; Rothgänger et al., 1998).
",3.2 Audio-video coding of laughter,[0],[0]
"All laughter events were categorised according to different parameters: formal and contextual aspects, semantic meaning and functions (see Table 2).",3.2 Audio-video coding of laughter,[0],[0]
"The formal and contextual level analysis include whether a laughter overlaps speech (speech laugh), whether it co-occurs with or immediately follows a partner’s laughter (dyadic/ antiphonal laughter), and its position in relation to the laughable.",3.2 Audio-video coding of laughter,[0],[0]
The semantic meaning level analysis include perceived arousal and whether it contains an element of incongruity could be identified by the coders.,3.2 Audio-video coding of laughter,[0],[0]
"The function analysis codes the effect of laughter on the interaction, and distinguishes whether the effect is cooperative, i.e., promotes interaction (e.g. showing enjoyment, smoothing) or non-cooperative, i.e., in some way disaffects interaction (e.g., mocking or evade questions).",3.2 Audio-video coding of laughter,[0],[0]
"Due to space constraints and current focus, we do not provide a detailed explanation of the multi-level laughter coding scheme, for which see (Mazzocconi et al., 2016).",3.2 Audio-video coding of laughter,[0],[0]
Reliability was assessed by having a masters student as a second coder for 10% of the material observed.,3.2 Audio-video coding of laughter,[0],[0]
"Percentage agreements between the two coders for French and Chinese data averaged respectively 87% and 87.76, with an overall Krippendorff α (Krippendorff, 2012) across all tiers of 0.672 and 0.636.
",3.2 Audio-video coding of laughter,[0],[0]
"For the main analysis, we include in our analysis both laughter and speech laughter (Nwokah et al., 1999).",3.2 Audio-video coding of laughter,[0],[0]
"In the current study we restrict our observations about the aspects pertaining to the form, to the contextual distribution and positioning of a laugh in relation to others’ laughter, the laughable and laugher’s herself speech.",3.2 Audio-video coding of laughter,[0],[0]
"We consider as the laughable the event which, after appraisal, produces a positive psychological shift in the laugher.",3.3 Identifying laughables,[0],[0]
"We distinguish three different kinds of laughable types: described events, metalinguistic stimuli and exophoric events.",3.3 Identifying laughables,[0],[0]
"We also mark whether they originated from the laugher him/herself or by the partner.
(4) Described event A: il y a (un: + un) de mes potes?",3.3 Identifying laughables,[0],[0]
"idiot comme il est, qui (< p = pose > po-",3.3 Identifying laughables,[0],[0]
< /p,3.3 Identifying laughables,[0],[0]
> qui pose) un steak sur le rebord (de: + du) balcon?,3.3 Identifying laughables,[0],[0]
B:< laughter/,3.3 Identifying laughables,[0],[0]
>.,3.3 Identifying laughables,[0],[0]
< laughspeech > ils sont bizarres tes potes < /laughspeech,3.3 Identifying laughables,[0],[0]
">
(Translation) A: There is (one + one) of my buddies, stupid as he is, who put a steak on the border of the: of the balcony B: < laughter/",3.3 Identifying laughables,[0],[0]
>.,3.3 Identifying laughables,[0],[0]
< laughspeech > you have weird buddies < /laughspeech,3.3 Identifying laughables,[0],[0]
">
Laughable= “who put a steak on the border of the balcony”: described event
(5) Metalinguistic stimuli B: Alors je viens pour {euh} avoir mon passeport?",3.3 Identifying laughables,[0],[0]
pour Inra:schabella?,3.3 Identifying laughables,[0],[0]
"< laughter/ >
(Translation) B:",3.3 Identifying laughables,[0],[0]
"So I’m here for, euh, having my passport?",3.3 Identifying laughables,[0],[0]
for Inraschabella? < laughter/,3.3 Identifying laughables,[0],[0]
">
Laughable= “Inraschabella” (linguistic form, laugh after laugher’s speech)
(6) Exophoric event The examiner is asking A to move the arms because of technical issues A: movement arms mimicking a robot B:",3.3 Identifying laughables,[0],[0]
< laughter/,3.3 Identifying laughables,[0],[0]
>,3.3 Identifying laughables,[0],[0]
"A: < laughter/ >
",3.3 Identifying laughables,[0],[0]
Laughable=the way A moved his arms: exophoric event,3.3 Identifying laughables,[0],[0]
"Every time a laugh was identified, coders would mark on the Praat TextGrid, based on personal inference, the laughable the laugh would refer to.
",3.4 Audio-video coding of laughable,[0],[0]
"The time boundaries were marked, the content (whether verbal or not) was annotated and an index was assigned in order to map laughter (or multiple laughters) and laughable.",3.4 Audio-video coding of laughable,[0],[0]
"Laughables were classified according to three main categories: described, metalinguistic and exhophoric event.",3.4 Audio-video coding of laughable,[0],[0]
Reliability of type assignement was assessed by having a masters student as a second coder for 10% of the material observed.,3.4 Audio-video coding of laughable,[0],[0]
"Percentage agreements between the two coders for French and Chinese averaged 92.5% with a Krippendorff α (Krippendorff, 2012) of 0.77.",3.4 Audio-video coding of laughable,[0],[0]
"In our data sample (summarized in Table2), laughter is very frequent, constituting 17% of the conversation duration in French and 7.2% in Chinese.",4 Results,[0],[0]
Each laughable is ”laughed about” more than once (1.7 times in French and 1.4 times in Chinese).,4 Results,[0],[0]
"To investigate the time alignment between laughter and laughable, we calculated “start of laughter minus start of laughable”, “end of laughter minus end of laughable”, and “start of laughter minus end of laughable”.",4.1 Does laughter always follow the laughable?,[0],[0]
"If laughter always follow the laughable, all three measurements should be above zero.",4.1 Does laughter always follow the laughable?,[0],[0]
This was not the case.,4.1 Does laughter always follow the laughable?,[0],[0]
"In both Chinese and French, on average, laughter starts during rather than after the laughable, and finishes af-
ter the laughable.",4.1 Does laughter always follow the laughable?,[0],[0]
"In general, laughs in Chinese are more likely to overlap with the laughable than in French.",4.1 Does laughter always follow the laughable?,[0],[0]
The distribution varies over a wide range.,4.1 Does laughter always follow the laughable?,[0],[0]
"Table 3 summarizes the gaps between the boundaries of laughter and laughable, and figure 3 plots specifically the gap between the end of the laughable and the start of laughter.",4.1 Does laughter always follow the laughable?,[0],[0]
"They show that it is common for laughs to start before, during and after the laughable.",4.1 Does laughter always follow the laughable?,[0],[0]
"When a laugh has no overlap with its laughable, they are not always adjacent to each other (average utterance duration is under 2 seconds while the gap can be up to 10 seconds).",4.1 Does laughter always follow the laughable?,[0],[0]
"In the following example, the first two instances of speech laugh refer to a laughable in a later utterance.
",4.1 Does laughter always follow the laughable?,[0],[0]
"(7) 那 个 老 师(要 他+要 求 小 诗) 用“不 约 而 同”造 句 子, 后 来 小 明< laughspeech >就 想 了 一",4.1 Does laughter always follow the laughable?,[0],[0]
想< /laughspeech,4.1 Does laughter always follow the laughable?,[0],[0]
">, 然后说呃说呃这样吧?",4.1 Does laughter always follow the laughable?,[0],[0]
"< laughspeech >(我 就+小 诗)< /laughspeech > 就想了想说,呃:呃:我在路上碰见一个美女,然后我 就问她,约吗?< laughspeech > 然后美女说, 滚,我 们不约儿童< /laughspeech",4.1 Does laughter always follow the laughable?,[0],[0]
">.
(Translation) B:",4.1 Does laughter always follow the laughable?,[0],[0]
The teacher asked Xiaoshi to make a sentence with ”bu yue er tong” (coincidentally together).,4.1 Does laughter always follow the laughable?,[0],[0]
"Xiaoshi < laughspeech > then < laughspeech/ > thought about it, and said, uh, < laughspeech >",4.1 Does laughter always follow the laughable?,[0],[0]
(,4.1 Does laughter always follow the laughable?,[0],[0]
I + Xiaoshi),4.1 Does laughter always follow the laughable?,[0],[0]
"< laughspeech/ > thought about it and said, uh, uh I saw a pretty girl in the street, and I asked her ”shall we go for a date?”, and < laughspeech >",4.1 Does laughter always follow the laughable?,[0],[0]
the girl said “shouldn’t date children” < laughspeech/ >.,4.1 Does laughter always follow the laughable?,[0],[0]
"(note: “shouldn’t date children” is phonologically identical to ”incidentally together”)
",4.1 Does laughter always follow the laughable?,[0],[0]
"Laughable= “the girl said ‘shouldn’t date children’ ”
Based on whether laughter occurs entirely outside or overlapping with the laughable, we grouped the laughters into 4 alignment categories: “before”, “overlap”, “immediately after” and “other after” (see figure 4).",4.1 Does laughter always follow the laughable?,[0],[0]
"We found that in both languages, laughters that immediately follow (within 0.3s) the laughable constitute 30% .",4.1 Does laughter always follow the laughable?,[0],[0]
"There are more overlapping laughters in Chinese than in French (χ2(1)=6.9, p= .008).",4.1 Does laughter always follow the laughable?,[0],[0]
"Our analysis mainly focuses on the distinction between self and partner produced laughables, and
between speech laugh and laughter bouts, presented separately below.",4.2 Does laughter-laughable alignment differ among different “types” of laughables and laughters?,[0],[0]
"Due to space constraints, the effect of the rest of the tiers are not discussed.",4.2 Does laughter-laughable alignment differ among different “types” of laughables and laughters?,[0],[0]
"We coded whether the laughables are described events, meta-linguistic, or exophoric events.",4.2.1 Self vs. partner produced laughables,[0],[0]
"In our corpus described events are the commonest (92% in French and 89% in Chinese), followed by exophoric laughables (7% in French and 10%).",4.2.1 Self vs. partner produced laughables,[0],[0]
"Metalinguistic (1% in both languages) laughables are rare, so we grouped them with described events in the current analysis.",4.2.1 Self vs. partner produced laughables,[0],[0]
"On average, there are more self-produced than partnerproduced laughables, supporting the idea that speakers laugh more often than the audience.",4.2.1 Self vs. partner produced laughables,[0],[0]
"Interestingly, 3% of the laughables are jointly produced (one person finishing the other’s sentence, or both saying roughly the same thing at the same time) (see (8)).",4.2.1 Self vs. partner produced laughables,[0],[0]
"With the former two categories, we also coded whether the laughable is produced by the laugher or her partner, which allow us to compare our results with studies of “speaker” or “audience” laughter.
",4.2.1 Self vs. partner produced laughables,[0],[0]
"(8) (totally overlapping turns are italicized)
",4.2.1 Self vs. partner produced laughables,[0],[0]
B: c’est une personne qui est aux toilettes dans < laughter,4.2.1 Self vs. partner produced laughables,[0],[0]
>,4.2.1 Self vs. partner produced laughables,[0],[0]
des toilettes publiques A: < laughter,4.2.1 Self vs. partner produced laughables,[0],[0]
>,4.2.1 Self vs. partner produced laughables,[0],[0]
X,4.2.1 Self vs. partner produced laughables,[0],[0]
ah: oui: oui un mec qui parle a cute‘ < laughter/,4.2.1 Self vs. partner produced laughables,[0],[0]
> B: dans < laughter > des toilettes,4.2.1 Self vs. partner produced laughables,[0],[0]
"publiques voila sauf que l’autre il est au
telephone et l’autre il lui croit qu’il parle .",4.2.1 Self vs. partner produced laughables,[0],[0]
C’est genant < laughter/,4.2.1 Self vs. partner produced laughables,[0],[0]
">
(Translation) B: it is a person who is in the bathroom in < laughter > in public bathroom",4.2.1 Self vs. partner produced laughables,[0],[0]
A:< laughter >,4.2.1 Self vs. partner produced laughables,[0],[0]
Ah yes yes a guy who is talking in the next stall,4.2.1 Self vs. partner produced laughables,[0],[0]
< laughter/ > B: in < laughter,4.2.1 Self vs. partner produced laughables,[0],[0]
>,4.2.1 Self vs. partner produced laughables,[0],[0]
in public bathroom exactly but the other is on the phone and the other thinks he is speaking with him.,4.2.1 Self vs. partner produced laughables,[0],[0]
That’s embarrassing < laughter/,4.2.1 Self vs. partner produced laughables,[0],[0]
">
",4.2.1 Self vs. partner produced laughables,[0],[0]
"Laughable= “exactly but the other is on the phone and the other thinks he is speaking with him”
We found that laughters about a partnerproduced laughable start later than those about a self-produced laughable, but still the average starting time is before the end of the laughable.",4.2.1 Self vs. partner produced laughables,[0],[0]
"With partner-produced laughables, the average gap between the end of laughable and start of laughter is -0.02s in French and -0.3s in Chinese, while with self-produced laughables, the average gap is -0.7s in French and -1.3s in Chinese.",4.2.1 Self vs. partner produced laughables,[0],[0]
Laughter frequently overlaps with speech.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
36% of laughter events in French and 47% of laughter events in Chinese contain speech laughter.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
Speech laughter is on average 0.3 seconds longer than stand alone laughter bouts.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
Speech laughs overlap with the laughable more than laughter bouts.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
52% of speech laughters in French and 70% in Chinese overlap with the laughables.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"In comparison, 33% of laughter bouts in French and 34% in Chinese overlap with the laughable.",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
The reason why speech laugh more often overlap with the laughables is likely to do with the difference in function between speech laugh and laughter bouts.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"Laughters that mark an upcoming laughable most frequently overlaps with speech, and these laughter events are also ones that tend to stretch until the middle or the end of the laughable.",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"A more detailed analysis of the function/effect of laughter is reported in (Mazzocconi et al., 2016).
",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"Notice that not all speech laughs overlap with the laughable, suggesting that often, laughter that co-occurs with speech is not about the cooccurring speech (47.8% in French and 30% in Chinese).",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"In the following example, speaker B says that she’ll take the bigger bedroom, and laughs.",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"Speaker A joins the laughter but starts a new utterance.
",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
(9) B:,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
okay.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
les chambres maintenant A:alo:rs F euh: bon évidemment F euh: B: je prends la plus grande <,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
laughter/,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
> A: c’est là < laughter >,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
où il y a un problème t’vois < /laughter,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
">
(Translation) B:",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
okay.,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
the bedrooms now,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
A: well euh:,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
well obviously euh:,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
B: I take the bigger one < laughter/,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
>,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"A: It’s there < laughspeech > where there is a problem you see < /laughspeech >
",4.2.2 Speech laugh vs. laughter bouts,[0],[0]
Laughable= “je prends la plus grande”,4.2.2 Speech laugh vs. laughter bouts,[0],[0]
"We investigated whether laughter occurs at utterance-medial positions when one party is speaking, and when the partner is speaking.
",4.3 Does laughter interrupt speech?,[0],[0]
Does laughter interrupt partners’ utterances?,4.3 Does laughter interrupt speech?,[0],[0]
Yes.,4.3 Does laughter interrupt speech?,[0],[0]
"We found that 51.8% of laughter bouts in French and 56.7% of laughter bouts in Chinese start during the partner’s utterances (not necessarily laughables), for example:
(10) B: pour faire un mur de son quoi < laughspeech > en fait c’est une < english > ra:",4.3 Does laughter interrupt speech?,[0],[0]
ve < /english > notre appartement < /laughspeech,4.3 Does laughter interrupt speech?,[0],[0]
>,4.3 Does laughter interrupt speech?,[0],[0]
"A: < laughter/ >
(Translation) B: to create a sound barrier which < laughspeech > in fact it is a rave, our apartment < /laughspeech",4.3 Does laughter interrupt speech?,[0],[0]
>,4.3 Does laughter interrupt speech?,[0],[0]
"A:< laughter/ >
Laughable= “in fact it is a rave, our apartment”
Does laughter interrupt one’s own utterances?
",4.3 Does laughter interrupt speech?,[0],[0]
We found 14 laughter bouts (5%) in French and 12 (8.6%) in Chinese that occurred in utterancemedial positions.,4.3 Does laughter interrupt speech?,[0],[0]
These proportions are statistically higher than zero:,4.3 Does laughter interrupt speech?,[0],[0]
"French χ2(1)=12.3, p=.0004; Chinese χ2(1)=10.5, p=.001.",4.3 Does laughter interrupt speech?,[0],[0]
Most of these interruptions at not at phrase boundaries.,4.3 Does laughter interrupt speech?,[0],[0]
"For example:
(11) 那你之前有没有啊:.有过什么...",4.3 Does laughter interrupt speech?,[0],[0]
< laughter/,4.3 Does laughter interrupt speech?,[0],[0]
"> < laughter >犯罪记录吗?
(Translation) Do you have, uh, have any < laughter/ >",4.3 Does laughter interrupt speech?,[0],[0]
"criminal records?
",4.3 Does laughter interrupt speech?,[0],[0]
Laughable= “criminal records”,4.3 Does laughter interrupt speech?,[0],[0]
"The aim of the current study was to deepen the little research available on the relation between laughter, laughable and speech in natural conversation, starting from the observation of their temporal sequence and alignment.",5 Discussion,[0],[0]
"We investigated three questions: whether laughter always follows, or at least is adjacent to its laughable, as is commonly assumed; whether this sequential alignment differ depending on differeht “types” of laughters; and whether laughter always punctuates speech.",5 Discussion,[0],[0]
"Our main findings are:
1.",5 Discussion,[0],[0]
Time alignment between laughter and laughable is rather free.,5 Discussion,[0],[0]
— Laughter and laughable does not have a one-to-one relationship.,5 Discussion,[0],[0]
A laughable can be referred to by more than one laughters.,5 Discussion,[0],[0]
"— Contrary to popular belief, only 30% of laughters occur immediately after the laughable.",5 Discussion,[0],[0]
Laughters frequently start during the laughable (more so with “speaker” laughter than “audience“ laughter).,5 Discussion,[0],[0]
"— Laughters can occur long before or long after the laughable, and not be adjacent to their laughable.",5 Discussion,[0],[0]
"— Between 30 to 50 percent of speech laughs do not overlap with the laughable, suggesting that frequently laughs are not about the cooccurring speech.",5 Discussion,[0],[0]
"If looking just at laughter bouts, about 40% occur immediately after the laughable.
2.",5 Discussion,[0],[0]
Laughter-laughable alignment may differ depending on the different “types” of laughable and laughter.,5 Discussion,[0],[0]
"Specifically, laughters about a partner-produced laughable (audience laughter) start later than those about a self-produced laughable (speaker laughter).",5 Discussion,[0],[0]
"Speech laughs occur earlier than laughter bouts, and overlaps more with the laughable.
3.",5 Discussion,[0],[0]
"Comparing Chinese and French, the majority of the patterns are similar, except that in Chinese, laughs are more likely to overlap with the laughable than in French.",5 Discussion,[0],[0]
"This provides an initial indication that while certain aspects of laughter behaviour are influenced by culture/language, generally we use laughter similarly in interaction.",5 Discussion,[0],[0]
"3
4.",5 Discussion,[0],[0]
Laughter does interrupt speech: we often laugh when others are speaking (half of all laughter bouts) and occasionally we insert stand-alone laughters mid-sentence (less than 10%).,5 Discussion,[0],[0]
"Moreover, very frequently laughter overlaps speech (around 40% of all laughters).
",5 Discussion,[0],[0]
"The relatively free alignment between laughter and speech seems analogous at a first approximation to the relation between manual gesture and speech (Rieser, 2015).",5 Discussion,[0],[0]
"We propose to consider
3Of course a caveat to this conclusion is the small number of speakers for each language.",5 Discussion,[0],[0]
"We will expand the study with more speakers and more genres of interaction.
laughter as a verbal gesture, having an independent channel from speech, with which it communicates through an interface.",5 Discussion,[0],[0]
Our results discredit the method of inferring what the laughter is about by looking at the elements that immediately precede or follow it.,5.1 Is laughter rarely about funny stimuli?,[0],[0]
"Therefore, previous conclusions using this method should be revisited (Provine, 1993; Provine, 1996; Provine, 2001; Provine and Emmorey, 2006; Vettin and Todt, 2004).",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"One such conclusion is that because they follow “banal comments”, laughter is mostly about not about funny stimuli.",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"We have shown that the logic does not hold, as very often, those preceding “banal comments” are not the laughables.",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"And even if they are, the “funniness” or incongruity may reside between the laughable and something else, e.g., the context of occurrence, world knowledge, cultural norms, experiences, informational and intentional states shared between interlocutors.",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"For example, in the following exchange, the exchange seems rather banal, but in fact, they are laughing about the exophoric situation that they are acting.
",5.1 Is laughter rarely about funny stimuli?,[0],[0]
(12) A: Oh comment allez-vous? <,5.1 Is laughter rarely about funny stimuli?,[0],[0]
laughter/,5.1 Is laughter rarely about funny stimuli?,[0],[0]
> B:,5.1 Is laughter rarely about funny stimuli?,[0],[0]
ça va et toi?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
tu vas bien?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
"A : très bien merci:
(Translation)",5.1 Is laughter rarely about funny stimuli?,[0],[0]
A:,5.1 Is laughter rarely about funny stimuli?,[0],[0]
Oh how are you?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
<,5.1 Is laughter rarely about funny stimuli?,[0],[0]
laughter/,5.1 Is laughter rarely about funny stimuli?,[0],[0]
> B: fine and you?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
are you ok?,5.1 Is laughter rarely about funny stimuli?,[0],[0]
"A: very well thanks
Laughable= exophoric situation (they started acting)
",5.1 Is laughter rarely about funny stimuli?,[0],[0]
Exactly what proportion of laughables contain funny incongruity is a topic for further research.,5.1 Is laughter rarely about funny stimuli?,[0],[0]
"For now, our results questions the validity of existing proposals on this score.",5.1 Is laughter rarely about funny stimuli?,[0],[0]
"It has been suggested (notably by Provine) that laughter bouts almost never (0.1%) disrupt phrases but punctuate them (Provine, 1993; Provine, 1996; Provine, 2001).",5.2 Laughter Punctuating Speech?,[0],[0]
He explains this finding on the basis of an organic constraint: laughter and speech share the same vocal apparatus and speech has “priority access”.,5.2 Laughter Punctuating Speech?,[0],[0]
"Curiously enough, Provine has always excluded speech-laughs from his investigations, without any justification.",5.2 Laughter Punctuating Speech?,[0],[0]
"A more recent study on laughter in deaf ASL signers (Provine and Emmorey, 2006) showed that signers rarely laugh during their own utterances, where no competition for the same channel of expression is present.",5.2 Laughter Punctuating Speech?,[0],[0]
"Provine and Emmory conclude that the
punctuation effect of laughter holds even for signers, and possibly is not a simple physical constraint that determines the placement of laughter in dialogues, but due to a higher order linguistic ordered structure (Provine, 2006).
",5.2 Laughter Punctuating Speech?,[0],[0]
"On the surface, their findings in speakers and signers are similar: speakers do not stop midsentence to insert a laugh, and signers do not laugh while signing a sentence.",5.2 Laughter Punctuating Speech?,[0],[0]
"However, this “similarity” may be a difference in disguise.",5.2 Laughter Punctuating Speech?,[0],[0]
We have shown that speakers frequently overlap laughter and speech.,5.2 Laughter Punctuating Speech?,[0],[0]
"If it were indeed true that signers do not laugh while signing, it raises the question why speech laughter is common for speakers but rare for signers.",5.2 Laughter Punctuating Speech?,[0],[0]
"(Provine and Emmory, 2006) hypothesised that the placement of laughter in dialogue is controlled by a higher linguistic ordered structure, where laughter is secondary to language.",5.2 Laughter Punctuating Speech?,[0],[0]
"Therefore, even when the two don’t occur in competing channels, e.g., for signers, laughter still only occurs at phrase boundaries.
",5.2 Laughter Punctuating Speech?,[0],[0]
We argue for a different explanation.,5.2 Laughter Punctuating Speech?,[0],[0]
"Assuming speech laughter data (laughter that overlaps utterances) were not excluded in the ASL study as they were in spoken dialogue studies, in deaf signers, since the laughter is perceived only visually and involves marked facial movements, it would interfere with the perception of the message conveyed by language.",5.2 Laughter Punctuating Speech?,[0],[0]
"In sign languages, body and face movements constitute important communicative elements at all linguistic levels from phonology to morphology, semantics, syntax and prosody (Liddell, 1978; Campbell, 1999).",5.2 Laughter Punctuating Speech?,[0],[0]
"Despite the fact that emotional facial expressions can overlap with linguistic facial movements (Dachkovsky and Sandler, 2009), a laugh, implying a significant alteration of facial configuration (see identification of a laughter episode) could be excessively disruptive for the message aimed to be conveyed.",5.2 Laughter Punctuating Speech?,[0],[0]
"While in verbal language the laughter signal can be completely fused in the speech as a paralinguistic feature (Crystal, 1976) and used in a sophisticated manner to enrich and facilitate communication, (Nwokah et al., 1999) report that not even from an acoustic perspective is laughter secondary to speech: when co-occurring the laugh indeed does not resemble the speech spectral patterns nor does the speech resemble the laughter ones, but together they create a new idiosyncratic pattern.",5.2 Laughter Punctuating Speech?,[0],[0]
"Laughter is fully meaningful and communicative in itself, universally across cultures, and the emo-
tional components that it carries are not secondary to speech or trivial.",5.2 Laughter Punctuating Speech?,[0],[0]
"Our study provides the first systematic analysis of laughables, and demonstrates the existence of a corpus, the DUEL corpus (Hough et al., 2016b) in which less than a third of the laughs immediately follow their referents.",6 Conclusion and future work,[0],[0]
"Instead, the laugh can occur before, during or after the laughable with wide time ranges.",6 Conclusion and future work,[0],[0]
"In addition, laughter does “interrupt” speech: we frequently start laughing in the middle of an utterance of the interlocutor or of ourselves (often speech-laugh).",6 Conclusion and future work,[0],[0]
"Our results challenge the assumption that what laughter follows is what it is about, and thus question previous claims based on this assumption.
",6 Conclusion and future work,[0],[0]
"In future work, we will study to what extent laughter-laughable alignment differs by the function/effect of laughter, and what the limit is for the “free” alignment.",6 Conclusion and future work,[0],[0]
This work may be useful for dialogue systems which allows a computer agent to generate laughter at appropriate times depending on the type and location of the laughable.,6 Conclusion and future work,[0],[0]
We would like to thank three anonymous reviewers for SigDial 2016 for their very helpful comments.,Acknowledgments,[0],[0]
"We acknowledge the support of the French Investissements d’Avenir-Labex EFL program (ANR-10-LABX-0083) and the Disfluency, Exclamations, and Laughter in Dialogue (DUEL) project within the projets franco-allemand en sciences humaines et sociales funded by the ANR and the DFG.",Acknowledgments,[0],[0]
Studies on laughter in dialogue have proposed resolving what laughter is about by looking at what laughter follows.,abstractText,[0],[0]
This paper investigates the sequential relation between the laughter and the laughable.,abstractText,[0],[0]
We propose a semantic/pragmatic account treating laughter as a gestural event anaphor referring to a laughable.,abstractText,[0],[0]
Data from a French and Chinese dialogue corpus suggest a rather free time alignment between laughter and laughable.,abstractText,[0],[0]
"Laughter can occur (long) before, during, or (long) after the laughable.",abstractText,[0],[0]
"Our results challenge the assumption that what laughter follows is what it is about, and thus question claims which rely on this assumption.",abstractText,[0],[0]
When do we laugh?,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 157–166 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"In this work, we present a model for grounding spatial descriptors in 3D scenes.",1 Introduction,[0],[0]
Consider interpreting the instructions: Take the book and put it on the shelf.,1 Introduction,[0],[0]
One critical element of being able to interpret this sentence is associating the referring expression the book with the corresponding object in the world.,1 Introduction,[0],[0]
Another important component of understanding the command above is translating the phrase on the shelf to a location in space.,1 Introduction,[0],[0]
We call such phrases spatial descriptors.,1 Introduction,[0],[0]
"While spatial descriptors are closely related to referring expressions, they are distinct in that they can refer to locations even when there is nothing there.",1 Introduction,[0],[0]
"An intuitive way to model this is to reason over spatial regions as first-class entities, rather than taking an object-centric approach.
",1 Introduction,[0],[0]
"Following a long tradition of using game environments for AI, we adopt Minecraft as the setting for our work.",1 Introduction,[0],[0]
"Minecraft has previously been used
for work on planning and navigation (Oh et al., 2016; Tessler et al., 2016), and we expand on this by using it for grounded language understanding.",1 Introduction,[0.9999999712115778],"['Minecraft has previously been used for work on planning and navigation (Oh et al., 2016; Tessler et al., 2016), and we expand on this by using it for grounded language understanding.']"
"As a sandbox game, it can be used to construct a wide variety of environments that capture many interesting aspects of the real world.",1 Introduction,[1.0],"['As a sandbox game, it can be used to construct a wide variety of environments that capture many interesting aspects of the real world.']"
"At the same time, it is easy to extract machine-interpretable representations from the game.
",1 Introduction,[0],[0]
"We construct a dataset of Minecraft scenes with natural-language annotations, and propose a task that evaluates understanding spatial descriptors.",1 Introduction,[0],[0]
"Our task is formulated in terms of locating a pink, cube-shaped character named Misty given a scene, a natural language description, and a set of locations to choose from.",1 Introduction,[1.0],"['Our task is formulated in terms of locating a pink, cube-shaped character named Misty given a scene, a natural language description, and a set of locations to choose from.']"
An example from our dataset is shown in Figure 1.,1 Introduction,[0],[0]
"The Minecraft scene representation does not provide ground-truth information about object identity or segmentation, reflecting the fact that perceptual ambiguity is always
157
present in real-world scenarios.",1 Introduction,[0],[0]
"We do, however, assume the availability of 3D depth information (which, for real-world conditions, can be acquired using depth sensors such as RGBD cameras or LiDAR).
",1 Introduction,[0],[0]
We propose and evaluate a neural network that combines convolutional layers operating over 3D regions in space with recurrent layers for processing language.,1 Introduction,[0],[0]
"Our model jointly learns to segment objects, associate them with words, and understand spatial relationships – all in an end-to-end manner.",1 Introduction,[0],[0]
"We compare with a strong neural baseline and demonstrate a relative error reduction of 32%.
",1 Introduction,[0],[0]
The dataset and model described in this paper are available online.1,1 Introduction,[0],[0]
Our task includes some of the same elements as referring-expression generation and interpretation.,2 Related Work,[0],[0]
"Past work on these tasks includes Golland et al. (2010), Krishnamurthy and Kollar (2013), Socher et al. (2014) and Kazemzadeh et al. (2014).",2 Related Work,[0],[0]
"A key difference is that spatial descriptors (as modeled in this paper) refer to locations in space, rather than to objects alone.",2 Related Work,[0],[0]
"For example, Krishnamurthy and Kollar (2013) convert natural language to a logical form that is matched against image segments, an approach that is only capable of reasoning about objects already present in the scene (and not skipped over by the segmentation process).",2 Related Work,[0],[0]
"Our model’s ability to reason over spatial regions also differentiates it from past approaches to tasks beyond referring expressions, such as the work by Tellex et al. (2011) on natural-language commanding of robots.",2 Related Work,[0],[0]
"Recent work by Hu et al. (2016) on interpreting referring expressions can capture relationships between objects, relying on the construction of (subject, object, relation) tuples.",2 Related Work,[0],[0]
Their model is limited in that it can only handle one such tuple per utterance.,2 Related Work,[0],[0]
"Our model does not have such a restriction, and it additionally expands to a 3D setting.
",2 Related Work,[0],[0]
"Our task is also related to work on Visual Question Answering, or VQA (Agrawal et al., 2015).",2 Related Work,[0],[0]
"While VQA uses free-form textual answers, our task places targeted emphasis on spatial reasoning by requiring outputs to be locations in the scene.",2 Related Work,[0],[0]
"Spatial reasoning remains an important capability for VQA systems, and is one of the elements featured in CLEVR (Johnson et al., 2016), a di-
1https://github.com/nikitakit/voxelworld
agnostic dataset for VQA.",2 Related Work,[0],[0]
"Like in our dataset, visual percepts in CLEVR are based on machinegenerated scenes.",2 Related Work,[0],[0]
"CLEVR also makes use of machine-generated language, while all language in our dataset is written by humans.
",2 Related Work,[0],[0]
"Another related task in NLP is spatial role labeling, which includes the identification of spatial descriptors and the assigning of roles to each of their constituent words.",2 Related Work,[0],[0]
"This task was studied by Kordjamshidi et al. (2011) and led to the creation of shared tasks such as SpaceEval (Pustejovsky et al., 2015).",2 Related Work,[0],[0]
"Our setting differs in that we consider grounded environments instead of studying text in isolation, and evaluate on task performance rather than logical correctness of interpretation.
",2 Related Work,[0],[0]
Spatial descriptors are also present in the task of generating 3D scenes given natural language descriptions.,2 Related Work,[0],[0]
"Compared to a recent model by Chang et al. (2017) for scene generation, our model works with lower-level 3D percepts rather than libraries of segmented and tagged objects.",2 Related Work,[0],[0]
"We are also able to incorporate learning of vocabulary, perception, and linguistic structure into a single neural network that is trainable end-to-end.",2 Related Work,[0.957096620587511],"['This shows that our model is capable of representing compositional objects, and can learn to do so in an end-to-end manner.']"
"At its core, the ability to understand spatial descriptors can be formulated as mapping from a natural-language description to a particular location in space.",3 Task,[0],[0]
"In Figure 1, we show an instance of our task, which consists of the following components:
• W : a perceptual representation of the world • x: the natural language description • {y1, y2, . . .",3 Task,[0.9967909956537323],"['In Figure 1, we show an instance of our task, which consists of the following components: • W : a perceptual representation of the world • x: the natural language description • {y1, y2, .']"
", yn}: the candidate set of loca-
tions that are under consideration
• y?: the true location that is being referred to in the scene
Given W and x, a model must select which candidate location yi best matches the description x.
We will address the particulars of the above representation as we discuss the process for constructing our dataset.",3 Task,[0],[0]
"Each example (W, x, {y1, . . .",3 Task,[0],[0]
", yn}, y?) in the dataset is made by generating a Minecraft scene (Section 3.1) and selecting a location as the target of description (Section 3.2).",3 Task,[0],[0]
We then crowdsource natural language descriptions of the target location in space.,3 Task,[0],[0]
"To better anchor the language, we populate
the target location with a cube-shaped character we name Misty, and ask workers to describe Misty’s location (Section 3.3).",3 Task,[1.0000000780396014],"['To better anchor the language, we populate the target location with a cube-shaped character we name Misty, and ask workers to describe Misty’s location (Section 3.3).']"
We repeat this process for each example in the dataset.,3 Task,[0],[0]
Each of our Minecraft scenes is set in a randomlygenerated room.,3.1 Scene Generation and Representation,[0],[0]
"We select a random size for this room, and then populate it with a variety of objects.",3.1 Scene Generation and Representation,[0],[0]
"We include objects that can be placed on the floor (e.g. tables), mounted on the wall (e.g. torches), embedded in the wall (e.g. doors), or hanging from the ceiling (e.g. cobwebs).
",3.1 Scene Generation and Representation,[0],[0]
We then discard ground-truth knowledge about object segmentation or identity in the process of saving our dataset.,3.1 Scene Generation and Representation,[0],[0]
"This allows our task to evaluate not only models’ capacity for understanding language, but also their ability to integrate with perceptual systems.",3.1 Scene Generation and Representation,[0],[0]
"One way of approximating real-world observations would be to take a screenshot of the scene – however, a 2D projection does not provide all of the spatial information that a language user would reasonably have access to.",3.1 Scene Generation and Representation,[0],[0]
"We would like to use a 3D encoding instead, and Minecraft naturally offers a low-level (albeit low-resolution) voxel-based representation that we adopt for this work.
",3.1 Scene Generation and Representation,[0],[0]
"Each Minecraft world W is encoded as a 3D grid of voxels, where a voxel may be empty or contain a particular type of “block,” e.g. stone or wood.",3.1 Scene Generation and Representation,[1.0],"['Each Minecraft world W is encoded as a 3D grid of voxels, where a voxel may be empty or contain a particular type of “block,” e.g. stone or wood.']"
"In general, what humans would interpret as single objects will be made of multiple Minecraft blocks – for example, the table in Figure 1 consists of a “wooden pressure plate” block on top of a “wooden fencepost” block.",3.1 Scene Generation and Representation,[1.0],"['In general, what humans would interpret as single objects will be made of multiple Minecraft blocks – for example, the table in Figure 1 consists of a “wooden pressure plate” block on top of a “wooden fencepost” block.']"
"These same blocks can be used for other purposes as well: the “wooden fencepost” block is also part of fences, lamp-posts, and pillars, while the “wooden pressure plate” block can form shelves, countertops, as well as being placed on the ground to detect when something walks over it.",3.1 Scene Generation and Representation,[0],[0]
"We construct our Minecraft scenes specifically to include examples of such re-use, so that models capable of achieving high performance on this task must demonstrate the capacity to work without ground-truth segmentation or perfect object labeling.
",3.1 Scene Generation and Representation,[0],[0]
The voxel-grid 3D representation is not specific to the virtual Minecraft setting: it is equally applicable to real-world data where depth information is available.,3.1 Scene Generation and Representation,[0],[0]
"The main difference is that each voxel would need to be associated with a fea-
ture vector rather than a block type.",3.1 Scene Generation and Representation,[0],[0]
"One use of such a representation is in Maturana and Scherer (2015)’s work on object classification from data collected with RGBD cameras and LiDAR, which uses a 3D convolutional neural network over a voxel grid.",3.1 Scene Generation and Representation,[0],[0]
"We do not explicitly handle occlusion in this work, but we imagine that real-world extensions can approach it using a combination of multi-viewpoint synthesis, occlusion-aware voxel embeddings, and restricting the set of voxels considered by the model.",3.1 Scene Generation and Representation,[0],[0]
"After constructing a scene with representation W , we proceed to sample a location y?",3.2 Location Sampling,[0],[0]
in the scene.,3.2 Location Sampling,[0],[0]
"Given our voxel-based scene representation, our location sampling is at voxel granularity.",3.2 Location Sampling,[0],[0]
"The candidate set we sample from, {y1, . . .",3.2 Location Sampling,[0],[0]
", yn}, consists of empty voxels in the scene.",3.2 Location Sampling,[0],[0]
"Locations that occur in the middle of a large section of empty space are hard to distinguish visually and to describe precisely, so we require that each candidate yi be adjacent to at least one object.",3.2 Location Sampling,[0],[0]
"For each scene-location pair (W, y?)",3.3 Natural Language Descriptions,[0],[0]
"we crowdsource a natural language description x.
The choice of prompt for human annotators is important in eliciting good descriptions.",3.3 Natural Language Descriptions,[0],[0]
"At the location we are asking workers to refer to, we insert a pink-colored cube that we personify and name “Misty.”",3.3 Natural Language Descriptions,[0],[0]
We then ask workers to describe Misty’s location such that someone can find her if she were to turn invisible.,3.3 Natural Language Descriptions,[0],[0]
"Having a visually salient target helps anchor human perception, which is why we chose a pink color that contrasts with other visual elements in the scene.",3.3 Natural Language Descriptions,[0],[0]
"We make sure to emphasize the name “Misty” in the instructions, which results in workers almost always referring to Misty by name or with the pronoun she.",3.3 Natural Language Descriptions,[0],[0]
"This avoids having to disambiguate a myriad of generic descriptions (the pink block, the block, the target, etc.) for what is fundamentally an artificial construct.
",3.3 Natural Language Descriptions,[0],[0]
"To make sure that humans understand the 3D structure of the scene as they describe it, we give them access to a 3D view of the environment and require that they move the camera before submitting a description.",3.3 Natural Language Descriptions,[0],[0]
"This helped increase the quality of our data.
",3.3 Natural Language Descriptions,[0],[0]
"just torch
under thetable andMisty",3.3 Natural Language Descriptions,[0],[0]
We next present our model for this task.,4 Model,[0],[0]
"Our model architecture is shown in Figure 2, with some of the quantities it operates over highlighted in Figure 3.",4 Model,[0],[0]
"Throughout this section, we will use the example description Misty is to the right of the table and just under the torch.",4 Model,[0],[0]
"Note that while the accompanying scene illustrations are shown in 2D for visual clarity, our actual model operates in 3D and on larger scene sizes.
",4 Model,[0],[0]
Our model first associates words with regions in the world.,4 Model,[0],[0]
"There is no notion of object segmentation in the dataset, so the references it produces are just activations over space given a word.",4 Model,[0],[0]
"Activations are computed for all words in the sentence, though they will only be meaningful for words such as table and torch (Figure 3a).",4 Model,[0],[0]
"Our model next determines the spatial relationships between referenced objects and Misty, using information provided by context words such as right and under.",4 Model,[0],[0]
These relationships are represented as 3D convolutional offset filters (Figure 3b).,4 Model,[0],[0]
"For each word, its reference and offset filter are convolved to get a localization, i.e. an estimate of Misty’s location (Figure 3c).",4 Model,[0],[0]
"Finally, our model aggregates localizations across all words in the sentence, combining the information provided by the phrases to the right of the table and just under the torch (Figure 3e).
",4 Model,[0],[0]
"The following sections describe in more detail how references (Section 4.1), offsets (Section 4.2),
and localizations (Section 4.3) are computed.",4 Model,[0],[0]
The first component of our model is responsible for associating words with the voxels that they refer to.,4.1 Input and References,[0],[0]
"It assigns a real-valued score s(xt, y) to each pair consisting of word xt and voxel coordinate y.
High scores correspond to high compatibility; for any given word, we can visualize the set s(xt, ·) of scores assigned to different voxels by interpreting it as logits that encode a probability distribution over blocks in the scene.",4.1 Input and References,[0],[0]
"In the example, the word table would ideally be matched to the uniform reference distribution over blocks that are part of a table, and similarly for the word torch (Figure 3a).
",4.1 Input and References,[0],[0]
The word-voxel scores are computed by combining word and block embeddings.,4.1 Input and References,[1.0],['The word-voxel scores are computed by combining word and block embeddings.']
"To take advantage of additional unsupervised language and world data, we start with pretrained word embeddings and context-aware location embeddings f(W, y).",4.1 Input and References,[0],[0]
The function f consists of the first two layers of a convolutional neural network that is pretrained on the task of predicting a voxel’s identity given the 5x5x5 neighborhood around it.,4.1 Input and References,[0],[0]
"Since f fails to take into account the actual voxel’s identity, we add additional embeddings V that only consider single blocks.",4.1 Input and References,[0],[0]
"The score is then computed as s(xt, y) =",4.1 Input and References,[0],[0]
"w>t Af(W, y) +",4.1 Input and References,[0],[0]
"w>t vy, where wt is the word embedding and vy is the single-block embedding.",4.1 Input and References,[0],[0]
"The parameter matrix
A and the single-block embedding matrix V are trained end-to-end with the rest of the model.
",4.1 Input and References,[0],[0]
References are computed for all words in the sentence – including function words like to or the.,4.1 Input and References,[0],[0]
"To signify that a word does not refer to any objects in the world, the next layer of the network expects that we output a uniform distribution over all voxels.",4.1 Input and References,[0],[0]
"Outputting uniform distributions also serves as a good initialization for our model, so we set the elements of A and V to zero at the start of training (our pretrained word embeddings are sufficient to break symmetry).",4.1 Input and References,[0],[0]
The per-word references described in Section 4.1 do not themselves indicate Misty’s location.,4.2 Offsets,[0],[0]
"Rather, they are used in a spatial descriptor like to the right of the table.",4.2 Offsets,[0],[0]
"For every word, our model outputs a distribution over offset vectors that is used to redistribute scores from object locations to possible locations for Misty (Figure 3b).",4.2 Offsets,[0],[0]
"For example, if probability mass is placed on the “oneblock-to-the-right” offset vector, this corresponds to predicting that Misty will be one block to the right of the voxels that a word refers to.",4.2 Offsets,[0],[0]
"Offset scores ot are assigned based on the context the word xt occurs in, which allows the model to incorporate information from words such as right or under in its decisions.",4.2 Offsets,[0],[0]
"This is accomplished by running a bidirectional LSTM over the embeddings wt of the words in the sentence, and using its output to compute offset probabilities:
",4.2 Offsets,[0],[0]
"[z0, z1, . . .]",4.2 Offsets,[0],[0]
"= BiLSTM([w0, w1, . . .]) o′t",4.2 Offsets,[0],[0]
"= Mzt
ot(i) ∝",4.2 Offsets,[0],[0]
exp ( o′t(i) ),4.2 Offsets,[0],[0]
"Each set of offset scores ot is reshaped into a 3x3x3 convolutional filter, except that we structurally disallow assigning any probability to the no-offset vector in the center.",4.2 Offsets,[0],[0]
"As a parametertying technique, the trainable matrix M is not fullrank; we instead decompose it such that the logprobability of an offset vector factors additively over the components in a cylindrical coordinate system.",4.2 Offsets,[0],[0]
"For each word, the 3D tensor of word-voxel scores s(xt, ·) is convolved with the offset distribution ot to produce a distribution of localizations for Misty, dt(y).",4.3 Localizations and Output,[0],[0]
A 2D illustration of the result is shown in Figure 3c.,4.3 Localizations and Output,[0],[0]
"Localizations are then summed across all words in the sentence, resulting in a single score for each voxel in the scene (Figure 3e).",4.3 Localizations and Output,[0],[0]
"These scores are interpreted as logits corresponding to a probability distribution over possible locations for Misty:
dt(y) = s(xt, y) ∗",4.3 Localizations and Output,[0],[0]
ot p(y) ∝,4.3 Localizations and Output,[0],[0]
"exp {∑ t dt(y) }
Not all words will have localizations that provide information about Misty – for some words
the localizations will just be a uniform distribution.",4.3 Localizations and Output,[0],[0]
"We will refer to words that have low-entropy localizations as landmarks, with the understanding that being a landmark is actually a soft notion in our model.
",4.3 Localizations and Output,[0],[0]
"Our offset filters ot are much smaller than our voxel grid, which means that convolving any offset filter with a uniform reference distribution over the voxel grid will also result in a uniform localization distribution (edge effects are immaterial given the small filter size and the fact that Misty is generally not at the immediate edges of the scene).",4.3 Localizations and Output,[0],[0]
"Conversely, given non-uniform references almost any set of offsets will result in a non-uniform localization.",4.3 Localizations and Output,[0],[0]
"The architecture for computing references can output uniform references for function words (like to or the), but it lacks the linguistic context to determine when words refer to objects but should not be interpreted as landmarks (e.g. when they are part of exposition or a negated expression).",4.3 Localizations and Output,[0],[0]
We therefore include an additional not-a-landmark class that is softmax-normalized jointly with the offset vector distribution ot.,4.3 Localizations and Output,[0],[0]
"Probability assigned to this class subtracts from the probability mass for the true offset directions (and therefore from the localizations) – if this class receives a probability of 1, the corresponding localizations will not contribute to the model output.",4.3 Localizations and Output,[0],[0]
We use a softmax cross-entropy loss for training our model.,4.4 Loss and Training,[0],[0]
"During training, we find that it helps to not use the candidate set {y1, y2, . . .",4.4 Loss and Training,[0],[0]
", yn} and instead calculate a probability p(y) for all blocks in the scene, including solid blocks that cannot possibly contain Misty (perhaps because this penalizes inferring nonsensical spatial relationships).
",4.4 Loss and Training,[0],[0]
"We run the Adam optimizer (Kingma and Ba, 2014) with step size 0.001 for 100 epochs using batch size 10.",4.4 Loss and Training,[0],[0]
"We keep an exponential moving average of our trainable parameters, which we save every two epochs.",4.4 Loss and Training,[0],[0]
"We then select the saved model that has the highest performance on our development set.
",4.4 Loss and Training,[0],[0]
We perform several regularization and data augmentation techniques in order to achieve better generalization.,4.4 Loss and Training,[0],[0]
"Each time we sample a training example, we select a random 19x19x19 crop from the full scene (as long as Misty’s location is not cropped out).",4.4 Loss and Training,[0],[0]
"We also disallow using the contextbased block embeddings for the first 20 epochs by
holding the parameter matrix A described in Section 4.1 fixed at zero, forcing the model to first learn to associate vocabulary with local features and only later expand to capture the compositional aspects of the environment.
",4.4 Loss and Training,[0],[0]
"For the natural language descriptions, all tokens are converted to lowercase as part of preprocessing.",4.4 Loss and Training,[0],[0]
During training we apply word-level dropout (i.e. replacing words with an UNK token) in the LSTM responsible for computing offsets.,4.4 Loss and Training,[0],[0]
"In evaluating this task, we would like to use a metric that can provide meaningful comparison of our model with baseline and human performance.",5.1 Evaluation Metric,[0],[0]
"The set of all possible locations for Misty is large enough that it is hard even for a human to guess the correct block on the first try, especially when some descriptions are only precise to within 1 or 2 blocks.",5.1 Evaluation Metric,[0],[0]
"The size of this set also varies from scene to scene.
",5.1 Evaluation Metric,[0],[0]
"Therefore for our evaluation, we restrict the set {y1, . . .",5.1 Evaluation Metric,[0],[0]
", yn} to 6 possible locations: Misty’s true location and 5 distractors.",5.1 Evaluation Metric,[0],[0]
"This represents a less ambiguous problem that is much easier for humans, while also allowing for the evaluation of future models that may require an expensive computation for each candidate location considered.",5.1 Evaluation Metric,[0],[0]
Our procedure for selecting the distractors is designed to ensure that we test both local and global scene understanding.,5.1 Evaluation Metric,[0],[0]
Each set of six choices is constructed to consist of three clusters of two candidates each.,5.1 Evaluation Metric,[0],[0]
"Each cluster location is anchored to a landmark – we sample a landmark block adjacent to Misty and two additional landmark blocks from the entire scene, such that the pairwise distances between landmarks are at least 4 units.",5.1 Evaluation Metric,[0],[0]
We then sample one distractor near Misty’s landmark and two distractors near both of the other landmarks.,5.1 Evaluation Metric,[0],[0]
"To make our development and test sets, we construct this six-option variation from a subset of our collected data.",5.2 Dataset,[0],[0]
For each such example we crowdsource two human solutions using Mechanical Turk.,5.2 Dataset,[0],[0]
Examples where both humans answered correctly are partitioned into a development and a test set.,5.2 Dataset,[0],[0]
"This filtering procedure serves as our primary method of excluding confusing or uninformative descriptions from the evaluation con-
ditions.",5.2 Dataset,[0],[0]
We also collect a third human solution to each example in the development and test sets to get an independent estimate of human performance on our task.,5.2 Dataset,[0],[0]
"The final dataset consists of 2321 training examples, 120 dev set examples, and 200 test set examples.
",5.2 Dataset,[0],[0]
The natural-language descriptions across the full dataset use a vocabulary of 1015 distinct tokens (case-insensitive but including punctuation).,5.2 Dataset,[0],[0]
"The average description length is 19.02 tokens, with a standard deviation of 10.00 tokens.",5.2 Dataset,[0],[0]
"The large spread partially reflects the fact that some people gave short descriptions that referenced a few landmarks, while others gave sequences of instructions on how to find Misty.",5.2 Dataset,[0],[0]
"As a point of comparison, the ReferIt dataset (Kazemzadeh et al., 2014) has a larger vocabulary of 9124 tokens, but a shorter average description length of 3.52 tokens (with a standard deviation of 2.67 tokens).
",5.2 Dataset,[0],[0]
A random sampling of descriptions from our dataset is shown in Table 1.,5.2 Dataset,[0],[0]
Quantitative results are shown in Table 2.,5.3 Quantitative Results,[0],[0]
Our evaluation metric is constructed such that there is an easily interpretable random baseline.,5.3 Quantitative Results,[0],[0]
We also evaluate a strong neural baseline that uses an approach we call Seq2Emb.,5.3 Quantitative Results,[0],[0]
"This baseline converts the sentence into a vector using a bidirectional LSTM encoder, and also assigns vector embeddings to each voxel using a two-layer convolutional neural network.",5.3 Quantitative Results,[0],[0]
"The voxel with an embedding that most closely matches the sentence embedding is chosen as the answer.
",5.3 Quantitative Results,[0],[0]
Our model achieves noticeable gains over the baseline approaches.,5.3 Quantitative Results,[0],[0]
"At the same time, there remains a gap between our model and individual human performance.",5.3 Quantitative Results,[1.0],"['At the same time, there remains a gap between our model and individual human performance.']"
"We see this as an indication that we have constructed a task with appropriate difficulty: it is approachable by building on the current state-of-the-art in machine learning and NLP, while presenting challenges that can motivate continued work on understanding language and how it
relates to descriptions of the world.",5.3 Quantitative Results,[0],[0]
We next conduct an ablation study to evaluate the contribution of the individual elements in our model.,5.4 Ablation Study,[0],[0]
"Our ablation results on the development set are shown in Table 3.
",5.4 Ablation Study,[0],[0]
"In our first ablation, we remove the compositional block embeddings that make use of multiple blocks.",5.4 Ablation Study,[0],[0]
"The resulting performance drop of 2.5% reflects the fact that our model uses multi-block information to match words with objects.
",5.4 Ablation Study,[0],[0]
We next replace the LSTM in our full model with a 3-word-wide convolutional layer.,5.4 Ablation Study,[0],[0]
"A single word of left- and right-context provides limited ability to incorporate spatial descriptor words like left and right, or to distinguish landmarks used to locate Misty from words providing exposition about the scene.",5.4 Ablation Study,[0],[0]
"This ablation solves 5% fewer examples than our full model, reflecting our LSTM’s ability to capture such phenomena.
",5.4 Ablation Study,[0],[0]
"Finally, we try holding the distribution over offset vectors fixed, by making it a trainable variable rather than a function of the language.",5.4 Ablation Study,[0],[0]
"This corresponds to enforcing the use of only one spatial
Misty is floating in the middle of the room.",5.4 Ablation Study,[0],[0]
"She in the upper half of the room, between the two poles.
",5.4 Ablation Study,[0],[0]
Figure 4: Reference distribution representing our model’s belief of which blocks the word poles refers to.,5.4 Ablation Study,[0],[0]
"Our model assigns the majority of the probability mass to the poles, while ignoring a table leg that is made of the same block type.",5.4 Ablation Study,[0],[0]
"Note that the seven numbers overlaid on top account for more than 99% of the total probability mass, and that each of the remaining blocks in the scene has a probability of at most 0.025%.
operator that roughly means ‘near.’",5.4 Ablation Study,[0],[0]
"We retain the LSTM for the sole purpose of assigning a score to the not-a-landmark class, meaning that contextual information is still incorporated in the decision of whether to classify a word as a landmark or not.",5.4 Ablation Study,[0],[0]
"The resulting accuracy is 5.8% lower than our full model, which makes this the worst-performing of our ablations.",5.4 Ablation Study,[0],[0]
These results suggest that the ability to infer spatial directions is important to our model’s overall performance.,5.4 Ablation Study,[0],[0]
"The modular design of our model allows us to examine the individual behavior of each component in the network, which we explore in this section.
",5.5 Qualitative Examination,[0],[0]
We find that our algorithm is able to learn to associate words with the corresponding voxels in the world.,5.5 Qualitative Examination,[0],[0]
"Figure 4 shows the reference distribution associated with the word poles, which is constructed by applying a softmax operation to the word-voxel scores for that word.",5.5 Qualitative Examination,[0],[0]
Our algorithm is able to correctly segment out the voxels that are a part of the pole.,5.5 Qualitative Examination,[0],[0]
"Moreover, the table on the right side of the scene has a table leg made of the same block type as the pole – and yet, it is is assigned a low probability.",5.5 Qualitative Examination,[0],[0]
"This shows that our model is capable of representing compositional objects, and can learn to do so in an end-to-end manner.
",5.5 Qualitative Examination,[0],[0]
We next examine the offset distributions computed by our model.,5.5 Qualitative Examination,[0],[0]
Consider the scene and description shown in Figure 5a.,5.5 Qualitative Examination,[0],[0]
"The offset vector distribution at the word platform, shown in Figure 5b, shows that the model assigns high proba-
Misty is between the wall and the flowers that are close to the corner.
",5.5 Qualitative Examination,[0],[0]
Figure 6: Our algorithm interprets this sentence as Misty is near the wall and the flowers and close to the corner.,5.5 Qualitative Examination,[0],[0]
"This intersective interpretation is sufficient to correctly guess Misty’s location in this scene (as well as others in the dataset).
",5.5 Qualitative Examination,[1.0000000306536032],['This intersective interpretation is sufficient to correctly guess Misty’s location in this scene (as well as others in the dataset).']
bility to Misty being above the platform.,5.5 Qualitative Examination,[1.0],['bility to Misty being above the platform.']
"In Figure 5c, we show the effects of replacing the phrase right above with the words in front of.",5.5 Qualitative Examination,[0],[0]
This example illustrates our model’s capacity for learning spatial directions.,5.5 Qualitative Examination,[1.0],['This example illustrates our model’s capacity for learning spatial directions.']
"We note that the offset distribution given the phrase in front of is not as peaked as it is for right above, and that distributions for descriptions saying left or right are even less peaked (and are mostly uniform on the horizontal plane).",5.5 Qualitative Examination,[0],[0]
One explanation for this is the ambiguity between speaker-centric and object-centric reference frames.,5.5 Qualitative Examination,[0],[0]
"The reference frame of our convolutional filters is the same as the initial camera frame for our our annotators, but this may not be the true speaker-centric frame because we mandate that annotators move the camera before submitting a description.
",5.5 Qualitative Examination,[0],[0]
We next highlight our model’s ability to incorporate multiple landmarks in making its decisions.,5.5 Qualitative Examination,[0],[0]
Consider the scene and description shown in Figure 6.,5.5 Qualitative Examination,[0],[0]
"The room has four walls, two flowers, and four corners – no single landmark is sufficient to correctly guess Misty’s location.",5.5 Qualitative Examination,[0],[0]
"Our model is able to localize the flowers, walls, and corners in this scene and intersect them to locate Misty.",5.5 Qualitative Examination,[0],[0]
"Strictly speaking, this approach is not logically equivalent to applying a two-argument between operator and recognizing the role of that as a relativizer.",5.5 Qualitative Examination,[0],[0]
"This is a limitation of our specific model, but the general approach of manipulating spatial region masks need not be constrained in this way.",5.5 Qualitative Examination,[0],[0]
It would be possible to introduce operations into the neural network to model recursive structure in the language.,5.5 Qualitative Examination,[0],[0]
"In practice, however, we find that the intersective interpretation suffices for many of the descriptions that occur in our dataset.",5.5 Qualitative Examination,[0],[0]
"In this paper, we define the task of interpreting spatial descriptors, construct a new dataset based on Minecraft, and propose a model for this task.",6 Conclusion,[0],[0]
We show that convolutional neural networks can be used to reason about regions in space as firstclass entities.,6 Conclusion,[1.0],['We show that convolutional neural networks can be used to reason about regions in space as firstclass entities.']
"This approach is trainable end-toend while also having interpretable values at the intermediate stages of the neural network.
",6 Conclusion,[0],[0]
"Our architecture handles many of the linguistic phenomena needed to solve this task, including object references and spatial regions.",6 Conclusion,[0],[0]
"However, there is more work to be done before we can say that the network completely understands the sentences that it reads.",6 Conclusion,[0],[0]
Our dataset can be used to investigate future models that expand to handle relativization and other recursive phenomena in language.,6 Conclusion,[0],[0]
"We thank the many workers on Mechanical Turk who contributed to the creation of our dataset.
",Acknowledgments,[0],[0]
"This work was made possible by the open source tooling developed around and inspired by Minecraft; in particular we would like to thank the developers of the voxel.js project and associated plugins, as well as the developers of mcedit2.
",Acknowledgments,[0],[0]
Nikita Kitaev is supported by an NSF Graduate Research Fellowship.,Acknowledgments,[0],[0]
This research was supported by DARPA through the XAI program.,Acknowledgments,[0],[0]
We present a model for locating regions in space based on natural language descriptions.,abstractText,[0],[0]
"Starting with a 3D scene and a sentence, our model is able to associate words in the sentence with regions in the scene, interpret relations such as on top of or next to, and finally locate the region described in the sentence.",abstractText,[0],[0]
All components form a single neural network that is trained end-to-end without prior knowledge of object segmentation.,abstractText,[0],[0]
"To evaluate our model, we construct and release a new dataset consisting of Minecraft scenes with crowdsourced natural language descriptions.",abstractText,[0],[0]
We achieve a 32% relative error reduction compared to a strong neural baseline.,abstractText,[0],[0]
Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space,title,[0],[0]
