29	1	We also find that dDMR tends to converge in many fewer iterations than LDA, and also does not suffer from tuning difficulties that DMR encounters when applied to high-dimensional document features.
33	1	We use neural networks since they are expressive, generalize well to unseen data, and can be jointly trained using straightforward gradient ascent with back-propagation.
38	1	For each topic k, generate word distribution: (a) φ̃k = exp(ωbias) (b) φk ∼ Dirichlet(φ̃k) 5.
48	1	ψ is the digamma function (derivative of the log-gamma function), nm is the number of tokens in document m, and nm,k is the count of how many tokens topic k was assigned to in document m. δL δθ̃m,k = ψ( K∑ k=1 θ̃m,k)− ψ( K∑ k=1 θ̃m,k + nm) +ψ(θ̃m,k + nm,k)− ψ(θ̃m,k) (1)
56	1	Articles contained a mean of 2.1 tags, with 738 articles not containing any of these tags.
67	1	Using these features as supervision to dDMR is similar to fine-tuning a pre-trained CNN to predict a new set of labels.
70	3	A standard stop list was used to remove frequent function words and we restricted the vocabulary to the 30,000 most frequent types.
82	1	For the architecture of the dDMR model we used single-hidden-layer multi-layer perceptrons (MLPs), with rectified linear unit (ReLU) activations on the hidden layer, and linear activation on the output layer.
96	1	Evaluation Each model was evaluated according to heldout perplexity, topic coherence by normalized pointwise mutual information (NPMI) (Lau et al., 2014), and a dataset-specific predictive task.
103	1	For New York Times articles we predicted 10 of the 200 most frequent descriptor tags restricting to articles with exactly one of these descriptors.
125	1	It is also worth noting that finding a lowdimensional linear projection of the supervision features with PCA does not improve model fit as well as dDMR.
135	1	dDMR performance is also insensitive to training parameters relative to DMR.
146	1	To do so, we computed and normalized the prior document distribution for a sample of documents for lowest perplexity DMR and dDMR Z = 200 topic models: p(k|m) = θ̃m∑Z k=1 θ̃m,k , the prior probability of sampling topic k, conditioned on the features for document m. We then marginalize over topics to yield the conditional probability of a word w given document m: p(w|m) = ∑Zk=1 p(w|k)p(k|m).
148	1	We find that dDMR identifies words likely to appear in a review of the product pictured.
158	1	Prior work showed that upstream supervised topic models, such as DMR, learn topic distributions that are effective at downstream prediction tasks (Benton et al., 2016).
182	5	We demonstrate the flexibility of our model on three corpora with different types of metadata: topic descriptor tags, images, and subreddit IDs.
183	1	dDMR is better able to fit text corpora with high-dimensional supervision compared to LDA or DMR.
184	10	Furthermore, we find that document supervision greatly reduces the number of Gibbs sampling iterations for a topic model to converge, and that the dDMR prior architecture makes it more robust to training parameters than DMR.
185	156	We also find that the topic distributions learned by dDMR are more predictive of external document labels such as known topic tags or product category as the number of topics grows and that dDMR topics are judged as more representative of the document metadata by human subjects.
186	156	Source code for training dDMR can be found at http://www.github.com/abenton/deep-dmr.
