1	50	Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph.
16	28	The first stage identifies the concepts evoked by words and phrases in an input sentence w = 〈w1, .
21	17	The relation identification stage (§4) is similar to a graph-based dependency parser.
29	27	In §7, we evaluate the parser against goldstandard annotated sentences from the AMR Bank corpus (Banarescu et al., 2013) under the Smatch score (Cai and Knight, 2013), presenting the first published results on automatic AMR parsing.
31	37	These graph fragments often consist of just one labeled concept node, but in some cases they are larger graphs with multiple nodes and edges.3 Concept identification is illustrated in Figure 2 using our running example, “The boy wants to visit New York City.” Let the concept lexicon be a mapping clex : W ∗ → 2F that provides candidate graph fragments for sequences of words.
42	52	Let S(i) denote the score of the best labeling of the first i words of the sentence, w0:i; it can be calculated using the recurrence: S(0) = 0 S(i) = max j:0≤j<i, c∈clex(wj:i)∪∅ { S(j) + θ>f(wj:i, j, i, c) } The best score will be S(n), and the best scoring concept labeling can be recovered using backpointers, as in typical implementations of the Viterbi algorithm.
44	20	When clex is called with a sequence of words, it looks up the sequence in a table that contains, for every word sequence that was labeled with a concept fragment in the training data, the set of concept fragments it was labeled with.
48	47	The relation identification stage adds edges among the concept subgraph fragments identified in the first stage (§3), creating a graph.
50	82	Consider the fully dense labeled multigraph D = 〈VD, ED〉 that includes the union of all labeled vertices and labeled edges in the concept graph fragments, as well as every possible labeled edge u `−→ v, for all u, v ∈ VD and every ` ∈ LE .4 We require a subgraph G = 〈VG, EG〉 that respects the following constraints: 1.
56	30	Deterministic: For each node u ∈ VG, and for each label ` ∈ L∗E , there is at most one outgoing edge in EG from u with label `.
65	14	The steps for constructing a maximum preserving, simple, spanning, connected (but not necessarily deterministic) subgraph are as follows.
66	16	These steps ensure the resulting graph G satisfies the constraints: the initialization step ensures the preserving constraint is satisfied, the pre-processing step ensures the graph is simple, and the core algorithm ensures the graph is connected.
71	67	(Pre-processing) We form the edge set E by including just one edge from ED between each pair of nodes: • For any edge e = u `−→ v in E(0), include e in E, omitting all other edges between u and v. • For any two nodes u and v, include only the highest scoring edge between u and v. Note that without the deterministic constraint, we have no constraints that depend on the label of an edge, nor its direction.
72	28	So it is clear that the edges omitted in this step could not be part of the maximum-scoring solution, as they could be replaced by a higher scoring edge without violating any constraints.
80	65	In a nutshell, MSCG first adds all positive edges to the graph, and then connects the graph by greedily adding the least negative edge that connects two previously unconnected components.
81	21	MSCG finds a maximum spanning, connected subgraph of 〈V,E〉 Proof.
83	42	We first show by induction that, at every iteration of MSCG, there exists some maximum spanning, connected subgraph that contains G(i) = 〈V,E(i)〉: input : weighted, connected graph 〈V,E〉 and set of edges E(0) ⊆ E to be preserved output: maximum spanning, connected subgraph of 〈V,E〉 that preserves E(0) let E(1) = E(0) ∪ {e ∈ E | ψ>g(e) > 0}; create a priority queue Q containing {e ∈ E | ψ>g(e) ≤ 0} prioritized by scores; i = 1; while Q nonempty and 〈V,E(i)〉 is not yet spanning and connected do i = i+ 1; E(i) = E(i−1); e = arg maxe′∈Qψ>g(e′); remove e from Q; if e connects two previously unconnected components of 〈V,E(i)〉 then add e to E(i) end end return G = 〈V,E(i)〉; Algorithm 1: MSCG algorithm.
96	20	The maximum spanning connected subgraph M that contains it cannot have a higher score, because G contains every positive edge.
97	23	If the subgraph resulting from MSCG satisfies constraint 4 (deterministic) then we are done.
100	13	In our case, we begin by encoding a graph G = 〈VG, EG〉 as a binary vector.
104	44	For example, the constraint that vertex u has no more than one outgoing ARG0 can be encoded with the inequality:∑ v∈V 1{u ARG0−−−→ v ∈ EG} = ∑ v∈V z u ARG0−−−→v ≤ 1.
105	25	All of the determinism constraints can collectively be encoded as one system of inequalities: Az ≤ b, with each row Ai inA and its corresponding entry bi in b together encoding one constraint.
108	77	To handle the constraint Az ≤ b, we introduce multipliers µ ≥ 0 to get the Lagrangian relaxation of the objective function: Lµ(z) = maxz (φ>z + µ>(b−Az)), z∗µ = arg maxz Lµ(z).
109	20	And the dual objective: L(z) = min µ≥0 Lµ(z), z∗ = arg maxz L(z).
111	59	So for any µ, we can find z∗µ by assigning edges the new Lagrangian adjusted weights φ − A>µ and reapplying the algorithm described in §4.1.
114	58	L(z) is an upper bound on the unrelaxed objective function φ>z, and is equal to it if and only if the constraints Az ≤ b are satisfied.
115	33	If L(z∗) = φ>z∗, then z∗ is also the optimal solution to the constrained solution.
117	37	In that case we still return the subgraph encoded by z∗, even though it might violate one or more constraints.
119	29	In our experiments, with a stepsize of 1 and max number of steps as 500, Lagrangian relaxation succeeds 100% of the time in our data.
123	34	Our system has two feature types for this edge: the concept it points to, and the shortest dependency path from a word in the span to the root of the dependency tree.
