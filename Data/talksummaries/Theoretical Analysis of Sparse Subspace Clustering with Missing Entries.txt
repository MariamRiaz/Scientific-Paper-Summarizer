3	18	Among a variety of subspace clustering methods (Vidal et al., 2016) including algebraic (Vidal et al., 2005; Tsakiris & Vidal, 2017b; 2018a), iterative (Bradley & Mangasarian, 2000), recursive (Fischler & Bolles, 1981; Tsakiris & Vidal, 2017a), and spectral (Aldroubi et al., 2017; Heckel & Bölcskei, 2015; Lu et al., 2012; Chen & Lerman, 2009) techniques, Sparse Subspace Clustering (SSC) (Elhamifar & Vidal, 2009; 2013) is one of the most popular methods.
5	18	In addition, SSC is able to cluster data from incomplete observations reasonably well (Yang et al., 2015), which is an important problem (Ongie et al., 2017; Pimentel-Alarcon & Nowak, 2016; Elhamifar, 2016; Yang et al., 2015; Heckel & Bölcskei, 2015; Pimentel-Alarcon et al., 2015; Eriksson et al., 2012; Recht, 2011; Balzano et al., 2010), since in many applications not all features are available for every data point: Users of recommendation systems only rate a few items, medical patients undergo a few tests and treatments, images are corrupted by occlusions, dynamic processes are observed across short time intervals and so on.
9	24	The works of Wang et al. (2016) and Charles et al. (2018) are important recent efforts towards understanding SSC with missing entries.
31	27	,X(n)]Γ ∈ RD×N (1) denote a data matrix as well as a set (formed by the columns of this matrix) of unit `2-norm points in the union of the linear subspaces Si, i ∈ [n], where X(i) = [x (i) 1 , .
47	34	, b (i) di is a basis for Si, then Ṡi is the subspace of RD spanned by the vectors P̄ (1)1 b (i) k ,∀k ∈ [di].
56	62	We begin by reviewing Sparse Subspace Clustering (SSC) for data with no corruptions (§2.1), as well as the two elementary approaches to SSC for incomplete data (§2.2), which this paper is devoted to analyzing.
60	72	Among a variety of methods (Vidal et al., 2016) for retrieving the clusters {X(i)}, one may apply Sparse Subspace Clustering (SSC) (Elhamifar & Vidal, 2009; 2013), whose main principle is to express each point in X as a sparse linear combination of other points in X .
62	24	x (1) 1 = X−1c, (14) and then form an affinity graph in which we connect x(1)1 to those points of X−1 that correspond to the support (nonzero coefficients) of the computed optimal solution of (14).
65	40	When this is true for the expression of each and every point in X , then the corresponding affinity graph contains no connections between points in different subspaces, i.e., it is a subspace preserving graph.
71	17	Recall the notation of Definition 1, and suppose that µλ < r and 1/ζ < λ.
72	29	(16) Then every optimal solution to the Lasso SSC problem (15) is non-zero and subspace preserving.
78	33	(17) Theorem 2 (SSC with uncorrupted data, probabilistic (Soltanolkotabi & Candès, 2012; Wang & Xu, 2016)).
80	27	If ρ is larger than a universal constant, λ > 1/α, and α > β, (18) then any optimal solution to the Lasso SSC problem (15) is non-zero and subspace preserving, with probability at least 1− 2/N2 − exp(−√ρd).
81	54	Condition (18) agrees with intuition, since it effectively says that the subspace preserving property is easier to achieve for small relative subspace dimensions d/D, fewer subspaces, and more points per subspace.
83	54	When the data are incomplete but otherwise uncorrupted, one may consider using a low-rank matrix completion algorithm to first complete the data and then apply SSC to the completed data.
85	19	As a simple alternative, one may fill with zeros the unobserved entries to obtain a zero-filled data matrix X̄ exactly as in Definition 1, and subsequently solve the problem min c,e ‖c‖1 + λ 2 ‖e‖22 s.t.
86	36	x̄ (1) 1 = X̄−1c + e, (19) a procedure called Zero-Filled SSC (ZF-SSC) (Yang et al., 2015).
87	104	In spite of its simplicity (after all we are just filling in the missing entries with zeros), as per Figs.
88	21	2(a) and 2(c) in Yang et al. (2015), ZF-SSC performs only slightly worse than low-rank matrix completion followed by SSC.
89	36	Even so, ZF-SSC has an evident shortcoming: it penalizes the reconstruction error of the zero vector along the unobserved part of the point being expressed, which is clearly an undesirable feature of the method.
90	19	More precisely, letting Ē(1)1 and Ẽ (1) 1 be, respectively, the observed and unobserved subspaces associated to point x(1)1 , and P̄ (1) 1 , P̃ (1) 1 the orthogonal projections onto them (see Definition 1), and recalling that (Ē(1)1 )⊥ = Ẽ (1) 1 , we have that x̄ (1) 1 = P̄ (1) 1 x̄ (1) 1 , and (20) X̄−1 = P̄ (1) 1 X̄−1 + P̃ (1) 1 X̄−1, (21) and so we can rewrite the objective function of ZF-SSC as ‖c‖1 + λ 2 ‖x̄(1)1 − X̄−1c‖ 2 2 = ‖c‖1+ (22) λ 2 ‖x̄(1)1 − P̄ (1) 1 X̄−1c‖ 2 2 + λ 2 ‖P̃ (1)1 X̄−1c‖ 2 2.
103	19	This is remarkable, because the projected and zero-filled data ˙̄X (see Definition 1 for notation) that PZF-SSC operates on contain more missing entries than the zero-filled data X̄ that ZF-SSC operates on.
106	16	Our main insight is the following observation: expressing point x̄(1)1 = ˙̄x (1) 1 as a sparse linear combination of ˙̄X−1, can be seen as ex- pressing the complete point x̄(1)1 from partial observations ˙̄X−1 of the complete points Ẋ−1, where now the underly- ing complete data Ẋ lie in the union of subspaces ⋃n i=1 Ṡi, i.e., the original subspaces projected onto the coordinate subspace defined by the observation pattern of the point being expressed (see Definition 1).
119	30	This is precisely the condition that Theorem 3 reduces to for complete data, which is a novel result itself: Theorem 4 (SSC with uncorrupted data, deterministic).
162	22	1(a)-1(b), the phase transition between subspace preserving solutions and non-subspace preserving ones is indeed of hyperbolic nature for both methods, as theoretically predicted by (29) and (33).
164	23	Third, both methods start breaking down rather quickly as the number of missing entries increases: for d = 10 PZFSSC and ZF-SSC can tolerate, respectively, at most 23 and 10 missing entries per point before their solutions become non-subspace preserving, while for d = 20 the maximal tolerable number of missing entries becomes 10 and 4, respectively.
177	18	Our study was solely in the context of SSC, yet we believe that working with PZF data instead of ZF data is advantageous regardless of the choice of self-expressive method (Liu et al., 2013; Lu et al., 2012; Elhamifar & Vidal, 2013; Wang et al., 2013; You et al., 2016); a conjecture to be established.
179	31	2(a), its rather poor subspace preserving accuracy shown in 2(b), suggests that PZF-SSC is still too simple a method to handle the subspace clustering problem for incomplete data, and that its performance relies to a significant extent on the robustness of spectral clustering.
180	28	2(a)-2(b), for three 60-dimensional subspaces inside R100 and 15 missing entries per point, about 40% of the points a point connects to live in different subspaces; yet the clustering accuracy is 99%.
182	27	Nevertheless, that approach comes with no theoretical guarantees and appears to be computationally burdensome, leaving as an open challenge the proposal of a theoretically sound, efficient and accurate algorithm for clustering incomplete data associated to a union of low-dimensional subspaces.
