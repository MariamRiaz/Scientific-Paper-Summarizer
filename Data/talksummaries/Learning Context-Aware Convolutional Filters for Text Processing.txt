0	60	In the last few years, convolutional neural networks (CNNs) have demonstrated remarkable progress in various natural language processing applications (Collobert et al., 2011), including sentence/document classification (Kim, 2014; Zhang et al., 2015; Wang et al., 2018), text sequence matching (Hu et al., 2014; Yin et al., 2016; Shen et al., 2017), generic text representations (Gan et al., 2016; Tang et al., 2018), language modeling (Dauphin et al., 2017), machine translation (Gehring et al., 2017) and abstractive sentence summarization (Gehring et al., 2017).
2	30	As an encoder network for text, CNNs typically convolve a set of filters, of window size n, with an inputsentence embedding matrix obtained via word2vec (Mikolov et al., 2013) or Glove (Pennington et al., 2014).
3	40	Different filter sizes n may be used within the same model, exploiting meaningful semantic features from different n-gram fragments.
4	27	The learned weights of CNN filters, in most cases, are assumed to be fixed regardless of the input text.
5	19	As a result, the rich contextual information inherent in natural language sequences may not be fully captured.
6	21	As demonstrated in Cohen and Singer (1999), the context of a word tends to greatly influence its contribution to the final supervised tasks.
7	41	This observation is consistent with the following intuition: when reading different types of documents, e.g., academic papers or newspaper articles, people tend to adopt distinct strategies for better and more effective understanding, leveraging the fact that the same words or phrases may have different meaning or imply different things, depending on context.
9	14	One common strategy is the attention mechanism, which is typically employed on top of a CNN (or Long ShortTerm Memory (LSTM)) layer to guide the extraction of semantic features.
16	14	In contrast to traditional CNNs, the convolution operation in our framework does not have a fixed set of filters, and thus provides the network with stronger modeling flexibility and capacity.
17	26	Specifically, we introduce a meta network to generate a set of contextaware filters, conditioned on specific input sentences; these filters are adaptively applied to either the same (Section 3.2) or different (Section 3.3) text sequences.
20	15	In this regard, we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context-aware representations.
51	14	Filter generation module Instead of utilizing fixed filter weightsW for different inputs (as (1)), our model generates a set of filters conditioned on the input sentence X .
61	43	Adaptive convolution module The adaptive convolution module takes as inputs the generated filters f and an input sentence.
63	29	With the sample-specific filters, the input sentence is adaptively encoded, again, via a basic CNN architecture as in Section 3.1, i.e., one convolutional and one pooling layer.
69	42	Note that the corresponding model can be readily adapted to other sentence matching problems as well (see Section 5.2).
84	80	The same process can be utilized to produce encodings za and filters fa for the answer input,A, with parameters θae and θ a d, respectively.
85	30	The two sets of filter weights are then passed to adaptive convolution modules, along with Q and A, to obtain the extracted question and answer embeddings.
91	19	[ha;hb] indicates that ha and hb are stacked as column vectors.
96	15	The adaptive context-aware filter generation mechanism proposed here bears close resemblance to attention mechanism (Yin et al., 2016; Bahdanau et al., 2015; Xiong et al., 2017) widely adopted in the NLP community, in the sense that both methods intend to incorporate rich contextual information into text representations.
98	28	By contrast, in our context-aware filter generation mechanism, the contextual information is inherently encoded into the convolutional filters, which directly interact with the input sentence during the convolution encoding operation.
100	16	Datasets We investigate the effectiveness of the proposed ACNN framework on both document classification and text sequence matching tasks.
130	21	We further experiment on both ACNN and CNN models with multiple filters.
138	107	More importantly, even with a filter size of 1000, the classification accuracy of the CNN is worse than that of the ACNN model with the filter number restricted to 100.
140	103	To elucidate the role of different parts (modules) in our AdaQA model, we implement several model variants for comparison: (i) a “vanilla” CNN model that independently encodes two sentence representations for matching; (ii) a self-adaptive ACNN-based model where the question/answer sentence generates adaptive filters only to convolve with the input itself; (iii) a one-way ACNN model where only the answer sentence representation is extracted with adaptive filters, which are generated conditioned on the question; (iv) a two-way AdaQA model as described in Section 2.4, where both sentences are adaptively encoded, with filters generated conditioned on the other sequence; (v) considering that the proposed filter generation mechanism is complementary to the attention layer typically employed in sequence matching tasks (see Section 3.4), we experiment with another model variant that combines the proposed context-aware filter generation mechanism with the multi-perspective attention layer introduced in (Wang et al., 2017b).
143	26	Combined with the results in document categorization experiments, we believe that our ACNN framework, in its simplest form, can be utilized as a powerful feature extractor for transforming natural language sentences into fixed-length vectors.
145	26	This observation indicates that the bidirectional filter generation mechanism is strongly associated with the performance gains.
157	101	Similar to the findings in (Miao et al., 2016), we observe that the ‘How’ question is the hardest to answer, with the lowest MAP scores.
158	41	However, our AdaQA model improves most over the basic CNN on the ‘How’ type question, see Figure 3(b).
160	21	Filter visualization To better understand what information has been encoded into our contextaware filters, we visualize one of the filters for sentences within the test set (on the DBpedia dataset) with t-SNE.
162	53	It can be observed that the filters for documents with the same label (ontology) are grouped into clusters, indicating that for different types of document, ACNN has leveraged distinct convolutional filters for better feature extraction.
