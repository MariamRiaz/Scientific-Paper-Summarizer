17	2	Code to reproduce all the experiments in the present paper is available at https://github.com/YunlongJiao/ weightedkendall.
18	1	Let us first fix some notations.
27	1	kernel, i.e., ∀σ, σ′ ∈ Sn, Kτ (σ, σ ′) = Φτ (σ) >Φτ (σ ′) = ∑ 1≤i6=j≤n 1σ(i)<σ(j)1σ′(i)<σ′(j) .
30	4	Besides being p.d., the Kendall kernel has another interesting property: it is right-invariant, in the sense that for any σ, σ′ and π ∈ Sn, it holds that Kτ (σ, σ ′) = Kτ (σπ, σ ′π) .
32	1	Note that any right-invariant kernel can be rewritten as, ∀σ, σ′ ∈ Sn, K(σ, σ′) = K(e, σ′σ−1) =: κ(σ′σ−1) , (2) where κ : Sn → R. Hence a right-invariant kernel is a semigroup kernel (Berg et al., 1984), and we say that a function κ : Sn → R is p.d.
34	1	In particular, note that K is symmetric if and only if κ satisfies κ(σ) = κ(σ−1) for any σ ∈ Sn.
70	2	While U ∈ Rn×n encodes the weights of pairs of positions, it is usually intuitive to start from individual positions.
101	1	(12) where 〈·, ·〉F denotes the Frobenius inner product for matrices.
109	2	The last equality follows from the relationship that vec(PXQ) = ( Q> ⊗ P ) vec(X) holds for any matrices P,Q,X .
110	4	Note that the first two inner products are over n × n matrices, while the last one is over n2 × n2 matrices.
117	1	Now, we propose to optimize over both U and B by joint optimization, which amounts to a non-convex optimization problem over vec(U)⊗ (vec(B))> according to Theorem 3.
121	1	Note that this naive alternating procedure implemented in kernel machines such as SVM, it will result in regularizing B and U in the same way, in the sense that the feasible region remain the same for both.
122	2	Alternatively, it is possible to bypass the need for alternative optimization of B and U .
124	1	This is similar to the supervised quantile normalisation (SUQUAN) model proposed by Le Morvan & Vert (2017), who studied a similar optimization over a quantile distribution by learning a rank-1 linear model.
128	1	, id) ∈ [1, n]d : 1 ≤ i1 < · · · < id ≤ n } , and define the p.d.
129	2	function associated with the order-d kernel for permutations by κ(d)(σ) = ∑ (i1,...,id)∈I(d) 1(σ(i1),...,σ(id))∈I(d) .
139	2	Recall that, in the context of supervised learning with discriminative models, working with the weighted order-d kernel GU for permutations with kernel machines amounts to fitting a linear model to data through the weighted order-d embedding ΦU .
149	4	Thanks to the conceptual interchangeability between B and U , alternating the optimization in B and U with a kernel machines amounts to simply changing the underlying kernel function accordingly.
150	3	Note that the square unfolding studied by Jiang et al. (2015, Definition 2.2) of the order-2d tensor Π⊗dσ is a matrix of dimension nd × nd, which is exactly the d-fold Kronecker product of the matrix Πσ with itself.
152	1	In this section, we demonstrate the use of the proposed weighted kernels compared with the standard Kendall kernel for classification on a real dataset from the European Union survey Eurobarometer 55.2 (Christensen, 2010).
154	1	The dataset also includes demographic information of the participants such as gender, nationality or age.
160	2	This random sub-sampling process was repeated 50 times and we report the classification accuracy on 50 test sets.
170	2	In conclusion, these results show that the weighted Kendall kernel can be promising for certain prediction tasks as they can outperform the standard one.
178	2	This is in line with the pattern widely recognized in such data involved in preference ranking.
180	1	This suggests that shifting the relative order of σ(i) > σ(j) or σ(i) < σ(j) results in opposite direction of contribution in evaluating the kernel GU (12).
181	9	Finally, we observe that, the more we move away from diagonal, the larger the magnitude of the values, which indicates that it is crucial to focus more on pairs of items whose ranks are more distinctively placed by a permutation.
182	19	This pattern of gradient on pairwise position importance in U identifies our foremost motivation of proposing the weighted kernels.
183	36	We have proposed a general framework to build computationally efficient kernels for permutations, extending the Kendall kernel to incorporate weights and higher-order information, and showing how weights can be systematically optimized in a data-driven way.
184	151	The price to pay for these extensions is that the dimensionality of the embedding and the number of free parameters of the model can quickly increase, raising computational and statistical challenges that could be addressed in future work.
185	144	On the theoretical side, the kernels we proposed could naturally be analyzed in the Fourier domain (Kondor & Barbosa, 2010; Mania et al., 2016) which may lead to new interpretations of their regularization properties and reproducing kernel Hilbert spaces (Berg et al., 1984).
