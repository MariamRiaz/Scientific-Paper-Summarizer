41	7	Finally, we remark that a non-greedy algorithm was studied in (Gillenwater et al., 2012) for better MAP qualities of DPP, but it is much slower than ours as reported in Section 5.
42	8	We start by defining a necessary notation.
57	14	It is well known that DPP has the submodular structure, i.e., f = log det is submodular.
61	27	Under some modifications of the standard greedy procedure, 2/5-approximation can be guaranteed even for non-monotone functions (Feige et al., 2011).
62	7	Irrespectively of such theoretical guarantees, it has been empirically observed that greedy selection (1) provides near optimal solutions in practice (Krause et al., 2008; Sharma et al., 2015; Yao et al., 2016; Zhang & Ou, 2016).
64	12	Since the exact computations of logdeterminants might be slow, i.e., requires O(d3) time for d-dimensional matrices, we introduce recent efficient logdeterminant approximation schemes (LDAS).
67	17	,v(m) are random vectors used for estimating the trace of pn(A).
68	14	Several polynomial expansions, including Taylor (Boutsidis et al., 2015), Chebyshev (Han et al., 2015) and Legendre (Peng & Wang, 2015) have been studied.
77	19	(2) This requires a linear solver to compute L−1X LX,i; conjugate gradient descent (CG) (Greenbaum, 1997) is a popular choice in practice.
78	12	Hence, if one applies CG to compute the max-marginal gain (1), the resulting greedy algorithm runs in Θ(d · T 3GR · TCG) time, where TCG denotes the number of iterations of each CG run.
79	25	In the worst case, CG converges to the exact solution when TCG grows with the matrix dimension, but for practical purposes, it typically provides a very accurate solution in few iterations, i.e., TCG = O(1).
81	43	Although it guarantees rigorous upper/lower bounds, CG is faster and accurate enough for most practical purposes.
83	6	The faster implementations proposed in this paper smartly employ both of them as key components utilizing their complementary benefits.
96	9	The first term (a) can be computed efficiently as we explained earlier, but we have to run CG p times for computing single columns of L (1) X , .
99	30	If p is large, the overall complexity becomes larger, but the approximation quality improves as well.
101	20	Instead, we use a simple random partitioning scheme because it is not only the fastest method but it also works well in our experiments.
106	11	Hence, the overall complexity becomes Θ(T 3GR ·TCG ·p+ d·T 2GR) = Θ(T 3GR+d·T 2GR), where we choose p, TCG = O(1).
108	11	In particular, if kernel matrix L is sparse, i.e., number of non-zeros of each column/row isO(1), ours has the complexity Θ(T 2GR + d · TGR) while the naı̈ve approaches are still worse having the complexity Θ(d · T 2GR).
110	10	Suppose the smallest eigenvalue of L is greater than 1.
111	9	Then, it holds that log detLX ≥ (1− 1/e) max Z⊆Y,|Z|=|X| log detLZ − 2|X|ε.
120	15	However, the line 9 of Algorithm 2 uses the LDAS and we remind that it runs in Θ(d2) time.
123	7	5: Partition [s] randomly into p subsets.
130	6	We explain more details of key components of the batch algorithm below.
133	11	(4) until no gain is attained.
134	6	The non-batch greedy procedure (1) corresponds to k = 1.
138	17	(Mirzasoleiman et al., 2015) first propose an uniformly random sampling to the standard non-batch greedy algorithm.
139	7	The authors show that it guarantees (1 − 1/e − O(e−s)) approximation ratio in expectation and report that it performs well in many applications.
140	12	In our experiments, we choose s = 50 batch samples.
141	23	High-order approximation of log-determinant.
152	76	Suppose A,B are positive definite matrices whose eigenvalues are in [δ, 1− δ] for δ > 0.
