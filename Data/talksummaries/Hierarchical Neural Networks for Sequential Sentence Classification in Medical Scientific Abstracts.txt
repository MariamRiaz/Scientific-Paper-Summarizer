6	25	In this paper, we develop a machine-learning based approach to automatically categorize sentences in scientific abstracts into rhetorical sections so that the desired information can be efficiently retrieved.
10	56	Previous state-of-the-art methods relied on Conditional Random Fields (CRFs) to take into account the inter-dependence between subsequent labels, which improved joint sentence classification performance by considering the label sequence information.
17	59	Together with the CRF algorithm, this allows us to make use of not only the preceding labels’ information but also the content and semantics of adjacent sentences to infer the label of the target sentence.
34	16	Colon notations xi:j and si:j are used to denote the se- quence of scalars (xi, xi+1, ..., xj) and vectors (si, si+1, ..., sj).
35	52	Our model is composed of four components: the word embedding layer, the sentence encoding layer, the context enriching layer, and the label sequence optimization layer.
43	13	This layer outputs a sequence of hidden states h1:N (h ∈ Rd hs ) for a sentence of N words with each hidden state corresponding to a word.
45	13	Here each row of Us is a context vector us ∈ Rda and it is expected to reflect an aspect or component of the semantics of a sentence.
51	14	Each of these vectors is subsequently input to a feed-forward neural network with only one hidden layer to get the corresponding probability vector r ∈ Rl, which represents the probability that this sentence belongs to each label, where l is the number of labels.
63	22	NICTA-PIBOSO This dataset2 was shared from the ALTA 2012 Shared Task (Amini et al., 2012), the goal of which is to build automatic sentence classifiers that can map the sentences from biomedical abstracts into a set of pre-defined categories for Evidence-Based Medicine (EBM).
65	34	It is based on the PubMed database of biomedical literature and each sentence of each abstract is labeled with its role in the abstract using one of the following classes: background, objective, method, result, and conclusion.
66	14	Table 2 presents an example abstract comprising structured sentences with their annotated labels.
75	31	For the version of dropout used in practice (e.g., the dropout function implemented in the TensorFlow and Pytorch libraries), the model ensemble generated by dropout in the training phase is approximated by a single model with scaled weights in the inference phase, resulting in a gap between training and inference.
79	18	The RNN encoder in the sentence encoding layer is set as LSTM for the PubMed datasets and gated recurrent unit (GRU) for the NICTA-PIBOSO dataset.
81	39	Table 4 compares our model against the best performing models in the literature (Dernoncourt et al., 2016; Liu et al., 2013).
88	17	To be noted, this performance gap between RNN and CNN sentence encoder gets larger as the dataset size increases from 20k to 200k for the PubMed dataset.
91	24	Table 5 presents the ablation analysis of our model (on the PubMed 20k dataset), where we remove one component at a time and quantify the performance drop (reported on F1 scores).
92	46	As can be seen from Table 5, our HSLN-CNN model uni- formly suffers a little more from the component removal than the HSLN-RNN model, indicating that the HSLN-RNN model is more robust.
93	15	When the context enriching layer is removed, both models experience the most significant performance drop and can only be on par with the previous stateof-the-art results, strongly demonstrating that this proposed component is the key to the performance improvement of our model.
96	46	Table 6 and 7 detail the results of classification for each label in terms of performance scores (precision, recall and F1) and confusion matrix, respectively (for our HSLN-RNN model trained on the PubMed 20k dataset).
98	49	One fifth of Background sentences are incorrectly classified as Objectives, while around one forth of Objectives sentences are wrongly assigned to the label of Background.
101	21	Table 8 presents a few examples of prediction errors that are produced by our HSLN-RNN model trained on the PubMed 20k dataset.
103	28	For example, the sentence “Depressive disorders are one of the leading components of the global burden of disease with a prevalence of up to 14% in the general population.” is indeed introducing the background of the problem (depressive disorders) on which this article is going to focus; however, the gold label classifies it into the Objective category.
104	13	For another instance, the sentence “A post hoc analysis was conducted with the use of data from the evaluation study of congestive heart failure and pulmonary artery catheterization effectiveness (escape).” belongs to the Result label according to the gold standard, but it makes more sense that it should be classified as a Method label.
111	23	According to Table 9, the training methods that create the word embeddings do not have a strong influence on model performance, but the corpus they are trained on does.
112	30	The combination of Wikipedia and PubMed abstracts as the corpus for unsupervised word embedding training yields the best result, and the individual use of either the Wikipedia corpus or the PubMed abstracts performs much worse.
113	69	Although the dataset we are using for evaluation is also from PubMed abstracts, using only the PubMed abstracts together with MIMIC notes without the Wikipedia corpus does not guarantee better result (see the “FastTextP.M.+MIMIC” embeddings in Table 9), which may be because the corpus size of PubMed abstracts plus MIMIC notes (about 12.8 million abstracts and 1 million notes) is not large enough for good embedding training compared with the corpus consisting of at least billion tokens such as the Wikipedia.
115	24	We demonstrate that incorporating the contextual information from surrounding sentences to help classify the current one by using an LSTM layer to sequentially process the encoded sentence representations can improve the overall quality of predictions.
117	74	We expect that our proposed model can be generalized to any problem that is related to sequential sentence classification, such as the paragraph-level sequential sentence categorization in full-text articles for better text mining and document retrieval (Westergaard et al., 2018).
118	60	Although the whole PubMed database contains over 2 million abstracts with part of them accompanied by full-text articles, only a small fraction of them are structured and contain the label information utilized in this work.
119	67	We plan to make use of the rest unannotated abstracts or full texts to pre-train our model and then fine tune it to the target annotated datasets inspired by the work from (Howard and Ruder, 2018) so that the performance can be further boosted.
