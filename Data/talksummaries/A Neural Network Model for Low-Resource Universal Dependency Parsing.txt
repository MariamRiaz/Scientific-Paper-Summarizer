7	31	Here we frame this representation learning as part of a neural network training.
10	37	Our approach works by jointly training a neural network dependency parser to model the syntax in both a source and target language.
12	22	In this way, the information can flow back and forth between languages, allowing for the learning of a compatible cross-lingual syntactic representation, while also allowing the parsers to mutually correct one another’s errors.
13	31	We include some language-specific components, in order to better model the lexicon of each language and allow learning of the syntactic idiosyncrasies of each language.
14	35	Our experiments show that this outperforms a purely supervised setting, on both small and large data conditions, with a gain as high as 10% for small training sets.
44	28	The model is based on a transition-based dependency parser (Nivre, 2006) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration.2 That is, for each configuration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted.
53	56	Joint training of a parser over the source and target languages can be achieved by simply adding two such cross-entropy objectives, i.e., Ljoint = − |Ds|∑ i=1 logP (Ys = ~y(i)s |Xs = ~x(i)s ) − |Dt|∑ i=1 logP (Yt = ~y (i) t |Xt = ~x(i)t ) , (2) where the training data, D = Ds ∪Dt, comprises data in both the source and target language.
60	44	The set of parameters for the model is W1,W2, Es, Et where Es, Et are the embedding matrices for the source and target languages.
62	23	Specifically, the model is the combination of two parts: the universal part (W1,W2) that is shared between the languages, and the conversion part (Es, Et) that maps a language-specific representation into the universal language.
75	24	Our model is flexible, enabling us to freely add additional components.
78	18	We can easily add bilingual dictionary constraints to the model in the form of regularization to minimize the l2 distance between word representations, i.e.,∑ 〈i,j〉∈D ‖Eword(i)s − Eword(j)t ‖2F , where D comprises translation pairs, word(i) and word(j).
84	40	We experiment with the Universal Dependency Treebank (UDT) V1.0 (Nivre et al., 2015), simulating low resource settings.5 This treebank has many desirable properties for our model: the dependency types (arc labels set) and coarse POS tagset are the same across languages.
86	21	Moreover, the dependency types are also common across languages allowing evaluation of the labelled attachment score (LAS).
112	21	For our initial experiments we assume that we have only a small target treebank with 3000 tokens (around 200 sentences).
115	19	The supervised neural network dependency parser performed worst, as expected, and the baseline cascade model consistently outperformed the supervised model on all languages by an average margin of 5.6% (absolute).8 The joint model also consistently out-performed both baselines giving a further 1.9% average improvement over the cascade.
122	35	Figure 2 shows the learning curve with respect to various models on different data sizes averaged over all target languages.
123	34	For small datasets of 1k training tokens, the cascaded model, joint model and joint + dict model performed similarly well, out-performing the supervised model by about 10% (absolute).
125	58	While the baseline cascade model still outperforms the supervised model, the improvement is diminishing, and by 15k, the difference is only 2.9%.
129	21	In this approach, the target language parameters are tied (softly) with the source language parameters through regularization.
133	81	In contrast the joint model learns a mutually compatible representation automatically during joint training.
149	35	However, with more data, the model is better able to learn the tagset mapping as part of joint training.
150	18	Beyond 15k tokens, the joint model using the language specific POS tagset outperforms UPOS.
157	18	We can see that English and French are mixed nicely together.
161	37	Some of the learned cross-lingual wordembeddings are shown in Table 2, which includes the five nearest neighbours to selected English words according to the monolingual word embedding (section 4.3) and our cross-lingual dependency word embeddings, trained using PanLex.
171	18	This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity.
175	32	Our embeddings encode not just cross-lingual correspondences, but also capture dependency relations which we expect might be beneficial for other NLP tasks based on dependency parsing, e.g., cross-lingual semantic role labelling where long-distance relationship can be captured by word embedding.
178	20	Compared with supervised learning, our joint model gives a consistent 8-10% improvement over several different datasets in simulation lowresource scenarios.
182	55	As the side-effect of training our joint model, we obtain cross-lingual word embeddings specialized for dependency parsing.
183	126	We expect these embeddings to be beneficial to other syntatic and se- mantic tasks.
184	73	In future work, we plan to extend joint training to several languages, and further explore the idea of learning and exploiting crosslingual embeddings.
