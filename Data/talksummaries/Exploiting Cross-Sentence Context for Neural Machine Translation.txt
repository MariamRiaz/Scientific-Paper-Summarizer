0	22	Neural machine translation (NMT) has been rapidly developed in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Tu et al., 2016).
1	39	The encoderdecoder architecture is widely employed, in which the encoder summarizes the source sentence into a vector representation, and the decoder generates the target sentence word by word from the vector representation.
2	115	Using the encoder-decoder framework as well as gating and attention techniques, it has been shown that the performance of NMT has surpassed the performance of traditional statistical machine translation (SMT) on various language pairs (Luong et al., 2015).
3	16	The continuous vector representation of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of a word.
4	1	Consequently, NMT needs to spend a substantial amount of its capacity in disambiguating source and target words based on the context defined by a source sentence (Choi et al., 2016).
5	62	Consistency is another critical issue in documentlevel translation, where a repeated term should keep the same translation throughout the whole document (Xiao et al., 2011; Carpuat and Simard, 2012).
6	71	Nevertheless, current NMT models still process a documents by translating each sentence alone, suffering from inconsistency and ambiguity arising from a single source sentence.
7	44	These problems are difficult to alleviate using only limited intra-sentence context.
8	39	The cross-sentence context, or global context, has proven helpful to better capture the meaning or intention in sequential tasks such as query suggestion (Sordoni et al., 2015) and dialogue modeling (Vinyals and Le, 2015; Serban et al., 2016).
9	30	The leverage of global context for NMT, however, has received relatively little attention from the research community.1 In this paper, we propose a cross-sentence context-aware NMT model, which considers the influence of previous source sentences in the same document.2 Specifically, we employ a hierarchy of Recurrent Neural Networks (RNNs) to summarize the cross-sentence context from source-side previous sentences, which deploys an additional documentlevel RNN on top of the sentence-level RNN encoder (Sordoni et al., 2015).
10	128	After obtaining the global context, we design several strategies to integrate it into NMT to translate the current sentence: • Initialization, that uses the history represen1To the best of our knowledge, our work and Jean et al. (2017) are two independently early attempts to model crosssentence context for NMT.
11	15	2826 tation as the initial state of the encoder, decoder, or both; • Auxiliary Context, that uses the history representation as static cross-sentence context, which works together with the dynamic intrasentence context produced by an attention model, to good effect.
12	78	• Gating Auxiliary Context, that adds a gate to Auxiliary Context, which decides the amount of global context used in generating the next target word at each step of decoding.
13	65	Experimental results show that the proposed initialization and auxiliary context (w/ or w/o gating) mechanisms significantly improve translation performance individually, and combining them achieves further improvement.
14	67	Given a source sentence xm to be translated, we consider its K previous sentences in the same document as cross-sentence context C = {xm−K , ...,xm−1}.
15	9	In this section, we first model C, which is then integrated into NMT.
16	15	As shown in Figure 1, we summarize the representation of C in a hierarchical way: Sentence RNN For a sentence xk in C, the sentence RNN reads the corresponding words {x1,k, ..., xn,k, .
17	97	, xN,k} sequentially and updates its hidden state: hn,k = f(hn−1,k, xn,k) (1) where f(·) is an activation function, and hn,k is the hidden state at time n. The last state hN,k stores order-sensitive information about all the words in xk, which is used to represent the summary of the whole sentence, i.e. Sk ≡ hN,k.
