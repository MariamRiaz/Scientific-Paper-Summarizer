3	38	For this reason efforts to gauge the quality of syntactic annotation are hampered by the need to fall back to simple accuracy measures.
4	33	As shown in Artstein and Poesio (2008), such measures are biased in favour of annotation schemes with fewer categories and do not account for skewed distributions between classes, which can give high observed agreement, even if the annotations are inconsistent.
6	38	First we give an overview of traditional agreement measures and why they are insufficient for syntax, before presenting our proposed metrics.
9	30	However, most evaluations of syntactic treebanks use simple accuracy measures such as bracket F1 scores for constituent trees (NEGRA, Brants, 2000; TIGER, Brants and Hansen, 2002; Cat3LB, Civit et al., 2003; The Arabic Treebank, Maamouri et al., 2008) or labelled or unlabelled attachment scores for dependency syntax (PDT, Hajič, 2004; PCEDT Mikulová and Štěpánek, 2010; Norwegian Dependency Treebank, Skjærholt, 2013).
10	59	The only work we know of using chance-corrected metrics 934 is Ragheb and Dickinson (2013), who use MASI (Passonneau, 2006) to measure agreement on dependency relations and head selection in multiheaded dependency syntax, and Bhat and Sharma (2012), who compute Cohen’s κ (Cohen, 1960) on dependency relations in single-headed dependency syntax.
11	28	A limitation of the first approach is that token ID becomes the relevant category for the purposes of agreement, while the second approach only computes agreements on relations, not on structure.
35	26	Let the function LAS(t1, t2) be the number of tokens with the same head and label in the two trees t1 and t2, T (i) the set of trees possible for an item i ∈ I , and tokens the number of tokens in the corpus.
37	50	First of all the number of possible trees for a sentence grows exponentially with sentence length, which means that explicitly iterating over all possible such pairs is computationally intractable, nor have we been able to easily derive an algorithm for this particular problem from standard algorithms.
41	36	As shown by the existence of three different metrics (κ, π and S (Bennett et al., 1954)) for the relatively simple task of nominal coding, the choice of model for P (t|c) will not be obvious, and thus differing choices of generative model as well as different choices for parameters such as smoothing will result in subtly different agreement metrics.
51	32	The tree edit distance (TED) problem is defined analogously to the more familiar problem of string edit distance: what is the minimum number of edit operations required to transform one tree into the other?
55	38	Therefore we remove the leaf nodes in the case of phrase structure trees, and in the case of dependency trees we compare trees whose edges are unlabelled and nodes are labelled with the dependency relation between that word and its head; the root node receives the label .
57	86	We propose three different distance functions for the agreement computation: the unmodified tree edit distance function, denoted δplain, a second function δdiff (x, y) = TED(x, y)−abs(|x|− |y|), the edit distance minus the difference in length between the two sentences, and finally δnorm(x, y) = TED(x,y)/|x|+|y|, the edit distance normalised to the range [0, 1].4 The plain TED is the simplest in terms of parsimony assumptions, however it may overestimate the difference between sentences, we intuitively find to be syntactically similar.
59	36	On the other hand, δdiff might underestimate some distances as well; for example the leftmost and rightmost trees also have distance zero using δdiff , despite our syntactic intuition that the difference between a transitive and an intransitive should be taken account of.
70	29	An already annotated corpus, in our case 100 randomly selected sentences from the Norwegian Dependency Treebank (Solberg et al., 2014), are taken as correct and then permuted to produce “annotations” of different quality.
76	28	A pre-order traversal would result in tokens close to the root having few options, and in particular if the root has a single child, that node has no possible new heads unless one of its children has been assigned the root as its new head first.
82	65	The αdiff metric is clearly extremely sensitive to noise, with p = 0.1 yielding mean αdiff = 15.8%, while αnorm is more lenient than both LAS and αplain, with mean αnorm = 14.5% at p = 1, quite high compared to LAS = 0.9%, αplain = −6.8% and αdiff = −246%.
83	41	To further study the sensitivity of the metrics to the two kinds of noise, we performed an additional set of experiments, setting one p = 0 while varying the other over the same range as in the previous experiment, the results of which are shown in Figures 4 and 5.
85	61	In comparison, mean LAS when only labels are perturbed is 4.1%, and since the sample space of trees of size n is clearly much larger than that of relabellings, a uniform random selection of tree would yield a LAS much closer to 0.
86	25	This shows that our tree shuffling algorithm has a non-uniform distribution over the sample space.
88	38	Whereas LAS responds linearly to perturbation of both labels and structure, with its parabolic behaviour in Figure 3 being simply the product of these two linear responses, the α metrics respond differently to structural noise and label noise, with label disagreements being penalised less harshly dency parsing: the percentage of tokens that receive the correct head and dependency relation.
108	33	A distinguishing feature of the tectogrammatical analyses, vis a vis the other treebanks we are using, is that semantically empty words only take part in the analytical annotation layer and nodes are inserted at the tectogrammatical layer to represent covert elements of the sentence not present in the surface syntax of the analytical layer.
114	48	When there are more than two annotators, we generalise the metric to be the average pairwise LAS for each sentence, weighted by the length of the sentence.
115	27	Let LAS(t1, t2) be the fraction of tokens with identical head and label in the trees t1 and t2; the pairwise labelled accuracy LASp(X) of a set of annotations X as described in section 1.2 is: LASp(X) = 1∑ i |xi1| ∑ |xi1|Λ(Xi) |Xi|(|Xi|−1)/2 (3) Λ(Xi) = |C|∑ c=1 |C|∑ c′=c+1 LAS(xic, xic′) This is equivalent to the traditional metric in the case where there are only two annotators.
129	25	The most important conclusion we draw from this work is the most appropriate agreement metric for syntactic annotation.
130	25	First of all, we disqualify the LAS metric, primarily due to the methodological inadequacies of using an uncorrected measure.
139	25	Furthermore, α metrics are far more flexible than simple accuracy metrics.
140	48	The use of a distance function to define the metric means that more fine-grained distinctions can be made; for example, if the set of labels on the structures is highly structured, partial credit can be given for differing annotations that overlap.
145	48	In future work, we would like to investigate the use of other distance functions, in particular the use of approximate tree edit distance functions such as the pq-gram algorithm (Augsten et al., 2005).
146	31	For large data sets such as the PCEDT set used in this work, computing α with tree edit distance as the distance measure can take a very long time.8 This is due to the fact that α requires O(n2) comparisons to be made, each of which is O(n2) using our current approach.
152	49	These techniques are interpreted more easily than agreement coefficients, and they allow us to assess the quality of individual annotators, a crucial property in crowd-sourcing settings and something that’s impossible using agreement coefficients.
