0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 78–88 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Word Sense Disambiguation (WSD) is a key task in computational lexical semantics, inasmuch as it addresses the lexical ambiguity of text by making explicit the meaning of words occurring in a given context (Navigli, 2009).",1 Introduction,[0],[0]
"Anyone who has struggled with frustratingly unintelligible translations from an automatic system, or with the meaning bias of search engines, can understand the importance for an intelligent system to go beyond the surface appearance of text.
",1 Introduction,[0],[0]
There are two mainstream lines of research in WSD: supervised and knowledge-based WSD.,1 Introduction,[0],[0]
"Supervised WSD frames the problem as a classical machine learning task in which, first a training phase occurs aimed at learning a classification model from sentences annotated with word senses and, second the model is applied to previouslyunseen sentences focused on a target word.",1 Introduction,[0],[0]
"A key
difference from many other problems, however, is that the classes to choose from (i.e., the senses of a target word) vary for each word, therefore requiring a separate training process to be performed on a word by word basis.",1 Introduction,[0],[0]
"As a result, hundreds of training instances are needed for each ambiguous word in the vocabulary.",1 Introduction,[0],[0]
"This would necessitate a million-item training set to be manually created for each language of interest, an endeavour that is currently beyond reach even in resource-rich languages like English.
",1 Introduction,[0],[0]
"The second paradigm, i.e., knowledge-based WSD, takes a radically different approach: the idea is to exploit a general-purpose knowledge resource like WordNet (Fellbaum, 1998) to develop an algorithm which can take advantage of the structural and lexical-semantic information in the resource to choose among the possible senses of a target word occurring in context.",1 Introduction,[0],[0]
"For example, a PageRank-based algorithm can be developed to determine the probability of a given sense being reached starting from the senses of its context words.",1 Introduction,[0],[0]
"Recent approaches of this kind have been shown to obtain competitive results (Agirre et al., 2014; Moro et al., 2014).",1 Introduction,[0.9534388766316834],"['The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).']"
"However, due to its inherent nature, knowledge-based WSD tends to adopt bag-of-word approaches which do not exploit the local lexical context of a target word, including function and collocation words, which limits this approach in some cases.
",1 Introduction,[0],[0]
"In this paper we get the best of both worlds and present Train-O-Matic, a novel method for generating huge high-quality training sets for all the words in a language’s vocabulary.",1 Introduction,[0],[0]
"The approach is language-independent, thanks to its use of a multilingual knowledge resource, BabelNet (Navigli and Ponzetto, 2012), and it can be applied to any kind of corpus.",1 Introduction,[0],[0]
"The training sets produced with Train-O-Matic are shown to provide competitive performance with those of manually and semi-
78
automatically tagged corpora.",1 Introduction,[0],[0]
"Moreover, state-ofthe-art performance is also reported for low resourced languages (i.e., Italian and Spanish) and domains, where manual training data is not available.",1 Introduction,[0],[0]
"In this Section we present Train-O-Matic, a language-independent approach to the automatic construction of a sense-tagged training set.",2 Building a Training Set from Scratch,[0],[0]
"TrainO-Matic takes as input a corpus C (e.g., Wikipedia) and a semantic network G = (V,E).",2 Building a Training Set from Scratch,[0],[0]
"We assume a WordNet-like structure of G, i.e., V is the set of concepts (i.e., synsets) such that, for each word w in the vocabulary, Senses(w) is the set of vertices in V that are expressed by w, e.g., the WordNet synsets that include w as one of their senses.
",2 Building a Training Set from Scratch,[0],[0]
"Train-O-Matic consists of three steps:
• Lexical profiling: for each vertex in the semantic network, we compute its Personalized PageRank vector, which provides its lexicalsemantic profile (Section 2.1).
",2 Building a Training Set from Scratch,[0],[0]
"• Sentence scoring: For each sentence containing a word w, we compute a probability distribution over all the senses of w based on its context (Section 2.2).
",2 Building a Training Set from Scratch,[0],[0]
"• Sentence ranking and selection: for each sense s of a word w in the vocabulary, we select those sentences that are most likely to use w in the sense of s (Section 2.3).",2 Building a Training Set from Scratch,[0],[0]
In terms of semantic networks the probability of reaching a node v′ starting from v can be interpreted as a measure of relatedness between the synsets v,2.1 Lexical profiling,[0],[0]
and v′.,2.1 Lexical profiling,[0],[0]
"Thus we define the lexical profile of a vertex v in a graph G = (V,E) as the probability distribution over all the vertices v′ in the graph.",2.1 Lexical profiling,[0],[0]
"Such distribution is computed by applying the Personalized PagaRank algorithm, a variant of the traditional PageRank (Brin and Page, 1998).",2.1 Lexical profiling,[0.951922285692239],['The second intuition of the criterion used by Srinivas and Babu (2015) is that neurons with small outgoing weights contribute very little overall.']
"While the latter is equivalent to performing random walks with uniform restart probability on every vertex at each step, PPR, on the other hand, makes the restart probability non-uniform, thereby concentrating more probability mass in the surroundings of those vertices having higher restart
probability.",2.1 Lexical profiling,[0],[0]
"Formally, (P)PR is computed as follows:
v(t+1) =",2.1 Lexical profiling,[0],[0]
"(1− α)v(0) + αMv(t) (1)
where M is the row-normalized adjacency matrix of the semantic network, the restart probability distribution is encoded by vector v(0), and α is the well-known damping factor usually set to 0.85 (Brin and Page, 1998).",2.1 Lexical profiling,[0],[0]
"If we set v(0) to a unit probability vector (0, . . .",2.1 Lexical profiling,[0],[0]
", 0, 1, 0, . . .",2.1 Lexical profiling,[0],[0]
", 0), i.e., restart is always on a given vertex, PPR outputs the probability of reaching every vertex starting from the restart vertex after a certain number of steps.",2.1 Lexical profiling,[0],[0]
"This approach has been used in the literature to create semantic signatures (i.e., profiles) of individual concepts, i.e., vertices of the semantic network (Pilehvar et al., 2013), and then to determine the semantic similarity of concepts.",2.1 Lexical profiling,[0.9552894020915363],"['Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT.']"
"As also done by Pilehvar and Collier (2016), we instead use the PPR vector as an estimate of the conditional probability of a word w′",2.1 Lexical profiling,[0],[0]
"given the target sense1 s ∈ V of word w:
P (w′|s, w) = maxs′∈Senses(w′) vs(s ′)
Z (2)
where Z = ∑
w” P (w”|s, w) is a normalization constant, vs is the vector resulting from an adequate number of random walks used to calculate PPR, and vs(s′) is the vector component corresponding to sense s′.",2.1 Lexical profiling,[0],[0]
"To fix the number of iterations needed to have a sufficiently accurate vector, we follow Lofgren et al. (2014) and set the error δ = 0.00001 and the number of iterations to 1 δ = 100, 000.
",2.1 Lexical profiling,[0],[0]
As a result of this lexical profiling step we have a probability distribution over vocabulary words for each given word sense of interest.,2.1 Lexical profiling,[0],[0]
The objective of the second step is to score the importance of word senses for each of the corpus sentences which contain the word of interest.,2.2 Sentence scoring,[0],[0]
Given a sentence σ =,2.2 Sentence scoring,[0],[0]
"w1, w2, . .",2.2 Sentence scoring,[0],[0]
.,2.2 Sentence scoring,[0],[0]
", wn, for a given target wordw in the sentence (w ∈ σ), and for each of its senses s ∈ Senses(w), we compute the probability P (s|σ,w).",2.2 Sentence scoring,[0],[0]
"Thanks to Bayes’ theorem we can determine the probability of sense s of w given the
1Note that we use senses and concepts (synsets) interchangeably, because – given a word – a word sense unambiguously determines a concept (i.e., the synset it is contained in) and vice versa.
sentence as follows:
P (s|σ,w) =",2.2 Sentence scoring,[0],[0]
"P (σ|s, w)P (s|w) P (σ|w) (3)
= P (w1, . . .",2.2 Sentence scoring,[0],[0]
", wn|s, w)P (s|w) P (w1, . . .",2.2 Sentence scoring,[0],[0]
", wn|w) ∝",2.2 Sentence scoring,[0],[0]
"P (w1, . .",2.2 Sentence scoring,[0],[0]
.,2.2 Sentence scoring,[0],[0]
", wn|s, w)P (s|w) (4) ≈ P (w1|s, w) . . .",2.2 Sentence scoring,[0],[0]
"P (wn|s, w)P (s|w)
(5)
where Formula 4 is proportional to the original probability (due to removing the constant in the denominator) and is approximated with Formula 5 due to the assumption of independence of the words in the sentence.",2.2 Sentence scoring,[0],[0]
"P (wi|s, w) is calculated as in Formula 2 and P (s|w) is set to 1/|Senses(w)| (recall that s is a sense of w).",2.2 Sentence scoring,[0],[0]
"For example, given the sentence σ =",2.2 Sentence scoring,[0],[0]
"“A match is a tool for starting a fire”, the target word w = match and its set of senses Smatch = {s1match, s2match}, where s1match is the sense of lighter and s2match is the sense of game match, we want to calculate the probability of each simatch ∈ Smatch of being the correct sense of match in the sentence σ.",2.2 Sentence scoring,[0],[0]
"Following Formula 5 we have:
P (s1match|σ,match)",2.2 Sentence scoring,[0],[0]
"≈ P (tool|s1match,match) · P (start|s1match,match) · P (fire|s1match,match) · P (s1match|match) = 2.1 · 10−4 · 2 · 10−3 · 10−2 · 5 · 10−1 = 2.1 · 10−9
P (s2match|σ,match)",2.2 Sentence scoring,[0],[0]
"≈ P (tool|s2match,match) ·",2.2 Sentence scoring,[0],[0]
"P (start|s2match,match) · P (fire|s2match,match) · P (s2match|match) = 10−5 · 2.9 · 10−4 · 10−6 · 5 · 10−1 = 1.45 · 10−15
As can be seen, the first sense of match has a much higher probability due to its stronger relatedness to the other words in the context (i.e. start, fire and tool).",2.2 Sentence scoring,[0],[0]
Note also that all the probabilities for the second sense are at least one magnitude less than the probability of the first sense.,2.2 Sentence scoring,[0],[0]
"Finally, for a given word w and a given sense s1 ∈ Senses(w), we score each sentence σ in which w appears and s1 is its most likely sense according to a formula that takes into account the difference between the first (i.e., s1) and the second most likely sense of w in σ:
∆s1(σ)",2.3 Sense-based sentence ranking and selection,[0],[0]
"= P (s1|σ,w)− P (s2|σ,w) (6) where s1 = arg maxs∈Senses(w) P (s|σ,w), and s2 = arg maxs∈Senses(w)\{s1} P (s|σ,w).",2.3 Sense-based sentence ranking and selection,[0],[0]
We then sort all sentences based on ∆s1(·) and return a ranked list of sentences where word w is most likely to be sense-annotated with s1.,2.3 Sense-based sentence ranking and selection,[0],[0]
"Although we recognize that other scoring strategies could have been used, this was experimentally the most effective one when compared to alternative strategies, i.e., the sense probability, the number of words related to the target word w, the sentence length or a combination thereof.",2.3 Sense-based sentence ranking and selection,[0],[0]
"In the previous Section we assumed that WordNet was our semantic network, with synsets as vertices and edges represented by its semantic relations.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"However, while its lexical coverage is high, with a rich set of fine-grained synsets, at the relation level WordNet provides mainly paradigmatic information, i.e., relations like hypernymy (is-a) and meronymy (part-of).",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"It lacks, on the other hand, syntagmatic relations, such as those that connect verb synsets to their arguments (e.g., the appropriate senses of eatv and foodn), or pairs of noun synsets (e.g., the appropriate senses of busn and drivern).
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Intuitively, Train-O-Matic would suffer from such a lack of syntagmatic relations, as the relevance of a sense for a given word in a sentence depends directly on the possibility of visiting senses of the other words in the same sentence (cf.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
Formula 5) via random walks as calculated with Formula 1.,3 Creating a Denser and Multilingual Semantic Network,[0],[0]
Such reachability depends on the connections available between synsets.,3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Because syntagmatic relations are sparse in WordNet, if it was used on its own, we would end up with a poor ranking of sentences for any given word sense.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Moreover, even though the methodology presented in Section 2 is languageindependent, Train-O-Matic would lack informa-
tion (e.g. senses for a word in an arbitrary vocabulary) for languages other than English.
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"To cope with these issues, we exploit BabelNet,2 a huge multilingual semantic network obtained from the automatic integration of WordNet, Wikipedia, Wiktionary and other resources (Navigli and Ponzetto, 2012), and create the BabelNet subgraph induced by the WordNet vertices.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
The result is a graph whose vertices are BabelNet synsets that contain at least one WordNet synset and whose edge set includes all those relations in BabelNet coming either from WordNet itself or from links in other resources mapped to WordNet (such as hyperlinks in a Wikipedia article connecting it to other articles).,3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"The greatest contribution of syntagmatic relations comes, indeed, from Wikipedia, as its articles are linked to related articles (e.g., the English Wikipedia Bus article3 is linked to Passenger, Tourism, Bus lane, Timetable, School, and many more).
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Because not all Wikipedia (and other resources’) pages are connected with the same degree of relatedness (e.g., countries are often linked, but they are not necessarily closely related to the source article in which the link occurs), we apply the following weighting strategy to each edge (s, s′) ∈ E of our WordNet-induced subgraph of BabelNet G = (V,E):
w(s, s′) =",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"{ 1 (s, s′) ∈ E(WordNet)",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"WO(s, s′)",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"otherwise
(7) where E(WordNet) is the edge set of the original WordNet graph andWO(s, s′) is the weighted
2http://babelnet.org 3Retrieved on February 3rd, 2017.
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"overlap measure which calculates the similarity between two synsets:
WO(s, s′) = ∑|S|",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"i=1(r 1 i + r
2 i ) −1∑|S|
i=1(2i)−1
where r1i and r 2",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"i are the rankings of the i-th synsets in the set S of the components in common between the vectors associated with s and s′, respectively.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Because at this stage we still have to calculate our synset vector representation, we use the precomputed NASARI vectors (Camacho-Collados et al., 2015) to calculate WO.",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"This choice is due to WO’s higher performance over cosine similarity for vectors with explicit dimensions (Pilehvar et al., 2013).
",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"As a result, each row of the original adjacency matrix M of G will be replaced with the weights calculated in Formula 7 and then normalized in order to be ready for PPR calculation (see Formula 1).",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"An idea of why a denser semantic network has more useful connections and thus leads to better results is provided by the example in
iment for the animal, and operating system and Windows for the device sense, among others).",3 Creating a Denser and Multilingual Semantic Network,[0],[0]
"Corpora for sense annotation We used two different corpora to extract sentences: Wikipedia and the United Nations Parallel Corpus (Ziemski et al., 2016).",4 Experimental Setup,[0],[0]
"The first is the largest and most up-to-date encyclopedic resource, containing definitional information, the second, on the other hand, is a public collection of parliamentary documents of the United Nations.",4 Experimental Setup,[0],[0]
"The application of TrainO-Matic to the two corpora produced two senseannotated datasets, which we named T-O-MWiki and T-O-MUN , respectively.
",4 Experimental Setup,[0],[0]
"Semantic Network We created sense-annotated corpora with Train-O-Matic both when using PPR vectors computed from vanilla WordNet and when using WordNetBN , our denser network obtained from the WordNet-induced subgraph of BabelNet (see Section 3).
",4 Experimental Setup,[0],[0]
"Gold standard datasets We performed our evaluations using the framework made available by Raganato et al. (2017a) on five different allwords datasets, namely: the Senseval-2 (Edmonds and Cotton, 2001), Senseval-3 (Snyder and Palmer, 2004), SemEval-2007 (Pradhan et al., 2007), SemEval-2013 (Navigli et al., 2013) and SemEval-2015 (Moro and Navigli, 2015)",4 Experimental Setup,[0],[0]
WSD datasets.,4 Experimental Setup,[0],[0]
"We focused on nouns only, given the fact that Wikipedia provides connections between nominal synsets only, and therefore contributes mainly to syntagmatic relations between nouns.
",4 Experimental Setup,[0],[0]
"Comparison sense-annotated corpora To show the impact of our T-O-M corpora in WSD, we compared its performance on the above gold standard datasets, against training with:
• SemCor (Miller et al., 1993), a corpus containing about 226,000 words annotated manually with WordNet senses.
",4 Experimental Setup,[0],[0]
"• One Million Sense-Tagged Instances (Taghipour and Ng, 2015, OMSTI), a sense-annotated dataset obtained via a semi-automatic approach based on the disambiguation of a parallel corpus, i.e., the United Nations Parallel Corpus, performed by exploiting manually translated word senses.",4 Experimental Setup,[0],[0]
"Because OMSTI integrates SemCor
to increase coverage, to keep a level playing field we excluded the latter from the corpus.
",4 Experimental Setup,[0],[0]
"We note that T-O-M, instead, is fully automatic and does not require any WSD-specific human intervention nor any aligned corpus.
Reference system In all our experiments, we used It Makes Sense (Zhong and Ng, 2010, IMS), a state-of-the-art WSD system based on linear Support Vector Machines, as our reference system for comparing its performance when trained on TO-M, against the same WSD system trained on other sense-annotated corpora (i.e., SemCor and OMSTI).",4 Experimental Setup,[0],[0]
"Following the WSD literature, unless stated otherwise, we report performance in terms of F1, i.e., the harmonic mean of precision and recall.
",4 Experimental Setup,[0],[0]
"We note that it is not the purpose of this paper to show that T-O-M, when integrated into IMS, beats all other configurations or alternative systems, but rather to fully automatize the WSD pipeline with performances which are competitive with the state of the art.
",4 Experimental Setup,[0],[0]
Baseline,4 Experimental Setup,[0],[0]
"As a traditional baseline in WSD, we used the Most Frequent Sense (MFS) baseline given by the first sense in WordNet.",4 Experimental Setup,[0],[0]
"The MFS is a very competitive baseline, due to the sense skewness phenomenon in language (Navigli, 2009).
",4 Experimental Setup,[0],[0]
"Number of training sentences per sense Given a target word w, we sorted its senses Senses(w) following the WordNet ordering and selected the top ki training sentences for the i-th sense according to Formula 6, where:
ki = 1 iz ∗K",4 Experimental Setup,[0],[0]
"(8)
with K = 500 and z = 2 which were tuned on a separate small in-house development dataset5.",4 Experimental Setup,[0],[0]
The first result we report regards the impact of vanilla WordNet vs. our WordNet-induced subgraph of BabelNet (WordNetBN ) when calculating PPR vectors.,5.1 Impact of syntagmatic relations,[0],[0]
"As can be seen from Table 2 – which shows the performance of the T-O-MWiki corpora generated with the two semantic networks – using WordNet for PPR computation decreases
550 word-sense pairs annotated manually.
",5.1 Impact of syntagmatic relations,[0],[0]
"the overall performance of IMS from 0.5 to around 4 points across the five datasets, with an overall loss of 1.6 F1 points.",5.1 Impact of syntagmatic relations,[0],[0]
Similar performance losses were observed when using T-O-MUN (see Table 3).,5.1 Impact of syntagmatic relations,[0],[0]
This corroborates our hunch discussed in Section 3 that a resource like BabelNet can contribute important syntagmatic relations that are beneficial for identifying (and ranking high) sentences which are semantically relevant for the target word sense.,5.1 Impact of syntagmatic relations,[0],[0]
"In the following experiments, we report only results using WordNetBN .",5.1 Impact of syntagmatic relations,[0],[0]
"We now move to comparing the performance of T-O-M, which is fully automatic, against corpora which are annotated manually (SemCor) and semi-automatically (OMSTI).",5.2 Comparison against sense-annotated corpora,[0],[0]
"In Table 3 we show the F1-score of IMS on each gold standard dataset in the evaluation framework and on all datasets merged together (last row), when it is trained with the various corpora described above.
",5.2 Comparison against sense-annotated corpora,[0],[0]
"As can be seen, T-O-MWiki and T-O-MUN obtain higher performance than OMSTI (up to 5.5 points above) on 3 out of 5 datasets, and, overall, T-O-MWiki scores 1 point above OMSTI.",5.2 Comparison against sense-annotated corpora,[0],[0]
"The MFS is in the same ballpark as T-O-MWiki, performing better on some datasets and worse on others.",5.2 Comparison against sense-annotated corpora,[0],[0]
We note that IMS trained on T-O-MWiki succeeds in surpassing or obtaining the same results as IMS trained on SemCor on SemEval15 and SemEval-13.,5.2 Comparison against sense-annotated corpora,[0],[0]
"We view this as a significant achievement given the total absence of manual effort involved in T-O-M. Because overall T-O-MWiki outperforms T-O-MUN , in what follows we report all the results with T-O-MWiki, except for the domain-oriented evaluation (see Section 5.4).",5.2 Comparison against sense-annotated corpora,[0],[0]
"IMS uses the MFS as a backoff strategy when no sense can be output for a target word in context (Zhong and Ng, 2010).",5.3 Performance without backoff strategy,[0],[0]
"Consequently, the performance of the MFS is mixed up with that of the SVM classifier.",5.3 Performance without backoff strategy,[0],[0]
"As shown in Table 4, OMSTI is able to provide annotated sentences for roughly half of the tokens in the datasets.",5.3 Performance without backoff strategy,[0],[0]
"Train-O-Matic, on the other hand, is able to cover almost all words in each dataset with at least one training sentence.",5.3 Performance without backoff strategy,[0],[0]
"This means that in around 50% of cases OMSTI gives an answer based on the IMS backoff strategy.
",5.3 Performance without backoff strategy,[0],[0]
"To determine the real impact of the different training data, we therefore decided to perform an additional analysis of the IMS performance when the MFS backoff strategy is disabled.",5.3 Performance without backoff strategy,[0],[0]
"Because we suspected the system would not always return a sense for each target word, in this experiment we measured precision, recall and their harmonic mean, i.e., F1.",5.3 Performance without backoff strategy,[0],[0]
"The results in Table 5 confirm our hunch, showing that OMSTI’s recall drops heavily, thereby affecting F1 considerably.",5.3 Performance without backoff strategy,[0],[0]
"T-O-M performances, instead, remain high in terms of precision, recall and F1.",5.3 Performance without backoff strategy,[0],[0]
"This confirms that OMSTI relies heavily on data (those obtained for the MFS and from SemCor) that are produced manually, rather than semi-automatically.",5.3 Performance without backoff strategy,[0],[0]
"To further inspect the ability of T-O-M to enable disambiguation in different domains, we decided to evaluate on specific documents from the various gold standard datasets which could be clearly assigned a domain label.",5.4 Domain-oriented WSD,[0],[0]
"Specifically, we tested on 13 SemEval-13 documents from various domains6 and 2 SemEval-15 documents (namely, maths & computers, and biomedicine) and carried out two separate tests and evaluations of T-O-M on each domain: once using the MFS backoff strategy, and once not using it.",5.4 Domain-oriented WSD,[0],[0]
"In Tables 6 and 7 we report the results of both T-O-MWiki and T-O-MUN to determine the impact of the corpus type.
",5.4 Domain-oriented WSD,[0],[0]
"As can be seen in the tables, T-O-MWiki systematically attains higher scores than OMSTI (except for the biology domain), and, in most cases, attains higher scores than MFS when the backoff is used, with a drastic, systematic increase over OMSTI with both Train-O-Matic configurations
6Namely biology, climate, finance, health care, politics, social issues and sport.
in recall and F1 when the backoff strategy is disabled.",5.4 Domain-oriented WSD,[0],[0]
"This demonstrates the usefulness of the corpora annotated by Train-O-Matic not only on open text, but also on specific domains.",5.4 Domain-oriented WSD,[0],[0]
"We note that T-O-MUN obtains the best results in the politics domain, which is the closest domain to the UN corpus from which its training sentences are obtained.",5.4 Domain-oriented WSD,[0],[0]
"Experimental Setup In this section we investigate the ability of Train-O-Matic to scale to lowresourced languages, such as Italian and Spanish, for which training data for WSD is not available.
",6 Scaling up to Multiple Languages,[0],[0]
"Thanks to BabelNet, in fact, Train-O-Matic can
be used to generate sense-annotated data for any language supported by the knowledge base.",6 Scaling up to Multiple Languages,[0],[0]
"Thus, in order to build new training datasets for the two languages, we ran Train-O-Matic on their corresponding versions of Wikipedia, then we tuned the two parameters K and z on an in-house development dataset7.",6 Scaling up to Multiple Languages,[0],[0]
"In contrast to the English setting, in order to calculate Formula 8 we sorted the senses of each word by vertex degree.",6 Scaling up to Multiple Languages,[0],[0]
"Finally we used the output data to train IMS.
Results To perform our evaluation we chose the most recent multilingual task (SemEval 2015 task 13) which includes gold data for Italian and Spanish.",6 Scaling up to Multiple Languages,[0],[0]
"As can be seen from Table 8 TrainO-Matic enabled IMS to perform better than the best participating system (Manion and Sainudiin, 2014, SUDOKU) in all three settings (All domains, Maths & Computer and Biomedicine).",6 Scaling up to Multiple Languages,[0],[0]
"Its performance was in fact, 1 to 3 points higher, with a 6-point peak on Maths & Computer in Spanish and on Biomedicine in Italian.",6 Scaling up to Multiple Languages,[0],[0]
This demonstrates the ability of Train-O-Matic to enable supervised WSD systems to surpass state-of-theart knowledge-based WSD approaches in lowresourced languages without relying on manually curated data for training.,6 Scaling up to Multiple Languages,[0],[0]
There are two mainstream approaches to Word Sense Disambiguation: supervised and knowledge-based approaches.,7 Related Work,[0],[0]
"Both suffer in different ways from the so-called knowledge acquisition bottleneck, that is, the difficulty in obtaining an adequate amount of lexical-semantic data: for training in the case of supervised systems, and for enriching semantic networks in the case of knowledge-based ones (Pilehvar and
7We set K = 100 and z",7 Related Work,[0],[0]
"= 2.3 for Spanish and K = 100 and z = 2.5 for Italian.
Navigli, 2014; Navigli, 2009).
",7 Related Work,[0],[0]
"State-of-the-art supervised systems include Support Vector Machines such as IMS (Zhong and Ng, 2010) and, more recently, LSTM neural networks with attention and multitask learning (Raganato et al., 2017b) as well as LSTMs paired with nearest neighbours classification (Melamud et al., 2016; Yuan et al., 2016).",7 Related Work,[0],[0]
The latter also integrates a label propagation algorithm in order to enrich the sense annotated dataset.,7 Related Work,[0],[0]
"The main difference from our approach is its need for a manually annotated dataset to start the label propagation algorithm, whereas Train-O-Matic is fully automatic.",7 Related Work,[0],[0]
"An evaluation against this system would have been interesting, but neither the proprietary training data nor the code are available at the time of writing.
",7 Related Work,[0],[0]
"In order to generalize effectively, these supervised systems require large numbers of training in-
stances annotated with senses for each target word occurrence.",7 Related Work,[0],[0]
"Overall, this amounts to millions of training instances for each language of interest, a number that is not within reach for any language.",7 Related Work,[0],[0]
"In fact, no supervised system has been submitted in major multilingual WSD competitions for languages other than English (Navigli et al., 2013; Moro and Navigli, 2015).",7 Related Work,[0],[0]
"To overcome this problem, new methodologies have recently been developed which aim to create sense-tagged corpora automatically.",7 Related Work,[0],[0]
Raganato et al. (2016) developed 7 heuristics to grow the number of hyperlinks in Wikipedia pages.,7 Related Work,[0],[0]
"Otegi et al. (2016) applied a different disambiguation pipeline for each language to parallel text in Europarl (Koehn, 2005) and QTLeap (Agirre et al., 2015) in order to enrich them with semantic annotations.",7 Related Work,[0],[0]
"Taghipour and Ng (2015), the work closest to ours, exploits the alignment from English to Chinese sentences of
the United Nation Parallel Corpus (Ziemski et al., 2016) to reduce the ambiguity of English words and sense-tag English sentences.",7 Related Work,[0],[0]
The assumption is that the second language is less ambiguous than the first one and that hand-made translations of senses are available for each WordNet synset.,7 Related Work,[0],[0]
"This approach is, therefore, semi-automatic and relies on certain assumptions, in contrast to TrainO-Matic which is, instead, fully automatic and can be applied to any kind of corpus (and language) depending on the specific need.",7 Related Work,[0],[0]
Earlier attempts at the automatic extraction of training samples were made by Agirre and De Lacalle (2004) and Fernández et al. (2004).,7 Related Work,[0],[0]
"Both exploited the monosemous relatives method (Leacock et al., 1998) in order to retrieve sentences from the Web which contained a given monosemous noun or a relative monosemous word (e.g., a synonym, a hypernym, etc.).",7 Related Work,[0],[0]
"As can be seen in (Fernández et al., 2004)",7 Related Work,[0],[0]
"this approach can lead to the retrieval of very accurate examples, but its main drawback lies in the number of senses covered.",7 Related Work,[0],[0]
"In fact, for all those synsets that do not have any monosemous relative, the system is unable to retrieve examples, thus heavily affecting the performance in terms of recall and F1.",7 Related Work,[0],[0]
"Knowledge-based WSD, instead, bypasses the heavy requirement of sense-annotated corpora by applying algorithms that exploit a general-purpose semantic network, such as WordNet, which encodes the relational information that interconnects synsets via different kinds of relation.",7 Related Work,[0],[0]
"Approaches include variants of Personalized PageRank (Agirre et al., 2014) and densest subgraph approximation algorithms (Moro et al., 2014) which, thanks to the availability of multilingual resources such as BabelNet, can easily be extended to perform WSD in arbitrary languages.",7 Related Work,[0],[0]
Other approaches to knowledge-based WSD exploit the definitional knowledge contained in a dictionary.,7 Related Work,[0],[0]
"The Lesk algorithm (Lesk, 1986) and its variants (Banerjee and Pedersen, 2002; Kilgarriff and Rosenzweig, 2000; Vasilescu et al., 2004) aim to determine the correct sense of a word by comparing each wordsense definition with the context in which the target word appears.",7 Related Work,[0],[0]
"The limit of knowledge-based WSD, however, lies in the absence of mechanisms that can take into account the very local context of a target word occurrence, including non-content words such as prepositions and articles.",7 Related Work,[0],[0]
"Furthermore, recent studies seem to suggest that such
approaches are barely able to surpass supervised WSD systems when they enrich their networks starting from a comparable amount of annotated data (Pilehvar and Navigli, 2014).",7 Related Work,[0],[0]
"With T-O-M, rather than further enriching an existing semantic network, we exploit the information available in the network to annotate raw sentences with sense information and train a state-of-the-art supervised WSD system without task-specific human annotations.",7 Related Work,[0],[0]
"In this paper we presented Train-O-Matic, a novel approach to the automatic construction of large training sets for supervised WSD in an arbitrary language.",8 Conclusion,[0],[0]
"Train-O-Matic removes the burden of manual intervention by leveraging the structural semantic information available in the WordNet graph enriched with additional relational information from BabelNet, and achieves performance competitive to that of semi-automatic approaches and, in some cases, of manually-curated training data.",8 Conclusion,[0],[0]
"T-O-M was shown to provide training data for virtually all the target ambiguous nouns, in marked contrast to alternatives like OMSTI, which covers in many cases around half of the tokens, resorting to the MFS otherwise.",8 Conclusion,[0],[0]
"Moreover Train-O-Matic has proven to scale well to lowresourced languages, for which no manually annotated dataset exists, surpassing the current state of the art of knowledge-based systems.
",8 Conclusion,[0],[0]
"We believe that the ability of T-O-M to overcome the current paucity of annotated data for WSD, coupled with video games with a purpose for validation purposes (Jurgens and Navigli, 2014; Vannella et al., 2014), paves the way for high-quality multilingual supervised WSD.",8 Conclusion,[0],[0]
"All the training corpora, including approximately one million sentences which cover English, Italian and Spanish, are made available to the community at http://trainomatic.org.
",8 Conclusion,[0],[0]
"As future work we plan to extend our approach to verbs, adjectives and adverbs.",8 Conclusion,[0],[0]
Following Bennett et al. (2016) we will also experiment on more realistic estimates of P (s|w) in Formula 5 as well as other assumptions made in our work.,8 Conclusion,[0],[0]
The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE,Acknowledgments,[0],[0]
No.,Acknowledgments,[0],[0]
726487.,Acknowledgments,[0],[0]
Annotating large numbers of sentences with senses is the heaviest requirement of current Word Sense Disambiguation.,abstractText,[0],[0]
"We present Train-O-Matic, a languageindependent method for generating millions of sense-annotated training instances for virtually all meanings of words in a language’s vocabulary.",abstractText,[0],[0]
The approach is fully automatic: no human intervention is required and the only type of human knowledge used is a WordNet-like resource.,abstractText,[0],[0]
"Train-O-Matic achieves consistently state-of-the-art performance across gold standard datasets and languages, while at the same time removing the burden of manual annotation.",abstractText,[0],[0]
All the training data is available for research purposes at http://trainomatic.org.,abstractText,[0],[0]
Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1968–1978 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
Neural machine translation has recently become a method of choice in machine translation research.,1 Introduction,[0],[0]
"Besides its success in traditional settings of machine translation, that is one-to-one translation between two languages, (Sennrich et al., 2016; Chung et al., 2016), neural machine translation has ventured into more sophisticated settings of machine translation.",1 Introduction,[0],[0]
"For instance, neural machine translation has successfully proven itself to be capable of
handling subword-level representation of sentences (Lee et al., 2016; Luong and Manning, 2016; Sennrich et al., 2015; Costa-Jussa and Fonollosa, 2016; Ling et al., 2015).",1 Introduction,[0],[0]
"Furthermore, several research groups have shown its potential in seamlessly handling multiple languages (Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016a,b; Lee et al., 2016; Ha et al., 2016; Viégas et al., 2016).
",1 Introduction,[0],[0]
A typical scenario of neural machine translation starts with training a model to maximize its log-likelihood.,1 Introduction,[0],[0]
"That is, we often train a model to maximize the conditional probability of a reference translation given a source sentence over a large parallel corpus.",1 Introduction,[0],[0]
"Once the model is trained in this way, it defines the conditional distribution over all possible translations given a source sentence, and the task of translation becomes equivalent to finding a translation to which the model assigns the highest conditional probability.",1 Introduction,[0],[0]
"Since it is computationally intractable to do so exactly, it is a usual practice to resort to approximate search/decoding algorithms such as greedy decoding or beam search.",1 Introduction,[0],[0]
"In this scenario, we have identified two points where improvements could be made.",1 Introduction,[0],[0]
"They are (1) training (including the selection of a model architecture) and (2) decoding.
",1 Introduction,[0],[0]
"Much of the research on neural machine translation has focused solely on the former, that is, on improving the model architecture.",1 Introduction,[0],[0]
"Neural machine translation started with with a simple encoderdecoder architecture in which a source sentence is encoded into a single, fixed-size vector (Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).",1 Introduction,[0],[0]
"It soon evolved with the attention mechanism (Bahdanau et al., 2014).",1 Introduction,[0],[0]
"A few variants of the attention mechanism, or its regularization, have been proposed recently to improve both the translation quality as well as the computational efficiency (Luong et al., 2015b; Cohn et al., 2016; Tu et al., 2016b).",1 Introduction,[0],[0]
"More recently, convolutional net-
1968
works have been adopted either as a replacement of or a complement to a recurrent network in order to efficiently utilize parallel computing (Kalchbrenner et al., 2016; Lee et al., 2016; Gehring et al., 2016).
",1 Introduction,[0],[0]
"On the aspect of decoding, only a few research groups have tackled this problem by incorporating a target decoding algorithm into training.",1 Introduction,[0],[0]
Wiseman and Rush (2016) and Shen et al. (2015) proposed a learning algorithm tailored for beam search.,1 Introduction,[0],[0]
"Ranzato et al. (2015) and (Bahdanau et al., 2016) suggested to use a reinforcement learning algorithm by viewing a neural machine translation model as a policy function.",1 Introduction,[0],[0]
"Investigation on decoding alone has, however, been limited.",1 Introduction,[0],[0]
Cho (2016) showed the limitation of greedy decoding by simply injecting unstructured noise into the hidden state of the neural machine translation system.,1 Introduction,[0],[0]
"Tu et al. (2016a) similarly showed that the exactness of beam search does not correlate well with actual translation quality, and proposed to augment the learning cost function with reconstruction to alleviate this problem.",1 Introduction,[0],[0]
"Li et al. (2016) proposed a modification to the existing beam search algorithm to improve its exploration of the translation space.
",1 Introduction,[0],[0]
"In this paper, we tackle the problem of decoding in neural machine translation by introducing a concept of trainable greedy decoding.",1 Introduction,[0],[0]
"Instead of manually designing a new decoding algorithm suitable for neural machine translation, we propose to learn a decoding algorithm with an arbitrary decoding objective.",1 Introduction,[0],[0]
"More specifically, we introduce a neural-network-based decoding algorithm that works on an already-trained neural machine translation system by observing and manipulating its hidden state.",1 Introduction,[0],[0]
"We treat such a neural network as an agent with a deterministic, continuous action and train it with a variant of the deterministic policy gradient algorithm (Silver et al., 2014).
",1 Introduction,[0],[0]
"We extensively evaluate the proposed trainable greedy decoding on four language pairs (En-Cs, En-De, En-Ru and En-Fi; in both directions) with two different decoding objectives; sentence-level BLEU and negative perplexity.",1 Introduction,[0],[0]
"By training such trainable greedy decoding using deterministic policy gradient with the proposed critic-aware actor learning, we observe that we can improve decoding performance with minimal computational overhead.",1 Introduction,[0],[0]
"Furthermore, the trained actors are found to improve beam search as well, suggesting a future research direction in extending the proposed idea of trainable decoding for more sophisticated
underlying decoding algorithms.",1 Introduction,[0.9666781156097238],"['In practice, Srinivas and Babu use a distance measure based on the difference of the incoming weight vectors to search for similar neurons as exact matches are very rare.']"
"Neural machine translation is a special case of conditional recurrent language modeling, where the source and target are natural language sentences.",2.1 Neural Machine Translation,[0],[0]
"Let us use X = {x1, . . .",2.1 Neural Machine Translation,[0],[0]
", xTs} and Y = {y1, . . .",2.1 Neural Machine Translation,[0],[0]
", yT } to denote source and target sentences, respectively.",2.1 Neural Machine Translation,[0],[0]
"Neural machine translation then models the target sentence given the source sentence as: p(Y |X) = ∏Tt=1 p(yt|y<t, X).",2.1 Neural Machine Translation,[0],[0]
Each term on the r.h.s.,2.1 Neural Machine Translation,[0],[0]
"of the equation above is modelled as a composite of two parametric functions:
p(yt|y<t, X) ∝",2.1 Neural Machine Translation,[0],[0]
"exp (g (yt, zt; θg)) ,
where zt = f(zt−1, yt−1, et(X; θe); θf ).",2.1 Neural Machine Translation,[0],[0]
"g is a read-out function that transforms the hidden state zt into the distribution over all possible symbols, and f is a recurrent function that compresses all the previous target words y<t and the time-dependent representation et(X; θe) of the source sentence X .",2.1 Neural Machine Translation,[0],[0]
"This time-dependent representation et is often implemented as a recurrent network encoder of the source sentence coupled with an attention mechanism (Bahdanau et al., 2014).
",2.1 Neural Machine Translation,[0],[0]
"Maximum Likelihood Learning We train a neural machine translation model, or equivalently estimate the parameters θg, θf and θe, by maximizing the log-probability of a reference translation Ŷ = {ŷ1, ..., ŷT } given a source sentence.",2.1 Neural Machine Translation,[0],[0]
"That is, we maximize the log-likelihood function:
JML(θg, θf , θe) = 1 N N∑ n=1",2.1 Neural Machine Translation,[0],[0]
"Tn∑ t=1 log pθ(ŷnt |ŷn<t, Xn),
given a training set consisting of N source-target sentence pairs.",2.1 Neural Machine Translation,[0],[0]
It is important to note that this maximum likelihood learning does not take into account how a trained model would be used.,2.1 Neural Machine Translation,[0],[0]
"Rather, it is only concerned with learning a distribution over all possible translations.",2.1 Neural Machine Translation,[0],[0]
"Once the model is trained, either by maximum likelihood learning or by any other recently proposed algorithms (Wiseman and Rush, 2016; Shen et al., 2015; Bahdanau et al., 2016; Ranzato et al., 2015), we can let the model translate a given sentence by
finding a translation that maximizes
Ŷ = arg max Y
log pθ(Y |X),
where θ = (θg, θf , θe).",2.2 Decoding,[0],[0]
"This is, however, computationally intractable, and it is a usual practice to resort to approximate decoding algorithms.
",2.2 Decoding,[0.9595645135960954],"['Furthermore, it is often much easier to stage a single NMT system than an ensemble in a commercial MT workflow, and it is crucial to be able to optimize quality at specific speed and memory constraints.']"
Greedy Decoding One such approximate decoding algorithm is greedy decoding.,2.2 Decoding,[0],[0]
"In greedy decoding, we follow the conditional dependency path and pick the symbol with the highest conditional probability so far at each node.",2.2 Decoding,[0],[0]
This is equivalent to picking the best symbol one at a time from left to right in conditional language modelling.,2.2 Decoding,[0],[0]
"A decoded translation of greedy decoding is Ŷ = (ŷ1, . . .",2.2 Decoding,[0],[0]
", ŷT ), where
ŷt = arg max y∈V
log pθ(y|ŷ<t, X).",2.2 Decoding,[0],[0]
"(1)
Despite its preferable computational complexity O(|V | × T ), greedy decoding has been over time found to be undesirably sub-optimal.
",2.2 Decoding,[0],[0]
"Beam Search Beam search keeps K > 1 hypotheses, unlike greedy decoding which keeps only a single hypothesis during decoding.",2.2 Decoding,[0],[0]
"At each time step t, beam search picks K hypotheses with the highest scores ( ∏t t′=1 p(yt|y<t, X)).",2.2 Decoding,[0],[0]
"When all the hypotheses terminate (outputting the end-of-thesentence symbol), it returns the hypothesis with the highest log-probability.",2.2 Decoding,[0],[0]
"Despite its superior performance compared to greedy decoding, the computational complexity grows linearly w.r.t.",2.2 Decoding,[0],[0]
"the size of beam K, which makes it less preferable especially in the production environment.",2.2 Decoding,[0],[0]
"Although we have described decoding in neural machine translation as a maximum-a-posteriori estimation in log p(Y |X), this is not necessarily the only nor the desirable decoding objective.
",3.1 Many Decoding Objectives,[0],[0]
"First, each potential scenario in which neural machine translation is used calls for a unique decoding objective.",3.1 Many Decoding Objectives,[0],[0]
"In simultaneous translation/interpretation, which has recently been studied in the context of neural machine translation (Gu et al., 2016), the decoding objective is formulated as a trade-off between the translation quality and delay.",3.1 Many Decoding Objectives,[0],[0]
"On the other hand, when a machine translation system is used as a part of a larger information
extraction system, it is more important to correctly translate named entities and events than to translate syntactic function words.",3.1 Many Decoding Objectives,[0],[0]
"The decoding objective in this case must account for how the translation is used in subsequent modules in a larger system.
",3.1 Many Decoding Objectives,[0],[0]
"Second, the conditional probability assigned by a trained neural machine translation model does not necessarily reflect our perception of translation quality.",3.1 Many Decoding Objectives,[0],[0]
"Although Cho (2016) provided empirical evidence of high correlation between the logprobability and BLEU, a de facto standard metric in machine translation, there have also been reports on large mismatch between the log-probability and BLEU.",3.1 Many Decoding Objectives,[0],[0]
"For instance, Tu et al. (2016a) showed that beam search with a very large beam, which is supposed to find translations with better logprobabilities, suffers from pathological translations of very short length, resulting in low translation quality.",3.1 Many Decoding Objectives,[0],[0]
"This calls for a way to design or learn a decoding algorithm with an objective that is more directly correlated to translation quality.
",3.1 Many Decoding Objectives,[0],[0]
"In short, there is a significant need for designing multiple decoding algorithms for neural machine translation, regardless of how it was trained.",3.1 Many Decoding Objectives,[0],[0]
It is however non-trivial to manually design a new decoding algorithm with an arbitrary objective.,3.1 Many Decoding Objectives,[0],[0]
"This is especially true with neural machine translation, as the underlying structure of the decoding/search process – the high-dimensional hidden state of a recurrent network – is accessible but not interpretable.",3.1 Many Decoding Objectives,[0],[0]
"Instead, in the remainder of this section, we propose our approach of trainable greedy decoding.",3.1 Many Decoding Objectives,[0],[0]
"We start from the noisy, parallel approximate decoding (NPAD) algorithm proposed in (Cho, 2016).",3.2 Trainable Greedy Decoding,[0],[0]
The main idea behind NPAD algorithm is that a better translation with a higher log-probability may be found by injecting unstructured noise in the transition function of a recurrent network.,3.2 Trainable Greedy Decoding,[0],[0]
"That is,
zt = f(zt−1 + t, yt−1, et(X; θe); θf ),
where t ∼ N (0, (σ0/t)2).",3.2 Trainable Greedy Decoding,[0],[0]
NPAD avoids potential degradation of translation quality by running such a noisy greedy decoding process multiple times in parallel.,3.2 Trainable Greedy Decoding,[0],[0]
"An important lesson of NPAD algorithm is that there exists a decoding strategy with the asymptotically same computational complexity that results in a better translation quality, and that such a better translation can be found by manipulating the hidden state of the recurrent network.
",3.2 Trainable Greedy Decoding,[0],[0]
"In this work, we propose to significantly extend NPAD by replacing the unstructured noise t with a parametric function approximator, or an agent, πφ.",3.2 Trainable Greedy Decoding,[0],[0]
"This agent takes as input the previous hidden state zt−1, previously decoded word ŷt−1 and the time-dependent context vector et(X; θe) and outputs a real-valued vectorial action at ∈ Rdim(zt).",3.2 Trainable Greedy Decoding,[0],[0]
"Such an agent is trained such that greedy decoding with the agent finds a translation that maximizes any predefined, arbitrary decoding objective, while the underlying neural machine translation model is pretrained and fixed.",3.2 Trainable Greedy Decoding,[0],[0]
"Once the agent is trained, we generate a translation given a source sentence by greedy decoding however augmented with this agent.",3.2 Trainable Greedy Decoding,[0],[0]
"We call this decoding strategy trainable greedy decoding.
",3.2 Trainable Greedy Decoding,[0],[0]
Related Work:,3.2 Trainable Greedy Decoding,[0],[0]
"Soothsayer prediction function Independently from and concurrently with our work here, Li et al. (2017) proposed, just two weeks earlier, to train a neural network that predicts an arbitrary decoding objective given a source sentence and a partial hypothesis, or a prefix of translation, and to use it as an auxiliary score in beam search.",3.2 Trainable Greedy Decoding,[0],[0]
"For training such a network, referred to as a Q network in their paper, they generate each training example by either running beam search or using a ground-truth translation (when appropriate) for each source sentence.",3.2 Trainable Greedy Decoding,[0],[0]
"This approach allows one to use an arbitrary decoding objective, but it still re-
lies heavily on the log-probability of the underlying neural translation system in actual decoding.",3.2 Trainable Greedy Decoding,[0],[0]
We expect a combination of these and our approaches may further improve decoding for neural machine translation in the future.,3.2 Trainable Greedy Decoding,[0],[0]
"While all the parameters—θg, θf and θe— of the underlying neural translation model are fixed, we only update the parameters φ of the agent π.",3.3 Learning and Challenges,[0],[0]
"This ensures the generality of the pretrained translation model, and allows us to train multiple trainable greedy decoding agents with different decoding objectives, maximizing the utility of a single trained translation model.
",3.3 Learning and Challenges,[0],[0]
Let us denote by R our arbitrary decoding objective as a function that scores a translation generated from trainable greedy decoding.,3.3 Learning and Challenges,[0],[0]
"Then, our learning objective for trainable greedy decoding is
JA(φ) = EŶ=Gπ(X)X∼D",3.3 Learning and Challenges,[0],[0]
"[ R(Ŷ ) ] ,
where we used Gπ(X) as a shorthand for trainable greedy decoding with an agent π.
",3.3 Learning and Challenges,[0],[0]
There are two major challenges in learning an agent with such an objective.,3.3 Learning and Challenges,[0],[0]
"First, the decoding objective R may not be differentiable with respect to the agent.",3.3 Learning and Challenges,[0],[0]
"Especially because our goal is to accommodate an arbitrary decoding objective, this becomes a problem.",3.3 Learning and Challenges,[0],[0]
"For instance, BLEU, a standard
quality metric in machine translation, is a piecewise linear function with zero derivatives almost everywhere.",3.3 Learning and Challenges,[0],[0]
"Second, the agent here is a real-valued, deterministic policy with a very high-dimensional action space (1000s of dimensions), which is well known to be difficult.",3.3 Learning and Challenges,[0],[0]
"In order to alleviate these difficulties, we propose to use a variant of the deterministic policy gradient algorithm (Silver et al., 2014; Lillicrap et al., 2015).",3.3 Learning and Challenges,[0],[0]
"It is highly unlikely for us to have access to the gradient of an arbitrary decoding objective R with respect to the agent π, or its parameters φ.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Furthermore, we cannot estimate it stochastically because our policy π is defined to be deterministic without a predefined nor learned distribution over the action.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Instead, following (Silver et al., 2014; Lillicrap et al., 2015), we use a parametric, differentiable approximator, called a critic Rc, for the non-differentiable objective R. We train the critic by minimizing
JC(ψ)",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
= EŶ=Gπ(X)X∼D [ Rcψ(z1:T )−R(Ŷ ),4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"]2 .
",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"The critic observes the state-action sequence of the agent π via the modified hidden states (z1, . . .",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
", zT )",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"of the recurrent network, and predicts the associated decoding objective.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"By minimizing the mean squared error above, we effectively encourage the critic to approximate the non-differentiable objective as closely as possible in the vicinity of the state-action sequence visited by the agent.
",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"We implement the critic Rc as a recurrent network, similarly to the underlying neural machine translation system.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"This implies that we can compute the derivative of the predicted decoding objective with respect to the input, that is, the state-action sequence z1:T , which allows us to update the actor π, or equivalently its parameters φ, to maximize the predicted decoding objective.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Effectively we avoid the issue of non-differentiability of the original decoding objective by working with its proxy.
",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"With the critic, the learning objective of the actor is now to maximize not the original decoding objective R but its proxy RC such that
ĴA(φ)",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
= EŶ=Gπ(X)X∼D,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"[ RC(Ŷ ) ] .
",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Algorithm 1 Trainable Greedy Decoding Require: NMT θ, actor φ, critic ψ, Nc, Na, Sc, Sa, τ
1: Train θ using MLE on training set D; 2: Initialize φ and ψ; 3:",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
Shuffle D twice into Dφ and Dψ 4: while stopping criterion is not met do 5: for t = 1 :,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Nc do 6: Draw a translation pair: (X,Y ) ∼ Dψ; 7: r, rc = DECODE(Sc, X, Y, 1) 8:",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
Update ψ using∇ψ ∑ k (r c,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
k,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"− rk)2/(Sc + 1)
9: for t = 1 : Na do 10: Draw a translation pair: (X,Y ) ∼ Dφ; 11: r, rc = DECODE(Sa, X, Y, 0) 12: Compute wk = exp
(− (rck − rk)2 /τ) 13: Compute w̃k = wk/ ∑ k wk
14: Update φ using −∑k",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
(w̃k · ∇φrck),4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Function: DECODE(S,X, Y, c)
1: Ys = {}, Zs = {}, r = {}, rc = {}; 2: for k = 1 : S do 3: Sample noise ∼ N (0, σ2) for each action; 4: Greedy decoding Ŷ k = Gθ,φ(X) with ; 5: Collect hidden states zk1:T given X , Ŷ , θ, φ 6: Ys ← Ys ∪ {Y k} 7:",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Zs ← Zs ∪ {zk1:T } 8: if c = 1 then 9: Collect hidden states z1:T given X , Y , θ
10: Ys ← Ys ∪ {Y } 11:",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Zs ← Zs ∪ {z1:T } 12: for Ŷ , Z ∈ Ys, Zs do 13: Compute the critic output rc ← Rcψ(Z, Ŷ ) 14: Compute true reward r ← R(Y, Ŷ ) 15: return r, rc
Unlike the original objective, this objective function is fully differentiable with respect to the agent π.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"We thus use a usual stochastic gradient descent algorithm to train the agent, while simultaneously training the critic.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
We do so by alternating between training the actor and critic.,4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
"Note that we maximize the return of a full episode rather than the Q value, unlike usual approaches in reinforcement learning.",4.1 Deterministic Policy Gradient for Trainable Greedy Decoding,[0],[0]
Challenges The most apparent challenge for training such a deterministic actor with a large action space is that most of action configurations will lead to zero return.,4.2 Critic-Aware Actor Learning,[0],[0]
It is also not trivial to devise an efficient exploration strategy with a deterministic actor with real-valued actions.,4.2 Critic-Aware Actor Learning,[0],[0]
"This issue has however turned out to be less of a problem than in a usual reinforcement learning setting, as the state and action spaces are well structured thanks to pretraining by maximum likelihood learning.",4.2 Critic-Aware Actor Learning,[0],[0]
"As observed by Cho (2016), any reasonable perturbation to the hidden state of the recurrent network generates a reasonable translation which would re-
ceive again a reasonable return.
",4.2 Critic-Aware Actor Learning,[0],[0]
"Although this property of dense reward makes the problem of trainable greedy decoding more manageable, we have observed other issues during our preliminary experiment with the vanilla deterministic policy gradient.",4.2 Critic-Aware Actor Learning,[0],[0]
"In order to avoid these issues that caused instability, we propose the following modifications to the vanilla algorithm.
",4.2 Critic-Aware Actor Learning,[0],[0]
"Critic-Aware Actor Learning A major goal of the critic is not to estimate the return of a given episode, but to estimate the gradient of the return evaluated given an episode.",4.2 Critic-Aware Actor Learning,[0],[0]
"In order to do so, the critic must be trained, or presented, with stateaction sequences z1:T ′ similar though not identical to the state-action sequence generated by the current actor π.",4.2 Critic-Aware Actor Learning,[0],[0]
"This is achieved, in our case, by injecting unstructured noise to the action at each
time step, similar to (Heess et al., 2015):
ãt = φ(zt, at−1) + σ · , (2)
where is a zero-mean, unit-variance normal variable.",4.2 Critic-Aware Actor Learning,[0],[0]
"This noise injection procedure is mainly used when training the critic.
",4.2 Critic-Aware Actor Learning,[0],[0]
We have however observed that the quality of the reward and its gradient estimate of the critic is very noisy even when the critic was trained with this kind of noisy actor.,4.2 Critic-Aware Actor Learning,[0],[0]
This imperfection of the critic often led to the instability in training the actor in our preliminary experiments.,4.2 Critic-Aware Actor Learning,[0],[0]
"In order to avoid this, we describe here a technique which we refer to as critic-aware actor gradient estimation.
",4.2 Critic-Aware Actor Learning,[0],[0]
"Instead of using the point estimate ∂R c
∂φ of the gradient of the predicted objective with respect to the actor’s parameters φ, we propose to use the expected gradient of the predicted objective with
respect to the critic-aware distribution Q.",4.2 Critic-Aware Actor Learning,[0],[0]
"That is,
EQ [ ∂Rcψ ∂φ ] , (3)
where we define the critic-aware distribution Q as
Q( ) ∝",4.2 Critic-Aware Actor Learning,[0],[0]
exp(−(Rcψ −R)2/τ︸,4.2 Critic-Aware Actor Learning,[0],[0]
︷︷ ︸,4.2 Critic-Aware Actor Learning,[0],[0]
"Critic-awareness
) exp(− 2
2σ2︸ ︷︷ ︸ Locality ).",4.2 Critic-Aware Actor Learning,[0],[0]
"(4)
",4.2 Critic-Aware Actor Learning,[0],[0]
"This expectation allows us to incorporate the noisy, non-uniform nature of the critic’s approximation of the objective by up-weighting the gradient computed at a point with a higher critic quality and down-weighting the gradient computed at a point with a lower critic quality.",4.2 Critic-Aware Actor Learning,[0],[0]
"The first term in Q reflects this, while the second term ensures that our estimation is based on a small region around the state-action sequence generated by the current, noise-free actor π.
",4.2 Critic-Aware Actor Learning,[0],[0]
Since it is intractable to compute Eq.,4.2 Critic-Aware Actor Learning,[0],[0]
"(3) exactly, we resort to importance sampling with the proposed distribution equal to the second term in Eq.",4.2 Critic-Aware Actor Learning,[0],[0]
(4).,4.2 Critic-Aware Actor Learning,[0],[0]
"Then, our gradient estimate for the actor becomes the sum of the gradients from multiple realizations of the noisy actor in Eq.",4.2 Critic-Aware Actor Learning,[0],[0]
"(2), where each gradient is weighted by the quality of the critic exp(−(Rcφ − R)2/τ).",4.2 Critic-Aware Actor Learning,[0],[0]
τ is a hyperparameter that controls the smoothness of the weights.,4.2 Critic-Aware Actor Learning,[0],[0]
"We observed in our preliminary experiment that the use of this criticaware actor learning significantly stabilizes general learning of both the actor and critic.
",4.2 Critic-Aware Actor Learning,[0],[0]
"Reference Translations for Training the Critic In our setting of neural machine translation, we have access to a reference translation for each source sentence X , unlike in a usual setting of reinforcement learning.",4.2 Critic-Aware Actor Learning,[0],[0]
"By force-feeding the reference translation into the underlying neural machine translation system (rather than feeding the decoded symbols), we can generate the reference state-action sequence.",4.2 Critic-Aware Actor Learning,[0],[0]
"This sequence is much less correlated with those sequences generated by the actor, and facilitates computing a better estimate of the gradient w.r.t.",4.2 Critic-Aware Actor Learning,[0],[0]
"the critic.
",4.2 Critic-Aware Actor Learning,[0],[0]
"In Alg. 1, we present the complete algorithm.",4.2 Critic-Aware Actor Learning,[0],[0]
"To make the description less cluttered, we only show the version of minibatch size = 1 which can be naturally extended.",4.2 Critic-Aware Actor Learning,[0],[0]
We also illustrate the proposed trainable greedy decoding and the proposed learning strategy in Fig. 1.,4.2 Critic-Aware Actor Learning,[0],[0]
"We empirically evaluate the proposed trainable greedy decoding on four language pairs – EnDe, En-Ru, En-Cs and En-Fi – using a standard attention-based neural machine translation system (Bahdanau et al., 2014).",5 Experimental Settings,[0],[0]
We train underlying neural translation systems using the parallel corpora made available from WMT’15.1 The same set of corpora are used for trainable greedy decoding as well.,5 Experimental Settings,[0],[0]
"All the corpora are tokenized and segmented into subword symbols using byte-pair encoding (BPE) (Sennrich et al., 2015).",5 Experimental Settings,[0],[0]
We use sentences of length up to 50 subword symbols for MLE training and 200 symbols for trainable decoding.,5 Experimental Settings,[0],[0]
"For validation and testing, we use newstest-2013 and newstest-2015, respectively.",5 Experimental Settings,[0],[0]
"Underlying NMT Model For each language pair, we implement an attention-based neural machine translation model whose encoder and decoder recurrent networks have 1,028 gated recurrent units (GRU, Cho et al., 2014) each.",5.1 Model Architectures and Learning,[0],[0]
Source and target symbols are projected into 512-dimensional embedding vectors.,5.1 Model Architectures and Learning,[0],[0]
"We trained each model for approximately 1.5 weeks using Adadelta (Zeiler, 2012).
",5.1 Model Architectures and Learning,[0],[0]
Actor π,5.1 Model Architectures and Learning,[0],[0]
We use a feedforward network with a single hidden layer as the actor.,5.1 Model Architectures and Learning,[0],[0]
"The input is a 2,056-dimensional vector which is the concatenation of the decoder hidden state and the timedependent context vector from the attention mech-
1http://www.statmt.org/wmt15/
anism, and it outputs a 1,028-dimensional action vector for the decoder.",5.1 Model Architectures and Learning,[0],[0]
"We use 32 units for the hidden layer with tanh activations.
",5.1 Model Architectures and Learning,[0],[0]
Critic Rc The critic is implemented as a variant of an attention-based neural machine translation model that takes a reference translation as a source sentence and a state-action sequence from the actor as a target sentence.,5.1 Model Architectures and Learning,[0],[0]
Both the size of GRU units and embedding vectors are the same with the underlying model.,5.1 Model Architectures and Learning,[0],[0]
"Unlike a usual neural machine translation system, the critic does not language-model the target sentence but simply outputs a scalar value to predict the true return.",5.1 Model Architectures and Learning,[0],[0]
"When we predict a bounded return, such as sentence BLEU, we use a sigmoid activation at the output.",5.1 Model Architectures and Learning,[0],[0]
"For other unbounded return like perplexity, we use a linear activation.
",5.1 Model Architectures and Learning,[0],[0]
Learning We train the actor and critic simultaneously by alternating between updating the actor and critic.,5.1 Model Architectures and Learning,[0],[0]
"As the quality of the critic’s approximation of the decoding objective has direct influence on the actor’s learning, we make ten updates to the critic before each time we update the actor once.",5.1 Model Architectures and Learning,[0],[0]
"We use RMSProp (Tieleman and Hinton, 2012) with the initial learning rates of 2× 10−6 and 2× 10−4, respectively, for the actor and critic.
",5.1 Model Architectures and Learning,[0],[0]
We monitor the progress of learning by measuring the decoding objective on the validation set.,5.1 Model Architectures and Learning,[0],[0]
"After training, we pick the actor that results in the best decoding objective on the validation set, and test it on the test set.
",5.1 Model Architectures and Learning,[0],[0]
"Decoding Objectives For each neural machine translation model, pretrained using maximum likelihood criterion, we train two trainable greedy decoding actors.",5.1 Model Architectures and Learning,[0],[0]
"One actor is trained to maximize BLEU (or its smoothed version for sentence-level
scoring (Lin and Och, 2004))",5.1 Model Architectures and Learning,[0],[0]
"as its decoding objective, and the other to minimize perplexity (or equivalently the negative log-probability normalized by the length.)
",5.1 Model Architectures and Learning,[0],[0]
We have chosen the first two decoding objectives for two purposes.,5.1 Model Architectures and Learning,[0],[0]
"First, we demonstrate that it is possible to build multiple trainable decoders with a single underlying model trained using maximum likelihood learning.",5.1 Model Architectures and Learning,[0],[0]
"Second, the comparison between these two objectives provides a glimpse into the relationship between BLEU (the most widely used automatic metric for evaluating translation systems) and log-likelihood (the most widely used learning criterion for neural machine translation).
",5.1 Model Architectures and Learning,[0],[0]
Evaluation We test the trainable greedy decoder with both greedy decoding and beam search.,5.1 Model Architectures and Learning,[0],[0]
"Although our decoder is always trained with greedy decoding, beam search in practice can be used together with the actor of the trainable greedy decoder.",5.1 Model Architectures and Learning,[0],[0]
Beam search is expected to work better especially when our training of the trainable greedy decoder is unlikely to be optimal.,5.1 Model Architectures and Learning,[0],[0]
"In both cases, we report both the perplexity and BLEU.",5.1 Model Architectures and Learning,[0],[0]
We present the improvements of BLEU and perplexity (or its negation) in Fig. 2 for all the language pair-directions.,5.2 Results and Analysis,[0],[0]
It is clear from these plots that the best result is achieved when the trainable greedy decoder was trained to maximize the target decoding objective.,5.2 Results and Analysis,[0],[0]
"When the decoder was trained to maximize sentence-level BLEU, we see the improvement in BLEU but often the degradation in the perplexity (see the left plots in Fig. 2.)",5.2 Results and Analysis,[0],[0]
"On the other hand, when the actor was trained to minimize the perplexity, we only see the improvement in per-
plexity (see the right plots in Fig. 2.)",5.2 Results and Analysis,[0],[0]
"This confirms our earlier claim that it is necessary and desirable to tune for the target decoding objective regardless of what the underlying translation system was trained for, and strongly supports the proposed idea of trainable decoding.
",5.2 Results and Analysis,[0],[0]
"The improvement from using the proposed trainable greedy decoding is smaller when used together with beam search, as seen in Fig. 2 (b).",5.2 Results and Analysis,[0],[0]
"However, we still observe statistically significant improvement in terms of BLEU (marked with red stars.)",5.2 Results and Analysis,[0],[0]
"This suggests a future direction in which we extend the proposed trainable greedy decoding to directly incorporate beam search into its training procedure to further improve the translation quality.
",5.2 Results and Analysis,[0],[0]
It is worthwhile to note that we achieved all of these improvements with negligible computational overhead.,5.2 Results and Analysis,[0],[0]
"This is due to the fact that our actor is a very small, shallow neural network, and that the more complicated critic is thrown away after training.",5.2 Results and Analysis,[0],[0]
We suspect the effectiveness of such a small actor is due to the well-structured hidden state space of the underlying neural machine translation model which was trained with a large amount of parallel corpus.,5.2 Results and Analysis,[0],[0]
"We believe this favourable computational complexity makes the proposed method suitable for production-grade neural machine translation (Wu et al., 2016; Crego et al., 2016).
",5.2 Results and Analysis,[0],[0]
"Importance of Critic-Aware Actor Learning In Fig. 3, we show sample learning curves with and without the proposed critic-aware actor learning.",5.2 Results and Analysis,[0],[0]
Both curves were from the models trained under the same condition.,5.2 Results and Analysis,[0],[0]
"Despite a slower start in the early stage of learning, we see that the critic-aware actor learning has greatly stabilized the learning progress.",5.2 Results and Analysis,[0],[0]
"We emphasize that we would not have been able to train all these 16 actors without the proposed critic-aware actor learning.
",5.2 Results and Analysis,[0],[0]
"Examples In Fig. 4, we present three examples from Ru-En.",5.2 Results and Analysis,[0],[0]
"We defined the influence as the KL divergence between the conditional distributions without the trainable greedy decoding and with the trainable greedy decoding, assuming the fixed previous hidden state and target symbol.",5.2 Results and Analysis,[0],[0]
"We colored a target word with magenta, when the influence of the trainable greedy decoding is large (> 0.001).",5.2 Results and Analysis,[0],[0]
Manual inspection of these examples as well as others has revealed that the trainable greedy decoder focuses on fixing prepositions and removing any unnecessary symbol generation.,5.2 Results and Analysis,[0],[0]
"More in-depth
analysis is however left as future work.",5.2 Results and Analysis,[0],[0]
We proposed trainable greedy decoding as a way to learn a decoding algorithm for neural machine translation with an arbitrary decoding objective.,6 Conclusion,[0],[0]
"The proposed trainable greedy decoder observes and manipulates the hidden state of a trained neural translation system, and is trained by a novel variant of deterministic policy gradient, called critic-aware actor learning.",6 Conclusion,[0],[0]
Our extensive experiments on eight language pair-directions and two objectives confirmed its validity and usefulness.,6 Conclusion,[0],[0]
"The proposed trainable greedy decoding is a generic idea that can be applied to any recurrent language modeling, and we anticipate future research both on the fundamentals of the trainable decoding as well as on the applications to more diverse tasks such as image caption generating and dialogue modeling.",6 Conclusion,[0],[0]
"KC thanks the support by TenCent, eBay, Facebook, Google (Google Faculty Award 2016) and NVidia.",Acknowledgement,[0],[0]
This work was partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI).,Acknowledgement,[0],[0]
"We sincerely thank Martin Arjovsky, Zihang Dai, Graham Neubig, Pengcheng Yin and Chunting Zhou for helpful discussions and insightful feedbacks.",Acknowledgement,[0],[0]
Recent research in neural machine translation has largely focused on two aspects; neural network architectures and end-toend learning algorithms.,abstractText,[0],[0]
"The problem of decoding, however, has received relatively little attention from the research community.",abstractText,[0],[0]
"In this paper, we solely focus on the problem of decoding given a trained neural machine translation model.",abstractText,[0],[0]
"Instead of trying to build a new decoding algorithm for any specific decoding objective, we propose the idea of trainable decoding algorithm in which we train a decoding algorithm to find a translation that maximizes an arbitrary decoding objective.",abstractText,[0],[0]
"More specifically, we design an actor that observes and manipulates the hidden state of the neural machine translation decoder and propose to train it using a variant of deterministic policy gradient.",abstractText,[0],[0]
"We extensively evaluate the proposed algorithm using four language pairs and two decoding objectives, and show that we can indeed train a trainable greedy decoder that generates a better translation (in terms of a target decoding objective) with minimal computational overhead.",abstractText,[0],[0]
Trainable Greedy Decoding for Neural Machine Translation,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1884–1895 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1884",text,[0],[0]
"The standard protocol for obtaining a labeled dataset is to have a human annotator view each example, assess its relevance, and provide a label (e.g., positive or negative for binary classification).",1 Introduction,[0],[0]
"However, this only provides one bit of information per example.",1 Introduction,[0],[0]
"This invites the question: how can we get more information per example, given that the annotator has already spent the effort reading and understanding an example?
",1 Introduction,[0],[0]
"Previous works have relied on identifying relevant parts of the input such as labeling features (Druck et al., 2009; Raghavan et al., 2005; Liang et al., 2009), highlighting rationale phrases in
text (Zaidan and Eisner, 2008; Arora and Nyberg, 2009), or marking relevant regions in images (Ahn et al., 2006).",1 Introduction,[0],[0]
"But there are certain types of information which cannot be easily reduced to annotating a portion of the input, such as the absence of a certain word, or the presence of at least two words.",1 Introduction,[0],[0]
"In this work, we tap into the power of natural language and allow annotators to provide supervision to a classifier via natural language explanations.
",1 Introduction,[0],[0]
"Specifically, we propose a framework in which annotators provide a natural language explanation for each label they assign to an example (see Figure 1).",1 Introduction,[0],[0]
"These explanations are parsed into logical forms representing labeling functions (LFs), functions that heuristically map examples to labels (Ratner et al., 2016).",1 Introduction,[0],[0]
"The labeling functions are
then executed on many unlabeled examples, resulting in a large, weakly-supervised training set that is then used to train a classifier.
",1 Introduction,[0],[0]
"Semantic parsing of natural language into logical forms is recognized as a challenging problem and has been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Liang, 2016).",1 Introduction,[0],[0]
"One of our major findings is that in our setting, even a simple rule-based semantic parser suffices for three reasons: First, we find that the majority of incorrect LFs can be automatically filtered out either semantically (e.g., is it consistent with the associated example?) or pragmatically (e.g., does it avoid assigning the same label to the entire training set?).",1 Introduction,[0],[0]
"Second, LFs near the gold LF in the space of logical forms are often just as accurate (and sometimes even more accurate).",1 Introduction,[0],[0]
"Third, techniques for combining weak supervision sources are built to tolerate some noise (Alfonseca et al., 2012; Takamatsu et al., 2012; Ratner et al., 2018).",1 Introduction,[0],[0]
The significance of this is that we can deploy the same semantic parser across tasks without task-specific training.,1 Introduction,[0],[0]
"We show how we can tackle a real-world biomedical application with the same semantic parser used to extract instances of spouses.
",1 Introduction,[0],[0]
"Our work is most similar to that of Srivastava et al. (2017), who also use natural language explanations to train a classifier, but with two important differences.",1 Introduction,[0],[0]
"First, they jointly train a task-specific semantic parser and classifier, whereas we use a
simple rule-based parser.",1 Introduction,[0],[0]
"In Section 4, we find that in our weak supervision framework, the rule-based semantic parser and the perfect parser yield nearly identical downstream performance.",1 Introduction,[0],[0]
"Second, while they use the logical forms of explanations to produce features that are fed directly to a classifier, we use them as functions for labeling a much larger training set.",1 Introduction,[0],[0]
"In Section 4, we show that using functions yields a 9.5 F1 improvement (26% relative improvement) over features, and that the F1 score scales with the amount of available unlabeled data.
",1 Introduction,[0],[0]
We validate our approach on two existing datasets from the literature (extracting spouses from news articles and disease-causing chemicals from biomedical abstracts) and one real-world use case with our biomedical collaborators at OccamzRazor to extract protein-kinase interactions related to Parkinson’s disease from text.,1 Introduction,[0],[0]
We find empirically that users are able to train classifiers with comparable F1 scores up to two orders of magnitude faster when they provide natural language explanations instead of individual labels.,1 Introduction,[0],[0]
"Our code and data can be found at https:// github.com/HazyResearch/babble.
2",1 Introduction,[0],[0]
"The BabbleLabble Framework
The BabbleLabble framework converts natural language explanations and unlabeled data into a noisily-labeled training set (see Figure 2).",1 Introduction,[0],[0]
"There are three key components: a semantic parser, a filter bank, and a label aggregator.",1 Introduction,[0],[0]
"The semantic
parser converts natural language explanations into a set of logical forms representing labeling functions (LFs).",1 Introduction,[0],[0]
The filter bank removes as many incorrect LFs as possible without requiring ground truth labels.,1 Introduction,[0],[0]
The remaining LFs are applied to unlabeled examples to produce a matrix of labels.,1 Introduction,[0],[0]
"This label matrix is passed into the label aggregator, which combines these potentially conflicting and overlapping labels into one label for each example.",1 Introduction,[0],[0]
The resulting labeled examples are then used to train an arbitrary discriminative model.,1 Introduction,[0],[0]
"To create the input explanations, the user views a subset S of an unlabeled dataset D (where |S| |D|) and provides for each input xi ∈ S a label yi and a natural language explanation ei, a sentence explaining why the example should receive that label.",2.1 Explanations,[0],[0]
"The explanation ei generally refers to specific aspects of the example (e.g., in Figure 2, the location of a specific string “his wife”).",2.1 Explanations,[0],[0]
"The semantic parser takes a natural language explanation ei and returns a set of LFs (logical forms or labeling functions) {f1, . . .",2.2 Semantic Parser,[0],[0]
", fk} of the form fi : X → {−1, 0, 1} in a binary classification setting, with 0 representing abstention.",2.2 Semantic Parser,[0],[0]
"We emphasize that the goal of this semantic parser is not to generate the single correct parse, but rather to have coverage over many potentially useful LFs.1
1Indeed, we find empirically that an incorrect LF nearby the correct one in the space of logical forms actually has higher end-task accuracy 57% of the time (see Section 4.2).
",2.2 Semantic Parser,[0],[0]
We choose a simple rule-based semantic parser that can be used without any training.,2.2 Semantic Parser,[0],[0]
"Formally, the parser uses a set of rules of the form α → β, where α can be replaced by the token(s) in β (see Figure 3 for example rules).",2.2 Semantic Parser,[0],[0]
"To identify candidate LFs, we recursively construct a set of valid parses for each span of the explanation, based on the substitutions defined by the grammar rules.",2.2 Semantic Parser,[0],[0]
"At the end, the parser returns all valid parses (LFs in our case) corresponding to the entire explanation.
",2.2 Semantic Parser,[0],[0]
We also allow an arbitrary number of tokens in a given span to be ignored when looking for a matching rule.,2.2 Semantic Parser,[0],[0]
"This improves the ability of the parser to handle unexpected input, such as unknown words or typos, since the portions of the input that are parseable can still result in a valid parse.",2.2 Semantic Parser,[0],[0]
"For example, in Figure 3, the word “person” is ignored.
",2.2 Semantic Parser,[0],[0]
"All predicates included in our grammar (summarized in Table 1) are provided to annotators, with minimal examples of each in use (Appendix A).",2.2 Semantic Parser,[0],[0]
"Importantly, all rules are domain independent (e.g., all three relation extraction tasks that we tested used the same grammar), making the semantic parser easily transferrable to new domains.",2.2 Semantic Parser,[0],[0]
"Additionally, while this paper focuses on the task of relation extraction, in principle the BabbleLabble framework can be applied to other tasks or settings by extending the grammar with the necessary primitives (e.g., adding primitives for rows and columns to enable explanations about the alignments of words in tables).",2.2 Semantic Parser,[0],[0]
"To guide the construction of the grammar, we collected 500 explanations for the Spouse domain from workers
on Amazon Mechanical Turk and added support for the most commonly used predicates.",2.2 Semantic Parser,[0],[0]
These were added before the experiments described in Section 4.,2.2 Semantic Parser,[0],[0]
Altogether the grammar contains 200 rule templates.,2.2 Semantic Parser,[0],[0]
The input to the filter bank is a set of candidate LFs produced by the semantic parser.,2.3 Filter Bank,[0],[0]
The purpose of the filter bank is to discard as many incorrect LFs as possible without requiring additional labels.,2.3 Filter Bank,[0],[0]
"It consists of two classes of filters: semantic and pragmatic.
",2.3 Filter Bank,[0],[0]
"Recall that each explanation ei is collected in the context of a specific labeled example (xi, yi).",2.3 Filter Bank,[0],[0]
"The semantic filter checks for LFs that are inconsistent with their corresponding example; formally, any LF f for which f(xi) 6=",2.3 Filter Bank,[0],[0]
yi is discarded.,2.3 Filter Bank,[0],[0]
"For example, in the first explanation in Figure 2, the word “right” can be interpreted as either “immediately” (as in “right before”) or simply “to the
right.”",2.3 Filter Bank,[0],[0]
"The latter interpretation results in a function that is inconsistent with the associated example (since “his wife” is actually to the left of person 2), so it can be safely removed.
",2.3 Filter Bank,[0],[0]
"The pragmatic filters removes LFs that are constant, redundant, or correlated.",2.3 Filter Bank,[0],[0]
"For example, in Figure 2, LF 2a is constant, as it labels every example positively (since all examples contain two people from the same sentence).",2.3 Filter Bank,[0],[0]
"LF 3b is redundant, since even though it has a different syntax tree from LF 3a, it labels the training set identically and therefore provides no new signal.
",2.3 Filter Bank,[0],[0]
"Finally, out of all LFs from the same explanation that pass all the other filters, we keep only the most specific (lowest coverage) LF.",2.3 Filter Bank,[0],[0]
"This prevents multiple correlated LFs from a single example from dominating.
",2.3 Filter Bank,[0],[0]
"As we show in Section 4, over three tasks, the filter bank removes 86% of incorrect parses, and the incorrect ones that remain have average endtask accuracy within 2.5% of the corresponding correct parses.",2.3 Filter Bank,[0],[0]
The label aggregator combines multiple (potentially conflicting) suggested labels from the LFs and combines them into a single probabilistic label per example.,2.4 Label Aggregator,[0],[0]
"Concretely, if m LFs pass the filter bank and are applied to n examples, the label aggregator implements a function f : {−1, 0, 1}m×n",2.4 Label Aggregator,[0],[0]
"→ [0, 1]n.
",2.4 Label Aggregator,[0],[0]
"A naive solution would be to use a simple majority vote, but this fails to account for the fact that LFs can vary widely in accuracy and coverage.",2.4 Label Aggregator,[0],[0]
"Instead, we use data programming (Ratner et al., 2016), which models the relationship between the true labels and the output of the labeling functions as a factor graph.",2.4 Label Aggregator,[0],[0]
"More specifically, given the true labels Y ∈ {−1, 1}n (latent) and label matrix Λ ∈ {−1, 0, 1}m×n (observed) where Λi,j = LFi(xj), we define two types of factors representing labeling propensity and accuracy:
φLabi,j (Λ, Y ) = 1{Λi,j 6= 0} (1) φAcci,j (Λ, Y ) = 1{Λi,j = yj}.",2.4 Label Aggregator,[0],[0]
"(2)
Denoting the vector of factors pertaining to a given data point xj as φj(Λ, Y ) ∈ Rm, define the model:
pw(Λ, Y )",2.4 Label Aggregator,[0],[0]
= Z −1 w exp,2.4 Label Aggregator,[0],[0]
"( n∑ j=1 w · φj(Λ, Y ) )",2.4 Label Aggregator,[0],[0]
", (3)
where w ∈ R2m is the weight vector and Zw is the normalization constant.",2.4 Label Aggregator,[0],[0]
"To learn this model without knowing the true labels Y , we minimize the negative log marginal likelihood given the observed labels Λ:
ŵ = arg min w − log ∑ Y pw(Λ, Y ) (4)
using SGD and Gibbs sampling for inference, and then use the marginals pŵ(Y | Λ) as probabilistic training labels.
",2.4 Label Aggregator,[0],[0]
"Intuitively, we infer accuracies of the LFs based on the way they overlap and conflict with one another.",2.4 Label Aggregator,[0],[0]
"Since noisier LFs are more likely to have high conflict rates with others, their corresponding accuracy weights in w will be smaller, reducing their influence on the aggregated labels.",2.4 Label Aggregator,[0],[0]
The noisily-labeled training set that the label aggregator outputs is used to train an arbitrary discriminative model.,2.5 Discriminative Model,[0],[0]
One advantage of training a discriminative model on the task instead of using the label aggregator as a classifier directly is that the label aggregator only takes into account those signals included in the LFs.,2.5 Discriminative Model,[0],[0]
"A discriminative model, on the other hand, can incorporate features that were not identified by the user but are nevertheless informative.2 Consequently, even examples for which all LFs abstained can still be classified correctly.",2.5 Discriminative Model,[0],[0]
"On the three tasks we evaluate, using the discriminative model averages 4.3 F1 points higher than using the label aggregator directly.
",2.5 Discriminative Model,[0],[0]
"For the results reported in this paper, our discriminative model is a simple logistic regression classifier with generic features defined over dependency paths.3 These features include unigrams,
2We give an example of two such features in Section 4.3.",2.5 Discriminative Model,[0],[0]
"3https://github.com/HazyResearch/treedlib
bigrams, and trigrams of lemmas, dependency labels, and part of speech tags found in the siblings, parents, and nodes between the entities in the dependency parse of the sentence.",2.5 Discriminative Model,[0],[0]
"We found this to perform better on average than a biLSTM, particularly for the traditional supervision baselines with small training set sizes; it also provided easily interpretable features for analysis.",2.5 Discriminative Model,[0],[0]
"We evaluate the accuracy of BabbleLabble on three relation extraction tasks, which we refer to as Spouse, Disease, and Protein.",3 Experimental Setup,[0],[0]
"The goal of each task is to train a classifier for predicting whether the two entities in an example are participating in the relationship of interest, as described below.",3 Experimental Setup,[0],[0]
"Statistics for each dataset are reported in Table 2, with one example and one explanation for each given in Figure 4 and additional explanations shown in Appendix B.
In the Spouse task, annotators were shown a sentence with two highlighted names and asked to label whether the sentence suggests that the two people are spouses.",3.1 Datasets,[0],[0]
"Sentences were pulled from the Signal Media dataset of news articles (Corney
et al., 2016).",3.1 Datasets,[0],[0]
"Ground truth data was collected from Amazon Mechanical Turk workers, accepting the majority label over three annotations.",3.1 Datasets,[0],[0]
"The 30 explanations we report on were sampled randomly from a pool of 200 that were generated by 10 graduate students unfamiliar with BabbleLabble.
",3.1 Datasets,[0],[0]
"In the Disease task, annotators were shown a sentence with highlighted names of a chemical and a disease and asked to label whether the sentence suggests that the chemical causes the disease.",3.1 Datasets,[0],[0]
"Sentences and ground truth labels came from a portion of the 2015 BioCreative chemical-disease relation dataset (Wei et al., 2015), which contains abstracts from PubMed.",3.1 Datasets,[0],[0]
"Because this task requires specialized domain expertise, we obtained explanations by having someone unfamiliar with BabbleLabble translate from Python to natural language labeling functions from an existing publication that explored applying weak supervision to this task (Ratner et al., 2018).
",3.1 Datasets,[0],[0]
"The Protein task was completed in conjunction with OccamzRazor, a neuroscience company targeting biological pathways of Parkinson’s disease.",3.1 Datasets,[0],[0]
"For this task, annotators were shown a sentence from the relevant biomedical literature with highlighted names of a protein and a kinase and asked to label whether or not the kinase influences the protein in terms of a physical interaction or phosphorylation.",3.1 Datasets,[0],[0]
"The annotators had domain expertise but minimal programming experience, making BabbleLabble a natural fit for their use case.",3.1 Datasets,[0],[0]
Text documents are tokenized with spaCy.4,3.2 Experimental Settings,[0],[0]
"The semantic parser is built on top of the Python-based
4https://github.com/explosion/spaCy
implementation SippyCup.5 On a single core, parsing 360 explanations takes approximately two seconds.",3.2 Experimental Settings,[0],[0]
"We use existing implementations of the label aggregator, feature library, and discriminative classifier described in Sections 2.4–2.5 provided by the open-source project Snorkel (Ratner et al., 2018).
",3.2 Experimental Settings,[0],[0]
Hyperparameters for all methods we report were selected via random search over thirty configurations on the same held-out development set.,3.2 Experimental Settings,[0],[0]
"We searched over learning rate, batch size, L2 regularization, and the subsampling rate (for improving balance between classes).6 All reported F1 scores are the average value of 40 runs with random seeds and otherwise identical settings.",3.2 Experimental Settings,[0],[0]
"We evaluate the performance of BabbleLabble with respect to its rate of improvement by number of user inputs, its dependence on correctly parsed logical forms, and the mechanism by which it utilizes logical forms.",4 Experimental Results,[0],[0]
In Table 3 we report the average F1 score of a classifier trained with BabbleLabble using 30 explanations or traditional supervision with the indicated number of labels.,4.1 High Bandwidth Supervision,[0],[0]
"On average, it took the same amount of time to collect 30 explanations as 60 labels.7 We observe that in all three tasks, BabbleLabble achieves a given F1 score with far fewer user inputs than traditional supervision, by
5https://github.com/wcmac/sippycup 6Hyperparameter ranges: learning rate (1e-2 to 1e-4), batch size (32 to 128), L2 regularization (0 to 100), subsampling rate (0 to 0.5)
7Zaidan and Eisner (2008) also found that collecting annotator rationales in the form of highlighted substrings from the sentence only doubled annotation time.
as much as 100 times in the case of the Spouse task.",4.1 High Bandwidth Supervision,[0],[0]
"Because explanations are applied to many unlabeled examples, each individual input from the user can implicitly contribute many (noisy) labels to the learning algorithm.
",4.1 High Bandwidth Supervision,[0],[0]
"We also observe, however, that once the number of labeled examples is sufficiently large, traditional supervision once again dominates, since ground truth labels are preferable to noisy ones generated by labeling functions.",4.1 High Bandwidth Supervision,[0],[0]
"However, in domains where there is much more unlabeled data available than labeled data (which in our experience is most domains), we can gain in supervision efficiency from using BabbleLabble.
",4.1 High Bandwidth Supervision,[0],[0]
"Of those explanations that did not produce a correct LF, 4% were caused by the explanation referring to unsupported concepts (e.g., one explanation referred to “the subject of the sentence,” which our simple parser doesn’t support).",4.1 High Bandwidth Supervision,[0],[0]
Another 2% were caused by human errors (the correct LF for the explanation was inconsistent with the example).,4.1 High Bandwidth Supervision,[0],[0]
"The remainder were due to unrecognized paraphrases (e.g., the explanation said “the order of appearance is X, Y” instead of a supported phrasing like “X comes before Y”).",4.1 High Bandwidth Supervision,[0],[0]
"In Table 4, we report LF summary statistics before and after filtering.",4.2 Utility of Incorrect Parses,[0],[0]
LF correctness is based on exact match with a manually generated parse for each explanation.,4.2 Utility of Incorrect Parses,[0],[0]
"Surprisingly, the simple heuristic-based filter bank successfully removes over 95% of incorrect LFs in all three tasks, resulting in final LF sets that are 86% correct on average.",4.2 Utility of Incorrect Parses,[0],[0]
"Furthermore, among those LFs that pass through the filter bank, we found that the average difference in end-task accuracy between correct and incorrect parses is less than 2.5%.",4.2 Utility of Incorrect Parses,[0],[0]
"Intuitively, the filters are effective because it is quite difficult for an LF to be parsed from the explana-
tion, label its own example correctly (passing the semantic filter), and not label all examples in the training set with the same label or identically to another LF (passing the pragmatic filter).
",4.2 Utility of Incorrect Parses,[0],[0]
"We went one step further: using the LFs that would be produced by a perfect semantic parser as starting points, we searched for “nearby” LFs (LFs differing by only one predicate) with higher endtask accuracy on the test set and succeeded 57% of the time (see Figure 5 for an example).",4.2 Utility of Incorrect Parses,[0],[0]
"In other words, when users provide explanations, the signals they describe provide good starting points, but they are actually unlikely to be optimal.",4.2 Utility of Incorrect Parses,[0],[0]
"This observation is further supported by Table 5, which shows that the filter bank is necessary to remove clearly irrelevant LFs, but with that in place, the simple rule-based semantic parser and a perfect parser have nearly identical average F1 scores.",4.2 Utility of Incorrect Parses,[0],[0]
"Once we have relevant logical forms from userprovided explanations, we have multiple options for how to use them.",4.3 Using LFs as Functions or Features,[0],[0]
Srivastava et al. (2017) propose using these logical forms as features in a linear classifier.,4.3 Using LFs as Functions or Features,[0],[0]
"We choose instead to use them as functions for weakly supervising the creation of a larger training set via data programming (Ratner et al., 2016).",4.3 Using LFs as Functions or Features,[0],[0]
"In Table 6, we compare the two approaches directly, finding that the the data programming approach outperforms a feature-based one by 9.5 F1 points with the rule-based parser, and by 4.5 points with a perfect parser.
",4.3 Using LFs as Functions or Features,[0],[0]
We attribute this difference primarily to the ability of data programming to utilize unlabeled data.,4.3 Using LFs as Functions or Features,[0],[0]
"In Figure 6, we show how the data programming approach improves with the number of unlabeled examples, even as the number of LFs remains constant.",4.3 Using LFs as Functions or Features,[0],[0]
We also observe qualitatively that data programming exposes the classifier to additional patterns that are correlated with our explanations but not mentioned directly.,4.3 Using LFs as Functions or Features,[0],[0]
"For example, in the Disease task, two of the features weighted most
highly by the discriminative model were the presence of the trigrams “could produce a” or “support diagnosis of” between the chemical and disease, despite none of these words occurring in the explanations for that task.",4.3 Using LFs as Functions or Features,[0],[0]
In Table 6 we see a 4.3 F1 point improvement (10%) when we use the discriminative model that can take advantage of these features rather than applying the LFs directly to the test set and making predictions based on the output of the label aggregator.,4.3 Using LFs as Functions or Features,[0],[0]
Our work has two themes: modeling natural language explanations/instructions and learning from weak supervision.,5 Related Work and Discussion,[0],[0]
The closest body of work is on “learning from natural language.”,5 Related Work and Discussion,[0],[0]
"As mentioned earlier, Srivastava et al. (2017) convert natural language explanations into classifier features (whereas we convert them into labeling functions).",5 Related Work and Discussion,[0],[0]
"Goldwasser and Roth (2011) convert natural lan-
guage into concepts (e.g., the rules of a card game).",5 Related Work and Discussion,[0],[0]
Ling and Fidler (2017) use natural language explanations to assist in supervising an image captioning model.,5 Related Work and Discussion,[0],[0]
Weston (2016); Li et al. (2016) learn from natural language feedback in a dialogue.,5 Related Work and Discussion,[0],[0]
"Wang et al. (2017) convert natural language definitions to rules in a semantic parser to build up progressively higher-level concepts.
",5 Related Work and Discussion,[0],[0]
"We lean on the formalism of semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang, 2016).",5 Related Work and Discussion,[0],[0]
"One notable trend is to learn semantic parsers from weak supervision (Clarke et al., 2010; Liang et al., 2011), whereas our goal is to obtain weak supervision signal from semantic parsers.
",5 Related Work and Discussion,[0],[0]
The broader topic of weak supervision has received much attention; we mention some works most related to relation extraction.,5 Related Work and Discussion,[0],[0]
"In distant supervision (Craven et al., 1999; Mintz et al., 2009) and multi-instance learning (Riedel et al., 2010; Hoffmann et al., 2011), an existing knowledge base is used to (probabilistically) impute a training set.",5 Related Work and Discussion,[0],[0]
"Various extensions have focused on aggregating a variety of supervision sources by learning generative models from noisy labels (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth and Klakow, 2013; Ratner et al., 2016; Varma et al., 2017).
",5 Related Work and Discussion,[0],[0]
"Finally, while we have used natural language explanations as input to train models, they can also be output to interpret models (Krening et al., 2017; Lei et al., 2016).",5 Related Work and Discussion,[0],[0]
"More generally, from a machine learning perspective, labels are the primary asset, but they are a low bandwidth signal between annotators and the learning algorithm.",5 Related Work and Discussion,[0],[0]
Natural language opens up a much higher-bandwidth communication channel.,5 Related Work and Discussion,[0],[0]
"We have shown promising results in relation extraction (where one explanation can be “worth” 100 labels), and it would be interesting to extend our framework to other tasks and more interactive settings.",5 Related Work and Discussion,[0],[0]
"The code, data, and experiments for this paper are available on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x900e7e41deaa4ec5b2fe41dc50594548/.",Reproducibility,[0],[0]
We gratefully acknowledge the support of the following organizations: DARPA under No.,Acknowledgments,[0],[0]
"N66001-15-C-4043 (SIMPLEX), No. FA8750-17-2-0095 (D3M), No. FA8750-122-0335 (XDATA), and No. FA8750-13-2-0039 (DEFT), DOE under No. 108845, NIH under No. U54EB020405 (Mobilize), ONR under No. N000141712266 and No. N000141310129, AFOSR under No. 580K753, the Intel/NSF CPS Security grant No. 1505728, the Michael J. Fox Foundation for Parkinsons Research under Grant No. 14672, the Secure Internet of Things Project, Qualcomm, Ericsson, Analog Devices, the Moore Foundation, the Okawa Research Grant, American Family Insurance, Accenture, Toshiba, the National Science Foundation Graduate Research Fellowship under Grant No.",Acknowledgments,[0],[0]
"DGE-114747, the Stanford Finch Family Fellowship, the Joseph W. and Hon Mai Goodman Stanford Graduate Fellowship, an NSF CAREER Award IIS-1552635, and the members of the Stanford DAWN project: Facebook, Google, Intel, Microsoft, NEC, Teradata, and VMware.
",Acknowledgments,[0],[0]
"We thank Alex Ratner and the developers of Snorkel for their assistance with data programming, as well as the many members of the Hazy Research group and Stanford NLP group who provided feedback and tested early prototyptes.",Acknowledgments,[0],[0]
"Thanks as well to the OccamzRazor team: Tarik Koc, Benjamin Angulo, Katharina S. Volz, and Charlotte Brzozowski.
",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, DOE, NIH, ONR, AFOSR, NSF, or the U.S. Government.",Acknowledgments,[0],[0]
"Below are the predicates in the rule-based semantic parser grammar, each of which may have many supported paraphrases, only one of which is listed here in a minimal example.",A Predicate Examples,[0],[0]
and: X is true and Y is true or: X is true or Y is true not: X is not true any: Any of X or Y or Z is true all: All of X and Y and Z are true none:,Logic,[0],[0]
None of X or Y or Z is true,Logic,[0],[0]
=: X is equal to Y 6=: X is not Y <: X is smaller than Y ≤: X is no more than Y >: X is larger than Y ≥: X is at least Y,Comparison,[0],[0]
"lower: X is lowercase upper: X is upper case capital: X is capitalized all caps: X is in all caps starts with: X starts with ""cardio"" ends with: X ends with ""itis"" substring: X contains ""-induced""",Syntax,[0],[0]
person:,Named-entity Tags,[0],[0]
A person is between X and Y location: A place is within two words of X date: A date is between X and Y number: There are three numbers in the sentence organization: An organization is right after X,Named-entity Tags,[0],[0]
"list: (X, Y) is in Z set: X, Y, and Z are true count:",Lists,[0],[0]
There is one word between X and Y contains: X is in Y intersection: At least two of X are in Y map: X is at the start of a word in Y filter: There are three capitalized words to the left of X alias: A spouse word is in the sentence (“spouse” is a predefined list from the user),Lists,[0],[0]
word distance: X is two words before Y char distance: X is twenty characters after Y left: X is before Y right: X is after Y between: X is between Y and Z within: X is within five words of Y,Position,[0],[0]
The following are a sample of the explanations provided by users for each task.,B Sample Explanations,[0],[0]
"Users referred to the first person in the sentence as “X” and the second as “Y”.
",Spouse,[0],[0]
"Label true because ""and"" occurs between X and Y and ""marriage"" occurs one word after person1.
",Spouse,[0],[0]
"Label true because person Y is preceded by ‘beau’.
",Spouse,[0],[0]
"Label false because the words ""married"", ""spouse"", ""husband"", and ""wife"" do not occur in the sentence.
",Spouse,[0],[0]
"Label false because there are more than 2 people in the sentence and ""actor"" or ""actress"" is left of person1 or person2.",Spouse,[0],[0]
"Label true because the disease is immediately after the chemical and ’induc’ or ’assoc’ is in the chemical name.
",Disease,[0],[0]
"Label true because a word containing ’develop’ appears somewhere before the chemical, and the word ’following’ is between the disease and the chemical.
",Disease,[0],[0]
"Label true because ""induced by"", ""caused by"", or ""due to"" appears between the chemical and the disease.",Disease,[0],[0]
"""
Label false because ""none"", ""not"", or ""no"" is within 30 characters to the left of the disease.",Disease,[0],[0]
"Label true because ""Ser"" or ""Tyr"" are within 10 characters of the protein.
",Protein,[0],[0]
"Label true because the words ""by"" or ""with"" are between the protein and kinase and the words ""no"", ""not"" or ""none"" are not in between the protein and kinase and the total number of words between them is smaller than 10.
",Protein,[0],[0]
"Label false because the sentence contains ""mRNA"", ""DNA"", or ""RNA"".
",Protein,[0],[0]
"Label false because there are two "","" between the protein and the kinase with less than 30 characters between them.",Protein,[0],[0]
"Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification).",abstractText,[0],[0]
"In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision.",abstractText,[0],[0]
"A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier.",abstractText,[0],[0]
"On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5–100 faster by providing explanations instead of just labels.",abstractText,[0],[0]
"Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.",abstractText,[0],[0]
Training Classifiers with Natural Language Explanations,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2775–2779 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2775",text,[0],[0]
"End-to-end dialogue systems, based on neural architectures like bidirectional LSTMs or Memory Networks (Sukhbaatar et al., 2015) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple contexts (Wen et al., 2016; Serban et al., 2016; Bordes et al., 2016).",1 Introduction,[0],[0]
One of their main advantages is that they can rely on large data sources of existing dialogues to learn to cover various domains without requiring any expert knowledge.,1 Introduction,[0],[0]
"However, the flip side is that they also exhibit limited engagement, especially in chit-chat settings: they lack consistency and do not leverage proactive engagement strategies as (even partially) scripted chatbots do.
Zhang et al. (2018) introduced the PERSONACHAT dataset as a solution to cope with this issue.",1 Introduction,[0],[0]
"This dataset consists of dialogues between pairs of agents with text profiles, or personas, attached to
each of them.",1 Introduction,[0],[0]
"As shown in their paper, conditioning an end-to-end system on a given persona improves the engagement of a dialogue agent.",1 Introduction,[0],[0]
"This paves the way to potentially end-to-end personalized chatbots because the personas of the bots, by being short texts, could be easily edited by most users.",1 Introduction,[0],[0]
"However, the PERSONA-CHAT dataset was created using an artificial data collection mechanism based on Mechanical Turk.",1 Introduction,[0],[0]
"As a result, neither dialogs nor personas can be fully representative of real user-bot interactions and the dataset coverage remains limited, containing a bit more than 1k different personas.
",1 Introduction,[0],[0]
"In this paper, we build a very large-scale persona-based dialogue dataset using conversations previously extracted from REDDIT1.",1 Introduction,[0],[0]
"With simple heuristics, we create a corpus of over 5 million personas spanning more than 700 million conversations.",1 Introduction,[0],[0]
We train persona-based end-to-end dialogue models on this dataset.,1 Introduction,[0],[0]
"These models outperform their counterparts that do not have access to personas, confirming results of Zhang et al. (2018).",1 Introduction,[0],[0]
"In addition, the coverage of our dataset seems very good since pre-training on it also leads to state-of-the-art results on the PERSONA-CHAT dataset.",1 Introduction,[0],[0]
"With the rise of end-to-end dialogue systems, personalized trained systems have started to appear.",2 Related work,[0],[0]
Li et al. (2016) proposed to learn latent variables representing each speaker’s bias/personality in a dialogue model.,2 Related work,[0],[0]
"Other classic strategies include extracting explicit variables from structured knowledge bases or other symbolic sources as in (Ghazvininejad et al., 2017; Joshi et al., 2017; Young et al., 2017).",2 Related work,[0],[0]
"Still, in the context of per-
1https://www.reddit.com/r/datasets/ comments/3bxlg7/
sonal chatbots, it might be more desirable to condition on data that can be generated and interpreted by the user itself such as text rather than relying on some knowledge base facts that might not exist for everyone or a great variety of situations.",2 Related work,[0],[0]
"PERSONA-CHAT (Zhang et al., 2018) recently introduced a dataset of conversations revolving around human habits and preferences.",2 Related work,[0],[0]
"In their experiments, they showed that conditioning on a text description of each speaker’s habits, their persona, improved dialogue modeling.
",2 Related work,[0],[0]
"In this paper, we use a pre-existing REDDIT data dump as data source.",2 Related work,[0],[0]
REDDIT is a massive online message board.,2 Related work,[0],[0]
Dodge et al. (2015) used it to assess chit-chat qualities of generic dialogue models.,2 Related work,[0],[0]
Yang et al. (2018) used response prediction on REDDIT as an auxiliary task in order to improve prediction performance on natural language inference problems.,2 Related work,[0],[0]
Our goal is to learn to predict responses based on a persona for a large variety of personas.,3 Building a dataset of millions of persona-based dialogues,[0],[0]
"To that end, we build a dataset of examples of the following form using data from REDDIT:
• Persona:",3 Building a dataset of millions of persona-based dialogues,[0],[0]
"[“I like sport”, “I work a lot”] • Context: “I love running.”",3 Building a dataset of millions of persona-based dialogues,[0],[0]
• Response: “Me too!,3 Building a dataset of millions of persona-based dialogues,[0],[0]
"But only on weekends.”
",3 Building a dataset of millions of persona-based dialogues,[0],[0]
"The persona is a set of sentences representing the personality of the responding agent, the context is the utterance that it responds to, and the response is the answer to be predicted.",3 Building a dataset of millions of persona-based dialogues,[0],[0]
"As in (Dodge et al., 2015), we use a preexisting dump of REDDIT that consists of 1.7 billion comments.",3.1 Preprocessing,[0],[0]
We tokenize sentences by padding all special characters with a space and splitting on whitespace characters.,3.1 Preprocessing,[0],[0]
We create a dictionary containing the 250k most frequent tokens.,3.1 Preprocessing,[0],[0]
We truncate comments that are longer than 100 tokens.,3.1 Preprocessing,[0],[0]
"We construct the persona of a user by gathering all the comments they wrote, splitting them into sentences, and selecting the sentences that satisfy the following rules: (i) each sentence must contain between 4 and 20 words or punctuation marks, (ii) it contains either the word I or my, (iii) at least
one verb, and (iv) at least one noun, pronoun or adjective.
",3.2 Persona extraction,[0],[0]
"To handle the quantity of data involved, we limit the size of a persona to N sentences for each user.",3.2 Persona extraction,[0],[0]
We compare four different setups for persona creation.,3.2 Persona extraction,[0],[0]
"In the rules setup, we select up to N random sentences that satisfy the rules above.",3.2 Persona extraction,[0],[0]
"In the rules + classifier setup, we filter with the rules then score the resulting sentences using a bag-of-words classifier that is trained to discriminate PERSONACHAT persona sentences from random comments.",3.2 Persona extraction,[0],[0]
We manually tune a threshold on the score in order to select sentences.,3.2 Persona extraction,[0],[0]
"If there are more than N eligible persona sentences for a given user, we keep the highest-scored ones.",3.2 Persona extraction,[0],[0]
"In the random from user setup, we randomly select sentences uttered by the user while keeping the sentence length requirement above (we ignore the other rules).",3.2 Persona extraction,[0],[0]
The random from dataset baseline refers to random sentences from the dataset.,3.2 Persona extraction,[0],[0]
They do not necessarily come from the same user.,3.2 Persona extraction,[0],[0]
"This last setup serves as a control mechanism to verify that the gains in prediction accuracy are due to the user-specific information contained in personas.
",3.2 Persona extraction,[0],[0]
"In the example at the beginning of this section, the response is clearly consistent with the persona.",3.2 Persona extraction,[0],[0]
"There may not always be such an obvious relationship between the two: the discussion topic may not be covered by the persona, a single user may write contradictory statements, and due to errors in the extraction process, some persona sentences may not represent a general trait of the user (e.g. I am feeling happy today).",3.2 Persona extraction,[0],[0]
We take each pair of successive comments in a thread to form the context and response of an example.,3.3 Dataset creation,[0],[0]
The persona corresponding to the response is extracted using one of the methods of Section 3.2.,3.3 Dataset creation,[0],[0]
"We split the dataset randomly between training, validation and test.",3.3 Dataset creation,[0],[0]
Validation and test sets contain 50k examples each.,3.3 Dataset creation,[0],[0]
"We extract personas using training data only: test set responses cannot be contained explicitly in the persona.
",3.3 Dataset creation,[0],[0]
"In total, we select personas covering 4.6m users in the rule-based setups and 7.2m users in the random setups.",3.3 Dataset creation,[0],[0]
"This is a sizable fraction of the total 13.2m users of the dataset; depending on the persona selection setup, between 97 and 99.4 % of the training set examples are linked to a persona.",3.3 Dataset creation,[0],[0]
"We model dialogue by next utterance retrieval (Lowe et al., 2016), where a response is picked among a set of candidates and not generated.",4 End-to-end dialogue models,[0],[0]
The overall architecture is depicted in Fig. 1.,4.1 Architecture,[0],[0]
We encode the persona and the context using separate modules.,4.1 Architecture,[0],[0]
"As in Zhang et al. (2018), we combine the encoded context and persona using a 1-hop memory network with a residual connection, using the context as query and the set of persona sentences as memory.",4.1 Architecture,[0],[0]
We also encode all candidate responses and compute the dot-product between all those candidate representations and the joint representation of the context and the persona.,4.1 Architecture,[0],[0]
"The predicted response is the candidate that maximizes the dot product.
",4.1 Architecture,[0],[0]
We train by passing all the dot products through a softmax and maximizing the log-likelihood of the correct responses.,4.1 Architecture,[0],[0]
"We use mini-batches of training examples and, for each example therein, all the responses of the other examples of the same batch are used as negative responses.",4.1 Architecture,[0],[0]
Both context and response encoders share the same architecture and word embeddings but have different weights in the subsequent layers.,4.2 Context and response encoders,[0],[0]
"We train three different encoder architectures.
",4.2 Context and response encoders,[0],[0]
Bag-of-words applies two linear projections separated by a tanh non-linearity to the word embeddings.,4.2 Context and response encoders,[0],[0]
"We then sum the resulting sentence representation across all positions in the sentence and divide the result by √ n where n is the length of the sequence.
",4.2 Context and response encoders,[0],[0]
LSTM applies a 2-layer bidirectional LSTM.,4.2 Context and response encoders,[0],[0]
"We use the last hidden state as encoded sentence.
",4.2 Context and response encoders,[0],[0]
"Transformer is a variation of an End-to-end Memory Network (Sukhbaatar et al., 2015) introduced by Vaswani et al. (2017).",4.2 Context and response encoders,[0],[0]
"Based solely on attention mechanisms, it exhibited state-of-the-art performance on next utterance retrieval tasks in dialogues (Yang et al., 2018).",4.2 Context and response encoders,[0],[0]
Here we use only its encoding module.,4.2 Context and response encoders,[0],[0]
"We subsequently average the resulting representation across all positions in the sentence, yielding a fixed-size representation.",4.2 Context and response encoders,[0],[0]
The persona encoder encodes each persona sentence separately.,4.3 Persona encoder,[0],[0]
It relies on the same word embeddings as the context encoder and applies a linear layer on top of them.,4.3 Persona encoder,[0],[0]
"We then sum the representations across the sentence.
",4.3 Persona encoder,[0],[0]
We deliberately choose a simpler architecture than the other encoders for performance reasons as the number of personas encoded for each batch is an order of magnitude greater than the number of training examples.,4.3 Persona encoder,[0.95395909280326],['This kind of aggregation of multiple networks with the same topology is not only possible for single-layer feedforward architectures but also for complex networks consisting of multiple GRU layers and attention.']
Most personas are short sentences; we therefore expect a bag-of-words representation to encode them well.,4.3 Persona encoder,[0],[0]
We train models on the persona-based dialogue dataset described in Section 3.3 and we evaluate its accuracy both on the original task and when transferring onto PERSONA-CHAT.,5 Experiments,[0],[0]
We optimize network parameters using Adamax with a learning rate of 8e−4 on mini-batches of size 512.,5.1 Experimental details,[0],[0]
"We initialize embeddings with FastText word vectors and optimize them during learning.
",5.1 Experimental details,[0],[0]
"REDDIT LSTMs use a hidden size of 150; we concatenate the last hidden states for both directions and layers, resulting in a final representation of size 600.",5.1 Experimental details,[0],[0]
"Transformer architectures on reddit use 4 layers with a hidden size of 300 and 6 attention heads, resulting in a final representation of size 300.",5.1 Experimental details,[0],[0]
We use Spacy for part-of-speech tagging in order to verify the persona extraction rules.,5.1 Experimental details,[0],[0]
"We distribute the training by splitting each batch across 8 GPUs; we stop training after 1 full epoch, which takes about 3 days.
",5.1 Experimental details,[0],[0]
"PERSONA-CHAT We used the revised version of the dataset where the personas have been rephrased, making it a harder task.",5.1 Experimental details,[0],[0]
"The dataset being only a few thousands samples, we had to reduce the architecture to avoid overfitting for the models trained purely on PERSONA-CHAT.",5.1 Experimental details,[0],[0]
"2 layers, 2 attention heads, a dropout of 0.2 and keeping the size of the word embeddings to 300 units yield the highest accuracy on the validation set.
",5.1 Experimental details,[0],[0]
IR Baseline,5.1 Experimental details,[0],[0]
"As basic baseline, we use an information retrieval (IR) system that ranks candidate responses according to a TF-IDF weighted exactmatch similarity with the context alone.",5.1 Experimental details,[0],[0]
Impact of personas We report the accuracy of the different architectures on the reddit task in Table 1.,5.2 Results,[0],[0]
Conditioning on personas improves the prediction performance regardless of the encoder architecture.,5.2 Results,[0],[0]
"Table 2 gives some examples of how the persona affects the predicted answer.
",5.2 Results,[0],[0]
"Influence of the persona extraction In Table 3, we report precision results for several persona extraction setups.",5.2 Results,[0],[0]
"The rules setup improves the results somewhat, however adding the persona classifier actually degrades the results.",5.2 Results,[0],[0]
"A possible interpretation is that the persona classifier is trained only on the PERSONA-CHAT revised personas, and that this selection might be too narrow and lack di-
versity.",5.2 Results,[0],[0]
"Increasing the maximum persona size also improves the prediction performance.
",5.2 Results,[0],[0]
Transfer learning,5.2 Results,[0],[0]
We compare the performance of transformer models trained on REDDIT and on PERSONA-CHAT on both datasets.,5.2 Results,[0],[0]
We report results in Table 4.,5.2 Results,[0],[0]
"This architecture provides a strong improvement over the results of (Zhang et al., 2018), jumping from 35.4% hits@1 to 42.1%.",5.2 Results,[0],[0]
"Pretraining the model on REDDIT and then fine-tuning on PERSONA-CHAT pushes this score to 60.7%, largely improving the state of the art.",5.2 Results,[0],[0]
"As expected, fine-tuning on PERSONA-CHAT reduces the performance on REDDIT.",5.2 Results,[0],[0]
"However, directly testing on PERSONA-CHAT the model trained on REDDIT without fine-tuning yields a very low result.",5.2 Results,[0],[0]
"This could be a consequence of a discrepancy
between the style of personas of the two datasets.",5.2 Results,[0],[0]
This paper shows how to create a very large dataset for persona-based dialogue.,6 Conclusion,[0],[0]
We show that training models to align answers both with the persona of their author and the context improves the predicting performance.,6 Conclusion,[0],[0]
The trained models show promising coverage as exhibited by the stateof-the-art transfer results on the PERSONA-CHAT dataset.,6 Conclusion,[0],[0]
"As pretraining leads to a considerable improvement in performance, future work could be done fine-tuning this model for various dialog systems.",6 Conclusion,[0],[0]
Future work may also entail building more advanced strategies to select a limited number of personas for each user while maximizing the prediction performance.,6 Conclusion,[0],[0]
"Current dialogue systems are not very engaging for users, especially when trained end-toend without relying on proactive reengaging scripted strategies.",abstractText,[0],[0]
Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model.,abstractText,[0],[0]
"However, the dataset used in (Zhang et al., 2018) is synthetic and of limited size as it contains around 1k different personas.",abstractText,[0],[0]
In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues.,abstractText,[0],[0]
"Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems.",abstractText,[0],[0]
"In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from (Zhang et al., 2018) and achieving state-of-the-art results.",abstractText,[0],[0]
Training Millions of Personalized Dialogue Agents,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 130–135 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Structured prediction, or the task of predicting multiple inter-dependent variables, is important in many domains, including computer vision, computational biology and natural language processing.",1 Introduction,[0],[0]
"For example, in sequence labelling, image segmentation, and parsing we are given input variables x, and must predict output variables y, where the number of possible y values are typically exponential in the number of variables that comprise it.",1 Introduction,[0],[0]
"Not only does this sometimes give rise to computational difficulties, it also leads to statistical parameter estimation issues, where learning precise models requires large amounts of labeled training data.
",1 Introduction,[0],[0]
"In some cases, unsupervised learning from plentiful unlabeled data may provide helpful outputs (Daumé III, 2009; Ammar et al., 2014).",1 Introduction,[0],[0]
But usually some form of more direct supervision is required to create a model truly useful to the task at hand.,1 Introduction,[0],[0]
In the absence of abundant labeled data we may consider alternative forms of supervision.,1 Introduction,[0],[0]
"For example, rather than providing labeled data instances, humans may more easily inject their
domain knowledge by providing “labels on features,” or “expectations” about correct outputs, as in generalized expectation criteria (Mann and McCallum, 2010), or by providing constraints, as in posterior regularization (Ganchev et al., 2010) or constraint driven learning (Chang et al., 2007).",1 Introduction,[0],[0]
"A major weakness of these methods, however, is that at training time inference must be done in the factor graph encompassing the union of the model’s factor graph and the expectation dependencies— often leading to prohibitively expensive inference.",1 Introduction,[0],[0]
"Moreover, these methods cannot learn from nondecomposable domain knowledge, where the domain knowledge is not in a form of a set of labeled features or constraints.
",1 Introduction,[0],[0]
"An easy way for humans to express domain knowledge is by writing a simple scalar scoring function that indicates preferences among choices for y given x. These human-coded functions may, for example, be based on arbitrary rule systems (or even Turing-complete programs) of the sort written by humans to solve problems before machine learning became so wide-spread.
",1 Introduction,[0],[0]
"In general, the human written domain knowledge functions are not expected to be perfect— most likely only examining a subset of features and not covering all cases.",1 Introduction,[0],[0]
"Thus we are now faced with two challenges: (1) the domain knowledge functions have limited generalization; (2) the domain knowledge functions provide a ranking, but do not provide an inference (search) procedure.
",1 Introduction,[0],[0]
"This paper presents a new training method for structured prediction energy networks (SPENs) (Belanger and McCallum, 2016; Belanger et al., 2017) that aims to address both these challenges, yielding efficient inference for structured prediction, trained from human-coded domain knowledge plus unlabeled data, but not requiring any labeled data instances.",1 Introduction,[0],[0]
"In SPENs, the factor graph that typically represents
130
output variable dependencies is replaced with a deep neural network that takes y and x as input and outputs a scalar energy score, but is able to learn much richer correlations than are typically captured in factor graphs.",1 Introduction,[0],[0]
"Inference in SPENs is performed by gradient descent in the energy, back-propagated to cause steps in a relaxed y space.",1 Introduction,[0],[0]
"Whereas previous training procedures for SPENs used labeled data, here we train SPENs from only unlabeled data plus human-coded domain knowledge in the form of a scoring function.",1 Introduction,[0],[0]
"We do so by building on SampleRank (Rohanimanesh et al., 2011; Singh et al., 2010), which enforces that the rank of two sampled ys according to the trained factor graph is consistent with their rank according to distance to the labeled, true y.",1 Introduction,[0],[0]
"In our training method, pairs of y’s are obtained from successive steps of training-time gradient-descent inference on y; when their rank is not consistent with that of the domain knowledge function, we accordingly update the energy network parameters.
",1 Introduction,[0],[0]
"We demonstrate our method on a citation field extraction task, for which we learn a neural network (1) that generalizes beyond the original domain knowledge function, and (2) that provides efficient test-time inference by gradient descent.",1 Introduction,[0],[0]
"In general, SPEN parameterizes an energy function Ew(y,x) using deep neural networks over output variables y as well as input variables x, where w denotes the neural network’s parameters.",2 Structured Prediction Energy Networks,[0],[0]
Belanger and McCallum (2016) separate the energy function into global and local terms.,2 Structured Prediction Energy Networks,[0],[0]
"The role of the local terms is to capture the dependency among input x and each individual output variable yi, while the global term aims to capture long-range dependencies among output variables.
",2 Structured Prediction Energy Networks,[0],[0]
Prediction in SPENs requires finding ŷ,2 Structured Prediction Energy Networks,[0],[0]
=,2 Structured Prediction Energy Networks,[0],[0]
argminy∈Y,2 Structured Prediction Energy Networks,[0],[0]
"Ew(y,x)",2 Structured Prediction Energy Networks,[0],[0]
for the given input x.,2 Structured Prediction Energy Networks,[0],[0]
This inference problem is solved using gradient descent.,2 Structured Prediction Energy Networks,[0],[0]
"However, the energy surface is non-convex, which prevents gradient descent inference from finding the exact structure ymin that globally minimizes the energy function.",2 Structured Prediction Energy Networks,[0],[0]
One approach to address this problem is to parameterize the energy function such that the SPEN is convex in the output variables y,2 Structured Prediction Energy Networks,[0],[0]
"(Amos et al., 2017), but this limits the representational power of SPENs.",2 Structured Prediction Energy Networks,[0],[0]
"Al-
though gradient descent inference does not guarantee an exact solution, it has successfully been used in several domains such as multi-label classification (Belanger and McCallum, 2016), imagesegmentation (Gygli et al., 2017), and semantic role labeling (Belanger et al., 2017).",2 Structured Prediction Energy Networks,[0],[0]
"Different methods have been introduced for training SPENs: margin-based training (Belanger and McCallum, 2016), end-to-end learning (Belanger et al., 2017), and value matching (Gygli et al., 2017).",3 Rank-Based Training of SPENs,[0],[0]
"Margin-based training enforces the energy of the ground truth structure to be lower than the energy of every incorrect structure by a margin, which is calculated as the Hamming loss between the two structures.",3 Rank-Based Training of SPENs,[0],[0]
End-to-end learning unrolls the energy minimization into a differentiable computation graph to output the predicted structure.,3 Rank-Based Training of SPENs,[0],[0]
It then trains the model by directly minimizing the loss between the predicted and ground-truth structures.,3 Rank-Based Training of SPENs,[0],[0]
"Finally, the value matching approach trains SPENs such that the energy value matches the value of a given target function, such as the L2 distance between the ground-truth and predicted structures.
",3 Rank-Based Training of SPENs,[0],[0]
All of these methods strongly depend on the existence of the ground truth values either as labeled data or as the value of a function applied to it.,3 Rank-Based Training of SPENs,[0],[0]
"While dependence of the margin-based and endto-end learning approaches on the labeled data is explicit, this dependency in the case of valuematching may not be obvious.",3 Rank-Based Training of SPENs,[0],[0]
"In the absence of labeled data, we have to use the model’s predictions instead, for training.",3 Rank-Based Training of SPENs,[0],[0]
"These predictions are often incorrect, especially at early stages of training.",3 Rank-Based Training of SPENs,[0],[0]
"As a result, value-matching training is constrained to match the score of these predictions with the value of the energy function defined by SPEN.",3 Rank-Based Training of SPENs,[0],[0]
"This requires matching several incorrect structures for a given input, which hinders gradient descent inference from finding the exact solution by introducing many local optima.",3 Rank-Based Training of SPENs,[0],[0]
"To address this problem, we use a ranking objective similar to SampleRank",3 Rank-Based Training of SPENs,[0],[0]
"(Rohanimanesh et al., 2011) such that it preserves the optimum points of the score function.
",3 Rank-Based Training of SPENs,[0.9524615094445256],"['The ensemble decoder computes predictions from each of the individual models which are then combined using the arithmetic average (Sutskever et al., 2014) or the geometric average (Cromieres et al., 2016).']"
"In general, if SPEN ranks every pair of output structures identical to the score function, the optimum points of the score function match those of SPEN.",3 Rank-Based Training of SPENs,[0],[0]
"However, forcing the ranking constraint for every pair of output structures is not tractable, so
we need to approximate it by sampling some candidate pairs.",3 Rank-Based Training of SPENs,[0],[0]
"Given a score function V (y,x), we are able to rank every two consecutive candidate structures based on their score values.",3 Rank-Based Training of SPENs,[0],[0]
Consider two candidate output structures y1 and y2 for the given input x.,3 Rank-Based Training of SPENs,[0],[0]
"We define yh and yl based on the score function as the following:
yh = argmax y∈{y1,y2} V (y,x),
yl = argmin y∈{y1,y2} V (y,x).",3 Rank-Based Training of SPENs,[0],[0]
"(1)
We expect that these two structures have the same ranking with respect to Ew(.,x), which can be described as: α(V (yh,x)",3 Rank-Based Training of SPENs,[0],[0]
"− V (yl,x))",3 Rank-Based Training of SPENs,[0],[0]
"< Ew(yh,x)−Ew(yl,x), where α is a tunable positive scalar.",3 Rank-Based Training of SPENs,[0],[0]
"Therefore, the rank-based objective minimizes the constraint violations:
min w
∑ x∈D [α(V (yh,x)− V (yl,x))−
Ew(yh,x) +",3 Rank-Based Training of SPENs,[0],[0]
"Ew(yl,x)]+ (2)
",3 Rank-Based Training of SPENs,[0],[0]
"[.]+ is max(., 0).",3 Rank-Based Training of SPENs,[0],[0]
Figure 1 shows a ranking violation for two structures y1 and y2 for a given x. The arrows indicate the direction of update over the energy surface.,3 Rank-Based Training of SPENs,[0],[0]
"Note that we ignore the dependence of y on w, which introduces approximation in the gradient of Eq. 2.",3 Rank-Based Training of SPENs,[0],[0]
"For the supervised setting, Belanger et al. (2017) address this problem by unrolling the inference steps as an inference network and back-propagating through the inference network.",3 Rank-Based Training of SPENs,[0],[0]
We leave exploring similar approaches for rank-based training for future work.,3 Rank-Based Training of SPENs,[0],[0]
"To compute Eq. 2, we need to find configurations yi and yj such that both are candidate solutions for argminy∈Y Ew(y,x).",3 Rank-Based Training of SPENs,[0],[0]
"If not, the number of required samples would be exponential in |Y|.",3 Rank-Based Training of SPENs,[0],[0]
"Since at test time we use gradient descent inference, a similar method is used for generating candidate structures: the trajectory of points in the inference mechanism is used as the set of possible candidates.",3 Rank-Based Training of SPENs,[0],[0]
The idea of deterministic sampling from SPEN energy surface was first introduced by David Belanger (2017).,3 Rank-Based Training of SPENs,[0],[0]
"We define the inference trajectory, T (x), as a sequence of output structures generated using projected gradient descent inference in order to find the minimum solution of Ew(.,x).
",3 Rank-Based Training of SPENs,[0],[0]
"Given a random initial structure y0, we define the inference trajectory as: T (x) =
{y1, · · · ,ym}, where yt+1 = Py∈∆L(yt − η ∂∂yEw(yt,x)).",3 Rank-Based Training of SPENs,[0],[0]
Py∈∆L projects the values of y onto the probability simplex ∆L overL values that each variable y can take.,3 Rank-Based Training of SPENs,[0],[0]
"For each input x in the training data, we find the first consecutive structures yi, yi+1 ∈ T (x) that violate the ranking constraint, then use Eq. 2 to reduce the number of violations.",3 Rank-Based Training of SPENs,[0],[0]
"Algorithm 1 describes the complete training algorithm.
",3 Rank-Based Training of SPENs,[0],[0]
"Algorithm 1 Rank-based training of SPEN D ← unlabeled mini-batch of training data V (., .)← scoring function Ew(., .)← input SPEN for each x in D do T (x)← samples using GD inference in Ew(.,x).",3 Rank-Based Training of SPENs,[0],[0]
"ξ ← ∅. for each pair (yi,yi+1) in T (x) do
Construct yh and yl using Eq.1 if α(V (yh,x)",3 Rank-Based Training of SPENs,[0],[0]
"− V (yl,x))",3 Rank-Based Training of SPENs,[0],[0]
"> Ew(yh,x)",3 Rank-Based Training of SPENs,[0],[0]
"−
Ew(yl,x) then ξ ← ξ ∪ (x,yh,yl).
end if end for Optimize Eq.2 using ξ.
end for",3 Rank-Based Training of SPENs,[0],[0]
"To show the success of rank-based learning with indirect supervision, we conduct experiments on citation field extraction as an instance of structured prediction problems.",4 Citation Field Extraction,[0],[0]
"The goal of citation field extraction is to segment citation text into its constituent parts such as Author, Title, Journal, Page, and Date.",4 Citation Field Extraction,[0],[0]
"We used the Cora citation dataset (Seymore et al., 1999), which includes 100 labeled examples as the test set and another 100 labeled examples for the validation set.",4 Citation Field Extraction,[0],[0]
"Our training data consists of 300 training examples from the Cora citation data set for which we dismiss the labels,
as well as another 700 unlabeled citations acquired across the web, which adds up to 1000 unlabeled data points.",4 Citation Field Extraction,[0],[0]
Each token can be labeled with one of 13 possible tags.,4 Citation Field Extraction,[0],[0]
"We use fixed-length input data by padding all citation text to the maximum citation length in the dataset, which is 118 tokens.",4 Citation Field Extraction,[0],[0]
"We report token-level accuracy measured on non-pad tokens.
",4 Citation Field Extraction,[0],[0]
We provide the learning algorithm with a human written score function that takes the citation text and predicted tags as input.,4 Citation Field Extraction,[0],[0]
The score function then checks for violations of its rules and penalizes the predicted tags accordingly.,4 Citation Field Extraction,[0],[0]
Figure 2 shows examples of rules in the score function.,4 Citation Field Extraction,[0],[0]
"Our complete score function consists of around 50 rules.
",4 Citation Field Extraction,[0],[0]
We used two 2-layer neural networks with 1000 and 500 hidden nodes to parameterize the local and global energy functions of SPEN.,4 Citation Field Extraction,[0],[0]
"We examine different α (Eq. 2) values of 0.1, 1.0, 2.0, 5.0, and 10.0, and setting α value to 2.0 has the best performance on the validation set.",4 Citation Field Extraction,[0],[0]
"We use gradient descent inference with ten gradient descent steps and η = 0.1 for both training and test.
",4 Citation Field Extraction,[0],[0]
We include the results of generalized expectation (GE) from Mann and McCallum (2010) that use the same dataset and setting.,4 Citation Field Extraction,[0],[0]
"Our results show that R-SPEN achieves significantly better tokenlevel accuracy as compared to GE.
",4 Citation Field Extraction,[0],[0]
We also compare R-SPEN with different inference algorithms that search using the score function to find the best configuration with maximum score.,4 Citation Field Extraction,[0],[0]
The results of these are listed in Table 1.,4 Citation Field Extraction,[0],[0]
Greedy search first randomly initializes the output tags and then iteratively replaces each assigned tag with a tag that results in the maximum score until the end of the citation is reached.,4 Citation Field Extraction,[0],[0]
"This process is repeated until convergence, measured by no tag changing in an iteration.",4 Citation Field Extraction,[0],[0]
"To avoid the effects of random initialization, this is repeated with varied number of random restarts, as reported in Table 1, where the best configuration is used in the scores reported.",4 Citation Field Extraction,[0],[0]
"For the baseline that implements beam search, each citation is labeled by employing a beam search on the space of all tags for each token and their subsequent configurations, while keeping track of the best k configurations from one token to the next.",4 Citation Field Extraction,[0],[0]
"This search is further augmented by restarting the search from the best k found after one complete search, for a total of 10 times and 10 random restarts.
",4 Citation Field Extraction,[0],[0]
"Consulting Table 1, we can confirm that both greedy search and beam search find much better output structures in term of score values as compared to R-SPEN; however, they achieve poor accuracy because the domain knowledge function does not comprehensively provide rules regarding all possible output structures.",4 Citation Field Extraction,[0],[0]
We report the average score values of the R-SPEN predictions on test data as a function of training iterations in Figure 3.,4 Citation Field Extraction,[0],[0]
"Within 1000 iterations, R-SPEN is able to achieve a test set accuracy of 38%, outperforming all baselines, while the average score is -18.0.",4 Citation Field Extraction,[0],[0]
"R-SPEN generalizes beyond the domain knowledge function because it successfully captures the correlation among output variables through rank-based training on unlabeled data, so its predictions may have lower score values but are more accurate.
",4 Citation Field Extraction,[0],[0]
The test time inference of R-SPEN is much faster than search algorithms because SPEN provides efficient approximate inference.,4 Citation Field Extraction,[0],[0]
"Generalized Expectation (GE) (Mann and McCallum, 2010), Posterior Regularization (Ganchev et al., 2010) and Constraint Driven Learning (Chang et al., 2007) are among well-known approaches to learn from domain knowledge decomposed over a set of constraints or labeled features.",5 Related Work,[0],[0]
"However, these methods cannot learn from black box domain knowledge based score functions.",5 Related Work,[0],[0]
"Score functions of this type are abundant in
various fields, for example, when the score is the result of evaluating a non-differentiable function over output structures.
",5 Related Work,[0],[0]
Stewart and Ermon (2017) train a neural network using a score function that guides the training based on physics of moving objects.,5 Related Work,[0],[0]
They have defined a differentiable score function which provides the learning algorithm with the gradient of the score function.,5 Related Work,[0],[0]
"However, in our approach the score function could be any complex non-differentiable function.
",5 Related Work,[0],[0]
Peng et al. (2017) and Iyyer et al. (2017) use energy-based max-margin training for learning from an implicit source of supervision.,5 Related Work,[0],[0]
This can be viewed as a score function evaluating the predicted output structure based on some real-world domain.,5 Related Work,[0],[0]
"For example, if the output structure is the SQL query associated with a natural language question, the score function can be specified as the Jaccard similarity of the extracted cells from the table using the generated SQL query and the set of
gold answers for the natural language query as in Iyyer et al (2017).",5 Related Work,[0],[0]
We have introduced a method to train structured prediction energy networks with indirect supervision that is derived from domain knowledge.,6 Conclusion and Future Work,[0],[0]
"This domain knowledge is a scalar function that is represented in the form of certain set of rules, easily provided by humans.",6 Conclusion and Future Work,[0],[0]
"By using a rank-based training we are able to effectively generalize beyond the domain knowledge function in problem instances where we do not have access to labeled data, thus establishing a viable option for solving structured prediction problems in those regimes.
",6 Conclusion and Future Work,[0],[0]
R-SPEN only uses unlabeled data and domain knowledge for training.,6 Conclusion and Future Work,[0],[0]
We should also effectively benefit from annotated data if any is available for the task.,6 Conclusion and Future Work,[0],[0]
"This can be accomplished by augmenting the domain knowledge with rules that take into account the distance between predicted and ground truth labels.
",6 Conclusion and Future Work,[0],[0]
"In the future, we wish to explore the effectiveness of R-SPEN on various tasks using domain knowledge functions with varying degrees of complexity.",6 Conclusion and Future Work,[0],[0]
This research was funded by DARPA grant FA8750-17-C-0106.,Acknowledgments,[0],[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA or the U.S. Government.",Acknowledgments,[0],[0]
This paper introduces rank-based training of structured prediction energy networks (SPENs).,abstractText,[0],[0]
Our method samples from output structures using gradient descent and minimizes the ranking violation of the sampled structures with respect to a scalar scoring function defined using domain knowledge.,abstractText,[0],[0]
"We have successfully trained SPEN for citation field extraction without any labeled data instances, where the only source of supervision is a simple human-written scoring function.",abstractText,[0],[0]
Such scoring functions are often easy to provide; the SPEN then furnishes an efficient structured prediction inference procedure.,abstractText,[0],[0]
Training Structured Prediction Energy Networks with Indirect Supervision,title,[0],[0]
"Inspired by human beings’ capabilities to transfer knowledge across tasks, transfer learning aims to leverage knowledge from a source domain to improve the learning performance or minimize the number of labeled examples required in a target domain.",1. Introduction,[0],[0]
It is of particular significance when tackling tasks with limited labeled examples.,1. Introduction,[0],[0]
"Transfer learning has proved its wide applicability in, for example,
1Hong Kong University of Science and Technology, Hong Kong 2Tencent AI Lab, Shenzhen, China.",1. Introduction,[0],[0]
"Correspondence to: Ying Wei <judyweiying@gmail.com>, Qiang Yang <qyang@cse.ust.hk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
image classification (Long et al., 2015), sentiment classification (Blitzer et al., 2006), dialog systems (Mo et al., 2016), and urban computing (Wei et al., 2016).
",1. Introduction,[0],[0]
"Three key research issues in transfer learning, pointed by Pan & Yang, are when to transfer, how to transfer, and what to transfer.",1. Introduction,[0],[0]
"Once transfer learning from a source domain is considered to benefit a target domain (when to transfer), an algorithm (how to transfer) discovers the transferable knowledge across domains (what to transfer).",1. Introduction,[0],[0]
"Different algorithms are likely to discover different transferable knowledge, and thereby lead to uneven transfer learning effectiveness which is evaluated by the performance improvement over non-transfer baselines in a target domain.",1. Introduction,[0],[0]
"To achieve the optimal performance improvement for a target domain given a source domain, researchers may try tens to hundreds of transfer learning algorithms covering instance (Dai et al., 2007), parameter (Tommasi et al., 2014), and feature (Pan et al., 2011) based algorithms.",1. Introduction,[0],[0]
Such bruteforce exploration is computationally expensive and practically impossible.,1. Introduction,[0],[0]
"As a tradeoff, a sub-optimal improvement is usually obtained from a heuristically selected algorithm, which unfortunately requires considerable expertise in an ad-hoc and unsystematic manner.
",1. Introduction,[0],[0]
Exploring different algorithms is not the only way to optimize what to transfer.,1. Introduction,[0],[0]
"Previous transfer learning experiences do also help, which has been widely accepted in educational psychology (Luria, 1976; Belmont et al., 1982).",1. Introduction,[0],[0]
Human beings sharpen transfer learning skills of deciding what to transfer by conducting meta-cognitive reflection on diverse transfer learning experiences.,1. Introduction,[0],[0]
"For example, children who are good at playing chess may transfer mathematical skills, visuospatial skills, and decision making skills learned from chess to solve arithmetic problems, to solve pattern matching puzzles, and to play basketball, respectively.",1. Introduction,[0],[0]
"At a later age, it will be easier for them to decide to transfer mathematical and decision making skills learned from chess, rather than visuospatial skills, to market investment.",1. Introduction,[0],[0]
"Unfortunately, all existing transfer learning algorithms transfer from scratch and ignore previous transfer learning experiences.
",1. Introduction,[0],[0]
"Motivated by this, we propose a novel transfer learning framework called Learning to Transfer (L2T).",1. Introduction,[0],[0]
"The key idea of the L2T is to enhance the transfer learning effectiveness from a source to a target domain by leveraging previous
transfer learning experiences to optimize what and how to transfer between them.",1. Introduction,[0],[0]
"To achieve the goal, we establish the L2T in two stages.",1. Introduction,[0],[0]
"During the first stage, we encode each transfer learning experience into three components: a pair of source and target domains, the transferred knowledge between them parameterized as latent feature factors, and performance improvement.",1. Introduction,[0],[0]
We learn from all experiences a reflection function which maps a pair of domains and the transferred knowledge between them to the performance improvement.,1. Introduction,[0],[0]
"The reflection function, therefore, is believed to encrypt transfer learning skills of deciding what and how to transfer.",1. Introduction,[0],[0]
"In the second stage, what to transfer between a newly arrived pair of domains is optimized so that the value of the learned reflection function, matching to the performance improvement, is maximized.
",1. Introduction,[0],[0]
The contribution of this paper lies in that we propose a novel transfer learning framework which opens a new door to improve transfer learning effectiveness by taking advantage of previous transfer learning experiences.,1. Introduction,[0],[0]
The L2T can discover more transferable knowledge in a systematic and automatic fashion without requiring considerable expertise.,1. Introduction,[0],[0]
"We have also provided theoretic analyses to its algorithmic stability and generalization bound, and conducted comprehensive empirical studies showing the L2T’s superiority over state-of-the-art transfer learning algorithms.",1. Introduction,[0],[0]
"Transfer Learning Pan & Yang identified three key research issues in transfer learning as what, how, and when to transfer.",2. Related Work,[0],[0]
"Parameters (Yang et al., 2007a; Tommasi et al., 2014), instances (Dai et al., 2007), or latent feature factors (Pan et al., 2011) can be transferred between domains.",2. Related Work,[0],[0]
"A few works (Yang et al., 2007a; Tommasi et al., 2014) transfer parameters from source domains to regularize parameters of SVM-based models in a target domain.",2. Related Work,[0],[0]
"In (Dai et al., 2007), a basic learner in a target domain is boosted by borrowing the most useful source instances.",2. Related Work,[0],[0]
Various techniques capable of learning transferable latent feature factors between domains have been investigated extensively.,2. Related Work,[0],[0]
"These techniques include manually selected pivot features (Blitzer et al., 2006), dimension reduction (Pan et al., 2011; Baktashmotlagh et al., 2013; 2014), collective matrix factorization (Long et al., 2014), dictionary learning and sparse coding (Raina et al., 2007; Zhang et al., 2016), manifold learning (Gopalan et al., 2011; Gong et al., 2012), and deep learning (Yosinski et al., 2014; Long et al., 2015; Tzeng et al., 2015).",2. Related Work,[0],[0]
"Unlike L2T, all existing transfer learning studies transfer from scratch, i.e., only considering the pair of domains of interest but ignoring previous transfer learning experiences.",2. Related Work,[0],[0]
"Better yet, L2T can even collect all algorithms’ wisdom together, considering that any algorithm mentioned above can be applied in a transfer learning experience.
",2. Related Work,[0],[0]
"Multi-task Learning Multi-task learning (Caruana, 1997; Argyriou et al., 2007) trains multiple related tasks simultaneously and learns shared knowledge among tasks, so that all tasks reinforce each other in generalization abilities.",2. Related Work,[0],[0]
"However, multi-task learning assumes that training and testing examples follow the same distribution, as Figure 1 shows, which is different from transfer learning we focus on.
",2. Related Work,[0],[0]
Lifelong Learning,2. Related Work,[0],[0]
"Assuming a new learning task to lie in the same environment as training tasks, learning to learn (Thrun & Pratt, 1998) or meta-learning (Maurer, 2005; Finn et al., 2017; Al-Shedivat et al., 2018) transfers the knowledge shared among training tasks to the new task.",2. Related Work,[0],[0]
"(Ruvolo & Eaton, 2013; Pentina & Lampert, 2015) consider lifelong learning as online meta-learning.",2. Related Work,[0],[0]
"Though L2T and lifelong (meta) learning both aim to improve a learning system by leveraging histories, L2T differs from them in that each historical experience we consider is a transfer learning task rather than a traditional learning task as Figure 1 illustrates.",2. Related Work,[0],[0]
Thus we learn transfer learning skills instead of task-sharing knowledge.,2. Related Work,[0],[0]
We begin by first briefing the proposed L2T framework.,3. Learning to Transfer,[0],[0]
"Then we detail the two stages in L2T, i.e., learning transfer learning skills from previous transfer learning experiences and applying those skills to infer what and how to transfer for a future pair of source and target domains.",3. Learning to Transfer,[0],[0]
"A L2T agent previously conducted transfer learning several times, and kept a record of Ne transfer learning experiences.",3.1. The L2T Framework,[0],[0]
"We define each transfer learning experience as Ee = (〈Se, Te〉, ae, le) in which Se = {Xse,yse} and Te = {Xte,yte} denote a source domain and a target domain, respectively.",3.1. The L2T Framework,[0],[0]
X∗e ∈,3.1. The L2T Framework,[0],[0]
"Rn ∗ e×m represents the feature matrix if either domain has n∗e examples in a m-dimensional feature space X ∗e , where the superscript ∗ can be either s or t to denote a source or a target domain.",3.1. The L2T Framework,[0],[0]
y∗e ∈ Y∗e denotes the vector of labels with the length being n∗le.,3.1. The L2T Framework,[0],[0]
"The number of target labeled examples is much smaller than that of source labeled examples, i.e., ntle nsle.",3.1. The L2T Framework,[0],[0]
"We focus on the
learned reflection function f ( 1 ), we optimize the transferred knowledge between them, i.e., W∗Ne+1, by maximizing the value of f ( 2 ).
setting X se = X te and Yse = Yte for each pair of domains.",3.1. The L2T Framework,[0],[0]
ae ∈,3.1. The L2T Framework,[0],[0]
"A = {a1, · · · , aNa} denotes a transfer learning algorithm having been applied between Se and Te.",3.1. The L2T Framework,[0],[0]
Suppose that the transferred knowledge by the algorithm ae can be parameterized as We.,3.1. The L2T Framework,[0],[0]
"Finally, each transfer learning experience is labeled by the performance improvement ratio le = pste /p",3.1. The L2T Framework,[0],[0]
"t e, where pte is the learning performance (e.g., classification accuracy) on a test dataset in Te without transfer and pste is that on the same test dataset after transferring We from Se.",3.1. The L2T Framework,[0],[0]
"With Ne transfer learning experiences {E1, · · · , ENe} as the input, the L2T agent learns a function f such that f(Se, Te,We) approximates le as shown in the training stage of Figure 2.",3.1. The L2T Framework,[0],[0]
We call f a reflection function which encrypts meta-cognitive transfer learning skills - what and how to transfer can maximize the improvement ratio given a pair of domains.,3.1. The L2T Framework,[0],[0]
"Whenever a new pair of domains 〈SNe+1, TNe+1〉 arrives, the L2T agent can optimize the knowledge to be transferred, i.e., W∗Ne+1, by maximizing the value of f (see step 2 of the testing stage in Figure 2).",3.1. The L2T Framework,[0],[0]
Transfer learning algorithms applied can vary from experience to experience.,3.2. Parameterizing What to Transfer,[0],[0]
Uniformly parameterizing “what to transfer” for any algorithm out of the base algorithm set A is a prerequisite for learning the reflection function.,3.2. Parameterizing What to Transfer,[0],[0]
"In this work, we consider A to contain algorithms transferring single-level latent feature factors, because existing parameter-based and instance-based algorithms cannot address the transfer learning setting we focus on (i.e., X es = X et and Yes = Yet ).",3.2. Parameterizing What to Transfer,[0],[0]
"Though limited parameter-based algorithms (Yang et al., 2007a; Tommasi et al., 2014) can transfer across domains in heterogeneous label spaces, they can only handle binary classification problems.",3.2. Parameterizing What to Transfer,[0],[0]
"Deep neural network based algorithms (Yosinski et al., 2014; Long et al., 2015; Tzeng et al., 2015) transferring latent feature factors in multiple levels are left for our future research.",3.2. Parameterizing What to Transfer,[0],[0]
"As a result, we parameterize what to transfer with a latent feature factor matrix W which is elaborated in the following.
",3.2. Parameterizing What to Transfer,[0],[0]
Latent feature factor based algorithms aim to learn domaininvariant feature factors across domains.,3.2. Parameterizing What to Transfer,[0],[0]
Consider classifying dog pictures as a source domain and cat pictures as a target domain.,3.2. Parameterizing What to Transfer,[0],[0]
"The domain-invariant feature factors may include eyes, mouth, tails, etc.",3.2. Parameterizing What to Transfer,[0],[0]
"What to transfer, in this case, is the shared feature factors across domains.",3.2. Parameterizing What to Transfer,[0],[0]
"The way of defining domain-invariant feature factors dictates two groups of latent feature factor based algorithms, i.e., common latent space based and manifold ensemble based algorithms.
",3.2. Parameterizing What to Transfer,[0],[0]
"Common Latent Space Based This line of algorithms, including but not limited to TCA (Pan et al., 2011),",3.2. Parameterizing What to Transfer,[0],[0]
"LSDT (Zhang et al., 2016), and DIP (Baktashmotlagh et al., 2013), assumes that domain-invariant feature factors lie in a single shared latent space.",3.2. Parameterizing What to Transfer,[0],[0]
We denote by ϕ,3.2. Parameterizing What to Transfer,[0],[0]
the function mapping original feature representation into the latent space.,3.2. Parameterizing What to Transfer,[0],[0]
"If ϕ is linear, it can be represented as an embedding matrix W ∈ Rm×u where u is the dimensionality of the latent space.",3.2. Parameterizing What to Transfer,[0],[0]
"Therefore, we can parameterize what to transfer we focus on with W which describes u latent feature factors.",3.2. Parameterizing What to Transfer,[0],[0]
"Otherwise, if ϕ is nonlinear, what to transfer can still be parameterized with W. Though a nonlinear ϕ is not explicitly specified in most cases such as LSDT using sparse coding, target examples represented in the latent space Zte=ϕ(X t e)∈Rn t e×u are always available.",3.2. Parameterizing What to Transfer,[0],[0]
"Consequently, we obtain the similarity metric matrix (Cao et al., 2013) in the latent space, i.e., G=(Xte) †Zte(Z t e) T",3.2. Parameterizing What to Transfer,[0],[0]
"[(Xte) T ]†∈Rm×m according to XteG(X t e) T =Zte(Z t e) T , where (Xte) † is the pseudo-inverse of Xte.",3.2. Parameterizing What to Transfer,[0],[0]
"LDL decomposition on G = LDL T brings the latent feature factor matrix W = LD1/2.
",3.2. Parameterizing What to Transfer,[0],[0]
"Manifold Ensemble Based Initiated by Gopalan et al., manifold ensemble based algorithms consider that a source and a target domain share multiple subspaces (of the same dimension) as points on the Grassmann manifold between them.",3.2. Parameterizing What to Transfer,[0],[0]
"The representation of target examples on u domain-invariant latent factors turns to Zt(nu)e =[ϕ1(Xte), · · ·, ϕnu(Xte)] ∈",3.2. Parameterizing What to Transfer,[0],[0]
"Rn t e×nuu, if nu subspaces on the manifold are sampled.",3.2. Parameterizing What to Transfer,[0],[0]
"When all continuous subspaces on the manifold are sampled, i.e., nu →∞, Gong et al. proved
that Zt(∞)e (Z t(∞) e )T =XteG(X t e)
T where G is the similarity metric matrix.",3.2. Parameterizing What to Transfer,[0],[0]
"For computational details of G, please refer to (Gong et al., 2012).",3.2. Parameterizing What to Transfer,[0],[0]
"W=LD1/2 with L and D obtained from performing LDL decomposition on G=LDLT , therefore, is also qualified to represent latent feature factors distributed in a series of subspaces on a manifold.",3.2. Parameterizing What to Transfer,[0],[0]
"The goal here is to learn a reflection function f such that f(Se, Te,We) can approximate le for all experiences {E1, · · · , ENe}.",3.3. Learning from Experiences,[0],[0]
"The improvement ratio le is closely related to two aspects: 1) the difference between a source and a target domain in the shared latent space, and 2) the discriminative ability of a target domain in the latent space.",3.3. Learning from Experiences,[0],[0]
"The smaller difference guarantees more overlap between domains in the latent space, which signifies more transferable latent feature factors and higher improvement ratios as a result.",3.3. Learning from Experiences,[0],[0]
The discriminative ability of a target domain in the latent space is also vital to improve performances.,3.3. Learning from Experiences,[0],[0]
"Therefore, we build f to take both aspects into consideration.
",3.3. Learning from Experiences,[0],[0]
"The Difference between a Source and a Target Domain We follow (Pan et al., 2011) and adopt the maximum mean discrepancy (MMD) (Gretton et al., 2012b) to measure the difference between domains.",3.3. Learning from Experiences,[0],[0]
"By mapping two domains into the reproducing kernel Hilbert space (RKHS), MMD empirically evaluates the distance between the mean of source examples and that of target examples:
d̂2e(X s eWe,X t eWe)
= ∥∥∥∥",3.3. Learning from Experiences,[0],[0]
1nse nse∑ i=1 φ(xseiWe)−,3.3. Learning from Experiences,[0],[0]
1 nte nte∑ j=1 φ(xtejWe) ∥∥∥∥,3.3. Learning from Experiences,[0],[0]
2,3.3. Learning from Experiences,[0],[0]
"H
= 1
(nse)2 nse∑ i,i′=1 K(xseiWe,xsei′We)
",3.3. Learning from Experiences,[0],[0]
"+ 1
(nte)2 nte∑ j,j′=1 K(xtejWe,xtej′We)
",3.3. Learning from Experiences,[0],[0]
"− 2 nsente
nse,n t e∑
i,j=1
K(xseiWe,xtejWe), (1)
where xtej is the j-th example in X t e, and φ maps from the u-dimensional latent space to the RKHS H. K(·, ·) = 〈φ(·), φ(·)〉 is the kernel function.",3.3. Learning from Experiences,[0],[0]
Different kernels K lead to different MMD distances and thereby different values of f .,3.3. Learning from Experiences,[0],[0]
Thus learning the reflection function f is equivalent to optimizing K so that the MMD distance can well characterize the improvement ratio le for all pairs of domains.,3.3. Learning from Experiences,[0],[0]
"Inspired by multi-kernel MMD (Gretton et al., 2012b), we parameterize K as a linear combination of Nk PSD kernels, i.e., K=∑Nkk=1 βkKk (βk≥0, ∀k), and learn the coefficients β=[β1,· · ·, βNk ] instead.",3.3. Learning from Experiences,[0],[0]
"Using β, the MMD can be rewritten as d̂2e(XseWe,XteWe)= ∑Nk k=1 βkd̂ 2 e(k)(X s eWe,X t eWe)=
βT",3.3. Learning from Experiences,[0],[0]
"d̂e, where d̂e=[d̂2e(1),· · ·, d̂2e(Nk)] with d̂ 2 e(k) computed by the k-th kernel Kk.",3.3. Learning from Experiences,[0],[0]
"In this paper, we consider RBF kernels Kk(a,b)=exp(−‖a−b‖2/δk) by varying the bandwidth δk.
",3.3. Learning from Experiences,[0],[0]
"Unfortunately, the MMD alone is insufficient to measure the difference between domains.",3.3. Learning from Experiences,[0],[0]
The distance variance among all pairs of instances across domains is also required to fully characterize the difference.,3.3. Learning from Experiences,[0],[0]
A pair of domains with small MMD but extremely high variance still have little overlap.,3.3. Learning from Experiences,[0],[0]
"Equation (1) is actually the empirical estimation of d2e(XseWe,XteWe) =",3.3. Learning from Experiences,[0],[0]
"Exsexs′e xtext′e h(x s e,x s′ e ,x t e,x t′ e )",3.3. Learning from Experiences,[0],[0]
"(Gretton et al., 2012b) where h(xse,x s′ e ,x t e,x t′ e ) = K(xseWe,xs′e We)+K(xteWe,xt′e",3.3. Learning from Experiences,[0],[0]
"We)− K(xseWe, xt′e",3.3. Learning from Experiences,[0],[0]
We),3.3. Learning from Experiences,[0],[0]
"− K(xs′e We,xteWe).",3.3. Learning from Experiences,[0],[0]
"Consequently, the distance variance, σ2e , equals σ2e(X s eWe,X t eWe) =Exsexs′e xtext′e",3.3. Learning from Experiences,[0],[0]
"[(h(x s e,x s′ e ,x t e,x t′ e )
−Exsexs′e xtext′e h(x s e,x s′ e ,x t e,x t′ e )) 2].
",3.3. Learning from Experiences,[0],[0]
"To be consistent with the MMD characterized with Nk PSD kernels, we rewrite σ2e = βTQeβ where Qe = cov(h) =[
σe(1,1) ··· σe(1,Nk)··· ··· ··· σe(Nk,1) ···σe(Nk,Nk)
] .",3.3. Learning from Experiences,[0],[0]
"Each element σe(k1,k2) = cov(hk1 ,
hk2) =",3.3. Learning from Experiences,[0],[0]
E,3.3. Learning from Experiences,[0],[0]
[(hk1−Ehk1)(hk2−Ehk2)].,3.3. Learning from Experiences,[0],[0]
Note that Ehk1 is shorthand for Exsexs′e xtext′e,3.3. Learning from Experiences,[0],[0]
"hk1(x s e,x s′ e ,x t e,x t′ e ) where hk1 is calculated using the k1-th kernel.",3.3. Learning from Experiences,[0],[0]
"We detail the empirical estimate Q̂e of Qe in the supplementary due to page limit.
",3.3. Learning from Experiences,[0],[0]
"The Discriminative Ability of a Target Domain In view of limited labeled examples in a target domain, we resort to unlabeled examples to evaluate the discriminative ability.",3.3. Learning from Experiences,[0],[0]
The principles of the unlabeled discriminant criterion are two-fold: 1) similar examples should still be neighbours after being embedded into the latent space; and 2) dissimilar examples should be far away.,3.3. Learning from Experiences,[0],[0]
"We adopt the unlabeled discriminant criterion proposed in (Yang et al., 2007b),
τe = tr(WTe S N e We)/tr(W T e S L e We),
where SLe = ∑nte
j,j′=1 Hjj′ (nte) 2",3.3. Learning from Experiences,[0],[0]
(x t ej,3.3. Learning from Experiences,[0],[0]
− xtej′)(xtej,3.3. Learning from Experiences,[0],[0]
"− xtej′)T
is the local scatter covariance matrix with the neighbour information Hjj′ defined as Hjj′ ={ K(xtej ,xtej′), if xtej ∈ Nr(xtej′) and xtej′ ∈ Nr(xtej) 0, otherwise .
",3.3. Learning from Experiences,[0],[0]
"If xtej and x t ej′ are mutual r-nearest neighbours to each other, Hjj′ equals the kernel value K(xtej ,xtej′).",3.3. Learning from Experiences,[0],[0]
"By maximizing the unlabeled discriminant criterion τe, the local scatter covariance matrix guarantees the first principle, while
SNe = ∑nte",3.3. Learning from Experiences,[0],[0]
"j,j′=1 K(xtej ,xtej′ )−Hjj′
(nte) 2 (x
t ej",3.3. Learning from Experiences,[0],[0]
− xtej′)(xtej,3.3. Learning from Experiences,[0],[0]
"− xtej′)T ,
the non-local scatter covariance matrix, enforces the second principle.",3.3. Learning from Experiences,[0],[0]
τe also depends on kernels which in this case indicate different neighbour information and different degrees of similarity between neighboured examples.,3.3. Learning from Experiences,[0],[0]
"With τe(k) obtained from the k-th kernel Kk, the unlabeled discriminant criterion τe can be written as τe = ∑Nk k=1 βkτe(k) = β
T τ e where τ e =",3.3. Learning from Experiences,[0],[0]
"[τe(1), · · · , τe(Nk)].
",3.3. Learning from Experiences,[0],[0]
"The Optimization Problem Combining the two aspects abovementioned to model the reflection function f , we finally formulate the optimization problem as follows,
β∗, λ∗, μ∗, b∗ =
arg min β,λ,μ,",3.3. Learning from Experiences,[0],[0]
b Ne∑ e=1,3.3. Learning from Experiences,[0],[0]
Lh ( βT d̂e,3.3. Learning from Experiences,[0],[0]
+ λβ T Q̂eβ,3.3. Learning from Experiences,[0],[0]
+ μ βT,3.3. Learning from Experiences,[0],[0]
τ,3.3. Learning from Experiences,[0],[0]
"e + b, 1 le )
+ γ1R(β, λ, μ, b),
s.t.",3.3. Learning from Experiences,[0],[0]
"βk ≥ 0, ∀k ∈ {1, · · · , Nk}, λ ≥ 0, μ ≥ 0, (2) where 1/f = βT",3.3. Learning from Experiences,[0],[0]
d̂e +,3.3. Learning from Experiences,[0],[0]
"λβT Q̂eβ + μβT τe + b and Lh(·) is the Huber regression loss (Huber et al., 1964) constraining the value of 1/f to be as close to 1/le as possible.",3.3. Learning from Experiences,[0],[0]
γ1 controls the complexity of the parameters by l2-regularization.,3.3. Learning from Experiences,[0],[0]
"Minimizing the difference between domains, including the MMD distance βT",3.3. Learning from Experiences,[0],[0]
d̂e and the distance variance βT,3.3. Learning from Experiences,[0],[0]
"Q̂eβ, and meanwhile maximizing the discriminant criterion βT τ",3.3. Learning from Experiences,[0],[0]
"e in the target domain will contribute a large performance improvement ratio le (i.e., a small 1/le).",3.3. Learning from Experiences,[0],[0]
"λ and μ balance the importance of the three terms in f , and b is the bias term.",3.3. Learning from Experiences,[0],[0]
"Once the L2T agent has learned the reflection function f(S, T ,W;β∗, λ∗, μ∗, b∗), it takes advantage of the function to optimize what to transfer, i.e., the latent feature factor matrix W, for a newly arrived source domain SNe+1 and a target domain TNe+1.",3.4. Inferring What to Transfer,[0],[0]
The optimal latent feature factor matrix W∗Ne+1 should maximize the value of f .,3.4. Inferring What to Transfer,[0],[0]
"To this end, we optimize the following objective with regard to W, W
∗ Ne+1",3.4. Inferring What to Transfer,[0],[0]
"=argmax W f(SNe+1, TNe+1,W;β ∗ , λ ∗ , μ ∗ , b ∗ )",3.4. Inferring What to Transfer,[0],[0]
"− γ2‖W‖2F
=argmin W
(β ∗ )",3.4. Inferring What to Transfer,[0],[0]
T d̂W + λ ∗ (β ∗ ),3.4. Inferring What to Transfer,[0],[0]
"T Q̂Wβ ∗ + μ ∗ 1 (β∗)T τW + γ2‖W‖2F , (3)
where ‖ · ‖F denotes the matrix Frobenius norm and γ2 controls the complexity of W. The first and second terms in problem (3) can be calculated as
(β ∗ )",3.4. Inferring What to Transfer,[0],[0]
T d̂W = Nk∑ k=1 β,3.4. Inferring What to Transfer,[0],[0]
∗,3.4. Inferring What to Transfer,[0],[0]
"k
[ 1
a2 a∑ i,i′=1 Kk(viW,vi′W)+
1 b2 b∑ j,j′=1 Kk(wjW,wj′W)",3.4. Inferring What to Transfer,[0],[0]
"− 2 ab a,b∑ i,j=1 Kk(viW,wjW) ] ,
(β ∗ )",3.4. Inferring What to Transfer,[0],[0]
"T Q̂Wβ ∗ =
1
n2 − 1 n∑ i,i′=1 Nk∑ k=1 { β ∗",3.4. Inferring What to Transfer,[0],[0]
k,3.4. Inferring What to Transfer,[0],[0]
"[ Kk(viW,vi′W)+
Kk(wiW,wi′W)",3.4. Inferring What to Transfer,[0],[0]
"− 2Kk(viW,wi′W) − 1
n2 n∑ i,i′=1 ( Kk(viW,vi′W)
+ Kk(wiW,wi′W)",3.4. Inferring What to Transfer,[0],[0]
"− 2Kk(viW,wi′W) )",3.4. Inferring What to Transfer,[0],[0]
"]}2 ,
where the shorthand vi = xs(Ne+1)i, vi′ = x s (Ne+1)i′ , wj = xt(Ne+1)j , wj′ =x t (Ne+1)j′ , a=n s Ne+1, and b=n t Ne+1 are used due to space limit.",3.4. Inferring What to Transfer,[0],[0]
"Note that n=min(nsNe+1, n t Ne+1
).",3.4. Inferring What to Transfer,[0],[0]
"The third term in problem (3) can be computed as (β∗)T τW =∑Nk
k=1 β ∗ k tr(WTSNk W) tr(WTSLkW) .",3.4. Inferring What to Transfer,[0],[0]
"We optimize the non-convex prob-
lem (3) w.r.t W by employing a conjugate gradient method in which the gradient is listed in the supplementary material.",3.4. Inferring What to Transfer,[0],[0]
"In this section, we would theoretically investigate how previous transfer learning experiences influence a transfer learning task of interest.",4. Stability and Generalization Bounds,[0],[0]
"We also provide and prove the algorithmic stability and generalization bound for latent feature factor based transfer learning algorithms without experiences considered in the supplementary.
",4. Stability and Generalization Bounds,[0],[0]
"Consider S = {〈S1, T1〉,· · ·, 〈SNe , TNe〉} to be Ne transfer learning experiences or the so-called meta-samples (Maurer, 2005).",4. Stability and Generalization Bounds,[0],[0]
"Let L(S) be our algorithm that learns meta-cognitive knowledge from Ne transfer learning experiences in S and applies the knowledge to the (Ne+1)-th transfer learning task 〈SNe+1, TNe+1〉.",4. Stability and Generalization Bounds,[0],[0]
"To analyse the stability and give the generalization bound, we make an assumption on the distribution from which all Ne transfer learning experiences as meta-samples are sampled.",4. Stability and Generalization Bounds,[0],[0]
"For every environment E we have, all Ne pairs of source and target domains in S are drawn according to an algebraic β-mixing stationary distribution (DE)Ne , which is not i.i.d.. Intuitively, the algebraical β-mixing stationary distribution (see Definition 2 in (Mohri & Rostamizadeh, 2010)) with the β-mixing coefficient β(m)≤β0/mr models the dependence between future samples and past samples by a distance of at least m. The independent block technique (Bernstein, 1927) has been widely adopted to deal with non-i.i.d.",4. Stability and Generalization Bounds,[0],[0]
learning problems.,4. Stability and Generalization Bounds,[0],[0]
"Under this assumption, L(S) is uniformly stable.
",4. Stability and Generalization Bounds,[0],[0]
Theorem 1.,4. Stability and Generalization Bounds,[0],[0]
Suppose that for any xte and for any yte we have ‖xte‖2≤rx and |yte|≤B.,4. Stability and Generalization Bounds,[0],[0]
"Meanwhile, for any e-th transfer learning experience, we assume that the latent feature factor matrix ‖We‖≤ rW .",4. Stability and Generalization Bounds,[0],[0]
"To meet the assumption above, we reasonably simplify L(S) so that the latent feature factor matrix for the (Ne+1)-th transfer learning task is a linear combination of all Ne historical latent factor feature matrices plus a noisy latent feature matrix W satisfying ‖W ‖≤r , i.e., WNe+1= ∑Ne e=1 ceWe+W with each coefficient 0≤ce≤1.",4. Stability and Generalization Bounds,[0],[0]
Our algorithm L(S) is uniformly stable.,4. Stability and Generalization Bounds,[0],[0]
"For any 〈S, T 〉 as the coming transfer learning task, the following inequality holds:∣∣lemp(L(S), (S, T ))",4. Stability and Generalization Bounds,[0],[0]
"− lemp(L(Se0), (S, T ))∣∣
≤",4. Stability and Generalization Bounds,[0],[0]
4(4Ne − 3 + r /rW ),4. Stability and Generalization Bounds,[0],[0]
"B 2rx
λN2e ∼",4. Stability and Generalization Bounds,[0],[0]
O,4. Stability and Generalization Bounds,[0],[0]
"( B2rx λNe ) , (4)
where S = {〈S1, T1〉, · · · , 〈Se0−1, Te0−1〉, 〈Se0 , Te0〉, 〈Se0+1, Te0+1〉, · · · , 〈SNe , TNe〉} denotes the full set of meta-samples, and Se0 = {〈S1, T1〉, · · · , 〈Se0−1, Te0−1〉, 〈Se′0 , Te′0〉, 〈Se0+1, Te0+1〉, · · · , 〈SNe , TNe〉} represents the meta-samples with the e0-th meta-example replaced as 〈Se′0 , Te′0〉.",4. Stability and Generalization Bounds,[0],[0]
"By generalizing S to be meta-samples S and hS to be L2T L(S), we apply Corollary 21 in (Mohri & Rostamizadeh,
2010) to give the generalization bound of our algorithm L(S) in Theorem 2.
Theorem 2.",4. Stability and Generalization Bounds,[0],[0]
Let δ′ = δ−(Ne) 1 2(r+1),4. Stability and Generalization Bounds,[0],[0]
− 14 (r > 1 is required).,4. Stability and Generalization Bounds,[0],[0]
"Then for any sample S of size Ne drawn according to an algebraic β-mixing stationary distribution, and δ ≥ 0 such that δ′ ≥ 0, the following generalization bound holds with probability at least 1− δ: ∣∣R(L(S))−RNe(L(S))∣∣ < O ( (Ne) 1 2(r+1)",4. Stability and Generalization Bounds,[0],[0]
"− 1 4 √ log( 1
δ′ )
) ,
where R(L(S)) and RNe(L(S)) denote the expected risk and the empirical risk of L2T over meta-samples, respectively.",4. Stability and Generalization Bounds,[0],[0]
"A larger mixing parameter r, indicating more independence, would lead to a tighter bound.
",4. Stability and Generalization Bounds,[0],[0]
"Theorem 2 tells that as the number of transfer learning experiences, i.e., Ne, increases, L2T tends to produce a tighter generalization bound.",4. Stability and Generalization Bounds,[0],[0]
This fact lays the foundation for further conducting L2T in an online manner which can gradually assimilate transfer learning experiences and continuously improve.,4. Stability and Generalization Bounds,[0],[0]
The detailed proofs for Theorem 1 and 2 can be found in the supplementary.,4. Stability and Generalization Bounds,[0],[0]
"Datasets We evaluate the L2T framework on two image datasets, Caltech-256 (Griffin et al., 2007) and Sketches (Eitz et al., 2012).",5. Experiments,[0],[0]
"Caltech-256, collected from Google Images, contains a total of 30,607 images in 256 categories.",5. Experiments,[0],[0]
"The Sketches dataset, however, consists of 20,000 unique sketches by human beings that are evenly distributed over 250 different categories.",5. Experiments,[0],[0]
"We construct each pair of source and target domains by randomly sampling three categories from Caltech-256 as the source domain and randomly sampling three categories from Sketches as the target domain, which we give an example in the supplementary material.",5. Experiments,[0],[0]
"Consequently, there are 20, 000/250× 3 = 720 examples in a target domain of each pair.",5. Experiments,[0],[0]
"In total, we generate 1,000 training pairs for preparing transfer learning experiences, 500 validation pairs to determine hyperparameters of the reflection function, and 500 testing pairs to evaluate the reflection function.",5. Experiments,[0],[0]
"We characterize each image from both datasets with 4,096-dimensional features extracted by a convolutional neural network pre-trained by ImageNet.
",5. Experiments,[0],[0]
"In this paper we generate transfer learning experiences by ourselves, because we are the first to consider transfer learning experiences and there exists no off-the-shelf datasets.",5. Experiments,[0],[0]
"In real-world applications, either the number of labeled examples in a target domain or the transfer learning algorithm could vary from experience to experience.",5. Experiments,[0],[0]
"In order to mimic the real environment, we prepare each transfer learning experience by randomly selecting a transfer learning algorithm from a base set A and randomly setting the number of labeled target examples in the range of [3, 120].",5. Experiments,[0],[0]
"The randomly
generated training experiences, lying in the same environment (generated by one dataset), are non i.i.d., which fit the algebraical β-mixing assumption theoretically in Section 4.
",5. Experiments,[0],[0]
"Baselines and Evaluation Metrics We compare L2T with the following nine baseline algorithms in three classes:
• Non-transfer: Original builds a model using labeled data in a target domain only.",5. Experiments,[0],[0]
"• Common latent space based transfer learning algorithms: TCA (Pan et al., 2011), ITL (Shi & Sha, 2012), CMF (Long et al., 2014), LSDT (Zhang et al., 2016), STL (Raina et al., 2007), DIP (Baktashmotlagh et al., 2013) and SIE (Baktashmotlagh et al., 2014).",5. Experiments,[0],[0]
"• Manifold ensemble based algorithms: GFK (Gong et al., 2012).
",5. Experiments,[0],[0]
The eight feature-based transfer learning algorithms also constitute the base set A.,5. Experiments,[0],[0]
"Based on feature representations obtained by different algorithms, we use the nearestneighbor classifier to perform three-class classification for the target domain.
",5. Experiments,[0],[0]
One evaluation metric is classification accuracy on testing examples of a target domain.,5. Experiments,[0],[0]
"However, accuracies are incomparable for different target domains at different levels of difficulty.",5. Experiments,[0],[0]
"The other evaluation metric we adopt is the performance improvement ratio defined in Section 3.1, so as to compare the L2T over different pairs of domains.
",5. Experiments,[0],[0]
"Performance Comparison In this experiment, we learn a reflection function from 1,000 transfer learning experiences, and evaluate the reflection function on 500 testing pairs of source and target domains by comparing the average performance improvement ratio to the baselines.",5. Experiments,[0],[0]
"In building the reflection function, we use 33 RBF kernels with the bandwidth δk in the range of [2−8η : 20.5η : 28η] where η = 1
nsen t eNe
∑Ne e=1 ∑nse,nte i,j=1 ‖xseiW",5. Experiments,[0],[0]
"− xtejW‖22 follows
the median trick (Gretton et al., 2012a).",5. Experiments,[0],[0]
"As Figure 4 shows, on average the proposed L2T framework outperforms the baselines up to 10% when varying the number of labeled samples in the target domain.",5. Experiments,[0],[0]
"As the number of labeled target examples increases from 3 to 120, the performance improvement ratio becomes smaller because the accuracy of Original without transfer tends to increase.",5. Experiments,[0],[0]
"The baseline
algorithms behave differently.",5. Experiments,[0],[0]
"The transferable knowledge learned by LSDT helps a target domain a lot when training examples are scarce, while GFK performs poorly until training examples become more.",5. Experiments,[0],[0]
STL is almost the worst baseline because it learns a dictionary from the source domain only but ignores the target domain.,5. Experiments,[0],[0]
It runs at a high risk of failure especially when two domains are distant.,5. Experiments,[0],[0]
"DIP and SIE, which minimize the MMD and Hellinger distance between domains subject to manifold constraints, are competent.",5. Experiments,[0],[0]
"Note that we have run the paired t-test between L2T and each baseline with all the p-values in the order of 10−12, concluding that the L2T is significantly superior.
",5. Experiments,[0],[0]
We also randomly select six of the 500 testing pairs and compare classification accuracies by different algorithms for each pair in Figure 3.,5. Experiments,[0],[0]
The performance of all baselines varies from pair to pair.,5. Experiments,[0],[0]
"Among all the baseline methods, TCA performs the best when transferring between domains in Figure 3a and LSDT is the most superior in Figure 3c.",5. Experiments,[0],[0]
"However, L2T consistently outperforms the baselines on all the settings.",5. Experiments,[0],[0]
"For some pairs, e.g., Figures 3a, 3c and 3f, the three classes in a target domain are comparably easy to tell apart, hence Original without transfer can achieve even better results than some transfer learning algorithms.",5. Experiments,[0],[0]
"In this case, L2T still improves by discovering the best transferable knowledge from the source domain, especially when the number of labeled examples is small (see Figure 3c and 3f).",5. Experiments,[0],[0]
"If two domains are very related, e.g., the source with “galaxy” and “saturn” and the target with “sun” in Figure 3a, L2T even finds out more transferable knowledge and contributes more significant improvement.
",5. Experiments,[0],[0]
"Varying the Experiences We further investigate how transfer learning experiences used to learn the reflection function influence the performance of L2T. In this experiment, we evaluate on 50 randomly sampled pairs out of the 500 testing pairs in order to efficiently investigate a wide range of cases in the following.",5. Experiments,[0],[0]
"The sampled set is unbiased and sufficient to characterize such influence, evidenced by the asymptotic consistency between the average performance improvement ratio on the 500 pairs in Figure 4 and that on the 50 pairs in the last line of Table 1.",5. Experiments,[0],[0]
"First, we fix the number of transfer learning experiences to be 1,000 and vary the set of base transfer learning algorithms.",5. Experiments,[0],[0]
The results are shown in Table 1.,5. Experiments,[0],[0]
"Even with experiences generated by single base algorithm, e.g., ITL or DIP, the L2T can still learn a reflection function that significantly better (p-value < 0.05) decides what to transfer than using ITL or DIP directly.",5. Experiments,[0],[0]
"With more base algorithms involved, the transfer learning experiences are more diverse to cover more situations of source-target pairs and the knowledge transferred between them.",5. Experiments,[0],[0]
"As a result, the L2T learns a better reflection function and thereby achieves higher performance improvement ratios, which coincides with Theorem 2 where a larger r indicating more independence between experiences gives a tighter bound.",5. Experiments,[0],[0]
"Second, we fix the set of base algorithms to include all the eight baselines and vary the number of transfer learning experiences used for training.",5. Experiments,[0],[0]
"As shown in Figure 5, the average performance improvement ratio achieved by L2T tends to increase as the number of labeled examples in the target domain decreases, given that Original without transfer performs extremely poor with scarce labeled examples.
",5. Experiments,[0],[0]
Figure 6.,5. Experiments,[0],[0]
"Varying the components constituted in the f .
Figure 7.",5. Experiments,[0],[0]
"Varying the number of kernels considered in the f .
",5. Experiments,[0],[0]
"More importantly, it increases as the number of experiences increases, which coincides with Theorem 2.
",5. Experiments,[0],[0]
"Varying the Reflection Function We also study the influence of different configurations of the reflection function on the performance of L2T. First, we vary the components to be considered in building the reflection function f as shown in Figure 6.",5. Experiments,[0],[0]
"Considering single type, either MMD, variance, or the discriminant criterion, brings inferior performance and even negative transfer.",5. Experiments,[0],[0]
"L2T taking all the three factors into consideration outperforms the others, demonstrating that the three components are all necessary and mutually reinforcing.",5. Experiments,[0],[0]
"With all the three components included, we plot values of the learned β∗ in the supplementary material.",5. Experiments,[0],[0]
"Second, we change the kernels used.",5. Experiments,[0],[0]
"In Figure 7, we present results by either narrowing down or extending the range [2−8η : 20.5η : 28η].",5. Experiments,[0],[0]
"Obviously, more kernels (e.g., [2−12η : 20.5η : 212η]), capable of encrypting better trans-
fer learning skills in the reflection function, achieve larger performance improvement ratios.",5. Experiments,[0],[0]
"In this paper, we propose a novel L2T framework for transfer learning which automatically optimizes what and how to transfer between a source and a target domain by leveraging previous transfer learning experiences.",6. Conclusion,[0],[0]
"In particular, L2T learns a reflection function mapping a pair of domains and the knowledge transferred between them to the performance improvement ratio.",6. Conclusion,[0],[0]
"When a new pair of domains arrives, L2T optimizes what and how to transfer by maximizing the value of the learned reflection function.",6. Conclusion,[0],[0]
We believe that L2T opens a new door to improve transfer learning by leveraging transfer learning experiences.,6. Conclusion,[0],[0]
"Many research issues, e.g., incorporating hierarchical latent feature factors as what to transfer and designing online L2T, can be further examined.",6. Conclusion,[0],[0]
We thank the reviewers for their valuable comments to improve this paper.,Acknowledgements,[0],[0]
"The research has been supported by National Grant Fundamental Research (973 Program) of China under Project 2014CB340304, Hong Kong CERG projects 16211214/16209715/16244616, Hong Kong ITF ITS/391/15FX and NSFC 61673202.",Acknowledgements,[0],[0]
"In transfer learning, what and how to transfer are two primary issues to be addressed, as different transfer learning algorithms applied between a source and a target domain result in different knowledge transferred and thereby the performance improvement in the target domain.",abstractText,[0],[0]
Determining the optimal one that maximizes the performance improvement requires either exhaustive exploration or considerable expertise.,abstractText,[0],[0]
"Meanwhile, it is widely accepted in educational psychology that human beings improve transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning practices.",abstractText,[0],[0]
"Motivated by this, we propose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging previous transfer learning experiences.",abstractText,[0],[0]
We establish the L2T framework in two stages: 1) we learn a reflection function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer are the best for a future pair of domains by optimizing the reflection function.,abstractText,[0],[0]
"We also theoretically analyse the algorithmic stability and generalization bound of L2T, and empirically demonstrate its superiority over several state-ofthe-art transfer learning algorithms.",abstractText,[0],[0]
Transfer Learning via Learning to Transfer,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 946–956 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
946",text,[0],[0]
"Target-oriented (also mentioned as “target-level” or “aspect-level” in some works) sentiment classification aims to determine sentiment polarities over “opinion targets” that explicitly appear in the sentences (Liu, 2012).",1 Introduction,[0],[0]
"For example, in the sentence “I am pleased with the fast log on, and the long battery life”, the user mentions two targets
∗The work was done when Xin Li was an intern at Tencent AI Lab.",1 Introduction,[0],[0]
"This project is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414).
",1 Introduction,[0],[0]
"1Our code is open-source and available at https:// github.com/lixin4ever/TNet
“log on” and “better life”, and expresses positive sentiments over them.",1 Introduction,[0],[0]
"The task is usually formulated as predicting a sentiment category for a (target, sentence) pair.
",1 Introduction,[0],[0]
"Recurrent Neural Networks (RNNs) with attention mechanism, firstly proposed in machine translation (Bahdanau et al., 2014), is the most commonly-used technique for this task.",1 Introduction,[0],[0]
"For example, Wang et al. (2016); Tang et al. (2016b); Yang et al. (2017); Liu and Zhang (2017); Ma et al. (2017) and Chen et al. (2017) employ attention to measure the semantic relatedness between each context word and the target, and then use the induced attention scores to aggregate contextual features for prediction.",1 Introduction,[0],[0]
"In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy.",1 Introduction,[0],[0]
"For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favorite”.",1 Introduction,[0],[0]
"To some extent, this drawback is rooted in the attention mechanism, as also observed in machine translation (Luong et al., 2015) and image captioning (Xu et al., 2015).
",1 Introduction,[0],[0]
Another observation is that the sentiment of a target is usually determined by key phrases such as “is my favorite”.,1 Introduction,[0],[0]
"By this token, Convolutional Neural Networks (CNNs)—whose capability for extracting the informative n-gram features (also called “active local features”) as sentence representations has been verified in (Kim, 2014; Johnson and Zhang, 2015)— should be a suitable model for this classification problem.",1 Introduction,[0],[0]
"However, CNN likely fails in cases where a sentence expresses different sentiments over multiple targets, such as “great food but the service was dreadful!”.",1 Introduction,[0],[0]
"One reason is that CNN cannot fully explore the target information as done by RNN-based meth-
ods (Tang et al.,",1 Introduction,[0],[0]
"2016a).2 Moreover, it is hard for vanilla CNN to differentiate opinion words of multiple targets.",1 Introduction,[0],[0]
"Precisely, multiple active local features holding different sentiments (e.g., “great food” and “service was dreadful”) may be captured for a single target, thus it will hinder the prediction.
",1 Introduction,[0],[0]
"We propose a new architecture, named TargetSpecific Transformation Networks (TNet), to solve the above issues in the task of target sentiment classification.",1 Introduction,[0],[0]
TNet firstly encodes the context information into word embeddings and generates the contextualized word representations with LSTMs.,1 Introduction,[0],[0]
"To integrate the target information into the word representations, TNet introduces a novel Target-Specific Transformation (TST) component for generating the target-specific word representations.",1 Introduction,[0],[0]
"Contrary to the previous attention-based approaches which apply the same target representation to determine the attention scores of individual context words, TST firstly generates different representations of the target conditioned on individual context words, then it consolidates each context word with its tailor-made target representation to obtain the transformed word representation.",1 Introduction,[0],[0]
"Considering the context word “long” and the target “battery life” in the above example, TST firstly measures the associations between “long” and individual target words.",1 Introduction,[0],[0]
Then it uses the association scores to generate the target representation conditioned on “long”.,1 Introduction,[0],[0]
"After that, TST transforms the representation of “long” into its target-specific version with the new target representation.",1 Introduction,[0],[0]
"Note that “long” could also indicate a negative sentiment (say for “startup time”), and the above TST is able to differentiate them.
",1 Introduction,[0],[0]
"As the context information carried by the representations from the LSTM layer will be lost after the non-linear TST, we design a contextpreserving mechanism to contextualize the generated target-specific word representations.",1 Introduction,[0],[0]
Such mechanism also allows deep transformation structure to learn abstract features3.,1 Introduction,[0],[0]
"To help the CNN feature extractor locate sentiment indicators more accurately, we adopt a proximity strategy to scale the input of convolutional layer with positional relevance between a word and the target.
",1 Introduction,[0],[0]
"2One method could be concatenating the target representation with each word representation, but the effect as shown in (Wang et al., 2016) is limited.
3Abstract features usually refer to the features ultimately useful for the task (Bengio et al., 2013; LeCun et al., 2015).
",1 Introduction,[0],[0]
"In summary, our contributions are as follows: • TNet adapts CNN to handle target-level sentiment classification, and its performance dominates the state-of-the-art models on benchmark datasets.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
A novel Target-Specific Transformation component is proposed to better integrate target information into the word representations.,1 Introduction,[0],[0]
"• A context-preserving mechanism is designed to forward the context information into a deep transformation architecture, thus, the model can learn more abstract contextualized word features from deeper networks.",1 Introduction,[0],[0]
"Given a target-sentence pair (wτ ,w), where wτ = {wτ1 , wτ2 , ..., wτm} is a sub-sequence of w = {w1, w2, ..., wn}, and the corresponding word embeddings xτ = {xτ1 , xτ2 , ..., xτm} and x = {x1, x2, ..., xn}, the aim of target sentiment classification is to predict the sentiment polarity y ∈ {P,N,O} of the sentence w over the target wτ , where P , N and O denote “positive”, “negative” and “neutral” sentiments respectively.
",2 Model Description,[0],[0]
The architecture of the proposed TargetSpecific Transformation Networks (TNet) is shown in Fig. 1.,2 Model Description,[0],[0]
"The bottom layer is a BiLSTM which transforms the input x = {x1, x2, ..., xn} ∈ Rn×dimw into the contextualized word representations h(0) = {h(0)1 , h (0) 2 , ..., h (0) n } ∈ Rn×2dimh (i.e. hidden states of BiLSTM), where dimw and dimh denote the dimensions of the word embeddings and the hidden representations respectively.",2 Model Description,[0],[0]
"The middle part, the core part of our TNet, consists of L Context-Preserving Transformation (CPT) layers.",2 Model Description,[0],[0]
The CPT layer incorporates the target information into the word representations via a novel Target-Specific Transformation (TST) component.,2 Model Description,[0],[0]
"CPT also contains a contextpreserving mechanism, resembling identity mapping (He et al., 2016a,b) and highway connection (Srivastava et al., 2015a,b), allows preserving the context information and learning more abstract word-level features using a deep network.",2 Model Description,[0],[0]
"The top most part is a position-aware convolutional layer which first encodes positional relevance between a word and a target, and then extracts informative features for classification.",2 Model Description,[0],[0]
"As observed in Lai et al. (2015), combining contextual information with word embeddings is an
effective way to represent a word in convolutionbased architectures.",2.1 Bi-directional LSTM Layer,[0],[0]
"TNet also employs a BiLSTM to accumulate the context information for each word of the input sentence, i.e., the bottom part in Fig. 1.",2.1 Bi-directional LSTM Layer,[0],[0]
"For simplicity and space issue, we denote the operation of an LSTM unit on xi as LSTM(xi).",2.1 Bi-directional LSTM Layer,[0],[0]
"Thus, the contextualized word representation h(0)i ∈ R2dimh is obtained as follows:
h (0)",2.1 Bi-directional LSTM Layer,[0],[0]
i =,2.1 Bi-directional LSTM Layer,[0],[0]
"[ −−−−→ LSTM(xi); ←−−−− LSTM(xi)], i ∈",2.1 Bi-directional LSTM Layer,[0],[0]
"[1, n].",2.1 Bi-directional LSTM Layer,[0],[0]
(1),2.1 Bi-directional LSTM Layer,[0],[0]
The above word-level representation has not considered the target information yet.,2.2 Context-Preserving Transformation,[0],[0]
Traditional attention-based approaches keep the word-level features static and aggregate them with weights as the final sentence representation.,2.2 Context-Preserving Transformation,[0],[0]
"In contrast, as shown in the middle part in Fig. 1, we introduce multiple CPT layers and the detail of a single CPT is shown in Fig. 2.",2.2 Context-Preserving Transformation,[0],[0]
"In each CPT layer, a tailor-made TST component that aims at better consolidating word representation and target representation is proposed.",2.2 Context-Preserving Transformation,[0],[0]
"Moreover, we design a context-preserving mechanism enabling the learning of target-specific word representations in a deep neural architecture.",2.2 Context-Preserving Transformation,[0],[0]
TST component is depicted with the TST block in Fig. 2.,2.2.1 Target-Specific Transformation,[0],[0]
The first task of TST is to generate the representation of the target.,2.2.1 Target-Specific Transformation,[0],[0]
"Previous methods (Chen
et al., 2017; Liu and Zhang, 2017) average the embeddings of the target words as the target representation.",2.2.1 Target-Specific Transformation,[0],[0]
This strategy may be inappropriate in some cases because different target words usually do not contribute equally.,2.2.1 Target-Specific Transformation,[0],[0]
"For example, in the target “amd turin processor”, the word “processor” is more important than “amd” and “turin”, because the sentiment is usually conveyed over the phrase head, i.e.,“processor”, but seldom over modifiers (such as brand name “amd”).",2.2.1 Target-Specific Transformation,[0],[0]
Ma et al. (2017) attempted to overcome this issue by measuring the importance score between each target word representation and the averaged sentence vector.,2.2.1 Target-Specific Transformation,[0],[0]
"However, it may be ineffective for sentences expressing multiple sentiments (e.g., “Air has higher resolution but the fonts are small.”), because taking the average tends to neutralize different sentiments.
",2.2.1 Target-Specific Transformation,[0],[0]
We propose to dynamically compute the importance of target words based on each sentence word rather than the whole sentence.,2.2.1 Target-Specific Transformation,[0],[0]
"We first employ another BiLSTM to obtain the target word representations hτ ∈ Rm×2dimh :
hτj =",2.2.1 Target-Specific Transformation,[0],[0]
[ −−−−→ LSTM(xτj ); ←−−−− LSTM(xτj ),2.2.1 Target-Specific Transformation,[0],[0]
"], j ∈",2.2.1 Target-Specific Transformation,[0],[0]
"[1,m].",2.2.1 Target-Specific Transformation,[0],[0]
"(2)
Then, we dynamically associate them with each word wi in the sentence to tailor-make target representation rτi at the time step",2.2.1 Target-Specific Transformation,[0],[0]
"i:
rτi = m∑ j=1 hτj ∗ F(h (l) i , h τ j ), (3)
where the function F measures the relatedness between the j-th target word representation hτj and
the i-th word-level representation h(l)i :
F(h(l)i , h τ j ) =
exp (h (l)>",2.2.1 Target-Specific Transformation,[0],[0]
i h τ j ),2.2.1 Target-Specific Transformation,[0],[0]
"∑m
k=1 exp (h (l)>",2.2.1 Target-Specific Transformation,[0],[0]
i h τ k) .,2.2.1 Target-Specific Transformation,[0],[0]
"(4)
Finally, the concatenation of rτi and h (l) i is fed into a fully-connected layer to obtain the i-th targetspecific word representation h̃i (l) :
h̃ (l) i = g(W τ",2.2.1 Target-Specific Transformation,[0],[0]
[h (l) i :,2.2.1 Target-Specific Transformation,[0],[0]
r τ,2.2.1 Target-Specific Transformation,[0],[0]
i ],2.2.1 Target-Specific Transformation,[0],[0]
"+ b τ ), (5)
where g(∗) is a non-linear activation function and “:” denotes vector concatenation.",2.2.1 Target-Specific Transformation,[0],[0]
W τ,2.2.1 Target-Specific Transformation,[0],[0]
and bτ are the weights of the layer.,2.2.1 Target-Specific Transformation,[0],[0]
"After the non-linear TST (see Eq. 5), the context information captured with contextualized representations from the BiLSTM layer will be lost since the mean and the variance of the features within the feature vector will be changed.",2.2.2 Context-Preserving Mechanism,[0],[0]
"To take advantage of the context information, which has been proved to be useful in (Lai et al., 2015), we investigate two strategies: Lossless Forwarding (LF) and Adaptive Scaling (AS), to pass the context information to each following layer, as depicted by the block “LF/AS” in Fig. 2.",2.2.2 Context-Preserving Mechanism,[0],[0]
"Accordingly, the model variants are named TNet-LF and TNet-AS.
Lossless Forwarding.",2.2.2 Context-Preserving Mechanism,[0],[0]
This strategy preserves context information by directly feeding the features before the transformation to the next layer.,2.2.2 Context-Preserving Mechanism,[0],[0]
"Specifically, the input h(l+1)i of the (l+1)-th CPT layer is formulated as:
h (l+1)",2.2.2 Context-Preserving Mechanism,[0],[0]
i = h,2.2.2 Context-Preserving Mechanism,[0],[0]
(l) i + h̃ (l),2.2.2 Context-Preserving Mechanism,[0],[0]
"i , i ∈",2.2.2 Context-Preserving Mechanism,[0],[0]
"[1, n], l ∈",2.2.2 Context-Preserving Mechanism,[0],[0]
"[0, L], (6)
where h(l)i is the input of the l-th layer and h̃ (l) i is the output of TST in this layer.",2.2.2 Context-Preserving Mechanism,[0],[0]
"We unfold the recursive form of Eq. 6 as follows:
h (l+1)",2.2.2 Context-Preserving Mechanism,[0],[0]
i = h (0),2.2.2 Context-Preserving Mechanism,[0],[0]
i +TST(h (0) i )+· · ·+TST(h (l) i ).,2.2.2 Context-Preserving Mechanism,[0],[0]
"(7)
Here, we denote h̃(l)i as TST(h (l) i ).",2.2.2 Context-Preserving Mechanism,[0],[0]
"From Eq. 7, we can see that the output of each layer will contain the contextualized word representations (i.e., h (0) i ), thus, the context information is encoded into the transformed features.",2.2.2 Context-Preserving Mechanism,[0],[0]
"We call this strategy “Lossless Forwarding” because the contextualized representations and the transformed representations (i.e., TST(h(l)i )) are kept unchanged during the feature combination.
",2.2.2 Context-Preserving Mechanism,[0],[0]
Adaptive Scaling.,2.2.2 Context-Preserving Mechanism,[0],[0]
"Lossless Forwarding introduces the context information by directly adding back the contextualized features to the transformed features, which raises a question: Can the weights of the input and the transformed features be adjusted dynamically?",2.2.2 Context-Preserving Mechanism,[0],[0]
"With this motivation, we propose another strategy, named “Adaptive Scaling”.",2.2.2 Context-Preserving Mechanism,[0],[0]
"Similar to the gate mechanism in RNN variants (Jozefowicz et al., 2015), Adaptive Scaling introduces a gating function to control the passed proportions of the transformed features and the input features.",2.2.2 Context-Preserving Mechanism,[0],[0]
"The gate t(l) as follows:
t (l)",2.2.2 Context-Preserving Mechanism,[0],[0]
"i = σ(Wtransh (l) i + btrans), (8)
where t(l)i is the gate for the i-th input of the l-th CPT layer, and σ is the sigmoid activation function.",2.2.2 Context-Preserving Mechanism,[0],[0]
"Then we perform convex combination of h(l)i and h̃(l)i based on the gate:
h (l+1)",2.2.2 Context-Preserving Mechanism,[0],[0]
i = t,2.2.2 Context-Preserving Mechanism,[0],[0]
(l) i h̃ (l) i + (1− t (l) i ),2.2.2 Context-Preserving Mechanism,[0],[0]
h,2.2.2 Context-Preserving Mechanism,[0],[0]
(l) i .,2.2.2 Context-Preserving Mechanism,[0],[0]
"(9)
Here, denotes element-wise multiplication.",2.2.2 Context-Preserving Mechanism,[0],[0]
"The non-recursive form of this equation is as follows (for clarity, we ignore the subscripts):
h(l+1) =",2.2.2 Context-Preserving Mechanism,[0],[0]
"[ l∏ k=0 (1− t(k))] h(0)
+[t(0) l∏
k=1
(1− t(k))]",2.2.2 Context-Preserving Mechanism,[0],[0]
TST(h(0)),2.2.2 Context-Preserving Mechanism,[0],[0]
"+ · · ·
+t(l−1)(1− t(l))",2.2.2 Context-Preserving Mechanism,[0],[0]
TST(h(l−1)),2.2.2 Context-Preserving Mechanism,[0],[0]
"+ t(l) TST(h(l)).
",2.2.2 Context-Preserving Mechanism,[0],[0]
"Thus, the context information is integrated in each upper layer and the proportions of the contextualized representations and the transformed representations are controlled by the computed gates in different transformation layers.",2.2.2 Context-Preserving Mechanism,[0],[0]
Recall that the second issue that blocks CNN to perform well is that vanilla CNN may associate a target with unrelated general opinion words which are frequently used as modifiers for different targets across domains.,2.3 Convolutional Feature Extractor,[0],[0]
"For example, “service” in “Great food but the service is dreadful” may be associated with both “great” and “dreadful”.",2.3 Convolutional Feature Extractor,[0],[0]
"To solve it, we adopt a proximity strategy, which is observed effective in (Chen et al., 2017; Li and Lam, 2017).",2.3 Convolutional Feature Extractor,[0],[0]
"The idea is a closer opinion word is more likely to be the actual modifier of the target.
",2.3 Convolutional Feature Extractor,[0],[0]
"Specifically, we first calculate the position relevance vi between the i-th word and the target4:
vi =  1− (k+m−i)C i < k",2.3 Convolutional Feature Extractor,[0],[0]
+m 1− i−kC k +m ≤,2.3 Convolutional Feature Extractor,[0],[0]
i ≤ n 0,2.3 Convolutional Feature Extractor,[0],[0]
"i > n
(10)
where k is the index of the first target word, C is a pre-specified constant, and m is the length of the target wτ .",2.3 Convolutional Feature Extractor,[0],[0]
"Then, we use v to help CNN locate the correct opinion w.r.t.",2.3 Convolutional Feature Extractor,[0],[0]
"the given target:
ĥ (l) i = h",2.3 Convolutional Feature Extractor,[0],[0]
"(l) i ∗ vi, i ∈",2.3 Convolutional Feature Extractor,[0],[0]
"[1, n], l ∈",2.3 Convolutional Feature Extractor,[0],[0]
"[1, L].",2.3 Convolutional Feature Extractor,[0],[0]
"(11)
",2.3 Convolutional Feature Extractor,[0],[0]
"Based on Eq. 10 and Eq. 11, the words close to the target will be highlighted and those far away will be downgraded.",2.3 Convolutional Feature Extractor,[0],[0]
v is also applied on the intermediate output to introduce the position information into each CPT layer.,2.3 Convolutional Feature Extractor,[0],[0]
"Then we feed the weighted h(L) to the convolutional layer, i.e., the top-most layer in Fig. 1, to generate the feature map c ∈",2.3 Convolutional Feature Extractor,[0],[0]
"Rn−s+1 as follows:
ci = ReLU(w > convh (L) i:i+s−1 + bconv), (12)
where h(L)i:i+s−1 ∈ Rs·dimh is the concatenated vector of ĥ(L)i , · · · , ĥ (L) i+s−1, and s is the kernel size.",2.3 Convolutional Feature Extractor,[0],[0]
wconv ∈ Rs·dimh and bconv ∈ R are learnable weights of the convolutional kernel.,2.3 Convolutional Feature Extractor,[0],[0]
"To capture the most informative features, we apply max pooling (Kim, 2014) and obtain the sentence representation z ∈",2.3 Convolutional Feature Extractor,[0],[0]
"Rnk by employing nk kernels:
z =",2.3 Convolutional Feature Extractor,[0],[0]
"[max(c1), · · · ,max(cnk)]",2.3 Convolutional Feature Extractor,[0],[0]
>.,2.3 Convolutional Feature Extractor,[0],[0]
"(13)
Finally, we pass z to a fully connected layer for sentiment prediction:
p(y|wτ ,w) = Softmax(Wfz + bf ).",2.3 Convolutional Feature Extractor,[0],[0]
"(14)
where Wf and bf are learnable parameters.",2.3 Convolutional Feature Extractor,[0],[0]
"4As we perform sentence padding, it is possible that the index i is larger than the actual length n of the sentence.",2.3 Convolutional Feature Extractor,[0],[0]
"As shown in Table 1, we evaluate the proposed TNet on three benchmark datasets: LAPTOP and REST are from SemEval ABSA challenge (Pontiki et al., 2014), containing user reviews in laptop domain and restaurant domain respectively.",3.1 Experimental Setup,[0],[0]
"We also remove a few examples having the “conflict label” as done in (Chen et al., 2017); TWITTER is built by Dong et al. (2014), containing twitter posts.",3.1 Experimental Setup,[0],[0]
"All tokens are lowercased without removal of stop words, symbols or digits, and sentences are zero-padded to the length of the longest sentence in the dataset.",3.1 Experimental Setup,[0],[0]
Evaluation metrics are Accuracy and Macro-Averaged F1 where the latter is more appropriate for datasets with unbalanced classes.,3.1 Experimental Setup,[0],[0]
"We also conduct pairwise t-test on both Accuracy and Macro-Averaged F1 to verify if the improvements over the compared models are reliable.
",3.1 Experimental Setup,[0],[0]
"TNet is compared with the following methods.
",3.1 Experimental Setup,[0],[0]
"• SVM (Kiritchenko et al., 2014):",3.1 Experimental Setup,[0],[0]
"It is a traditional support vector machine based model with extensive feature engineering;
• AdaRNN (Dong et al., 2014):",3.1 Experimental Setup,[0],[0]
"It learns the sentence representation toward target for sentiment prediction via semantic composition over dependency tree;
• AE-LSTM, and ATAE-LSTM (Wang et al., 2016): AE-LSTM is a simple LSTM model incorporating the target embedding as input, while ATAE-LSTM extends AE-LSTM with attention;
• IAN (Ma et al., 2017): IAN employs two LSTMs to learn the representations of the context and the target phrase interactively;
• CNN-ASP: It is a CNN-based model implemented by us which directly concatenates target representation to each word embedding;
• TD-LSTM (Tang et al., 2016a):",3.1 Experimental Setup,[0],[0]
"It employs two LSTMs to model the left and right contexts of the target separately, then performs predictions based on concatenated context representations;
• MemNet (Tang et al., 2016b):",3.1 Experimental Setup,[0],[0]
"It applies attention mechanism over the word embeddings multiple times and predicts sentiments
based on the top-most sentence representations;
• BILSTM-ATT-G (Liu and Zhang, 2017):",3.1 Experimental Setup,[0],[0]
"It models left and right contexts using two attention-based LSTMs and introduces gates to measure the importance of left context, right context, and the entire sentence for the prediction;
• RAM (Chen et al., 2017): RAM is a multilayer architecture where each layer consists of attention-based aggregation of word features and a GRU cell to learn the sentence representation.
",3.1 Experimental Setup,[0],[0]
"We run the released codes of TD-LSTM and BILSTM-ATT-G to generate results, since their papers only reported results on TWITTER.",3.1 Experimental Setup,[0],[0]
"We also rerun MemNet on our datasets and evaluate it with both accuracy and Macro-Averaged F1.5
We use pre-trained GloVe vectors (Pennington et al., 2014) to initialize the word embeddings and the dimension is 300 (i.e., dimw = 300).",3.1 Experimental Setup,[0],[0]
"For out-of-vocabulary words, we randomly sample their embeddings from the uniform distribution U(−0.25, 0.25), as done in (Kim, 2014).",3.1 Experimental Setup,[0],[0]
"We only use one convolutional kernel size because it was observed that CNN with single optimal kernel size is comparable with CNN having multiple kernel sizes on small datasets (Zhang and Wallace, 2017).",3.1 Experimental Setup,[0],[0]
"To alleviate overfitting, we apply dropout on the input word embeddings of the LSTM and",3.1 Experimental Setup,[0],[0]
the ultimate sentence representation z.,3.1 Experimental Setup,[0],[0]
"All weight matrices are initialized with the uniform distribution U(−0.01, 0.01) and the biases are initialized
5The codes of TD-LSTM/MemNet and BILSTM-ATTG are available at: http://ir.hit.edu.cn/˜dytang and http://leoncrashcode.github.io.",3.1 Experimental Setup,[0],[0]
"Note that MemNet was only evaluated with accuracy.
as zeros.",3.1 Experimental Setup,[0],[0]
"The training objective is cross-entropy, and Adam (Kingma and Ba, 2015) is adopted as the optimizer by following the learning rate and the decay rates in the original paper.
",3.1 Experimental Setup,[0],[0]
The hyper-parameters of TNet-LF and TNetAS are listed in Table 2.,3.1 Experimental Setup,[0],[0]
"Specifically, all hyperparameters are tuned on 20% randomly held-out training data and the hyper-parameter collection producing the highest accuracy score is used for testing.",3.1 Experimental Setup,[0],[0]
Our model has comparable number of parameters compared to traditional LSTM-based models as we reuse parameters in the transformation layers and BiLSTM.6,3.1 Experimental Setup,[0],[0]
"As shown in Table 3, both TNet-LF and TNet-AS consistently achieve the best performance on all datasets, which verifies the efficacy of our whole TNet model.",3.2 Main Results,[0],[0]
"Moreover, TNet can perform well for different kinds of user generated content, such as product reviews with relatively formal sentences in LAPTOP and REST, and tweets with more ungrammatical sentences in TWITTER.",3.2 Main Results,[0],[0]
The reason is the CNN-based feature extractor arms TNet with more power to extract accurate features from ungrammatical sentences.,3.2 Main Results,[0],[0]
"Indeed, we can also observe that another CNN-based baseline, i.e., CNNASP implemented by us, also obtains good results on TWITTER.
",3.2 Main Results,[0],[0]
"On the other hand, the performance of those comparison methods is mostly unstable.",3.2 Main Results,[0],[0]
"For the tweet in TWITTER, the competitive BILSTMATT-G and RAM cannot perform as effective as they do for the reviews in LAPTOP and REST, due to the fact that they are heavily rooted in LSTMs and the ungrammatical sentences hinder their ca-
6All experiments are conducted on a single NVIDIA GTX 1080.",3.2 Main Results,[0],[0]
"The prediction cost of a sentence is about 2 ms.
pability in capturing the context features.",3.2 Main Results,[0],[0]
"Another difficulty caused by the ungrammatical sentences is that the dependency parsing might be errorprone, which will affect those methods such as AdaRNN using dependency information.
",3.2 Main Results,[0],[0]
"From the above observations and analysis, some takeaway message for the task of target sentiment classification could be:
• LSTM-based models relying on sequential information can perform well for formal sentences by capturing more useful context features;
",3.2 Main Results,[0],[0]
"• For ungrammatical text, CNN-based models may have some advantages because CNN aims to extract the most informative n-gram features and is thus less sensitive to informal texts without strong sequential patterns.",3.2 Main Results,[0],[0]
"To investigate the impact of each component such as deep transformation, context-preserving mechanism, and positional relevance, we perform comparison between the full TNet models and its ablations (the third group in Table 3).",3.3 Performance of Ablated TNet,[0],[0]
"After removing the deep transformation (i.e., the techniques introduced in Section 2.2), both TNet-LF and TNetAS are reduced to TNet w/o transformation (where
position relevance is kept), and their results in both accuracy and F1 measure are incomparable with those of TNet.",3.3 Performance of Ablated TNet,[0],[0]
"It shows that the integration of target information into the word-level representations is crucial for good performance.
",3.3 Performance of Ablated TNet,[0],[0]
"Comparing the results of TNet and TNet w/o context (where TST and position relevance are kept), we observe that the performance of TNet w/o context drops significantly on LAPTOP and REST7, while on TWITTER, TNet w/o context performs very competitive (p-values with TNetLF and TNet-AS are 0.066 and 0.053 respectively for Accuracy).",3.3 Performance of Ablated TNet,[0],[0]
"Again, we could attribute this phenomenon to the ungrammatical user generated content of twitter, because the contextpreserving component becomes less important for such data.",3.3 Performance of Ablated TNet,[0],[0]
TNet,3.3 Performance of Ablated TNet,[0],[0]
"w/o context performs consistently better than TNet w/o transformation, which verifies the efficacy of the target specific transformation (TST), before applying context-preserving.
",3.3 Performance of Ablated TNet,[0],[0]
"As for the position information, we conduct statistical t-test between TNet-LF/AS and TNetLF/AS w/o position together with performance comparison.",3.3 Performance of Ablated TNet,[0],[0]
"All of the produced p-values are less than 0.05, suggesting that the improvements brought in by position information are significant.
",3.3 Performance of Ablated TNet,[0],[0]
"7Without specification, the significance level is set to 0.05.",3.3 Performance of Ablated TNet,[0],[0]
"The next interesting question is what if we replace the transformation module (i.e., the CPT layers in Fig.1) of TNet with other commonly-used components?",3.4 CPT versus Alternatives,[0],[0]
"We investigate two alternatives: attention mechanism and fully-connected (FC) layer, resulting in three pipelines as shown in the second group of Table 3 (position relevance is kept for them).
",3.4 CPT versus Alternatives,[0],[0]
"LSTM-ATT-CNN applies attention as the alternative8, and it does not need the contextpreserving mechanism.",3.4 CPT versus Alternatives,[0],[0]
It performs unexceptionally worse than the TNet variants.,3.4 CPT versus Alternatives,[0],[0]
We are surprised that LSTM-ATT-CNN is even worse than TNet w/o transformation (a pipeline simply removing the transformation module) on TWITTER.,3.4 CPT versus Alternatives,[0],[0]
"More concretely, applying attention results in negative effect on TWITTER, which is consistent with the observation that all those attention-based state-of-the-art methods (i.e., TD-LSTM, MemNet, BILSTM-ATT-G, and RAM) cannot perform well on TWITTER.
",3.4 CPT versus Alternatives,[0],[0]
"LSTM-FC-CNN-LF and LSTM-FC-CNN-AS are built by applying FC layer to replace TST and keeping the context-preserving mechanism (i.e., LF and AS).",3.4 CPT versus Alternatives,[0],[0]
"Specifically, the concatenation of word representation and the averaged target vector is fed to the FC layer to obtain targetspecific features.",3.4 CPT versus Alternatives,[0],[0]
Note that LSTM-FC-CNNLF/AS are equivalent to TNet-LF/AS when processing single-word targets (see Eq. 3).,3.4 CPT versus Alternatives,[0],[0]
They obtain competitive results on all datasets: comparable with or better than the state-of-the-art methods.,3.4 CPT versus Alternatives,[0],[0]
"The TNet variants can still outperform LSTMFC-CNN-LF/AS with significant gaps, e.g., on LAPTOP and REST, the accuracy gaps between TNet-LF and LSTM-FC-CNN-LF are 0.42% (p < 0.03) and 0.38% (p < 0.04) respectively.",3.4 CPT versus Alternatives,[0],[0]
"As our TNet involves multiple CPT layers, we investigate the effect of the layer number L. Specifically, we conduct experiments on the held-out training data of LAPTOP and vary L from 2 to 10, increased by 2.",3.5 Impact of CPT Layer Number,[0],[0]
The cases L=1 and L=15 are also included.,3.5 Impact of CPT Layer Number,[0],[0]
The results are illustrated in Figure 3.,3.5 Impact of CPT Layer Number,[0],[0]
We can see that both TNet-LF and TNetAS achieve the best results when L=2.,3.5 Impact of CPT Layer Number,[0],[0]
"While increasing L, the performance is basically becoming worse.",3.5 Impact of CPT Layer Number,[0],[0]
"For large L, the performance of TNet-AS
8We tried different attention mechanisms and report the best one here, namely, dot attention (Luong et al., 2015).
generally becomes more sensitive, it is probably because AS involves extra parameters (see Eq 9) that increase the training difficulty.",3.5 Impact of CPT Layer Number,[0],[0]
Table 4 shows some sample cases.,3.6 Case Study,[0],[0]
The input targets are wrapped in the brackets with true labels given as subscripts.,3.6 Case Study,[0],[0]
"The notations P, N and O in the table represent positive, negative and neutral respectively.",3.6 Case Study,[0],[0]
"For each sentence, we underline the target with a particular color, and the text of its corresponding most informative n-gram feature9 captured by TNet-AS (TNet-LF captures very similar features) is in the same color (so color printing is preferred).",3.6 Case Study,[0],[0]
"For example, for the target “resolution” in the first sentence, the captured feature is “Air has higher”.",3.6 Case Study,[0],[0]
"Note that as discussed above, the CNN layer of TNet captures such features with the size-three kernels, so that the features are trigrams.",3.6 Case Study,[0],[0]
"Each of the last features of the second and seventh sentences contains a padding token, which is not shown.
",3.6 Case Study,[0],[0]
Our TNet variants can predict target sentiment more accurately than RAM and BILSTM-ATT-G in the transitional sentences such as the first sentence by capturing correct trigram features.,3.6 Case Study,[0],[0]
"For the third sentence, its second and third most informative trigrams are “100% .",3.6 Case Study,[0],[0]
"PAD” and “’ s not”, being used together with “features make up”, our models can make correct predictions.",3.6 Case Study,[0],[0]
"Moreover, TNet can still make correct prediction when the explicit opinion is target-specific.",3.6 Case Study,[0],[0]
"For example,
9For each convolutional filter, only one n-gram feature in the feature map will be kept after the max pooling.",3.6 Case Study,[0],[0]
"Among those from different filters, the n-gram with the highest frequency will be regarded as the most informative n-gram w.r.t.",3.6 Case Study,[0],[0]
"the given target.
“long” in the fifth sentence is negative for “startup time”, while it could be positive for other targets such as “battery life” in the sixth sentence.",3.6 Case Study,[0],[0]
The sentiment of target-specific opinion word is conditioned on the given target.,3.6 Case Study,[0],[0]
"Our TNet variants, armed with the word-level feature transformation w.r.t.",3.6 Case Study,[0],[0]
"the target, is capable of handling such case.
",3.6 Case Study,[0],[0]
"We also find that all these models cannot give correct prediction for the last sentence, a commonly used subjunctive style.",3.6 Case Study,[0],[0]
"In this case, the difficulty of prediction does not come from the detection of explicit opinion words but the inference based on implicit semantics, which is still quite challenging for neural network models.",3.6 Case Study,[0],[0]
"Apart from sentence level sentiment classification (Kim, 2014; Shi et al., 2018), aspect/target level sentiment classification is also an important research topic in the field of sentiment analysis.",4 Related Work,[0],[0]
"The early methods mostly adopted supervised learning approach with extensive hand-coded features (Blair-Goldensohn et al., 2008; Titov and McDonald, 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014; Wagner et al., 2014; Vo and Zhang, 2015), and they fail to model the semantic relatedness between a target and its context which is critical for target sentiment analysis.",4 Related Work,[0],[0]
Dong et al. (2014) incorporate the target information into the feature learning using dependency trees.,4 Related Work,[0],[0]
"As observed in previous works, the performance heavily relies on the quality of dependency parsing.",4 Related Work,[0],[0]
Tang et al. (2016a) propose to split the context into two parts and associate target with contextual features separately.,4 Related Work,[0],[0]
"Similar to (Tang et al., 2016a), Zhang et al. (2016) develop a three-way gated neural network to model the in-
teraction between the target and its surrounding contexts.",4 Related Work,[0],[0]
"Despite the advantages of jointly modeling target and context, they are not capable of capturing long-range information when some critical context information is far from the target.",4 Related Work,[0],[0]
"To overcome this limitation, researchers bring in the attention mechanism to model target-context association (Tang et al., 2016a,b; Wang et al., 2016; Yang et al., 2017; Liu and Zhang, 2017; Ma et al., 2017; Chen et al., 2017; Zhang et al., 2017; Tay et al., 2017).",4 Related Work,[0],[0]
"Compared with these methods, our TNet avoids using attention for feature extraction so as to alleviate the attended noise.",4 Related Work,[0],[0]
"We re-examine the drawbacks of attention mechanism for target sentiment classification, and also investigate the obstacles that hinder CNN-based models to perform well for this task.",5 Conclusions,[0],[0]
Our TNet model is carefully designed to solve these issues.,5 Conclusions,[0],[0]
"Specifically, we propose target specific transformation component to better integrate target information into the word representation.",5 Conclusions,[0],[0]
"Moreover, we employ CNN as the feature extractor for this classification problem, and rely on the contextpreserving and position relevance mechanisms to maintain the advantages of previous LSTM-based models.",5 Conclusions,[0],[0]
The performance of TNet consistently dominates previous state-of-the-art methods on different types of data.,5 Conclusions,[0],[0]
"The ablation studies show the efficacy of its different modules, and thus verify the rationality of TNet’s architecture.",5 Conclusions,[0],[0]
Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence.,abstractText,[0],[0]
"RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance.",abstractText,[0],[0]
"After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task, we propose a new model to overcome these issues.",abstractText,[0],[0]
"Instead of attention, our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer.",abstractText,[0],[0]
"Between the two layers, we propose a component to generate target-specific representations of words in the sentence, meanwhile incorporate a mechanism for preserving the original contextual information from the RNN layer.",abstractText,[0],[0]
Experiments show that our model achieves a new state-of-the-art performance on a few benchmarks.1,abstractText,[0],[0]
Transformation Networks for Target-Oriented Sentiment Classification,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2313–2318, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Transition-based parsing, one of the most prominent dependency parsing techniques, constructs a dependency structure by reading words sequentially from the sentence, and making a series of local decisions (called transitions) which incrementally build the structure.",1 Introduction,[0],[0]
"Transition-based parsing has been shown to be both fast and accurate; the number of transitions required to fully parse the sentence is linear relative to the number of words in the sentence.
",1 Introduction,[0],[0]
"In recent years, the field has seen dramatic improvements in the ability to correctly predict transitions.",1 Introduction,[0],[0]
Recent models include the greedy StackLSTM model of Dyer et al. (2015) and the globally normalized feed-forward networks of Andor et al. (2016).,1 Introduction,[0],[0]
"These models output a local decision at each transition point, so searching the space of possible paths to the predicted tree is an important component of high-accuracy parsers.
",1 Introduction,[0],[0]
One common search technique is beam search.,1 Introduction,[0],[0]
"(Zhang and Clark, 2008; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015; Weiss et al., 2015; Yazdani and Henderson, 2015)",1 Introduction,[0],[0]
"In beamsearch, a fixed number of candidate transition sequences are generated, and the highest-scoring sequence is chosen as the answer.",1 Introduction,[0],[0]
One downside to beam search is that it often results in a significant amount of wasted predictions.,1 Introduction,[0],[0]
"A constant number of beams are explored at all points throughout the sentence, leading to some unnecessary exploration towards the beginning of the sentence, and potentially insufficient exploration towards the end.
",1 Introduction,[0],[0]
"One way that this problem can be mitigated is by using a dynamically-sized beam (Mejia-Lavalle and Ramos, 2013).",1 Introduction,[0],[0]
"When using this technique, at each step, prune all beams whose scores are below some value s, where s is calculated based upon the distribution of scores of available beams.",1 Introduction,[0],[0]
"Common methods for pruning are removing all beams below some percentile, or any beams which scored below some constant percentage of the highest-scoring beam.
",1 Introduction,[0],[0]
Another approach to solving this issue is given by Choi and McCallum (2013).,1 Introduction,[0],[0]
"They introduced selectional branching, which involves performing an initial greedy parse, and then using confidence estimates on each prediction to spawn additional beams.",1 Introduction,[0],[0]
"Relative to standard beam-search, this reduces the average number of predictions required to parse a sentence, resulting in a speed-up.
",1 Introduction,[0.9520692052975024],"['Our best results on Japanese-English either yield a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a 3.4×CPU decoding speed up with only a minor drop in BLEU.']"
"In this paper, we introduce heuristic backtracking, which expands on the ideas of selectional branching by integrating a search strategy based on a heuristic function (Pearl, 1984): a function which estimates
2313
the future cost of taking a particular decision.",1 Introduction,[0],[0]
"When paired with a good heuristic, heuristic backtracking maintains the property of reducing wasted predictions, but allows us to more fully explore the space of possible transition sequences (as compared to selectional branching).",1 Introduction,[0],[0]
"In this paper, we use a heuristic based on the confidence of transition predictions.
",1 Introduction,[0],[0]
We also introduce a new optimization: heuristic backtracking with cutoff.,1 Introduction,[0],[0]
"Since heuristic backtracking produces results incrementally, it is possible to stop the search early if we have found an answer that we believe to be the gold parse, saving time proportional to the number of backtracks remaining.
",1 Introduction,[0],[0]
"We compare the performance of these various decoding algorithms with the Stack-LSTM parser (Dyer et al., 2015), and achieve slightly higher accuracy than beam search, in significantly less time.",1 Introduction,[0],[0]
Our starting point is the model described by Dyer et al. (,2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"2015).1 The parser implements the arc-standard algorithm (Nivre, 2004) and it therefore makes use of a stack and a buffer.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"In (Dyer et al., 2015), the stack and the buffer are encoded with Stack-LSTMs, and a third sequence with the history of actions taken by the parser is encoded with another Stack-LSTM.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"The three encoded sequences form the parser state pt defined as follows,
pt = max {0,W[st;bt;at] + d} , (1)
where W is a learned parameter matrix, bt, st and at are the stack LSTM encoding of buffer, stack and the history of actions, and d is a bias term.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"The output pt (after a component-wise rectified linear unit (ReLU) nonlinearity (Glorot et al., 2011)) is then used to compute the probability of the parser action at time t as:
p(zt | pt) = exp
( g>ztpt + qzt ) ∑
z′∈A(S,B) exp",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"( g>z′pt + qz′ ) , (2)
where gz is a column vector representing the (output) embedding of the parser action z, and qz is a bias term for action z.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"The set A(S,B) represents
1We refer to the original work for details.
",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
the valid transition actions that may be taken in the current state.,2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"The objective function is:
Lθ(w, z) = |z|∑
t=1
log p(zt | pt) (3)
where z refers to parse transitions.",2 Transition-Based Parsing With Stack-LSTM,[0],[0]
"Using the Stack-LSTM parsing model of Dyer et al. (2015) to predict each decision greedily yields very high accuracy; however, it can only explore one path, and it therefore can be improved by conducting a larger search over the space of possible parses.",3 Heuristic Backtracking,[0],[0]
"To do this, we introduce a new algorithm, heuristic backtracking.",3 Heuristic Backtracking,[0],[0]
We also introduce a novel cutoff approach to further increase speed.,3 Heuristic Backtracking,[0],[0]
"We model the space of possible parses as a tree, where each node represents a certain parse state (with complete values for stack, buffer, and action history).",3.1 Decoding Strategy,[0],[0]
"Transitions connect nodes of the tree, and leaves of the tree represent final states.
",3.1 Decoding Strategy,[0],[0]
"During the first iteration, we start at the root of the tree, and greedily parse until we reach a leaf.",3.1 Decoding Strategy,[0],[0]
"That is, for each node, we use the Stack-LSTM model to calculate scores for each transition (as described in Section 2), and then execute the highest-scoring transition, generating a child node upon which we repeat the procedure.",3.1 Decoding Strategy,[0],[0]
"Additionally, we save an ordered list of the transition scores, and calculate the confidence of the node (as described in Section 3.2).
",3.1 Decoding Strategy,[0],[0]
"When we reach the leaf node, we backtrack to the location that is most likely to fix a mistake.",3.1 Decoding Strategy,[0],[0]
"To find this, we look at all explored nodes that still have at least one unexplored child, and choose the node with the lowest heuristic confidence (see Section 3.2).",3.1 Decoding Strategy,[0],[0]
"We rewind our stack, buffer, and action history to that state, and execute the highest-scoring transition from that node that has not yet been explored.",3.1 Decoding Strategy,[0],[0]
"At this point, we are again in a fully-unexplored node, and can greedily parse just as before until we reach another leaf.
",3.1 Decoding Strategy,[0],[0]
"Once we have generated b leaves, we score them all and return the transition sequence leading up to the highest-scoring leaf as the answer.",3.1 Decoding Strategy,[0],[0]
"Just as in previous studies (Collins and Roark, 2004), we use the
sum of the log probabilities of all individual transitions as the overall score for the parse.",3.1 Decoding Strategy,[0],[0]
"Let n indicate a node, which consists of a state, a buffer, and an action history.",3.2 Calculating Error Likelihood,[0],[0]
"We may refer to a specific node as nji , which means it has i actions in its action history and it is part of the history of the jth leaf (and possibly subsequent leaves).",3.2 Calculating Error Likelihood,[0],[0]
"Let the function T (n) represent a sorted vector containing all possible transitions from n, and S(n) represent a sorted vector containing the scores of all of these transitions, in terms of log probabilities of each score.",3.2 Calculating Error Likelihood,[0],[0]
"We can index the scores in order of value, so T1(n) is the highest-scoring transition and S1(n) is its score, T2(n) is the second-highest-scoring transition, etc.",3.2 Calculating Error Likelihood,[0],[0]
"Here, let un indicate the ranking of the transition leading to the first unexplored child of a node n. Also, let V (n) represent the total score of all nodes in the history of n, i.e. the sum of all the scores of individual transitions that allowed us to get to n.
To calculate the confidence of an individual node, Choi and McCallum (2013) simply found the score margin, or difference in probability between the topscoring transition and the second-highest scoring transition: C(n) = S1(n)",3.2 Calculating Error Likelihood,[0],[0]
− S2(n).,3.2 Calculating Error Likelihood,[0],[0]
"In selectional branching, the only states for which the confidence was relevant were the states in the first greedy parse, i.e. states n1i for all i. For heuristic backtracking, we wish to generalize this to any state nji for all i and j.
We do this in the following way:
H(nji )",3.2 Calculating Error Likelihood,[0],[0]
=,3.2 Calculating Error Likelihood,[0],[0]
(V (n 1 i ),3.2 Calculating Error Likelihood,[0],[0]
− V (nji )),3.2 Calculating Error Likelihood,[0],[0]
"+ (S(u
n",3.2 Calculating Error Likelihood,[0],[0]
j,3.2 Calculating Error Likelihood,[0],[0]
"i
)−1(n",3.2 Calculating Error Likelihood,[0],[0]
"j i ) + S(u
n j",3.2 Calculating Error Likelihood,[0],[0]
"i
)(n j i ))
",3.2 Calculating Error Likelihood,[0],[0]
"(4) Intuitively, this formula means that the node that will be explored first is the node that will yield a parse that scores as close to the greedy choice as possible.",3.2 Calculating Error Likelihood,[0],[0]
"The first term ensures that it has a history of good choices, and the second term ensures that the new child node being explored will be nearly as good as the prior child.",3.2 Calculating Error Likelihood,[0],[0]
"As discussed earlier, we use number of predictions made by the model as a proxy for the speed; execution speed may vary based on system and algorithmic implementation, but prediction count gives a good estimate of the overall work done by the algorithm.
",3.3 Number of Predictions,[0],[0]
"Consider a sentence of length l, which requires at most 2l transitions with the greedy decoder (Nivre, 2004).",3.3 Number of Predictions,[0],[0]
"The number of predictions required for heuristic backtracking for b leaves is guaranteed to be less than or equal to a beam search with b beams.
",3.3 Number of Predictions,[0],[0]
"When doing a beam search, the first transition will require 1 prediction, and then every subsequent transition will require 1 prediction per beam, or b predictions.",3.3 Number of Predictions,[0],[0]
This results in a total of b(2l,3.3 Number of Predictions,[0],[0]
"− 1) + 1 predictions.
",3.3 Number of Predictions,[0],[0]
"When doing heuristic backtracking, the first greedy search will require 2l predictions.",3.3 Number of Predictions,[0],[0]
"Every
subsequent prediction will require a number of predictions dependent on the target of the backtrack: backtracking to nji will require 2l − (i + 1) predictions.",3.3 Number of Predictions,[0],[0]
Note that 0,3.3 Number of Predictions,[0],[0]
<,3.3 Number of Predictions,[0],[0]
i < 2l.,3.3 Number of Predictions,[0],[0]
"Thus, each backtrack will require at maximum 2l − 1 predictions.",3.3 Number of Predictions,[0],[0]
"Therefore, the maximum total amount of predictions is 2l + (b− 1)(2l",3.3 Number of Predictions,[0],[0]
"− 1) = b(2l − 1) + 1.
",3.3 Number of Predictions,[0],[0]
"However, note that on average, there are significantly fewer.",3.3 Number of Predictions,[0],[0]
"Assuming that all parts of a sentence have approximately equal score distributions, the average backtrack will be where i = l, and reduce predictions by 50%.
",3.3 Number of Predictions,[0],[0]
An intuitive understanding of this difference can be gained by viewing the graphs of various decoding methods in Figure 1.,3.3 Number of Predictions,[0],[0]
"Beam search has many nodes which never yield children that reach an end-state; dynamic beam search has fewer, but still several.",3.3 Number of Predictions,[0],[0]
"Selectional branching has none, but suffers from the restriction that every parse candidate can be no more than one decision away from the greedy parse.",3.3 Number of Predictions,[0],[0]
"With heuristic backtracking, there is no such restriction, but yet every node explored is directly useful for generating a candidate parse.",3.3 Number of Predictions,[0],[0]
Another inefficiency inherent to beam search is the fact that all b beams are always fully explored.,3.4 Early Cutoff,[0],[0]
"Since the beams are calculated in parallel, this is inevitable.",3.4 Early Cutoff,[0],[0]
"However, with heuristic backtracking, the beams are calculated incrementally; this gives us the opportunity to cut off our search at any point.",3.4 Early Cutoff,[0],[0]
"In order to leverage this into more efficient parsing, we constructed a second Stack-LSTM model, which we call the cutoff model.",3.4 Early Cutoff,[0],[0]
"The cutoff model uses a single Stack-LSTM2 that takes as input the sequence of parser states (see Eq 1), and outputs a boolean variable predicting whether the entire parse is correct or incorrect.
",3.4 Early Cutoff,[0],[0]
"To train the cutoff model, we used stochastic gradient descent over the training set.",3.4 Early Cutoff,[0],[0]
"For each training example, we first parse it greedily using the StackLSTM parser.",3.4 Early Cutoff,[0],[0]
"Then, for as long as the parse has at least one mistake, we pass it to the cutoff model as a negative training example.",3.4 Early Cutoff,[0],[0]
"Once the parse is completely correct, we pass it to the cutoff model as a positive training example.",3.4 Early Cutoff,[0],[0]
"The loss function that we
22 layers and 300 dimensions.
use is:
Lθ = − log p(t | s) (5)
where s is the LSTM encoded vector and t is the truth (parse correct/incorrect).
",3.4 Early Cutoff,[0],[0]
"When decoding using early cutoff, we follow the exact same procedure as for normal heuristic backtracking, but after every candidate parse is generated, we use it as input to our cutoff model.",3.4 Early Cutoff,[0],[0]
"When our cutoff model returns our selection as correct, we stop backtracking and return it as the answer.",3.4 Early Cutoff,[0],[0]
"If we make b attempts without finding a correct parse, we follow the same procedure as before.",3.4 Early Cutoff,[0],[0]
"To test the effectiveness of heuristic backtracking, we compare it with other decoding techniques: greedy, beam search,3, dynamic beam search (Mejia-Lavalle and Ramos, 2013), and selectional branching (Choi and McCallum, 2013).",4 Experiments and Results,[0],[0]
"We then try heuristic backtracking (see Section 3.1), and heuristic backtracking with cutoff (see Section 3.4).",4 Experiments and Results,[0],[0]
"Note that beam search was not used for early-update training (Collins and Roark, 2004).",4 Experiments and Results,[0],[0]
"We use the same greedy training strategy for all models, and we only change the decoding strategy.
",4 Experiments and Results,[0],[0]
We tested the performance of these algorithms on the English SD and Chinese CTB.4,4 Experiments and Results,[0],[0]
"A single model was trained using the techniques described in Section 2, and used as the transition model for all decoding algorithms.",4 Experiments and Results,[0],[0]
"Each decoding technique was tested with varying numbers of beams; as b increased, both the predictions per sentence and accuracy trended upwards.",4 Experiments and Results,[0],[0]
"The results are summarized in Table 1.5 Note that we report results for only the highestaccuracy b (in the development set) for each.
",4 Experiments and Results,[0],[0]
We also report the results of the cutoff model in Table 2.,4 Experiments and Results,[0],[0]
"The same greedily-trained model as above was used to generate candidate parses and confidence estimates for each transition, and then the cutoff model was trained to use these confidence esti-
3Greedy and beam-search were already explored by Dyer et al. (2015)
4Using the exact same settings as Dyer et al. (2015) with pretrained embeddings and part-of-speech tags.
",4 Experiments and Results,[0],[0]
"5The development sets are used to set the model parameters; results on the development sets are similar to the ones obtained in the test sets.
",4 Experiments and Results,[0],[0]
mates to discriminate between correctly-parsed and incorrectly-parsed sentences.,4 Experiments and Results,[0],[0]
"In Table 1 we see that in both English and Chinese, the best heuristic backtracking performs approximately as well as the best beam search, while making less than half the predictions.",5 Discussion,[0],[0]
"This supports our hypothesis that heuristic backtracking can perform at the same level as beam search, but with increased efficiency.
",5 Discussion,[0],[0]
"Dynamic beam search also performed as well as full beam search, despite demonstrating a reduction in predictions on par with that of heuristic backtracking.",5 Discussion,[0],[0]
"Since the implementation of dynamic beam search is very straightforward for systems which have already implemented beam search, we believe this will prove to be a useful finding.
",5 Discussion,[0],[0]
"Heuristic backtracking with cutoff outperformed greedy decoding, and reduced transitions by an additional 50%.",5 Discussion,[0],[0]
"However, it increased accuracy slightly less than full heuristic backtracking.",5 Discussion,[0],[0]
"We believe this difference could be mitigated with an improved cutoff model; as can be seen in Table 2, the cutoff model was only able to discriminate between correct and incorrect parses around 75% of the time.",5 Discussion,[0],[0]
"Also, note that while predictions per sentence were low, the overall runtime was increased due to running the cutoff LSTM multiple times per sentence.",5 Discussion,[0],[0]
"Heuristic backtracking is most similar to the work of Choi and McCallum (2013), but is distinguished from theirs by allowing new beams to be initialized from any point in the parse, rather than only from points in the initial greedy parse.",6 Related Work,[0],[0]
"Heuristic backtracking also bears similarity to greedy-best-firstsearch (Pearl, 1984), but is unique in that it guarantees that b candidate solutions will be found within b(2l",6 Related Work,[0],[0]
− 1) + 1 predictions.,6 Related Work,[0],[0]
"Our work also relates to beam-search parsers (Zhang and Clark, 2008, inter alia).",6 Related Work,[0],[0]
"We have introduced a novel decoding algorithm, called heuristic backtracking, and presented evidence that it performs at the same level as beam search for decoding, while being significantly more efficient.",7 Conclusions,[0],[0]
"We have demonstrated this for both English and Chinese, using a parser with strong results with a greedy decoder.",7 Conclusions,[0],[0]
"We expect that heuristic backtracking could be applied to any other transition-based parser with similar benefits.
",7 Conclusions,[0],[0]
"We plan on experimenting with various heuristics and cutoff models, such as adapting the attentionbased models of Bahdanau et al. (2014) to act as a guide for both the heuristic search and cutoff.",7 Conclusions,[0],[0]
Miguel Ballesteros was supported by the European Commission under the contract numbers FP7ICT-610411 (project MULTISENSOR) and H2020RIA-645012 (project KRISTINA).,Acknowledgments,[0],[0]
We introduce a novel approach to the decoding problem in transition-based parsing: heuristic backtracking.,abstractText,[0],[0]
"This algorithm uses a series of partial parses on the sentence to locate the best candidate parse, using confidence estimates of transition decisions as a heuristic to guide the starting points of the search.",abstractText,[0],[0]
"This allows us to achieve a parse accuracy comparable to beam search, despite using fewer transitions.",abstractText,[0],[0]
"When used to augment a Stack-LSTM transition-based parser, the parser shows an unlabeled attachment score of up to 93.30% for English and 87.61% for Chinese.",abstractText,[0],[0]
Transition-Based Dependency Parsing with Heuristic Backtracking,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 232–242 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1022
Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents’ messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.1",text,[0],[0]
Several recent papers have described approaches for learning deep communicating policies (DCPs): decentralized representations of behavior that enable multiple agents to communicate via a differentiable channel that can be formulated as a recurrent neural network.,1 Introduction,[0],[0]
"DCPs have been shown to solve a variety of coordination problems, including reference games (Lazaridou et al., 2016b), logic puzzles (Foerster et al., 2016), and simple control (Sukhbaatar et al., 2016).",1 Introduction,[0],[0]
"Appealingly, the agents’ communication protocol can be learned via direct
1 We have released code and data at http://github.",1 Introduction,[0],[0]
"com/jacobandreas/neuralese.
",1 Introduction,[0],[0]
"backpropagation through the communication channel, avoiding many of the challenging inference problems associated with learning in classical decentralized decision processes (Roth et al., 2005).
",1 Introduction,[0],[0]
But analysis of the strategies induced by DCPs has remained a challenge.,1 Introduction,[0],[0]
"As an example, Figure 1 depicts a driving game in which two cars, which are unable to see each other, must both cross an intersection without colliding.",1 Introduction,[0],[0]
"In order to ensure success, it is clear that the cars must communicate with each other.",1 Introduction,[0],[0]
"But a number of successful communication strategies are possible—for example, they might report their exact (x, y) coordinates at every timestep, or they might simply announce whenever they are entering and leaving the intersection.",1 Introduction,[0],[0]
"If these messages were communicated in natural language, it would be straightforward to determine which strategy was being employed.",1 Introduction,[0],[0]
"However, DCP agents instead communicate with an automatically induced protocol of unstructured, real-valued recurrent state vectors—an artificial language we might call “neuralese,” which superficially bears little resemblance to natural language, and thus frustrates attempts at direct interpretation.
232
We propose to understand neuralese messages by translating them.",1 Introduction,[0],[0]
"In this work, we present a simple technique for inducing a dictionary that maps between neuralese message vectors and short natural language strings, given only examples of DCP agents interacting with other agents, and humans interacting with other humans.",1 Introduction,[0],[0]
"Natural language already provides a rich set of tools for describing beliefs, observations, and plans—our thesis is that these tools provide a useful complement to the visualization and ablation techniques used in previous work on understanding complex models (Strobelt et al., 2016; Ribeiro et al., 2016).
",1 Introduction,[0],[0]
"While structurally quite similar to the task of machine translation between pairs of human languages, interpretation of neuralese poses a number of novel challenges.",1 Introduction,[0],[0]
"First, there is no natural source of parallel data: there are no bilingual “speakers” of both neuralese and natural language.",1 Introduction,[0],[0]
"Second, there may not be a direct correspondence between the strategy employed by humans and DCP agents: even if it were constrained to communicate using natural language, an automated agent might choose to produce a different message from humans in a given state.",1 Introduction,[0],[0]
We tackle both of these challenges by appealing to the grounding of messages in gameplay.,1 Introduction,[0],[0]
"Our approach is based on one of the core insights in natural language semantics: messages (whether in neuralese or natural language) have similar meanings when they induce similar beliefs about the state of the world.
",1 Introduction,[0],[0]
"Based on this intuition, we introduce a translation criterion that matches neuralese messages with natural language strings by minimizing statistical distance in a common representation space of distributions over speaker states.",1 Introduction,[0],[0]
"We explore several related questions:
•",1 Introduction,[0],[0]
"What makes a good translation, and under what conditions is translation possible at all?",1 Introduction,[0],[0]
"(Section 4)
",1 Introduction,[0],[0]
• How can we build a model to translate between neuralese and natural language?,1 Introduction,[0],[0]
"(Section 5)
•",1 Introduction,[0],[0]
What kinds of theoretical guarantees can we provide about the behavior of agents communicating via this translation model?,1 Introduction,[0],[0]
"(Section 6)
",1 Introduction,[0],[0]
"Our translation model and analysis are general, and in fact apply equally to human–computer and
human–human translation problems grounded in gameplay.",1 Introduction,[0],[0]
"In this paper, we focus our experiments specifically on the problem of interpreting communication in deep policies, and apply our approach to the driving game in Figure 1 and two reference games of the kind shown in Figure 2.",1 Introduction,[0],[0]
We find that this approach outperforms a more conventional machine translation criterion both when attempting to interoperate with neuralese speakers and when predicting their state.,1 Introduction,[0],[0]
A variety of approaches for learning deep policies with communication were proposed essentially simultaneously in the past year.,2 Related work,[0],[0]
"We have broadly labeled these as “deep communicating policies”; concrete examples include Lazaridou et al. (2016b), Foerster et al. (2016), and Sukhbaatar et al. (2016).",2 Related work,[0],[0]
"The policy representation we employ in this paper is similar to the latter two of these, although the general framework is agnostic to low-level modeling details and could be straightforwardly applied to other architectures.",2 Related work,[0],[0]
"Analysis of communication strategies in all these papers has been largely adhoc, obtained by clustering states from which similar messages are emitted and attempting to manually assign semantics to these clusters.",2 Related work,[0],[0]
"The present work aims at developing tools for performing this analysis automatically.
",2 Related work,[0],[0]
"Most closely related to our approach is that of Lazaridou et al. (2016a), who also develop a model for assigning natural language interpretations to learned messages; however, this approach relies on supervised cluster labels and is targeted specifically towards referring expression games.",2 Related work,[0],[0]
"Here we attempt to develop an approach that can handle general multiagent interactions without assuming a prior discrete structure in space of observations.
",2 Related work,[0],[0]
"The literature on learning decentralized multiagent policies in general is considerably larger (Bernstein et al., 2002; Dibangoye et al., 2016).",2 Related work,[0],[0]
"This includes work focused on communication in multiagent settings (Roth et al., 2005) and even communication using natural language messages (Vogel et al., 2013b).",2 Related work,[0],[0]
"All of these approaches employ structured communication schemes with manually engineered messaging protocols; these are, in some sense, automatically interpretable, but at the cost of introducing considerable complexity into both training and inference.
",2 Related work,[0],[0]
"Our evaluation in this paper investigates communication strategies that arise in a number of different games, including reference games and an extended-horizon driving game.",2 Related work,[0],[0]
"Communication strategies for reference games were previously explored by Vogel et al. (2013a), Andreas and Klein (2016) and Kazemzadeh et al. (2014), and reference games specifically featuring end-to-end communication protocols by Yu et al. (2016).",2 Related work,[0],[0]
"On the control side, a long line of work considers nonverbal communication strategies in multiagent policies (Dragan and Srinivasa, 2013).
",2 Related work,[0],[0]
Another group of related approaches focuses on the development of more general machinery for interpreting deep models in which messages have no explicit semantics.,2 Related work,[0],[0]
"This includes both visualization techniques (Zeiler and Fergus, 2014; Strobelt et al., 2016), and approaches focused on generating explanations in the form of natural language (Hendricks et al., 2016; Vedantam et al., 2017).",2 Related work,[0],[0]
Games Consider a cooperative game with two players a and b of the form given in Figure 3.,3 Problem formulation,[0],[0]
"At every step t of this game, player a makes an observation x(t)a and receives a message z (t−1) b from b.",3 Problem formulation,[0],[0]
It then takes an action u(t)a and sends a message z (t) a to b.,3 Problem formulation,[0],[0]
(The process is symmetric for b.),3 Problem formulation,[0],[0]
"The distributions p(ua|xa, zb) and p(za|xa) together define a policy π which we assume is shared by both players, i.e. p(ua|xa, zb) = p(ub|xb, za) and p(za|xa) = p(zb|xb).",3 Problem formulation,[0],[0]
"As in a standard Markov decision process, the actions (u(t)a , u (t) b ) alter the world state, generating new observations for both players and a reward shared by both.
",3 Problem formulation,[0],[0]
"The distributions p(z|x) and p(u|x, z) may also be viewed as defining a language: they specify how a speaker will generate messages based on world states, and how a listener will respond to these mes-
sages.",3 Problem formulation,[0],[0]
Our goal in this work is to learn to translate between pairs of languages generated by different policies.,3 Problem formulation,[0],[0]
"Specifically, we assume that we have access to two policies for the same game: a “robot policy” πr and a “human policy” πh.",3 Problem formulation,[0],[0]
"We would like to use the representation of πh, the behavior of which is transparent to human users, in order to understand the behavior of πr (which is in general an uninterpretable learned model); we will do this by inducing bilingual dictionaries that map message vectors zr of πr to natural language strings zh of πh and vice-versa.
",3 Problem formulation,[0],[0]
Learned agents πr,3 Problem formulation,[0],[0]
Our goal is to present tools for interpretation of learned messages that are agnostic to the details of the underlying algorithm for acquiring them.,3 Problem formulation,[0],[0]
We use a generic DCP model as a basis for the techniques developed in this paper.,3 Problem formulation,[0],[0]
"Here each agent policy is represented as a deep recurrent Q network (Hausknecht and Stone, 2015).",3 Problem formulation,[0],[0]
This network is built from communicating cells of the kind depicted in Figure 4.,3 Problem formulation,[0],[0]
"At every timestep, this agent receives three pieces of information: an
observation of the current state of the world, the agent’s memory vector from the previous timestep, and a message from the other player.",3 Problem formulation,[0],[0]
"It then produces three outputs: a predicted Q value for every possible action, a new memory vector for the next timestep, and a message to send to the other agent.
",3 Problem formulation,[0],[0]
Sukhbaatar et al. (2016) observe that models of this form may be viewed as specifying a single RNN in which weight matrices have a particular block structure.,3 Problem formulation,[0],[0]
"Such models may thus be trained using the standard recurrent Q-learning objective, with communication protocol learned end-to-end.
",3 Problem formulation,[0],[0]
Human agents πh The translation model we develop requires a representation of the distribution over messages p(za|xa) employed by human speakers (without assuming that humans and agents produce equivalent messages in equivalent contexts).,3 Problem formulation,[0],[0]
"We model the human message generation process as categorical, and fit a simple multilayer perceptron model to map from observations to words and phrases used during human gameplay.",3 Problem formulation,[0],[0]
What does it mean for a message zh to be a “translation” of a message zr?,4 What’s in a translation?,[0],[0]
"In standard machine translation problems, the answer is that zh is likely to co-occur in parallel data with zr; that is, p(zh|zr) is large.",4 What’s in a translation?,[0],[0]
"Here we have no parallel data: even if we could observe natural language and neuralese messages produced by agents in the same state, we would have no guarantee that these messages actually served the same function.",4 What’s in a translation?,[0],[0]
Our answer must instead appeal to the fact that both natural language and neuralese messages are grounded in a common environment.,4 What’s in a translation?,[0],[0]
"For a given neuralese message zr, we will first compute a grounded representation of that message’s meaning; to translate, we find a natural-language message whose meaning is most similar.",4 What’s in a translation?,[0],[0]
The key question is then what form this grounded meaning representation should take.,4 What’s in a translation?,[0],[0]
"The existing literature suggests two broad approaches:
Semantic representation The meaning of a message za is given by its denotations: that is, by the set of world states of which za may be felicitously predicated, given the existing context available to a listener.",4 What’s in a translation?,[0],[0]
"In probabilistic terms, this says that the meaning of a message za is represented by the distribution p(xa|za, xb) it induces over speaker states.",4 What’s in a translation?,[0],[0]
"Examples of this approach include Guerin and Pitt (2001) and Pasupat and Liang (2016).
",4 What’s in a translation?,[0],[0]
Pragmatic representation,4 What’s in a translation?,[0],[0]
The meaning of a message za is given by the behavior it induces in a listener.,4 What’s in a translation?,[0],[0]
"In probabilistic terms, this says that the meaning of a message za is represented by the distribution p(ub|za, xb) it induces over actions given the listener’s observation xb.",4 What’s in a translation?,[0],[0]
"Examples of this approach include Vogel et al. (2013a) and Gauthier and Mordatch (2016).
",4 What’s in a translation?,[0],[0]
These two approaches can give rise to rather different behaviors.,4 What’s in a translation?,[0],[0]
"Consider the following example:
square hexagon circle
few many many
The top language (in blue) has a unique name for every kind of shape, while the bottom language (in red) only distinguishes between shapes with few sides and shapes with many sides.",4 What’s in a translation?,[0],[0]
"Now imagine a simple reference game with the following form: player a is covertly assigned one of these three shapes as a reference target, and communicates that reference to b; b must then pull a lever labeled large or small depending on the size of the target shape.",4 What’s in a translation?,[0],[0]
"Blue language speakers can achieve perfect success at this game, while red language speakers can succeed at best two out of three times.
",4 What’s in a translation?,[0],[0]
How should we translate the blue word hexagon into the red language?,4 What’s in a translation?,[0],[0]
"The semantic approach suggests that we should translate hexagon as many: while many does not uniquely identify the hexagon, it produces a distribution over shapes that is closest to the truth.",4 What’s in a translation?,[0],[0]
"The pragmatic approach instead suggests that we should translate hexagon as few, as this is the only message that guarantees that the listener will pull the correct lever large.",4 What’s in a translation?,[0],[0]
"So in order to produce a correct listener action, the translator might have to “lie” and produce a maximally inaccurate listener belief.
",4 What’s in a translation?,[0],[0]
"If we were exclusively concerned with building a translation layer that allowed humans and DCP agents to interoperate as effectively as possible, it would be natural to adopt a pragmatic representation strategy.",4 What’s in a translation?,[0],[0]
"But our goals here are broader: we also want to facilitate understanding, and specifically to help users of learned systems form true beliefs about the systems’ computational processes and representational abstractions.",4 What’s in a translation?,[0],[0]
"The example above demonstrates that “pragmatically” optimizing directly for task performance can sometimes lead to translations that produce inaccurate beliefs.
",4 What’s in a translation?,[0],[0]
We instead build our approach around semantic representations of meaning.,4 What’s in a translation?,[0],[0]
"By preserving semantics, we allow listeners to reason accurately about the content and interpretation of messages.",4 What’s in a translation?,[0],[0]
"We might worry that by adopting a semantics-first view, we have given up all guarantees of effective interoperation between humans and agents using a translation layer.",4 What’s in a translation?,[0],[0]
"Fortunately, this is not so: as we will see in Section 6, it is possible to show that players communicating via a semantic translator perform only boundedly worse (and sometimes better!)",4 What’s in a translation?,[0],[0]
than pairs of players with a common language.,4 What’s in a translation?,[0],[0]
"In this section, we build on the intuition that messages should be translated via their semantics to define a concrete translation model—a procedure for constructing a natural language ↔ neuralese dictionary given agent and human interactions.
",5 Translation models,[0],[0]
"We understand the meaning of a message za to be represented by the distribution p(xa|za, xb) it induces over speaker states given listener context.",5 Translation models,[0],[0]
"We can formalize this by defining the belief distribution β for a message z and context xb as:
β(za, xb) = p(xa|za, xb) = p(za|xa)p(xb|xa)∑ x′a p(za|x′a)p(xb|x′a)
",5 Translation models,[0],[0]
"Here we have modeled the listener as performing a single step of Bayesian inference, using the listener state and the message generation model (by assumption shared between players) to compute the posterior over speaker states.",5 Translation models,[0],[0]
"While in general neither humans nor DCP agents compute explicit representations of this posterior, past work has found that both humans and suitably-trained neural networks can be modeled as Bayesian reasoners (Frank et al., 2009; Paige and Wood, 2016).
",5 Translation models,[0],[0]
"This provides a context-specific representation of belief, but for messages z and z′ to have the same semantics, they must induce the same belief over all contexts in which they occur.",5 Translation models,[0],[0]
"In our probabilistic formulation, this introduces an outer expectation over contexts, providing a final measure q of the quality of a translation from z to z′:
q(z, z′) =",5 Translation models,[0],[0]
E,5 Translation models,[0],[0]
"[ DKL(β(z,Xb) || β(z′, Xb))",5 Translation models,[0],[0]
"| z, z′ ]
= ∑
xa,xb
p(xa, xb|z, z′)DKL(β(z, xb) || β(z′, xb))
∝",5 Translation models,[0],[0]
"∑
xa,xb p(xa, xb) · p(z|xa) · p(z′|xa) ·",5 Translation models,[0],[0]
"DKL(β(z, xb) || β(z′, xb));",5 Translation models,[0],[0]
"(1)
Algorithm 1 Translating messages
given: a phrase inventory L function TRANSLATE(z)
return argminz′∈L q̂(z, z′)
function q̂(z, z′) // sample contexts and distractors xai, xbi ∼ p(Xa, Xb) for i = 1..n",5 Translation models,[0],[0]
x′ai ∼ p(Xa|xbi) //,5 Translation models,[0],[0]
compute context weights w̃i ← p(z|xai) ·,5 Translation models,[0],[0]
p(z′|xai),5 Translation models,[0],[0]
"wi ← w̃i/ ∑ j w̃j
// compute divergences ki ← ∑ x∈{xa,x′a} p(z|x) log p(z|x) p(z′|x)
return ∑
iwiki
recalling that in this setting
DKL(β || β′)",5 Translation models,[0],[0]
"= ∑
xa
p(xa|z, xb) log p(xa|z, xb) p(xa|z′, xb)
∝",5 Translation models,[0],[0]
"∑
xa
p(xa|xb)p(z|xa) log p(z|xa) p(z′|xa)
(2)
which is zero when the messages z and z′ give rise to identical belief distributions and increases as they grow more dissimilar.",5 Translation models,[0],[0]
"To translate, we would like to compute tr(zr) = argminzh q(zr, zh) and tr(zh) = argminzr q(zh, zr).",5 Translation models,[0],[0]
"Intuitively, Equation 1 says that we will measure the quality of a proposed translation z 7→ z′ by asking the following question: in contexts where z is likely to be used, how frequently does z′ induce the same belief about speaker states as z?
While this translation criterion directly encodes the semantic notion of meaning described in Section 4, it is doubly intractable: the KL divergence and outer expectation involve a sum over all observations xa and xb respectively; these sums are not in general possible to compute efficiently.",5 Translation models,[0],[0]
"To avoid this, we approximate Equation 1 by sampling.",5 Translation models,[0],[0]
"We draw a collection of samples (xa, xb) from the prior over world states, and then generate for each sample a sequence of distractors (x′a, xb) from p(x ′ a|xb)",5 Translation models,[0],[0]
(we assume access to both of these distributions from the problem representation).,5 Translation models,[0],[0]
"The KL term in Equation 1 is computed over each true sample and its distractors, which are then normalized and averaged to compute the final score.
",5 Translation models,[0],[0]
"Sampling accounts for the outer p(xa, xb) in Equation 1 and the inner p(xa|xb) in Equation 2.
",5 Translation models,[0],[0]
The only quantities remaining are of the form p(z|xa).,5 Translation models,[0],[0]
"In the case of neuralese, this distribution already is part of the definition of the agent policy πr and can be reused directly.",5 Translation models,[0],[0]
"For natural language, we use transcripts of human interactions to fit a model that maps from world states to a distribution over frequent utterances as discussed in Section 3.",5 Translation models,[0],[0]
"Details of these model implementations are provided in Appendix B, and the full translation procedure is given in Algorithm 1.",5 Translation models,[0],[0]
The translation criterion in the previous section makes no reference to listener actions at all.,6 Belief and behavior,[0],[0]
The shapes example in Section 4 shows that some model performance might be lost under translation.,6 Belief and behavior,[0],[0]
It is thus reasonable to ask whether this translation model of Section 5 can make any guarantees about the effect of translation on behavior.,6 Belief and behavior,[0],[0]
"In this section we explore the relationship between beliefpreserving translations and the behaviors they produce, by examining the effect of belief accuracy and strategy mismatch on the reward obtained by cooperating agents.
",6 Belief and behavior,[0],[0]
"To facilitate this analysis, we consider a simplified family of communication games with the structure depicted in Figure 5.",6 Belief and behavior,[0],[0]
"These games can be viewed as a subset of the family depicted in Figure 3; and consist of two steps: a listener makes an observation xa and sends a single message z to a speaker, which makes its own observation xb, takes a single action u, and receives a reward.",6 Belief and behavior,[0],[0]
"We emphasize that the results in this section concern the theoretical properties of idealized games, and are presented to provide intuition about high-level properties of our approach.",6 Belief and behavior,[0],[0]
"Section 8 investigates empirical behavior of this approach on real-world tasks where these ideal conditions do not hold.
",6 Belief and behavior,[0],[0]
"Our first result is that translations that minimize semantic dissimilarity q cause the listener to take near-optimal actions:2
2Proof is provided in Appendix A.
Proposition 1.",6 Belief and behavior,[0],[0]
Semantic translations reward rational listeners.,6 Belief and behavior,[0],[0]
"Define a rational listener as one that chooses the best action in expectation over the speaker’s state:
U(z, xb) = argmax u
∑
xa
p(xa|xb, z)r(xa, xb, u)
for a reward function r ∈",6 Belief and behavior,[0],[0]
"[0, 1] that depends only on the two observations and the action.3 Now let a be a speaker of a language r, b be a listener of the same language r, and b′ be a listener of a different language h.",6 Belief and behavior,[0],[0]
Suppose that we wish for a and b′ to interact via the translator tr :,6 Belief and behavior,[0],[0]
"zr 7→ zh (so that a produces a message zr, and b′ takes an action U(zh = tr(zr), xb′)).",6 Belief and behavior,[0],[0]
"If tr respects the semantics of zr, then the bilingual pair a and b′ achieves only boundedly worse reward than the monolingual pair a and",6 Belief and behavior,[0],[0]
"b. Specifically, if q(zr, zh) ≤ D, then
Er(Xa, Xb, U(tr(Z))
",6 Belief and behavior,[0],[0]
"≥ Er(Xa, Xb, U(Z))− √ 2D (3)
",6 Belief and behavior,[0],[0]
"So as discussed in Section 4, even by committing to a semantic approach to meaning representation, we have still succeeded in (approximately) capturing the nice properties of the pragmatic approach.
",6 Belief and behavior,[0],[0]
Section 4 examined the consequences of a mismatch between the set of primitives available in two languages.,6 Belief and behavior,[0],[0]
In general we would like some measure of our approach’s robustness to the lack of an exact correspondence between two languages.,6 Belief and behavior,[0],[0]
"In the case of humans in particular we expect that a variety of different strategies will be employed, many of which will not correspond to the behavior of the learned agent.",6 Belief and behavior,[0],[0]
It is natural to want some assurance that we can identify the DCP’s strategy as long as some human strategy mirrors it.,6 Belief and behavior,[0],[0]
"Our second observation is that it is possible to exactly recover a translation of a DCP strategy from a mixture of humans playing different strategies:
Proposition 2.",6 Belief and behavior,[0],[0]
Semantic translations find hidden correspondences.,6 Belief and behavior,[0],[0]
"Consider a fixed robot policy πr and a set of human policies {πh1 , πh2 , . . . }",6 Belief and behavior,[0],[0]
"(recalling from Section 3 that each π is defined by distributions
3This notion of rationality is a fairly weak one: it permits many suboptimal communication strategies, and requires only that the listener do as well as possible given a fixed speaker— a first-order optimality criterion likely to be satisfied by any richly-parameterized model trained via gradient descent.
",6 Belief and behavior,[0],[0]
"p(z |xa) and p(u |z , xb)).",6 Belief and behavior,[0],[0]
"Suppose further that the messages employed by these human strategies are disjoint; that is, if phi(z |xa)",6 Belief and behavior,[0],[0]
"> 0, then phj (z |xa) = 0 for all j 6=",6 Belief and behavior,[0],[0]
i.,6 Belief and behavior,[0],[0]
"Now suppose that all q(zr , zh) = 0 for all messages in the support of some phi(z |xa) and > 0 for all j 6=",6 Belief and behavior,[0],[0]
i.,6 Belief and behavior,[0],[0]
"Then every message zr is translated into a message produced by πhi , and messages from other strategies are ignored.
",6 Belief and behavior,[0],[0]
"This observation follows immediately from the definition of q(zr, zh), but demonstrates one of the key distinctions between our approach and a conventional machine translation criterion.",6 Belief and behavior,[0],[0]
"Maximizing p(zh|zr) will produce the natural language message most often produced in contexts where zr is observed, regardless of whether that message is useful or informative.",6 Belief and behavior,[0],[0]
"By contrast, minimizing q(zh, zr) will find the zh that corresponds most closely to zr even when zh is rarely used.
",6 Belief and behavior,[0],[0]
"The disjointness condition, while seemingly quite strong, in fact arises naturally in many circumstances—for example, players in the driving game reporting their spatial locations in absolute vs. relative coordinates, or speakers in a color reference game (Figure 6) discriminating based on lightness vs. hue.",6 Belief and behavior,[0],[0]
"It is also possible to relax the above condition to require that strategies be only locally disjoint (i.e. with the disjointness condition holding for each fixed xa), in which case overlapping human strategies are allowed, and the recovered robot strategy is a context-weighted mixture of these.",6 Belief and behavior,[0],[0]
"In the remainder of the paper, we evaluate the empirical behavior of our approach to translation.",7.1 Tasks,[0],[0]
Our evaluation considers two kinds of tasks: reference games and navigation games.,7.1 Tasks,[0],[0]
"In a reference game (e.g. Figure 6a), both players observe a pair of candidate referents.",7.1 Tasks,[0],[0]
"A speaker is assigned a target referent; it must communicate this target to a listener, who then performs a choice action corresponding to its belief about the true target.",7.1 Tasks,[0],[0]
"In this paper we consider two variants on the reference game: a simple color-naming task, and a more complex task involving natural images of birds.",7.1 Tasks,[0],[0]
"For examples of human communication strategies for these tasks, we obtain the XKCD color dataset (McMahan and Stone, 2015; Monroe et al., 2016) and the Caltech Birds dataset (Welinder et al., 2010) with accom-
panying natural language descriptions (Reed et al., 2016).",7.1 Tasks,[0],[0]
"We use standard train / validation / test splits for both of these datasets.
",7.1 Tasks,[0],[0]
The final task we consider is the driving task (Figure 6c) first discussed in the introduction.,7.1 Tasks,[0],[0]
"In this task, two cars, invisible to each other, must each navigate between randomly assigned start and goal positions without colliding.",7.1 Tasks,[0],[0]
"This task takes a number of steps to complete, and potentially involves a much broader range of communication strategies.",7.1 Tasks,[0],[0]
"To obtain human annotations for this task, we recorded both actions and messages generated by pairs of human Amazon Mechanical Turk workers playing the driving game with each other.",7.1 Tasks,[0],[0]
"We collected close to 400 games, with a total of more than 2000 messages exchanged, from which we held out 100 game traces as a test set.",7.1 Tasks,[0],[0]
"A mechanism for understanding the behavior of a learned model should allow a human user both to correctly infer its beliefs and to successfully interoperate with it; we accordingly report results of both “belief” and “behavior” evaluations.
",7.2 Metrics,[0],[0]
"To support easy reproduction and comparison (and in keeping with standard practice in machine
translation), we focus on developing automatic measures of system performance.",7.2 Metrics,[0],[0]
"We use the available training data to develop simulated models of human decisions; by first showing that these models track well with human judgments, we can be confident that their use in evaluations will correlate with human understanding.",7.2 Metrics,[0],[0]
"We employ the following two metrics:
Belief evaluation This evaluation focuses on the denotational perspective in semantics that motivated the initial development of our model.",7.2 Metrics,[0],[0]
We have successfully understood the semantics of a message,7.2 Metrics,[0],[0]
"zr if, after translating zr 7→ zh, a human listener can form a correct belief about the state in which zr was produced.",7.2 Metrics,[0],[0]
"We construct a simple state-guessing game where the listener is presented with a translated message and two state observations, and must guess which state the speaker was in when the message was emitted.
",7.2 Metrics,[0],[0]
"When translating from natural language to neuralese, we use the learned agent model to directly guess the hidden state.",7.2 Metrics,[0],[0]
"For neuralese to natural language we must first construct a “model human listener” to map from strings back to state representations; we do this by using the training data to fit a simple regression model that scores (state, sentence) pairs using a bag-of-words sentence representation.",7.2 Metrics,[0],[0]
"We find that our “model human” matches the judgments of real humans 83% of the time on the colors task, 77% of the time on the birds task, and 77% of the time on the driving task.",7.2 Metrics,[0],[0]
"This gives us confidence that the model human gives a reasonably accurate proxy for human interpretation.
",7.2 Metrics,[0],[0]
Behavior evaluation This evaluation focuses on the cooperative aspects of interpretability: we measure the extent to which learned models are able to interoperate with each other by way of a translation layer.,7.2 Metrics,[0],[0]
"In the case of reference games, the goal of this semantic evaluation is identical to the goal of the game itself (to identify the hidden state of the speaker), so we perform this additional pragmatic evaluation only for the driving game.",7.2 Metrics,[0],[0]
We found that the most data-efficient and reliable way to make use of human game traces was to construct a “deaf” model human.,7.2 Metrics,[0],[0]
"The evaluation selects a full game trace from a human player, and replays both the human’s actions and messages exactly (disregarding any incoming messages); the evaluation measures the quality of the natural-language-toneuralese translator, and the extent to which the
learned agent model can accommodate a (real) human given translations of the human’s messages.
",7.2 Metrics,[0],[0]
"Baselines We compare our approach to two baselines: a random baseline that chooses a translation of each input uniformly from messages observed during training, and a direct baseline that directly maximizes p(z′|z) (by analogy to a conventional machine translation system).",7.2 Metrics,[0],[0]
This is accomplished by sampling from a DCP speaker in training states labeled with natural language strings.,7.2 Metrics,[0],[0]
"In all below, “R” indicates a DCP agent, “H” indicates a real human, and “H*” indicates a model human player.
",8 Results,[0],[0]
Reference games Results for the two reference games are shown in Table 1.,8 Results,[0],[0]
"The end-to-end trained model achieves nearly perfect accuracy in both
cases, while a model trained to communicate in natural language achieves somewhat lower performance.",8 Results,[0],[0]
"Regardless of whether the speaker is a DCP and the listener a model human or vice-versa, translation based on the belief-matching criterion in Section 5 achieves the best performance; indeed, when translating neuralese color names to natural language, the listener is able to achieve a slightly higher score than it is natively.",8 Results,[0],[0]
"This suggests that the automated agent has discovered a more effective strategy than the one demonstrated by humans in the dataset, and that the effectiveness of this strategy is preserved by translation.",8 Results,[0],[0]
"Example translations from the reference games are depicted in Figure 2 and Figure 7.
",8 Results,[0],[0]
"Driving game Behavior evaluation of the driving game is shown in Table 3, and belief evaluation is shown in Table 2.",8 Results,[0],[0]
"Translation of messages in the driving game is considerably more challenging than in the reference games, and scores are uniformly lower; however, a clear benefit from the beliefmatching model is still visible.",8 Results,[0],[0]
"Belief matching leads to higher scores on the belief evaluation in both directions, and allows agents to obtain a higher reward on average (though task completion rates remain roughly the same across all agents).",8 Results,[0],[0]
Some example translations of driving game messages are shown in Figure 8.,8 Results,[0],[0]
We have investigated the problem of interpreting message vectors from deep networks by translating them.,9 Conclusion,[0],[0]
"After introducing a translation criterion based on matching listener beliefs about speaker states, we presented both theoretical and empirical evidence that this criterion outperforms a conventional machine translation approach at recovering the content of message vectors and facilitating collaboration between humans and learned agents.
",9 Conclusion,[0],[0]
"While our evaluation has focused on understanding the behavior of deep communicating policies, the framework proposed in this paper could be much more generally applied.",9 Conclusion,[0],[0]
"Any encoder– decoder model (Sutskever et al., 2014) can be thought of as a kind of communication game played between the encoder and the decoder, so we can analogously imagine computing and translating “beliefs” induced by the encoding to explain what features of the input are being transmitted.",9 Conclusion,[0],[0]
"The current work has focused on learning a purely categorical model of the translation process, supported by an unstructured inventory of translation candidates, and future work could explore the compositional structure of messages, and attempt to synthesize novel natural language or neuralese messages from scratch.",9 Conclusion,[0],[0]
"More broadly, the work here shows that the denotational perspective from formal semantics provides a framework for precisely framing the demands of interpretable machine learning (Wilson et al., 2016), and particularly for ensuring that human users without prior exposure to a learned model are able to interoperate with it, predict its behavior, and diagnose its errors.",9 Conclusion,[0],[0]
JA is supported by a Facebook Graduate Fellowship and a Berkeley AI / Huawei Fellowship.,Acknowledgments,[0],[0]
We are grateful to Lisa Anne Hendricks for assistance with the Caltech Birds dataset.,Acknowledgments,[0],[0]
Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel.,abstractText,[0],[0]
"While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge.",abstractText,[0],[0]
Here we propose to interpret agents’ messages by translating them.,abstractText,[0],[0]
"Unlike in typical machine translation problems, we have no parallel data to learn from.",abstractText,[0],[0]
Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener.,abstractText,[0],[0]
We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.1,abstractText,[0],[0]
Translating Neuralese,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 56–65 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
56",text,[0],[0]
"In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017).",1 Introduction,[0],[0]
"Being an end-to-end architecture, an NMT system first encodes the input sentence into a sequence of real vectors, based on which the decoder generates the target sequence word by word with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015).",1 Introduction,[0],[0]
"During training, NMT systems are optimized to maximize the translation probability of a given language pair
∗Contribution during internship at MSRA.
with the Maximum Likelihood Estimation (MLE) method, which requires large bilingual data to fit the large parameter space.",1 Introduction,[0],[0]
"Without adequate data, which is common especially when it comes to a rare language, NMT usually falls short on low-resource language pairs (Zoph et al., 2016).
",1 Introduction,[0],[0]
"In order to deal with the data sparsity problem for NMT, exploiting monolingual data (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016) is the most common method.",1 Introduction,[0],[0]
"With monolingual data, the back-translation method (Sennrich et al., 2015) generates pseudo bilingual sentences with a targetto-source translation model to train the source-totarget one.",1 Introduction,[0],[0]
"By extending back-translation, sourceto-target and target-to-source translation models can be jointly trained and boost each other (Cheng et al., 2016; Zhang et al., 2018).",1 Introduction,[0],[0]
"Similar to joint training (Cheng et al., 2016; Zhang et al., 2018), dual learning (He et al., 2016) designs a reinforcement learning framework to better capitalize on monolingual data and jointly train two models.
",1 Introduction,[0],[0]
"Instead of leveraging monolingual data (X or Z) to enrich the low-resource bilingual pair (X,Z), in this paper, we are motivated to introduce another rich language Y , by which additionally acquired bilingual data (Y,Z) and (X,Y ) can be exploited to improve the translation performance of (X,Z).",1 Introduction,[0],[0]
"This requirement is easy to satisfy, especially when Z is a rare language but X is not.",1 Introduction,[0],[0]
"Under this scenario, (X,Y ) can be a rich-resource pair and provide much bilingual data, while (Y,Z) would also be a low-resource pair mostly because Z is rare.",1 Introduction,[0],[0]
"For example, in the dataset IWSLT2012, there are only 112.6K bilingual sentence pairs of English-Hebrew, since Hebrew is a rare language.",1 Introduction,[0],[0]
"If French is introduced as the third language, we can have another lowresource bilingual data of French-Hebrew (116.3K sentence pairs), and easily-acquired bilingual data
of the rich-resource pair English-French.
",1 Introduction,[0],[0]
"With the introduced rich language Y , in this paper, we propose a novel triangular architecture (TA-NMT) to exploit the additional bilingual data of (Y, Z) and (X,Y ), in order to get better translation performance on the low-resource pair (X,Z), as shown in Figure 1.",1 Introduction,[0],[0]
"In this architecture, (Y,Z) is used for training another translation model to score the translation model of (X,Z), while (X,Y ) is used to provide large bilingual data with favorable alignment information.
",1 Introduction,[0],[0]
"Under the motivation to exploit the richresource pair (X,Y ), instead of modeling X ⇒ Z directly, our method starts from modeling the translation task X ⇒ Y while taking Z as a latent variable.",1 Introduction,[0],[0]
"Then, we decompose X ⇒ Y into two phases for training two translation models of low-resource pairs ((X,Z) and (Y,Z)) respectively.",1 Introduction,[0],[0]
"The first translation model generates a sequence in the hidden space of Z from X , based on which the second one generates the translation in Y .",1 Introduction,[0],[0]
These two models can be optimized jointly with an Expectation Maximization (EM) framework with the goal of maximizing the translation probability p(y|x).,1 Introduction,[0],[0]
"In this framework, the two models can boost each other by generating pseudo bilingual data for model training with the weights scored from the other.",1 Introduction,[0],[0]
"By reversing the translation direction of X ⇒ Y , our method can be used to train another two translation models p(z|y) and p(x|z).",1 Introduction,[0],[0]
"Therefore, the four translation models (p(z|x), p(x|z), p(z|y) and p(y|z)) of the rare language Z can be optimized jointly with our proposed unified bidirectional EM algorithm.
",1 Introduction,[0],[0]
Experimental results on the MultiUN and IWSLT2012 datasets demonstrate that our method can achieve significant improvements for rare languages translation.,1 Introduction,[0],[0]
"By incorporating backtranslation (a method leveraging more monolingual data) into our method, TA-NMT can achieve even further improvements.
",1 Introduction,[0],[0]
"Our contributions are listed as follows:
• We propose a novel triangular training architecture (TA-NMT) to effectively tackle the data sparsity problem for rare languages in NMT with an EM framework.
",1 Introduction,[0],[0]
"• Our method can exploit two additional bilingual datasets at both the model and data levels by introducing another rich language.
",1 Introduction,[0],[0]
"• Our method is a unified bidirectional EM algorithm, in which four translation models on two low-resource pairs are trained jointly and boost each other.",1 Introduction,[0],[0]
"As shown in Figure 1, our method tries to leverage (X,Y ) (a rich-resource pair) and (Y, Z) to improve the translation performance of low-resource pair (X,Z), during which translation models of (X,Z) and (Y, Z) can be improved jointly.
",2 Method,[0],[0]
"Instead of directly modeling the translation probabilities of low-resource pairs, we model the rich-resource pair translation X ⇒ Y , with the language Z acting as a bridge to connect X and Y .",2 Method,[0],[0]
We decompose X ⇒ Y into two phases for training two translation models.,2 Method,[0],[0]
"The first model p(z|x) generates the latent translation in Z from the input sentence in X , based on which the second one p(y|z) generate the final translation in language Y .",2 Method,[0],[0]
"Following the standard EM procedure (Borman, 2004) and Jensen’s inequality, we derive the lower bound of p(y|x) over the whole training data D as follows:
L(Θ;D) =",2 Method,[0],[0]
"∑
(x,y)∈D
log p(y|x)
= ∑
(x,y)∈D
log ∑ z p(z|x)p(y|z)
= ∑
(x,y)∈D
log ∑ z Q(z) p(z|x)p(y|z) Q(z)
≥ ∑
(x,y)∈D
∑ z Q(z) log p(z|x)p(y|z) Q(z)
.",2 Method,[0],[0]
"= L(Q)
(1)
where Θ is the model parameters set of p(z|x) and p(y|z), and Q(z) is an arbitrary posterior distribution of z.",2 Method,[0],[0]
"We denote the lower-bound in the last
but one line as L(Q).",2 Method,[0],[0]
"Note that we use an approximation that p(y|x, z)",2 Method,[0],[0]
"≈ p(y|z) due to the semantic equivalence of parallel sentences x and y.
In the following subsections, we will first propose our EM method in subsection 2.1 based on the lower-bound derived above.",2 Method,[0],[0]
"Next, we will extend our method to two directions and give our unified bidirectional EM training in subsection 2.2.",2 Method,[0],[0]
"Then, in subsection 2.3, we will discuss more training details of our method and present our algorithm in the form of pseudo codes.",2 Method,[0],[0]
"To maximize L(Θ;D), the EM algorithm can be leveraged to maximize its lower bound L(Q).",2.1 EM Training,[0],[0]
"In the E-step, we calculate the expectation of the variable z using current estimate for the model, namely find the posterior distribution Q(z).",2.1 EM Training,[0],[0]
"In the M-step, with the expectation Q(z), we maximize the lower bound L(Q).",2.1 EM Training,[0],[0]
"Note that conditioned on the observed data and current model, the calculation of Q(z) is intractable, so we choose Q(z) = p(z|x) approximately.
",2.1 EM Training,[0],[0]
"M-step: In the M-step, we maximize the lower bound L(Q) w.r.t model parameters given Q(z).",2.1 EM Training,[0],[0]
"By substituting Q(z) = p(z|x) into L(Q), we can get the M-step as follows:
Θy|z = arg max Θy|z
L(Q)
= arg max Θy|z ∑ (x,y)∈D ∑ z p(z|x) log p(y|z)
",2.1 EM Training,[0],[0]
"= arg max Θy|z ∑ (x,y)∈D Ez∼p(z|x) log p(y|z)
(2) E-step: The approximate choice of Q(z) brings in a gap between L(Q) and L(Θ;D), which can be minimized in the E-step with Generalized EM method (McLachlan and Krishnan, 2007).",2.1 EM Training,[0],[0]
"According to Bishop (2006), we can write this gap explicitly as follows:
L(Θ;D)− L(Q) = ∑ z Q(z) log Q(z) p(z|y)
= KL(Q(z)||p(z|y))",2.1 EM Training,[0],[0]
"= KL(p(z|x)||p(z|y))
(3)
where KL(·) is the KullbackLeibler divergence, and the approximation that p(z|x, y)",2.1 EM Training,[0],[0]
"≈ p(z|y) is also used above.
",2.1 EM Training,[0],[0]
"In the E-step, we minimize the gap between L(Q) and L(Θ;D) as follows:
Θz|x = arg min Θz|x KL(p(z|x)||p(z|y))",2.1 EM Training,[0],[0]
"(4)
To sum it up, the E-step optimizes the model p(z|x) by minimizing the gap between L(Q) and L(Θ;D) to get a better lower bound L(Q).",2.1 EM Training,[0],[0]
This lower bound is then maximized in the M-step to optimize the model p(y|z).,2.1 EM Training,[0],[0]
"Given the new model p(y|z), the E-step tries to optimize p(z|x) again to find a new lower bound, with which the M-step is re-performed.",2.1 EM Training,[0],[0]
"This iteration process continues until the models converge, which is guaranteed by the convergence of the EM algorithm.",2.1 EM Training,[0],[0]
"The model p(z|y) is used as an approximation of p(z|x, y) in the E-step optimization (Equation 3).",2.2 Unified Bidirectional Training,[0],[0]
"Due to the low resource property of the language pair (Y, Z), p(z|y) cannot be well trained.",2.2 Unified Bidirectional Training,[0],[0]
"To solve this problem, we can jointly optimize p(x|z) and p(z|y) similarly by maximizing the reverse translation probability p(x|y).
",2.2 Unified Bidirectional Training,[0],[0]
"We now give our unified bidirectional generalized EM procedures as follows:
• Direction of X ⇒ Y E: Optimize Θz|x.
arg min Θz|x
KL(p(z|x)||p(z|y))",2.2 Unified Bidirectional Training,[0],[0]
"(5)
M: Optimize Θy|z .
arg max Θy|z ∑ (x,y)∈D Ez∼p(z|x) log p(y|z) (6)
",2.2 Unified Bidirectional Training,[0],[0]
• Direction of Y ⇒,2.2 Unified Bidirectional Training,[0],[0]
X,2.2 Unified Bidirectional Training,[0],[0]
"E: Optimize Θz|y.
arg min Θz|y
KL(p(z|y)||p(z|x))",2.2 Unified Bidirectional Training,[0],[0]
"(7)
M: Optimize Θx|z .
",2.2 Unified Bidirectional Training,[0],[0]
"arg max Θx|z ∑ (x,y)∈D Ez∼p(z|y) log p(x|z)",2.2 Unified Bidirectional Training,[0],[0]
"(8)
Based on the above derivation, the whole architecture of our method can be illustrated in Figure 2, where the dash arrows denote the direction of p(y|x), in which p(z|x) and p(y|z) are trained jointly with the help of p(z|y), while the solid ones denote the direction of p(x|y), in which p(z|y) and p(x|z) are trained jointly with the help of p(z|x).",2.2 Unified Bidirectional Training,[0],[0]
"A major difficulty in our unified bidirectional training is the exponential search space of the translation candidates, which could be addressed by either sampling (Shen et al., 2015; Cheng et al., 2016) or mode approximation (Kim and Rush, 2016).",2.3 Training Details,[0],[0]
"In our experiments, we leverage the sampling method and simply generate the top target sentence for approximation.
",2.3 Training Details,[0],[0]
"In order to perform gradient descend training, the parameter gradients for Equations 5 and 7 are formulated as follows:
∇Θz|xKL(p(z|x)||p(z|y))
",2.3 Training Details,[0],[0]
= Ez∼p(z|x) log p(z|x) p(z|y),2.3 Training Details,[0],[0]
"∇Θz|x log p(z|x)
∇Θz|yKL(p(z|y)||p(z|x))
",2.3 Training Details,[0],[0]
"= Ez∼p(z|y) log p(z|y) p(z|x) ∇Θz|y log p(z|y)
(9)
Similar to reinforcement learning, models p(z|x) and p(z|y) are trained using samples generated by the models themselves.",2.3 Training Details,[0],[0]
"According to our observation, some samples are noisy and detrimental to the training process.",2.3 Training Details,[0],[0]
"One way to tackle this is to filter out the bad ones using some additional metrics (BLEU, etc.).",2.3 Training Details,[0],[0]
"Nevertheless, in our settings, BLEU scores cannot be calculated during training due to the absence of the golden targets (z is generated based on x or y from the richresource pair (x, y)).",2.3 Training Details,[0],[0]
"Therefore we choose IBM model1 scores to weight the generated translation candidates, with the word translation probabilities calculated based on the given bilingual data (the low-resource pair (x, z) or (y, z)).",2.3 Training Details,[0],[0]
"Additionally, to stabilize the training process, the pseudo samples generated by model p(z|x) or p(z|y) are mixed with true bilingual samples in the same mini-batch with the ratio of 1-1.",2.3 Training Details,[0],[0]
"The whole training procedure is described in the following Algorithm 1, where the 5th and 9th steps are generating pseudo data.
",2.3 Training Details,[0],[0]
Algorithm 1 Training low-resource translation models with the triangular architecture,2.3 Training Details,[0],[0]
"Input: Rich-resource bilingual data (x, y); low-
resource bilingual data (x, z) and (y, z) Output:",2.3 Training Details,[0],[0]
"Parameters Θz|x, Θy|z , Θz|y and Θx|z
1: Pre-train p(z|x), p(z|y), p(x|z), p(y|z) 2: while not convergence do 3: Sample (x, y), (x∗, z∗), (y∗, z∗) ∈ D 4: .",2.3 Training Details,[0],[0]
"X ⇒ Y : Optimize Θz|x and Θy|z 5: Generate z′ from p(z′|x) and build the
training batches B1 = (x, z′)∪(x∗, z∗), B2 = (y, z′) ∪ (y∗, z∗) 6: E-step: update Θz|x with B1 (Equation 5) 7: M-step: update Θy|z with B2 (Equation 6) 8: .",2.3 Training Details,[0],[0]
"Y ⇒ X: Optimize Θz|y and Θx|z 9: Generate z′ from p(z′|y) and build the
training batches B3 = (y, z′)∪(y∗, z∗), B4 = (x, z′) ∪ (x∗, z∗) 10:",2.3 Training Details,[0],[0]
"E-step: update Θz|y with B3 (Equation 7) 11: M-step: update Θx|z with B4 (Equation 8) 12: end while 13: return Θz|x, Θy|z , Θz|y and Θx|z",2.3 Training Details,[0],[0]
"In order to verify our method, we conduct experiments on two multilingual datasets.",3.1 Datasets,[0],[0]
"The one is MultiUN (Eisele and Chen, 2010), which is a collection of translated documents from the United Nations, and the other is IWSLT2012 (Cettolo et al., 2012), which is a set of multilingual transcriptions of TED talks.",3.1 Datasets,[0],[0]
"As is mentioned in section 1, our method is compatible with methods exploiting monolingual data.",3.1 Datasets,[0],[0]
"So we also find some extra monolingual data of rare languages in both datasets and conduct experiments incorporating back-translation into our method.
",3.1 Datasets,[0],[0]
"MultiUN: English-French (EN-FR) bilingual data are used as the rich-resource pair (X,Y ).",3.1 Datasets,[0],[0]
"Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z. We randomly choose subsets of bilingual data of (X,Z) and (Y, Z) in the original dataset to simulate low-resource situations, and make sure there is no overlap in Z between chosen data of (X,Z) and (Y,Z).
",3.1 Datasets,[0],[0]
"IWSLT20121: English-French is used as the rich-resource pair (X,Y ), and two rare languages Z are Hebrew (HE) and Romanian (RO) in our
1https://wit3.fbk.eu/mt.php?release=2012-02-plain
choice.",3.1 Datasets,[0],[0]
"Note that in this dataset, low-resource pairs (X,Z) and (Y,Z) are severely overlapped in Z.",3.1 Datasets,[0],[0]
"In addition, English-French bilingual data from WMT2014 dataset are also used to enrich the rich-resource pair.",3.1 Datasets,[0],[0]
"We also use additional EnglishRomanian bilingual data from Europarlv7 dataset (Koehn, 2005).",3.1 Datasets,[0],[0]
"The monolingual data of Z (HE and RO) are taken from the web2.
",3.1 Datasets,[0],[0]
"In both datasets, all sentences are filtered within the length of 5 to 50 after tokenization.",3.1 Datasets,[0],[0]
"Both the validation and the test sets are 2,000 parallel sentences sampled from the bilingual data, with the left as training data.",3.1 Datasets,[0],[0]
The size of training data of all language pairs are shown in Table 1.,3.1 Datasets,[0],[0]
We compare our method with four baseline systems.,3.2 Baselines,[0],[0]
"The first baseline is the RNNSearch model (Bahdanau et al., 2014), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data.",3.2 Baselines,[0],[0]
"The trained translation models are also used as pre-trained models for our subsequent training processes.
",3.2 Baselines,[0],[0]
"The second baseline is PBSMT (Koehn et al., 2003), which is a phrase-based statistical machine translation system.",3.2 Baselines,[0],[0]
"PBSMT is known to perform well on low-resource language pairs, so we want to compare it with our proposed method.",3.2 Baselines,[0],[0]
And we use the public available implementation of Moses5 for training and test in our experiments.,3.2 Baselines,[0],[0]
"The third baseline is a teacher-student alike method (Chen et al., 2017).",3.2 Baselines,[0],[0]
"For the sake of brevity, we will denote it as T-S. The process is illustrated in Figure 3.",3.2 Baselines,[0],[0]
"We treat this method as a second baseline because it can also be regarded as a method exploiting (Y, Z) and (X,Y ) to improve
2https://github.com/ajinkyakulkarni14/TEDMultilingual-Parallel-Corpus
3together with WMT2014 4together with Europarlv7 5http://www.statmt.org/moses/
the translation of (X,Z) if we regard (X,Z) as the zero-resource pair and p(x|y) as the teacher model when training p(z|x) and p(x|z).
",3.2 Baselines,[0],[0]
"The fourth baseline is back-translation (Sennrich et al., 2015).",3.2 Baselines,[0],[0]
We will denote it as BackTrans.,3.2 Baselines,[0],[0]
"More concretely, to train the model p(z|x), we use extra monolingual Z described in Table 1 to do back-translation; to train the model p(x|z), we use monolingual X taken from (X,Y ).",3.2 Baselines,[0],[0]
Procedures for training p(z|y) and p(y|z) are similar.,3.2 Baselines,[0],[0]
This method use extra monolingual data of Z compared with our TA-NMT method.,3.2 Baselines,[0],[0]
But we can incorporate it into our method.,3.2 Baselines,[0],[0]
"Experimental results on both datasets are shown in Table 3 and 4 respectively, in which RNNSearch, PBSMT, T-S and BackTrans are four baselines.",3.3 Overall Results,[0],[0]
"TA-NMT is our proposed method, and TA-NMT(GI) is our method incorporating backtranslation as good initialization.",3.3 Overall Results,[0],[0]
"For the purpose of clarity and a fair comparison, we list the resources that different methods exploit in Table 2.
",3.3 Overall Results,[0],[0]
"From Table 3 on MultiUN, the performance of RNNSearch is relatively poor.",3.3 Overall Results,[0],[0]
"As is expected, PBSMT performs better than RNNSearch on lowresource pairs by the average of 1.78 BLEU.",3.3 Overall Results,[0],[0]
"The T-S method which can doubling the training data
for both (X,Z) and (Y, Z) by generating pseudo data from each other, leads up to 1.1 BLEU points improvement on average over RNNSearch.",3.3 Overall Results,[0],[0]
"Compared with T-S, our method gains a further improvement of about 0.9 BLEU on average, because our method can better leverage the rich-resource pair (X,Y ).",3.3 Overall Results,[0],[0]
"With extra large monolingual Z introduced, BackTrans can improve the performance of p(z|x) and p(z|y) significantly compared with all the methods without monolingual Z. However TA-NMT is comparable with or even better than BackTrans for p(x|z) and p(y|z) because both of the methods leverage resources from richresource pair (X,Y ), but BackTrans does not use the alignment information it provides.",3.3 Overall Results,[0],[0]
"Moreover, with back-translation as good initialization, further improvement is achieved by TA-NMT(GI) of about 0.7 BLEU on average over BackTrans.
",3.3 Overall Results,[0],[0]
"In Table 4, we can draw the similar conclusion.",3.3 Overall Results,[0],[0]
"However, different from MultiUN, in the EN-FR-HE group of IWSLT, (X,Z) and (Y,Z) are severely overlapped in Z. Therefore, T-S cannot improve the performance obviously (only about 0.2 BLEU) on RNNSearch because it fails to essentially double training data via the teacher model.",3.3 Overall Results,[0],[0]
"As for EN-FR-RO, with the additionally introduced EN-RO data from Europarlv7, which has no overlap in RO with FR-RO, T-S can improve the average performance more than the ENFR-HE group.",3.3 Overall Results,[0],[0]
TA-NMT outperforms T-S by 0.93 BLEU on average.,3.3 Overall Results,[0],[0]
"Note that even though Back-
Trans uses extra monolingual Z, the improvements are not so obvious as the former dataset, the reason for which we will delve into in the next subsection.",3.3 Overall Results,[0],[0]
"Again, with back-translation as good initialization, TA-NMT(GI) can get the best result.
",3.3 Overall Results,[0],[0]
Note that BLEU scores of TA-NMT are lower than BackTrans in the directions of X⇒Z and Y⇒Z.,3.3 Overall Results,[0],[0]
"The reason is that the resources used by these two methods are different, as shown in Table 2.",3.3 Overall Results,[0],[0]
"To do back translation in two directions (e.g., X⇒Z and Z⇒X), we need monolingual data from both sides (e.g., X and Z), however, in TA-NMT, the monolingual data of Z is not necessary.",3.3 Overall Results,[0],[0]
"Therefore, in the translation of X⇒Z or Y⇒Z, BackTrans uses additional monolingual data of Z while TA-NMT does not, that is why BackTrans outperforms TA-NMT in these directions.",3.3 Overall Results,[0],[0]
"Our method can leverage back translation as a good initialization, aka TA-NMT(GI) , and outperforms BackTrans on all translation directions.
",3.3 Overall Results,[0],[0]
"The average test BLEU scores of different methods in each data group (EN-FR-AR, EN-FRES, EN-FR-HE, and EN-FR-RO) are listed in the column Ave of the tables for clear comparison.",3.3 Overall Results,[0],[0]
"Comparing the results of BackTrans and TANMT(GI) on both datasets, we notice the improvements of both methods on IWSLT are not as significant as MultiUN.",3.4 The Effect of Extra Monolingual Data,[0],[0]
"We speculate the reason is the relatively less amount of monolingual Z we use in
the experiments on IWSLT as shown in Table 1.",3.4 The Effect of Extra Monolingual Data,[0],[0]
"So we conduct the following experiment to verify the conjecture by changing the scale of monolingual Arabic data in the MultiUN dataset, of which the data utilization rates are set to 0%, 10%, 30%, 60% and 100% respectively.",3.4 The Effect of Extra Monolingual Data,[0],[0]
Then we compare the performance of BackTrans and TA-NMT(GI) in the EN-FR-AR group.,3.4 The Effect of Extra Monolingual Data,[0],[0]
"As Figure 4 shows, the amount of monolingual Z actually has a big effect on the results, which can also verify our conjecture above upon the less significant improvement of BackTrans and TA-NMT(GI) on IWSLT.",3.4 The Effect of Extra Monolingual Data,[0],[0]
"In addition, even with poor ”good-initialization”, TANMT(GI) still get the best results.",3.4 The Effect of Extra Monolingual Data,[0],[0]
"To better illustrate the behavior of our method, we print the training curves in both the M-steps and Esteps of TA-NMT and TA-NMT(GI) in Figure 5 above.",3.5 EM Training Curves,[0],[0]
"The chosen models printed in this figure are EN2AR and AR2FR on MultiUN, and EN2RO and RO2FR on IWLST.
",3.5 EM Training Curves,[0],[0]
"From Figure 5, we can see that the two lowresource translation models are improved nearly simultaneously along with the training process, which verifies our point that two weak models could boost each other in our EM framework.",3.5 EM Training Curves,[0],[0]
"Notice that at the early stage, the performance of all models stagnates for several iterations, especially of TA-NMT.",3.5 EM Training Curves,[0],[0]
"The reason could be that the pseudo bilingual data and the true training data are heterogeneous, and it may take some time for the models to adapt to a new distribution which both models agree.",3.5 EM Training Curves,[0],[0]
"Compared with TA-NMT, TA-NMT(GI) are more stable, because the models may have
adapted to a mixed distribution of heterogeneous data in the preceding back-translation phase.",3.5 EM Training Curves,[0],[0]
"As shown in Equation 9, the E-step actually works as a reinforcement learning (RL) mechanism.",3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
Models p(z|x) and p(z|y) generate samples by themselves and receive rewards to update their parameters.,3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
"Note that the reward here is described by the log terms in Equation 9, which is derived from our EM algorithm rather than defined artificially.",3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
"In Table 5, we do a case study of the EN2ES translation sampled by p(z|x) as well as its time-step rewards during the E-step.
",3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
"In the first case, the best translation of ”political” is ”polı́ticos”.",3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
"When the model p(z|x) generates an inaccurate one ”polı́ticas”, it receives a negative reward (-0.01), with which the model parameters will be updated accordingly.",3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
"In the sec-
ond case, the output misses important words and is not fluent.",3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
"Rewards received by the model p(z|x) are zero for nearly all tokens in the output, leading to an invalid updating.",3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
"In the last case, the output sentence is identical to the human reference.",3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
"The rewards received are nearly all positive and meaningful, thus the RL rule will update the parameters to encourage this translation candidate.",3.6 Reinforcement Learning Mechanism in Our Method,[0],[0]
"NMT systems, relying heavily on the availability of large bilingual data, result in poor translation quality for low-resource pairs (Zoph et al., 2016).",4 Related Work,[0],[0]
This low-resource phenomenon has been observed in much preceding work.,4 Related Work,[0],[0]
"A very common approach is exploiting monolingual data of both source and target languages (Sennrich et al., 2015; Zhang and Zong, 2016; Cheng et al., 2016; Zhang et al., 2018; He et al., 2016).
",4 Related Work,[0],[0]
"As a kind of data augmentation technique, exploiting monolingual data can enrich the training data for low-resource pairs.",4 Related Work,[0],[0]
"Sennrich et al. (2015) propose back-translation, exploits the monolingual data of the target side, which is then used to generate pseudo bilingual data via an additional target-to-source translation model.",4 Related Work,[0],[0]
"Different from back-translation, Zhang and Zong (2016) propose two approaches to use source-side monolingual data, of which the first is employing a self-learning algorithm to generate pseudo data, while the second is using two NMT models to predict the translation and to reorder the source-side monolingual
sentences.",4 Related Work,[0],[0]
"As an extension to these two methods, Cheng et al. (2016) and Zhang et al. (2018) combine two translation directions and propose a training framework to jointly optimize the sourceto-target and target-to-source translation models.",4 Related Work,[0],[0]
"Similar to joint training, He et al. (2016) propose a dual learning framework with a reinforcement learning mechanism to better leverage monolingual data and make two translation models promote each other.",4 Related Work,[0],[0]
"All of these methods are concentrated on exploiting either the monolingual data of the source and target language or both of them.
",4 Related Work,[0],[0]
"Our method takes a different angle but is compatible with existing approaches, we propose a novel triangular architecture to leverage two additional language pairs by introducing a third rich language.",4 Related Work,[0],[0]
"By combining our method with existing approaches such as back-translation, we can make a further improvement.
",4 Related Work,[0],[0]
"Another approach for tackling the low-resource translation problem is multilingual neural machine translation (Firat et al., 2016), where different encoders and decoders for all languages with a shared attention mechanism are trained.",4 Related Work,[0],[0]
This method tends to exploit the network architecture to relate low-resource pairs.,4 Related Work,[0],[0]
"Our method is different from it, which is more like a training method rather than network modification.",4 Related Work,[0],[0]
"In this paper, we propose a triangular architecture (TA-NMT) to effectively tackle the problem
of low-resource pairs translation with a unified bidirectional EM framework.",5 Conclusion,[0],[0]
"By introducing another rich language, our method can better exploit the additional language pairs to enrich the original low-resource pair.",5 Conclusion,[0],[0]
"Compared with the RNNSearch (Bahdanau et al., 2014), a teacherstudent alike method (Chen et al., 2017) and the back-translation (Sennrich et al., 2015) on the same data level, our method achieves significant improvement on the MutiUN and IWSLT2012 datasets.",5 Conclusion,[0],[0]
"Note that our method can be combined with methods exploiting monolingual data for NMT low-resource problem such as backtranslation and make further improvements.
",5 Conclusion,[0],[0]
"In the future, we may extend our architecture to other scenarios, such as totally unsupervised training with no bilingual data for the rare language.",5 Conclusion,[0],[0]
We thank Zhirui Zhang and Shuangzhi Wu for useful discussions.,Acknowledgments,[0],[0]
"This work is supported in part by NSFC U1636210, 973 Program 2014CB340300, and NSFC 61421003.",Acknowledgments,[0],[0]
"Neural Machine Translation (NMT) performs poor on the low-resource language pair (X,Z), especially when Z is a rare language.",abstractText,[0],[0]
"By introducing another rich language Y , we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y,Z) (may be small) and (X,Y ) (can be rich) to improve the translation performance of lowresource pairs.",abstractText,[0],[0]
"In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X,Y ).",abstractText,[0],[0]
"Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.",abstractText,[0],[0]
Triangular Architecture for Rare Language Translation,title,[0],[0]
"Deep neural networks have recently received much limelight for their enormous success in a variety of applications across many different areas of artificial intelligence, computer vision, speech recognition, and natural language processing (LeCun et al., 2015; Hinton et al., 2012; Krizhevsky et al., 2012; Bahdanau et al., 2014; Kalchbrenner & Blunsom, 2013).",1. Introduction,[0],[0]
"Nevertheless, it is also well-known that our theoretical understanding of their efficacy remains incomplete.
",1. Introduction,[0],[0]
There have been several attempts to analyze deep neural networks from different perspectives.,1. Introduction,[0],[0]
"Notably, earlier studies have suggested that a deep architecture could use parameters more efficiently and requires exponentially fewer parameters to express certain families of functions than a shallow architecture (Delalleau & Bengio, 2011; Bengio & Delal-
1Department of Computer Science, University of Chicago, Chicago, IL 2Department of Statistics, University of Chicago, Chicago, IL 3Computational and Applied Mathematics Initiative, University of Chicago, Chicago, IL.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Lek-Heng Lim <lekheng@galton.uchicago.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"leau, 2011; Montufar et al., 2014; Eldan & Shamir, 2016; Poole et al., 2016; Arora et al., 2018).",1. Introduction,[0],[0]
"Recent work (Zhang et al., 2016) showed that several successful neural networks possess a high representation power and can easily shatter random data.",1. Introduction,[0],[0]
"However, they also generalize well to data unseen during training stage, suggesting that such networks may have some implicit regularization.",1. Introduction,[0],[0]
Traditional measures of complexity such as VC-dimension and Rademacher complexity fail to explain this phenomenon.,1. Introduction,[0],[0]
"Understanding this implicit regularization that begets the generalization power of deep neural networks remains a challenge.
",1. Introduction,[0],[0]
The goal of our work is to establish connections between neural network and tropical geometry in the hope that they will shed light on the workings of deep neural networks.,1. Introduction,[0],[0]
Tropical geometry is a new area in algebraic geometry that has seen an explosive growth in the recent decade but remains relatively obscure outside pure mathematics.,1. Introduction,[0],[0]
"We will focus on feedforward neural networks with rectified linear units (ReLU) and show that they are analogues of rational functions, i.e., ratios of two multivariate polynomials f, g in variables x1, . . .",1. Introduction,[0],[0]
", xd,
fpx1, . . .",1. Introduction,[0],[0]
", xdq gpx1, . . .",1. Introduction,[0],[0]
", xdq ,
in tropical algebra.",1. Introduction,[0],[0]
"For standard and trigonometric polynomials, it is known that rational approximation — approximating a target function by a ratio of two polynomials instead of a single polynomial — vastly improves the quality of approximation without increasing the degree.",1. Introduction,[0],[0]
"This gives our analogue: An ReLU neural network is the tropical ratio of two tropical polynomials, i.e., a tropical rational function.",1. Introduction,[0],[0]
"More precisely, if we view a neural network as a function ν :",1. Introduction,[0],[0]
"Rd Ñ Rp, x “ px1, . . .",1. Introduction,[0],[0]
", xdq ÞÑ pν1pxq, . . .",1. Introduction,[0],[0]
", νppxqq, then each ν is a tropical rational map, i.e., each νi is a tropical rational function.",1. Introduction,[0],[0]
"In fact, we will show that:
the family of functions represented by feedforward neural networks with rectified linear units and integer weights is exactly the family of tropical rational maps.
",1. Introduction,[0],[0]
It immediately follows that there is a semifield structure on this family of functions.,1. Introduction,[0],[0]
"More importantly, this establishes a
ar X
iv :1
80 5.
07 09
1v 1
[ cs
.L G
] 1
8 M
ay 2
01 8
bridge between neural networks1 and tropical geometry that allows us to view neural networks as well-studied tropical geometric objects.",1. Introduction,[0],[0]
This insight allows us to closely relate boundaries between linear regions of a neural network to tropical hypersurfaces and thereby facilitate studies of decision boundaries of neural networks in classification problems as tropical hypersurfaces.,1. Introduction,[0],[0]
"Furthermore, the number of linear regions, which captures the complexity of a neural network (Montufar et al., 2014; Raghu et al., 2017; Arora et al., 2018), can be bounded by the number of vertices of the polytopes associated with the neural network’s tropical rational representation.",1. Introduction,[0],[0]
"Lastly, a neural network with one hidden layer can be completely characterized by zonotopes, which serve as building blocks for deeper networks.
",1. Introduction,[0],[0]
In Sections 2 and 3 we introduce basic tropical algebra and tropical algebraic geometry of relevance to us.,1. Introduction,[0],[0]
We state our assumptions precisely in Section 4 and establish the connection between tropical geometry and multilayer neural networks in Section 5.,1. Introduction,[0],[0]
"We analyze neural networks with tropical tools in Section 6, proving that a deeper neural network is exponentially more expressive than a shallow network — though our objective is not so much to perform state-of-the-art analysis but to demonstrate that tropical algebraic geometry can provide useful insights.",1. Introduction,[0],[0]
All proofs are deferred to Section D of the supplement.,1. Introduction,[0],[0]
"Roughly speaking, tropical algebraic geometry is an analogue of classical algebraic geometry over C, the field of complex numbers, but where one replaces C by a semifield2 called the tropical semiring, to be defined below.",2. Tropical algebra,[0],[0]
We give a brief review of tropical algebra and introduce some relevant notations.,2. Tropical algebra,[0],[0]
"See (Itenberg et al., 2009; Maclagan & Sturmfels, 2015) for an in-depth treatment.
",2. Tropical algebra,[0],[0]
"The most fundamental component of tropical algebraic geometry is the tropical semiring T :“ ` R Y t´8u,‘,d ˘
.",2. Tropical algebra,[0],[0]
"The two operations ‘ and d, called tropical addition and tropical multiplication respectively, are defined as follows.
",2. Tropical algebra,[0],[0]
Definition 2.1.,2. Tropical algebra,[0],[0]
"For x, y P R, their tropical sum is x‘ y :“ maxtx, yu; their tropical product is x",2. Tropical algebra,[0],[0]
d y :“ x ` y; the tropical quotient of x over y is xm y,2. Tropical algebra,[0],[0]
":“ x´ y.
For any x P R, we have ´8 ‘ x “ 0 d x “ x and ´8d x “ ´8.",2. Tropical algebra,[0],[0]
Thus ´8 is the tropical additive identity and 0 is the tropical multiplicative identity.,2. Tropical algebra,[0],[0]
"Furthermore, these operations satisfy the usual laws of arithmetic: associativity, commutativity, and distributivity.",2. Tropical algebra,[0],[0]
The set RY t´8u is therefore a semiring under the operations‘ andd.,2. Tropical algebra,[0],[0]
"While it is not a ring (lacks additive inverse), one may nonetheless
1Henceforth a “neural network” will always mean a feedforward neural network with ReLU activation.
",2. Tropical algebra,[0],[0]
"2A semifield is a field sans the existence of additive inverses.
",2. Tropical algebra,[0],[0]
"generalize many algebraic objects (e.g., matrices, polynomials, tensors, etc) and notions (e.g., rank, determinant, degree, etc) over the tropical semiring — the study of these, in a nutshell, constitutes the subject of tropical algebra.
",2. Tropical algebra,[0],[0]
Let N “ tn P Z : n ě 0u.,2. Tropical algebra,[0],[0]
"For an integer a P N, raising x P R to the ath power is the same as multiplying x to itself a times.",2. Tropical algebra,[0],[0]
"When standard multiplication is replaced by tropical multiplication, this gives us tropical power:
xda",2. Tropical algebra,[0],[0]
":“ xd ¨ ¨ ¨ d x “ a ¨ x,
where the last ¨ denotes standard product of real numbers; it is extended to RY t´8u by defining, for any a P N,
´8da :“ #
´8 if a ą 0, 0",2. Tropical algebra,[0],[0]
"if a “ 0.
A tropical semiring, while not a field, possesses one quality of a field: Every x P R has a tropical multiplicative inverse given by its standard additive inverse, i.e., xdp´1q :“ ´x. Though not reflected in its name, T is in fact a semifield.
",2. Tropical algebra,[0],[0]
"One may therefore also raise x P R to a negative power a P Z by raising its tropical multiplicative inverse ´x to the positive power ´a, i.e., xda “ p´xqdp´aq.",2. Tropical algebra,[0],[0]
"As is the case in standard real arithmetic, the tropical additive inverse ´8 does not have a tropical multiplicative inverse and ´8da is undefined for a ă 0.",2. Tropical algebra,[0],[0]
"For notational simplicity, we will henceforth write xa instead of xda for tropical power when there is no cause for confusion.",2. Tropical algebra,[0],[0]
"Other algebraic rules of tropical power may be derived from definition; see Section B in the supplement.
",2. Tropical algebra,[0],[0]
We are now in a position to define tropical polynomials and tropical rational functions.,2. Tropical algebra,[0],[0]
"In the following, x and xi will denote variables (i.e., indeterminates).
",2. Tropical algebra,[0],[0]
Definition 2.2.,2. Tropical algebra,[0],[0]
"A tropical monomial in d variables x1, . . .",2. Tropical algebra,[0],[0]
", xd is an expression of the form
cd xa11 d x a2 2 d ¨ ¨ ¨ d x ad d
where c P R Y t´8u and a1, . . .",2. Tropical algebra,[0],[0]
", ad P N.",2. Tropical algebra,[0],[0]
"As a convenient shorthand, we will also write a tropical monomial in multiindex notation as cxα where α “ pa1, . .",2. Tropical algebra,[0],[0]
.,2. Tropical algebra,[0],[0]
", adq P Nd and x “ px1, . . .",2. Tropical algebra,[0],[0]
", xdq.",2. Tropical algebra,[0],[0]
"Note that xα “ 0 d xα as 0 is the tropical multiplicative identity.
",2. Tropical algebra,[0],[0]
Definition 2.3.,2. Tropical algebra,[0],[0]
"Following notations above, a tropical polynomial fpxq “ fpx1, . . .",2. Tropical algebra,[0],[0]
", xdq is a finite tropical sum of tropical monomials
fpxq “ c1xα1 ‘",2. Tropical algebra,[0],[0]
"¨ ¨ ¨ ‘ crxαr ,
where αi “ pai1, . . .",2. Tropical algebra,[0],[0]
", aidq P Nd and ci P R Y t´8u, i “ 1, . . .",2. Tropical algebra,[0],[0]
", r. We will assume that a monomial of a given multiindex appears at most once in the sum, i.e., αi ‰ αj for any i ‰ j.
Definition 2.4.",2. Tropical algebra,[0],[0]
"Following notations above, a tropical rational function is a standard difference, or, equivalently, a tropical quotient of two tropical polynomials fpxq and gpxq: fpxq ´ gpxq “ fpxq m gpxq.",2. Tropical algebra,[0],[0]
"We will denote a tropical rational function by f m g, where f and g are understood to be tropical polynomial functions.
",2. Tropical algebra,[0],[0]
"It is routine to verify that the set of tropical polynomials Trx1, . . .",2. Tropical algebra,[0],[0]
", xds forms a semiring under the standard extension of ‘ and d to tropical polynomials, and likewise the set of tropical rational functions Tpx1, . . .",2. Tropical algebra,[0],[0]
", xdq forms a semifield.",2. Tropical algebra,[0],[0]
"We regard a tropical polynomial f “ f m 0 as a special case of a tropical rational function and thus Trx1, . . .",2. Tropical algebra,[0],[0]
", xds Ď Tpx1, . . .",2. Tropical algebra,[0],[0]
", xdq.",2. Tropical algebra,[0],[0]
"Henceforth any result stated for a tropical rational function would implicitly also hold for a tropical polynomial.
",2. Tropical algebra,[0],[0]
A d-variate tropical polynomial fpxq defines a function f :,2. Tropical algebra,[0],[0]
"Rd Ñ R that is a convex function in the usual sense as taking max and sum of convex functions preserve convexity (Boyd & Vandenberghe, 2004).",2. Tropical algebra,[0],[0]
"As such, a tropical rational function f m g : Rd Ñ R is a DC function or differenceconvex function (Hartman, 1959; Tao & Hoai An, 2005).
",2. Tropical algebra,[0],[0]
We will need a notion of vector-valued tropical polynomials and tropical rational functions.,2. Tropical algebra,[0],[0]
Definition 2.5.,2. Tropical algebra,[0],[0]
F :,2. Tropical algebra,[0],[0]
"Rd Ñ Rp, x “ px1, . . .",2. Tropical algebra,[0],[0]
", xdq ÞÑ pf1pxq, . . .",2. Tropical algebra,[0],[0]
", fppxqq, is called a tropical polynomial map if each fi :",2. Tropical algebra,[0],[0]
"Rd Ñ R is a tropical polynomial, i “ 1, . . .",2. Tropical algebra,[0],[0]
", p, and a tropical rational map if f1, . . .",2. Tropical algebra,[0],[0]
", fp are tropical rational functions.",2. Tropical algebra,[0],[0]
"We will denote the set of tropical polynomial maps by Polpd, pq and the set of tropical rational maps by Ratpd, pq.",2. Tropical algebra,[0],[0]
"So Polpd, 1q “ Trx1, . . .",2. Tropical algebra,[0],[0]
", xds and Ratpd, 1q “ Tpx1, . . .",2. Tropical algebra,[0],[0]
", xdq.",2. Tropical algebra,[0],[0]
"There are tropical analogues of many notions in classical algebraic geometry (Itenberg et al., 2009; Maclagan & Sturmfels, 2015), among which are tropical hypersurfaces, tropical analogues of algebraic curves in classical algebraic geometry.",3. Tropical hypersurfaces,[0],[0]
Tropical hypersurfaces are a principal object of interest in tropical geometry and will prove very useful in our approach towards neural networks.,3. Tropical hypersurfaces,[0],[0]
"Intuitively, the tropical hypersurface of a tropical polynomial f is the set of points x where f is not linear at x. Definition 3.1.",3. Tropical hypersurfaces,[0],[0]
"The tropical hypersurface of a tropical polynomial fpxq “ c1xα1 ‘ ¨ ¨ ¨ ‘ crxαr is
T pfq :“ x P Rd : cixαi “ cjxαj “ fpxq for some αi ‰ αj ( .
i.e., the set of points x at which the value of f at x is attained by two or more monomials in f .
",3. Tropical hypersurfaces,[0],[0]
A tropical hypersurface divides the domain of f into convex cells on each of which f is linear.,3. Tropical hypersurfaces,[0],[0]
"These cells are convex polyhedrons, i.e., defined by linear inequalities with integer coefficients: tx P Rd :",3. Tropical hypersurfaces,[0],[0]
Ax ď bu for A P Zmˆd and b P Rm.,3. Tropical hypersurfaces,[0],[0]
"For example, the cell where a tropical monomial cjx
αj attains its maximum is tx P Rd : cj ` αTjx ě ci ` αTix for all i ‰ ju.",3. Tropical hypersurfaces,[0],[0]
"Tropical hypersurfaces of polynomials in two variables (i.e., in R2) are called tropical curves.
",3. Tropical hypersurfaces,[0],[0]
"Just like standard multivariate polynomials, every tropical polynomial comes with an associated Newton polygon.
",3. Tropical hypersurfaces,[0],[0]
Definition 3.2.,3. Tropical hypersurfaces,[0],[0]
"The Newton polygon of a tropical polynomial fpxq “ c1xα1 ‘ ¨ ¨ ¨ ‘ crxαr is the convex hull of α1, . . .",3. Tropical hypersurfaces,[0],[0]
", αr P Nd, regarded as points in Rd,
∆pfq :“ Conv αi P Rd : ci ‰ ´8, i “ 1, . . .",3. Tropical hypersurfaces,[0],[0]
", r ( .
",3. Tropical hypersurfaces,[0],[0]
"A tropical polynomial f determines a dual subdivision of ∆pfq, constructed as follows.",3. Tropical hypersurfaces,[0],[0]
"First, lift each αi from Rd into Rd`1 by appending ci as the last coordinate.",3. Tropical hypersurfaces,[0],[0]
"Denote the convex hull of the lifted α1, . . .",3. Tropical hypersurfaces,[0],[0]
", αr as
Ppfq :“ Convtpαi, ciq P Rd ˆ R : i “ 1, . . .",3. Tropical hypersurfaces,[0],[0]
", ru. (1)
Next let UF ` Ppfq ˘
denote the collection of upper faces in Ppfq and π : Rd ˆ R Ñ Rd be the projection that drops the last coordinate.",3. Tropical hypersurfaces,[0],[0]
"The dual subdivision determined by f is then
δpfq :“ πppq Ă Rd : p P UF ` Ppfq ˘( .
",3. Tropical hypersurfaces,[0],[0]
δpfq forms a polyhedral complex with support ∆pfq.,3. Tropical hypersurfaces,[0],[0]
"By (Maclagan & Sturmfels, 2015, Proposition 3.1.6), the tropical hypersurface T pfq is the pd´ 1q-skeleton of the polyhedral complex dual to δpfq.",3. Tropical hypersurfaces,[0],[0]
This means that each vertex in δpfq corresponds to one “cell” in Rd where the function f is linear.,3. Tropical hypersurfaces,[0],[0]
"Thus, the number of vertices in Ppfq provides an upper bound on the number of linear regions of f .
",3. Tropical hypersurfaces,[0],[0]
"Figure 1 shows the Newton polygon and dual subdivision for the tropical polynomial fpx1, x2q “ 1d x21 ‘ 1d x22 ‘ 2d x1x2",3. Tropical hypersurfaces,[0],[0]
‘ 2d x1 ‘ 2d x2 ‘ 2.,3. Tropical hypersurfaces,[0],[0]
"Figure 2 shows how we
may find the dual subdivision for this tropical polynomial by following the aforementioned procedures; with step-by-step details given in Section C.1.
",3. Tropical hypersurfaces,[0],[0]
Tropical polynomials and tropical rational functions are clearly piecewise linear functions.,3. Tropical hypersurfaces,[0],[0]
"As such a tropical rational map is a piecewise linear map and the notion of linear region applies.
",3. Tropical hypersurfaces,[0],[0]
Definition 3.3.,3. Tropical hypersurfaces,[0],[0]
"A linear region of F P Ratpd,mq is a maximal connected subset of the domain on which F is linear.",3. Tropical hypersurfaces,[0],[0]
"The number of linear regions of F is denoted N pF q.
Note that a tropical polynomial map F P Polpd,mq has convex linear regions but a tropical rational map F P Ratpd, nq generally has nonconvex linear regions.",3. Tropical hypersurfaces,[0],[0]
"In Section 6.3, we will use N pF q as a measure of complexity for an F P Ratpd, nq given by a neural network.",3. Tropical hypersurfaces,[0],[0]
"Our analysis of neural networks will require figuring out how the polytope Ppfq transforms under tropical power, sum, and product.",3.1. Transformations of tropical polynomials,[0],[0]
"The first is straightforward.
",3.1. Transformations of tropical polynomials,[0],[0]
Proposition 3.1.,3.1. Transformations of tropical polynomials,[0],[0]
"Let f be a tropical polynomial and let a P N. Then
Ppfaq “ aPpfq.
",3.1. Transformations of tropical polynomials,[0],[0]
"aPpfq “ tax : x P Ppfqu Ď Rd`1 is a scaled version of Ppfq with the same shape but different volume.
",3.1. Transformations of tropical polynomials,[0],[0]
"To describe the effect of tropical sum and product, we need a few notions from convex geometry.",3.1. Transformations of tropical polynomials,[0],[0]
"The Minkowski sum of two sets P1 and P2 in Rd is the set
P1 ` P2 :“ x1 ` x2 P Rd :",3.1. Transformations of tropical polynomials,[0],[0]
"x1 P P1, x2 P P2 ( ;
and for λ1, λ2 ě 0, their weighted Minkowski sum is
λ1P1 ` λ2P2 :“ λ1x1 ` λ2x2 P Rd : x1 P P1, x2 P P2 ( .
",3.1. Transformations of tropical polynomials,[0],[0]
Weighted Minkowski sum is clearly commutative and associative and generalizes to more than two sets.,3.1. Transformations of tropical polynomials,[0],[0]
"In particular, the Minkowski sum of line segments is called a zonotope.
Let VpP q denote the set of vertices of a polytope P .",3.1. Transformations of tropical polynomials,[0],[0]
"Clearly, the Minkowski sum of two polytopes is given by the convex hull of the Minkowski sum of their vertex sets, i.e., P1 ` P2 “ Conv ` VpP1q ` VpP2q ˘
.",3.1. Transformations of tropical polynomials,[0],[0]
"With this observation, the following is immediate.
",3.1. Transformations of tropical polynomials,[0],[0]
Proposition 3.2.,3.1. Transformations of tropical polynomials,[0],[0]
"Let f, g P Polpd, 1q “ Trx1, . . .",3.1. Transformations of tropical polynomials,[0],[0]
", xds be tropical polynomials.",3.1. Transformations of tropical polynomials,[0],[0]
"Then
Ppf d gq “ Ppfq ` Ppgq, Ppf ‘ gq “ Conv ` VpPpfqq Y VpPpgqq ˘ .
",3.1. Transformations of tropical polynomials,[0],[0]
"We reproduce below part of (Gritzmann & Sturmfels, 1993, Theorem 2.1.10) and derive a corollary for bounding the number of verticies on the upper faces of a zonotope.
Theorem 3.3 (Gritzmann–Sturmfels).",3.1. Transformations of tropical polynomials,[0],[0]
"Let P1, . . .",3.1. Transformations of tropical polynomials,[0],[0]
", Pk be polytopes in Rd and let m denote the total number of nonparallel edges of P1, . . .",3.1. Transformations of tropical polynomials,[0],[0]
", Pk.",3.1. Transformations of tropical polynomials,[0],[0]
"Then the number of vertices of P1 ` ¨ ¨ ¨ ` Pk does not exceed
2 d´1 ÿ j“0 pm",3.1. Transformations of tropical polynomials,[0],[0]
´,3.1. Transformations of tropical polynomials,[0],[0]
"1j q .
",3.1. Transformations of tropical polynomials,[0],[0]
"The upper bound is attained if all Pi’s are zonotopes and all their generating line segments are in general positions.
",3.1. Transformations of tropical polynomials,[0],[0]
Corollary 3.4.,3.1. Transformations of tropical polynomials,[0],[0]
"Let P Ď Rd`1 be a zonotope generated by m line segments P1, . . .",3.1. Transformations of tropical polynomials,[0],[0]
", Pm.",3.1. Transformations of tropical polynomials,[0],[0]
Let π : Rd ˆ RÑ Rd be the projection.,3.1. Transformations of tropical polynomials,[0],[0]
"Suppose P satisfies:
(i) the generating line segments are in general positions;
(ii) the set of projected vertices tπpvq : v P VpP qu Ď Rd are in general positions.
",3.1. Transformations of tropical polynomials,[0],[0]
"Then P has d ÿ
j“0 pmj q
vertices on its upper faces.",3.1. Transformations of tropical polynomials,[0],[0]
"If either (i) or (ii) is violated, then this becomes an upper bound.
",3.1. Transformations of tropical polynomials,[0],[0]
"As we mentioned, linear regions of a tropical polynomial f correspond to vertices on UF ` Ppfq ˘
and the corollary will be useful for bounding the number of linear regions.",3.1. Transformations of tropical polynomials,[0],[0]
"While we expect our readership to be familiar with feedforward neural networks, we will nevertheless use this short
section to define them, primarily for the purpose of fixing notations and specifying the assumptions that we retain throughout this article.",4. Neural networks,[0],[0]
"We restrict our attention to fully connected feedforward neural networks.
",4. Neural networks,[0],[0]
"Viewed abstractly, an L-layer feedforward neural network is a map ν",4. Neural networks,[0],[0]
: Rd Ñ,4. Neural networks,[0],[0]
"Rp given by a composition of functions
ν “ σpLq ˝ ρpLq ˝",4. Neural networks,[0],[0]
"σpL´1q ˝ ρpL´1q ¨ ¨ ¨ ˝ σp1q ˝ ρp1q.
",4. Neural networks,[0],[0]
"The preactivation functions ρp1q, . . .",4. Neural networks,[0],[0]
", ρpLq are affine transformations to be determined and the activation functions σp1q, . . .",4. Neural networks,[0],[0]
", σpLq are chosen and fixed in advanced.
",4. Neural networks,[0],[0]
"We denote the width, i.e., the number of nodes, of the lth layer by nl, l “ 1, ¨ ¨ ¨ , L´ 1.",4. Neural networks,[0],[0]
"We set n0 :“ d and nL :“ p, respectively the dimensions of the input and output of the network.",4. Neural networks,[0],[0]
"The output from the lth layer will be denoted by
νplq :“ σplq ˝ ρplq ˝",4. Neural networks,[0],[0]
σpl´1q ˝,4. Neural networks,[0],[0]
ρpl´1q ¨ ¨ ¨ ˝ σp1q ˝ ρp1q,4. Neural networks,[0],[0]
",
i.e., it is a map νplq : Rd Ñ Rnl .",4. Neural networks,[0],[0]
"For convenience, we assume νp0qpxq",4. Neural networks,[0],[0]
":“ x.
The affine function ρplq :",4. Neural networks,[0],[0]
"Rnl´1 Ñ Rnl is given by a weight matrix Aplq P Znlˆnl´1 and a bias vector bplq P Rnl :
ρplqpνpl´1qq :“ Aplqνpl´1q ` bplq.
",4. Neural networks,[0],[0]
"The pi, jqth coordinate of Aplq will be denoted aplqij and the ith coordinate of bplq by bplqi .",4. Neural networks,[0],[0]
"Collectively they form the parameters of the lth layer.
",4. Neural networks,[0],[0]
"For a vector input x P Rnl , σplqpxq is understood to be in coordinatewise sense; so σ",4. Neural networks,[0],[0]
:,4. Neural networks,[0],[0]
Rnl Ñ Rnl .,4. Neural networks,[0],[0]
We assume the final output of a neural network νpxq is fed into a score function s : Rp Ñ Rm that is application specific.,4. Neural networks,[0],[0]
"When used as an m-category classifier, s may be chosen, for example, to be a soft-max or sigmoidal function.",4. Neural networks,[0],[0]
The score function is quite often regarded as the last layer of a neural network but this is purely a matter of convenience and we will not assume this.,4. Neural networks,[0],[0]
"We will make the following mild assumptions on the architecture of our feedforward neural networks and explain next why they are indeed mild:
(a) the weight matrices Ap1q, . . .",4. Neural networks,[0],[0]
", ApLq are integer-valued;
(b) the bias vectors bp1q, . . .",4. Neural networks,[0],[0]
", bpLq are real-valued;
(c) the activation functions σp1q, . . .",4. Neural networks,[0],[0]
", σpLq take the form
σplqpxq :“ maxtx, tplqu,
where tplq P pRYt´8uqnl is called a threshold vector.
",4. Neural networks,[0],[0]
"Henceforth all neural networks in our subsequent discussions will be assumed to satisfy (a)–(c).
",4. Neural networks,[0],[0]
"(b) is completely general but there is also no loss of generality in (a), i.e., in restricting the weights Ap1q, . . .",4. Neural networks,[0],[0]
", ApLq from real matrices to integer matrices, as:
• real weights can be approximated arbitrarily closely by rational weights;
• one may then ‘clear denominators’ in these rational weights by multiplying them by the least common multiple of their denominators to obtain integer weights;
• keeping in mind that scaling all weights and biases by the same positive constant has no bearing on the workings of a neural network.
",4. Neural networks,[0],[0]
The activation function in (c) includes both ReLU activation (tplq “ 0) and identity map (tplq “ ´8) as special cases.,4. Neural networks,[0],[0]
"Aside from ReLU, our tropical framework will apply to piecewise linear activations such as leaky ReLU and absolute value, and with some extra effort, may be extended to max pooling, maxout nets, etc.",4. Neural networks,[0],[0]
"But it does not, for example, apply to activations such as hyperbolic tangent and sigmoid.
",4. Neural networks,[0],[0]
"In this work, we view an ReLU network as the simplest and most canonical model of a neural network, from which other variants that are more effective at specific tasks may be derived.",4. Neural networks,[0],[0]
"Given that we seek general theoretical insights and not specific practical efficacy, it makes sense to limit ourselves to this simplest case.",4. Neural networks,[0],[0]
"Moreover, ReLU networks already embody some of the most important elements (and mysteries) common to a wider range of neural networks (e.g., universal approximation, exponential expressiveness); they work well in practice and are often the go-to choice for feedforward networks.",4. Neural networks,[0],[0]
"We are also not alone in limiting our discussions to ReLU networks (Montufar et al., 2014; Arora et al., 2018).",4. Neural networks,[0],[0]
"We now describe our tropical formulation of a multilayer feedforward neural network satisfying (a)–(c).
",5. Tropical algebra of neural networks,[0],[0]
"A multilayer feedforward neural network is generally nonconvex, whereas a tropical polynomial is always convex.",5. Tropical algebra of neural networks,[0],[0]
"Since most nonconvex functions are a difference of two convex functions (Hartman, 1959), a reasonable guess is that a feedforward neural network is the difference of two tropical polynomials, i.e., a tropical rational function.",5. Tropical algebra of neural networks,[0],[0]
"This is indeed the case, as we will see from the following.
",5. Tropical algebra of neural networks,[0],[0]
"Consider the output from the first layer in neural network
νpxq “ maxtAx` b, tu,
where A P Zpˆd, b P Rp, and t P pR Y t´8uqp.",5. Tropical algebra of neural networks,[0],[0]
"We will decompose A as a difference of two nonnegative integervalued matrices, A “ A`´A´ with A`, A´ P Npˆd; e.g., in the standard way with entries
a`ij :“ maxtaij , 0u, a ´ ij :“ maxt´aij , 0u
respectively.",5. Tropical algebra of neural networks,[0],[0]
"Since
maxtAx` b, tu “ maxtA`x` b, A´x` tu ´A´x,
we see that every coordinate of one-layer neural network is a difference of two tropical polynomials.",5. Tropical algebra of neural networks,[0],[0]
"For networks with more layers, we apply this decomposition recursively to obtain the following result.
",5. Tropical algebra of neural networks,[0],[0]
Proposition 5.1.,5. Tropical algebra of neural networks,[0],[0]
"LetA P Zmˆn, b P Rm be the parameters of the pl ` 1qth layer, and let t P pR Y t´8uqm be the threshold vector in the pl` 1qth layer.",5. Tropical algebra of neural networks,[0],[0]
"If the nodes of the lth layer are given by tropical rational functions,
νplqpxq “ F plqpxq mGplqpxq “ F plqpxq ´Gplqpxq,
i.e., each coordinate of F plq and Gplq is a tropical polynomial in x, then the outputs of the preactivation and of the pl ` 1qth layer are given by tropical rational functions
ρpl`1q ˝ νplqpxq “ Hpl`1qpxq ´Gpl`1qpxq, νpl`1qpxq “ σ ˝ ρpl`1q ˝ νplqpxq “ F pl`1qpxq ´Gpl`1qpxq
respectively, where
F pl`1qpxq “ max Hpl`1qpxq, Gpl`1qpxq ` t ( ,
Gpl`1qpxq “ A`Gplqpxq `A´F plqpxq, Hpl`1qpxq “ A`F plqpxq `A´Gplqpxq ` b.
We will write f plqi ,",5. Tropical algebra of neural networks,[0],[0]
"g plq i and h plq i for the ith coordinate of F plq, Gplq and Hplq respectively.",5. Tropical algebra of neural networks,[0],[0]
"In tropical arithmetic, the recurrence above takes the form
f pl`1q i “ h pl`1q",5. Tropical algebra of neural networks,[0],[0]
"i ‘ pg pl`1q i d tiq,
g pl`1q i “
„ n ä
j“1 pf plqj q",5. Tropical algebra of neural networks,[0],[0]
"a´ij
 d „",5. Tropical algebra of neural networks,[0],[0]
"n ä
j“1 pgplqj q a`ij

,
",5. Tropical algebra of neural networks,[0],[0]
h pl`1q,5. Tropical algebra of neural networks,[0],[0]
"i “
„ n ä
j“1 pf plqj q",5. Tropical algebra of neural networks,[0],[0]
"a`ij
 d „",5. Tropical algebra of neural networks,[0],[0]
"n ä
j“1",5. Tropical algebra of neural networks,[0],[0]
"pgplqj q a´ij

d bi.
(2)
Repeated applications of Proposition 5.1 yield the following.
",5. Tropical algebra of neural networks,[0],[0]
Theorem 5.2 (Tropical characterization of neural networks).,5. Tropical algebra of neural networks,[0],[0]
A feedforward neural network under assumptions (a)–(c) is a function ν,5. Tropical algebra of neural networks,[0],[0]
: Rd Ñ,5. Tropical algebra of neural networks,[0],[0]
"Rp whose coordinates are tropical rational functions of the input, i.e.,
νpxq “ F pxq mGpxq “ F pxq ´Gpxq
where F and G are tropical polynomial maps.",5. Tropical algebra of neural networks,[0],[0]
"Thus ν is a tropical rational map.
",5. Tropical algebra of neural networks,[0],[0]
"Note that the tropical rational functions above have real coefficients, not integer coefficients.",5. Tropical algebra of neural networks,[0],[0]
"The integer weights Aplq P Znlˆnl´1 have gone into the powers of tropical monomials in f and g, which is why we require our weights to be integer-valued, although as we have explained, this requirement imposes little loss of generality.
",5. Tropical algebra of neural networks,[0],[0]
"By setting tp1q “ ¨ ¨ ¨ “ tpL´1q “ 0 and tpLq “ ´8, we obtain the following corollary.
",5. Tropical algebra of neural networks,[0],[0]
Corollary 5.3.,5. Tropical algebra of neural networks,[0],[0]
Let ν :,5. Tropical algebra of neural networks,[0],[0]
Rd Ñ R be an ReLU activated feedforward neural network with integer weights and linear output.,5. Tropical algebra of neural networks,[0],[0]
"Then ν is a tropical rational function.
",5. Tropical algebra of neural networks,[0],[0]
"A more remarkable fact is the converse of Corollary 5.3.
",5. Tropical algebra of neural networks,[0],[0]
"Theorem 5.4 (Equivalence of neural networks and tropical rational functions).
",5. Tropical algebra of neural networks,[0],[0]
(i) Let ν,5. Tropical algebra of neural networks,[0],[0]
": Rd Ñ R. Then ν is a tropical rational function if and only if ν is a feedforward neural network satisfying assumptions (a)–(c).
",5. Tropical algebra of neural networks,[0],[0]
"(ii) A tropical rational function f m g can be represented as an L-layer neural network, with
L ď maxtrlog2 rf s, rlog2 rgsu ` 2,
where rf and rg are the number of monomials in the tropical polynomials f and g respectively.
",5. Tropical algebra of neural networks,[0],[0]
"We would like to acknowledge the precedence of (Arora et al., 2018, Theorem 2.1), which demonstrates the equivalence between ReLU-activatedL-layer neural networks with real weights and d-variate continuous piecewise functions with real coefficients, where L ď rlog2pd` 1qs` 1.
",5. Tropical algebra of neural networks,[0],[0]
"By construction, a tropical rational function is a continuous piecewise linear function.",5. Tropical algebra of neural networks,[0],[0]
The continuity of a piecewise linear function automatically implies that each of the pieces on which it is linear is a polyhedral region.,5. Tropical algebra of neural networks,[0],[0]
"As we saw in Section 3, a tropical polynomial f",5. Tropical algebra of neural networks,[0],[0]
: Rd Ñ R gives a tropical hypersurface that divides Rd into convex polyhedral regions defined by linear inequalities with integer coefficients: tx P Rd :,5. Tropical algebra of neural networks,[0],[0]
Ax ď bu with A P Zmˆd and b P Rm.,5. Tropical algebra of neural networks,[0],[0]
"A tropical rational function f m g : Rd Ñ R must also be a continuous piecewise linear function and divide Rd into polyhedral regions on each of which f m g is linear, although these regions are nonconvex in general.",5. Tropical algebra of neural networks,[0],[0]
"We will show the converse — any continuous piecewise linear function with integer coefficients is a tropical rational function.
",5. Tropical algebra of neural networks,[0],[0]
Proposition 5.5.,5. Tropical algebra of neural networks,[0],[0]
Let ν :,5. Tropical algebra of neural networks,[0],[0]
"Rd Ñ R. Then ν is a continuous piecewise linear function with integer coefficients if and only if ν is a tropical rational function.
",5. Tropical algebra of neural networks,[0],[0]
"Corollary 5.3, Theorem 5.4, and Proposition 5.5 collectively imply the equivalence of
(i) tropical rational functions,
(ii) continuous piecewise linear functions with integer coefficients,
(iii) neural networks satisfying assumptions (a)–(c).
",5. Tropical algebra of neural networks,[0],[0]
"An immediate advantage of this characterization is that the set of tropical rational functions Tpx1, . . .",5. Tropical algebra of neural networks,[0],[0]
", xdq has a semifield structure as we pointed out in Section 2, a fact that we have implicitly used in the proof of Proposition 5.5.",5. Tropical algebra of neural networks,[0],[0]
"However, what is more important is not the algebra but the
algebraic geometry that arises from our tropical characterization.",5. Tropical algebra of neural networks,[0],[0]
"We will use tropical algebraic geometry to illuminate our understanding of neural networks in the next section.
",5. Tropical algebra of neural networks,[0],[0]
The need to stay within tropical algebraic geometry is the reason we did not go for a simpler and more general characterization (that does not require the integer coefficients assumption).,5. Tropical algebra of neural networks,[0],[0]
"A tropical signomial takes the form
ϕpxq “ m à
i“1 bi
n ä j“1",5. Tropical algebra of neural networks,[0],[0]
"x aij j ,
where aij P R and bi P R Y t´8u.",5. Tropical algebra of neural networks,[0],[0]
Note that aij is not required to be integer-valued nor nonnegative.,5. Tropical algebra of neural networks,[0],[0]
"A tropical rational signomial is a tropical quotientϕmψ of two tropical signomials ϕ,ψ.",5. Tropical algebra of neural networks,[0],[0]
"A tropical rational signomial map is a function ν “ pν1, . . .",5. Tropical algebra of neural networks,[0],[0]
", νpq :",5. Tropical algebra of neural networks,[0],[0]
Rd Ñ Rp where each νi : Rd Ñ R is a tropical rational signomial νi “ ϕi m ψi.,5. Tropical algebra of neural networks,[0],[0]
The same argument we used to establish Theorem 5.2 gives us the following.,5. Tropical algebra of neural networks,[0],[0]
Proposition 5.6.,5. Tropical algebra of neural networks,[0],[0]
"Every feedforward neural network with ReLU activation is a tropical rational signomial map.
",5. Tropical algebra of neural networks,[0],[0]
Nevertheless tropical signomials fall outside the realm of tropical algebraic geometry and we do not use Proposition 5.6 in the rest of this article.,5. Tropical algebra of neural networks,[0],[0]
"Section 5 defines neural networks via tropical algebra, a perspective that allows us to study them via tropical algebraic geometry.",6. Tropical geometry of neural networks,[0],[0]
We will show that the decision boundary of a neural network is a subset of a tropical hypersurface of a corresponding tropical polynomial (Section 6.1).,6. Tropical geometry of neural networks,[0],[0]
"We will see that, in an appropriate sense, zonotopes form the geometric building blocks for neural networks (Section 6.2).",6. Tropical geometry of neural networks,[0],[0]
We then prove that the geometry of the function represented by a neural network grows vastly more complex as its number of layers increases (Section 6.3).,6. Tropical geometry of neural networks,[0],[0]
"We will use tropical geometry and insights from Section 5 to study decision boundaries of neural networks, focusing on the case of two-category classification for clarity.",6.1. Decision boundaries of a neural network,[0],[0]
"As explained in Section 4, a neural network ν :",6.1. Decision boundaries of a neural network,[0],[0]
Rd Ñ Rp together with a choice of score function s :,6.1. Decision boundaries of a neural network,[0],[0]
Rp Ñ R give us a classifier.,6.1. Decision boundaries of a neural network,[0],[0]
"If the output value spνpxqq exceeds some decision threshold c, then the neural network predicts x is from one class (e.g., x is a CAT image), and otherwise x is from the other category (e.g., a DOG image).",6.1. Decision boundaries of a neural network,[0],[0]
The input space is thereby partitioned into two disjoint subsets by the decision boundary B :“ tx P Rd : νpxq “ s´1pcqu.,6.1. Decision boundaries of a neural network,[0],[0]
"Connected regions with value above the threshold and connected regions with value below the threshold will be called the positive regions and negative regions respectively.
",6.1. Decision boundaries of a neural network,[0],[0]
"We provide bounds on the number of positive and negative regions and show that there is a tropical polynomial whose tropical hypersurface contains the decision boundary.
",6.1. Decision boundaries of a neural network,[0],[0]
Proposition 6.1 (Tropical geometry of decision boundary).,6.1. Decision boundaries of a neural network,[0],[0]
Let ν :,6.1. Decision boundaries of a neural network,[0],[0]
Rd Ñ R be an L-layer neural network satisfying assumptions (a)–(c) with tpLq “ ´8.,6.1. Decision boundaries of a neural network,[0],[0]
Let the score function s : RÑ R be injective with decision threshold c in its range.,6.1. Decision boundaries of a neural network,[0],[0]
"If ν “ f m g where f and g are tropical polynomials, then
(i) its decision boundary B “ tx P Rd : νpxq “ s´1pcqu divides Rd into at most N pfq connected positive regions and at most N pgq connected negative regions;
(ii) its decision boundary is contained in the tropical hypersurface of the tropical polynomial s´1pcq d gpxq ‘ fpxq “ maxtfpxq, gpxq ` s´1pcqu, i.e.,
B Ď T ps´1pcq d g ‘ fq.",6.1. Decision boundaries of a neural network,[0],[0]
"(3)
",6.1. Decision boundaries of a neural network,[0],[0]
The function s´1pcqdg‘f is not necessarily linear on every positive or negative region and so its tropical hypersurface T ps´1pcqdg‘fqmay further divide a positive or negative region derived from B into multiple linear regions.,6.1. Decision boundaries of a neural network,[0],[0]
Hence the “Ď” in (3) cannot in general be replaced by ““”.,6.1. Decision boundaries of a neural network,[0],[0]
"From Section 3, we know that the number of regions a tropical hypersurface T pfq divides the space into equals the number of vertices in the dual subdivision of the Newton polygon associated with the tropical polynomial f .",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"This allows us to bound the number of linear regions of a neural network by bounding the number of vertices in the dual subdivision of the Newton polygon.
",6.2. Zonotopes as geometric building blocks of neural networks,[0.9529476211696671],"['The first neurons in the hidden layer of the combined network correspond to the hidden layer in the first single network, and the others to the hidden layer of the second network.']"
"We start by examining how geometry changes from one layer to the next in a neural network, more precisely:
Question.",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"How are the tropical hypersurfaces of the tropical polynomials in the pl ` 1qth layer of a neural network related to those in the lth layer?
",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"The recurrent relation (2) describes how the tropical polynomials occurring in the pl ` 1qth layer are obtained from those in the lth layer, namely, via three operations: tropical sum, tropical product, and tropical powers.",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"Recall that a tropical hypersurface of a tropical polynomial is dual to the dual subdivision of the Newton polytope of the tropical polynomial, which is given by the projection of the upper faces on the polytopes defined by (1).",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"Hence the question boils down to how these three operations transform the polytopes, which is addressed in Propositions 3.1 and 3.2.",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"We follow notations in Proposition 5.1 for the next result.
",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
Lemma 6.2.,6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"Let f plqi ,",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"g plq i , h plq i be the tropical polynomials produced by the ith node in the lth layer of a neural network,
i.e., they are defined by (2).",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"Then P ` f plq i ˘ , P ` g plq i ˘ , P ` h plq",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"i ˘ are subsets of Rd`1 given as follows:
",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"(i) P ` g p1q i ˘ and P ` h p1q i ˘ are points.
",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"(ii) P ` f p1q i ˘ is a line segment.
",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
(iii) P ` g p2q,6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
i ˘ and P ` h p2q i ˘ are zonotopes.,6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"(iv) For l ě 1,
P ` f plq i ˘ “ Conv “ P ` g plq i d t plq",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
i ˘ Y P ` h plq,6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"i ˘‰
if tplqi P R, and P ` f plq",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"i ˘ “ P ` h plq i ˘ if tplqi “ ´8.
(v) For l ě 1, P ` g pl`1q i ˘ and P ` h pl`1q i ˘
are weighted Minkowski sums,
",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"P ` g pl`1q i ˘
“ nl ÿ
j“1 a´ijP ` f plq j ˘
` nl ÿ
j“1 a`ijP",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"` g plq j ˘ ,
P ` h pl`1q",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"i ˘
“ nl ÿ
j“1 a`ijP ` f plq j ˘
` nl ÿ
j“1 a´ijP",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"` g plq j ˘
` tbieu,
where aij , bi are entries of the weight matrix Apl`1q P Znl`1ˆnl and bias vector bpl`1q P Rnl`1 , and e :“ p0, . . .",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
", 0, 1q P Rd`1.
",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
A conclusion of Lemma 6.2 is that zonotopes are the building blocks in the tropical geometry of neural networks.,6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"Zonotopes are studied extensively in convex geometry and, among other things, are intimately related to hyperplane arrangements (Greene & Zaslavsky, 1983; Guibas et al., 2003; McMullen, 1971; Holtz & Ron, 2011).",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
Lemma 6.2 connects neural networks to this extensive body of work but its full implication remains to be explored.,6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"In Section C.2 of the supplement, we show how one may build these polytopes for a two-layer neural network.",6.2. Zonotopes as geometric building blocks of neural networks,[0],[0]
"We apply the tools in Section 3 to study the complexity of a neural network, showing that a deep network is much more expressive than a shallow one.",6.3. Geometric complexity of deep neural networks,[0],[0]
"Our measure of complexity is geometric: we will follow (Montufar et al., 2014; Raghu et al., 2017) and use the number of linear regions of a piecewise linear function ν :",6.3. Geometric complexity of deep neural networks,[0],[0]
"Rd Ñ Rp to measure the complexity of ν.
",6.3. Geometric complexity of deep neural networks,[0],[0]
"We would like to emphasize that our upper bound below does not improve on that obtained in (Raghu et al., 2017) — in fact, our version is more restrictive given that it applies only to neural networks satisfying (a)–(c).",6.3. Geometric complexity of deep neural networks,[0],[0]
"Nevertheless our goal here is to demonstrate how tropical geometry may be used to derive the same bound.
",6.3. Geometric complexity of deep neural networks,[0],[0]
Theorem 6.3.,6.3. Geometric complexity of deep neural networks,[0],[0]
Let ν :,6.3. Geometric complexity of deep neural networks,[0],[0]
Rd Ñ R be an L-layer real-valued feedforward neural network satisfying (a)–(c).,6.3. Geometric complexity of deep neural networks,[0],[0]
"Let tpLq “
´8 and nl ě d for all l “ 1, . . .",6.3. Geometric complexity of deep neural networks,[0],[0]
", L ´ 1.",6.3. Geometric complexity of deep neural networks,[0],[0]
"Then ν “ νpLq has at most
L´1 ź
l“1
d ÿ i“0",6.3. Geometric complexity of deep neural networks,[0],[0]
pnl,6.3. Geometric complexity of deep neural networks,[0],[0]
"i q
linear regions.",6.3. Geometric complexity of deep neural networks,[0],[0]
"In particular, if d ď n1, . . .",6.3. Geometric complexity of deep neural networks,[0],[0]
", nL´1 ď n, the number of linear regions of ν is bounded by O ` ndpL´1q ˘ .
",6.3. Geometric complexity of deep neural networks,[0],[0]
Proof.,6.3. Geometric complexity of deep neural networks,[0],[0]
"If L “ 2, this follows directly from Lemma 6.2 and Corollary 3.4.",6.3. Geometric complexity of deep neural networks,[0],[0]
"The case of L ě 3 is in Section D.7 in the supplement.
",6.3. Geometric complexity of deep neural networks,[0],[0]
"As was pointed out in (Raghu et al., 2017), this upper bound closely matches the lower bound Ω ` pn{dqpL´1qdnd ˘ in (Montufar et al., 2014, Corollary 5) when n1 “ ¨ ¨ ¨ “ nL´1 “ n",6.3. Geometric complexity of deep neural networks,[0],[0]
ě d.,6.3. Geometric complexity of deep neural networks,[0],[0]
Hence we surmise that the number of linear regions of the neural network grows polynomially with the width n and exponentially with the number of layers L.,6.3. Geometric complexity of deep neural networks,[0],[0]
"We argue that feedforward neural networks with rectified linear units are, modulo trivialities, nothing more than tropical rational maps.",7. Conclusion,[0],[0]
"To understand them we often just need to understand the relevant tropical geometry.
",7. Conclusion,[0],[0]
"In this article, we took a first step to provide a proof-ofconcept: questions regarding decision boundaries, linear regions, how depth affect expressiveness, etc, can be translated into questions involving tropical hypersurfaces, dual subdivision of Newton polygon, polytopes constructed from zonotopes, etc.
",7. Conclusion,[0],[0]
"As a new branch of algebraic geometry, the novelty of tropical geometry stems from both the algebra and geometry as well as the interplay between them.",7. Conclusion,[0],[0]
It has connections to many other areas of mathematics.,7. Conclusion,[0],[0]
"Among other things, there is a tropical analogue of linear algebra (Butkovič, 2010) and a tropical analogue of convex geometry (Gaubert & Katz, 2006).",7. Conclusion,[0],[0]
We cannot emphasize enough that we have only touched on a small part of this rich subject.,7. Conclusion,[0],[0]
We hope that further investigation from this tropical angle might perhaps unravel other mysteries of deep neural networks.,7. Conclusion,[0],[0]
"The authors thank Ralph Morrison, Yang Qi, Bernd Sturmfels, and the anonymous referees for their very helpful comments.",Acknowledgments,[0],[0]
"The work in this article is generously supported by DARPA D15AP00109, NSF IIS 1546413, the Eckhardt Faculty Fund, and a DARPA Director’s Fellowship.",Acknowledgments,[0],[0]
"As in Section 2, we write xa “ xda; aside from this slight abuse of notation, ‘ and d denote tropical sum and product, ` and ¨ denote standard sum and product in all other contexts.",B. Tropical power,[0],[0]
"Tropical power evidently has the following properties:
• For x, y P R and a P R, a ě 0,
px‘ yqa “ xa ‘ ya and pxd yqa “ xa d ya.
",B. Tropical power,[0],[0]
"If a is allowed negative values, then we lose the first property.",B. Tropical power,[0],[0]
In general px‘ yqa ‰ xa ‘ ya for a ă 0.,B. Tropical power,[0],[0]
•,B. Tropical power,[0],[0]
"For x P R,
x0 “ 0.
•",B. Tropical power,[0],[0]
"For x P R and a, b P N, pxaqb “ xa¨b.
•",B. Tropical power,[0],[0]
"For x P R and a, b P Z, xa d xb “ xa`b.
•",B. Tropical power,[0],[0]
"For x P R and a, b P Z, xa ‘ xb “ xa d pxa´b ‘ 0q “ xa d p0‘ xa´bq.",B. Tropical power,[0],[0]
C.1.,C. Examples,[0],[0]
"Examples of tropical curves and dual subdivision of Newton polygon
Let f P Polp2, 1q “ Trx1, x2s, i.e., a bivariate tropical polynomial.",C. Examples,[0],[0]
"It follows from our discussions in Section 3 that the tropical hypersurface T pfq is a planar graph dual to the dual subdivision δpfq in the following sense:
(i)",C. Examples,[0],[0]
Each two-dimensional face in δpfq corresponds to a vertex in T pfq.,C. Examples,[0],[0]
(ii) Each one-dimensional edge of a face in δpfq corresponds to an edge in T pfq.,C. Examples,[0],[0]
"In particular, an edge from the Newton
polygon ∆pfq corresponds to an unbounded edge in T pfq while other edges correspond to bounded edges.
",C. Examples,[0],[0]
"Figure 2 illustrates how we may find the dual subdivision for the tropical polynomial fpx1, x2q",C. Examples,[0],[0]
“,C. Examples,[0],[0]
1d x21 ‘ 1d x22 ‘ 2d x1x2,C. Examples,[0],[0]
‘ 2d x1 ‘ 2d x2 ‘ 2.,C. Examples,[0],[0]
"First, find the convex hull
Ppfq “ Convtp2, 0, 1q, p0, 2, 1q, p1, 1, 2q, p1, 0, 2q, p0, 1, 2q, p0, 0, 2qu.
",C. Examples,[0],[0]
"Then, by projecting the upper envelope of Ppfq to R2, we obtain δpfq, the dual subdivision of the Newton polygon.
",C. Examples,[0],[0]
C.2.,C. Examples,[0],[0]
"Polytopes of a two-layer neural network
We illustrate our discussions in Section 6.2 with a two-layer example.",C. Examples,[0],[0]
"Let ν : R2 Ñ R be with n0 “ 2 input nodes, n1 “ 5 nodes in the first layer, and n2 “ 1 nodes in the output:
y “ νp1qpxq “ max
$
’ ’ ’ ’ &
’ ’ ’ ’",C. Examples,[0],[0]
"%
»
— — — — –
´1 1 1 ´3 1 2 ´4 1 3 2
fi
ffi ffi ffi ffi",C. Examples,[0],[0]
"fl „ x1 x2  `
»
— — — — –
1 ´1
2 0 ´2
fi
ffi ffi ffi ffi",C. Examples,[0],[0]
"fl , 0
,
/ / / / .
/ / / / -
,
νp2qpyq",C. Examples,[0],[0]
"“ maxty1 ` 2y2 ` y3 ´ y4 ´ 3y5, 0u.
We first express νp1q and νp2q as tropical rational maps,
νp1q “ F p1q mGp1q, νp2q “ f p2q m gp2q,
where
y :“ F p1qpxq “ Hp1qpxq ‘Gp1qpxq,
z :“ Gp1qpxq “
»
— — — — – x1 x32 0",C. Examples,[0],[0]
"x41 0
fi
ffi ffi ffi ffi",C. Examples,[0],[0]
"fl , Hp1qpxq “
»
— — — — –
1d x2 p´1q",C. Examples,[0],[0]
"d x1 2d x1x22
x2 p´2q",C. Examples,[0],[0]
"d x31x22
fi
ffi ffi ffi ffi",C. Examples,[0],[0]
"fl ,
and
f p2qpxq “ gp2qpxq ‘ hp2qpxq, gp2qpxq",C. Examples,[0],[0]
“ y4 d y35 d z1 d z22 d z3 “ px2 ‘ x41q d pp´2q d x31x22,C. Examples,[0],[0]
"‘ 0q3 d x1 d px32q2, hp2qpxq “ y1 d y22 d y3 d z4 d z35
“ p1d x2 ‘ x1q d pp´1q",C. Examples,[0],[0]
"d x1 ‘ x32q2 d p2d x1x22 ‘ 0q d x41.
",C. Examples,[0],[0]
"We will write F p1q “ pf p1q1 , . . .",C. Examples,[0],[0]
", f p1q 5 q and likewise for Gp1q and Hp1q.",C. Examples,[0],[0]
The monomials occurring in g p1q j pxq and h p1q,C. Examples,[0],[0]
j pxq are all of the form cxa11 x a2 2 .,C. Examples,[0],[0]
"Therefore Ppg p1q j q and Pph p1q j q, j “ 1, . . .",C. Examples,[0],[0]
", 5, are points in R3.
",C. Examples,[0],[0]
"Since F p1q “ Gp1q ‘Hp1q, Ppf p1qj q is a convex hull of two points, and thus a line segment in R3.",C. Examples,[0],[0]
"The Newton polygons associated with f p1qj , equal to their dual subdivisions in this case, are obtained by projecting these line segments back to the plane spanned by a1, a2, as shown on the left in Figure C.1.
",C. Examples,[0],[0]
"The line segments Ppf p1qj q, j “ 1, . . .",C. Examples,[0],[0]
", 5, and points Ppg p1q j q, j “ 1, . . .",C. Examples,[0],[0]
", 5, serve as building blocks for Pphp2qq and Ppgp2qq, which are constructed as weighted Minkowski sums:
Pphp2qq “ Ppf p1q4 q ` 3Ppf p1q 5 q ` Ppg p1q 1 q ` 2Ppg p1q 2 q",C. Examples,[0],[0]
"` Ppg p1q 3 q, Ppgp2qq “ Ppf p1q1 q ` 2Ppf p1q 2 q",C. Examples,[0],[0]
` Ppf p1q 3 q,C. Examples,[0],[0]
"` Ppg p1q 4 q ` 3Ppg p1q 5 q.
Ppgp2qq and the dual subdivision of its Newton polygon are shown on the right in Figure C.1.",C. Examples,[0],[0]
Pphp2qq and the dual subdivision of its Newton polygon are shown on the left in Figure C.2.,C. Examples,[0],[0]
Ppf p2qq is the convex hull of the union of Ppgp2qq and Pphp2qq.,C. Examples,[0],[0]
"The dual subdivision of its Newton polygon is obtained by projecting the upper faces of Ppf p2qq to the plane spanned by a1, a2.",C. Examples,[0],[0]
These are shown on the right in Figure C.2.,C. Examples,[0],[0]
Proof.,D.1. Proof of Corollary 3.4,[0],[0]
Let V1 and V2 be the sets of vertices on the upper and lower envelopes of P respectively.,D.1. Proof of Corollary 3.4,[0],[0]
"By Theorem 3.3, P has
n1 :“ 2 d ÿ j“0 pm",D.1. Proof of Corollary 3.4,[0],[0]
´,D.1. Proof of Corollary 3.4,[0],[0]
"1j q
vertices in total.",D.1. Proof of Corollary 3.4,[0],[0]
"By construction, we have |V1 Y V2| “ n1.",D.1. Proof of Corollary 3.4,[0],[0]
"It is well-known that zonotopes are centrally symmetric and so there are equal number of vertices on the upper and lower envelopes, i.e., |V1| “ |V2|.",D.1. Proof of Corollary 3.4,[0],[0]
Let P 1 :“ πpP q be the projection of P into Rd.,D.1. Proof of Corollary 3.4,[0],[0]
"Since the projected vertices are assumed to be in general positions, P 1 must be a d-dimensional zonotope generated by m nonparallel line segments.",D.1. Proof of Corollary 3.4,[0],[0]
"Hence, by Theorem 3.3 again, P 1 has
n2 :“ 2 d´1 ÿ j“0 pm",D.1. Proof of Corollary 3.4,[0],[0]
´,D.1. Proof of Corollary 3.4,[0],[0]
"1j q
vertices.",D.1. Proof of Corollary 3.4,[0],[0]
"For any vertex v P P , πpvq is a vertex of P 1 if and only if v belongs to both the upper and lower envelopes, i.e., v P V1 X V2.",D.1. Proof of Corollary 3.4,[0],[0]
Therefore the number of vertices on P 1 equals |V1 X V2|.,D.1. Proof of Corollary 3.4,[0],[0]
"By construction, we have |V1 X V2| “ n2.",D.1. Proof of Corollary 3.4,[0],[0]
"Consequently the number of vertices on the upper envelope is
|V1| “ 1
2 p|V1 Y V2| ´ |V1 X V2|q ` |V1 X V2| “
1 2 pn1 ´ n2q ` n2 “
d ÿ j“0 pmj q .",D.1. Proof of Corollary 3.4,[0],[0]
Proof.,D.2. Proof of Proposition 5.1,[0],[0]
"Writing A “ A` ´A´, we have
ρpl`1qpxq “ ` A` ´A´ ˘` F plqpxq ´Gplqpxq ˘ ` b “ ` A`F plqpxq `A´Gplqpxq ` b ˘ ´ ` A`G plqpxq `A´F plqpxq ˘
“ Hpl`1qpxq ´Gpl`1qpxq, νpl`1qpxq “ max ρpl`1qpyq, t (
“ max Hpl`1qpxq ´Gpl`1qpxq, t (
“ max Hpl`1qpxq, Gpl`1qpxq ` t ( ´Gpl`1qpxq “ F pl`1qpxq ´Gpl`1qpxq.",D.2. Proof of Proposition 5.1,[0],[0]
Proof.,D.3. Proof of Theorem 5.4,[0],[0]
It remains to establish the “only if” part.,D.3. Proof of Theorem 5.4,[0],[0]
"We will write σtpxq :“ maxtx, tu.",D.3. Proof of Theorem 5.4,[0],[0]
"Any tropical monomial bixαi is clearly such a neural network as
bix αi “ pσ´8 ˝ ρiqpxq “ maxtαTix` bi,´8u.
",D.3. Proof of Theorem 5.4,[0],[0]
"If two tropical polynomials p and q are represented as neural networks with lp and lq layers respectively,
ppxq “ ` σ´8 ˝ ρplpqp ˝ σ0 ˝ . . .",D.3. Proof of Theorem 5.4,[0],[0]
σ0 ˝,D.3. Proof of Theorem 5.4,[0],[0]
"ρp1qp ˘ pxq, qpxq “ `
σ´8 ˝ ρplqqq ˝ σ0 ˝ . . .",D.3. Proof of Theorem 5.4,[0],[0]
σ0 ˝,D.3. Proof of Theorem 5.4,[0],[0]
"ρp1qq ˘ pxq,
then pp‘ qqpxq “ maxtppxq, qpxqu can also be written as a neural network with maxtlp, lqu ` 1 layers:
pp‘ qqpxq “ σ´8 ` rσ0 ˝ ρ1spypxqq ` rσ0 ˝ ρ2spypxqq ´ rσ0 ˝ ρ3spypxqq ˘ ,
where y : Rd Ñ R2 is given by ypxq “ pppxq, qpxqq and ρi : R2 Ñ R, i “ 1, 2, 3, are linear functions defined by
ρ1pyq",D.3. Proof of Theorem 5.4,[0],[0]
"“ y1 ´ y2, ρ2pyq “ y2, ρ3pyq",D.3. Proof of Theorem 5.4,[0],[0]
"“ ´y2.
",D.3. Proof of Theorem 5.4,[0],[0]
"Thus, by induction, any tropical polynomial can be written as a neural network with ReLU activation.",D.3. Proof of Theorem 5.4,[0],[0]
"Observe also that if a tropical polynomial is the tropical sum of r monomials, then it can be written as a neural network with no more than rlog2 rs` 1 layers.
",D.3. Proof of Theorem 5.4,[0],[0]
Next we consider a tropical rational function ppm qqpxq “ ppxq ´ qpxq where p and q are tropical polynomials.,D.3. Proof of Theorem 5.4,[0],[0]
"Under the same assumptions, we can represent pm q",D.3. Proof of Theorem 5.4,[0],[0]
"as
ppm qqpxq “ σ´8 ` rσ0 ˝ ρ4spypxqq ´ rσ0 ˝ ρ5spypxqq ` rσ0 ˝ ρ6spypxqq ´ rσ0 ˝ ρ7spypxqq ˘
where ρi : R2 Ñ R2, i “ 4, 5, 6, 7, are linear functions defined by
ρ4pyq “ y1, ρ5pyq",D.3. Proof of Theorem 5.4,[0],[0]
"“ ´y1, ρ6pyq",D.3. Proof of Theorem 5.4,[0],[0]
"“ ´y2, ρ7pyq “ y2.
",D.3. Proof of Theorem 5.4,[0],[0]
"Therefore pm q is also a neural network with at most maxtlp, lqu ` 1 layers.
",D.3. Proof of Theorem 5.4,[0],[0]
"Finally, if f and g are tropical polynomials that are respectively tropical sums of rf and rg monomials, then the discussions above show that pf m gqpxq “ fpxq ´ gpxq is a neural network with at most maxtrlog2 rf s, rlog2 rgsu ` 2 layers.",D.3. Proof of Theorem 5.4,[0],[0]
Proof.,D.4. Proof of Proposition 5.5,[0],[0]
It remains to establish the “if” part.,D.4. Proof of Proposition 5.5,[0],[0]
"Let Rd be divided into N polyhedral region on each of which ν restricts to a linear function `ipxq “ aTix` bi, ai P Zd, bi P R, i “ 1, . . .",D.4. Proof of Proposition 5.5,[0],[0]
", L, i.e., for any x P Rd, νpxq “ `ipxq for some i P t1, . . .",D.4. Proof of Proposition 5.5,[0],[0]
", Lu.",D.4. Proof of Proposition 5.5,[0],[0]
"It follows from (Tarela & Martinez, 1999) that we can find N subsets of t1, . . .",D.4. Proof of Proposition 5.5,[0],[0]
", Lu, denoted by Sj , j “ 1, . . .",D.4. Proof of Proposition 5.5,[0],[0]
", N , so that ν has a representation
νpxq “ max j“1,...,N min iPSj `i.
It is clear that each `i is a tropical rational function.",D.4. Proof of Proposition 5.5,[0],[0]
"Now for any tropical rational functions p and q,
mintp, qu “ ´maxt´p,´qu “ 0m rp0m pq ‘ p0m qqs “ rpd qs m rp‘ qs.
",D.4. Proof of Proposition 5.5,[0],[0]
"Since pd q and p‘ q are both tropical rational functions, so is their tropical quotient.",D.4. Proof of Proposition 5.5,[0],[0]
"By induction, miniPSj `i is a tropical rational function for any j “ 1, . . .",D.4. Proof of Proposition 5.5,[0],[0]
", N , and therefore so is their tropical sum ν.",D.4. Proof of Proposition 5.5,[0],[0]
Proof.,D.5. Proof of Proposition 5.6,[0],[0]
"For a one-layer neural network νpxq “ maxtAx ` b, tu “ pν1pxq, . . .",D.5. Proof of Proposition 5.6,[0],[0]
", νppxqq with A P Rpˆd, b P Rp, x P Rd, t P pRY t´8uqp, we have
νkpxq “ ˆ",D.5. Proof of Proposition 5.6,[0],[0]
bk,D.5. Proof of Proposition 5.6,[0],[0]
"d d ä
j“1 x akj j
˙ ‘ tk “ ˆ bk",D.5. Proof of Proposition 5.6,[0],[0]
"d d ä
j“1 x akj j
˙ ‘ ˆ tk d d ä
j“1 x0j
˙
, k “ 1, . . .",D.5. Proof of Proposition 5.6,[0],[0]
", p.
",D.5. Proof of Proposition 5.6,[0],[0]
"So for any k “ 1, . .",D.5. Proof of Proposition 5.6,[0],[0]
.,D.5. Proof of Proposition 5.6,[0],[0]
", p, if we write b̄1 “ bk, b̄2 “ tk, ā1j “ akj , ā2j “ 0, j “ 1, . . .",D.5. Proof of Proposition 5.6,[0],[0]
", d, then
νkpxq “ 2 à
i“1 b̄i
d ä j“1",D.5. Proof of Proposition 5.6,[0],[0]
"x āij j
is clearly a tropical signomial function.",D.5. Proof of Proposition 5.6,[0],[0]
Therefore ν is a tropical signomial map.,D.5. Proof of Proposition 5.6,[0],[0]
"The result for arbitrary number of layers then follows from using the same recurrence as in the proof in Section D.2, except that now the entries in the weight matrix are allowed to take real values, and the maps Hplqpxq, Gplqpxq, F plqpxq are tropical signomial maps.",D.5. Proof of Proposition 5.6,[0],[0]
Hence every layer can be written as a tropical rational signomial map νplq “ F plq mGplq.,D.5. Proof of Proposition 5.6,[0],[0]
"We prove a slightly more general result.
",D.6. Proof of Proposition 6.1,[0],[0]
Proposition D.1 (Level sets).,D.6. Proof of Proposition 6.1,[0],[0]
"Let f m g P Ratpd, 1q “ Tpx1, . . .",D.6. Proof of Proposition 6.1,[0],[0]
", xdq.
(i)",D.6. Proof of Proposition 6.1,[0],[0]
"Given a constant c ą 0, the level set B :“ tx P Rd : fpxq m gpxq “ cu
divides Rd into at most N pfq connected polyhedral regions where fpxq m gpxq ą c, and at most N pgq such regions where fpxq m gpxq ă c.
(ii)",D.6. Proof of Proposition 6.1,[0],[0]
"If c P R is such that there is no tropical monomial in fpxq that differs from any tropical monomial in gpxq by c, then the level set B is contained in a tropical hypersurface,
B Ď T pmaxtfpxq, gpxq ` cuq “ T pcd g ‘ fq.
",D.6. Proof of Proposition 6.1,[0],[0]
Proof.,D.6. Proof of Proposition 6.1,[0],[0]
"We show that the bounds on the numbers of connected positive (i.e., above c) and negative (i.e., below c) regions are as we claimed in (i).",D.6. Proof of Proposition 6.1,[0],[0]
"The tropical hypersurface of f divides Rd into N pfq convex regions C1, . . .",D.6. Proof of Proposition 6.1,[0],[0]
", CN pfq such that f is linear on each Ci.",D.6. Proof of Proposition 6.1,[0],[0]
"As g is piecewise linear and convex over Rd, f m g “ f ´ g is piecewise linear and concave on each Ci.",D.6. Proof of Proposition 6.1,[0],[0]
Since the level set tx : fpxq ´ gpxq “ cu and the superlevel set tx : fpxq ´ gpxq ě cu must be convex by the concavity of f,D.6. Proof of Proposition 6.1,[0],[0]
"´ g, there is at most one positive region in each Ci.",D.6. Proof of Proposition 6.1,[0],[0]
Therefore the total number of connected positive regions cannot exceed N pfq.,D.6. Proof of Proposition 6.1,[0],[0]
"Likewise, the tropical hypersurface of g divides Rd into N pgq convex regions on each of which f m g is convex.",D.6. Proof of Proposition 6.1,[0],[0]
"The same argument shows that the number of connected negative regions does not exceed N pgq.
",D.6. Proof of Proposition 6.1,[0],[0]
We next address (ii).,D.6. Proof of Proposition 6.1,[0],[0]
"Upon rearranging terms, the level set becomes
B “ x P Rd : fpxq “ gpxq ` c ( .
",D.6. Proof of Proposition 6.1,[0],[0]
"Since fpxq and gpxq ` c are both tropical polynomial, we have
fpxq “ b1xα1 ‘ ¨ ¨ ¨ ‘ brxαr , gpxq ` c “ c1xβ1 ‘ ¨ ¨ ¨ ‘ csxβs ,
with appropriate multiindices α1, . . .",D.6. Proof of Proposition 6.1,[0],[0]
", αr, β1, . . .",D.6. Proof of Proposition 6.1,[0],[0]
", βs, and real coefficients b1, . . .",D.6. Proof of Proposition 6.1,[0],[0]
", br, c1, . . .",D.6. Proof of Proposition 6.1,[0],[0]
", cs.",D.6. Proof of Proposition 6.1,[0],[0]
"By the assumption on the monomials, we have that x0 P B only if there exist i, j so that αi ‰ βj",D.6. Proof of Proposition 6.1,[0],[0]
and bixαi0 “ cjx βj 0 .,D.6. Proof of Proposition 6.1,[0],[0]
"This completes the proof since if we combine the monomials of fpxq and gpxq ` c by (tropical) summing them into a single tropical polynomial, maxtfpxq, gpxq ` cu, the above implies that on the level set, the value of the combined tropical polynomial is attained by at least two monomials and therefore x0 P T pmaxtfpxq, gpxq ` cuq.
",D.6. Proof of Proposition 6.1,[0],[0]
Proposition 6.1 follows immediately from Proposition D.1 since the decision boundary tx P Rd : νpxq “ s´1pcqu is a level set of the tropical rational function ν.,D.6. Proof of Proposition 6.1,[0],[0]
"The linear regions of a tropical polynomial map F P Polpd,mq are all convex but this is not necessarily the case for a tropical rational map F P Ratpd, nq.",D.7. Proof of Theorem 6.3,[0],[0]
"Take for example a bivariate real-valued function fpx, yq whose graph in R3 is a pyramid with base tpx, yq P R2 : x, y P r´1, 1su and zero everywhere else, then the linear region where f vanishes is R2ztpx, yq P R2 : x,",D.7. Proof of Theorem 6.3,[0],[0]
"y P r´1, 1su, which is nonconvex.",D.7. Proof of Theorem 6.3,[0],[0]
The nonconvexity invalidates certain geometric arguments that only apply in the convex setting.,D.7. Proof of Theorem 6.3,[0],[0]
Nevertheless there is a way to subdivide each of the nonconvex linear regions into convex ones to get ourselves back into the convex setting.,D.7. Proof of Theorem 6.3,[0],[0]
"We will start with the number of convex linear regions for tropical rational maps although later we will deduce the required results for the number of linear regions (without imposing convexity).
",D.7. Proof of Theorem 6.3,[0],[0]
We first extend the notion of tropical hypersurface to tropical rational maps:,D.7. Proof of Theorem 6.3,[0],[0]
"Given a tropical rational map F P Ratpd,mq, we define T pF q to be the boundaries between adjacent linear regions.",D.7. Proof of Theorem 6.3,[0],[0]
"When F “ pf1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", fmq P Polpd,mq, i.e., a tropical polynomial map, this set is exactly the union of tropical hypersurfaces T pfiq, i “ 1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
",m. Therefore this definition of T pF q extends Definition 3.1.
",D.7. Proof of Theorem 6.3,[0],[0]
"For a tropical rational map F , we will examine the smallest number of convex regions that form a refinement of T pF q. For brevity, we will call this the convex degree of F ; for consistency, the number of linear regions of F we will call its linear degree.",D.7. Proof of Theorem 6.3,[0],[0]
We define convex degree formally below.,D.7. Proof of Theorem 6.3,[0],[0]
We will write F |C to mean the restriction of map F to C Ď Rd.,D.7. Proof of Theorem 6.3,[0],[0]
Definition D.1.,D.7. Proof of Theorem 6.3,[0],[0]
"The convex degree of a tropical rational map F P Ratpd, nq is the minimum division of Rd into convex regions over which F is linear, i.e.
NcpF q",D.7. Proof of Theorem 6.3,[0],[0]
":“ min n : C1 Y ¨ ¨ ¨ Y Cn “ Rd, Ci convex, F |Ci linear ( .
",D.7. Proof of Theorem 6.3,[0],[0]
"Note that C1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", CNcpF q either divide Rd into the same regions as T pF q or form a refinement.
",D.7. Proof of Theorem 6.3,[0],[0]
"For m ď d, we will denote by NcpF",D.7. Proof of Theorem 6.3,[0],[0]
"| mq the maximum convex degree obtained by restricting F to an m-dimensional affine subspace in Rd, i.e.,
NcpF",D.7. Proof of Theorem 6.3,[0],[0]
"| mq :“ max NcpF |Ωq : Ω Ď Rd is an m-dimensional affine space ( .
",D.7. Proof of Theorem 6.3,[0],[0]
"For any F P Ratpd, nq, there is at least one tropical polynomial map that subdivides T pF q, and so convex degree is welldefined (e.g., if F “ pp1 m q1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", pn m qnq P Ratpd, nq, then we may choose P “ pp1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", pn, q1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", qnq P Polpd, 2nq).",D.7. Proof of Theorem 6.3,[0],[0]
"Since the linear regions of a tropical polynomial map are always convex, we have N pF q",D.7. Proof of Theorem 6.3,[0],[0]
"“ NcpF q for any F P Polpd, nq.
Let F “ pf1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", fnq P Ratpd, nq and α “ pa1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", anq P Zn.",D.7. Proof of Theorem 6.3,[0],[0]
"Consider the tropical rational function3
Fα :“ αTF “ a1f1 ` ¨ ¨ ¨ ` anfn “ n ä
j“1 f aj j P Ratpd, 1q.
",D.7. Proof of Theorem 6.3,[0],[0]
"For some α, Fα may have fewer linear regions than F , e.g, α “ p0, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", 0q.",D.7. Proof of Theorem 6.3,[0],[0]
"As such, we need the following notion.",D.7. Proof of Theorem 6.3,[0],[0]
Definition D.2.,D.7. Proof of Theorem 6.3,[0],[0]
"α “ pa1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", anq P Zn is said to be a general exponent of F P Ratpd, nq if the linear regions of Fα and the linear regions of F are identical.
",D.7. Proof of Theorem 6.3,[0],[0]
"We show that general exponent always exists for any F P Ratpd, nq and may be chosen to have all entries nonnegative.",D.7. Proof of Theorem 6.3,[0],[0]
Lemma D.2.,D.7. Proof of Theorem 6.3,[0],[0]
"Let F P Ratpd, nq.",D.7. Proof of Theorem 6.3,[0],[0]
"Then
(i) N pFαq “ N pF q if and only if α is a general exponent; (ii) F has a general exponent α P Nn.
Proof.",D.7. Proof of Theorem 6.3,[0],[0]
It follows from the definition of tropical hypersuface that T pFαq and T pF q comprise respectively the points x P Rd at which Fα and F are not differentiable.,D.7. Proof of Theorem 6.3,[0],[0]
"Hence T pFαq Ď T pF q, which implies that N pFαq ă N pF q unless T pFαq “ T pF q.",D.7. Proof of Theorem 6.3,[0],[0]
"This concludes (i).
",D.7. Proof of Theorem 6.3,[0],[0]
"For (ii), we need to show that there always exists an α P Nn such that Fα divides its domain Rd into the same set of linear regions as F .",D.7. Proof of Theorem 6.3,[0],[0]
"In other words, for every pair of adjacent linear regions of F , the pd ´ 1q-dimensional face in T pF q that separates them is also present in T pFαq and so T pFαq Ě T pF q.
Let L and M be adjacent linear regions of F .",D.7. Proof of Theorem 6.3,[0],[0]
"The differentials of F |L and F |M must have integer coordinates, i.e., dF |L, dF |M P Znˆd.",D.7. Proof of Theorem 6.3,[0],[0]
"Since L and M are distinct linear regions, we must have dF |L ‰ dF |M (or otherwise L and M can be merged into a single linear region).",D.7. Proof of Theorem 6.3,[0],[0]
"Note that the differentials of Fα|L and Fα|M are given by αTdF |L and αTdF |M .
",D.7. Proof of Theorem 6.3,[0],[0]
"To ensure the pd´ 1q-dimensional face separating L and M still exists in T pFαq, we need to choose α so that αTdF |L ‰ αTdF |M .",D.7. Proof of Theorem 6.3,[0],[0]
Observe that the solution to pdF |L,D.7. Proof of Theorem 6.3,[0],[0]
"´ dF |M qTα “ 0 is contained in a one-dimensional subspace of Rn.
Let ApF q be the collection of all pairs of adjacent linear regions of F .",D.7. Proof of Theorem 6.3,[0],[0]
"Since the set of α that degenerates two adjacent linear regions into a single one, i.e.,
S :“ ď
pL,MqPApF q
α P Nn : pdF |L ´ dF |M",D.7. Proof of Theorem 6.3,[0],[0]
q,D.7. Proof of Theorem 6.3,[0],[0]
"Tα “ 0q
(
,
is contained in a union of a finite number of hyperplanes in Rn, S cannot cover the entire lattice of nonnegative integers",D.7. Proof of Theorem 6.3,[0],[0]
Nn.,D.7. Proof of Theorem 6.3,[0],[0]
"Therefore the set Nn X pRnzSq is nonempty and any of its element is a general exponent for F .
",D.7. Proof of Theorem 6.3,[0],[0]
"Lemma D.2 shows that we may study the linear degree of a tropical rational map by studying that of a tropical rational function, for which the results in Section 3.1 apply.
",D.7. Proof of Theorem 6.3,[0],[0]
"We are now ready to prove a key result on the convex degree of composition of tropical rational maps.
",D.7. Proof of Theorem 6.3,[0],[0]
Theorem D.3.,D.7. Proof of Theorem 6.3,[0],[0]
"Let F “ pf1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", fmq P Ratpn,mq and G P Ratpd, nq.",D.7. Proof of Theorem 6.3,[0],[0]
"Define H “ ph1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", hmq P Ratpd,mq by
hi :“ fi ˝G, i “ 1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
",m.
Then N pHq ď NcpHq ď NcpF | dq ¨NcpGq.
",D.7. Proof of Theorem 6.3,[0],[0]
Proof.,D.7. Proof of Theorem 6.3,[0],[0]
Only the upper bound requires a proof.,D.7. Proof of Theorem 6.3,[0],[0]
Let k “ NcpGq.,D.7. Proof of Theorem 6.3,[0],[0]
"By the definition of NcpGq, there exist convex sets C1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", Ck Ď Rd whose union is Rd and on each of which G is linear.",D.7. Proof of Theorem 6.3,[0],[0]
So G|Ci is some affine function ρi.,D.7. Proof of Theorem 6.3,[0],[0]
"For any i,
NcpF ˝ ρiq ď",D.7. Proof of Theorem 6.3,[0],[0]
"NcpF | dq, 3This is in the sense of a tropical power but we stay consistent to our slight abuse of notation and write Fα instead of Fdα.
by the definition of NcpF | dq.",D.7. Proof of Theorem 6.3,[0],[0]
Since F ˝G,D.7. Proof of Theorem 6.3,[0],[0]
"“ F ˝ ρi on Ci, we have
NcpF ˝Gq ď",D.7. Proof of Theorem 6.3,[0],[0]
"k ÿ
i“1 NcpF ˝",D.7. Proof of Theorem 6.3,[0],[0]
"ρiq.
",D.7. Proof of Theorem 6.3,[0],[0]
"Hence
NcpF ˝Gq ď k ÿ
i“1 NcpF ˝ ρiq ď
k ÿ",D.7. Proof of Theorem 6.3,[0],[0]
i“1 NcpF,D.7. Proof of Theorem 6.3,[0],[0]
|,D.7. Proof of Theorem 6.3,[0],[0]
dq,D.7. Proof of Theorem 6.3,[0],[0]
"“ NcpF | dq ¨NcpGq.
",D.7. Proof of Theorem 6.3,[0],[0]
We now apply our observations on tropical rational functions to neural networks.,D.7. Proof of Theorem 6.3,[0],[0]
"The next lemma follows directly from Corollary 3.4.
",D.7. Proof of Theorem 6.3,[0],[0]
Lemma D.4.,D.7. Proof of Theorem 6.3,[0],[0]
Let σplq ˝ ρplq :,D.7. Proof of Theorem 6.3,[0],[0]
Rnl´1 Ñ Rnl where σplq and ρplq are the affine transformation and activation of the lth layer of a neural network.,D.7. Proof of Theorem 6.3,[0],[0]
"If d ď nl, then
Ncpσplq ˝ ρplq | dq ď d ÿ",D.7. Proof of Theorem 6.3,[0],[0]
i“0 pnl,D.7. Proof of Theorem 6.3,[0],[0]
"i q .
",D.7. Proof of Theorem 6.3,[0],[0]
Proof.,D.7. Proof of Theorem 6.3,[0],[0]
"Ncpσplq ˝ ρplq | dq is the maximum convex degree of a tropical rational map F “ pf1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", fnlq : Rd Ñ Rnl of the form
fipxq :“ σplqi ˝ ρ",D.7. Proof of Theorem 6.3,[0],[0]
plq,D.7. Proof of Theorem 6.3,[0],[0]
˝,D.7. Proof of Theorem 6.3,[0],[0]
pb1,D.7. Proof of Theorem 6.3,[0],[0]
"d xα1 , . . .",D.7. Proof of Theorem 6.3,[0],[0]
", bnl´1 d x αnl´1 q, i “ 1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", nl.
",D.7. Proof of Theorem 6.3,[0],[0]
"For a general affine transformation ρplq,
ρplqpb1 d xα1 , . . .",D.7. Proof of Theorem 6.3,[0],[0]
", bnl´1 d x αnl´1 q “ ` b11 d xα 1 1 , . . .",D.7. Proof of Theorem 6.3,[0],[0]
", b1nl d x α1nl ˘ “: Gpxq
for some α11, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", α 1 nl and b11, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", b 1 nl , and we denote this map by G : Rd Ñ Rnl .",D.7. Proof of Theorem 6.3,[0],[0]
"So fi “ σplqi ˝G. By Theorem D.3, we have Ncpσplq ˝ ρplq | dq “ Ncpσplq | dq ¨NcpGq “ Ncpσplq | dq; note that NcpGq “ 1 as G is a linear function.
",D.7. Proof of Theorem 6.3,[0],[0]
"We have thus reduced the problem to determining a bound on the convex degree of a single layer neural network with nl nodes ν “ pν1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", νnlq :",D.7. Proof of Theorem 6.3,[0],[0]
Rd Ñ Rnl .,D.7. Proof of Theorem 6.3,[0],[0]
"Let γ “ pc1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", cnlq P Nnl be a nonnegative general exponent for ν.",D.7. Proof of Theorem 6.3,[0],[0]
"Note that
nl ä j“1",D.7. Proof of Theorem 6.3,[0],[0]
ν,D.7. Proof of Theorem 6.3,[0],[0]
cj,D.7. Proof of Theorem 6.3,[0],[0]
j,D.7. Proof of Theorem 6.3,[0],[0]
“ nl ä j“1,D.7. Proof of Theorem 6.3,[0],[0]
„ˆ d ä i“1 bi d xa ` ji ˙ ‘ ˆ d ä i“1 xa ´ ji,D.7. Proof of Theorem 6.3,[0],[0]
˙ d tj cj ´,D.7. Proof of Theorem 6.3,[0],[0]
nl ä j“1,D.7. Proof of Theorem 6.3,[0],[0]
ˆ d ä i“1,D.7. Proof of Theorem 6.3,[0],[0]
xa,D.7. Proof of Theorem 6.3,[0],[0]
"´ ji ˙cj .
",D.7. Proof of Theorem 6.3,[0],[0]
"Since the last term is linear in x, we may drop it without affecting the convex degree of the entire expression.",D.7. Proof of Theorem 6.3,[0],[0]
"It remains to determine an upper bound for the number of linear regions of the tropical polynomial
hpxq “ nl ä
j“1
„ˆ d ä
i“1 bi d xa
` ji
˙ ‘ ˆ d ä
i“1 xa ´",D.7. Proof of Theorem 6.3,[0],[0]
"ji
˙ d tj cj ,
which we will obtain by counting vertices of the polytope Pphq.",D.7. Proof of Theorem 6.3,[0],[0]
"By Propositions 3.1 and 3.2 the polytope Pphq is given by a weighted Minkowski sum
nl ÿ j“1 cjP",D.7. Proof of Theorem 6.3,[0],[0]
"„ˆ d ä i“1 bi d xa ` ji ˙ ‘ ˆ d ä i“1 xa ´ ji ˙ d tj  .
",D.7. Proof of Theorem 6.3,[0],[0]
"By Proposition 3.2 again,
P „ˆ d ä
i“1 bi d xa
` ji
˙ ‘ ˆ d ä
i“1 xa ´ ji
˙ d tj  “ Conv ` VpPpfqq Y VpPpgqq ˘
where
fpxq “ d ä
i“1 bi d xa
` ji and gpxq “
ˆ",D.7. Proof of Theorem 6.3,[0],[0]
"d ä
i“1 xa ´ ji
˙
d tj
are tropical monomials.",D.7. Proof of Theorem 6.3,[0],[0]
"Therefore Ppfq, Ppgq are just points in Rd`1 and Conv ` VpPpfqq Y VpPpgqq ˘ is a line in Rd`1.",D.7. Proof of Theorem 6.3,[0],[0]
"Hence Pphq is a Minkowski sum of nl line segments in Rd`1, i.e., a zonotope, and Corollary 3.4 completes the proof.
",D.7. Proof of Theorem 6.3,[0],[0]
"Using Lemma D.4, we obtain a bound on the number of linear regions created by one layer of a neural network.
",D.7. Proof of Theorem 6.3,[0],[0]
Theorem D.5.,D.7. Proof of Theorem 6.3,[0],[0]
Let ν :,D.7. Proof of Theorem 6.3,[0],[0]
"Rd Ñ RnL be an L-layer neural network satisfying assumptions (a)–(c) with F plq, Gplq,Hplq, and νplq as defined in Proposition 5.1.",D.7. Proof of Theorem 6.3,[0],[0]
"Let nl ě d for all l “ 1, . . .",D.7. Proof of Theorem 6.3,[0],[0]
", L. Then
Ncpνp1qq “ N pGp1qq",D.7. Proof of Theorem 6.3,[0],[0]
"“ N pHp1qq “ 1, Ncpνpl`1qq ď Ncpνplqq ¨ d ÿ i“0 pnl`1",D.7. Proof of Theorem 6.3,[0],[0]
"i q .
",D.7. Proof of Theorem 6.3,[0],[0]
Proof.,D.7. Proof of Theorem 6.3,[0],[0]
"The l “ 1 case follows from the fact that Gp1qpxq “ Ap1q´ x and Hp1qpxq “ A p1q ` x` bp1q are both linear, which in turn forces Ncpνp1qq “ 1 as in the proof of Lemma D.4.",D.7. Proof of Theorem 6.3,[0],[0]
"Since νplq “ pσplq ˝ ρplqq ˝ νpl´1q, the recursive bound follows from Theorem D.3 and Lemma D.4.
",D.7. Proof of Theorem 6.3,[0],[0]
Theorem 6.3 follows from applying Theorem D.5 recursively.,D.7. Proof of Theorem 6.3,[0],[0]
"We establish, for the first time, connections between feedforward neural networks with ReLU activation and tropical geometry — we show that the family of such neural networks is equivalent to the family of tropical rational maps.",abstractText,[0],[0]
"Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks for deeper networks; we relate decision boundaries of such neural networks to tropical hypersurfaces, a major object of study in tropical geometry; and we prove that linear regions of such neural networks correspond to vertices of polytopes associated with tropical rational functions.",abstractText,[0],[0]
An insight from our tropical formulation is that a deeper network is exponentially more expressive than a shallow network.,abstractText,[0],[0]
Tropical Geometry of Deep Neural Networks,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2931–2937 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
We present an analytic study on the language of news media in the context of political fact-checking and fake news detection. We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale. Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.",text,[0],[0]
Words in news media and political discourse have a considerable power in shaping people’s beliefs and opinions.,1 Introduction,[0],[0]
"As a result, their truthfulness is often compromised to maximize impact.",1 Introduction,[0],[0]
"Recently, fake news has captured worldwide interest, and the number of organized efforts dedicated solely to fact-checking has almost tripled since 2014.1 Organizations, such as PolitiFact.com, actively investigate and rate the veracity of comments made by public figures, journalists, and organizations.
",1 Introduction,[0],[0]
Figure 1 shows example quotes rated for truthfulness by PolitiFact.,1 Introduction,[0],[0]
"Per their analysis, one component of the two statements’ ratings is the misleading phrasing (bolded in green in the figure).",1 Introduction,[0],[0]
"For instance, in the first example, the statement is true as stated, though only because the speaker hedged their meaning with the quantifier just.",1 Introduction,[0],[0]
"In the second example, two correlated events – Brexit
1https://www.poynter.org/2017/there-are-now-114-factchecking-initiatives-in-47-countries/450477/
“By declaring that Pluto was no longer a planet, the (International Astronomical Union) put into place a planetary definition that would have even declassified Earth as a planet if it existed as far from the sun as Pluto does.”
",1 Introduction,[0],[0]
"Half TrueTrue False
-Rated Half True by PunditFact, (July 2015)
and Google search trends – are presented ambiguously as if they were directly linked.
",1 Introduction,[0],[0]
"Importantly, like above examples, most factchecked statements on PolitiFact are rated as neither entirely true nor entirely false.",1 Introduction,[0],[0]
"Analysis indicates that falsehoods often arise from subtle differences in phrasing rather than outright fabrication (Rubin et al., 2015).",1 Introduction,[0],[0]
"Compared to most prior work on deception literature that focused on binary categorization of truth and deception, political fact-checking poses a new challenge as it involves a graded notion of truthfulness.
",1 Introduction,[0],[0]
"While political fact-checking generally focuses on examining the accuracy of a single quoted statement by a public figure, the reliability of general news stories is also a concern (Connolly et al., 2016; Perrott, 2016).",1 Introduction,[0],[0]
"Figure 2 illustrates news types categorized along two dimensions: the intent of the authors (desire to deceive) and the content of the articles (true, mixed, false).
",1 Introduction,[0],[0]
"2931
In this paper, we present an analytic study characterizing the language of political quotes and news media written with varying intents and degrees of truth.",1 Introduction,[0],[0]
"We also investigate graded deception detection, determining the truthfulness on a 6-point scale using the political fact-checking database available at PolitiFact.2",1 Introduction,[0],[0]
"News Corpus with Varying Reliability To analyze linguistic patterns across different types of articles, we sampled standard trusted news articles from the English Gigaword corpus and crawled articles from seven different unreliable news sites of differing types.",2 Fake News Analysis,[0],[0]
"Table 1 displays sources identified under each type according to US News & World Report.3 These news types include: • Satire: mimics real news but still cues the reader
that it is not meant to be taken seriously • Hoax: convinces readers of the validity of a
paranoia-fueled story • Propaganda: misleads readers so that they be-
lieve a particular political/social agenda Unlike hoaxes and propaganda, satire is intended to be notably different from real news so that audiences will recognize the humorous intent.",2 Fake News Analysis,[0],[0]
"Hoaxes and satire are more likely to invent stories, while propaganda frequently combines truths, falsehoods, and ambiguities to confound readers.
",2 Fake News Analysis,[0],[0]
"To characterize differences between news types, we applied various lexical resources to trusted and fake news articles.",2 Fake News Analysis,[0],[0]
We draw lexical resources from prior works in communication theory and stylistic analysis in computational linguistics.,2 Fake News Analysis,[0],[0]
"We tokenize
2All resources created for this paper including corpus of news articles from unreliable sources, collection of Politifact ratings, and compiled Wiktionary lexicons have been made publicly available at homes.cs.washington. edu/˜hrashkin/factcheck.html
3www.usnews.com/news/national-news/articles/2016-1114/avoid-these-fake-news-sites-at-all-costs
the text with NLTK (Bird et al., 2009) and compute per-document count for each lexicon, and report averages per article of each type.
",2 Fake News Analysis,[0],[0]
"First among these lexicons is the Linguistic Inquiry and Word Count (LIWC), a lexicon widely used in social science studies (Pennebaker et al., 2015).",2 Fake News Analysis,[0],[0]
"In addition, we estimate the use of strongly and weakly subjective words with a sentiment lexicon (Wilson et al., 2005).",2 Fake News Analysis,[0],[0]
Subjective words can be used to dramatize or sensationalize a news story.,2 Fake News Analysis,[0],[0]
"We also use lexicons for hedging from (Hyland, 2015) because hedging can indicate vague, obscuring language.",2 Fake News Analysis,[0],[0]
"Lastly, we introduce intensifying lexicons that we crawled from Wiktionary based on a hypothesis that fake news articles try to enliven stories to attract readers.",2 Fake News Analysis,[0],[0]
"We compiled five lists from Wiktionary of words that imply a degree a dramatization (comparatives, superlatives, action adverbs, manner adverbs, and modal adverbs) and measured their presence.
",2 Fake News Analysis,[0],[0]
Discussion Table 2 summarizes the ratio of averages between unreliable news and truthful news for a handful of the measured features.,2 Fake News Analysis,[0],[0]
"Ratios greater than one denote features more prominent in fake news, and ratios less than one denote features more prominent in truthful news.",2 Fake News Analysis,[0],[0]
"The ratios between unreliable/reliable news reported are statistically significant (p < 0.01) with Welsch t-test after Bonferroni correction.
",2 Fake News Analysis,[0],[0]
Our results show that first-person and secondperson pronouns are used more in less reliable or deceptive news types.,2 Fake News Analysis,[0],[0]
"This contrasts studies in other domains (Newman et al., 2003), which found fewer self-references in people telling lies about their personal opinions.",2 Fake News Analysis,[0],[0]
"Unlike that domain, news writers are trying to appear indifferent.",2 Fake News Analysis,[0],[0]
"Editors at trustworthy sources are possibly more
rigorous about removing language that seems too personal, which is one reason why this result differs from other lie detection domains.",2 Fake News Analysis,[0],[0]
"This finding instead corroborates previous work in written domains found by Ott et al. (2011) and Rayson et al. (2001), who found that such pronouns were indicative of imaginative writing.",2 Fake News Analysis,[0],[0]
"Perhaps imaginative storytelling domains is a closer match to detecting unreliable news than lie detection on opinions.
",2 Fake News Analysis,[0],[0]
"Our results also show that words that can be used to exaggerate – subjectives, superlatives, and modal adverbs – are all used more by fake news.",2 Fake News Analysis,[0],[0]
"Words used to offer concrete figures – comparatives, money, and numbers – appear more in truthful news.",2 Fake News Analysis,[0],[0]
"This also builds on previous findings by Ott et al. (2011) on the difference between superlative/comparative usage.
",2 Fake News Analysis,[0],[0]
"Trusted sources are more likely to use assertive words and less likely to use hedging words, indicating that they are less vague about describing events, as well.",2 Fake News Analysis,[0],[0]
"This relates to psychology theories (Buller and Burgoon, 1996) that deceivers show more “uncertainty and vagueness” and “indirect forms of expression”.",2 Fake News Analysis,[0],[0]
"Similarly, the trusted sources use the hear category words more often, possibly indicating that they are citing primary sources more often.
",2 Fake News Analysis,[0],[0]
"The last column in Table 2 shows the fake news type that uses the corresponding lexicon most
prominently.",2 Fake News Analysis,[0],[0]
We found that one distinctive feature of satire compared to other types of untrusted news is its prominent use of adverbs.,2 Fake News Analysis,[0],[0]
Hoax stories tend to use fewer superlatives and comparatives.,2 Fake News Analysis,[0],[0]
"In contrast, compared to other types of fake news, propaganda uses relatively more assertive verbs and superlatives.
",2 Fake News Analysis,[0],[0]
"News Reliability Prediction We study the feasibility of predicting the reliability of the news article into four categories: trusted, satire, hoax, or propaganda.",2 Fake News Analysis,[0],[0]
"We split our collected articles into balanced training (20k total articles from the Onion, American News, The Activist, and the Gigaword news excluding ‘APW’, ‘WPB’ sources) and test sets (3k articles from the remaining sources).",2 Fake News Analysis,[0],[0]
"Because articles in the training and test set come from different sources, the models must classify articles without relying on author-specific cues.",2 Fake News Analysis,[0],[0]
We also use 20% of the training articles as an in-domain development set.,2 Fake News Analysis,[0],[0]
"We trained a Max-Entropy classifier with L2 regularization on n-gram tf-idf feature vectors (up to trigrams).4
The model achieves F1 scores of 65% on the out-of-domain test set (Table 3).",2 Fake News Analysis,[0],[0]
"This is a promising result as it is much higher than random, but still leaves room for improvement compared to the
4N-gram tfidf vectors have acted as competitive means of cross-domain text-classification.",2 Fake News Analysis,[0],[0]
"Zhang et al. (2015) found that for data sets smaller than a million examples, this was the best model, outperforming neural models.
",2 Fake News Analysis,[0],[0]
"performance on the development set consisting of articles from in-domain sources.
",2 Fake News Analysis,[0],[0]
We examined the 50 highest weighted n-gram features in the MaxEnt classifier for each class.,2 Fake News Analysis,[0],[0]
"The highest weighted n-grams for trusted news were often specific places (e.g., “washington”) or times (“on monday”).",2 Fake News Analysis,[0],[0]
"Many of the highest weighted from satire were vaguely facetious hearsay (“reportedly”, “confirmed”).",2 Fake News Analysis,[0],[0]
"For hoax articles, heavily weighted features included divisive topics (“liberals”, “trump”) and dramatic cues (“breaking”).",2 Fake News Analysis,[0],[0]
"Heavily weighted features for propaganda tend towards abstract generalities (“truth”, “freedom”) as well as specific issues (“vaccines”, “syria”).",2 Fake News Analysis,[0],[0]
"Interestingly, “youtube” and “video” are highly weighted for the propaganda and hoax classes respectively; indicating that they often rely on video clips as sources.",2 Fake News Analysis,[0],[0]
Politifact Data Related to the issue of identifying the truthfulness of a news article is the factchecking of individual statements made by public figures.,3 Predicting Truthfulness,[0],[0]
"Misleading statements can also have a variety of intents and levels of reliability depending on whom is making the statement.
",3 Predicting Truthfulness,[0],[0]
PolitiFact5 is a site led by Tampa Bay Times journalists who actively fact-check suspicious statements.,3 Predicting Truthfulness,[0],[0]
One unique quality of PolitiFact is that each quote is evaluated on a 6-point scale of truthfulness ranging from “True” (factual) to “Pantson-Fire False” (absurdly false).,3 Predicting Truthfulness,[0],[0]
"This scale allows for distinction between categories like mostly true (the facts are correct but presented in an incomplete manner) or mostly false (the facts are not correct but are connected to a small kernel of truth).
",3 Predicting Truthfulness,[0],[0]
"We collected labelled statements from PolitiFact and its spin-off sites (PunditFact, etc.)",3 Predicting Truthfulness,[0],[0]
"(10,483 statements in total).",3 Predicting Truthfulness,[0],[0]
"We analyze a subset of 4,366 statements that are direct quotes by the original speaker.",3 Predicting Truthfulness,[0],[0]
"The distributions of ratings on the PolitiFact scale for this subset are shown
5www.politifact.com/
in Table 4.",3 Predicting Truthfulness,[0],[0]
"Most statements are labeled as neither completely true nor false.
",3 Predicting Truthfulness,[0],[0]
We formulate a fine-grained truthfulness prediction task with Politifact data.,3 Predicting Truthfulness,[0],[0]
"We split quotes into training/development/test set of {2575, 712, 1074} statements, respectively, so that all of each speaker’s quotes are in a single set.",3 Predicting Truthfulness,[0],[0]
"Given a statement, the model returns a rating for how reliable the statement is (Politifact ratings are used as gold labels).",3 Predicting Truthfulness,[0],[0]
"We ran the experiment in two settings, one considering all 6 classes and the other considering only 2 (treating the top three truthful ratings as true and the lower three as false).
",3 Predicting Truthfulness,[0],[0]
"Model We trained an LSTM model (Hochreiter and Schmidhuber, 1997)",3 Predicting Truthfulness,[0],[0]
that takes the sequence of words as the input and predicts the Politifact rating.,3 Predicting Truthfulness,[0],[0]
"We also compared this model with Maximum Entropy (MaxEnt) and Naive Bayes models, frequently used for text categorization.
",3 Predicting Truthfulness,[0],[0]
"For input to the MaxEnt and Naive Bayes models, we tried two variants: one with the word tfidf vectors as input, and one with the LIWC measurements concatenated to the tf-idf vectors.",3 Predicting Truthfulness,[0],[0]
"For the LSTM model, we used word sequences as input and also a version where LSTM output is concatenated with LIWC feature vectors before undergoing the activation layer.",3 Predicting Truthfulness,[0],[0]
"The LSTM word embeddings are initialized with 100-dim embeddings from GLOVE (Pennington et al., 2014) and fine-tuned during training.",3 Predicting Truthfulness,[0],[0]
The LSTM was implemented with Theano and Keras with 300-dim hidden state and a batch size of 64.,3 Predicting Truthfulness,[0],[0]
"Training was done with ADAM to minimize categorical crossentropy loss over 10 epochs.
",3 Predicting Truthfulness,[0],[0]
Classifier Results Table 5 summarizes the performance on the development set.,3 Predicting Truthfulness,[0],[0]
We report macro averaged F1 score in all tables.,3 Predicting Truthfulness,[0],[0]
"The LSTM outperforms the other models when only using text as input; however the other two models improve substantially with adding LIWC features, particu-
larly in the case of the multinomial naive Bayes model.",3 Predicting Truthfulness,[0],[0]
"In contrast, the LIWC features do not improve the neural model much, indicating that some of this lexical information is perhaps redundant to what the model was already learning from text.
",3 Predicting Truthfulness,[0],[0]
We report results on the test set in Table 6.,3 Predicting Truthfulness,[0],[0]
We again find that LIWC features improves MaxEnt and NB models to perform similarly to the LSTM model.,3 Predicting Truthfulness,[0],[0]
"As in the dev. set results, the LIWC features do not improve the LSTM’s performance, and even seem to hurt the performance slightly.",3 Predicting Truthfulness,[0],[0]
"Deception Detection Psycholinguistic work in interpersonal deception theory (Buller and Burgoon, 1996) has postulated that certain speech patterns can be signs of a speaker trying to purposefully obscure the truth.",4 Related Work,[0],[0]
"Hedge words and other vague qualifiers (Choi et al., 2012; Recasens et al., 2013), for example, may add indirectness to a statement that obscures its meaning.
",4 Related Work,[0],[0]
"Linguistic aspects deception detection has been well-studied in a variety of NLP applications (Ott et al., 2011; Mihalcea and Strapparava, 2009; Jindal and Liu, 2008; Girlea et al., 2016; Zhou et al., 2004).",4 Related Work,[0],[0]
"In these applications, people purposefully tell lies to receive an extrinsic payoff.",4 Related Work,[0],[0]
"In our study, we compare varying types of unreliable news source, created with differing intents and levels of veracity.
",4 Related Work,[0],[0]
"Fact-Checking and Fake News There is research in political science exploring how effective fact-checking is at improving people’s awareness
(Lord et al., 1979; Thorson, 2016; Nyhan and Reifler, 2015).",4 Related Work,[0],[0]
"Prior computational works (Vlachos and Riedel, 2014; Ciampaglia et al., 2015) have proposed fact-checking through entailment from knowledge bases.",4 Related Work,[0],[0]
"Our work takes a more linguistic approach, performing lexical analysis over varying types of falsehood.
",4 Related Work,[0],[0]
"Biyani et al. (2016) examine the unique linguistic styles found in clickbait articles, and Kumar et al. (2016) also characterize hoax documents on Wikipedia.",4 Related Work,[0],[0]
"The differentiation between these fake news types is also proposed in previous work (Rubin et al., 2015).",4 Related Work,[0],[0]
"Our paper extends this work by offering a quantitative study of linguistic differences found in articles of different types of fake news, and build predictive models for graded deception across multiple domains – PolitiFact and news articles.",4 Related Work,[0],[0]
"More recent work (Wang, 2017) has also investigated PolitiFact data though they investigated meta-data features for prediction whereas our investigation is focused on linguistic analysis through stylistic lexicons.",4 Related Work,[0],[0]
"We examine truthfulness and its contributing linguistic attributes across multiple domains e.g., online news sources and public statements.",5 Conclusion,[0],[0]
"We perform multiple prediction tasks on fact-checked statements of varying levels of truth (graded deception) as well as a deeper linguistic comparison of differing types of fake news e.g., propaganda, satire and hoaxes.",5 Conclusion,[0],[0]
We have shown that factchecking is indeed a challenging task but that various lexical features can contribute to our understanding of the differences between more reliable and less reliable digital news sources.,5 Conclusion,[0],[0]
We would like to thank anonymous reviewers for providing insightful feedback.,6 Acknowledgements,[0],[0]
"The research described in this paper was conducted under the Laboratory Directed Research and Development Program at Pacific Northwest National Laboratory, a multiprogram national laboratory operated by Battelle for the U.S. Department of Energy, the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1256082, in part by NSF grants IIS-1408287, IIS-1714566, and gifts by Google and Facebook.",6 Acknowledgements,[0],[0]
We present an analytic study on the language of news media in the context of political fact-checking and fake news detection.,abstractText,[0],[0]
"We compare the language of real news with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text.",abstractText,[0],[0]
"To probe the feasibility of automatic political fact-checking, we also present a case study based on PolitiFact.com using their factuality judgments on a 6-point scale.",abstractText,[0],[0]
"Experiments show that while media fact-checking remains to be an open research question, stylistic cues can help determine the truthfulness of text.",abstractText,[0],[0]
Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking,title,[0],[0]
"Deep Neural Networks (LeCun et al., 2015) have been successful on numerous difficult machine learning tasks, including image recognition(Krizhevsky et al., 2012; Donahue et al., 2015), speech recognition(Hinton et al., 2012) and natural language processing(Collobert et al., 2011;
",1. Introduction,[0],[0]
"*Equal contribution 1Massachusetts Institute of Technology 2New York University, Facebook AI Research.",1. Introduction,[0],[0]
"Correspondence to: Li Jing <ljing@mit.edu>, Yichen Shen <ycshen@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Bahdanau et al., 2014; Sutskever et al., 2014).",1. Introduction,[0],[0]
"However, deep neural networks can suffer from vanishing and exploding gradient problems(Hochreiter, 1991; Bengio et al., 1994), which are known to be caused by matrix eigenvalues far from unity being raised to large powers.",1. Introduction,[0],[0]
"Because the severity of these problems grows with the the depth of a neural network, they are particularly grave for Recurrent Neural Networks (RNNs), whose recurrence can be equivalent to thousands or millions of equivalent hidden layers.
",1. Introduction,[0],[0]
Several solutions have been proposed to solve these problems for RNNs.,1. Introduction,[0],[0]
"Long Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997), which help RNNs contain information inside hidden layers with gates, remains one of the the most popular RNN implementations.",1. Introduction,[0],[0]
"Other recently proposed methods such as GRUs(Cho et al., 2014) and Bidirectional RNNs (Berglund et al., 2015) also perform well in numerous applications.",1. Introduction,[0],[0]
"However, none of these approaches has fundamentally solved the vanishing and exploding gradient problems, and gradient clipping is often required to keep gradients in a reasonable range.
",1. Introduction,[0],[0]
"A recently proposed solution strategy is using orthogonal hidden weight matrices or their complex generalization (unitary matrices) (Saxe et al., 2013; Le et al., 2015; Arjovsky et al., 2015; Henaff et al., 2016), because all their eigenvalues will then have absolute values of unity, and can safely be raised to large powers.",1. Introduction,[0],[0]
"This has been shown to help both when weight matrices are initialized to be unitary (Saxe et al., 2013; Le et al., 2015) and when they are kept unitary during training, either by restricting them to a more tractable matrix subspace (Arjovsky et al., 2015) or by alternating gradient-descent steps with projections onto the unitary subspace (Wisdom et al., 2016).
",1. Introduction,[0],[0]
"In this paper, we will first present an Efficient Unitary Neural Network (EUNN) architecture that parametrizes the entire space of unitary matrices in a complete and computationally efficient way, thereby eliminating the need for time-consuming unitary subspace-projections.",1. Introduction,[0],[0]
Our architecture has a wide range of capacity-tunability to represent subspace unitary models by fixing some of our parameters; the above-mentioned unitary subspace models correspond to special cases of our architecture.,1. Introduction,[0],[0]
"We also implemented
an EUNN with an earlier introduced FFT-like architecture which efficiently approximates the unitary space with minimum number of required parameters(Mathieu & LeCun, 2014b).
",1. Introduction,[0],[0]
"We then benchmark EUNN’s performance on both simulated and real tasks: the standard copying task, the pixelpermuted MNIST task, and speech prediction with the TIMIT dataset (Garofolo et al., 1993).",1. Introduction,[0],[0]
We show that our EUNN algorithm with an O(N) hidden layer size can compute up to the entire N × N gradient matrix using O(1) computational steps and memory access per parameter.,1. Introduction,[0],[0]
"This is superior to theO(N) computational complexity of the existing training method for a full-space unitary network (Wisdom et al., 2016) and O(logN) more efficient than the subspace Unitary RNN(Arjovsky et al., 2015).",1. Introduction,[0],[0]
"A recurrent neural network takes an input sequence and uses the current hidden state to generate a new hidden state during each step, memorizing past information in the hidden layer.",2.1. Basic Recurrent Neural Networks,[0],[0]
"We first review the basic RNN architecture.
",2.1. Basic Recurrent Neural Networks,[0],[0]
"Consider an RNN updated at regular time intervals t = 1, 2, ... whose input is the sequence of vectors x(t) whose hidden layer h(t) is updated according to the following rule:
h(t) = σ(Ux(t) +Wh(t−1)), (1)
where σ is the nonlinear activation function.",2.1. Basic Recurrent Neural Networks,[0],[0]
"The output is generated by
y(t)",2.1. Basic Recurrent Neural Networks,[0],[0]
"= Wh(t) + b, (2)
where b is the bias vector for the hidden-to-output layer.",2.1. Basic Recurrent Neural Networks,[0],[0]
"For t = 0, the hidden layer h(0) can be initialized to some special vector or set as a trainable variable.",2.1. Basic Recurrent Neural Networks,[0],[0]
"For convenience of notation, we define z(t) = Ux(t) + Wh(t−1) so that h(t) = σ(z(t)).",2.1. Basic Recurrent Neural Networks,[0],[0]
"When training the neural network to minimize a cost function C that depends on a parameter vector a, the gradient descent method updates this vector to a − λ∂C∂a , where λ is a fixed learning rate and ∂C∂a ≡ ∇C.",2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
"For an RNN, the vanishing or exploding gradient problem is most significant during back propagation from hidden to hidden layers, so we will only focus on the gradient for hidden layers.",2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
"Training the input-to-hidden and hidden-to-output matrices is relatively trivial once the hidden-to-hidden matrix has been successfully optimized.
",2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
"In order to evaluate ∂C∂Wij , one first computes the derivative
∂C ∂h(t) using the chain rule:
∂C
∂h(t) =
∂C ∂h(T ) ∂h(T ) ∂h(t) (3)
= ∂C
∂h(T )",2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
"T−1∏ k=t ∂h(k+1) ∂h(k) (4)
= ∂C
∂h(T )",2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
"T−1∏ k=t D(k)W, (5)
where D(k) = diag{σ′(Ux(k) + Wh(k−1))} is the Jacobian matrix of the pointwise nonlinearity.",2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
"For large times T , the term ∏ W plays a significant role.",2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
"As long as the eigenvalues of D(k) are of order unity, then if W has eigenvalues λi 1, they will cause gradient explosion | ∂C ∂h(T )
| → ∞, while if W has eigenvalues λi 1, they can cause gradient vanishing, | ∂C
∂h(T )",2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
| → 0.,2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
"Either situation
prevents the RNN from working efficiently.",2.2. The Vanishing and Exploding Gradient Problems,[0],[0]
"In a breakthrough paper, Arjovsky, Shah & Bengio (Arjovsky et al., 2015) showed that unitary RNNs can overcome the exploding and vanishing gradient problems and perform well on long term memory tasks if the hiddento-hidden matrix in parametrized in the following unitary form:
W = D3T2F−1D2ΠT1FD1.",3.1. Partial Space Unitary RNNs,[0],[0]
"(6)
Here D1,2,3 are diagonal matrices with each element eiωj , j = 1, 2, · · · , n. T1,2 are reflection matrices, and T = I − 2 v̂v̂ †
||v̂||2 , where v̂ is a vector with each of its entries as a parameter to be trained.",3.1. Partial Space Unitary RNNs,[0],[0]
Π is a fixed permutation matrix.,3.1. Partial Space Unitary RNNs,[0],[0]
F and F−1 are Fourier and inverse Fourier transform matrices respectively.,3.1. Partial Space Unitary RNNs,[0],[0]
"Since each factor matrix here is unitary, the product W is also a unitary matrix.
",3.1. Partial Space Unitary RNNs,[0],[0]
"This model uses O(N) parameters, which spans merely a part of the whole O(N2)-dimensional space of unitary N × N matrices to enable computational efficiency.",3.1. Partial Space Unitary RNNs,[0],[0]
"Several subsequent papers have tried to expand the space to O(N2) in order to achieve better performance, as summarized below.",3.1. Partial Space Unitary RNNs,[0],[0]
"In order to maximize the power of Unitary RNNs, it is preferable to have the option to optimize the weight matrix W over the full space of unitary matrices rather than a subspace as above.",3.2. Full Space Unitary RNNs,[0],[0]
"A straightforward method for implementing this is by simply updating W with standard backpropagation and then projecting the resulting matrix (which will typically no longer be unitary) back onto to the space
of unitary matrices.",3.2. Full Space Unitary RNNs,[0],[0]
"Defining Gij ≡ ∂C∂Wij as the gradient with respect to W, this can be implemented by the procedure defined by (Wisdom et al., 2016):
A(t) ≡ G(t) † W(t) −W(t) † G(k), (7) W(t+1) ≡",3.2. Full Space Unitary RNNs,[0],[0]
"( I + λ
2 A(t)
)−1",3.2. Full Space Unitary RNNs,[0],[0]
"( I− λ
2 A(t)
) W(t).(8)
",3.2. Full Space Unitary RNNs,[0],[0]
"This method shows that full space unitary networks are superior on many RNN tasks (Wisdom et al., 2016).",3.2. Full Space Unitary RNNs,[0],[0]
"A key limitation is that the back-propation in this method cannot avoid N -dimensional matrix multiplication, incurring O(N3) computational cost.",3.2. Full Space Unitary RNNs,[0],[0]
"In the following, we first describe a general parametrization method able to represent arbitrary unitary matrices with up to N2 degrees of freedom.",4. Efficient Unitary Neural Network (EUNN) Architectures,[0],[0]
"We then present an efficient algorithm for this parametrization scheme, requiring only O(1) computational and memory access steps to obtain the gradient for each parameter.",4. Efficient Unitary Neural Network (EUNN) Architectures,[0],[0]
"Finally, we show that our scheme performs significantly better than the above mentioned methods on a few well-known benchmarks.",4. Efficient Unitary Neural Network (EUNN) Architectures,[0],[0]
"Any N × N unitary matrix WN can be represented as a product of rotation matrices {Rij} and a diagonal matrix D, such that WN = D ∏N i=2 ∏i−1",4.1. Unitary Matrix Parametrization,[0],[0]
"j=1 Rij , where Rij is defined as the N -dimensional identity matrix with the elements Rii, Rij , Rji and Rjj replaced as follows (Reck et al., 1994; Clements et al., 2016):(
Rii Rij Rji Rjj
) =",4.1. Unitary Matrix Parametrization,[0],[0]
"( eiφij cos θij −eiφij sin θij
sin θij cos θij
) .",4.1. Unitary Matrix Parametrization,[0],[0]
"(9)
where θij and φij are unique parameters corresponding to Rij.",4.1. Unitary Matrix Parametrization,[0],[0]
"Each of these matrices performs a U(2) unitary transformation on a two-dimensional subspace of the Ndimensional Hilbert space, leaving an (N−2)-dimensional subspace unchanged.",4.1. Unitary Matrix Parametrization,[0],[0]
"In other words, a series of U(2) rotations can be used to successively make all off-diagonal elements of the given N × N unitary matrix zero.",4.1. Unitary Matrix Parametrization,[0],[0]
This generalizes the familiar factorization of a 3D rotation matrix into 2D rotations parametrized by the three Euler angles.,4.1. Unitary Matrix Parametrization,[0],[0]
"To provide intuition for how this works, let us briefly describe a simple way of doing this that is similar to Gaussian elimination by finishing one column at a time.",4.1. Unitary Matrix Parametrization,[0],[0]
"There are infinitely many alternative decomposition schemes as well; Fig. 1 shows two that are particularly convenient to implement in software (and even in neuromorphic hardware (Shen et al., 2016)).",4.1. Unitary Matrix Parametrization,[0],[0]
"The unitary matrix WN is multiplied from the right by a succession of unitary matrices
RNj for j = N − 1, · · · , 1.",4.1. Unitary Matrix Parametrization,[0],[0]
"Once all elements of the last row except the one on the diagonal are zero, this row will not be affected by later transformations.",4.1. Unitary Matrix Parametrization,[0],[0]
"Since all transformations are unitary, the last column will then also contain only zeros except on the diagonal:
WNRN,N−1RN,N−2 · ·RN,1 = (
WN−1 0 0",4.1. Unitary Matrix Parametrization,[0],[0]
"eiwN
) (10)
The effective dimensionality of the the matrix W is thus reduced toN−1.",4.1. Unitary Matrix Parametrization,[0],[0]
"The same procedure can then be repeated N − 1 times until the effective dimension of W is reduced to 1, leaving us with a diagonal matrix:1
WNRN,N−1RN,N−2 · · ·Ri,jRi,j−1 · · ·R3,1R2,1 = D, (11)
where D is a diagonal matrix whose diagonal elements are eiwj , from which we can write the direct representation of WN as
WN = DR −1 2,1R −1 3,1 . .",4.1. Unitary Matrix Parametrization,[0],[0]
.R −1,4.1. Unitary Matrix Parametrization,[0],[0]
"N,N−2R −1",4.1. Unitary Matrix Parametrization,[0],[0]
"N,N−1
= DR′2,1R ′ 3,1 . .",4.1. Unitary Matrix Parametrization,[0],[0]
.R ′,4.1. Unitary Matrix Parametrization,[0],[0]
"N,N−2R ′ N,N−1.",4.1. Unitary Matrix Parametrization,[0],[0]
"(12)
where
R′ij = R(−θij ,−φij) = R(θij , φij)−1 = R−1ij (13)
1Note that Gaussian Elimination would make merely the upper triangle of a matrix vanish, requiring a subsequent series of rotations (complete Gauss-Jordan Elimination) to zero the lower triangle.",4.1. Unitary Matrix Parametrization,[0],[0]
"We need no such subsequent series because since W is unitary: it is easy to show that if a unitary matrix is triangular, it must be diagonal.
",4.1. Unitary Matrix Parametrization,[0],[0]
"This parametrization thus involves N(N − 1)/2 different θij-values, N(N − 1)/2 different φij-values and N different wi-values, combining to N2 parameters in total and spans the entire unitary space.",4.1. Unitary Matrix Parametrization,[0],[0]
"Note we can always fix a portion of our parameters, to span only a subset of unitary space – indeed, our benchmark test below will show that for certain tasks, full unitary space parametrization is not necessary.",4.1. Unitary Matrix Parametrization,[0],[0]
2,4.1. Unitary Matrix Parametrization,[0],[0]
"The representation in Eq. 12 can be made more compact by reordering and grouping specific rotational matrices, as was shown in the optical community (Reck et al., 1994; Clements et al., 2016) in the context of universal multiport interferometers.",4.2. Tunable space implementation,[0],[0]
"For example (Clements et al., 2016), a unitary matrix can be decomposed as
WN = D ( R (1) 1,2R (1) 3,4 . .",4.2. Tunable space implementation,[0],[0]
.R,4.2. Tunable space implementation,[0],[0]
"(1) N/2−1,N/2 )",4.2. Tunable space implementation,[0],[0]
"× ( R
(2) 2,3R (2) 4,5 . .",4.2. Tunable space implementation,[0],[0]
.R,4.2. Tunable space implementation,[0],[0]
"(2) N/2−2,N/2−1 )",4.2. Tunable space implementation,[0],[0]
"× . . .
",4.2. Tunable space implementation,[0],[0]
= DF (1) A F (2) B . .,4.2. Tunable space implementation,[0],[0]
".F (L) B , (14)
",4.2. Tunable space implementation,[0],[0]
"where every
F (l) A = R (l) 1,2R (l) 3,4 . .",4.2. Tunable space implementation,[0],[0]
.R,4.2. Tunable space implementation,[0],[0]
"(l) N/2−1,N/2
is a block diagonal matrix, with N angle parameters in total, and
",4.2. Tunable space implementation,[0],[0]
F (l) B = R,4.2. Tunable space implementation,[0],[0]
"(l) 2,3R (l) 4,5 . .",4.2. Tunable space implementation,[0],[0]
.R,4.2. Tunable space implementation,[0],[0]
"(l) N/2−2,N/2−1
withN−1 parameters, as is schematically shown in Fig.",4.2. Tunable space implementation,[0],[0]
1a.,4.2. Tunable space implementation,[0],[0]
"By choosing different values for L , WN will span a different subspace of the unitary space.",4.2. Tunable space implementation,[0],[0]
"Specifically,when L = N , WN will span the entire unitary space.
",4.2. Tunable space implementation,[0],[0]
"Following this physics-inspired scheme, we decompose our unitary hidden-to-hidden layer matrix W as
W = DF (1) A F (2) B F (3) A F (4) B · · ·F (L) B .",4.2. Tunable space implementation,[0],[0]
(15),4.2. Tunable space implementation,[0],[0]
"Inspired by (Mathieu & LeCun, 2014a), an alternative way to organize the rotation matrices is implementing an FFTstyle architecture.",4.3. FFT-style approximation,[0],[0]
"Instead of using adjacent rotation matrices, each F here performs a certain distance pairwise rotations as shown in Fig.",4.3. FFT-style approximation,[0],[0]
"1b:
W = DF1F2F3F4 · · ·Flog(N).",4.3. FFT-style approximation,[0],[0]
"(16)
",4.3. FFT-style approximation,[0],[0]
"The rotation matrices in Fi are performed between pairs of coordinates
(2pk + j, p(2k + 1) + j) (17)
2Our preliminary experimental tests even suggest that a fullcapacity unitary RNN is even undesirable for some tasks.
where p = N2i , k ∈ {0, ..., 2 i−1} and j ∈ {1, ..., p}.",4.3. FFT-style approximation,[0],[0]
"This requires only log(N) matrices, so there are a total of N log(N)/2 rotational pairs.",4.3. FFT-style approximation,[0],[0]
"This is also the minimal number of rotations that can have all input coordinates interacting with each other, providing an approximation of arbitrary unitary matrices.",4.3. FFT-style approximation,[0],[0]
"To implement this decomposition efficiently in an RNN, we apply vector element-wise multiplications and permutations: we evaluate the product Fx as
Fx = v1 ∗ x + v2 ∗ permute(x) (18)
where ∗ represents element-wise multiplication, F refers to general rotational matrices such as FA/B in Eq. 14 and Fi in Eq. 16.",4.4. Efficient implementation of rotation matrices,[0],[0]
"For the case of the tunable-space implementation, if we want to implement F(l)A in Eq. 14, we define v and the permutation as follows:
v1 = (e iφ (l) 1 cos θ (l) 1 , cos θ (l) 1 , e iφ (l) 2",4.4. Efficient implementation of rotation matrices,[0],[0]
"cos θ (l) 2 , cos θ (l) 2 , · · · )
v2 = (−eiφ (l) 1 sin θ",4.4. Efficient implementation of rotation matrices,[0],[0]
"(l) 1 , sin θ (l) 1 ,−eiφ (l) 2 sin θ2, sin θ (l) 2 , · · · )
",4.4. Efficient implementation of rotation matrices,[0],[0]
permute(x) =,4.4. Efficient implementation of rotation matrices,[0],[0]
"(x2, x1, x4, x3, x6, x5, · · · ).
",4.4. Efficient implementation of rotation matrices,[0],[0]
"For the FFT-style approach, if we want to implement F1 in Eq 16, we define v and the permutation as follows:
v1 = (e iφ (l) 1",4.4. Efficient implementation of rotation matrices,[0],[0]
"cos θ (l) 1 , e iφ (l) 2 cos θ (l) 2 , · · · , cos θ (l) 1 , · · · )
v2 = (−eiφ (l) 1 sin θ",4.4. Efficient implementation of rotation matrices,[0],[0]
"(l) 1 ,−eiφ (l) 2 sin θ2, · · · , sin θ(l)1 , · · · )
",4.4. Efficient implementation of rotation matrices,[0],[0]
permute(x) =,4.4. Efficient implementation of rotation matrices,[0],[0]
"(xn 2 +1 , xn 2 +2 · · ·xn, x1, x2 · · · ).
",4.4. Efficient implementation of rotation matrices,[0],[0]
"In general, the pseudocode for implementing operation F is as follows:
Algorithm 1 Efficient implementation for F with parameter θi and φi.
",4.4. Efficient implementation of rotation matrices,[0],[0]
"Input: input x, size N ; parameters θ and φ, size N/2; constant permuatation index list ind1 and ind2.",4.4. Efficient implementation of rotation matrices,[0],[0]
"Output: output y, size N .",4.4. Efficient implementation of rotation matrices,[0],[0]
"v1← concatenate(cos θ, cos θ * exp(iφ))",4.4. Efficient implementation of rotation matrices,[0],[0]
v2← concatenate(sin,4.4. Efficient implementation of rotation matrices,[0],[0]
"θ, - sin θ * exp(iφ)) v1← permute(v1, ind1) v2← permute(v2, ind1) y← v1 ∗ x + v2 ∗ permute(x, ind2)
",4.4. Efficient implementation of rotation matrices,[0],[0]
"Note that ind1 and ind2 are different for different F.
From a computational complexity viewpoint, since the operations ∗ and permute take O(N) computational steps, evaluating Fx only requires O(N) steps.",4.4. Efficient implementation of rotation matrices,[0],[0]
"The product Dx is trivial, consisting of an element-wise vector multiplication.",4.4. Efficient implementation of rotation matrices,[0],[0]
"Therefore, the product Wx with the total unitary
matrix W can be computed in only O(NL) steps, and only requires O(NL) memory access (for full-space implementation L = N , for FFT-style approximation gives L = logN ).",4.4. Efficient implementation of rotation matrices,[0],[0]
A detailed comparison on computational complexity of the existing unitary RNN architectures is given in Table 1.,4.4. Efficient implementation of rotation matrices,[0],[0]
"We use the same nonlinearity as (Arjovsky et al., 2015):
(modReLU(z,b))i = zi |zi| ∗ ReLU(|zi|+ bi) (19)
where the bias vector b is a shared trainable parameter, and |zi| is the norm of the complex number zi.
",4.5. Nonlinearity,[0],[0]
"For real number input, modReLU can be simplified to:
(modReLU(z,b))i = sign(zi) ∗ ReLU(|zi|+ bi) (20)
where |zi| is the absolute value of the real number zi.
",4.5. Nonlinearity,[0],[0]
We empirically find that this nonlinearity function performs the best.,4.5. Nonlinearity,[0],[0]
We believe that this function possibly also serves as a forgetting filter that removes the noise using the bias threshold.,4.5. Nonlinearity,[0],[0]
"In this section, we compare the performance of our Efficient Unitary Recurrent Neural Network (EURNN) with
1.",5. Experimental tests of our method,[0],[0]
"an LSTM RNN (Hochreiter & Schmidhuber, 1997),
2.",5. Experimental tests of our method,[0],[0]
"a Partial Space URNN (Arjovsky et al., 2015), and
3.",5. Experimental tests of our method,[0],[0]
"a Projective full-space URNN (Wisdom et al., 2016).
",5. Experimental tests of our method,[0],[0]
"All models are implemented in both Tensorflow and Theano, available from https://github.com/ jingli9111/EUNN-tensorflow and https: //github.com/iguanaus/EUNN-theano.",5. Experimental tests of our method,[0],[0]
"We compare these networks by applying them all to the well defined Copying Memory Task (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al., 2016).",5.1. Copying Memory Task,[0],[0]
"The copying task is a synthetic task that is commonly used to test the network’s ability to remember information seen T time steps earlier.
",5.1. Copying Memory Task,[0],[0]
"Specifically, the task is defined as follows (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2015; Henaff et al., 2016).",5.1. Copying Memory Task,[0],[0]
"An alphabet consists of symbols {ai}, the first n of which represent data, and the remaining two representing “blank” and “start recall”, respectively; as illustrated by the following example where T = 20 and M = 5:
Input: BACCA--------------------:---Output: -------------------------BACCA
In the above example, n = 3 and {ai} = {A,B,C,−, :}.",5.1. Copying Memory Task,[0],[0]
"The input consists of M random data symbols (M = 5 above) followed by T − 1 blanks, the “start recall” symbol and M more blanks.",5.1. Copying Memory Task,[0],[0]
The desired output consists of M +T blanks followed by the data sequence.,5.1. Copying Memory Task,[0],[0]
"The cost function C is defined as the cross entropy of the input and output sequences, which vanishes for perfect performance.
",5.1. Copying Memory Task,[0],[0]
We use n = 8 and input length M = 10.,5.1. Copying Memory Task,[0],[0]
The symbol for each input is represented by an n-dimensional one-hot vector.,5.1. Copying Memory Task,[0],[0]
We trained all five RNNs for T = 1000 with the same batch size 128 using RMSProp optimization with a learning rate of 0.001.,5.1. Copying Memory Task,[0],[0]
"The decay rate is set to 0.5 for EURNN, and 0.9 for all other models respectively.",5.1. Copying Memory Task,[0],[0]
(Fig. 2).,5.1. Copying Memory Task,[0],[0]
"This results show that the EURNN architectures introduced in both Sec.4.2 (EURNN with N=512, selecting L=2) and Sec.4.3 (FFT-style EURNN with N=512) outperform the LSTM model (which suffers from long term memory problems and only performs well on the copy task for small time delays T ) and all other unitary RNN models, both in-terms of learnability and in-terms of convergence rate.",5.1. Copying Memory Task,[0],[0]
"Note that the only other unitary RNN model that is able to beat the baseline for T = 1000 (Wisdom et al., 2016) is significantly slower than our method.
",5.1. Copying Memory Task,[0],[0]
"Moreover, we find that by either choosing smaller L or by using the FFT-style method (so that W spans a smaller unitary subspace), the EURNN converges toward optimal performance significantly more efficiently (and also faster in wall clock time) than the partial (Arjovsky et al., 2015) and projective (Wisdom et al., 2016) unitary methods.",5.1. Copying Memory Task,[0],[0]
The EURNN also performed more robustly.,5.1. Copying Memory Task,[0],[0]
This means that a fullcapacity unitary matrix is not necessary for this particular task.,5.1. Copying Memory Task,[0],[0]
The MNIST handwriting recognition problem is one of the classic benchmarks for quantifying the learning ability of neural networks.,5.2. Pixel-Permuted MNIST Task,[0],[0]
"MNIST images are formed by a 28×28 grayscale image with a target label between 0 and 9.
",5.2. Pixel-Permuted MNIST Task,[0],[0]
"To test different RNN models, we feed all pixels of the MNIST images into the RNN models in 28×28 time steps, where one pixel at a time is fed in as a floating-point number.",5.2. Pixel-Permuted MNIST Task,[0],[0]
A fixed random permutation is applied to the order of input pixels.,5.2. Pixel-Permuted MNIST Task,[0],[0]
The output is the probability distribution quantifying the digit prediction.,5.2. Pixel-Permuted MNIST Task,[0],[0]
"We used RMSProp with a learning rate of 0.0001 and a decay rate of 0.9, and set the batch size to 128.
",5.2. Pixel-Permuted MNIST Task,[0],[0]
"As shown in Fig. 3, EURNN significantly outperforms LSTM with the same number of parameters.",5.2. Pixel-Permuted MNIST Task,[0],[0]
"It learns faster, in fewer iteration steps, and converges to a higher classifi-
Model hidden size number of validation test (capacity) parameters accuracy accuracy LSTM 80 16k 0.908 0.902 URNN 512 16k 0.942 0.933
PURNN 116 16k 0.922 0.921 EURNN (tunable style) 1024 (2) 13.3k 0.940 0.937
EURNN (FFT style) 512 (FFT) 9.0k 0.928 0.925
cation accuracy.",5.2. Pixel-Permuted MNIST Task,[0],[0]
"In addition, the EURNN reaches a similar accuracy with fewer parameters.",5.2. Pixel-Permuted MNIST Task,[0],[0]
In Table.,5.2. Pixel-Permuted MNIST Task,[0],[0]
"2, we compare the performance of different RNN models on this task.",5.2. Pixel-Permuted MNIST Task,[0],[0]
We also apply our EURNN to real-world speech prediction task and compare its performance to LSTM.,5.3. Speech Prediction on TIMIT dataset,[0],[0]
"The main task we consider is predicting the log-magnitude of future frames of a short-time Fourier transform (STFT) (Wisdom et al., 2016; Sejdi et al., 2009).",5.3. Speech Prediction on TIMIT dataset,[0],[0]
"We use the TIMIT dataset (Garofolo et al., 1993) sampled at 8 kHz.",5.3. Speech Prediction on TIMIT dataset,[0],[0]
The audio .wav file is initially diced into different time frames (all frames have the same duration referring to the Hann analysis window below).,5.3. Speech Prediction on TIMIT dataset,[0],[0]
"The audio amplitude in each frame is then
Fourier transformed into the frequency domain.",5.3. Speech Prediction on TIMIT dataset,[0],[0]
The logmagnitude of the Fourier amplitude is normalized and used as the data for training/testing each model.,5.3. Speech Prediction on TIMIT dataset,[0],[0]
In our STFT operation we uses a Hann analysis window of 256 samples (32 milliseconds) and a window hop of 128 samples (16 milliseconds).,5.3. Speech Prediction on TIMIT dataset,[0],[0]
"The frame prediction task is as follows: given all the log-magnitudes of STFT frames up to time t, predict the log-magnitude of the STFT frame at time t+ 1 that has the minimum mean square error (MSE).",5.3. Speech Prediction on TIMIT dataset,[0],[0]
"We use
a training set with 2400 utterances, a validation set of 600 utterances and an evaluation set of 1000 utterances.",5.3. Speech Prediction on TIMIT dataset,[0],[0]
"The training, validation, and evaluation sets have distinct speakers.",5.3. Speech Prediction on TIMIT dataset,[0],[0]
"We trained all RNNs for with the same batch size 32 using RMSProp optimization with a learning rate of 0.001, a momentum of 0.9 and a decay rate of 0.1.
",5.3. Speech Prediction on TIMIT dataset,[0],[0]
The results are given in Table.,5.3. Speech Prediction on TIMIT dataset,[0],[0]
"3, in terms of the meansquared error (MSE) loss function.",5.3. Speech Prediction on TIMIT dataset,[0],[0]
Figure.,5.3. Speech Prediction on TIMIT dataset,[0],[0]
"4 shows prediction examples from the three types of networks, illustrat-
ing how EURNNs generally perform better than LSTMs.",5.3. Speech Prediction on TIMIT dataset,[0],[0]
"Furthermore, in this particular task, full-capacity EURNNs outperform small capacity EURNNs and FFT-style EURNNs.",5.3. Speech Prediction on TIMIT dataset,[0],[0]
"We have presented a method for implementing an Efficient Unitary Neural Network (EUNN) whose computational cost is merely O(1) per parameter, which is O(logN) more efficient than the other methods discussed above.",6. Conclusion,[0],[0]
"It significantly outperforms existing RNN architectures on the standard Copying Task, and the pixel-permuted MNIST Task using a comparable parameter count, hence demonstrating the highest recorded ability to memorize sequential information over long time periods.
",6. Conclusion,[0],[0]
"It also performs well on real tasks such as speech prediction, outperforming an LSTM on TIMIT data speech prediction.
",6. Conclusion,[0],[0]
We want to emphasize the generality and tunability of our method.,6. Conclusion,[0],[0]
The ordering of the rotation matrices we presented in Fig. 1 are merely two of many possibilities; we used it simply as a concrete example.,6. Conclusion,[0],[0]
"Other ordering options that can result in spanning the full unitary matrix space can be used for our algorithm as well, with identical speed and memory performance.",6. Conclusion,[0],[0]
"This tunability of the span of the unitary space and, correspondingly, the total number of parameters makes it possible to use different capacities for different tasks, thus opening the way to an optimal performance of the EUNN.",6. Conclusion,[0],[0]
"For example, as we have shown, a small subspace of the full unitary space is preferable for the copying task, whereas the MNIST task and TIMIT task are better performed by EUNN covering a considerably larger unitary space.",6. Conclusion,[0],[0]
"Finally, we note that our method remains applicable even if the unitary matrix is decomposed into a different product of matrices (Eq. 12).
",6. Conclusion,[0],[0]
This powerful and robust unitary RNN architecture also might be promising for natural language processing because of its ability to efficiently handle tasks with long-term correlation and very high dimensionality.,6. Conclusion,[0],[0]
"We thank Hugo Larochelle and Yoshua Bengio for helpful discussions and comments.
",Acknowledgment,[0],[0]
"This work was partially supported by the Army Research Office through the Institute for Soldier Nanotechnologies under contract W911NF-13-D0001, the National Science Foundation under Grant No. CCF-1640012 and the Rothberg Family Fund for Cognitive Science.",Acknowledgment,[0],[0]
"Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data.",abstractText,[0],[0]
This approach appears particularly promising for Recurrent Neural Networks (RNNs).,abstractText,[0],[0]
"In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows.",abstractText,[0],[0]
"Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space.",abstractText,[0],[0]
"Secondly, the computational complexity for training an EUNN is merelyO(1) per parameter.",abstractText,[0],[0]
"Finally, we test the performance of EUNNs on the standard copying task, the pixelpermuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT).",abstractText,[0],[0]
"We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed.",abstractText,[0],[0]
EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.,abstractText,[0],[0]
Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1369–1379 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1369",text,[0],[0]
"Now that algorithms have started to produce relevant and realistic natural language that can describe images and videos, we would like to understand what these models truly comprehend.",1 Introduction,[0],[0]
The Visual Question Answering (VQA) task provides a nice tool for fine-grained evaluation of such multimodal algorithms.,1 Introduction,[0],[0]
"VQA systems take as input an image (or video) along with relevant natural language questions, and produce answers to those questions.",1 Introduction,[0],[0]
"By asking algorithms to answer different types of questions, ranging from object identification, counting, or appearance, to more complex questions about interactions, social relationships, or inferences about why or how something is occurring, we can evaluate different aspects of a model’s multimodal semantic understanding.
",1 Introduction,[0],[0]
"As a result, several popular image-based VQA datasets have been introduced, including DAQUAR (Malinowski and Fritz, 2014), COCO-QA (Ren et al., 2015a), FM-IQA (Gao
et al., 2015), Visual Madlibs (Yu et al., 2015), VQA (Antol et al., 2015), Visual7W (Zhu et al., 2016), etc.",1 Introduction,[0],[0]
"In addition, multiple video-based QA datasets have also been collected recently, e.g., MovieQA (Tapaswi et al., 2016), MovieFIB (Maharaj et al., 2017a), PororoQA (Kim et al., 2017), TGIF-QA (Jang et al., 2017), etc.",1 Introduction,[0],[0]
"However, there exist various shortcomings for each such video QA dataset.",1 Introduction,[0],[0]
"For example, MovieFIB’s video clips are typically short (∼4 secs), and focused on purely visual concepts (since they were collected from audio descriptions for the visually impaired); MovieQA collected QAs based on text summaries only, making them very plot-focused and less relevant for visual information; PororoQA’s video domain is cartoon-based; and TGIF-QA used predefined templates for generation on short GIFs.
",1 Introduction,[0],[0]
"With video-QA in particular, as opposed to image-QA, the video itself often comes with associated natural language in the form of (subtitle) dialogue.",1 Introduction,[0],[0]
"We argue that this is an important area to study because it reflects the real world, where people interact through language, and where many computational systems like robots or other intelligent agents will ultimately have to operate.",1 Introduction,[0],[0]
"As such, systems will need to combine information from what they see with what they hear, to pose and answer questions about what is happening.
",1 Introduction,[0],[0]
We aim to provide a dataset that merges the best qualities from all of the previous datasets as well as focus on multimodal compositionality.,1 Introduction,[0],[0]
"In particular, we collect a new large-scale dataset that is built on natural video content with rich dynamics and realistic social interactions, where questionanswer pairs are written by people observing both videos and their accompanying dialogues, encouraging the questions to require both vision and language understanding to answer.",1 Introduction,[0],[0]
"To further encourage this multimodal-QA quality, we ask people to write compositional questions consisting
of two parts, a main question part, e.g. “What are Leonard and Sheldon arguing about” and a grounding part, e.g. “when they are sitting on the couch”.",1 Introduction,[0],[0]
"This also leads to an interesting secondary task of QA temporal localization.
",1 Introduction,[0],[0]
"Our contribution is the TVQA dataset, built on 6 popular TV shows spanning 3 genres: medical dramas, sitcoms, and crime shows.",1 Introduction,[0],[0]
"On this data, we collected 152.5K human-written QA pairs (examples shown in Fig.1).",1 Introduction,[0],[0]
There are 4 salient advantages of our dataset.,1 Introduction,[0],[0]
"First, it is large-scale and natural, containing 21,793 video clips from 925 episodes.",1 Introduction,[0],[0]
"On average, each show has 7.3 seasons, providing long range character interactions and evolving relationships.",1 Introduction,[0],[0]
"Each video clip is associated with 7 questions, with 5 answers (1 correct) for each question.",1 Introduction,[0],[0]
"Second, our video clips are relatively long (60-90 seconds), thereby containing more social interactions and activities, making video understanding more challenging.",1 Introduction,[0],[0]
"Third, we provide the dialogue (character name + subtitle) for each QA video clip.",1 Introduction,[0],[0]
Understanding the relationship between the provided dialogue and the question-answer pairs is crucial for correctly answering many of the collected questions.,1 Introduction,[0],[0]
"Fourth, our questions are compositional, requiring algorithms to localize relevant moments (START and END points are provided for each question).
",1 Introduction,[0],[0]
"With the above rich annotation, our dataset supports three tasks: QA on the grounded clip, question-driven moment localization, and QA on the full video clip.",1 Introduction,[0],[0]
We provide baseline experiments on both QA tasks and introduce a state-ofthe-art language and vision-based model (leaving moment localization for future work).,1 Introduction,[0],[0]
"Visual Question Answering: Several imagebased VQA datasets have recently been constructed, e.g., DAQUAR (Malinowski and Fritz, 2014), VQA (Antol et al., 2015), COCO-Q (Ren et al., 2015a), FM-IQA (Gao et al., 2015), Visual Madlibs (Yu et al., 2015), Visual7W (Zhu et al., 2016), CLEVR (Johnson et al., 2017), etc.",2 Related Work,[0],[0]
"Additionally, several video-based QA datasets have also been proposed, e.g. TGIF-QA (Jang et al., 2017), MovieFIB (Maharaj et al., 2017b), VideoQA",2 Related Work,[0],[0]
"(Zhu et al., 2017), LSMDC (Rohrbach et al., 2015), TRECVID (Over et al., 2014), MovieQA (Tapaswi et al., 2016), PororoQA (Kim et al., 2017) and MarioQA (Mun et al., 2017).",2 Related Work,[0],[0]
"However, none of these datasets provides a truly realistic, multimodal QA scenario where both visual and language understanding are required to answer a large portion of questions, either due to unrealistic video sources (PororoQA, MarioQA) or data collection strategy being more focused on either visual (MovieFIB, VideoQA, TGIF-QA) or language (MovieQA) sources.",2 Related Work,[0],[0]
"In comparison, our TVQA collection strategy takes a directly multimodal approach to construct a large-scale, realvideo dataset by letting humans ask and answer questions while watching TV-show videos with associated dialogues.",2 Related Work,[0],[0]
"Text Question Answering: The related task of text-based question answering has been extensively explored (Richardson et al., 2013; Weston et al., 2015; Rajpurkar et al., 2016; Hermann et al., 2015; Hill et al., 2015).",2 Related Work,[0],[0]
"Richardson et al. (2013) collected MCTest, a multiple choice QA dataset intended for open-domain reading comprehension.
",2 Related Work,[0],[0]
"With the same goal in mind, Rajpurkar et al. (2016) introduced the SQuAD dataset, but their answers are specific spans from long passages.",2 Related Work,[0],[0]
Weston et al. (2015) designed a set of tasks with automatically generated QAs to evaluate the textual reasoning ability of artificial agents and Hermann et al. (2015); Hill et al. (2015) constructed the cloze dataset on top of an existing corpus.,2 Related Work,[0],[0]
"While questions in these text QA datasets are specifically designed for language understanding, TVQA questions require both vision understanding and language understanding.",2 Related Work,[0],[0]
"Although methods developed for text QA are not directly applicable to TVQA tasks, they can provide inspiration for designing suitable models.",2 Related Work,[0],[0]
Natural Language Object Retrieval: Language grounding addresses the task of object or moment localization in an image or video from a natural language description.,2 Related Work,[0],[0]
"For image-based object grounding, there has been much work on phrase grounding (Plummer et al., 2015; Wang et al., 2016b; Rohrbach et al., 2016) and referring expression comprehension (Hu et al., 2016; Yu et al., 2016; Nagaraja et al., 2016; Yu et al., 2017, 2018b).",2 Related Work,[0],[0]
"Recent work (Vasudevan et al., 2018) extends the grounding task to the video domain.",2 Related Work,[0],[0]
"Most recently, moment localization was proposed in (Hendricks et al., 2017; Gao et al., 2017), where the goal is to localize a short moment from a long video sequence given a query description.",2 Related Work,[0],[0]
Accurate temporal grounding is a necessary step to answering our compositional questions.,2 Related Work,[0],[0]
"We collected our dataset on 6 long-running TV shows from 3 genres: 1) sitcoms: The Big Bang Theory, How I Met Your Mother, Friends, 2) medical dramas: Grey’s Anatomy, House, 3) crime drama: Castle.",3.1 Dataset Collection,[0],[0]
There are in total 925 episodes spanning 461 hours.,3.1 Dataset Collection,[0],[0]
Each episode was then segmented into short clips.,3.1 Dataset Collection,[0],[0]
"We first created clips every 60/90 seconds, then shifted temporal boudaries to avoid splitting subtitle sentences between clips.",3.1 Dataset Collection,[0],[0]
"Shows that are mainly conversational based, e.g., The Big Bang Theory, were segmented into 60 seconds clips, while shows that are less cerebral, e.g. Castle, were segmented into 90 seconds clips.",3.1 Dataset Collection,[0],[0]
"In the end, 21,793 clips were prepared for QA collection, accompanied with subtitles and aligned with transcripts to add character names.",3.1 Dataset Collection,[0],[0]
"A
sample clip is shown in Fig. 1.",3.1 Dataset Collection,[0],[0]
"Amazon Mechanical Turk was used for VQA collection on video clips, where workers were presented with both videos and aligned named subtitles, to encourage multimodal questions requiring both vision and language understanding to answer.",3.1 Dataset Collection,[0],[0]
Workers were asked to create questions using a compositional-question format:,3.1 Dataset Collection,[0],[0]
[What/How/Where/Why/...],3.1 Dataset Collection,[0],[0]
[when/before/after] .,3.1 Dataset Collection,[0],[0]
"The second part of each question serves to localize the relevant video moment within a clip, while the first part poses a question about that moment.",3.1 Dataset Collection,[0],[0]
"This compositional format also serves to encourage questions that require both visual and language understanding to answer, since people often naturally use visual signals to ground questions in time, e.g. What was House saying before he leaned over the bed?",3.1 Dataset Collection,[0],[0]
"During data collection, we only used prompt words (when/before/after) to encourage workers to propose the desired, complex compositional questions.",3.1 Dataset Collection,[0],[0]
There were no additional template constraints.,3.1 Dataset Collection,[0],[0]
"Therefore, most of the language in the questions is relatively free-form and complex.
",3.1 Dataset Collection,[0],[0]
"Ultimately, workers pose 7 different questions for each video clip.",3.1 Dataset Collection,[0],[0]
"For each question, we asked workers to annotate the exact video portion required to answer the question by marking the START and END timestamps as in Krishna et al. (2017).",3.1 Dataset Collection,[0],[0]
"In addition, they provide 1 correct and 4 wrong answers for each question.",3.1 Dataset Collection,[0],[0]
Workers get paid $1.3 for a single video clip annotation.,3.1 Dataset Collection,[0],[0]
"The whole collection process took around 3 months.
",3.1 Dataset Collection,[0],[0]
"To ensure the quality of the questions and answers, we set up an online checker in our collection interface to verify the question format, allowing only questions that reflect our two-step format to be submitted.",3.1 Dataset Collection,[0],[0]
The collection was done in batches of 500 videos.,3.1 Dataset Collection,[0],[0]
"For each harvested batch, we sampled 3 pairs of submitted QAs from each worker and checked the semantic correctness of the questions, answers, and timestamps.",3.1 Dataset Collection,[0],[0]
Multiple Choice QAs:,3.2 Dataset Analysis,[0],[0]
"Our QAs are multiple choice questions with 5 candidate answers for each question, for which only one is correct.",3.2 Dataset Analysis,[0],[0]
Table 1 provides statistics of the QAs based on the first question word.,3.2 Dataset Analysis,[0],[0]
"On average, our questions contain 13.5 words, which is fairly long compared to other datasets.",3.2 Dataset Analysis,[0],[0]
"In general, correct answers tend
to be slightly longer than wrong answers.",3.2 Dataset Analysis,[0],[0]
Fig. 2 shows the distribution of different questions types.,3.2 Dataset Analysis,[0],[0]
"Note “what” (Abstract, Object, Action), “who” (Person), “why” (Reasoning) and “where” (Location) questions form a large part of our data.
",3.2 Dataset Analysis,[0],[0]
The negative answers in TVQA are written by human annotators.,3.2 Dataset Analysis,[0],[0]
They are instructed to write false but relevant answers to make the negatives challenging.,3.2 Dataset Analysis,[0],[0]
"Alternative methods include sampling negative answers from other questions’ correct answers, either based on semantic similarity (Das et al., 2017; Jang et al., 2017) or randomly (Antol et al., 2015; Das et al., 2017).",3.2 Dataset Analysis,[0],[0]
"The former is prone to introducing paraphrases of the ground-truth answer (Zhu et al., 2016).",3.2 Dataset Analysis,[0],[0]
"The latter avoids the problem of paraphrasing, but generally produces irrelevant negative choices.",3.2 Dataset Analysis,[0],[0]
We show in Table 8 that our human written negatives are more challenging than randomly sampled negatives.,3.2 Dataset Analysis,[0],[0]
Moment Localization:,3.2 Dataset Analysis,[0],[0]
The second part of our question is used to localize the most relevant video portion to answer the question.,3.2 Dataset Analysis,[0],[0]
"The prompt of “when”, “after”, “before” account for 60.03%, 30.19% and 9.78% respectively of our dataset.",3.2 Dataset Analysis,[0],[0]
TVQA provides the annotated START and END timestamps for each QA.,3.2 Dataset Analysis,[0],[0]
"We show the annotated
segment lengths in Fig. 3.",3.2 Dataset Analysis,[0],[0]
We found most of the questions rely on relatively short moments (less than 15 secs) within a longer clip (60-90 secs).,3.2 Dataset Analysis,[0],[0]
Differences among our 6 TV Shows: The videos used in our dataset are from 6 different TV shows.,3.2 Dataset Analysis,[0],[0]
Table 2 provides statistics for each show.,3.2 Dataset Analysis,[0],[0]
A good way to demonstrate the difference among questions from TV shows is to show their top unique nouns.,3.2 Dataset Analysis,[0],[0]
"In Table 3, we present such an analysis.",3.2 Dataset Analysis,[0],[0]
"The top unique nouns in sitcoms (BBT, Friends, HIMYM) are mostly daily objects, scenes and actions, while medical dramas (Grey, House) questions contain more medical terms, and crime shows (Castle) feature detective terms.",3.2 Dataset Analysis,[0],[0]
"Although similar, there are also notable differences among shows in the same genre.",3.2 Dataset Analysis,[0],[0]
"For example, BBT con-
tains “game” and “laptop” while HIMYM contains “bar” and “beer”, indicating the different major activities and topics in each show.",3.2 Dataset Analysis,[0],[0]
"Additionally, questions about different characters also mention different words, as shown in Table 4.",3.2 Dataset Analysis,[0],[0]
Comparison with Other Datasets: Table 5 presents a comparison of our dataset to some recently proposed video question answering datasets.,3.2 Dataset Analysis,[0],[0]
"In terms of total length of videos, TVQA is the largest, with a total of 461.2 hours of videos.",3.2 Dataset Analysis,[0],[0]
"MovieQA (Tapaswi et al., 2016) is most similar to our dataset, with both multiple choice questions and timestamp annotation.",3.2 Dataset Analysis,[0],[0]
"However, their questions and answers are constructed by people posing questions from a provided plot summary, then later aligned to the video clips, which makes most of their questions text oriented.",3.2 Dataset Analysis,[0],[0]
"Human Evaluation on Usefulness of Video and Subtitle in Dataset: To gain a better understand-
ing of the roles of videos and subtitles in the our dataset, we perform a human study, asking different groups of workers to complete the QA task in settings while observing different sources (subsets) of information:
• Question only.",3.2 Dataset Analysis,[0],[0]
• Video and Question.,3.2 Dataset Analysis,[0],[0]
•,3.2 Dataset Analysis,[0],[0]
Subtitle and Question.,3.2 Dataset Analysis,[0],[0]
"• Video, Subtitle, and Question.
",3.2 Dataset Analysis,[0],[0]
We made sure the workers that have written the questions did not participate in this study and that workers see only one of the above settings for answering each question.,3.2 Dataset Analysis,[0],[0]
Human accuracy on our test set under these 4 settings are reported in Table 5.,3.2 Dataset Analysis,[0],[0]
"As expected, compared to human accuracy based only on question-answer pairs (Q), adding videos (V+Q), or subtitles (S+Q) significantly improves human performance.",3.2 Dataset Analysis,[0],[0]
Adding both videos and subtitles (V+S+Q) brings the accuracy to 89.41%.,3.2 Dataset Analysis,[0],[0]
"This indicates that in order to answer the questions correctly, both visual and textual understanding are essential.",3.2 Dataset Analysis,[0],[0]
"We also observe that workers obtain 31.84% accuracy given questionanswer pairs only, which is higher than random guessing (20%).",3.2 Dataset Analysis,[0],[0]
We ascribe this to people’s prior knowledge about the shows.,3.2 Dataset Analysis,[0],[0]
"Note, timestamp annotations are not provided in these experiments.",3.2 Dataset Analysis,[0],[0]
We introduce a multi-stream end-to-end trainable neural network for Multi-Modal Video Question Answering.,4 Methods,[0],[0]
Fig. 4 gives an overview of our model.,4 Methods,[0],[0]
"Formally, we define the inputs to the model as: a 60-90 second video clip V , a subtitle S, a question q, and five candidate answers {ai}4i=0.",4 Methods,[0],[0]
Frames are extracted at 3 fps.,4.1 Video Features,[0],[0]
"We run Faster RCNN (Ren et al., 2015b) trained on the Visual
Genome (Krishna et al., 2017) to detect object and attribute regions in each frame.",4.1 Video Features,[0],[0]
Both regional features and predicted detection labels can be used as model inputs.,4.1 Video Features,[0],[0]
"We also use ResNet101 (He et al., 2016) trained on ImageNet (Deng et al., 2009) to extract whole image features.",4.1 Video Features,[0],[0]
"Regional Visual Features: On average, our videos contain 229 frames, with 16 detections per frame.",4.1 Video Features,[0],[0]
It is not trivial to model such long sequences.,4.1 Video Features,[0],[0]
"For simplicity, we follow (Anderson et al., 2018; Karpathy and Fei-Fei, 2015) selecting the top-K regions1 from each detected label across all frames.",4.1 Video Features,[0],[0]
Their regional features are L2normalized and stacked together to form our visual representation V reg ∈ Rnreg×2048.,4.1 Video Features,[0],[0]
Here nreg is the number of selected regions.,4.1 Video Features,[0],[0]
"Visual Concept Features: Recent work (Yin and Ordonez, 2017) found that using detected object
1Based on cross-validation, we find K=6 to perform best.
labels as input to an image captioning system gave comparable performance to using CNN features directly.",4.1 Video Features,[0],[0]
"Inspired by this work, we also experiment with using detected labels as visual inputs.",4.1 Video Features,[0],[0]
"As shown in Fig. 5, we are able to detect rich visual concepts, including both objects and attributes, e.g. ”white basket”, which could be used to answer “What is Sheldon holding in his hand when everyone is at the door”.",4.1 Video Features,[0],[0]
We first gather detected concepts over all the frames to represent concept presence.,4.1 Video Features,[0],[0]
"After removing duplicate concepts, we use GloVe (Pennington et al., 2014) to embed the words.",4.1 Video Features,[0],[0]
"The resulting video representation is denoted as V cpt ∈ Rncpt×300, where ncpt is the number of unique concepts.",4.1 Video Features,[0],[0]
ImageNet Features: We extract the pooled 2048D feature of the last block of ResNet101.,4.1 Video Features,[0],[0]
"Features from the same video clip are L2 normalized and stacked, denoted as V img ∈ Rnimg×2048, where nimg is the number of frames extracted from the video clip.",4.1 Video Features,[0],[0]
We use a bi-directional LSTM (BiLSTM) to encode both textual and visual sequences.,4.2 LSTM Encoders for Video and Text,[0],[0]
"A subtitle S, which contains a set of sentences, is flattened into a long sequence of words and GloVe (Pennington et al., 2014) is used to embed the words.",4.2 LSTM Encoders for Video and Text,[0],[0]
"We stack the hidden states of the BiLSTM from both directions at each timestep to obtain the subtitle representation HS ∈ RnS×2d, where nS is the number of subtitle words, d is the hidden size of the BiLSTM (set to 150 in our experiments).",4.2 LSTM Encoders for Video and Text,[0],[0]
"Similarly, we encode question Hq ∈ Rnq×2d, candidate answers Hai ∈ Rnai×2d, and visual con-
cepts Hcpt ∈ Rncpt×2d.",4.2 LSTM Encoders for Video and Text,[0],[0]
"nq and nai are the number of words in question and answer ai, respectively.",4.2 LSTM Encoders for Video and Text,[0],[0]
"Regional features V reg and ImageNet features V img are first projected into word vector space using a non-linear layer with tanh activation, then encoded using the same BiLSTM to obtain the regional representations Hreg ∈ Rnreg×2d and H img ∈ Rnimg×2d, respectively.",4.2 LSTM Encoders for Video and Text,[0],[0]
"We use a context matching module and BiLSTM to jointly model the contextual inputs (subtitle, video) and query (question-answer pair).",4.3 Joint Modeling of Context and Query,[0],[0]
"The context matching module is adopted from the contextquery attention layer from previous works (Seo et al., 2017; Yu et al., 2018a).",4.3 Joint Modeling of Context and Query,[0],[0]
"It takes context vectors and query vectors as inputs and produces a set of context-aware query vectors based on the similarity between each context-query pair.
",4.3 Joint Modeling of Context and Query,[0],[0]
"Taking the regional visual feature stream as an example (Fig. 4 upper stream), where Hreg is used as context input2.",4.3 Joint Modeling of Context and Query,[0],[0]
"The question embedding, Hq, and answer embedding, Hai , are used as queries.",4.3 Joint Modeling of Context and Query,[0],[0]
"After feeding context-query pairs into the context matching module, we obtain a video-aware-question representation, Greg,q ∈ Rnreg×2d, and video-aware-answer representation, Greg,ai ∈ Rnreg×2d, which are then fused with video context:
M reg,ai = [Hreg;Greg,q;Greg,ai ;
Hreg Greg,q;Hreg Greg,ai ],
where is element-wise product.",4.3 Joint Modeling of Context and Query,[0],[0]
"The fused feature, M reg,ai ∈ Rnreg×10d, is fed into another BiLSTM.",4.3 Joint Modeling of Context and Query,[0],[0]
"Its hidden states, U reg,ai ∈ Rnreg×10d, are max-pooled temporally to get the final vector, ureg,ai ∈ R10d, for answer ai.",4.3 Joint Modeling of Context and Query,[0],[0]
"We use a linear layer with softmax to convert {ureg,ai}4i=0 into answer probabilities.",4.3 Joint Modeling of Context and Query,[0],[0]
"Similarly, we can compute the answer probabilities given subtitle as context (Fig. 4 bottom stream).",4.3 Joint Modeling of Context and Query,[0],[0]
"When multiple streams are used, we simply sum up the scores from each stream as the final score (Wang et al., 2016a).",4.3 Joint Modeling of Context and Query,[0],[0]
"For evaluation, we introduce several baselines and compare them to our proposed model.
",5 Experiments,[0],[0]
"2For visual concept features and ImageNet features, we simply replace Hreg with Hcpt or Himg as the context.
",5 Experiments,[0],[0]
"In all experiments, setup is as follows.",5 Experiments,[0],[0]
"We split the TVQA dataset into 80% training, 10% validation, and 10% testing splits such that videos and their corresponding QA pairs appear in only one split.",5 Experiments,[0],[0]
"This results in 122,039 QA pairs for training, 15,253 QA pairs for validation, and 15,253 QA pairs for testing.",5 Experiments,[0],[0]
We evaluate each model using multiple-choice question answering accuracy.,5 Experiments,[0],[0]
"Longest Answer: Table 1 indicates that the average length of the correct answers is longer than the wrong ones; thus, our first baseline simply selects the longest answer for each question.",5.1 Baselines,[0],[0]
"Nearest Neighbor Search: In this baseline, we use Nearest Neighbor Search (NNS) to compute the closest answer to our question or subtitle.",5.1 Baselines,[0],[0]
"We embed sentences into vectors using TFIDF, SkipThought (Kiros et al., 2015), or averaged GloVe (Pennington et al., 2014) word vectors, then compute the cosine similarity for each questionanswer pair or subtitle-answer pair.",5.1 Baselines,[0],[0]
"For TFIDF, we use bag-of-words to represent the sentences, assigning a TFIDF value for each word.",5.1 Baselines,[0],[0]
Retrieval:,5.1 Baselines,[0],[0]
"Due to the size of TVQA, there may exist similar questions and answers in the dataset.",5.1 Baselines,[0],[0]
"Thus, we also implement a baseline two-step retrieval approach: given a question and a set of candidate answers, we first retrieve the most relevant question in the training set, then pick the candidate answer that is closest to the retrieved question’s correct answer.",5.1 Baselines,[0],[0]
"Similar approaches have also been used in dialogue systems (Jafarpour and Burges, 2010; Leuski and Traum, 2011), picking the appropriate responses to an utterance from a predefined human conversational corpus.",5.1 Baselines,[0],[0]
"Similar to NNS, we use TFIDF, SkipThought, and GloVe vectors with cosine similarity.",5.1 Baselines,[0],[0]
Table 6 shows results from baseline methods and our proposed neural model.,5.2 Results,[0],[0]
"Our main results are obtained by using full-length video clips and subtitles, without using timestamps (w/o ts).",5.2 Results,[0],[0]
We also run the same experiments using the localized video and subtitle segment specified by the ground truth timestamps (w/ ts).,5.2 Results,[0],[0]
"If not indicated explicitly, the numbers described below are from the experiments on full-length video clips and subtitles.",5.2 Results,[0],[0]
"Baseline Comparison: Row 1 shows results of the longest answer baseline, achieving 30.41%
(compared to random chance at 20%).",5.2 Results,[0],[0]
"As expected, the retrieval-based methods (row 2-4) and the answer-question similarity based methods (row 5-7) perform rather poorly, since no contexts (video or subtitle) are considered.",5.2 Results,[0],[0]
"When using subtitle-answer similarity to choose correct answers, Glove, SkipThought, and TFIDF based approaches (row 8-10) all achieve significant improvement over question-answer similarity.",5.2 Results,[0],[0]
"Notably, TFIDF (row 10) answers 49.94% of the questions correctly.",5.2 Results,[0],[0]
"Since our questions are raised by people watching the videos, it is natural for them to ask questions about specific and unique objects/locations/etc., mentioned in the subtitle.",5.2 Results,[0],[0]
"Thus, it is not surprising that TFIDF based similarity between answer and subtitle performs so well.",5.2 Results,[0],[0]
Variants of Our Model: Rows 11-18 show results of our model with different contextual inputs and features.,5.2 Results,[0],[0]
The model that only uses questionanswer pairs (row 11) achieves 43.34% accuracy.,5.2 Results,[0],[0]
"Compared to the subtitle model (row 15), adding video as additional sources (row 16-18) improves performance.",5.2 Results,[0],[0]
"Interestingly, adding video to the question only model (row 11) do not work as well (row 12-14).",5.2 Results,[0],[0]
"Our hypothesis is that the video feature streams may be struggling to learn models for answering textual questions, which degrades
their ability to answer visual questions.",5.2 Results,[0],[0]
"Overall, the best performance is achieved by using all the contextual sources, including subtitles and videos (using concept features, row 18).",5.2 Results,[0],[0]
Comparison with Human Performance: Human performance without timestamp annotation is shown in Table 5.,5.2 Results,[0],[0]
"When using only questions (Table 6 row 11), our model outperforms humans (43.34% vs 31.84%) as it has access to all statistics of the questions and answers.",5.2 Results,[0],[0]
"When using videos or subtitles or both, humans perform significantly better than the models.",5.2 Results,[0],[0]
Models with Timestamp Annotation: Columns under w/o ts and w/ ts show a comparison between the same model using full-length videos/subtitles and using timestamp localized videos/subtitles.,5.2 Results,[0],[0]
"With timestamp annotation, the models perform consistently better than their counterpart without this information, indicating that localization is helpful for question answering.",5.2 Results,[0],[0]
"Accuracy for Different Question Types: To gain further insight, we examined the accuracy of our models on different question types on the validation set (results in Table 7), all models using timestamp annotation.",5.2 Results,[0],[0]
"Compared to S+Q model, S+V+Q models get the most improvements on “what” and “where” questions, indicating these questions require additional visual information.",5.2 Results,[0],[0]
"On the other hand, adding video features did not improve S+Q performance on questions relying more on textual reasoning, e.g., “how” questions.",5.2 Results,[0],[0]
"Human-Written Negatives vs. RandomlySampled Negatives For comparison, we create a new answer set by replacing the original human written negative answers with randomly sampled negative answers.",5.2 Results,[0],[0]
"To produce relevant negative answers, for each question, negatives are sampled (from the other QA pairs) within the same show.
",5.2 Results,[0],[0]
Results are shown in Table 8.,5.2 Results,[0],[0]
"Performance on randomly sampled negatives is much higher than that of human written negatives, indicating that human written negatives are more challenging.",5.2 Results,[0],[0]
Qualitative Analysis: Fig. 6 shows example predictions from our S+V+Q model (row 18) using full-length video and subtitle.,5.2 Results,[0],[0]
Fig. 6a and Fig.,5.2 Results,[0],[0]
6b demonstrate its ability to solve both grounded visual questions and textual reasoning question.,5.2 Results,[0],[0]
Bottom row shows two incorrect predictions.,5.2 Results,[0],[0]
We found that wrong inferences are mainly due to incorrect language inferences and the model’s lack of common sense knowledge.,5.2 Results,[0],[0]
"For example, Fig.",5.2 Results,[0],[0]
"6c, the characters are talking about radiology, the model is distracted to believe they are in the radiology department, while Fig. 6d shows a case of questions that need common sense to answer, rather than simply textual or visual cues.",5.2 Results,[0],[0]
"We presented the TVQA dataset, a large-scale, localized, compositional video question answering dataset.",6 Conclusion,[0],[0]
We also proposed two QA tasks (with/without timestamps) and provided baseline experiments as a benchmark for future comparison.,6 Conclusion,[0],[0]
"Our experiments show both visual and textual understanding are necessary for TVQA.
",6 Conclusion,[0],[0]
There is still a significant gap between the proposed baselines and human performance on the QA accuracy.,6 Conclusion,[0],[0]
We hope this novel multimodal dataset and the baselines will encourage the community to develop stronger models in future work.,6 Conclusion,[0],[0]
"To narrow the gap, one possible direction is to enhance the interactions between videos and subtitles to improve multimodal reasoning ability.",6 Conclusion,[0],[0]
"Another direction is to exploit human-object relations in the video and subtitle, as we observe that a large number of questions involve such relations.",6 Conclusion,[0],[0]
"Additionally, temporal reasoning is crucial for answering the TVQA questions.",6 Conclusion,[0],[0]
"Thus, future work also includes integrating better temporal cues.",6 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments and discussions.,Acknowledgments,[0],[0]
"This research is supported by NSF Awards #1633295, 1562098, 1405822 and a Google Faculty Research Award, Bloomberg Data Science Research Grant, and ARO-YIP Award #W911NF-18-1-0336.",Acknowledgments,[0],[0]
The views contained in this article are those of the authors and not of the funding agency.,Acknowledgments,[0],[0]
Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks.,abstractText,[0],[0]
"However, due to data limitations, there has been much less work on video-based QA.",abstractText,[0],[0]
"In this paper, we present TVQA, a largescale video QA dataset based on 6 popular TV shows.",abstractText,[0],[0]
"TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video.",abstractText,[0],[0]
"Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts.",abstractText,[0],[0]
We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task.,abstractText,[0],[0]
The dataset is publicly available at http://tvqa.cs.unc.edu.,abstractText,[0],[0]
"TVQA: Localized, Compositional Video Question Answering",title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1415–1425 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1415",text,[0],[0]
"Language on Twitter diverges from well-edited Mainstream American English (MAE, also called Standard American English) in a number of ways, presenting significant challenges to current NLP tools.",1 Introduction,[0],[0]
"It contains, among other phenomena, nonstandard spelling, punctuation, capitalization, and syntax, as well as Twitter-specific conventions such as hashtags, usernames, and retweet tokens (Eisenstein, 2013).",1 Introduction,[0],[0]
"Additionally, it contains an abundance of dialectal language, includ-
ing African-American English (AAE), a dialect of American English spoken by millions of individuals, which contains lexical, phonological, and syntactic features not present in MAE (Green, 2002; Stewart, 2014; Jones, 2015).
",1 Introduction,[0],[0]
"Since standard English NLP tools are typically trained on well-edited MAE text, their performance is degraded on Twitter, and even more so for AAE tweets compared to MAE tweets— gaps exist for part-of-speech tagging (Jørgensen et al., 2016), language identification, and dependency parsing (Blodgett et al., 2016; Blodgett and O’Connor, 2017).",1 Introduction,[0],[0]
"Expanding the linguistic coverage of NLP tools to include minority and colloquial dialects would help support equitable language analysis across sociolinguistic communities, which could help information retrieval, translation, or opinion analysis applications (Jurgens et al., 2017).",1 Introduction,[0],[0]
"For example, sentiment analysis systems ought to count the opinions of all types of people, whether they use standard dialects or not.
",1 Introduction,[0],[0]
"In this work, we broaden Universal Dependencies (Nivre et al., 2016)",1 Introduction,[0],[0]
"parsing1 to better handle social media English, in particular social media AAE.",1 Introduction,[0],[0]
"First, we develop standards to handle Twitter-specific and AAE-specific features within Universal Dependencies 2.0 (§3), by selecting and annotating a new dataset of 500 tweets, 250 of which are in AAE.
",1 Introduction,[0],[0]
"Second, we evaluate several state-of-the-art dependency parsers, finding that, as expected, they perform poorly on our dataset relative to the UD English Treebank (§4).",1 Introduction,[0],[0]
"Third, since the UD English Treebank contains substantial amounts of traditional MAE data for training, we investigate cross-domain training methods to improve Twitter AAE dependency parsing with no, or very little,
1http://universaldependencies.org/
in-domain labeled data, by using Twitter-specific taggers, embeddings, and a novel heuristic training data synthesis procedure.",1 Introduction,[0],[0]
This helps close some of the gap between MAE and AAE performance.,1 Introduction,[0],[0]
"Finally, we provide an error analysis of the parsers’ performance on AAE lexical and syntactic constructions in our dataset (§5.4).2",1 Introduction,[0],[0]
Parsing for noisy social media data presents interesting and significant challenges.,2.1 Parsing for Twitter,[0],[0]
"Foster et al. (2011) develop a dataset of 519 constituencyannotated English tweets, which were converted to Stanford dependencies.",2.1 Parsing for Twitter,[0],[0]
Their analysis found a substantial drop in performance of an off-the-shelf dependency parser on the new dataset compared to a WSJ test set.,2.1 Parsing for Twitter,[0],[0]
"Sanguinetti et al. (2017) annotated a dataset of 6,738 Italian tweets according to UD 2.0 and examined the performance of two parsers on the dataset, finding that they lagged considerably relative to performance on the Italian UD Treebank.
Kong et al. (2014) develop an English dependency parser designed for Twitter, annotating a dataset of 929 tweets (TWEEBANK V1) according to the unlabeled FUDG dependency formalism (Schneider et al., 2013).",2.1 Parsing for Twitter,[0],[0]
"It has substantially different structure than UD (for example, prepositions head PPs, and auxiliaries govern main verbs).
",2.1 Parsing for Twitter,[0],[0]
"More recently, Liu et al. (2018) developed TWEEBANK V2, fully annotating TWEEBANK V1 according to UD 2.0 and annotating additionally sampled tweets, for a total of 3,550 tweets.",2.1 Parsing for Twitter,[0],[0]
"They found that creating consistent annotations was challenging, due to frequent ambiguities in interpreting tweets; nevertheless, they were able to train a pipeline for tokenizing, tagging, and parsing the tweets, and develop ensemble and distillation models to improve parsing accuracy.",2.1 Parsing for Twitter,[0],[0]
"Our work encounters similar challenges; in our approach, we intentionally oversample AAE-heavy messages for annotation, detail specific annotation decisions for AAE-specific phenomena (§3.2), and analyze parser performance between dialects and for particular constructions (§5.3–5.4).",2.1 Parsing for Twitter,[0],[0]
"Future work may be able to combine these annotations for effective multi-dialect Twitter UD parsers, which
2Our annotated dataset and trained dependency parser are available at http://slanglab.cs.umass.edu/TwitterAAE/ and annotations are available in the public Universal Dependencies repository.
may allow for the use of pre-existing downstream tools like semantic relation extractors (e.g. White et al. (2016)).
",2.1 Parsing for Twitter,[0],[0]
"One line of work for parsing noisy social media data, including Khan et al. (2013) and Nasr et al. (2016), examines the effects of the domain mismatches between traditional sources of training data and social media data, finding that matching the data as closely as possible aids performance.",2.1 Parsing for Twitter,[0],[0]
"Other work focuses on normalization, including Daiber and van der Goot (2016) and van der Goot and van Noord (2017), which develop a dataset of 500 manually normalized and annotated tweets, and uses normalization within a parser.",2.1 Parsing for Twitter,[0],[0]
"Separately, Zhang et al. (2013) created a domain-adaptable, parser-focused system by directly linking parser performance to normalization performance.",2.1 Parsing for Twitter,[0],[0]
"For Arabic dialects, Chiang et al. (2006) parse Levantine Arabic by projecting parses from Modern Standard Arabic translations, while Green and Manning (2010) conduct extensive error analysis of Arabic constituency parsers and the Penn Arabic Treebank.",2.2 Parsing for Dialects,[0],[0]
Scherrer (2011) parse Swiss German dialects by transforming Standard German phrase structures.,2.2 Parsing for Dialects,[0],[0]
"We continue in this line of work in our examination of AAE-specific syntactic structures and generation of synthetic data with such structures (§4.2.1).
",2.2 Parsing for Dialects,[0],[0]
Less work has examined parsing dialectal language on social media.,2.2 Parsing for Dialects,[0],[0]
"Recently, Wang et al. (2017) annotate 1,200 Singlish (Singaporean English) sentences from a Singaporean talk forum, selecting sentences containing uniquely Singaporean vocabulary items.",2.2 Parsing for Dialects,[0],[0]
"Like other work, they observe a drop in performance on dialectal Singlish text, but increase performance through a stacking-based domain adaptation method.",2.2 Parsing for Dialects,[0],[0]
"Our dataset contains 500 tweets, with a total of 5,951 non-punctuation edges, sampled from the publicly available TwitterAAE corpus.3 Each tweet in that corpus is accompanied by a model’s demographically-aligned topic model probabilities jointly inferred from Census demographics and word likelihood by Blodgett et al. (2016), including the African-American and White topics.
",3.1 Dataset,[0],[0]
"3http://slanglab.cs.umass.edu/TwitterAAE/
We create a balanced sample to get a range of dialectal language, sampling 250 tweets from those where the African-American topic has at least 80% probability, and 250 from those where the White topic has at least 80% probability.",3.1 Dataset,[0],[0]
"We refer to these two subcorpora as AA and WH; Blodgett et al. (2016) showed the former exhibits linguistic features typical of AAE.
",3.1 Dataset,[0],[0]
"The 250 AA tweets include many alternate spellings of common words that correspond to well-known phonological phenomena—including da, tha (the), dat, dhat (that), dis, dhis (this), ion, iont (I don’t), ova (over), yo (your), dere, der (there), den, dhen (then), ova (over), and nall, null (no, nah)—where each of the mentioned italicized AAE terms appears in the AAE data, but never in the MAE data.",3.1 Dataset,[0],[0]
We examine these lexical variants more closely in §5.4.,3.1 Dataset,[0],[0]
"Across the AA tweets, 18.0% of tokens were not in a standard English dictionary, while the WH tweets’ OOV rate was 10.7%.4 We further observe a variety of AAE syntactic phenomena in our AA tweets, several of which are described in §3.2 and §5.4.",3.1 Dataset,[0],[0]
"To effectively measure parsing quality and develop better future models, we first focus on developing high-quality annotations for our dataset, for which we faced a variety of challenges.",3.2 Annotation,[0],[0]
"We detail our annotation principles using Universal Dependency 2.0 relations (Nivre et al., 2016).
",3.2 Annotation,[0],[0]
"All tweets were initially annotated by two annotators, and disagreements resolved by one of the annotators.",3.2 Annotation,[0],[0]
"Annotation decisions for several dozen tweets were discussed in a group of three annotators early in the annotation process.
",3.2 Annotation,[0],[0]
"Our annotation principles are in alignment with those proposed by Liu et al. (2018), with the exception of contraction handling, which we discuss briefly in §3.2.2.",3.2 Annotation,[0],[0]
"The AAE dialect is prominently characterized by the drop of copulas, which can occur when the copula is present tense, not first person, not accented, not negative, and expressing neither the habitual nor the remote present perfect tenses (Green, 2002).",3.2.1 Null Copulas,[0],[0]
"We frequently observed null copulas, as in:
4The dictionary of 123,377 words with American spellings was generated using http://wordlist.aspell.net/.
If u wit me den u pose to RESPECT ME
nsubjnsubj
“If you (are) with me, then you (are) supposed to respect me”
The first dropped are is a null copula; UD2.0 would analyze the MAE version as you nsubj←−− me cop−→ are, which we naturally extend to analyze the null copula by simply omitting cop (which is now over a null element, so cannot exist in a dependency graph).",3.2.1 Null Copulas,[0],[0]
"The second are is a null auxiliary (in MAE, you nsubj←−− supposed aux−→ are), a tightly related phenomenon (for example, Green et al. (2007) studies both null copulas and null auxiliary be in infant AAE), which we analyze similarly by simply omitting the aux edge.",3.2.1 Null Copulas,[0],[0]
"We observed AAE verbal auxiliaries, e.g.,
fees be looking upside my head
aux
Now we gone get fucked up
aux
damnnn I done let alot of time pass by
aux
including habitual be (“Continually, over and over, fees are looking at me...”), future gone (“we are going to get...”), and completive done (“I did let time pass by,” emphasizing the speaker completed a time-wasting action).
",3.2.2 AAE Verbal Auxiliaries,[0],[0]
"We attach the auxiliary to the main verb with the aux relation, as UD2.0 analyzes other English auxiliaries (e.g. would or will).",3.2.2 AAE Verbal Auxiliaries,[0],[0]
"We observed many instances of quasi-auxiliary, “- to” shortened verbs such as wanna, gotta, finna, bouta, tryna, gonna, which can be glossed as want to, got to, fixing to, about to, etc.",3.2.3 Verbs: Auxiliaries vs. Main Verbs,[0],[0]
"They control modality, mood and tense—for example, finna and bouta denote an immediate future tense; Green (2002, ch. 2) describes finna as a preverbal marker.",3.2.3 Verbs: Auxiliaries vs. Main Verbs,[0],[0]
"From UD’s perspective, it is difficult to decide if they should be subordinate auxiliaries or main verbs.",3.2.3 Verbs: Auxiliaries vs. Main Verbs,[0],[0]
"In accordance with the UD Treebank’s handling of MAE want to X and going to X as main verbs (want xcomp−−→ X), we analyzed them similarly, e.g.
Lol he bouta piss me off “He is about to piss me off”
xcomp
This is an instance of a general principle that, if there is a shortening of an MAE multiword phrase into a single word, the annotations on that word should mirror the edges in and out of the original phrase’s subgraph (as in Schneider et al. (2013)’s fudge expressions).
",3.2.3 Verbs: Auxiliaries vs. Main Verbs,[0],[0]
"However, in contrast to the UD Treebank, we did not attempt to split up these words into their component words (e.g. wanna → want to), since to do this well, it would require a more involved segmentation model over the dozens or even hundreds of alternate spellings each of the above can take;5",3.2.3 Verbs: Auxiliaries vs. Main Verbs,[0],[0]
we instead rely on Owoputi et al. (2013); O’Connor et al. (2010)’s rule-based tokenizer that never attempts to segment within such shortenings.,3.2.3 Verbs: Auxiliaries vs. Main Verbs,[0],[0]
"This annotation principle is in contrast to that of Liu et al. (2018), which follows UD tokenization for contractions.",3.2.3 Verbs: Auxiliaries vs. Main Verbs,[0],[0]
We also encountered many issues general to Twitter but not AAE; these are still important to deal with since AAE tweets include more non-standard linguistic phenomena overall.,3.2.4 Non-AAE Twitter issues,[0],[0]
"When possible, we adapted Kong et al. (2014)’s annotation conventions into the Universal Dependencies context, which are the only published conventions we know of for Twitter dependencies (for the FUDG dependency formalism).",3.2.4 Non-AAE Twitter issues,[0],[0]
"Issues include:
• @-mentions, which require different treatment when they are terms of address, versus nominal elements within a sentence.
",3.2.4 Non-AAE Twitter issues,[0],[0]
"• Hashtags, which in their tag-like usage are utterances by themselves (#tweetliketheoppositegender",3.2.4 Non-AAE Twitter issues,[0],[0]
Oh damn .).,3.2.4 Non-AAE Twitter issues,[0],[0]
"or sometimes can be words with standard syntactic relations within the sentence (#She’s A Savage, having #She’s nsubj←−− Savage).",3.2.4 Non-AAE Twitter issues,[0],[0]
"Both hashtag and @- mention ambiguities are handled by Owoputi et al. (2013)’s POS tagger.
",3.2.4 Non-AAE Twitter issues,[0],[0]
"• Multiple utterances, since we do not attempt sentence segmentation, and in many cases sentential utterances are not separated by explicit punctuation.",3.2.4 Non-AAE Twitter issues,[0],[0]
"FUDG allows for multiple roots for a text, but UD does not; instead we follow UD’s convention of the parataxis relation for what they describe as “side-by-side run-on sentences.”
5For example, Owoputi et al. (2013)’s Twitter word cluster 0011000 has 36 forms of gonna alone: http://www.cs.",3.2.4 Non-AAE Twitter issues,[0],[0]
"cmu.edu/∼ark/TweetNLP/cluster viewer.html
• Emoticons and emoji, which we attach as discourse relations to the utterance root, following UD’s treatment of interjections.
",3.2.4 Non-AAE Twitter issues,[0],[0]
"• Collapsed phrases, like omw for “on my way.”",3.2.4 Non-AAE Twitter issues,[0],[0]
"When possible, we used the principle of annotating according to the root of the subtree of the original phrase.",3.2.4 Non-AAE Twitter issues,[0],[0]
"For example, UD 2.0 prescribes way xcomp−−→ get for the sentence",3.2.4 Non-AAE Twitter issues,[0],[0]
"On my way to get...; therefore we use omw xcomp−−→ get for omw to get.
",3.2.4 Non-AAE Twitter issues,[0],[0]
"• Separated words, like uh round for “around,” which we analyze as multiword phrases (flat or compound).
",3.2.4 Non-AAE Twitter issues,[0],[0]
We discuss details for these and other cases in the online appendix.,3.2.4 Non-AAE Twitter issues,[0],[0]
Our experiments use the following two parsers.,4.1 Models,[0],[0]
"UDPipe (Straka et al., 2016) is a neural pipeline containing a tokenizer, morphological analyzer, tagger, and transition-based parser intended to be easily retrainable.",4.1 Models,[0],[0]
"The parser attains 80.2% LAS (labeled attachment score) on the UD English treebank with automatically generated POS tags, and was a baseline system used in the CoNLL 2017 Shared Task (Zeman et al., 2017).6
Deep Biaffine (Dozat et al., 2017; Dozat and Manning, 2016) is a graph-based parser incorporating neural attention and biaffine classifiers for arcs and labels.",4.1 Models,[0],[0]
"We used the version of the parser in the Stanford CoNLL 2017 Shared Task submission, which attained 82.2% LAS on the UD English treebank with automatically generated tags, and achieved the best performance in the task.",4.1 Models,[0],[0]
The model requires pre-trained word embeddings.,4.1 Models,[0],[0]
7,4.1 Models,[0],[0]
"We considered a series of experiments within both a cross-domain scenario (§4.2.1), where we trained only on UD Treebank data, and an indomain scenario (§4.2.2) using small amounts of our labeled data.",4.2 Experimental Setup,[0],[0]
"We use the parsing systems’ default hyperparameters (e.g. minibatch size and learning rate) and the default training/development split of the treebank (both systems perform early stopping based on development set performance).
",4.2 Experimental Setup,[0],[0]
6https://github.com/ufal/udpipe 7https://github.com/tdozat/UnstableParser/,4.2 Experimental Setup,[0],[0]
"Morpho-Tagger vs. ARK POS tags: The UD Treebank contains extensive fine-grained POS and morphological information, on which UDPipe’s morphological analyzer and tagging system is originally trained.",4.2.1 Cross-Domain Settings,[0],[0]
"This rich information should be useful for parsing, but the analyzers may be highly error-prone on out-of-domain, dialectal Twitter data, and contribute to poor parsing performance.",4.2.1 Cross-Domain Settings,[0],[0]
"We hypothesize that higher quality, even if coarser, POS information should improve parsing.
",4.2.1 Cross-Domain Settings,[0],[0]
"To test this, we retrain UDPipe in two different settings.",4.2.1 Cross-Domain Settings,[0],[0]
We first retrain the parser component with fine-grained PTB-style POS tags and morphological information provided by the tagger component;8 we call this the Morpho-Tagger setting.,4.2.1 Cross-Domain Settings,[0],[0]
"Second, we retrain the parser with morphological information stripped and its tags predicted from the ARK Twitter POS tagger (Owoputi et al., 2013), which is both tailored for Twitter and displays a smaller AAE vs MAE performance gap than traditional taggers (Jørgensen et al., 2016); we call this the ARK Tagger setting.9 The ARK Tagger’s linguistic representation is impoverished compared to Morpho-Tagger: its coarse-grained POS tag system does not include tense or number information, for example.10
Synthetic Data:",4.2.1 Cross-Domain Settings,[0],[0]
"Given our knowledge of Twitter- and AAE-specific phenomena that do not occur in the UD Treebank, we implemented a rulebased method to help teach the machine-learned parser these phenomena; we generated synthetic data for three Internet-specific conventions and one set of AAE syntactic features.",4.2.1 Cross-Domain Settings,[0],[0]
(This is inspired by Scherrer (2011)’s rule transforms between Standard and Swiss German.),4.2.1 Cross-Domain Settings,[0],[0]
"We performed each of the following transformations separately on a copy of the UD Treebank data and concatenated the transformed files together for the final training and development files, so that each final file contained several transformed copies of the original UD Treebank data.
1. @-mentions, emojis, emoticons, expressions, and hashtags: For each sentence in the UD Treebank we inserted at least one @-mention, emoji, emoticon, expression (Internet-specific words and
8We also retrained this component, to maintain consistency of training and development split.",4.2.1 Cross-Domain Settings,[0],[0]
"We also remove the universal (coarse) POS tags it produces, replacing them with the same PTB tags.
",4.2.1 Cross-Domain Settings,[0],[0]
"9We strip lemmas from training and development files for both settings.
",4.2.1 Cross-Domain Settings,[0],[0]
"10Derczynski et al. (2013)’s English Twitter tagger, which outputs PTB-style tags, may be of interest for future work.
",4.2.1 Cross-Domain Settings,[0],[0]
"abbreviations such as lol, kmsl, and xoxo), or hashtag, annotated with the correct relation, at the beginning of the sentence.",4.2.1 Cross-Domain Settings,[0],[0]
"An item of the same type was repeated with 50% probability, and a second item was inserted with 50% probability.",4.2.1 Cross-Domain Settings,[0],[0]
@- mentions were inserted using the ATMENTION token and emojis using the EMOJI token.,4.2.1 Cross-Domain Settings,[0],[0]
"Emoticons were inserted from a list of 20 common emoticons, expressions were inserted from a list of 16 common expressions, and hashtags were sampled for insertion according to their frequency in a list of all hashtags observed in the TwitterAAE corpus.
2.",4.2.1 Cross-Domain Settings,[0],[0]
"Syntactically participating @-mentions: To replicate occurrences of syntactically participating @-mentions, for each sentence in the UD Treebank with at least one token annotated with an nsubj or obj relation and an NNP POS tag, we replaced one at random with the ATMENTION token.
3.",4.2.1 Cross-Domain Settings,[0],[0]
"Multiple utterances: To replicate occurrences of multiple utterances, we randomly collapsed pairs of two short sentences (< 15 tokens) together, attaching the root of the second to the root of the first with the parataxis relation.
4.",4.2.1 Cross-Domain Settings,[0],[0]
AAE preverbal markers and auxiliaries: We introduced instances of verbal constructions present in AAE that are infrequent or non-existent in the UD Treebank data.,4.2.1 Cross-Domain Settings,[0],[0]
"First, constructions such as going to, about to, and want to are frequently collapsed to gonna, bouta, and wanna, respectively (see §3.2.2); for each sentence with at least one of these constructions, we randomly chose one to collapse.",4.2.1 Cross-Domain Settings,[0],[0]
"Second, we randomly replaced instances of going to with finna, a preverbal marker occurring in AAE and in the American South (Green, 2002).",4.2.1 Cross-Domain Settings,[0],[0]
"Third, we introduced the auxiliaries gone and done, which denote future tense and past tense, respectively; for the former, for each sentence containing at least one auxiliary will, we replace it with gone, and for the latter, for each sentence containing at least one nonauxiliary, non-passive, past-tense verb, we choose one and insert done before it.",4.2.1 Cross-Domain Settings,[0],[0]
"Finally, for each sentence containing at least one copula, we delete one at random.
",4.2.1 Cross-Domain Settings,[0],[0]
"Word Embeddings: Finally, since a tremendous variety of Twitter lexical items are not present in the UD Treebank, we use 200- dimensional word embeddings that we trained with word2vec11 (Mikolov et al., 2013) on the
11https://github.com/dav/word2vec
TwitterAAE corpus, which contains 60.8 million tweets.",4.2.1 Cross-Domain Settings,[0],[0]
"Before training, we processed the corpus by replacing @-mentions with ATMENTION, replacing emojis with EMOJI, and replacing sequences of more than two repeated letters with two repeated letters (e.g. partyyyyy → partyy).",4.2.1 Cross-Domain Settings,[0],[0]
"This resulted in embeddings for 487,450 words.
",4.2.1 Cross-Domain Settings,[0],[0]
"We retrain and compare UDPipe on each of the Morpho-Tagger and ARK Tagger settings with synthetic data and pre-trained embeddings, and without.",4.2.1 Cross-Domain Settings,[0],[0]
We additionally retrain Deep Biaffine with and without synthetic data and embeddings.12,4.2.1 Cross-Domain Settings,[0],[0]
We additionally investigate the effects of small amounts of in-domain training data from our dataset.,4.2.2 In-domain Training,[0],[0]
"We perform 2-fold cross-validation, randomly partitioning our dataset into two sets of 250 tweets.",4.2.2 In-domain Training,[0],[0]
"We compare two different settings (all using the UDPipe ARK Tagger setting):
Twitter-only: To explore the effect of training with Twitter data alone, for each set of 250 we trained on that set alone, along with our Twitter embeddings, and tested on the remaining 250.
",4.2.2 In-domain Training,[0],[0]
"UDT+Twitter: To explore the additional signal provided by the UD Treebank, for each set of 250 we trained on the UD Treebank concatenated with that set (with the tweets upweighted to approximately match the size of the UD Treebank, in order to use similar hyperparameters) and tested on the remaining 250.",4.2.2 In-domain Training,[0],[0]
"In our evaluation, we ignored punctuation tokens (labeled with punct) in our LAS calculation.",5 Results and Analysis,[0],[0]
Morpho-Tagger vs. ARK,5.1 Effects of Cross-Domain Settings,[0],[0]
"Tagger: As hypothesized, UDPipe’s ARK Tagger setting outperformed the Morpho-Tagger across all settings, ranging from a 2.8% LAS improvement when trained only on the UD Treebank with no pre-trained word embeddings, to 4.7% and 5.4% improvements when trained with Twitter embeddings and both Twitter embeddings and synthetic data, respectively.",5.1 Effects of Cross-Domain Settings,[0],[0]
"The latter improvements suggest that the ARK Tagger setup is able to take better advantage of Twitterspecific lexical information from the embeddings
12As the existing implementation of Deep Biaffine requires pre-trained word embeddings, for the Deep Biaffine baseline experiments we use the CoNLL 2017 Shared Task 100- dimensional embeddings that were pretrained on the English UD Treebank.
and syntactic patterns from the synthetic data.",5.1 Effects of Cross-Domain Settings,[0],[0]
"Table 1 shows the LAS for our various settings.
",5.1 Effects of Cross-Domain Settings,[0],[0]
"After observing the better performance of the ARK Tagger setting, we opted not to retrain the Deep Biaffine parser in any Morpho-Tagger settings due to the model’s significantly longer training time; all our Deep Biaffine results are reported for models trained with an ARK Tagger setting.
",5.1 Effects of Cross-Domain Settings,[0],[0]
"Synthetic data and embeddings: We observed that synthetic data and Twitter-trained embeddings were independently helpful; embeddings provided a 1.4–5.3% boost across the UDPipe and Deep Biaffine models, while synthetic data provided a 1.3– 5.7% additional boost (Table 1).
",5.1 Effects of Cross-Domain Settings,[0],[0]
"UDPipe vs. Deep Biaffine: While the baseline models for UDPipe and Deep Biaffine are not directly comparable (since the latter required pretrained embeddings), in the Twitter embeddings setting Deep Biaffine outperformed UDPipe by 5.1%.",5.1 Effects of Cross-Domain Settings,[0],[0]
"However, given access to both synthetic data and Twitter embeddings, UDPipe’s performance approached that of Deep Biaffine.",5.1 Effects of Cross-Domain Settings,[0],[0]
"Perhaps surprisingly, training with even limited amounts of in-domain training data aided in parsing performance; training with just in-domain data produced an LAS comparable to that of the baseline Deep Biaffine model, and adding UD Treebank data further increased LAS by 8.1%, indicat-
ing that they independently provide critical signal.",5.2 Effects of In-Domain Training,[0],[0]
"For each model in each of the cross-domain settings, we calculated the LAS on the 250 tweets drawn from highly African-American tweets and the 250 from highly White tweets (see §3 for details); we will refer to these as the AA and WH tweets, respectively.",5.3 AAE/MAE Performance Disparity,[0],[0]
"We observed clear disparities in performance between the two sets of tweets, ranging from 5.9% to 15.7% (Table 3).",5.3 AAE/MAE Performance Disparity,[0],[0]
"Additionally, across settings, we observed several patterns.
",5.3 AAE/MAE Performance Disparity,[0],[0]
"First, the UDPipe ARK Tagger settings produced significantly smaller gaps (5.9–8.4%) than the corresponding Morpho-Tagger settings (14.0– 15.7%).",5.3 AAE/MAE Performance Disparity,[0],[0]
"Indeed, most of the performance improvement of the ARK Tagger setting comes from the AA tweets; the LAS on the AA tweets jumps 7.2–9.2% from each Morpho-Tagger setting to the corresponding ARK Tagger setting, compared to differences of −0.9–1.9% for the WH tweets.
",5.3 AAE/MAE Performance Disparity,[0],[0]
"Second, the Deep Biaffine ARK Tagger settings produced larger gaps (8.0–11.6%) than the UDPipe ARK Tagger settings, with the exception of the embeddings-only setting.
",5.3 AAE/MAE Performance Disparity,[0],[0]
"Finally, we observed the surprising result that adding Twitter-trained embeddings and synthetic data, which contains both Twitter-specific and AAE-specific features, increases the performance gap across both UDPipe settings.",5.3 AAE/MAE Performance Disparity,[0],[0]
"We hypothesize that while UDPipe is able to effectively make use of both Twitter-specific lexical items and annotation conventions within MAE-like syntactic structures, it continues to be stymied by AAE-like syntactic structures, and is therefore unable to make use of the additional information.
",5.3 AAE/MAE Performance Disparity,[0],[0]
"We further calculated recall for each relation
type across the AA tweets and WH tweets, and the resulting performance gap, under the UDPipe Morpho-Tagger and ARK Tagger models trained with synthetic data and embeddings.",5.3 AAE/MAE Performance Disparity,[0],[0]
"Table 4 shows these calculations for the 15 relation types for which the performance gap was highest and which had at least 15 instances in each of the AA and WH tweet sets, along with the corresponding calculation under the ARK Tagger model.",5.3 AAE/MAE Performance Disparity,[0],[0]
The amount by which the performance gap is reduced from the first setting to the second setting is also reported.,5.3 AAE/MAE Performance Disparity,[0],[0]
"Of the 15 relations shown, the gap was reduced for 14, and 7 saw a reduction of at least 10%.",5.3 AAE/MAE Performance Disparity,[0],[0]
"In this section, we discuss AAE lexical and syntactic variations observed in our dataset, with the aim of providing insight into decreased AA parsing accuracy, and the impact of various parser settings on their parsing accuracy.
",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"AAE contains a variety of phonological features which present themselves on Twitter through a number of lexical variations (Green, 2002; Jones, 2015), many of which are listed in §3.1, instances of which occur a total of 80 times in the AA tweets; notably, none occur in the WH tweets.
",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"We investigated the accuracy of various crossdomain parser settings on these lexical variants; for each of the baseline Morpho-Tagger, baseline ARK Tagger, ARK Tagger with embeddings, and ARK Tagger with synthetic data and embeddings models, we counted the number of instances of lexical variants from §3.1 for which the model gave the correct head with the correct label.
",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"While the lexical variants challenged all four models, switching from the Morpho-Tagger set-
ting to the ARK Tagger settings produced significant accuracy increases (Table 6).",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
We observed that the greatest improvement came from using the ARK Tagger setting with Twitter-trained embeddings; the Twitter-specific lexical information provided by the embeddings was critical to recognizing the variants.,5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"Surprisingly, adding synthetic data decreased the model’s ability to parse the variants.
",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
We next investigated the presence of AAE syntactic phenomena in our dataset.,5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"Table 5 shows examples of seven well-documented AAE morphological and syntactic features and counts of their occurrences in our AA and WH tweet sets; again, while several of the phenomena, such as dropped
copulas and habitual be, occur frequently in our AA tweets, there is only one instance of any of these features occurring in the WH tweet set.
",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"We measured the parsing accuracy for the two most frequent syntactic features, dropped copulas and habitual be, across the four models; accuracies are given in Table 6.",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"For dropped copulas, we measured parsing correctness by checking if the parser correctly attached the subject to the correct predicate word via the nsubj relation; for the first example in Table 5, for example, we considered the parser correct if it attached bestfrienddd to mad via the nsubj relation.",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"For habitual be, we checked for correct attachment via the aux or cop relations as in the first and second examples in Ta-
ble 5, respectively.",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"As before, we observed significant increases in accuracy moving from the Morpho-Tagger to the ARK Tagger settings.",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"However, neither adding embeddings nor synthetic data appeared to significantly increase accuracy for these features.",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"From manual inspection, most of the dropped copulas errors appear to arise either from challenging questions (e.g. ATMENTION what yo number ?) or from mis-identification of the word to which to attach the subject (e.g. He claim he in love llh, where he was attached to llh rather than to love).",5.4 Lexical and Syntactic Analysis of AAE,[0],[0]
"While current neural dependency parsers are highly accurate on MAE, our analyses suggest that AAE text presents considerable challenges due to lexical and syntactic features which diverge systematically from MAE.",6 Conclusion,[0],[0]
"While the cross-domain strategies we presented can greatly increase accurate parsing of these features, narrowing the performance gap between AAE- and MAE-like tweets, much work remains to be done for accurate parsing of even linguistically well-documented features.
",6 Conclusion,[0],[0]
"It remains an open question whether it is better to use a model with a smaller accuracy disparity (e.g. UDPipe), or a model with higher average accuracy, but a worse disparity (e.g. Deep Biaffine).",6 Conclusion,[0],[0]
"The emerging literature on fairness in algorithms suggests interesting further challenges; for example, Kleinberg et al. (2017) and CorbettDavies et al. (2017) argue that as various commonly applied notions of fairness are mutually incompatible, algorithm designers must grapple with such trade-offs.",6 Conclusion,[0],[0]
"Regardless, the modeling decision should be made in light of the application of interest; for example, applications like opinion analysis and information retrieval may benefit from equal (and possibly weaker) performance between groups, so that concepts or opinions in-
ferred from groups of authors (e.g. AAE speakers) are not under-counted or under-represented in results returned to a user or analyst.",6 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
"This work was supported by a Google Faculty Research Award, and a National Science Foundation Graduate Research Fellowship (No. 1451512).",Acknowledgments,[0],[0]
"Due to the presence of both Twitterspecific conventions and non-standard and dialectal language, Twitter presents a significant parsing challenge to current dependency parsing tools.",abstractText,[0],[0]
"We broaden English dependency parsing to handle social media English, particularly social media African-American English (AAE), by developing and annotating a new dataset of 500 tweets, 250 of which are in AAE, within the Universal Dependencies 2.0 framework.",abstractText,[0],[0]
"We describe our standards for handling Twitterand AAE-specific features and evaluate a variety of crossdomain strategies for improving parsing with no, or very little, in-domain labeled data, including a new data synthesis approach.",abstractText,[0],[0]
"We analyze these methods’ impact on performance disparities between AAE and Mainstream American English tweets, and assess parsing accuracy for specific AAE lexical and syntactic features.",abstractText,[0],[0]
Our annotated data and a parsing model are available at: http://slanglab.cs.umass.edu/ TwitterAAE/.,abstractText,[0],[0]
Twitter Universal Dependency Parsing for African-American and Mainstream American English,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 810–820 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
810",text,[0],[0]
In this paper we study two bilingual tasks that strongly depend on bilingual word embeddings (BWEs).,1 Introduction,[0],[0]
"Previously, specialized domain adaptation approaches to such tasks were proposed.",1 Introduction,[0],[0]
We instead show experimentally that a simple adaptation process involving only unlabeled text is highly effective.,1 Introduction,[0],[0]
"We then show that a semisupervised classification method from computer vision can be applied successfully for further gains in cross-lingual classification.
",1 Introduction,[0],[0]
Our BWE adaptation method is delightfully simple.,1 Introduction,[0],[0]
We begin by adapting monolingual word embeddings to the target domain for source and target languages by simply building them using both general and target-domain unlabeled data.,1 Introduction,[0],[0]
"As
a second step we use post-hoc mapping (Mikolov et al., 2013b), i.e., we use a seed lexicon to transform the word embeddings of the two languages into the same vector space.",1 Introduction,[0],[0]
We show experimentally for the first time that the domain-adapted bilingual word embeddings we produce using this extremely simple technique are highly effective.,1 Introduction,[0],[0]
"We study two quite different tasks and domains, where resources are lacking, showing that our simple technique performs well for both of them: cross-lingual twitter sentiment classification and medical bilingual lexicon induction.",1 Introduction,[0],[0]
"In previous work, task-dependent approaches were used for this type of domain adaptation.",1 Introduction,[0],[0]
"Our approach is simple and task independent.
",1 Introduction,[0],[0]
"Second, we adapt the semi-supervised image classification system of Häusser et al. (2017) for NLP problems for the first time.",1 Introduction,[0],[0]
This approach is broadly applicable to many NLP classification tasks where unlabeled data is available.,1 Introduction,[0],[0]
We tailor it to both of our cross-lingual tasks.,1 Introduction,[0],[0]
"The system exploits unlabeled data during the training of classifiers by learning similar features for similar labeled and unlabeled training examples, thereby extracting information from unlabeled examples as well.",1 Introduction,[0],[0]
"As we show experimentally, the system further improves cross-lingual knowledge transfer for both of our tasks.
",1 Introduction,[0],[0]
"After combining both techniques, the results of sentiment analysis are competitive with systems that use annotated data in the target language, an impressive result considering that we require no target-language annotated data.",1 Introduction,[0],[0]
The method also yields impressive improvements for bilingual lexicon induction compared with baselines trained on in-domain data.,1 Introduction,[0],[0]
We show that this system requires the high-quality domain-adapted bilingual word embeddings we previously created to use unlabeled data well.,1 Introduction,[0],[0]
Many approaches have been proposed for creating high quality BWEs using different bilingual signals.,2.1 Bilingual Word Embeddings,[0],[0]
"Following Mikolov et al. (2013b), many authors (Faruqui and Dyer, 2014; Xing et al., 2015; Lazaridou et al., 2015; Vulić and Korhonen, 2016) map monolingual word embeddings (MWEs) into the same bilingual space.",2.1 Bilingual Word Embeddings,[0],[0]
"Others leverage parallel texts (Hermann and Blunsom, 2014; Gouws et al., 2015) or create artificial cross-lingual corpora using seed lexicons or document alignments (Vulić and Moens, 2015; Duong et al., 2016) to train BWEs.
",2.1 Bilingual Word Embeddings,[0],[0]
"In contrast, our aim is not to improve the intrinsic quality of BWEs, but to adapt BWEs to specific domains to enhance their performance on bilingual tasks in these domains.",2.1 Bilingual Word Embeddings,[0],[0]
"Faruqui et al. (2015), Gouws and Søgaard (2015), Rothe et al. (2016) have previously studied domain adaptation of bilingual word embeddings, showing it to be highly effective for improving downstream tasks.",2.1 Bilingual Word Embeddings,[0],[0]
"However, importantly, their proposed methods are based on specialized domain lexicons (such as, e.g., sentiment lexicons) which contain task specific word relations.",2.1 Bilingual Word Embeddings,[0],[0]
"Our delightfully simple approach is, in contrast, effectively task independent (in that it only requires unlabeled in-domain text), which is an important strength.",2.1 Bilingual Word Embeddings,[0],[0]
"Sentiment analysis is widely applied, and thus ideally we would have access to high quality supervised models in all human languages.",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"Unfortunately, good quality labeled datasets are missing for many languages.",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
Training models on resource rich languages and applying them to resource poor languages is therefore highly desirable.,2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"Crosslingual sentiment classification (CLSC) tackles this problem (Mihalcea et al., 2007; Banea et al., 2010; Wan, 2009; Lu et al., 2011; Balamurali and Joshi, 2012; Gui et al., 2013).",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"Recent CLSC approaches use BWEs as features of deep learning architectures which allows us to use a model for target-language sentiment classification, even when the model was trained only using sourcelanguage supervised training data.",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
Following this approach we perform CLSC on Spanish tweets using English training data.,2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"Even though Spanish is not resource-poor we simulate this by using only English annotated data.
",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"Xiao and Guo (2013) proposed a cross-lingual log-bilinear document model to learn distributed representations of words, which can capture both the semantic similarities of words across languages and the predictive information with respect to the classification task.",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"Similarly, Tang and Wan (2014) jointly embedded texts in different languages into a joint semantic space representing sentiment.",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"Zhou et al. (2014) employed aligned sentences in the BWE learning process, but in the sentiment classification process only representations in the source language are used for training, and representations in the target language are used for predicting labels.",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"An important weakness of these three works was that aligned sentences were required.
",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"Some work has trained sentiment-specific BWEs using annotated sentiment information in both languages (Zhou et al., 2015, 2016), which is desirable, but this is not applicable to our scenario.",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
Our goal is to adapt BWEs to a specific domain without requiring additional task-specific engineering or knowledge sources beyond having access to plentiful target-language in-domain unlabeled text.,2.2 Cross-Lingual Sentiment Analysis,[0],[0]
"Both of the approaches we study in this work fit this criterion, the delightfully simple method for adapting BWEs can improve the performance of any off-the-shelf classifier that is based on BWEs, while the broadly applicable semi-supervised approach of Häusser et al. (2017) can improve the performance of any off-the-shelf classifier.",2.2 Cross-Lingual Sentiment Analysis,[0],[0]
BLI is an important task that has been addressed by a large amount of previous work.,2.3 Bilingual Lexicon Induction (BLI),[0],[0]
The goal of BLI is to automatically extract word translation pairs using BWEs.,2.3 Bilingual Lexicon Induction (BLI),[0],[0]
"While BLI is often used to provide an intrinsic evaluation of BWEs (Lazaridou et al., 2015; Vulić and Moens, 2015; Vulić and Korhonen, 2016)",2.3 Bilingual Lexicon Induction (BLI),[0],[0]
"it is also useful for tasks such as machine translation (Madhyastha and España Bohnet, 2017).",2.3 Bilingual Lexicon Induction (BLI),[0],[0]
Most work on BLI using BWEs focuses on frequent words in high-resource domains such as parliament proceedings or news texts.,2.3 Bilingual Lexicon Induction (BLI),[0],[0]
Recently Heyman et al. (2017) tackled BLI of words in the medical domain.,2.3 Bilingual Lexicon Induction (BLI),[0],[0]
This task is useful for many applications such as terminology extraction or OOV mining for machine translation of medical texts.,2.3 Bilingual Lexicon Induction (BLI),[0],[0]
"Heyman et al. (2017) show that when only a small amount of medical data is available,
BLI using BWEs tends to perform poorly.",2.3 Bilingual Lexicon Induction (BLI),[0],[0]
"Especially BWEs obtained using post-hoc mapping (Mikolov et al., 2013b; Lazaridou et al., 2015) fail on this task.",2.3 Bilingual Lexicon Induction (BLI),[0],[0]
"Consequently, Heyman et al. (2017) build BWEs using aligned documents and then engineer a specialized classification-based approach to BLI.",2.3 Bilingual Lexicon Induction (BLI),[0],[0]
"In contrast, our delightfully simple approach to create high-quality BWEs for the medical domain requires only monolingual data.",2.3 Bilingual Lexicon Induction (BLI),[0],[0]
We show that our adapted BWEs yield impressive improvements over non-adapted BWEs in this task with both cosine similarity and with the classifier of Heyman et al. (2017).,2.3 Bilingual Lexicon Induction (BLI),[0],[0]
"In addition, we show that the broadly applicable method can push performance further using easily accessible unlabeled data.",2.3 Bilingual Lexicon Induction (BLI),[0],[0]
BWEs trained on general domain texts usually result in lower performance when used in a system for a specific domain.,3 Adaptation of BWEs,[0],[0]
There are two reasons for this.,3 Adaptation of BWEs,[0],[0]
"(i) Vocabularies of specific domains contain words that are not used in the general case, e.g., names of medicines or diseases.",3 Adaptation of BWEs,[0],[0]
"(ii) The meaning of a word varies across domains; e.g., “apple” mostly refers to a fruit in general domains, but is an electronic device in many product reviews.
",3 Adaptation of BWEs,[0],[0]
The delightfully simple method adapts general domain BWEs in a way that preserves the semantic knowledge from general domain data and leverages monolingual domain specific data to create domain-specific BWEs.,3 Adaptation of BWEs,[0],[0]
Our domain-adaptation approach is applicable to any language-pair in which monolingual data is available.,3 Adaptation of BWEs,[0],[0]
"Unlike other methods, our approach is task independent: it only requires unlabeled in-domain target language text.",3 Adaptation of BWEs,[0],[0]
"To create domain adapted BWEs, we first train MWEs (monolingual word embeddings) in both languages and then map those into the same space using post-hoc mapping (Mikolov et al., 2013b).",3.1 Approach,[0],[0]
We train MWEs for both languages by concatenating monolingual out-of-domain and in-domain data.,3.1 Approach,[0],[0]
The out-of-domain data allows us to create accurate distributed representations of common vocabulary while the in-domain data embeds domain specific words.,3.1 Approach,[0],[0]
We then map the two MWEs using a small seed lexicon to create the adapted BWEs.,3.1 Approach,[0],[0]
"Because post-hoc mapping only requires a seed lexicon as bilingual signal it can
easily be used with (cheap) monolingual data.",3.1 Approach,[0],[0]
"For post-hoc mapping, we use Mikolov et al. (2013b)’s approach.",3.1 Approach,[0],[0]
This model assumes a W ∈ Rd1×d2 matrix which maps vectors from the source to the target MWEs where d1 and d2 are the embedding space dimensions.,3.1 Approach,[0],[0]
"A seed lexicon of (xi, yi) ∈ L ⊆ Rd1×Rd2 pairs is needed where xi and yi are source and target MWEs.",3.1 Approach,[0],[0]
"W can be learned using ridge regression by minimizing the L2-regularized mapping error between the source xi and the target yi vectors:
min W ∑ i ||Wxi − yi||22 + λ||W ||22 (1)
where λ is the regularization weight.",3.1 Approach,[0],[0]
"Based on the source embedding x, we then compute a target embedding as Wx.
We create MWEs with word2vec skipgram (Mikolov et al., 2013a)1 and estimate W with scikit-learn (Pedregosa et al., 2011).",3.1 Approach,[0],[0]
We use default parameters.,3.1 Approach,[0],[0]
"In CLSC, an important application of BWEs, we train a supervised sentiment model on training data available in the source (a resource rich language) and apply it to the target (a resource poor language, for which there is typically no training data available).",4 Cross-Lingual Sentiment Classification,[0],[0]
"Because BWEs embed source and target words in the same space, annotations in the source (represented as BWEs) enable transfer learning.",4 Cross-Lingual Sentiment Classification,[0],[0]
"For CLSC of tweets, a drawback of BWEs trained on non-twitter data is that they do not produce embeddings for twitter-specific vocabulary, e.g., slang words like English coool and (Mexican) Spanish chido, resulting in lost information when a sentiment classifier uses them.",4 Cross-Lingual Sentiment Classification,[0],[0]
"As comparable non-twitter data we use OpenSubtitles (Lison and Tiedemann, 2016) which contains 49.2M English and Spanish subtitle sentences respectively (Subtitle).",4.1 Training Data for Twitter Specific BWEs,[0],[0]
The reason behind choosing Subtitles is that although it is out-of-domain it contains slang words similar to tweets thus serving as a strong baseline in our setup.,4.1 Training Data for Twitter Specific BWEs,[0],[0]
"We experiment with two monolingual twitter data sets:
(i) 22M tweets: Downloaded2 English (17.2M) and Spanish (4.8M) tweets using the public
1https://github.com/dav/word2vec 2We downloaded for a month starting on 2016-10-15.
",4.1 Training Data for Twitter Specific BWEs,[0],[0]
"Twitter Streaming API3 with language filters en and es
(ii) a BACKGROUND corpus of 296K English and 150K Spanish (non-annotated) tweets released with the test data of the RepLab task (Amigó et al., 2013) described below
All twitter data was tokenized using Bird et al. (2009) and lowercased.",4.1 Training Data for Twitter Specific BWEs,[0],[0]
"User names, URLs, numbers, emoticons and punctuation were removed.
",4.1 Training Data for Twitter Specific BWEs,[0],[0]
"As lexicon for the mapping, we use the BNC word frequency list (Kilgarriff, 1997), a list of 6,318 frequent English lemmas and their Spanish translations, obtained from Google Translate.",4.1 Training Data for Twitter Specific BWEs,[0],[0]
Note that we do not need a domain-specific lexicon in order to get good quality adapted BWEs.,4.1 Training Data for Twitter Specific BWEs,[0],[0]
"For sentiment classification, we use data from the RepLab 2013 shared task (Amigó et al., 2013).",4.2 Training Data for Sentiment Classifiers,[0],[0]
"The data is annotated with positive, neutral and negative labels and contains English and Spanish tweets.",4.2 Training Data for Sentiment Classifiers,[0],[0]
We used the official English training set (26.6K tweets) and the Spanish test set (14.9K) in the resource-poor setup.,4.2 Training Data for Sentiment Classifiers,[0],[0]
"We only use the 7.2K Spanish labeled training data for comparison reasons in §6.2, which we will discuss later.
",4.2 Training Data for Sentiment Classifiers,[0],[0]
"The shared task was on target-level sentiment analysis, i.e., given a pair (document, target entity), the gold annotation is based on whether the sentiment expressed by the document is about the target.",4.2 Training Data for Sentiment Classifiers,[0],[0]
For example: I cried on the back seat of my BMW!,4.2 Training Data for Sentiment Classifiers,[0],[0]
where BMW is the target would be negative in the sentence-level scenario.,4.2 Training Data for Sentiment Classifiers,[0],[0]
"However, it is neutral in the target-level case because the negative sentiment is not related to BMW.",4.2 Training Data for Sentiment Classifiers,[0],[0]
The reason for using this dataset is that it contains comparable English and Spanish tweets annotated for sentiment.,4.2 Training Data for Sentiment Classifiers,[0],[0]
"There are other twitter datasets for English (Nakov et al., 2016) and Spanish (GarcıaCumbreras et al., 2016), but they were downloaded at different times and were annotated using different annotation methodologies, thus impeding a clean and consistent evaluation.",4.2 Training Data for Sentiment Classifiers,[0],[0]
For evaluating our adapted BWEs on the RepLab dataset we used a target-aware sentiment classifier introduced by Zhang et al. (2016).,4.3 Sentiment Systems,[0],[0]
"The network first embeds input words using pre-trained
3dev.twitter.com/streaming/overview
BWEs and feeds them to a bi-directional gated neural network.",4.3 Sentiment Systems,[0],[0]
Pooling is applied on the hidden representations of the left and right context of the target mention respectively.,4.3 Sentiment Systems,[0],[0]
"Finally, gated neurons are used to model the interaction between the target mention and its surrounding context.",4.3 Sentiment Systems,[0],[0]
"During training we hold our pre-trained BWEs fixed and keep the default parameters of the model.
",4.3 Sentiment Systems,[0],[0]
"We also implement Kim (2014)’s CNN-nonstatic system, which does not use the target information in a given document (target-ignorant).",4.3 Sentiment Systems,[0],[0]
The network first embeds input words using pretrained BWEs and feeds them to a convolutional layer with multiple window sizes.,4.3 Sentiment Systems,[0],[0]
Max pooling is applied on top of convolution followed by a fully connected network with one hidden layer.,4.3 Sentiment Systems,[0],[0]
We used this system as well because it performed comparably to the target-aware system.,4.3 Sentiment Systems,[0],[0]
"The reason for this is that only 1% of the used data contains more than one target and out of these rare cases only 14% have differing sentiment labels in the same sentence, which are the difficult cases of target-level sentiment analysis.",4.3 Sentiment Systems,[0],[0]
"We used the default parameters as described in (Kim, 2014) with the exception of using 1000 feature maps and 30 epochs, based on our initial experiments.",4.3 Sentiment Systems,[0],[0]
Word embeddings are fixed during the training just as for the target-aware classifier.,4.3 Sentiment Systems,[0],[0]
As we previously explained we evaluate our adaptation method on the task of target-level sentiment classification using both target-aware and target-ignorant classifiers.,4.4 Results,[0],[0]
"For all experiments, our two baselines are off-the-shelf classifiers using non-adapted BWEs, i.e., BWEs trained only using Subtitles.",4.4 Results,[0],[0]
Our goal is to show that our BWE adaptation method can improve the performance of such classifiers.,4.4 Results,[0],[0]
We train our adapted BWEs on the concatenation of Subtitle and 22M tweets or BACKGROUND respectively.,4.4 Results,[0],[0]
"In addition, we also report results with BWEs trained only on tweets.
",4.4 Results,[0],[0]
To train the sentiment classifiers we use the English Replab training set and we evaluate on the Spanish test set.,4.4 Results,[0],[0]
"To show the performance that can be reached in a monolingual setup, we report results obtained by using annotated Spanish sentiment data instead of English (oracle).",4.4 Results,[0],[0]
"We train two oracle sentiment classifiers using (i) MWEs trained on only the Spanish part of Subtitle and (ii)
BWEs trained on Subtitle using posthoc mapping.",4.4 Results,[0],[0]
"The difference between the two is that the embeddings of (ii) are enriched with English words which can be beneficial for the classification of Spanish tweets because they often contain a few English words.
",4.4 Results,[0],[0]
We do not compare with word embedding adaptation methods relying on specialized resources.,4.4 Results,[0],[0]
The point of our work is to study task-independent methods and to the best of our knowledge ours is the first such attempt.,4.4 Results,[0],[0]
"Similarly, we do not compare against machine translation based sentiment classifiers (e.g., (Zhou et al., 2016))",4.4 Results,[0],[0]
"because for their adaptation in-domain parallel data would be needed.
",4.4 Results,[0],[0]
Table 1 gives results for both classifiers.,4.4 Results,[0],[0]
It shows that the adaptation of Subtitle based BWEs with data from Twitter (22M tweets and BACKGROUND) clearly outperforms the Baseline in all cases.,4.4 Results,[0],[0]
The target-aware system performed poorly with the baseline BWEs and could benefit significantly from the adaptation approach.,4.4 Results,[0],[0]
The target-ignorant performed better with the baseline BWEs but could also benefit from the adaptation.,4.4 Results,[0],[0]
"Comparing results with the Twitter-dataset-only based BWEs, the 22M tweets performed better even though the BACKGROUND dataset is from the same topic as the RepLab train and test sets.",4.4 Results,[0],[0]
Our conjecture is that the latter is too small to create good BWEs.,4.4 Results,[0],[0]
"In combination with Subtitles, 22M tweets also yields better results than when combined with BACKGROUND.",4.4 Results,[0],[0]
"Although the best accuracy was reached using the 22M tweetsonly based BWEs, it is only slightly better then the adapted Subtitles+22M tweets based BWEs.",4.4 Results,[0],[0]
"In §6 we show that both the semantic knowledge from Subtitles and the domain-specific information from tweets are needed to further improve results.
",4.4 Results,[0],[0]
Comparing the two classifiers we can say that they performed similarly in terms of their best results.,4.4 Results,[0],[0]
"On the other hand, the target-ignorant system had better results on average.",4.4 Results,[0],[0]
This might seem surprising at first because the system does not use the target as information.,4.4 Results,[0],[0]
"But considering the characteristics of RepLab, i.e., that the number of tweets that contains multiple targets is negligible, using the target offers no real advantage.
",4.4 Results,[0],[0]
"Although we did not focus on the impact of the seed lexicon size, we ran post-hoc mapping with different sizes during our preliminary experiments.",4.4 Results,[0],[0]
"With 1,000 and 100 word pairs in the lexicon the target-ignorant system suffered 0.5% and 4.0% drop in average of our setups respectively.
",4.4 Results,[0],[0]
To summarize the result: using adapted BWEs for the Twitter CLSC task improves the performance of off-the-shelf classifiers.,4.4 Results,[0],[0]
Another interesting downstream task for BWEs is bilingual lexicon induction.,5 Medical Bilingual Lexicon Induction,[0],[0]
"Given a list of words in a source language, the goal of BLI is to mine translations for each word in a chosen target language.",5 Medical Bilingual Lexicon Induction,[0],[0]
"The medical bilingual lexicon induction task proposed in (Heyman et al., 2017) aims to mine medical words using BWEs trained on a very small amount of English and Dutch monolingual medical data.",5 Medical Bilingual Lexicon Induction,[0],[0]
"Due to the lack of resources in this domain, good quality BWEs are hard to build using in-domain data only.",5 Medical Bilingual Lexicon Induction,[0],[0]
We show that by enriching BWEs with general domain knowledge (in the form of general domain monolingual corpora) better results can be achieved on this medical domain task.,5 Medical Bilingual Lexicon Induction,[0],[0]
We evaluate our improved BWEs on the dataset provided by Heyman et al. (2017).,5.1 Experimental Setup,[0],[0]
The monolingual medical data consists of English and Dutch medical articles from Wikipedia.,5.1 Experimental Setup,[0],[0]
The English (resp.,5.1 Experimental Setup,[0],[0]
"Dutch) articles contain 52,336 (resp.",5.1 Experimental Setup,[0],[0]
"21,374) sentences.",5.1 Experimental Setup,[0],[0]
"A total of 7,368 manually annotated word translation pairs occurring in the English (source) and Dutch (target) monolingual corpora are provided as gold data.",5.1 Experimental Setup,[0],[0]
This set is split 64%/16%/20% into trn/dev/test.,5.1 Experimental Setup,[0],[0]
20% of the English words have multiple translations.,5.1 Experimental Setup,[0],[0]
"Given an English word, the task is to find the correct Dutch translation.
",5.1 Experimental Setup,[0],[0]
"As monolingual general-domain data we use
the English and Dutch data from Europarl (v7) (Koehn, 2005), a corpus of 2 million sentence pairs.",5.1 Experimental Setup,[0],[0]
"Although Europarl is a parallel corpus, we use it in a monolingual way and shuffle each side of the corpus before training.",5.1 Experimental Setup,[0],[0]
By using massive cheap data we create high-quality MWEs in each language which are still domain-specific (due to inclusion of medical data).,5.1 Experimental Setup,[0],[0]
"To obtain an out-ofdomain seed lexicon, we translated the English words in BNC to Dutch using Google Translate (just as we did before for the Twitter CLSC task).",5.1 Experimental Setup,[0],[0]
We then use the out-of-domain BNC and the indomain medical seed lexicons in separate experiments to create BWEs with post-hoc mapping.,5.1 Experimental Setup,[0],[0]
"Note, we did not concatenate the two lexicons because (i) they have a small common subset of source words which have different target words, thus having a negative effect on the mapping and (ii) we did not want to modify the medical seed lexicon because it was taken from previous work.",5.1 Experimental Setup,[0],[0]
To perform BLI we use two methods.,5.2 BLI Systems,[0],[0]
"Because BWEs represent words from different languages in a shared space, BLI can be performed via cosine similarity in this space.",5.2 BLI Systems,[0],[0]
"In other words, given a BWE representing two languages Vs and Vt, the translation of each word s ∈",5.2 BLI Systems,[0],[0]
"Vs can be induced by taking the word t ∈ Vt whose representation ~xt in the BWE is closest to the representation ~xs.
",5.2 BLI Systems,[0],[0]
As the second approach we use a classifier based system proposed by Heyman et al. (2017).,5.2 BLI Systems,[0],[0]
This neural network based system is comprised of two main modules.,5.2 BLI Systems,[0],[0]
The first is a character-level LSTM which aims to learn orthographic similarity of word pairs.,5.2 BLI Systems,[0],[0]
The other is the concatenation of the embeddings of the two words using embedding layers with the aim of learning the similarity among semantic representations of the words.,5.2 BLI Systems,[0],[0]
Dense layers are applied on top of the two modules before the output soft-max layer.,5.2 BLI Systems,[0],[0]
"The classifier is trained using positive and negative word
pair examples and a pre-trained word embedding model.",5.2 BLI Systems,[0],[0]
Negative examples are randomly generated for each positive one in the training lexicon.,5.2 BLI Systems,[0],[0]
We used default parameters as reported by Heyman et al. (2017) except for the t classification thresholds (used at prediction time).,5.2 BLI Systems,[0],[0]
We finetuned these on dev.,5.2 BLI Systems,[0],[0]
"We note that the system works with pre-trained MWEs as well (and report these as official baseline results) but it requires BWEs for candidate generation at prediction time, thus we use BWEs for the system’s input for all experiments.",5.2 BLI Systems,[0],[0]
"In preliminary work, we had found that MWE and BWE results are similar.",5.2 BLI Systems,[0],[0]
Heyman et al. (2017)’s results are our baseline.,5.3 Results,[0],[0]
"Table 2 compares its performance with our adapted BWEs, with both cosine similarity and classification based systems.",5.3 Results,[0],[0]
“top” F1 scores are based on the most probable word as prediction only; “all” F1 scores use all words as prediction whose probability is above the threshold.,5.3 Results,[0],[0]
It can be seen that the cosine similarity based system using adapted BWEs clearly outperforms the nonadapted BWEs which were trained in a resource poor setup.4,5.3 Results,[0],[0]
"Moreover, the best performance was reached using the general seed lexicon for the mapping which is due to the fact that general domain words have better quality embeddings in the MWE models, which in turn gives a better quality mapping.
",5.3 Results,[0],[0]
The classification based system performs significantly better comparing to cosine similarity by exploiting the seed lexicon better.,5.3 Results,[0],[0]
Using adapted BWEs as input word embeddings for the system further improvements were achieved which shows the better quality of our BWEs.,5.3 Results,[0],[0]
"Simulating an even poorer setup by using a general lexicon, the
4The results for cosine similarity in (Heyman et al., 2017) are based on BWESG-based BWEs (Vulić and Moens, 2016) trained on a small document aligned parallel corpus without using a seed lexicon.
performance gain of the classifier is lower.",5.3 Results,[0],[0]
This shows the significance of the medical seed lexicon for this system.,5.3 Results,[0],[0]
"On the other hand, adapted BWEs have better performance compared to non-adapted ones using the best translation while they have just slightly lower F1 using multiple translations.",5.3 Results,[0],[0]
"This result shows that while with adapted BWEs the system predicts better “top” translations, it has a harder time when predicting “all” due to the increased vocabulary size.
",5.3 Results,[0],[0]
To summarize: we have shown that adapted BWEs increase performance for this task and domain; and they do so independently of the taskspecific system that is used.,5.3 Results,[0],[0]
"In addition to the experiments that show our BWEadaptation method’s task and language independence, we investigate ways to further incorporate unlabeled data to overcome data sparsity.
",6 Semi-Supervised Learning,[0],[0]
Häusser et al. (2017) introduce a semisupervised method for neural networks that makes associations from the vector representation of labeled samples to those of unlabeled ones and back.,6 Semi-Supervised Learning,[0],[0]
This lets the learning exploit unlabeled samples as well.,6 Semi-Supervised Learning,[0],[0]
"While Häusser et al. (2017) use their model for image classification, we adapt it to CLSC of tweets and medical BLI.",6 Semi-Supervised Learning,[0],[0]
We show that our semisupervised model requires adapted BWEs to be effective and yields significant improvements.,6 Semi-Supervised Learning,[0],[0]
This innovative method is general and can be applied to any classification when unlabeled text is available.,6 Semi-Supervised Learning,[0],[0]
"Häusser et al. (2017)’s basic assumption is that the embeddings of labeled and unlabeled samples – i.e., the representations in the neural network on which the classification layer is applied – are similar within the same class.",6.1 Model,[0],[0]
"To achieve this, walking cycles are introduced: a cycle starts from a labeled sample, goes to an unlabeled one and ends at a labeled one.",6.1 Model,[0],[0]
A cycle is correct if the start and end samples are in the same class.,6.1 Model,[0],[0]
The probability of going from sample A to B is proportional to the cosine similarity of their embeddings.,6.1 Model,[0],[0]
"To maximize the number of correct cycles, two loss functions are employed: Walker loss and Visit loss.
",6.1 Model,[0],[0]
"Walker loss penalizes incorrect walks and encourages a uniform probability distribution of
walks to the correct class.",6.1 Model,[0],[0]
"It is defined as:
Lwalker := H(T, P aba) (2)
where H is the cross-entropy function, P abaij is the probability that a cycle starts from sample i and ends at j and T is the uniform target distribution:
",6.1 Model,[0],[0]
"Tij :=
{ 1/(#c(i)) if c(i) = c(j)
0",6.1 Model,[0],[0]
"otherwise (3)
where c(i) is the class of sample i and #c(i) is the number of occurrences of c(i) in the labeled set.
",6.1 Model,[0],[0]
"Visit loss encourages cycles to visit all unlabeled samples, rather than just those which are the most similar to labeled samples.",6.1 Model,[0],[0]
"It is defined as:
Lvisit := H(V, P visit)
P visitj := 〈P abij 〉i (4)
",6.1 Model,[0],[0]
"Vj := 1
U
whereH is cross-entropy, P abij is the probability that a cycle starts from sample i and goes to j and U is the number of unlabeled samples.
",6.1 Model,[0],[0]
"The total loss during training is the sum of the walker, visit and classification (cross-entropy between predicted and gold labels) losses which is minimized using Adam (Kingma and Ba, 2015).
",6.1 Model,[0],[0]
"We adapt this model (including the two losses) to sentiment classification, focusing on the targetignorant classifier, and the classifier based approach for BLI.",6.1 Model,[0],[0]
We will call these systems semisup5.,6.1 Model,[0],[0]
Due to the fact that we initialize the embedding layers for both classifiers with BWEs the models are able to make some correct cycles at the beginning of the training and improve them later on.,6.1 Model,[0],[0]
"We will describe the labeled and unlabeled datasets used in the subsequent sections below.
",6.1 Model,[0],[0]
"We use Häusser et al. (2017)’s implementation of the losses, with 1.0, 0.5 and 1.0 weights for the walker, visit and classification losses, respectively, for CLSC based on preliminary experiments.",6.1 Model,[0],[0]
"We fine-tuned the weights for BLI on dev for each experiment.
",6.1 Model,[0],[0]
5We publicly release our implementation: https:// github.com/hangyav/biadapt,6.1 Model,[0],[0]
"As in §4.4, we use pre-trained BWEs to initialize the classifier and use English sentiment training data as the labeled set.",6.2 Semi-Supervised CLSC,[0],[0]
"Furthermore, we use the Spanish sentiment training data as the unlabeled set, ignoring its annotation.",6.2 Semi-Supervised CLSC,[0],[0]
"This setup is very similar to real-word low-resource scenarios: unlabeled target-language tweets are easy to download while labeled English ones are available.
",6.2 Semi-Supervised CLSC,[0],[0]
Table 3 gives results for adapted BWEs and shows that semisup helps only when word embeddings are adapted to the Twitter domain.,6.2 Semi-Supervised CLSC,[0],[0]
"As mentioned earlier, semisup compares labeled and unlabeled samples based on their vector representations.",6.2 Semi-Supervised CLSC,[0],[0]
"By using BWEs based on only Subtitles, we lose too many embeddings of similar English and Spanish tweets.",6.2 Semi-Supervised CLSC,[0],[0]
"On the other hand, if we use only tweet-based BWEs we lose good quality semantic knowledge which can be learned from more standard text domains.",6.2 Semi-Supervised CLSC,[0],[0]
By combining the two domains we were able to capture both sides.,6.2 Semi-Supervised CLSC,[0],[0]
"For Subtitle+22M tweets, we even get very close to the best oracle (BWE Subtitle) in Table 1 getting only 0.27% less accuracy – an impressive result keeping in mind that we did not use labeled Spanish data.
",6.2 Semi-Supervised CLSC,[0],[0]
"The RepLab dataset contains tweets from 4 topics: automotive, banking, university, music.",6.2 Semi-Supervised CLSC,[0],[0]
We manually analyzed similar tweets from the labeled and unlabeled sets.,6.2 Semi-Supervised CLSC,[0],[0]
"We found that when using semisup, English and Spanish tweets from the same topics are more similar in the embedding space than occurs without the additional losses.",6.2 Semi-Supervised CLSC,[0],[0]
"Topics differ in how they express sentiment – this may explain why semisup increases performance for RepLab.
",6.2 Semi-Supervised CLSC,[0],[0]
Adding supervision.,6.2 Semi-Supervised CLSC,[0],[0]
"To show how well semisup can exploit the unlabeled data we used both English and Spanish sentiment training data together to train the sentiment classifiers.
",6.2 Semi-Supervised CLSC,[0],[0]
Table 4 shows that by using annotated data in both languages we get clearly better results than when using only one language.,6.2 Semi-Supervised CLSC,[0],[0]
"Tables 3 and 4 show that for Subtitle+22M tweets based BWEs, the semisup approach achieved high improvement (2.17%) comparing to targetignorant with English training data only, while it achieved lower improvement (0.97%) with the Subtitle+BACKGROUND based BWEs.",6.2 Semi-Supervised CLSC,[0],[0]
"On the other hand, adding labeled Spanish data caused just a slight increase comparing to semisup with Subtitle+22M tweets based BWEs (0.59%), while in case of Subtitle+BACKGROUND we got significant additional improvement (2.61%).",6.2 Semi-Supervised CLSC,[0],[0]
"This means that with higher quality BWEs, unlabeled target-language data can be exploited better.
",6.2 Semi-Supervised CLSC,[0],[0]
It can also be seen that the target-aware system outperformed the target-ignorant system using additional labeled target-language data.,6.2 Semi-Supervised CLSC,[0],[0]
"The reason could be that it is a more complex network and therefore needs more data to reach high performance.
",6.2 Semi-Supervised CLSC,[0],[0]
The results in table 4 are impressive: our targetlevel system is strongly competitive with the official shared task results.,6.2 Semi-Supervised CLSC,[0],[0]
We achieved high accuracy on the Spanish test set by using only English training data.,6.2 Semi-Supervised CLSC,[0],[0]
"Comparing our best system which used all training data to the official results (Amigó et al., 2013)",6.2 Semi-Supervised CLSC,[0],[0]
we would rank 2nd even though our system is not fine-tuned for the RepLab dataset.,6.2 Semi-Supervised CLSC,[0],[0]
"Furthermore, we also outperformed the oracles when using annotated data from both languages which shows the additional advantage of using BWEs.",6.2 Semi-Supervised CLSC,[0],[0]
For BLI experiments with semisup we used word pairs from the medical seed lexicon as the labeled set (with negative word pairs generated as described in §5.2).,6.3 Semi-Supervised BLI,[0],[0]
"As opposed to CLSC and the work of (Häusser et al., 2017), for this task we do not have an unlabeled set, and therefore we need to generate it.",6.3 Semi-Supervised BLI,[0],[0]
We developed two scenarios.,6.3 Semi-Supervised BLI,[0],[0]
"For the first, BNC, we generate a general unlabeled set using English words from the BNC lexicon and generate 10 pairs out of each word by using the 5 most similar Dutch words based on the corresponding BWEs and 5 random Dutch words.",6.3 Semi-Supervised BLI,[0],[0]
"For the second scenario, medical, we generate an in-domain unlabeled set by generating for each English word in the medical lexicon the 3 most similar Dutch
words based on BWEs and for each of these we use the 5 most similar English words (ignoring the words which are in the original medical lexicon) and 5 negative words.",6.3 Semi-Supervised BLI,[0],[0]
"The idea behind these methods is to automatically generate an unlabeled set that hopefully has a similar positive and negative word pair distribution to the distribution in the labeled set.
",6.3 Semi-Supervised BLI,[0],[0]
Results in Table 5 show that adding semisup to the classifier further increases performance for BLI as well.,6.3 Semi-Supervised BLI,[0],[0]
"For the baseline system, when using only in-domain text for creating BWEs, only the medical unlabeled set was effective, general domain word pairs could not be exploited due to the lack of general semantic knowledge in the BWE model.",6.3 Semi-Supervised BLI,[0],[0]
"On the other hand, by using our domain adapted BWEs, which contain both general domain and in-domain semantical knowledge, we can exploit word pairs from both domains.",6.3 Semi-Supervised BLI,[0],[0]
"Results for adapted BWEs increased in 3 out of 4 cases, where the only exception is when using multiple translations for a given source word (which may have been caused by the bigger vocabulary size).
",6.3 Semi-Supervised BLI,[0],[0]
"These results show that adapted BWEs are needed to exploit unlabeled data well which leads to an impressive overall 3.71 increase compared with the best result in previous work (Heyman et al., 2017), by using only unlabeled data.",6.3 Semi-Supervised BLI,[0],[0]
Bilingual word embeddings trained on general domain data yield poor results in out-of-domain tasks.,7 Conclusion,[0],[0]
We presented experiments on two different low-resource task/domain combinations.,7 Conclusion,[0],[0]
Our delightfully simple task independent method to adapt BWEs to a specific domain uses unlabeled monolingual data only.,7 Conclusion,[0],[0]
We showed that with the support of adapted BWEs the performance of offthe-shelf methods can be increased for both crosslingual Twitter sentiment classification and medical bilingual lexicon induction.,7 Conclusion,[0],[0]
"Furthermore, by adapting the broadly applicable semi-supervised approach of Häusser et al. (2017) (which until now has only been applied in computer vision) we were able to effectively exploit unlabeled data to further improve performance.",7 Conclusion,[0],[0]
"We showed that, when also using high-quality adapted BWEs, the performance of the semi-supervised systems can be significantly increased by using unlabeled data at classifier training time.",7 Conclusion,[0],[0]
"In addition, CLSC results are competitive with a system that uses targetlanguage labeled data, even when we use no such target-language labeled data.",7 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their valuable input.,Acknowledgments,[0],[0]
This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement№ 640550).,Acknowledgments,[0],[0]
"Bilingual tasks, such as bilingual lexicon induction and cross-lingual classification, are crucial for overcoming data sparsity in the target language.",abstractText,[0],[0]
"Resources required for such tasks are often out-of-domain, thus domain adaptation is an important problem here.",abstractText,[0],[0]
We make two contributions.,abstractText,[0],[0]
"First, we test a delightfully simple method for domain adaptation of bilingual word embeddings.",abstractText,[0],[0]
We evaluate these embeddings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction.,abstractText,[0],[0]
"Second, we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks.",abstractText,[0],[0]
We show that this method also helps in low-resource setups.,abstractText,[0],[0]
"Using both methods together we achieve large improvements over our baselines, by using only additional unlabeled data.",abstractText,[0],[0]
Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 105–114 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
105
We propose to consider these two aspects jointly. We develop TWOWINGOS (twowing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence. Given the claim, TWOWINGOS attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim. We treat this challenge as coupled optimization problems, training a joint model for it. TWOWINGOS offers two advantages: (i) Unlike pipeline systems, it facilitates flexible-size evidence set, and (ii) Joint training improves both the claim verification and the evidence identification. Experiments on a benchmark dataset show state-of-the-art performance.1",text,[0],[0]
"A claim, e.g., “Marilyn Monroe worked with Warner Brothers”, is an assertive sentence that may be true or false.",1 Introduction,[0],[0]
"While the task of claim verification will not tell us the absolute truth of this claim, it is expected to determine whether the claim is supported by evidence in a given text collection.",1 Introduction,[0],[0]
"Specifically, given a claim and a text corpus, evidential claim verification, demonstrated in
1cogcomp.org/page/publication_view/847
Figure 1, aims at identifying text snippets in the corpus that act as evidence that supports or refutes the claim.
",1 Introduction,[0],[0]
This problem has broad applications.,1 Introduction,[0],[0]
"For example, knowledge bases (KB), such as Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007), can be augmented with a new relational statement such as “(Afghanistan, is source of, Kushan Dynasty)”.",1 Introduction,[0],[0]
"This needs to be first verified by a claim verification process and supported by evidence (Roth et al., 2009; Chaganty et al., 2017).",1 Introduction,[0],[0]
"More broadly, claim verification is a key component in any technical solution addressing recent concerns about the trustworthiness of online content (Vydiswaran et al., 2011; Pasternack and Roth, 2013; Hovy et al., 2013).",1 Introduction,[0],[0]
"In both scenarios, we care about whether or not a claim holds, and seek reliable evidence in support of this decision.
",1 Introduction,[0],[0]
Evidential claim verification requires that we address three challenges.,1 Introduction,[0],[0]
"First, to locate text snippets in the given corpus that can potentially be used to determine the truth value of the given claim.",1 Introduction,[0],[0]
"This differs from the conventional textual entailment (TE) problem (Dagan et al., 2013) as here we first look for the premises given a hypothesis.",1 Introduction,[0],[0]
"Clearly, the evidence one seeks depends on the claim, as well as on the eventual entailment
decision – the same claim would require different supporting than refuting evidence.",1 Introduction,[0],[0]
This motivates us to develop an approach that can transfer knowledge from claim verification to evidence identification.,1 Introduction,[0],[0]
"Second, the evidence for a claim might require aggregating information from multiple sentences and even multiple documents (rf. #3 in Table 4).",1 Introduction,[0],[0]
"Therefore, a set, rather than a collection of independent text snippets, should be chosen to act as evidence.",1 Introduction,[0],[0]
"And, finally, in difference from TE, given a set of evidence sentences as a premise, the truth value of the claim should depend on all of the evidence, rather than on a single sentence there.
",1 Introduction,[0],[0]
The discussion above suggests that claim verification and evidence identification are tightly coupled.,1 Introduction,[0],[0]
"Claim should influence the identification of appropriate evidence, and “trusted evidence boosts the claim’s veracity” (Vydiswaran et al., 2011).",1 Introduction,[0],[0]
"Consequently, we propose TWOWINGOS, a twowing optimization strategy2, to support this process.",1 Introduction,[0],[0]
"As shown in Figure 2, we consider a set of sentences S as the candidate evidence space, a claim x, and a decision space Y for the claim verification.",1 Introduction,[0],[0]
"In the optimal condition, a one-hot vector over Y indicates which decision to make towards the claim, and a binary vector over S indicates a subset of sentences Se (in blue in Figure 2) to act as evidence.
",1 Introduction,[0],[0]
"Prior work mostly approached this problem as a pipeline procedure – first, given a claim x, determine Se by some similarity matching; then, conduct textual entailment over (Se, x) pairs.",1 Introduction,[0],[0]
"Our framework, TWOWINGOS, optimizes the two
2By “two-wing optimization”, we mean that the same object, i.e., the claim, is mapped into two target spaces in a joint optimization scheme.
subtasks jointly, so that both claim verification and evidence identification can enhance each other.",1 Introduction,[0],[0]
"TWOWINGOS is a generic framework making use of a shared representation of the claim to cotrain evidence identification and claim verification.
",1 Introduction,[0],[0]
"TWOWINGOS is tested on the FEVER benchmark (Thorne et al., 2018), showing≈30% F1 improvement for evidence identification, and ≈23% accuracy increase in claim verification.",1 Introduction,[0],[0]
Our analysis shows that (i) entity mentions in claims provide a strong clue for retrieving relevant passages; (ii) composition of evidence clues across sentences helps claim verification; and that (iii) the joint training scheme provides significant benefits of a pipeline architecture.,1 Introduction,[0],[0]
Most work focuses on the dataset construction while lacking advanced models to handle the problem.,2 Related Work,[0],[0]
"Vlachos and Riedel (2014) propose and define the “fact checking” problem, without a concrete solution.",2 Related Work,[0],[0]
Ferreira and Vlachos (2016) release the dataset “Emergent” for rumor debunking.,2 Related Work,[0],[0]
Each claim is accompanied by an article headline as evidence.,2 Related Work,[0],[0]
Then a three-way logistic regression model is used over some rule-based features.,2 Related Work,[0],[0]
No need to search for evidence.,2 Related Work,[0],[0]
"Wang (2017) release a larger dataset for fake news detection, and propose a hybrid neural network to integrate the statement and the speaker’s meta data to do classification.",2 Related Work,[0],[0]
"However, the presentation of evidences is ignored.",2 Related Work,[0],[0]
"Kobayashi et al. (2017) release a similar dataset to (Thorne et al., 2018), but they do not consider the evaluation of evidence reasoning.
",2 Related Work,[0],[0]
"Some work mainly pays attention to determining whether the claim is true or false, assuming evidence facts are provided or neglecting presenting evidence totally, e.g., (Angeli and Manning, 2014) – given a database of true facts as premises, predicting whether an unseen fact is true and should belong to the database by natural logic inference.",2 Related Work,[0],[0]
"Open-domain question answering (QA) against a text corpus (Yin et al., 2016; Chen et al., 2017; Wang et al., 2018) can also be treated as claim verification problem, if we treat (question, correct answer) as a claim.",2 Related Work,[0],[0]
"However, little work has studied how well a QA system can identify all the answer evidence.
",2 Related Work,[0],[0]
"Only a few works considered improving the evidence presentation in claim verification problems.
",2 Related Work,[0],[0]
"Roth et al. (2009) introduce the task of Entailed Relation Recognition – given a set of short paragraphs and a relational fact in the triple form of (argument1, relation, argument2), finding the paragraphs that can entail this fact.",2 Related Work,[0],[0]
"They first use Expanded Lexical Retrieval to rank and keep the topk paragraphs as candidates, then build a TE classifier over each (candidate, statement) pair.",2 Related Work,[0],[0]
The work directly related to us is by Thorne et al. (2018).,2 Related Work,[0],[0]
"Given claims and a set of Wikipages, Thorne et al. (2018) use a retrieval model based on TF-IDF to locate top-5 sentences in top-5 pages as evidence, then utilize a neural entailment model to classify (evidence, claim) pairs.
",2 Related Work,[0],[0]
"In contrast, our work tries to optimize the claim verification as well as the evidence identification in a joint training scheme, which is more than just supporting or refuting the claims.",2 Related Work,[0],[0]
"Figure 2 illustrates the two-wing optimization problem addressed in this work: given a collection of evidence candidates S={s1, s2, · · · , si, · · · , sm}, a claim x and a decision set Y = {y1 · · · , yn}, the model TWOWINGOS predicts a binary vector p over S and a one-hot vector o over Y against the ground truth, a binary vector q and a one-hot vector z, respectively.",3 The TWOWINGOS Model,[0],[0]
"A binary vector over S means a subset of sentences (Se) act as evidence, and the one-hot vector indicates a single decision (yi) to be made towards the claim x given the evidence Se.",3 The TWOWINGOS Model,[0],[0]
"Next, we use two separate subsections to elaborate the process of evidence identification (i.e., optimize p to q) and the claim verification (i.e., optimize o to z).",3 The TWOWINGOS Model,[0],[0]
"A simple approach to identifying evidence is to detect the top-k sentences that are lexically similar to the claim, as some pipeline systems (Roth et al., 2009; Thorne et al., 2018) do.",3.1 Evidence identification,[0],[0]
"However, a claimunaware fixed k is less optimal, adding noise or missing key supporting factors, consequently limiting the performance.
",3.1 Evidence identification,[0],[0]
"In this work, we approach the evidence by modeling sentences S={s1, · · · , si, · · · , sm} with the claim x as context in a supervised learning scheme.",3.1 Evidence identification,[0],[0]
"For each si, the problem turns out to be learning a probability: how likely si can entail the claim conditioned on other candidates as context, as shown by the blue items in Figure 2.
To start, a piece of text t (t ∈ S ∪ {x}) is represented as a sequence of l hidden states, forming a feature map T ∈ Rd×l, where d is the dimensionality of hidden states.",3.1 Evidence identification,[0],[0]
"We first stack a vanilla CNN (convolution & max-pooling) (LeCun et al., 1998) over T to get a representation for t. As a result, each evidence candidate si has a representation si, and the claim x has a representation x.",3.1 Evidence identification,[0],[0]
"To get a probability for each si, we need first to build its claim-aware representation ri.
Coarse-grained representation.",3.1 Evidence identification,[0],[0]
"We directly concatenate the representation of si and x, generated by the vanilla CNN, as:
ri =",3.1 Evidence identification,[0],[0]
"[si,x, si · xT ]",3.1 Evidence identification,[0],[0]
"(1)
This coarse-grained approach makes use of merely the sentence-level representations while neglecting more fine-grained interactions between the sentences and the claim.
",3.1 Evidence identification,[0],[0]
Fine-grained representation,3.1 Evidence identification,[0],[0]
.,3.1 Evidence identification,[0],[0]
"Instead of directly employing the sentence-level representations, here we explore claim-aware representations for each word in sentence si, then compose them as the sentence representation ri, inspired by the Attentive Convolution (Yin and Schütze, 2017).
",3.1 Evidence identification,[0],[0]
"For each word sji in si, we first calculate its matching score towards each word xz in x, by dot product over their hidden states.",3.1 Evidence identification,[0],[0]
"Then the representation of the claim, as the context for the word sji , is formed as:
cji = ∑ z softmax(sji · (x z)T ) ·",3.1 Evidence identification,[0],[0]
"xz (2)
Now, word sji has left context s j−1 i , right context sj+1i in si, and the claim-aware context c j i from x.",3.1 Evidence identification,[0],[0]
"A convolution encoder generates its claim-aware representation iji :
iji = tanh(W ·",3.1 Evidence identification,[0],[0]
"[s j−1 i , s j i , s j+1 i , c j",3.1 Evidence identification,[0],[0]
"i ] + b) (3) where parameters W ∈ Rd×4d, b ∈ Rd.",3.1 Evidence identification,[0],[0]
"To compose those claim-aware word representations as the representation for sentence si, we use a max-pooling over {iji} along with j, generating ii.
",3.1 Evidence identification,[0],[0]
"We use term fint(si, x) to denote this whole process, so that:
ii = fint(si, x) (4)
",3.1 Evidence identification,[0],[0]
"At this point, the fine-grained representation for evidence candidate si is:
ri =",3.1 Evidence identification,[0],[0]
"[si,x, si · xT , ii] (5)
Loss function.",3.1 Evidence identification,[0],[0]
"With a claim-aware representation ri, the sentence si subsequently gets a probability, acting as the evidence, αi ∈ (0, 1) via a non-linear sigmoid function:
αi = sigmoid(v · rTi ) (6)
where parameter vector v has the same dimensionality as ri.
",3.1 Evidence identification,[0],[0]
"In the end, all evidence candidates in S have a ground-truth binary vector q and the predicted probability vector α; then loss lev (“ev”: evidence) is implemented as a binary cross-entropy:
lev =",3.1 Evidence identification,[0],[0]
m∑ i=1,3.1 Evidence identification,[0],[0]
−(qi log(αi)+(1−qi) log(1−αi)),3.1 Evidence identification,[0],[0]
"(7)
As the output of this evidence identification module, we binarize the probability vector α by pi =",3.1 Evidence identification,[0],[0]
[αi > 0.5] (“[x]” is 1 if x is true or 0 otherwise).,3.1 Evidence identification,[0],[0]
pi indicates si is evidence or not.,3.1 Evidence identification,[0],[0]
All {si} with pi = 1 act as evidence set Se.,3.1 Evidence identification,[0],[0]
"As shown in Figure 2, to figure out an entailment decision yi for the claim x, the evidence Se possibly consists of more than one sentence.",3.2 Claim verification,[0],[0]
"Furthermore, those evidence sentences are not necessarily in textual order nor from the same passage.",3.2 Claim verification,[0],[0]
"So, we need a mechanism that enables each evidence or even each word inside to be aware of the content from other evidence sentences.",3.2 Claim verification,[0],[0]
"Similar to the aforementioned approach to evidence identification, we come up with three methods, with different representation granularity, to learn a representation for (Se, x), i.e., the input for claim verification, shown in Figure 3.
Coarse-grained representation.",3.2 Claim verification,[0],[0]
"In this case, we treat Se as a whole, constructing its representation e by summing up the representations of all sentences in Se in a weighted way:
e =",3.2 Claim verification,[0],[0]
m∑ i=1,3.2 Claim verification,[0],[0]
"αi · pi · si (8)
where αi, from Equation 6, is the probability of si being the evidence.
",3.2 Claim verification,[0],[0]
"Then the (Se, x) pair gets a coarse-grained concatenated representation:",3.2 Claim verification,[0],[0]
"[e,x].",3.2 Claim verification,[0],[0]
It does not model the interactions within the evidence nor the interactions between the evidence and the claim.,3.2 Claim verification,[0],[0]
"Based on our experience in evidence identification
module, the representation of a sentence is better learned by composing context-aware word-level representations.",3.2 Claim verification,[0],[0]
"Next, we introduce how to learn fine-grained representation for the (Se, x) pair.
",3.2 Claim verification,[0],[0]
Single-channel fine-grained representation.,3.2 Claim verification,[0],[0]
"By “single-channel,” we mean each sentence si is aware of the claim x as its single context.
",3.2 Claim verification,[0],[0]
"For a single pair (si, x), we utilize the function fint() in Equation 4 to build the fine-grained representations for both si and x, obtaining ii =
fint(si, x) for si and xi = fint(x, si) for x.",3.2 Claim verification,[0],[0]
"For (Se, x), we compose all the {ii} and all the {xi} along with i, via a weighted max-pooling:
e = maxpooli(αi · pi · ii) (9) x = maxpooli(αi · pi · xi) (10)
",3.2 Claim verification,[0],[0]
This weighted max-pooling ensures that the sentences with higher probabilities of being evidence have a higher chance to present their features.,3.2 Claim verification,[0],[0]
"As a result, (Se, x) gets a concatenated representation:",3.2 Claim verification,[0],[0]
"[e, x]
Two-channel fine-grained representation.",3.2 Claim verification,[0],[0]
"By “two-channel,” we mean that each evidence si is aware of two kinds of context, one from the claim x, the other from the remaining evidences.
",3.2 Claim verification,[0],[0]
Our first step is to accumulate evidence clues within Se.,3.2 Claim verification,[0],[0]
"To start, we concatenate all sentences in Se as a fake long sentence Ŝ consisting of hidden states {ŝ}.",3.2 Claim verification,[0],[0]
"Similar to Equation 2, for each word sji in sentence si, we accumulate all of its related clues (cji ) from Ŝ as follows:
cji = ∑ z softmax(sji · (ŝ z)T ) · ŝz (11)
",3.2 Claim verification,[0],[0]
"Then we update sji , the representation of word sji , by element-wise addition:
sji = s j i ⊕ c j",3.2 Claim verification,[0],[0]
"i (12)
",3.2 Claim verification,[0],[0]
This step enables the word sji to “see” all related clues from Se.,3.2 Claim verification,[0],[0]
The reason we add s j i and c j,3.2 Claim verification,[0],[0]
"i is motivated by a simple experience: Assume the claim “Lily lives in the biggest city in Canada”, and one sentence contains a clue “· · · Lily lives in Toronto · · · ” and another sentence contains a clue “· · · Toronto is Canada’s largest city· · · ”.",3.2 Claim verification,[0],[0]
"The most simple yet effective approach to aggregating the two clues is to sum up their representation vectors (Blacoe and Lapata, 2012) (we do not concatenate them, as those clues have no consistent textual order across different sji ).
",3.2 Claim verification,[0],[0]
"After updating the representation of each word in si, we perform the aforementioned “singlechannel fine-grained representation” between the updated si and the claim x, generating [e, x].
",3.2 Claim verification,[0],[0]
Loss function.,3.2 Claim verification,[0],[0]
"For the claim verification input (Se, x), we forward its representation",3.2 Claim verification,[0],[0]
"[e, x] to a
logistic regression layer in order to infer a probability distribution o over the label space Y :
o = softmax(W ·",3.2 Claim verification,[0],[0]
"[e,x] + b) (13)
where W ∈ Rn×2d, b ∈",3.2 Claim verification,[0],[0]
Rn,3.2 Claim verification,[0],[0]
"The loss lcv (“cv”: claim verification) is implemented as negative log-likelihood:
lcv = − log(o · zT )",3.2 Claim verification,[0],[0]
"(14)
where z is the ground truth one-hot label vector for the claim x on the space Y .",3.2 Claim verification,[0],[0]
"Given the loss lev in evidence identification and the loss lcv in claim verification, the overall training loss is represented by:
l = lev + lcv (15)
",3.3 Joint optimization,[0],[0]
"To ensure that we jointly train the two coupled subtasks with intensive knowledge communication instead of simply putting two pipeline neural networks together, our TWOWINGOS has following configurations: • Both subsystems share the same set of word embeddings as parameters; the vanilla CNNs for learning sentence and claim representations share parameters as well.",3.3 Joint optimization,[0],[0]
•,3.3 Joint optimization,[0],[0]
"The output binary vector p by the evidence identification module is forwarded to the module of claim verification, as shown in Equations 8-10.",3.3 Joint optimization,[0],[0]
•,3.3 Joint optimization,[0],[0]
"Though the representation of a claim’s decision yi is not put explicitly into the module of evidence identification, the claim’s representation x will be fine-tuned by the yi, so that the evidence candidates can get adjustment from the decision yi, since the claims are shared by two modules.",3.3 Joint optimization,[0],[0]
Dataset.,4.1 Setup,[0],[0]
"In this work, we use FEVER (Thorne et al., 2018).",4.1 Setup,[0],[0]
"The claims in FEVER were generated from the introductory parts of about 50K
Wikipedia pages of a June 2017 dump.",4.1 Setup,[0],[0]
Annotators construct claims about a single fact of the title entity with arbitrarily complex expressions and entity forms.,4.1 Setup,[0],[0]
"To increase the claim complexity so that claims would not be trivially verified, annotators adopt two routes: (i) Providing additional knowledge: Annotators can explore a dictionary of terms that were (hyper-)linked, along with their pages; (ii) Mutate claims in six ways: negation, paraphrasing, substitution of a relation/entity with a similar/dissimilar one, and making the claims more general/specific.",4.1 Setup,[0],[0]
All resulting claims have 9.4 tokens in average.,4.1 Setup,[0],[0]
"Apart from claims, FEVER also provides a Wikipedia corpus in size of about 5.4 million.
",4.1 Setup,[0],[0]
"Each claim is labeled as SUPPORTED, REFUTED or NOTENOUGHINFO (NEI).",4.1 Setup,[0],[0]
"In addition, evidence sentences, from any wiki page, are required to be provided for SUPPORTED and REFUTED.",4.1 Setup,[0],[0]
Table 1 lists the data statistics.,4.1 Setup,[0],[0]
Figure 4 shows the distributions of sentence sizes and page sizes in FEVER’s evidence set.,4.1 Setup,[0],[0]
"We can see that roughly 28% of the evidence covers more than one sentence, and approximately 16.3% of the evidence covers more than one wiki page.
",4.1 Setup,[0],[0]
"This task has three evaluations: (i) NOSCOREEV – accuracy of claim verification, neglecting the validity of evidence; (ii) SCOREEV – accuracy of claim verification with a requirement that the predicted evidence fully covers the gold evidence for SUPPORTED and REFUTED; (iii) F1 – between the predicted evidence sentences and the ones chosen by annotators.",4.1 Setup,[0],[0]
"We use the officially released evaluation scorer 3.
",4.1 Setup,[0],[0]
"3https://github.com/sheffieldnlp/fever-scorer
Wiki page retrieval4.",4.1 Setup,[0],[0]
"For each claim, we search in the given dictionary of wiki pages in the form of {title: sentence list}, and keep the top-5 ranked pages for fair comparison with Thorne et al. (2018).",4.1 Setup,[0],[0]
Algorithm 1 briefly shows the steps of wiki page retrieval.,4.1 Setup,[0],[0]
"To speed up, we first build an inverted index from words to titles, then for each claim, we only search in the titles that cover at least one claim word.
",4.1 Setup,[0],[0]
"Input: A claim, wiki={title: page vocab} Output: A ranked top-k wiki titles Generate entity mentions from the claim; while each title do
if claim.vocab∩title.vocab is empty then discard this title else title score = the max recall value of title.vocab
in claim and in entity mentions of the claim; if title score = 1.0 then
title.score = title score else
page score = recall of claim in page vocab;
title.score = title score + page score end
end end Sort titles by title.score in descending order
Algorithm 1: Algorithm description of wiki page retrieval for FEVER claims.
",4.1 Setup,[0],[0]
"All sentences of the top-5 retrieved wiki pages are kept as evidence candidates for claims in train, dev and test.",4.1 Setup,[0],[0]
"It is worth mentioning that this page retrieval step is a reasonable preprocessing which controls the complexity of evidence searching in real-world, such as the big space – 5.4 million – in this work.
",4.1 Setup,[0],[0]
Training setup.,4.1 Setup,[0],[0]
"All words are initialized by 300D Word2Vec (Mikolov et al., 2013) embeddings, and are fine-tuned during training.",4.1 Setup,[0],[0]
"The whole system is trained by AdaGrad (Duchi et al., 2011).",4.1 Setup,[0],[0]
"Other hyperparameter values include: learning rate 0.02, hidden size 300, mini-batch size 50, filter width 3.
Baselines.",4.1 Setup,[0],[0]
"In this work, we first consider the two systems reported by Thorne et al. (2018): (i) MLP: A multi-layer perceptron with one hidden layer, based on TF-IDF cosine similarity between the claim and the evidence (all evidence sentences are concatenated as a longer text piece) (Riedel et al., 2017); (ii) Decomp-Att (Parikh et al., 2016):",4.1 Setup,[0],[0]
"A decomposable attention model that develops atten-
4Our retrieval results are released as well.
tion mechanisms to decompose the problem into subproblems to solve in parallel.",4.1 Setup,[0],[0]
"Note that both systems first employed an IR system to keep top5 relevant sentences from the retrieved top-5 wiki pages as static evidence for claims.
",4.1 Setup,[0],[0]
"We further consider the following variants of our own system TWOWINGOS: • Coarse-coarse: Both evidence identification and claim verification adopt coarse-grained representations.
",4.1 Setup,[0],[0]
"To further study our system, we test this “coarse-coarse” in three setups: (i) “pipeline” – train the two modules independently.",4.1 Setup,[0],[0]
"Forward the predicted evidence to do entailment for claims; (ii) “diff-CNN” – joint training with separate CNN parameters to learn sentence/claim representations; (iii) “share-CNN” – joint training with shared CNN parameters.
",4.1 Setup,[0],[0]
The following variants are in joint training.,4.1 Setup,[0],[0]
"• Fine&sentence-wise: Given the evidence with multiple sentences, a natural baseline is to do entailment reasoning for each (sentence, claim), then compose.",4.1 Setup,[0],[0]
"We do entailment reasoning between each predicted evidence sentence and the claim, generating a probability distribution over the label space Y .",4.1 Setup,[0],[0]
"Then we sum up all the distribution vectors element-wise, as an ensemble system, to predict the label; • Four combinations of different grained representation learning: “coarse&fine(single)”, “coarse&fine(two)”, “fine&coarse” and “fine&fine(two)”.",4.1 Setup,[0],[0]
“Single” and “two” refer to the single/two-channel cases respectively.,4.1 Setup,[0],[0]
Performance of passage retrieval.,4.2 Results,[0],[0]
"Table 2 compares our wikipage retriever with the one in
(Thorne et al., 2018), which used a document retriever5 from DrQA (Chen et al., 2017).
",4.2 Results,[0],[0]
Our document retrieval module surpasses the competitor by a big margin in terms of the coverage of gold passages: 89.63% vs. 55.30% (k = 5 in all experiments).,4.2 Results,[0],[0]
Its powerfulness should be attributed to: (i) Entity mention detection in the claims.,4.2 Results,[0],[0]
"(ii) As wiki titles are entities, we have a bi-channel way to match the claim with the wiki page: one with the title, the other with the page body, as shown in Algorithm 1.
",4.2 Results,[0],[0]
Performance on FEVER Table 3 lists the performances of baselines and the TWOWINGOS variants on FEVER (dev&test).,4.2 Results,[0],[0]
"From the dev block, we observe that: • TWOWINGOS (from “share-CNN”) surpasses prior systems in big margins.",4.2 Results,[0],[0]
"Overall, fine-grained schemes in each subtask contribute more than the coarse-grained counterparts; • In the three setups – “pipeline”, “diff-CNN” and “share-CNN” – of coarse-coarse, “pipeline” gets better scores than (Thorne et al., 2018) in terms of evidence identification.",4.2 Results,[0],[0]
“Share-CNN” has comparable F1 as “diff-CNN” while gaining a lot on NOSCOREEV (72.32 vs. 39.22) and SCOREEV (50.12 vs. 21.04).,4.2 Results,[0],[0]
This clearly shows that the claim verification gains much knowledge transferred from the evidence identification module.,4.2 Results,[0],[0]
Both “diff-CNN” and “share-CNN” perform better than “pipeline” (except for the slight inferiority at SCOREEV: 21.04 vs. 22.26).,4.2 Results,[0],[0]
"• Two-channel fine-grained representations show more effective than the single-channel counterpart in claim verification (NOSCOREEV: 78.77 vs. 75.65, SCOREEV: 53.64 vs. 52.65).",4.2 Results,[0],[0]
"As we expected, evidence sentences should collaborate in inferring the truth value of the claims.",4.2 Results,[0],[0]
Two-channel setup enables an evidence candidate aware of other candidates as well as the claim.,4.2 Results,[0],[0]
"• In the last three rows of dev, there is no clear difference among their evidence identification scores.",4.2 Results,[0],[0]
"Recall that “sent-wise” is essentially an ensemble system over each (sentence, claim) entailment result.",4.2 Results,[0],[0]
"“Coarse-grained”, instead, first sums up all sentence representation, then performs ( ∑
(sentence), claim) reasoning.",4.2 Results,[0],[0]
We can also treat this “sum up” as an ensemble.,4.2 Results,[0],[0]
"Their comparison shows that these two kinds of tricks do not
5It compares passages and claims as TF-IDF weighted bag-of-bigrams.
make much difference.",4.2 Results,[0],[0]
"If we adopt “two-channel fine-grained representation” in claim verification, big improvements are observed in both NOSCOREEV (+7.42%) and SCOREEV (+3%).
",4.2 Results,[0],[0]
"In the test block, our system (fine&fine(two)) beats the prior top system across all measurements by big margins – F1: 47.15 vs. 17.47; SCOREEV: 54.33 vs. 31.87; NOSCOREEV: 75.99 vs. 50.91.
",4.2 Results,[0],[0]
"In both dev and test blocks, we can observe that our evidence identification module consistently
obtains balanced recall and precision.",4.2 Results,[0],[0]
"In contrast, the pipeline system by Thorne et al. (2018) has much higher recall than precision (45.89 vs. 10.79).",4.2 Results,[0],[0]
"It is worth mentioning that the SCOREEV metric is highly influenced by the recall value, since SCOREEV is computed on the claim instances whose evidences are fully retrieved, regardless of the precision.",4.2 Results,[0],[0]
"So, ideally, a system can set all sentences as evidence, so that SCOREEV can be promoted to be equal to NOSCOREEV.",4.2 Results,[0],[0]
"Our system is more reliable in this perspective.
",4.2 Results,[0],[0]
Performance vs. #sent.,4.2 Results,[0],[0]
in evidence.,4.2 Results,[0],[0]
Figure 5 shows the results of the five evaluation measures against different sizes of gold evidence sentences in test set.,4.2 Results,[0],[0]
We observe that: (i),4.2 Results,[0],[0]
"Our system has robust precisions across #sentence; however, the recall decreases.",4.2 Results,[0],[0]
"This is not that surprising, since the more ground-truth sentences in evidence, the harder it is to retrieve all of them; (ii) Due to the decrease in recall, the SCOREEV also gets influenced for bigger #sentence.",4.2 Results,[0],[0]
"Interestingly, high precision and worse recall in evidence with more sentences still make consistently strong overall performance, i.e., NOSCOREEV.",4.2 Results,[0],[0]
"This should be due to the fact that the majority (83.18% (Thorne et al., 2018)) of claims can be correctly entailed by a single ground truth sentence, even if any remaining ground truth sentences are unavailable.
",4.2 Results,[0],[0]
Error analysis.,4.2 Results,[0],[0]
"The case #1 in Table 4 shows that our system identifies two pieces of evidence
(i.e., (Telemundo, 0) and (Telemundo, 4)) correctly; however, it falsely predicts the claim label.",4.2 Results,[0],[0]
"(Telemundo, 0): Telemundo is an American Spanish-language terrestrial television · · · .",4.2 Results,[0],[0]
We can easily find that the keyword “Spanishlanguage” should refute the claim.,4.2 Results,[0],[0]
"However, both “Spanish-language” in this evidence and the “English-language” in the claim are unknown tokens with randomly initialized embeddings.",4.2 Results,[0],[0]
This hints that a more careful data preprocessing may be helpful.,4.2 Results,[0],[0]
"In addition, to refute the claim, another clue comes from the combination of (Telemundo, 4) and (Hispanic and Latino Americans, 0).",4.2 Results,[0],[0]
"(Telemundo, 4): “The channel · · · aimed at Hispanic and Latino American audiences”; (Hispanic and Latino Americans, 0): “Hispanic Americans and Latino Americans · · · are descendants of people from countries of Latin America and Spain.”.",4.2 Results,[0],[0]
"Our system only retrieved (Telemundo, 4).",4.2 Results,[0],[0]
"And this clue is hard to grasp as it requires some background knowledge – people from Latin America and Spain usually are not treated as English-speaking.
",4.2 Results,[0],[0]
"In the case #2, our system fails to identify any evidence.",4.2 Results,[0],[0]
"This is due to the failure of our passage retrieval module: it detects entity mentions “Home”, “Holidays” and “American”, and the top-5 retrieved passages are “Home”, “Home for the Holidays”, “American Home”, “American” and “Home for the Holidays (song)”, which unfortunately cover none of the four ground truth passages.",4.2 Results,[0],[0]
"Interestingly, (i) given the falsely retrieved passages, our system predicts “no sentence is valid evidence” (denoted as ∅ in Table 4); (ii) given the empty evidence, our system predicts “NoEnoughInfo” for this claim.",4.2 Results,[0],[0]
"Both make sense.
",4.2 Results,[0],[0]
"In the case #3, a successful classification of the
claim requires information aggregation over the three gold evidence sentences: (Weekly Idol, 0): “Weekly Idol is a South Korean variety show · · · ”; (Weekly Idol, 1): “The show is hosted by comedian Jeong Hyeong-don and rapper Defconn.”; (Defconn, 0): “Defconn (born Yoo Dae-joon; January 6 , 1977 ) is a · · · ”.",4.2 Results,[0],[0]
To successfully retrieve the three sentences as a whole set of evidence is challenging in evidence identification.,4.2 Results,[0],[0]
"Additionally, this example relies on the recognition and matching of digital numbers (1983 vs. 1977), which is beyond the expressivity of word embeddings, and is expected to be handled by rules more easily.",4.2 Results,[0],[0]
"In this work, we build TWOWINGOS, a two-wing optimization framework to address the claim verification problem by presenting precise evidence.",5 Summary,[0],[0]
"Differing from a pipeline system, TWOWINGOS ensures the evidence identification module and the claim verification module are trained jointly, in an end-to-end scheme.",5 Summary,[0],[0]
Experiments show the superiority of TWOWINGOS in the FEVER benchmark.,5 Summary,[0],[0]
We thank group colleagues (Nitish Gupta and Jennifer Sheffield) and Dr. Mo Yu from IBM AI Foundations Lab for providing insightful comments and critiques.,Acknowledgments,[0],[0]
This work was supported by Contract HR0011-15-2-0025 with the US Defense Advanced Research Projects Agency (DARPA).,Acknowledgments,[0],[0]
"Approved for Public Release, Distribution Unlimited.",Acknowledgments,[0],[0]
The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.,Acknowledgments,[0],[0]
Determining whether a given claim is supported by evidence is a fundamental NLP problem that is best modeled as Textual Entailment.,abstractText,[0],[0]
"However, given a large collection of text, finding evidence that could support or refute a given claim is a challenge in itself, amplified by the fact that different evidence might be needed to support or refute a claim.",abstractText,[0],[0]
"Nevertheless, most prior work decouples evidence identification from determining the truth value of the claim given the evidence.",abstractText,[0],[0]
We propose to consider these two aspects jointly.,abstractText,[0],[0]
"We develop TWOWINGOS (twowing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence.",abstractText,[0],[0]
"Given the claim, TWOWINGOS attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim.",abstractText,[0],[0]
"We treat this challenge as coupled optimization problems, training a joint model for it.",abstractText,[0],[0]
"TWOWINGOS offers two advantages: (i) Unlike pipeline systems, it facilitates flexible-size evidence set, and (ii) Joint training improves both the claim verification and the evidence identification.",abstractText,[0],[0]
Experiments on a benchmark dataset show state-of-the-art performance.1,abstractText,[0],[0]
TWOWINGOS: A Two-Wing Optimization Strategy for Evidential Claim Verification,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 87–96 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
87",text,[0],[0]
Entities can often be described by very fine grained types.,1 Introduction,[0],[0]
Consider the sentences “Bill robbed John.,1 Introduction,[0],[0]
He was arrested.”,1 Introduction,[0],[0]
"The noun phrases “John,” “Bill,” and “he” have very specific types that can be inferred from the text.",1 Introduction,[0],[0]
"This includes the facts that “Bill” and “he” are both likely “criminal” due to the “robbing” and “arresting,” while “John” is more likely a “victim” because he was “robbed.”",1 Introduction,[0],[0]
"Such fine-grained types (victim, criminal) are important for context-sensitive tasks such
1Our data and model can be downloaded from: http://nlp.cs.washington.edu/entity_type
as coreference resolution and question answering (e.g. “Who was the victim?”).",1 Introduction,[0],[0]
"Inferring such types for each mention (John, he) is not possible given current typing models that only predict relatively coarse types and only consider named entities.
",1 Introduction,[0],[0]
"To address this challenge, we present a new task: given a sentence with a target entity mention, predict free-form noun phrases that describe appropriate types for the role the target entity plays in the sentence.",1 Introduction,[0],[0]
Table 1 shows three examples that exhibit a rich variety of types at different granularities.,1 Introduction,[0],[0]
"Our task effectively subsumes existing finegrained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, including named entities, nominals, and pronouns.
",1 Introduction,[0],[0]
"Incorporating fine-grained entity types has improved entity-focused downstream tasks, such as relation extraction (Yaghoobzadeh et al., 2017a), question answering (Yavuz et al., 2016),",1 Introduction,[0],[0]
"query analysis (Balog and Neumayer, 2012), and coreference resolution (Durrett and Klein, 2014).",1 Introduction,[0],[0]
These systems used a relatively coarse type ontology.,1 Introduction,[0],[0]
"However, manually designing the ontology is a challenging task, and it is difficult to cover all pos-
12/14/2017 https://homes.cs.washington.edu/~eunsol/finetype_visualization/onto_index.html
https://homes.cs.washington.edu/~eunsol/finetype_visualization/onto_index.html 1/1
sible concepts even within a limited domain.",1 Introduction,[0],[0]
"This can be seen empirically in existing datasets, where the label distribution of fine-grained entity typing datasets is heavily skewed toward coarse-grained types.",1 Introduction,[0],[0]
"For instance, annotators of the OntoNotes dataset (Gillick et al., 2014) marked about half of the mentions as “other,” because they could not find a suitable type in their ontology (see Figure 1 for a visualization and Section 2.2 for details).
",1 Introduction,[0],[0]
"Our more open, ultra-fine vocabulary, where types are free-form noun phrases, alleviates the need for hand-crafted ontologies, thereby greatly increasing overall type coverage.",1 Introduction,[0],[0]
"To better understand entity types in an unrestricted setting, we crowdsource a new dataset of 6,000 examples.",1 Introduction,[0],[0]
"Compared to previous fine-grained entity typing datasets, the label distribution in our data is substantially more diverse and fine-grained.",1 Introduction,[0],[0]
Annotators easily generate a wide range of types and can determine with 85% agreement if a type generated by another annotator is appropriate.,1 Introduction,[0],[0]
"Our evaluation data has over 2,500 unique types, posing a challenging learning problem.
",1 Introduction,[0],[0]
"While our types are harder to predict, they also allow for a new form of contextual distant supervision.",1 Introduction,[0],[0]
"We observe that text often contains cues that explicitly match a mention to its type, in the form of the mention’s head word.",1 Introduction,[0],[0]
"For example, “the incumbent chairman of the African Union” is a type of “chairman.”",1 Introduction,[0],[0]
"This signal complements the supervision derived from linking entities to knowledge bases, which is context-oblivious.",1 Introduction,[0],[0]
"For example, “Clint Eastwood” can be described
with dozens of types, but context-sensitive typing would prefer “director” instead of “mayor” for the sentence “Clint Eastwood won ‘Best Director’ for Million Dollar Baby.”
",1 Introduction,[0],[0]
"We combine head-word supervision, which provides ultra-fine type labels, with traditional signals from entity linking.",1 Introduction,[0],[0]
"Although the problem is more challenging at finer granularity, we find that mixing fine and coarse-grained supervision helps significantly, and that our proposed model with a multitask objective exceeds the performance of existing entity typing models.",1 Introduction,[0],[0]
"Lastly, we show that head-word supervision can be used for previous formulations of entity typing, setting the new state-of-the-art performance on an existing finegrained NER benchmark.",1 Introduction,[0],[0]
"Given a sentence and an entity mention e within it, the task is to predict a set of natural-language phrases T that describe the type of e. The selection of T is context sensitive; for example, in “Bill Gates has donated billions to eradicate malaria,” Bill Gates should be typed as “philanthropist” and not “inventor.”",2 Task and Data,[0],[0]
"This distinction is important for context-sensitive tasks such as coreference resolution and question answering (e.g. “Which philanthropist is trying to prevent malaria?”).
",2 Task and Data,[0],[0]
"We annotate a dataset of about 6,000 mentions via crowdsourcing (Section 2.1), and demonstrate that using an large type vocabulary substantially increases annotation coverage and diversity over existing approaches (Section 2.2).",2 Task and Data,[0],[0]
"To capture multiple domains, we sample sentences from Gigaword (Parker et al., 2011), OntoNotes (Hovy et al., 2006), and web articles (Singh et al., 2012).",2.1 Crowdsourcing Entity Types,[0],[0]
"We select entity mentions by taking maximal noun phrases from a constituency parser (Manning et al., 2014) and mentions from a coreference resolution system (Lee et al., 2017).
",2.1 Crowdsourcing Entity Types,[0],[0]
"We provide the sentence and the target entity mention to five crowd workers on Mechanical Turk, and ask them to annotate the entity’s type.",2.1 Crowdsourcing Entity Types,[0],[0]
"To encourage annotators to generate fine-grained types, we require at least one general type (e.g. person, organization, location) and two specific types (e.g. doctor, fish, religious institute), from a type vocabulary of about 10K frequent noun phrases.",2.1 Crowdsourcing Entity Types,[0],[0]
"We use WordNet (Miller, 1995) to expand these types automatically by generating all their synonyms and hypernyms based on the most common sense, and ask five different annotators to validate the generated types.",2.1 Crowdsourcing Entity Types,[0],[0]
Each pair of annotators agreed on 85% of the binary validation decisions (i.e. whether a type is suitable or not) and 0.47 in Fleiss’s κ.,2.1 Crowdsourcing Entity Types,[0],[0]
"To further improve consistency, the final type set contained only types selected by at least 3/5 annotators.",2.1 Crowdsourcing Entity Types,[0],[0]
"Further crowdsourcing details are available in the supplementary material.
",2.1 Crowdsourcing Entity Types,[0],[0]
Our collection process focuses on precision.,2.1 Crowdsourcing Entity Types,[0],[0]
"Thus, the final set is diverse but not comprehensive, making evaluation non-trivial (see Section 5).",2.1 Crowdsourcing Entity Types,[0],[0]
"We collected about 6,000 examples.",2.2 Data Analysis,[0],[0]
"For analysis, we classified each type into three disjoint bins: • 9 general types: person, location, object, orga-
nization, place, entity, object, time, event • 121 fine-grained types, mapped to fine-grained
entity labels from prior work (Ling and Weld, 2012; Gillick et al., 2014) (e.g. film, athlete) • 10,201 ultra-fine types, encompassing every
other label in the type space (e.g. detective, lawsuit, temple, weapon, composer)
",2.2 Data Analysis,[0],[0]
"On average, each example has 5 labels: 0.9 general, 0.6 fine-grained, and 3.9 ultra-fine types.",2.2 Data Analysis,[0],[0]
"Among the 10,000 ultra-fine types, 2,300 unique types were actually found in the 6,000 crowdsourced examples.",2.2 Data Analysis,[0],[0]
"Nevertheless, our distant supervision data (Section 3) provides positive training examples for every type in the entire vocabulary, and our model (Section 4) can and does predict from a 10K type vocabulary.",2.2 Data Analysis,[0],[0]
"For example,
the model correctly predicts “television network” and “archipelago” for some mentions, even though that type never appears in the 6,000 crowdsourced examples.
",2.2 Data Analysis,[0],[0]
Improving Type Coverage,2.2 Data Analysis,[0],[0]
We observe that prior fine-grained entity typing datasets are heavily focused on coarse-grained types.,2.2 Data Analysis,[0],[0]
"To quantify our observation, we calculate the distribution of types in FIGER (Ling and Weld, 2012), OntoNotes (Gillick et al., 2014), and our data.",2.2 Data Analysis,[0],[0]
"For examples with multiple types (|T | > 1), we counted each type 1/|T | times.
",2.2 Data Analysis,[0],[0]
Figure 2 shows the percentage of labels covered by the top N labels in each dataset.,2.2 Data Analysis,[0],[0]
"In previous enitity typing datasets, the distribution of labels is highly skewed towards the top few labels.",2.2 Data Analysis,[0],[0]
"To cover 80% of the examples, FIGER requires only the top 7 types, while OntoNotes needs only 4; our dataset requires 429 different types.
",2.2 Data Analysis,[0],[0]
"Figure 1 takes a deeper look by visualizing the types that cover 90% of the data, demonstrating the diversity of our dataset.",2.2 Data Analysis,[0],[0]
"It is also striking that more than half of the examples in OntoNotes are classified as “other,” perhaps because of the limitation of its predefined ontology.
",2.2 Data Analysis,[0],[0]
"Improving Mention Coverage Existing datasets focus mostly on named entity mentions, with the exception of OntoNotes, which contained nominal expressions.",2.2 Data Analysis,[0],[0]
"This has implications on the transferability of FIGER/OntoNotes-based models to tasks such as coreference resolution, which need to analyze all types of entity mentions (pronouns, nominal expressions, and named entity
mentions).",2.2 Data Analysis,[0],[0]
"Our new dataset provides a wellrounded benchmark with roughly 40% pronouns, 38% nominal expressions, and 22% named entity mentions.",2.2 Data Analysis,[0],[0]
"The case of pronouns is particularly interesting, since the mention itself provides little information.",2.2 Data Analysis,[0],[0]
Training data for fine-grained NER systems is typically obtained by linking entity mentions and drawing their types from knowledge bases (KBs).,3 Distant Supervision,[0],[0]
"This approach has two limitations: recall can suffer due to KB incompleteness (West et al., 2014), and precision can suffer when the selected types do not fit the context (Ritter et al., 2011).",3 Distant Supervision,[0],[0]
"We alleviate the recall problem by mining entity mentions that were linked to Wikipedia in HTML, and extract relevant types from their encyclopedic definitions (Section 3.1).",3 Distant Supervision,[0],[0]
"To address the precision issue (context-insensitive labeling), we propose a new source of distant supervision: automatically extracted nominal head words from raw text (Section 3.2).",3 Distant Supervision,[0],[0]
Using head words as a form of distant supervision provides fine-grained information about named entities and nominal mentions.,3 Distant Supervision,[0],[0]
"While a KB may link “the 44th president of the United States” to many types such as author, lawyer, and professor, head words provide only the type “president”, which is relevant in the context.
",3 Distant Supervision,[0],[0]
We experiment with the new distant supervision sources as well as the traditional KB supervision.,3 Distant Supervision,[0],[0]
Table 2 shows examples and statistics for each source of supervision.,3 Distant Supervision,[0],[0]
We annotate 100 examples from each source to estimate the noise and usefulness in each signal (precision in Table 2).,3 Distant Supervision,[0],[0]
"For KB supervision, we leveraged training data from prior work (Ling and Weld, 2012; Gillick et al., 2014) by manually mapping their ontology to our 10,000 noun type vocabulary, which covers 130 of our labels (general and fine-grained).2 Section 6 defines this mapping in more detail.
",3.1 Entity Linking,[0],[0]
"To improve both entity and type coverage of KB supervision, we use definitions from Wikipedia.",3.1 Entity Linking,[0],[0]
"We follow Shnarch et al. () who observed that the first sentence of a Wikipedia article often states the entity’s type via an “is a” relation; for example, “Roger Federer is a Swiss professional tennis player.”",3.1 Entity Linking,[0],[0]
"Since we are using a large type vocabulary, we can now mine this typing information.3",3.1 Entity Linking,[0],[0]
"We extracted descriptions for 3.1M entities which contain 4,600 unique type labels such as “competition,” “movement,” and “village.”
",3.1 Entity Linking,[0],[0]
"We bypass the challenge of automatically linking entities to Wikipedia by exploiting existing hyperlinks in web pages (Singh et al., 2012), following prior work (Ling and Weld, 2012; Yosef et al., 2012).",3.1 Entity Linking,[0],[0]
"Since our heuristic extraction of types from the definition sentence is somewhat noisy, we use a more conservative entity linking policy4 that yields a signal with similar overall accuracy to KB-linked data.
2Data from: https://github.com/ shimaokasonse/NFGEC
3We extract types by applying a dependency parser (Manning et al., 2014) to the definition sentence, and taking nouns that are dependents of a copular edge or connected to nouns linked to copulars via appositive or conjunctive edges.
",3.1 Entity Linking,[0],[0]
4Only link if the mention contains the Wikipedia entity’s name and the entity’s name contains the mention’s head.,3.1 Entity Linking,[0],[0]
Many nominal entity mentions include detailed type information within the mention itself.,3.2 Contextualized Supervision,[0],[0]
"For example, when describing Titan V as “the newlyreleased graphics card”, the head words and phrases of this mention (“graphics card” and “card”) provide a somewhat noisy, but very easy to gather, context-sensitive type signal.
",3.2 Contextualized Supervision,[0],[0]
"We extract nominal head words with a dependency parser (Manning et al., 2014) from the Gigaword corpus as well as the Wikilink dataset.",3.2 Contextualized Supervision,[0],[0]
"To support multiword expressions, we included nouns that appear next to the head if they form a phrase in our type vocabulary.",3.2 Contextualized Supervision,[0],[0]
"Finally, we lowercase all words and convert plural to singular.
",3.2 Contextualized Supervision,[0],[0]
Our analysis reveals that this signal has a comparable accuracy to the types extracted from entity linking (around 80%).,3.2 Contextualized Supervision,[0],[0]
"Many errors are from the parser, and some errors stem from idioms and transparent heads (e.g. “parts of capital” labeled as “part”).",3.2 Contextualized Supervision,[0],[0]
"While the headword is given as an input to the model, with heavy regularization and multitasking with other supervision sources, this supervision helps encode the context.",3.2 Contextualized Supervision,[0],[0]
We design a model for predicting sets of types given a mention in context.,4 Model,[0],[0]
"The architecture resembles the recent neural AttentiveNER model (Shimaoka et al., 2017), while improving the sentence and mention representations, and introducing a new multitask objective to handle multiple sources of supervision.",4 Model,[0],[0]
"The hyperparameter settings are listed in the supplementary material.
",4 Model,[0],[0]
"Context Representation Given a sentence x1, . .",4 Model,[0],[0]
.,4 Model,[0],[0]
", xn, we represent each token xi using a pre-trained word embedding wi.",4 Model,[0],[0]
"We concatenate an additional location embedding li which indicates whether xi is before, inside, or after the mention.",4 Model,[0],[0]
"We then use [xi; li] as an input to a bidirectional LSTM, producing a contextualized representation hi for each token; this is different from the architecture of Shimaoka et al. 2017, who used two separate bidirectional LSTMs on each side of the mention.",4 Model,[0],[0]
"Finally, we represent the context c as a weighted sum of the contextualized token representations using MLP-based attention:
ai = SoftMaxi(va · relu(Wahi))
",4 Model,[0],[0]
"Where Wa and va are the parameters of the attention mechanism’s MLP, which allows interaction
between the forward and backward directions of the LSTM before computing the weight factors.
",4 Model,[0],[0]
"Mention Representation We represent the mention m as the concatenation of two items: (a) a character-based representation produced by a CNN on the entire mention span, and (b) a weighted sum of the pre-trained word embeddings in the mention span computed by attention, similar to the mention representation in a recent coreference resolution model (Lee et al., 2017).",4 Model,[0],[0]
The final representation is the concatenation of the context and mention representations: r =,4 Model,[0],[0]
"[c;m].
Label Prediction",4 Model,[0],[0]
We learn a type label embedding matrix Wt ∈ Rn×d where n is the number of labels in the prediction space and d is the dimension of r.,4 Model,[0],[0]
"This matrix can be seen as a combination of three sub matrices, Wgeneral,Wfine,Wultra, each of which contains the representations of the general, fine, and ultra-fine types respectively.",4 Model,[0],[0]
We predict each type’s probability via the sigmoid of its inner product with r: y = σ(Wtr).,4 Model,[0],[0]
"We predict every type t for which yt > 0.5, or argmax yt if there is no such type.
",4 Model,[0],[0]
"Multitask Objective The distant supervision sources provide partial supervision for ultra-fine types; KBs often provide more general types, while head words usually provide only ultra-fine types, without their generalizations.",4 Model,[0],[0]
"In other words, the absence of a type at a different level of abstraction does not imply a negative signal; e.g. when the head word is “inventor”, the model should not be discouraged to predict “person”.
",4 Model,[0],[0]
"Prior work used a customized hinge loss (Abhishek et al., 2017) or max margin loss (Ren et al., 2016a) to improve robustness to noisy or incomplete supervision.",4 Model,[0],[0]
We propose a multitask objective that reflects the characteristic of our training dataset.,4 Model,[0],[0]
"Instead of updating all labels for each example, we divide labels into three bins (general, fine, and ultra-fine), and update labels only in bin containing at least one positive label.",4 Model,[0],[0]
"Specifically, the training objective is to minimize J where t is the target vector at each granularity:
Jall = Jgeneral · 1general(t)",4 Model,[0],[0]
"+ Jfine · 1fine(t) + Jultra · 1ultra(t)
Where 1category(t) is an indicator function that checks if t contains a type in the category, and
Jcategory is the category-specific logistic regression objective:
J =",4 Model,[0],[0]
− ∑ i ti · log(yi) + (1− ti) · log(1− yi),4 Model,[0],[0]
"Experiment Setup The crowdsourced dataset (Section 2.1) was randomly split into train, development, and test sets, each with about 2,000 examples.",5 Evaluation,[0],[0]
We use this relatively small manuallyannotated training set (Crowd in Table 4) alongside the two distant supervision sources: entity linking (KB and Wikipedia definitions) and head words.,5 Evaluation,[0],[0]
"To combine supervision sources of different magnitudes (2K crowdsourced data, 4.7M entity linking data, and 20M head words), we sample a batch of equal size from each source at each iteration.",5 Evaluation,[0],[0]
"We reimplement the recent AttentiveNER model (Shimaoka et al., 2017) for reference.5
We report macro-averaged precision, recall, and F1, and the average mean reciprocal rank (MRR).
",5 Evaluation,[0],[0]
Results Table 3 shows the performance of our model and our reimplementation of AttentiveNER.,5 Evaluation,[0],[0]
"Our model, which uses a multitask objective to learn finer types without punishing more general types, shows recall gains at the cost of drop in precision.",5 Evaluation,[0],[0]
"The MRR score shows that our
5We use the AttentiveNER model with no engineered features or hierarchical label encoding (as a hierarchy is not clear in our label setting) and let it predict from the same label space, training with the same supervision data.
model is slightly better than the baseline at ranking correct types above incorrect ones.
",5 Evaluation,[0],[0]
Table 4 shows the performance breakdown for different type granularity and different supervision.,5 Evaluation,[0],[0]
"Overall, as seen in previous work on finegrained NER literature (Gillick et al., 2014; Ren et al., 2016a), finer labels were more challenging to predict than coarse grained labels, and this issue is exacerbated when dealing with ultra-fine types.",5 Evaluation,[0],[0]
"All sources of supervision appear to be useful, with crowdsourced examples making the biggest impact.",5 Evaluation,[0],[0]
"Head word supervision is particularly helpful for predicting ultra-fine labels, while entity linking improves fine label prediction.",5 Evaluation,[0],[0]
"The low general type performance is partially because of nominal/pronoun mentions (e.g. “it”), and because of the large type inventory (sometimes “location” and “place” are annotated interchangeably).
",5 Evaluation,[0],[0]
"Analysis We manually analyzed 50 examples from the development set, four of which we present in Table 5.",5 Evaluation,[0],[0]
"Overall, the model was able to generate accurate general types and a diverse set of type labels.",5 Evaluation,[0],[0]
"Despite our efforts to annotate a comprehensive type set, the gold labels still miss many potentially correct labels (example (a): “man” is reasonable but counted as incorrect).",5 Evaluation,[0],[0]
"This makes the precision estimates lower than the actual performance level, with about half the precision errors belonging to this category.",5 Evaluation,[0],[0]
"Real precision errors include predicting co-hyponyms (example (b): “accident” instead of “attack”), and types that
may be true, but are not supported by the context.",5 Evaluation,[0],[0]
We found that the model often abstained from predicting any fine-grained types.,5 Evaluation,[0],[0]
"Especially in challenging cases as in example (c), the model predicts only general types, explaining the low recall numbers (28% of examples belong to this category).",5 Evaluation,[0],[0]
"Even when the model generated correct fine-grained types as in example (d), the recall was often fairly low since it did not generate a complete set of related fine-grained labels.
",5 Evaluation,[0],[0]
Estimating the performance of a model in an incomplete label setting and expanding label coverage are interesting areas for future work.,5 Evaluation,[0],[0]
"Our task also poses a potential modeling challenge; sometimes, the model predicts two incongruous types (e.g. “location” and “person”), which points towards modeling the task as a joint set prediction task, rather than predicting labels individually.",5 Evaluation,[0],[0]
We provide sample outputs on the project website.,5 Evaluation,[0],[0]
We show that our model and distant supervision can improve performance on an existing finegrained NER task.,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"We chose the widely-used OntoNotes (Gillick et al., 2014) dataset which includes nominal and named entity mentions.6
6While we were inspired by FIGER (Ling and Weld, 2012), the dataset presents technical difficulties.",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"The test set has only 600 examples, and the development set was labeled with distant supervision, not manual annotation.",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"We therefore focus our evaluation on OntoNotes.
",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
Augmenting the Training Data,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
The original OntoNotes training set (ONTO in Tables 6 and 7) is extracted by linking entities to a KB.,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
We supplement this dataset with our two new sources of distant supervision: Wikipedia definition sentences (WIKI) and head word supervision (HEAD) (see Section 3).,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"To convert the label space, we manually map a single noun from our natural-language vocabulary to each formal-language type in the OntoNotes ontology.",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"77% of OntoNote’s types directly correspond to suitable noun labels (e.g. “doctor” to “/person/doctor”), whereas the other cases were mapped with minimal manual effort (e.g. “musician” to “person/artist/music”, “politician” to “/person/political figure”).",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
We then expand these labels according to the ontology to include their hypernyms (“/person/political figure” will also generate “/person”).,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"Lastly, we create negative examples by assigning the “/other” label to examples that are not mapped to the ontology.",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"The augmented dataset contains 2.5M/0.6M new positive/negative examples, of which 0.9M/0.1M are from Wikipedia definition sentences and 1.6M/0.5M from head words.
",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"Experiment Setup We compare performance to other published results and to our reimplementation of AttentiveNER (Shimaoka et al., 2017).",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
We also compare models trained with different sources of supervision.,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"For this dataset, we did not use our multitask objective (Section 4), since expanding types to include their ontological hypernyms largely eliminates the partial supervision as-
sumption.",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"Following prior work, we report macroand micro-averaged F1 score, as well as accuracy (exact set match).
",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
Results Table 6 shows the overall performance on the test set.,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"Our combination of model and training data shows a clear improvement from prior work, setting a new state-of-the art result.7
In Table 7, we show an ablation study.",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
Our new supervision sources improve the performance of both the AttentiveNER model and our own.,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
We observe that every supervision source improves performance in its own right.,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"Particularly, the naturally-occurring head-word supervision seems to be the prime source of improvement, increasing performance by about 10% across all metrics.
",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"Predicting Miscellaneous Types While analyzing the data, we observed that over half of the mentions in OntoNotes’ development set were annotated only with the miscellaneous type (“/other”).",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"For both models in our evaluation, detecting the miscellaneous category is substantially easier than
7We did not compare to a system from (Yogatama et al., 2015), which reports slightly higher test number (72.98 micro F1) as they used a different, unreleased test set.
",6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
producing real types (94% F1 vs. 58% F1 with our best model).,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
We provide further details of this analysis in the supplementary material.,6 Improving Existing Fine-Grained NER with Better Distant Supervision,[0],[0]
"Fine-grained NER has received growing attention, and is used in many applications (Gupta et al., 2017; Ren et al., 2017; Yaghoobzadeh et al., 2017b; Raiman and Raiman, 2018).",7 Related Work,[0],[0]
"Researchers studied typing in varied contexts, including mentions in specific sentences (as we consider) (Ling and Weld, 2012; Gillick et al., 2014; Yogatama et al., 2015; Dong et al., 2015; Schutze et al., 2017), corpus-level prediction (Yaghoobzadeh and Schütze, 2016), and lexicon level (given only a noun phrase with no context) (Yao et al., 2013).
",7 Related Work,[0],[0]
"Recent work introduced fine-grained type ontologies (Rabinovich and Klein, 2017; Murty et al., 2017; Corro et al., 2015), defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K).",7 Related Work,[0],[0]
"However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision.",7 Related Work,[0],[0]
"In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels.
",7 Related Work,[0],[0]
"Contextualized fine-grained entity typing is related to selectional preference (Resnik, 1996; Pantel et al., 2007; Zapirain et al., 2013; de Cruys, 2014), where the goal is to induce semantic generalizations on the type of arguments a predicate prefers.",7 Related Work,[0],[0]
"Rather than focusing on predicates, we condition on the entire sentence to deduce the arguments’ types, which allows us to capture more nuanced types.",7 Related Work,[0],[0]
"For example, not every type that fits “He played the violin in his room” is also suitable for “He played the violin in the Carnegie Hall”.",7 Related Work,[0],[0]
"Entity typing here can be connected to argument finding in semantic role labeling.
",7 Related Work,[0],[0]
"To deal with noisy distant supervision for KB population and entity typing, researchers used multi-instance multi-label learning (Surdeanu et al., 2012; Yaghoobzadeh et al., 2017b) or custom losses (Abhishek et al., 2017; Ren et al., 2016a).",7 Related Work,[0],[0]
Our multitask objective handles noisy supervision by pooling different distant supervision sources across different levels of granularity.,7 Related Work,[0],[0]
Using virtually unrestricted types allows us to expand the standard KB-based training methodology with typing information from Wikipedia definitions and naturally-occurring head-word supervision.,8 Conclusion,[0],[0]
These new forms of distant supervision boost performance on our new dataset as well as on an existing fine-grained entity typing benchmark.,8 Conclusion,[0],[0]
"These results set the first performance levels for our evaluation dataset, and suggest that the data will support significant future work.",8 Conclusion,[0],[0]
The research was supported in part the ARO (W911NF-16-1-0121),Acknowledgement,[0],[0]
"the NSF (IIS-1252835, IIS1562364), and an Allen Distinguished Investigator Award.",Acknowledgement,[0],[0]
We would like to thank the reviewers for constructive feedback.,Acknowledgement,[0],[0]
Also thanks to Yotam Eshel and Noam Cohen for providing the Wikilink dataset.,Acknowledgement,[0],[0]
Special thanks to the members of UW NLP for helpful discussions and feedback.,Acknowledgement,[0],[0]
"We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity.",abstractText,[0],[0]
"This formulation allows us to use a new type of distant supervision at large scale: head words, which indicate the type of the noun phrases they appear in.",abstractText,[0],[0]
"We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks.",abstractText,[0],[0]
"We present a model that can predict open types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking.",abstractText,[0],[0]
"Experimental results demonstrate that our model is effective in predicting entity types at varying granularity; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.1",abstractText,[0],[0]
Ultra-Fine Entity Typing,title,[0],[0]
Data-driven decision-making has become the subject of increased interest and been used in a number of practical applications.,1. Introduction,[0],[0]
One of the most promising approaches is mathematical programming based on predictive models generated by machine learning.,1. Introduction,[0],[0]
"Recent advances in machine learning have made it easier to create accurate predictive models, and resulting predictions have been used to build mathematical programming problems (we refer to such approaches as predictive optimization).",1. Introduction,[0],[0]
"Predictive optimization is employed in applications for which frequent trial-and-error process are not practical, such as water distribution optimization (Draper et al., 2003), energy generation planning (Baos et al., 2011), retail price optimization (Johnson
1NEC Corporation.",1. Introduction,[0],[0]
"Correspondence to: Shinji Ito <sito@me.jp.nec.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"et al., 2016; Ito & Fujimaki, 2016), supply chain management (Thomas et al., 1996; Jung et al., 2004; Bertsimas & Thiele, 2004), and portfolio optimization (Markowitz, 1952; Chan et al., 1999; Konno & Yamazaki, 1991).",1. Introduction,[0],[0]
"Another important use for data-driven decision-making is in reinforcement learning (Kaelbling et al., 1996; Sutton & Barto, 2013).",1. Introduction,[0],[0]
"Here it is employed in situations mainly in which frequent trial-and-error operations are possible, except for batch reinforcement learning (Lange et al., 2012).",1. Introduction,[0],[0]
"The focus of this paper is on the first approach, i.e., predictive optimization.
",1. Introduction,[0],[0]
"In many practical applications of predictive optimization, it is essential to estimate the quality of the computed strategy because executing a strategy is often costly and risky.",1. Introduction,[0],[0]
"For example, predictive price optimization has been used to estimate revenue functions through regressions of demand as functions of product prices, and then, to optimize pricing strategies by maximizing estimated revenue functions (Johnson et al., 2016; Ito & Fujimaki, 2016; 2017; Yabe et al., 2017).",1. Introduction,[0],[0]
"In practice, users need to assess the return for the computed “optimal” strategy before changing prices, in order to prevent unforeseen heavy losses.",1. Introduction,[0],[0]
"In a situation in which costs for trial-and-error processes are unrealistically high, a key challenge in predictive optimization is how to assess the quality (or expected return) of the “optimal” solution by means of an estimated objective function.
",1. Introduction,[0],[0]
Predictive optimization consists of two steps: estimation and optimization.,1. Introduction,[0],[0]
"In the estimation step, we construct an estimated objective function f(z, θ̂) for the true objective function f(z, θ∗), where θ is a parameter of f , and z is a decision variable corresponding to the strategy to be optimized.",1. Introduction,[0],[0]
"In the optimization step, we compute the estimated optimal strategy ẑ = arg maxz∈Z f(z, θ̂), where Z is the domain of z. Because it would be expensive to observe f(ẑ, θ∗) (i.e., to perform ẑ in a real environment), we usually estimate it by f(ẑ, θ̂), which we call simple evaluation, in order to assess the quality of ẑ.
It has been empirically seen, however, that this simple evaluation tends to be too optimistic.",1. Introduction,[0],[0]
"For example, in the contexts of algorithmic investment and portfolio optimization, it has been reported (Michaud, 1989; Chapados, 2011; Harvey & Liu, 2015) that f(ẑ, θ̂) is much better than the acutual return.",1. Introduction,[0],[0]
"Michaud (Michaud, 1989) argued that this bias ap-
pears because the mean-variance optimizers act as “error maximizers”, i.e., optimizers tend to choose solutions containing large errors.",1. Introduction,[0],[0]
"According to (Harvey & Liu, 2015), a common practice in evaluating trading strategies is simple heuristics that discount the estimated objective to 50%, i.e., consider 0.5f(ẑ, θ̂) to be an estimator of f(ẑ, θ∗).",1. Introduction,[0],[0]
"Heuristics referred to as portfolio resampling techniques (Michaud, 1998; Scherer, 2002) have been studied for nearly 20 years but have not yet to be theoretically justified.",1. Introduction,[0],[0]
"A few recent studies (Bailey & Marcos, 2016; Bailey et al., 2014; Harvey & Liu, 2015) have statistically analyzed and proposed algorithms to mitigate the bias issue, but their algorithms are restricted to particular applications (e.g., algorithmic investment) and, as far as we know, there exists no principled algorithm for an unbiased estimator of f(z, θ∗) in general predictive optimization problems.
",1. Introduction,[0],[0]
"The goal of this study is to address this optimistic bias issue, and to propose methods for unbiased estimation of true objective values.",1. Introduction,[0],[0]
"Our key contributions are summarized as follows.
",1. Introduction,[0],[0]
"First, we prove that the estimated optimal value f(ẑ, θ̂) is biased even if the estimated objective function f(z, θ̂) is an unbiased estimator of the true objective function f(z, θ∗).",1. Introduction,[0],[0]
"Further, we correlate the bias issue to overfitting in machine learning, which yields a valuable insight into bias correction methods.
",1. Introduction,[0],[0]
"Second, we propose two algorithms for estimating the value of true objective functions under mild assumptions.",1. Introduction,[0],[0]
The first algorithm is based on a procedure similar to crossvalidation and has been inspired by the analogy between our problem and overfitting in supervised learning.,1. Introduction,[0],[0]
"This algorithm corrects the optimistic bias, but suffers from pessimistic bias, i.e., the estimated value is biased in a direction suggesting a poorer result, similar to that which occurs in cross-validation.",1. Introduction,[0],[0]
"The magnitude of this pessimistic bias tends to be larger than that of cross-validation, and hence, it is not negligible in many cases.",1. Introduction,[0],[0]
"To mitigate this issue, we propose another algorithm, which we refer to as a parameter perturbation method.",1. Introduction,[0],[0]
"This algorithm employs a resampling technique and is theoretically proven here to achieve asymptotically unbiased estimation.
",1. Introduction,[0],[0]
"Our experimental results show that the proposed algorithms are able to estimate the value of a true objective function more accurately than a state-of-the-art hold-out validation technique commonly used in algorithmic investment (Bailey & Marcos, 2016; Bailey et al., 2014).",1. Introduction,[0],[0]
"In a simulation experiment with real-world retail datasets for price optimization, we have observed that our evaluation algorithms estimate a 17% increase in the gross profit, which seems to be more realistic and convincing than the value estimated without bias correction.
",1. Introduction,[0],[0]
The remainder of this paper is structured as follows.,1. Introduction,[0],[0]
"In Section 2, we introduce the framework of the combination of machine learning and mathematical optimization in examples of usage.",1. Introduction,[0],[0]
We also show that such a framework suffers from bias w.r.t.,1. Introduction,[0],[0]
optimal values.,1. Introduction,[0],[0]
Section 4 gives solutions to this problem and theoretical guarantees for them.,1. Introduction,[0],[0]
"In Section 5, the empirical performance of our algorithms is demonstrated.",1. Introduction,[0],[0]
"Suppose we have a sequence of training data x = (x1, . . .",2. Predictive Optimization,[0],[0]
", xN ) ∈ XN , where N is the number of data instances.",2. Predictive Optimization,[0],[0]
Each xn is generated from a probabilistic model {p(x|θ) : θ ∈ Θ} parameterized by θ ∈ Θ.,2. Predictive Optimization,[0],[0]
"We further suppose having a set of objective functions {f(z, θ) : θ ∈ Θ} where z ∈ Z is a decision variable that corresponds to strategies to be optimized.",2. Predictive Optimization,[0],[0]
"The goal of predictive optimization is to find z∗ ∈ arg maxz∈Z f(z, θ∗), where θ∗ is the true parameter.",2. Predictive Optimization,[0],[0]
"However, such a true parameter is unknown in practice, and therefore we estimate θ∗ by θ̂ from x, and compute the estimated optimal solution ẑ ∈ arg maxz∈Z f(z, θ̂) rather than z∗.",2. Predictive Optimization,[0],[0]
"This section discusses three examples of predictive optimization problems in order to provide a better picture of the process.
",2. Predictive Optimization,[0],[0]
Example 1 (Coin-Tossing).,2. Predictive Optimization,[0],[0]
"Suppose that we have a coin coming up heads with probability θ∗ and tails with probability 1− θ∗, where",2. Predictive Optimization,[0],[0]
"θ∗ ∈ Θ := [0, 1].",2. Predictive Optimization,[0],[0]
Consider predicting heads or tails for this coin.,2. Predictive Optimization,[0],[0]
"If we predict the subsequent face correctly, we win $1, and, otherwise, nothing.",2. Predictive Optimization,[0],[0]
"Predicting heads, then, will result in earning $1 with probability θ∗ and $0 with probability 1 − θ∗, and hence, the expectation value of the earnings for predicting heads is f(‘head’, θ∗) = 1 · θ∗+ 0 · (1− θ∗) = θ∗.",2. Predictive Optimization,[0],[0]
"Similarly, the expected earnings for predicting tails is f(‘tail’, θ∗) = 1− θ∗.",2. Predictive Optimization,[0],[0]
"If we knew the true parameter θ∗, we could maximize the expected earnings by choosing z∗ ∈ arg maxz∈Z f(z, θ∗), where Z = {‘head’, ‘tail’} stands for a set of feasible strategies.",2. Predictive Optimization,[0],[0]
"Since we do not know the true parameter θ∗, however, we use, rather, past data x ∈",2. Predictive Optimization,[0],[0]
"XN := {‘head’, ‘tail’}N of N tossings, for estimating θ∗.
Table 1 illustrates how the optimistic bias occurs in predictive optimization.",2. Predictive Optimization,[0],[0]
Suppose θ∗ = 1/2 (a) and that there are four cases of the observed pattern for three tossings (b).,2. Predictive Optimization,[0],[0]
"The estimators of θ∗ might then be obtained as (c), using maximum likelihood estimation.",2. Predictive Optimization,[0],[0]
"On the basis of θ̂, the “best” strategies are estimated as (d), and the estimated and true optimal values are summarized in (e) and (f).",2. Predictive Optimization,[0],[0]
"It is worth noting that the expectation of (e) over four cases (bottom middle), which is 3/4, is larger than the true expectation (bottom right), which is 1/2 even if the θ̂ is unbiased, i.e., the expectation of θ̂ matches θ∗ (bottom left).
",2. Predictive Optimization,[0],[0]
"Example 2 (Portfolio optimization (Markowitz, 1952)).
",2. Predictive Optimization,[0],[0]
"Suppose that there are d assets, and let Rj stand for the return on each component asset for j ∈ {1, . . .",2. Predictive Optimization,[0],[0]
", d}.",2. Predictive Optimization,[0],[0]
"Let µ∗ = (µ∗1, . . .",2. Predictive Optimization,[0],[0]
", µ ∗ d)",2. Predictive Optimization,[0],[0]
"> ∈ Rd be the expected return for each asset, i.e., µ∗j = E[Rj ].",2. Predictive Optimization,[0],[0]
"Then the portfolio expressed as Rz = ∑d j=1 zjRj , where zj ≥ 0 is the weighting of the j-th component asset and z = (z1, . . .",2. Predictive Optimization,[0],[0]
", zd)> ∈ Rd≥0, has expected return E[Rz] = ∑d j=1 zjµ ∗",2. Predictive Optimization,[0],[0]
"j = µ
∗>z.",2. Predictive Optimization,[0],[0]
"Variance in the portfolio return can be expressed as var[Rz] = z>Σ∗z, where Σ∗ is the covariance matrix of (R1, . . .",2. Predictive Optimization,[0],[0]
", Rd).",2. Predictive Optimization,[0],[0]
"Denote θ∗ = (µ∗,Σ∗).",2. Predictive Optimization,[0],[0]
"Then, with a given risk tolerance λ ≥ 0, the optimal portfolio is obtained as the solution of the following problem:
Maximize f(z, θ∗) := µ∗>z",2. Predictive Optimization,[0],[0]
"− λz>Σ∗z, (1) subject to d∑ j=1",2. Predictive Optimization,[0],[0]
"zj = 1, zj ≥ 0",2. Predictive Optimization,[0],[0]
"(j = 1, . . .",2. Predictive Optimization,[0],[0]
", d).
",2. Predictive Optimization,[0],[0]
"In practice, however, since θ∗ is never available, we estimate it from historical data x = (x1, . . .",2. Predictive Optimization,[0],[0]
", xN ), where xn ∈ Rd is an observation of past returns for individual component assets (Qiu et al., 2015; Agarwal et al., 2006; Li & Hoi, 2012).",2. Predictive Optimization,[0],[0]
"Under the assumption that xn follow the same distribution,1 the estimators of µ∗ and Σ∗ are obtained by µ̂ = 1N ∑N n=1 xn and Σ̂ = 1 N−1 ∑N n=1(xn− µ̂)(xn− µ̂)>.",2. Predictive Optimization,[0],[0]
"We obtain the optimal solution by solving (1) with the replacement of µ∗ and Σ∗ by µ̂ and Σ̂, respectively.
",2. Predictive Optimization,[0],[0]
"Example 3 (Predictive price optimization(Ito & Fujimaki, 2017; 2016)).",2. Predictive Optimization,[0],[0]
"Suppose we have d products whose prices are denoted by z = (z1, . . .",2. Predictive Optimization,[0],[0]
", zd).",2. Predictive Optimization,[0],[0]
Let us denote their sales quantities by q∗(z) =,2. Predictive Optimization,[0],[0]
"(q∗j (z)) d j=1 ∈ Rd, which are functions of the price z.",2. Predictive Optimization,[0],[0]
"The gross revenue function is then defined by f(z, θ∗) = q∗(z)>z, and the true optimal solution is obtained by solving the following problem:
Maximize q∗(z)>z subject to z ∈ Z, (2)
where Z ⊆ Rd is a pre-defined domain of prices (e.g., list price, 3%-off, 5%-off, and so on).",2. Predictive Optimization,[0],[0]
"However, we can never know the true demand-price relationship q∗(z), and
1This condition can easily be relaxed.
",2. Predictive Optimization,[0],[0]
"the predictive price optimization approximates q∗(z) by the following regression functions:
q(z, θ) = K∑ k=1 θkψk(z) + , ∼ N(0,Σ), (3)
where {ψk : Rd → Rd}Kk=1 are fixed basis functions and {θk}Kk=1 ⊆ R are regression coefficients.",2. Predictive Optimization,[0],[0]
"We estimate θ = (θ1, . . .",2. Predictive Optimization,[0],[0]
", θK) as a standard regression problem and then solve (2) after replacing q∗(z) by q(z, θ̂), where θ̂ is the estimator of θ∗.",2. Predictive Optimization,[0],[0]
This section formally proves the existence of optimistic bias in estimated optimal values.,3.1. Existence of Optimistic Bias,[0],[0]
"In the above examples, the objective functions f(z, θ) w.r.t.",3.1. Existence of Optimistic Bias,[0],[0]
θ were affine functions and θ̂ were unbiased estimators of θ∗.,3.1. Existence of Optimistic Bias,[0],[0]
"Hence, the constructed objective function f(z, θ̂) was an unbiased estimator of the true objective function f(z, θ∗), i.e., it holds that
Ex[f(z, θ̂)]",3.1. Existence of Optimistic Bias,[0],[0]
"= Ex[f(z, θ∗)], z ∈ Z. (4)
From this equation, one might expect that Ex[f(ẑ, θ̂)] and f(ẑ, θ̂) would be reasonable estimators of Ex[f(ẑ, θ∗)] and f(ẑ, θ∗), respectively.",3.1. Existence of Optimistic Bias,[0],[0]
"However, the following proposition contradicts this intuition.
",3.1. Existence of Optimistic Bias,[0],[0]
Proposition 1 (Optimistic Bias).,3.1. Existence of Optimistic Bias,[0],[0]
Suppose (4) holds.,3.1. Existence of Optimistic Bias,[0],[0]
"For ẑ ∈ arg maxz∈Z f(z, θ̂) and z∗ ∈ arg maxz∈Z f(z, θ∗), it holds that
Ex[f(ẑ, θ̂)]",3.1. Existence of Optimistic Bias,[0],[0]
"≥ f(z∗, θ∗) ≥",3.1. Existence of Optimistic Bias,[0],[0]
"Ex[f(ẑ, θ∗)].",3.1. Existence of Optimistic Bias,[0],[0]
"(5)
",3.1. Existence of Optimistic Bias,[0],[0]
The right inequality is strict if ẑ is suboptimal w.r.t.,3.1. Existence of Optimistic Bias,[0],[0]
"the true objective function f(z, θ∗) with non-zero probability.
",3.1. Existence of Optimistic Bias,[0],[0]
Proof.,3.1. Existence of Optimistic Bias,[0],[0]
"By taking the expectation of both sides of f(ẑ, θ̂) ≥ f(z∗, θ̂), we obtain the left inequality of (5) as follows:
Ex[f(ẑ, θ̂)]",3.1. Existence of Optimistic Bias,[0],[0]
"≥ Ex[f(z∗, θ̂)]",3.1. Existence of Optimistic Bias,[0],[0]
"= f(z∗, θ∗),
where the equality comes from (4).",3.1. Existence of Optimistic Bias,[0],[0]
"Similarly, the right inequality of (5) comes from f(z∗, θ∗) ≥ f(ẑ, θ∗).",3.1. Existence of Optimistic Bias,[0],[0]
"Further, if ẑ /∈ arg maxz∈Z f(z, θ∗) holds with non-zero probability, then f(z∗, θ∗) > f(ẑ, θ∗) holds with non-zero probability and f(z∗, θ∗) ≥ f(ẑ, θ∗) always holds, which implies f(z∗, θ∗) >",3.1. Existence of Optimistic Bias,[0],[0]
"E[f(ẑ, θ∗)].
",3.1. Existence of Optimistic Bias,[0],[0]
"This proposition implies that the estimated optimal value f(ẑ, θ̂) is not an unbiased estimator of f(ẑ, θ∗) even if the estimated objective function f(z, θ̂) is an unbiased estimator of the true objective function f(z, θ∗).",3.1. Existence of Optimistic Bias,[0],[0]
"This optimistic bias
has been empirically learned in the context of portfolio optimization (Michaud, 1989).",3.1. Existence of Optimistic Bias,[0],[0]
"Recently, (Harvey & Liu, 2015; Harvey et al., 2016) have proposed bias correction methods based on statistical tests, though their methods are applicable only to cases in which the objective function is the Sharpe ratio.",3.1. Existence of Optimistic Bias,[0],[0]
"Other recent studies (Bailey & Marcos, 2016; Bailey et al., 2014) have also focused on the Sharpe ratio and proposed a hold-out validation method.",3.1. Existence of Optimistic Bias,[0],[0]
"Although their methods apply to general predictive optimization problems, they have not been proven to obtain unbiased estimators.",3.1. Existence of Optimistic Bias,[0],[0]
"Note that a similar inequality has been discovered in the context of stochastic programs,2 one that corresponds to the left inequality of (5).",3.1. Existence of Optimistic Bias,[0],[0]
"For the special case in which Z is a finite set, the same inequality as (5) has been shown in the context of decision analysis (Smith & Winkler, 2006).",3.1. Existence of Optimistic Bias,[0],[0]
"This subsection discusses the connection of the optimistic bias issue to overfitting in machine learning, which connection has led to the ideas underlying our proposed algorithms.",3.2. Connection to Empirical Risk Minimization,[0],[0]
"In supervised machine learning, we choose the prediction rule ĥ from a hypothesis space H by minimizing the empirical error, i.e., we let ĥ ∈ arg minh∈H 1 n ∑N n=1 `(h, xn), where xn is the observed data generated from a distribution D and ` is a loss function.",3.2. Connection to Empirical Risk Minimization,[0],[0]
The empirical error 1N ∑N n=1,3.2. Connection to Empirical Risk Minimization,[0],[0]
"`(h, xn) is an unbiased estimator of the generalization error `D(h) := Ex∼D[`(h, x)] for arbitrary fixed prediction rule h, i.e., it holds that Exn∼D[ 1N ∑N n=1 `(h, xn)] =",3.2. Connection to Empirical Risk Minimization,[0],[0]
"`D(h) for any fixed h. De-
spite this equation, the empirical error 1N ∑N n=1 `(ĥ, xn) for the computed parameter ĥ is smaller than the generalization error `D(ĥ) in most cases, because ĥ overfits the observed samples, as is well known (Vapnik, 2013).",3.2. Connection to Empirical Risk Minimization,[0],[0]
"The analogy between the optimistic bias in our setting and the overfitting issue in machine learning suggests the reuse of datasets for estimation of their objective functions and evaluation of objective values.
",3.2. Connection to Empirical Risk Minimization,[0],[0]
A comparison between empirical risk minimization (ERM) and our prediction-based optimization is summarized in Table 2.,3.2. Connection to Empirical Risk Minimization,[0],[0]
"As is shown in the Table, our problem concerning bias in predictive optimization has a structure similar to that of the problem of overfitting in empirical risk minimization.",3.2. Connection to Empirical Risk Minimization,[0],[0]
"Typical methods for estimating generalization error in machine learning would be cross-validation and such asymptotic bias correction as AIC (Akaike, 1973).",3.2. Connection to Empirical Risk Minimization,[0],[0]
"This paper follows the concept of cross-validation in the context of predictive optimization and, in the following section, proposes a more accurate algorithm.
2",3.2. Connection to Empirical Risk Minimization,[0],[0]
"In stochastic programs, the objective is a random function, and it has been shown in, e.g., (Mak et al., 1999), that the expectation of the minimum of the objective is a lower bound of the minimum of the expectation of the objective.",3.2. Connection to Empirical Risk Minimization,[0],[0]
"Our goal is to construct unbiased estimators for the value f(ẑ, θ∗) of the true objective function, i.e., to construct ρ : Xn → R such that Ex[ρ(x)]",4. Bias Correction Algorithms,[0],[0]
"= Ex[f(ẑ, θ∗)], where ẑ ∈ arg max
z∈Z f(z, θ̂) is the computed strategy.",4. Bias Correction Algorithms,[0],[0]
"We assume
the following conditions.
",4. Bias Correction Algorithms,[0],[0]
Assumption 2.,4. Bias Correction Algorithms,[0],[0]
"(i) f(z, θ) is affine in θ, i.e., ∃a : Z → R, ∃b : Z → R, f(z, θ) =",4. Bias Correction Algorithms,[0],[0]
"θ>a(z) + b(z).
",4. Bias Correction Algorithms,[0],[0]
"(ii) The optimal solution z(θ) ∈ arg maxz∈Z f(z, θ) is uniquely determined for almost all θ.
(iii)",4. Bias Correction Algorithms,[0],[0]
"One of the following holds: (iii.a) Z is a finite set, or (iii.b) Z is a compact subset of Rd, and z 7→ (a(z), b(z)) is a continuous injective function.
(iv) θ̂ is an unbiased estimator of θ∗, i.e., we have Ex[θ̂] = θ∗.
The assumptions (i)-(iii) are conditions on mathematical programming problems, and such typical ones as (mixedinteger) linear/quadratic/semidefinite programming problems satisfy these conditions.",4. Bias Correction Algorithms,[0],[0]
"Assumption (iv) is a condition on the machine learning algorithm for estimating the objective function in the optimization problem, and we can employ any standard unbiased estimation algorithm.",4. Bias Correction Algorithms,[0],[0]
Note that the examples in Section 3 satisfy all these assumptions.,4. Bias Correction Algorithms,[0],[0]
"We assume (i) and (iv) in Section 4.1, and assume (i)-(iv) in Section 4.2.",4. Bias Correction Algorithms,[0],[0]
"As noted in Section 3.2, our problem is closely related to the problem of estimating generalization error.",4.1. Cross-Validation Method,[0],[0]
"Inspired by the cross-validation method, one of the most popular methods for estimating generalization error in machine learning, we propose a cross-validation method for estimating the value of the true objective function in predictive optimization.",4.1. Cross-Validation Method,[0],[0]
"In the context of algorithmic investment, a similar method, referred to as the hold-out method is mentioned in (Bailey et al., 2014).",4.1. Cross-Validation Method,[0],[0]
"The method discussed below is essentially an extension of the hold-out method for general predictive optimization problems.
",4.1. Cross-Validation Method,[0],[0]
"One of the reasons that the value f(ẑ, θ̂) contains biases is that ẑ and θ̂ are dependent random variables.",4.1. Cross-Validation Method,[0],[0]
"Indeed,
Algorithm 1 k-fold cross-validation Input: data x ∈ XN , the number K ≥ 2 of partition Divide data x into K parts x1, . . . ,xK .",4.1. Cross-Validation Method,[0],[0]
"for k = 1 to K do
Compute θ̂k, θ̃k from xk,x−k respectively, where we define x−k to be all samples in x except for xk, and compute z̃k ∈ arg maxz∈Z f(z, θ̃k).
end for Output ρCV (x) := 1K ∑K k=1 f(z̃k, θ̂k).
if ẑ and θ̂ are independent, Ex[f(ẑ, θ̂)]",4.1. Cross-Validation Method,[0],[0]
"= Ex[f(ẑ, θ∗)] straightforwardly holds from assumptions (i) and (iv).",4.1. Cross-Validation Method,[0],[0]
The main idea of the cross-validation method (as with the standard cross-validation in machine learning) is to divide the data x ∈ XN into two parts x1 ∈,4.1. Cross-Validation Method,[0],[0]
"XN1 ,x2 ∈",4.1. Cross-Validation Method,[0],[0]
"XN2 , where N1 + N2 = N .",4.1. Cross-Validation Method,[0],[0]
"Note that each element in x1 and x2 follows p(x, θ∗) independently, and, hence, x1 and x2 are independent random variables.",4.1. Cross-Validation Method,[0],[0]
"Let us denote the estimators based on x1 and x2 by θ̂1 and θ̂2, respectively.",4.1. Cross-Validation Method,[0],[0]
"Also, the optimal strategy on each estimator is denoted by ẑi := arg maxz∈Z f(z, θ̂i) for i = 1, 2.",4.1. Cross-Validation Method,[0],[0]
"Then ẑ1 and θ̂2 are independent (the opposite also holds), and we have Ex[f(ẑ1, θ̂2)]",4.1. Cross-Validation Method,[0],[0]
"= Ex1 [f(ẑ1,Ex2 [θ̂2])]",4.1. Cross-Validation Method,[0],[0]
"= Ex1 [f(ẑ1, θ∗)].",4.1. Cross-Validation Method,[0],[0]
"Further, if N1 is sufficiently close to N , Ex1 [f(z̃1, θ∗)] is close to Ex[f(ẑ, θ∗)].",4.1. Cross-Validation Method,[0],[0]
"This idea can be extended to k-fold cross-validation, in which we divide data x ∈ RN into K parts x1, . . .",4.1. Cross-Validation Method,[0],[0]
",xK ∈ RN ′",4.1. Cross-Validation Method,[0],[0]
", where KN ′ = N .",4.1. Cross-Validation Method,[0],[0]
"We compute z̃k from {x1, . . .",4.1. Cross-Validation Method,[0],[0]
",xK} \ {xk}, and compute θ̂k from xk.",4.1. Cross-Validation Method,[0],[0]
"Then the value ρCV (x) := 1K ∑K k=1 f(z̃k, θ̂k) satisfies
Ex[ρCV (x)]",4.1. Cross-Validation Method,[0],[0]
"= Ex′ [f(z̃, θ∗)], (6)
where z̃ stands for the strategy computed from (K − 1)N ′ samples, under assumptions (i) and (iv).
",4.1. Cross-Validation Method,[0],[0]
"A major drawback to Algorithm 1 is that it can only estimate the objective value attained byN −N ′ samples, as is shown in (6), even though the value attained by all N samples is desired.",4.1. Cross-Validation Method,[0],[0]
"In machine learning, to mitigate this gap, a leave-one-out method (i.e., setting N ′",4.1. Cross-Validation Method,[0],[0]
= 1) can be used.,4.1. Cross-Validation Method,[0],[0]
"In predictive optimization, however, the number N ′ of holdout samples needs to be large enough to compute another estimator, θ̂k, which limits the accuracy of the estimation of f(ẑ, θ∗).",4.1. Cross-Validation Method,[0],[0]
The accuracy of Algorithm 1 is considered in Sec. 5 in an empirical evaluation.,4.1. Cross-Validation Method,[0],[0]
This subsection proposes another algorithm that addresses the drawbacks of Algorithm 1.,4.2. Parameter perturbation method,[0],[0]
Denote the error in the estimated parameter by δ := θ̂−θ∗.,4.2. Parameter perturbation method,[0],[0]
The error δ depends on the training data x and can be regarded as a random variable when x is considered to be a random variable.,4.2. Parameter perturbation method,[0],[0]
"For γ ≥ 0,
let us first define η(γ) as follows:
η(γ) = Eδ[f(z(θ∗ + γδ), θ∗)],
where z(θ) := arg maxz∈Z f(z, θ).",4.2. Parameter perturbation method,[0],[0]
"Since ẑ = z(θ̂) = z(θ∗ + δ), we have η(1)",4.2. Parameter perturbation method,[0],[0]
"= E[f(ẑ, θ∗)].",4.2. Parameter perturbation method,[0],[0]
"Hence, our goal, unbiased estimation of f(ẑ, θ∗), is equivalent to unbiased estimation of η(1).",4.2. Parameter perturbation method,[0],[0]
"Let us next define φ(γ) as follows:
φ(γ) = Eδ[f(z(θ∗ + γδ), θ∗ + γδ)].",4.2. Parameter perturbation method,[0],[0]
"(7)
Note that we have φ(1) = E[f(ẑ, θ̂)].",4.2. Parameter perturbation method,[0],[0]
"Further, φ(γ) and η(γ) satisfy φ(0) = η(0)",4.2. Parameter perturbation method,[0],[0]
"= f(z∗, θ∗) and φ(γ) ≥ f(z∗, θ∗) ≥ η(γ) for all γ ≥ 0, which can be proved in a way similar to that of the proof of Proposition 1.
",4.2. Parameter perturbation method,[0],[0]
The following proposition plays a key role in our second algorithm.,4.2. Parameter perturbation method,[0],[0]
Proposition 3.,4.2. Parameter perturbation method,[0],[0]
Suppose that assumptions (i)-(iv) hold.,4.2. Parameter perturbation method,[0],[0]
"For all γ > 0, φ(γ) is differentiable, and its derivative φ′(γ) satisfies
η(γ) = φ(γ)− γφ′(γ).",4.2. Parameter perturbation method,[0],[0]
"(8)
The proof of this proposition is summarized in the supplementary material.
",4.2. Parameter perturbation method,[0],[0]
"Let us explain this proposition using Figure 1, which is based on the simulation experiment for portfolio optimization used in Section 5 and shows how the values of φ and η behave for some γ ≥ 0.",4.2. Parameter perturbation method,[0],[0]
"The tangent to φ(γ) at γ = γ0 (the blue broken-line) has a y-intercept (the red broken-line) equal to the value of η(γ0), for all γ0 > 0.",4.2. Parameter perturbation method,[0],[0]
"From this relationship, the derivative φ′(1) of φ(γ) at γ = 1 satisfies φ′(1) = φ(1)",4.2. Parameter perturbation method,[0],[0]
"− η(1) = E[f(ẑ, θ̂)]",4.2. Parameter perturbation method,[0],[0]
"− E[f(ẑ, θ∗)], i.e., the value of φ′(1) is equal to the value of the bias in our predictive optimization problem.
",4.2. Parameter perturbation method,[0],[0]
"Our problem is now to obtain an unbiased estimator ζ of φ′(1) that will give us an unbiased estimator of f(ẑ, θ∗), i.e. ρ = f(ẑ, θ̂)− ζ.",4.2. Parameter perturbation method,[0],[0]
"From the definition of the derivative, the value of φ′(1) can be approximated by (φ(1+h)−φ(1))/h for small h. Further, from the definition of φ, the estimated optimal value f(ẑ, θ̂) is an unbiased estimator of φ(1).",4.2. Parameter perturbation method,[0],[0]
"Also, the value of φ(1 + h) = E[maxz∈Z f(z, θ∗ + (1 + h)δ)] is the expectation of the optimal value for the objective function with a parameter having an “enhanced” error.",4.2. Parameter perturbation method,[0],[0]
"If we get samples θ̂h following the distribution of θ∗ + (1 + h)δ, we can develop an estimator of φ(1 + h), and accordingly, we can estimate η(1)",4.2. Parameter perturbation method,[0],[0]
"= E[f(ẑ, θ∗)].
",4.2. Parameter perturbation method,[0],[0]
"Suppose that θ̂(1)h , . . .",4.2. Parameter perturbation method,[0],[0]
", θ̂ (s) h follows the distribution of θ ∗ + (1 + h)δ, and define
ρh := 1 + h
h max z∈Z f(z, θ̂)− 1 hs s∑ j=1 max z∈Z f(z, θ̂ (j) h ).",4.2. Parameter perturbation method,[0],[0]
"(9)
The value ρh, then, has the following property.
",4.2. Parameter perturbation method,[0],[0]
Algorithm 2,4.2. Parameter perturbation method,[0],[0]
Parameter perturbation method Input:,4.2. Parameter perturbation method,[0],[0]
"data x ∈ Xn, parameters h > 0, s ∈ {1, 2, . . .",4.2. Parameter perturbation method,[0],[0]
"} Compute θ̂ from x and set v̂0 = maxz∈Z f(z, θ̂).",4.2. Parameter perturbation method,[0],[0]
"Generate {θ̂(j)h }sj=1 by (i) for asymptotic normal estimators or (ii) for M-estimators.
",4.2. Parameter perturbation method,[0],[0]
(i) Set θ̂(j)h to be the estimator computed from N/(1 +,4.2. Parameter perturbation method,[0],[0]
"h)2 samples randomly chosen from x without replacement.
",4.2. Parameter perturbation method,[0],[0]
"(ii) Generate δ̂j by (10), and set θ̂ (j) h = θ̂ + δ̂j .
for j = 1 to s do Set v̂j = maxz∈Z f(z, θ̂ (j) h ).",4.2. Parameter perturbation method,[0],[0]
"end for Output ρh := 1+hh v̂0 − 1 hs ∑s j=1 v̂j .
",4.2. Parameter perturbation method,[0],[0]
Proposition 4.,4.2. Parameter perturbation method,[0],[0]
"Under assumptions (i)-(iv), the value ρh defined by (9) is an asymptotically unbiased estimator of f(ẑ, θ∗), i.e., it holds that limh→0 E [ρh] = E[f(ẑ, θ∗)].
",4.2. Parameter perturbation method,[0],[0]
Proof.,4.2. Parameter perturbation method,[0],[0]
"From the definition of ρh and φ(γ), we have E[ρh]",4.2. Parameter perturbation method,[0],[0]
= ρ(1),4.2. Parameter perturbation method,[0],[0]
− φ(1+h)−φ(1)h .,4.2. Parameter perturbation method,[0],[0]
"Hence, we have limh→0 E",4.2. Parameter perturbation method,[0],[0]
[ρh] = φ(1),4.2. Parameter perturbation method,[0],[0]
− φ′(1).,4.2. Parameter perturbation method,[0],[0]
"From Proposition 3, this value is equal to η(1)",4.2. Parameter perturbation method,[0],[0]
"= E[f(ẑ, θ∗)].
",4.2. Parameter perturbation method,[0],[0]
"The remaining problem is how to obtain samples θ̂h, with enhanced errors, from the distribution of θ∗+(1+h)δ.",4.2. Parameter perturbation method,[0],[0]
"If θ̂ is an asymptotically normal estimator of θ∗, its distribution can be approximated by the normal distribution N (θ∗, 1NΣ
∗), where Σ∗ is a constant matrix not dependent on N .",4.2. Parameter perturbation method,[0],[0]
"Further, when we compute an estimator θ̂h fromN/(1+h)2 data, the distribution of θ̂h can be approximated byN (θ∗, (1+h) 2 N Σ ∗).",4.2. Parameter perturbation method,[0],[0]
This is an approximation of the distribution of θ∗ + (1 + h)δ.,4.2. Parameter perturbation method,[0],[0]
"This procedure for generating θ̂h is used in (i) of Algorithm 2.
",4.2. Parameter perturbation method,[0],[0]
"If θ̂ is an M-estimator, an asymptotically normal estimator commonly used in machine learning, we can eliminate repetitive computation in (i) of Algorithm 2.",4.2. Parameter perturbation method,[0],[0]
"For M-estimators,
Σ̂ is given in a closed form, as described in (van der Vaart, 1998), such that N (0, 1N Σ̂) approximates the error distribution of the estimator.",4.2. Parameter perturbation method,[0],[0]
"Once we have computed Σ̂, we generate samples from an approximated distribution of θ∗ + (1 + h)δ, by adding δ̂ to θ̂, which is obtained by
δ̂ ∼ N (0, (1 + h) 2 − 1
N Σ̂).",4.2. Parameter perturbation method,[0],[0]
"(10)
We can, in fact, confirm that the distribution of θ̂+ δ̂ approximates that of θ∗+ (1 +h)δ by applying the normal approximation to θ̂− θ∗ = δ.",4.2. Parameter perturbation method,[0],[0]
"From the normal approximation δ ∼ N (0, 1N Σ̂), we obtain θ ∗+(1+h)δ ∼ N (θ∗, (1+h) 2 N Σ̂) and θ̂+ δ̂ ∼ N (θ∗+0, 1N Σ̂+ (1+h)2−1 N Σ̂) = N",4.2. Parameter perturbation method,[0],[0]
"(θ ∗, (1+h) 2
N Σ̂).",4.2. Parameter perturbation method,[0],[0]
This procedure corresponds to (ii) in Algorithm 2.,4.2. Parameter perturbation method,[0],[0]
"We have compared our Algorithm 1 and Algorithm 2 with the hold-out method (Bailey & Marcos, 2016; Bailey et al., 2014) and the portfolio resampling method (Scherer, 2002) by means of the simulation models of the examples in Section 2.",5. Experiments,[0],[0]
"We used GUROBI Optimizer 6.0.43 for portfolio optimization, and the algorithm in (Ito & Fujimaki, 2016) for price optimization.",5. Experiments,[0],[0]
"The portfolio optimization problem described in Example 2 of Section 2 was constructed with θ∗ = (µ∗,Σ∗) defined by µ∗ = 1 + and Σ∗",5.1. Predictive Portfolio Optimization,[0],[0]
"= X>X , where ∈ Rd were generated by N(0, I) and each entry of X ∈ RD×D was drawn from N (0, D−1).",5.1. Predictive Portfolio Optimization,[0],[0]
"We generated datasets {xn}Nn=1 following N (µ∗,Σ∗), from which we computed θ̂, as in Example 2, and solved the optimization problem (1) with θ∗ replaced by θ̂, to obtain ẑ. We chose D = 50, N = 20, and λ = 1.0 for our simulation experiments.",5.1. Predictive Portfolio Optimization,[0],[0]
"When using the portfolio resampling method, we computed z̄ by means of 10 bootstrap resamplings and outputted f(z̄, θ̂) ≤ f(ẑ, θ̂).",5.1. Predictive Portfolio Optimization,[0],[0]
"For details regarding portfolio resampling, see, e.g., (Scherer, 2002).",5.1. Predictive Portfolio Optimization,[0],[0]
"For the hold-out validation, we first divided N data into N ′ and N −N ′, then computed ẑ1 from the former N ′ data and estimated θ̂2 from the letter N −N ′ data, and then calculated f(ẑ1, θ̂2).
",5.1. Predictive Portfolio Optimization,[0],[0]
"Accuracy Comparisons Figure 2 shows the means and the standard deviations of computed values of f(z∗, θ∗), f(ẑ, θ̂) and f(ẑ, θ∗) for 400 randomly-initialized datasets.",5.1. Predictive Portfolio Optimization,[0],[0]
"We have observed that:
• f(ẑ, θ̂) was much larger than f(ẑ, θ∗), which is consistent with Proposition 1.",5.1. Predictive Portfolio Optimization,[0],[0]
"• The hold-out method performed much worse than our 3 http://www.gurobi.com/
CV and perturbation methods, though its performance improved with an increasingN ′. Also, the variance in the proposed methods was much smaller.",5.1. Predictive Portfolio Optimization,[0],[0]
Note that we could not set N ′,5.1. Predictive Portfolio Optimization,[0],[0]
to be larger than N ′,5.1. Predictive Portfolio Optimization,[0],[0]
= 18 since the estimation of θ̂1 and θ̂2 would fail.,5.1. Predictive Portfolio Optimization,[0],[0]
•,5.1. Predictive Portfolio Optimization,[0],[0]
"The portfolio resampling method computed slightly less optimistic value than f(ẑ, θ̂), but a large amount of optimistic bias remained.",5.1. Predictive Portfolio Optimization,[0],[0]
• The perturbation method corrected bias better than the CV method w.r.t.,5.1. Predictive Portfolio Optimization,[0],[0]
both bias and variance.,5.1. Predictive Portfolio Optimization,[0],[0]
"Indeed, it almost perfectly corrected the optimistic bias in expectation.",5.1. Predictive Portfolio Optimization,[0],[0]
Note that K = 10 was the largest possible value because at least two samples are necessary for estimating the covariance matrix.,5.1. Predictive Portfolio Optimization,[0],[0]
This means that the value of CV (K = 10) achieved the minimum bias for the CV method.,5.1. Predictive Portfolio Optimization,[0],[0]
•,5.1. Predictive Portfolio Optimization,[0],[0]
"The CV method and the hold-out method produced
conservative estimates.",5.1. Predictive Portfolio Optimization,[0],[0]
"The pessimistic bias in the CV method came from the difference between ẑ ∈ arg maxz∈Z f(z, θ̂) and z̃ in (6).
",5.1. Predictive Portfolio Optimization,[0],[0]
"Note that E[f(ẑ, θ∗)] was poorer than E[f(z∗, θ∗)], where the former was the best objective value achieved with the available finite training samples.",5.1. Predictive Portfolio Optimization,[0],[0]
"This negative difference is unavoidable with our bias correction, which appears to raise an interesting open challenge w.r.t.",5.1. Predictive Portfolio Optimization,[0],[0]
"the combination of our bias correction with robust optimization (Bertsimas et al., 2011), i.e., the former mitigates the optimistic bias, and the later mitigates uncertainty in objective functions.
",5.1. Predictive Portfolio Optimization,[0],[0]
Sensitivity of the Perturbation Method We investigated the sensitivity of the perturbation method w.r.t.,5.1. Predictive Portfolio Optimization,[0],[0]
"h > 0, which is the important trade-off parameter in bias and variance.",5.1. Predictive Portfolio Optimization,[0],[0]
"We applied it to 100 different randomly-initialized datasets, for which we set h = 0.05, 0.10, . . .",5.1. Predictive Portfolio Optimization,[0],[0]
", 0.50.",5.1. Predictive Portfolio Optimization,[0],[0]
"Because s is not sensitive, we fixed it to s = 10.",5.1. Predictive Portfolio Optimization,[0],[0]
"Figure 3 demonstrates the changes in bias and variance (top figure) and RMSE against f(ẑ, θ∗), over h.",5.1. Predictive Portfolio Optimization,[0],[0]
"As the value
of h increased, the bias increased though the variance decreased (top figure), as was implied in Proposition 4, and this resulted in significantly larger RMSE values with smaller values of h. This observation indicates that an appropriate balance between bias and variance must be determined, and that a variance-sensitive measure such as RMSE can be used as a guide to determine the trade-off.",5.1. Predictive Portfolio Optimization,[0],[0]
We applied our algorithms to the predictive price optimization discussed as Example 3 in Section 2.,5.2. Predictive Price Optimization,[0],[0]
"As reported in (Ito & Fujimaki, 2017), the optimal value in this problem contains optimistic bias, which is consistent with Proposition 1.",5.2. Predictive Price Optimization,[0],[0]
"Unlike in the portfolio optimization, the parameter θ̂ is estimated by regression techniques, and the set of feasible strategies Z is discrete.
",5.2. Predictive Price Optimization,[0],[0]
"Simulation Experiment In this experiment, we investigated the effect of the optimistic bias and our bias correction over parameter dimensionality, i.e., the number of products d.",5.2. Predictive Price Optimization,[0],[0]
"We generated the same simulation data as in (Ito & Fujimaki, 2017).",5.2. Predictive Price Optimization,[0],[0]
"The sales quantity qi of the i-th product was generated from the regression model qi = αi + ∑d j=1 βijpj , where αi and βij were generated by uniform distributions, where αi ∈",5.2. Predictive Price Optimization,[0],[0]
"[d, 3d], βij ∈",5.2. Predictive Price Optimization,[0],[0]
"[0, 2] for i 6= j, and βii ∈",5.2. Predictive Price Optimization,[0],[0]
"[−2d,−d].",5.2. Predictive Price Optimization,[0],[0]
"The feasible region Z was defined by Z = {0.6, 0.7, . . .",5.2. Predictive Price Optimization,[0],[0]
", 1.0}d.",5.2. Predictive Price Optimization,[0],[0]
"We chose N = 500 for our experiments.
",5.2. Predictive Price Optimization,[0],[0]
"Figure 4 shows the change in the objective values normalized by the ideal objective value f(z∗, θ∗) over the number
of products d. For Algorithm 1 (CV method), we chose K = 2 so that the hold-out samples would be sufficient to estimate parameters {αi} and {βij}.",5.2. Predictive Price Optimization,[0],[0]
"We observed that:
• f(ẑ, θ∗) degraded against f(z∗, θ∗) with increasing d because the estimation error in machine learning increased.",5.2. Predictive Price Optimization,[0],[0]
•,5.2. Predictive Price Optimization,[0],[0]
"The optimistic bias, f(ẑ, θ̂)− f(ẑ, θ∗), rapidly increased because f(ẑ, θ̂)− f(z∗, θ∗) also increased in addition to the increase in f(z∗, θ∗)− f(ẑ, θ∗).",5.2. Predictive Price Optimization,[0],[0]
•,5.2. Predictive Price Optimization,[0],[0]
"The CV method suffered from pessimistic bias, which increased as d increased.",5.2. Predictive Price Optimization,[0],[0]
•,5.2. Predictive Price Optimization,[0],[0]
"The perturbation method corrected the bias accurately even if the parameter dimensionality, i.e., d, increased.
",5.2. Predictive Price Optimization,[0],[0]
"These results confirm the robustness of our proposed method over parameter dimensionality and also its general applicability to a wide range of problems (the portfolio optimization in Section 5.1 is continuous and convex while the price optimization in this section is discrete and non-convex).
",5.2. Predictive Price Optimization,[0],[0]
Real-World Retail Dataset,5.2. Predictive Price Optimization,[0],[0]
"The real-world retail dataset used in (Ito & Fujimaki, 2017; 2016) contains sales information for a middle-size supermarket located in Tokyo.4 Using this information, we selected 50 regularly-sold beer products.",5.2. Predictive Price Optimization,[0],[0]
The data range was approximately the three years from 2012/01 to 2014/11.,5.2. Predictive Price Optimization,[0],[0]
We used the first 35 months (1063 samples) for training regression models and simulated the best price strategy for the next day 2014/12/1.,5.2. Predictive Price Optimization,[0],[0]
"We estimated parameters in regression models, using the least squares method.",5.2. Predictive Price Optimization,[0],[0]
"The other settings were same as in (Ito & Fujimaki, 2016).
",5.2. Predictive Price Optimization,[0],[0]
"The actual (non-optimized) gross profit in the past data was 106, 348 JPY, while the estimated optimal value f(ẑ, θ̂) was 490, 502 JPY, which represents an approximately 361% increase in gross profit, but this value was obviously unreal-
4 The data were provided by KSP-SP Co., LTD, http:// www.ksp-sp.com.
istically huge and unreliable (price changes alone could not increase a profit 4.6 by times!).",5.2. Predictive Price Optimization,[0],[0]
"The bias-corrected optimal gross profit with the perturbation method at h = 0.1 and s = 100 was 124, 477 JPY, which represents an approximately 17% increase in the gross profit.",5.2. Predictive Price Optimization,[0],[0]
"Although we were unable to confirm the validity of this value since this experiment was conducted on past historical data, intuitively speaking, a 17% increase in gross profit seems much more realistic than one of 361%, and considering the facts noted in the simulation studies, our result would surely seem more convincing to domain users.",5.2. Predictive Price Optimization,[0],[0]
One of important remaining issues in real applications is the estimation of the confidence region.,5.2. Predictive Price Optimization,[0],[0]
"As noted above, we can never learn the value of f(ẑ, θ∗) without performing ẑ, but the user has to make a decision as to whether to perform it or not without knowing the value.",5.2. Predictive Price Optimization,[0],[0]
"In such a case, it would be helpful to provide a confidence region w.r.t.",5.2. Predictive Price Optimization,[0],[0]
"the bias-corrected optimal value, which is available with neither the CV method nor the perturbation method.",5.2. Predictive Price Optimization,[0],[0]
"In this paper, we have focused on the framework of a combination of mathematical optimization and machine learning with which we solve an optimization problem whose objective is formulated with the aid of predictive models or estimators.",6. Conclusion,[0],[0]
We have demonstrated that such a framework suffers from a kind of bias w.r.t. optimal values because of overfitting of the solution to the constructed objective function.,6. Conclusion,[0],[0]
We have proposed a solution to this bias problem by means of developed methods that are guaranteed to compute an asymptotically unbiased estimator of the value of the true objective function.,6. Conclusion,[0],[0]
"Empirical results have demonstrated that the proposed approach results in successful estimates of the value of the true objective function.
",6. Conclusion,[0],[0]
A major open question remaining in this work is how to evaluate and reduce variance in the estimators of objective functions.,6. Conclusion,[0],[0]
"The variance in estimators, i.e., uncertainty in estimation, is essential information for decision makers in many situations, and reducing variance in the estimator would help them make better decisions.",6. Conclusion,[0],[0]
"For data-driven decision-making, one promising approach, called predictive optimization, is to solve maximization problems i n which the objective function to be maximized is estimated from data.",abstractText,[0],[0]
"Predictive optimization, however, suffers from the problem of a calculated optimal solution’s being evaluated too optimistically, i.e., the value of the objective function is overestimated.",abstractText,[0],[0]
This paper investigates such optimistic bias and presents two methods for correcting it.,abstractText,[0],[0]
"The first, which is analogous to cross-validation, successfully corrects the optimistic bias but results in underestimation of the true value.",abstractText,[0],[0]
Our second method employs resampling techniques to avoid both overestimation and underestimation.,abstractText,[0],[0]
"We show that the second method, referred to as the parameter perturbation method, achieves asymptotically unbiased estimation.",abstractText,[0],[0]
Empirical results for both artificial and real-world datasets demonstrate that our proposed approach successfully corrects the optimistic bias.,abstractText,[0],[0]
Unbiased Objective Estimation in Predictive Optimization,title,[0],[0]
The advent of “big data” in recent years has generated countless opportunities for the prediction of real world phenomena with unprecedented accuracy and at unprecedented scale.,1 Introduction,[0],[0]
Statistical methods for prediction exploit associations in existing data to predict some response variable.,1 Introduction,[0],[0]
"However, the task at hand is often not to predict the response variable from pre-existing data, but rather to determine how a change in one or more of the explanatory variables will cause changes in the response variable.
1Department of Mathematics, University of Virginia, Charlottesville, VA 22904, USA 2Department of Computer Science, University of Virginia, Charlottesville, VA 22904, USA.",1 Introduction,[0],[0]
"Correspondence to: Quanquan Gu <qg5w@virginia.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1 Introduction,[0],[0]
"In statistics, causality is often established by means of a controlled, randomized experiment.",1 Introduction,[0],[0]
"Nevertheless, controlled, randomized experiments are often infeasible, leaving researchers with only access to observational data.",1 Introduction,[0],[0]
This situation arises routinely when working with time series data.,1 Introduction,[0],[0]
"Areas that must cope with this obstacle frequently include genetics (Shojaie & Michailidis, 2010) and neuroscience (Seth et al., 2015).",1 Introduction,[0],[0]
The natural question that arises is: how can one determine which factors cause changes in a certain response variable using only data in which all variables change simultaneously?,1 Introduction,[0],[0]
"Causal inference seeks to address this problem.
",1 Introduction,[0],[0]
"The classic method for causal inference among time series is a concept from econometrics known as Granger causality, named after Nobel Prize winning econometrician Clive Granger (Granger, 1969).",1 Introduction,[0],[0]
"Granger causality formalizes the intuitive notion that in a causal system, the cause must precede the effect, and the cause must hold some unique information that helps predict the effect.",1 Introduction,[0],[0]
"For example, let X
1 , . . .",1 Introduction,[0],[0]
", X T and Y 1 , . . .",1 Introduction,[0],[0]
", Y T be two stationary one-dimensional time series.",1 Introduction,[0],[0]
"We can model time series Y using the following auto-regressive model (Stock & Watson, 2011):
Y t =
pX
i=1
a",1 Introduction,[0],[0]
i Y t,1 Introduction,[0],[0]
"i + ✏t, (1.1)
where a 1 , . .",1 Introduction,[0],[0]
.,1 Introduction,[0],[0]
", a p are the coefficient parameters for the regression, p < T is the maximal lag of the model, and ✏
t is the error term.",1 Introduction,[0],[0]
"To determine whether or not X is a Granger cause of Y , we also form a second auto-regressive model:
Y t =
pX
i=1
b",1 Introduction,[0],[0]
i Y t,1 Introduction,[0],[0]
"i +
pX
i=1
c",1 Introduction,[0],[0]
i X t,1 Introduction,[0],[0]
i + !,1 Introduction,[0],[0]
"t, (1.2)
with coefficient parameters b 1 , . . .",1 Introduction,[0],[0]
", b p , c 1 , . . .",1 Introduction,[0],[0]
", c p , and error term !
t .",1 Introduction,[0],[0]
"In this classical regime, where the number of observations exceeds the number of variables (T p > 2p), one can fit both of these models with ordinary least squares (OLS).",1 Introduction,[0],[0]
"We can conduct an F-test between models (1.1) and (1.2) as well as hypothesis tests on coefficients c
i , 1  i  p, to determine if the extra information encompassed by previous values of X significantly aides in the prediction of
future values of Y .",1 Introduction,[0],[0]
"If this pair of models passes the F-test and at least one of the coefficient hypothesis tests at some significance level ↵, then we may reject the null hypothesis that X is not a Granger cause of Y (Granger, 1969).
",1 Introduction,[0],[0]
"Although the concept of Granger causality has existed for decades, Granger (1969) only rigorously treated the bivariate case.",1 Introduction,[0],[0]
"However, as noted by Arnold et al. (2007), Eichler (2006) provided one framework for multivariate analysis by applying graphical models to Granger causal inference.
",1 Introduction,[0],[0]
"Multivariate Granger causal inference relies on hypothesis testing of model coefficients in a fitted vector-autoregressive (VAR) model (Lutkepohl, 2007).",1 Introduction,[0],[0]
VAR models are fit with OLS.,1 Introduction,[0],[0]
"In the high-dimensional regime, where the number of parameters exceeds the number of observations (T p < pd, where d is the number of time series in the VAR model), OLS estimation is impossible.",1 Introduction,[0],[0]
"Hence, one must employ regularized regression methods.",1 Introduction,[0],[0]
"Perhaps the most wellknown such method is the Lasso (Tibshirani, 1996), which encourages sparsity in the coefficient parameter vector via an `
1 penalty.",1 Introduction,[0],[0]
"To conduct Granger causal inference in the high-dimensional regime, Arnold et al. (2007) proposed the “Lasso Granger” estimator, which we fully specify in (3.3).",1 Introduction,[0],[0]
"Unfortunately, since the limiting distribution of the underlying Lasso estimator is not normal (Knight & Fu, 2000) and intractable in general (Javanmard & Montanari, 2014), one cannot construct confidence intervals or compute test statistics for hypothesis tests of Lasso Granger coefficient point estimates.",1 Introduction,[0],[0]
"Thus, existing methods for high-dimensional Granger causal inference do not allow for the assessment of uncertainty.",1 Introduction,[0],[0]
"Uncertainty characterization proves an important, and often necessary, element of research in the natural sciences.",1 Introduction,[0],[0]
"Therefore, uncertainty assessment techniques would augment the versatility of high-dimensional Granger causal inference methods, and drive their wider adoption by the scientific community.
",1 Introduction,[0],[0]
Another issue in high-dimensional causal inference is how to limit the number of false positives generated when testing a large number of explanatory variables without sacrificing identification of the true causal effects.,1 Introduction,[0],[0]
"That is, the researcher wants to attain high power while still maintaining a low type I error rate.",1 Introduction,[0],[0]
"To this end, false discovery rate (FDR) control (Benjamini & Hochberg, 1995) proves an important part of any method for high-dimensional causal inference.",1 Introduction,[0],[0]
"Unfortunately, existing FDR control methods cannot cope with the two challenges posed by our setting: dependent test statistics and dependent observations.",1 Introduction,[0],[0]
"These methods thus prove unsuitable in many practical applications.
",1 Introduction,[0],[0]
"In this paper, we make two contributions.",1 Introduction,[0],[0]
"First, we propose a novel asymptotically unbiased estimator for highdimensional Granger causal inference inspired by Javanmard & Montanari (2014).",1 Introduction,[0],[0]
"We leverage this estimator’s unbiasedness to construct confidence intervals and p-values
for coefficient point estimates.",1 Introduction,[0],[0]
"In this way, we allow, for the first time, uncertainty characterization in high-dimensional Granger causal inference.",1 Introduction,[0],[0]
"Second, we propose a novel FDR control technique that can cope with dependent test-statistics and dependent observations.",1 Introduction,[0],[0]
"In addition to surmounting these theoretical obstacles to existing methods, our FDR control technique also achieves higher power in multiple testing than existing methods.",1 Introduction,[0],[0]
"Additionally, the proof techniques we use to extend high-dimensional results from the independent and identically distributed (i.i.d.) setting to our time series setting are of independent interest.",1 Introduction,[0],[0]
"Specifically, to establish the asymptotic unbiasedness and normality of our estimator, we appeal to Talagrand’s generic chaining (Talagrand, 2006) and martingale theory.",1 Introduction,[0],[0]
"We further employ martingale theory, along with empirical process theory, to prove the asymptotic validity of our FDR control procedure.
",1 Introduction,[0],[0]
The rest of this paper is organized as follows.,1 Introduction,[0],[0]
Section 2 contextualizes our contributions in the existing literature.,1 Introduction,[0],[0]
Section 3 sets up the problem of high-dimensional Granger causal inference.,1 Introduction,[0],[0]
Section 4 presents our novel de-biased estimator and FDR control procedure.,1 Introduction,[0],[0]
"Section 5 establishes our main theoretical results, which we corroborate empirically in Section 6.",1 Introduction,[0],[0]
Section 7 concludes the paper.,1 Introduction,[0],[0]
"As mentioned above, Clive Granger examined bivariate Granger causality in 1969 (Granger, 1969).",2 Related Work,[0],[0]
"Advances in the area of graphical models provided a strong framework for multivariate causal inference in general (Pearl, 2009).",2 Related Work,[0],[0]
"Graphical models were first applied specifically to Granger causal inference by Eichler (2001) and Eichler (2006), and have provided a foundation for more complex models.
",2 Related Work,[0],[0]
"However, these methods rely on OLS estimation, which is impossible in the high-dimensional regime.",2 Related Work,[0],[0]
Meinshausen & Bühlmann (2006) applied Lasso to the estimation of highdimensional graphical models.,2 Related Work,[0],[0]
"Arnold et al. (2007) then applied the method proposed by Meinshausen & Bühlmann (2006) to multivariate Granger causal inference, and introduced the estimator of primary interest for this work: the Lasso Granger estimator.",2 Related Work,[0],[0]
"The Lasso Granger estimator yields a coefficient vector in which non-zero coefficients indicate conditional Granger causes of the response variable.
",2 Related Work,[0],[0]
Classical methods for uncertainty analysis prove impossible for the Lasso Granger estimator.,2 Related Work,[0],[0]
"Recent work (Lee et al., 2013; Lockhart et al., 2014; Taylor et al., 2014) in the area of high-dimensional inference has made great strides toward addressing this issue.",2 Related Work,[0],[0]
"Early work focussed on constructing pvalues and confidence intervals for Lasso coefficients via the bootstrap (Chatterjee et al., 2013; Liu et al., 2013a).",2 Related Work,[0],[0]
"However, later work found that these methods perform poorly compared to more recent methods (Dezeure et al., 2015), especially in non-i.i.d. settings (Karoui & Purdom, 2016).
",2 Related Work,[0],[0]
"Perhaps the most promising work in high-dimensional inference has emerged from the perspective of bias correction (Bühlmann et al., 2013; Zhang & Zhang, 2014).",2 Related Work,[0],[0]
Subsequent work by Van de Geer et al. (2014) and Javanmard & Montanari (2014) introduced a method to de-bias the Lasso solution to yield asymptotically valid confidence intervals and hypothesis tests for coefficient point estimates.,2 Related Work,[0],[0]
"Nevertheless, these existing methods assume that the observations forming the design matrix are independent, and so cannot tackle causal inference among time series.",2 Related Work,[0],[0]
Our method applies the Lasso de-biasing technique to the original Lasso Granger estimator.,2 Related Work,[0],[0]
We overcome the inability of existing methods to cope with non-i.i.d.,2 Related Work,[0],[0]
"data by using Talagrand’s generic chaining (Talagrand, 2006) and the martingale technique to derive the asymptotic distribution of our novel de-biased Lasso Granger estimator.
",2 Related Work,[0],[0]
Hypothesis testing in the high-dimensional setting raises the need for procedures to address the multiple testing problem.,2 Related Work,[0],[0]
FDR control is one such way to control type I error in multiple testing.,2 Related Work,[0],[0]
Our setting poses two challenges to existing FDR control procedures.,2 Related Work,[0],[0]
"First, the most-widely used methods, such as the Benjamini-Hochberg procedure (Benjamini & Hochberg, 1995), assume the test statistics under consideration are independent.",2 Related Work,[0],[0]
"While Benjamini & Yekutieli (2001) proposed a slight variation on the Benjamini-Hochberg procedure that could control FDR under “positive regression dependency” (e.g., when the covariance matrix of the explanatory variables is strictly positive), in our setting where the explanatory variables interact in complex ways, the test statistics will not satisfy this property.",2 Related Work,[0],[0]
"This version of the Benjamini-Hochberg procedure achieves only low power in the presence of a general dependence structure (Romano et al., 2008), and is thus unsuitable for our setting.",2 Related Work,[0],[0]
"Recent methods from the area of graphical models, which explicitly model the dependency of explanatory variables, have made progress in addressing the case of dependent test statistics (Xie et al., 2011; Liu et al., 2013b).",2 Related Work,[0],[0]
"However, these methods still encounter the second challenge of our setting: dependent observations arising from time series data.",2 Related Work,[0],[0]
"To control FDR for dependent observations, one must resort to assumption-free methods, such as the Bonferroni technique, that achieve low power in practice.",2 Related Work,[0],[0]
"We propose a FDR control procedure that can cope with dependent test statistics and observations, and that achieves high power.
",2 Related Work,[0],[0]
Notation We denote matrix A,2 Related Work,[0],[0]
"= [A i,j ] 2 Rm⇥n and column vector v =",2 Related Work,[0],[0]
"[v
i ] 2 RT .",2 Related Work,[0],[0]
"We write the ` p
norm of vector v as kvk
p
= ⌃
i=T
i=1
|v i |p.",2 Related Work,[0],[0]
"Furthermore, kvk1 denotes the max-norm of vector v: kvk1 = max1iT |vi|.",2 Related Work,[0],[0]
"Additionally, kvk
0 = supp(v) designates the cardinality of the support (the set of all non-zero entries) of v. We represent the the max-norm of matrix A as kAk1 = maxi,j |Ai,j |.",2 Related Work,[0],[0]
"The minimum and maximum singular values of A are denoted by
min (A) and max (A), respectively.",2 Related Work,[0],[0]
"(x) ⌘
(1/ p 2⇡) R x
1 e t2/2dt refers to the cumulative distribu-
tion function of the standard normal distribution.",2 Related Work,[0],[0]
"For a random variable X and a sequence of random variables X
n , we write X n P ! X",2 Related Work,[0],[0]
"if X n
converges in probability to X , and X
n D ! X",2 Related Work,[0],[0]
"if X n
converges in distribution to X .",2 Related Work,[0],[0]
"For sequences of random variables X
n and Y n , we say X
n ⇣ Y n if X n has the “same asymptotic order” as Y n
, that is, if both sequences bound each other up to some universal multiplicative constant.",2 Related Work,[0],[0]
"In this section, we set up the problem of high dimensional Granger causal inference.",3 Granger Causality and its Estimator,[0],[0]
"Denote the design matrix X, the number of parameters d, the number of observations T , and the maximal lag p.",3 Granger Causality and its Estimator,[0],[0]
For a given design matrix X =,3 Granger Causality and its Estimator,[0],[0]
"[X
t,j ] 2 RT⇥d define the sample covariance matrix b⌃ = X>X/T 2 Rd⇥d.",3 Granger Causality and its Estimator,[0],[0]
"The j-th column of X represents time series X
j , 1  j  d.",3 Granger Causality and its Estimator,[0],[0]
"We can further denote lagged versions of each column in the design matrix with fX
t,j
=
(X t p,j , Xt p+1,j , . . .",3 Granger Causality and its Estimator,[0],[0]
",",3 Granger Causality and its Estimator,[0],[0]
"Xt 1,j)> 2",3 Granger Causality and its Estimator,[0],[0]
Rp.,3 Granger Causality and its Estimator,[0],[0]
"Note that Xt 1,j represents the observation immediately before X t,j
in time series X
j .",3 Granger Causality and its Estimator,[0],[0]
"In Granger causal analysis, the response variable is one of the explanatory variables.",3 Granger Causality and its Estimator,[0],[0]
"Hence, we can model an arbitrary variable X
t,j , with 1  j  d and p+ 1  t  T , by using the lagged values of all explanatory variables as predictors:
X t,j =
dX
i=1
✓ j⇤> i f X t,i + ✏ t,j .",3 Granger Causality and its Estimator,[0],[0]
"(3.1)
",3 Granger Causality and its Estimator,[0],[0]
"Here ✓j⇤ i 2 Rp and ✏ t,j ⇠ N(0, 2 j ).",3 Granger Causality and its Estimator,[0],[0]
"Time series X i is a conditional Granger cause of time series X j
(conditioned on the other d - 2 time series) if ✓j⇤
i contains any non-zero elements (i.e., k✓j⇤
i
k 0
> 0).",3 Granger Causality and its Estimator,[0],[0]
"We can vectorize the sets of all ✓j⇤
i
and all fX t,i , for 1  i  d, as ✓j⇤ = (✓j⇤>
1 ,✓j⇤> 2 , . . .",3 Granger Causality and its Estimator,[0],[0]
",✓j⇤> d )",3 Granger Causality and its Estimator,[0],[0]
"> 2 Rpd and fX t =
( f X > t,1 , fX> t,2 , . . .",3 Granger Causality and its Estimator,[0],[0]
", fX> t,d )",3 Granger Causality and its Estimator,[0],[0]
"> 2 Rpd, respectively.",3 Granger Causality and its Estimator,[0],[0]
"Based on (3.1), the Lasso Granger estimator (Arnold et al., 2007) is given by
b ✓ j = argmin
✓j
1 2(T p) TX
t=p+1
(X t,j ✓j>fX t ) 2 + k",3 Granger Causality and its Estimator,[0],[0]
"✓jk 1 ,
where > 0 is the regularization parameter.
",3 Granger Causality and its Estimator,[0],[0]
"Equivalently, letting eX = (fX p+1 , fX p+2 , . . .",3 Granger Causality and its Estimator,[0],[0]
", fX T )",3 Granger Causality and its Estimator,[0],[0]
"> 2 R(T p)⇥pd and Y
j = X p+1:T,j represent the lower T p elements of the j-th column of X , we can re-express our model in more standard notation as:
Y
j
= eX✓j⇤ + ✏, (3.2)
where ✏ ⇠ N(0, 2I (T p)⇥(T p)).",3 Granger Causality and its Estimator,[0],[0]
"We can now re-express
the Lasso Granger estimator as:
b ✓ j = argmin
✓j
",3 Granger Causality and its Estimator,[0],[0]
1 2(T p)kYj eX✓jk2 2 + k✓jk 1 .,3 Granger Causality and its Estimator,[0],[0]
"(3.3)
",3 Granger Causality and its Estimator,[0],[0]
"For ease of presentation, we will henceforth omit the identifying variable j from ✓j⇤, b✓j , and Y
j , and assume we are referring to some arbitrary response variable.",3 Granger Causality and its Estimator,[0],[0]
"Using the above notation, we can now denote the sample covariance matrix of eX as e⌃
n
= eX> eX/(T p) 2 Rpd⇥pd and the true covariance matrix as e⌃ = E[e⌃
n
].",3 Granger Causality and its Estimator,[0],[0]
"In this section, we introduce our de-biased Lasso Granger estimator, and construct confidence intervals and p-values for its elements.",4 Asymptotic Inference for Lasso Granger,[0],[0]
We will then present our method for false discovery rate control in multiple testing.,4 Asymptotic Inference for Lasso Granger,[0],[0]
"In deriving a de-biased version of the Lasso Granger estimator, we employ a variation of the Lasso de-biasing procedure proposed by Javanmard & Montanari (2014).",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"In particular, we define the de-biased Lasso Granger estimator b✓u as follows:
",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"b ✓ u = b ✓ + 1 T pM eX>(Y eXb✓), (4.1)
where b✓ 2 Rpd is the parameter vector yielded when computing the Lasso Granger estimator (3.3) for an arbitrary response variable Y = Y
j .",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"M = (m 1 ,m 2 , . . .",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
",m pd )",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"> 2 Rpd⇥pd is an estimate of e⌃ 1
n , the inverse sample covariance matrix of eX, where each m
i is the solution to the following optimization algorithm:
minimize m> e⌃ n m subject to ke⌃ n",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"m e i k1  µ, (4.2)
where e i 2 Rpd is the i-th column of I pd⇥pd, and the choice of µ will be clear after we deliver theory.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"Our unbiased estimator b✓u, though inspired by Javanmard & Montanari (2014), diverges sharply from their work in several respects.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"While Javanmard and Montanari use the observed design matrix X in their estimator, we use the transformed design matrix eX.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"Although in our time series setting the rows of design matrix X are already dependent, transforming X to eX exacerbates this dependency and renders the i.i.d. results underpinning Javanmard and Montanari’s work unusable.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"Hence, we appeal to Talagrand’s generic chaining (Talagrand, 2006) and martingale theory to establish our theoretical results about b✓u.
Theorem 5.5 in Section 5 below proves that for any i 2 {1, 2, . . .",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
", pd}, the standardized estimate of the i-th element
of b✓u converges in distribution to the standard normal distribution:
p T p
b✓u i ✓",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
⇤,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"i
[Me⌃ n",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
M>]1/2,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"i,i
D !",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"N(0, 1).",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"(4.3)
Unfortunately, the true noise level, denoted here by , is unknown in most real-world applications.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"Hence, we replace with a consistent estimator, denoted b , yielded by the Scaled Lasso (Sun & Zhang, 2012):
{b✓( ), b ( )} =
argmin
✓2Rpd, >0
⇢ 1
2 (T P )kY eX",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"✓k2 2 + 2 + k✓k 1 ,
(4.4)
where is the regularization parameter.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
Sun & Zhang (2012) prove b is a consistent estimator of when the penalized loss function is convex.,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
Sun & Zhang (2012) use the i.i.d assumption to establish convexity.,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"In our non-i.i.d. setting, we establish convexity via a restricted eigenvalue condition for martingale difference sequences.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"Thus, b is consistent in our setting as well.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"Then by the Slutsky Theorem (Van der Vaart, 2000), we can replace in (4.3) with b .
",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"One can easily apply (4.3) to construct confidence intervals for ✓⇤
i , for 1  i  pd.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"If the significance level is ↵ > 0, the 1 ↵ confidence interval for ✓⇤
i
is:
I i =",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"[ b✓u i
(↵, T p), b",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"✓u i
+ (↵, T p)], (4.5) where
(↵, T p) = 1(1 ↵/2)(b /pT p)[Me⌃ n",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
M>]1/2,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"i,i .
",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"We prove the asymptotic validity of this confidence interval in Corollary 5.6.
",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"Similarly, we can also conduct hypothesis tests on the individual regression coefficients ✓⇤
i , for 1  i  pd.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"In the context of Granger causality, the relevant null and alternative hypotheses are Hi
0 :",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"✓⇤ i = 0 and Hi a : ✓⇤ i 6= 0, respectively.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
Having zero-coefficients for all variables p(x 1) <,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
i  px implies that time series 1  x  d is not a conditional Granger cause of the response time series.,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"Conversely, rejecting Hi
0 for any variable p(x 1)",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
<,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
i  px amounts to rejecting the null hypothesis that time series x is not a Granger cause of the response time series.,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"We thus consider the following test statistic for Hi
0 :",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"✓⇤ i = 0:
cZ i =
b✓u i
p T p
b",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
[Me⌃ n,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
M>]1/2,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"i,i
.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"(4.6)
Note that under the null hypothesis cZ",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
i D !,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"N(0, 1) by (4.3).",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"The hypothesis test at significance level ↵ is thus given by
Z (↵) = 1( |cZ i | < z ↵/2 ), (4.7)
where z ↵/2 is the quantile of the standard normal distribution such that (z
↵/2 )",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
= ↵/2.,4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"We reject the null hypothesis if and only if
Z (↵) = 1.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"The p-value for this hypothesis test is
P i = 2(1 (|cZ i |)).",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"(4.8) As usual, one would reject Hi
0 at a pre-specified significance level ↵ if P
i < ↵.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"We establish that the type I error of the hypothesis test
Z (↵) converges to the specified significance level, and that the p-value P
i is asymptotically uniformly distributed in Corollary 5.7.",4.1 Confidence Intervals and Hypothesis Tests,[0],[0]
"Having established test statistics for individual coefficients of the de-biased Lasso Granger estimator, we now address the issue of FDR control.",4.2 False Discovery Rate Control,[0],[0]
"First, denote the set of coefficient indices i such that ✓⇤
i
= 0",4.2 False Discovery Rate Control,[0],[0]
"as H 0 = {i|✓⇤ i = 0, 1  i  pd}.",4.2 False Discovery Rate Control,[0],[0]
"Define the complement of this set as H
1 = {i|✓⇤ i 6= 0, 1  i  pd}.",4.2 False Discovery Rate Control,[0],[0]
"We define FDR and false discovery proportion (FDP) as follows:
FDP(⌫) =",4.2 False Discovery Rate Control,[0],[0]
P i2H 0,4.2 False Discovery Rate Control,[0],[0]
1(|cZ,4.2 False Discovery Rate Control,[0],[0]
"i
| ⌫)",4.2 False Discovery Rate Control,[0],[0]
"max{P 1jpd 1(|cZi| ⌫), 1} ,
FDR(⌫) = E[FDP(⌫)].
",4.2 False Discovery Rate Control,[0],[0]
When conducting hypothesis tests at significance 0,4.2 False Discovery Rate Control,[0],[0]
<,4.2 False Discovery Rate Control,[0],[0]
"↵ < 1, we seek the smallest ⌫ such that FDR(⌫)  ↵.",4.2 False Discovery Rate Control,[0],[0]
"In this way, we will be able to reject the null hypothesis as often as possible (i.e., we maximize power) while still guaranteeing that our type I error rate does not exceed ↵.",4.2 False Discovery Rate Control,[0],[0]
"Thus, the ideal choice of ⌫ is
b⌫ = inf ⇢ ⌫",4.2 False Discovery Rate Control,[0],[0]
> 0,4.2 False Discovery Rate Control,[0],[0]
":
P i2H
0
1{|cZ",4.2 False Discovery Rate Control,[0],[0]
"i | ⌫} max{P 1jpd 1{|cZj | ⌫}, 1}  ↵ .
(4.9)
Note that the left hand side of the inequality in (4.9) is FDP, whose expectation is FDR.",4.2 False Discovery Rate Control,[0],[0]
"Unfortunately, b⌫ cannot be computed under the unknown H
0",4.2 False Discovery Rate Control,[0],[0]
"(Liu et al., 2013b).",4.2 False Discovery Rate Control,[0],[0]
"However, following Liu & Luo (2014), we use the asymptotic normality of cZ
i under the null hypothesis to approximateP i2H
0
1{|cZ",4.2 False Discovery Rate Control,[0],[0]
i | ⌫} by 2(1 (⌫))pd.,4.2 False Discovery Rate Control,[0],[0]
"In multiple hypothesis testing, we use b⌫ as the threshold for rejecting the null hypothesis, instead of z
↵/2
, in hypothesis test Z (↵) (4.7).",4.2 False Discovery Rate Control,[0],[0]
Theorem 5.9 below demonstrates the asymptotic validity of this FDR control method.,4.2 False Discovery Rate Control,[0],[0]
"In this section we present our main theoretical results: the test statistic cZ
i from (4.6) converges in distribution to the standard normal under the null hypothesis, and the FDR control procedure presented in (4.9) asymptotically controls both FDR and FDP.",5 Main Theory,[0],[0]
"To begin, we present several definitions.
",5 Main Theory,[0],[0]
Definition 5.1.,5 Main Theory,[0],[0]
"(Vershynin, 2012)",5 Main Theory,[0],[0]
A random variable X is sub-Gaussian if there exists a constant C > 0,5 Main Theory,[0],[0]
"such that
P(|X|",5 Main Theory,[0],[0]
"> t)  2 exp[ t2/C2], for all t > 0.
",5 Main Theory,[0],[0]
"A random vector X 2 Rn is sub-Gaussian if the onedimensional marginals < X,v > are sub-Gaussian random variables for all v 2 Rn.",5 Main Theory,[0],[0]
Definition 5.2.,5 Main Theory,[0],[0]
"(Javanmard & Montanari, 2014)",5 Main Theory,[0],[0]
"The subGaussian norm of a random scalar variable X is:
kXk
2
= sup q 1 q 1/2(E[|X|q])1/q.
",5 Main Theory,[0],[0]
"The sub-Gaussian norm of a random vector X 2 Rn is: kXk
2
= sup u2Sn 1 khX,uik 2 ,
where Sn 1 is the unit sphere in Rn space.
",5 Main Theory,[0],[0]
"Having established these definitions, we impose two assumptions on the design matrix and the true covariance matrix of the design matrix.",5 Main Theory,[0],[0]
Assumption 5.3.,5 Main Theory,[0],[0]
"There exist universal constants C
min , C max such that 0 < C min  min ( e⌃)  ",5 Main Theory,[0],[0]
max ( e⌃)  C max .,5 Main Theory,[0],[0]
Assumption 5.4.,5 Main Theory,[0],[0]
"The rows of eX are sub-Gaussian and the sub-Gaussian norm of each row is bounded by some constant  so that kfX
i
k
2  , for i 2 {1, 2, . . .",5 Main Theory,[0],[0]
", T p}.",5 Main Theory,[0],[0]
"We use Assumption 5.3 to demonstrate that the restricted eigenvalue condition holds for e⌃
n in order to prove the asymptotic unbiasedness of b✓u. Assumption 5.4 plays a role at multiple stages of the proof of Theorem 5.5, including proving the restricted eigenvalue condition for e⌃
n and establishing a high-probability bound for the regularization parameter .",5 Main Theory,[0],[0]
"Both of these assumptions prove common in the high-dimensional inference literature.
",5 Main Theory,[0],[0]
We leverage Assumptions 5.3 and 5.4 to present the following theorem.,5 Main Theory,[0],[0]
Theorem 5.5.,5 Main Theory,[0],[0]
Suppose Assumptions 5.3 and 5.4 are satisfied.,5 Main Theory,[0],[0]
"Let s
0 = supp(✓⇤) ⇣ pT p/ log(pd) and µ ⇣ plog(pd)/(T p).",5 Main Theory,[0],[0]
"Then for any element b✓u
i of the de-biased Lasso Granger estimator b✓u defined in (4.1), we have
p T p
b✓u i ✓⇤ i
[Me⌃ n",5 Main Theory,[0],[0]
M>]1/2,5 Main Theory,[0],[0]
"i,i
D !",5 Main Theory,[0],[0]
"N(0, 1).
",5 Main Theory,[0],[0]
Theorem 5.5 immediately yields several useful results.,5 Main Theory,[0],[0]
"We first demonstrate the asymptotic validity of confidence interval (4.5) for any element of b✓u in the following corollary.
",5 Main Theory,[0],[0]
Corollary 5.6.,5 Main Theory,[0],[0]
"Denote significance level ↵ > 0, and for 1  i  pd, define interval",5 Main Theory,[0],[0]
"I
i
=",5 Main Theory,[0],[0]
"[ b✓u i
(↵, T p), b",5 Main Theory,[0],[0]
"✓u
i + (↵, T p)].",5 Main Theory,[0],[0]
"Here, (↵, T p) = (1 ↵/2)( / p T p)[Me⌃
n",5 Main Theory,[0],[0]
M>]1/2,5 Main Theory,[0],[0]
"i,i .",5 Main Theory,[0],[0]
"Then
lim T p!1 P(✓⇤ i 2",5 Main Theory,[0],[0]
I i ),5 Main Theory,[0],[0]
"= 1 ↵.
",5 Main Theory,[0],[0]
"By Corollary 5.6, the asymptotic coverage probability corresponds the the given confidence level.",5 Main Theory,[0],[0]
Note that we can replace with b by the Slutsky Theorem.,5 Main Theory,[0],[0]
"Similarly, we confirm in the following corollary that the type I error for hypothesis test
Z (↵), introduced in (4.7), matches the given significance level ↵.",5 Main Theory,[0],[0]
"Furthermore, we prove that the CDF of the p-value P
i for Z (↵), which we introduced in (4.8), converges in distribution to a uniform distribution.",5 Main Theory,[0],[0]
Corollary 5.7.,5 Main Theory,[0],[0]
"With
Z (↵) and P i defined as above, and significance level ↵ > 0, we have:
P( Z (↵) = 1|Hi 0 )",5 Main Theory,[0],[0]
(T p)!1 !,5 Main Theory,[0],[0]
↵ and P i D !,5 Main Theory,[0],[0]
U,5 Main Theory,[0],[0]
"[0, 1].
We now turn our attention to demonstrating the asymptotic validity of the FDR control method we present in Section 4.2.",5 Main Theory,[0],[0]
"To control FDR we desire the following property:
P i2H
0
1(|cZ",5 Main Theory,[0],[0]
"i | b⌫) 2|H
0
|(1 (b⌫))",5 Main Theory,[0],[0]
P !,5 Main Theory,[0],[0]
"1. (5.1)
",5 Main Theory,[0],[0]
"Unfortunately, in this application, the test statistics cZ",5 Main Theory,[0],[0]
"i
are correlated, rendering the convergence in (5.1) non-trivial.",5 Main Theory,[0],[0]
"In order to prove (5.1), we will leverage martingale theory, empirical process theory, and the following assumption.",5 Main Theory,[0],[0]
Assumption 5.8.,5 Main Theory,[0],[0]
"For constant c > 2,
X
i2H 1
1
✓ |✓⇤ i
| e⌃ 1/2
i,i
s
c log(pd) (T p) ◆ !",5 Main Theory,[0],[0]
"1,
as (T p, pd) !",5 Main Theory,[0],[0]
1.,5 Main Theory,[0],[0]
Assumption 5.8 implies that the number of true alternative hypotheses approaches infinity.,5 Main Theory,[0],[0]
"This property proves important because, as demonstrated by Liu et al. (2014), FDR control is impossible when the number of true alternative hypotheses is fixed.",5 Main Theory,[0],[0]
This assumption allows us to present the following theorem: Theorem 5.9.,5 Main Theory,[0],[0]
Assume pd  (T p)r and log(pd) = o,5 Main Theory,[0],[0]
( p T p) for some r > 0.,5 Main Theory,[0],[0]
"Furthermore, suppose that Assumption 5.8 and the assumptions of Theorem 5.5 hold.",5 Main Theory,[0],[0]
"Then at significance level ↵,
lim (T p,pd) FDR(b⌫) ↵|H 0 |/(pd) = 1 and FDP(b⌫) ↵|H 0 |/(pd) P ! 1,
as (T p, pd) !",5 Main Theory,[0],[0]
"1.
Theorem 5.9 establishes that the FDR control procedure we present in Section 4.2 asymptotically controls both FDR and FDP.",5 Main Theory,[0],[0]
Note that the upper bound rate imposed on pd is very mild and will pose no issues in the vast majority of applications.,5 Main Theory,[0],[0]
"The assumptions of Theorem 5.5 guarantee the asymptotic normality of test statistic cZ
i
.",5 Main Theory,[0],[0]
"In this section, we establish the effectiveness of our debiased Lasso Granger estimator and our FDR control procedure via experimental results.",6 Numerical Experiments,[0],[0]
We also demonstrate that our methods outperform existing techniques.,6 Numerical Experiments,[0],[0]
"In this section, we corroborate our theoretical results and compare our contributions to existing methods with numerical experiments on synthetic data.",6.1 Synthetic Data,[0],[0]
The data for these experiments are generated by model (3.1).,6.1 Synthetic Data,[0],[0]
"In order to satisfy the assumptions of Theorem 5.5, each ✓j⇤ is a sparse vector such that the probability of each element being non-zero is p T p/(2pd log(pd)) for 1  j  d. We use the R package“flare” (Li et al., 2012) to generate sparse transition matrices, and the “glmnet” package (Friedman et al., 2010) to compute the biased Lasso Granger estimate.",6.1 Synthetic Data,[0],[0]
"We examine multiple different transition matrix patterns (“random” and “cluster”, as generated by the “flare” package) and multiple different configurations of (T, d, p).
",6.1 Synthetic Data,[0],[0]
"In Table 1, we see that the empirical type 1 error of hypothesis test
Z (↵) (4.7) corresponds to the given significance level across multiple configurations of (T, d, p).",6.1 Synthetic Data,[0],[0]
"Figure 1(a) corroborates Theorem 5.5 by demonstrating that the empirical distribution of test statistic cZ
i under the null hypothesis is the standard normal distribution.",6.1 Synthetic Data,[0],[0]
Figure 1(a) also illustrates that coefficient point estimates for the biased Lasso Granger estimator do not follow the standard normal distribution.,6.1 Synthetic Data,[0],[0]
Figure 1(b) validates Corollary 5.7 by demonstrating that the empirical CDF of p-value (4.8) for a true zero parameter is the uniform distribution.,6.1 Synthetic Data,[0],[0]
"Furthermore, Figures 1(c) and 1(d) exhibit that hypothesis test
Z (↵) (4.7) attains higher power than the biased Lasso Granger estimator when testing a single true non-zero parameter.",6.1 Synthetic Data,[0],[0]
"Table 2 demonstrates the accuracy of the de-biased Lasso Granger estimator via computations of the `
1 and ` 2 norms of the
error vector between b✓u and ✓⇤.
",6.1 Synthetic Data,[0],[0]
"Table 3 exhibits that, as suggested by theory, our FDR control procedure outperforms the Bonferroni and BenjaminiHochberg (B-H) (Benjamini & Hochberg, 1995; Benjamini & Yekutieli, 2001) methods in terms of power, while still maintaining low FDP.",6.1 Synthetic Data,[0],[0]
"While the Bonferroni method generally achieves only low power, the Benjamini-Hochberg method performs poorly in this application because the test statistics exhibit complex dependency, and thus violate a theoretical assumption of the Benjamini-Hochberg method.
",6.1 Synthetic Data,[0],[0]
"Lastly, Figure 2 demonstrates that our de-biased Lasso Granger estimator paired with our FDR control procedure outperform the original biased Lasso Granger estimator in terms of precision and recall.",6.1 Synthetic Data,[0],[0]
"Define sets TP = {i 2 H
1 |1(✓⇤ i identified as non-zero)} and FP = {i 2 H
0 |1(✓⇤ i identified as non-zero)}, so precision is |TP|/max{|TP| + |FP|, 1}, and recall is |TP|/|H
1 |.",6.1 Synthetic Data,[0],[0]
Note that precision is equivalent to 1 FDP and recall is equivalent to power.,6.1 Synthetic Data,[0],[0]
We calculate precision and recall at each point along the Lasso-path of the regularization parameter to generate the curves in Figure 2.,6.1 Synthetic Data,[0],[0]
These curves demonstrate that our de-biased Lasso Granger estimator and FDR control procedure achieve higher recall than the original Lasso Granger estimator without sacrificing precision.,6.1 Synthetic Data,[0],[0]
"Thus, not only does our method provide the interpretability and flexibility of uncertainty characterization, it also achieves higher power
than the original Lasso Granger estimator while maintaining low FDP.",6.1 Synthetic Data,[0],[0]
"Therefore, our method proves more suitable for high-dimensional Granger causal inference.",6.1 Synthetic Data,[0],[0]
"To demonstrate the applicability of our method to real-world data, we consider the climatological data set made available by Lozano et al. (2009).",6.2 Real Data,[0],[0]
"This data set contains monthly observations for seventeen climatological variables (e.g., temperature, precipitation, CO2, CH4, etc.) for 128 grid points spanning the continental United States (latitudes 32.975 to 45.475 and longitudes 84.75 to 117.25) from 1990 to
2002.",6.2 Real Data,[0],[0]
"Following the setup from Lozano et al. (2009), we enforce stationarity by deseasonlaizing the data using the R package “deseasonalize” (McLeod & Gweon, 2013).",6.2 Real Data,[0],[0]
We model the monthly temperature change of each grid point as a linear model of the first three lagged values of all explanatory variables in the surrounding 3⇥ 3 grid.,6.2 Real Data,[0],[0]
"Thus, for each of the 81 interior grid points, we obtain design matricies with dimensions T = 13⇥ 12 = 156 (13 years of monthly data), d = 17 ⇥ 9 = 153 (17 climatological variables observed at 9 grid points), and p = 3.",6.2 Real Data,[0],[0]
"For each of these design matricies, we use the R package “glmnet”(Friedman et al., 2010) to produce the biased Lasso Granger estimate from (3.3), and then apply (4.1) to construct the de-biased Lasso Granger estimate.
",6.2 Real Data,[0],[0]
"For each grid point, we test the significance of the three lagged values of monthly changes in Carbon Dioxide (CO2) emissions for that grid point to determine if local CO2 emissions are a Granger cause of temperature changes when conditioned on many other climatological variables.",6.2 Real Data,[0],[0]
Recall that an explanatory variable is a Granger cause of the response variable if and only if any of the coefficients for any of the lags prove significant.,6.2 Real Data,[0],[0]
"We use the Bonferroni method, the Benjamini-Hochberg (B-H) procedure, and our FDR control method from Section 4.2 to control for multiple testing.",6.2 Real Data,[0],[0]
"At significance level ↵ = .05, the Bonferroni and Benjamini-Hochberg methods found that CO2 emissions are a Granger cause of monthly temperature changes for 10 of the 81 grid points, whereas our FDR control method discovered 13 such grid points.",6.2 Real Data,[0],[0]
"We thus corroborate the findings of Lozano et al. (2009), who employed graphical Granger modeling methods to establish Granger causality between CO2 emissions and temperature changes, and those of many climate researchers who have found increased CO2 emissions to “cause” higher temperatures.",6.2 Real Data,[0],[0]
We also find empirical evidence that our FDR control method achieves higher power than the Bonferroni and Benjamini-Hochberg methods.,6.2 Real Data,[0],[0]
Figure 3 displays the results of this simulation.,6.2 Real Data,[0],[0]
"In this paper, we propose a novel unbiased estimator for conducting Granger causal inference in the high-dimensional
(a) Bonferroni, B-H (b) FDR Control
regime.",7 Conclusion,[0],[0]
"We introduce test statistics and confidence intervals for our estimator, thereby accomplishing the previously impossible task of uncertainty characterization in high-dimensional Granger causal inference.",7 Conclusion,[0],[0]
"Additionally, we introduce a novel method for false discovery rate control that achieves higher-power in multiple testing than existing alternatives in our setting.",7 Conclusion,[0],[0]
"Lastly, we validate our theoretical results with experiments on both synthetic data and real-world climatological data.",7 Conclusion,[0],[0]
Future extensions of our work may include generalizations of our method to cope with non-Gaussian noise and non-linear causality.,7 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their helpful comments.,Acknowledgements,[0],[0]
This research was sponsored in part by the National Science Foundation IIS-1618948 and IIS1652539.,Acknowledgements,[0],[0]
The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.,Acknowledgements,[0],[0]
Causal inference among high-dimensional time series data proves an important research problem in many fields.,abstractText,[0],[0]
"While in the classical regime one often establishes causality among time series via a concept known as “Granger causality,” existing approaches for Granger causal inference in high-dimensional data lack the means to characterize the uncertainty associated with Granger causality estimates (e.g., p-values and confidence intervals).",abstractText,[0],[0]
We make two contributions in this work.,abstractText,[0],[0]
"First, we introduce a novel asymptotically unbiased Granger causality estimator with corresponding test statistics and confidence intervals to allow, for the first time, uncertainty characterization in high-dimensional Granger causal inference.",abstractText,[0],[0]
"Second, we introduce a novel method for false discovery rate control that achieves higher power in multiple testing than existing techniques and that can cope with dependent test statistics and dependent observations.",abstractText,[0],[0]
We corroborate our theoretical results with experiments on both synthetic data and real-world climatological data.,abstractText,[0],[0]
Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference,title,[0],[0]
"A fundamental task in machine learning (ML) is to discover latent patterns underlying data, for instance, extracting topics from documents and communities from social networks.",1. Introduction,[0],[0]
"Latent space models (Bishop, 1998; Knott & Bartholomew, 1999; Blei, 2014) are effective tools to accomplish this task.",1. Introduction,[0],[0]
"An LSM contains a collection of learnable components such as hidden units in neural networks and factors in factor analysis (Harman, 1960).",1. Introduction,[0],[0]
Each component is aimed at capturing a hidden pattern.,1. Introduction,[0],[0]
"In most LSMs, components are parameterized by vectors.
",1. Introduction,[0],[0]
"Among the many challenges encountered in latent space modeling, two of them are of particular interest to us.
",1. Introduction,[0],[0]
"1Machine Learning Department, Carnegie Mellon University 2Petuum Inc.",1. Introduction,[0],[0]
"Correspondence to: Pengtao Xie <pengtaox@cs.cmu.edu>, Eric P. Xing <eric.xing@petuum.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"First, under many circumstances, the frequency of patterns is highly imbalanced.",1. Introduction,[0],[0]
Some patterns have very high frequency while others occur less frequently.,1. Introduction,[0],[0]
"As a typical example, in a news corpus, politics and economics are frequent topics (patterns) while furniture and gardening are infrequent.",1. Introduction,[0],[0]
"Classic LSMs are sensitive to the skewness of pattern frequency and less capable of capturing the infrequent patterns (Wang et al., 2014).",1. Introduction,[0],[0]
"Second, when using LSMs, one needs to carefully balance the tradeoff between model size (precisely, the number of components) and modeling power (Xie, 2015).",1. Introduction,[0],[0]
"Larger-sized LSMs are more expressive, but incur higher computational complexity.",1. Introduction,[0],[0]
"It is desirable but challenging to achieve sufficient modeling power with a small number of components.
",1. Introduction,[0],[0]
"To address these two challenges, recent studies (Zou & Adams, 2012; Cogswell et al., 2015; Xie et al., 2015; 2016) investigate a “diversification” strategy which encourages the components in LSMs to be mutually different, either through frequentist-style regularization (Zou & Adams, 2012; Cogswell et al., 2015; Xie et al., 2015) or Bayesian learning (Xie et al., 2016).",1. Introduction,[0],[0]
"They conjecture that: (1) through “diversification”, some components that are originally aggregated around frequent patterns can be pushed apart to cover infrequent patterns; (2) “diversified” components bear less redundancy and are mutually complementary; a small number of such components are sufficient to model data well.
",1. Introduction,[0],[0]
"Along this line of research, several diversity-promoting regularizers have been proposed, based upon determinantal point process (Kulesza & Taskar, 2012; Zou & Adams, 2012), cosine similarity (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015) and covariance (Malkin & Bilmes, 2008; Cogswell et al., 2015).",1. Introduction,[0],[0]
"While these regularizers demonstrate notable efficacy, they have certain limitations, such as sensitivity to vector scaling (Zou & Adams, 2012; Malkin & Bilmes, 2008), inability to measure diversity in a global manner (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015) and computational inefficiency (Cogswell et al., 2015).",1. Introduction,[0],[0]
"To address these limitations, we propose a new diversity-promoting regularizer gaining inspiration from principal component analysis (Jolliffe, 2002), biological diversity (Magurran, 2013) and information theory (Cover & Thomas, 2012).
",1. Introduction,[0],[0]
We characterize “diversity” by considering two factors: uncorrelation and evenness.,1. Introduction,[0],[0]
"Uncorrelation (Cogswell et al., 2015) encourages the components to be uncorrelated, such that each component can independently capture a unique pattern.",1. Introduction,[0],[0]
"Evenness is inspired from biological diversity (Magurran, 2013) where an ecosystem is deemed to be more diverse if different species contribute equally to the maintenance of biological balance.",1. Introduction,[0],[0]
"Analogously, when measuring component diversity, we assign an “importance” score to each component and encourage these scores to be even.",1. Introduction,[0],[0]
"In the context of latent space modeling, evenness ensures each component plays a significant role in pattern discovery rather than being dominated by others.
",1. Introduction,[0],[0]
We study uncorrelation and evenness from a statistical perspective.,1. Introduction,[0],[0]
The components are considered as random variables and the eigenvalues of their covariance matrix can be leveraged to characterize these two factors.,1. Introduction,[0],[0]
"First, according to Principle Component Analysis (Jolliffe, 2002), the disparity of eigenvalues reflects the correlation among components: the more uniform the eigenvalues, the less correlated the components.",1. Introduction,[0],[0]
"Second, eigenvalues represent the variance along principal directions and can be used to measure the “importance” of components.",1. Introduction,[0],[0]
"Promoting uniform importance amounts to encouraging evenness among eigenvalues.
",1. Introduction,[0],[0]
"To promote uniformity among the eigenvalues, we encourage the discrete distribution parametrized by the normalized eigenvalues to have small Kullback-Leibler divergence with the uniform distribution, based on which, we define a uniform eigenvalue regularizer (UER) and make a connection with the von Neumann entropy (Bengtsson & Zyczkowski, 2007) and with the von Neumann divergence (Kulis et al., 2009).",1. Introduction,[0],[0]
"We apply UER to two LSMs – distance metric learning (DML) (Xing et al., 2002) and long shortterm memory (LSTM) network (Hochreiter & Schmidhuber, 1997) – to encourage their components to be diverse and develop an efficient optimization algorithm.",1. Introduction,[0],[0]
"Experiments on healthcare, image and text data demonstrate that UER (1) greatly improves the performance of LSMs; (2) better captures infrequent patterns; (3) reduces model size without sacrificing modeling power; (4) outperforms other diversity-promoting regularizers.
",1. Introduction,[0],[0]
"The major contributions of this paper are: • We propose a new diversity-promoting regularizer
from the perspectives of uncorrelation and evenness.
",1. Introduction,[0],[0]
"• We propose to simultaneously promote uncorrelation and evenness by encouraging uniformity among the eigenvalues of the covariance matrix of components.
",1. Introduction,[0],[0]
"• We develop an efficient projected gradient descent algorithm to solve UE regularized LSM problems.
",1. Introduction,[0],[0]
"• In experiments, we demonstrate the effectiveness of this regularizer on two LSMs: DML and LSTM.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
Section 2 reviews related works.,1. Introduction,[0],[0]
Section 3 introduces the uniform eigenvalue regularizer.,1. Introduction,[0],[0]
Section 4 presents experimental results and Section 5 concludes the paper.,1. Introduction,[0],[0]
"Diversity promoting regularization has been widely used in classification (Malkin & Bilmes, 2008), ensemble learning (Yu et al., 2011) and latent space modeling (Zou & Adams, 2012; Xie et al., 2015; 2017).",2. Related Works,[0],[0]
"In the sequel, we present a brief review of existing diversity-promoting regularizers.",2. Related Works,[0],[0]
"Several regularizers (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015; 2017) are based on pairwise dissimilarity of components: if every two components are dissimilar, then overall the set of components are “diverse”.",2. Related Works,[0],[0]
"Given the weight vectors {aj}mj=1 of m components, Yu et al. (2011) define the regularizer as∑
1≤j<k≤m(1−cjk), where cjk is the cosine similarity between component j and k.",2. Related Works,[0],[0]
"In (Bao et al., 2013), the score is defined as − log( 1m(m−1) ∑ 1≤j<k≤m β|cjk|) 1 β where β > 0.",2. Related Works,[0],[0]
"In (Xie et al., 2015), the score is defined as mean of {arccos(|cjk|)} minus the variance of {arccos(|cjk|)}, where the variance term is utilized to encourage the dissimilarity scores {arccos(|cjk|)} to be even.",2. Related Works,[0],[0]
"Xie et al. (2017) define the regularizer as ∑ 1≤i<j≤m k(ai,aj) where k(·, ·) is a kernel function.",2. Related Works,[0],[0]
"These regularizers are applied to classifiers ensemble, neural network and restricted Boltzmann machine.",2. Related Works,[0],[0]
"While these regularizers can capture pairwise dissimilarities between components, they are unable to capture higher-order “diversity”.
",2. Related Works,[0],[0]
"Determinantal Point Process (DPP) (Kulesza & Taskar, 2012) was used by (Zou & Adams, 2012; Mariet & Sra, 2015) to encourage the topic vectors in Latent Dirichlet Allocation (Blei et al., 2003), Gaussian mean vectors in Gaussian Mixture Model and hidden units in neural network to be “diverse”.",2. Related Works,[0],[0]
"The DPP regularizer is defined as − log det(L), where L is a m × m kernel matrix and det(·) denotes the determinant of the matrix.",2. Related Works,[0],[0]
"Lij equals to k(ai,aj) and k(·, ·) is a kernel function.",2. Related Works,[0],[0]
"In geometry, det(L) is the volume of the parallelepiped formed by vectors in the feature space associated with kernel k. Vectors that result in a larger volume are considered to be more “diverse”.",2. Related Works,[0],[0]
"Since volume depends on all vectors simultaneously, DPP is able to measure diversity in a global way.",2. Related Works,[0],[0]
The drawback of DPP lies in its sensitivity to the scaling of vectors.,2. Related Works,[0],[0]
"The volume increases with the `2 norm of vectors, but “diversity” does not.",2. Related Works,[0],[0]
Malkin & Bilmes (2008) propose to promote diversity by maximizing the determinant of vectors’ covariance matrix.,2. Related Works,[0],[0]
"Similar to DPP, this regularizer is sensitive to vector scaling.
",2. Related Works,[0],[0]
"Unlike the aforementioned regularizers which are defined directly on weight vectors, Cogswell et al. (2015) design a
regularizer on hidden activations in the neural network and influence the parameters indirectly.",2. Related Works,[0],[0]
"The number of hidden activations could be much larger than that of weight parameters (like in a convolutional neural network), which may render this regularizer to be computationally inefficient.",2. Related Works,[0],[0]
"In this section, we develop a uniform eigenvalue regularizer and apply it to promote “diversity” in two LSMs.",3. Method,[0],[0]
A latent space model (LSM) is equipped with a set of m components and each component is represented with a vector a ∈ Rd.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"To achieve broader coverage of infrequent patterns and reduce model size without sacrificing modeling power, previous works (Zou & Adams, 2012; Xie et al., 2015) propose to “diversify” the components by imposing a regularizer over them.
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"As a subjective concept, “diversity” has been defined in various ways as reviewed in Section 2.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"In this paper, we define a new measure of “diversity” by taking two factors into consideration: uncorrelation and evenness.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
Uncorrelation is a measure of how uncorrelated the components are.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Literally, less correlation is equivalent to more diversity.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Evenness is borrowed from biological diversity (Magurran, 2013), which measures how equally important different species are in maintaining the ecological balance within an ecosystem.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"If no species dominates another, the ecosystem is deemed as more diverse.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Likewise, in latent space modeling, we desire the components to play equally important roles and no one dominates another, such that each component contributes significantly to the modeling of data.
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
We characterize the uncorrelation among components from a statistical perspective: treating the components as random variables and measuring their covariance which is proportional to their correlation.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
Let A ∈ Rd×m denote the component matrix where in the k-th column is the parameter vector ak of component k.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Alternatively, we can take a row view (Figure 1(b)) of A: each component is treated as a random variable and each row vector ã>i can be seen as a sample drawn from the random vector formed by the m components.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
Let µ = 1d ∑d i=1,3.1. Uniform Eigenvalue Regularizer,[0],[0]
ãi = 1 dA,3.1. Uniform Eigenvalue Regularizer,[0],[0]
">1 be the sample mean, where the elements of 1 ∈ Rd are all 1.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"We compute the empirical covariance matrix of the components as
G = 1d ∑d i=1(ãi − µ)(ãi − µ)>
= 1dA >A− ( 1dA >1)( 1dA >1)>
(1)
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Imposing the constraint A>1 = 0, we have G = 1dA >A.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Suppose A is a full rank matrix and m < d, then G is a full-rank matrix with rank m.
For the next step, we show that the eigenvalues of G play important roles in characterizing the uncorrelation and evenness of components.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
We start with uncorrelation.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
Let G = ∑m k=1,3.1. Uniform Eigenvalue Regularizer,[0],[0]
λkuku > k be the eigendecomposition where λk is an eigenvalue and uk is the associated eigenvector.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"As is well known in Principle Component Analysis (Jolliffe, 2002), an eigenvector uk of the covariance matrix G represents a principal direction of the data points and the associated eigenvalue λk tells the variability of points along that direction.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"As shown in Figure 2(a), the larger λk is, the more spread out the points along the direction uk.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"When the eigenvectors (principal directions) are not aligned with coordinate axis (as shown in Figure 2), the level of disparity among eigenvalues indicates the level of correlation among the m components (random variables).",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"The more different the eigenvalues are, the higher the correlation is.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"As shown in Figure 2(a), λ1 is about three times larger than λ2 and there is a high correlation along the direction u1.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"On the other hand, in Figure 2(b), the two eigenvalues are close to each other and the points evenly spread out in both directions with negligible correlation.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"In light of this, we would utilize the uniformity among eigenvalues of G to measure how uncorrelated the components are.
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Secondly, we relate the eigenvalues with the other factor of diversity: evenness.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"When the eigenvectors are aligned with the coordinate axis (as shown in Figure 3(a)), the components are uncorrelated.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"In this case, we bring in evenness to measure diversity.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"As stated earlier, we first need to assign each component an importance score.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Since the eigenvectors are in parallel to the coordinate axis, the eigenvalues reflect the variance of components.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Analogous to PCA which posits that random variables with larger variance are
more important, we use variance to measure importance.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"As shown in Figure 3(a), component 1 has a larger eigenvalue λ1 and accordingly larger variability, hence is more important than component 2.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"According to the evenness criteria, the components are more diverse if their importance match, which motivates us to encourage the eigenvalues to be uniform.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"As shown in Figure 3(b), the two eigenvalues are close and the two components have roughly the same variability, hence are similarly important.
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"To sum up, we desire to encourage the eigenvalues to be even in both cases: (1) when the eigenvectors are not aligned with the coordinate axis, they are preferred to be even to reduce the correlation of components; (2) when the eigenvectors are aligned with the coordinate axis, they are encouraged to be even such that different components contribute equally in modeling data.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Previously, encouraging evenness among variances (eigenvalues) is investigated in other problems, such as learning compact representations for efficient hashing (Kong & Li, 2012; Ge et al., 2013).
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Next, we discuss how to promote uniformity among eigenvalues.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
The basic idea is: we normalize the eigenvalues into a probability simplex and encourage the discrete distribution parameterized by the normalized eigenvalues to have small Kullback-Leibler (KL) divergence with the uniform distribution.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Given the eigenvalues {λk}mk=1, we first normalize them into a probability simplex λ̂k = λk∑m
j=1 λj based on which we define a
distribution on a discrete random variable X = 1, · · · ,m where p(X = k) = λ̂k.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"In addition, to guarantee the eigenvalues are strictly positive, we require A>A to be positive definite.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"To encourage {λ̂k}mk=1 to be uniform, we encourage the distribution p(X) to be “close” to a uniform distribution q(X = k) = 1m , where the “closeness” is measured using KL divergence KL(p||q):∑m k=1",3.1. Uniform Eigenvalue Regularizer,[0],[0]
λ̂k log λ̂k 1/m = ∑m k=1 λk log λk∑m j=1,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"λj −log ∑m j=1 λj+logm.
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"In this equation, ∑m k=1 λk log λk is equivalent to tr(( 1dA >A)log( 1dA
>A)), where log(·) denotes matrix logarithm.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"To show this, note that log( 1dA
>A) =∑m k=1 log(λk)uku > k , according to the property of matrix logarithm.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Then we have tr(( 1dA >A) log( 1dA
>A)) equals to tr(( ∑m k=1 λkuku > k )",3.1. Uniform Eigenvalue Regularizer,[0],[0]
( ∑m k=1 log(λk)uku > k )),3.1. Uniform Eigenvalue Regularizer,[0],[0]
"which
equals to ∑m",3.1. Uniform Eigenvalue Regularizer,[0],[0]
k=1,3.1. Uniform Eigenvalue Regularizer,[0],[0]
λk log λk.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"According to the property of trace, we have tr( 1dA >A) =",3.1. Uniform Eigenvalue Regularizer,[0],[0]
∑m k=1 λk.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Then the KL divergence can be turned into a diversity-promoting uniform eigenvalue regularizer (UER):
tr(( 1dA >A) log( 1dA >A))
tr( 1dA >A)
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"− log tr(1 d A>A) (2)
subject to A>A 0 and A>1 = 0.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Compared with previous diversity-promoting regularizers, UER has the following benefits: (1) It measures the diversity of all components in a holistic way, rather than reducing to pairwise
dissimilarities as other regularizers (Yu et al., 2011; Bao et al., 2013; Xie et al., 2015) do.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
This enables UER to capture global relations among components.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"(2) Unlike determinant-based regularizers (Malkin & Bilmes, 2008; Zou & Adams, 2012) that are sensitive to vector scaling, UER is derived from normalized eigenvalues where the normalization effectively removes scaling.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
(3) UER is amenable for computation.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"First, unlike DoCev (Cogswell et al., 2015) that is defined over data-dependent intermediate variables incurring computational inefficiency, UER is directly defined on model parameters independent of data.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Second, unlike the regularizers proposed in (Bao et al., 2013; Xie et al., 2015) that are non-smooth, UER is a smooth function.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
The dominating computation in UER is the matrix logarithm.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"It does not substantially increase computational overhead as long as the number of components is not too large (e.g., less than 1000).
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
We apply UER to promote diversity in LSMs.,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Let L(A) denote the objective function of an LSM, then an UEregularized LSM problem can be defined as
minA L(A) + λ",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"( tr(( 1dA >A) log( 1dA >A))
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"tr( 1dA >A)
− log tr( 1dA >A))
s.t. A>1 = 0, A>A 0
where λ is the regularization parameter.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Similar to other diversity-promoting regularizers, UER is non-convex.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Since L(A) in most LSMs is non-convex, adding UER does not substantially increase difficulty for optimization.
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
Connection with von Neumann Entropy,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"In this section, we make a connection between UER and von Neumann entropy.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"A matrix M is referred to as a density matrix (Bengtsson & Zyczkowski, 2007) if its eigenvalues are strictly positive and sum to one, equivalently, M 0 and tr(M) = 1.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"The von Neumann entropy (Bengtsson & Zyczkowski, 2007) of M is defined as S(M) = −tr(M log M), which is essentially the Shannon entropy of its eigenvalues.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"If the covariance matrix G of components is a density matrix, then we can use its von Neumann entropy to define a UER.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"To encourage the eigenvalues {λk}mk=1 of G to be even, we directly encourage the KL divergence between the distribution parameterized by the eigenvalues (without normalization) and the uniform distribution to be small: ∑m k=1 λk log λk 1/m = ∑m k=1",3.1. Uniform Eigenvalue Regularizer,[0],[0]
λk log,3.1. Uniform Eigenvalue Regularizer,[0],[0]
"λk+ logm, which is equivalent to encouraging the Shannon entropy of the eigenvalues",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"− ∑m k=1 λk log λk, i.e., the von Neumann entropy of G to be large.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Then a new UER can be defined as the negative von Neumann entropy of G: tr(( 1dA >A) log( 1dA >A)), subject to the constraints: (1) A>A 0; (2) tr( 1dA >A) = 1; (3) A>1 = 0.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"This new UER is a special case of the previous one (Eq.(2)).
",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Connection with von Neumann Divergence Next we make a connection between the UER and von Neumann divergence (Kulis et al., 2009).",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Given two positive defi-
nite matrices X and Y, their von Neumann divergence is defined as tr(X log X −X log Y −X + Y), which measures the closeness between the two matrices.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Given two vectors x,y ∈ Rm, their generalized KL divergence can be defined as ∑m k=1 xk log( xk yk
)",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"− (xk − yk), which measures the closeness between two vectors.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"To encourage uniformity among the eigenvalues of the covariance matrix G, we can decrease the generalized KL divergence between these eigenvalues and an all-1 vector:∑m
k=1 λk log( λk 1 )",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"− (λk − 1)
= tr(( 1dA >A) log( 1dA >A))−",3.1. Uniform Eigenvalue Regularizer,[0],[0]
tr( 1dA >A)),3.1. Uniform Eigenvalue Regularizer,[0],[0]
"+m
(3)
which is the von Neumann divergence between G and an identity matrix.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"Hence, encouraging uniformity among eigenvalues can be achieved by making G to be close to an identity matrix based on the von Neumann divergence.",3.1. Uniform Eigenvalue Regularizer,[0],[0]
"In this section, we apply the uniform eigenvalue regularizer to promote diversity in two latent space models: DML and LSTM.",3.2. Case Studies,[0],[0]
"We also applied it to latent Dirichlet allocation (Blei et al., 2003) and classifier ensemble (Yu et al., 2011).",3.2. Case Studies,[0],[0]
"Due to space limit, the results of the latter two are deferred to the supplements.
",3.2. Case Studies,[0],[0]
Distance Metric Learning (DML),3.2. Case Studies,[0],[0]
"Given data pairs either labeled as “similar” or “dissimilar”, DML (Xing et al., 2002; Davis et al., 2007; Guillaumin et al., 2009) aims to learn a distance metric under which similar pairs would be placed close to each other and dissimilar pairs are separated apart.",3.2. Case Studies,[0],[0]
"The learned distance can benefit a wide range of tasks, including retrieval, clustering and classification.",3.2. Case Studies,[0],[0]
"Following (Weinberger & Saul, 2009), we define the distance metric between x,y ∈ Rd as ‖A>x−A>y‖22 where A ∈ Rd×m is a parameter matrix whose column vectors are components.",3.2. Case Studies,[0],[0]
"Built upon the DML formulation in (Xie, 2015), an uniform-eigenvalue regularized DML (DML-UE) problem can be formulated as
minA ∑
(x,y)∈S ‖A>x−A>y‖22 + ∑
(x,y)∈D max(0, 1− ‖A>x−A>y‖22)
+λ( tr(( 1dA >A) log( 1dA >A))
",3.2. Case Studies,[0],[0]
"tr( 1dA >A)
− log tr( 1dA >A))
s.t. A>1 = 0, A>A 0 (4) where S and D are the set of similar and dissimilar pairs respectively.",3.2. Case Studies,[0],[0]
The first and second term in the objective function encourage similar pairs to have small distance and dissimilar pairs to have large distance respectively.,3.2. Case Studies,[0],[0]
"The learned metrics are applied for information retrieval.
",3.2. Case Studies,[0],[0]
"Long Short-Term Memory (LSTM) Network LSTM (Hochreiter & Schmidhuber, 1997) is a type of recurrent neural network, that is better at capturing long-term dependency in sequential modeling.",3.2. Case Studies,[0],[0]
"At each time step t where
the input is xt, there is an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht.",3.2. Case Studies,[0],[0]
"The transition equations among them are
it = σ(W (i)xt + U (i)ht−1 + b",3.2. Case Studies,[0],[0]
(i)),3.2. Case Studies,[0],[0]
ft = σ(W (f)xt + U (f)ht−1 + b (f)),3.2. Case Studies,[0],[0]
ot = σ(W (o)xt + U (o)ht−1 + b (o)),3.2. Case Studies,[0],[0]
ct = it tanh(W(c)xt + U(c)ht−1 + b(c)),3.2. Case Studies,[0],[0]
"+ ft ct−1 ht = ot tanh(ct)
where W = {W(s)|s ∈ S = {i, f, o, c}} and U = {U(s)|s ∈ S} are gate-specific weight matrices and B",3.2. Case Studies,[0],[0]
= {b(s)|s ∈ S} are bias vectors.,3.2. Case Studies,[0],[0]
The row vectors in W and U are treated as components.,3.2. Case Studies,[0],[0]
"Let L(W,U ,B) denote the loss function of an LSTM network and R(·) denote the UER (including constraints), then a UE-regularized LSTM problem can be defined as
minW,U,B L(W,U ,B) + λ ∑ s∈S(R(W(s))",3.2. Case Studies,[0],[0]
"+R(U(s)))
(5) The LSTM network is applied for cloze-style reading comprehension (CSRC).",3.2. Case Studies,[0],[0]
"The network architecture follows that in (Seo et al., 2017), which achieves the state of the art performance on CSRC.",3.2. Case Studies,[0],[0]
We develop a projected gradient descent (PGD) algorithm to solve the UE-regularized LSM problem in Eq.(5).,3.3. Algorithm,[0],[0]
"The constraint A>A 0 ensures the eigenvalues of A>A are positive, such that log(A>A) is well-defined.",3.3. Algorithm,[0],[0]
"However, it makes optimization very nasty.",3.3. Algorithm,[0],[0]
"To address this issue, we add a small perturbation I over A>A where is a close-to-zero positive scalar and I is an identity matrix, to ensure log(A>A + I) is always well-defined.",3.3. Algorithm,[0],[0]
"Accordingly, the constraint A>A 0 can be eliminated.",3.3. Algorithm,[0],[0]
The PGD algorithm iteratively performs three steps: (1) compute (sub)gradient 4A of the objective function; (2) update A using gradient descent:,3.3. Algorithm,[0],[0]
Ã ← A − η 4 A; (3) project Ã to the constraint set {A|A>1 = 0}.,3.3. Algorithm,[0],[0]
"In step (1), the derivative of tr(( 1dA >A + I) log( 1dA >",3.3. Algorithm,[0],[0]
A + I)) is 2dA(log( 1 dA >,3.3. Algorithm,[0],[0]
A + I) + I).,3.3. Algorithm,[0],[0]
To compute the logarithm of 1dA >,3.3. Algorithm,[0],[0]
"A + I, we perform an eigen-decomposition of this matrix into UΛU>, transform Λ into another diagonal matrix Λ̃",3.3. Algorithm,[0],[0]
"where Λ̃jj = log(Λjj) and then compute log( 1dA
>A + I) as UΛ̃U>.",3.3. Algorithm,[0],[0]
The complexity of eigendecomposing this m-by-m matrix is O(m3).,3.3. Algorithm,[0],[0]
"In our applications, m is no more than 500, so O(m3) is not a big bottleneck.",3.3. Algorithm,[0],[0]
"In addition, this matrix is symmetric and the symmetry can be leveraged for fast eigen-decomposition.",3.3. Algorithm,[0],[0]
"In implementation, we use the MAGMA library that supports efficient eigen-decomposition of symmetric matrices on both CPUs and GPUs.",3.3. Algorithm,[0],[0]
"In step (3), the projection operation amounts to solving the following problem:",3.3. Algorithm,[0],[0]
minA 12‖A,3.3. Algorithm,[0],[0]
"− Ã‖ 2 F subject to A
>1 = 0.",3.3. Algorithm,[0],[0]
"According to KKT conditions (Boyd & Vandenberghe, 2004), we have
A− Ã + 1λ> = 0 and A>1 = 0.",3.3. Algorithm,[0],[0]
"Solving this system of equations, we get A = (I − 1d11
>)",3.3. Algorithm,[0],[0]
"Ã, which centers the row vectors in Ã to have zero mean.",3.3. Algorithm,[0],[0]
"In this section, we present experimental results.
",4. Experiments,[0],[0]
"Dataset We used five datasets in the experiments: an electronic health record dataset MIMIC-III (Johnson et al., 2016); two image datasets Stanford-Cars (Krause et al., 2013) and Caltech-UCSD-Birds (Welinder et al., 2010); two question answering (QA) datasets CNN and DailyMail (Hermann et al., 2015).",4. Experiments,[0],[0]
The first three were used for DML and the last two for LSTM.,4. Experiments,[0],[0]
Their statistics are summarized in Table 1.,4. Experiments,[0],[0]
MIMIC-III contains hospital admissions of patients.,4. Experiments,[0],[0]
The class label of each admission is the primarily diagnosed disease.,4. Experiments,[0],[0]
"For Stanford-Cars, CNN and DailyMail, we use a single train/test split specified by the data providers; for the other two, five random splits are performed and the results are averaged over the five runs.",4. Experiments,[0],[0]
"For the MIMIC-III dataset, we extract 7207-dimensional features: (1) 2 dimensions from demographics, including age and gender; (2) 5300 dimensions from clinical notes, including 5000-dimensional bag-of-words (weighted using tf-idf) and 300-dimensional Word2Vec (Mikolov et al., 2013); (3) 1905-dimensions from lab tests where the zeroorder, first-order and second-order temporal features are extracted for each of the 635 lab items.",4. Experiments,[0],[0]
"For bag-of-words, we remove stop words, then select the 5000 words with largest document frequency.",4. Experiments,[0],[0]
"For Word2Vec, we train 300- dimensional embeddings for each word; to represent a document, we average the embeddings of all words in this document.",4. Experiments,[0],[0]
"For the two image datasets, we use the VGG16 (Simonyan & Zisserman, 2014) convolutional neural network trained on the ImageNet (Deng et al., 2009) dataset to extract features, which are the 4096-dimensional outputs of the second fully-connected layer.",4. Experiments,[0],[0]
"In the two QA datasets, each instance consists of a passage, a question and an answer.",4. Experiments,[0],[0]
"The question is a cloze-style task where an entity is replaced by a placeholder and the goal is to infer this missing entity (answer) from all the possible entities appearing in the passage.
",4. Experiments,[0],[0]
"Experimental Setup In DML experiments, two samples are labeled as similar if belonging to the same class and dissimilar otherwise.",4. Experiments,[0],[0]
"The learned distance metrics are ap-
plied for retrieval whose performance is evaluated using precision@K. We compare with two sets of regularizers: (1) diversity-promoting regularizers based on determinant of covariance (DC) (Malkin & Bilmes, 2008), cosine similarity (CS) (Yu et al., 2011), determinantal point process (DPP) (Kulesza & Taskar, 2012; Zou & Adams, 2012), InCoherence (IC) (Bao et al., 2013), mutual angles (MA) (Xie et al., 2015), and decorrelation (DeCov) (Cogswell et al., 2015); (2) regularizers that are designed for other purposes, including L2 norm for small norm, L1 norm for sparsity, low-rankness (Recht et al., 2010) and Dropout (Srivastava et al., 2014).",4. Experiments,[0],[0]
All these regularizers are applied to the same DML formulation (Eq.(4) without the regularizer).,4. Experiments,[0],[0]
"In addition, we compare with vanilla Euclidean distance (EUC) and other distance learning methods including information theoretic metric learning (ITML)",4. Experiments,[0],[0]
"(Davis et al., 2007), logistic discriminant metric learning (LDML) (Guillaumin et al., 2009), and geometric mean metric learning (GMML) (Zadeh et al., 2016).",4. Experiments,[0],[0]
"We use 5- fold cross validation to tune the regularization parameter in {10−5, 10−4, · · · , 105} and the number of components in {50, 100, 200, · · · , 500}.",4. Experiments,[0],[0]
"The best tuned regularization parameters of UER are: 0.001 for MIMIC, 0.01 for Cars and Birds.",4. Experiments,[0],[0]
"The best tuned component numbers are: 200 for MIMIC, 100 for Cars and 200 for Birds.",4. Experiments,[0],[0]
"The learning rate of the PGD algorithm is set to 0.001.
",4. Experiments,[0],[0]
"In LSTM experiments, the model architecture and experimental settings follow the Bidirectional Attention Flow (BIDAF) (Seo et al., 2017) model, which consists of the following layers: character embedding, word embedding, contextual embedding, attention flow, modeling and output.",4. Experiments,[0],[0]
"The contextual and modeling layers use long shortterm memory (LSTM) networks (Seo et al., 2017).",4. Experiments,[0],[0]
"In char-
acter embedding based on convolutional neural network, 100 1D filters are used, each with a width of 5.",4. Experiments,[0],[0]
The hidden state size is set to 100.,4. Experiments,[0],[0]
"AdaDelta (Zeiler, 2012) is used for optimization with a minibatch size of 48.",4. Experiments,[0],[0]
"Dropout (Srivastava et al., 2014) with probability 0.2 is used for all LSTM layers.",4. Experiments,[0],[0]
The model is trained for 8 epochs with early stop when the validation accuracy starts to drop.,4. Experiments,[0],[0]
"We compare UER with other diversity-promoting regularizers including DC, CS, DPP, IC, MA and DeCov.
",4. Experiments,[0],[0]
"Results Table 2 shows the retrieval precision (K = 10) on three datasets, where we observe: (1) DML-UE achieves much better precision than DML, proving that UER is an effective regularizer in improving generalization performance; (2) UER outperforms other diversity-promoting regularizers possibly due to its capability to capture global relations among all components and insensitivity to vector scaling; (3) diversity-promoting regularizers perform better than other types of regularizers such as L2, L1, low rank and Dropout, corroborating the efficacy of inducing diversity; (4) DML-UE outperforms other popular distance learning methods such as ITML, LDML and GMML.
",4. Experiments,[0],[0]
Table 3 shows the number of components that achieves the precision in Table 2.,4. Experiments,[0],[0]
"Compared with DML, DMLUE uses much fewer components to achieve better precision.",4. Experiments,[0],[0]
"For example, on the Cars dataset, DML-UE achieves 58.2% precision with 100 components.",4. Experiments,[0],[0]
"In contrast, with more components (300), DML achieves a much lower precision (53.1%).",4. Experiments,[0],[0]
"This demonstrates that by encouraging the components to be diverse, UER is able to reduce model size without sacrificing modeling power.",4. Experiments,[0],[0]
UER encourages equal “importance” among components such that each component plays a significant role in modeling data.,4. Experiments,[0],[0]
"As a result, it suffices to use a small number of components to achieve larger modeling power.",4. Experiments,[0],[0]
"Compared with other diversity-promoting regularizers, UER achieves better precision with fewer components, demonstrating its ability to better promote diversity.
",4. Experiments,[0],[0]
"Next, we verify whether “diversifying” the components in DML can better capture infrequent patterns.",4. Experiments,[0],[0]
"In the MIMICIII dataset, we consider diseases as patterns and consider a disease as “frequent” if more than 1000 hospital admissions are diagnosed with this disease and “infrequent” if otherwise.",4. Experiments,[0],[0]
Table 4 shows the retrieval precision on frequent diseases and infrequent diseases.,4. Experiments,[0],[0]
"As can be seen, compared with the baselines, DML-UE achieves more improvement on infrequent diseases than on frequent diseases.",4. Experiments,[0],[0]
"This indicates that by encouraging the components to diversely spread out, UER is able to better capture infrequent patterns (diseases in this case) without compromising the performance on frequent patterns.",4. Experiments,[0],[0]
"On infrequent diseases, DMLUE outperforms other diversity-promoting methods, showing the advantage of UER over other diversity-promoting regularizers.",4. Experiments,[0],[0]
"To further verify this, we select 3 most frequent diseases (hypertension, AFib, CAD) and randomly select 5 infrequent ones (helicobacter pylori, acute cholecystitis, joint pain-shlder, dysarthria, pressure ulcer), and show the precision@10 on each individual disease in Table 5.",4. Experiments,[0],[0]
"As can be seen, on the five infrequent diseases, DML-UE achieves higher precision than baselines while on the three frequent diseases, DML-UE achieves comparable precision.
",4. Experiments,[0],[0]
We empirically verify whether UER can promote uncorrelation and evenness.,4. Experiments,[0],[0]
"Givenm component vectors, we compute the empirical correlation (cosine similarity) of every two vectors, then average these pairwise correlation scores to measure the overall correlation of m vectors.",4. Experiments,[0],[0]
"We perform the study by learning distance metrics that have 200 components, on the MIMIC-III dataset.",4. Experiments,[0],[0]
The average correlation under unregularized DML and DML-UE is 0.73 and 0.57 respectively.,4. Experiments,[0],[0]
"This shows that UER can reduce corre-
lation.",4. Experiments,[0],[0]
"To measure evenness, we first measure the “importance” of components.",4. Experiments,[0],[0]
"For each component with parameter vector a, we project the training examples {xi}Ni=1 onto a: {x>i a}Ni=1, then use the variance of {x>i a}Ni=1 to measure the importance of this component.",4. Experiments,[0],[0]
"After that, we map these importance scores into a probabilistic simplex using softmax.",4. Experiments,[0],[0]
"Finally, the evenness is measured by the KL divergence between the discrete distribution parameterized by these probabilities and a uniform distribution.",4. Experiments,[0],[0]
A smaller KL divergence indicates larger evenness.,4. Experiments,[0],[0]
"On MIMIC-III with 200 components, the KL divergence under unregularized DML and DML-UE is 3.54 and 2.92 respectively.",4. Experiments,[0],[0]
"This suggests that our regularizer is able to encourage evenness.
",4. Experiments,[0],[0]
Table 6 shows the runtime taken by DML methods to reach convergence.,4. Experiments,[0],[0]
"Compared with unregularized DML, DMLUE does not increase the training time substantially.",4. Experiments,[0],[0]
"The relative increase is 11.2% on MIMIC, 15.4% on Cars and 13.9% on Birds.",4. Experiments,[0],[0]
"The runtime of DML-UE is close to DML regularized by other diversity-promoting regularizers.
",4. Experiments,[0],[0]
"In the LSTM experiments, Table 7 shows state of the art accuracy on the two QA datasets.",4. Experiments,[0],[0]
"Compared with the original BIDAF (Seo et al., 2017), our method BIDAF-UE achieves better accuracy, further demonstrating UER’s ability to improve generalization performance.",4. Experiments,[0],[0]
"Besides, UER outperforms other regularizers.",4. Experiments,[0],[0]
We propose a new diversity-promoting regularizer from the perspectives of uncorrelation which prefers the components in LSMs to be uncorrelated and evenness which encourages the components to contribute equally to the modeling of data.,5. Conclusions,[0],[0]
"Gaining insight from PCA, promoting uncorrelation and evenness both amount to encouraging the covariance matrix of components to have uniform eigenvalues, which leads to a uniform eigenvalue regularizer (UER).",5. Conclusions,[0],[0]
The UER is applied to DML and LSTM.,5. Conclusions,[0],[0]
"Experimental studies reveal that UER greatly boosts the performance of LSMs, better captures infrequent patterns, reduces model size without compromising modeling power and outperforms other diversity-promoting regularizers.",5. Conclusions,[0],[0]
We would like to thank the anonymous reviewers for the helpful suggestions and comments.,Acknowledgements,[0],[0]
"P.X and E.X are supported by National Institutes of Health P30DA035778, R01GM114311, National Science Foundation IIS1617583, DARPA FA872105C0003 and Pennsylvania Department of Health BD4BH4100070287.",Acknowledgements,[0],[0]
Latent space models (LSMs) provide a principled and effective way to extract hidden patterns from observed data.,abstractText,[0],[0]
"To cope with two challenges in LSMs: (1) how to capture infrequent patterns when pattern frequency is imbalanced and (2) how to reduce model size without sacrificing their expressiveness, several studies have been proposed to “diversify” LSMs, which design regularizers to encourage the components therein to be “diverse”.",abstractText,[0],[0]
"In light of the limitations of existing approaches, we design a new diversitypromoting regularizer by considering two factors: uncorrelation and evenness, which encourage the components to be uncorrelated and to play equally important roles in modeling data.",abstractText,[0],[0]
"Formally, this amounts to encouraging the covariance matrix of the components to have more uniform eigenvalues.",abstractText,[0],[0]
We apply the regularizer to two LSMs and develop an efficient optimization algorithm.,abstractText,[0],[0]
"Experiments on healthcare, image and text data demonstrate the effectiveness of the regularizer.",abstractText,[0],[0]
Uncorrelation and Evenness: a New Diversity-Promoting Regularizer,title,[0],[0]
"Keywords. Hawkes Process, Causality Inference, Cumulants, Generalized Method of Moments",text,[0],[0]
"In many applications, one needs to deal with data containing a very large number of irregular timestamped events that are recorded in continuous time.",1 Introduction,[0],[0]
"These events can reflect, for instance, the activity of users on a social network, see Subrahmanian et al. (2016), the high-frequency variations of signals in finance, see Bacry et al. (2015), the earthquakes and aftershocks in geophysics, see Ogata (1998), the crime activity, see Mohler et al. (2011) or the position of genes in genomics, see Reynaud-Bouret and Schbath (2010).",1 Introduction,[0],[0]
The succession of the precise timestamps carries a great deal of information about the dynamics of the underlying systems.,1 Introduction,[0],[0]
"In this context, multidimensional counting processes based models play a paramount role.",1 Introduction,[0],[0]
"Within this framework, an important task is to recover the mutual influence of the nodes (i.e., the different components of the counting process), by leveraging on their timestamp patterns, see, for instance, Bacry and Muzy (2016); Lemonnier and Vayatis (2014); Lewis and Mohler (2011); Zhou et al. (2013a); Gomez-Rodriguez et al. (2013); Farajtabar et al. (2015); Xu et al. (2016).
",1 Introduction,[0],[0]
"Consider a set of nodes I = {1, . . .",1 Introduction,[0],[0]
", d}.",1 Introduction,[0],[0]
For each i ∈,1 Introduction,[0],[0]
"I , we observe a set Zi of events, where each τ ∈ Zi labels the occurrence time of an event related to the activity of i.",1 Introduction,[0],[0]
"The events of all nodes can ∗massil.achab@m4x.org
ar X
iv :1
60 7.
06 33
3v 3
[ st
at .M
L ]
3 0
M ay
be represented as a vector of counting processes N t",1 Introduction,[0],[0]
=,1 Introduction,[0],[0]
"[N1t · · ·Ndt ]>, where N it counts the number of events of node i until time t ∈ R+, namely N it = ∑ τ∈Zi 1{τ≤t}.",1 Introduction,[0],[0]
The vector of stochastic intensities λt =,1 Introduction,[0],[0]
"[λ 1 t · · ·λdt ]> associated with the multivariate counting processN t is defined as
λit = lim dt→0 P(N it+dt −N it = 1|Ft) dt
for i ∈",1 Introduction,[0],[0]
"I , where the filtration Ft encodes the information available up to time t.",1 Introduction,[0],[0]
"The coordinate λit gives the expected instantaneous rate of event occurrence at time t for node i. The vector λt characterizes the distribution of N t, see Daley and Vere-Jones (2003), and patterns in the events time-series can be captured by structuring these intensities.
",1 Introduction,[0],[0]
"The Hawkes process introduced in Hawkes (1971) corresponds to an autoregressive structure of the intensities in order to capture self-excitation and cross-excitation of nodes, which is a phenomenon typically observed, for instance, in social networks, see for instance Crane and Sornette (2008).",1 Introduction,[0],[0]
"Namely, N t is called a Hawkes point process if the stochastic intensities can be written as
λit = µ i + d∑ j=1 ∫ t 0",1 Introduction,[0],[0]
φij(t−,1 Introduction,[0],[0]
"t′)dN jt′ ,
where µi ∈ R+ is an exogenous intensity and φij are positive, integrable and causal (with support in R+) functions called kernels encoding the impact of an action by node j on the activity of node i. Note that when all kernels are zero, the process is a simple homogeneous multivariate Poisson process.
",1 Introduction,[0],[0]
Most of the litterature uses a parametric approach for estimating the kernels.,1 Introduction,[0],[0]
"With no doubt, the most popular parametrization form is the exponential kernel φij(t)",1 Introduction,[0],[0]
"= αijβije−βijt because it definitely simplifies the inference algorithm (e.g., the complexity needed for computing the likelihood is much smaller).",1 Introduction,[0],[0]
"When d is large, in order to reduce the number of parameters, some authors choose to arbitrarily share the kernel shapes across the different nodes.",1 Introduction,[0],[0]
"Thus, for instance, in Yang and Zha (2013); Zhou et al. (2013b); Farajtabar et al. (2015), they choose φij(t) = αijh(t) with αij ∈ R+ quantifies the intensity of the influence of j on i and h(t) a (normalized) function that characterizes the time-profile of this influence and that is shared by all couples of nodes (i, j) (most often, it is chosen to be either exponential h(t) = βe−βt or power law h(t) = βt−(β+1)).",1 Introduction,[0],[0]
"Both approaches are, most of the time, highly non-realistic.",1 Introduction,[0],[0]
"On the one hand there is a priori no reason for assuming that the time-profile of the influence of a node j on a node i does not depend on the pair (i, j).",1 Introduction,[0],[0]
"On the other hand, assuming an exponential shape or a power law shape for a kernel arbitrarily imposes an event impact that is always instantly maximal and that can only decrease with time, while in practice, there may exist a latency between an event and its maximal impact.
",1 Introduction,[0],[0]
"In order to have more flexibility on the shape of the kernels, nonparametric estimation can be considered.",1 Introduction,[0],[0]
Expectation-Maximization algorithms can be found in Lewis and Mohler (2011) (for d = 1) or in Zhou et al. (2013a) (d > 1).,1 Introduction,[0],[0]
An alternative method is proposed in Bacry and Muzy (2016) where the nonparametric estimation is formulated as a numerical solving of a Wiener-Hopf equation.,1 Introduction,[0],[0]
"Another nonparametric strategy considers a decomposition of kernels on a dictionary of function h1, . . .",1 Introduction,[0],[0]
", hK , namely φij(t) = ∑K",1 Introduction,[0],[0]
k=1,1 Introduction,[0],[0]
"a ij k hk(t), where the coefficients a ij k are estimated, see Hansen et al. (2015); Lemonnier and Vayatis (2014) and Xu et al. (2016), where group-lasso is used to induce a sparsity pattern on the coefficients aijk that is shared across k = 1, . . .",1 Introduction,[0],[0]
",K.
Such methods are heavy when d is large, since they rely on likelihood maximization or least squares minimization within an over-parametrized space in order to gain flexibility on the shape of the kernels.",1 Introduction,[0],[0]
"This is problematic, since the original motivation for the use of Hawkes processes is to estimate the influence and causality of nodes, the knowledge of the full parametrization of the model being of little
interest for causality purpose.
",1 Introduction,[0],[0]
Our paper solves this problem with a different and more direct approach.,1 Introduction,[0],[0]
"Instead of trying to estimate the kernels φij , we focus on the direct estimation of their integrals.",1 Introduction,[0],[0]
"Namely, we want to estimate the matrixG =",1 Introduction,[0],[0]
"[gij ] where
gij = ∫",1 Introduction,[0],[0]
+∞ 0 φij(u),1 Introduction,[0],[0]
du ≥ 0 for 1 ≤,1 Introduction,[0],[0]
"i, j ≤ d. (1)
",1 Introduction,[0],[0]
"As it can be seen from the cluster representation of Hawkes processes (Hawkes and Oakes (1974)), this integral represents the mean total number of events of type i directly triggered by an event of type j, and then encodes a notion of causality.",1 Introduction,[0],[0]
"Actually, as detailed below (see Section 2.1), such integral can be related to the Granger causality (Granger (1969)).
",1 Introduction,[0],[0]
The main idea of the method we developed in this paper is to estimate the matrixG directly using a matching cumulants (or moments) method.,1 Introduction,[0],[0]
"Apart from the mean, we shall use second and third-order cumulants which correspond respectively to centered second and third-order moments.",1 Introduction,[0],[0]
We first compute an estimation M̂ of these centered moments M(G) (they are uniquely defined byG).,1 Introduction,[0],[0]
"Then, we look for a matrix Ĝ that minimizes the L2 error ‖M(Ĝ)− M̂‖2.",1 Introduction,[0],[0]
Thus the integral matrix Ĝ is directly estimated without making hardly any assumptions on the shape the involved kernels.,1 Introduction,[0],[0]
"As it will be shown, this approach turns out to be particularly robust to the kernel shapes, which is not the case of all previous Hawkes-based approaches that aim causality recovery.",1 Introduction,[0],[0]
"We call this method NPHC (Non Parametric Hawkes Cumulant), since our approach is of nonparametric nature.",1 Introduction,[0],[0]
We provide a theoretical analysis that proves the consistency of the NPHC estimator.,1 Introduction,[0],[0]
Our proof is based on ideas from the theory of Generalized Method of Moments (GMM) but requires an original technical trick since our setting strongly departs from the standard parametric statistics with i.i.d observations.,1 Introduction,[0],[0]
"Note that moment and cumulant matching techniques proved particularly powerful for latent topic models, in particular Latent Dirichlet Allocation, see Podosinnikova et al. (2015).",1 Introduction,[0],[0]
"A small set of previous works, namely Da Fonseca and Zaatour (2014); Aït-Sahalia et al. (2010), already used method of moments with Hawkes processes, but only in a parametric setting.",1 Introduction,[0],[0]
"Our work is the first to consider such an approach for a nonparametric counting processes framework.
",1 Introduction,[0],[0]
"The paper is organized as follows: in Section 2, we provide the background on the integrated kernels and the integrated cumulants of the Hawkes process.",1 Introduction,[0],[0]
"We then introduce the method, investigate its complexity and explain the consistency result we prove.",1 Introduction,[0],[0]
"In Section 3, we estimate the matrix of Hawkes kernels’ integrals for various simulated datasets and for real datasets, namely the MemeTracker database and financial order book data.",1 Introduction,[0],[0]
We then provide in Section 4 the technical details skipped in the previous parts and the proof of our consistency result.,1 Introduction,[0],[0]
Section 5 contains concluding remarks.,1 Introduction,[0],[0]
"In this Section, we provide the background on integrals of Hawkes kernels and integrals of Hawkes cumulants.",2 NPHC: The Non Parametric Hawkes Cumulant method,[0],[0]
We then explain how the NPHC method enables estimatingG.,2 NPHC: The Non Parametric Hawkes Cumulant method,[0],[0]
"From the definition of Hawkes process as a Poisson cluster process, see Jovanović et al. (2015) or Hawkes and Oakes (1974), gij can be simply interpreted as the average total number of events of node i whose direct ancestor is a given event of node j (by direct we mean that interactions mediated by any other intermediate event are not counted).",2.1 Branching structure and Granger causality,[0],[0]
"In that respect,G not only describes the mutual influences between
nodes, but it also quantifies their direct causal relationships.",2.1 Branching structure and Granger causality,[0],[0]
"Namely, introducing the counting function N i←jt that counts the number of events of i whose direct ancestor is an event of j, we know from Bacry et al. (2015) that
E[dN i←jt ]",2.1 Branching structure and Granger causality,[0],[0]
"= g ijE[dN jt ] = g ijΛjdt, (2)
where we introduced Λi as the intensity expectation, namely satisfying E[dN it ] = Λidt.",2.1 Branching structure and Granger causality,[0],[0]
"Note that Λi does not depend on time by stationarity ofN t, which is known to hold under the stability condition ‖G‖ < 1, where ‖G‖ stands for the spectral norm ofG.",2.1 Branching structure and Granger causality,[0],[0]
"In particular, this condition implies the non-singularity of Id −G.
Since the question of a real causality is too complex in general, most econometricians agreed on the simpler definition of Granger causality Granger (1969).",2.1 Branching structure and Granger causality,[0],[0]
Its mathematical formulation is a statistical hypothesis test: X causes Y in the sense of Granger causality if forecasting future values of Y is more successful while taking X past values into account.,2.1 Branching structure and Granger causality,[0],[0]
"In Eichler et al. (2016), it is shown that for N t a multivariate Hawkes process, N jt does not Granger-cause N i t w.r.t N t if and only if φ
ij(u) = 0 for u ∈ R+.",2.1 Branching structure and Granger causality,[0],[0]
"Since the kernels take positive values, the latter condition is equivalent to ∫∞ 0",2.1 Branching structure and Granger causality,[0],[0]
"φ
ij(u)du = 0.",2.1 Branching structure and Granger causality,[0],[0]
"In the following, we’ll refer to learning the kernels’ integrals as uncovering causality since each integral encodes the notion of Granger causality, and is also linked to the number of events directly caused from a node to another node, as described above at Eq.",2.1 Branching structure and Granger causality,[0],[0]
(2).,2.1 Branching structure and Granger causality,[0],[0]
A general formula for the integral of the cumulants of a multivariate Hawkes process is provided in Jovanović et al. (2015).,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"As explained below, for the purpose of our method, we only need to consider cumulants up to the third order.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
Given 1 ≤,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"i, j, k ≤ d, the first three integrated cumulants of the Hawkes process can be defined as follows thanks to stationarity:
Λidt = E(dN it ) (3)
Cijdt = ∫ τ∈R",2.2 Integrated cumulants of the Hawkes process,[0],[0]
( E(dN itdN j t+τ ),2.2 Integrated cumulants of the Hawkes process,[0],[0]
− E(dN it ),2.2 Integrated cumulants of the Hawkes process,[0],[0]
E(dN j t+τ ) ),2.2 Integrated cumulants of the Hawkes process,[0],[0]
"(4)
Kijkdt = ∫ ∫ τ,τ ′∈R2 ( E(dN itdN j t+τdN k t+τ ′) + 2E(dN i t )",2.2 Integrated cumulants of the Hawkes process,[0],[0]
E(dN j t+τ ),2.2 Integrated cumulants of the Hawkes process,[0],[0]
"E(dN k t+τ ′)
",2.2 Integrated cumulants of the Hawkes process,[0],[0]
− E(dN itdN j t+τ ),2.2 Integrated cumulants of the Hawkes process,[0],[0]
E(dN k t+τ ′)− E(dN itdNkt+τ ′)E(dN,2.2 Integrated cumulants of the Hawkes process,[0],[0]
j t+τ ),2.2 Integrated cumulants of the Hawkes process,[0],[0]
− E(dN j t+τdN k t+τ ′)E(dN,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"i t ) ) ,
(5)
",2.2 Integrated cumulants of the Hawkes process,[0],[0]
where Eq.,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"(3) is the mean intensity of the Hawkes process, the second-order cumulant (4) refers to the integrated covariance density matrix and the third-order cumulant (5) measures the skewness ofN t.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"Using the martingale representation from Bacry and Muzy (2016) or the Poisson cluster process representation from Jovanović et al. (2015), one can obtain an explicit relationship between these integrated cumulants and the matrixG.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"If one sets R = (Id −G)−1, (6) straightforward computations (see Section 4) lead to the following identities:
Λi = d∑
m=1
Rimµm (7)
Cij = d∑ m=1",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"ΛmRimRjm (8)
Kijk = d∑ m=1 (RimRjmCkm +RimCjmRkm + CimRjmRkm − 2ΛmRimRjmRkm).",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"(9)
Equations (8) and (9) are proved in Section 4.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
Our strategy is to use a convenient subset of Eqs.,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"(3), (4) and (5) to define M , while we use Eqs.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"(7), (8) and (9) in order to construct the operator that maps a candidate matrix R to the corresponding cumulants M(R).",2.2 Integrated cumulants of the Hawkes process,[0],[0]
By looking for R̂ that minimizes R 7→ ‖M(R),2.2 Integrated cumulants of the Hawkes process,[0],[0]
"− M̂‖2, we obtain, as illustrated below, good recovery of the ground truth matrix G using Equation (6).
",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"The simplest case d = 1 has been considered in Hardiman and Bouchaud (2014), where it is shown that one can choose M = {C11} in order to compute the kernel integral.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
Eq.,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"(8) then reduces to a simple second-order equation that has a unique solution inR (and consequently a uniqueG) that accounts for the stability condition (‖G‖ < 1).
",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"Unfortunately, for d > 1, the choice M = {Cij}1≤i≤j≤d is not sufficient to uniquely determine the kernels integrals.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"In fact, the integrated covariance matrix provides d(d+ 1)/2 independent coefficients, while d2 parameters are needed.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"It is straightforward to show that the remaining d(d− 1)/2 conditions can be encoded in an orthogonal matrixO, reflecting the fact that Eq. (8) is invariant under the change R→ OR, so that the system is under-determined.
",2.2 Integrated cumulants of the Hawkes process,[0],[0]
Our approach relies on using the third order cumulant tensorK,2.2 Integrated cumulants of the Hawkes process,[0],[0]
=,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"[Kijk] which contains (d3 + 3d2 + 2d)/6 > d2 independent coefficients that are sufficient to uniquely fix the matrixG. This can be justified intuitively as follows: while the integrated covariance only contains symmetric information, and is thus unable to provide causal information, the skewness given by the third order cumulant in the estimation procedure can break the symmetry between past and future so as to uniquely fixG. Thus, our algorithm consists of selecting d2 third-order cumulant components, namely M = {Kiij}1≤i,j≤d.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"In particular, we define the estimator ofR as R̂ ∈ argminRL(R), where
L(R) =",2.2 Integrated cumulants of the Hawkes process,[0],[0]
(1− κ)‖Kc(R)− K̂c‖22 + κ‖C(R)−,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"Ĉ‖22, (10)
where ‖ ·‖2 stands for the Frobenius norm,Kc = {Kiij}1≤i,j≤d is the matrix obtained by the contraction of the tensorK to d2 indices,C is the covariance matrix, while K̂c and Ĉ are their respective estimators, see Equations (12), (13) below.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"It is noteworthy that the above mean square error approach can be seen as a peculiar Generalized Method of Moments (GMM), see Hall (2005).",2.2 Integrated cumulants of the Hawkes process,[0],[0]
This framework allows us to determine the optimal weighting matrix involved in the loss function.,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"However, this approach is unusable in practice, since the associated complexity is too high.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"Indeed, since we have d2 parameters, this matrix has d4 coefficients and GMM calls for computing its inverse leading to a O(d6) complexity.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"In this work, we use the coefficient κ to scale the two terms, as
κ = ‖K̂c‖22
‖K̂c‖22 + ‖Ĉ‖22 ,
see Section 4.4 for an explanation about the link between κ and the weighting matrix.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"Finally, the estimator ofG is straightforwardly obtained as
Ĝ = Id − R̂ −1 ,
from the inversion of Eq.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
(6).,2.2 Integrated cumulants of the Hawkes process,[0],[0]
Let us mention an important point: the matrix inversion in the previous formula is not the bottleneck of the algorithm.,2.2 Integrated cumulants of the Hawkes process,[0],[0]
"Indeed, its has a complexity O(d3)",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"that is cheap compared to the computation of the cumulants when n = maxi |Zi| d, which is the typical scaling satisfied in applications.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"Solving the considered problem on a larger scale, say d 103, is an open question, even with state-of-the-art parametric and nonparametric approaches, see for instance Zhou et al. (2013a); Xu et al. (2016); Zhou et al. (2013b); Bacry and Muzy (2016), where the number of components d in experiments is always around 100 or smaller.",2.2 Integrated cumulants of the Hawkes process,[0],[0]
"Note that, actually, our approach leads to a much faster algorithm than the considered state-of-the-art baselines, see Tables 1–4 from Section 3 below.",2.2 Integrated cumulants of the Hawkes process,[0.9546988588695003],"['5 we observe that 3-Unfold-Normal yields a gain of 2.2 BLEU with respect to the original single system and a slight improvement in decoding speed at the same time.7 Networks with the size factor 1.0 like 3-Unfold-Normal are very likely to yield about the same decoding speed as the Single network regardless of the decoder implementation, machine learning framework, and hardware.']"
"In this section we present explicit formulas to estimate the three moment-based quantities listed in the previous section, namely, Λ, C and K. We first assume there exists H > 0 such that the truncation from (−∞,+∞) to [−H,H] of the domain of integration of the quantities appearing in Eqs.",2.3 Estimation of the integrated cumulants,[0],[0]
"(4) and (5), introduces only a small error.",2.3 Estimation of the integrated cumulants,[0],[0]
"In practice, this amounts to neglecting border effects in the covariance density and in the skewness density that is a good approximation if the support of the kernel φij(t) is smaller than H and the spectral norm ‖G‖ satisfies ‖G‖ < 1.",2.3 Estimation of the integrated cumulants,[0],[0]
"In this case, given a realization of a stationary Hawkes process {N t : t ∈",2.3 Estimation of the integrated cumulants,[0],[0]
"[0, T ]}, as shown in Section 4, we can write the estimators of the first three cumulants (3), (4) and (5) as
Λ̂i = 1
T ∑ τ∈Zi 1 = N iT T
(11)
Ĉij",2.3 Estimation of the integrated cumulants,[0],[0]
"= 1
T ∑ τ∈Zi ( N jτ+H −N j τ−H",2.3 Estimation of the integrated cumulants,[0],[0]
"− 2HΛ̂ j )
(12)
K̂ijk = 1
T ∑ τ∈Zi ( N jτ+H −N j τ−H",2.3 Estimation of the integrated cumulants,[0],[0]
− 2HΛ̂ j ) · ( Nkτ+H −Nkτ−H − 2HΛ̂k ),2.3 Estimation of the integrated cumulants,[0],[0]
"− Λ̂ i
T ∑ τ∈Zj ∑ τ ′∈Zk",2.3 Estimation of the integrated cumulants,[0],[0]
(2H − |τ ′,2.3 Estimation of the integrated cumulants,[0],[0]
− τ |)+ + 4H2Λ̂iΛ̂jΛ̂k.,2.3 Estimation of the integrated cumulants,[0],[0]
"(13)
Let us mention the following facts.
",2.3 Estimation of the integrated cumulants,[0],[0]
Bias.,2.3 Estimation of the integrated cumulants,[0],[0]
"While the first cumulant Λ̂i is an unbiased estimator of Λi, the other estimators Ĉij and K̂ijk introduce a bias.",2.3 Estimation of the integrated cumulants,[0],[0]
"However, as we will show, in practice this bias is small and hardly affects numerical estimations (see Section 3).",2.3 Estimation of the integrated cumulants,[0],[0]
"This is confirmed by our theoretical analysis, which proves that if H does not grow too fast compared to T , then these estimated cumulants are consistent estimators of the theoretical cumulants (see Section 2.6).
",2.3 Estimation of the integrated cumulants,[0],[0]
Complexity.,2.3 Estimation of the integrated cumulants,[0],[0]
"The computations of all the estimators of the first, second and third-order cumulants have complexity respectively O(nd), O(nd2) and O(nd3), where n = maxi |Zi|.",2.3 Estimation of the integrated cumulants,[0],[0]
"However, our algorithm requires a lot less than that: it computes only d2 third-order terms, of the form K̂iij , leaving us with only O(nd2) operations to perform.
Symmetry.",2.3 Estimation of the integrated cumulants,[0],[0]
"While the values of Λi, Cij and Kijk are symmetric under permutation of the indices, their estimators are generally not symmetric.",2.3 Estimation of the integrated cumulants,[0],[0]
We have thus chosen to symmetrize the estimators by averaging their values over permutations of the indices.,2.3 Estimation of the integrated cumulants,[0],[0]
"Worst case is for the estimator of Kc, which involves only an extra factor of 2 in the complexity.",2.3 Estimation of the integrated cumulants,[0],[0]
The objective to minimize in Equation (10) is non-convex.,2.4 The NPHC algorithm,[0],[0]
"More precisely, the loss function is a polynomial ofR of degree 6.",2.4 The NPHC algorithm,[0],[0]
"However, the expectations of cumulants Λ andC defined in Eq.",2.4 The NPHC algorithm,[0],[0]
(4) and (5) that appear in the definition of L(R) are unknown and should be replaced with Λ̂ and Ĉ.,2.4 The NPHC algorithm,[0],[0]
"We denote L̃(R) the objective function, where the expectations of cumulants Λi and Cij have been replaced with their estimators in the right-hand side of Eqs.",2.4 The NPHC algorithm,[0],[0]
"(8) and (9):
L̃(R) =",2.4 The NPHC algorithm,[0],[0]
"(1− κ)‖R 2Ĉ >
+ 2[R (Ĉ −RL̂)]R>",2.4 The NPHC algorithm,[0],[0]
"− K̂c‖22 + κ‖RL̂R> − Ĉ‖22 (14)
As explained in Choromanska et al. (2015), the loss function of a typical multilayer neural network with simple nonlinearities can be expressed as a polynomial function of the weights in the network,
whose degree is the number of layers.",2.4 The NPHC algorithm,[0],[0]
"Since the loss function of NPHC writes as a polynomial of degree 6, we expect good results using optimization methods designed to train deep multilayer neural networks.",2.4 The NPHC algorithm,[0],[0]
"We used the AdaGrad from Duchi et al. (2011), a variant of the Stochastic Gradient Descent with adaptive learning rates.",2.4 The NPHC algorithm,[0],[0]
"AdaGrad scales the learning rates coordinate-wise using the online variance of the previous gradients, in order to incorporate second-order information during training.",2.4 The NPHC algorithm,[0],[0]
"The NPHC method is summarized schematically in Algorithm 1.
",2.4 The NPHC algorithm,[0],[0]
"Algorithm 1 Non Parametric Hawkes Cumulant method Input: N t Output: Ĝ
1: Estimate Λ̂i, Ĉij , K̂iij from Eqs.",2.4 The NPHC algorithm,[0],[0]
"(11, 12, 13) 2: Design L̃(R) using the computed estimators.",2.4 The NPHC algorithm,[0],[0]
"3: Minimize numerically L̃(R) so as to obtain R̂ 4: Return Ĝ = Id − R̂ −1 .
",2.4 The NPHC algorithm,[0],[0]
"Our problem being non-convex, the choice of the starting point has a major effect on the convergence.",2.4 The NPHC algorithm,[0],[0]
"Here, the key is to notice that the matrices R that match Equation (8) writes C1/2OL−1/2, with L = diag(Λ) andO an orthogonal matrix.",2.4 The NPHC algorithm,[0],[0]
Our starting point is then simply chosen by settingO =,2.4 The NPHC algorithm,[0],[0]
"Id in the previous formula, leading to nice convergence results.",2.4 The NPHC algorithm,[0],[0]
"Even though our main concern is to retrieve the matrixG, let us notice we can also obtain an estimation of the baseline intensities’ from Eq.",2.4 The NPHC algorithm,[0],[0]
"(3), which leads to µ̂ = R̂ −1 Λ̂.",2.4 The NPHC algorithm,[0],[0]
"An efficient implementation of this algorithm with TensorFlow, see Abadi et al. (2016), is available on GitHub: https://github.com/achab/nphc.",2.4 The NPHC algorithm,[0],[0]
"Compared with existing state-of-the-art methods to estimate the kernel functions, e.g., the ordinary differential equations-based (ODE) algorithm in Zhou et al. (2013a), the Granger Causality-based algorithm in Xu et al. (2016), the ADM4 algorithm in Zhou et al. (2013b), and the Wiener-Hopf-based algorithm in Bacry and Muzy (2016), our method has a very competitive complexity.",2.5 Complexity of the algorithm,[0],[0]
"This can be understood by the fact that those methods estimate the kernel functions, while in NPHC we only estimate their integrals.",2.5 Complexity of the algorithm,[0],[0]
"The ODE-based algorithm is an EM algorithm that parametrizes the kernel function with M basis functions, each being discretized to L points.",2.5 Complexity of the algorithm,[0],[0]
The basis functions are updated after solving M Euler-Lagrange equations.,2.5 Complexity of the algorithm,[0],[0]
If n denotes the maximum number of events per component (i.e. n = max1≤i≤d |Zi|) then the complexity of one iteration of the algorithm isO(Mn3d2 +ML(nd+n2)).,2.5 Complexity of the algorithm,[0],[0]
"The Granger Causality-based algorithm is similar to the previous one, without the update of the basis functions, that are Gaussian kernels.",2.5 Complexity of the algorithm,[0],[0]
The complexity per iteration is O(Mn3d2).,2.5 Complexity of the algorithm,[0],[0]
"The algorithm ADM4 is similar to the two algorithms above, as EM algorithm as well, with only one exponential kernel as basis function.",2.5 Complexity of the algorithm,[0],[0]
The complexity per iteration is then O(n3d2).,2.5 Complexity of the algorithm,[0],[0]
"The Wiener-Hopf-based algorithm is not iterative, on the contrary to the previous ones.",2.5 Complexity of the algorithm,[0],[0]
"It first computes the empirical conditional laws on many points, and then invert the Wiener-Hopf system, leading to a O(nd2L+ d4L3) computation.",2.5 Complexity of the algorithm,[0],[0]
"Similarly, our method first computes the integrated cumulants, then minimize the objective function with Niter iterations, and invert the resulting matrix R̂ to obtain Ĝ.",2.5 Complexity of the algorithm,[0],[0]
"In the end, the complexity of the NPHC method is O(nd2 +Niterd3).",2.5 Complexity of the algorithm,[0],[0]
"According to this analysis, summarized in Table 1 below, one can see that in the regime n d, the NPHC method outperforms all the other ones.",2.5 Complexity of the algorithm,[0],[0]
The NPHC method can be phrased using the framework of the Generalized Method of Moments (GMM).,2.6 Theoretical guarantee: consistency,[0],[0]
GMM is a generic method for estimating parameters in statistical models.,2.6 Theoretical guarantee: consistency,[0],[0]
"In order to apply GMM,
we have to find a vector-valued function g(X, θ) of the data, where X is distributed with respect to a distribution Pθ0 , which satisfies the moment condition: E[g(X, θ)]",2.6 Theoretical guarantee: consistency,[0],[0]
"= 0 if and only if θ = θ0, where θ0 is the “ground truth” value of the parameter.",2.6 Theoretical guarantee: consistency,[0],[0]
"Based on i.i.d. observed copies x1, . . .",2.6 Theoretical guarantee: consistency,[0],[0]
", xn of X , the GMM method minimizes the norm of the empirical mean over n samples, ‖",2.6 Theoretical guarantee: consistency,[0],[0]
"1n ∑n i=1 g(xi, θ)‖, as a function of θ, to obtain an estimate of θ0.",2.6 Theoretical guarantee: consistency,[0],[0]
"In the theoretical analysis of NPHC, we use ideas from the consistency proof of the GMM, but the proof actually relies on very different arguments.",2.6 Theoretical guarantee: consistency,[0],[0]
"Indeed, the integrated cumulants estimators used in NPHC are not unbiased, as the theory of GMM requires, but asymptotically unbiased.",2.6 Theoretical guarantee: consistency,[0],[0]
"Moreover, the setting considered here, where data consists of a single realization {N t} of a Hawkes process strongly departs from the standard i.i.d setting.",2.6 Theoretical guarantee: consistency,[0],[0]
"Our approach is therefore based on the GMM idea but the proof is actually not using the theory of GMM.
",2.6 Theoretical guarantee: consistency,[0],[0]
"In the following, we use the subscript T to refer to quantities that only depend on the process (Nt) in the interval",2.6 Theoretical guarantee: consistency,[0],[0]
"[0, T ] (e.g., the truncation term HT , the estimated integrated covariance ĈT or the estimated kernel norm matrix ĜT ).",2.6 Theoretical guarantee: consistency,[0],[0]
"In the next equation, stands for the Hadamard product and 2 stands for the entrywise square of a matrix.",2.6 Theoretical guarantee: consistency,[0],[0]
We denoteG0,2.6 Theoretical guarantee: consistency,[0],[0]
"= Id −R−10 the true value ofG, and the R2d×d valued vector functions
g0(R) =
[ C −RLR> Kc −R 2C>",2.6 Theoretical guarantee: consistency,[0],[0]
"− 2[R (C −RL)]R> ]
ĝT (R) =
[ ĈT −RL̂TR>
K̂cT −R 2Ĉ > T",2.6 Theoretical guarantee: consistency,[0],[0]
− 2[R (ĈT −RL̂T ),2.6 Theoretical guarantee: consistency,[0],[0]
"]R> .
]
Using these notations, L̃T (R) can be seen as the weighted squared Frobenius norm of ĝT (R).",2.6 Theoretical guarantee: consistency,[0],[0]
"Moreover, when T → +∞, one has ĝT (R) P→ g0(R) under the conditions of the following theorem, where P→ stands for convergence in probability.
",2.6 Theoretical guarantee: consistency,[0],[0]
Theorem 2.1 (Consistency of NPHC).,2.6 Theoretical guarantee: consistency,[0],[0]
"Suppose that (Nt) is observed on R+ and assume that
1. g0(R) = 0",2.6 Theoretical guarantee: consistency,[0],[0]
"if and only ifR = R0;
2.",2.6 Theoretical guarantee: consistency,[0],[0]
"R ∈ Θ, where Θ is a compact set;
3.",2.6 Theoretical guarantee: consistency,[0],[0]
"the spectral radius of the kernel norm matrix satisfies ‖G0‖ < 1;
4.",2.6 Theoretical guarantee: consistency,[0],[0]
HT →∞ and H2T /T,2.6 Theoretical guarantee: consistency,[0],[0]
"→ 0.
Then
ĜT = Id − (
arg min R∈Θ L̃T (R)
)−1",2.6 Theoretical guarantee: consistency,[0],[0]
"P→ G0.
",2.6 Theoretical guarantee: consistency,[0],[0]
The proof of the Theorem is given in Section 4.5 below.,2.6 Theoretical guarantee: consistency,[0],[0]
"Assumption 3 is mandatory for stability of the Hawkes process, and Assumptions 3 and 4 are sufficient to prove that the estimators of the integrated cumulants defined in Equations (11), (12) and (13) are asymptotically consistent.",2.6 Theoretical guarantee: consistency,[0],[0]
Assumption 2 is a very mild standard technical assumption allowing to prove consistency for estimators based on moments.,2.6 Theoretical guarantee: consistency,[0],[0]
"Assumption 1 is a standard asymptotic moment condition, that allows to identity parameters from the integrated cumulants.",2.6 Theoretical guarantee: consistency,[0],[0]
"In this Section, we provide a comparison of NPHC with the state-of-the art, on simulated datasets with different kernel shapes, the MemeTracker dataset (social networks) and the order book dynamics dataset (finance).
",3 Numerical Experiments,[0],[0]
Simulated datasets.,3 Numerical Experiments,[0],[0]
"We simulated several datasets with Ogata’s Thinning algorithm Ogata (1981) using the open-source library tick1, each corresponding to a shape of kernel: rectangular, exponential or power law kernel, see Figure 1 below.
",3 Numerical Experiments,[0],[0]
"The integral of each kernel on its support equals α, 1/β can be regarded as a characteristic time-scale and γ is the scaling exponent for the power law distribution and a delay parameter for the rectangular one.",3 Numerical Experiments,[0],[0]
"We consider a non-symmetric block-matrix G to show that our method can effectively uncover causality between the nodes, see Figure 3.",3 Numerical Experiments,[0],[0]
"The matrix G has constant entries α on the three blocks - α = gij = 1/6 for dimension 10 and α = gij = 1/10 for dimension 100 -, and zero outside.",3 Numerical Experiments,[0],[0]
The two other parameters’ values are the same for dimensions 10 and 100.,3 Numerical Experiments,[0],[0]
"The parameter γ is set to 1/2 on the three blocks as well, but we set three very different β0, β1 and β2 from one block to the other, with ratio βi+1/βi = 10 and β0 = 0.1.",3 Numerical Experiments,[0],[0]
The number of events is roughly equal to 105 on average over the nodes.,3 Numerical Experiments,[0],[0]
"We ran the algorithm on three simulated datasets: a 10-dimensional process with rectangular kernels named Rect10, a 10-dimensional process with power law kernels named PLaw10 and a 100-dimensional process with exponential kernels named Exp100.
MemeTracker dataset.",3 Numerical Experiments,[0],[0]
We use events of the most active sites from the MemeTracker dataset2.,3 Numerical Experiments,[0],[0]
"This dataset contains the publication times of articles in many websites/blogs from August 2008 to April 2009, and hyperlinks between posts.",3 Numerical Experiments,[0],[0]
"We extract the top 100 media sites with the largest number of documents, with about 7 million of events.",3 Numerical Experiments,[0],[0]
"We use the links to trace the flow of information and establish an estimated ground truth for the matrixG. Indeed, when an hyperlink j appears in a post in website i, the link j can be regarded as a direct ancestor of the event.",3 Numerical Experiments,[0],[0]
"Then, Eq. (2) shows gij can be estimated by N i←jT /N",3 Numerical Experiments,[0],[0]
j T =,3 Numerical Experiments,[0],[0]
"#{links j → i}/N j T .
",3 Numerical Experiments,[0],[0]
"1https://github.com/X-DataInitiative/tick 2https://www.memetracker.org/data.html
Order book dynamics.",3 Numerical Experiments,[0],[0]
"We apply our method to financial data, in order to understand the self and crossinfluencing dynamics of all event types in an order book.",3 Numerical Experiments,[0],[0]
"An order book is a list of buy and sell orders for a specific financial instrument, the list being updated in real-time throughout the day.",3 Numerical Experiments,[0],[0]
"This model has first been introduced in Bacry et al. (2016), and models the order book via the following 8-dimensional point process:",3 Numerical Experiments,[0],[0]
"Nt = (P (a) t , P (b) t , T (a) t , T (b) t , L (a) t , L (b) t , C (a) t , C (b) t ), where P
(a) (resp.",3 Numerical Experiments,[0],[0]
"P (b)) counts the number of upward (resp. downward) price moves, T (a) (resp.",3 Numerical Experiments,[0],[0]
T (b)) counts the number of market orders at the ask3 (resp.,3 Numerical Experiments,[0],[0]
"at the bid) that do not move the price, L(a) (resp. L(b)) counts the number of limit orders at the ask4 (resp.",3 Numerical Experiments,[0],[0]
"at the bid) that do not move the price, and C(a) (resp. C(b)) counts the number of cancel orders at the ask5 (resp.",3 Numerical Experiments,[0],[0]
at the bid) that do not move the price.,3 Numerical Experiments,[0],[0]
"The financial data has been provided by QuantHouse EUROPE/ASIA, and consists of DAX future contracts between 01/01/2014 and 03/01/2014.
Baselines.",3 Numerical Experiments,[0],[0]
"We compare NPHC to state-of-the art baselines: the ODE-based algorithm (ODE) by Zhou et al. (2013a), the Granger Causality-based algorithm (GC) by Xu et al. (2016), the ADM4 algorithm (ADM4) by Zhou et al. (2013b), and the Wiener-Hopf-based algorithm (WH) by Bacry and Muzy (2016).
",3 Numerical Experiments,[0],[0]
Metrics.,3 Numerical Experiments,[0],[0]
"We evaluate the performance of the proposed methods using the computing time, the Relative Error
RelErr(A,B) = 1
d2 ∑ i,j |aij − bij",3 Numerical Experiments,[0],[0]
| |aij | 1{aij,3 Numerical Experiments,[0],[0]
"6=0} + |bij |1{aij=0}
and the Mean Kendall Rank Correlation
MRankCorr(A,B) = 1
d d∑ i=1",3 Numerical Experiments,[0],[0]
"RankCorr([ai•], [bi•]),
where RankCorr(x, y) = 2d(d−1)(Nconcordant(x, y) −Ndiscordant(x, y)) with Nconcordant(x, y)",3 Numerical Experiments,[0],[0]
"the number of pairs (i, j) satisfying xi > xj and yi > yj or xi < xj and yi < yj and Ndiscordant(x, y) the number of pairs (i, j) for which the same condition is not satisfied.
",3 Numerical Experiments,[0],[0]
"Note that RankCorr score is a value between −1 and 1, representing rank matching, but can take smaller values (in absolute value) if the entries of the vectors are not distinct.
3i.e.",3 Numerical Experiments,[0],[0]
buy orders that are executed and removed from the list 4i.e.,3 Numerical Experiments,[0],[0]
buy orders added to the list 5i.e.,3 Numerical Experiments,[0],[0]
"the number of times a limit order at the ask is canceled: in our dataset, almost 95% of limit orders are canceled before
execution.
",3 Numerical Experiments,[0],[0]
Discussion.,3 Numerical Experiments,[0],[0]
"We perform the ADM4 estimation, with exponential kernel, by giving the exact value β = β0 of one block.",3 Numerical Experiments,[0],[0]
"Let us stress that this helps a lot this baseline, in comparison to NPHC where nothing is specified on the shape of the kernel functions.",3 Numerical Experiments,[0],[0]
"We used M = 10 basis functions for both ODE and GC algorithms, and L = 50 quadrature points for WH.",3 Numerical Experiments,[0],[0]
"We did not run WH on the 100-dimensional datasets, for computing time reasons, because its complexity scales with d4.",3 Numerical Experiments,[0],[0]
"We ran multi-processed versions of the baseline methods on 56 cores, to decrease the computing time.
",3 Numerical Experiments,[0],[0]
"Our method consistently performs better than all baselines, on the three synthetic datasets, on MemeTracker and on the financial dataset, both in terms of Kendall rank correlation and estimation error.",3 Numerical Experiments,[0],[0]
"Moreover, we observe that our algorithm is roughly 50 times faster than all the considered baselines.
",3 Numerical Experiments,[0],[0]
"On Rect10, PLaw10 and Exp100 our method gives very impressive results, despite the fact that it does not uses any prior shape on the kernel functions, while for instance the ADM4 baseline do.",3 Numerical Experiments,[0],[0]
"On Figure 3, we observe that the matrix Ĝ estimated with ADM4 recovers well the block for which β = β0, i.e. the value we gave to the method, but does not perform well on the two other blocks, while the matrix Ĝ estimated with NPHC approximately reaches the true value for each of the three blocks.",3 Numerical Experiments,[0],[0]
"On these simulated datasets, NPHC obtains a comparable or slightly better Kendall rank correlation, but improves a lot the relative error.
",3 Numerical Experiments,[0],[0]
"On MemeTracker, the baseline methods obtain a high relative error between 9% and 19% while our method achieves a relative error of 7% which is a strong improvement.",3 Numerical Experiments,[0],[0]
"Moreover, NPHC reaches a much better Kendall rank correlation, which proves that it leads to a much better recovery of the relative order of estimated influences than all the baselines.",3 Numerical Experiments,[0],[0]
"Indeed, it has been shown in Zhou et al. (2013a) that kernels of MemeTracker data are not exponential, nor power law.",3 Numerical Experiments,[0],[0]
"This partly explains why our approach behaves
better.",3 Numerical Experiments,[0],[0]
"On the financial data, the estimated kernel norm matrix obtained via NPHC, see Figure 3, gave some interpretable results (see also Bacry et al. (2016)):
1.",3 Numerical Experiments,[0],[0]
Any 2× 2 sub,3 Numerical Experiments,[0],[0]
-,3 Numerical Experiments,[0],[0]
"matrix with same kind of inputs (i.e. Prices changes, Trades, Limits or Cancels) is symmetric.",3 Numerical Experiments,[0],[0]
"This shows empirically that ask and bid have symmetric roles.
2.",3 Numerical Experiments,[0],[0]
"The prices are mostly cross-excited, which means that a price increase is very likely to be followed by a price decrease, and conversely.",3 Numerical Experiments,[0],[0]
"This is consistent with the wavy prices we observe on financial markets.
",3 Numerical Experiments,[0],[0]
3.,3 Numerical Experiments,[0],[0]
"The market, limit and cancel orders are strongly self-excited.",3 Numerical Experiments,[0],[0]
"This can be explained by the persistence of order flows, and by the splitting of meta-orders into sequences of smaller orders.",3 Numerical Experiments,[0],[0]
"Moreover, we observe that orders impact the price without changing it.",3 Numerical Experiments,[0],[0]
"For example, the increase of cancel orders at the bid causes downward price moves.",3 Numerical Experiments,[0],[0]
"We show in this section how to obtain the equations stated above, the estimators of the integrated cumulants and the scaling coefficient κ that appears in the objective function.",4 Technical details,[0],[0]
"We then prove the theorem of the paper.
",4 Technical details,[0],[0]
"4.1 Proof of Equation (8)
We denote ν(z) the matrix
νij(z) = Lz ( t→ E(dN iudN j u+t)
dudt − ΛiΛj
) ,
where Lz(f) is the Laplace transform of f , and",4 Technical details,[0],[0]
"ψt = ∑ n≥1 φ (?n) t , where φ (?n)",4 Technical details,[0],[0]
"t refers to the n
th autoconvolution of φt.",4 Technical details,[0],[0]
"Then we use the characterization of second-order statistics, first formulated in Hawkes (1971) and fully generalized in Bacry and Muzy (2016),
ν(z) =",4 Technical details,[0],[0]
(Id + L−z(Ψ))L(Id + Lz(Ψ)),4 Technical details,[0],[0]
">,
where Lij = Λiδij with δij the Kronecker symbol.",4 Technical details,[0],[0]
Since Id +Lz(Ψ) =,4 Technical details,[0],[0]
"(Id −Lz(Φ))−1, taking z = 0 in the previous equation gives
ν(0) =",4 Technical details,[0],[0]
"(Id −G)−1L(Id −G>)−1, C = RLR>,
which gives us the result since the entry (i, j) of the last equation gives Cij = ∑
m Λ mRimRjm.
4.2 Proof of Equation (9)
",4 Technical details,[0],[0]
"We start from Jovanović et al. (2015), cf. Eqs.",4 Technical details,[0],[0]
"(48) to (51), and group some terms:
Kijk = ∑ m ΛmRimRjmRkm
+ ∑ m RimRjm ∑ n ΛnRknL0(ψmn)
+ ∑ m RimRkm ∑ n ΛnRjnL0(ψmn)
+ ∑ m RjmRkm ∑ n ΛnRinL0(ψmn).
",4 Technical details,[0],[0]
Using the relations L0(ψmn) =,4 Technical details,[0],[0]
"Rmn − δmn and Cij = ∑ m Λ mRimRjm, proves Equation (9).",4 Technical details,[0],[0]
For H > 0 let us denote ∆HN it = N,4.3 Integrated cumulants estimators,[0],[0]
i t+H −N it−H .,4.3 Integrated cumulants estimators,[0],[0]
"Let us first remark that, if one restricts the integration domain to (−H,H) in Eqs.",4.3 Integrated cumulants estimators,[0],[0]
"(4) and (5), one gets by permuting integrals and expectations:
Λidt = E(dN it )",4.3 Integrated cumulants estimators,[0],[0]
Cijdt = E ( dN it (∆HN j t − 2HΛj) ),4.3 Integrated cumulants estimators,[0],[0]
"Kijkdt = E ( dN it (∆HN j t − 2HΛj)(∆HNkt − 2HΛk)
)",4.3 Integrated cumulants estimators,[0],[0]
"− dtΛiE ( (∆HN j t − 2HΛj)(∆HNkt − 2HΛk) ) .
",4.3 Integrated cumulants estimators,[0],[0]
"The estimators (11) and (12) are then naturally obtained by replacing the expectations by their empirical counterparts, notably
E(dN itf(t))",4.3 Integrated cumulants estimators,[0],[0]
"dt → 1 T ∑ τ∈Zi f(τ).
",4.3 Integrated cumulants estimators,[0],[0]
"For the estimator (13), we shall also notice that
E((∆HN jt",4.3 Integrated cumulants estimators,[0],[0]
"− 2HΛj)(∆HNkt − 2HΛk))
",4.3 Integrated cumulants estimators,[0],[0]
"= ∫ ∫ 1[−H,H](t)1[−H,H](t ′)Cjkt−t′dtdt ′
= ∫",4.3 Integrated cumulants estimators,[0],[0]
"(2H − |t|)+Cjkt dt.
",4.3 Integrated cumulants estimators,[0],[0]
We estimate the last integral with the remark above.,4.3 Integrated cumulants estimators,[0],[0]
"Following the theory of GMM, we denote m(X, θ) a function of the data, where X is distributed with respect to a distribution Pθ0 , which satisfies the moment conditions g(θ) = E[m(X, θ)]",4.4 Choice of the scaling coefficient κ,[0],[0]
"= 0 if and only if θ = θ0, the parameter θ0 being the ground truth.",4.4 Choice of the scaling coefficient κ,[0],[0]
"For x1, . . .",4.4 Choice of the scaling coefficient κ,[0],[0]
", xN observed copies of X , we denote ĝi(θ) = m(xi, θ), the usual choice of weighting matrix is ŴN (θ) = 1N ∑N i=1 ĝi(θ)ĝi(θ)
",4.4 Choice of the scaling coefficient κ,[0],[0]
">, and the objective to minimize is then(
1
N N∑ i=1 ĝi(θ)
)",4.4 Choice of the scaling coefficient κ,[0],[0]
"( ŴN (θ1) )−1( 1 N N∑ i=1 ĝi(θ) ) , (15)
where θ1 is a constant vector.",4.4 Choice of the scaling coefficient κ,[0],[0]
"Instead of computing the inverse weighting matrix, we rather use its projection on {αId : α ∈ R}.",4.4 Choice of the scaling coefficient κ,[0],[0]
It can be shown that the projection choses α as the mean eigenvalue of ŴN (θ1).,4.4 Choice of the scaling coefficient κ,[0],[0]
"We can easily compute the sum of its eigenvalues:
Tr(ŴN (θ1))",4.4 Choice of the scaling coefficient κ,[0],[0]
"= 1
N N∑ i=1 Tr(ĝi(θ1)ĝi(θ1)>)",4.4 Choice of the scaling coefficient κ,[0],[0]
= 1 N N∑ i=1,4.4 Choice of the scaling coefficient κ,[0],[0]
Tr(ĝi(θ1)>ĝi(θ1)),4.4 Choice of the scaling coefficient κ,[0],[0]
= 1 N N∑ i=1,4.4 Choice of the scaling coefficient κ,[0],[0]
"||ĝi(θ1)||22.
",4.4 Choice of the scaling coefficient κ,[0],[0]
"In our case, ĝ(R) =",4.4 Choice of the scaling coefficient κ,[0],[0]
"[ vec[K̂c −Kc(R)], vec[Ĉ −C(R)]",4.4 Choice of the scaling coefficient κ,[0],[0]
]> ∈ R2d2 .,4.4 Choice of the scaling coefficient κ,[0],[0]
"Considering a block-wise
weighting matrix, one block for K̂c−Kc(R) and the other for Ĉ−C(R), the sum of the eigenvalues of the first block becomes ‖K̂c −Kc(R)‖22, and ‖Ĉ −C(R)‖22 for the second.",4.4 Choice of the scaling coefficient κ,[0],[0]
We compute the previous terms withR1 = 0.,4.4 Choice of the scaling coefficient κ,[0],[0]
"All together, the objective function to minimize is
1
‖K̂c‖22 ‖Kc(R)− K̂c‖22",4.4 Choice of the scaling coefficient κ,[0],[0]
"+
1
‖Ĉ‖22 ‖C(R)− Ĉ‖22. (16)
Dividing this function by ( 1/‖K̂c‖22 + 1/‖Ĉ‖22 )−1
, and setting κ = ‖K̂c‖22/(‖K̂c‖22 + ‖Ĉ‖22), we obtaind the loss function given in Equation (10).",4.4 Choice of the scaling coefficient κ,[0],[0]
"The main difference with the usual Generalized Method of Moments, see Hansen (1982), relies in the relaxation of the moment conditions, since we have E[ĝT (θ0)]",4.5 Proof of the Theorem,[0],[0]
= mT 6= 0.,4.5 Proof of the Theorem,[0],[0]
"We adapt the proof of consistency given in Newey and McFadden (1994).
",4.5 Proof of the Theorem,[0],[0]
"We can relate the integral of the Hawkes process’s kernels to the integrals of the cumulant densities, from Jovanović et al. (2015).",4.5 Proof of the Theorem,[0],[0]
"Our cumulant matching method would fall into the usual GMM framework if we could estimate - without bias - the integral of the covariance on R, and the integral of the skewness on R2.",4.5 Proof of the Theorem,[0],[0]
"Unfortunately, we can’t do that easily.",4.5 Proof of the Theorem,[0],[0]
"We can however estimate without bias ∫ fTt C ij t dt
and ∫ fTt K ijk t dt with f
T a compact supported function on [−HT , HT ] that weakly converges to 1, with HT −→ ∞. In most cases we will take fTt = 1[−HT ,HT ](t).",4.5 Proof of the Theorem,[0],[0]
"Denoting Ĉ
ij,(T ) the estimator of∫ fTt C ij t dt, the term |E[Ĉij,(T )]−Cij | = | ∫ fTt C ij t dt−Cij",4.5 Proof of the Theorem,[0],[0]
| can be considered a proxy to the distance to the classical GMM.,4.5 Proof of the Theorem,[0],[0]
"This distance has to go to zero to make the rest of GMM’s proof work: the estimator Ĉij,(T ) is then asymptotically unbiased towards Cij when T goes to infinity.",4.5 Proof of the Theorem,[0],[0]
"We observe the multivariate point process (N t) on R+, with Zi the events of the ith component.",4.5.1 Notations,[0],[0]
We will often write covariance / skewness instead of integrated covariance / skewness.,4.5.1 Notations,[0],[0]
"In the rest of the document, we use the following notations.",4.5.1 Notations,[0],[0]
Hawkes kernels’ integrals Gtrue =,4.5.1 Notations,[0],[0]
∫ Φtdt = ( ∫ φijt dt)ij =,4.5.1 Notations,[0],[0]
"Id − (Rtrue)−1
Theoretical mean matrix L = diag(Λ1, . . .",4.5.1 Notations,[0],[0]
",Λd)
Theoretical covariance C = RtrueL(Rtrue)>
",4.5.1 Notations,[0],[0]
Theoretical skewness Kc = (Kiij)ij = (Rtrue) 2 C> + 2[Rtrue (C −RtrueL)](Rtrue)>,4.5.1 Notations,[0],[0]
Filtering function fT ≥ 0,4.5.1 Notations,[0],[0]
supp(fT ) ⊂,4.5.1 Notations,[0],[0]
"[−HT , HT ] F T = ∫ fTs ds f̃ T t = f T −t
Events sets Zi,T,1 = Zi ∩",4.5.1 Notations,[0],[0]
"[HT , T +HT ] Zj,T,2 = Zj ∩ [0, T + 2HT ]
Estimators of the mean Λ̂i = N iT+HT −N iHT T Λ̃ j = NjT+2HT",4.5.1 Notations,[0],[0]
"T+2HT
Estimator of the covariance Ĉij,(T )",4.5.1 Notations,[0],[0]
"= 1T ∑ τ∈Zi,T,1 (∑ τ ′∈Zj,T,2 fτ ′−τ − Λ̃jF T )
Estimator of the skewness6
K̂ijk,(T )",4.5.1 Notations,[0],[0]
"= 1
T ∑ τ∈Zi,T,1  ∑ τ ′∈Zj,",4.5.1 Notations,[0],[0]
"T,2 fτ ′−τ − Λ̃jF T  ∑ τ ′′∈Zk,T,2 fτ ′−τ − Λ̃kF T  ",4.5.1 Notations,[0],[0]
"− Λ̂ i
T + 2HT ∑ τ ′∈Zj,T,2  ∑ τ ′′∈Zk,T,2 (fT ?",4.5.1 Notations,[0],[0]
f̃T )τ ′−τ ′′,4.5.1 Notations,[0],[0]
"− Λ̃k(F T )2 
6When fTt = 1[−HT ,HT ](t), we remind that (f T ?",4.5.1 Notations,[0],[0]
f̃T )t = (2HT − |t|)+.,4.5.1 Notations,[0],[0]
"This leads to the estimator we showed in the
article.
",4.5.1 Notations,[0],[0]
"GMM related notations
θ = R and θ0 = Rtrue g0(θ) = vec [ C −RLR>
Kc −R 2C>",4.5.1 Notations,[0],[0]
"− 2[R (C −RL)]R>
] ∈ R2d2
ĝT (θ) = vec
[ Ĉ (T ) −RL̂R",4.5.1 Notations,[0],[0]
">
K̂c (T ) −R 2(Ĉ (T ) )",4.5.1 Notations,[0],[0]
>,4.5.1 Notations,[0],[0]
"− 2[R (Ĉ (T ) −RL̂)]R>
] ∈",4.5.1 Notations,[0],[0]
"R2d2
Q0(θ) = g0(θ) >Wg0(θ)",4.5.1 Notations,[0],[0]
Q̂T (θ) = ĝT (θ) >ŴT ĝT (θ),4.5.1 Notations,[0],[0]
"First, let’s remind a useful theorem for consistency in GMM from Newey and McFadden (1994).
",4.5.2 Consistency,[0],[0]
Theorem 4.1.,4.5.2 Consistency,[0],[0]
"If there is a function Q0(θ) such that (i) Q0(θ) is uniquely maximized at θ0; (ii) Θ is compact; (iii) Q0(θ) is continuous; (iv) Q̂T (θ) converges uniformly in probability to Q0(θ), then θ̂T = arg max Q̂T (θ) P−→ θ0.
",4.5.2 Consistency,[0],[0]
"We can now prove the consistency of our estimator.
",4.5.2 Consistency,[0],[0]
Theorem 4.2.,4.5.2 Consistency,[0],[0]
"Suppose that (Nt) is observed on R+, ŴT P−→W , and
1.",4.5.2 Consistency,[0],[0]
W is positive semi-definite and Wg0(θ),4.5.2 Consistency,[0],[0]
= 0,4.5.2 Consistency,[0],[0]
"if and only if θ = θ0,
2. θ ∈ Θ, which is compact,
3.",4.5.2 Consistency,[0],[0]
the spectral radius of the kernel norm matrix satisfies ||Φ||∗,4.5.2 Consistency,[0],[0]
"< 1, 4.",4.5.2 Consistency,[0],[0]
"∀i, j, k ∈",4.5.2 Consistency,[0],[0]
"[d], ∫ fTu C ij u du→ ∫",4.5.2 Consistency,[0],[0]
"Ciju du and ∫ fTu f T v K ijk u,vdudv → ∫ Kijku,vdudv,
5.",4.5.2 Consistency,[0],[0]
"(F T )2/T P−→ 0 and ||f ||∞ = O(1).
",4.5.2 Consistency,[0],[0]
"Then
θ̂T P−→ θ0.
",4.5.2 Consistency,[0],[0]
Remark 1.,4.5.2 Consistency,[0],[0]
"In practice, we use a constant sequence of weighting matrices: ŴT = Id.
Proof.",4.5.2 Consistency,[0],[0]
Proceed by verifying the hypotheses of Theorem 2.1 from Newey and McFadden (1994).,4.5.2 Consistency,[0],[0]
Condition 2.1(i) follows by (i) and by Q0(θ) =,4.5.2 Consistency,[0],[0]
[W 1/2g0(θ)]>[W 1/2g0(θ)],4.5.2 Consistency,[0],[0]
> 0,4.5.2 Consistency,[0],[0]
= Q0(θ0).,4.5.2 Consistency,[0],[0]
"Indeed, there exists a neighborhood N of θ0 such that θ ∈ N\{θ0} and g0(θ) 6= 0",4.5.2 Consistency,[0],[0]
since g0(θ) is a polynom.,4.5.2 Consistency,[0],[0]
Condition 2.1(ii) follows by (ii).,4.5.2 Consistency,[0],[0]
Condition 2.1(iii) is satisfied since Q0(θ) is a polynom.,4.5.2 Consistency,[0],[0]
Condition 2.1(iv) is harder to prove.,4.5.2 Consistency,[0],[0]
"First, since ĝT (θ) is a polynom of θ, we prove easily that E[supθ∈Θ |ĝT (θ)|] <∞. Then, by Θ compact, g0(θ) is bounded on Θ, and by the triangle and Cauchy-Schwarz inequalities,∣∣Q̂T (θ)−Q0(θ)∣∣
≤ ∣∣(ĝT (θ)− g0(θ))>ŴT (ĝT (θ)− g0(θ))∣∣
+ ∣∣g0(θ)>(ŴT + Ŵ>T )(ĝT (θ)− g0(θ))∣∣+ ∣∣g0(θ)>(ŴT −W )",4.5.2 Consistency,[0],[0]
g0(θ)∣∣ ≤ ‖ĝT (θ)− g0(θ)‖2‖ŴT ‖+,4.5.2 Consistency,[0],[0]
2‖g0(θ)‖‖ĝT (θ)− g0(θ)‖‖ŴT ‖+ ‖g0(θ)‖2‖ŴT,4.5.2 Consistency,[0],[0]
"−W‖.
",4.5.2 Consistency,[0],[0]
"To prove supθ∈Θ ∣∣Q̂T (θ)−Q0(θ)∣∣ P−→ 0, we should now prove that supθ∈Θ‖ĝT (θ)− g0(θ)‖ P−→ 0.",4.5.2 Consistency,[0],[0]
"By Θ compact, it is sufficient to prove that ‖L̂−L‖ P−→ 0, ‖Ĉ (T ) −C‖ P−→ 0, and ‖K̂c (T ) −Kc‖",4.5.2 Consistency,[0],[0]
"P−→ 0.
",4.5.2 Consistency,[0],[0]
"Proof that ‖L̂−L‖ P−→ 0
",4.5.2 Consistency,[0],[0]
"The estimator of L is unbiased so let’s focus on the variance of L̂.
E[(Λ̂i",4.5.2 Consistency,[0],[0]
"− Λi)2] = E
[( 1
T ∫ T+HT HT (dN it − Λidt) )2]
= 1
T 2 ∫",4.5.2 Consistency,[0],[0]
T+HT HT ∫ T+HT HT E[(dN,4.5.2 Consistency,[0],[0]
it − Λidt)(dN it′,4.5.2 Consistency,[0],[0]
"− Λidt′)]
= 1
T 2 ∫",4.5.2 Consistency,[0],[0]
T+HT HT ∫ T+HT,4.5.2 Consistency,[0],[0]
"HT Ciit′−tdtdt ′
≤ 1 T 2 ∫",4.5.2 Consistency,[0],[0]
"T+HT HT Ciidt = Cii T −→ 0
By Markov inequality, we have just proved that ‖L̂−L‖ P−→ 0.
",4.5.2 Consistency,[0],[0]
"Proof that ‖Ĉ (T ) −C‖ P−→ 0
First, let’s remind that E(Ĉ (T ) )",4.5.2 Consistency,[0],[0]
6=,4.5.2 Consistency,[0],[0]
"C. Indeed,
E ( Ĉij,(T ) )",4.5.2 Consistency,[0],[0]
"= E ( 1
T ∫ T+HT HT dN it ∫ T+2HT 0 dN jt′ft′−t − Λ̂ iΛ̃jF T ) = E (",4.5.2 Consistency,[0],[0]
"1
T ∫ T+HT HT dN it ∫ T+2HT−t −t dN jt+sfs",4.5.2 Consistency,[0],[0]
− ΛiΛjF T ) +,4.5.2 Consistency,[0],[0]
"ij,T,HTF T
= 1
",4.5.2 Consistency,[0],[0]
T ∫ T+HT HT ∫ HT −HT fsE,4.5.2 Consistency,[0],[0]
( dN itdN j t+s − ΛiΛjds ),4.5.2 Consistency,[0],[0]
"+ ij,T,HTF T
= ∫ fsC ij s ds+ ij,T,HTF T
Now,
ij,T,HT = E",4.5.2 Consistency,[0],[0]
(,4.5.2 Consistency,[0],[0]
"ΛiΛj − Λ̂iΛ̃j )
=",4.5.2 Consistency,[0],[0]
− 1 T 2 ∫,4.5.2 Consistency,[0],[0]
T+HT HT ∫ T+2HT 0 E ( dN itdN j t′,4.5.2 Consistency,[0],[0]
"− Λ iΛjdtdt′ )
",4.5.2 Consistency,[0],[0]
=,4.5.2 Consistency,[0],[0]
− 1 T 2 ∫,4.5.2 Consistency,[0],[0]
T+HT HT ∫ T+2HT 0,4.5.2 Consistency,[0],[0]
"Cijt−t′dtdt ′
=",4.5.2 Consistency,[0],[0]
"− 1 T
∫ ( 1 + (",4.5.2 Consistency,[0],[0]
"HT − |t|
T
)−)+ Cijt dt
Since f satisfies F T = o(T ), we have E(Ĉ (T ) )",4.5.2 Consistency,[0],[0]
−→ C. It remains now to prove that ‖Ĉ (T ),4.5.2 Consistency,[0],[0]
"− E(Ĉ (T )
)‖ P−→ 0.",4.5.2 Consistency,[0],[0]
"Let’s now focus on the variance of Ĉij,(T ) : V(Ĉij,(T ))",4.5.2 Consistency,[0],[0]
"= E ( (Ĉij,(T ))2 )",4.5.2 Consistency,[0],[0]
"− E(Ĉij,(T ))",4.5.2 Consistency,[0],[0]
"2.
",4.5.2 Consistency,[0],[0]
"Now, E ( (Ĉij,(T ))2 )
",4.5.2 Consistency,[0],[0]
"= E  1 T 2 ∑ (τ,η,τ ′,η′)∈(Zi,T,1)2×(Zj,T,2)2 (fτ ′−τ − F T /(T + 2HT ))",4.5.2 Consistency,[0],[0]
"(fη′−η − F T /(T + 2HT ))  = E ( 1
T 2 ∫ t,s∈[HT ,T+HT ] ∫ t′,s′ dN",4.5.2 Consistency,[0],[0]
itdN j t′dN i sdN j s′(ft′−t,4.5.2 Consistency,[0],[0]
"− F T /(T + 2HT ))(fs′−s − F T /(T + 2HT )) )
",4.5.2 Consistency,[0],[0]
"= 1
T 2 ∫ t,s∈[HT ,T+HT ] ∫ t′,s′∈[0,T+2HT ] E ( dN itdN j t′dN i sdN j s′ ) · (ft′−t − F T /(T + 2HT ))(fs′−s − F T /(T + 2HT ))
",4.5.2 Consistency,[0],[0]
"And,
E(Ĉij,(T ))2
= 1
T 2 ∫ t,s∈[HT ,T+HT ] ∫ t′,s′∈[0,T+2HT ] E ( dN itdN j t′ )",4.5.2 Consistency,[0],[0]
E ( dN isdN j s′ ) · (ft′−t − F T /(T + 2HT )),4.5.2 Consistency,[0],[0]
"(fs′−s − F T /(T + 2HT ))
",4.5.2 Consistency,[0],[0]
"Then, the variance involves the integration towards the difference of moments µr,s,t,u − µr,sµt,u. Let’s write it as a sum of cumulants, since cumulants density are integrable.
",4.5.2 Consistency,[0],[0]
"µr,s,t,u − µr,sµt,u = κr,s,t,u + κr,s,tκu[4] + κr,sκt,u[3] + κr,sκtκu[6] + κrκsκtκu",4.5.2 Consistency,[0],[0]
"− (κr,s + κrκs)(κt,u + κtκu) = κr,s,t,u
+ κr,s,tκu + κu,r,sκt + κt,u,rκs + κs,t,uκr
+ κr,tκs,u + κr,uκs,t
+ κr,tκsκu + κr,uκsκt + κs,tκrκu + κs,tκrκu
In the rest of the proof, we denote at = 1t∈[HT ,T+HT ], bt = 1t∈[0,T+2HT ], ct = 1t∈[−HT ,HT ], gt = ft − 1T+2HT F T Before starting the integration of each term, let’s remark that:
1.",4.5.2 Consistency,[0],[0]
"Ψt = ∑ n≥1 Φ (?n) t ≥ 0 since Φt ≥ 0.
2.",4.5.2 Consistency,[0],[0]
"The regular parts ofCiju ,K ijk u,v (skewness density) andM ijkl u,v,w (fourth cumulant density) are positive
as polynoms of integrals of ψab· with positive coefficients.",4.5.2 Consistency,[0],[0]
"The integrals of the singular parts are positive as well.
",4.5.2 Consistency,[0],[0]
3.,4.5.2 Consistency,[0],[0]
(a) ∫ atbt′ft′−tdtdt ′,4.5.2 Consistency,[0],[0]
"= TF T
(b) ∫ atbt′gt′−tdtdt ′",4.5.2 Consistency,[0],[0]
"= 0
(c) ∫ atbt′ |gt′−t|dtdt′ ≤ 2TF",4.5.2 Consistency,[0],[0]
"T
4.",4.5.2 Consistency,[0],[0]
"∀t ∈ R, at(b ?",4.5.2 Consistency,[0],[0]
g̃)t,4.5.2 Consistency,[0],[0]
"= 0, where g̃s = g−s.
Fourth cumulant",4.5.2 Consistency,[0],[0]
"We want here to compute ∫ κi,j,i,jt,t′,s,s′atbt′asbs′gt′−tgs′−sdtdt
′dsds′. We remark that |gt′−tgs′−s| ≤ (||f ||∞(1 + 2HT /T ))2 ≤",4.5.2 Consistency,[0],[0]
"4||f ||2∞.∣∣∣ 1 T 2 ∫ κi,j,i,jt,t′,s,s′atbt′asbs′gt′−tgs′−sdtdt ′dsds′ ∣∣∣ ≤ (2||f ||∞ T )2 ∫ dtat ∫ dt′bt′ ∫",4.5.2 Consistency,[0],[0]
"dsas ∫ ds′bs′M ijij t′−t,s−t,s′−t
≤",4.5.2 Consistency,[0],[0]
"(
2||f ||∞ T
)2 ∫",4.5.2 Consistency,[0],[0]
dtat ∫ dt′bt′ ∫ dsas ∫ dwM,4.5.2 Consistency,[0],[0]
"ijijt′−t,s−t,w
≤ (
2||f ||∞ T
)2 ∫ dtat ∫ M ijiju,v,wdudvdw
≤ 4||f || 2 ∞
T M ijij −→ T→∞ 0
",4.5.2 Consistency,[0],[0]
"Third × First We have four terms, but only two different forms since the roles of (s, s′) and (t, t′) are symmetric.",4.5.2 Consistency,[0],[0]
"First form ∫
κi,j,it,t′,sΛ jGtdt =
Λj
T 2
∫ κi,j,it,t′,satbt′asbs′gt′−tgs′−sdtdt ′dsds′
= Λj
T 2
∫ κi,j,it,t′,satbt′as(b ?",4.5.2 Consistency,[0],[0]
"g̃)sgt′−tdtdt ′ds
= 0",4.5.2 Consistency,[0],[0]
since as(b ?,4.5.2 Consistency,[0],[0]
"g̃)s = 0
Second form∣∣∣ ∫ κi,j,jt,t′,s′ΛiGtdt∣∣∣ = ∣∣∣ΛiT 2 ∫ κi,j,jt,t′,s′atbt′asbs′gt′−tgs′−sdtdt ′dsds′ ∣∣∣
= ∣∣∣Λi T 2 ∫ κi,j,jt,t′,s′atbt′gt′−tbs′(a ?",4.5.2 Consistency,[0],[0]
g)s′dtdt,4.5.2 Consistency,[0],[0]
′ds′ ∣∣∣ ≤,4.5.2 Consistency,[0],[0]
"Λ i
T 2 2||f ||∞
∫ ds′bs′(a ?",4.5.2 Consistency,[0],[0]
"|g|)s′ ∫ dtat ∫ dt′bt′K ijj t′−s′,t−s′
≤",4.5.2 Consistency,[0],[0]
"4||f ||∞KijjΛi F T
T −→ T→∞ 0
",4.5.2 Consistency,[0],[0]
"Second × Second First form ∣∣∣ ∫ κi,it,sκj,jt′,s′Gtdt∣∣∣ ≤ 2||f ||∞T 2 ∫ Ciit−sC jj t′−s′atbt′ |gt′−t|asbs′dtdt ′dsds′
≤ 2||f ||∞",4.5.2 Consistency,[0],[0]
"T 2
CiiCjj ∫ atbt′ |gt′−t|dtdt′
≤ 4||f ||∞CiiCjj F T
T −→ T→∞ 0
Second form ∣∣∣ ∫ κi,jt,s′κi,jt′,sGtdt∣∣∣ ≤",4.5.2 Consistency,[0],[0]
4||f ||∞(Cij)2F TT −→T→∞ 0,4.5.2 Consistency,[0],[0]
"Second × First × First First form ∫
κi,jt,t′Λ",4.5.2 Consistency,[0],[0]
"iΛjGtdt =
ΛiΛj
T 2
∫ κi,jt,t′atbt′gt′−tdtdt ′",4.5.2 Consistency,[0],[0]
∫ asbs′gs′−sdsds ′,4.5.2 Consistency,[0],[0]
"= 0
Second form ∫ κi,it,sΛ",4.5.2 Consistency,[0],[0]
jΛjGtdt,4.5.2 Consistency,[0],[0]
"= ( Λj
T
)2 ∫ κi,it,satbt′gt′−tas(b ?",4.5.2 Consistency,[0],[0]
"g̃)sdtdt ′ds = 0
We have just proved that V(Ĉ (T ) ) P−→",4.5.2 Consistency,[0],[0]
0,4.5.2 Consistency,[0],[0]
.,4.5.2 Consistency,[0],[0]
"By Markov inequality, it ensures us that ‖Ĉ (T ) −E(Ĉ (T ) )",4.5.2 Consistency,[0],[0]
"‖ P−→ 0, and finally that ‖Ĉ (T ) −C‖ P−→ 0.
",4.5.2 Consistency,[0],[0]
Proof that ‖K̂c (T ) −Kc‖,4.5.2 Consistency,[0],[0]
"P−→ 0
",4.5.2 Consistency,[0],[0]
The scheme of the proof is similar to the previous one.,4.5.2 Consistency,[0],[0]
"The upper bounds of the integrals involve the same kind of terms, plus the new term (F T )2/T that goes to zero thanks to the assumption 5 of the theorem.",4.5.2 Consistency,[0],[0]
"In this paper, we introduce a simple nonparametric method (the NPHC algorithm) that leads to a fast and robust estimation of the matrixG of the kernel integrals of a Multivariate Hawkes process that encodes Granger causality between nodes.",5 Conclusion,[0],[0]
"This method relies on the matching of the integrated order 2 and order 3 empirical cumulants, which represent the simplest set of global observables containing sufficient information to recover the matrixG. Since this matrix fully accounts for the self- and cross- influences of the process nodes (that can represent agents or users in applications), our approach can naturally be used to quantify the degree of endogeneity of a system and to uncover the causality structure of a network.
",5 Conclusion,[0],[0]
"By performing numerical experiments involving very different kernel shapes, we show that the baselines, involving either parametric or non-parametric approaches are very sensible to model misspecification, do not lead to accurate estimation, and are numerically expensive, while NPHC provides fast, robust and reliable results.",5 Conclusion,[0],[0]
"This is confirmed on the MemeTracker database, where we show that NPHC outperforms classical approaches based on EM algorithms or the Wiener-Hopf equations.",5 Conclusion,[0],[0]
"Finally, the NPHC algorithm provided very satisfying results on financial data, that are consistent with well-known stylized facts in finance.",5 Conclusion,[0],[0]
"This work benefited from the support of the chair “Changing markets”, CMAP École Polytechnique and École Polytechnique fund raising - Data Science Initiative.
",Acknowledgements,[0],[0]
The authors want to thank Marcello Rambaldi for fruitful discussions on order book data’s experiments.,Acknowledgements,[0],[0]
We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process.,abstractText,[0],[0]
"This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them.",abstractText,[0],[0]
Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves.,abstractText,[0],[0]
"As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime.",abstractText,[0],[0]
"For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process.",abstractText,[0],[0]
A theoretical analysis allows us to prove that this new estimation technique is consistent.,abstractText,[0],[0]
"Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data.",abstractText,[0],[0]
Uncovering Causality from Multivariate Hawkes Integrated Cumulants,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 203–208 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2032",text,[0],[0]
Argumentation plays a crucial role in persuasion and decision-making processes.,1 Introduction,[0],[0]
An argument usually consists of a central claim (or conclusion) and several supporting premises.,1 Introduction,[0],[0]
"Constructing arguments of high quality would require the inclusion of diverse information, such as factual evidence and solid reasoning (Rieke et al., 1997; Park and Cardie, 2014).",1 Introduction,[0],[0]
"For instance, as shown in Figure 1, the editor on idebate.org – a Wikipedia-style website for gathering pro and con arguments on controversial issues, utilizes arguments based on study, factual evidence, and expert opinion to support the anti-gun claim “legally owned guns are frequently stolen and used by criminals”.",1 Introduction,[0],[0]
"However, it would require substantial human effort to collect information from diverse resources to support argument construction.",1 Introduction,[0],[0]
"In order to facilitate this process, there is a pressing need for tools that can automatically detect supporting arguments.
",1 Introduction,[0],[0]
"To date, most of the argument mining research focuses on recognizing argumentative components
and their structures from constructed arguments based on curated corpus (Mochales and Moens, 2011; Stab and Gurevych, 2014; Feng and Hirst, 2011; Habernal and Gurevych, 2015; Nguyen and Litman, 2016).",1 Introduction,[0],[0]
Limited work has been done for retrieving supporting arguments from external resources.,1 Introduction,[0],[0]
Initial effort by Rinott et al. (2015) investigates the detection of relevant factual evidence from Wikipedia articles.,1 Introduction,[0],[0]
"However, it is unclear whether their method can perform well on documents of different genres (e.g. news articles vs. blogs) for detecting distinct types of supporting information.
",1 Introduction,[0],[0]
"In this work, we present a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim.",1 Introduction,[0],[0]
"Take Figure 2 as an example: assume we are given a claim on the topic of “banning cosmetic surgery” and a relevant article (cited for argument construction), we aim to automatically pinpoint the sentence(s) (in italics) among all sentences in the cited article that can be used to back up the claim.",1 Introduction,[0],[0]
We define such tasks as supporting argument detection.,1 Introduction,[0],[0]
"Furthermore, another goal of
203
this work is to understand and characterize different types of supporting arguments.",1 Introduction,[0],[0]
"Indeed, human editors do use different types of information to promote persuasiveness as we will show in Section 3.",1 Introduction,[0],[0]
"Prediction performance also varies among different types of supporting arguments.
",1 Introduction,[0],[0]
"Given that none of the existing datasets is suitable for our study, we collect and annotate a corpus from Idebate, which contains hundreds of debate topics and corresponding claims.1 As is shown in Figure 2, each claim is supported with some human constructed argument, with cited articles marked on sentence level.",1 Introduction,[0],[0]
"After careful inspection on the supporting arguments, we propose to label them as STUDY, FACTUAL, OPINION, or REASONING.",1 Introduction,[0],[0]
"Substantial inter-annotator agreement rate is achieved for both supporting argument labeling (with Cohen’s κ of 0.8) and argument type annotation, on 200 topics with 621 reference articles.
",1 Introduction,[0],[0]
"Based on the new corpus, we first carry out a study on characterizing arguments of different types via type prediction.",1 Introduction,[0],[0]
"We find that arguments
1The labeled dataset along with the annotation guideline will be released at xyhua.me.
of STUDY and FACTUAL tend to use more concrete words, while arguments of OPINION contain more named entities of person names.",1 Introduction,[0],[0]
We then investigate whether argument type can be leveraged to assist supporting argument detection.,1 Introduction,[0],[0]
"Experimental results based on LambdaMART (Burges, 2010) show that utilizing features composite with argument types achieves a Mean Reciprocal Rank (MRR) score of 57.65, which outperforms an unsupervised baseline and the same ranker trained without type information.",1 Introduction,[0],[0]
"Feature analysis also demonstrates that salient features have significantly different distribution over different argument types.
",1 Introduction,[0],[0]
"For the rest of the paper, we summarize related work in Section 2.",1 Introduction,[0],[0]
"The data collection and annotation process is described in Section 3, which is followed by argument type study (Section 4).",1 Introduction,[0],[0]
Experiment on supporting argument detection is presented in Section 5.,1 Introduction,[0],[0]
We finally conclude in Section 6.,1 Introduction,[0],[0]
"Our work is in line with argumentation mining, which has recently attracted significant research interest.",2 Related Work,[0],[0]
"Existing work focuses on argument extraction from news articles, legal documents, or online comments without given userspecified claim (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011; Park and Cardie, 2014).",2 Related Work,[0],[0]
"Argument scheme classification is also widely studied (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014; Al Khatib et al., 2016), which emphasizes on distinguishing different types of arguments.",2 Related Work,[0],[0]
"To the best of our knowledge, none of them studies the interaction between types of arguments and their usage to support a user-specified claim.",2 Related Work,[0],[0]
This is the gap we aim to fill.,2 Related Work,[0],[0]
"We rely on data from idebate.org, where human editors construct paragraphs of arguments, either supporting or opposing claims under controversial topics.",3 Data and Annotation,[0],[0]
We also extract textual citation articles as source of information used by editors during argument construction.,3 Data and Annotation,[0],[0]
"In total we collected 383 unique debates, out of which 200 debates are randomly selected for study.",3 Data and Annotation,[0],[0]
"After removing invalid ones, our final dataset includes 450 claims
and 621 citation articles with about 53,000 sentences.",3 Data and Annotation,[0],[0]
Annotation Process.,3 Data and Annotation,[0],[0]
"As shown in Figure 2, we first annotate which sentence(s) from a citation articles is used by the editor as supporting arguments.",3 Data and Annotation,[0],[0]
"Then we annotate the type for each of them as STUDY, FACTUAL, OPINION, or REASONING, based on the scheme in Table 1.2 For instance, the highlighted supporting argument in Figure 2 is labeled as REASONING.
",3 Data and Annotation,[0],[0]
Two experienced annotators were hired to identify supporting arguments by reading through the whole cited article and locating the sentences that best match the reference human constructed argument.,3 Data and Annotation,[0],[0]
"This task is rather complicated since human do not just repeat or directly quote the original sentences from citation articles, they also paraphrase, summarize, and generalize.",3 Data and Annotation,[0],[0]
"For instance, the original sentence is “The global counterfeit drug trade, a billion-dollar industry, is thriving in Africa”, which is paraphrased to “This is exploited by the billion dollar global counterfeit drug trade” in human constructed argument.
",3 Data and Annotation,[0],[0]
"The annotators were asked to annotate independently, then discuss and resolve disagreements and give feedback about current scheme.",3 Data and Annotation,[0],[0]
We compute inter-annotator agreement based on Cohen’s κ for both supporting arguments labeling and argument type annotation.,3 Data and Annotation,[0],[0]
"For supporting arguments we have a high degree of consensus, with Cohen’s κ ranges from 0.76 to 0.83 in all rounds and 0.80 overall.",3 Data and Annotation,[0],[0]
"For argument type annotation, we achieve Cohen’s κ of 0.61 for STUDY, 0.75 for FACTUAL, 0.71 for OPINION, and 0.29 for REASONING3
2We end up with the four-type scheme as a trade-off between complexity and its coverage of the arguments.
",3 Data and Annotation,[0],[0]
"3Many times annotators have different interpretation on REASONING, and frequently label it as OPINION.",3 Data and Annotation,[0],[0]
"This results
Statistics.",3 Data and Annotation,[0],[0]
In total 995 sentences are identified as supporting arguments.,3 Data and Annotation,[0],[0]
"Among those, 95 (9.55%) are labeled as STUDY, 497 (49.95%) as FACTUAL, 363 (36.48%) as OPINION, and 40 (4.02%) as REASONING.
",3 Data and Annotation,[0],[0]
We further analyze the source of the supporting arguments.,3 Data and Annotation,[0],[0]
"Domain names of the citation articles are collected based on their URL, and then categorized into “news”, “organization”, “scientific”, “blog”, “reference”, and others, according to a taxonomy provided by Alexa4 with a few edits to fit our dataset.",3 Data and Annotation,[0],[0]
"News articles are the major source for all types, which account for roughly 50% for each.",3 Data and Annotation,[0],[0]
We show the distribution of other four types in Figure 3.,3 Data and Annotation,[0],[0]
"Arguments of STUDY and REASONING are mostly from “scientific” websites (14.9% and 22.9%), whereas “organization” websites contribute a large portion of arguments of FACTUAL (18.5%) and OPINION (16.7%).",3 Data and Annotation,[0],[0]
Here we characterize arguments of different types based on diverse features under the task of predicting argument types.,4 A Study On Argument Type Prediction,[0],[0]
Supporting arguments identified from previous section are utilized for experiments.,4 A Study On Argument Type Prediction,[0],[0]
"We also leverage the learned classifier in this section to label the sentences that are not supporting arguments, which will be used for supporting argument detection in the next section.",4 A Study On Argument Type Prediction,[0],[0]
Four major types of features are considered.,4 A Study On Argument Type Prediction,[0],[0]
Basic Features.,4 A Study On Argument Type Prediction,[0],[0]
"We calculate frequencies of unigram and bigram words, number of four major types of part-of-speech tags (verb, noun, adjective, and adverb), number of dependency relations, and
in a low agreement for REASONING.",4 A Study On Argument Type Prediction,[0],[0]
"4http://www.alexa.com/topsites/category
number of seven types of named entities (Chinchor and Robinson, 1997).",4 A Study On Argument Type Prediction,[0],[0]
Sentiment Features.,4 A Study On Argument Type Prediction,[0],[0]
"We also compute number of positive, negative and neutral words in MPQA lexicon (Wilson et al., 2005), and number of words from a subset of semantic categories from General Inquirer (Stone et al., 1966).5 Discourse Features.",4 A Study On Argument Type Prediction,[0],[0]
"We use the number of discourse connectives from the top two levels of Penn Discourse Tree Bank (Prasad et al., 2007).",4 A Study On Argument Type Prediction,[0],[0]
Style Features.,4 A Study On Argument Type Prediction,[0],[0]
"We measure word attributes for their concreteness (perceptible vs. conceptual), valence (or pleasantness), arousal (or intensity of emotion), and dominance (or degree of control) based on the lexicons collected by Brysbaert et al. (2014) and Warriner et al. (2013).
",4 A Study On Argument Type Prediction,[0],[0]
We utilize Log-linear model for argument type prediction with one-vs-rest setup.,4 A Study On Argument Type Prediction,[0],[0]
"Three baselines are considered: (1) random guess, (2) majority class, and (3) unigrams and bigrams as features for Log-linear model.",4 A Study On Argument Type Prediction,[0],[0]
"Identified supporting arguments are used for experiments, and divided into training set (50%), validation set (25%) and test set (25%).",4 A Study On Argument Type Prediction,[0],[0]
"From Table 2, we can see that Loglinear model trained with all features outperforms the ones trained with ngram features.",4 A Study On Argument Type Prediction,[0],[0]
"To further characterize arguments of different types, we display sample features with significant different values in Figure 4.",4 A Study On Argument Type Prediction,[0],[0]
"As can be seen, arguments of STUDY and FACTUAL tend to contain more concrete words and named entities.",4 A Study On Argument Type Prediction,[0],[0]
"Arguments of OPINION mention more person names, which implies that expert opinions are commonly quoted.",4 A Study On Argument Type Prediction,[0],[0]
"We cast the sentence-level supporting argument detection problem as a ranking task.6 Features
5Categories used: Strong, Weak, Virtue, Vice, Ovrst (Overstated), Undrst (Understated), Academ (Academic), Doctrin (Doctrine), Econ (Economic), Relig (Religious), Causal, Ought, and Perceiv (Perception).
6Many sentences in the citation article is relevant to the topic to various degrees.",5 Supporting Argument Detection,[0],[0]
"We focus on detecting the most relevant ones, and thus treat it as a ranking problem instead of a
in Section 4 are also utilized here as “Sentence features” with additional features considering the sentence position in the article.",5 Supporting Argument Detection,[0],[0]
"We further employ features that measure similarity between claims and sentences, and the composite features that leverage argument type information.
",5 Supporting Argument Detection,[0],[0]
Similarity Features.,5 Supporting Argument Detection,[0],[0]
We compute similarity between claim and candidate sentence based on TFIDF and average word embeddings.,5 Supporting Argument Detection,[0],[0]
"We also consider ROUGE (Lin, 2004), a recall oriented metric for summarization evaluation.",5 Supporting Argument Detection,[0],[0]
"In particular, ROUGE-L, a variation based on longest common subsequence, is computed by treating claim as reference and each candidate sentence as sample summary.",5 Supporting Argument Detection,[0],[0]
"In similar manner we use BLEU (Papineni et al., 2002), a precision oriented metric.
",5 Supporting Argument Detection,[0],[0]
Composite Features.,5 Supporting Argument Detection,[0],[0]
We adopt composite features to study the interaction of other features with type of the sentence.,5 Supporting Argument Detection,[0],[0]
"Given claim c and sentence s with any feature mentioned above, a composite feature function φM(type, feature)(s, c) is set to the actual feature value if and only if the argument type matches.",5 Supporting Argument Detection,[0],[0]
"For instance, if the ROUGE-L score is 0.2, and s is of type STUDY, then φM(study, ROUGE)(s, c)",5 Supporting Argument Detection,[0],[0]
"= 0.2 φM(factual, ROUGE)(s, c), φM(opinion, ROUGE)(s, c), φM(reasoning, ROUGE)(s, c) are all set to 0.
binary classification task.
",5 Supporting Argument Detection,[0],[0]
"We choose LambdaMART (Burges, 2010) for experiments, which is shown to be successful for many text ranking problems (Chapelle and Chang, 2011).",5 Supporting Argument Detection,[0],[0]
Our model is evaluated by Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) using 5-fold cross validation.,5 Supporting Argument Detection,[0],[0]
"We compare to TFIDF and Word embedding similarity baselines, and LambdaMART trained with ngrams (unigrams and bigrams).
",5 Supporting Argument Detection,[0],[0]
"Results in Table 3 show that using composite features with argument type information (Comp(type, Sen) +",5 Supporting Argument Detection,[0],[0]
"Comp(type, Simi)) can improve the ranking performance.",5 Supporting Argument Detection,[0],[0]
"Specifically, the best performance is achieved by adding composite features to sentence features, similarity features, and ngram features.",5 Supporting Argument Detection,[0],[0]
"As can be seen, supervised methods outperform unsupervised baseline methods.",5 Supporting Argument Detection,[0],[0]
And similarity features have similar performance as those baselines.,5 Supporting Argument Detection,[0],[0]
"The best performance is achieved by combination of sentence features, Ngrams, similarity, and two composite types, which is boldfaced.",5 Supporting Argument Detection,[0],[0]
"Feature sets that significantly outperform all three baselines are marked with ∗.
For feature analysis, we conduct t-test for individual feature values between supporting arguments and the others.",5 Supporting Argument Detection,[0],[0]
We breakdown features according to their argument types and show top salient composite features in Table 4.,5 Supporting Argument Detection,[0],[0]
"For all sentences of type STUDY, relevant ones tend to contain more “percentage” and more concrete words.",5 Supporting Argument Detection,[0],[0]
We also notice those sentences with more hedging words are more likely to be considered.,5 Supporting Argument Detection,[0],[0]
"For sentences of FACTUAL, position of sentence in article
plays an important role, as well as their similarity to the claim based on ROUGE scores.",5 Supporting Argument Detection,[0],[0]
"For type OPINION, unlike all other types, position of sentence seems to be insignificant.",5 Supporting Argument Detection,[0],[0]
"As we could imagine, opinionated information might scatter around the whole documents.",5 Supporting Argument Detection,[0],[0]
"For sentences of REASONING, the ones that can be used as supporting arguments tend to be less concrete and less emotional, as opposed to opinion.",5 Supporting Argument Detection,[0],[0]
We presented a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim.,6 Conclusion,[0],[0]
"Based on our newly-collected dataset, we characterized arguments of different types with a rich feature set.",6 Conclusion,[0],[0]
We also showed that leveraging argument type information can further improve the performance of supporting argument detection.,6 Conclusion,[0],[0]
This work was supported in part by National Science Foundation Grant IIS-1566382 and a GPU gift from Nvidia.,Acknowledgments,[0],[0]
We thank Kechen Qin for his help on data collection.,Acknowledgments,[0],[0]
We also appreciate the valuable suggestions on various aspects of this work from three anonymous reviewers.,Acknowledgments,[0],[0]
We investigate the problem of sentence-level supporting argument detection from relevant documents for user-specified claims.,abstractText,[0],[0]
A dataset containing claims and associated citation articles is collected from online debate website idebate.org.,abstractText,[0],[0]
"We then manually label sentence-level supporting arguments from the documents along with their types as STUDY, FACTUAL, OPINION, or REASONING.",abstractText,[0],[0]
"We further characterize arguments of different types, and explore whether leveraging type information can facilitate the supporting arguments detection task.",abstractText,[0],[0]
"Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information.",abstractText,[0],[0]
Understanding and Detecting Supporting Arguments of Diverse Types,title,[0],[0]
A key question often asked of machine learning systems is “Why did the system make this prediction?”,1. Introduction,[0],[0]
We want models that are not just high-performing but also explainable.,1. Introduction,[0],[0]
"By understanding why a model does what it does, we can hope to improve the model (Amershi et al., 2015), discover new science (Shrikumar et al., 2017), and provide end-users with explanations of actions that impact them (Goodman & Flaxman, 2016).
",1. Introduction,[0],[0]
"However, the best-performing models in many domains — e.g., deep neural networks for image and speech recognition (Krizhevsky et al., 2012) — are complicated, blackbox models whose predictions seem hard to explain.",1. Introduction,[0],[0]
"Work on interpreting these black-box models has focused on understanding how a fixed model leads to particular predictions, e.g., by locally fitting a simpler model around the test
1Stanford University, Stanford, CA.",1. Introduction,[0],[0]
"Correspondence to: Pang Wei Koh <pangwei@cs.stanford.edu>, Percy Liang <pliang@cs.stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
point (Ribeiro et al., 2016) or by perturbing the test point to see how the prediction changes (Simonyan et al., 2013; Li et al., 2016b; Datta et al., 2016; Adler et al., 2016).",1. Introduction,[0],[0]
"These works explain the predictions in terms of the model, but how can we explain where the model came from?
",1. Introduction,[0],[0]
"In this paper, we tackle this question by tracing a model’s predictions through its learning algorithm and back to the training data, where the model parameters ultimately derive from.",1. Introduction,[0],[0]
"To formalize the impact of a training point on a prediction, we ask the counterfactual: what would happen if we did not have this training point, or if the values of this training point were changed slightly?
",1. Introduction,[0],[0]
Answering this question by perturbing the data and retraining the model can be prohibitively expensive.,1. Introduction,[0],[0]
"To overcome this problem, we use influence functions, a classic technique from robust statistics (Hampel, 1974) that tells us how the model parameters change as we upweight a training point by an infinitesimal amount.",1. Introduction,[0],[0]
"This allows us to “differentiate through the training” to estimate in closed-form the effect of a variety of training perturbations.
",1. Introduction,[0],[0]
"Despite their rich history in statistics, influence functions have not seen widespread use in machine learning; to the best of our knowledge, the work closest to ours is Wojnowicz et al. (2016), which introduced a method for approximating a quantity related to influence in generalized linear models.",1. Introduction,[0],[0]
"One obstacle to adoption is that influence functions require expensive second derivative calculations and assume model differentiability and convexity, which limits their applicability in modern contexts where models are often non-differentiable, non-convex, and highdimensional.",1. Introduction,[0],[0]
"We address these challenges by showing that we can efficiently approximate influence functions using second-order optimization techniques (Pearlmutter, 1994; Martens, 2010; Agarwal et al., 2016), and that they remain accurate even as the underlying assumptions of differentiability and convexity degrade.
",1. Introduction,[0],[0]
Influence functions capture the core idea of studying models through the lens of their training data.,1. Introduction,[0],[0]
"We show that they are a versatile tool that can be applied to a wide variety of seemingly disparate tasks: understanding model behavior, debugging models, detecting dataset errors, and creating visually-indistinguishable adversarial training examples that can flip neural network test predictions, the training set analogue of Goodfellow et al. (2015).
",1. Introduction,[0],[0]
"ar X
iv :1
70 3.
04 73
0v 3
[ st
at .M
L ]
2 9
D ec
2 02
0",1. Introduction,[0],[0]
"Consider a prediction problem from some input space X (e.g., images) to an output space Y (e.g., labels).",2. Approach,[0],[0]
"We are given training points z1, . . .",2. Approach,[0],[0]
", zn, where zi = (xi, yi) ∈ X × Y .",2. Approach,[0],[0]
"For a point z and parameters θ ∈ Θ, let L(z, θ) be the loss, and let",2. Approach,[0],[0]
1n,2. Approach,[0],[0]
∑n i=1,2. Approach,[0],[0]
"L(zi, θ) be the empirical risk.",2. Approach,[0],[0]
The empirical risk minimizer is given by θ̂ def = arg minθ∈Θ 1 n,2. Approach,[0],[0]
∑n i=1,2. Approach,[0],[0]
"L(zi, θ).
",2. Approach,[0],[0]
1 Assume that the empirical risk is twice-differentiable and strictly convex in θ; in Section 4 we explore relaxing these assumptions.,2. Approach,[0],[0]
Our goal is to understand the effect of training points on a model’s predictions.,2.1. Upweighting a training point,[0],[0]
"We formalize this goal by asking the counterfactual: how would the model’s predictions change if we did not have this training point?
",2.1. Upweighting a training point,[0],[0]
Let us begin by studying the change in model parameters due to removing a point z from the training set.,2.1. Upweighting a training point,[0],[0]
"Formally, this change is θ̂−z − θ̂, where θ̂−z def = arg minθ∈Θ ∑ zi 6=z L(zi, θ).",2.1. Upweighting a training point,[0],[0]
"However, retraining the model for each removed z is prohibitively slow.
",2.1. Upweighting a training point,[0],[0]
"Fortunately, influence functions give us an efficient approximation.",2.1. Upweighting a training point,[0],[0]
"The idea is to compute the parameter change if z were upweighted by some small , giving us new parameters θ̂ ,z def = arg",2.1. Upweighting a training point,[0],[0]
minθ∈Θ 1 n,2.1. Upweighting a training point,[0],[0]
∑n i=1,2.1. Upweighting a training point,[0],[0]
"L(zi, θ) + L(z, θ).",2.1. Upweighting a training point,[0],[0]
"A classic result (Cook & Weisberg, 1982) tells us that the influence of upweighting z on the parameters θ̂ is given by
Iup,params(z) def = dθ̂ ,z d ∣∣∣ =0 = −H−1 θ̂ ∇θL(z, θ̂), (1)
where Hθ̂ def = 1n",2.1. Upweighting a training point,[0],[0]
"∑n i=1∇2θL(zi, θ̂) is the Hessian and is positive definite (PD) by assumption.",2.1. Upweighting a training point,[0],[0]
"In essence, we are forming a quadratic approximation to the empirical risk around θ̂ and take a single Newton step; see appendix A for a derivation.",2.1. Upweighting a training point,[0],[0]
"Since removing a point z is the same as upweighting it by = − 1n , we can linearly approximate the parameter change due to removing z without retraining the model by computing θ̂−z",2.1. Upweighting a training point,[0],[0]
− θ̂,2.1. Upweighting a training point,[0],[0]
"≈ − 1nIup,params(z).
",2.1. Upweighting a training point,[0],[0]
"Next, we apply the chain rule to measure how upweighting z changes functions of θ̂. In particular, the influence of upweighting z on the loss at a test point ztest again has a closed-form expression:
Iup,loss(z, ztest) def =
dL(ztest, θ̂ ,z)
",2.1. Upweighting a training point,[0],[0]
"d
∣∣∣",2.1. Upweighting a training point,[0],[0]
"=0
(2)
= ∇θL(ztest, θ̂)>",2.1. Upweighting a training point,[0],[0]
"dθ̂ ,z d ∣∣∣ =0
= −∇θL(ztest, θ̂)>H−1θ̂ ∇θL(z, θ̂).",2.1. Upweighting a training point,[0],[0]
1We fold in any regularization terms into L.,2.1. Upweighting a training point,[0],[0]
"Let us develop a finer-grained notion of influence by studying a different counterfactual: how would the model’s predictions change if a training input were modified?
",2.2. Perturbing a training input,[0],[0]
"For a training point z = (x, y), define zδ def = (x + δ, y).",2.2. Perturbing a training input,[0],[0]
"Consider the perturbation z 7→ zδ , and let θ̂zδ,−z be the empirical risk minimizer on the training points with zδ in place of z. To approximate its effects, define the parameters resulting from moving mass from z onto zδ: θ̂ ,zδ,−z def = arg minθ∈Θ 1 n",2.2. Perturbing a training input,[0],[0]
∑n i=1,2.2. Perturbing a training input,[0],[0]
"L(zi, θ) + L(zδ, θ)",2.2. Perturbing a training input,[0],[0]
"− L(z, θ).",2.2. Perturbing a training input,[0],[0]
"An analogous calculation to (1) yields:
dθ̂ ,zδ,−z d ∣∣∣ =0 = Iup,params(zδ)− Iup,params(z)
= −H−1 θ̂
( ∇θL(zδ, θ̂)−∇θL(z, θ̂) ) .",2.2. Perturbing a training input,[0],[0]
"(3)
As before, we can make the linear approximation θ̂zδ,−z − θ̂",2.2. Perturbing a training input,[0],[0]
"≈ 1n (Iup,params(zδ) − Iup,params(z)), giving us a closedform estimate of the effect of z 7→ zδ on the model.",2.2. Perturbing a training input,[0],[0]
"Analogous equations also apply for changes in y. While influence functions might appear to only work for infinitesimal (therefore continuous) perturbations, it is important to note that this approximation holds for arbitrary δ: the - upweighting scheme allows us to smoothly interpolate between z and zδ .",2.2. Perturbing a training input,[0],[0]
"This is particularly useful for working with discrete data (e.g., in NLP) or with discrete label changes.
",2.2. Perturbing a training input,[0],[0]
"If x is continuous and δ is small, we can further approximate (3).",2.2. Perturbing a training input,[0],[0]
"Assume that the input domain X ⊆ Rd, the parameters Θ ⊆ Rp, and L is differentiable in θ and x.",2.2. Perturbing a training input,[0],[0]
"As ‖δ‖ → 0,∇θL(zδ, θ̂)−∇θL(z, θ̂)",2.2. Perturbing a training input,[0],[0]
≈,2.2. Perturbing a training input,[0],[0]
"[∇x∇θL(z, θ̂)]δ, where ∇x∇θL(z, θ̂) ∈ Rp×d.",2.2. Perturbing a training input,[0],[0]
"Substituting into (3),
dθ̂ ,zδ,−z d ∣∣∣ =0",2.2. Perturbing a training input,[0],[0]
≈ −H−1 θ̂,2.2. Perturbing a training input,[0],[0]
"[∇x∇θL(z, θ̂)]δ.",2.2. Perturbing a training input,[0],[0]
"(4)
We thus have θ̂zδ,−z − θ̂ ≈",2.2. Perturbing a training input,[0],[0]
− 1nH −1 θ̂,2.2. Perturbing a training input,[0],[0]
"[∇x∇θL(z, θ̂)]δ.",2.2. Perturbing a training input,[0],[0]
Differentiating w.r.t.,2.2. Perturbing a training input,[0],[0]
"δ and applying the chain rule gives us
Ipert,loss(z, ztest) def = ∇δL(ztest, θ̂zδ,−z) ∣∣∣ δ=0
(5)
",2.2. Perturbing a training input,[0],[0]
"= −∇θL(ztest, θ̂)>H−1θ̂ ∇x∇θL(z, θ̂).
",2.2. Perturbing a training input,[0],[0]
"[Ipert,loss(z, ztest)]δ tells us the approximate effect that z 7→",2.2. Perturbing a training input,[0],[0]
z+ δ has on the loss at ztest.,2.2. Perturbing a training input,[0],[0]
"By setting δ in the direction of Ipert,loss(z, ztest)>, we can construct local perturbations of z that maximally increase the loss at ztest.",2.2. Perturbing a training input,[0],[0]
"In Section 5.2, we will use this to construct training-set attacks.",2.2. Perturbing a training input,[0],[0]
"Finally, we note that Ipert,loss(z, ztest) can help us identify the features of z that are most responsible for the prediction on ztest.",2.2. Perturbing a training input,[0],[0]
"To find the training points most relevant to a test point, it is common to look at its nearest neighbors in Euclidean
space",2.3. Relation to Euclidean distance,[0],[0]
"(e.g., Ribeiro et al. (2016)); if all points have the same norm, this is equivalent to choosing x with the largest x ·xtest.",2.3. Relation to Euclidean distance,[0],[0]
"For intuition, we compare this to Iup,loss(z, ztest) on a logistic regression model and show that influence is much more accurate at accounting for the effect of training.
",2.3. Relation to Euclidean distance,[0],[0]
"Let p(y | x) = σ(yθ>x), with y ∈ {−1, 1} and σ(t) = 1 1+exp(−t) .",2.3. Relation to Euclidean distance,[0],[0]
We seek to maximize the probability of the training set.,2.3. Relation to Euclidean distance,[0],[0]
"For a training point z = (x, y), L(z, θ) = log(1 + exp(−yθ>x)), ∇θL(z, θ) = −σ(−yθ>x)yx, and Hθ = 1n",2.3. Relation to Euclidean distance,[0],[0]
∑n i=1,2.3. Relation to Euclidean distance,[0],[0]
"σ(θ
>xi)σ(−θ>xi)xix>i .",2.3. Relation to Euclidean distance,[0],[0]
"From (2), Iup,loss(z, ztest) is:
−ytesty · σ(−ytestθ>xtest) ·",2.3. Relation to Euclidean distance,[0],[0]
σ(−yθ>x) ·,2.3. Relation to Euclidean distance,[0],[0]
"x>testH−1θ̂ x.
We highlight two key differences from x · xtest.",2.3. Relation to Euclidean distance,[0],[0]
"First, σ(−yθ>x) gives points with high training loss more influence, revealing that outliers can dominate the model parameters.",2.3. Relation to Euclidean distance,[0],[0]
"Second, the weighted covariance matrix H−1 θ̂ measures the “resistance” of the other training points to the removal of z; if ∇θL(z, θ̂) points in a direction of little variation, its influence will be higher since moving in that direction will not significantly increase the loss on other training points.",2.3. Relation to Euclidean distance,[0],[0]
"As we show in Fig 1, these differences mean that influence functions capture the effect of model training much more accurately than nearest neighbors.",2.3. Relation to Euclidean distance,[0],[0]
"There are two challenges to efficiently computing Iup,loss(z, ztest) = −∇θL(ztest, θ̂)>H−1θ̂ ∇θL(z, θ̂).",3. Efficiently calculating influence,[0],[0]
"First, it requires forming and inverting Hθ̂",3. Efficiently calculating influence,[0],[0]
"= 1 n ∑n i=1∇2θL(zi, θ̂), the Hessian of the empirical risk.",3. Efficiently calculating influence,[0],[0]
"With n training points and θ ∈ Rp, this requires O(np2 + p3) operations, which
is too expensive for models like deep neural networks with millions of parameters.",3. Efficiently calculating influence,[0],[0]
"Second, we need to calculate Iup,loss(zi, ztest) across all training points zi.
",3. Efficiently calculating influence,[0],[0]
The first problem is well-studied in second-order optimization.,3. Efficiently calculating influence,[0],[0]
"The idea is to avoid explicitly computing H−1
θ̂ ; in-
stead, we use implicit Hessian-vector products (HVPs) to efficiently approximate stest def = H−1
θ̂ ∇θL(ztest, θ̂) and then
compute Iup,loss(z, ztest) = −stest · ∇θL(z, θ̂).",3. Efficiently calculating influence,[0],[0]
"This also solves the second problem: for each test point of interest, we can precompute stest and then efficiently compute −stest · ∇θL(zi, θ̂) for each training point zi.
",3. Efficiently calculating influence,[0],[0]
"We discuss two techniques for approximating stest, both relying on the fact that the HVP of a single term in Hθ̂, [∇2θL(zi, θ̂)]v, can be computed for arbitrary v in the same time that ∇θL(zi, θ̂) would take, which is typically O(p) (Pearlmutter, 1994).
",3. Efficiently calculating influence,[0],[0]
Conjugate gradients (CG).,3. Efficiently calculating influence,[0],[0]
The first technique is a standard transformation of matrix inversion into an optimization problem.,3. Efficiently calculating influence,[0],[0]
Since Hθ̂ 0,3. Efficiently calculating influence,[0],[0]
"by assumption, H −1 θ̂ v ≡ arg mint{t",3. Efficiently calculating influence,[0],[0]
>,3. Efficiently calculating influence,[0],[0]
Hθ̂t − v >t}.,3. Efficiently calculating influence,[0],[0]
"We can solve this with CG approaches that only require the evaluation of Hθ̂t, which takesO(np) time, without explicitly formingHθ̂. While an exact solution takes p CG iterations, in practice we can get a good approximation with fewer iterations; see Martens (2010) for more details.
",3. Efficiently calculating influence,[0],[0]
Stochastic estimation.,3. Efficiently calculating influence,[0],[0]
"With large datasets, standard CG can be slow; each iteration still goes through all n training points.",3. Efficiently calculating influence,[0],[0]
"We use a method developed by Agarwal et al. (2016) to get an estimator that only samples a single point per iteration, which results in significant speedups.
",3. Efficiently calculating influence,[0],[0]
"Dropping the θ̂ subscript for clarity, letH−1j def = ∑j i=0(I− H)i, the first j terms in the Taylor expansion of H−1.",3. Efficiently calculating influence,[0],[0]
Rewrite this recursively as H−1j = I +,3. Efficiently calculating influence,[0],[0]
(I − H)H −1 j−1.,3. Efficiently calculating influence,[0],[0]
"From the validity of the Taylor expansion, H−1j → H−1 as j → ∞.2 The key is that at each iteration, we can substitute the full H with a draw from any unbiased (and fasterto-compute) estimator of H to form H̃j .",3. Efficiently calculating influence,[0],[0]
"Since E[H̃−1j ] = H−1j , we still have E[H̃ −1",3. Efficiently calculating influence,[0],[0]
"j ]→ H−1.
",3. Efficiently calculating influence,[0],[0]
"In particular, we can use ∇2θL(zi, θ̂), for any zi, as an unbiased estimator of H .",3. Efficiently calculating influence,[0],[0]
"This gives us the following procedure: uniformly sample t points zs1 , . . .",3. Efficiently calculating influence,[0],[0]
", zst from the training data; define H̃−10 v = v; and recursively compute H̃−1j v = v",3. Efficiently calculating influence,[0],[0]
"+ ( I − ∇2θL(zsj , θ̂) )",3. Efficiently calculating influence,[0],[0]
"H̃−1j−1v, taking H̃ −1",3. Efficiently calculating influence,[0],[0]
t v,3. Efficiently calculating influence,[0],[0]
as our final unbiased estimate of H−1v.,3. Efficiently calculating influence,[0],[0]
"We pick t to be large enough such that H̃t stabilizes, and to reduce variance we repeat this procedure r times and average results.",3. Efficiently calculating influence,[0],[0]
"Empirically, we found this significantly faster than CG.
",3. Efficiently calculating influence,[0],[0]
"We note that the original method of Agarwal et al. (2016) dealt only with generalized linear models, for which [∇2θL(zi, θ̂)]v can be efficiently computed in O(p) time.",3. Efficiently calculating influence,[0],[0]
"In our case, we rely on Pearlmutter (1994)’s more general algorithm for fast HVPs, described above, to achieve the same time complexity.3
With these techniques, we can compute Iup,loss(zi, ztest) on all training points zi in O(np + rtp) time; we show in Section 4.1 that empirically, choosing rt = O(n) gives accurate results.",3. Efficiently calculating influence,[0],[0]
"Similarly, we can compute Ipert,loss(zi, ztest) =",3. Efficiently calculating influence,[0],[0]
"− 1n∇θL(ztest, θ̂) >H−1 θ̂ ∇x∇θL(zi, θ̂) with two matrix-vector products: we first compute stest, then find s>test∇x∇θL(zi, θ̂) with the same HVP trick.",3. Efficiently calculating influence,[0],[0]
"These computations are easy to implement in auto-grad systems like TensorFlow (Abadi et al., 2015) and Theano (Theano D. Team, 2016), as users need only specify the loss; the rest is automatically handled.",3. Efficiently calculating influence,[0],[0]
"Recall that influence functions are asymptotic approximations of leave-one-out retraining under the assumptions that (i) the model parameters θ̂ minimize the empirical risk, and that (ii) the empirical risk is twice-differentiable and strictly convex.",4. Validation and extensions,[0],[0]
"Here, we empirically show that influence functions are accurate approximations (Section 4.1) that
2We assume w.l.o.g.",4. Validation and extensions,[0],[0]
"that ∀i,∇2θL(zi, θ̂) 4 I; if this is not true, we can scale the loss down without affecting the parameters.",4. Validation and extensions,[0],[0]
"In some cases, we can get an upper bound on∇2θL(zi, θ̂) (e.g., for linear models and bounded input), which makes this easy.",4. Validation and extensions,[0],[0]
"Otherwise, we treat the scaling as a separate hyperparameter and tune it such that the Taylor expansion converges.
",4. Validation and extensions,[0],[0]
"3To increase stability, especially with non-convex models (see Section 4.2), we can also sample a minibatch of training points at each iteration, instead of relying on a single training point.
provide useful information even when these assumptions are violated (Sections 4.2, 4.3).",4. Validation and extensions,[0],[0]
Influence functions assume that the weight on a training point is changed by an infinitesimally small .,4.1. Influence functions vs. leave-one-out retraining,[0],[0]
"To investigate the accuracy of using influence functions to approximate the effect of removing a training point and retraining, we compared − 1nIup,loss(z, ztest) with L(ztest, θ̂−z)",4.1. Influence functions vs. leave-one-out retraining,[0],[0]
"− L(ztest, θ̂) (i.e., actually doing leave-one-out retraining).",4.1. Influence functions vs. leave-one-out retraining,[0],[0]
"With a logistic regression model on 10-class MNIST,4 the predicted and actual changes matched closely (Fig 2-Left).
",4.1. Influence functions vs. leave-one-out retraining,[0],[0]
"The stochastic approximation from Agarwal et al. (2016) was also accurate with r = 10 repeats and t = 5, 000 iterations (Fig 2-Mid).",4.1. Influence functions vs. leave-one-out retraining,[0],[0]
"Since each iteration only requires one HVP [∇2θL(zi, θ̂)]v, this runs quickly: in fact, we accurately estimated H−1v without even looking at every data point, since n = 55, 000 > rt.",4.1. Influence functions vs. leave-one-out retraining,[0],[0]
"Surprisingly, even r = 1 worked; while results were noisier, it was still able to identify the most influential points.",4.1. Influence functions vs. leave-one-out retraining,[0],[0]
"In Section 2, we took θ̂ as the global minimum.",4.2. Non-convexity and non-convergence,[0],[0]
"In practice, if we obtain our parameters θ̃ by running SGD with early stopping or on non-convex objectives, θ̃ 6= θ̂. As a result, Hθ̃ could have negative eigenvalues.",4.2. Non-convexity and non-convergence,[0],[0]
"We show that influence functions on θ̃ still give meaningful results in practice.
",4.2. Non-convexity and non-convergence,[0],[0]
"Our approach is to form a convex quadratic approximation of the loss around θ̃, i.e., L̃(z, θ) = L(z, θ̃) + ∇L(z, θ̃)>(θ− θ̃)+ 12 (θ− θ̃)
>(Hθ̃+λI)(θ− θ̃).",4.2. Non-convexity and non-convergence,[0],[0]
"Here, λ is a damping term that we add ifHθ̃ has negative eigenvalues;
4We trained with L-BFGS (Liu & Nocedal, 1989), with L2 regularization of 0.01, n = 55, 000, and p = 7, 840 parameters.
",4.2. Non-convexity and non-convergence,[0],[0]
this corresponds to adding L2 regularization on the parameters.,4.2. Non-convexity and non-convergence,[0],[0]
"We then calculate Iup,loss using L̃.",4.2. Non-convexity and non-convergence,[0],[0]
"If θ̃ is close to a local minimum, this is correlated with the result of taking a Newton step from θ̃ after removing weight from z (see appendix B).
",4.2. Non-convexity and non-convergence,[0],[0]
"We checked the behavior of Iup,loss in a non-convergent, non-convex setting by training a convolutional neural network for 500k iterations.5",4.2. Non-convexity and non-convergence,[0],[0]
"The model had not converged and Hθ̃ was not PD, so we added a damping term with λ = 0.01.",4.2. Non-convexity and non-convergence,[0],[0]
"Even in this difficult setting, the predicted and actual changes in loss were highly correlated (Pearson’s R = 0.86, Fig 2-Right).",4.2. Non-convexity and non-convergence,[0],[0]
"What happens when the derivatives of the loss, ∇θL and ∇2θL, do not exist?",4.3. Non-differentiable losses,[0],[0]
"In this section, we show that influence functions computed on smooth approximations to non-differentiable losses can predict the behavior of the original, non-differentiable loss under leave-one-out retraining.",4.3. Non-differentiable losses,[0],[0]
"The robustness of this approximation suggests that we can train non-differentiable models and swap out non-differentiable components for smoothed versions for the purposes of calculating influence.
",4.3. Non-differentiable losses,[0],[0]
"To see this, we trained a linear SVM on the same 1s vs. 7s MNIST task in Section 2.3.",4.3. Non-differentiable losses,[0],[0]
"This involves minimizing Hinge(s) = max(0, 1 − s); this simple piece-
5The network had 7 sets of convolutional layers with tanh(·) non-linearities, modeled after the all-convolutional network from (Springenberg et al., 2014).",4.3. Non-differentiable losses,[0],[0]
"For speed, we used 10% of the MNIST training set and only 2,616 parameters, since repeatedly retraining the network was expensive.",4.3. Non-differentiable losses,[0],[0]
"Training was done with mini-batches of 500 examples and the Adam optimizer (Kingma & Ba, 2015).",4.3. Non-differentiable losses,[0],[0]
"The model had not converged after 500k iterations; training it for another 500k iterations, using a full training pass for each iteration, reduced train loss from 0.14 to 0.12.
",4.3. Non-differentiable losses,[0],[0]
"wise linear function is similar to ReLUs, which cause nondifferentiability in neural networks.",4.3. Non-differentiable losses,[0],[0]
"We set the derivatives at the hinge to 0 and calculated Iup,loss.",4.3. Non-differentiable losses,[0],[0]
"As one might expect, this was inaccurate (Fig 3b-Left): the second derivative carries no information about how close a support vector z is to the hinge, so the quadratic approximation of L(z, θ̂) is linear, which leads to Iup,loss(z, ztest) overestimating the influence of z.
For the purposes of calculating influence, we approximated Hinge(s) with SmoothHinge(s, t) =",4.3. Non-differentiable losses,[0],[0]
"t log(1+exp( 1−st )), which approaches the hinge loss as t → 0 (Fig 3a).",4.3. Non-differentiable losses,[0],[0]
"Using the same SVM weights as before, we found that calculating Iup,loss using SmoothHinge(s, 0.001) closely matched the actual change due to retraining in the original Hinge(s) (Pearson’s R = 0.95; Fig 3b-Mid) and remained accurate over a wide range of t (Fig 3b-Right).",4.3. Non-differentiable losses,[0],[0]
"By telling us the training points “responsible” for a given prediction, influence functions reveal insights about how models rely on and extrapolate from the training data.",5.1. Understanding model behavior,[0],[0]
"In this section, we show that two models can make the same correct predictions but get there in very different ways.
",5.1. Understanding model behavior,[0],[0]
"We compared (a) the state-of-the-art Inception v3 network (Szegedy et al., 2016) with all but the top layer frozen6 and (b) an SVM with an RBF kernel on a dog vs. fish image classification dataset we extracted from ImageNet (Russakovsky et al., 2015), with 900 training examples for each class.",5.1. Understanding model behavior,[0],[0]
"Freezing neural networks in this way is not uncommon in computer vision and is equivalent to training a
6We used pre-trained weights from Keras (Chollet, 2015).
",5.1. Understanding model behavior,[0],[0]
"logistic regression model on the bottleneck features (Donahue et al., 2014).",5.1. Understanding model behavior,[0],[0]
"We picked a test image both models got correct (Fig 4-Top) and used SmoothHinge(·, 0.001) to compute the influence for the SVM.
",5.1. Understanding model behavior,[0],[0]
"As expected, Iup,loss in the RBF SVM varied inversely with raw pixel distance, with training images far from the test image in pixel space having almost no influence; the Inception influences were much less correlated with distance in pixel space (Fig 4-Left).",5.1. Understanding model behavior,[0],[0]
"Looking at the two most helpful images (most positive −Iup,loss) for each model in Fig 4-Right, we see that the Inception network picked on the distinctive characteristics of clownfish, whereas the RBF SVM pattern-matched training images superficially.
",5.1. Understanding model behavior,[0],[0]
"Moreover, in the RBF SVM, fish (green points) close to the test image were mostly helpful, while dogs (red) were mostly harmful, with the RBF acting as a soft nearest neighbor function (Fig 4-Left).",5.1. Understanding model behavior,[0],[0]
"In contrast, in the Inception network, fish and dogs could be helpful or harmful for correctly classifying the test image as a fish; in fact, the 5th most helpful training image was a dog that, to the model, looked very different from the test fish (Fig 4-Top).",5.1. Understanding model behavior,[0],[0]
"In this section, we show that models that place a lot of influence on a small number of points can be vulnerable to training input perturbations, posing a serious security risk in real-world ML systems where attackers can influence the training data (Huang et al., 2011).",5.2. Adversarial training examples,[0],[0]
"Recent work has generated adversarial test images that are visually indistinguish-
able from real test images but completely fool a classifier (Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016).",5.2. Adversarial training examples,[0],[0]
We demonstrate that influence functions can be used to craft adversarial training images that are similarly visuallyindistinguishable and can flip a model’s prediction on a separate test image.,5.2. Adversarial training examples,[0],[0]
"To the best of our knowledge, this is the first proof-of-concept that visually-indistinguishable training attacks can be executed on otherwise highly-accurate neural networks.
",5.2. Adversarial training examples,[0],[0]
"The key idea is that Ipert,loss(z, ztest) tells us how to modify training point z to most increase the loss on ztest.",5.2. Adversarial training examples,[0],[0]
"Concretely, for a target test image ztest, we can construct z̃i, an adversarial version of a training image zi, by initializing z̃i",5.2. Adversarial training examples,[0],[0]
:= zi and then iterating z̃i,5.2. Adversarial training examples,[0],[0]
":= Π(z̃i + α sign(Ipert,loss(z̃i, ztest))), where α is a step size and Π projects onto the set of valid images that share the same 8- bit representation with zi.",5.2. Adversarial training examples,[0],[0]
"After each iteration, we retrain the model.",5.2. Adversarial training examples,[0],[0]
"This is an iterated, training-set analogue of the methods used by, e.g., Goodfellow et al. (2015); MoosaviDezfooli et al. (2016) for test-set attacks.
",5.2. Adversarial training examples,[0],[0]
"We tested these adversarial training perturbations on the same Inception network on dogs vs. fish from Section 5.1, choosing this pair of animals to provide a stark contrast between the classes.",5.2. Adversarial training examples,[0],[0]
We set α = 0.02 and ran the attack for 100 iterations on each test image.,5.2. Adversarial training examples,[0],[0]
"As before, we froze all but the top layer for training; note that computing Ipert,loss still involves differentiating through the entire network.",5.2. Adversarial training examples,[0],[0]
"Originally, the model correctly classified 591 / 600 test images.",5.2. Adversarial training examples,[0],[0]
"For each of these 591 test images, considered separately, we tried to find a visually-indistinguishable perturbation (i.e., same 8-bit representation) to a single training image, out of 1,800 total training images, that would flip the model’s prediction.",5.2. Adversarial training examples,[0],[0]
We were able to do this on 335 (57%) of the 591 test images.,5.2. Adversarial training examples,[0],[0]
"If we perturbed 2 training images for each test image, we could flip predictions on 77% of the 591 test images; and if we perturbed 10 training images, we could flip all but 1 of the 591.",5.2. Adversarial training examples,[0],[0]
"The above results are from attacking each test image separately, i.e., we use a different training set to attack each test image.",5.2. Adversarial training examples,[0],[0]
"We next tried to attack multiple test images simultaneously by increasing their average test loss, and found that single training image perturbations could simultaneously flip multiple test predictions as well (Fig 5).
",5.2. Adversarial training examples,[0],[0]
We make three observations about these attacks.,5.2. Adversarial training examples,[0],[0]
"First, though the change in pixel values is small, the change in the final Inception feature layer is significantly larger: in pixel space and using L2 distance, the training values change by less than 1% of the mean distance of a training point to the class centroid, whereas in Inception feature space, the change is on the same order as the mean distance.",5.2. Adversarial training examples,[0],[0]
"Second, the attack tries to perturb the training example in a direction of low variance, causing the model to overfit in that
direction and consequently incorrectly classify the test images; we expect the attack to become harder as the number of training examples grows.",5.2. Adversarial training examples,[0],[0]
"Third, ambiguous or mislabeled training images are effective points to attack, since the model has low confidence and thus high loss on them, making them highly influential (recall Section 2.3).",5.2. Adversarial training examples,[0],[0]
"For example, the image in Fig 5 contains both a dog and a fish and is highly ambiguous; as a result, it is the training example that the model is least confident on (with a confidence of 77%, compared to the next lowest confidence of 90%).
",5.2. Adversarial training examples,[0],[0]
This attack is mathematically equivalent to the gradientbased training set attacks explored by Biggio et al. (2012); Mei & Zhu (2015b) and others in the context of different models.,5.2. Adversarial training examples,[0],[0]
"Biggio et al. (2012) constructed a dataset poisoning attack against a linear SVM on a two-class MNIST task, but had to modify the training points in an obviously distinguishable way to be effective.",5.2. Adversarial training examples,[0],[0]
"Measuring the magnitude of Ipert,loss gives model developers a way of quantifying how vulnerable their models are to training-set attacks.",5.2. Adversarial training examples,[0],[0]
"Domain mismatch — where the training distribution does not match the test distribution — can cause models with high training accuracy to do poorly on test data (Ben-David et al., 2010).",5.3. Debugging domain mismatch,[0],[0]
"We show that influence functions can identify the training examples most responsible for the errors, helping model developers identify domain mismatch.
",5.3. Debugging domain mismatch,[0],[0]
"As a case study, we predicted whether a patient would be readmitted to a hospital.",5.3. Debugging domain mismatch,[0],[0]
"Domain mismatches are common in biomedical data; for example, different hospitals can serve very different populations, and readmission models trained on one population can do poorly on another (Kansagara et al., 2011).",5.3. Debugging domain mismatch,[0],[0]
"We used logistic regression to predict readmission with a balanced training dataset of 20K diabetic patients from 100+ US hospitals, each represented by 127 features (Strack et al., 2014).7
7Hospital readmission was defined as whether a patient would be readmitted within the next 30 days.",5.3. Debugging domain mismatch,[0],[0]
"Features were demo-
3 out of the 24 children under age 10 in this dataset were re-admitted.",5.3. Debugging domain mismatch,[0],[0]
"To induce a domain mismatch, we filtered out 20 children who were not re-admitted, leaving 3 out of 4 readmitted.",5.3. Debugging domain mismatch,[0],[0]
This caused the model to wrongly classify many children in the test set.,5.3. Debugging domain mismatch,[0],[0]
"Our aim is to identify the 4 children in the training set as being “responsible” for these errors.
",5.3. Debugging domain mismatch,[0],[0]
"As a baseline, we tried the common practice of looking at the learned parameters θ̂ to see if the indicator variable for being a child was obviously different.",5.3. Debugging domain mismatch,[0],[0]
"However, this did not work: 14/127 features had a larger coefficient.
",5.3. Debugging domain mismatch,[0],[0]
"Picking a random child ztest that the model got wrong, we calculated−Iup,loss(zi, ztest) for each training point zi.",5.3. Debugging domain mismatch,[0],[0]
"This clearly highlighted the 4 training children, each of whom were 30-40 times as influential as the next most influential examples.",5.3. Debugging domain mismatch,[0],[0]
"The 1 child in the training set who was not readmitted had a very positive influence, while the other 3 had very negative influences.",5.3. Debugging domain mismatch,[0],[0]
"Calculating Ipert,loss on these 4 children showed that a change in the ‘child’ indicator variable had by far the largest effect on Iup,loss.",5.3. Debugging domain mismatch,[0],[0]
"Labels in the real world are often noisy, especially if crowdsourced (Frénay & Verleysen, 2014), and can even be adversarially corrupted, as in Section 5.2.",5.4. Fixing mislabeled examples,[0],[0]
"Even if a human expert could recognize wrongly labeled examples, it is impossible in many applications to manually review all of the training data.",5.4. Fixing mislabeled examples,[0],[0]
"We show that influence functions can help human experts prioritize their attention, allowing them to inspect only the examples that actually matter.
",5.4. Fixing mislabeled examples,[0],[0]
The key idea is to flag the training points that exert the most influence on the model.,5.4. Fixing mislabeled examples,[0],[0]
"Because we do not have access to the test set, we measure the influence of zi with Iup,loss(zi, zi), which approximates the error incurred on zi if we remove zi from the training set.
",5.4. Fixing mislabeled examples,[0],[0]
"Our case study is email spam classification, which relies
graphic (e.g., age, race, gender), administrative (e.g., length of hospital stay), or medical (e.g., test results).
on user-provided labels and is also vulnerable to adversarial attack (Biggio et al., 2011).",5.4. Fixing mislabeled examples,[0],[0]
"We flipped the labels of a random 10% of the training data and then simulated manually inspecting a fraction of the training points, correcting them if they had been flipped.",5.4. Fixing mislabeled examples,[0],[0]
"Using influence functions to prioritize the training points to inspect allowed us to repair the dataset (Fig 6, blue) without checking too many points, outperforming the baselines of checking points with the highest train loss (Fig 6, green) or at random (Fig 6, red).",5.4. Fixing mislabeled examples,[0],[0]
No method had access to the test data.,5.4. Fixing mislabeled examples,[0],[0]
"The use of influence-based diagnostics originated in statistics in the 70s, driven by the seminal papers of Hampel (1974) and Jaeckel (1972) (where it was called the infinitesimal jackknife).",6. Related work,[0],[0]
"It was further developed in the book by Hampel et al. (1986) and many other contemporary papers (Cook, 1977; Cook & Weisberg, 1980; Pregibon et al., 1981; Cook & Weisberg, 1982).",6. Related work,[0],[0]
"Earlier work focused on removing training points from linear models, with later work extending this to more general models and a wider variety of perturbations (Hampel et al., 1986; Cook, 1986; Thomas & Cook, 1990; Chatterjee & Hadi, 1986; Wei et al., 1998).",6. Related work,[0],[0]
"Prior work mostly focused on experiments with small datasets, e.g., n = 24 and p = 10 in Cook & Weisberg (1980), and thus paid special attention to exact solutions, or if not possible, characterizations of the error terms.
",6. Related work,[0],[0]
"Influence functions have not been used much in the ML literature, with some exceptions.",6. Related work,[0],[0]
Christmann & Steinwart (2004); Debruyne et al. (2008); Liu et al. (2014) use influence functions to study model robustness and to do fast cross-validation in kernel methods.,6. Related work,[0],[0]
"Wojnowicz et al. (2016) use matrix sketching to estimate Cook’s distance,
which is closely related to influence; they focus on prioritizing training points for human attention and derive methods specific to generalized linear models.",6. Related work,[0],[0]
"Kabra et al. (2015) define a different notion of influence that is specialized to finite hypothesis classes.
",6. Related work,[0],[0]
"As noted in Section 5.2, our training-set attack is mathematically equivalent to an approach first explored by Biggio et al. (2012) in the context of SVMs, with follow-up work extending the framework and applying it to linear and logistic regression (Mei & Zhu, 2015b), topic modeling (Mei & Zhu, 2015a), and collaborative filtering (Li et al., 2016a).",6. Related work,[0],[0]
"These papers derived the attack directly from the KKT conditions without considering influence, though for continuous data, the end result is equivalent.",6. Related work,[0],[0]
"Influence functions additionally let us consider attacks on discrete data (Section 2.2), but we have not tested this empirically.",6. Related work,[0],[0]
"Our work connects the literature on trainingset attacks with work on “adversarial examples” (Goodfellow et al., 2015; Moosavi-Dezfooli et al., 2016), visuallyimperceptible perturbations on test inputs.
",6. Related work,[0],[0]
"In contrast to training-set attacks, Cadamuro et al. (2016) consider the task of taking an incorrect test prediction and finding a small subset of training data such that changing the labels on this subset makes the prediction correct.",6. Related work,[0],[0]
They provide a solution for OLS and Gaussian process models when the labels are continuous.,6. Related work,[0],[0]
Our work with influence functions allow us to solve this problem in a much larger range of models and in datasets with discrete labels.,6. Related work,[0],[0]
"We have discussed a variety of applications, from creating training-set attacks to debugging models and fixing datasets.",7. Discussion,[0],[0]
"Underlying each of these applications is a common tool, influence functions, which are based on a simple idea—we can better understand model behavior by looking at how it was derived from its training data.
",7. Discussion,[0],[0]
"At their core, influence functions measure the effect of local changes: what happens when we upweight a point by an infinitesimally-small ?",7. Discussion,[0],[0]
"This locality allows us to derive efficient closed-form estimates, and as we show, they can be surprisingly effective.",7. Discussion,[0],[0]
"However, we might want to ask about more global changes, e.g., how does a subpopulation of patients from this hospital affect the model?",7. Discussion,[0],[0]
"Since influence functions depend on the model not changing too much, how to tackle this is an open question.
",7. Discussion,[0],[0]
"It seems inevitable that high-performing, complex, blackbox models will become increasingly prevalent and important.",7. Discussion,[0],[0]
"We hope that the approach presented here—of looking at the model through the lens of the training data—will become a standard part of the toolkit of developing, understanding, and diagnosing machine learning.
",7. Discussion,[0],[0]
Reproducibility The code and data for replicating our experiments is available on GitHub http://bit.ly/gt-influence and Codalab http://bit.ly/cl-influence.,7. Discussion,[0],[0]
"We thank Jacob Steinhardt, Zhenghao Chen, and Hongseok Namkoong for helpful discussions and comments.",Acknowledgements,[0],[0]
"We are also grateful to Doug Martin, Swee Keat Lim, and Teresa Yeo for finding typos and omissions in a previous version of the manuscript.",Acknowledgements,[0],[0]
"This work was supported by a Future of Life Research Award and a Microsoft Research Faculty Fellowship.
A.",Acknowledgements,[0],[0]
"Deriving the influence function Iup,params For completeness, we provide a standard derivation of the influence function Iup,params in the context of loss minimization (M-estimation).",Acknowledgements,[0],[0]
"This derivation is based on asymptotic arguments and is not fully rigorous; see van der Vaart (1998) and other statistics textbooks for a more thorough treatment.
",Acknowledgements,[0],[0]
"Recall that θ̂ minimizes the empirical risk:
R(θ) def =
1
n n∑ i=1",Acknowledgements,[0],[0]
"L(zi, θ).",Acknowledgements,[0],[0]
"(6)
We further assume that R is twice-differentiable and strongly convex in θ, i.e.,
Hθ̂ def = ∇2R(θ̂) = 1
n n∑ i=1 ∇2θL(zi, θ̂) (7)
exists and is positive definite.",Acknowledgements,[0],[0]
"This guarantees the existence of H−1
θ̂ , which we will use in the subsequent derivation.
",Acknowledgements,[0],[0]
"The perturbed parameters θ̂ ,z can be written as
θ̂ ,z = arg min θ∈Θ {R(θ) + L(z, θ)} .",Acknowledgements,[0],[0]
"(8)
Define the parameter change ∆ = θ̂ ,z",Acknowledgements,[0],[0]
"− θ̂,",Acknowledgements,[0],[0]
"and note that, as θ̂ doesn’t depend on , the quantity we seek to compute can be written in terms of it:
dθ̂ ,z d =",Acknowledgements,[0],[0]
d∆ d .,Acknowledgements,[0],[0]
"(9)
Since θ̂ ,z is a minimizer of (8), let us examine its firstorder optimality conditions:
0 = ∇R(θ̂ ,z) + ∇L(z, θ̂ ,z).",Acknowledgements,[0],[0]
"(10)
Next, since θ̂ ,z → θ̂ as → 0, we perform a Taylor expansion of the right-hand side:
0",Acknowledgements,[0],[0]
"≈ [ ∇R(θ̂) + ∇L(z, θ̂) ]",Acknowledgements,[0],[0]
"+ (11)[
∇2R(θ̂) + ∇2L(z, θ̂) ] ∆ ,
where we have dropped o(‖∆ ‖) terms.
",Acknowledgements,[0],[0]
"Solving for ∆ , we get: ∆",Acknowledgements,[0],[0]
"≈− [ ∇2R(θ̂) + ∇2L(z, θ̂) ]−1 (12)[
∇R(θ̂) + ∇L(z, θ̂) ] .
",Acknowledgements,[0],[0]
"Since θ̂ minimizes R, we have ∇R(θ̂) = 0.",Acknowledgements,[0],[0]
"Dropping o( ) terms, we have
∆ ≈−∇2R(θ̂)−1∇L(z, θ̂) .",Acknowledgements,[0],[0]
"(13)
Combining with (7) and (9), we conclude that:
dθ̂ ,z d ∣∣∣ =0 = −H−1 θ̂ ∇L(z, θ̂) (14)
def = Iup,params(z).",Acknowledgements,[0],[0]
"(15)
B. Influence at non-convergence Consider a training point z.",Acknowledgements,[0],[0]
"When the model parameters θ̃ are close to but not at a local minimum, Iup,params(z) is approximately equal to a constant (which does not depend on z) plus the change in parameters after upweighting z and then taking a single Newton step from θ̃.",Acknowledgements,[0],[0]
"The high-level idea is that even though the gradient of the empirical risk at θ̃ is not 0, the Newton step from θ̃ can be decomposed into a component following the existing gradient (which does not depend on the choice of z) and a second component responding to the upweighted z (which Iup,params(z) tracks).
",Acknowledgements,[0],[0]
Let g def=,Acknowledgements,[0],[0]
1n,Acknowledgements,[0],[0]
"∑n i=1∇θL(zi, θ̃) be the gradient of the empirical risk at θ̃; since θ̃ is not a local minimum, g 6= 0.",Acknowledgements,[0],[0]
"After upweighting z by , the gradient at θ̃ goes from g 7→ g + ∇θL(z, θ̃), and the empirical Hessian goes from Hθ̃ 7→ Hθ̃",Acknowledgements,[0],[0]
"+ ∇2θL(z, θ̃).",Acknowledgements,[0],[0]
"A Newton step from θ̃ therefore changes the parameters by:
N ,z def = −",Acknowledgements,[0],[0]
[ Hθ̃ + ∇ 2,Acknowledgements,[0],[0]
"θL(z, θ̃) ]−1",Acknowledgements,[0],[0]
"[ g + ∇θL(z, θ̃) ] .
(16)
Ignoring terms in g, 2, and higher, we get N ,z",Acknowledgements,[0],[0]
"≈ −H−1
θ̃
( g + ∇θL(z, θ̃) ) .",Acknowledgements,[0],[0]
"Therefore, the actual change
due to a Newton step N ,z is equal to a constant −H−1θ̃ g (that doesn’t depend on z) plus times Iup,params(z) = −H−1
θ̃ ∇θL(z, θ̃) (which captures the contribution of z).",Acknowledgements,[0],[0]
How can we explain the predictions of a blackbox model?,abstractText,[0],[0]
"In this paper, we use influence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction.",abstractText,[0],[0]
"To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products.",abstractText,[0],[0]
"We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information.",abstractText,[0],[0]
"On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visuallyindistinguishable training-set attacks.",abstractText,[0],[0]
Understanding Black-box Predictions via Influence Functions,title,[0],[0]
"One method for interpreting deep neural networks (DNNs) is to examine model predictions for specific input examples, e.g. testing for shape bias as in Ritter et al. (2017).",1 Introduction,[0],[0]
"In the traditional classification task, the difficulty of the test set examples is not taken into account.",1 Introduction,[0],[0]
The number of correctlylabeled examples is tallied up and reported.,1 Introduction,[0],[0]
"However, we hypothesize that it may be worthwhile to use difficulty when evaluating DNNs.",1 Introduction,[0],[0]
"For example, what does it mean if a trained model answers the more difficult examples correctly, but cannot correctly classify what are seemingly simple cases?",1 Introduction,[0],[0]
"Recent work has shown that for NLP tasks such as Natural Language Inference (NLI), models can achieve strong results by simply using the hypothesis of a premise-hypothesis pair and ignoring the premise entirely (Gururangan et al., 2016; Tsuchiya, 2018; Poliak et al., 2018).
",1 Introduction,[0],[0]
In this work we consider understanding DNNs by looking at the difficulty of specific test set examples and comparing DNN performance under different training scenarios.,1 Introduction,[0],[0]
Do DNN models learn examples of varying difficulty at different rates?,1 Introduction,[0],[0]
"If
a model does well on hard examples and poor on easy examples, then can we say that it has really learned anything?",1 Introduction,[0],[0]
"In contrast, if a model does well on easy items, because a dataset is all easy, have we really “solved” anything?
",1 Introduction,[0],[0]
"To model difficulty we use Item Response Theory (IRT) from psychometrics (Baker and Kim, 2004).",1 Introduction,[0],[0]
IRT models characteristics such as difficulty and discrimination ability of specific examples (called “items”1) in order to estimate a latent ability trait of test-takers.,1 Introduction,[0],[0]
Here we use IRT to model the difficulty of test items to determine how DNNs learn items of varying difficulty.,1 Introduction,[0],[0]
IRT provides a well-studied methodology for modeling item difficulty as opposed to more heuristic-based difficulty estimates such as sentence length.,1 Introduction,[0],[0]
"IRT was previously used to build a new test set for the NLI task (Lalor et al., 2016) and show that model performance is dependent on test set difficulty.",1 Introduction,[0],[0]
"In this work we use IRT to probe specific items to try to analyze model performance at a more finegrained level, and expand the analysis to include the task of SA.
",1 Introduction,[0],[0]
We train three DNNs models with varying training set sizes to compare performance on two NLP tasks: NLI and Sentiment Analysis (SA).,1 Introduction,[0],[0]
Our experiments show that a DNN model’s likelihood of classifying an item correctly is dependent on the item’s difficulty.,1 Introduction,[0],[0]
"In addition, as the models are trained with more data, the odds of answering easy examples correctly increases at a faster rate than the odds of answering a difficult example correctly.",1 Introduction,[0],[0]
"That is, performance starts to look more human, in the sense that humans learn easy items faster than they learn hard items.
",1 Introduction,[0],[0]
That the DNNs are better at easy items than hard items seems intuitive but is a surprising and interesting result since the item difficulties are modeled from human data.,1 Introduction,[0],[0]
"There is no underlying reason
1For the remainder of the paper we will refer to a single test set example as an “item” for consistency.
",1 Introduction,[0],[0]
"ar X
iv :1
70 2.
04 81
1v 3
[ cs
.C",1 Introduction,[0],[0]
"L
] 7
S ep
2 01
8
that the DNNs would find items that are easy for humans inherently easy.",1 Introduction,[0],[0]
To our knowledge this is the first work to use a grounded measure of difficulty learned from human responses to understand DNN performance.,1 Introduction,[0],[0]
"Our contributions are as follows: (i) we use a well-studied methodology, IRT, to estimate item difficulty in two NLP tasks and show that this human-estimated difficulty is a useful predictor of DNN model performance, (ii) we show that as training size increases DNN performance trends towards expected human performance.2",1 Introduction,[0],[0]
"To model item difficulty we use the Three Parameter Logistic (3PL) model from IRT (Baker, 2001; Baker and Kim, 2004; Lalor et al., 2016).",2.1 Estimating Item Difficulty,[0],[0]
"The 3PL model in IRT models an individual’s latent ability (θ) on a task as a function of three item characteristics: discrimination ability (a), difficulty (b), and guessing (c).",2.1 Estimating Item Difficulty,[0],[0]
"For a particular item i, the probability that an individual j will answer item i correctly is a function of the individual’s ability and the three item characteristics:
pij(θj) = ci + 1− ci
1 + e−ai(θj−bi) (1)
where ai is the discrimination parameter (the value of the function slope at it’s steepest point), bi is the difficulty parameter (the value where pij(θj) = 0.5), and ci is the guessing parameter (the lower asymptote of the function).",2.1 Estimating Item Difficulty,[0],[0]
"For a set of items I and a set of individuals J , the likelihood of each individual in J’s responses to the items in I is:
L = J∏ j=1 I∏ i=1 pij(θj) yijqij(θj) (1−yij) (2)
where qij(θj) = 1− pij(θj) and yij = 1 if individual j answered item i correctly and yij = 0 otherwise.",2.1 Estimating Item Difficulty,[0],[0]
"Item parameters and individual ability are jointly estimated from a set of individuals’ response patterns using an Expectation-Maximization algorithm (Bock and Aitkin, 1981).
",2.1 Estimating Item Difficulty,[0],[0]
"In this work we focus on the difficulty parameter bi, which represents the latent ability level at which an individual has a 50% chance of answering item
2Code and data available at http://jplalor.github.io
i correctly.",2.1 Estimating Item Difficulty,[0],[0]
"Low values of bi are associated with easier items (since an individual with low ability has a 50% chance of answering correctly), and higher values of bi represent more difficult items.",2.1 Estimating Item Difficulty,[0],[0]
"To estimate item difficulties for NLI, we used the pre-trained IRT models of Lalor et al. (2016) and extracted the difficulty item parameters.",2.2 Data,[0],[0]
"The data consists of approximately 1000 human annotator responses from Amazon Mechanical Turk (AMT) for a selection of 180 premise-hypothesis pairs from the SNLI data set (Bowman et al., 2015).",2.2 Data,[0],[0]
"Each AMT worker (Turker) was shown the premisehypothesis pairs and was asked to indicate whether, if the premise was taken to be true, the hypothesis was (a) definitely true (entailment), (b) maybe true (neutral), or (c) definitely not true (contradiction).
",2.2 Data,[0],[0]
"For SA, we collected a new data set of labels for 134 examples randomly selected from the Stanford Sentiment Treebank (SSTB) (Socher et al., 2013), using a similar AMT setup as Lalor et al. (2016).",2.2 Data,[0],[0]
"For each randomly selected example, we had 1000 Turkers label the sentence as very negative, negative, neutral, positive, or very positive.",2.2 Data,[0],[0]
"We converted these responses to binary positive/negative labels and fit a new IRT 3PL model (§2.1) using the mirt R package (Chalmers et al., 2015).",2.2 Data,[0],[0]
"Very negative and negative labels were binned together, and neutral, positive, and very positive were binned together.
",2.2 Data,[0],[0]
"Tables 1 and 2 show examples of the items in our data sets, and the difficulty values estimated from the IRT models.",2.2 Data,[0],[0]
"The first example in Table 1 is a clear case of entailment, where if we assume that the premise is true, we can infer that the hypothesis is also true.",2.2 Data,[0],[0]
"The label of the second example in SNLI is contradiction, but in this case the result is not as clear.",2.2 Data,[0],[0]
"There are sports stadiums that offer lawn seating, and therefore this could potentially be a case of entailment (or neutral).",2.2 Data,[0],[0]
"Either way, one could argue that the second example here is more difficult than the first.",2.2 Data,[0],[0]
"Similarly, the first two examples of Table 2 are interesting.",2.2 Data,[0],[0]
Both of these items are labeled as negative examples in the data set.,2.2 Data,[0],[0]
"The first example is clear, but the second one is more ambiguous.",2.2 Data,[0],[0]
"It could be considered a mild complement, since the author still endorses renting the movie.",2.2 Data,[0],[0]
Therefore you could argue again that the second example is more difficult than the first.,2.2 Data,[0],[0]
"The learned difficulty parameters reflect this difference
in difficulty in both cases.",2.2 Data,[0],[0]
Inter-rater reliability scores for the collected annotations are showin in Table 3.,2.2 Data,[0],[0]
"Scores for the NLI annotations were calculated when the original dataset was collected and are reproduced here (Lalor et al., 2016).",2.2 Data,[0],[0]
Human annotations for the SA annotations were converted to binary before calculating the agreement.,2.2 Data,[0],[0]
"We see that the agreement scores are in the range of 0.4 to 0.6 which is considered moderate agreement (Landis and Koch, 1977).",2.2 Data,[0],[0]
With the large number of annotators it is to be expected that there is some disagreement in the labels.,2.2 Data,[0],[0]
"However this disagreement can be interpreted as varying difficulty of the items, which is what we expect when we fit the IRT models.",2.2 Data,[0],[0]
Our goal in this work is to understand how DNN performance on items of varying difficulty changes under different training scenarios.,2.3 Experiments,[0],[0]
"To test this, we trained three DNN models using subsets of the original SNLI and SSTB training data sets: (i) Long Short Term Memory Network (LSTM) (Bowman et al., 2015), (ii) Convolutional Neural Network (CNN) (Kim, 2014), and (iii) Neural Seman-
tic Encoder (NSE), a type of memory-augmented RNN (Munkhdalai and Yu, 2017).3 For each task (NLI and SA), we randomly sampled subsets of training data, from 100 examples up to and including the full training data sets.4",2.3 Experiments,[0],[0]
"We trained each model on the training data subsets, using the original development sets for early stopping to prevent overfitting.",2.3 Experiments,[0],[0]
"The IRT data with difficulty estimates were used as test sets for the trained models.
",2.3 Experiments,[0],[0]
"Once the models were trained and had classified the IRT data sets, we fit logistic regression models to predict whether a DNN model would label an item correctly, using the training set size and item difficulty as the dependent parameters.",2.3 Experiments,[0],[0]
Figure 1 plots the contour plots of our learned regression models.,3 Results,[0],[0]
"The top row plots results for the NLI task, and the bottom row plots results for the SA task.",3 Results,[0],[0]
"From left to right in both rows, the plots show results for the LSTM, CNN, and NSE models.",3 Results,[0],[0]
"In each plot, the x-axis is the training set size, the y-axis is the item difficulty, and the contour lines represent the log-odds that the DNN model would classify an item correctly.",3 Results,[0],[0]
"As the plots show, item difficulty has a clear effect on classification.",3 Results,[0],[0]
Easier items have higher odds of being classified correctly across all of the training set sizes.,3 Results,[0],[0]
"In addition, the slopes of the contour lines are steeper at lower levels of difficulty.",3 Results,[0],[0]
"This indicates that, moving left to right along the x-axis, a model’s odds of answering
3Please refer to the appendix for model details.",3 Results,[0],[0]
"4We sampled 100, 1000, 2000, 5000, 10000, 50000, 100000, 200000, and 500000 examples for NLI, and sampled 100, 1000, 5000, 10000, 50000, and 75000 examples for SA.
an easy item correctly increase more quickly than the odds of answering a harder item correctly.
",3 Results,[0],[0]
"The contour plots for the CNN and NSE models on the SA task (Figure 1, second row middle and right plots) show that the easier items have higher likelihood of being classified correctly, but the odds for the most difficult items decrease as training size increases.",3 Results,[0],[0]
This suggests that these models are learning in such a way that improves performance on easy items but has a negative effect on hard items.,3 Results,[0],[0]
"This result is important for interpretability, as it could inform stakeholder decisions if they need to have difficult examples classified.
",3 Results,[0],[0]
The idea that easy items should be easier than hard items is consistent with learning strategies in humans.,3 Results,[0],[0]
"For example, when teaching new concepts to students, easier concepts are presented first so that the students can learn patterns and core information before moving to more difficult concepts (Collins et al., 1988; Arroyo et al., 2010).",3 Results,[0],[0]
"As students do more examples, all questions get easier, but easy questions get easier at a faster rate.",3 Results,[0],[0]
"Our result is also consistent with the key assumptions of curriculum learning (Bengio et al., 2009).",3 Results,[0],[0]
Lalor et al. (2016) introduced the idea of applying IRT evaluation to NLP tasks.,4 Related Work,[0],[0]
"They built a set of scales using IRT for NLI and evaluated a single LSTM neural network to demonstrate the effectiveness of the evaluation, but did not evaluate other NLP models or tasks.",4 Related Work,[0],[0]
"Martı́nez-Plumed et al. (2016) consider IRT in the context of evaluating ML models, but they do not use a human population to calibrate the models, and obtain results that are difficult to interpret under IRT assumptions.
",4 Related Work,[0],[0]
"There has been work in the NLP community around modeling latent characteristics of data (Bruce and Wiebe, 1999) and annotators (Hovy et al., 2013), but none that apply the resulting metrics to interpret DNN models.",4 Related Work,[0],[0]
"Passonneau and Carpenter (2014) model the probability a label is correct with the probability of an annotator to label an item correctly according to the Dawid and Skene (1979) model, but do not consider difficulty or discriminatory ability of the data points.
",4 Related Work,[0],[0]
"One-shot learning is an attempt to build ML models that can generalize after being trained on one or a few examples of a class as opposed to a large
training set (Lake et al., 2013).",4 Related Work,[0],[0]
"One-shot learning attempts to mimic human learning behaviors (i.e., generalization after being exposed to a small number of training examples) (Lake et al., 2016).",4 Related Work,[0],[0]
"Our work instead looks at comparisons to human performance, where any learning (on the part of models) has been completed beforehand.",4 Related Work,[0],[0]
Our goal is to analyze DNN models and training set variations as they affect ability in the context of IRT.,4 Related Work,[0],[0]
In this work we have shown that DNN model performance is affected by item difficulty as well as training set size.,5 Discussion,[0],[0]
This is the first work that has used a well-established method for estimating difficulty to analyze DNN model performance as opposed to heuristics.,5 Discussion,[0],[0]
"DNN models perform better on easy items, and as more data is introduced in training, easy items are learned more quickly than hard items.",5 Discussion,[0],[0]
Learning easy examples faster than harder examples is what would be expected when examining human response patterns as they learn more about a subject.,5 Discussion,[0],[0]
"However this has not previously been shown to be true in DNN models.
",5 Discussion,[0],[0]
That the results are consistent across NLI and SA shows that the methods can be applied to a number of NLP tasks.,5 Discussion,[0],[0]
The SA results do show that the odds of labeling a difficult item correctly decrease with more training data 1.,5 Discussion,[0],[0]
"It could be the case that these difficult items in the SA task are more subjective than the easier items, for example a review that is fairly neutral and is split between positive and negative annotations.",5 Discussion,[0],[0]
"These cases would be more difficult for a model to label, and are worth examining in more detail.",5 Discussion,[0],[0]
"By identifying items such as these as difficult makes it easier to see where the model is going wrong and allows for research on better way to represent these cases.
",5 Discussion,[0],[0]
This result has implications for how machine learning models are evaluated across tasks.,5 Discussion,[0],[0]
"The traditional assumption that the test data is drawn from the same distribution as the training data, makes it difficult to understand how a model will perform in settings where that assumption does not hold.",5 Discussion,[0],[0]
"However, if the difficulty of test set data is known, we can better understand what kind of examples a given model performs well on, and specific instances where a model underperforms (e.g. the most difficult examples).",5 Discussion,[0],[0]
"In addition, researhers can build test sets that consist of a specific type of data (very easy, very hard, or a mix) to evalu-
ate a trained model under specific assumptions to test generalization ability in a controlled way.",5 Discussion,[0],[0]
"This could allow for more confidence in model performance in more varied deployment settings, since there would be a set of tests a model would have to pass before being deployed.
",5 Discussion,[0],[0]
"It is important to note that the difficulty parameters were estimated from a human population, meaning that those items that are difficult for humans are in fact more difficult for the DNN models as well.",5 Discussion,[0],[0]
"This does not need to be the case given that DNNs learn very different patterns, etc. than humans.",5 Discussion,[0],[0]
In fact there were exceptions in our results which shows that these models should be carefully examined using techniques like those described here.,5 Discussion,[0],[0]
Future work can investigate why this is the case and how we can leverage this information to improve model performance and interpretability.,5 Discussion,[0],[0]
We thank the AMT Turkers who completed our annotation task.,Acknowledgments,[0],[0]
This work was supported in part by the HSR&D award IIR 1I01HX001457 from the United States Department of Veterans Affairs (VA).,Acknowledgments,[0],[0]
We also acknowledge the support of LM012817 from the National Institutes of Health.,Acknowledgments,[0],[0]
This work was also supported in part by the Center for Intelligent Information Retrieval.,Acknowledgments,[0],[0]
"The contents of this paper do not represent the views of CIIR, NIH, VA, or the United States Government.",Acknowledgments,[0],[0]
Here we provide a brief overview of the model architectures for the deep neural network (DNN) models used in our experiments.,A DNN Model Architecture,[0],[0]
"For additional details we refer the reader to the original papers.
",A DNN Model Architecture,[0],[0]
"A.1 Long Short Term Memory
The Long Short Term Memory (LSTM) model used here was provided by (Bowman et al., 2015) with the release of the SNLI corpus.",A DNN Model Architecture,[0],[0]
"The model consists of two LSTM sequence-embedding models (Hochreiter and Schmidhuber, 1997), one to encode the premise and another to encode the hypothesis.",A DNN Model Architecture,[0],[0]
The two sentence encodings are then concatenated and passed through three tanh layers.,A DNN Model Architecture,[0],[0]
"Finally, the output is passed to a softmax classifier layer to output probabilities over the task classes.",A DNN Model Architecture,[0],[0]
"For SA, we kept the same architecture but used a single LSTM layer to encode the input text.
",A DNN Model Architecture,[0],[0]
"A.2 Convolutional Neural Network
We used the convolutional neural network (CNN) model of (Kim, 2014) in our experiments.",A DNN Model Architecture,[0],[0]
"For each input, the word vector representation of the input tokens were concatenated together to form a matrix.",A DNN Model Architecture,[0],[0]
"A series of convolutional operations were applied, followed by a max-pooling operation and a fully connected softmax classifier layer.",A DNN Model Architecture,[0],[0]
"More concretely, for an input sentence x, let xi be the word vector representation of the i-th word in x.",A DNN Model Architecture,[0],[0]
"The convolution operation of filter w over a window of length h starting with word xi results in a context vector ci:
ci = f(w · xi:i+h−1 + b) (3)
where b is a bias term (Kim, 2014).",A DNN Model Architecture,[0],[0]
"The filter is applied over all windows in the sentence to generate a feature-map, and max-pooling is used to identify the feature for this particular filter.",A DNN Model Architecture,[0],[0]
"The process is repeated with multiple filters, and the output features are then passed to a softmax classification layer to output probabilities over the class labels (Kim, 2014).",A DNN Model Architecture,[0],[0]
"For NLI, the premise and hypothesis sentences were concatenated before encoding.
",A DNN Model Architecture,[0],[0]
"A.3 Neural Semantic Encoder
Neural Semantic Encoder (NSE) is a memoryaugmented neural network that uses read, compose, and write operations to evolve and maintain an external memory (Munkhdalai and Yu, 2017):
ot = f LSTM r (xt) (4) zt = softmax(o > t Mt−1) (5)",A DNN Model Architecture,[0],[0]
"mr,t = z > t Mt−1 (6)
",A DNN Model Architecture,[0],[0]
"ct = f MLP c (ot,mr,t) (7) ht = f LSTM w (ct) (8)
Mt =Mt−1(1− (zt ⊗ ek)>)",A DNN Model Architecture,[0],[0]
+ (ht ⊗ el)(zt ⊗ ek)>,A DNN Model Architecture,[0],[0]
"(9)
where fLSTMr is the read function, f MLP c is the composition function, fLSTMw is the write function, Mt is the external memory at time t, and",A DNN Model Architecture,[0],[0]
el ∈ Rl and ek ∈,A DNN Model Architecture,[0],[0]
"Rk are vectors of ones (Munkhdalai and Yu, 2017).
",A DNN Model Architecture,[0],[0]
"For NLI, the premise and hypothesis sentences were each encoded with an NSE module.",A DNN Model Architecture,[0],[0]
The outputs were combined and passed through a softmax classifier layer to output probabilities.,A DNN Model Architecture,[0],[0]
"For SA, we kept the same architecture but used a single NSE layer to encode the input text.",A DNN Model Architecture,[0],[0]
Interpreting the performance of deep learning models beyond test set accuracy is challenging.,abstractText,[0],[0]
"Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally.",abstractText,[0],[0]
We examine the impact of a test set question’s difficulty to determine if there is a relationship between difficulty and performance.,abstractText,[0],[0]
We model difficulty using well-studied psychometric methods on human response patterns.,abstractText,[0],[0]
Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the likelihood of answering a question correctly is impacted by the question’s difficulty.,abstractText,[0],[0]
"As DNNs are trained with more data, easy examples are learned more quickly than hard examples.",abstractText,[0],[0]
Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study,title,[0],[0]
"√ θ%̃/n) where θ
denotes freedom degree of the network parameters and %̃ = O(log(∏li=1 bi(ki − si + 1)/p) + log(bl+1)) encapsulates architecture parameters including the kernel size ki, stride si, pooling size p and parameter magnitude bi. To our best knowledge, this is the first generalization bound that only depends on O(log(∏l+1i=1 bi)), tighter than existing ones that all involve an exponential term likeO(∏l+1i=1 bi). Besides, we prove that for an arbitrary gradient descent algorithm, the computed approximate stationary point by minimizing empirical risk is also an approximate stationary point to the population risk. This well explains why gradient descent training algorithms usually perform sufficiently well in practice. Furthermore, we prove the one-to-one correspondence and convergence guarantees for the non-degenerate stationary points between the empirical and population risks. It implies that the computed local minimum for the empirical risk is also close to a local minimum for the population risk, thus ensuring the good generalization performance of CNNs.",text,[0],[0]
"Deep convolutional neural networks (CNNs) have been successfully applied to various fields, such as image classifica-
1Department of Electrical & Computer Engineering (ECE), National University of Singapore, Singapore.",1. Introduction,[0],[0]
"Correspondence to: Pan Zhou <pzhou@u.nus.edu>, Jiashi Feng <elefjia@nus.edu.sg>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
tion (Szegedy et al., 2015; He et al., 2016; Wang et al., 2017), speech recognition (Sainath et al., 2013; Abdel-Hamid et al., 2014), and classic games (Silver et al., 2016; Brown & Sandholm, 2017).",1. Introduction,[0],[0]
"However, theoretical analyses and understandings on deep CNNs still largely lag their practical applications.",1. Introduction,[0],[0]
"Recently, although many works establish theoretical understandings on deep feedforward neural networks (DNNs) from various aspects, e.g. (Neyshabur et al., 2015; Kawaguchi, 2016; Zhou & Feng, 2018; Tian, 2017; Lee et al., 2017), only a few (Sun et al., 2016; Kawaguchi et al., 2017; Du et al., 2017a;b) provide explanations on deep CNNs due to their more complex architectures and operations.",1. Introduction,[0],[0]
"Besides, these existing works all suffer certain discrepancy between their theories and practice.",1. Introduction,[0],[0]
"For example, the developed generalization error bound either exponentially grows along with the depth of a CNN model (Sun et al., 2016) or is data-dependent (Kawaguchi et al., 2017), and the convergence guarantees for optimization algorithms over CNNs are achieved by assuming an over-simplified CNN model consisting of only one non-overlapping convolutional layer (Du et al., 2017a;b).
",1. Introduction,[0],[0]
"As an attempt to explain the practical success of deep CNNs and mitigate the gap between theory and practice, this work aims to provide tighter data-independent generalization error bound and algorithmic optimization guarantees for the commonly used deep CNN models in practice.",1. Introduction,[0],[0]
"Specifically, we theoretically analyze the deep CNNs from following two aspects: (1) how their generalization performance varies with different network architecture choices and (2) why gradient descent based algorithms such as stochastic gradient descend (SGD) (Robbins & Monro, 1951), adam (Kingma & Ba, 2015) and RMSProp (Tieleman & Hinton, 2012), on minimizing empirical risk usually offer models with satisfactory performance.",1. Introduction,[0],[0]
"Moreover, we theoretically demonstrate the benefits of (stride) convolution and pooling operations, which are unique for CNNs, to the generalization performance, compared with feedforward networks.
",1. Introduction,[0],[0]
"Formally, we consider a CNN model g(w;D) parameterized by w ∈ Rd, consisting of l convolutional layers and one subsequent fully connected layer.",1. Introduction,[0],[0]
It maps the input D ∈ Rr0×c0 to an output vector v ∈ Rdl+1 .,1. Introduction,[0],[0]
"Its i-th convolutional layer takes Z(i−1) ∈ Rr̃i−1×c̃i−1×di−1 as input and outputs Z(i) ∈ Rr̃i×c̃i×di through spatial convolution, non-linear activation and pooling operations sequentially.
",1. Introduction,[0],[0]
"ar X
iv :1
80 5.
10 76
7v 1
[ cs
.L G
] 2
8 M
ay 2
01 8
Here r̃i × c̃i and di respectively denote resolution and the number of feature maps.",1. Introduction,[0],[0]
"Specifically, the computation with the i-th convolutional layer is described as
X(i)(:, :, j) = Z(i−1) ~W",1. Introduction,[0],[0]
"j(i) ∈ Rri×ci ,∀j = 1, · · · , di, Y(i) = σ1(X(i))",1. Introduction,[0],[0]
"∈ Rri×ci×di , Z(i) = pool ( Y(i) )",1. Introduction,[0],[0]
"∈ Rr̃i×c̃i×di ,
where X(i)(:, :, j) denotes the j-th feature map output by the i-th layer; W j(i) ∈ Rki×ki×di−1 denotes the j-th convolutional kernel of size ki×ki and there are in total di kernels in the i-th layer; ~, pool (·) and σ1(·) respectively denote the convolutional operation with stride si, pooling operation with window size p × p without overlap and the sigmoid function.",1. Introduction,[0],[0]
"In particular, Z(0) = D is the input sample.",1. Introduction,[0],[0]
"The last layer is a fully connected one and formulated as
u = W(l+1)z(l) ∈ Rdl+1 and v = σ2(u) ∈ Rdl+1 , where z(l) ∈ Rr̃lc̃ldl is vectorization of the output Z(l) of the last convolutional layer; W(l+1) ∈ Rdl+1×r̃lc̃ldl denotes the connection weight matrix; σ2(·) is a softmax activation function (for classification) and dl+1 is the class number.
",1. Introduction,[0],[0]
"In practice, a deep CNN model is trained by minimizing the following empirical risk in terms of squared loss on the training data pairs (D(i),y(i)) drawn from an unknown distribution D,
Q̃n(w) , 1
n
n∑
i=1
f(g(w;D(i)),y(i)), (1)
where f(g(w;D),y) = 12‖v",1. Introduction,[0],[0]
− y‖22 is the squared loss function.,1. Introduction,[0],[0]
One can obtain the model parameter w̃ via SGD or its variants like adam and RMSProp.,1. Introduction,[0],[0]
"However, this empirical solution is different from the desired optimum w∗ that minimizes the population risk:
Q(w) , E(D,y)∼D f(g(w;D),y).",1. Introduction,[0],[0]
"This raises an important question: why CNNs trained by minimizing the empirical risk usually perform well in practice, considering the high model complexity and nonconvexity?",1. Introduction,[0],[0]
"This work answers this question by (1) establishing the generalization performance guarantee for CNNs and (2) expounding why the computed solution w̃ by gradient descent based algorithms for minimizing the empirical risk usually performs sufficiently well in practice.
",1. Introduction,[0],[0]
"To be specific, we present three new theoretical guarantees for CNNs.",1. Introduction,[0],[0]
"First, we prove that the generalization error of deep CNNs decreases at the rate of O( √ θ%̃/(2n))",1. Introduction,[0],[0]
"where θ denotes parameter freedom degree1, and %̃ depends on the
1We use the terminology of “parameter freedom degree” here for characterizing redundancy of parameters.",1. Introduction,[0],[0]
"For example, for a rank-r matrix A ∈ Rm1×m2 , the parameter freedom degree in this work is r(m1 +m2 + 1) instead of the commonly used one r(m1 +m2 − r).
network architecture parameters including the convolutional kernel size ki, stride si, pooling size p, channel number di and parameter magnitudes.",1. Introduction,[0],[0]
"It is worth mentioning that our generalization error bound is the first one that does not exponentially grow with depth.
",1. Introduction,[0],[0]
"Secondly, we prove that for any gradient descent based optimization algorithm, e.g. SGD, RMSProp or adam, if its output w̃ is an approximate stationary point of the empirical risk Q̃n(w), w̃ is also an approximate stationary point of the population risk Q(w).",1. Introduction,[0],[0]
This result is important as it explains why CNNs trained by minimizing the empirical risk have good generalization performance on test samples.,1. Introduction,[0],[0]
"We achieve such results by analyzing the convergence behavior of the empirical gradient to its population counterpart.
",1. Introduction,[0],[0]
"Finally, we go further and quantitatively bound the distance between w̃ and w∗.",1. Introduction,[0],[0]
"We prove that when the samples are sufficient, a non-degenerate stationary point wn of Q̃n(w) uniquely corresponds to a non-degenerate stationary point w∗ of the population risk Q(w), with a distance shrinking at the rate of O((β/ζ) √ d%̃/n) where β also depends on the CNN architecture parameters (see Thereom 2).",1. Introduction,[0],[0]
Here ζ accounts for the geometric topology of non-degenerate stationary points.,1. Introduction,[0],[0]
"Besides, the corresponding pair (wn,w∗) shares the same geometrical property—if one in (wn,w∗) is a local minimum or saddle point, so is the other one.",1. Introduction,[0],[0]
"All these results guarantee that for an arbitrary algorithm provided with sufficient samples, if the computed w̃ is close to the stationary point wn, then w̃ is also close to the optimum w∗ and they share the same geometrical property.
",1. Introduction,[0],[0]
"To sum up, we make multiple contributions to understand deep CNNs theoretically.",1. Introduction,[0],[0]
"To our best knowledge, this work presents the first theoretical guarantees on both generalization error bound without exponential growth over network depth and optimization performance for deep CNNs.",1. Introduction,[0],[0]
"We substantially extend prior works on CNNs (Du et al., 2017a;b) from the over-simplified single-layer models to the multi-layer ones, which is of more practical significance.",1. Introduction,[0],[0]
"Our generalization error bound is much tighter than the one derived from Rademacher complexity (Sun et al., 2016) and is also independent of data and specific training procedure, which distinguishes it from (Kawaguchi et al., 2017).",1. Introduction,[0],[0]
"Recently, many works have been devoted to explaining the remarkable success of deep neural networks.",2. Related Works,[0],[0]
"However, most works only focus on analyzing fully feedforward networks from aspects like generalization performance (Bartlett & Maass, 2003; Neyshabur et al., 2015), loss surface (Saxe et al., 2014; Dauphin et al., 2014; Choromanska et al., 2015; Kawaguchi, 2016; Nguyen & Hein, 2017; Zhou & Feng, 2018), optimization algorithm convergence (Tian, 2017; Li
& Yuan, 2017) and expression ability (Eldan & Shamir, 2016; Soudry & Hoffer, 2017; Lee et al., 2017).
",2. Related Works,[0],[0]
"The literature targeting at analyzing CNNs is very limited, mainly because CNNs have much more complex architectures and computation.",2. Related Works,[0],[0]
"Among the few existing works, Du et al. (2017b) presented results for a simple and shallow CNN consisting of only one non-overlapping convolutional layer and ReLU activations, showing that gradient descent (GD) algorithms with weight normalization can converge to the global minimum.",2. Related Works,[0],[0]
"Similarly, Du et al. (2017a) also analyzed optimization performance of GD and SGD with nonGaussian inputs for CNNs with only one non-overlapping convolutional layer.",2. Related Works,[0],[0]
"By utilizing the kernel method, Zhang et al. (2017) transformed a CNN model into a single-layer convex model which has almost the same loss as the original CNN with high probability and proved that the transformed model has higher learning efficiency.
",2. Related Works,[0],[0]
"Regarding generalization performance of CNNs, Sun et al. (2016) provided the Rademacher complexity of a deep CNN model which is then used to establish the generalization error bound.",2. Related Works,[0],[0]
"But the Rademacher complexity exponentially depends on the magnitude of total parameters per layer, leading to loose results.",2. Related Works,[0],[0]
"In contrast, the generalization error bound established in this work is much tighter, as discussed in details in Sec. 3.",2. Related Works,[0],[0]
"Kawaguchi et al. (2017) introduced two generalization error bounds of CNN, but both depend on a specific dataset as they involve the validation error or the intermediate outputs for the network model on a provided dataset.",2. Related Works,[0],[0]
"They also presented dataset-independent generalization error bound, but with a specific two-phase training procedure required, where the second phase need fix the states of ReLU activation functions.",2. Related Works,[0],[0]
"However, such two-phase training procedure is not used in practice.
",2. Related Works,[0],[0]
There are also some works focusing on convergence behavior of nonconvex empirical risk of a single-layer model to the population risk.,2. Related Works,[0],[0]
Our proof techniques essentially differ from theirs.,2. Related Works,[0],[0]
"For example, (Gonen & Shalev-Shwartz, 2017) proved that the empirical risk converges to the population risk for those nonconvex problems with no degenerated saddle points.",2. Related Works,[0],[0]
"Unfortunately, due to existence of degenerated saddle points in deep networks (Dauphin et al., 2014; Kawaguchi, 2016), their results are not applicable here.",2. Related Works,[0],[0]
"A very recent work (Mei et al., 2017) focuses on single-layer nonconvex problems but requires the gradient and Hessian of the empirical risk to be strong sub-Gaussian and subexponential respectively.",2. Related Works,[0],[0]
"Besides, it assumes a linearity property for the gradient which hardly holds in practice.",2. Related Works,[0],[0]
"Comparatively, our assumptions are much milder.",2. Related Works,[0],[0]
We only assume magnitude of the parameters to be bounded.,2. Related Works,[0],[0]
"Furthermore, we also explore the parameter structures of optimized CNNs, i.e. the low-rankness property, and derive bounds matching empirical observations better.",2. Related Works,[0],[0]
"Finally, we analyze
the convergence rate of the empirical risk and generalization error of CNN which is absent in (Mei et al., 2017).
",2. Related Works,[0],[0]
"Our work is also critically different from the recent work (Zhou & Feng, 2018)",2. Related Works,[0],[0]
although we adopt a similar analysis road map with it.,2. Related Works,[0],[0]
Zhou & Feng (2018) analyzed DNNs while this work focuses on CNNs with more complex architectures and operations which are more challenging and requires different analysis techniques.,2. Related Works,[0],[0]
"Besides, this work provides stronger results in the sense of several tighter bounds with much milder assumptions.",2. Related Works,[0],[0]
"(1) For nonlinear DNNs, Zhou & Feng (2018) assumed the input data to be Gaussian, while this work gets rid of such a restrictive assumption.",2. Related Works,[0],[0]
"(2) The generalization error bound O(r̂l+1 √ d/n) in (Zhou & Feng, 2018) exponentially depends on the upper magnitude bound r̂ of the weight matrix per layer and linearly depends on the total parameter number d, while ours is O( √ θ%̃/n) which only depends on the logarithm term %̃ = log( ∏l+1 i=1 bi) and the freedom degree θ of the network parameters, where bi and bl+1 respectively denote the upper magnitude bounds of each kernel per layer and the weight matrix of the fully connected layer.",2. Related Works,[0],[0]
"Note, the exponential term O(r̂l+1) in (Zhou & Feng, 2018) cannot be further improved due to their proof techniques.",2. Related Works,[0],[0]
"The results on empirical gradient and stationary point pairs in (Zhou & Feng, 2018) rely on O(r̂2(l+1)), while ours is O(∏l+1i=1 bi) which only depends on bi instead of bi2.",2. Related Works,[0],[0]
"(3) This work explores the parameter structures, i.e. the low-rankness property, and derives tighter bounds as the parameter freedom degree θ is usually smaller than the total parameter number d.",2. Related Works,[0],[0]
"In this section, we present the generalization error bound for deep CNNs and reveal effects of different architecture parameters on their generalization performance, providing some principles on model architecture design.",3. Generalization Performance of Deep CNNs,[0],[0]
"We derive these results by establishing uniform convergence of the empirical risk Q̃n(w) to its population one Q(w).
",3. Generalization Performance of Deep CNNs,[0],[0]
We start with explaining our assumptions.,3. Generalization Performance of Deep CNNs,[0],[0]
"Similar to (Xu & Mannor, 2012; Tian, 2017; Zhou & Feng, 2018), we assume that the parameters of the CNN have bounded magnitude.",3. Generalization Performance of Deep CNNs,[0],[0]
"But we get rid of the Gaussian assumptions on the input data, meaning our assumption is milder than the ones in (Tian, 2017; Soudry & Hoffer, 2017; Zhou & Feng, 2018).",3. Generalization Performance of Deep CNNs,[0],[0]
Assumption 1.,3. Generalization Performance of Deep CNNs,[0],[0]
The magnitudes of the j-th kernel W j(i) ∈ Rki×ki×di−1 in the i-th convolutional layer and,3. Generalization Performance of Deep CNNs,[0],[0]
"the weight matrix W(l+1) ∈ Rdl+1×r̃lc̃ldl in the the fully connected layer are respectively bounded as follows
‖W j(i)‖F ≤bi (1≤j≤di;",3. Generalization Performance of Deep CNNs,[0],[0]
"1≤ i≤ l), ‖W(l+1)‖F ≤bl+1, where bi (1 ≤ i ≤ l) and bl+1 are positive constants.
",3. Generalization Performance of Deep CNNs,[0],[0]
"We also assume that the entry value of the target output y
always falls in [0, 1], which can be achieved by scaling the entry value in y conveniently.
",3. Generalization Performance of Deep CNNs,[0],[0]
"In this work, we also consider possible emerging structure of the learned parameters after training—the parameters usually present redundancy and low-rank structures (Lebedev et al., 2014; Jaderberg et al., 2014) due to high model complexity.",3. Generalization Performance of Deep CNNs,[0],[0]
"So we incorporate low-rankness of the parameters or more concretely the parameter matrix consisting of kernels per layer, into our analysis.",3. Generalization Performance of Deep CNNs,[0],[0]
"Denoting by vec(A) the vectorization of a matrix A, we have Assumption 2.
Assumption 2.",3. Generalization Performance of Deep CNNs,[0],[0]
Assume the matrices W̃(i) andW(l+1),3. Generalization Performance of Deep CNNs,[0],[0]
"obey
rank(W̃(i))",3. Generalization Performance of Deep CNNs,[0],[0]
≤ ai (1 ≤ i ≤ l) and rank(W(l+1)),3. Generalization Performance of Deep CNNs,[0],[0]
"≤ al+1,
where W̃(i) =",3. Generalization Performance of Deep CNNs,[0],[0]
"[vec(W 1(i)), vec(W 2 (i)), · · · , vec(W di(i))",3. Generalization Performance of Deep CNNs,[0],[0]
"] ∈ Rk2i di−1×di denotes the matrix consisting of all kernels in the i-th layer (1 ≤ i ≤ l).
",3. Generalization Performance of Deep CNNs,[0],[0]
"The parameter low-rankness can also be defined on kernels individually by using the tensor rank (Tucker, 1966; Zhou et al., 2017; Zhou & Feng, 2017).",3. Generalization Performance of Deep CNNs,[0],[0]
Our proof techniques are extensible to this case and similar results can be expected.,3. Generalization Performance of Deep CNNs,[0],[0]
We now proceed to establish generalization error bound for deep CNNs.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Let S = {(D(1),y(1)), · · · , (D(n),y(n))} denote the set of training samples i.i.d. drawn from D. When the optimal solution w̃ to problem (1) is computed by a deterministic algorithm, the generalization error is defined as g = ∣∣Q̃n(w̃)−Q(w̃) ∣∣.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"But in practice, a CNN model is usually optimized by randomized algorithms, e.g. SGD.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
So we adopt the following generalization error in expectation.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
Definition 1.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"(Generalization error) (Shalev-Shwartz et al., 2010)",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
Assume a randomized algorithm,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"A is employed for optimization over training samples S = {(D(1),y(1)), · · ·, (D(n),y(n))} ∼ D and w̃ = argminwQ̃n(w) is the empirical risk minimizer (ERM).",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
Then if we have ES∼D ∣∣EA(Q(w̃),3.1. Generalization Error Bound for Deep CNNs,[0],[0]
− Q̃n(w̃)) ∣∣,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"≤ k, the ERM is said to have generalization error with rate k under distribution D. We bound the generalization error in expectation for deep CNNs by first establishing uniform convergence of the empirical risk to its corresponding population risk, as stated in Lemma 1 with proof in Sec.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
D.1 in supplement.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
Lemma 1.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Assume that in CNNs, σ1 and σ2 are respectively the sigmoid and softmax activation functions and the loss function f(g(w;D),y) is squared loss.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Under Assumptions 1 and 2, if n ≥ cf ′ l2(bl+1",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"+∑l
i=1 dibi) 2 maxi √ rici/(θ%ε
2) where cf ′ is a universal constant, then with probability at least 1− ε, we have
sup w∈Ω
∣∣∣Q̃n(w)−Q(w) ∣∣∣ ≤
√ θ%+ log ( 4 ε )
",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"2n , (2)
where the total freedom degree θ of the network is θ = al+1(dl+1 + r̃lc̃ldl + 1) + ∑l i=1 ai(ki 2di−1 + di + 1) and % = ∑l i=1log (√dibi(ki−si+1) 4p ) + log(bl+1)",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"+ log ( n 128p2 ) .
",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"To our best knowledge, this generalization error rate is the first one that grows linearly (in contrast to exponentially) with depth",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
l without needing any special training procedure.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"This can be observed from the fact that our result only depends on O(∑li=1 log(bi)), rather than an exponential factor O(∏l+1i=1 bi) which appears in some existing works, e.g. the uniform convergence of the empirical risk in deep CNNs (Sun et al., 2016) and fully feedforward networks (Bartlett & Maass, 2003; Neyshabur et al., 2015; Zhou & Feng, 2018).",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"This faster convergence rate is achieved by adopting similar analysis technique in (Mei et al., 2017; Zhou & Feng, 2018) but we derive tighter bounds on the related parameters featuring distributions of the empirical risk and its gradient, with milder assumptions.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"For instance, both (Zhou & Feng, 2018) and this work show that the empirical risk follows a sub-Gaussian distribution.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
But Zhou & Feng (2018) used Gaussian concentration inequality and thus need Lipschitz constant of loss which exponentially depends on the depth.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"In contrast, we use -net to decouple the dependence between input D and parameter w and then adopt Hoeffding’s inequality, only requiring the constant magnitude bound of loss and geting rid of exponential term.
",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Based on Lemma 1, we derive generalization error of deep CNNs in Theorem 1 with proof in Sec. D.2 in supplement.
",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
Theorem 1.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Assume that in CNNs, σ1 and σ2 are respectively the sigmoid and softmax functions and the loss function f(g(w;D),y) is squared loss.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
Suppose Assumptions 1 and 2 hold.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Then with probability at least 1− ε, the generalization error of a deep CNN model is bounded as
ES∼D ∣∣∣EA ( Q(w̃)− Q̃n(w̃) )",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"∣∣∣ ≤
√ θ%+ log ( 4 ε )
2n ,
where θ and % are given in Lemma 1.
",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"By inspecting Theorem 1, one can find that the generalization error diminishes at the rate of O (1/√n) (up to a log factor).",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Besides, Theorem 1 explicitly reveals the roles of network parameters in determining model generalization performance.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Such transparent results form stark contrast to the works (Sun et al., 2016) and (Kawaguchi et al., 2017) (see more comparison in Sec. 3.2).",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Notice, our technique also applies to other third-order differentiable activation functions, e.g. tanh, and other losses, e.g. cross entropy, with only slight difference in the results.
",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"First, the freedom degree θ of network parameters, which depends on the network size and the redundancy in parameters, plays an important role in the generalization error bound.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"More specifically, to obtain smaller generalization
error, more samples are needed to train a deep CNN model having larger freedom degree θ.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"As aforementioned, although the results in Theorem 1 are obtained under the low-rankness condition defined on the parameter matrix consisting of kernels per layer, they are easily extended to the (tensor) low-rankness defined on each kernel individually.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
The low-rankness captures common parameter redundancy in practice.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"For instance, (Lebedev et al., 2014; Jaderberg et al., 2014) showed that parameter redundancy exists in a trained network model and can be squeezed by low-rank tensor decomposition.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"The classic residual function (He et al., 2016; Zagoruyko & Komodakis, 2016) with three-layer bottleneck architecture (1× 1, 3× 3 and 1× 1 convs) has rank 1 in generalized block term decomposition (Chen et al., 2017; Cohen & Shashua, 2016).",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Similarly, inception networks (Szegedy et al., 2017) explicitly decomposes a convolutional kernel of large size into two separate convolutional kernels of smaller size (e.g. a 7× 7 kernel is replaced by two multiplying kernels of size 7×1 and 1×7).",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
Employing these low-rank approximation techniques helps reduce the freedom degree and provides smaller generalization error.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Notice, the low-rankness assumption only affects the freedom degree θ.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Without this assumption, θ will be replaced by the total parameter number of the network.
",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"From the factor %, one can observe that the kernel size ki and its stride si determine the generalization error but in opposite ways.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Larger kernel size ki leads to larger generalization error, while larger stride si provides smaller one.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"This is because both larger kernel and smaller stride increase the model complexity, since larger kernel means more trainable parameters and smaller stride implies larger size of feature maps in the subsequent layer.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Also, the pooling operation in the first l convolutional layers helps reduce the generalization error, as reflected by the factor 1/p in %.
",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Furthermore, the number of feature maps (i.e. channels) di appearing in the θ",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
and % also affects the generalization error.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
A wider network with larger di requires more samples for training such that it can generalize well.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"This is because (1) a larger di indicates more trainable parameters, which usually increases the freedom degree θ, and (2) a larger di also requires larger kernels W j(i) with more channel-wise dimensions since there are more channels to convolve, leading to a larger magnitude bound bi for the kernel W j (i).",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Therefore, as suggested by Theorem 1, a thin network is more preferable than a fat network.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Such an observation is consistent with other analysis works on the network expression ability (Eldan & Shamir, 2016; Lu et al., 2017) and the architecture-engineering practice, such as (He et al., 2016; Szegedy et al., 2015).",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"By comparing contributions of the architecture and parameter magnitude to the generalization performance, we find that the generalization error usually depends on the network architecture parameters linearly or more heavily, and also on parameter magnitudes but
with a logarithm term log bi.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
This implies the architecture plays a more important role than the parameter magnitudes.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Therefore, for achieving better generalization performance in practice, architecture engineering is indeed essential.
",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Finally, by observing the factor %, we find that imposing certain regularization, such as ‖w‖22, on the trainable parameters is useful.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
The effectiveness of such a regularization will be more significant when imposing on the weight matrix of the fully connected layer due to its large size.,3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"Such a regularization technique, in deep learning literature, is well known as “weight decay”.",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
"This conclusion is consistent with other analysis works on the deep forward networks, such as (Bartlett & Maass, 2003; Neyshabur et al., 2015; Zhou & Feng, 2018).",3.1. Generalization Error Bound for Deep CNNs,[0],[0]
Sun et al. (2016) also analyzed generalization error bound in deep CNNs but employing different techniques.,3.2. Discussions,[0],[0]
They proved that the Rademacher complexity Rm(F) of a deep CNN model with sigmoid activation functions is O(̃bx(2pb̃)l+1 √ log(r0c0)/ √ n) where F denotes the function hypothesis that maps the input data D to v ∈ Rdl+1,3.2. Discussions,[0],[0]
by the analyzed CNN model.,3.2. Discussions,[0],[0]
"Here b̃x denotes the upper bound of the absolute entry values in the input datum D, i.e. b̃x ≥ |Di,j | (∀i, j), and b̃ obeys b̃ ≥ max{maxi ∑di j=1",3.2. Discussions,[0],[0]
"‖W j (i)‖1, ‖W(l+1)‖1}.",3.2. Discussions,[0],[0]
"Sun et al. (2016) showed that with probability at least 1 − ε, the difference between the empirical margin error errγe (g) (g ∈ F) and the population margin error errγp(g) can be bounded as
errγp(g) ≤ inf γ>0
[ errγe (g) +
8dl+1(2dl+1",3.2. Discussions,[0],[0]
"− 1) γ Rm(F)
+
√ log log2(2/γ)
n",3.2. Discussions,[0],[0]
"+
√ log(2/ε)
n
] , (3)
where γ controls the error margin since it obeys γ ≥ vy",3.2. Discussions,[0],[0]
− maxk 6=y vk and y denotes the label of v.,3.2. Discussions,[0],[0]
"However, the bound in Eqn.",3.2. Discussions,[0],[0]
"(3) is practically loose, sinceRm(F) involves the exponential factor (2b̃)l+1 which is usually very large.",3.2. Discussions,[0],[0]
"In this case,Rm(F) is extremely large.",3.2. Discussions,[0],[0]
"By comparison, the bound provided in our Theorem 1 only depends on∑l+1 i=1 log(bi) which avoids the exponential growth along with the depth l, giving a much tighter and more practically meaningful bound.",3.2. Discussions,[0],[0]
"The generalization error bounds in (Kawaguchi et al., 2017) either depend on a specific dataset or rely on restrictive and rarely used training procedure, while our Theorem 1 is independent of any specific dataset or training procedure, rendering itself more general.",3.2. Discussions,[0],[0]
"More importantly, the results in Theorem 1 make the roles of network parameters transparent, which could benefit understanding and architecture design of CNNs.",3.2. Discussions,[0],[0]
"Although deep CNNs are highly non-convex, gradient descent based algorithms usually perform quite well on optimizing the models in practice.",4. Optimization Guarantees for Deep CNNs,[0],[0]
"After characterizing the roles of different network parameters for the generalization performance, here we present optimization guarantees for gradient descent based algorithms in training CNNs.
",4. Optimization Guarantees for Deep CNNs,[0],[0]
"Specifically, in practice one usually adopts SGD or its variants, such as adam and RMSProp, to optimize the CNN models.",4. Optimization Guarantees for Deep CNNs,[0],[0]
Such algorithms usually terminate when the gradient magnitude decreases to a low level and the training hardly proceeds.,4. Optimization Guarantees for Deep CNNs,[0],[0]
"This implies that the algorithms in fact compute an -approximate stationary point w̃ for the loss function Q̃n(w), i.e. ‖∇wQ̃n(w̃)‖22 ≤ .",4. Optimization Guarantees for Deep CNNs,[0],[0]
"Here we explore such a problem: by computing an -stationary point w̃ of the empirical risk Q̃n(w), can we also expect w̃ to be sufficiently good for generalization, or in other words expect that it is also an approximate stationary point for the population risk Q(w)?",4. Optimization Guarantees for Deep CNNs,[0],[0]
"To answer this question, first we analyze the relationship between the empirical gradient∇wQ̃n(w) and its population counterpart∇wQ(w).",4. Optimization Guarantees for Deep CNNs,[0],[0]
"Founded on this, we further establish convergence of the empirical gradient of the computed solution to its corresponding population gradient.",4. Optimization Guarantees for Deep CNNs,[0],[0]
"Finally, we present the bounded distance between the computed solution w̃ and the optimum w∗.
To our best knowledge, this work is the first one that analyzes the optimization behavior of gradient descent based algorithms for training multi-layer CNN models with the commonly used convolutional and pooling operations.",4. Optimization Guarantees for Deep CNNs,[0],[0]
Here we present guarantees on convergence of the empirical gradient to the population one in Theorem 2.,4.1. Convergence Guarantees on Gradients,[0],[0]
"As aforementioned, such results imply good generalization performance of the computed solution w̃ to the empirical risk Q̃n(w).
",4.1. Convergence Guarantees on Gradients,[0],[0]
Theorem 2.,4.1. Convergence Guarantees on Gradients,[0],[0]
"Assume that in CNNs, σ1 and σ2 respectively are the sigmoid and softmax functions and the loss function f(g(w;D),y) is squared loss.",4.1. Convergence Guarantees on Gradients,[0],[0]
Suppose Assumptions 1 and 2 hold.,4.1. Convergence Guarantees on Gradients,[0],[0]
Then the empirical gradient uniformly converges to the population gradient in Euclidean norm.,4.1. Convergence Guarantees on Gradients,[0],[0]
"More specifically, there exist universal constants cg′ and cg such that if n ≥",4.1. Convergence Guarantees on Gradients,[0],[0]
"cg′ l 2bl+1 2(bl+1+ ∑l i=1 dibi) 2(r0c0d0) 4
d40b1 8(d log(6)+θ%)ε2 maxi(rici)
, then
sup w∈Ω
∥∥∥∇wQ̃n(w)−∇wQ(w) ∥∥∥
2 ≤cgβ
√ 2d+θ%+log ( 4 ε )
2n
holds with probability at least 1 − ε, where % is provided in Lemma 1.",4.1. Convergence Guarantees on Gradients,[0],[0]
Here β and d are defined as β =[ rlcldl 8p2 +,4.1. Convergence Guarantees on Gradients,[0],[0]
∑l i=1 bl+1 2di−1,4.1. Convergence Guarantees on Gradients,[0],[0]
"8p2bi2di ri−1ci−1 ∏l j=i djbj 2(kj−sj+1)2 16p2 ]1/2 and d = r̃lc̃ldldl+1 + ∑l i=1 ki 2di−1di, respectively.
",4.1. Convergence Guarantees on Gradients,[0],[0]
Its proof is given in Sec.,4.1. Convergence Guarantees on Gradients,[0],[0]
D.3 in supplement.,4.1. Convergence Guarantees on Gradients,[0],[0]
"From Theorem 2, the empirical gradient converges to the population one at the rate of O(1/√n) (up to a log factor).",4.1. Convergence Guarantees on Gradients,[0],[0]
"In Sec. 3.1, we have discussed the roles of the network architecture parameters in %.",4.1. Convergence Guarantees on Gradients,[0],[0]
Here we further analyze the effects of the network parameters on the optimization behavior through the factor β.,4.1. Convergence Guarantees on Gradients,[0],[0]
"The roles of the kernel size ki, the stride si, the pooling size p and the channel number di in β are consistent with those in Theorem 1.",4.1. Convergence Guarantees on Gradients,[0],[0]
The extra factor rici advocates not building such CNN networks with extremely large feature map sizes.,4.1. Convergence Guarantees on Gradients,[0],[0]
The total number of parameters d is involved here instead of the degree of freedom because the gradient ∇wQ̃n(w) may not have low-rank structures.,4.1. Convergence Guarantees on Gradients,[0],[0]
"Based on Theorem 2, we can further conclude that if the computed solution w̃ is an -approximate stationary point of the empirical risk, then it is also a 4 -approximate stationary point of the population risk.",4.1. Convergence Guarantees on Gradients,[0],[0]
We state this result in Corollary 1 with proof in Sec.,4.1. Convergence Guarantees on Gradients,[0],[0]
D.4 in supplement.,4.1. Convergence Guarantees on Gradients,[0],[0]
Corollary 1.,4.1. Convergence Guarantees on Gradients,[0],[0]
Suppose assumptions in Theorem 2 hold,4.1. Convergence Guarantees on Gradients,[0],[0]
and we have n ≥,4.1. Convergence Guarantees on Gradients,[0],[0]
(d% + log(4/ε))β2/ .,4.1. Convergence Guarantees on Gradients,[0],[0]
"Then if the solution w̃ computed by minimizing the empirical risk obeys ‖∇Q̃n(w̃)‖22 ≤ , we have ‖∇Q(w̃)‖22 ≤ 4 with probability at least 1− ε.
",4.1. Convergence Guarantees on Gradients,[0],[0]
"Corollary 1 shows that by using full gradient descent algorithms to minimize the empirical risk, the computed approximate stationary point w̃ is also close to the desired stationary point w∗ of the population risk.",4.1. Convergence Guarantees on Gradients,[0],[0]
"This guarantee is also applicable to other stochastic gradient descent based algorithms, like SGD, adam and RMSProp, by applying recent results on obtaining -approximate stationary point for nonconvex problems (Ghadimi & Lan, 2013; Tieleman & Hinton, 2012; Kingma & Ba, 2015).",4.1. Convergence Guarantees on Gradients,[0],[0]
"Accordingly, the computed solution w̃ has guaranteed generalization performance on new data.",4.1. Convergence Guarantees on Gradients,[0],[0]
It partially explains the success of gradient descent based optimization algorithms for CNNs.,4.1. Convergence Guarantees on Gradients,[0],[0]
Here we go further and directly characterize the distance between stationary points in the empirical risk Q̃n(w) and its population counterpart Q(w).,4.2. Convergence of Stationary Points,[0],[0]
"Compared with the results for the risk and gradient, the results on stationary points give more direct performance guarantees for CNNs.",4.2. Convergence of Stationary Points,[0],[0]
"Here we only analyze the non-degenerate stationary points including local minimum/maximum and non-degenerate saddle points, as they are geometrically isolated and thus are unique in local regions.",4.2. Convergence of Stationary Points,[0],[0]
We first introduce some necessary definitions.,4.2. Convergence of Stationary Points,[0],[0]
Definition 2.,4.2. Convergence of Stationary Points,[0],[0]
"(Non-degenerate stationary points and saddle points) (Gromoll & Meyer, 1969)",4.2. Convergence of Stationary Points,[0],[0]
"A stationary point w is said to be a non-degenerate stationary point of Q(w) if
inf i
∣∣λi ( ∇2Q(w) )∣∣",4.2. Convergence of Stationary Points,[0],[0]
"≥ ζ,
where λi ( ∇2Q(w) ) is the i-th eigenvalue of the Hessian
∇2Q(w) and ζ is a positive constant.",4.2. Convergence of Stationary Points,[0],[0]
"A stationary point is said to be a saddle point if the smallest eigenvalue of its Hessian∇2Q(w) has a negative value.
",4.2. Convergence of Stationary Points,[0],[0]
"Suppose Q(w) has m non-degenerate stationary points which are denoted as {w(1), w(2), · · · ,w(m)}.",4.2. Convergence of Stationary Points,[0],[0]
We have following results on the geometry of these stationary points in Theorem 3.,4.2. Convergence of Stationary Points,[0],[0]
"The proof is given in Sec. D.5 in supplement.
",4.2. Convergence of Stationary Points,[0],[0]
Theorem 3.,4.2. Convergence of Stationary Points,[0],[0]
"Assume in CNNs, σ1 and σ2 are respectively the sigmoid and softmax activation functions and the loss f(g(w;D),y) is squared loss.",4.2. Convergence of Stationary Points,[0],[0]
Suppose Assumptions 1 and 2 hold.,4.2. Convergence of Stationary Points,[0],[0]
"Then if n ≥ ch max ( d+θ% ζ2 , l2bl+1 2(bl+1+ ∑l i=1 dibi) 2(r0c0d0) 4
d40b1 8d%ε2 maxi(rici)
) where ch is a constant,
for k ∈ {1, · · · ,m}, there exists a non-degenerate stationary point w(k)n of Q̃n(w) which uniquely corresponds to the non-degenerate stationary point w(k) of Q(w) with probability at least 1− ε.",4.2. Convergence of Stationary Points,[0],[0]
"Moreover, with same probability the distance between w(k)n and w(k) is bounded as
‖w(k)n −w(k)‖2≤ 2cgβ
ζ
√ 2d+θ%+log ( 4 ε )
2n , (1≤k≤m),
where % and β are given in Lemma 1 and Theorem 2, respectively.
",4.2. Convergence of Stationary Points,[0],[0]
"Theorem 3 shows that there exists exact one-to-one correspondence between the non-degenerate stationary points of the empirical risk Q̃n(w) and the popular risk Q(w) for CNNs, if the sample size n is sufficiently large.",4.2. Convergence of Stationary Points,[0],[0]
"Moreover, the non-degenerate stationary point w(k)n of Q̃n(w) is very close to its corresponding non-degenerate stationary point w(k) of Q(w).",4.2. Convergence of Stationary Points,[0],[0]
"More importantly, their distance shrinks at the rate of O (1/√n) (up to a log factor).",4.2. Convergence of Stationary Points,[0],[0]
The network parameters have similar influence on the distance bounds as explained in the above subsection.,4.2. Convergence of Stationary Points,[0],[0]
"Compared with gradient convergence rate in Theorem 2, the convergence rate of corresponding stationary point pairs in Theorem 3 has an extra factor 1/ζ that accounts for the geometric topology of non-degenerate stationary points, similar to other works like stochastic optimization analysis (Duchi & Ruan, 2016).
",4.2. Convergence of Stationary Points,[0],[0]
"For degenerate stationary points to which the corresponding Hessian matrix has zero eigenvalues, one cannot expect to establish unique correspondence for stationary points in empirical and population risks, since they are not isolated anymore and may reside in flat regions.",4.2. Convergence of Stationary Points,[0],[0]
But Theorem 2 guarantees that the gradients of Q̃n(w) and Q(w) at these points are close.,4.2. Convergence of Stationary Points,[0],[0]
"This implies a degenerate stationary point of Q(w) will also give a near-zero gradient for Q̃n(w), indicating it is also a stationary point for Q̃n(w).
",4.2. Convergence of Stationary Points,[0],[0]
"Du et al. (2017a;b) showed that for a simple and shallow CNN consisting of only one non-overlapping convolutional layer, (stochastic) gradient descent algorithms with weight
normalization can converge to the global minimum.",4.2. Convergence of Stationary Points,[0],[0]
"In contrast to their simplified models, we analyze complex multi-layer CNNs with the commonly used convolutional and pooling operations.",4.2. Convergence of Stationary Points,[0],[0]
"Besides, we provide results on both gradient and the distance between the computed solution and desired stationary points, which are applicable to arbitrary gradient descent based algorithms.
",4.2. Convergence of Stationary Points,[0],[0]
"Next, based on Theorem 3, we derive that the corresponding pair (w(k)n ,w(k)) in the empirical and population risks shares the same geometrical property stated in Corollary 2.",4.2. Convergence of Stationary Points,[0],[0]
Corollary 2.,4.2. Convergence of Stationary Points,[0],[0]
Suppose the assumptions in Theorem 3 hold.,4.2. Convergence of Stationary Points,[0],[0]
"If any one in the pair (w(k)n ,w(k)) in Theorem 3 is a local minimum or saddle point, so is the other one.
",4.2. Convergence of Stationary Points,[0],[0]
See the proof of Corollary 2 in Sec.,4.2. Convergence of Stationary Points,[0],[0]
D.6 in supplement.,4.2. Convergence of Stationary Points,[0],[0]
"Corollary 2 tells us that the corresponding pair, w(k)n and w(k), has the same geometric property.",4.2. Convergence of Stationary Points,[0],[0]
"Namely, if either one in the pair is a local minimum or saddle point, so is the other one.",4.2. Convergence of Stationary Points,[0],[0]
This result is important for optimization.,4.2. Convergence of Stationary Points,[0],[0]
"If the computed solution w̃ by minimizing the empirical risk Q̃n(w) is a local minimum, then it is also a local minimum of the population risk Q(w).",4.2. Convergence of Stationary Points,[0],[0]
Thus it partially explains why the computed solution w̃ can generalize well on new data.,4.2. Convergence of Stationary Points,[0],[0]
This property also benefits designing new optimization algorithms.,4.2. Convergence of Stationary Points,[0],[0]
"For example, Saxe et al. (2014) and Kawaguchi (2016) pointed out that degenerate stationary points indeed exist for deep linear neural networks and Dauphin et al. (2014) empirically validated that saddle points are usually surrounded by high error plateaus in deep forward neural networks.",4.2. Convergence of Stationary Points,[0],[0]
So it is necessary to avoid the saddle points and find the local minimum of population risk.,4.2. Convergence of Stationary Points,[0],[0]
"From Theorem 3, it is clear that one only needs to find the local minimum of empirical risk by using escaping saddle points algorithms, e.g. (Ge et al., 2015; Jin et al., 2017; Agarwal et al., 2017).",4.2. Convergence of Stationary Points,[0],[0]
"Here we compare deep feedforward neural networks (DNNs) with deep CNNs from their generalization error and optimization guarantees to theoretically explain why CNNs are more preferable than DNNs, to some extent.
",5. Comparison on DNNs And CNNs,[0],[0]
"By assuming the input to be standard Gaussian N (0, τ2), Zhou & Feng (2018) proved that if n ≥ 18r2/(dτ2ε2 log(l+ 1)), with probability 1 − ε, the generalization error of an (l + 1)-layer DNN model with sigmoid activation functions is bounded by n:
n , cnτ √
(1 + crl) max i di
√ d log(n(l+1))",5. Comparison on DNNs And CNNs,[0],[0]
"+ log(4/ε)
n ,
where cn is a universal constant; di denotes the width of the i-th layer; d is the total parameter number of the network; cr = max(r̂2/16, ( r̂2/16 )l ) where r̂ upper bounds Frobenius norm of the weight matrix in each
layer.",5. Comparison on DNNs And CNNs,[0],[0]
"Recall that the generalization bound of CNN provided in this work is O( √ θ%̃/(2n)), where %̃ =∑l
i=1 log (√ dibi(ki − si + 1)/(4p) )",5. Comparison on DNNs And CNNs,[0],[0]
"+ log(bl+1).
",5. Comparison on DNNs And CNNs,[0],[0]
"By observing the above two generalization bounds, one can see when the layer number is fixed, CNN usually has smaller generalization error than DNN because: (1) CNN usually has much fewer parameters, i.e. smaller d, than DNN due to parameter sharing mechanism of convolutions.",5. Comparison on DNNs And CNNs,[0],[0]
(2) The generalization error of CNN has a smaller factor than DNN in the network parameter magnitudes.,5. Comparison on DNNs And CNNs,[0],[0]
"Generalization error bound of CNN depends on a logarithm termO(log∏l+1i=1 bi) of the magnitude bi of each kernel/weight matrix, while the bound for DNN depends on O(r̂l+1).",5. Comparison on DNNs And CNNs,[0],[0]
"Since the kernel size is much smaller than that of the weight matrix in the fully connected layer by unfolding the convolutional layer, the upper magnitude bound bi (i = 1, · · · , l) of each kernel is usually much smaller than r̂. (3) The pooling operation and the stride in convolutional operation in CNN also benefit its generalization performance.",5. Comparison on DNNs And CNNs,[0],[0]
This is because the factor %̃ involves O(2(l + 1) log(1/p)) and (ki − si + 1) which also reduce the generalization error.,5. Comparison on DNNs And CNNs,[0],[0]
"Notice, by applying our analysis technique, it might be possible to remove the exponential term in DNN.",5. Comparison on DNNs And CNNs,[0],[0]
"But as mentioned above, the unique operations, e.g. convolution, pooling and striding, still benefit CNN, making it generalize better than DNN.
",5. Comparison on DNNs And CNNs,[0],[0]
"Because of the above factors, the empirical gradient in CNN converges to its population counterpart faster, as well as the paired non-degenerate stationary points for empirical risk and population risk.",5. Comparison on DNNs And CNNs,[0],[0]
"All these results guarantee that for an arbitrary gradient descent based algorithm, it is faster to compute an approximate stationary point or a local minimum in population risk of CNN compared with DNN.",5. Comparison on DNNs And CNNs,[0],[0]
Here we briefly introduce the proof roadmap.,6. Proof of Roadmap,[0],[0]
"Due to space limitation, all the proofs of our theoretical results are deferred to the supplement.",6. Proof of Roadmap,[0],[0]
"Firstly, our analysis relies on bounding the gradient magnitude and the spectral norm of Hessian of the loss f(g(w;D),y).",6. Proof of Roadmap,[0],[0]
"By considering multilayer architecture of CNN, we establish recursive relation of their magnitudes in the k and k + 1 layers (Lemmas 9 ∼ 14 in supplement) and get their overall magnitude upper bound.
",6. Proof of Roadmap,[0],[0]
"For the uniform convergence supw∈Ω |Q̃n(w) − Q(w)| in Lemma 1, we resort to bound three distances: A1 = supw∈Ω |Q̃n(w) − Q̃n(wkw)|, A2 = supwkw∈Θ |Q̃n(wkw)−EQ̃n(wkw)| and A3 = supw∈Ω |EQ̃n(wkw) −EQ(w)|, where wkw belongs to the -net",6. Proof of Roadmap,[0],[0]
"Θ of parameter domain Ω. Using Markov inequality and Lipschitz property of loss, we can bound A1 and A3.",6. Proof of Roadmap,[0],[0]
"To bound A2, we prove the empirical risk is sub-Gaussian.",6. Proof of Roadmap,[0],[0]
Considering the element wkw in -net,6. Proof of Roadmap,[0],[0]
"Θ is independent of input D, we use Hoeffd-
ing’s inequality to prove empirical risk at point wkw to be sub-Gaussian for any wkw in Θ. By this decoupling of -net, our bound on A2 depends on the constant magnitude of loss and gets rid of exponential term.",6. Proof of Roadmap,[0],[0]
"Combining these bounds together, we obtain the uniform convergence of empirical risk and can derive the generalization bound.
",6. Proof of Roadmap,[0],[0]
We use a similar decomposition and decoupling strategy mentioned above to bound gradient uniform convergence supw∈Ω‖∇wQ̃n(w)−∇wQ(w)‖2 in Theorem 2.,6. Proof of Roadmap,[0],[0]
"But here we need to bound gradient and spectral norm of Hessian.
",6. Proof of Roadmap,[0],[0]
"To prove correspondence and bounded distance of stationary points, we define a set G= {w∈Ω : ||∇Q̃n(w)||≤ and infi |λi(∇2Q̃n(w))|",6. Proof of Roadmap,[0],[0]
≥ ζ} where λi is the i-th eigenvalue of∇2Q̃n(w).,6. Proof of Roadmap,[0],[0]
Then G is decomposed into countable components each of which has one or zero non-degenerate stationary point.,6. Proof of Roadmap,[0],[0]
Next we prove the uniform convergence between empirical and population Hessian by using a similar strategy as above.,6. Proof of Roadmap,[0],[0]
"Combining uniform convergence of gradient and Hessian and the results in differential topology (Lemmas 4 & 5 in supplement), we obtain that for each component of G, if there is a unique non-degenerate stationary point in Q(w), then Q̃n(w) also has a unique one, and vice versa.",6. Proof of Roadmap,[0],[0]
This gives the one-to-one correspondence relation.,6. Proof of Roadmap,[0],[0]
"Finally, the uniform convergence of gradient and Hessian can bound the distance between the corresponding points.",6. Proof of Roadmap,[0],[0]
"In this work, we theoretically analyzed why deep CNNs can achieve remarkable success, from its generalization performance and the optimization guarantees of (stochastic) gradient descent based algorithms.",7. Conclusion,[0],[0]
We proved that the generalization error of deep CNNs can be bounded by a factor which depends on the network parameters.,7. Conclusion,[0],[0]
"Moreover, we analyzed the relationship between the computed solution by minimizing the empirical risk and the optimum solutions in population risk from their gradient and their Euclidean distance.",7. Conclusion,[0],[0]
"All these results show that with sufficient training samples, the generalization performance of deep CNN models can be guaranteed.",7. Conclusion,[0],[0]
"Besides, these results also reveal that the kernel size ki, the stride si, the pooling size p, the channel number di and the freedom degree θ of the network parameters are critical to the generalization performance of deep CNNs.",7. Conclusion,[0],[0]
We also showed that the weight parameter magnitude is also important.,7. Conclusion,[0],[0]
These suggestions on network designs accord with the widely used network architectures.,7. Conclusion,[0],[0]
"Jiashi Feng was partially supported by NUS startup R-263000-C08-133, MOE Tier-I R-263-000-C21-112, NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112.",Acknowledgements,[0],[0]
This document gives some other necessary notations and preliminaries for our analysis in Sec. B.1 and provides auxiliary lemmas in Sec. B.2.,A Structure of This Document,[0],[0]
"Then in Sec. C, we present the technical lemmas for proving our final results and their proofs.",A Structure of This Document,[0],[0]
"Next, in Sec. D, we utilize these technical lemmas to prove our desired results.",A Structure of This Document,[0],[0]
"Finally, we give the proofs of other auxiliary lemmas in Sec.",A Structure of This Document,[0],[0]
"E.
As for the results in manuscript, the proofs of Lemma 1 and Theorem 1 in Sec. 3.1 in the manuscript are respectively provided in Sec.",A Structure of This Document,[0],[0]
D.1 and Sec. D.2.,A Structure of This Document,[0],[0]
"As for the results in Sec. 4 in the manuscript, Sec.",A Structure of This Document,[0],[0]
"D.3 and D.4 present the proofs of Theorem 2 and Corollary 1, respectively.",A Structure of This Document,[0],[0]
"Finally, we respectively introduce the proofs of Theorem 3 and Corollary 2 in Sec. D.5 and D.6.",A Structure of This Document,[0],[0]
"Beyond the notations introduced in the manuscript, we need some other notations used in this document.",B Notations and Preliminary Tools,[0],[0]
Then we introduce several lemmas that will be used later.,B Notations and Preliminary Tools,[0],[0]
"Throughout this document, we use 〈·, ·〉 to denote the inner product and use ~̃ to denote the convolution operation with stride 1.",B.1 Notations,[0],[0]
A⊗C denotes the Kronecker product betweenA,B.1 Notations,[0],[0]
andC.,B.1 Notations,[0],[0]
Note thatA andC inA⊗C can be matrices or vectors.,B.1 Notations,[0],[0]
"For a matrixA ∈ Rn1×n2 , we use ‖A‖F = √∑ i,jA 2 ij to denote its Frobenius norm, whereAij is the (i, j)-th entry of A.",B.1 Notations,[0],[0]
"We use ‖A‖op = maxi |λi(A)| to denote the operation norm of a matrixA ∈ Rn1×n1 , where λi(A) denotes the i-th eigenvalue of the matrixA. For a 3-way tensorA ∈ Rs×t×q , its operation norm is computed as
‖A‖op = sup ‖λ‖2≤1
〈 λ⊗ 3 ,A 〉 = ∑
i,j,k
Aijkλiλjλk,
whereAijk denotes the (i, j, k)-th entry ofA.
For brevity, in this document we use f(w,D) to denote f(g(w;D),y) in the manuscript.",B.1 Notations,[0],[0]
Let w(i) =,B.1 Notations,[0],[0]
(w1(i); · · · ;wdi(i)) ∈,B.1 Notations,[0],[0]
Rki2di−1di,B.1 Notations,[0],[0]
"(i = 1, · · · , l) be the parameter of the i-th layer where wk(i) = vec ( W k(i) )",B.1 Notations,[0],[0]
∈ Rki2di−1 is the vectorization of W k(i).,B.1 Notations,[0],[0]
"Similarly, let w(l+1)",B.1 Notations,[0],[0]
= vec ( W(l+1) ) ∈ Rrlcldldl+1 .,B.1 Notations,[0],[0]
"Then, we further define w = (w(1), · · · ,w(l),w(l+1)) ∈ Rrlcldldl+1+ ∑l i=1 ki
2di−1di which contains all the parameter in the network.",B.1 Notations,[0],[0]
"Here we useW k(i) to denote the k-th kernel in
the i-th convolutional layer.",B.1 Notations,[0],[0]
"For brevity, letW k,j(i) denotes the j-th slice ofW k",B.1 Notations,[0],[0]
"(i), i.e. W k,j (i) =",B.1 Notations,[0],[0]
"W k (i)(:, :, j).
",B.1 Notations,[0],[0]
"For a matrixM ∈ Rs×t, M̂ denotes the matrix which is obtained by rotating the matrixM by 180 degrees.",B.1 Notations,[0],[0]
"Then we use δi to denote the gradient of f(w,D) w.r.t. X(i):
δi = ∇X(i)f(w,D) ∈ Rri×ci×di , (i = 1, · · · , l),
Based on δi, we further define δ̃i ∈ R(r̃i−1−ki+1)×(c̃i−1−ki+1)×di .",B.1 Notations,[0],[0]
Each slice δ̃ki+1 can be computed as follows.,B.1 Notations,[0],[0]
"Firstly, let δ̃ki+1 = δ k i+1.",B.1 Notations,[0],[0]
"Then, we pad zeros of si − 1 rows between the neighboring rows in δ̃ki+1 and similarly we pad zeros of si",B.1 Notations,[0],[0]
− 1 columns between the neighboring columns in δ̃ki+1.,B.1 Notations,[0],[0]
"Accordingly, the size of δ̃ k i+1 is (si(ri − 1) + 1)× (si(ci − 1) + 1).",B.1 Notations,[0],[0]
"Finally, we pad zeros of width ki",B.1 Notations,[0],[0]
− 1 around δ̃ki+1 to obtain new δ̃ki+1 ∈ R(si(ri−1)+2ki−1)×(si(ci−1)+2ki−1).,B.1 Notations,[0],[0]
"Note that since ri+1 = (r̃i − ki+1)/si+1 + 1 and ri+1 = (r̃i − ki+1)/si+1 + 1, we have si(ri − 1) +",B.1 Notations,[0],[0]
2ki,B.1 Notations,[0],[0]
− 1 = r̃i−1,B.1 Notations,[0],[0]
− ki + 1 and si(ci,B.1 Notations,[0],[0]
− 1) +,B.1 Notations,[0],[0]
2ki,B.1 Notations,[0],[0]
− 1 = c̃i−1 − ki + 1.,B.1 Notations,[0],[0]
"Then we define four operators G (·), Q (·), up (·) and vec(·), which are useful for explaining the following analysis.
",B.1 Notations,[0],[0]
Operation G (·): It maps an arbitrary vector z ∈ Rd into a diagonal matrix G (z) ∈ Rd×d with its i-th diagonal entry equal to σ1(zi)(1− σ1(zi)) in which zi denotes the i-th entry of z. Operation Q (·): it maps a vector z ∈ Rd into a matrix of size d2 × d whose ((i − 1)d,B.1 Notations,[0],[0]
+,B.1 Notations,[0],[0]
"i, i) (i = 1, · · · , d) entry equal to σ1(zi)(1− σ1(zi))(1− 2σ1(zi)) and rest entries are all 0.",B.1 Notations,[0],[0]
Operation up (·): up (M) represents conducting upsampling on M ∈ Rs×t×q.,B.1 Notations,[0],[0]
Let N = up (M) ∈ Rps×pt×q.,B.1 Notations,[0],[0]
"Specifically, for each sliceN(:, :, i) (i = 1, · · · , q), we haveN(:, :, i) = up (M(:, :, i)).",B.1 Notations,[0],[0]
"It actually upsamples each entryM(g, h, i) into a matrix of p2 same entries 1p2M(g, h, i).
",B.1 Notations,[0],[0]
"Operation vec(·): For a matrix A ∈ Rs×t, vec(A) is defined as vec(A) =",B.1 Notations,[0],[0]
"(A(:, 1); · · · ;A(:, t)) ∈ Rst that vectorizes A ∈ Rs×t along its columns.",B.1 Notations,[0],[0]
"If A ∈ Rt×s×q is a 3-way tensor, vec(A) =",B.1 Notations,[0],[0]
"[vec(A(:, :, 1)); vec(A(:, : , 2)), · · · , vec(A(:, :, q))]",B.1 Notations,[0],[0]
∈,B.1 Notations,[0],[0]
Rstq .,B.1 Notations,[0],[0]
We first introduce Lemmas 2 and 3 which are respectively used for bounding the `2-norm of a vector and the operation norm of a matrix.,B.2 Auxiliary Lemmas,[0],[0]
Then we introduce Lemmas 4 and 5 which discuss the topology of functions.,B.2 Auxiliary Lemmas,[0],[0]
"In Lemma 6, we introduce the Hoeffding’s inequality which provides an upper bound on the probability that the sum of independent random variables deviates from its expected value.",B.2 Auxiliary Lemmas,[0],[0]
"In Lemma 7, we provide the covering number of an -net for a low-rank matrix.",B.2 Auxiliary Lemmas,[0],[0]
"Finally, several commonly used inequalities are presented in Lemma 8 for our analysis.",B.2 Auxiliary Lemmas,[0],[0]
Lemma 2.,B.2 Auxiliary Lemmas,[0],[0]
"(Vershynin, 2012)",B.2 Auxiliary Lemmas,[0],[0]
"For any vector x ∈ Rd, its `2-norm can be bounded as
‖x‖2 ≤ 1
1−",B.2 Auxiliary Lemmas,[0],[0]
"supλ∈λ 〈λ,x〉 .
where λ = {λ1, . . .",B.2 Auxiliary Lemmas,[0],[0]
",λkw} be an -covering net of Bd(1).",B.2 Auxiliary Lemmas,[0],[0]
Lemma 3.,B.2 Auxiliary Lemmas,[0],[0]
"(Vershynin, 2012)",B.2 Auxiliary Lemmas,[0],[0]
"For any symmetric matrixX ∈ Rd×d, its operator norm can be bounded as
‖X‖op ≤ 1
1− 2 supλ∈λ |〈λ,Xλ〉| .
where λ = {λ1, . . .",B.2 Auxiliary Lemmas,[0],[0]
",λkw} be an -covering net of Bd(1).",B.2 Auxiliary Lemmas,[0],[0]
Lemma 4.,B.2 Auxiliary Lemmas,[0],[0]
"(Mei et al., 2017) Let D ⊆ Rd be a compact set with a C2 boundary ∂D, and f, g : A→ R be C2 functions defined on an open set A, with D ⊆ A. Assume that for all w ∈ ∂D and all t ∈",B.2 Auxiliary Lemmas,[0],[0]
"[0, 1], t∇f(w) + (1 − t)∇g(w) 6= 0.",B.2 Auxiliary Lemmas,[0],[0]
"Finally, assume that the Hessian ∇2f(w) is non-degenerate and has index equal to r for all w ∈",B.2 Auxiliary Lemmas,[0],[0]
"D. Then the following properties hold:
(1)",B.2 Auxiliary Lemmas,[0],[0]
"If g has no critical point in D, then f has no critical point in D.
(2)",B.2 Auxiliary Lemmas,[0],[0]
"If g has a unique critical point w in D that is non-degenerate with an index of r, then f also has a unique critical point w′",B.2 Auxiliary Lemmas,[0],[0]
"in D with the index equal to r.
Lemma 5.",B.2 Auxiliary Lemmas,[0],[0]
"(Mei et al., 2017) Suppose that F (w) :",B.2 Auxiliary Lemmas,[0],[0]
Θ→ R is a C2 function where w ∈,B.2 Auxiliary Lemmas,[0],[0]
"Θ. Assume that {w(1), . . .",B.2 Auxiliary Lemmas,[0],[0]
", w(m)} is its non-degenerate critical points and let D = {w ∈",B.2 Auxiliary Lemmas,[0],[0]
Θ : ‖∇F (w)‖2 < and infi ∣∣λi ( ∇2F (w) )∣∣ ≥ ζ}.,B.2 Auxiliary Lemmas,[0],[0]
"Then D can be decomposed into (at most) countably components, with each component containing either exactly one critical point, or no critical point.",B.2 Auxiliary Lemmas,[0],[0]
"Concretely, there exist disjoint open sets {Dk}k∈N, with Dk possibly empty for k ≥ m+ 1, such that
D = ∪∞k=1Dk .",B.2 Auxiliary Lemmas,[0],[0]
"Furthermore, w(k) ∈ Dk for 1 ≤ k ≤ m and each Di, k ≥",B.2 Auxiliary Lemmas,[0],[0]
"m+ 1 contains no stationary points.
",B.2 Auxiliary Lemmas,[0],[0]
Lemma 6.,B.2 Auxiliary Lemmas,[0],[0]
"(Hoeffding, 1963) Let x1, · · · , xn be independent random variables where xi is bounded by the interval [ai, bi].",B.2 Auxiliary Lemmas,[0],[0]
Suppose sn = 1n,B.2 Auxiliary Lemmas,[0],[0]
∑n i=1,B.2 Auxiliary Lemmas,[0],[0]
"xi, then the following properties hold:
P (sn − Esn ≥ t) ≤ exp ( − 2n
2t2∑n i=1(bi",B.2 Auxiliary Lemmas,[0],[0]
"− ai)2
) .
where t is an arbitrary positive value.
",B.2 Auxiliary Lemmas,[0],[0]
Lemma 7.,B.2 Auxiliary Lemmas,[0],[0]
"(Candes & Plan, 2009) Let Sr = {X ∈ Rn1×n2 :",B.2 Auxiliary Lemmas,[0],[0]
rank (X) ≤,B.2 Auxiliary Lemmas,[0],[0]
"r, ‖X‖F = b}.",B.2 Auxiliary Lemmas,[0],[0]
Then there exists an -net,B.2 Auxiliary Lemmas,[0],[0]
"S̃r for the Frobenius norm obeying
|S̃r| ≤",B.2 Auxiliary Lemmas,[0],[0]
( 9b ),B.2 Auxiliary Lemmas,[0],[0]
"r(n1+n2+1) .
",B.2 Auxiliary Lemmas,[0],[0]
"Then we give a lemma to summarize the properties of G (·), Q (·) and up (·) defined in Section B.1, the convolutional operation ~ defined in manuscript.",B.2 Auxiliary Lemmas,[0],[0]
Lemma 8.,B.2 Auxiliary Lemmas,[0],[0]
"For G (·), Q (·) and up (·) defined in Section B.1, the convolutional operation ~ defined in manuscript, we have the following properties:
(1) For arbitrary vector z, and arbitrary matricesM andN of proper sizes, we have
‖G (z)M‖2F ≤ 1
16 ‖M‖2F and ‖NG (z)‖2F ≤
1
16 ‖N‖2F .
",B.2 Auxiliary Lemmas,[0],[0]
"(2) For arbitrary vector z, and arbitrary matricesM andN of proper sizes, we have
‖Q (z)M‖2F ≤ 26
38 ‖M‖2F and ‖NQ (z)‖2F ≤
26 38 ‖N‖2F .
",B.2 Auxiliary Lemmas,[0],[0]
"(3) For any tensorM ∈ Rs×t×q , we have
‖up (M)‖2F ≤ 1
p2 ‖M‖2F .
",B.2 Auxiliary Lemmas,[0],[0]
(4) For any kernelW ∈ Rki×ki×di and δ̃i+1 ∈,B.2 Auxiliary Lemmas,[0],[0]
"R(r̃i−1−ki+1)×(c̃i−1−ki+1)×di defined in Sec. B.1, then we have
‖δ̃i+1~̃W ‖2F ≤",B.2 Auxiliary Lemmas,[0],[0]
(ki − si + 1)2‖W ‖2F,B.2 Auxiliary Lemmas,[0],[0]
"‖δ̃i+1‖2F .
",B.2 Auxiliary Lemmas,[0],[0]
"(5) For softmax activation function σ2, we can bound the norm of difference between output v and its corresponding ont-hot label as
0 ≤",B.2 Auxiliary Lemmas,[0],[0]
‖v,B.2 Auxiliary Lemmas,[0],[0]
"− y‖22 ≤ 2.
",B.2 Auxiliary Lemmas,[0],[0]
It should be pointed out that we defer the proof of Lemma 8 to Sec. E.,B.2 Auxiliary Lemmas,[0],[0]
Here we present the key lemmas and theorems for proving our desired results.,C Technical Lemmas and Their Proofs,[0],[0]
"For brevity, in this document we use f(w,D) to denote f(g(w;D),y) in the manuscript.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 9.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
"Then the gradient of f(w,D) with respect to w(j) can be written as
∇W(l+1)f(w,D) =S(v − y)zT(l) ∈ Rdl+1×r̃lc̃ldl , ∇W k,j (i) f(w,D) =Zj(i−1)~̃δ̃ k i ∈",C Technical Lemmas and Their Proofs,[0],[0]
"Rki×ki , (j = 1, · · · , di−1; k = 1, · · · , di; i = 1, · · · , l),
where δki is the k-slice (i.e. δi(:, :, k)) of δi which is defined as
δi = ∇X(i)f(w,D) ∈ Rri×ci×di , (i = 1, · · · , l),
and further satisfies
δji =up
  di+1∑
k=1
δ̃ki+1~̃Ŵ",C Technical Lemmas and Their Proofs,[0],[0]
"k,j(i+1)   σ′1(Xj(i)) ∈ Rri×ci , (j = 1, · · · , di; i = 1, · · · , l − 1),
where the matrix Ŵ k,j(i+1) ∈ Rki+1×ki+1 is obtained by rotatingW k,j (i+1) with 180 degrees.",C Technical Lemmas and Their Proofs,[0],[0]
"Also, δl is computed as follows:
∇uf(w,D) = S(v − y) ∈ Rdl+1",C Technical Lemmas and Their Proofs,[0],[0]
", ∇z(l)f(w,D) = (W(l+1))TS(v",C Technical Lemmas and Their Proofs,[0],[0]
"− y) ∈ Rr̃lc̃ldl , ∇Z(l)f(w,D)= reshape ( ∇z(l)f(w,D) )",C Technical Lemmas and Their Proofs,[0],[0]
"∈ Rr̃l×c̃l×dl , δl=σ′1(X(l))",C Technical Lemmas and Their Proofs,[0],[0]
"up ( ∂f(w,D)
∂Z(l)
) ∈ Rri×ci×di .
where S = G (u).
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
"We use chain rule to compute the gradient of f(w,D) with respect to Z(i).",C Technical Lemmas and Their Proofs,[0],[0]
We first compute several basis gradient.,C Technical Lemmas and Their Proofs,[0],[0]
"According to the relationship betweenX(i),Y(i),Z(i) and f(w,D), we have
∇uf(w,D) = S(v − y) ∈ Rdl+1 , ∇z(l)f(w,D) =",C Technical Lemmas and Their Proofs,[0],[0]
"(W(l+1))TS(v − y) ∈ Rr̃lc̃ldl , ∇Z(l)f(w,D) = reshape ( ∇z(l)f(w,D) )",C Technical Lemmas and Their Proofs,[0],[0]
"∈ Rr̃l×c̃l×dl ,
∇X(l)f(w,D) = ∂Y(l)
∂X(l)
",C Technical Lemmas and Their Proofs,[0],[0]
"∂Z(l)
∂Y(l)
",C Technical Lemmas and Their Proofs,[0],[0]
"∂f(w,D)
∂Z(l) = σ′1(X(l))",C Technical Lemmas and Their Proofs,[0],[0]
"up
( ∂f(w,D)
∂Z(l)
)",C Technical Lemmas and Their Proofs,[0],[0]
", δl ∈ Rri×ci×di .
",C Technical Lemmas and Their Proofs,[0],[0]
"Then we can further obtain
δji =up
  di+1∑
k=1
δ̃ki+1~̃Ŵ k,j(i+1)   σ′1(Xj(i))",C Technical Lemmas and Their Proofs,[0],[0]
"∈ Rri×ci , (j = 1, · · · , di; i = 1, · · · , l − 1).
where Ŵ k,j(i) denotes the j-th slice Ŵ k (i)(:, : j) of Ŵ k (i).",C Technical Lemmas and Their Proofs,[0],[0]
"Note, we clockwise rotate the matrix W k,j (i+1) by 180 degrees to obtain Ŵ k,j(i+1).",C Technical Lemmas and Their Proofs,[0],[0]
"Finally, we can compute the gradient w.r.t. W(l+1)",C Technical Lemmas and Their Proofs,[0],[0]
"andW (i) (i = 1, · · · , l):
∇W(l+1)f(w,D) =S(v − y)zT(l) ∈ Rdl+1×r̃lc̃ldl , ∇W k,j (i) f(w,D) =Xj(i−1)~̃δ̃",C Technical Lemmas and Their Proofs,[0],[0]
"k i ∈ Rki×ki , (j = 1, · · · , di−1; k = 1, · · · , di; i = 1, · · · , l).
",C Technical Lemmas and Their Proofs,[0],[0]
"The proof is completed.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 10.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
"Then the gradient of f(w,D) with respect to w(j) can be written as
‖δl‖2F ≤ ϑ
16p2 bl+1
2, ‖δi‖2F ≤ di+1bi+1 2(ki+1",C Technical Lemmas and Their Proofs,[0],[0]
− si+1 + 1)2 16p2 ∥∥δi+1 ∥∥2,C Technical Lemmas and Their Proofs,[0],[0]
"F , ‖δi‖2F ≤ ϑbl+1 2 16p2",C Technical Lemmas and Their Proofs,[0],[0]
"l∏
s=i+1
dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2 ,
where ϑ = 1/8.
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
We first bound δl.,C Technical Lemmas and Their Proofs,[0],[0]
"By Lemma 9, we have
‖δl‖2F = ∥∥∥∥σ′1(X(l)) up ( ∂f(w,D)
∂Z(l)
)∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
¬ ≤ 1
16p2
∥∥∥∥ ∂f(w,D)
∂Z(l)
∥∥∥∥ 2
F
= 1
16p2
∥∥∥∥ ∂f(w,D)
∂z(l)
∥∥∥∥ 2
2
≤ 1 16p2
∥∥(W(l+1))TS(v",C Technical Lemmas and Their Proofs,[0],[0]
"− y) ∥∥2
2
 ≤ ϑ
16p2 bl+1
2,
where ¬ holds since the values of the entries in the tensor σ′1(X(l)) ∈ Rrl×cl×dl belong to [0, 1/4], and the up (·) operation does repeat one entry into p2 entries but the entry value becomes 1/p2 of the original entry value.  holds since we have ‖S(v",C Technical Lemmas and Their Proofs,[0],[0]
− y)‖22 ≤ 1/8 in Lemma 8.,C Technical Lemmas and Their Proofs,[0],[0]
"Also by Lemma 9, we can further bound ‖δji ‖2F",C Technical Lemmas and Their Proofs,[0],[0]
"as follows:
‖δi‖2F = di∑
j=1
∥∥∥δji ∥∥∥ 2
F =
di∑
j=1
∥∥∥∥∥∥ up   di+1∑
k=1
δ̃ki+1~̃Ŵ",C Technical Lemmas and Their Proofs,[0],[0]
"k,j(i+1)   σ′1(Xj(i))",C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ 1 16p2
di∑
j=1
∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"di+1∑
k=1
δ̃ki+1~̃Ŵ",C Technical Lemmas and Their Proofs,[0],[0]
"k,j(i+1) ∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ di+1",C Technical Lemmas and Their Proofs,[0],[0]
"16p2
di∑
j=1
di+1∑
k=1
∥∥∥δ̃ki+1~̃Ŵ",C Technical Lemmas and Their Proofs,[0],[0]
"k,j(i+1) ∥∥∥ 2 F ≤ di+1(ki+1",C Technical Lemmas and Their Proofs,[0],[0]
"− si+1 + 1) 2 16p2 di∑
j=1
di+1∑
k=1
∥∥∥δ̃ki+1 ∥∥∥ 2
F
∥∥∥Ŵ k,j(i+1) ∥∥∥ 2
F
¬ = di+1(ki+1 − si+1 + 1)2
16p2
di+1∑
k=1
∥∥δki+1 ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
F ∥∥∥W k(i+1),C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥ 2 F ≤ di+1bi+1 2(ki+1,C Technical Lemmas and Their Proofs,[0],[0]
− si+1 + 1)2 16p2 ∥∥δi+1 ∥∥2,C Technical Lemmas and Their Proofs,[0],[0]
"F ,
where Ŵ k,j(i+1) denotes the j-th slice Ŵ k (i+1)(:, :, j) of the tensor Ŵ k (i+1).",C Technical Lemmas and Their Proofs,[0],[0]
"¬ holds since we rotate the matrix W k,j (i+1)
by 180 degrees to obtain Ŵ k(i+1), indicating ‖W k,j (i+1)‖2F = ‖Ŵ k,j (i+1)‖2F and ∑di",C Technical Lemmas and Their Proofs,[0],[0]
"j=1 ∥∥∥W k,j(i+1) ∥∥∥ 2 F = ∥∥∥W k(i+1)",C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥ 2 F ≤ bi+12.,C Technical Lemmas and Their Proofs,[0],[0]
"Accordingly, the above inequality gives
‖δi‖2F ≤ di+1bi+1",C Technical Lemmas and Their Proofs,[0],[0]
2(ki+1,C Technical Lemmas and Their Proofs,[0],[0]
− si+1 + 1)2 16p2 ∥∥δi+1 ∥∥2,C Technical Lemmas and Their Proofs,[0],[0]
"F ≤· · ·≤‖δl‖2F l∏
s=i+1
dsbs 2(ks − ss + 1)2
16p2
≤ϑbl+1 2
16p2
l∏
s=i+1
dsbs 2(ks − ss + 1)2
16p2 .
",C Technical Lemmas and Their Proofs,[0],[0]
"The proof is completed.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 11.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
"Then the gradient of f(w,D) with respect toW (l+1) and w can be respectively bounded as follows:
∥∥∇W(l+1)f(w,D) ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
"F ≤ ϑr̃lc̃ldl and ‖∇wf(w,D)‖22 ≤ β2,
where ϑ = 1/8 and β , [ ϑr̃lc̃ldl + ∑l i=1 ϑbl+1 2di−1",C Technical Lemmas and Their Proofs,[0],[0]
p2bi2di ri−1ci−1,C Technical Lemmas and Their Proofs,[0],[0]
"∏l s=i dsbs 2(ks−ss+1)2 16p2 ]1/2 .
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
"By utilizing Lemma 10, we can bound
l∑
i=1
",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∇w(i)f(w,D) ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
"F = l∑
i=1
di∑
k=1
di−1∑
j=1
∥∥∥∇W k,j (i) f(w,D) ∥∥∥ 2 F = l∑
i=1
",C Technical Lemmas and Their Proofs,[0],[0]
"di∑
k=1
di−1∑
j=1
∥∥∥Zj(i−1)~̃δki ∥∥∥ 2
F
¬ ≤
l∑
i=1
",C Technical Lemmas and Their Proofs,[0],[0]
"di∑
k=1
di−1∑
j=1
(ki − si + 1)2 ∥∥∥Zj(i−1) ∥∥∥ 2
F
∥∥δki ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
"F
 ≤
l∑
i=1
di∑
k=1
di−1∑
j=1
r̃i−1c̃i−1(ki − si + 1)2 ∥∥δki",C Technical Lemmas and Their Proofs,[0],[0]
∥∥2,C Technical Lemmas and Their Proofs,[0],[0]
"F
≤ l∑
i=1
r̃i−1c̃i−1di−1(ki",C Technical Lemmas and Their Proofs,[0],[0]
"− si + 1)2 ‖δi‖2F
≤ l∑
i=1
r̃i−1c̃i−1di−1(ki",C Technical Lemmas and Their Proofs,[0],[0]
"− si + 1)2 ϑbl+1
2
16p2
l∏
s=i+1
dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2
= l∑
i=1
ϑbl+1 2di−1
p2bi",C Technical Lemmas and Their Proofs,[0],[0]
"2di
ri−1ci−1
l∏
s=i
dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2 ,
where ¬ holds since ∥∥∥Zj(i−1)~̃δki ∥∥∥ 2
F ≤",C Technical Lemmas and Their Proofs,[0],[0]
"(ki− si + 1)2
∥∥∥Zj(i−1) ∥∥∥ 2
F
∥∥δki ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
"F
; and  holds since the values of entries in Zj(i−1) belong to [0, 1].
",C Technical Lemmas and Their Proofs,[0],[0]
"On the other hand, we can bound
∥∥∇W(l+1)f(w,D) ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
F = ∥∥∥S(v,C Technical Lemmas and Their Proofs,[0],[0]
− y)zT(l),C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥ 2 F ≤ ϑr̃lc̃ldl.
",C Technical Lemmas and Their Proofs,[0],[0]
"So we can bound the `2 norm of the gradient as follows:
‖∇wf(w,D)‖2F =",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∇W(l+1)f(w,x) ∥∥2 F + l∑
i=1
",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∇w(i)f(w,D) ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
"F
≤ϑr̃lc̃ldl + l∑
i=1
ϑbl+1 2di−1
p2bi",C Technical Lemmas and Their Proofs,[0],[0]
"2di
ri−1ci−1
l∏
s=i
dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2 .
",C Technical Lemmas and Their Proofs,[0],[0]
"The proof is completed.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 12.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
"Then for both cases, the gradient of x(i) with respect to w(j) can be bounded as follows:
∥∥∥∥∥∥ ∂vec ( X(i) ) ∂w(j) ∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
= ∥∥∥∥ ∂x(i)
∂w(j)
∥∥∥∥ 2
F
≤",C Technical Lemmas and Their Proofs,[0],[0]
"diricir̃j−1c̃j−1dj−1(kj − sj + 1)2 i∏
s=j+1
dsbs 2(ks − ss + 1)2
16p2
and
max s ∥∥∥∥∥∥ ∂vec ( Xs(i) ) ∂w(j) ∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
= max s
∥∥∥∥ ∂xs(i)
∂w(j)
∥∥∥∥ 2
F
≤",C Technical Lemmas and Their Proofs,[0],[0]
"ricir̃j−1c̃j−1dj−1(kj − sj + 1)2 i∏
s=j+1
dsbs 2(ks − ss + 1)2
16p2 .
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
"For brevity, letXk(i)(s, t) denotes the (s, t)-th entry in the matrixX k (i) ∈ Rri×ci .",C Technical Lemmas and Their Proofs,[0],[0]
"We also let φ(i,m) =
∂Xk(i)(s,t)
∂X(m) ∈
Rrm×cm×dm .",C Technical Lemmas and Their Proofs,[0],[0]
"So similar to Lemma 9, we have
φq(i,m) =up
  dm+1∑
k=1
φ̃k(i,m+1)~̃Ŵ k,q (m+1)   σ′1(Xq(m))",C Technical Lemmas and Their Proofs,[0],[0]
"∈ Rrm×cm , (q = 1, · · · , dm),
where the matrix Ŵ k,q(m+1) ∈ Rkm+1×km+1 is obtained by rotating W k,q (m+1) with 180 degrees.",C Technical Lemmas and Their Proofs,[0],[0]
"Then according to the relationship betweenX(j) andW(j), we can compute
∂Xk(i)(s, t)
∂W g,h(j) = Zh(j−1)~̃φ g (i,j) ∈ Rkj×kj , (h = 1, · · · , dj−1; g = 1, · · · , dj).
",C Technical Lemmas and Their Proofs,[0],[0]
"Therefore, we can further obtain ∥∥∥∥∥ ∂Xk(i)(s, t)
∂w(j)
∥∥∥∥∥ 2
F
=
dj∑
g=1
dj−1∑
h=1
∥∥∥∥∥ ∂Xk(i)(s, t)
∂W g,h(j)
",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥ 2
F
=
dj∑
g=1
dj−1∑
h=1
∥∥∥∥∥ ∂Xk(i)(s, t)
∂W",C Technical Lemmas and Their Proofs,[0],[0]
"g,h(j)
",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥ 2
F
=
dj∑
g=1
dj−1∑
h=1
∥∥∥Zh(j−1)~̃φg(i,j) ∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ dj∑
g=1
dj−1∑
h=1
(kj − sj + 1)2 ∥∥∥Zh(j−1) ∥∥∥ 2
F
∥∥∥φg(i,j) ∥∥∥ 2
F ≤ (kj − sj + 1)2
∥∥∥Z(j−1) ∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
∥∥∥φ(i,j) ∥∥∥ 2
F
≤r̃j−1c̃j−1dj−1(kj − sj + 1)2 ∥∥∥φ(i,j) ∥∥∥ 2
F .
",C Technical Lemmas and Their Proofs,[0],[0]
"On the other hand, by Lemma 9, we can further bound ‖φ(i,j)‖2F as follows:
∥∥∥φ(i,m) ∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F =
dm∑
q=1
∥∥∥φq(i,m) ∥∥∥ 2
F =
dm∑
q=1
∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"up   dm+1∑
k=1
φ̃k(i,m+1)~̃Ŵ k,q (m+1)   σ′1(Xq(m))",C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ 1 16p2
dm∑
q=1
∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"dm+1∑
k=1
φ̃k(i,m+1)~̃Ŵ k,q (m+1) ∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ dm+1 16p2
dm∑
q=1
dm+1∑
k=1
∥∥∥φ̃k(i,m+1)~̃Ŵ k,q(m+1) ∥∥∥ 2
F
≤dm+1(km+1",C Technical Lemmas and Their Proofs,[0],[0]
"− sm+1 + 1) 2
16p2
dm∑
q=1
dm+1∑
k=1
∥∥∥φ̃k(i,m+1) ∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
∥∥∥Ŵ k,q(m+1) ∥∥∥ 2
F
¬ = dm+1(km+1",C Technical Lemmas and Their Proofs,[0],[0]
"− sm+1 + 1)2
16p2
dm+1∑
k=1
∥∥∥φ̃k(i,m+1) ∥∥∥ 2
F
∥∥∥Ŵ k(m+1) ∥∥∥ 2
F
≤dm+1bm+1",C Technical Lemmas and Their Proofs,[0],[0]
2(km+1,C Technical Lemmas and Their Proofs,[0],[0]
"− sm+1 + 1)2
16p2
∥∥∥φ(i,m+1) ∥∥∥ 2
F ,
where ¬ holds since ‖W k,q(m+1)‖2F = ‖Ŵ k,q (m+1)‖2F .",C Technical Lemmas and Their Proofs,[0],[0]
"It further yields
∥∥∥φ(i,m) ∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2 F ≤ ∥∥∥φ(i,i) ∥∥∥ 2 F i∏
s=m+1
dsbs 2(ks − ss + 1)2
16p2 ¬ =
i∏
s=m+1
dsbs 2(ks − ss + 1)2
16p2 .
",C Technical Lemmas and Their Proofs,[0],[0]
"where ¬ holds since we have ∥∥∥φ(i,i) ∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F =
∥∥∥∥ ∂Xk(i)(s,t)
∂X(i)
∥∥∥∥ 2
F
= 1.
",C Technical Lemmas and Their Proofs,[0],[0]
"Therefore, we have ∥∥∥∥∥ ∂xk(i)
∂w(j)
∥∥∥∥∥ 2
F
=
ri∑
s=1
ci∑
t=1
∥∥∥∥∥ ∂Xk(i)(s, t)
∂w(j)
∥∥∥∥∥ 2
F
≤ ri∑
s=1
ci∑
t=1
r̃j−1c̃j−1dj−1(kj − sj + 1)2 ∥∥∥φ(i,j) ∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
=ricir̃j−1c̃j−1dj−1(kj − sj + 1)2 ∥∥∥φ(i,j) ∥∥∥ 2
F
≤ricir̃j−1c̃j−1dj−1(kj − sj + 1)2 i∏
s=j+1
dsbs 2(ks − ss + 1)2
16p2 .
",C Technical Lemmas and Their Proofs,[0],[0]
"It further gives ∥∥∥∥ ∂x(i)
∂w(j)
∥∥∥∥ 2
F
=
di∑
s=1
∥∥∥∥ ∂xs(i)
∂w(j)
∥∥∥∥ 2
F
≤ diricir̃j−1c̃j−1dj−1(kj − sj + 1)2 i∏
s=j+1
dsbs 2(ks − ss + 1)2
16p2 .
",C Technical Lemmas and Their Proofs,[0],[0]
"The proof is completed.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 13.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
"Then the gradient of δsl with respect to w(j) can be bounded as follows:
∥∥∥∥ ∂δsl ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ ϑ̃bl+1 2
16p2
∥∥∥W s(l+1)",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥ 2
F dlrlclr̃j−1c̃j−1dj−1(kj − sj + 1)2
l∏
s=j+1
dsbs 2(ks − ss + 1)2
16p2
and ∥∥∥∥ ∂δl ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ ϑ̃bl+1 4
16p2 dlrlclr̃j−1c̃j−1dj−1(kj − sj + 1)2
l∏
s=j+1
dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2 ,
where ϑ̃ = 364 .
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
Assume thatW(l+1) =,C Technical Lemmas and Their Proofs,[0],[0]
"[W 1(l+1),W 2 (l+1), · · · ,W dl(l+1)] whereW i(l+1) ∈ Rdl+1×r̃lc̃l is a submatrix inW(l+1).",C Technical Lemmas and Their Proofs,[0],[0]
Then we have v = σ2( ∑dl k=1W k,C Technical Lemmas and Their Proofs,[0],[0]
(l+1)z k (l)).,C Technical Lemmas and Their Proofs,[0],[0]
"For brevity, we further define a matrixG(k) as follows:
",C Technical Lemmas and Their Proofs,[0],[0]
"G(k)= [ σ′1 ( x(k) ) , σ′1 ( x(k) ) , · · · , σ′1 ( x(k) ) ︸",C Technical Lemmas and Their Proofs,[0],[0]
"︷︷ ︸
rkck columns
] ∈ Rrkckdk×rkck ,
Then we have
∂
∂z(l)
( ∂f(w,D)
∂zs(l)
) =",C Technical Lemmas and Their Proofs,[0],[0]
[ (v − y)T ⊗ (W s(l+1))T ] Q (u)W(l+1) +,C Technical Lemmas and Their Proofs,[0],[0]
"(W s(l+1)) TG (u)G (u)W(l+1),
where Q (u) is a matrix of size d2l+1 × dl+1",C Technical Lemmas and Their Proofs,[0],[0]
"whose (s, (s− 1)dl+1 + s) entry equal to σ1(us)(1− σ1(us))(1− 2σ1(us)) and rest entries are all 0.",C Technical Lemmas and Their Proofs,[0],[0]
"Accordingly, we have
∥∥∥∥∥ ∂
∂z(l)
( ∂f(w,D)
∂zs(l)
)∥∥∥∥∥ 2
F
≤2 (∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
[ (v − y)T ⊗ (W s(l+1))T ] Q (u)W(l+1) ∥∥∥ 2 F + ∥∥∥(W s(l+1))TG (u)G (u)W(l+1) ∥∥∥ 2 F ) ≤2,C Technical Lemmas and Their Proofs,[0],[0]
"( 26
38 ‖v",C Technical Lemmas and Their Proofs,[0],[0]
"− y‖2F
∥∥∥W s(l+1)",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥ 2
F
∥∥W(l+1)",C Technical Lemmas and Their Proofs,[0],[0]
∥∥2,C Technical Lemmas and Their Proofs,[0],[0]
"F + 1
162
∥∥∥W s(l+1) ∥∥∥ 2
F
∥∥W(l+1) ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
"F
)
¬ ≤2 ( 27
38 +
1
162
) bl+1 2 ∥∥∥W s(l+1) ∥∥∥ 2
F
≤ 3 64 bl+1
2 ∥∥∥W s(l+1) ∥∥∥ 2
F ,
where we have ‖v − y‖2F ≤ 2 by Lemma 8.",C Technical Lemmas and Their Proofs,[0],[0]
"Then by similar way, we can have
∥∥∥∥ ∂δsl ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
= ∥∥∥∥∥ ∂
∂z(l)
( ∂f(w,D)
∂zs(l)
) ∂z(l)
",C Technical Lemmas and Their Proofs,[0],[0]
∂yl ∂yl ∂x(l),C Technical Lemmas and Their Proofs,[0],[0]
"∂x(l) ∂w(j) ∥∥∥∥∥ 2
F
≤ 3 64 ∗ 16p2 bl+1
2 ∥∥∥W",C Technical Lemmas and Their Proofs,[0],[0]
s(l+1),C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
∥∥∥∥ ∂x(l)
∂w(j)
∥∥∥∥ 2
F
≤ 3 64 ∗ 16p2 bl+1
2 ∥∥∥W s(l+1) ∥∥∥ 2
F dlrlclr̃j−1c̃j−1dj−1(kj − sj + 1)2
l∏
s=j+1
dsbs 2(ks − ss + 1)2
16p2 .
",C Technical Lemmas and Their Proofs,[0],[0]
"Therefore, we can further obtain:
∥∥∥∥ ∂δl ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
=
dl+1∑
s=1
∥∥∥∥ ∂δsl ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ 3 64 ∗ 16p2 bl+1
4dlrlclr̃j−1c̃j−1dj−1(kj − sj + 1)2",C Technical Lemmas and Their Proofs,[0],[0]
"l∏
s=j+1
dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2 .
",C Technical Lemmas and Their Proofs,[0],[0]
"The proof is completed.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 14.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
"Then the Hessian of f(w,x) with respect to w can be bounded as follows:
∥∥∇2wf(w,D) ∥∥2 F ≤",C Technical Lemmas and Their Proofs,[0],[0]
"O (γ2) ,
where γ = ( ϑbl+1
2d20 b14d21",C Technical Lemmas and Their Proofs,[0],[0]
l2r20c 2 0,C Technical Lemmas and Their Proofs,[0],[0]
[∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]2)1/2 .,C Technical Lemmas and Their Proofs,[0],[0]
"With the same condition, we can bound the operation norm of
∇3wf(w,D).",C Technical Lemmas and Their Proofs,[0],[0]
"That is, there exists a universal constant ν such that ∥∥∇3wf(w,D) ∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"op ≤ ν.
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
"From Lemma 9, we can further compute the Hessian matrix ∇2wf(w,D).",C Technical Lemmas and Their Proofs,[0],[0]
"Recall that wk(i) ∈ Rki 2di−1 (k =
1, · · · , dl) is the vectorization of W k(i) ∈ Rki×ki×di−1 , i.e. wk(i) =",C Technical Lemmas and Their Proofs,[0],[0]
"[ vec ( W 1(i)(:, : 1) ) ; · · · ; vec ( W di(i)(:, :, di−1) )] .
",C Technical Lemmas and Their Proofs,[0],[0]
Let w(i) =,C Technical Lemmas and Their Proofs,[0],[0]
"[ w1(i); · · · ;wdi(i) ] ∈ Rki2di−1×di (i = 1, · · · , l).",C Technical Lemmas and Their Proofs,[0],[0]
"Also, w(l+1) ∈ Rr̃lc̃ldldl+1 is the vectorization of the weight matrixW(l+1).",C Technical Lemmas and Their Proofs,[0],[0]
"Then if 1 ≤ i, j ≤",C Technical Lemmas and Their Proofs,[0],[0]
"l, we can have
∂2f(w,D)
∂w(j)∂w",C Technical Lemmas and Their Proofs,[0],[0]
"k (i)
=   ",C Technical Lemmas and Their Proofs,[0],[0]
"∂2f(w,D) ∂w(j)∂w",C Technical Lemmas and Their Proofs,[0],[0]
"k,1 (i) ∂2f(w,D) ∂w(j)∂w",C Technical Lemmas and Their Proofs,[0],[0]
"k,2 (i) ...",C Technical Lemmas and Their Proofs,[0],[0]
"∂2f(w,D)
∂w(j)∂w",C Technical Lemmas and Their Proofs,[0],[0]
"k,di−1 (i)
  =   ∂(vec(Z1(i−1)~̃δ̃ki )) ∂w(j) ∂(vec(Z2(i−1)~̃δ̃ki )) ∂w(j) ... ∂",C Technical Lemmas and Their Proofs,[0],[0]
"( vec ( Z di−1 (i−1)~̃δ̃ki ))
∂w(j)
 
=   P1 ( δ̃ki ) ∂(vec(Z1(i−1))) ∂w(j) +",C Technical Lemmas and Their Proofs,[0],[0]
P2 ( Z1(i−1) ),C Technical Lemmas and Their Proofs,[0],[0]
∂(vec(δ̃ki )) ∂w(j) P1 ( δ̃ki ) ∂(vec(Z2(i−1))) ∂w(j) + P2 ( Z2(i−1) ),C Technical Lemmas and Their Proofs,[0],[0]
"∂(vec(δ̃ki )) ∂w(j) ...
",C Technical Lemmas and Their Proofs,[0],[0]
P1 ( δ̃ki )∂(vec(Zdi−1 (i−1) )) ∂w(j) + P2 ( Z di−1 (i−1) ),C Technical Lemmas and Their Proofs,[0],[0]
"∂(vec(δ̃ki )) ∂w(j)
  ∈ Rki2di−1×kj2djdj−1 ,
(4)
where P1 ( δ̃ki ) ∈ Rki2×r̃i−1c̃i−1di−1 and P2 ( Z di−1 (i−1) ) ∈ Rki2×(r̃i−ki+1)(c̃i−ki+1) satisfy: each row in P1 ( δ̃ki ) contains
the vectorization of (δ̃ki ) T at the right position and the remaining entries are 0s, and each row in P2 ( Z di−1 (i−1) ) is the submatrix in Zdi−1(i−1) that need to conduct inner product with δ̃ k i in turn.",C Technical Lemmas and Their Proofs,[0],[0]
"Note that there are si− 1 rows and columns between
each neighboring nonzero entries inN which is decided by the definition of δ̃i+1 in Sec. B.1.",C Technical Lemmas and Their Proofs,[0],[0]
"Accordingly, we have
∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
P1 ( δ̃ki )∂ ( vec ( Z di−1 (i−1) )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ (ki − si + 1)2‖δ̃ki ‖2F ∥∥∥∥∥∥ ∂",C Technical Lemmas and Their Proofs,[0],[0]
( vec ( Z di−1 (i−1) )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
= (ki − si + 1)2‖δki ‖2F ∥∥∥∥∥∥ ∂",C Technical Lemmas and Their Proofs,[0],[0]
( vec ( Z di−1 (i−1) )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
and
∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
P2 ( Z di−1 (i−1) )∂,C Technical Lemmas and Their Proofs,[0],[0]
( vec ( δ̃ki )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ (ki − si + 1)2‖Zdi−1(i−1)‖2F ∥∥∥∥∥∥ ∂",C Technical Lemmas and Their Proofs,[0],[0]
( vec ( δ̃ki )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
= (ki − si + 1)2‖Zdi−1(i−1)‖2F ∥∥∥∥∥ ∂ ( vec ( δki ))
∂w(j)
∥∥∥∥∥ 2
F
.
",C Technical Lemmas and Their Proofs,[0],[0]
"Then in order to bound
‖∇2wf(w,D)‖2F = l+1∑
i=1
l+1∑
j=1
∥∥∥∥ ∂2f(w,D)
∂w(j)∂w(i)
∥∥∥∥ 2
F
,
we try to bound each term separately.",C Technical Lemmas and Their Proofs,[0],[0]
So we consider the following five cases:,C Technical Lemmas and Their Proofs,[0],[0]
l ≥,C Technical Lemmas and Their Proofs,[0],[0]
"i ≥ j, i ≤ j ≤",C Technical Lemmas and Their Proofs,[0],[0]
"l, l + 1 = i > j, l + 1 = j >",C Technical Lemmas and Their Proofs,[0],[0]
i,C Technical Lemmas and Their Proofs,[0],[0]
"and l + 1 = i = j.
Case 1:",C Technical Lemmas and Their Proofs,[0],[0]
l ≥,C Technical Lemmas and Their Proofs,[0],[0]
i ≥,C Technical Lemmas and Their Proofs,[0],[0]
"j
In the following, we first consider the first case, i.e. i ≥ j, and bound ∥∥∥∥∥∥ ∂",C Technical Lemmas and Their Proofs,[0],[0]
( vec ( δ̃ki )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
= ∥∥∥∥∥ ∂ ( vec ( δki ))
∂w(j)
∥∥∥∥∥ 2
F
= ∥∥∥∥∥∥ ∂ ∂w(j)",C Technical Lemmas and Their Proofs,[0],[0]
"vec  up   di+1∑
s=1
δ̃si+1~̃Ŵ s,k(i+1)   σ′1(Xk(i))   ",C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
¬ ≤ 2
16 ∥∥∥∥∥∥ ∂ ∂w(j) vec  up   di+1∑
s=1
δ̃si+1~̃Ŵ s,k(i+1)     ",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥∥ 2
F
+ 2 ∥∥∥∥∥∥ up   di+1∑
s=1
δ̃si+1~̃Ŵ s,k(i+1)   ",C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
∥∥∥∥∥ ∂σ′1(X",C Technical Lemmas and Their Proofs,[0],[0]
"k (i))
∂w(j)
∥∥∥∥∥ 2
F
= 2
16p2 ∥∥∥∥∥∥ ∂ ∂w(j) vec   di+1∑
s=1
δ̃si+1~̃Ŵ s,k(i+1)   ",C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ 2
p2 ∥∥∥∥∥∥ di+1∑
s=1
δ̃si+1~̃Ŵ s,k(i+1) ∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
∥∥∥∥∥∥ ∂vec ( σ′(xk(i)) )",C Technical Lemmas and Their Proofs,[0],[0]
∂xk(i) ∂xk(i) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
 ≤ 2
16p2 ∥∥∥∥∥∥ ∂ ∂w(j) vec   di+1∑
s=1
δ̃si+1~̃Ŵ s,k(i+1)   ",C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ 2 · 26 38p2 ∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"di+1∑
s=1
δ̃si+1~̃Ŵ s,k(i+1)",C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
∥∥∥∥∥ ∂Xk(i)
∂w(j)
∥∥∥∥∥ 2
F
® ≤2di+1
16p2 (ki+1−si+1+1)2
di+1∑
s=1
‖Ŵ s,k(i+1)‖2F ∥∥∥∥∥ ∂δ̃si+1 ∂w(j) ∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ 2 · 26 38p2 di+1(ki+1−si+1+1)2 di+1∑
s=1
∥∥∥Ŵ s,k(i+1) ∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
∥∥∥δ̃si+1 ∥∥∥ 2
F ∥∥∥∥∥ ∂Xk(i)
∂w(j)
",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥ 2
F
¯ = 2di+1 16p2
(ki+1−si+1+1)2 di+1∑
s=1
‖W s,k(i+1)‖2F ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
∂δsi+1 ∂w(j),C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ 2 · 26 38p2 di+1(ki+1−si+1+1)2 di+1∑
s=1
∥∥∥W s,k(i+1) ∥∥∥ 2
F
∥∥δsi+1 ∥∥2 F ∥∥∥∥∥ ∂Xk(i)
∂w(j)
∥∥∥∥∥ 2
F
.
(5)
¬ holds sinceXk(i) is independent onw(j) and the values of entries in σ ′",C Technical Lemmas and Their Proofs,[0],[0]
1(X k,C Technical Lemmas and Their Proofs,[0],[0]
"(i)) is not larger than 1/4 since for any constant a, σ′(a) = σ(a)(1− σ(a)) ≤ 1/4.  ",C Technical Lemmas and Their Proofs,[0],[0]
"holds since for arbitrary tensorM , we have ‖up (M)‖2F ≤ ‖M‖2F /p2",C Technical Lemmas and Their Proofs,[0],[0]
"in Lemma 8, and we also have
∥∥∥∥∥∥ ∂vec ( σ′(xk(i)) )",C Technical Lemmas and Their Proofs,[0],[0]
∂xk(i) ∂xk(i) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
= ∥∥∥∥∥Q ( xk(i) )",C Technical Lemmas and Their Proofs,[0],[0]
"∂xk(i) ∂w(j) ∥∥∥∥∥ 2
F
≤ 2 6
38 ∥∥∥∥∥ ∂xk(i)
∂w(j)
∥∥∥∥∥ 2
F
= 26
38 ∥∥∥∥∥ ∂Xk(i)
∂w(j)
∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
.
® holds since we can just adopt similar strategy in Eqn.",C Technical Lemmas and Their Proofs,[0],[0]
"(4) to separate Ŵ s,k(i+1) and the conclusion in Lemma 8; ¯ holds since the difference between δ̃si+1 and δ s i+1 is that we pad 0 around δ s i+1 to obtain δ̃ s i+1, indicating ‖δsi+1‖2F = ‖δ̃si+1‖2F .
",C Technical Lemmas and Their Proofs,[0],[0]
"Accordingly, we can further bound ∥∥∥∥∥ ∂δ̃i ∂w(j) ∥∥∥∥∥ 2
F
=
di−1∑
k=1
∥∥∥∥∥ ∂ ( vec ( δki ))
∂w(j)
∥∥∥∥∥ 2
F
≤2di+1 16p2
(ki+1−si+1+1)2 di−1∑
k=1
di+1∑
s=1
‖W s,k(i+1)‖2F ∥∥∥∥ ∂δsi+1 ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ 2 · 26 38p2 di+1(ki+1−si+1+1)2 di−1∑
k=1
di+1∑
s=1
∥∥∥W s,k(i+1) ∥∥∥ 2
F
∥∥δsi+1 ∥∥2 F ∥∥∥∥∥ ∂Xk(i)
∂w(j)
∥∥∥∥∥ 2
F
≤2di+1 16p2
(ki+1−si+1+1)2 di+1∑
s=1
‖W s(i+1)‖2F ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
∂δsi+1 ∂w(j),C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ 2 · 26 38p2 di+1(ki+1−si+1+1)2 ∥∥δi+1",C Technical Lemmas and Their Proofs,[0],[0]
∥∥2,C Technical Lemmas and Their Proofs,[0],[0]
F max s ∥∥∥W s(i+1),C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥ 2 F max k ∥∥∥∥∥ ∂Xk(i)
",C Technical Lemmas and Their Proofs,[0],[0]
"∂w(j)
∥∥∥∥∥ 2
F
¬ ≤2di+1
16p2 (ki+1−si+1+1)2bi+12 ∥∥∥∥ ∂δi+1 ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ 2 · 26 38p2 di+1(ki+1−si+1+1)2bi+12 ∥∥δi+1 ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
"F max k ∥∥∥∥∥ ∂Xk(i)
∂w(j)
",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥ 2
F
 ≤2di+1
16p2 (ki+1−si+1+1)2bi+12 ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"∂δi+1 ∂w(j) ∥∥∥∥ 2
F
+ ϑbl+1 2dj−1 3p2bj 2dj ricirj−1cj−1
l∏
s=j
dsbs 2(ks − ss + 1)2
16p2 ,
where ¬ holds since we have ‖W s(i+1)‖F ≤ rw;  holds due to the bounds of ∥∥δi+1 ∥∥2 F and ∥∥∥∥ ∂Xk(i) ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
in Lemma 10
and 12.
",C Technical Lemmas and Their Proofs,[0],[0]
"Then, we can use the above recursion inequality to further obtain
∥∥∥∥ ∂δi ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ [ i+1∏
s=i+1
2ds 16p2
(ks−ss+1)2bs2 ] 2di+2
16p2 (ki+2−si+2+1)2bi+22 ∥∥∥∥ ∂δi+2 ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ ϑbl+1 2dj−1 3p2bj 2dj rj−1cj−1ri+1ci+1
",C Technical Lemmas and Their Proofs,[0],[0]
"l∏
s=j
dsbs 2(ks−ss+1)2
16p2
 
+ ϑbl+1 2dj−1 3p2bj 2dj ricirj−1cj−1
l∏
s=j
dsbs 2(ks − ss + 1)2
16p2
≤",C Technical Lemmas and Their Proofs,[0],[0]
"[ l∏
s=i+1
2ds 16p2 (ks−ss+1)2bs2 ]∥∥∥∥ ∂δl ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ ϑbl+1 2dj−1 3p2bj 2dj rj−1cj−1
l∏
s=j
dsbs 2(ks − ss + 1)2
16p2
[ rici + ri+1ci+1",C Technical Lemmas and Their Proofs,[0],[0]
"[ i+1∏
s=i+1
2ds 16p2
(ks−ss+1)2bs2 ]
+ri+2ci+2
[ i+2∏
s=i+1
2ds 16p2
(ks−ss+1)2bs2 ] + · · ·+ rlcl [ l∏
s=i+1
2ds 16p2
(ks−ss+1)2bs2 ]] .
",C Technical Lemmas and Their Proofs,[0],[0]
"By Lemma 13, we have
∥∥∥∥ ∂δl ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ ϑ̃bl+1",C Technical Lemmas and Their Proofs,[0],[0]
"4dj−1
p2bj 2dj
dlrlclrj−1cj−1
l∏
s=j
dsbs 2(ks − ss + 1)2
16p2 ,
where ϑ̃ = 364 .",C Technical Lemmas and Their Proofs,[0],[0]
"Thus, we can establish
∥∥∥∥ ∂δi ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤ ϑ̃bl+1 2dj−1
p2bj 2dj
rj−1cj−1
[ τ
3 + bl+1
2dlrlcl
l∏
s=i+1
dsbs 2(ks − ss + 1)2
16p2
]",C Technical Lemmas and Their Proofs,[0],[0]
"l∏
s=j
dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2 .
",C Technical Lemmas and Their Proofs,[0],[0]
where τ = rici + ri+1ci+1,C Technical Lemmas and Their Proofs,[0],[0]
[∏i+1 s=i+1 2ds 16p2 (ks−ss+1)2bs 2 ] + · · ·+ rlcl [∏l s=i+1 2ds 16p2 (ks−ss+1)2bs 2 ] .,C Technical Lemmas and Their Proofs,[0],[0]
"It further gives the bound of ∥∥∥ ∂
2f(w,x) ∂w(j)∂w(i)
∥∥∥ 2
F as follows:
∥∥∥∥ ∂2f(w,D)
∂w(j)∂w(i)
∥∥∥∥ 2
F
=
di−1∑
k=1
∥∥∥∥∥ ∂2f(w,D)
∂w(j)∂w",C Technical Lemmas and Their Proofs,[0],[0]
"k (i)
∥∥∥∥∥ 2
F
=
di−1∑
k=1
di∑
s=1
∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
P1 ( δki )∂,C Technical Lemmas and Their Proofs,[0],[0]
( vec ( Xs(i−1) )) ∂w(j) +,C Technical Lemmas and Their Proofs,[0],[0]
P2 ( Xs(i−1) )∂ ( vec ( δki )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
≤2 di−1∑
k=1
di∑
s=1
  ∥∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
P1 ( δki )∂,C Technical Lemmas and Their Proofs,[0],[0]
( vec ( Xs(i−1) )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ ∥∥∥∥∥P2 ( Xs(i−1) )∂",C Technical Lemmas and Their Proofs,[0],[0]
"( vec ( δki ))
∂w(j)
∥∥∥∥∥ 2
F
 
≤2(ki−si+1)2 di−1∑
k=1
di∑
s=1
  ∥∥δki ∥∥2",C Technical Lemmas and Their Proofs,[0],[0]
F ∥∥∥∥∥∥ ∂,C Technical Lemmas and Their Proofs,[0],[0]
( vec ( Xs(i−1) )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ ∥∥∥Xs(i−1) ∥∥∥ 2
F ∥∥∥∥∥ ∂ ( vec ( δki ))
∂w(j)
",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥ 2
F
 
≤2(ki−si+1)2  ‖δi‖2F ∥∥∥∥∥∥ ∂",C Technical Lemmas and Their Proofs,[0],[0]
( vec ( X(i−1) )) ∂w(j) ∥∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
+ ∥∥∥X(i−1) ∥∥∥ 2
F ∥∥∥∥ ∂ (vec (δi)) ∂w(j) ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"2
F
 
¬ ≤32ϑbl+1 2di−1 bi 2bj 2didj ri−1ci−1rj−1cj−1
l∏
s=i+1
dsbs 2(ks − ss + 1)2
16p2 + ri−1ci−1di−1 ∥∥∥∥ ∂",C Technical Lemmas and Their Proofs,[0],[0]
(vec (δi)) ∂w(j) ∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"2
F
 ≤O  ϑbl+1
2di−1dj−1 bi 2bj 2didj ri−1ci−1rj−1cj−1
  l∏
s=j
dsbs 2(ks − ss + 1)2
16p2
  [ l∏
s=i
2dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2
]  .
",C Technical Lemmas and Their Proofs,[0],[0]
"where ¬ holds because of Lemma 12, while  holds due to 13.
",C Technical Lemmas and Their Proofs,[0],[0]
Case 2: i ≤ j ≤,C Technical Lemmas and Their Proofs,[0],[0]
"l
Since ∂ 2f(w,D) ∂w∂wT is symmetrical, we have ∂ 2f(w,D)
",C Technical Lemmas and Their Proofs,[0],[0]
"∂wT (i) ∂w(j)
=
( ∂2f(w,D)
∂wT (j) ∂w(i)
)T (1 ≤ i, j ≤ l).",C Technical Lemmas and Their Proofs,[0],[0]
"Thus, it yields
∥∥∥∥∥ ∂2f(w,D)
∂wT(i)∂w(j)
∥∥∥∥∥ 2
F
= ∥∥∥∥∥ ∂2f(w,D)
∂wT(j)∂w(i)
∥∥∥∥∥ 2
F
.
",C Technical Lemmas and Their Proofs,[0],[0]
Case 3: l + 1,C Technical Lemmas and Their Proofs,[0],[0]
= i >,C Technical Lemmas and Their Proofs,[0],[0]
"j
In the following, we first consider the first case, i.e. cross entropy and softmax activation, and bound
∥∥∥∥ ∂2f(w,D)
∂w(j)∂w(l+1)
∥∥∥∥ 2
F
= ∥∥∥∥∥ ∂(v",C Technical Lemmas and Their Proofs,[0],[0]
"− y)zT(l)
∂w(j)
∥∥∥∥∥ 2
F
= ∥∥∥∥∥[Ir̃lc̃ldl ⊗ (v − y)]",C Technical Lemmas and Their Proofs,[0],[0]
"∂zT(l)
",C Technical Lemmas and Their Proofs,[0],[0]
"∂x(l)
∂x(l) ∂w(j)",C Technical Lemmas and Their Proofs,[0],[0]
"+ [ z(l) ⊗ Idl+1 ] ∂v ∂z(l) ∂z(l) ∂x(l) ∂x(l) ∂w(j) ∥∥∥∥∥ 2
F
= ∥∥∥∥ũp ( [Ir̃lc̃ldl ⊗ (v − y)] + [ z(l) ⊗ Idl+1 ] diag (σ′2(u))W(l+1) ) G(l) ∂x(l)
∂w(j)
∥∥∥∥ 2
F
,
whereG(l) is defined as
G(l)=",C Technical Lemmas and Their Proofs,[0],[0]
"[ σ′1 ( x(l) ) , σ′1 ( x(l) ) , · · · , σ′1 ( x(l) ) ︸",C Technical Lemmas and Their Proofs,[0],[0]
"︷︷ ︸
rlcl columns
] ∈",C Technical Lemmas and Their Proofs,[0],[0]
"Rrlcldl×rlcl .
",C Technical Lemmas and Their Proofs,[0],[0]
"Thus, we can further obtain ∥∥∥∥ ∂2f(w,D)
∂w(j)∂w(l+1)
∥∥∥∥ 2
F
≤ 1 16p2
∥∥[Ir̃lc̃ldl ⊗ (v − y)] +",C Technical Lemmas and Their Proofs,[0],[0]
[ z(l) ⊗ Idl+1 ] diag (σ′2(u))W(l+1) ∥∥2,C Technical Lemmas and Their Proofs,[0],[0]
"F ∥∥∥∥ ∂x(l)
∂w(j)
∥∥∥∥ 2
F
≤ 2 16p2
( ‖Ir̃lc̃ldl ⊗ (v − y)‖2F + ∥∥[z(l) ⊗ Idl+1 ] diag (σ′2(u))W(l+1) ∥∥2 F )∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"∂x(l)
∂w(j)
∥∥∥∥ 2
F
¬ ≤ 2
16p2
( ‖Ir̃lc̃ldl ⊗ (v − y)‖2F + ∥∥z(l) ⊗",C Technical Lemmas and Their Proofs,[0],[0]
[ diag (σ′2(u))W(l+1) ]∥∥2 F )∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"∂x(l)
∂w(j)
∥∥∥∥ 2
F
 ≤ 1
8p2 r̃lc̃ldl
( 2 + 1
16 bl+1
2 )",C Technical Lemmas and Their Proofs,[0],[0]
"dlrlclr̃j−1c̃j−1dj−1(kj − sj + 1)2 l∏
s=j+1
dsbs 2(ks − ss + 1)2
16p2
= 2dj−1 p4bj 2dj r2l",C Technical Lemmas and Their Proofs,[0],[0]
c 2,C Technical Lemmas and Their Proofs,[0],[0]
"l d 2 l rj−1cj−1
( 2 + 1
16 bl+1
2
)",C Technical Lemmas and Their Proofs,[0],[0]
"l∏
s=j
dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2
where ¬ holds since for an arbitrary vector u ∈",C Technical Lemmas and Their Proofs,[0],[0]
Rk and an arbitrary matrixM ∈,C Technical Lemmas and Their Proofs,[0],[0]
"Rk×k, we have (u⊗ Ik)M = u⊗M ;  holds since we use Lemma 12 and the assumption that ‖W(l+1)‖2F ≤ bl+12.",C Technical Lemmas and Their Proofs,[0],[0]
Now we consider the least square loss and softmax activation function.,C Technical Lemmas and Their Proofs,[0],[0]
"In such a case, we can further obtain:
∥∥∥∥ ∂2f(w,D)
∂w(j)∂w(l+1)
∥∥∥∥ 2
F
= ∥∥∥∥∥ ∂(v",C Technical Lemmas and Their Proofs,[0],[0]
"− y)G (u)zT(l)
∂w(j)
∥∥∥∥∥ 2
F
= ∥∥∥∥∥[Ir̃lc̃ldl ⊗ (v − y)] ∂zT(l)
∂x(l)
∂x(l) ∂w(j) + [ z(l) ⊗ (v − y) ] ∂vec (G (u))",C Technical Lemmas and Their Proofs,[0],[0]
"∂u ∂u ∂z(l) ∂z(l) ∂x(l) ∂x(l) ∂w(j) + [ z(l) ⊗ Idl+1 ] ∂v ∂z(l) ∂z(l) ∂x(l) ∂x(l) ∂w(j) ∥∥∥∥∥ 2
F
= ∥∥∥∥ũp",C Technical Lemmas and Their Proofs,[0],[0]
( [Ir̃lc̃ldl ⊗ (v − y)],C Technical Lemmas and Their Proofs,[0],[0]
+ [ z(l) ⊗ (v − y) ],C Technical Lemmas and Their Proofs,[0],[0]
Q (u)W(l+1) +,C Technical Lemmas and Their Proofs,[0],[0]
[ z(l) ⊗ Idl+1 ] Q (u)G (u)W(l+1) ),C Technical Lemmas and Their Proofs,[0],[0]
G(l),C Technical Lemmas and Their Proofs,[0],[0]
"∂x(l)
∂w(j)
∥∥∥∥ 2
F
.
",C Technical Lemmas and Their Proofs,[0],[0]
"Thus, we can further obtain
∥∥∥∥ ∂2f(w,D)
∂w(j)∂w(l+1)
∥∥∥∥ 2
F
≤ 1 16p2
∥∥[Ir̃lc̃ldl ⊗ (v − y)] + [ z(l) ⊗ (v − y) ]",C Technical Lemmas and Their Proofs,[0],[0]
Q (u)W(l+1) +,C Technical Lemmas and Their Proofs,[0],[0]
"[ z(l) ⊗ Idl+1 ] Q (u)G (u)W(l+1) ∥∥2 F ∥∥∥∥ ∂x(l)
∂w(j)
∥∥∥∥ 2
F
≤ 3 16p2
( ‖Ir̃lc̃ldl ⊗ (v − y)‖2F + ∥∥[z(l) ⊗ (v − y) ]",C Technical Lemmas and Their Proofs,[0],[0]
Q (u)W(l+1) ∥∥2 F + ∥∥[z(l),C Technical Lemmas and Their Proofs,[0],[0]
⊗ Idl+1 ],C Technical Lemmas and Their Proofs,[0],[0]
Q (u)G (u)W(l+1) ∥∥2 F )∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"∂x(l)
∂w(j)
∥∥∥∥ 2
F
¬ ≤ 3
16p2 r̃lc̃ldl
( 2 + 3
100 bl+1
2 )",C Technical Lemmas and Their Proofs,[0],[0]
"dlrlclr̃j−1c̃j−1dj−1(kj − sj + 1)2 l∏
s=j+1
dsbs 2(ks − ss + 1)2
16p2
= 3dj−1 p4bj 2dj rj−1cj−1d",C Technical Lemmas and Their Proofs,[0],[0]
2 l r 2,C Technical Lemmas and Their Proofs,[0],[0]
"l c 2 l
( 2 + 27
38 bl+1
2 + 26
16 · 38 bl+1 2
)",C Technical Lemmas and Their Proofs,[0],[0]
"l∏
s=j
dsbs 2(ks",C Technical Lemmas and Their Proofs,[0],[0]
"− ss + 1)2
16p2 ,
where ¬ holds since we use Lemma 12 and the fact that ‖W(l+1)‖2F ≤ bl+12.",C Technical Lemmas and Their Proofs,[0],[0]
"Case 4: i < j = l + 1
Similar to the Case 2, we also can have
∥∥∥∥∥ ∂2f(w,D)
∂wT(i)∂w(j)
∥∥∥∥∥ 2
F
= ∥∥∥∥∥ ∂2f(w,D)
∂wT(j)∂w(i)
∥∥∥∥∥ 2
F
.
",C Technical Lemmas and Their Proofs,[0],[0]
"So in this case, we can just directly use the bound in case 3 to bound ∥∥∥∥ ∂2f(w,D)
∂wT (i) ∂w(j)
∥∥∥∥ 2
F
.
",C Technical Lemmas and Their Proofs,[0],[0]
"Case 5: i = j = l + 1
In the following, we first consider the first case, i.e. i = l + 1, and bound
∥∥∥∥ ∂2f(w,D)
∂w(l+1)∂w(l+1)
",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥ 2
F
= ∥∥∥∥∥ ∂(v",C Technical Lemmas and Their Proofs,[0],[0]
− y)zT(l) ∂w(l+1),C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥ 2
F
= ∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"[ z(l) ⊗ Idl+1
]",C Technical Lemmas and Their Proofs,[0],[0]
"∂v ∂w(l+1)
∥∥∥∥ 2
F
= ∥∥∥ [ z(l) ⊗ Idl+1 ]",C Technical Lemmas and Their Proofs,[0],[0]
G (u) [ z(l) ⊗ Idl+1 ],C Technical Lemmas and Their Proofs,[0],[0]
"T∥∥∥ 2
F
¬ = ∥∥∥∥ [ z(l) ( z(l) ⊗G (u) )T ]T ∥∥∥∥ 2
F
≤ ∥∥z(l)‖4F",C Technical Lemmas and Their Proofs,[0],[0]
‖G (u) ∥∥2 F ≤ 1 16,C Technical Lemmas and Their Proofs,[0],[0]
r̃2l c̃ 2,C Technical Lemmas and Their Proofs,[0],[0]
"l d 2 l dl+1,
where ¬ holds since for an arbitrary vector u ∈",C Technical Lemmas and Their Proofs,[0],[0]
Rk and an arbitrary matrixM ∈,C Technical Lemmas and Their Proofs,[0],[0]
"Rk×k, we have (u⊗ Ik)M = u⊗M .",C Technical Lemmas and Their Proofs,[0],[0]
"Now we can bound ∥∥∥∂
2f(w,D) ∂w∂w
∥∥∥ 2
F as follows:
∥∥∥∥ ∂2f(w,D)
∂w∂w
∥∥∥∥ 2
F
= ∥∥∥∥ ∂2f(w,D)
∂w(l+1)∂w(l+1)
∥∥∥∥ 2
F
+ 2 l∑
j=1
∥∥∥∥ ∂2f(w,D)
∂w(j)∂w(l+1)
∥∥∥∥ 2
F
+ 2 l∑
j=1
l∑
i=j
∥∥∥∥ ∂2f(w,D)
∂w(j)∂w(i)
∥∥∥∥ 2
F
≤O ( l2
k1 4 max1≤i,j≤l
r̃ic̃irjcj bl+1
4
16p2
( rw 2
8 √ 2p2
)2l−2 l∏
s=1
(dsks 2)2
)
",C Technical Lemmas and Their Proofs,[0],[0]
"≤O  ϑbl+1
2d20 b1 4d21 l2r20c 2 0
[ l∏
s=1
dsbs 2(ks − ss + 1)2
8 √ 2p2
]2  .
",C Technical Lemmas and Their Proofs,[0],[0]
"On the other hand, if the activation functions σ1 and σ2 are respectively sigmoid function and softmax function, f(w,D) is infinitely differentiable.",C Technical Lemmas and Their Proofs,[0],[0]
"Also σ(a), σ′(a), σ′′(a) and σ′′′(a) are all bounded.",C Technical Lemmas and Their Proofs,[0],[0]
"This means that ∇3wf(w,D) exists.",C Technical Lemmas and Their Proofs,[0],[0]
"Also since inputD and the parameter w are bounded, we can always find a universal constant ν such that
‖∇3wf(w,D)‖op = sup ‖λ‖2≤1
〈 λ⊗ 3 ,∇3wf(w,D) 〉 = ∑
i,j,k
[∇3wf(w,D)]ijkλiλjλk ≤ ν",C Technical Lemmas and Their Proofs,[0],[0]
"< +∞.
The proof is completed.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 15.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
Suppose Assumption 1 on the input dataD holds.,C Technical Lemmas and Their Proofs,[0],[0]
"Then for any t > 0, the objective f(w,x) obeys
P
( 1
n
n∑
i=1
( f(w,D(i))−E(f(w,D(i))) )",C Technical Lemmas and Their Proofs,[0],[0]
">t ) ≤ 2 exp ( −2nt 2
α2
) ,
where α = 1.
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
"Since the input D(i) (i = 1, · · · , n) are independent from each other, then the output f(w,D(i))",C Technical Lemmas and Their Proofs,[0],[0]
"(i = 1, · · · , n) are also independent.",C Technical Lemmas and Their Proofs,[0],[0]
"Meanwhile, when the loss is the square loss, we can easily bound 0 ≤",C Technical Lemmas and Their Proofs,[0],[0]
"f(w,D(i))",C Technical Lemmas and Their Proofs,[0],[0]
= 12‖v,C Technical Lemmas and Their Proofs,[0],[0]
"− y‖22 ≤ 1, since the value of entries in v belongs to [0, 1] and y is a one-hot vector label of v.
Besides, for arbitrary random variable x, |x− Ex| ≤ |x|.",C Technical Lemmas and Their Proofs,[0],[0]
"So by Hoeffding’s inequality in Lemma 6, we have
P
( 1
n
n∑
i=1
( f(w,D(i))−E(f(w,D(i))) )",C Technical Lemmas and Their Proofs,[0],[0]
">t ) ≤ exp ( −2nt 2
α2
) ,
where α = 1.",C Technical Lemmas and Their Proofs,[0],[0]
This means that 1n,C Technical Lemmas and Their Proofs,[0],[0]
∑n i=1,C Technical Lemmas and Their Proofs,[0],[0]
"( f(w,D(i))−E(f(w,D(i))) ) has exponential tails.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 16.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
Suppose Assumption 1 on the input dataD holds.,C Technical Lemmas and Their Proofs,[0],[0]
Then for any t > 0 and arbitrary unit vector λ ∈,C Technical Lemmas and Their Proofs,[0],[0]
"Sd−1, the gradient ∇f(w,x) obeys
P
( 1
n
n∑
i=1
(〈 λ,∇wf(w,D(i))−ED∼D∇wf(w,D(i)) 〉) >t ) ≤ exp ( − nt 2
2β2
) .
",C Technical Lemmas and Their Proofs,[0],[0]
"where β , [ ϑr̃lc̃ldl + ∑l i=1 ϑbl+1 2di−1",C Technical Lemmas and Their Proofs,[0],[0]
p2bi2di ri−1ci−1,C Technical Lemmas and Their Proofs,[0],[0]
"∏l s=i dsbs 2(ks−ss+1)2 16p2 ]1/2 in which ϑ = 1/8.
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
"Since the inputD(i) (i = 1, · · · , n) are independent from each other, then the output∇wf(w,D(i))",C Technical Lemmas and Their Proofs,[0],[0]
"(i = 1, · · · , n) are also independent.",C Technical Lemmas and Their Proofs,[0],[0]
"Furthermore, for arbitrary vector x, ‖x−Ex‖22 ≤ ‖x‖22.",C Technical Lemmas and Their Proofs,[0],[0]
"Hence, for an arbitrary unit vector λ ∈ Sd−1 where d = r̃lc̃ldldl+1 + ∑l i=1 ki 2di−1di, we have
〈λ,∇wf(w,D(i))− ED∼D∇wf(w,D(i))〉 ≤‖λ‖2‖∇wf(w,D(i))− ED∼D∇wf(w,D(i))‖2
≤‖λ‖2‖∇wf(w,D(i))‖2 ¬ ≤ β,
where ¬ holds since ‖λ‖2 = 1",C Technical Lemmas and Their Proofs,[0],[0]
"(λ ∈ Sd−1) and by Lemma 11, we have ‖∇wf(w,D(i))‖ ≤ β where β ,[ ϑr̃lc̃ldl + ∑l i=1 ϑbl+1 2di−1",C Technical Lemmas and Their Proofs,[0],[0]
p2bi2di ri−1ci−1,C Technical Lemmas and Their Proofs,[0],[0]
"∏l s=i dsbs 2(ks−ss+1)2 16p2 ]1/2 in which ϑ = 1/8.
",C Technical Lemmas and Their Proofs,[0],[0]
"Thus, we can use Hoeffding’s inequality in Lemma 6 to bound
P
( 1
n
n∑
i=1
(〈 λ,∇wf(w,D(i))−ED∼D∇wf(w,D(i)) 〉) >t ) ≤ exp",C Technical Lemmas and Their Proofs,[0],[0]
"( − nt 2
2β2
) .
",C Technical Lemmas and Their Proofs,[0],[0]
"The proof is completed.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 17.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
Suppose that Assumption 1 on the input data D and the parameter w holds.,C Technical Lemmas and Their Proofs,[0],[0]
Then for any t > 0 and arbitrary unit vector λ ∈,C Technical Lemmas and Their Proofs,[0],[0]
"Sd−1, the Hessian∇2f(w,D) obeys
P
( 1
n
n∑
i=1
(〈 λ, (∇2wf(w,D(i))− ED∼D∇2wf(w,D(i)))λ 〉) > t )",C Technical Lemmas and Their Proofs,[0],[0]
"≤ 2 exp ( −nt 2
2γ2
) .
",C Technical Lemmas and Their Proofs,[0],[0]
"where γ = ( ϑbl+1
2d20 b14d21",C Technical Lemmas and Their Proofs,[0],[0]
l2r20c 2 0,C Technical Lemmas and Their Proofs,[0],[0]
"[∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]2)1/2 .
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
"Since the inputD(i) (i = 1, · · · , n) are independent from each other, then the output∇wf(w,D(i))",C Technical Lemmas and Their Proofs,[0],[0]
"(i = 1, · · · , n) are also independent.",C Technical Lemmas and Their Proofs,[0],[0]
"On the other hand, for arbitrary random matrix X , ‖X",C Technical Lemmas and Their Proofs,[0],[0]
− EX‖2F ≤ ‖X‖2F .,C Technical Lemmas and Their Proofs,[0],[0]
"Thus, for an arbitrary unit vector λ ∈ Sd−1 where d = r̃lc̃ldldl+1 + ∑l i=1 ki 2di−1di, we have
〈λ, (∇2wf(w,D(i))− ED∼D∇2wf(w,D(i)))λ〉 ≤‖λ‖2‖(∇2wf(w,D(i))− ED∼D∇2wf(w,D(i)))λ‖2 ≤‖∇2wf(w,D(i))− ED∼D∇2wf(w,D(i))‖op‖λ‖2 ≤‖∇2wf(w,D(i))− ED∼D∇2wf(w,D(i))‖F ≤‖∇2wf(w,D(i))‖F ¬ ≤γ,
where ¬ holds since ‖λ‖2 = 1",C Technical Lemmas and Their Proofs,[0],[0]
"(λ ∈ Sd−1) and by Lemma 14, we have ‖∇2wf(w,D(i))‖ ≤ γ where γ =( ϑbl+1
2d20 b14d21",C Technical Lemmas and Their Proofs,[0],[0]
l2r20c 2 0,C Technical Lemmas and Their Proofs,[0],[0]
"[∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]2)1/2 .
",C Technical Lemmas and Their Proofs,[0],[0]
"Thus, we can use Hoeffding’s inequality in Lemma 6 to bound
P
( 1
n
n∑
i=1
(〈 λ, (∇2wf(w,D(i))− ED∼D∇2wf(w,D(i)))λ 〉) > t ) ≤ exp",C Technical Lemmas and Their Proofs,[0],[0]
"( −nt 2
2γ2
) .
",C Technical Lemmas and Their Proofs,[0],[0]
"The proof is completed.
",C Technical Lemmas and Their Proofs,[0],[0]
Lemma 18.,C Technical Lemmas and Their Proofs,[0],[0]
"Suppose that the activation function σ1 is sigmoid and σ2 is softmax, and the loss function f(w,D) is squared loss.",C Technical Lemmas and Their Proofs,[0],[0]
Suppose that Assumption 1 on the input dataD and the parameter w holds.,C Technical Lemmas and Their Proofs,[0],[0]
Then the empirical Hessian converges uniformly to the population Hessian in operator norm.,C Technical Lemmas and Their Proofs,[0],[0]
"Specifically, there exit two universal constants cv′ and cv such that if n ≥ cv′ ν",C Technical Lemmas and Their Proofs,[0],[0]
"2
d%ε2 [∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]−1 , then with probability at least 1−
sup w∈Ω
∥∥∥∇2Q̃n(w)−∇2Q(w) ∥∥∥
op ≤ cvγ
√ 2d+ θ%+ log ( 4 ε )
",C Technical Lemmas and Their Proofs,[0],[0]
"2n ,
holds with probability at least 1 − ε, where d = r̃lc̃ldldl+1 + ∑l i=1 ki
2di−1di, θ = al+1(dl+1 + r̃lc̃ldl − 2al+1 + 1) + ∑l i=1",C Technical Lemmas and Their Proofs,[0],[0]
ai(ki,C Technical Lemmas and Their Proofs,[0],[0]
2di +,C Technical Lemmas and Their Proofs,[0],[0]
di−1,C Technical Lemmas and Their Proofs,[0],[0]
"− 2ai + 1), % = ∑l i=1log (√ dibi(ki−si+1) 4p ) + log(bl+1)",C Technical Lemmas and Their Proofs,[0],[0]
"+ log ( n 128p2 ) , and
γ =
( ϑbl+1
2d20 b14d21",C Technical Lemmas and Their Proofs,[0],[0]
l2r20c 2 0,C Technical Lemmas and Their Proofs,[0],[0]
"[∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]2)1/2 .
",C Technical Lemmas and Their Proofs,[0],[0]
Proof.,C Technical Lemmas and Their Proofs,[0],[0]
"Recall that the weight of each kernel and the feature maps has magnitude bound separately, i.e. wk(i) ∈",C Technical Lemmas and Their Proofs,[0],[0]
"Bki
2di−1(rw)",C Technical Lemmas and Their Proofs,[0],[0]
"(i = 1, · · · , l; k = 1, · · · , di) and w(l+1) ∈ Br̃lc̃ldldl+1(bl+1).",C Technical Lemmas and Their Proofs,[0],[0]
"Since W̃(i) = [vec(W 1(i)), vec(W 2(i)), · · · , vec(W di−1(i) )",C Technical Lemmas and Their Proofs,[0],[0]
],C Technical Lemmas and Their Proofs,[0],[0]
∈,C Technical Lemmas and Their Proofs,[0],[0]
Rk 2,C Technical Lemmas and Their Proofs,[0],[0]
"i di×di−1 , we have ‖W̃(i)‖F ≤ dibi.
",C Technical Lemmas and Their Proofs,[0],[0]
"So here we assume W̃(i, ) is the dibi /(bl+1",C Technical Lemmas and Their Proofs,[0],[0]
+,C Technical Lemmas and Their Proofs,[0],[0]
∑l i=1 dibi)-covering net of the matrix W̃(i),C Technical Lemmas and Their Proofs,[0],[0]
which is the set of all parameters in the i-th layer.,C Technical Lemmas and Their Proofs,[0],[0]
"Then by Lemma 7, we have the covering number
n",C Technical Lemmas and Their Proofs,[0],[0]
"i ≤
( 9(bl+1 + ∑l i=1 dibi) )",C Technical Lemmas and Their Proofs,[0],[0]
"ai(ki2di+di−1−2ai+1) ,
since the rank of W̃(i) obeys rank(W̃(i))",C Technical Lemmas and Their Proofs,[0],[0]
≤ ai for 1 ≤,C Technical Lemmas and Their Proofs,[0],[0]
i ≤,C Technical Lemmas and Their Proofs,[0],[0]
l.,C Technical Lemmas and Their Proofs,[0],[0]
"For the last layer, we also can construct an bl+1 /(bl+1",C Technical Lemmas and Their Proofs,[0],[0]
+∑l i=1 dibi)-covering net for the weight matrixW(l+1).,C Technical Lemmas and Their Proofs,[0],[0]
"Here we have
n l+1 ≤
( 9(bl+1 + ∑l i=1 dibi) )",C Technical Lemmas and Their Proofs,[0],[0]
al+1(dl+1+r̃lc̃ldl−2al+1,C Technical Lemmas and Their Proofs,[0],[0]
"+1) ,
since the rank of W(l+1) obeys rank(W(l+1))",C Technical Lemmas and Their Proofs,[0],[0]
≤ al+1.,C Technical Lemmas and Their Proofs,[0],[0]
"Finally, we arrange them together to construct a set Θ and claim that there is always an -covering net w in Θ for any parameter w. Accordingly, we have
|Θ| ≤ l+1∏
i=1
n i=
( 9(bl+1 + ∑l i=1 dibi) )",C Technical Lemmas and Their Proofs,[0],[0]
al+1(dl+1+r̃lc̃ldl−2al+1,C Technical Lemmas and Their Proofs,[0],[0]
+1)+∑li=1 ai(ki2di+di−1−2ai+1) =,C Technical Lemmas and Their Proofs,[0],[0]
( 9(bl+1 + ∑l i=1 dibi) ),C Technical Lemmas and Their Proofs,[0],[0]
"θ ,
where θ = al+1(dl+1 + r̃lc̃ldl",C Technical Lemmas and Their Proofs,[0],[0]
− 2al+1 + 1) +,C Technical Lemmas and Their Proofs,[0],[0]
∑l i=1,C Technical Lemmas and Their Proofs,[0],[0]
"ai(ki
2di +",C Technical Lemmas and Their Proofs,[0],[0]
di−1,C Technical Lemmas and Their Proofs,[0],[0]
− 2ai + 1) which is the total freedom degree of the network.,C Technical Lemmas and Their Proofs,[0],[0]
So we can always find a vector wkw ∈ Θ such that ‖w −wkw‖2 ≤ .,C Technical Lemmas and Their Proofs,[0],[0]
"Now we use the decomposition strategy to bound our goal:
∥∥∥∇2Q̃n(w)−∇2Q(w) ∥∥∥
op
= ∥∥∥∥∥ 1 n n∑
i=1
∇2f(w,D(i))− ED∼D(∇2f(w,D))",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥
",C Technical Lemmas and Their Proofs,[0],[0]
"op
= ∥∥∥∥∥ 1 n",C Technical Lemmas and Their Proofs,[0],[0]
"n∑
i=1
( ∇2f(w,D(i))−∇2f(wkw ,D(i)) )",C Technical Lemmas and Their Proofs,[0],[0]
"+ 1
n
n∑
i=1
∇2f(wkw ,D(i))− E(∇2f(wkw ,D))
+",C Technical Lemmas and Their Proofs,[0],[0]
"ED∼D(∇2f(wkw ,D))− ED∼D(∇2f(w,D))",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥
",C Technical Lemmas and Their Proofs,[0],[0]
"op
≤ ∥∥∥∥∥ 1 n",C Technical Lemmas and Their Proofs,[0],[0]
"n∑
i=1
( ∇2f(w,D(i))−∇2f(wkw ,D(i)) )∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"op + ∥∥∥∥∥ 1 n n∑ i=1 ∇2f(wkw ,D(i))− ED∼D(∇2f(wkw ,D)) ∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"op
+ ∥∥∥∥∥ED∼D(∇",C Technical Lemmas and Their Proofs,[0],[0]
"2f(wkw ,D))− ED∼D(∇2f(w,D)) ∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"op .
",C Technical Lemmas and Their Proofs,[0],[0]
"Here we also define four events E0, E1, E2 and E3 as
E0 = { sup w∈Ω ∥∥∥∇2Q̃n(w)−∇2Q(w) ∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"op ≥ t } ,
E1 =    supw∈Ω ∥∥∥∥∥ 1 n",C Technical Lemmas and Their Proofs,[0],[0]
"n∑
i=1
( ∇2f(w,D(i))−∇2f(wkw ,D(i)) )",C Technical Lemmas and Their Proofs,[0],[0]
∥∥∥∥∥,C Technical Lemmas and Their Proofs,[0],[0]
"op ≥ t 3    ,
E2 =    supwkw∈Θ ∥∥∥∥∥ 1 n n∑
i=1
∇2f(wkw ,D(i))− ED∼D(∇2f(wkw ,D))",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥
op ≥ t 3
   ,
E3 = { sup w∈Ω ∥∥ED∼D(∇2f(wkw ,D))− ED∼D(∇2f(w,D)) ∥∥",C Technical Lemmas and Their Proofs,[0],[0]
"op ≥ t 3 } .
",C Technical Lemmas and Their Proofs,[0],[0]
"Accordingly, we have P (E0) ≤",C Technical Lemmas and Their Proofs,[0],[0]
"P (E1) + P (E2) + P (E3) .
",C Technical Lemmas and Their Proofs,[0],[0]
"So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).
",C Technical Lemmas and Their Proofs,[0],[0]
Step 1.,C Technical Lemmas and Their Proofs,[0],[0]
Bound P (E1):,C Technical Lemmas and Their Proofs,[0],[0]
"We first bound P (E1) as follows:
P (E1) =P ( sup w∈Ω ∥∥∥∥∥",C Technical Lemmas and Their Proofs,[0],[0]
1 n,C Technical Lemmas and Their Proofs,[0],[0]
"n∑
i=1
( ∇2f(w,D(i))−∇2f(wkw ,D(i)) )",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥ 2 ≥ t 3 )
¬ ≤3 t ED∼D ( sup w∈Ω ∥∥∥∥∥ 1 n",C Technical Lemmas and Their Proofs,[0],[0]
"n∑
i=1
( ∇2f(w,D(i))−∇2f(wkw ,D(i)) )",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥ 2 )
",C Technical Lemmas and Their Proofs,[0],[0]
"≤3 t ED∼D ( sup w∈Ω ∥∥∇2f(w,D)−∇2f(wkw ,D) ∥∥ 2 )
",C Technical Lemmas and Their Proofs,[0],[0]
≤3 t ED∼D ( sup w∈Ω ∥∥ 1 n ∑n i=1,C Technical Lemmas and Their Proofs,[0],[0]
"( ∇2f(w,D(i))−∇2f(wkw ,D(i)) )",C Technical Lemmas and Their Proofs,[0],[0]
∥∥ 2 ‖w −wkw‖2,C Technical Lemmas and Their Proofs,[0],[0]
"sup w∈Ω ‖w −wkw‖2 )
 ",C Technical Lemmas and Their Proofs,[0],[0]
"≤3ν
t ,
where ¬ holds since by Markov inequality and  holds because of Lemma 14.",C Technical Lemmas and Their Proofs,[0],[0]
"Therefore, we can set
t ≥ 6ν ε .
",C Technical Lemmas and Their Proofs,[0],[0]
"Then we can bound P(E1): P(E1) ≤ ε
2 .
",C Technical Lemmas and Their Proofs,[0],[0]
Step 2.,C Technical Lemmas and Their Proofs,[0],[0]
"Bound P (E2): By Lemma 3, we know that for any matrixX ∈ Rd×d, its operator norm can be computed as
‖X‖op ≤ 1
1− 2 supλ∈λ |〈λ,Xλ〉| .
where λ = {λ1, . . .",C Technical Lemmas and Their Proofs,[0],[0]
",λkw} be an -covering net of Bd(1).
",C Technical Lemmas and Their Proofs,[0],[0]
"Let λ1/4 be the 14 -covering net of B d(1), where d = r̃lc̃ldldl+1 + ∑l i=1 ki 2di−1di.",C Technical Lemmas and Their Proofs,[0],[0]
Recall that we use Θ to denote the -net of wkw and we have |Θ| ≤ ∏l+1 i=1,C Technical Lemmas and Their Proofs,[0],[0]
n,C Technical Lemmas and Their Proofs,[0],[0]
i =,C Technical Lemmas and Their Proofs,[0],[0]
( 3(bl+1+ ∑l i=1 dibi) ),C Technical Lemmas and Their Proofs,[0],[0]
θ .,C Technical Lemmas and Their Proofs,[0],[0]
"Then we can bound P (E2) as follows:
P (E2) =P ( sup
wkw∈Θ
∥∥∥∥∥ 1 n n∑
i=1
∇2f(wkw ,D(i))− ED∼D(∇2f(wkw ,D))",C Technical Lemmas and Their Proofs,[0],[0]
"∥∥∥∥∥
2
≥ t 3
)
",C Technical Lemmas and Their Proofs,[0],[0]
"≤P (
sup wkw∈Θ,λ∈λ1/4",C Technical Lemmas and Their Proofs,[0],[0]
"2
∣∣∣∣∣ 〈 λ, ( 1 n n∑
i=1
∇2f(wkw ,D(i))− ED∼D ( ∇2f(wkw ,D) ) )",C Technical Lemmas and Their Proofs,[0],[0]
"λ 〉∣∣∣∣∣ ≥ t 3 )
≤12d",C Technical Lemmas and Their Proofs,[0],[0]
( 3(bl+1 + ∑l i=1 dibi) ),C Technical Lemmas and Their Proofs,[0],[0]
"θ sup wkw∈Θ,λ∈λ1/4",C Technical Lemmas and Their Proofs,[0],[0]
"P (∣∣∣∣ 1 n n∑
i=1
〈 λ, ( ∇2f(wkw ,D(i))− ED∼D ( ∇2f(wkw ,D) ))",C Technical Lemmas and Their Proofs,[0],[0]
"λ 〉∣∣∣∣≥ t
6
)
¬ ≤12d
( 3(bl+1 + ∑l i=1 dibi) )",C Technical Lemmas and Their Proofs,[0],[0]
"θ 2 exp ( − nt 2
72γ2
) ,
where ¬ holds since by Lemma 17, we have
P
( 1
n
n∑
i=1
(〈 λ, (∇2wf(w,D(i))− ED∼D∇2wf(w,D(i)))λ 〉) > t ) ≤ exp ( −nt 2
2γ2
) .
where γ = ( ϑbl+1
2d20 b14d21",C Technical Lemmas and Their Proofs,[0],[0]
l2r20c 2 0,C Technical Lemmas and Their Proofs,[0],[0]
"[∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]2)1/2 .
",C Technical Lemmas and Their Proofs,[0],[0]
"Thus, if we set
t ≥
√√√√72γ2 ( d log(12) + θ log ( 3(bl+1+ ∑l i=1 dibi) )",C Technical Lemmas and Their Proofs,[0],[0]
"+ log ( 4 ε ))
n ,
then we have P (E2) ≤",C Technical Lemmas and Their Proofs,[0],[0]
"ε
2 .
",C Technical Lemmas and Their Proofs,[0],[0]
Step 3.,C Technical Lemmas and Their Proofs,[0],[0]
Bound P (E3):,C Technical Lemmas and Their Proofs,[0],[0]
"We first bound P (E3) as follows:
P (E3) =P (
sup w∈Ω
∥∥ED∼D(∇2f(wkw ,D))− ED∼D(∇2f(w,D)) ∥∥ 2 ≥ t
3
)
≤P ( ED∼D sup
w∈Ω
∥∥(∇2f(wkw ,D)−∇2f(w,D) ∥∥ 2 ≥ t
3
)
≤P (
sup w∈Ω
∣∣ 1 n ∑n i=1",C Technical Lemmas and Their Proofs,[0],[0]
"( ∇2f(w,D(i))−∇2f(wkw ,D(i)) )∣∣",C Technical Lemmas and Their Proofs,[0],[0]
‖w −wkw‖2,C Technical Lemmas and Their Proofs,[0],[0]
sup w∈Ω ‖w −wkw‖2,C Technical Lemmas and Their Proofs,[0],[0]
≥,C Technical Lemmas and Their Proofs,[0],[0]
"t 3 )
¬ ≤P ( ν ≥ t
3
) ,
where ¬ holds because of Lemma 14.",C Technical Lemmas and Their Proofs,[0],[0]
We set enough small such that ν < t/3 always holds.,C Technical Lemmas and Their Proofs,[0],[0]
"Then it yields P (E3) = 0.
",C Technical Lemmas and Their Proofs,[0],[0]
Step 4.,C Technical Lemmas and Their Proofs,[0],[0]
"Final result: To ensure P(E0) ≤ ε, we just set = 36(bl+1+ ∑l i=1 dibi)
",C Technical Lemmas and Their Proofs,[0],[0]
"ϑ2nbl+1
[∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]",C Technical Lemmas and Their Proofs,[0],[0]
− 12 .,C Technical Lemmas and Their Proofs,[0],[0]
"Note that
6 ν ε > 3 ν.",C Technical Lemmas and Their Proofs,[0],[0]
"Thus we can obtain
t ≥ max   6ν
ε ,
√√√√72γ2 ( d log(12) + θ log ( 3(bl+1+ ∑l i=1 dibi) )",C Technical Lemmas and Their Proofs,[0],[0]
"+ log ( 4 ε ))
n
  .
",C Technical Lemmas and Their Proofs,[0],[0]
"Thus, if n ≥ cv′ ν 2
d%ε2 [∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]−1 where cv′ is a constant, there exists a universal constant cv such that
sup w∈Ω
∥∥∥∇2Q̃n(w)−∇2Q(w) ∥∥∥
op ≤ĉvγ
√√√√d log(12)",C Technical Lemmas and Their Proofs,[0],[0]
+ θ,C Technical Lemmas and Their Proofs,[0],[0]
(∑l i=1 log (√ dsbs(ks−ss+1) 4p ) + log(bl+1),C Technical Lemmas and Their Proofs,[0],[0]
+ log ( n 128p2 )),C Technical Lemmas and Their Proofs,[0],[0]
"+ log ( 4 ε )
n
=cvγ
√ 2d+ θ%+ log",C Technical Lemmas and Their Proofs,[0],[0]
(,C Technical Lemmas and Their Proofs,[0],[0]
"4 ε )
2n
holds with probability at least 1 − ε, where d = r̃lc̃ldldl+1 + ∑l i=1 ki
2di−1di, θ = al+1(dl+1 + r̃lc̃ldl − 2al+1 + 1) + ∑l i=1",C Technical Lemmas and Their Proofs,[0],[0]
ai(ki,C Technical Lemmas and Their Proofs,[0],[0]
2di +,C Technical Lemmas and Their Proofs,[0],[0]
di−1,C Technical Lemmas and Their Proofs,[0],[0]
"− 2ai + 1), % = ∑l i=1log (√ dibi(ki−si+1) 4p ) + log(bl+1)",C Technical Lemmas and Their Proofs,[0],[0]
"+ log ( n 128p2 ) , and
γ =
( ϑbl+1
2d20 b14d21",C Technical Lemmas and Their Proofs,[0],[0]
l2r20c 2 0,C Technical Lemmas and Their Proofs,[0],[0]
[∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]2)1/2 .,C Technical Lemmas and Their Proofs,[0],[0]
The proof is completed.,C Technical Lemmas and Their Proofs,[0],[0]
Proof.,D.1 Proof of Lemma 1,[0],[0]
"Recall that the weight of each kernel and the feature maps has magnitude bound separately, i.e. wk(i) ∈",D.1 Proof of Lemma 1,[0],[0]
"Bki
2di−1(rw)",D.1 Proof of Lemma 1,[0],[0]
"(i = 1, · · · , l; k = 1, · · · , di) and w(l+1) ∈ Br̃lc̃ldldl+1(bl+1).",D.1 Proof of Lemma 1,[0],[0]
"Since W̃(i) = [vec(W 1(i)), vec(W 2(i)), · · · , vec(W di−1(i) )",D.1 Proof of Lemma 1,[0],[0]
],D.1 Proof of Lemma 1,[0],[0]
∈,D.1 Proof of Lemma 1,[0],[0]
Rk 2,D.1 Proof of Lemma 1,[0],[0]
"i di×di−1 , we have ‖W̃(i)‖F ≤ dibi.
",D.1 Proof of Lemma 1,[0],[0]
"So here we assume W̃(i, ) is the dibi /(bl+1",D.1 Proof of Lemma 1,[0],[0]
+,D.1 Proof of Lemma 1,[0],[0]
∑l i=1 dibi)-covering net of the matrix W̃(i),D.1 Proof of Lemma 1,[0],[0]
which is the set of all parameters in the i-th layer.,D.1 Proof of Lemma 1,[0],[0]
"Then by Lemma 7, we have the covering number
n",D.1 Proof of Lemma 1,[0],[0]
"i ≤
( 9(bl+1 + ∑l i=1 dibi) )",D.1 Proof of Lemma 1,[0],[0]
"ai(ki2di+di−1−2ai+1) ,
since the rank of W̃(i) obeys rank(W̃(i))",D.1 Proof of Lemma 1,[0],[0]
≤ ai for 1 ≤,D.1 Proof of Lemma 1,[0],[0]
i ≤,D.1 Proof of Lemma 1,[0],[0]
l.,D.1 Proof of Lemma 1,[0],[0]
"For the last layer, we also can construct an bl+1 /(bl+1",D.1 Proof of Lemma 1,[0],[0]
+∑l i=1 dibi)-covering net for the weight matrixW(l+1).,D.1 Proof of Lemma 1,[0],[0]
"Here we have
n l+1 ≤
( 9(bl+1 + ∑l i=1 dibi) )",D.1 Proof of Lemma 1,[0],[0]
al+1(dl+1+r̃lc̃ldl−2al+1,D.1 Proof of Lemma 1,[0],[0]
"+1) ,
since the rank of W(l+1) obeys rank(W(l+1))",D.1 Proof of Lemma 1,[0],[0]
≤ al+1.,D.1 Proof of Lemma 1,[0],[0]
"Finally, we arrange them together to construct a set Θ and claim that there is always an -covering net w in Θ for any parameter w. Accordingly, we have
|Θ| ≤ l+1∏
i=1
n i=
( 9(bl+1 + ∑l i=1 dibi) )",D.1 Proof of Lemma 1,[0],[0]
al+1(dl+1+r̃lc̃ldl−2al+1,D.1 Proof of Lemma 1,[0],[0]
+1)+∑li=1 ai(ki2di+di−1−2ai+1) =,D.1 Proof of Lemma 1,[0],[0]
( 9(bl+1 + ∑l i=1 dibi) ),D.1 Proof of Lemma 1,[0],[0]
"θ ,
where θ = al+1(dl+1 + r̃lc̃ldl",D.1 Proof of Lemma 1,[0],[0]
− 2al+1 + 1) +,D.1 Proof of Lemma 1,[0],[0]
∑l i=1,D.1 Proof of Lemma 1,[0],[0]
"ai(ki
2di +",D.1 Proof of Lemma 1,[0],[0]
di−1,D.1 Proof of Lemma 1,[0],[0]
− 2ai + 1) which is the total freedom degree of the network.,D.1 Proof of Lemma 1,[0],[0]
So we can always find a vector wkw ∈ Θ such that ‖w −wkw‖2 ≤ .,D.1 Proof of Lemma 1,[0],[0]
"Now we use the decomposition strategy to bound our goal:
∣∣∣Q̃n(w)−Q(w) ∣∣∣= ∣∣∣∣∣",D.1 Proof of Lemma 1,[0],[0]
"1 n n∑
i=1
f(w,D(i))− ED∼D(f(w,D)) ∣∣∣∣∣
= ∣∣∣∣∣ 1 n n∑
i=1
( f(w,D(i))−f(wkw ,D(i)) )",D.1 Proof of Lemma 1,[0],[0]
"+ 1
n
n∑
i=1
f(wkw ,D (i))−Ef(wkw ,D)+ED∼Df(wkw ,D)−ED∼Df(w,D) ∣∣∣∣∣
≤",D.1 Proof of Lemma 1,[0],[0]
∣∣∣∣∣,D.1 Proof of Lemma 1,[0],[0]
1 n,D.1 Proof of Lemma 1,[0],[0]
"n∑
i=1
( f(w,D(i))−f(wkw ,D(i)) )∣∣∣∣∣+ ∣∣∣∣∣",D.1 Proof of Lemma 1,[0],[0]
"1 n n∑
i=1
f(wkw ,D (i))−ED∼Df(wkw ,D) ∣∣∣∣∣
+ ∣∣∣∣∣ED∼Df(wkw ,D)−ED∼Df(w,D) ∣∣∣∣∣.
Then, we define four events E0, E1, E2 and E3 as
E0 = { sup w∈Ω ∣∣∣Q̃n(w)−Q(w) ∣∣∣ ≥ t } ,
E1 = { sup w∈Ω",D.1 Proof of Lemma 1,[0],[0]
∣∣∣∣∣ 1 n,D.1 Proof of Lemma 1,[0],[0]
"n∑
i=1
( f(w,D(i))− f(wkw ,x(i)) )",D.1 Proof of Lemma 1,[0],[0]
∣∣∣∣∣,D.1 Proof of Lemma 1,[0],[0]
"≥ t 3 } ,
E2 =
{ sup
wkw∈Θ
∣∣∣∣∣ 1 n n∑
i=1
f(wkw ,D (i))−ED∼D(f(wkw ,D)) ∣∣∣∣∣≥ t 3 } ,
E3 = { sup w∈Ω ∣∣∣∣∣ED∼D(f(wkw ,",D.1 Proof of Lemma 1,[0],[0]
"D))−ED∼D(f(w,D))",D.1 Proof of Lemma 1,[0],[0]
"∣∣∣∣∣≥ t 3 } .
",D.1 Proof of Lemma 1,[0],[0]
"Accordingly, we have P (E0) ≤",D.1 Proof of Lemma 1,[0],[0]
"P (E1) + P (E2) + P (E3) .
",D.1 Proof of Lemma 1,[0],[0]
"So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).
",D.1 Proof of Lemma 1,[0],[0]
Step 1.,D.1 Proof of Lemma 1,[0],[0]
Bound P (E1):,D.1 Proof of Lemma 1,[0],[0]
"We first bound P (E1) as follows:
P (E1) =P ( sup w∈Ω ∣∣∣∣∣",D.1 Proof of Lemma 1,[0],[0]
1 n,D.1 Proof of Lemma 1,[0],[0]
"n∑
i=1
( f(w,D(i))− f(wkw ,D(i)) )",D.1 Proof of Lemma 1,[0],[0]
∣∣∣∣∣,D.1 Proof of Lemma 1,[0],[0]
"≥ t 3 )
¬ ≤3 t ED∼D ( sup w∈Ω",D.1 Proof of Lemma 1,[0],[0]
∣∣∣∣∣ 1 n,D.1 Proof of Lemma 1,[0],[0]
"n∑
i=1
( f(w,D(i))− f(wkw ,D(i)) )",D.1 Proof of Lemma 1,[0],[0]
"∣∣∣∣∣ )
",D.1 Proof of Lemma 1,[0],[0]
≤3 t ED∼D ( sup w∈Ω ∣∣ 1 n,D.1 Proof of Lemma 1,[0],[0]
∑n i=1,D.1 Proof of Lemma 1,[0],[0]
"( f(w,D(i))− f(wkw ,D(i)) )∣∣",D.1 Proof of Lemma 1,[0],[0]
‖w −wkw‖2,D.1 Proof of Lemma 1,[0],[0]
"sup w∈Ω ‖w −wkw‖2 )
",D.1 Proof of Lemma 1,[0],[0]
"≤3 t ED∼D ( sup w∈Ω ∥∥∥∇Q̃n(w,D) ∥∥∥ 2 ) ,
where ¬ holds since by Markov inequality, we have that for an arbitrary nonnegative random variable x, then
P(x ≥ t) ≤ E(x) t .
",D.1 Proof of Lemma 1,[0],[0]
"Now we only need to bound ED∼D ( supw∈Ω ∥∥∥∇Q̃n(w,D) ∥∥∥
2
) .",D.1 Proof of Lemma 1,[0],[0]
"Therefore, by Lemma 11, we have
ED∼D (
sup w∈Ω
∥∥∥∇Q̃n(w,D) ∥∥∥
2
) =ED∼D ( sup w∈Ω ∥∥∥∥∥ 1 n n∑
i=1
∇f(w,D(i))",D.1 Proof of Lemma 1,[0],[0]
"∥∥∥∥∥
",D.1 Proof of Lemma 1,[0],[0]
"2
) ≤ED∼D",D.1 Proof of Lemma 1,[0],[0]
"( sup w∈Ω ‖∇f(w,D)‖2 ) ≤β.
",D.1 Proof of Lemma 1,[0],[0]
"where β , [ ϑr̃lc̃ldl + ∑l i=1 ϑbl+1 2di−1",D.1 Proof of Lemma 1,[0],[0]
p2bi2di ri−1ci−1,D.1 Proof of Lemma 1,[0],[0]
∏l s=i dsbs 2(ks−ss+1)2 16p2 ]1/2 in which ϑ = 1/8.,D.1 Proof of Lemma 1,[0],[0]
"Therefore, we have
P (E1) ≤ 3",D.1 Proof of Lemma 1,[0],[0]
"β
t .
",D.1 Proof of Lemma 1,[0],[0]
"We further let
t ≥ 6 β ε .
",D.1 Proof of Lemma 1,[0],[0]
"Then we can bound P(E1): P(E1) ≤ ε
2 .
",D.1 Proof of Lemma 1,[0],[0]
Step 2.,D.1 Proof of Lemma 1,[0],[0]
Bound P (E2): Recall that we use Θ to denote the index of wkw and we have |Θ| ≤ ∏l+1 i=1,D.1 Proof of Lemma 1,[0],[0]
"n
",D.1 Proof of Lemma 1,[0],[0]
"i =( 9(bl+1+ ∑l i=1 dibi)
)θ .",D.1 Proof of Lemma 1,[0],[0]
"We can bound P (E2) as follows:
P (E2) =P ( sup
wkw∈Θ
∣∣∣∣∣ 1 n n∑
i=1
f(wkw ,D (i))− ED∼D(f(wkw ,D))",D.1 Proof of Lemma 1,[0],[0]
∣∣∣∣∣,D.1 Proof of Lemma 1,[0],[0]
"≥ t 3 )
≤",D.1 Proof of Lemma 1,[0],[0]
( 9(bl+1 + ∑l i=1 dibi) ),D.1 Proof of Lemma 1,[0],[0]
"θ sup
wkw∈Θ P
(∣∣∣ 1 n n∑
i=1
f(wkw ,D (i))− ED∼D(f(wkw ,D))",D.1 Proof of Lemma 1,[0],[0]
"∣∣∣ ≥ t 3
)
¬ ≤ ( 9(bl+1 + ∑l i=1 dibi) )",D.1 Proof of Lemma 1,[0],[0]
"θ 2 exp ( −2nt 2
α2
) ,
where ¬ holds because in Lemma 15, we have
P
( 1
n
n∑
i=1
( f(w,D(i))−E(f(w,D(i))) )",D.1 Proof of Lemma 1,[0],[0]
">t ) ≤ exp ( −2nt 2
α2
) ,
where α = 1.",D.1 Proof of Lemma 1,[0],[0]
"Thus, if we set
t ≥
√√√√α2 ( θ log ( 9(bl+1+ ∑l i=1 dibi) )",D.1 Proof of Lemma 1,[0],[0]
"+ log ( 4 ε ))
",D.1 Proof of Lemma 1,[0],[0]
"2n ,
then we have P (E2) ≤",D.1 Proof of Lemma 1,[0],[0]
"ε
2 .
",D.1 Proof of Lemma 1,[0],[0]
Step 3.,D.1 Proof of Lemma 1,[0],[0]
Bound P (E3):,D.1 Proof of Lemma 1,[0],[0]
"We first bound P (E3) as follows:
P (E3) =P (
sup w∈Ω ‖ED∼D(f(wkw ,D))− ED∼D(f(w,D))‖2 ≥
t
3
)
",D.1 Proof of Lemma 1,[0],[0]
"=P (
sup w∈Ω ‖ED∼D",D.1 Proof of Lemma 1,[0],[0]
"(f(wkw ,D)− f(w,D)‖2) ‖w −wkw‖2",D.1 Proof of Lemma 1,[0],[0]
sup w∈Ω ‖w −wkw‖2,D.1 Proof of Lemma 1,[0],[0]
≥,D.1 Proof of Lemma 1,[0],[0]
"t 3
)
",D.1 Proof of Lemma 1,[0],[0]
"≤P ( ED∼D sup
w∈Ω ‖∇Qw(w,D)‖2 ≥
t
3
)
¬ ≤P ( β ≥ t
3
) ,
where ¬ holds since we utilize Lemma 11.",D.1 Proof of Lemma 1,[0],[0]
We set enough small such that β < t/3 always holds.,D.1 Proof of Lemma 1,[0],[0]
"Then it yields P (E3) = 0.
",D.1 Proof of Lemma 1,[0],[0]
Step 4.,D.1 Proof of Lemma 1,[0],[0]
"Final result: To ensure P(E0) ≤ ε, we just set = 18p 2(bl+1+
∑l i=1 dibi)
",D.1 Proof of Lemma 1,[0],[0]
"ϑ2nbl+1
[∏l s=1 dsbs 2(ks−ss+1)2 16p2 ]",D.1 Proof of Lemma 1,[0],[0]
− 12 .,D.1 Proof of Lemma 1,[0],[0]
"Note that
6 β ε > 3 β due to ε ≤ 1.",D.1 Proof of Lemma 1,[0],[0]
"Thus we can obtain
t≥max   6 β
ε ,
√√√√α2 ( θ log ( 9(bl+1+ ∑l i=1 dibi) )",D.1 Proof of Lemma 1,[0],[0]
"+ log ( 4 ε ))
",D.1 Proof of Lemma 1,[0],[0]
"2n
  .
",D.1 Proof of Lemma 1,[0],[0]
"By comparing the values of α, we can observe that if n ≥ cf ′",D.1 Proof of Lemma 1,[0],[0]
"l 2(bl+1+
∑l i=1 dibi) 2 maxi √ rici
θ%ε2 where cf ′ is a constant, there exists such a universal constant cf such that
sup w∈Ω
∣∣∣Q̃n(w)−Q(w) ∣∣∣≤α
√√√√θ (∑l i=1 log (√ dsbs(ks−ss+1) 4p ) + log(bl+1)+log ( n 128p2 ))",D.1 Proof of Lemma 1,[0],[0]
"+log ( 4 ε )
2n =
√ θ%+log",D.1 Proof of Lemma 1,[0],[0]
"( 4 ε )
2n
holds with probability at least 1 − ε, where θ = al+1(dl+1 + r̃lc̃ldl − 2al+1 + 1) +",D.1 Proof of Lemma 1,[0],[0]
∑l i=1,D.1 Proof of Lemma 1,[0],[0]
"ai(ki
2di +",D.1 Proof of Lemma 1,[0],[0]
di−1,D.1 Proof of Lemma 1,[0],[0]
"− 2ai + 1), % = ∑l i=1log (√ dibi(ki−si+1) 4p ) + log(bl+1)",D.1 Proof of Lemma 1,[0],[0]
"+ log ( n 128p2 ) , and α = 1.",D.1 Proof of Lemma 1,[0],[0]
The proof is completed.,D.1 Proof of Lemma 1,[0],[0]
Proof.,D.2 Proof of Theorem 1,[0],[0]
"By Lemma 1 in the manuscript, we know that if n ≥ cf ′ l2(bl+1",D.2 Proof of Theorem 1,[0],[0]
"+ ∑l i=1 dibi) 2 maxi √ rici/(θ%ε
2) where cf ′ is a universal constant, then with probability at least 1− ε, we have
sup w∈Ω
∣∣∣Q̃n(w)−Q(w) ∣∣∣ ≤
√ θ%+ log ( 4 ε )
2n ,
where the total freedom degree θ of the network is θ = al+1(dl+1 + r̃lc̃ldl + 1) +",D.2 Proof of Theorem 1,[0],[0]
∑l i=1 ai(ki 2di−1 + di + 1) and % = ∑l i=1log (√ dibi(ki−si+1) 4p ) + log(bl+1),D.2 Proof of Theorem 1,[0],[0]
+,D.2 Proof of Theorem 1,[0],[0]
"log ( n 128p2 ) .
",D.2 Proof of Theorem 1,[0],[0]
"Thus based on such a result, we can derive the following generalization bound:
ES∼D ∣∣∣EA(Q(w̃)− Q̃n(w̃))",D.2 Proof of Theorem 1,[0],[0]
∣∣∣ ≤,D.2 Proof of Theorem 1,[0],[0]
"ES∼D (
sup w∈Ω ∣∣∣Q̃n(w)−Q(w) ∣∣∣ )",D.2 Proof of Theorem 1,[0],[0]
≤ sup w∈Ω ∣∣∣Q̃n(w)−Q(w),D.2 Proof of Theorem 1,[0],[0]
"∣∣∣ ≤
√ θ%+ log ( 4 ε )
2n .
",D.2 Proof of Theorem 1,[0],[0]
"Thus, the conclusion holds.",D.2 Proof of Theorem 1,[0],[0]
The proof is completed.,D.2 Proof of Theorem 1,[0],[0]
Proof.,D.3 Proof of Theorem 2,[0],[0]
"Recall that the weight of each kernel and the feature maps has magnitude bound separately, i.e. wk(i) ∈",D.3 Proof of Theorem 2,[0],[0]
"Bki
2di−1(rw)",D.3 Proof of Theorem 2,[0],[0]
"(i = 1, · · · , l; k = 1, · · · , di) and w(l+1) ∈ Br̃lc̃ldldl+1(bl+1).",D.3 Proof of Theorem 2,[0],[0]
"Since W̃(i) = [vec(W 1(i)), vec(W 2(i)), · · · , vec(W di−1(i) )",D.3 Proof of Theorem 2,[0],[0]
],D.3 Proof of Theorem 2,[0],[0]
∈,D.3 Proof of Theorem 2,[0],[0]
Rk 2,D.3 Proof of Theorem 2,[0],[0]
"i di−1×di , we have ‖W̃(i)‖F ≤ dibi.",D.3 Proof of Theorem 2,[0],[0]
"So here we assume W̃(i, ) is the dibi /(bl+1",D.3 Proof of Theorem 2,[0],[0]
+,D.3 Proof of Theorem 2,[0],[0]
∑l i=1 dibi)-covering net of the matrix W̃(i),D.3 Proof of Theorem 2,[0],[0]
which is the set of all parameters in the i-th layer.,D.3 Proof of Theorem 2,[0],[0]
"Then by Lemma 7, we have the covering number
n",D.3 Proof of Theorem 2,[0],[0]
"i ≤
( 9(bl+1 + ∑l i=1 dibi) )",D.3 Proof of Theorem 2,[0],[0]
"ai(ki2di−1+di−2ai+1) ,
since the rank of W̃(i) obeys rank(W̃(i))",D.3 Proof of Theorem 2,[0],[0]
≤ ai for 1 ≤,D.3 Proof of Theorem 2,[0],[0]
i ≤,D.3 Proof of Theorem 2,[0],[0]
l.,D.3 Proof of Theorem 2,[0],[0]
"For the last layer, we also can construct an bl+1 /(bl+1",D.3 Proof of Theorem 2,[0],[0]
+∑l i=1 dibi)-covering net for the weight matrixW(l+1).,D.3 Proof of Theorem 2,[0],[0]
"Here we have
n l+1 ≤
( 9(bl+1 + ∑l i=1 dibi) )",D.3 Proof of Theorem 2,[0],[0]
al+1(dl+1+r̃lc̃ldl−2al+1,D.3 Proof of Theorem 2,[0],[0]
"+1) ,
since the rank of W(l+1) obeys rank(W(l+1))",D.3 Proof of Theorem 2,[0],[0]
≤ al+1.,D.3 Proof of Theorem 2,[0],[0]
"Finally, we arrange them together to construct a set Θ and claim that there is always an -covering net w in Θ for any parameter w. Accordingly, we have
|Θ| ≤ l+1∏
i=1
n i=
( 9(bl+1 + ∑l i=1 dibi) )",D.3 Proof of Theorem 2,[0],[0]
al+1(dl+1+r̃lc̃ldl−2al+1,D.3 Proof of Theorem 2,[0],[0]
+1)+∑li=1 ai(ki2di−1+di−2ai+1) = ( 9(bl+1 + ∑l i=1 dibi) ),D.3 Proof of Theorem 2,[0],[0]
"θ ,
where θ = al+1(dl+1 + r̃lc̃ldl",D.3 Proof of Theorem 2,[0],[0]
− 2al+1 + 1) +,D.3 Proof of Theorem 2,[0],[0]
∑l i=1,D.3 Proof of Theorem 2,[0],[0]
"ai(ki
2di−1 + di − 2ai + 1) which is the total freedom degree of the network.",D.3 Proof of Theorem 2,[0],[0]
So we can always find a vector wkw ∈ Θ such that ‖w − wkw‖2 ≤ .,D.3 Proof of Theorem 2,[0],[0]
"Accordingly, we can decompose∥∥∥∇Q̃n(w)−∇Q(w)
∥∥∥ 2 as
∥∥∥∇Q̃n(w)−∇Q(w) ∥∥∥
2
= ∥∥∥∥∥ 1 n n∑
i=1
∇f(w,D(i))− ED∼D(∇f(w,D))",D.3 Proof of Theorem 2,[0],[0]
"∥∥∥∥∥
",D.3 Proof of Theorem 2,[0],[0]
"2
= ∥∥∥∥∥ 1 n n∑
i=1
( ∇f(w,D(i))−∇f(wkw ,D(i)) )",D.3 Proof of Theorem 2,[0],[0]
"+ 1
n
n∑
i=1
∇f(wkw ,D(i))− ED∼D(∇f(wkw ,D))
",D.3 Proof of Theorem 2,[0],[0]
"+ ED∼D(∇f(wkw ,D))− ED∼D(∇f(w,D))",D.3 Proof of Theorem 2,[0],[0]
"∥∥∥∥∥
",D.3 Proof of Theorem 2,[0],[0]
2 ≤ ∥∥∥∥∥ 1 n,D.3 Proof of Theorem 2,[0],[0]
"n∑
i=1
( ∇f(w,D(i))−∇f(wkw ,D(i)) )",D.3 Proof of Theorem 2,[0],[0]
∥∥∥∥∥ 2,D.3 Proof of Theorem 2,[0],[0]
+ ∥∥∥∥∥ 1 n n∑ i=1,D.3 Proof of Theorem 2,[0],[0]
"∇f(wkw ,D(i))− ED∼D(∇f(wkw ,D))",D.3 Proof of Theorem 2,[0],[0]
∥∥∥∥∥,D.3 Proof of Theorem 2,[0],[0]
"2
+ ∥∥∥∥∥ED∼D(∇f(wkw ,D))− ED∼D(∇f(w,D)) ∥∥∥∥∥
2
.
",D.3 Proof of Theorem 2,[0],[0]
"Here we also define four events E0, E1, E2 and E3 as
E0 = { sup w∈Ω ∥∥∥∇Q̃n(w)−∇Q(w) ∥∥∥ 2 ≥ t } ,
E1 = { sup w∈Ω ∥∥∥∥∥",D.3 Proof of Theorem 2,[0],[0]
"1 n n∑
i=1
( ∇f(w,D(i))−∇f(wkw ,D(i)) )",D.3 Proof of Theorem 2,[0],[0]
∥∥∥∥∥,D.3 Proof of Theorem 2,[0],[0]
"2 ≥ t 3 } ,
E2 =
{ sup
wkw∈Θ
∥∥∥∥∥ 1 n n∑
i=1
∇f(wkw ,D(i))− ED∼D(∇f(wkw ,D))",D.3 Proof of Theorem 2,[0],[0]
"∥∥∥∥∥
2
≥ t 3
} ,
",D.3 Proof of Theorem 2,[0],[0]
"E3 = { sup w∈Ω ∥∥∥∥∥ED∼D(∇f(wkw ,D))−",D.3 Proof of Theorem 2,[0],[0]
"ED∼D(∇f(w,D))",D.3 Proof of Theorem 2,[0],[0]
"∥∥∥∥∥
2
≥ t 3
} .
",D.3 Proof of Theorem 2,[0],[0]
"Accordingly, we have P (E0) ≤",D.3 Proof of Theorem 2,[0],[0]
"P (E1) + P (E2) + P (E3) .
",D.3 Proof of Theorem 2,[0],[0]
"So we can respectively bound P (E1), P (E2) and P (E3) to bound P (E0).
",D.3 Proof of Theorem 2,[0],[0]
Step 1.,D.3 Proof of Theorem 2,[0],[0]
Bound P (E1):,D.3 Proof of Theorem 2,[0],[0]
"We first bound P (E1) as follows:
P (E1) =P ( sup w∈Ω ∥∥∥∥∥",D.3 Proof of Theorem 2,[0],[0]
"1 n n∑
i=1
( ∇f(w,D(i))−∇f(wkw ,D(i)) )",D.3 Proof of Theorem 2,[0],[0]
"∥∥∥∥∥ 2 ≥ t 3 )
¬ ≤3 t ED∼D ( sup w∈Ω ∥∥∥∥∥ 1 n",D.3 Proof of Theorem 2,[0],[0]
"n∑
i=1
( ∇f(w,D(i))−∇f(wkw ,D(i)) )",D.3 Proof of Theorem 2,[0],[0]
"∥∥∥∥∥ 2 )
",D.3 Proof of Theorem 2,[0],[0]
≤3 t ED∼D ( sup w∈Ω ∥∥ 1 n ∑n i=1,D.3 Proof of Theorem 2,[0],[0]
"( ∇f(w,D(i))−∇f(wkw ,D(i)) )",D.3 Proof of Theorem 2,[0],[0]
"∥∥ 2 ‖w −wkw‖2 sup w∈Ω ‖w −wkw‖2 )
",D.3 Proof of Theorem 2,[0],[0]
"≤3 t ED∼D ( sup w∈Ω ∥∥∥∇2Q̃n(w,D) ∥∥∥ 2 ) ,
where ¬ holds since by Markov inequality, we have that for an arbitrary nonnegative random variable x, then P(x ≥ t) ≤ E(x) t .",D.3 Proof of Theorem 2,[0],[0]
"Now we only need to bound ED∼D ( supw∈Ω ∥∥∥∇2Q̃n(w,D) ∥∥∥
2
) .",D.3 Proof of Theorem 2,[0],[0]
"Here we utilize Lemma 14 to achieve this goal:
ED∼D (
sup w∈Ω
∥∥∥∇2Q̃n(w,D) ∥∥∥
2
) ≤ ED∼D",D.3 Proof of Theorem 2,[0],[0]
"( sup w∈Ω ∥∥∇2f(w,D)−∇2f(w∗,D) ∥∥ 2 ) ≤ γ.
where γ = ( ϑbl+1
2d20 b14d21",D.3 Proof of Theorem 2,[0],[0]
l2r20c 2 0,D.3 Proof of Theorem 2,[0],[0]
[∏l s=1 dsbs 2(ks−ss+1)2 8 √ 2p2 ]2)1/2 .,D.3 Proof of Theorem 2,[0],[0]
"Therefore, we have
P (E1) ≤ 3γ
t .
",D.3 Proof of Theorem 2,[0],[0]
"We further let t ≥ 6γ
ε .
",D.3 Proof of Theorem 2,[0],[0]
"Then we can bound P(E1): P(E1) ≤ ε
2 .
",D.3 Proof of Theorem 2,[0],[0]
Step 2.,D.3 Proof of Theorem 2,[0],[0]
"Bound P (E2): By Lemma 2, we know that for any vector x ∈ Rd, its `2-norm can be computed as
‖x‖2 ≤ 1
1− supλ∈λ 〈λ,x〉 .
",D.3 Proof of Theorem 2,[0],[0]
"where λ = {λ1, . . .",D.3 Proof of Theorem 2,[0],[0]
",λkw} be an -covering net of Bd(1).
",D.3 Proof of Theorem 2,[0],[0]
"Let λ be the 12 -covering net of B d(1), where d = r̃lc̃ldldl+1 + ∑l i=1 ki 2di−1di.",D.3 Proof of Theorem 2,[0],[0]
Recall that we use Θ to denote the index of wkw so that ‖w −wkw‖ ≤ .,D.3 Proof of Theorem 2,[0],[0]
"Besides, |Θ| ≤ ∏l+1 i=1",D.3 Proof of Theorem 2,[0],[0]
n,D.3 Proof of Theorem 2,[0],[0]
i =,D.3 Proof of Theorem 2,[0],[0]
( 3(bl+1+ ∑l i=1 dibi) ),D.3 Proof of Theorem 2,[0],[0]
θ .,D.3 Proof of Theorem 2,[0],[0]
"Then we can bound P (E2) as follows:
P (E2) =P ( sup
wkw∈Θ
∥∥∥∥∥ 1 n n∑
i=1
∇f(wkw ,D(i))− ED∼D(∇f(wkw ,D))",D.3 Proof of Theorem 2,[0],[0]
"∥∥∥∥∥
2
≥ t 3
)
",D.3 Proof of Theorem 2,[0],[0]
"=P ( sup
wkw∈Θ,λ∈λ1/2 2
〈 λ, 1
n
n∑
i=1
∇f(wkw ,D(i))− ED∼D (∇f(wkw ,D)) 〉 ≥ t
3
)
",D.3 Proof of Theorem 2,[0],[0]
≤6d ( 9(bl+1 + ∑l i=1 dibi) ),D.3 Proof of Theorem 2,[0],[0]
"θ sup wkw∈Θ,λ∈λ1/2 P ( 1 n n∑
i=1
〈 λ,∇f(wkw ,D(i))− ED∼D (∇f(wkw ,D)) 〉 ≥ t
6
)
¬ ≤6d
( 9(bl+1 + ∑l i=1 dibi) )",D.3 Proof of Theorem 2,[0],[0]
"θ 2 exp ( − nt 2
72β2
) ,
where ¬ holds since by Lemma 16, we have
P
( 1
n
n∑
i=1
(〈 λ,∇wf(w,D(i))−ED∼D∇wf(w,D(i)) 〉) >t ) ≤ exp ( − nt 2
2β2
) .
",D.3 Proof of Theorem 2,[0],[0]
"where β , [ ϑr̃lc̃ldl + ∑l i=1 ϑbl+1 2di−1",D.3 Proof of Theorem 2,[0],[0]
p2bi2di ri−1ci−1,D.3 Proof of Theorem 2,[0],[0]
"∏l s=i dsbs 2(ks−ss+1)2 16p2 ]1/2 in which ϑ = 1/8.
",D.3 Proof of Theorem 2,[0],[0]
"Thus, if we set
t≥
√√√√72β2 ( d log(6) + θ log ( 9(bl+1+ ∑l i=1 dibi) )",D.3 Proof of Theorem 2,[0],[0]
"+ log ( 4 ε ))
n ,
then we have P (E2) ≤",D.3 Proof of Theorem 2,[0],[0]
"ε
2 .
",D.3 Proof of Theorem 2,[0],[0]
Step 3.,D.3 Proof of Theorem 2,[0],[0]
Bound P (E3):,D.3 Proof of Theorem 2,[0],[0]
"We first bound P (E3) as follows:
P (E3) =P (
sup w∈Ω ‖E(∇f(wkw ,x))−",D.3 Proof of Theorem 2,[0],[0]
"ED∼D(∇f(w,x))‖2 ≥
t
3
)
=P (
sup w∈Ω ‖ED∼D",D.3 Proof of Theorem 2,[0],[0]
(,D.3 Proof of Theorem 2,[0],[0]
"∇f(wkw ,x)−∇f(w,x)‖2) ‖w −wkw‖2",D.3 Proof of Theorem 2,[0],[0]
sup w∈Ω ‖w −wkw‖2,D.3 Proof of Theorem 2,[0],[0]
≥,D.3 Proof of Theorem 2,[0],[0]
"t 3
)
",D.3 Proof of Theorem 2,[0],[0]
"≤P ( ED∼D sup
w∈Ω
∥∥∥∇2Q̃n(w,x) ∥∥∥
2 ≥ t 3
)
",D.3 Proof of Theorem 2,[0],[0]
"≤P ( γ ≥ t
3
) .
",D.3 Proof of Theorem 2,[0],[0]
We set enough small such that γ < t/3 always holds.,D.3 Proof of Theorem 2,[0],[0]
"Then it yields P (E3) = 0.
",D.3 Proof of Theorem 2,[0],[0]
Step 4.,D.3 Proof of Theorem 2,[0],[0]
Final result: Note that 6β ε ≥ 3β .,D.3 Proof of Theorem 2,[0],[0]
"Finally, to ensure P(E0)",D.3 Proof of Theorem 2,[0],[0]
"≤ ε, we just set = 18p2(bl+1+ ∑l i=1 dibi)
",D.3 Proof of Theorem 2,[0],[0]
"ϑ2nbl+1
[∏l s=i dsbs 2(ks−ss+1)2 16p2 ]",D.3 Proof of Theorem 2,[0],[0]
"− 12 .
",D.3 Proof of Theorem 2,[0],[0]
"t ≥ max   6γ
ε ,
√√√√72β2 ( d log(6)",D.3 Proof of Theorem 2,[0],[0]
+ θ log ( 9(bl+1+ ∑l i=1 dibi) ),D.3 Proof of Theorem 2,[0],[0]
"+ log ( 4 ε ))
n
  .
",D.3 Proof of Theorem 2,[0],[0]
"By comparing the values of β and γ, we have if n ≥",D.3 Proof of Theorem 2,[0],[0]
"cg′ l 2bl+1
2(bl+1+ ∑l i=1 dibi) 2(r0c0d0) 4
d40b1 8(d log(6)+θ%)ε2 maxi(rici) where cg′ is a universal constant, then there exists a universal constant cg such that
sup w∈Ω
∥∥∥∇wQ̃n(w)−∇wQ(w) ∥∥∥
2 ≤cgβ
√√√√d+ 1log(6)θ [(∑l i=1 log (√ dsbs(ks−ss+1) 4p )",D.3 Proof of Theorem 2,[0],[0]
+log(bl+1)+log ( n 128p2 )),D.3 Proof of Theorem 2,[0],[0]
"+log ( 4 ε )]
n
≤cgβ
√ d+ 12θ%+ 1 2 log ( 4 ε )
n ,
holds with probability at least 1 − ε, where d = r̃lc̃ldldl+1 + ∑l i=1 ki 2di−1di, θ = al+1(dl+1 + r̃lc̃ldl + 1) + ∑l i=1 ai(ki 2di−1 + di + 1), % = ∑l i=1log (√ dibi(ki−si+1) 4p ) + log(bl+1)",D.3 Proof of Theorem 2,[0],[0]
"+ log ( n 128p2 ) , and β , [ ϑr̃lc̃ldl + ∑l i=1 ϑbl+1 2di−1",D.3 Proof of Theorem 2,[0],[0]
p2bi2di ri−1ci−1,D.3 Proof of Theorem 2,[0],[0]
∏l s=i dsbs 2(ks−ss+1)2 16p2 ]1/2 in which ϑ = 1/8.,D.3 Proof of Theorem 2,[0],[0]
The proof is completed.,D.3 Proof of Theorem 2,[0],[0]
Proof.,D.4 Proof of Corollary 1,[0],[0]
"By Theorem 2, we know that there exist universal constants cg′ and cg such that if n ≥ cg′ l2bl+1 2(bl+1+ ∑l i=1 dibi) 2(r0c0d0) 4
d40b1 8(d log(6)+θ%)ε2 maxi(rici)
, then
sup w∈Ω
∥∥∥∇wQ̃n(w)−∇wQ(w) ∥∥∥
2 ≤ cgβ
√ 2d+ θ%+ log ( 4 ε )
2n
holds with probability at least 1 − ε, where % is provided in Lemma 1.",D.4 Proof of Corollary 1,[0],[0]
Here β and d are defined as β =[ rlcldl 8p2 +,D.4 Proof of Corollary 1,[0],[0]
∑l i=1 bl+1 2di−1,D.4 Proof of Corollary 1,[0],[0]
"8p2bi2di ri−1ci−1 ∏l j=i djbj 2(kj−sj+1)2 16p2 ]1/2 and d = r̃lc̃ldldl+1 + ∑l i=1 ki 2di−1di, respectively.
",D.4 Proof of Corollary 1,[0],[0]
"So based on such a result, we can derive that if n ≥ c2g(2d+ θ%+ log(4/ε))β2/(2 ), then we have
‖∇Q(w̃)‖2 ≤ ∥∥∥∇wQ̃n(w̃) ∥∥∥ 2 + ∥∥∥∇wQ̃n(w̃)−∇wQ(w̃)",D.4 Proof of Corollary 1,[0],[0]
∥∥∥ 2 ≤,D.4 Proof of Corollary 1,[0],[0]
"√ + cgβ
√ 2d+ θ%+ log ( 4 ε )
2n ≤ 2√ .
",D.4 Proof of Corollary 1,[0],[0]
"Thus, we have ‖∇Q(w̃)‖22 ≤ 4 , which means that w̃ is a 4 -approximate stationary point in population risk with probability at least 1− ε.",D.4 Proof of Corollary 1,[0],[0]
The proof is completed.,D.4 Proof of Corollary 1,[0],[0]
Proof.,D.5 Proof of Theorem 3,[0],[0]
"Suppose that {w(1),w(2), · · · ,w(m)} are the non-degenerate critical points ofQ(w).",D.5 Proof of Theorem 3,[0],[0]
"So for any w(k), it obeys
inf i
∣∣λki ( ∇2Q(w(k)) )∣∣",D.5 Proof of Theorem 3,[0],[0]
"≥ ζ,
where λki ( ∇2Q(w(k)) ) denotes the i-th eigenvalue of the Hessian ∇2Q(w(k)) and ζ is a constant.",D.5 Proof of Theorem 3,[0],[0]
"We further define a
set D = {w ∈",D.5 Proof of Theorem 3,[0],[0]
Rd,D.5 Proof of Theorem 3,[0],[0]
| ‖∇Q(w)‖2 ≤ and infi |λi ( ∇2Q(w(k)) ),D.5 Proof of Theorem 3,[0],[0]
| ≥ ζ}.,D.5 Proof of Theorem 3,[0],[0]
"According to Lemma 5, D = ∪∞k=1Dk where each Dk is a disjoint component with w(k) ∈ Dk for k ≤ m",D.5 Proof of Theorem 3,[0],[0]
and Dk does not contain any critical point ofQ(w) for k ≥ m+ 1.,D.5 Proof of Theorem 3,[0],[0]
"On the other hand, by the continuity of∇Q(w), it yields ‖∇Q(w)‖2 = for w ∈ ∂Dk.",D.5 Proof of Theorem 3,[0],[0]
"Notice, we set the value of blow which is actually a function related to n.
Then by utilizing Theorem 2, we let sample number n sufficient large such that
sup w∈Ω
∥∥∥∇Q̃n(w)−∇Q(w) ∥∥∥
2 ≤ 2
holds with probability at least 1− ε, where is defined as
2 , cgβ
√√√√d log(6)",D.5 Proof of Theorem 3,[0],[0]
+ θ,D.5 Proof of Theorem 3,[0],[0]
(∑l i=1 log (√ dsbs(ks−ss+1) 4p ) + log(bl+1),D.5 Proof of Theorem 3,[0],[0]
+ log ( n 128p2 )),D.5 Proof of Theorem 3,[0],[0]
"+ log ( 4 ε )
n .
",D.5 Proof of Theorem 3,[0],[0]
This further gives that for arbitrary w ∈,D.5 Proof of Theorem 3,[0],[0]
"Dk, we have
inf w∈Dk
∥∥∥t∇Q̃n(w)",D.5 Proof of Theorem 3,[0],[0]
+ (,D.5 Proof of Theorem 3,[0],[0]
"1− t)∇Q(w) ∥∥∥
2 = inf w∈Dk
∥∥∥t ( ∇Q̃n(w)−∇Q(w) )",D.5 Proof of Theorem 3,[0],[0]
"+∇Q(w) ∥∥∥ 2
≥ inf w∈Dk",D.5 Proof of Theorem 3,[0],[0]
‖∇Q(w)‖2,D.5 Proof of Theorem 3,[0],[0]
"− sup w∈Dk
t ∥∥∥∇Q̃n(w)−∇Q(w) ∥∥∥ 2
≥ 2 .",D.5 Proof of Theorem 3,[0],[0]
"(6)
Similarly, by utilizing Lemma 18, let n be sufficient large such that
sup w∈Ω
∥∥∥∇2Q̃n(w)−∇2Q(w) ∥∥∥
op ≤ ζ 2
holds with probability at least 1− ε, where ζ satisfies
ζ 2 ≥ cvγ
√ d+",D.5 Proof of Theorem 3,[0],[0]
"θ%+ log ( 4 ε )
n .
",D.5 Proof of Theorem 3,[0],[0]
Assume that b ∈ Rd is a vector and satisfies bT b = 1.,D.5 Proof of Theorem 3,[0],[0]
"In this case, we can bound λki ( ∇2Q̃n(w) ) for arbitrary w ∈ Dk as follows:
inf w∈Dk
∣∣∣λki ( ∇2Q̃n(w) )∣∣∣",D.5 Proof of Theorem 3,[0],[0]
"= inf w∈Dk min bT b=1 ∣∣∣bT∇2Q̃n(w)b ∣∣∣
= inf w∈Dk min bT b=1
∣∣∣bT ( ∇2Q̃n(w)−∇2Q(w) )",D.5 Proof of Theorem 3,[0],[0]
"b+ bT∇2Q(w)b ∣∣∣
",D.5 Proof of Theorem 3,[0],[0]
"≥ inf w∈Dk min bT b=1
∣∣bT∇2Q(w)b ∣∣− min
bT b=1
∣∣∣bT ( ∇2Q̃n(w)−∇2Q(w) )",D.5 Proof of Theorem 3,[0],[0]
"b ∣∣∣
≥ inf w∈Dk min bT b=1
∣∣bT∇2Q(w)b ∣∣− max
bT b=1
∣∣∣bT ( ∇2Q̃n(w)−∇2Q(w) )",D.5 Proof of Theorem 3,[0],[0]
"b ∣∣∣
= inf w∈Dk inf i |λki ( ∇2f(w(k),x) )",D.5 Proof of Theorem 3,[0],[0]
| − ∥∥∥∇2Q̃n(w)−∇2Q(w) ∥∥∥,D.5 Proof of Theorem 3,[0],[0]
"op
≥ζ 2 .
(7)
",D.5 Proof of Theorem 3,[0],[0]
"This means that in each set Dk,∇2Q̃n(w) has no zero eigenvalues.",D.5 Proof of Theorem 3,[0],[0]
"Then, combine this and Eqn.",D.5 Proof of Theorem 3,[0],[0]
"(6), by Lemma 4 we know that if the population riskQ(w) has no critical point in Dk, then the empirical risk Q̃n(w) has also no critical point in Dk; otherwise it also holds.
",D.5 Proof of Theorem 3,[0],[0]
Now we bound the distance between the corresponding critical points ofQ(w) and Q̃n(w).,D.5 Proof of Theorem 3,[0],[0]
"Assume that in Dk,Q(w) has a unique critical point w(k) and Q̃n(w) also has a unique critical point w (k) n .",D.5 Proof of Theorem 3,[0],[0]
"Then, there exists t ∈",D.5 Proof of Theorem 3,[0],[0]
"[0, 1] such that for any z ∈ ∂Bd(1), we have ≥‖∇Q(w(k)n )",D.5 Proof of Theorem 3,[0],[0]
"‖2
= max zT z=1
〈∇Q(w(k)n ), z〉
= max zT z=1",D.5 Proof of Theorem 3,[0],[0]
"〈∇Q(w(k)), z〉+ 〈∇2Q(w(k) + t(w(k)n −w(k)))(w(k)n −w(k)), z〉 ¬ ≥ 〈( ∇2Q(w(k)) )",D.5 Proof of Theorem 3,[0],[0]
"2 (w(k)n −w(k)), (w(k)n −w(k))",D.5 Proof of Theorem 3,[0],[0]
"〉1/2
 ",D.5 Proof of Theorem 3,[0],[0]
"≥ζ‖w(k)n −w(k)‖2,
where ¬ holds since ∇Q(w(k))",D.5 Proof of Theorem 3,[0],[0]
= 0 and  holds since w(k) + t(w(k)n −w(k)) is in Dk and for any w ∈,D.5 Proof of Theorem 3,[0],[0]
Dk we have infi |λi ( ∇2Q(w) ),D.5 Proof of Theorem 3,[0],[0]
| ≥ ζ.,D.5 Proof of Theorem 3,[0],[0]
"So if n ≥ ch max ( l2bl+1 2(bl+1+ ∑l i=1 dibi) 2(r0c0d0) 4
d40b1 8d%ε2 maxi(rici)
, d+θ%ζ2 ) where ch is a constant, then
‖w(k)n −w(k)‖2 ≤ 2cgβ
ζ
√√√√d log(6)",D.5 Proof of Theorem 3,[0],[0]
+ θ,D.5 Proof of Theorem 3,[0],[0]
(∑l i=1 log (√ dsbs(ks−ss+1) 4p ) + log(bl+1),D.5 Proof of Theorem 3,[0],[0]
+ log ( n 128p2 )),D.5 Proof of Theorem 3,[0],[0]
"+ log ( 4 ε )
n
holds with probability at least 1− ε.",D.5 Proof of Theorem 3,[0],[0]
Proof.,D.6 Proof of Corollary 2,[0],[0]
"By Theorem 3, we know that the non-degenerate stationary point w(k) in the m non-degenerate stationary points in population risk, denoted by {w(1),w(2), · · · ,w(m)} uniquely corresponding to a non-degenerate stationary point w(k)n in the empirical risk.
",D.6 Proof of Corollary 2,[0],[0]
"On the other hand, for any w(k), it obeys
inf i
∣∣λki ( ∇2Q(w(k)) )∣∣",D.6 Proof of Corollary 2,[0],[0]
"≥ ζ,
where λki ( ∇2Q(w(k)) ) denotes the i-th eigenvalue of the Hessian ∇2Q(w(k)) and ζ is a constant.",D.6 Proof of Corollary 2,[0],[0]
"We further define a
set D = {w ∈",D.6 Proof of Corollary 2,[0],[0]
Rd,D.6 Proof of Corollary 2,[0],[0]
| ‖∇Q(w)‖2 ≤ and infi |λi ( ∇2Q(w(k)) ),D.6 Proof of Corollary 2,[0],[0]
| ≥ ζ}.,D.6 Proof of Corollary 2,[0],[0]
"According to Lemma 5, D = ∪∞k=1Dk where each Dk is a disjoint component with w(k) ∈ Dk for k ≤ m",D.6 Proof of Corollary 2,[0],[0]
and Dk does not contain any critical point ofQ(w) for k ≥ m+ 1.,D.6 Proof of Corollary 2,[0],[0]
Thenw(k)n also belong to the component Dk due to the unique corresponding relation betweenw(k) andw (k) n .,D.6 Proof of Corollary 2,[0],[0]
"Then from Eqn. (6) and (7), we know that if the assumptions in Theorem 3 hold, then for arbitrary w ∈",D.6 Proof of Corollary 2,[0],[0]
"Dk and t ∈ (0, 1),
inf w∈Dk
∥∥∥t∇Q̃n(w)",D.6 Proof of Corollary 2,[0],[0]
+ (,D.6 Proof of Corollary 2,[0],[0]
"1− t)∇Q(w) ∥∥∥
2 ≥ 2 and inf w∈Dk
∣∣∣λki ( ∇2Q̃n(w) )∣∣∣ ≥ ζ 2 ,
where and ζ are constants.",D.6 Proof of Corollary 2,[0],[0]
"This means that in each set Dk,∇2Q̃n(w) has no zero eigenvalues.",D.6 Proof of Corollary 2,[0],[0]
"Then, combine this and Eqn.",D.6 Proof of Corollary 2,[0],[0]
"(6), we can obtain that in Dk, ifQ(w) has a unique critical pointw(k) with non-degenerate index sk, then Q̃n(w) also has a unique critical point wn(k) in Dk with the same non-degenerate index sk.",D.6 Proof of Corollary 2,[0],[0]
"Namely, the number of negative eigenvalues of the Hessian matrices∇2Q(w(k)) and ∇2Q(w(k)n ) are the same.",D.6 Proof of Corollary 2,[0],[0]
"This further gives that if one of the pair (w(k),w(k)n ) is a local minimum or saddle point, then another one is also a local minimum or a saddle point.",D.6 Proof of Corollary 2,[0],[0]
The proof is completed.,D.6 Proof of Corollary 2,[0],[0]
Proof.,E.1 Proof of Lemma 8,[0],[0]
(1) Since G (z) is a diagonal matrix and its diagonal values are upper bounded by σ1(zi)(1− σ1(z)),E.1 Proof of Lemma 8,[0],[0]
"≤ 1/4 where zi denotes the i-th entry of zi, we can conclude
‖G (z)M‖2F ≤ 1
16 ‖M‖2F and ‖NG (z)‖2F ≤
1
16 ‖N‖2F .
",E.1 Proof of Lemma 8,[0],[0]
"(2) The operator Q (·) maps a vector z ∈ Rd into a matrix of size d2 × d whose ((i− 1)d+ i, i) (i = 1, · · · , d) entry equal to σ1(zi)(1− σ1(zi))(1− 2σ1(zi)) and rest entries are all 0.",E.1 Proof of Lemma 8,[0],[0]
"This gives
σ1(zi)(1− σ1(zi))(1− 2σ1(zi))",E.1 Proof of Lemma 8,[0],[0]
"= 1
3 (3σ1(zi))(1− σ1(zi))(1− 2σ1(zi))
",E.1 Proof of Lemma 8,[0],[0]
"≤1 3
( 3σ1(zi) + 1− σ1(zi) + 1− 2σ1(zi)
3
)3
≤2 3
34 .
",E.1 Proof of Lemma 8,[0],[0]
"This means the maximal value in Q (z) is at most 2 3
34 .",E.1 Proof of Lemma 8,[0],[0]
"Consider the structure in Q (z), we can obtain
‖Q (z)M‖2F ≤ 26
38 ‖M‖2F and ‖NQ (z)‖2F ≤
26 38 ‖N‖2F .
",E.1 Proof of Lemma 8,[0],[0]
(3) up (M) represents conducting upsampling on M ∈ Rs×t×q.,E.1 Proof of Lemma 8,[0],[0]
Let N = up (M) ∈ Rps×pt×q.,E.1 Proof of Lemma 8,[0],[0]
"Specifically, for each slice N(:, :, i) (i = 1, · · · , q), we have N(:, :, i) = up (M(:, :, i)).",E.1 Proof of Lemma 8,[0],[0]
"It actually upsamples each entry M(g, h, i) into a matrix of p2 same entries 1p2M(g, h, i).",E.1 Proof of Lemma 8,[0],[0]
"So it is easy to obtain
‖up (M)‖2F ≤ 1
p2 ‖M‖2F .
",E.1 Proof of Lemma 8,[0],[0]
"(4) Let M = W (:, :, i) and N = δ̃i+1(:, : i).",E.1 Proof of Lemma 8,[0],[0]
"Assume that H = M ~N ∈ Rm1×m2 , where m1 = r̃i−1",E.1 Proof of Lemma 8,[0],[0]
− 2ki + 2 and m2 = c̃i−1,E.1 Proof of Lemma 8,[0],[0]
− 2ki + 2.,E.1 Proof of Lemma 8,[0],[0]
"Then we have
‖H‖2F = m1∑
i=1
m2∑
j=1
|H(i, j)|2 = m1∑
i=1
m2∑
j=1
〈MΩi,j ,N〉2 ≤ m1∑
i=1
m2∑
j=1
‖MΩi,j‖2F ‖N‖2F ,
where Ωi,j denotes the entry index ofM for the (i, j)-th convolution operation (i.e. computing theH(i, j)).
",E.1 Proof of Lemma 8,[0],[0]
"Since for each convolution computing, each element in M is involved at most one time, we can claim that any element in M in ∑m1 i=1 ∑m2",E.1 Proof of Lemma 8,[0],[0]
"j=1 ‖MΩi,j‖2F occurs at most (ki − si + 1)2 since there are si − 1 rows and columns between each neighboring nonzero entries inN which is decided by the definition of δ̃i+1 in Sec. B.1.",E.1 Proof of Lemma 8,[0],[0]
"Therefore, we have
m1∑
i=1
m2∑
j=1
‖MΩi,j‖2F ≤ (ki − si + 1)2‖M‖2F ,
which further gives
‖M~̃N‖2F ≤",E.1 Proof of Lemma 8,[0],[0]
"(ki − si + 1)2‖M‖2F ‖N‖2F .
",E.1 Proof of Lemma 8,[0],[0]
"Consider all the slices in δ̃i+1, we can obtain
‖δ̃i+1~̃W ‖2F ≤",E.1 Proof of Lemma 8,[0],[0]
(ki − si + 1)2‖W ‖2F,E.1 Proof of Lemma 8,[0],[0]
"‖δ̃i+1‖2F .
",E.1 Proof of Lemma 8,[0],[0]
"(5) Since for softmax activation function σ2, we have ∑dl+1 i=1",E.1 Proof of Lemma 8,[0],[0]
vi,E.1 Proof of Lemma 8,[0],[0]
= 1 (vi ≥ 0),E.1 Proof of Lemma 8,[0],[0]
"and there is only one nonzero entry (i.e. 1) in y, we can obtain
0 ≤",E.1 Proof of Lemma 8,[0],[0]
‖v,E.1 Proof of Lemma 8,[0],[0]
− y‖22 = ‖v‖22 + ‖y‖22,E.1 Proof of Lemma 8,[0],[0]
"− 2〈v,y〉 = 2− 2〈v,y〉 ≤ 2.
",E.1 Proof of Lemma 8,[0],[0]
The proof is completed.,E.1 Proof of Lemma 8,[0],[0]
This work aims to provide understandings on the remarkable success of deep convolutional neural networks (CNNs) by theoretically analyzing their generalization performance and establishing optimization guarantees for gradient descent based training algorithms.,abstractText,[0],[0]
"Specifically, for a CNN model consisting of l convolutional layers and one fully connected layer, we prove that its generalization error is bounded by O( √ θ%̃/n) where θ denotes freedom degree of the network parameters and %̃ = O(log(li=1 bi(ki",abstractText,[0],[0]
− si + 1)/p) + log(bl+1)),abstractText,[0],[0]
"encapsulates architecture parameters including the kernel size ki, stride si, pooling size p and parameter magnitude bi.",abstractText,[0],[0]
"To our best knowledge, this is the first generalization bound that only depends on O(log(∏l+1 i=1",abstractText,[0],[0]
"bi)), tighter than existing ones that all involve an exponential term likeO(∏l+1 i=1",abstractText,[0],[0]
bi).,abstractText,[0],[0]
"Besides, we prove that for an arbitrary gradient descent algorithm, the computed approximate stationary point by minimizing empirical risk is also an approximate stationary point to the population risk.",abstractText,[0],[0]
This well explains why gradient descent training algorithms usually perform sufficiently well in practice.,abstractText,[0],[0]
"Furthermore, we prove the one-to-one correspondence and convergence guarantees for the non-degenerate stationary points between the empirical and population risks.",abstractText,[0],[0]
"It implies that the computed local minimum for the empirical risk is also close to a local minimum for the population risk, thus ensuring the good generalization performance of CNNs.",abstractText,[0],[0]
Understanding Generalization and Optimization Performance of Deep CNNs,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1131–1141, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"The pattern of language use in a multilingual society is a complex interplay of socio-linguistic, discursive and pragmatic factors.",1 Introduction,[0],[0]
"Sometimes speakers have a preference for a particular language for certain conversational and discourse settings; on other occasions, there is fluid alteration between two or more languages in a single conversation, also known as Code-switching (CS) or Code-mixing1. Under-
∗*",1 Introduction,[0],[0]
"This work was done when the author was a Research Fellow at Microsoft Research Lab India.
",1 Introduction,[0],[0]
"1Although some linguists differentiate between Codeswitching and Code-mixing, this paper will use the two terms interchangeably.
",1 Introduction,[0],[0]
"standing and characterizing language preference in multilingual societies has been the subject matter of linguistic inquiry for over half a century (see Milroy and Muysken (1995) for an overview).
",1 Introduction,[0],[0]
"Conversational phenomena such as CS were observed only in speech and therefore, all previous studies are based on data collected from a small set of speakers or from interviews.",1 Introduction,[0],[0]
"With the growing popularity of social media, we now have an abundance of conversation-like data that exhibit CS and other speech phenomena, hitherto unseen in text (Bali et al., 2014).",1 Introduction,[0],[0]
"Leveraging such data from Twitter, we conduct a large-scale study on language preference, if any, for the expression of opinion and sentiment by Hindi-English (Hi-En) bilinguals.
",1 Introduction,[0],[0]
"We first build a corpus of 430,000 unique Indiaspecific tweets across four domains (sports, entertainment, politics and current events) and automatically classify the tweets by their language: English, Hindi and Hi-En CS.",1 Introduction,[0],[0]
We then develop an opinion detector for each language class to further categorize them into opinionated and non-opinionated tweets.,1 Introduction,[0],[0]
"Sentiment detectors further classify the opinionated tweets as positive, negative or neutral.",1 Introduction,[0],[0]
Our study shows that there is a strong preference towards Hindi (i.e. the native language or L1) over English (L2) for expression of negative opinion.,1 Introduction,[0],[0]
"The effect is clearly visible in CS tweets, where a switch from English to Hindi is often correlated with a switch from a positive to negative sentiment.",1 Introduction,[0],[0]
"This is referred to as the polarity–switch function of CS (Sanchez, 1983).",1 Introduction,[0],[0]
"Using the same experimental technique, we also explore other pragmatic functions of CS, such as reinforcement and narrative–evaluative.
",1 Introduction,[0],[0]
"1131
Apart from being the first large-scale quantitative study of language preference in multilingual societies, this work also has several other contributions: (a) We develop one of the first opinion and sentiment classifiers for Romanized Hindi and CS Hi-En tweets with higher accuracy than the only known previous attempt (Sharma et al., 2015b).",1 Introduction,[0],[0]
"(b) We present a novel methodology for automatically detecting pragmatic functions of codeswitching through opinion and sentiment detection.
",1 Introduction,[0],[0]
"The rest of the paper is organized as follows: Sec. 2 introduces language preference, functions of CS and Hindi-English bilingualism on the web.",1 Introduction,[0],[0]
Sec. 3 formulates the problem and presents the fundamental questions that this paper seeks to answer.,1 Introduction,[0],[0]
Sec. 4 and 5 discuss dataset creation and opinion and sentiment detection techniques respectively.,1 Introduction,[0],[0]
Sec. 6 evaluates the hypotheses in light of the observations on the tweet corpus.,1 Introduction,[0],[0]
"We conclude in Sec. 7, and raise some interesting sociolinguistic questions for future studies.",1 Introduction,[0],[0]
"In order to situate the questions addressed in our work in existing literature, we present a brief overview of the past research in pragmatic and discursive analysis of code-switching, and specifically, on language preference for emotional expression.",2 Background and Related Work,[0],[0]
A primer to Hi-En bilingualism and its presence in social media shall follow.,2 Background and Related Work,[0],[0]
"In multilingual communities, where there are more than one linguistic channels for information exchange, the choice of the channel depends on a variety of factors, and is usually unpredictable (Auer, 1995).",2.1 CS Functions and Language Preference,[0],[0]
"Nevertheless, linguistic studies point out certain frequently-observed patterns.",2.1 CS Functions and Language Preference,[0],[0]
"For instance, certain speech activities might be exclusively or more commonly related to a certain language choice (e.g. Fishman (1971) reports use of English for professional purposes and Spanish for informal chat for English-Spanish bilinguals from Puerto Rico).",2.1 CS Functions and Language Preference,[0],[0]
"Apart from association between such conversational contexts and language preference, language alteration is often found to be used as a signaling device to imply certain pragmatic functions (Barredo, 1997; Sanchez, 1983; Nishimura, 1995; Maschler,
1991; Maschler, 1994) such as: (a) reported speech (b) narrative to evaluative switch (c) reiterations or emphasis (d) topic shift (e) puns and language play (f) topic/comment structuring etc.",2.1 CS Functions and Language Preference,[0],[0]
"Attempts of predicting the preferred language, or even exhaustively listing such functions, have failed.",2.1 CS Functions and Language Preference,[0],[0]
"However, linguists agree that language alteration in multilingual communities is not a random process.
",2.1 CS Functions and Language Preference,[0],[0]
Of specific interest to us are the studies on language preference for expression of emotions.,2.1 CS Functions and Language Preference,[0],[0]
"Through large-scale interviews and two decades of research, Dewaele (2004; 2010) argued that for most multilinguals, L1 (the dominant language, which is often, but not always, the native or mother tongue) is the language preference for emotions, which include emotional inner speech, swearing and even emotional conversations.",2.1 CS Functions and Language Preference,[0],[0]
"Dewaele argues that emotionally charged words in L1 elicit stronger emotions than those in other languages, and hence L1 is preferred for emotion expression.",2.1 CS Functions and Language Preference,[0],[0]
"Around 125 million people in India speak English, half of whom have Hindi as their mother-tongue.",2.2 Hindi-English Bilingualism,[0],[0]
"The large proportion of the remaining half, especially those residing in the metropolitan cities, also know at least some Hindi.",2.2 Hindi-English Bilingualism,[0],[0]
"This makes Hi-En codeswitching, commonly called Hinglish, extremely widespread in India.",2.2 Hindi-English Bilingualism,[0],[0]
"There is historical attestation, as well as recent studies on the growing use of Hinglish in general conversation, and in entertainment and media (see Parshad et al. (2016) and references therein).",2.2 Hindi-English Bilingualism,[0],[0]
"Several recent studies (Bali et al., 2014; Barman et al., 2014; Solorio et al., 2014; Sequiera et al., 2015) also provide evidence of Hinglish and other instances of CS on online social media such as Twitter and Facebook.",2.2 Hindi-English Bilingualism,[0],[0]
"In a Facebook dataset analyzed by Bali et al. (2014), almost all sufficiently long conversation threads were found to be multilingual, and as much as 17% of the comments had CS.",2.2 Hindi-English Bilingualism,[0],[0]
"This study also indicates that on online social media, Hindi is seldom written in the Devanagari script.",2.2 Hindi-English Bilingualism,[0],[0]
"Instead, loose Roman transliteration, or Romanized Hindi, is common, especially when users code-switch between Hindi and English.
",2.2 Hindi-English Bilingualism,[0],[0]
"While there has been some effort towards computational processing of CS text (Solorio and Liu, 2008; Solorio and Liu, 2010; Vyas et al., 2014; Peng
et al., 2014), to the best of our knowledge, there has been no study on automatic identification of functional aspects of CS or any large-scale, data-driven study of language preference.",2.2 Hindi-English Bilingualism,[0],[0]
"The current study adds to the growing repertoire of work on quantitative analysis of social media data for understanding socio-linguistic and pragmatic issues, such as detection of depression (De Choudhury et al., 2013), politeness (Danescu-Niculescu-Mizil et al., 2013), speech acts (Vosoughi and Roy, 2016), and social status (Tchokni et al., 2014).",2.2 Hindi-English Bilingualism,[0],[0]
"Along the lines of (Dewaele, 2010), we ask the following question: Is there a preferred language for expression of opinion and sentiment by the Hi-En bilinguals on Twitter?",3 Problem Formulation,[0],[0]
"More formally, let Λ = {h, e,m} be the set of languages: Hindi (h), English (e) and Mixed (m), i.e., code-switched.",3.1 Definitions,[0],[0]
"Let Σ = {d, r}, be the set of scripts:2 Devanagari (d) and Roman (r).",3.1 Definitions,[0],[0]
"Let us further introduce a set of sentiments, 3 = {+,−, 0,⊗}, where +, − and 0 respectively denote utterances with positive, negative and neutral opinions.",3.1 Definitions,[0],[0]
"⊗ denote non-opinionated (like factual) texts.
",3.1 Definitions,[0],[0]
"Let T = {t1, t2, . . .",3.1 Definitions,[0],[0]
t|T |} be a set of tweets (or any text) generated by Hi-En bilinguals.,3.1 Definitions,[0],[0]
"We define:
• λ(T ), σ(T ) and (T ) as the subsets of T that respectively contain all tweets in language λ, script σ and sentiment .
",3.1 Definitions,[0],[0]
• λσ (T ) = λ(T )∩ σ(T )∩ (T ).,3.1 Definitions,[0],[0]
"Likewise, we also define λ (T ) = λ(T ) ∩ (T ), λσ(T ) =",3.1 Definitions,[0],[0]
"λ(T ) ∩ σ(T ) and σ (T ) = σ(T ) ∩ (T ).
",3.1 Definitions,[0],[0]
"The preference towards a language-script pair λσ for expressing a type of sentiment is given by the probability
pr(λσ| ;T ) = pr( |λσ;T )",3.1 Definitions,[0],[0]
"pr(λσ|T ) pr( |T ) (1)
However, pr(λσ), which defines the prior probability of choosing λσ for a tweet is dependent on a large
2Tweets in mixed script are rare and hence we do not include a symbol for it, though the framework does not preclude such possibilities.
number of socio-linguistic parameters beyond sentiment.",3.1 Definitions,[0],[0]
"For instance, on social media, English is overwhelmingly more common than any Indic language (Bali et al., 2014).",3.1 Definitions,[0],[0]
This is because (a) English tweets come from a large number of users apart from Hi-En bilinguals and (b) English is the preferred language for tweeting even for Hi-En bilinguals because it expands the target audience of the tweet by manifolds.,3.1 Definitions,[0],[0]
"The preference of λσ for expressing , therefore, can be quantified as:
pr( |λσ;T ) = |λσ (T )||λσ(T )| (2)
",3.1 Definitions,[0],[0]
"We say λσ is the preferred language-script choice over λ′σ′ for expressing sentiment if and only if
pr( |λσ;T ) > pr( |λ′σ′;T ) (3)
",3.1 Definitions,[0],[0]
The strength of the preference is directly proportionate the ratio of the probabilities: pr( |λσ;T )/pr( |λ′σ′;T ).,3.1 Definitions,[0],[0]
"An alternative but related way of characterizing the preference is through comparing the odds of choosing a sentiment type to its polar opposite - ′. We say, λσ is the preferred language-script pair for expressing , if
pr( |λσ;T ) pr( ′|λσ;T ) >",3.1 Definitions,[0],[0]
pr( |λ′σ′;T ) pr( ′|λ′σ′;T ) (4),3.1 Definitions,[0],[0]
"Now we can formally define the two hypotheses, we intend to test here.",3.2 Hypotheses,[0],[0]
Hypothesis I:,3.2 Hypotheses,[0],[0]
"For Hi-En bilinguals, Hindi is the preferred language for expression of opinion on Twitter.",3.2 Hypotheses,[0],[0]
"Therefore, we expect
pr({+,−, 0}|hd;T )",3.2 Hypotheses,[0],[0]
"> pr({+,−, 0}|er;T ) (5)
i.e., pr(⊗|hd;T ) < pr(⊗|er;T ) (6) And similarly,
pr(⊗|hr;T ) < pr(⊗|er;T ) (7)
Hypothesis II:",3.2 Hypotheses,[0],[0]
"For Hi-En bilinguals, Hindi is the preferred language for expression of negative sentiment.",3.2 Hypotheses,[0],[0]
"Therefore,
pr(−|hd;T )",3.2 Hypotheses,[0],[0]
≈ pr(−|hr;T ) >,3.2 Hypotheses,[0],[0]
"pr(−|er;T ) (8)
In particular, we would like to hypothesize that the odds of choosing Hindi for negative over positive is really high compared to the odds for English.",3.2 Hypotheses,[0],[0]
"I.e.,
pr(−|hd;T ) pr(+|hd;T )",3.2 Hypotheses,[0],[0]
≈ pr(−|hr;T ) pr(+|hr;T ) >,3.2 Hypotheses,[0],[0]
"pr(−|er;T ) pr(+|er;T ) (9)
",3.2 Hypotheses,[0],[0]
"A special case of the above hypotheses arise in the context of code-mixing, i.e., for the set mr(T ).",3.2 Hypotheses,[0],[0]
"Since the mixed tweets certainly come from proficient bilinguals and have both Hi and En fragments, we can reformulate our hypotheses at a tweet level.",3.2 Hypotheses,[0],[0]
Let mhr(T ) and mer(T ) respectively denote the set of Hi and En fragments in mr(T ).,3.2 Hypotheses,[0],[0]
Hypothesis Ia: Hindi is the preferred language for expression of opinion in Hi-En code-mixed tweets.,3.2 Hypotheses,[0],[0]
"Therefore, we expect
i.e., pr(⊗|mhr;T ) < pr(⊗|mer;T ) (10)
Hypothesis IIa:",3.2 Hypotheses,[0],[0]
Hindi is the preferred language for expression of negative sentiment in Hi-En codeswitched tweets.,3.2 Hypotheses,[0],[0]
"Therefore,
pr(−|mhr;T )",3.2 Hypotheses,[0],[0]
"> pr(−|mer;T ) (11)
pr(−|mhr;T ) pr(+|mhr;T )",3.2 Hypotheses,[0],[0]
"> pr(−|mer;T ) pr(+|mer;T ) (12)
",3.2 Hypotheses,[0],[0]
"Likewise, the above hypotheses also apply for the Devanagari script, though for technical reasons, we do not test them here.
",3.2 Hypotheses,[0],[0]
"Besides comparing aggregate statistics onmr(T ), it is also interesting to look at the sentiment of mhr(ti) andmer(ti) for each tweet ti.",3.2 Hypotheses,[0],[0]
"In particular, for every pair of 6= ′, we want to study the fraction of tweets in mr(T ) where mhr(ti) has sentiment and mer(ti) has ′. Let this fraction be pr(h ↔",3.2 Hypotheses,[0],[0]
e ′;mr(T )).,3.2 Hypotheses,[0],[0]
"Under “no-preference for language” (i.e., the null) hypothesis, we would expect pr(h ↔",3.2 Hypotheses,[0],[0]
e ′;mr(T )),3.2 Hypotheses,[0],[0]
≈ pr(h ′,3.2 Hypotheses,[0],[0]
↔ e ;mr(T )).,3.2 Hypotheses,[0],[0]
"However, if pr(h ↔ ′;mr(T )) is significantly higher than pr(h ′",3.2 Hypotheses,[0],[0]
"↔ e ;mr(T )), it means that speakers prefer to switch from English to Hindi when they want to express a sentiment and vice versa.",3.2 Hypotheses,[0],[0]
"Pragmatic Functions of Code-Switching: When native speakers tend to switch from Hindi to English when they switch from an expression with sentiment to one with ′, or in other words ↔ ′, we
say this is an observed pragmatic function of codeswitching between Hindi and English (note that the order of the languages is important), if and only if
pr(h ↔",3.2 Hypotheses,[0],[0]
e ′;mr(T )) pr(h ′,3.2 Hypotheses,[0],[0]
↔ e ;mr(T )),3.2 Hypotheses,[0],[0]
> 1 (13),3.2 Hypotheses,[0],[0]
"All the statistics defined here are likelihoods; Equations 9, 12 and 13, in particular, state our hypothesis in the form of the Likelihood Ratio Test.",3.3 A Note on Statistical Significance,[0],[0]
"However, the true classes λ and are unknown; we predict the class labels using automatic language and sentiment detection techniques that have non-negligible errors.",3.3 A Note on Statistical Significance,[0],[0]
"Under such a situation, the likelihoods cannot be considered as true test statistics, and consequently, hypothesis testing cannot be done per se.",3.3 A Note on Statistical Significance,[0],[0]
"Nevertheless, we can use these as descriptive statistics and investigate the status of the aforementioned hypotheses.",3.3 A Note on Statistical Significance,[0],[0]
"We collected tweets with certain India-specific hashtags (Table 1) using the Twitter Search API (Twi, 2015b) over three months (December 2014 – February 2015).",4 Datasets,[0],[0]
"In this paper, we use tweets in Devanagari script Hindi (hd), and Roman script English (er), Hindi (hr) and Hi-En Mixed (mr).",4 Datasets,[0],[0]
"English and mixed tweets written in Devanagari are extremely rare (Bali et al., 2014) and we do not study them here.",4 Datasets,[0],[0]
"We filter out tweets labeled by the Twitter API (Twi, 2015a) as German, Spanish, French, Portuguese, Turkish, and all non-Roman script languages (except Hindi).
",4 Datasets,[0],[0]
We experiment on the following different corpora:,4 Datasets,[0],[0]
TAll: All tweets after filtering.,4 Datasets,[0],[0]
"This corpus contains 430,000 unique tweets posted by 1,25,396 unique users.
TBL: Tweets from users who are certainly Hi-En bilinguals, which are approximately 55% (240,000) of the tweets in TAll.",4 Datasets,[0],[0]
"We define a user to be a Hi-En bilingual if there is at least one mr tweet from the user, or if the user has tweeted at least once in Hindi (hd or hr) and once in English (er).",4 Datasets,[0],[0]
"Tspo,Tmov,Tpol,Teve: Topic-wise corpora for sports, movies, politics and events (Table 1).",4 Datasets,[0],[0]
TCS: Tweets with inter-sentential CS.,4 Datasets,[0],[0]
We define these as tweets containing at least one sequence of 5 contiguous Hindi words and one sequence of 5 contiguous English words.,4 Datasets,[0],[0]
"The corpus has 3,357 tweets.
SAC: 1000 monolingual tweets (er, hr, hd) and 260 mixed (mr) tweets manually annotated with sentiment and opinion labels.",4 Datasets,[0],[0]
"These were annotated by two linguists, both fluent Hi-En speakers.",4 Datasets,[0],[0]
"The annotators first checked whether the tweet is opinionated or⊗ and then identified polarity of the opinionated tweets (+, − or 0).",4 Datasets,[0],[0]
"Thus, the tweets are classified into the four classes in the set 3.",4 Datasets,[0],[0]
"If a tweet contains both opinion and ⊗, each fragment was individually annotated.",4 Datasets,[0],[0]
The inter-annotator agreement is 77.5% (κ = 0.59) for opinion annotation and 68.4% (κ = 0.64) over all four classes.,4 Datasets,[0],[0]
A third linguist independently corrected the disagreements.,4 Datasets,[0],[0]
"LLCTest: 141 er, 137 hr, and 241 mr tweets annotated by a Hi-En bilingual form the test set for the Language Labeling system (Sec. 5.1).",4 Datasets,[0],[0]
SAC and LLCTest can be downloaded and used for research purposes3.,4 Datasets,[0],[0]
"Note that apart from SAC and LLCTest, all corpora are subsets of TAll.",4 Datasets,[0],[0]
"For generalizability of our observations, it is important to ensure that the tweets in TAll come from a large number of users and the datasets do not over-represent a small set of users.",4 Datasets,[0],[0]
"In Figure 1, we plot the minimum fraction of users required (x-axis) to cover a certain percentage of the tweets in TAll (y-axis).",4 Datasets,[0],[0]
"Tweets from at least 10%, i.e., 12.5K users are needed to cover 50% of the corpus.",4 Datasets,[0],[0]
"As expected, we do observe a powerlaw-like distribution, where a few users contribute a large number of tweets, and a large number of users contribute a few tweets each.",4 Datasets,[0],[0]
"We believe that 12.5K users is sufficient to ensure an unbiased study.
",4 Datasets,[0],[0]
"Further, we classify the users into three specific groups (i) news channels, (ii) general users (having
3http://www.cnergres.iitkgp.ac.in/codemixing
≤ 10,000 followers), (iii) popular users or celebrities (having > 10,000 followers).",4 Datasets,[0],[0]
"Interestingly, for both TAll, and TBL corpora, we observe that around 98% of all users are general, and 96% of all tweets come from such users.",4 Datasets,[0],[0]
"Hence, most observations from these corpora are expected to be representative of the average online linguistic behavior of a Hi-En bilingual.",4 Datasets,[0],[0]
Fig. 2 diagrammatically summarizes our experimental method.,5 Method,[0],[0]
We identify the language used in each tweet before detecting opinion and sentiment.,5 Method,[0],[0]
"Tweets in Devanagari script are accurately detected by the Twitter API as Hindi tweets – we label these as hd, though a small fraction of them could also be md.",5.1 Language Labeling,[0],[0]
"To classify Roman script tweets as er, hr or mr, we use the system that performed best in the FIRE 2013 shared task for word-level language detection of Hi-En text (Gella et al., 2013).",5.1 Language Labeling,[0],[0]
This system uses character n-gram features with a Maximum Entropy model for labeling each input word with a language label (either English or Hindi).,5.1 Language Labeling,[0],[0]
"We design minor modifications to the system to improve its performance on Twitter data, which are omitted here due to paucity of space.",5.1 Language Labeling,[0],[0]
"Most of the existing research in opinion detection (Qadir, 2009; Brun, 2012; Rajkumar et al.,
2014) and sentiment analysis (Mohammad, 2012; Mohammad et al., 2013; Mittal et al., 2013; Rosenthal et al., 2015) focus on monolingual tweets and sentences.",5.2 Opinion and Sentiment Detection,[0],[0]
"Recently, there has been a couple of studies on sentiment detection of code-switched tweets (Vilares et al., 2015; Sharma et al., 2015b).",5.2 Opinion and Sentiment Detection,[0],[0]
"Sharma et al. (2015b) use Hindi SentiWordNet and normalization techniques to detect sentiment in HiEn CS tweets.
",5.2 Opinion and Sentiment Detection,[0],[0]
We propose a two-step classification model.,5.2 Opinion and Sentiment Detection,[0],[0]
We first identify whether a tweet is opinionated or nonopinionated (⊗).,5.2 Opinion and Sentiment Detection,[0],[0]
"If the tweet is opinionated, we further classify it according to its sentiment (+,− or 0).",5.2 Opinion and Sentiment Detection,[0],[0]
Fig. 2 shows the architecture of the proposed model.,5.2 Opinion and Sentiment Detection,[0],[0]
"Two-step classification was empirically found to be better than a single four-class classifier.
",5.2 Opinion and Sentiment Detection,[0],[0]
"We develop individual classifiers for each language class (er, hr, hd, mr) using an SVM with RBF kernel from Scikit-learn (Pedregosa et al.,
2011).",5.2 Opinion and Sentiment Detection,[0],[0]
We use the SAC dataset (Sec. 4) as training data and features as described in Sec. 5.3.,5.2 Opinion and Sentiment Detection,[0],[0]
"For opinion classification (opinion or ⊗), we propose a set of event-independent lexical features and Twitter-specific features.",5.3 Classifier Features,[0],[0]
(i) Subjective words: Expected to be present in opinion tweets.,5.3 Classifier Features,[0],[0]
We use lexicons from Volkova et al. (2013) for er and Bakliwal et al. (2012) for hd.,5.3 Classifier Features,[0],[0]
"We Romanize the hd lexicon for the hr classifiers (ii) Elongated words: Words with one character repeated more than two times, e.g. sooo, naaahhhhi (iii) Exclamations:",5.3 Classifier Features,[0],[0]
Presence of contiguous exclamation marks (iv) Emoticons4 (v) Question marks: Queries are generally nonopinionated.,5.3 Classifier Features,[0],[0]
"(vi) Wh-words: These are used to form questions (vii) Modal verbs: e.g. should, could, would, cud, shud (viii) Excess hashtags:",5.3 Classifier Features,[0],[0]
"Presence of more than two hashtags (ix) Intensifiers: Generally used to emphasize sentiment, e.g., we shouldn’t get too comfortable (x) Swear words5: Prevalent in opinionated tweets, e.g. that was a f ing no ball!!!!",5.3 Classifier Features,[0],[0]
#indvssa (xi) Hashtags:,5.3 Classifier Features,[0],[0]
"Hashtags might convey user sentiment (Barbosa et al., 2012).",5.3 Classifier Features,[0],[0]
We manually identify hashtags in our corpus that represent explicit opinion.,5.3 Classifier Features,[0],[0]
(xii) Domain lexicon:,5.3 Classifier Features,[0],[0]
"For hr, & hd category tweets, we construct sentiment lexicons from 1000 manually annotated tweets.",5.3 Classifier Features,[0],[0]
"Each word or phrase in this lexicon represents +, or −, or 0 sentiment.",5.3 Classifier Features,[0],[0]
(xiii) Twitter user mentions (xiv) Pronouns:,5.3 Classifier Features,[0],[0]
"Opinion is often in first person using pronouns like I and we.
",5.3 Classifier Features,[0],[0]
"For sentiment classification, we use emoticons, swear words, exclamation marks and elongated words as described above.",5.3 Classifier Features,[0],[0]
"We also use subjective words from various lexicons (Mohammad and Turney, 2013; Volkova et al., 2013; Bakliwal et al., 2012; Sharma et al., 2015a).",5.3 Classifier Features,[0],[0]
"Additionally, we use – (i) Sentiment words: From Hashtag Sentiment and Sentiment140 lexicons (Mohammad et al., 2013).",5.3 Classifier Features,[0],[0]
We also manually annotate hashtags from our dataset that represent sentiment.,5.3 Classifier Features,[0],[0]
(ii) Negation:,5.3 Classifier Features,[0],[0]
"A negated context is tweet segment that begins with a negation word and ends with a punctuation mark (Pang et al., 2002).",5.3 Classifier Features,[0],[0]
"The list of negation words are
4The list of emoticons was extracted from Wikipedia 5Swear word lexicons from noswearing.com, youswear.com
taken from Christopher Potts’ sentiment tutorial6.",5.3 Classifier Features,[0],[0]
"Themr opinion classifier uses the output from the er and hr classifiers as features (Fig. 2), along with an additional feature that represents whether the majority of the words in the tweet are Hindi or not.",5.3 Classifier Features,[0],[0]
A similar strategy is used for mr sentiment detection.,5.3 Classifier Features,[0],[0]
"We evaluated the language labeling system on the LLCTest corpus, on which the precision (recall) values were 0.93(0.91), 0.90(0.85) and 0.88(0.92) for er, hr and mr classes respectively.",5.4 Evaluation,[0],[0]
"The tweetlevel classification accuracy was 89.8%.
",5.4 Evaluation,[0],[0]
The opinion and sentiment classifiers were evaluated using 10-fold cross validation on the SAC dataset.,5.4 Evaluation,[0],[0]
Table 2 details the class-wise accuracy.,5.4 Evaluation,[0],[0]
"For comparison, we also reimplemented the dictionary and dependency-based method by Qadir (2009).",5.4 Evaluation,[0],[0]
"The accuracy of the opinion classifier on the er tweets was found to be 65.7%, 7% lower than our system.",5.4 Evaluation,[0],[0]
We also compared our mr sentiment classifier with that of Sharma et al. (2015b).,5.4 Evaluation,[0],[0]
"As their method performs two class sentiment detection (+ and −), we select such tweets from SAC.",5.4 Evaluation,[0],[0]
"Their system achieves an accuracy of 68.2%, which is 4% lower than the accuracy of our system.
",5.4 Evaluation,[0],[0]
"An analysis of the errors showed more false negatives (i.e., opinions labeled⊗) than false positives in opinion classification.",5.4 Evaluation,[0],[0]
"Sentiment misclassification is uniformly distributed.
",5.4 Evaluation,[0],[0]
Table 3 reports the accuracy of the opinion classifier for feature ablation experiments.,5.4 Evaluation,[0],[0]
"For all three language-script pairs, lexicon and non-word (emoticons, elongated words, hashtags, exclamation) features are the most effective, though all features have some positive contribution towards the final accuracy of opinion detection.",5.4 Evaluation,[0],[0]
"For hr and hd tweets, domain knowledge is significant, as shown by the 4% accuracy drop with removing the domain lexicon.
",5.4 Evaluation,[0],[0]
6http://sentiment.christopherpotts.net/lingstruc.html,5.4 Evaluation,[0],[0]
"In this section, we report our experiments on 430,000 unique tweets (TAll), and its various subsets as defined in Sec 4.",6 Experiments and Observations,[0],[0]
"First, we run the language detection system on the corpora.",6 Experiments and Observations,[0],[0]
Table 4 shows the language-wise distribution.,6 Experiments and Observations,[0],[0]
"We see that language preference varies by topic, which is not surprising.",6 Experiments and Observations,[0],[0]
"Due to paucity of space, the correlation between language usage and topic will not be discussed at length here, but we will highlight cases where the differences are striking.
",6 Experiments and Observations,[0],[0]
We apply the language-specific opinion and sentiment classifiers to tweets detected as the corresponding language class.,6 Experiments and Observations,[0],[0]
"In the following subsections, we empirically investigate the hypotheses.",6 Experiments and Observations,[0],[0]
"Table 5 shows pr(⊗|λσ;T ), pr(−|λσ;T ) and pr(−|λσ;T )/pr(+|λσ;T ) for TAll, TBL and two randomly selected topics – Movie and Politics.",6.1 Status of Hypotheses I and II,[0],[0]
"The statistics are fairly consistent over the corpora, with slight differences but similar trends in Tmov.
",6.1 Status of Hypotheses I and II,[0],[0]
"We need the first statistic in order to investigate Hypothesis I (Eqs. 6 and 7), and the two latter ones for verifying Hypothesis II (Eqs. 8 and 9).
",6.1 Status of Hypotheses I and II,[0],[0]
Contrary to Eqs.,6.1 Status of Hypotheses I and II,[0],[0]
"6 and 7, for all corpora except Tmov, we observe the following trend:
pr(⊗|hd;T ) > pr(⊗|hr;T ) ≥ pr(⊗|er;T )
",6.1 Status of Hypotheses I and II,[0],[0]
"In other words, hd is more commonly preferred for expressing non-opinions than hr and er.",6.1 Status of Hypotheses I and II,[0],[0]
"Hypothesis I is clearly untrue for these corpora, though due to the small differences between hr and er, we cannot claim that English is the preferred language for expressing opinions.",6.1 Status of Hypotheses I and II,[0],[0]
"A closer scrutiny of the corpora revealed that hd tweets mostly come from official sources (news channels, political parties, production houses) and celebrities, which are mostly factual.",6.1 Status of Hypotheses I and II,[0],[0]
hr tweets are from general users and show similar trends as English.,6.1 Status of Hypotheses I and II,[0],[0]
"Thus, in general, there seems to be no preferred language for expressing opinion by the Hi-En bilinguals on Twitter.
",6.1 Status of Hypotheses I and II,[0],[0]
"In the context of Hypothesis II, we see the general pattern (with some topic specific variations):
pr(−|hr;T ) >",6.1 Status of Hypotheses I and II,[0],[0]
pr(−|hd;T ),6.1 Status of Hypotheses I and II,[0],[0]
≥,6.1 Status of Hypotheses I and II,[0],[0]
"pr(−|er;T )
",6.1 Status of Hypotheses I and II,[0],[0]
"The pattern emerges even more strongly, when we look at pr(−|λσ;T )/pr(+|λσ;T ).",6.1 Status of Hypotheses I and II,[0],[0]
"The odds of expressing a negative opinion over positive opinion in Hindi is between 1.5 and 6 (Tmov exhibits a slightly different pattern but similar preference, Tpol shows a stronger preference towards Hindi for negative sentiment), whereas the same for English is between 0.1 and 0.6.",6.1 Status of Hypotheses I and II,[0],[0]
"In other words, English is more preferred
for expressing positive opinion, and Hindi for negative opinion.",6.1 Status of Hypotheses I and II,[0],[0]
These observations provide very strong evidence in favor of Hypothesis II.,6.1 Status of Hypotheses I and II,[0],[0]
"Recall that Hypothesis Ia and Hypothesis IIa are essentially same as Hypotheses I and II, but applied on mhr and mer fragments from the TCS corpus.
Table 6 reports the three statistics necessary for testing these hypotheses.",6.2 Status of Hypotheses Ia and IIa,[0],[0]
"pr(⊗|mer;TCS) is slightly greater than pr(⊗|mhr;TCS), which is what we would expect if Hypothesis Ia was true.",6.2 Status of Hypotheses Ia and IIa,[0],[0]
"However, since the difference is small, we view it as a trend rather than a proof of Hypothesis Ia.
The statistics clearly show that Hypothesis IIa holds true for TCS .",6.2 Status of Hypotheses Ia and IIa,[0],[0]
The fraction of negative sentiment in mhr is over 1.5 times higher than that of mer.,6.2 Status of Hypotheses Ia and IIa,[0],[0]
"Further, the odds of expressing a negative sentiment in Hindi over positive sentiment in Hindi in a code-switched tweet is 6.5 times higher than the same odds for English.",6.2 Status of Hypotheses Ia and IIa,[0],[0]
"Recall that using Eq. 13 (Sec. 3), we can estimate the preference, if any, for switching to a particular language while changing the sentiment.",6.3 Switching Functions,[0],[0]
"In particular, research in socio-linguistics has shown that users often switch between languages when they switch from non-opinion (⊗) to opinion ({+,−, 0}).",6.3 Switching Functions,[0],[0]
"This is called the Narrative-Evaluative function of CS (Sanchez, 1983).",6.3 Switching Functions,[0],[0]
This function appears in 46.1% of the tweets in TCS .,6.3 Switching Functions,[0],[0]
"We find that
pr(h{+,−, 0} ↔ e⊗;TCS) pr(h⊗",6.3 Switching Functions,[0],[0]
"↔ e{+,−, 0};TCS) = 0.86
which indicates that there is no preference for switching to Hindi (or English) while switching between opinion and non-opinion.",6.3 Switching Functions,[0],[0]
"This is also confirmed above in the context of hypotheses I and Ia. While switching between opinion and non-opinion in a tweet, users do switch language.",6.3 Switching Functions,[0],[0]
"However, we
observe no particular preference for the languages chosen for each part.
",6.3 Switching Functions,[0],[0]
"We also report two other pragmatic functions:
pr(h− ↔",6.3 Switching Functions,[0],[0]
"e{+, 0,⊗};TCS)",6.3 Switching Functions,[0],[0]
"pr(h{+, 0,⊗} ↔ e−;TCS) = 1.98
pr(h− ↔ e+;TCS) pr(h+↔ e−;TCS) = 10.27
The latter function is called polarity switch.",6.3 Switching Functions,[0],[0]
"The extremely high value for these ratios is an evidence for a strong preference towards switching language from English to Hindi while switching to negative sentiment (and switching to English when sentiment changes from negative to positive).
",6.3 Switching Functions,[0],[0]
"We also observe cases where there is a language switch, but no sentiment switch and hence, we cannot evaluate language preference using Eq. 13 (because = ′).",6.3 Switching Functions,[0],[0]
"In TCS , 15.3% of the tweets show Positive Reinforcement, where both fragments are of positive sentiment.",6.3 Switching Functions,[0],[0]
Negative Reinforcement is defined similarly and is seen in 8.7% of the tweets.,6.3 Switching Functions,[0],[0]
Other tweets in TCS likely have pragmatic functions that cannot be identified based on sentiment.,6.3 Switching Functions,[0],[0]
"Since there is evidence that the native language (Hindi, in this case) is preferred for swearing (De-
waele, 2004), we computed the fraction of tweets that contain swear words in each language class.",6.4 Language Preference for Swearing,[0],[0]
Fig. 3a shows the distribution across topics.,6.4 Language Preference for Swearing,[0],[0]
The languages hr and mr have a much higher fraction of abusive tweets than er and hd.,6.4 Language Preference for Swearing,[0],[0]
Fig.,6.4 Language Preference for Swearing,[0],[0]
3b shows the distribution of abusive mhr and mer fragments for tweets in TCS .,6.4 Language Preference for Swearing,[0],[0]
"Interestingly, over 90% of the swear words occur in mhr.",6.4 Language Preference for Swearing,[0],[0]
Both distributions strongly suggest a preference for swearing in Hindi.,6.4 Language Preference for Swearing,[0],[0]
"In this paper, through a large scale empirical study of nearly half a million tweets, we tried to answer a fundamental question regarding multilingualism, namely, is there a preferred language for expression of sentiment.",7 Conclusion,[0],[0]
We also looked at some of the pragmatic functions of code-switching.,7 Conclusion,[0],[0]
"Our results indicate a strong preference for using Hindi, L1 for the users from whom these tweets come, for expressing negative sentiment, including swearing.",7 Conclusion,[0],[0]
"However, we do not observe any particular preference towards Hindi for expressing opinions.
",7 Conclusion,[0],[0]
"Previous linguistic studies (Dewaele, 2004; Dewaele, 2010) have already shown a preference for L1 for expressing emotion and swearing.",7 Conclusion,[0],[0]
"However, we observe that for expressing positive emotion, English (which would be L2) is the language of preference.",7 Conclusion,[0],[0]
This raises some intriguing socio-linguistic questions.,7 Conclusion,[0],[0]
"Is it the case that English being the language of aspiration in India, it is preferred for positive expression?",7 Conclusion,[0],[0]
"Or is it because Hindi is specifically preferred for swearing and therefore, is the language of preference for negative emotion?",7 Conclusion,[0],[0]
"How do such preferences vary across topics, users and other multilingual communities?",7 Conclusion,[0],[0]
How representative of the society is this kind of social media study?,7 Conclusion,[0],[0]
"We plan to explore some of these questions in the future.
",7 Conclusion,[0],[0]
"Our study also indicates that inferences drawn on multilingual societies by analyzing data in just one language (usually English), which has been the norm so far, are likely to be incorrect.",7 Conclusion,[0],[0]
Koustav Rudra was supported by a fellowship from Tata Consultancy Services.,Acknowledgement,[0],[0]
"Linguistic research on multilingual societies has indicated that there is usually a preferred language for expression of emotion and sentiment (Dewaele, 2010).",abstractText,[0],[0]
Paucity of data has limited such studies to participant interviews and speech transcriptions from small groups of speakers.,abstractText,[0],[0]
"In this paper, we report a study on 430,000 unique tweets from Indian users, specifically Hindi-English bilinguals, to understand the language of preference, if any, for expressing opinion and sentiment.",abstractText,[0],[0]
"To this end, we develop classifiers for opinion detection in these languages, and further classifying opinionated tweets into positive, negative and neutral sentiments.",abstractText,[0],[0]
"Our study indicates that Hindi (i.e., the native language) is preferred over English for expression of negative opinion and swearing.",abstractText,[0],[0]
"As an aside, we explore some common pragmatic functions of codeswitching through sentiment detection.",abstractText,[0],[0]
Understanding Language Preference for Expression of Opinion and Sentiment: What do Hindi-English Speakers do on Twitter?,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1108–1118, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Negation is a complex phenomenon present in all human languages, allowing for the uniquely human capacities of denial, contradiction, misrepresentation, lying, and irony (Horn and Wansing, 2015).",1 Introduction,[0],[0]
"Despite negation always being marked—in the absence of a negation cue, statements are positive— acquiring and understanding sentences that contain negation is more challenging than those that do not.",1 Introduction,[0],[0]
"Children acquire negation after learning to communicate (Nordmeyer and Frank, 2013), and adults take longer to process negated statements than positive ones (Clark and Chase, 1972).
",1 Introduction,[0],[0]
"In any given language, humans communicate in positive terms most of the time, and use negation to express something unusual or an exception (Horn, 1989).",1 Introduction,[0],[0]
"Albeit most sentences are affirmative, negation is ubiquitous (Morante and Sporleder, 2012):",1 Introduction,[0],[0]
"In scientific papers, 13.76% of statements contain a negation (Szarvas et al., 2008); in product reviews, 19% (Councill et al., 2010); and in Conan Doyle stories, 22.23% (Morante and Daelemans, 2012).",1 Introduction,[0],[0]
"In
OntoNotes (Hovy et al., 2006), 10.15% of statements contain a verb negated with not, n’t or never.
",1 Introduction,[0],[0]
"From a theoretical point of view, it is accepted that negation conveys positive meaning (Rooth, 1992; Huddleston and Pullum, 2002).",1 Introduction,[0],[0]
"For example, when reading (1) John didn’t order the right parts, humans intuitively understand that (1a) John ordered something, or more specifically, (1b) John ordered the wrong parts.",1 Introduction,[0],[0]
"Interpretation (1a) can be obtained after determining that n’t does not negate verb order, but its THEME, i.e., the right parts.",1 Introduction,[0],[0]
"Interpretation (1b) can be obtained after determining that n’t is actually negating right, an adjective modifying parts.
",1 Introduction,[0],[0]
"Determining which words are intended to be negated—identifying the foci of negation, thereby revealing positive interpretations—is challenging.",1 Introduction,[0],[0]
"First, as exemplified in (1a, 1b), there is a granularity continuum yielding interpretations that entail each other, e.g., (1b) entails (1a).",1 Introduction,[0],[0]
"Second, a single negation often yields several positive interpretations, e.g., from (2) John doesn’t eat meat, we can extract that (2a) John eats something other than meat and (2b)",1 Introduction,[0],[0]
"Some people eat meat, but not John.
",1 Introduction,[0],[0]
This paper presents a methodology to extract positive interpretations from verbal negation.,1 Introduction,[0],[0]
"The main contributions are: (1) deterministic procedure to generate potential interpretations by manipulating syntactic dependencies; (2) analysis showing that dependencies yield finer-grained interpretations and better results than previous work using semantic roles; (3) a corpus of negations and their positive interpretations;1 and (4) experimental results with gold-standard and predicted linguistic information.
",1 Introduction,[0],[0]
"1Available at http://www.cse.unt.edu/˜blanco/
1108",1 Introduction,[0],[0]
"Negation is well-understood in grammars, which detail the valid ways to form a negation (Quirk et al., 2000; van der Wouden, 1997).","2 Terminology, Scope and Focus",[0],[0]
"Negation can be expressed by verbs (e.g., avoid running), nouns (e.g., the absence of evidence), adjectives (e.g., it is pointless), adverbs (e.g., I never tried Persian food before), prepositions (e.g., you can exchange it without a problem), determiners (e.g., the new law has no direct implications), pronouns (e.g., nobody will keep election promises), and others.","2 Terminology, Scope and Focus",[0],[0]
"In this paper, we focus on verbal negation, i.e., when the negation mark—usually an adverb such as never and not—is grammatically associated with a verb.","2 Terminology, Scope and Focus",[0],[0]
Positive Interpretations.,"2 Terminology, Scope and Focus",[0],[0]
"In philosophy and linguistics, it is generally accepted that negation conveys positive meaning (Horn, 1989).","2 Terminology, Scope and Focus",[0],[0]
"This positive meaning ranges from implicatures, i.e., what is suggested in an utterance even though neither expressed nor strictly implied (Blackburn, 2008), to entailments.","2 Terminology, Scope and Focus",[0],[0]
"Other terms used in the literature include implied meanings (Mitkov, 2005), implied alternatives (Rooth, 1985) and semantically similars (Agirre et al., 2013).","2 Terminology, Scope and Focus",[0],[0]
"We do not strictly fit into any of this terminology, we reveal positive interpretations as intuitively done by humans when reading text.","2 Terminology, Scope and Focus",[0],[0]
"From a theoretical perspective, it is accepted that negation has scope and focus, and that the focus— not just the scope—yields positive interpretations (Horn, 1989; Rooth, 1992; Taglicht, 1984).",2.1 Scope and Focus,[0],[0]
"Scope is “the part of the meaning that is negated” and focus “the part of the scope that is most prominently or explicitly negated” (Huddleston and Pullum, 2002).
",2.1 Scope and Focus,[0],[0]
Consider the following statement in the context of the recent refugee crisis: (2) Mr. Haile was not looking for heaven in Europe.,2.1 Scope and Focus,[0],[0]
"By definition, scope refers to “all elements whose individual falsity would make the negated statement strictly true”, and focus is “the element of the scope that is intended to be interpreted as false to make the overall negative true” (Huddleston and Pullum, 2002).",2.1 Scope and Focus,[0],[0]
"The falsity of any of the truth conditions below makes statement (2) true, thus the scope of the negation is (2a–2d): 2a.",2.1 Scope and Focus,[0],[0]
"Somebody was looking for something some-
where.",2.1 Scope and Focus,[0],[0]
"[verb looking]
2b.",2.1 Scope and Focus,[0],[0]
Mr. Haile was looking for something somewhere.,2.1 Scope and Focus,[0],[0]
"[AGENT of looking, Mr. Haile] 2c.",2.1 Scope and Focus,[0],[0]
Somebody was looking for heaven somewhere.,2.1 Scope and Focus,[0],[0]
"[THEME of looking, heaven] 2d.",2.1 Scope and Focus,[0],[0]
Somebody was looking for something in Europe.,2.1 Scope and Focus,[0],[0]
"[LOCATION of looking, in Europe]
Determining the focus is almost always more challenging than the scope.",2.1 Scope and Focus,[0],[0]
"The challenge relies on determining which of the truth conditions (2a–2d) is intended to be interpreted as false to make the negated statement true: all of them qualify, but some are more likely.",2.1 Scope and Focus,[0],[0]
"A natural reading of statement (2) suggests that Mr. Haile was looking for something (a regular life, a job, etc.) in Europe, but not heaven.",2.1 Scope and Focus,[0],[0]
"Determining that the focus is heaven, i.e., that everything in statement (2) is positive except the THEME of looking, is the key to reveal the intended positive interpretation.",2.1 Scope and Focus,[0],[0]
"Note that scope on its own does not identify positive interpretations, and other foci yield unlikely positive interpretations, e.g., Mr. Haile was looking for heaven somewhere, but not in Europe.
",2.1 Scope and Focus,[0],[0]
"It is worth noting that while scope is defined from a logical standpoint, in most negations there are several possible foci and corresponding positive interpretations.",2.1 Scope and Focus,[0],[0]
"For example, given (3) Most jobs now don’t last for decades, the following are valid positive interpretations: (3a) Few jobs now last for decades, (3b) Most jobs in the past lasted for decades, and (3c) Most jobs now last for a few years.",2.1 Scope and Focus,[0],[0]
Granularity of Focus.,2.1 Scope and Focus,[0],[0]
The definition of focus does not provide guidelines about identifying the element of the scope that is the focus.,2.1 Scope and Focus,[0],[0]
"The larger the focus, the more generic the corresponding positive interpretation; and the smaller the focus, the more specific the corresponding positive interpretation.",2.1 Scope and Focus,[0],[0]
Let us consider statement (3) again.,2.1 Scope and Focus,[0],[0]
"A possible focus is Most jobs, yielding the positive interpretation Something now lasts for decades, but not most jobs.",2.1 Scope and Focus,[0],[0]
"Another possible focus is Most, yielding the interpretation Few (not most) jobs now last for decades.",2.1 Scope and Focus,[0],[0]
"We argue that the latter is preferable, as it yields a more specific interpretation and it entails the former: if some jobs last for decades, then something lasts for decades, but not the other way around.
",2.1 Scope and Focus,[0],[0]
"We use the term coarse-grained focus to refer to foci that include all tokens belonging to an argument of a verb (e.g., Most Jobs above), and fine-grained focus to refer to foci that do not (e.g., Most above).",2.1 Scope and Focus,[0],[0]
"Within computational linguistics, approaches to process negation are shallow, or target scope and focus detection.",3 Previous Work,[0],[0]
"Popular semantic representations such as semantic roles (Palmer et al., 2005; Baker et al., 1998) or AMR (Banarescu et al., 2013) do not reveal the positive interpretations we target in this paper.",3 Previous Work,[0],[0]
Shallow approaches are usually application-specific.,3 Previous Work,[0],[0]
"In sentiment and opinion analysis, negation has been reduced to marking as negated all words between a negation cue and the first punctuation mark (Pang et al., 2002), or within a five-word window of a negation cue (Hu and Liu, 2004).",3 Previous Work,[0],[0]
The examples throughout this paper show that these techniques are insufficient to reveal implicit positive interpretations.,3 Previous Work,[0],[0]
"Scope of negation detection has received a lot of attention, mostly using two corpora: BioScope in the medical domain (Szarvas et al., 2008) and CDSCO (Morante and Daelemans, 2012).",3.1 Scope Annotations and Detection,[0],[0]
BioScope annotates negation cues and linguistic scopes exclusively in biomedical texts.,3.1 Scope Annotations and Detection,[0],[0]
"CD-SCO annotates negation cues, scopes, and negated events or properties in selected Conan Doyle stories.
",3.1 Scope Annotations and Detection,[0],[0]
"There have been several supervised proposals to detect the scope of negation using BioScope and CD-SCO (Özgür and Radev, 2009; Øvrelid et al., 2010).",3.1 Scope Annotations and Detection,[0],[0]
"Automatic approaches are mature (AbuJbara and Radev, 2012): F-scores are 0.96 for negation cue detection, and 0.89 for negation cue and scope detection (Velldal et al., 2012; Li et al., 2010).",3.1 Scope Annotations and Detection,[0],[0]
"Fancellu et al. (2016) present the best results to date using CD-SCO, and analyze the main sources of errors.",3.1 Scope Annotations and Detection,[0],[0]
"Outside BioScope and CD-SCO, Reitan et al. (2015) present a negation scope detector for tweets, and show that it improves sentiment analysis.",3.1 Scope Annotations and Detection,[0],[0]
"As shown in Section 2, scope detection is insufficient to reveal positive interpretations from negation.",3.1 Scope Annotations and Detection,[0],[0]
"While focus of negation has been studied for decades in philosophy and linguistics (Section 2), corpora and automated tools are scarce.",3.2 Focus Annotation and Detection,[0],[0]
"Blanco and Moldovan (2011) annotate focus of negation in the 3,993 negations marked with ARGM-NEG semantic role in PropBank (Palmer et al., 2005).",3.2 Focus Annotation and Detection,[0],[0]
"Their an-
notations, PB-FOC, were used in the *SEM-2012 Shared Task (Morante and Blanco, 2012).",3.2 Focus Annotation and Detection,[0],[0]
Their guidelines require annotators to choose as focus the semantic role that “is most prominently negated” or the verb.,3.2 Focus Annotation and Detection,[0],[0]
"If several roles may be the focus, they prioritize “the one that yields the most meaningful implicit [positive] information”, but do not specify what most meaningful means.",3.2 Focus Annotation and Detection,[0],[0]
Their approach has 2 limitations.,3.2 Focus Annotation and Detection,[0],[0]
"First, because they select one focus per negation, they only extract one positive interpretation per negation.",3.2 Focus Annotation and Detection,[0],[0]
"Second, because they select as focus a semantic role, they only consider coarsegrained foci.",3.2 Focus Annotation and Detection,[0],[0]
Consider again statement (3) from Section 2.1.,3.2 Focus Annotation and Detection,[0],[0]
"By design, their approach is limited to extract a single interpretation even though interpretations (3a–3c) are valid.",3.2 Focus Annotation and Detection,[0],[0]
"Similarly, their approach is limited to select as focus Most jobs—all tokens belonging to a semantic role—although Most yields a “more meaningful” interpretation: Something now lasts for decades (generic, worse) vs. Few jobs now last for decades (specific, better).
",3.2 Focus Annotation and Detection,[0],[0]
Blanco and Sarabi (2016) present a complimentary approach to extract and score several positive interpretations from a single verbal negation.,3.2 Focus Annotation and Detection,[0],[0]
Their methodology is grounded on semantic roles and does not consider fine-grained foci.,3.2 Focus Annotation and Detection,[0],[0]
"In this paper, we improve upon their work: we extract both coarse- and fine-grained interpretations, and also extract several interpretations from one negation.
",3.2 Focus Annotation and Detection,[0],[0]
Anand and Martell (2012) reannotate PB-FOC and argue that positive interpretations arising from scalar implicatures and neg-raising predicates should be separated from those arising from focus detection.,3.2 Focus Annotation and Detection,[0],[0]
They argue that 27.4% of negations with a focus annotated in PB-FOC do not have one.,3.2 Focus Annotation and Detection,[0],[0]
"In this paper, we are not concerned about annotating foci per se, but about extracting positive interpretations from negation, as intuitively done by humans.
",3.2 Focus Annotation and Detection,[0],[0]
Automatic systems to detect the focus of negation yield modest results.,3.2 Focus Annotation and Detection,[0],[0]
Blanco and Moldovan (2011) obtain an accuracy of 65.5 using supervised learning and features derived from gold-standard linguistic information.,3.2 Focus Annotation and Detection,[0],[0]
"With predicted linguistic information, Rosenberg and Bergler (2012) report an Fmeasure of 58.4 using 4 linguistically sound heuristics, and Zou et al. (2014) an F-measure of 65.62 using contextual discourse information.",3.2 Focus Annotation and Detection,[0],[0]
"Blanco and Sarabi (2016) obtain Pearson correlation of 0.642
ranking coarse-grained interpretations.",3.2 Focus Annotation and Detection,[0],[0]
"Unlike the work presented here, none of these systems extract fine-grained interpretations from a single negation.",3.2 Focus Annotation and Detection,[0],[0]
Our goal is to create a corpus of negations and their positive interpretations.,4 Corpus Creation,[0],[0]
We put a strong emphasis on automation and simplicity.,4 Corpus Creation,[0],[0]
"First, we deterministically generate potential positive interpretations from verbal negations by manipulating syntactic dependencies (Section 4.1).",4 Corpus Creation,[0],[0]
"Second, we ask annotators to score potential positive interpretations (Section 4.2).",4 Corpus Creation,[0],[0]
Positive interpretations and their scores are later used to learn models to rank potential interpretations automatically (Section 6).,4 Corpus Creation,[0],[0]
Generating potential interpretations deterministically prior to scoring them proved very beneficial.,4 Corpus Creation,[0],[0]
"After pilot experiments, it became clear that asking annotators to propose positive interpretations complicates the annotation effort (lower agreements) as well as learning.
",4 Corpus Creation,[0],[0]
"We decided to work on top of OntoNotes (Hovy et al., 2006)2 instead of plain text or other corpora for several reasons.",4 Corpus Creation,[0],[0]
"First, OntoNotes includes gold linguistic annotations such as part-of-speech tags, parse trees and semantic roles.",4 Corpus Creation,[0],[0]
"Second, unlike BioScope, CD-SCO and PB-FOC (Section 3.2), OntoNotes includes sentences from several genres, e.g., newswire, broadcast news and conversations, magazines, the web.",4 Corpus Creation,[0],[0]
We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP,4 Corpus Creation,[0],[0]
"(Manning et al., 2014).",4 Corpus Creation,[0],[0]
"OntoNotes contains 63,918 sentences.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
Annotating all positive interpretations from all negations is outside the scope of this paper.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Instead, we target selected representative negations.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
Selecting Negations.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
We first select all verbal negations by retrieving all tokens whose syntactic head is a verb and dependency type neg.3,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Then, we discard negations from sentences that contain two negations, conditionals, commas or questions.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Finally, we dis-
2We use the CoNLL-2011 Shared Task distribution (Pradhan et al., 2011), http://conll.cemantix.org/2011/
3The Stanford manual describes and exemplifies all syntactic dependencies (de Marneffe and Manning, 2008).
card negations if the negated verb is to be or it does not have a subject (dependency nsubj or nsubjpass).",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
Converting Negated Statements into their positive counterparts.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"We apply 3 steps inspired after the grammatical rules to form negation detailed by Huddleston and Pullum (2002, Ch. 9):
1.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
Remove the negation mark by deleting the token with syntactic dependency neg.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
2.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Remove auxiliaries, expand contractions, and fix third-person singular and past tense.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"For example (before: after), doesn’t go: goes, didn’t go: went, won’t go: will go.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"We loop through the tokens whose head is the negated verb with dependency aux, and use a list of irregular verbs and grammar rules to convert to thirdperson singular and past tense.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
3. Rewrite negatively-oriented polarity-sensitive items.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"For example (before: after), anyone: someone, any longer: still, yet: already.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
at all: somewhat.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"We use the correspondences between negatively-oriented and positively-oriented polarity-sensitive items by (Huddleston and Pullum, 2002, pp. 831).",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
Selecting Relevant tokens.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
Verbal negation often occurs in multi-clause sentences.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"In order to identify the relevant (syntactically negated) eventuality, we simplify the original statement by including only the negated verb and all tokens that are dependents of the verb, i.e., tokens reachable from the negated verb traversing dependencies.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"For example, from Individuals familiar with the Justice Department’s policy said that Justice officials hadn’t any knowledge of the IRS’s actions in the last week, after getting the positive counterpart and selecting relevant tokens, we obtain Justice officials had some knowledge of the IRS’s actions in the last week.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
Generating Interpretations.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Given the simplified positive counterpart, generating all combinations of tokens as potential foci would result in 2t potential positive interpretations for t tokens.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"To avoid a brute-force approach that generates many nonsensical potential interpretations, we define a procedure grounded on syntactic dependencies.
",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
The main idea is to run a modified breadth-first traversal of the dependency tree to select subtrees that are potential foci.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"We start the traversal from the negated verb and stop it at depth 3, selecting as potential foci the subtrees rooted at all tokens except
those whose syntactic dependency is aux, auxpass or punct (auxiliary, passive auxiliary and punctuation).",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Additionally, we discard potential foci that consist only of (1) the determiners the, a and an, or (2) a single token with part-of-speech tag TO, CC, UH, POS, XX, IN, WP or dependency relation prt.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
These rules were defined after manually observing several examples and concluding that the corresponding positive interpretation was useless.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"For example, from the negated statement And our credit standards haven’t changed one iota, we avoid generating the useless potential interpretation Our credit standards X changed one iota, but not have changed.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"(focus would be have, with dependency aux).",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Similarly, from It is not supported by the text or history of the Constitution, we avoid generating potential interpretation It is supported by X text or history of the Constitution, but not by the text or history of the Constitution (focus would be the); and from You don’t want to get yourself too upset about these things, potential interpretation You want X get yourself too upset about these things, but not to get (focus would be to, with part-of-speech tag TO).
",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Once potential foci are selected, we generate positive interpretations by rewriting each focus with “someone/some people/something/etc.”",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
and appending “but not text of focus” at the end.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Additionally, if the first token of the focus is a preposition, we include it to improve readability, e.g., didn’t
leave [by noon]: left by sometime, but not by noon.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"Note that potential interpretations obtained from foci that are direct syntactic dependents of the negated verb are coarse-grained interpretations, and the rest are fine-grained interpretations.",4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
Table 1 exemplifies the procedure step by step.,4.1 Manipulating Syntactic Dependencies to Generate Potential Positive Interpretations,[0],[0]
"After generating potential positive interpretations automatically, we asked annotators to score them.",4.2 Scoring Potential Positive Interpretations,[0],[0]
"Annotators had access to the original negated sentence, the previous and next sentence as context, and one potential positive interpretation at a time.",4.2 Scoring Potential Positive Interpretations,[0],[0]
"The interface asked Given the three sentences [previous sentence, negated sentence and next sentence] above, do you think the statement [positive interpretation] below is true?",4.2 Scoring Potential Positive Interpretations,[0],[0]
"Annotators were forced to answer with a score from 0 to 5, where 0 means absolutely disagree and 5 means absolutely agree.",4.2 Scoring Potential Positive Interpretations,[0],[0]
We did not provide descriptions for intermediate scores or use categorical labels.,4.2 Scoring Potential Positive Interpretations,[0],[0]
This simple guidelines were sufficient to reliably score plausible positive interpretations automatically generated (Section 5).,4.2 Scoring Potential Positive Interpretations,[0],[0]
The procedure described in Section 4.1 generates 9729 potential positive interpretations (5865 coarsegrained and 3864 fine-grained) from 1671 verbal negations.,5 Corpus Analysis,[0],[0]
"Out of all these potential positive interpretations, we annotate 1700 (1008 coarse- and 692
fine-grained).",5 Corpus Analysis,[0],[0]
"Overall, the mean score is 3.20, and the standard deviation is 1.66.",5 Corpus Analysis,[0],[0]
"Table 2 shows basic statistics for potential foci, where dependency indicates the dependency from the potential focus to a token outside the potential focus.",5 Corpus Analysis,[0],[0]
"Most foci are nsubj, dobj and pobj, and the mean scores and standard deviation are similar for most dependencies.
",5 Corpus Analysis,[0],[0]
Annotation Quality.,5 Corpus Analysis,[0],[0]
"In order to ensure annotation quality, we calculated Pearson correlation.",5 Corpus Analysis,[0],[0]
"Kappa and other measures designed for categorical labels are ill-suited for our annotations, since not all disagreements between numeric scores are the same, e.g., 4 vs. 5 should be counted as higher agreement, than 1 vs. 5.",5 Corpus Analysis,[0],[0]
Overall Pearson correlation was 0.75.,5 Corpus Analysis,[0],[0]
"Table 3 presents 2 statements that contain verbal negation, the list of positive interpretations automatically generated and the annotated scores.
",5.1 Annotation Examples,[0],[0]
"Example (1) is a simple negated clause, yet we generate 7 potential positive interpretations and 3 of them receive high scores (4 or 5).",5.1 Annotation Examples,[0],[0]
"Given You’re not paying me for my overtime work and the previous statement, it is reasonable to believe that the author is in an employee-employer relationship, and the employer is not fair to the employee.",5.1 Annotation Examples,[0],[0]
"Interpretations 1.1, 1.4 and 1.6 are implicit positive interpretations intuitively understood by humans when reading the original negated statement.",5.1 Annotation Examples,[0],[0]
"Namely, Interpretation 1.1: You (the employer) are nickel-and-diming me for my overtime work (focus is paying), Interpretation 1.4:",5.1 Annotation Examples,[0],[0]
"You (the employer) are paying me for something (focus is my overtime work), and Interpretation 1.6: You (the employer) are paying me for my regular work (focus is overtime).",5.1 Annotation Examples,[0],[0]
"These interpretations show the benefits of fine-grained interpretations: Interpretation 1.6 is a refinement of Interpretation 1.4, and the former is more desirable than the latter as it reveals more specific positive knowledge.",5.1 Annotation Examples,[0],[0]
"The remaining interpretations are legible, but do not make sense given the negated statement, e.g., interpretation 1.2:",5.1 Annotation Examples,[0],[0]
"Somebody (but not the employer) pays me for my overtime (focus is You).
",5.1 Annotation Examples,[0],[0]
"Example (2) is also a simple negated clause, and 4 out of 5 interpretations receive high scores, capturing valid positive meaning.",5.1 Annotation Examples,[0],[0]
"Specifically, Interpreta-
tion 2.1: Those concerns are avoided in public (focus is expressed), Interpretation 2.2:",5.1 Annotation Examples,[0],[0]
"Something is expressed in public (focus is Those concerns), Interpretation 2.4: Some concerns (but not problematic or secret concerns) are expressed in public (focus is Those), and Interpretation 2.5: Those concerns are expressed in private (focus is in public).",5.1 Annotation Examples,[0],[0]
The procedure presented in Section 4.1 is not the first to generate potential positive interpretations from negation (Section 3.2).,5.2 Syntactic Dependencies vs. Semantic Roles,[0],[0]
"Our approach has 2 advantages with respect to those grounded on semantic roles (Blanco and Sarabi, 2016): (1) it generates both coarse- and fine-grained interpretations, and (2) learning to score interpretations is easier because state-of-the-art tools extract dependencies more reliably than semantic roles.
",5.2 Syntactic Dependencies vs. Semantic Roles,[0],[0]
"To support claim (1), we compare the interpretations generated with our procedure and previous work using semantic roles.",5.2 Syntactic Dependencies vs. Semantic Roles,[0],[0]
96.12% of interpretations generated using roles are also generated using syntactic dependencies.,5.2 Syntactic Dependencies vs. Semantic Roles,[0],[0]
"Also, using dependencies allow us to generate 67.9% of additional (finegrained) interpretations not obtainable with roles.
",5.2 Syntactic Dependencies vs. Semantic Roles,[0],[0]
"To support claim (2), we compare interpretations generated with gold and predicted linguistic information (roles or dependencies).",5.2 Syntactic Dependencies vs. Semantic Roles,[0],[0]
"The overlap with semantic roles is 70.1%, and with syntactic dependencies, 92.8%.",5.2 Syntactic Dependencies vs. Semantic Roles,[0],[0]
Syntactic dependencies are thus better in a realistic scenario because they allow us to automatically generate (and score) most interpretations.,5.2 Syntactic Dependencies vs. Semantic Roles,[0],[0]
"Positive Interpretations
We follow a standard supervised machine learning approach.",6 Supervised Learning to Score Potential,[0],[0]
"The 1,700 potential positive interpretations along with their scores become instances, and we divide them into training (80%) and test splits (20%) making sure that all interpretations generated from a sentence are assigned to either the training or test splits.",6 Supervised Learning to Score Potential,[0],[0]
"Note that splitting instances randomly would not be sound: training with some interpretations generated from a negation, and testing with the rest of interpretations generated from the same negation would be an unfair evaluation.
",6 Supervised Learning to Score Potential,[0],[0]
"We train a Support Vector Machine for regression with RBF kernel using scikit-learn (Pedregosa et al., 2011), which in turn uses LIBSVM (Chang and Lin, 2011).",6 Supervised Learning to Score Potential,[0],[0]
"SVM parameters (C and γ) were tuned using 10-fold cross-validation with the training set, and results are calculated using the test set.",6 Supervised Learning to Score Potential,[0],[0]
Table 4 presents the full feature set.,6.1 Feature Selection,[0],[0]
"Features are relatively simple and characterize the verbal negation from which a potential interpretation was generated, as well as the interpretation per se, i.e., the dependency subgraph chosen as potential focus.
",6.1 Feature Selection,[0],[0]
"Basic features account for the negation mark, the negation verb (word form and part-of-speech tag) and a binary flag indicating whether we are scoring a coarse- or fine-grained interpretation.
",6.1 Feature Selection,[0],[0]
"Path features are derived from the syntactic path
between the subgraph selected as focus and the verb.",6.1 Feature Selection,[0],[0]
"We include the actual path (concatenation of dependencies and up/down symbols), and the modified path using part-of-speech tags.",6.1 Feature Selection,[0],[0]
"Additionally, we also include the last dependency and part-of-speech tag, i.e., the ones closest to the verb in the path.
",6.1 Feature Selection,[0],[0]
Focus features characterize the dependency subgraph chosen as focus to generate the potential interpretation.,6.1 Feature Selection,[0],[0]
"Specifically, we include the number of tokens, word form and part-of-speech tags of the first and last tokens, and whether the focus occurs before or after the verb.",6.1 Feature Selection,[0],[0]
"We also include features derived form the head of the focus, which we define as the token whose syntactic head is outside the focus.",6.1 Feature Selection,[0],[0]
"We include the word form and part-of-speech of the focus head, as well as its the dependency.",6.1 Feature Selection,[0],[0]
We report results obtained with several combinations of features in Table 5.,7 Experiments and Results,[0],[0]
"We detail results obtained with features extracted from gold-standard and predicted linguistic annotations (part-of-speech tags and syntactic dependencies) as annotated in the gold and auto files from the CoNLL-2011 Shared Task release of OntoNotes (Pradhan et al., 2011).",7 Experiments and Results,[0],[0]
"All models are trained with gold-standard linguistic annotations, and tested with either gold-standard or predicted linguistic annotations.",7 Experiments and Results,[0],[0]
Testing with gold-standard POS tags and syntactic dependencies.,7 Experiments and Results,[0],[0]
"Training with the word form of the negation mark is virtually useless, it yields a Pearson correlation of −0.109.",7 Experiments and Results,[0],[0]
"Basic features (negation mark, verb and flag indicating coarseor fine-grained interpretation) are also ineffective to score potential interpretations (Pearson: 0.033).",7 Experiments and Results,[0],[0]
"Including features derived from the syntactic path yields higher correlation, 0.474, even though these features only capture the syntactic relationship be-
tween the focus from which the interpretation was generated and the verb.",7 Experiments and Results,[0],[0]
"Finally, adding focus features yields the best results (Pearson: 0.53, +11.8%).",7 Experiments and Results,[0],[0]
Testing with predicted POS tags and syntactic dependencies.,7 Experiments and Results,[0],[0]
"We selected 20% of positive interpretations in our corpus as test instances, totalling 379 interpretations (Section 6).",7 Experiments and Results,[0],[0]
"When executing the procedure to generate potential interpretations (Section 4.1) with predicted linguistic information, however, we are unable to generate all of them due to incorrect and missing syntactic dependencies.",7 Experiments and Results,[0],[0]
"Specifically, 352 of the 379 interpretations are generated (92.8%).",7 Experiments and Results,[0],[0]
"While we do not generate 7.2% of instances, this percentage is substantially lower than previous work grounded on semantic roles (Section 5.2).
",7 Experiments and Results,[0],[0]
Pearson correlations with predicted linguistic information are calculated using the 352 instances that were also generated with gold dependencies (and thus assigned a score during the manual annotations).,7 Experiments and Results,[0],[0]
Correlations are slightly higher and follow a similar trend than the correlations obtained with gold-standard linguistic information.,7 Experiments and Results,[0],[0]
"These results should be taken with a grain of salt: the test instances are not exactly the same, and the 352 test instances in this scenario are presumably easier to score than the remainder 27, as dependencies were predicted correctly.",7 Experiments and Results,[0],[0]
Humans intuitively extract positive meaning from negation when reading text.,8 Conclusions,[0],[0]
"This paper presents an automated procedure to generate potential positive interpretations from verbal negation, and score them according to their likelihood.",8 Conclusions,[0],[0]
"Our procedure is grounded on syntactic dependencies, allowing us to extract fine-grained interpretations beyond semantic roles (67.9% additional interpretations).",8 Conclusions,[0],[0]
"Additionally, because dependencies are extracted automatically more reliably than semantic roles, we generate 92.8% of all potential interpretations when using predicted linguistic information, as opposed to 70.1% with semantic roles.
",8 Conclusions,[0],[0]
"On average, we generate 6.4 potential interpretations per verbal negation (coarse-grained: 3.8, finegrained: 2.6).",8 Conclusions,[0],[0]
Manual annotations show that potential interpretations are deemed likely.,8 Conclusions,[0],[0]
"The mean
score is 3.20 (out of 5.0), thus we extract a substantial amount of positive meaning.
",8 Conclusions,[0],[0]
The work presented in this paper is not tied to any existing semantic representation.,8 Conclusions,[0],[0]
"While we rely heavily on syntactic dependencies, positive interpretations are generated in plain text, and they could be processed, along with the original negated statement, with any NLP pipeline.",8 Conclusions,[0],[0]
This paper presents a two-step procedure to extract positive meaning from verbal negation.,abstractText,[0],[0]
We first generate potential positive interpretations manipulating syntactic dependencies.,abstractText,[0],[0]
"Then, we score them according to their likelihood.",abstractText,[0],[0]
Manual annotations show that positive interpretations are ubiquitous and intuitive to humans.,abstractText,[0],[0]
"Experimental results show that dependencies are better suited than semantic roles for this task, and automation is possible.",abstractText,[0],[0]
Understanding Negation in Positive Terms Using Syntactic Dependencies,title,[0],[0]
"Satire is a writing technique for passing criticism using humor, irony or exaggeration.",1 Introduction,[0],[0]
"It is often used in contemporary politics to ridicule individual politicians, political parties or society as a whole.",1 Introduction,[0],[0]
"We restrict ourselves in this paper to such political satire articles, broadly defined as articles whose purpose is not to report real events, but rather to mock their subject matter.",1 Introduction,[0],[0]
"Satirical writing often builds on real facts and expectations, pushed to absurdity to express humorous insights about the situation.",1 Introduction,[0],[0]
"As a result, the difference between real and satirical articles can be subtle and often confusing to readers.",1 Introduction,[0],[0]
"With the recent rise of social media outlets, satirical articles have become increasingly popular and have famously fooled several leading news agencies1.",1 Introduction,[0],[0]
"These misinterpretations can often
1https://newrepublic.com/article/118013/ satire-news-websites-are-cashing-gullibleoutraged-readers
be attributed to careless reading, as there is a clear line between unusual events finding their way to the news and satire, which intentionally places key political figures in unlikely humorous scenarios.",1 Introduction,[0],[0]
"The two can be separated by carefully reading the articles, exposing the satirical nature of the events described in such articles.
",1 Introduction,[0],[0]
In this paper we follow this intuition.,1 Introduction,[0],[0]
"We look into the satire detection task (Burfoot and Baldwin, 2009), predicting if a given news article is real or satirical, and suggest that this prediction task should be defined over common-sense inferences, rather than looking at it as a lexical text classification task (Pang and Lee, 2008; Burfoot and Baldwin, 2009), which bases the decision on word-level features.
",1 Introduction,[0],[0]
"To further motivate this observation, consider the two excerpts in Figure 1.",1 Introduction,[0],[0]
"Both excerpts mention top-ranking politicians (the President and Vice President) in a drug-related context, and contain informal slang utterances, inappropriate for the subjects’
537
Transactions of the Association for Computational Linguistics, vol. 4, pp.",1 Introduction,[0],[0]
"537–549, 2016.",1 Introduction,[0],[0]
Action Editor: Timothy Baldwin.,1 Introduction,[0],[0]
"Submission batch: 1/2016; Revision batch: 5/2016; Published 12/2016.
",1 Introduction,[0],[0]
c©2016 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
position.",1 Introduction,[0],[0]
"The difference between the two examples is apparent when analyzing the situation described in the two articles: The first example (top), describes the Vice President speaking inappropriately in a work setting, clearly an unrealistic situation.",1 Introduction,[0],[0]
"In the second (bottom) the President is spoken to inappropriately, an unlikely, yet not unrealistic, situation.",1 Introduction,[0],[0]
"From the perspective of our prediction task, it is advisable to base the prediction on a structured representation capturing the events and their participants, described in the text.
",1 Introduction,[0],[0]
The absurdity of the situation described in satirical articles is often not unique to the specific individuals appearing in the narrative.,1 Introduction,[0],[0]
"In our example, both politicians are interchangeable: placing the president in the situation described in the first excerpt would not make it less absurd.",1 Introduction,[0],[0]
"It is therefore desirable to make a common-sense inference about high-ranking politicians in this scenario.
",1 Introduction,[0],[0]
We follow these intuitions and suggest a novel approach for the satire prediction task.,1 Introduction,[0],[0]
"Our model, COMSENSE, makes predictions by making common-sense inferences over a simplified narrative representation.",1 Introduction,[0],[0]
"Similarly to prior work (Chambers and Jurafsky, 2008; Goyal et al., 2010; Wang and McAllester, 2015) we represent the narrative structure by capturing the main entities (and tracking their mentions throughout the text), their activities, and their utterances.",1 Introduction,[0],[0]
The result of this process is a Narrative Representation Graph (NRG).,1 Introduction,[0],[0]
"Figure 2 depicts examples of this representation for the excerpts in Figure 1.
",1 Introduction,[0],[0]
"Given an NRG, our model makes inferences quantifying how likely are each of the represented events and interactions to appear in a real, or satirical context.",1 Introduction,[0],[0]
"Annotating the NRG for such inferences is a challenging task, as the space of possible situations is extremely large.",1 Introduction,[0],[0]
"Instead, we frame the required inferences as a highly-structured latent variable model, trained discriminatively as part of the prediction task.",1 Introduction,[0],[0]
"Without explicit supervision, the model assigns categories to the NRG vertices (for example, by grouping politicians into a single category, or by grouping inappropriate slang utterances, regardless of specific word choice).",1 Introduction,[0],[0]
"These category assignments form the infrastructure for higher-level reasoning, as they allows the model to identify the commonalities between unrelated people, their ac-
tions and their words.",1 Introduction,[0],[0]
The model learns commonsense patterns leading to real or satirical decisions based on these categories.,1 Introduction,[0],[0]
"We express these patterns as parametrized rules (acting as global features in the prediction model), and base the prediction on their activation values.",1 Introduction,[0],[0]
"In our example, these rules can capture the combination of (EPolitician) ∧ (Qslang)→ Satire, where EPolitician and Qslang are latent variable assignments to entity and utterance categories respectively.
",1 Introduction,[0],[0]
"Our experiments look into two variants of satire prediction: using full articles, and the more challenging sub-task of predicting if a quote is real given its speaker.",1 Introduction,[0],[0]
We use two datasets collected 6 years apart.,1 Introduction,[0],[0]
"The first collected in 2009 (Burfoot and Baldwin, 2009) and an additional dataset collected recently.",1 Introduction,[0],[0]
"Since satirical articles tend to focus on current events, the two datasets describe different people and world events.",1 Introduction,[0],[0]
"To demonstrate the robustness of our COMSENSE approach we use the first dataset for training, and the second as out-of-domain test data.",1 Introduction,[0],[0]
"We compare COMSENSE to several competing systems including a state-of-the-art Convolutional Neural Network (Kim, 2014).",1 Introduction,[0],[0]
Our experiments show that COMSENSE outperforms all other models.,1 Introduction,[0],[0]
"Most interestingly, it does so with a larger margin when tested over the out-of-domain dataset, demonstrating that it is more resistant to overfitting compared to other models.",1 Introduction,[0],[0]
"The problem of building computational models dealing with humor, satire, irony and sarcasm has attracted considerable interest in the the Natural Language Processing (NLP) and Machine Learning (ML) communities in recent years (Wallace et al., 2014; Riloff et al., 2013; Wallace et al., 2015; Davidov et al., 2010; Karoui et al., 2015; Burfoot and Baldwin, 2009; Tepperman et al., 2006; GonzálezIbánez et al., 2011; Lukin and Walker, 2013; Filatova, 2012; Reyes et al., 2013).",2 Related Work,[0],[0]
"Most work has looked into ironic expressions in shorter texts, such as tweets and forum comments.",2 Related Work,[0],[0]
Most related to our work is Burfoot and Baldwin (2009) which focused on satirical articles.,2 Related Work,[0],[0]
In that work the authors suggest a text classification approach for satire detection.,2 Related Work,[0],[0]
"In addition to using bag-of-words features, the
authors also experiment with semantic validity features which pair entities mentioned in the article, thus capturing combinations unlikely to appear in a real context.",2 Related Work,[0],[0]
"This paper follows a similar intuition; however, it looks into structured representations of this information, and studies their advantages.
",2 Related Work,[0],[0]
"Our structured representation is related to several recent reading comprehension tasks (Richardson et al., 2013; Berant et al., 2014) and work on narrative representation such, as event-chains (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2008), plotunits (Goyal et al., 2010; Lehnert, 1981) and Story Intention Graphs (Elson, 2012).",2 Related Work,[0],[0]
"Unlike these works, narrative representation is not the focus of this work, but rather provides the basis for making inferences, and as result we choose a simpler (and more robust) representation, most closely resembling event chains (Chambers and Jurafsky, 2008)
",2 Related Work,[0],[0]
"Making common-sense inferences is one of the core missions of AI, applicable to a wide range of tasks.",2 Related Work,[0],[0]
"Early work (Reiter, 1980; McCarthy, 1980; Hobbs et al., 1988) focused on logical inference, and manual construction of such knowledge repositories (Lenat, 1995; Liu and Singh, 2004).",2 Related Work,[0],[0]
"More recently, several researchers have looked into automatic common-sense knowledge construction and expansion using common-sense inferences (Tandon et al., 2011; Bordes et al., 2011; Socher et al., 2013; Angeli and Manning, 2014).",2 Related Work,[0],[0]
"Several works have looked into combining NLP with commonsense (Gerber et al., 2010; Gordon et al., 2011; LoBue and Yates, 2011; Labutov and Lipson, 2012; Gordon et al., 2012).",2 Related Work,[0],[0]
"Most relevant to our work is a SemEval-2012 task (Gordon et al., 2012), looking into common-sense causality identification prediction.
",2 Related Work,[0],[0]
"In this work we focus on a different task, satire detection in news articles.",2 Related Work,[0],[0]
"We argue that this task is inherently a common-sense reasoning task, as identifying the satirical aspects in narrative text does not require any specialized training, but instead relies heavily on common expectations of normative behavior and deviation from it in satirical text.",2 Related Work,[0],[0]
"We design our model to capture these behavioral expectations using (weighted) rules, instead of relying on lexical features as is often the case in text categorization tasks.",2 Related Work,[0],[0]
"Other common-sense frameworks typically build on existing knowledge bases represent-
ing world knowledge; however, specifying in advance the behaviors commonly associated with people based on their background and situational context, to the extent it can provide good coverage for our task, requires considerable effort.",2 Related Work,[0],[0]
"Instead, we suggest to learn this information from data directly, and our model learns jointly to predict and represent the satirical elements of the article.",2 Related Work,[0],[0]
"Given a news article, our COMSENSE system first constructs a graph-based representation of the narrative, denoted Narrative Representation Graph (NRG), capturing its participants, their actions and utterances.",3 Modeling,[0],[0]
We describe this process in more detail in Section 3.1.,3 Modeling,[0],[0]
"Based on the NRG, our model makes a set of inferences, mapping the NRG vertices to general categories abstracting over the specific NRG.",3 Modeling,[0],[0]
These abstractions are formulated as latent variables in our model.,3 Modeling,[0],[0]
"The system makes a prediction by reasoning over the abstract NRG, by
decomposing it into paths, where each path captures a partial view of the abstract NRG.",3 Modeling,[0],[0]
Finally we associate the paths with the satire decision output.,3 Modeling,[0],[0]
"The COMSENSE model then solves a global inference problem, formulated as an Integer Linear Program (ILP) instance, looking for the most likely explanation of the satire prediction output, consistent with the extracted patterns.",3 Modeling,[0],[0]
"We explain this process in detail in Section 3.2.
",3 Modeling,[0],[0]
"NRG Abstraction as Common-Sense The main goal of the COMSENSE approach is to move away from purely lexical models, and instead base its decisions on common-sense inferences.",3 Modeling,[0],[0]
"We formulate these inferences as parameterized rules, mapping elements of the narrative, represented using the NRG, to a classification decision.",3 Modeling,[0],[0]
The rules’ ability to capture common-sense inferences hinges on two key elements.,3 Modeling,[0],[0]
"First, the abstraction of NRG nodes into typed narrative elements allows the model to find commonalities across entities and their actions.",3 Modeling,[0],[0]
This is done by associating each NRG node with a set of latent variables.,3 Modeling,[0],[0]
"Second, constructing the decision rules according to the structure of the NRG graph allows us to model the dependencies between narrative elements.",3 Modeling,[0],[0]
"This is done by following the paths in the abstract NRG, generating rules by combining the latent variables representing nodes on the path, and associating them with a satire decision variable.
",3 Modeling,[0],[0]
"Computational Considerations When setting up the learning system, there is a clear expressivity/efficiency tradeoff over these two elements.",3 Modeling,[0],[0]
Increasing the number of latent variables associated with each NRG node would allow the model to learn a more nuanced representation.,3 Modeling,[0],[0]
"Similarly, generating rules by following longer NRG paths would allow the model to condition its satire decision on multiple entities and events jointly.",3 Modeling,[0],[0]
The added expressivity does not come without price.,3 Modeling,[0],[0]
"Given the limited supervision afforded to the model when learning these rules, additional expressivity would result in a more difficult learning problem which could lead to overfitting.",3 Modeling,[0],[0]
"Our experiments demonstrate this tradeoff, and in Figure 4 we show the effect of increasing the number of latent variables on performance.",3 Modeling,[0],[0]
An additional concern with increasing the model’s expressivity is computational efficiency.,3 Modeling,[0],[0]
"Satire prediction is formulated as an ILP
inference process jointly assigning values to the latent variables and making the satire decision.",3 Modeling,[0],[0]
"Since ILP is exponential in the number of variables, increasing the number of latent variables would be computationally challenging.",3 Modeling,[0],[0]
In this paper we take a straight-forward approach to ensuring computational tractability by limiting the length of NRG paths considered by our model to a constant size c=2.,3 Modeling,[0],[0]
"Assuming that we have m latent categories associated with each node, each path would generate mc ILP variables (see Section 3.3 for details), hence the importance of limiting the length of the path.",3 Modeling,[0],[0]
"In the future we intend to study approximate inference methods that can help alleviate this computational difficultly, such as using LP-approximation (Martins et al., 2009).",3 Modeling,[0],[0]
"The Narrative Representation Graph (NRG) is a simple graph-based representation for narrative text, describing the connections between entities and their actions.",3.1 Narrative Representation Graph for News Articles,[0],[0]
"The key motivation behind NRG was to provide the structure necessary for making inferences, and as a result we chose a simple representation that does not take into account cross-event relationships, and nuanced differences between some of the event argument types.",3.1 Narrative Representation Graph for News Articles,[0],[0]
"While other representations (Mani, 2012; Goyal et al., 2010; Elson, 2012) capture more information, they are harder to construct and more prone to error.",3.1 Narrative Representation Graph for News Articles,[0],[0]
"We will look into adapting these models for our purpose in future work.
",3.1 Narrative Representation Graph for News Articles,[0],[0]
"Since satirical articles tend to focus on political figures, we design the NRG around animate entities that drive the events described in the text, their actions (represented as predicate nodes), their contextualizing information (location-modifiers, temporal modifiers, negations), and their utterances.",3.1 Narrative Representation Graph for News Articles,[0],[0]
We omitted from the graph other non-animate entity types.,3.1 Narrative Representation Graph for News Articles,[0],[0]
"In Figure 2 we show an example of this representation.
",3.1 Narrative Representation Graph for News Articles,[0],[0]
"Similar in spirit to previous work (Goyal et al., 2010; Chambers and Jurafsky, 2008), we represent the relations between the entities that appear in the story using a Semantic Role Labeling system (Punyakanok et al., 2008) and collapse all the entity mentions into a single entity using a Co-Reference resolution system (Manning et al., 2014).",3.1 Narrative Representation Graph for News Articles,[0],[0]
"We attribute
utterances to their speaker based on a previously published rule based system (O’Keefe et al., 2012).
",3.1 Narrative Representation Graph for News Articles,[0],[0]
"Formally, we construct a graph G = {V,E}, where V consists of three types of vertices: ANIMATE ENTITY (e.g., people), PREDICATE (e.g., actions) and ARGUMENT (e.g., utterances, locations).",3.1 Narrative Representation Graph for News Articles,[0],[0]
The edges E capture the relationships between vertices.,3.1 Narrative Representation Graph for News Articles,[0],[0]
The graph contains several different edges.,3.1 Narrative Representation Graph for News Articles,[0],[0]
"COREF edges collapse the mentions of the same entity into a single entity, ARGUMENT-TYPE edges connect ANIMATE ENTITY nodes to PREDICATE nodes2, and PREDICATE nodes to argument nodes (modifiers).",3.1 Narrative Representation Graph for News Articles,[0],[0]
Finally we add QUOTE edges connecting ANIMATE ENTITY nodes to utterances (ARGUMENT).,3.1 Narrative Representation Graph for News Articles,[0],[0]
Satire prediction is inherently a text classification problem.,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
Such problems are often approached using a Bag-of-Words (BoW) model which ignores the document structure when making predictions.,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"Instead, the NRG provides a structured representation for making the satire prediction.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"We begin by showing how the NRG can be used directly and then discuss how to enhance it by mapping the graph into abstract categories.
",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"Directly Using NRG for Satire Prediction We suggest a simple approach for extracting features directly from the NRG, by decomposing it into graph paths, without mapping the graph into abstract categories.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"This simple, word-based representation for prediction structured according to the NRG (denoted NARRLEX), generates features by using the words in the original document, corresponding to the graph decomposition.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"For example, consider the path connecting “a man” to an utterance in Figure 2(b).",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"Simple features could associate the utterances words with that entity, rather than with the President.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"The resulting NARRLEX model generates Bag-of-Words features based on words corresponding to NRG path vertices, conditioned on their connected entity vertex.
",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"Using Common-Sense for Satire Prediction Unlike the NARRLEX model, which relies on directly
2These edges are typed according to their semantic roles.
",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"observed information, our COMSENSE model performs inference over higher level patterns.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"In this model the prediction is a global inference process, taking into account the relationships between NRG elements (and their abstraction into categories) and the final prediction.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"This process is described in Figure 3.
",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"First, the model associates a high level category, that can be reused even when other, previously unseen, entities are discussed in the text.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"We associate a set of Boolean variables with each NRG vertex, capturing higher level abstraction over this node.
",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"We define three types of categories corresponding to the three types of vertices, and denote them E,A,Q for Entity category, Action category and Quote category, respectively.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
Each category variable can take k different values.,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"As a convention we denote X = i as category assignment, where X ∈ {E,A,Q} is the category type, and i is its assignment.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"Since these category assignments are not directly observed, they are treated as latent variables in our model.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"This process is exemplified at the top right corner of Figure 3.
",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
Combinations of category assignments form patterns used for determining the prediction.,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
These patterns can be viewed as parameterized rules.,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
Each weighted rule associates a combination with an output variable (SATIRE or REAL).,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
Examples of such rules are provided in the middle of the right corner of Figure 3.,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"We formulate the activations of these rules as Boolean variables, whose assignments are highly interconnected.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"For example, the variables representing the following rules (E = 0)→ SATIRE and (E = 0)→",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"REAL are mutually exclusive, since assigning a T value to either one entails a satire (or real) prediction.",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"To account for this interdependency, we add constraints capturing the relations between rules.
",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
The model makes predictions by combining the rule weights and predicting the top scoring output value.,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"The prediction can be viewed as a derivation process, mapping article entities to categories (e.g., ENTITY(“A MAN”)→ (E=0), is an example of such derivation), combinations of categories compose into prediction patterns (e.g., (E=0)→SATIRE).",3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
We use an ILP solver to find the optimal derivation sequence.,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
We describe the inference process as an Integer Linear Program in the following section.,3.2 Satire Prediction using the Narrative Representation Graph,[0],[0]
"We formulate the decision as a 0-1 Integer Linear Programming problem, consisting of three types of Boolean variables: category assignments indicator variables, indicator variables for common-sense patterns, and finally the output decision variables.",3.3 Identifying Relevant Interactions using Constrained Optimization,[0],[0]
"Each indicator variable is also represented using a feature set, used to score its activation.",3.3 Identifying Relevant Interactions using Constrained Optimization,[0],[0]
"Each node in the NRG is assigned a set of competing variables, mapping the node to different categories according to its type.
",3.3.1 Category Assignment Variables,[0],[0]
"• ANIMATE ENTITY Category Variables, denoted hi,j,E , indicating the Entity category i for NRG vertex j.
• ACTION Category Variables, denoted hi,j,A, indicating the Action category i for NRG vertex j.
• QUOTE Category Variables, denoted hi,j,Q, indicating the Quote category i for NRG vertex j.
The number of possible categories for each variable type is a hyper-parameter of the model.
",3.3.1 Category Assignment Variables,[0],[0]
Variable activation constraints Category assignments to the same node are mutually exclusive (a node can only have a single category).,3.3.1 Category Assignment Variables,[0],[0]
"We encode this fact by constraining the decision with a linear constraint (where X ∈ {E,A,Q}):
∀j ∑
i
hi,j,X = 1.
",3.3.1 Category Assignment Variables,[0],[0]
"Category Assignment Features Each decision variable decomposes into a set of features, φ(x, hi,j,X) capturing the words associated with the j-th vertex, conditioned on X and i.",3.3.1 Category Assignment Variables,[0],[0]
"We represent common-sense prediction rules using an additional set of Boolean variables, connecting the category assignments variables with the output prediction.",3.3.2 Common-sense Patterns Variables,[0],[0]
"The space of possible variables is determined by decomposing the NRG into paths of size up to 2, and associating two Boolean variables with category assignment variables corresponding to the vertices on these paths.",3.3.2 Common-sense Patterns Variables,[0],[0]
"One of the variables associates the sequence of category assignment variables with a REAL output value, and one with a SATIRE output value.
",3.3.2 Common-sense Patterns Variables,[0],[0]
"• Single Vertex Path Patterns Variables, denoted by hBhi,j,X , indicating that the category assignment captured by hi,j,X is associated with output value B (where B∈ {SATIRE, REAL}).
",3.3.2 Common-sense Patterns Variables,[0],[0]
"• Two Vertex Path Patterns Variables, denoted by hB(hi,j,X1),(hk,l,X2 )
, indicating that the pattern captured by category assignment along the NRG path of hi,j,X1 and hi,j,X2 is associated with output value B (where B∈ {SATIRE, REAL}).",3.3.2 Common-sense Patterns Variables,[0],[0]
"Decision Consistency constraints It is clear that the activation of the common-sense Patterns Variables entails the activation of the category assignment variables, corresponding to the elements of the common-sense patterns.",3.3.2 Common-sense Patterns Variables,[0],[0]
"For readability we only write the constraint for the Single Vertex Path Variables:
(hBhi,j,X ) =⇒ (hi,j,X).
",3.3.2 Common-sense Patterns Variables,[0],[0]
"Features Similar to the category assignment variable features, each decision variable decomposes into a set of features, φ(x, hBhi,j,X ).",3.3.2 Common-sense Patterns Variables,[0],[0]
"These features captures the words associated with each of the category assignment variables (in this example, the words associated with the j-th vertex) conditioned on the category assignments and the output prediction value (in this example, X, i and B).",3.3.2 Common-sense Patterns Variables,[0],[0]
"We also add a feature φ(hi,j,X , B) capturing the connection between the output value B, and category assignment.",3.3.2 Common-sense Patterns Variables,[0],[0]
"Finally, we add two more Boolean variables corresponding to the output prediction: hSatire and hReal.",3.3.3 Satire Prediction Variables,[0],[0]
"The activation of these two variables is mutually exclusive, we encode that by adding the constraint:
hSatire + hReal = 1.
",3.3.3 Satire Prediction Variables,[0],[0]
"We ensure the consistency of our model adding constraints forcing agreement between the final prediction variables, and the common-sense patterns variables:
hBhi,j,X =⇒ h B.
Overall Optimization Function The Boolean variables described in the previous section define a space of competing inferences.",3.3.3 Satire Prediction Variables,[0],[0]
"We find the optimal output value derivation by finding the optimal set of variables assignments, by solving the following objective:
max y,h ∑ i hiw Tφ(x, hi, y)
s.t. C, ∀i;hi ∈ {0, 1}, (1)
where hi ∈ H is the set of all variables defined above and C is the set of constraints defined over the activation of these variables.",3.3.3 Satire Prediction Variables,[0],[0]
"w is the weight vector, used to quantify the feature representation of each h, obtained using a feature function φ(·).
",3.3.3 Satire Prediction Variables,[0],[0]
Note that the Boolean variable acts as a 0-1 indicator variable.,3.3.3 Satire Prediction Variables,[0],[0]
We formalize Eq.,3.3.3 Satire Prediction Variables,[0],[0]
"(1) as an ILP instance, which we solve using the highly optimized Gurobi toolkit3.
3http://www.gurobi.com/",3.3.3 Satire Prediction Variables,[0],[0]
"The COMSENSE approach models the decision as interactions between high-level categories of entities, actions and utterances.",4 Parameter Estimation for COMSENSE,[0],[0]
"However, the high level categories assigned to the NRG vertices are not observed, and as a result we view it as a weakly supervised learning problem, where the category assignments correspond to latent variable assignments.",4 Parameter Estimation for COMSENSE,[0],[0]
"We learn the parameters of these assignments by using a discriminative latent structure learning framework.
",4 Parameter Estimation for COMSENSE,[0],[0]
"The training data is a collection D ={(xi, yi)}ni=1, where xi is an article, parsed into an NRG representation, and y is a binary label, indicating if the article is satirical or real.
",4 Parameter Estimation for COMSENSE,[0],[0]
"Given this data we estimate the models’ parameters by minimizing the following objective function.
",4 Parameter Estimation for COMSENSE,[0],[0]
LD(w),4 Parameter Estimation for COMSENSE,[0],[0]
"= min w
λ 2 ||w||2",4 Parameter Estimation for COMSENSE,[0],[0]
"+ 1 n
n∑
i=1
ξi (2)
ξi is the slack variable, capturing the margin violation penalty for a given training example, and defined as follows:
ξi = max y,h f(x,h, y,w) +",4 Parameter Estimation for COMSENSE,[0],[0]
"cost(y, yi)
−max h f(x,h, yi,w),
where f(·) is a scoring function, similar to the one used in Eq. 1.",4 Parameter Estimation for COMSENSE,[0],[0]
"The cost function is the margin that the true prediction must exceed over the competing label, and it is simply defined as the difference between the model prediction and the gold label.",4 Parameter Estimation for COMSENSE,[0],[0]
This formulation is an extension of the hinge loss for latent structure SVM.,4 Parameter Estimation for COMSENSE,[0],[0]
"λ is the regularization parameter controlling the tradeoff between the l2 regularizer and the slack penalty.
",4 Parameter Estimation for COMSENSE,[0],[0]
"We optimize this objective using the stochastic sub-gradient descent algorithm (Ratliff et al., 2007; Felzenszwalb et al., 2009).",4 Parameter Estimation for COMSENSE,[0],[0]
"We can compute the subgradient as follows:
∇LD(w) = λw + n∑
i=1
Φ(xi, yi, y ∗)
Φ(xi, yi, y ∗) = φ(xi,h∗, yi)− φ(xi,h∗, y∗),
where φ(xi,h∗, y∗) is the set of features representing the solution obtained after solving Eq. 14 and
4modified to accommodate the margin constraint
making a prediction.",4 Parameter Estimation for COMSENSE,[0],[0]
"φ(xi,h∗, yi) is the set of features representing the solution obtained by solving Eq. 1 while fixing the outcome of the inference process to the correct prediction (i.e., yi).",4 Parameter Estimation for COMSENSE,[0],[0]
"Intuitively, it can be considered as finding the best explanation for the correct label using the latent variables h.
",4 Parameter Estimation for COMSENSE,[0],[0]
In the stochastic version of the sub gradient descent algorithm we approximate ∇LD(w) by computing the sub gradient of a single example and making a local update.,4 Parameter Estimation for COMSENSE,[0],[0]
"This version resembles the latentstructure perceptron algorithm (Sun et al., 2009).",4 Parameter Estimation for COMSENSE,[0],[0]
"We repeatedly iterate over the training examples and for each example, if the current w leads to a correct prediction (and satisfies the margin constraint), we only shrink w according to λ.",4 Parameter Estimation for COMSENSE,[0],[0]
"If the model makes an incorrect prediction, the model is updated according Φ(xi, yi, y
∗).",4 Parameter Estimation for COMSENSE,[0],[0]
"The optimization objective LD(W ) is not convex, and the optimization procedure is guaranteed to converge to a local minimum.",4 Parameter Estimation for COMSENSE,[0],[0]
We design our experimental evaluation to help clarify several questions.,5 Empirical Study,[0],[0]
"First, we want to understand how our model compares with traditional text classification models.",5 Empirical Study,[0],[0]
"We hypothesize that these methods are more susceptible to overfitting, and design our experiments accordingly.",5 Empirical Study,[0],[0]
"We compare the models’ performance when using in-domain data (test and training data are from the same source), and out-ofdomain data, where the test data is collected from a different source.",5 Empirical Study,[0],[0]
We look into two tasks.,5 Empirical Study,[0],[0]
"One is the Satire detection task (Burfoot and Baldwin, 2009).",5 Empirical Study,[0],[0]
"We also introduce a new task, called “did I say that?” which only focuses on utterances and speakers.
",5 Empirical Study,[0],[0]
The second aspect of our evaluation focuses on the common-sense inferences learned by our model.,5 Empirical Study,[0],[0]
We examine how the size of the set of categories impacts the model performance.,5 Empirical Study,[0],[0]
"We also provide a qualitative analysis of the learned categories using a heat map, capturing the activation strength of learned inferences over the training data.
",5 Empirical Study,[0],[0]
"Prediction tasks We look into two prediction tasks: (1) Satire Detection (denoted SD), a binary classification task, in which the model has access to the complete article (2) “Did I say that?” (denoted DIST), a binary classification task, consisting only
of entities mentions (and their surrounding context in text) and direct quotes.",5 Empirical Study,[0],[0]
"The goal of the DIST is to predict if a given utterance is likely to be real, given its speaker.",5 Empirical Study,[0],[0]
"Since not all document contain direct quotes, we only use a subset of the documents in the SD task.
",5 Empirical Study,[0],[0]
"Datasets In both prediction tasks we look into two settings: (1) In-domain prediction: where the training and test data are collected from the same source, and (2) out-of-domain prediction, where the test data is collected from a different source.",5 Empirical Study,[0],[0]
"We use the data collected by Burfoot and Baldwin (2009) for training the model in both settings, and its test data for in-domain prediction (denoted TRAIN - SD’09, TEST - SD’09, TRAIN - SD’09 - DIST, TEST - SD’09 - DIST, respectively for training and testing in the SD and DIST tasks).",5 Empirical Study,[0],[0]
"In addition, we collected a second dataset of satirical and real articles (denoted SD’16).",5 Empirical Study,[0],[0]
"This collection of articles contains real articles from cnn.com and satirical articles from theonion.com, a well known satirical news website.",5 Empirical Study,[0],[0]
"The articles were published between 2010 to 2015, appearing in the political sections of both news websites.",5 Empirical Study,[0],[0]
"Following other work in the field, all datasets are highly skewed toward the negative class (real articles), as it better characterizes a realistic prediction scenario.",5 Empirical Study,[0],[0]
"The statistics of the datasets are summarized in Table 2.
",5 Empirical Study,[0],[0]
"Evaluated Systems We compare several systems, as follows:
System ALLPOS Always predict Satire
BB’09 Results by (Burfoot and Baldwin, 2009)
",5 Empirical Study,[0],[0]
CONV Convolutional NN.,5 Empirical Study,[0],[0]
"We followed (Kim, 2014), using pre-trained 300-dimensional word vectors (Mikolov et al., 2013).
",5 Empirical Study,[0],[0]
"LEX SVM with unigram (LEXU) or both unigram and bigram (LEXU+B) features
NARRLEX SVM with direct NRG-based features (see Sec 3.2)
",5 Empirical Study,[0],[0]
COMSENSE Our model.,5 Empirical Study,[0],[0]
"We denote the full model as COMSENSEF , and COMSENSEQ when using only the entity+quotes based patterns.
",5 Empirical Study,[0],[0]
"We tuned all the models’ hyper-parameters by using a small validation set, consisting of 15% of the training data.",5 Empirical Study,[0],[0]
"After setting the hyper-parameters, the
model was retrained using the entire dataset.",5 Empirical Study,[0],[0]
We used SVM-light5 to train our lexical baseline systems (LEX and NARRLEX).,5 Empirical Study,[0],[0]
"Since the data is highly skewed towards the negative class (REAL), we adjust the learner objective function cost factor for positive examples to outweigh negative examples.",5 Empirical Study,[0],[0]
The cost factor was tuned using the validation set.,5 Empirical Study,[0],[0]
"Since our goal is to identify satirical articles, given significantly more real articles, we report the Fmeasure of the positive class.",5.1 Experimental Results,[0],[0]
The results are summarized in Tables 1 and 3.,5.1 Experimental Results,[0],[0]
We can see that in all cases the COMSENSE model obtains the best results.,5.1 Experimental Results,[0],[0]
"We note that in both tasks, when learning in the outof-domain settings performance drops sharply, however the gap between the COMSENSE model and other models increases in these settings, showing that it is less prone to overfitting.
",5.1 Experimental Results,[0],[0]
"Interestingly, for the satire detection (SD) task, the COMSENSEQ model performs best for the indomain setting, and COMSENSEF gives the best performance in the out-of-domain settings.",5.1 Experimental Results,[0],[0]
We hypothesize that this is due to a phenomenon we call “overfitting to document structure”.,5.1 Experimental Results,[0],[0]
"Lexical models tend to base the decision on word choices specific to the training data, and as a result when tested on out of domain data, which describes new events and entities, performance drops sharply.",5.1 Experimental Results,[0],[0]
"Instead, the COMSENSEQ model focuses on properties of quotations and entities appearing in the text.",5.1 Experimental Results,[0],[0]
"In the SD’09 datasets, this information helps focus the learner, as the real and satire articles are structured differently (for example, satire articles frequently contain multiple quotes).",5.1 Experimental Results,[0],[0]
"This structure is not maintained when working with out-of-domain data, and indeed in these settings the model benefits from using additional information offered by the full model.
",5.1 Experimental Results,[0],[0]
"Number of Latent Categories Our COMSENSE model is parametrized with the number of latent categories it considers for each entity, predicate and quote.",5.1 Experimental Results,[0],[0]
This hyper-parameter can have a strong influence on the model performance (and running time).,5.1 Experimental Results,[0],[0]
"Increasing it adds to the model’s expressivity allowing it to learn more complex patterns, but also defines a more complex learning
5http://svmlight.joachims.org/
0.31
0.35
0.38
0.42
0.45
0.49
2 3 4 5 6
EV=2 EV=3
EV=1 Lex
Quote Vars
F -
S co
re
Figure 4: Different Number of Latent Categories.",5.1 Experimental Results,[0],[0]
"EV denotes the number entity categories used, and QuoteVars denotes the number of quote categories used.
problem (recall our non-convex learning objective function).",5.1 Experimental Results,[0],[0]
We focused on the DIST task when evaluating different configurations as it converged much faster than the full model.,5.1 Experimental Results,[0],[0]
Figure 4 plots the model behavior when using different numbers of latent categories.,5.1 Experimental Results,[0],[0]
"Interestingly, the number of entity categories saturates faster than the number of quote categories.",5.1 Experimental Results,[0],[0]
"This can be attributed to the limited text describing entities.
",5.1 Experimental Results,[0],[0]
Visualizing Latent COMSENSE Patterns,5.1 Experimental Results,[0],[0]
"Given the assignment to latent categories, our model learns common-sense patterns for identifying satirical and real articles based on these categories.",5.1 Experimental Results,[0],[0]
"Ideally, these patterns could be extracted directly from the data, however providing the resources for this additional prediction task is not straightforward.",5.1 Experimental Results,[0],[0]
"Instead, we view the category assignment as latent variables, which raises the question - what are the categories learned by the model?
",5.1 Experimental Results,[0],[0]
In this section we provide a qualitative evaluation of these categories and the prediction rules identified by the system using the heat map in Figure 5.,5.1 Experimental Results,[0],[0]
"For simplicity, we focus on the DIST task, which only has categories corresponding to entities and quotes.
",5.1 Experimental Results,[0],[0]
"(a) Prediction Rules These patterns are expressed as rules, mapping category assignments to output values.",5.1 Experimental Results,[0],[0]
"In the DIST task, we consider combinations of entity and quote category pairs, denoted Ei,Qj, in the heat map.",5.1 Experimental Results,[0],[0]
"The top part of Figure 5, in red, shows the activation strength of each of the category com-
binations when making predictions over the training data.",5.1 Experimental Results,[0],[0]
"Darker colors correspond to larger values, which were computed as:
cell(CE , CQ, B) = ∑ j hB(hCE,j,E),(hCQ,j,Q) ∑ j,k,l hB(hk,j,E),(hl,j,Q)
Intuitively, each cell value in Figure 5 is the number of times each category pattern appeared in REAL or
SATIRE output predictions, normalized by the overall number of pattern activations for each output.
",5.1 Experimental Results,[0],[0]
"We assume that different patterns will be associated with satirical and real articles, and indeed we can see that most entities and quotes appearing in REAL articles fall into a distinctive category pattern, E0, Q0.",5.1 Experimental Results,[0],[0]
"Interestingly, there is some overlap between the two predictions in the most active SATIRE category (E1, Q0).",5.1 Experimental Results,[0],[0]
"We hypothesize that this is due to the fact that the two article types have some overlap.
",5.1 Experimental Results,[0],[0]
"(b) Associating topic words with learned categories In order to understand the entity and quote categories emerging from the training phase, we look at the activation strength of each category pattern with respect to a set of topic words.",5.1 Experimental Results,[0],[0]
"We manually identified a set of entity types and quote topics, which are likely to appear in political articles.",5.1 Experimental Results,[0],[0]
"We associate a list of words with each one of these types.
",5.1 Experimental Results,[0],[0]
"For example, the entity topic PRESIDENT was associated with words such as president, vice-president, Obama, Biden, Bush, Clinton.",5.1 Experimental Results,[0],[0]
"Similarly, we associated with the quote topic PROFANITY a list of profanity words.",5.1 Experimental Results,[0],[0]
"We associate 7 types with quote categories corresponding to style and topic, namely PROFANITY, DRUGS, POLITENESS, SCIENCE, LEGAL, POLITICS, CONTROVERSY, and another set of seven types with entity types, namely PRESIDENT, LIBERAL, CONSERVATIVE, ANONYMOUS, POLITICS, SPEAKER, LAW ENFORCEMENT.
",5.1 Experimental Results,[0],[0]
"In the bottom left part of Figure 5 (in blue), we show the activation strength of each category with respect to the set of selected quote topics.",5.1 Experimental Results,[0],[0]
"Intuitively, we count the number of times the words associated with a given topic appeared in the text span corresponding to a category assignment pair, separately for each output prediction.",5.1 Experimental Results,[0],[0]
"We normalize this value by the total number of topic word occurrences, over all category assignment pairs.",5.1 Experimental Results,[0],[0]
Note that we only look at the text span corresponding to quote vertices in the NRG.,5.1 Experimental Results,[0],[0]
We provide a similar analysis for entity categories in the bottom right part of Figure 5 (in green).,5.1 Experimental Results,[0],[0]
We show the activation strength of each category with respect to the set of selected entity topic words.,5.1 Experimental Results,[0],[0]
"As can be expected, we can see that profanity words are only associated with satirical categories, and even more interestingly, when words appear in both satirical and real predictions, they tend to fall into different categories.",5.1 Experimental Results,[0],[0]
"For example, the topic words related to DRUGS can appear both in real articles discussing alcohol and drug policies.",5.1 Experimental Results,[0],[0]
But topic words related to drugs also appear in satirical articles portraying politicians using these substances.,5.1 Experimental Results,[0],[0]
"While these are only qualitative results, we believe they provide strong intuitions for future work, especially considering the fact that the activation values do not rely on direct supervision, and only reflect the common-sense patterns emerging from the learned model.",5.1 Experimental Results,[0.9511888152324156],"['Shrinking the Unfolded Network First, we investigate which shrinking methods are effective for which layers.']"
In this paper we presented a latent variable model for satire detection.,6 Summary and Future Work,[0],[0]
"We followed the observation that satire detection is inherently a semantic task and modeled the common-sense inferences required for it using a latent variable framework.
",6 Summary and Future Work,[0],[0]
We designed our experiments specifically to examine if our model can generalize better than unstructured lexical models by testing it on out-ofdomain data.,6 Summary and Future Work,[0],[0]
"Our experiments show that in these challenging settings, the performance gap between our approach and the unstructured models increases, demonstrating the effectiveness of our approach.
",6 Summary and Future Work,[0],[0]
In this paper we restricted ourselves to limited narrative representation.,6 Summary and Future Work,[0],[0]
"In the future we intend to study how to extend this representation to capture more nuanced information.
",6 Summary and Future Work,[0],[0]
Learning common-sense representation for prediction problems has considerable potential for NLP applications.,6 Summary and Future Work,[0],[0]
"As the NLP community considers increasingly challenging tasks focusing on semantic and pragmatic aspects, the importance of finding such common-sense representation will increase.",6 Summary and Future Work,[0],[0]
In this paper we demonstrated the potential of common-sense representations for one application.,6 Summary and Future Work,[0],[0]
We hope these results will serve as a starting point for other studies in this direction.,6 Summary and Future Work,[0],[0]
"Automatic satire detection is a subtle text classification task, for machines and at times, even for humans.",abstractText,[0],[0]
"In this paper we argue that satire detection should be approached using common-sense inferences, rather than traditional text classification methods.",abstractText,[0],[0]
We present a highly structured latent variable model capturing the required inferences.,abstractText,[0],[0]
"The model abstracts over the specific entities appearing in the articles, grouping them into generalized categories, thus allowing the model to adapt to previously unseen situations.",abstractText,[0],[0]
Understanding Satirical Articles Using Common-Sense,title,[0],[0]
"Neural networks can be represented as a graph of computational modules, and training these networks amounts to optimising the weights associated with the modules of this graph to minimise a loss.",1. Introduction,[0],[0]
"At present, training is usually performed with first-order gradient descent style algorithms, where the weights are adjusted along the direction of the negative gradient of the loss.",1. Introduction,[0],[0]
"In order to compute the gra-
1DeepMind, London, United Kingdom.",1. Introduction,[0],[0]
"Correspondence to: WM Czarnecki <lejlot@google.com>.
fi
fi+1
fi+2
…
…
…
…
fi
fi+1
fi+2
…
…
…
…
Mi+1
i ̂i
Mi+2
̂i+1 i+1
(a) (b) (c)
Differentiable
Legend:
fi
fi+1
fi+2
…
…
…
…
Mi+1
i ̂i
(b)
x y
L
h SG
LSG
x y
L
h
Forward connection, differentiable
Forward connection, non-differentiable
Error gradient, non-differentiable
Synthetic error gradient, differentiable
Legend:
Synthetic error gradient, nondifferentiable
Non-differentiable
Forward connection
Error gradient
Synthetic error gradient
Figure 1.",1. Introduction,[0],[0]
"Visualisation of SG-based learning (b) vs. regular backpropagation (a).
dient of the loss with respect to the weights of a module, one performs backpropagation (Williams & Hinton, 1986) – sequentially applying the chain rule to compute the exact gradient of the loss with respect to a module.",1. Introduction,[0],[0]
"However, this scheme has many potential drawbacks, as well as lacking biological plausibility (Marblestone et al., 2016; Bengio et al., 2015).",1. Introduction,[0],[0]
"In particular, backpropagation results in locking – the weights of a network module can only be updated after a full forwards propagation of the data through the network, followed by loss evaluation, then finally after waiting for the backpropagation of error gradients.",1. Introduction,[0],[0]
"This locking constrains us to updating neural network modules in a sequential, synchronous manner.
",1. Introduction,[0],[0]
"One way of overcoming this issue is to apply Synthetic Gradients (SGs) to build Decoupled Neural Interfaces (DNIs) (Jaderberg et al., 2016).",1. Introduction,[0],[0]
"In this approach, models of error gradients are used to approximate the true error gradient.",1. Introduction,[0],[0]
"These models of error gradients are local to the network modules they are predicting the error gradient for, so that an update to the module can be computed by using the predicted, synthetic gradients, thus bypassing the need for subsequent forward execution, loss evaluation, and backpropagation.",1. Introduction,[0],[0]
The gradient models themselves are trained at the same time as the modules they are feeding synthetic gradients to are trained.,1. Introduction,[0],[0]
"The result is effectively a complex
ar X
iv :1
70 3.
00 52
2v 1
[ cs
.L G
] 1
M ar
2 01
7
Understanding Synthetic Gradients and DNIs
dynamical system composed of multiple sub-networks cooperating to minimise the loss.
",1. Introduction,[0],[0]
"There is a very appealing potential of using DNIs e.g. the potential to distribute and parallelise training of networks across multiple GPUs and machines, the ability to asynchronously train multi-network systems, and the ability to extend the temporal modelling capabilities of recurrent networks.",1. Introduction,[0],[0]
"However, it is not clear that introducing DNIs and SGs into a learning system will not negatively impact the learning dynamics and solutions found.",1. Introduction,[0],[0]
"While the empirical evidence in Jaderberg et al. (2016) suggests that SGs do not have a negative impact and that this potential is attainable, this paper will dig deeper and analyse the result of using SGs to accurately answer the question of the impact of synthetic gradients on learning systems.
",1. Introduction,[0],[0]
"In particular, we address the following questions, using feed-forward networks as our probe network architecture: Does introducing SGs change the critical points of the neural network learning system?",1. Introduction,[0],[0]
In Section 3 we show that the critical points of the original optimisation problem are maintained when using SGs.,1. Introduction,[0],[0]
Can we characterise the convergence and learning dynamics for systems that use synthetic gradients in place of true gradients?,1. Introduction,[0],[0]
Section 4 gives first convergence proofs when using synthetic gradients and empirical expositions of the impact of SGs on learning.,1. Introduction,[0],[0]
What is the difference in the representations and functional decomposition of networks learnt with synthetic gradients compared to backpropagation?,1. Introduction,[0],[0]
"Through experiments on deep neural networks in Section 5, we find that while functionally the networks perform identically trained with backpropagation or synthetic gradients, the layer-wise functional decomposition is markedly different due to SGs.
",1. Introduction,[0],[0]
"In addition, in Section 6 we look at formalising the connection between SGs and other forms of approximate error propagation such as Feedback Alignment (Lillicrap et al., 2016), Direct Feedback Alignment (Nøkland, 2016; Baldi et al., 2016), and Kickback (Balduzzi et al., 2014), and show that all these error approximation schemes can be captured in a unified framework, but crucially only using synthetic gradients can one achieve unlocked training.",1. Introduction,[0],[0]
"The key idea of synthetic gradients and DNI is to approximate the true gradient of the loss with a learnt model which predicts gradients without performing full backpropagation.
Consider a feed-forward network consisting of N layers fn, n ∈ {1, . . .",2. DNI using Synthetic Gradients,[0],[0]
", N}, each taking an input hn−1i and producing an output hni = fn(h n−1 i ), where h 0",2. DNI using Synthetic Gradients,[0],[0]
i = xi is the input data point xi.,2. DNI using Synthetic Gradients,[0],[0]
"A loss is defined on the output of the net-
work Li = L(hNi , yi) where yi is the given label or supervision for xi (which comes from some unknown P (y|x)).",2. DNI using Synthetic Gradients,[0],[0]
"Each layer fn has parameters θn that can be trained jointly to minimise Li with the gradient-based update rule
θn ← θn",2. DNI using Synthetic Gradients,[0],[0]
"− α ∂L(hNi , yi)
∂hni
∂hni ∂θn
where α is the learning rate and ∂Li/∂hni is computed with backpropagation.
",2. DNI using Synthetic Gradients,[0],[0]
"The reliance on ∂Li/∂hNi means that an update to layer i can only occur after every subsequent layer fj , j ∈ {i + 1, . .",2. DNI using Synthetic Gradients,[0],[0]
.,2. DNI using Synthetic Gradients,[0],[0]
", N} has been computed, the loss Li has been computed, and the error gradient ∂L/∂hNi backpropgated to get ∂Li/∂h",2. DNI using Synthetic Gradients,[0],[0]
N i .,2. DNI using Synthetic Gradients,[0],[0]
"An update rule such as this is update locked as it depends on computing Li, and also backwards locked as it depends on backpropagation to form ∂Li/∂hni .
",2. DNI using Synthetic Gradients,[0],[0]
"Jaderberg et al. (2016) introduces a learnt prediction of the error gradient, the synthetic gradient SG(hni , yi) = ̂∂Li/∂hni ' ∂Li/∂hni resulting in the update
θk ← θk",2. DNI using Synthetic Gradients,[0],[0]
"− α SG(hni , yi) ∂hni",2. DNI using Synthetic Gradients,[0],[0]
"∂θk ∀k ≤ n
This approximation to the true loss gradient allows us to have both update and backwards unlocking – the update to layer n can be applied without any other network computation as soon as hni has been computed, since the SG module is not a function of the rest of the network (unlike ∂Li/∂hi).",2. DNI using Synthetic Gradients,[0],[0]
"Furthermore, note that since the true ∂Li/∂hni can be described completely as a function of just hni and yi, from a mathematical perspective this approximation is sufficiently parameterised.
",2. DNI using Synthetic Gradients,[0],[0]
"The synthetic gradient module SG(hni , yi) has parameters θSG which must themselves be trained to accurately predict the true gradient by minimising the L2 loss LSGi = ‖SG(hni , yi)− ∂Li/∂hni ‖2.
",2. DNI using Synthetic Gradients,[0],[0]
"The resulting learning system consists of three decoupled parts: first, the part of the network above the SG module which minimises L wrt.",2. DNI using Synthetic Gradients,[0],[0]
"to its parameters {θn+1, ..., θN}, then the SG module that minimises the LSG wrt.",2. DNI using Synthetic Gradients,[0],[0]
to θSG.,2. DNI using Synthetic Gradients,[0],[0]
"Finally the part of the network below the SG module which uses SG(h, y) as the learning signal to train {θ1, ...θn}, thus it is minimising the loss modeled internally by SG.",2. DNI using Synthetic Gradients,[0],[0]
"Throughout the remainder of this paper, we consider the use of a single synthetic gradient module at a single layer k and for a generic data sample j and so refer to h = hj = hkj ; unless specified we drop the superscript k and subscript j. This model is shown in Figure 1 (b).",Assumptions and notation,[0],[0]
"We also focus on SG modules which take the point’s true label/value as conditioning SG(h, y) as opposed to SG(h).",Assumptions and notation,[0],[0]
"Note that without label conditioning, a SG module is trying to approximate
Understanding Synthetic Gradients and DNIs
not ∂L/∂h but rather EP (y|x)∂L/∂h since L is a function of both input and label.",Assumptions and notation,[0],[0]
"In theory, the lack of label is a sufficient parametrisation but learning becomes harder, since the SG module has to additionally learn P (y|x).
",Assumptions and notation,[0],[0]
"We also focus most of our attention on models that employ linear SG modules, SG(h, y) = hA+ yB +C. Such modules have been shown to work well in practice, and furthermore are more tractable to analyse.
",Assumptions and notation,[0],[0]
"As a shorthand, we denote θ<h to denote the subset of the parameters contained in modules up to h (and symmetrically θ>h), i.e. if h is the kth layer then θ<h = {θ1 . . .",Assumptions and notation,[0],[0]
", θk}.",Assumptions and notation,[0],[0]
Consider an N -layer feed-forward network with a single SG module at layer k. This network can be decomposed into two sub-networks: the first takes an input x and produces an output h = Fh(x) = fk(fk−1(. . .,Synthetic gradients in operation,[0],[0]
"(f1(x)))), while the second network takes h as an input, produces an output p = Fp(h) = fN",Synthetic gradients in operation,[0],[0]
(. . .,Synthetic gradients in operation,[0],[0]
(fk+1(h))),Synthetic gradients in operation,[0],[0]
"and incurs a loss L = L(p, y) based on a label y.
With regular backpropagation, the learning signal for the first network Fh is ∂L/∂h, which is a signal that specifies how the input to Fp should be changed in order to reduce the loss.",Synthetic gradients in operation,[0],[0]
"When we attach a linear SG between these two networks, the first sub-network Fh no longer receives the exact learning signal from Fp, but an approximation SG(h, y), which implies that Fh will be minimising an approximation of the loss, because it is using approximate error gradients.",Synthetic gradients in operation,[0],[0]
"Since the SG module is a linear model of ∂L/∂h, the approximation of the true loss that Fh is being optimised for will be a quadratic function of h and y. Note that this is not what a second order method does when a function is locally approximated with a quadratic and used for optimisation – here we are approximating the current loss, which is a function of parameters θ with a quadratic which is a function of h. Three appealing properties of an approximation based on h is that h already encapsulates a lot of non-linearities due to the processing of Fh, h is usually vastly lower dimensional than θ<h which makes learning more tractable, and the error only depends on quantities (h) which are local to this part of the network rather than θ which requires knowledge of the entire network.
",Synthetic gradients in operation,[0],[0]
"With the SG module in place, the learning system decomposes into two tasks: the second sub-network Fp tasked with minimising L given inputs h, while the first subnetwork Fh is tasked with pre-processing x in such a way that the best fitted quadratic approximator of L (wrt. h) is minimised.",Synthetic gradients in operation,[0],[0]
"In addition, the SG module is tasked with best approximating L.
The approximations and changing of learning objectives (described above) that are imposed by using synthetic gradients may appear to be extremely limiting.",Synthetic gradients in operation,[0],[0]
"However, in
both the theoretical and empirical sections of this paper we show that SG models can, and do, learn solutions to highly non-linear problems (such as memorising noise).
",Synthetic gradients in operation,[0],[0]
The crucial mechanism that allows such rich behaviour is to remember that the implicit quadratic approximation to the loss implied by the SG module is local (per data point) and non-stationary – it is continually trained itself.,Synthetic gradients in operation,[0],[0]
"It is not a single quadratic fit to the true loss over the entire optimisation landscape, but a local quadratic approximation specific to each instantaneous moment in optimisation.",Synthetic gradients in operation,[0],[0]
"In addition, because the quadratic approximation is a function only of h and not θ, the loss approximation is still highly non-linear w.r.t.",Synthetic gradients in operation,[0],[0]
"θ.
",Synthetic gradients in operation,[0],[0]
"If, instead of a linear SG module, one uses a more complex function approximator of gradients such as an MLP, the loss is effectively approximated by the integral of the MLP.",Synthetic gradients in operation,[0],[0]
"More formally, the loss implied by the SG module in hypotheses spaceH is of class {l : ∃g ∈ H : ∂l/∂h = g}1.",Synthetic gradients in operation,[0],[0]
"In particular, this shows an attractive mathematical benefit over predicting loss directly: by modelling gradients rather than losses, we get to implicitly model higher order loss functions.",Synthetic gradients in operation,[0],[0]
We now consider the effect SG has on critical points of the optimisation problem.,3. Critical points,[0],[0]
"Concretely, it seems natural to ask whether a model augmented with SG is capable of learning the same functions as the original model.",3. Critical points,[0],[0]
"We ask this question under the assumption of a locally converging training method, such that we always end up in a critical point.",3. Critical points,[0],[0]
"In the case of a SG-based model this implies a set of parameters θ such that ∂L/∂θ>h = 0, SG(h, y)∂h/∂θ<h = 0 and ∂LSG/∂θSG = 0.",3. Critical points,[0],[0]
"In other words we are trying to establish whether SG introduces regularisation to the model class, which changes the critical points, or whether it merely introduces a modification to learning dynamics, but retains the same set of critical points.
",3. Critical points,[0],[0]
"In general, the answer is positive: SG does induce a regularisation effect.",3. Critical points,[0],[0]
"However, in the presence of additional assumptions, we can show families of models and losses for which the original critical points are not affected.
",3. Critical points,[0],[0]
Proposition 1.,3. Critical points,[0],[0]
Every critical point of the original optimisation problem where SG can produce ∂L/∂hi has a corresponding critical point of the SG-based model.,3. Critical points,[0],[0]
Proof.,3. Critical points,[0],[0]
"Directly from the assumption we have that there exists a set of SG parameters such that the loss is minimal, thus ∂LSG/∂θSG = 0 and also SG(h, y) = ∂L/∂h and SG(h, y)∂h/∂θ<h = 0.
",3. Critical points,[0],[0]
The assumptions of this proposition are true for example when L = 0,3. Critical points,[0],[0]
"(one attains global minimum), when
1We mean equality for all points where ∂l/∂h is defined.
",3. Critical points,[0],[0]
"Understanding Synthetic Gradients and DNIs
∂L/∂hi = 0 or a network is a deep linear model trained with MSE and SG is linear.
",3. Critical points,[0],[0]
"In particular, this shows that for a large enough SG module all the critical points of the original problem have a corresponding critical point in the SG-based model.",3. Critical points,[0],[0]
"Limiting the space of SG hypotheses leads to inevitable reduction of number of original critical points, thus acting as a regulariser.",3. Critical points,[0],[0]
"At first this might look like a somewhat negative result, since in practice we rarely use a SG module capable of exactly producing true gradients.",3. Critical points,[0],[0]
"However, there are three important observations to make: (1) Our previous observation reflects having an exact representation of the gradient at the critical point, not in the whole parameter space.",3. Critical points,[0],[0]
"(2) One does preserve all the critical points where the loss is zero, and given current neural network training paradigms these critical points are important.",3. Critical points,[0],[0]
For such cases even if SG is linear the critical points are preserved.,3. Critical points,[0],[0]
(3) In practice one rarely optimises to absolute convergence regardless of the approach taken; rather we obtain numerical convergence meaning that ‖∂L/∂θ‖ is small enough.,3. Critical points,[0],[0]
"Thus, all one needs from SG-based model is to have small enough ‖(∂L/∂h+e)∂h/∂θ<h‖ ≤",3. Critical points,[0],[0]
"‖∂L/∂θ<h‖+‖e‖‖∂h/∂θ<h‖, implying that the approximation error at a critical point just has to be small wrt to ‖∂h/∂θ<h‖ and need not be 0.
",3. Critical points,[0],[0]
To recap: so far we have shown that SG can preserve critical points of the optimisation problem.,3. Critical points,[0],[0]
"However, SG can also introduce new critical points, leading to premature convergence and spurious additional solutions.",3. Critical points,[0],[0]
"As with our previous observation, this does not effect SG modules which are able to represent gradients exactly.",3. Critical points,[0],[0]
"But if the SG hypothesis space does not include a good approximator2 of the true gradient, then we can get new critical points which end up being an equilibrium state between SG modules and the original network.",3. Critical points,[0],[0]
We provide an example of such an equilibrium in the Supplementary Materials Section A.,3. Critical points,[0],[0]
"Having demonstrated that important critical points are preserved and also that new ones might get created, we need a better characterisation of the basins of attraction, and to understand when, in both theory and practice, one can expect convergence to a good solution.",4. Learning dynamics,[0],[0]
We conduct an empirical analysis of the learning dynamics on easily analysable artificial data.,Artificial Data,[0],[0]
"We create 2 and 100 dimensional versions of four basic datasets (details in the Supplementary Materials Section C) and train four simple models (a linear model and a deep linear one with 10 hidden layers, trained to minimise MSE and log loss) with regular backprop and with a SG-based alternative to see
2In this case, our gradient approximation needs to be reasonable at every point through optimisation, not just the critical ones.
whether it (numerically) converges to the same solution.
",Artificial Data,[0],[0]
For MSE and both shallow and deep linear architectures the SG-based model converges to the global optimum (exact numerical results provided in Supplementary Material Table 2).,Artificial Data,[0],[0]
"However, this is not the case for logistic regression.",Artificial Data,[0],[0]
This effect is a direct consequence of a linear SG module being unable to model ∂L/∂p3,Artificial Data,[0],[0]
"(where p = xW + b is the output of logistic regression), which often approaches the step function (when data is linearly separable), and cannot be well approximated with a linear function SG(h, y) =",Artificial Data,[0],[0]
hA+ yB+C.,Artificial Data,[0],[0]
"Once one moves towards problems without this characteristic (e.g. random labeling) the problem vanishes, since now ∂L/∂p can be approximated much better.",Artificial Data,[0],[0]
"While this may not seem particularly significant, it illustrates an important characteristic of SG in the context of the log loss – it will struggle to overfit to training data, since it requires modeling step function type shapes, which is not possible with a linear model.",Artificial Data,[0],[0]
"In particular this means that for best performance one should adapt the SG module architecture to the loss function used —for MSE linear SG is a reasonable choice, however for log loss one should use architectures including a sigmoid σ applied pointwise to a linear SG, such as SG(h, y) = dσ(hA) + yB + C.
As described in Section 2, using a linear SG module makes the implicit assumption that loss is a quadratic function of the activations.",Artificial Data,[0],[0]
"Furthermore, in such setting we can actually reconstruct the loss being used up to some additive constant since ∂L/∂h = hA + yB + C implies that L(h) = 12hAh
T + (yB + C)hT + const.",Artificial Data,[0],[0]
"If we now construct a 2-dimensional dataset, where data points are arranged in a 2D grid, we can visualise the loss implicitly predicted by the SG module and compare it with the true loss for each point.
",Artificial Data,[0],[0]
Figure 2 shows the results of such an experiment when learning a highly non-linear model (5-hidden layer relu network).,Artificial Data,[0],[0]
"As one can see, the quality of the loss approximation has two main components to its dynamics.",Artificial Data,[0],[0]
"First, it is better in layers closer to the true loss (i.e. the topmost layers), which matches observations from Jaderberg et al. (2016) and the intuition that the lower layers solve a more complex problem (since they bootstrap their targets).",Artificial Data,[0],[0]
"Second, the loss is better approximated at the very beginning of the training and the quality of the approximation degrades slowly towards the end.",Artificial Data,[0],[0]
"This is a consequence of the fact that close to the end of training, the highly nonlinear model has quite complex derivatives which cannot be well represented in a space of linear functions.",Artificial Data,[0],[0]
"It is worth noting, that in these experiments, the quality of the loss approximation dropped significantly when the true loss was around 0.001, thus it created good approximations for the majority of the learning process.",Artificial Data,[0],[0]
"There is also an empirical
3∂L/∂p = exp(xW + b)/(1 + exp(xW + b))− y
confirmation of the previous claim, that with log loss and data that can be separated, linear SGs will have problems modeling this relation close to the end of training (Figure 2 (b) left), while there is no such problem for MSE loss (Figure 2 (a) left).",Artificial Data,[0],[0]
"It is trivial to note that if a SG module used is globally convergent to the true gradient, and we only use its output after it converges, then the whole model behaves like the one trained with regular backprop.",Convergence,[0],[0]
"However, in practice we never do this, and instead train the two models in parallel without waiting for convergence of the SG module.",Convergence,[0],[0]
"We now discuss some of the consequences of this, and begin by showing that as long as a synthetic gradient produced is close enough to the true one we still get convergence to the true critical points.",Convergence,[0],[0]
"Namely, only if the error introduced by SG, backpropagated to all the parameters, is consistently smaller than the norm of true gradient multiplied by some positive constant smaller than one, the whole system converges.",Convergence,[0],[0]
"Thus, we essentially need the SG error to vanish around critical points.
",Convergence,[0],[0]
Proposition 2.,Convergence,[0],[0]
"Let us assume that a SG module is trained in each iteration in such a way that it -tracks true gradient, i.e. that ‖SG(h, y)− ∂L/∂h‖ ≤ .",Convergence,[0],[0]
"If ‖∂h/∂θ<h‖ is upper bounded by some K and there exists a constant δ ∈ (0, 1) such that in every iteration K ≤",Convergence,[0],[0]
"‖∂L/∂θ<h‖ 1−δ1+δ , then the whole training process converges to the solution of the
original problem.",Convergence,[0],[0]
Proof.,Convergence,[0],[0]
"Proof follows from showing that, under the assumptions, effectively we are training with noisy gradients, where the noise is small enough for convergence guarantees given by Zoutendijk (1970); Gratton et al. (2011) to apply.",Convergence,[0],[0]
"Details are provided in the Supplementary Materials Section B.
As a consequence of Proposition 2 we can show that with specifically chosen learning rates (not merely ones that are small enough) we obtain convergence for deep linear models.
",Convergence,[0],[0]
Corollary 1.,Convergence,[0],[0]
"For a deep linear model minimising MSE, trained with a linear SG module attached between two of its hidden layers, there exist learning rates in each iteration such that it converges to the critical point of the original problem.",Convergence,[0],[0]
Proof.,Convergence,[0],[0]
Proof follows directly from Propositions 1 and 2.,Convergence,[0],[0]
"Full proof is given in Supplementary Materials Section B.
For a shallow model we can guarantee convergence to the global solution provided we have a small enough learning rate, which is the main theoretical result of this paper.
",Convergence,[0],[0]
Theorem 1.,Convergence,[0],[0]
Let us consider linear regression trained with a linear SG module attached between its output and the loss.,Convergence,[0],[0]
"If one chooses the learning rate of the SG module using line search, then in every iteration there exists small
enough, positive learning rate of the main network such that it converges to the global solution.
",Convergence,[0],[0]
Proof.,Convergence,[0],[0]
"The general idea (full proof in the Supplementary Materials Section B) is to show that with assumed learning rates the sum of norms of network error and SG error decreases in every iteration.
",Convergence,[0],[0]
"Despite covering a quite limited class of models, these are the very first convergence results for SG-based learning.",Convergence,[0],[0]
"Unfortunately, they do not seem to easily generalise to the non-linear cases, which we leave for future research.",Convergence,[0],[0]
We now shift our attention to more realistic data.,5. Trained models,[0],[0]
"We train deep relu networks of varied depth (up to 50 hidden layers) with batch-normalisation and with two different activation functions on MNIST and compare models trained with full backpropagation to variants that employ a SG module in the middle of the hidden stack.
",5. Trained models,[0],[0]
"Figure 4 shows, that SG-based architectures converge well even if there are many hidden layers both below and above the module.",5. Trained models,[0],[0]
"Interestingly, SG-based models actually seem to converge faster (compare for example 20- or 50 layer deep relu network).",5. Trained models,[0],[0]
"We believe this may be due to some amount of loss function smoothing since, as described in Section 2, a linear SG module effectively models the loss function to be quadratic – thus the lower network has a simpler optimisation task and makes faster learning progress.
",5. Trained models,[0],[0]
Obtaining similar errors on MNIST does not necessarily mean that trained models are the same or even similar.,5. Trained models,[0],[0]
"Since the use of synthetic gradients can alter learning dynamics and introduce new critical points, they might converge to different types of models.",5. Trained models,[0],[0]
"Assessing the representational similarity between different models is difficult, however.",5. Trained models,[0],[0]
"One approach is to compute and visualise Representational Dissimilarity Matrices (Kriegeskorte et al., 2008) for our data.",5. Trained models,[0],[0]
"We sample a subset of 400 points from MNIST, order them by label, and then record activations on each hidden layer when the network is presented with these points.",5. Trained models,[0],[0]
"We plot the pairwise correlation matrix for each layer, as shown in Figure 3.",5. Trained models,[0],[0]
"This representation is permutation invariant, and thus the emergence of a block-diagonal correlation matrix means that at a given layer, points from the same class already have very correlated representations.
",5. Trained models,[0],[0]
Under such visualisations one can notice qualitative differences between the representations developed under standard backpropagation training versus those delivered by a SG-based model.,5. Trained models,[0],[0]
"In particular, in the MNIST model with 20 hidden layers trained with standard backpropagation we see that the representation covariance after 9 layers is nearly the same as the final layer’s representation.",5. Trained models,[0],[0]
"However, by contrast, if we consider the same architecture but with a SG module in the middle we see that the layers before the SG module develop a qualitatively different style of representation.",5. Trained models,[0],[0]
Note: this does not mean that layers before SG do not learn anything useful.,5. Trained models,[0],[0]
"To confirm this, we also introduced linear classifier probes (Alain & Bengio, 2016) and observed that, as with the pure backpropagation trained model, such probes can achieve 100% training accuracy after the first two hidden-layers of the SGbased model, as shown in Supplementary Material’s Figure 8.",5. Trained models,[0],[0]
"With 20 SG modules (one between every pair of layers), the representation is scattered even more broadly: we see rather different learning dynamics, with each layer contributing a small amount to the final solution, and there is no longer a point in the progression of layers where the representation is more or less static in terms of correlation structure (see Figure 3).
",5. Trained models,[0],[0]
"Understanding Synthetic Gradients and DNIs
Another way to investigate whether the trained models are qualitatively similar is to examine the norms of the weight matrices connecting consecutive hidden layers, and to assess whether the general shape of such norms are similar.",5. Trained models,[0],[0]
"While this does not definitively say anything about how much of the original classification is being solved in each hidden layer, it is a reasonable surrogate for how much computation is being performed in each layer4.",5. Trained models,[0],[0]
"According
to our experiments (see Figure 5 for visualisation of one of the runs), models trained with backpropagation on MNIST tend to have norms slowly increasing towards the output of the network (with some fluctuations and differences coming from activation functions, random initialisations, etc.).",5. Trained models,[0],[0]
"If we now put a SG in between every two hidden layers, we get norms that start high, and then decrease towards the output of the network (with much more variance now).",5. Trained models,[0],[0]
"Finally, if we have a single SG module we can observe that the behaviour after the SG module resembles, at least to some degree, the distributions of norms obtained with backpropagation, while before the SG it is more chaotic, with some similarities to the distribution of weights with SGs in-between every two layers.
",5. Trained models,[0],[0]
These observations match the results of the previous experiment and the qualitative differences observed.,5. Trained models,[0],[0]
When synthetic gradients are used to deliver full unlocking we obtain a very basic model at the lowest layers and then see iterative corrections in deeper layers.,5. Trained models,[0],[0]
"For a one-point unlocked model with a single SG module, we have two slightly separated models where one behaves similarly to backprop, and the other supports it.",5. Trained models,[0],[0]
"Finally, a fully locked model (i.e. traditional backprop) solves the task relatively early on, and later just increases its confidence.
",5. Trained models,[0],[0]
"4We train with a small L2 penalty added to weights to make norm correspond roughly to amount of computation.
",5. Trained models,[0],[0]
"We note that the results of this section support our previous notion that we are effectively dealing with a multi-agent system, which looks for coordination/equilibrium between components, rather than a single model which simply has some small noise injected into the gradients (and this is especially true for more complex models).",5. Trained models,[0],[0]
We now shift our attention and consider a unified view of several different learning principles that work by replacing true gradients with surrogates.,6. SG and conspiring networks,[0],[0]
"We focus on three such approaches: Feedback Alignment (FA) (Lillicrap et al., 2016), Direct Feedback Alignment (DFA) (Nøkland, 2016), and Kickback (KB) (Balduzzi et al., 2014).",6. SG and conspiring networks,[0],[0]
"FA effectively uses a fixed random matrix during backpropagation, rather than the transpose of the weight matrix used in the forward pass.",6. SG and conspiring networks,[0],[0]
"DFA does the same, except each layer directly uses the learning signal from the output layer rather than the subsequent local one.",6. SG and conspiring networks,[0],[0]
KB also pushes the output learning signal directly but through a predefined matrix instead of a random one.,6. SG and conspiring networks,[0],[0]
"By making appropriate choices for targets, losses, and model structure we can cast all of these methods in the SG framework, and view them as comprising two networks with a SG module in between them, wherein the first module builds a representation which makes the task of the SG predictions easier.
",6. SG and conspiring networks,[0],[0]
We begin by noting that in the SG models described thus far we do not backpropagate the SG error back into the part of the main network preceding the SG module (i.e. we assume ∂LSG/∂h = 0).,6. SG and conspiring networks,[0],[0]
"However, if we relax this restriction, we can use this signal (perhaps with some scaling factor α) and obtain what we will refer to as a SG + prop model.",6. SG and conspiring networks,[0],[0]
"Intuitively, this additional learning signal adds capacity to our SG model and forces both the main network and the SG module to “conspire” towards a common goal of making better gradient predictions.",6. SG and conspiring networks,[0],[0]
"From a practical perspective, according to our experiments, this additional signal heavily stabilises learning system5.",6. SG and conspiring networks,[0],[0]
"However, this comes at the cost of no longer being unlocked.
",6. SG and conspiring networks,[0],[0]
"Our main observation in this section is that FA, DFA, and KB can be expressed in the language of “conspiring” networks (see Table 1), of two-network systems that use a SG module.",6. SG and conspiring networks,[0],[0]
The only difference between these approaches is how one parametrises SG and what target we attempt to fit it to.,6. SG and conspiring networks,[0],[0]
"This comes directly from the construction of these
5 In fact, ignoring the gradients predicted by SG and only using the derivative of the SG loss, i.e. ∂LSG/∂h, still provides enough learning signal to converge to a solution for the original task in the simple classification problems we considered.",6. SG and conspiring networks,[0],[0]
"We posit a simple rationale for this: if one can predict gradients well using a simple transformation of network activations (e.g. a linear mapping), this suggests that the loss itself can be predicted well too, and thus (implicitly) so can the correct outputs.
",6. SG and conspiring networks,[0],[0]
"Understanding Synthetic Gradients and DNIs
Network
fi
fi+1 fi+2
…
…
… … fi fi+1 fi+2
…
…
… …
Mi+1
i ̂i
Mi+2 ̂i+1 i+1
(a) (b) (c)
Differentiable Legend:
x y
L h SG LSG
x y
L h Forward connection, differentiable Forward connection, non-differentiable Error gradient, non-differentiable Synthetic error gradient, differentiable Legend: Synthetic error gradient, nondifferentiable
Non-differentiable Forward connection
Error gradient
Synthetic error gradient
L
h SG
LSG
SG
p
L
h h
LSG
Bprop
p
L
h SG
LSG
SG + prop
p
L
h hA
DFA
p
L
h hA
LSG
FA
p
g=hW
L
h h1
LSG
Kickback
p
LSG
Method
∂̂L/∂h SG(h, y) SG(h, y) + α∂LSG∂h ∂L/∂h (∂L/∂p)A T (∂L/∂g)AT (∂L/∂p)1T SG(h, y) SG(h, y) SG(h, y) h hA hA h1 SG trains",6. SG and conspiring networks,[0],[0]
yes yes,6. SG and conspiring networks,[0],[0]
no no,6. SG and conspiring networks,[0],[0]
no no SG target ∂L/∂h ∂L/∂h −∂L/∂h −∂L/∂p −∂L/∂g −∂L/∂p,6. SG and conspiring networks,[0],[0]
"LSG(t, s) ‖t− s‖2 ‖t− s‖2 −〈t, s〉 −〈t, s〉 −〈t, s〉 −〈t, s〉 Update locked no",6. SG and conspiring networks,[0],[0]
yes*,6. SG and conspiring networks,[0],[0]
yes,6. SG and conspiring networks,[0],[0]
yes yes,6. SG and conspiring networks,[0],[0]
yes Backw.,6. SG and conspiring networks,[0],[0]
locked,6. SG and conspiring networks,[0],[0]
no,6. SG and conspiring networks,[0],[0]
yes*,6. SG and conspiring networks,[0],[0]
yes no,6. SG and conspiring networks,[0],[0]
yes no Direct error,6. SG and conspiring networks,[0],[0]
no no,6. SG and conspiring networks,[0],[0]
no,6. SG and conspiring networks,[0],[0]
"yes no yes
Table 1.",6. SG and conspiring networks,[0],[0]
"Unified view of “conspiring” gradients methods, including backpropagation, synthetic gradients are other error propagating methods.",6. SG and conspiring networks,[0],[0]
"For each of them, one still trains with regular backpropagation (chain rule) however ∂L/∂h is substituted with a particular ∂̂L/∂h.",6. SG and conspiring networks,[0],[0]
"Black lines are forward signals, blue ones are synthetic gradients, and green ones are true gradients.",6. SG and conspiring networks,[0],[0]
Dotted lines represent non-differentiable operations.,6. SG and conspiring networks,[0],[0]
The grey modules are not trainable.,6. SG and conspiring networks,[0],[0]
"A is a fixed, random matrix and 1 is a matrix of ones of an appropriate dimension.",6. SG and conspiring networks,[0],[0]
*,6. SG and conspiring networks,[0],[0]
"In SG+Prop the network is locked if there is a single SG module, however if we have multiple ones, then propagating error signal only locks a module with the next one, not with the entire network.",6. SG and conspiring networks,[0],[0]
"Direct error means that a model tries to solve classification problem directly at layer h.
systems, and the fact that if we treat our targets as constants (as we do in SG methods), then the backpropagated error from each SG module (∂LSG/∂h) matches the prescribed update rule of each of these methods (∂̂L/∂h).",6. SG and conspiring networks,[0],[0]
One direct result from this perspective is the fact that Kickback is essentially DFA with A = 1.,6. SG and conspiring networks,[0],[0]
"For completeness, we note that regular backpropagation can also be expressed in this unified view – to do so, we construct a SG module such that the gradients it produces attempt to align the layer activations with the negation of the true learning signal (−∂L/∂h).",6. SG and conspiring networks,[0],[0]
"In addition to unifying several different approaches, our mapping also illustrates the potential utility and diversity in the generic idea of predicting gradients.",6. SG and conspiring networks,[0],[0]
This paper has presented new theory and analysis for the behaviour of synthetic gradients in feed forward models.,7. Conclusions,[0],[0]
"Firstly, we showed that introducing SG does not necessarily change the critical points of the original problem, however at the same time it can introduce new critical points into the learning process.",7. Conclusions,[0],[0]
This is an important result showing that SG does not act like a typical regulariser despite simplifying the error signals.,7. Conclusions,[0],[0]
"Secondly, we showed that (despite modifying learning dynamics) SG-based models converge
to analogous solutions to the true model under some additional assumptions.",7. Conclusions,[0],[0]
"We proved exact convergence for a simple class of models, and for more complex situations we demonstrated that the implicit loss model captures the characteristics of the true loss surface.",7. Conclusions,[0],[0]
It remains an open question how to characterise the learning dynamics in more general cases.,7. Conclusions,[0],[0]
"Thirdly, we showed that despite these convergence properties the trained networks can be qualitatively different from the ones trained with backpropagation.",7. Conclusions,[0],[0]
"While not necessarily a drawback, this is an important consequence one should be aware of when using synthetic gradients in practice.",7. Conclusions,[0],[0]
"Finally, we provided a unified framework that can be used to describe alternative learning methods such as Synthetic Gradients, FA, DFA, and Kickback, as well as standard Backprop.",7. Conclusions,[0],[0]
The approach taken shows that the language of predicting gradients is suprisingly universal and provides additional intuitions and insights into the models.,7. Conclusions,[0],[0]
"The authors would like to thank James Martens and Ross Goroshin for their valuable remarks and discussions.
",Acknowledgments,[0],[0]
Understanding Synthetic Gradients and DNIs,Acknowledgments,[0],[0]
We can show an example of SG introducing new critical points.,Critical points,[0],[0]
"Consider a small one-dimensional training dataset {−2,−1, 1, 2} ⊂ R, and let us consider a simple system where the model f : R → R is parametrised with two scalars, a and b and produces ax + b.",Critical points,[0],[0]
"We train it to minimise L(a, b) =",Critical points,[0],[0]
∑4 i=1 |axi,Critical points,[0],[0]
+ b|.,Critical points,[0],[0]
"This has a unique minimum which is obtained for a = b = 0, and standard gradient based methods will converge to this solution.",Critical points,[0],[0]
"Let us now attach a SG module betweenf and L. This module produces a (trainable) scalar value c ∈ R (thus it produces a single number, independent from the input).",Critical points,[0],[0]
"Regardless of the value of a, we have a critical point of the SG module when b = 0 and c = 0.",Critical points,[0],[0]
"However, solutions with a = 1 and c = 0 are clearly not critical points of the original system.",Critical points,[0],[0]
Figure 6 shows the loss surface and the fitting of SG module when it introduces new critical point.,Critical points,[0],[0]
Theorem 1 Let us consider linear regression trained with a linear SG module attached between its output and the loss.,B. Proofs,[0],[0]
"If one chooses the learning rate of the SG module using line search, then in every iteration there exists small
Understanding Synthetic Gradients and DNIs
enough, positive learning rate of the main network such that it converges to the global solution.
",B. Proofs,[0],[0]
Proof.,B. Proofs,[0],[0]
Let X = {xs}Ss=1 ∈,B. Proofs,[0],[0]
"Rd×S be the data, let {ys}Ss=1 ∈ R1×S be the labels.",B. Proofs,[0],[0]
Throughout the proof k will be the iteration of training.,B. Proofs,[0],[0]
We denote by 1 ∈ R1×S a row vector in which every element is 1.,B. Proofs,[0],[0]
We also follow the standard convention of including the bias in the weight matrix by augmenting the data X with one extra coordinate always equal to 1.,B. Proofs,[0],[0]
"Thus, we denote X̄ =",B. Proofs,[0],[0]
(,B. Proofs,[0],[0]
"XT |1T )T , X̄ ∈ R(d+1)×S and x̄s-the columns of X̄.",B. Proofs,[0],[0]
"Using that convention, the weight matrix is Wk ∈ R1×(d+1).",B. Proofs,[0],[0]
"We have
psk := Wkx̄ s,
L = 1
2 S∑ s=1 (ys − psk) 2 = 1 2 n∑ i=1",B. Proofs,[0],[0]
"(ys −Wkx̄s)2 .
",B. Proofs,[0],[0]
"Our aim is to find arg min
W,b L.
We use
∂L ∂W = ∂L",B. Proofs,[0],[0]
∂p ∂p ∂W = S∑ s=1,B. Proofs,[0],[0]
"∂L ∂ps ∂ps ∂W =
S∑ s=1",B. Proofs,[0],[0]
"∂L ∂ps x̄s = S∑ s=1 (ys −Wkx̄s) (x̄s)T
∂L ∂p =",B. Proofs,[0],[0]
"( p1 − y1, . . .",B. Proofs,[0],[0]
", pS − yS )",B. Proofs,[0],[0]
We will use the following parametrization of the synthetic gradient ∇̃Lk,B. Proofs,[0],[0]
= (αk+1)pk−(βk+1)y+γk1.,B. Proofs,[0],[0]
"The reason for using this form instead of simply akpk + bky + ck1 is that we are going to show that under DNI this synthetic gradient will converge to the “real gradient” ∂L∂p , which means showing that lim
k→∞ (αk, βk, γk) = (0, 0, 0).",B. Proofs,[0],[0]
"Thanks to this
choice of parameters αk, βk, γk we have the simple expression for the error
Ek = ∥∥∥∥∇̃Lk − ∂L∂p",B. Proofs,[0],[0]
"∥∥∥∥2 2 =
‖(αk + 1)pk − (βk + 1)y + γk1−( p1k",B. Proofs,[0],[0]
"− y1, . . .",B. Proofs,[0],[0]
", pSk − yS )∥∥2 2
=∥∥(αkp1k − βky1 + γk, . . .",B. Proofs,[0],[0]
", αkpSk",B. Proofs,[0],[0]
"− βkyS + γk)∥∥22 Parameters αk, βk, γk will be updated using the gradient descent minimizing the error E. We have
∂E ∂α = S∑ s=1",B. Proofs,[0],[0]
"(αkp s k − βkys + γk)psk
∂E",B. Proofs,[0],[0]
∂β = − S∑ s=1,B. Proofs,[0],[0]
"(αkp s k − βkys + γk)ys
∂E ∂γ",B. Proofs,[0],[0]
"= S∑ s=1 (αkp s k − βkys + γk).
",B. Proofs,[0],[0]
"As prescribed in Jaderberg et al. (2016), we start our iterative procedure from the synthetic gradient being equal to zero and we update the parameters by adding the (negative) gradient multiplied by a learning rate ν.",B. Proofs,[0],[0]
"This means that we apply the iterative procedure:
α0 = −1, β0 = −1, γ0 = 0
Wk+1 =Wk − µ S∑ s=1",B. Proofs,[0],[0]
"((αk + 1)p s k−
(βk + 1)y s + γk) (x̄ s)T
αk+1 =αk − ν S∑ s=1",B. Proofs,[0],[0]
"(αkp s k − βkys + γk)psk
βk+1",B. Proofs,[0],[0]
=βk + ν S∑ s=1,B. Proofs,[0],[0]
"(αkp s k − βkys + γk)ys
γk+1 =γk − ν S∑ s=1 (αkp s k − βkys + γk).
",B. Proofs,[0],[0]
"Using matrix notation
Wk+1 = Wk − µ((αk + 1)pk − (βk + 1)y + γk1)X̄T αk+1 = αk − ν",B. Proofs,[0],[0]
"( αk‖pk‖22 − βk〈y,pk〉+ γk〈1,pk〉 ) βk+1",B. Proofs,[0],[0]
= βk + ν,B. Proofs,[0],[0]
"( αk〈pk,y〉 − βk‖y‖22 + γk〈1,y〉
) γk+1",B. Proofs,[0],[0]
= γk,B. Proofs,[0],[0]
"− ν (αk〈1,pk〉 − βk〈1,y〉+",B. Proofs,[0],[0]
"Sγk)
Note, that the subspace given by α = β = γ = 0 is invariant under this mapping.",B. Proofs,[0],[0]
"As noted before, this corresponds to the synthetic gradient being equal to the real gradient.",B. Proofs,[0],[0]
"Proving the convergence of SG means showing, that a trajectory starting from α0 = −1, β0 = −1, γ0 = 0 converges to W = W0, α = β = γ = 0, where W0 are the “true” weigts of the linear regression.",B. Proofs,[0],[0]
"We are actually going to prove more, we will show that W = W0, α = β = γ = 0 is in fact a global attractor, i.e. that any trajectory converges to that point.",B. Proofs,[0],[0]
Denoting ω =,B. Proofs,[0],[0]
"(α, β, γ)t we get
Wk+1 =",B. Proofs,[0],[0]
Wk − µ((αk + 1)pk,B. Proofs,[0],[0]
− (βk + 1)y + γk1)X̄T ωk+1 =,B. Proofs,[0],[0]
ωk − ν,B. Proofs,[0],[0]
"[ pTk | − yT |1T ]T [ pTk | − yT |1T ] ωk
Wk+1 = Wk − µ(pk − y)X̄T − µωTk",B. Proofs,[0],[0]
"[ pTk | − yT |1T ]T X̄T
ωk+1 =",B. Proofs,[0],[0]
ωk − ν,B. Proofs,[0],[0]
"[ pTk | − yT |1T ]T [ pTk | − yT |1T ] ωk.
",B. Proofs,[0],[0]
Denoting by Ak =,B. Proofs,[0],[0]
"[ pTk | − yT |1T ] we get
Wk+1 = Wk − µ(pk − y)X̄T − µωTATk X̄T
ωk+1 = ωk − νATkAkωk.
",B. Proofs,[0],[0]
"Understanding Synthetic Gradients and DNIs
Multiplying both sides of the first equation by X̄ we obtain
Wk+1X̄ = WkX̄− µ(pk − y)X̄T X̄− µωTATk X̄T X̄",B. Proofs,[0],[0]
ωk+1 = ωk,B. Proofs,[0],[0]
"− νATkAkωk.
",B. Proofs,[0],[0]
Denote B = X̄T X̄.,B. Proofs,[0],[0]
"We get
pk+1 = pk",B. Proofs,[0],[0]
"− µpkB + µyB− µωTkATkB ωk+1 = ωk − νATkAkωk.
",B. Proofs,[0],[0]
"Denoting ek = (y − pk)T we get
ek+1 = ek − µBek + µBAkωk ωk+1 = ωk − νATkAkωk.
",B. Proofs,[0],[0]
We will use the symbol ξ = Akωk.,B. Proofs,[0],[0]
"Then
ek+1 = ek − µBek + µBξk",B. Proofs,[0],[0]
ξk+1,B. Proofs,[0],[0]
= ξk,B. Proofs,[0],[0]
"− νAkATk ξk.
(1)
",B. Proofs,[0],[0]
Every vector v can be uniquely expressed as a sum v = v⊥ + v‖ with X̄v⊥ = 0 and v‖ = X̄T θ for some θ (v‖ is a projection of v onto the linear subspace spanned by the columns of X̄).,B. Proofs,[0],[0]
"Applying this decomposition to ek = e⊥k + e ‖ k we get
e⊥k+1 = e ⊥ k",B. Proofs,[0],[0]
− µ(Bek)⊥,B. Proofs,[0],[0]
+ µ(Bξk)⊥ e ‖ k+1 = e ‖,B. Proofs,[0],[0]
k,B. Proofs,[0],[0]
− µ(Bek) ‖ + µ(Bξk) ‖,B. Proofs,[0],[0]
ξk+1,B. Proofs,[0],[0]
=,B. Proofs,[0],[0]
ξk,B. Proofs,[0],[0]
"− νAkATk ξk.
",B. Proofs,[0],[0]
"Note now, that as B = X̄T X̄, for any vector v there is (Bv)⊥ = 0, and (Bv)‖ = Bv (because the operator v 7→ v‖ is a projection).",B. Proofs,[0],[0]
"Moreover, Bv = Bv‖.",B. Proofs,[0],[0]
"Therefore
e⊥k+1 = e ⊥ k e ‖ k+1 = e ‖ k − µ(Be ‖ k) + µ(Bξk) ‖",B. Proofs,[0],[0]
ξk+1,B. Proofs,[0],[0]
=,B. Proofs,[0],[0]
ξk,B. Proofs,[0],[0]
"− νAkATk ξk.
",B. Proofs,[0],[0]
The value e⊥k does not change.,B. Proofs,[0],[0]
"Thus, we will be omitting the first equation.",B. Proofs,[0],[0]
"Note, that e⊥k is “the residue”, the smallest error that can be obtained by a linear regression.",B. Proofs,[0],[0]
"For the sake of visual appeal we will denote f = e‖k
fk+1 = fk − µBfk + µBξk ξk+1",B. Proofs,[0],[0]
= ξk,B. Proofs,[0],[0]
"− νAkATk ξk.
Taking norms and using ‖u+ v‖ ≤ ‖u‖+ ‖v‖ we obtain
‖fk+1‖2 ≤ ‖fk − µBfk‖2 + µ‖Bξk‖2 ‖ξk+1‖22 = ‖ξk‖22 − 2ν‖ATk ξk‖22 + ν2‖AkATk ξk‖22.
",B. Proofs,[0],[0]
Observe that ‖fk − µBfk‖22 = ‖fk‖22,B. Proofs,[0],[0]
− 2µfkBfk + µ2‖Bfk‖22.,B. Proofs,[0],[0]
"As B is a constant matrix, there exists a constant b > 0",B. Proofs,[0],[0]
"such that vTBv ≥ b‖v‖22 for any v satisfying
v‖ = v. Therefore ‖fk",B. Proofs,[0],[0]
− µBfk‖22 ≤ ‖fk‖22 − 2µb‖fk‖22 + µ2‖B‖2‖fk‖22.,B. Proofs,[0],[0]
"Using that and ‖Bξk‖2 ≤ ‖B‖‖ξk‖2 we get
‖fk+1‖2 ≤",B. Proofs,[0],[0]
"√
1− 2µb+ µ2‖B‖2‖fk‖2 + µ‖B‖‖ξk‖2 ‖ξk+1‖22",B. Proofs,[0],[0]
= ‖ξk‖22,B. Proofs,[0],[0]
"− 2ν‖ATk ξk‖22 + ν2‖AkATk ξk‖22.
",B. Proofs,[0],[0]
Let us assume that AkATk ξk 6= 0.,B. Proofs,[0],[0]
"In that case the righthand side of the second equation is a quadratic function is ν, whose minimum value is attained for ν = ‖A T k ξk‖ 2 2
‖AkATk ξk‖ 2 2 .",B. Proofs,[0],[0]
"For so-chosen ν we have
‖fk+1‖2 ≤ √ 1− 2µb+ µ2‖B‖2‖fk‖2 + µ‖B‖‖ξk‖2
‖ξk+1‖22 =",B. Proofs,[0],[0]
"( 1− ‖A T k ξk‖22
‖AkATk ξk‖22 ‖ATk ξk‖22 ‖ξk‖22
) ‖ξk‖22.
Consider a space {f}⊕{ξ} (concatenation of vectors) with a norm ‖{f} ⊕ {ξ}‖⊕ = ‖f‖2 + ‖ξ‖2.
‖{fk+1} ⊕ {ξk+1}‖⊕ ≤√ 1− 2µb+ µ2‖B‖2‖fk‖2 + µ‖B‖‖ξk‖2",B. Proofs,[0],[0]
+,B. Proofs,[0],[0]
"√
1− ‖ATk ξk‖22 ‖AkATk ξk‖22 ‖ATk ξk‖22 ‖ξk‖22 ‖ξk‖2 ≤
Using √
1− h ≤ 1− 12h we get
‖{fk+1} ⊕ {ξk+1}‖⊕ ≤ √
1− 2µb+ µ2‖B‖2‖fk‖2+( 1− ‖A T k ξk‖22
2‖AkATk ξk‖22 ‖ATk ξk‖22 ‖ξk‖22 + µ
) ‖ξk‖2
Note, that √
1− 2µb+ µ2‖B‖2 < 1 for 0 < µ ≤ b‖B‖2 .",B. Proofs,[0],[0]
"Thus, for
µ < min
{ b
‖B‖2 , 1− ‖A T k ξk‖22 2‖AkATk ξk‖22 ‖ATk ξk‖22 ‖ξk‖22
} ,
for every pair {fk+1} ⊕ {ξk+1} 6= {0} ⊕ {0} (and if they are zeros then we already converged) there is
‖{fk+1} ⊕ {ξk+1}‖⊕ < ‖{fk} ⊕ {ξk}‖⊕.
Therefore, by Theorem 2, the error pair {fk+1} ⊕ {ξk+1} has to converge to 0, which ends the proof in the case AkA T k ξk 6= 0.",B. Proofs,[0],[0]
"It remains to investigate what happens if AkA T k ξk = 0.
",B. Proofs,[0],[0]
We start by observing that either ξk = 0 or ATk ξk 6= 0 and AkA T k ξk 6= 0.,B. Proofs,[0],[0]
This follows directly from the definition ξk = Akωk.,B. Proofs,[0],[0]
"Indeed, if ξk 6= 0 there is 0 <",B. Proofs,[0],[0]
‖Akωk‖22 = ωTkA T k ξk and analogously 0,B. Proofs,[0],[0]
< ‖ATk,B. Proofs,[0],[0]
"ξk‖ = ξTkAkATk ξk.
",B. Proofs,[0],[0]
In case ξk = 0,B. Proofs,[0],[0]
"there is ‖{fk+1} ⊕ {ξk+1}‖⊕ = ‖ fk+1‖2 < √ 1− 2µb+ µ2‖B‖2‖fk‖2 =√
1− 2µb+ µ2‖B‖2‖{fk} ⊕ {ξk}‖⊕ and the theorem follows.
",B. Proofs,[0],[0]
"Understanding Synthetic Gradients and DNIs
Theorem 2.",B. Proofs,[0],[0]
Let B be a finite-dimensional Banach space.,B. Proofs,[0],[0]
Let f : B → B be a continuous map such that for every x ∈ B there is ‖f(x)‖ < ‖x‖.,B. Proofs,[0],[0]
"Then for every x there is lim n→∞ fn(x) = 0.
Proof.",B. Proofs,[0],[0]
Let ω(x) = {y : ∃i1<i2<...,B. Proofs,[0],[0]
lim n→∞ f in(x) = y}.,B. Proofs,[0],[0]
"Because ‖f(x)‖ < ‖x‖, the sequence x, f(x), f2(x), . .",B. Proofs,[0],[0]
.,B. Proofs,[0],[0]
"is contained in a ball of a radius ‖x‖, which due to a finite dimensionality of B is a compact set.",B. Proofs,[0],[0]
"Thus, ω(x) is nonempty.",B. Proofs,[0],[0]
"Moreover, from the definition, ω(x) is a closed set, and therefore it is a compact set.",B. Proofs,[0],[0]
"Let y0 = infy∈ω(x) ‖y‖ – which we know exists, due to the compactness of ω(x) and the continuity of ‖ · ‖ (Weierstraß theorem).",B. Proofs,[0],[0]
"But for every y ∈ ω(x) there is f(y) ∈ ω(x), thus there must be y0 = 0.",B. Proofs,[0],[0]
"By definition, for every ε, there exists n0 such that ‖fn0(x)‖ < ε.",B. Proofs,[0],[0]
"Therefore, for n > n0",B. Proofs,[0],[0]
‖fn(x)‖ < ε.,B. Proofs,[0],[0]
"Therefore, fn(x) must converge to 0.
",B. Proofs,[0],[0]
Proposition 2.,B. Proofs,[0],[0]
"Let us assume that a SG module is trained in each iteration in such a way that it -tracks true gradient, i.e. that ‖SG(h, y)− ∂L/∂h‖ ≤ .",B. Proofs,[0],[0]
"If ‖∂h/∂θ<h‖ is upper bounded by some K and there exists a constant δ ∈ (0, 1) such that in every iteration K ≤",B. Proofs,[0],[0]
"‖∂L/∂θ<h‖ 1−δ1+δ , then the whole training process converges to the solution of the original problem.
",B. Proofs,[0],[0]
Proof.,B. Proofs,[0],[0]
"Directly from construction we get that ‖∂L/∂θ<h− ∂̂L/∂̂θ<h‖ = ‖(∂L/∂h−SG(h, y))∂h/∂θ<h‖ ≤ K thus in each iteration there exists such a vector e, that ‖e‖ ≤ K and ∂̂L/∂̂θ<h = ∂L/∂θ<h + e. Consequently, we get a model trained with noisy gradients, where the noise of the gradient is bounded in norm by K so, directly from assumptions, it is also upper bounded by ‖∂L/∂θ<h‖ 1−δ1+δ",B. Proofs,[0],[0]
"and we we get that the direction followed is sufficient for convergence as this means that cosine between true gradient and synthetic gradient is uniformly bounded away (by δ) from zero (Zoutendijk, 1970; Gratton et al., 2011).",B. Proofs,[0],[0]
"At the same time, due to Proposition 1, we know that the assumptions do not form an empty set as the SG module can stay in an neighborhood of the gradient, and both norm of the synthetic gradient and ‖∂h/∂θ<h‖ can go to zero around the true critical point.
",B. Proofs,[0],[0]
Corollary 1.,B. Proofs,[0],[0]
"For a deep linear model and an MSE objective, trained with a linear SG module attached between two of its hidden layers, there exist learning rates in each iteration such that it converges to the critical point of the original problem.
",B. Proofs,[0],[0]
Proof.,B. Proofs,[0],[0]
"Denote the learning rate of the main model by µ and learning rate of the SG module by ν > 0 and put µ = max(0, ‖e‖ − 1/(3‖∂h/∂θ<h‖)‖∂L/∂θ<h‖), where is a small learning rate (for example found using line search)
and e is the error SG will make in the next iteration.",B. Proofs,[0],[0]
"The constant 1/3 appears here as it is equal to (1− δ)/(1 + δ) for δ = 0.5 which is a constant from Proposition 2, which we will need later on.",B. Proofs,[0],[0]
"Norm of e consists of the error fitting term LSG which we know, and the term depending on the previous µ value, since this is how much the solution for the SG problem evolved over last iteration.",B. Proofs,[0],[0]
"In such a setting, the main model changes iff
‖e‖‖∂h/∂θ<h‖ < 1/3‖∂L/∂θ<h‖. (2)
",B. Proofs,[0],[0]
"First of all, this takes place as long as ν is small enough since the linear SG is enough to represent ∂L/∂h with arbitrary precision (Proposition 1) and it is trained to do so in a way that always converges (as it is a linear regression fitted to a linear function).",B. Proofs,[0],[0]
So in the worst case scenario for a few first iterations we choose very small µ (it always exists since in the worst case scenario µ = 0 agrees with the inequality).,B. Proofs,[0],[0]
"Furthermore, once this happens we follow true gradient on θ>h and a noisy gradient on θ<h. Since the noise is equal to e∂h/∂θ<h we get that
‖e∂h/∂θ<h‖ ≤ ‖e‖‖∂h/∂θ<h‖ < 1/3‖∂L/∂θ<h‖,
which is equivalent to error for θ<h being upper bounded by (1 − δ)/(1 + δ)‖∂L/∂h‖ for δ = 0.5 which matches assumptions of Proposition 2, thus leading to the convergence of the model considered.",B. Proofs,[0],[0]
"If at any moment we lose track of the gradient again – the same mechanism kicks in - µ goes down for as long as the inequality (2) does not hold again (and it has to at some point, given ν is positive and small enough).",B. Proofs,[0],[0]
"All experiments were performed using TensorFlow (Abadi et al., 2016).",C. Technical details,[0],[0]
In all the experiments SG loss is the MSE between synthetic and true gradients.,C. Technical details,[0],[0]
"Since all SGs considered were linear, weights were initialized to zeros so initially SG produces zero gradients, and it does not affect convergence (since linear regression is convex).",C. Technical details,[0],[0]
"Each of the artificial datasets is a classification problem, consisting of X sampled from k-dimensional Gaussian distribution with zero mean and unit standard deviation.",Datasets,[0],[0]
For k = 2 we sample 100 points and for k = 100 we sample 1000.,Datasets,[0],[0]
"Labels y are generated in a way depending on the dataset name:
• lineark - we randomly sample an origin-crossing hyperplane (by sampling its parameters from standard Gaussians) and label points accordingly,
• noisyk - we label points according to lineark and then randomly swap labels of 10% of samples,
• randomk - points are labeled completely randomly.
",Datasets,[0],[0]
"We used one-hot encoding of binary labels to retain compatibility with softmax-based models, which is consistent with the rest of experiments.",Datasets,[0],[0]
However we also tested the same things with a single output neuron and regular sigmoid-based network and obtained analogous results.,Datasets,[0],[0]
"Optimisation is performed using the Adam optimiser (Kingma & Ba, 2014) with a learning rate of 3e−5.",Optimisation,[0],[0]
This applies to both main model and to SG module.,Optimisation,[0],[0]
"Table 2 shows results for training linear regression (shallow MSE), 10 hidden layer deep linear regression (deep MSE), logistic regression (shallow log loss) and 10 hidden layer deep linear classifier (deep log loss).",Artificial datasets,[0],[0]
"Since all these problems (after proper initialisation) converge to the global optima, we report the difference between final loss obtained for SG enriched models and the true global optimum.",Artificial datasets,[0],[0]
Networks used are simple feed forward networks with h layers of 512 hidden relu units followed by batch normalisation layers.,MNIST experiments,[0],[0]
The final layer is a regular 10-class softmax layer.,MNIST experiments,[0],[0]
"Inputs were scaled to [0, 1] interval, besides that there was no preprocessing applied.",MNIST experiments,[0],[0]
"In order to build RSMs for a layer h we sample 400 points (sorted according to their label) from the MNIST dataset, {xi}400i=1 and record activations on each of these points, hi = h(xi).",Representational Dissimilarity Matrices,[0],[0]
"Then we compute a matrix RSM such that RSMij = 1 − corr(hi, hj).",Representational Dissimilarity Matrices,[0],[0]
"Consequently a perfect RSM is a block diagonal matrix, thus elements of the same class have a representation with high correlation and the representations of points from two distinct classes are not correlated.",Representational Dissimilarity Matrices,[0],[0]
"Figure 7 is the extended version of the analogous Figure 3 from the main paper where we show RDMs for backpropagation, a single SG, SG in-between every two layers, and also the DFA model, when training 20 hidden layer deep relu network.
",Representational Dissimilarity Matrices,[0],[0]
"Understanding Synthetic Gradients and DNIs
dataset model MSE log loss
linear2 shallow 0.00000 0.03842 linear100 shallow 0.00002 0.08554 noisy2 shallow 0.00000 0.00036 noisy100 shallow",Representational Dissimilarity Matrices,[0],[0]
0.00002 0.00442 random2 shallow 0.00000 0.00000 random100 shallow 0.00004 0.00003 noisy2 deep 0.00000 0.00000 noisy100 deep 0.00001 0.00293 random2,Representational Dissimilarity Matrices,[0],[0]
"deep 0.00000 0.00000 random100 deep 0.00001 0.00004
Table 2.",Representational Dissimilarity Matrices,[0],[0]
Differences in final losses obtained for various models/datasets when trained with SG as compared to model trained with backpropagation.,Representational Dissimilarity Matrices,[0],[0]
Bolded entries denote experiments which converged to a different solution.,Representational Dissimilarity Matrices,[0],[0]
"lineark is k dimensional, linearly separable dataset, noisy is linearly separable up to 10% label noise, and random has completely random labeling.",Representational Dissimilarity Matrices,[0],[0]
"Shallow models means linear ones, while deep means 10 hidden layer deep linear models.",Representational Dissimilarity Matrices,[0],[0]
Reported differences are averaged across 10 different datasets from the same distributions.,Representational Dissimilarity Matrices,[0],[0]
One way of checking the degree to which the actual classification problem is solved at every layer of a feedforward network is to attach linear classifiers to every hidden layer and train them on the main task without backpropagating through the rest of the network.,Linear classifier/regression probes,[0],[0]
This way we can make a plot of train accuracy obtained from the representation at each layer.,Linear classifier/regression probes,[0],[0]
"As seen in Figure 8 (left) there is not much of the difference between such analysis for backpropagation and a single SG module, confirming our claim in the paper that despite different representations in both sections of SG based module - they are both good enough to solve the main problem.",Linear classifier/regression probes,[0],[0]
"We can also that DFA tries to solve the classification problem bottom-up as opposed to up-bottom – notice that for DFA we can have 100% accuracy after the very first hidden layer, which is not true even for backpropagation.
",Linear classifier/regression probes,[0],[0]
"We also introduced a new kind of linear probe, which tries to capture how much computation (non-linear transformations) are being used in each layer.",Linear classifier/regression probes,[0],[0]
"To achieve this, we at-
tach a linear regressor module after each hidden layer and regress it (with MSE) to the input of the network.",Linear classifier/regression probes,[0],[0]
"This is obviously label agnostic approach, but measures how non-linear the transformations are up to the given hidden layer.",Linear classifier/regression probes,[0],[0]
"Figure 8 (right) again confirms that with a single SG we have two parts of the network (thus results are similar to RDM experiments) which do have slightly different behaviour, and again show clearly that DFA performs lots of non-linear transformations very early on compared to all other methods.",Linear classifier/regression probes,[0],[0]
"In the main paper we show how SG modules using both activations and labels are able to implicitly describe the loss surface reasonably well for most of the training, with different datasets and losses.",Loss estimation,[0],[0]
"For completeness, we also include the same experiment for SG modules which do not use label information (Figure 9 (a) - (d)) as well as a module which does not use activations at all6 (Figure 9 (e) - (h))).",Loss estimation,[0],[0]
There are two important observations here:,Loss estimation,[0],[0]
"Firstly, none of these two approaches provide a loss estimation fidelity comparable with the full SG (conditioned on both activations and labels).",Loss estimation,[0],[0]
This gives another empirical confirmation for correct conditioning of the module.,Loss estimation,[0],[0]
"Secondly, models which used only labels did not converge to a good solutions after 100k iterations, while without the label SG was able to do so (however it took much longer and was far noisier).
",Loss estimation,[0],[0]
6This is more similar to a per-label stale gradient model.,Loss estimation,[0],[0]
"When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking – without waiting for a true error gradient to be backpropagated – resulting in Decoupled Neural Interfaces (DNIs).",abstractText,[0],[0]
This unlocked ability of being able to update parts of a neural network asynchronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016).,abstractText,[0],[0]
"However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view.",abstractText,[0],[0]
"In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation.",abstractText,[0],[0]
"We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network, and prove the convergence of the learning system for linear and deep linear models.",abstractText,[0],[0]
"On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations.",abstractText,[0],[0]
"Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.",abstractText,[0],[0]
Understanding Synthetic Gradients and Decoupled Neural Interfaces,title,[0],[0]
"ar X
iv :1
70 4.
05 75
3v 2
[ cs
.C L
] 1
9 O
ct 2
01 7
Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.",text,[0],[0]
"Paraphrases are useful for a range of tasks, including machine translation evaluation (Kauchak and Barzilay, 2006), semantic parsing (Wang et al., 2015), and question answering (Fader et al., 2013).",1 Introduction,[0],[0]
"Crowdsourcing has been widely used as a scalable and cost-effective means of generating paraphrases (Negri et al., 2012; Wang et al., 2012; Tschirsich and Hintz, 2013), but there has been limited analysis of the factors influencing diversity and correctness of the paraphrases workers write.
",1 Introduction,[0],[0]
"In this paper, we perform a systematic investigation of design decisions for crowdsourcing paraphrases, including the first exploration of worker incentives for paraphrasing.",1 Introduction,[0],[0]
"For worker incentives, we either provide a bonus payment when a paraphrase is novel (encouraging diversity) or
when it matches a paraphrase from another worker (encouraging agreement/correctness).",1 Introduction,[0],[0]
"We also varied the type of example paraphrases shown to workers, the number of paraphrases requested from each worker per sentence, the subject domain of the data, whether to show answers to questions, and whether the prompt sentence is the same for multiple workers or varies, with alternative prompts drawn from the output of other workers.
",1 Introduction,[0],[0]
Effective paraphrasing has two desired properties: correctness and diversity.,1 Introduction,[0],[0]
"To measure correctness, we hand-labeled all paraphrases with semantic equivalence and grammaticality scores.",1 Introduction,[0],[0]
"For diversity, we measure the fraction of paraphrases that are distinct, as well as Paraphrase In N-gram Changes (PINC), a measure of n-gram variation.",1 Introduction,[0],[0]
"We have released all 2,600 paraphrases along with accuracy annotations.",1 Introduction,[0],[0]
"Our analysis shows that the most important factor is how workers are primed for a task, with the choice of examples and the prompt sentence affecting diversity and correctness significantly.",1 Introduction,[0],[0]
"Previous work on crowdsourced paraphrase generation fits into two categories: work on modifying the creation process or workflow, and studying the effect of prompting or priming on crowd worker output.",2 Related Work,[0],[0]
"Beyond crowdsourced generation, other work has explored using experts or automated systems to generate paraphrases.",2 Related Work,[0],[0]
The most common approach to crowdsourcing paraphrase generation is to provide a sentence as a prompt and request a single paraphrase from a worker.,2.1 Workflows for Crowd-Paraphrasing,[0],[0]
"One frequent addition is to ask a different set of workers to evaluate whether a generated paraphrase is correct (Buzek et al., 2010; Burrows et al., 2013).",2.1 Workflows for Crowd-Paraphrasing,[0],[0]
"Negri et al. (2012) also explored an alternate workflow in which each worker writes
two paraphrases, which are then given to other workers as the prompt sentence, forming a binary tree of paraphrases.",2.1 Workflows for Crowd-Paraphrasing,[0],[0]
"They found that paraphrases deeper in the tree were more diverse, but understanding how correctness and grammaticality vary across such a tree still remains an open question.",2.1 Workflows for Crowd-Paraphrasing,[0],[0]
"Near real-time crowdsourcing (Bigham et al., 2010) allowed Lasecki et al. (2013a) to elicit variations on entire conversations by providing a setting and goal to pairs of crowd workers.",2.1 Workflows for Crowd-Paraphrasing,[0],[0]
"Continuous real-time crowdsourcing (Lasecki et al., 2011) allows Chorus Lasecki et al. (2013b) users to hold conversations with groups of crowd workers as if the crowd was a single individual, allowing for the collection of example conversations in more realistic settings.",2.1 Workflows for Crowd-Paraphrasing,[0],[0]
"The only prior work regarding incentives we are aware of is by Chklovski (2005), who collected paraphrases in a game where the goal was to match an existing paraphrase, with extra points awarded for doing so with fewer hints.",2.1 Workflows for Crowd-Paraphrasing,[0],[0]
The disadvantage of this approach was that 29% of the collected paraphrases were duplicates.,2.1 Workflows for Crowd-Paraphrasing,[0],[0]
"In our experiments, duplication ranged from 1% to 13% in each condition.",2.1 Workflows for Crowd-Paraphrasing,[0],[0]
"When crowd workers perform a task, they are primed (influenced) by the examples, instructions, and context that they see.",2.2 The Effects of Priming,[0],[0]
This priming can result in systematic variations in the resulting paraphrases.,2.2 The Effects of Priming,[0],[0]
"Mitchell et al. (2014) showed that providing context, in the form of previous utterances from a dialogue, only provides benefits once four or more are included.",2.2 The Effects of Priming,[0],[0]
"Kumaran et al. (2014) provided drawings as prompts, obtaining diverse paraphrases, but without exact semantic equivalence.",2.2 The Effects of Priming,[0],[0]
"When each sentence expresses a small set of slot-filler predicates, Wang et al. (2012) found that providing the list of predicates led to slightly faster paraphrasing than giving either a complete sentence or a short sentence for each predicate.",2.2 The Effects of Priming,[0],[0]
We further expand on this work by exploring how the type of examples shown affects paraphrasing.,2.2 The Effects of Priming,[0],[0]
"Finally, there are two general lines of research on paraphrasing not focused on using crowds.",2.3 Expert and Automated Generation,[0],[0]
"The first of these is the automatic collection of paraphrases from parallel data sources, such as translations of the same text or captions for the same image (Ganitkevitch et al., 2013; Chen and Dolan, 2011; Bouamor et al., 2012; Pavlick et al.,
2015).",2.3 Expert and Automated Generation,[0],[0]
"These resources are extremely large, but usually (1) do not provide the strong semantic equivalence we are interested in, and (2) focus on phrases rather than complete sentences.",2.3 Expert and Automated Generation,[0],[0]
"The second line of work explores the creation of lattices that compactly encode hundreds of thousands of paraphrases (Dreyer and Marcu, 2012; Bojar et al., 2013).",2.3 Expert and Automated Generation,[0],[0]
"Unfortunately, these lattices are typically expensive to produce, taking experts one to three hours per sentence.",2.3 Expert and Automated Generation,[0],[0]
We conducted a series of experiments to investigate factors in crowdsourced paraphrase creation.,3 Experimental Design,[0],[0]
"To do so in a controlled manner, we studied a single variation per condition.",3 Experimental Design,[0],[0]
This project was motivated by the need for strongly equivalent paraphrases in semantic parsing datasets.,3.1 Definition of Valid Paraphrases,[0],[0]
"We consider two sentences paraphrases if they would have equivalent interpretations when represented as a structured query, i.e., ”a pair of units of text deemed to be interchangeable” (Dras, 1999).",3.1 Definition of Valid Paraphrases,[0],[0]
"For example:
Prompt: Which upper-level classes are four credits?",3.1 Definition of Valid Paraphrases,[0],[0]
"Are there any four credit upper-level classes?
",3.1 Definition of Valid Paraphrases,[0],[0]
"We considered the above two questions as paraphrases since they are both requests for a list of classes, explicit and implicit, respectively, although the second one is a polar question and the first one is not.",3.1 Definition of Valid Paraphrases,[0],[0]
"However:
Prompt:Which is easier out of EECS 378 and EECS 280?",3.1 Definition of Valid Paraphrases,[0],[0]
"Is EECS 378 easier than EECS 280?
",3.1 Definition of Valid Paraphrases,[0],[0]
"We did not consider the above two questions as paraphrases since the first one is requesting one of
two class options and the second one is requesting a yes or no answer.",3.1 Definition of Valid Paraphrases,[0],[0]
"We used Amazon Mechanical Turk, presenting workers with the instructions and examples in Figure 1.",3.2 Baseline,[0],[0]
"Workers were shown prompt sentences one at a time, and asked to provide two paraphrases for each.",3.2 Baseline,[0],[0]
"To avoid confusion or training effects between different conditions, we only allowed workers to participate once across all conditions.",3.2 Baseline,[0],[0]
"The initial instructions shown to workers were the same across all conditions (variations were only seen after a worker accepted the task).
",3.2 Baseline,[0],[0]
"Workers were paid 5 cents per paraphrase they wrote plus, once all workers were done, a 5 cent bonus for paraphrases that matched another worker’s paraphrase in the same condition.",3.2 Baseline,[0],[0]
"While we do not actually want duplicate paraphrases, this incentive may encourage workers to more closely follow the instructions, producing grammatical and correct sentences.",3.2 Baseline,[0],[0]
"We chose this payment rate to give around minimum wage, estimating time based on prior work.",3.2 Baseline,[0],[0]
"Examples We provided workers with an example prompt sentence and two paraphrases, as shown in Figure 1.",3.3 Conditions,[0],[0]
"We showed either: no examples (No Examples), two examples with lexical changes only (Lexical Examples), one example with lexical changes and one with syntactic changes (Mixed Examples), or two examples that each contained both lexical and syntactic changes (Baseline).",3.3 Conditions,[0],[0]
"The variations between these conditions may prime workers differently, leading them to generate different paraphrases.
",3.3 Conditions,[0],[0]
"Incentive The 5 cent bonus payment per paraphrase was either not included (No Bonus), awarded for each sentence that was a duplicate at the end of the task (Baseline), or awarded for each sentence that did not match any other worker’s paraphrase (Novelty Bonus).",3.3 Conditions,[0],[0]
Bonuses that depend on other workers’ actions may encourage either creativity or conformity.,3.3 Conditions,[0],[0]
"We did not vary the base level of payment because prior work has found that workers work quality is not increased by increased financial incentives due to an anchoring effect relative to the base rate we define (Mason and Watts, 2010).
",3.3 Conditions,[0],[0]
Workflow We considered three variations to workflow.,3.3 Conditions,[0],[0]
"First, for each sentence, we either asked workers to provide two paraphrases (Baseline), or one (One Paraphrase).",3.3 Conditions,[0],[0]
"Asking for multiple paraphrases reduces duplication (since workers will not repeat themselves), but may result in lower diversity.",3.3 Conditions,[0],[0]
"Second, since our baseline prompt sentences are questions, we ran a condition with answers shown to workers (Answers).",3.3 Conditions,[0],[0]
"Third, we started all conditions with the same set of prompt sentences, but once workers had produced paraphrases, we had the option to either prompt future workers with the original prompt, or to use paraphrase from another worker.",3.3 Conditions,[0],[0]
"Treating sentences as points and the act of paraphrasing as creating an edge, the space can be characterized as a graph.",3.3 Conditions,[0],[0]
"We prompted workers with either the original sentences only (Baseline), or formed a chain structured graph by randomly choosing a sentence that was (1) not a duplicate, and (2) furthest from the original sentence (Chain).",3.3 Conditions,[0],[0]
"These changes could impact paraphrasing because the prompt sentence is a form of priming.
Data domains We ran with five data sources: questions about university courses (Baseline), messages from dialogues between two students in a simulated academic advising session (ADVISING), questions about US geography (GEOQUERY Tang and Mooney, 2001), text from the Wall Street Journal section of the Penn Treebank (WSJ Marcus et al., 1993), and discussions on the Ubuntu IRC channel (UBUNTU).",3.3 Conditions,[0],[0]
We randomly selected 20 sentences as prompts from each data source with the lengths representative of the sentence length distribution in that source.,3.3 Conditions,[0],[0]
"Semantic Equivalence For a paraphrase to be valid, its meaning must match the original sentence.",3.4 Metrics,[0],[0]
"To assess this match, two of the authors— one native speaker and one non-native but fluent speaker—rated every sentence independently, then discussed every case of disagreement to determine a consensus judgement.",3.4 Metrics,[0],[0]
"Prior to the consensusfinding step, the inter-annotator agreement kappa scores were .50 for correctness (moderate agreement), and .36 for grammaticality (fair agreement) (Altman, 1990).",3.4 Metrics,[0],[0]
"For the results in Table 1, we used a χ2 test to measure significance, since this is a binary classification process.
",3.4 Metrics,[0],[0]
"Grammaticality We also judged whether the sentences were grammatical, again with two annotators rating every sentence and resolving disagreements.",3.4 Metrics,[0],[0]
"Again, since this was a binary classification, we used a χ2 test for significance.
",3.4 Metrics,[0],[0]
"Time The time it takes to write paraphrases is important for estimating time-to-completion, and ensuring workers receive fair payment.",3.4 Metrics,[0],[0]
We measured the time between when a worker submitted one pair of paraphrases and the next.,3.4 Metrics,[0],[0]
The first paraphrase was excluded since it would skew the data by including the time spent reading the instructions and understanding the task.,3.4 Metrics,[0],[0]
"We report the median time to avoid skewing due to outliers, e.g. a value of five minutes when a worker probably took a break.",3.4 Metrics,[0],[0]
"We apply Mood’s Median test for statistical significance.
",3.4 Metrics,[0],[0]
"Diversity We use two metrics for diversity, measured over correct sentences only.",3.4 Metrics,[0],[0]
"First, a simple measurement of exact duplication: the number of distinct paraphrases divided by the total number of paraphrases, as a percentage (Distinct).",3.4 Metrics,[0],[0]
"Second, a measure of n-gram diversity (PINC Chen and Dolan, 2011)1.",3.4 Metrics,[0],[0]
"In both cases, a higher score means greater diversity.",3.4 Metrics,[0],[0]
"For PINC, we used a ttest for statistical significance, and for Distinct we used a permutation test.",3.4 Metrics,[0],[0]
"We collected 2600 paraphrases: 10 paraphrases per sentence, for 20 sentences, for each of the 13 conditions.",4 Results,[0],[0]
"The cost, including initial testing, was $196.30, of which $20.30 was for bonus payments.",4 Results,[0],[0]
Table 1 shows the results for all metrics.,4 Results,[0],[0]
"Qualitatively, we observed a wide variety of lexical and syntactic changes, as shown by these example prompts and paraphrases (one low PINC and one high PINC in each case):
Prompt: How long has EECS 280 been offered for?",4.1 Discussion: Task Variation,[0],[0]
How long has EECS 280 been offered?,4.1 Discussion: Task Variation,[0],[0]
"EECS 280 has been in the course listings how many years?
",4.1 Discussion: Task Variation,[0],[0]
Prompt: Can I take 280 on Mondays and Wednesdays?,4.1 Discussion: Task Variation,[0],[0]
"On Mondays and Wednesdays, can I take 280?",4.1 Discussion: Task Variation,[0],[0]
"Is 280 available as a Monday/Wednesday class?
",4.1 Discussion: Task Variation,[0],[0]
There was relatively little variation in grammaticality or time across the conditions.,4.1 Discussion: Task Variation,[0],[0]
"The times
1 We also considered BLEU (Papineni et al., 2002), which measures n-gram overlap and is used as a proxy for correctness in MT.",4.1 Discussion: Task Variation,[0],[0]
"As expected, it strongly correlated with PINC.
",4.1 Discussion: Task Variation,[0],[0]
"we observed are consistent with prior work: e.g. Wang et al. (2015) report ∼28 sec/paraphrase.
",4.1 Discussion: Task Variation,[0],[0]
"Priming had a major impact, with the shift to lexical examples leading to a significant improvement in correctness, but much lower diversity.",4.1 Discussion: Task Variation,[0],[0]
The surprising increase in correctness when providing no examples has a p-value of 0.07 and probably reflects random variation in the pool of workers.,4.1 Discussion: Task Variation,[0],[0]
"Meanwhile, changing the incentives by providing either a bonus for novelty, or no bonus at all, did not substantially impact any of the metrics.
",4.1 Discussion: Task Variation,[0],[0]
Changing the number of paraphrases written by each worker did not significantly impact diversity (we worried that collecting more than one may lead to a decrease).,4.1 Discussion: Task Variation,[0],[0]
"We further confirmed this by calculating PINC between the two paraphrases provided by each user, which produced scores similar to comparing with the prompt.",4.1 Discussion: Task Variation,[0],[0]
"However, the One Paraphrase condition did have lower grammaticality, emphasizing the value of evaluating and filtering out workers who write ungrammatical paraphrases.
",4.1 Discussion: Task Variation,[0],[0]
Changing the source of the prompt sentence to create a chain of paraphrases led to a significant increase in diversity.,4.1 Discussion: Task Variation,[0],[0]
This fits our intuition that the prompt is a form of priming.,4.1 Discussion: Task Variation,[0],[0]
"However, correctness decreases along the chain, suggesting the need to check paraphrases against the original sentence during the overall process, possibly using other workers as described in § 2.1.",4.1 Discussion: Task Variation,[0],[0]
"Meanwhile, showing the answer to the question being para-
phrased did not significantly affect correctness or diversity, and in 2.5% of cases workers incorrectly used the answer as part of their paraphrase.
",4.1 Discussion: Task Variation,[0],[0]
We also analyzed the distribution of incorrect or ungrammatical paraphrases by worker.,4.1 Discussion: Task Variation,[0],[0]
"7% of workers accounted for 25% of incorrect paraphrases, while the best 30% of workers made no mistakes at all.",4.1 Discussion: Task Variation,[0],[0]
"Similarly, 8% of workers wrote 50% of the ungrammatical paraphrases, while 70% of workers wrote only grammatical paraphrases.",4.1 Discussion: Task Variation,[0],[0]
"Many crowdsourcing tasks address these issues by showing workers some gold standard instances, to evaluate workers’ performance during annotation.",4.1 Discussion: Task Variation,[0],[0]
"Unfortunately, in paraphrasing there is no single correct answer, though other workers could be used to check outputs.
",4.1 Discussion: Task Variation,[0],[0]
"Finally, we checked the distribution of incorrect paraphrases per prompt sentence.",4.1 Discussion: Task Variation,[0],[0]
"Two prompts accounted for 22% of incorrect paraphrases:
",4.1 Discussion: Task Variation,[0],[0]
Prompt:Which is easier out of EECS 378 and EECS 280?,4.1 Discussion: Task Variation,[0],[0]
"Is EECS 378 easier than EECS 280?
",4.1 Discussion: Task Variation,[0],[0]
Prompt: Is Professor Stout the only person who teaches Algorithms?,4.1 Discussion: Task Variation,[0],[0]
"Are there professors other than Stout who teach Algorithms?
",4.1 Discussion: Task Variation,[0],[0]
"These paraphrases are not semantically equivalent to the original question, but they would elicit equivalent information, which explains why workers provided them.",4.1 Discussion: Task Variation,[0],[0]
Providing negative examples may help guide workers to avoid such mistakes.,4.1 Discussion: Task Variation,[0],[0]
"The bottom section of Table 1 shows measurements using the baseline setup, but with variations in the source domain of data.",4.2 Discussion: Domains,[0],[0]
"The only significant change in correctness is on UBUNTU, which is probably due to the extensive use of jargon in the dataset, for example:
",4.2 Discussion: Domains,[0],[0]
"Prompt: ok, what does journalctl show That journalistic show is about what?
",4.2 Discussion: Domains,[0],[0]
"For grammaticality, GEOQUERY is particularly low; common mistakes included confusion between singular/plural and has/have.",4.2 Discussion: Domains,[0],[0]
WSJ is the domain with the greatest variations.,4.2 Discussion: Domains,[0],[0]
"It has considerably longer sentences on average, which explains the greater time taken.",4.2 Discussion: Domains,[0],[0]
"This could also explain the lower distinctness and PINC score, because workers would often retain large parts of the sentence, sometimes re-arranged, but otherwise unchanged.",4.2 Discussion: Domains,[0],[0]
"While previous work has used crowdsourcing to generate paraphrases, we perform the first systematic study of factors influencing the process.",5 Conclusion,[0],[0]
"We find that the most substantial variations are caused by priming effects: using simpler examples leads to lower diversity, but more frequent semantic equivalence.",5 Conclusion,[0],[0]
"Meanwhile, prompting workers with paraphrases collected from other workers (rather than re-using the original prompt) increases diversity.",5 Conclusion,[0],[0]
"Our findings provide clear guidance for future paraphrase generation, supporting the creation of larger, more diverse future datasets.",5 Conclusion,[0],[0]
"We would like to thank the members of the UMich/IBM Sapphire project, as well as all of our study participants and the anonymous reviewers for their helpful suggestions on this work.
",6 Acknowledgements,[0],[0]
This material is based in part upon work supported by IBM under contract 4915012629 .,6 Acknowledgements,[0],[0]
"Any opinions, findings, conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of IBM.",6 Acknowledgements,[0],[0]
"Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts.",abstractText,[0],[0]
"Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks.",abstractText,[0],[0]
"In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection.",abstractText,[0],[0]
"We consider variations in instructions, incentives, data domains, and workflows.",abstractText,[0],[0]
"We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity.",abstractText,[0],[0]
"Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.",abstractText,[0],[0]
Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection,title,[0],[0]
"Local search algorithms like stochastic gradient descent [4] or variants have gained huge success in training deep neural networks (see, [5];",1 Introduction,[0],[0]
"[6]; [7], for example).",1 Introduction,[0],[0]
"Despite the spurious saddle points and local minima on the loss surface [3], it has been widely conjectured that all local minima of the empirical loss lead to similar training performance [1, 2].",1 Introduction,[0],[0]
"For example, [8] empirically showed that neural networks with identical architectures but different initialization points can converge to local minima with similar classification performance.",1 Introduction,[0],[0]
"However, it still remains a challenge to characterize the theoretical properties of the loss surface for neural networks.
",1 Introduction,[0],[0]
"In the setting of regression problems, theoretical justifications has been established to support the conjecture that all local minima lead to similar training performance.",1 Introduction,[0],[0]
"For shallow models, [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20] provide conditions under which the local search algorithms are guaranteed to converge to the globally optimal solution for the regression problem.",1 Introduction,[0],[0]
"For deep linear networks, it has been shown that every local minimum of the empirical loss is a global minimum [21, 22, 23, 24, 25].",1 Introduction,[0],[0]
"In order to characterize the loss surface of more general deep networks for regression tasks, [2] have proposed an interesting approach.",1 Introduction,[0],[0]
"Based on certain constructions on network models and additional assumptions, they relate the loss function to a spin glass model and show that the almost all local minima have similar empirical loss and the number of bad local minima decreases quickly with the distance to the global optimum.",1 Introduction,[0],[0]
"Despite the interesting results, it remains a concern to properly justify their assumptions.",1 Introduction,[0],[0]
"More recently, it has been shown [26, 27] that, when the dataset satisfies certain conditions, if one layer in the multilayer network has more neurons than the number of training samples, then a subset of local minima are global minima.
∗University of Illinois at Urbana-Champaign †Facebook Research
ar X
iv :1
80 3.
00 90
9v 2
[ cs
.L G
] 5
M ar
2 01
Although the loss surfaces in regression tasks have been well studied, the theoretical understanding of loss surfaces in classification tasks is still limited.",1 Introduction,[0],[0]
"[27, 28, 29] treat the classification problem as the regression problem by using quadratic loss, and show that (almost) all local minima are global minima.",1 Introduction,[0],[0]
"However, the global minimum of the quadratic loss does not necessarily have zero misclassification error even in the simplest cases (e.g., every global minimum of quadratic loss can have non-zero misclassification error even when the dataset is linearly separable and the network is a linear network).",1 Introduction,[0],[0]
"This issue was mentioned in [26] and a different loss function was used, but their result only studied the linearly separable case and a subset of the critical points.
",1 Introduction,[0],[0]
"In view of the prior work, the context and contributions of our paper are as follows:
• Prior work on quadratic and related loss functions suggest that one can achieve zero misclassification error at all local minima by overparameterizing the neural network.",1 Introduction,[0],[0]
"The reason for over-parameterization is that the quadratic loss function tries to match the output of the neural network to the label of each training sample.
",1 Introduction,[0],[0]
"• On the other hand, hinge loss-type functions only try to match the sign of the outputs with the labels.",1 Introduction,[0],[0]
So it may be possible to achieve zero misclassification error without over-parametrization.,1 Introduction,[0],[0]
"We provide conditions under which the misclassification error of neural networks is zero at all local minima for hinge-loss functions.
",1 Introduction,[0],[0]
"• Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcut-like connection and the surrogate loss function should be a smooth version of the hinge loss function.
",1 Introduction,[0],[0]
"• We also provide counterexamples to show that when these conditions are relaxed, the result may not hold.
",1 Introduction,[0],[0]
• We establish our results under the assumption that either the dataset is linearly separable or the positively and negatively labeled samples are located on different subspaces.,1 Introduction,[0],[0]
"Whether this assumption is necessary is an open problem, except in the case of certain special neurons.
",1 Introduction,[0],[0]
The outline of this paper is as follows.,1 Introduction,[0],[0]
"In Section 2, we present the necessary definitions.",1 Introduction,[0],[0]
"In Section 3, we present the main results and we discuss each condition in Section 4.",1 Introduction,[0],[0]
Conclusions are presented in Section 5.,1 Introduction,[0],[0]
All proofs are provided in Appendix.,1 Introduction,[0],[0]
Network models.,2 Preliminaries,[0],[0]
"Given an input vector x of dimension d, we consider a neural network with L layers for binary classification.",2 Preliminaries,[0],[0]
We denote by Ml the number of neurons on the l-th layer (note that M0 = d and ML = 1).,2 Preliminaries,[0],[0]
We denote the neuron activation function by σ.,2 Preliminaries,[0],[0]
Let Wl ∈ RMl−1×Ml denote the weight matrix connecting the (l − 1)-th layer and the l-th layer and bl ∈ RMl denote the bias vector for the neurons in the l-th layer.,2 Preliminaries,[0],[0]
"Therefore, the output of the network f : Rd → R can be expressed by
f(x;θ) =",2 Preliminaries,[0],[0]
W>L σ,2 Preliminaries,[0],[0]
"( ...σ(W>1 x+ b1) + bL−1 ) + bL,
where θ denotes all parameters in the neural network.",2 Preliminaries,[0],[0]
Data distribution.,2 Preliminaries,[0],[0]
"In this paper, we consider binary classification tasks where each sample (X,",2 Preliminaries,[0],[0]
Y ) ∈ Rd,2 Preliminaries,[0],[0]
"× {−1, 1} is drawn from an underlying data distribution PX×Y defined on Rd × {−1, 1}.",2 Preliminaries,[0],[0]
"The sample (X, Y ) is considered positive if Y = 1, and negative otherwise.",2 Preliminaries,[0],[0]
"Let E = {e1, ..., ed} denote a set of orthonormal basis on the space Rd.",2 Preliminaries,[0],[0]
"Let U+ and U− denote two subsets of E such that all
positive and negative samples are located on the linear span of the set U+ and U−, respectively, i.e., PX|Y (X ∈ Span(U+)|Y = 1) = 1 and PX|Y (X ∈ Span(U−)|Y = −1) = 1.",2 Preliminaries,[0],[0]
"Let r denote the size of the set U+ ∪ U−, r+ denote the size of the set U+ and r− denote the size of the set U−, respectively.",2 Preliminaries,[0],[0]
Loss and error.,2 Preliminaries,[0],[0]
"Let D = {(xi, yi)}ni=1 denote a dataset with n samples, each independently drawn from the distribution PX×Y .",2 Preliminaries,[0],[0]
"Given a neural network f(x;θ) parameterized by θ and a loss function ` : R → R, in binary classification tasks1, we define the empirical loss L̂n(θ) as the average loss of the network f on a sample in the dataset D, i.e.,
L̂n(θ)",2 Preliminaries,[0],[0]
"= 1
n n∑ i=1",2 Preliminaries,[0],[0]
"`(−yif(xi;θ)).
",2 Preliminaries,[0],[0]
"Furthermore, for a neural network f , we define a binary classifier gf :",2 Preliminaries,[0],[0]
"Rd → {−1, 1} of the form gf = sgn(f), where the sign function sgn(z) = 1, if z ≥ 0, and sgn(z) = 0",2 Preliminaries,[0],[0]
otherwise.,2 Preliminaries,[0],[0]
"We define the training error (also called the misclassification error) R̂n(θ) as the misclassification rate of the neural network f(x;θ) on the dataset D, i.e.,
R̂n(θ)",2 Preliminaries,[0],[0]
"= 1
n n∑ i=1",2 Preliminaries,[0],[0]
"I{yi 6= sgn(f(xi;θ))},
where I{·} is the indicator function.",2 Preliminaries,[0],[0]
The training error R̂n measures the classification performance of the network f on the finite samples in the dataset D.,2 Preliminaries,[0],[0]
"In this section, we present the main results.",3 Main Results,[0],[0]
"We first introduce several important conditions in order to derive the main results, and we will provide further discussions on these conditions in the next section.",3 Main Results,[0],[0]
"To fully specify the problem, we need to specify our assumptions on several components of the model, including: (1) the loss function, (2) the data distribution, (3) the network architecture and (4) the neuron activation function.
",3.1 Conditions,[0],[0]
"Assumption 1 (Loss function) Let `p : R → R denote a loss function satisfying the following conditions: (1) `p is a surrogate loss function, i.e., `p(z) ≥",3.1 Conditions,[0],[0]
"I{z ≥ 0} for all z ∈ R, where I(·) denotes the indicator function; (2) `p has continuous derivatives up to order p on R; (3) `p is non-decreasing (i.e., `′p(z) ≥ 0 for all z ∈ R) and there exists a positive constant z0 such that `′p(z) = 0 iff z ≤ −z0.
",3.1 Conditions,[0],[0]
"The first condition in Assumption 1 ensures that the training error R̂n is always upper bounded by the empirical loss L̂n, i.e., R̂n ≤ L̂n.",3.1 Conditions,[0],[0]
"This guarantees that the neural network can correctly classify all samples in the dataset (i.e., R̂n = 0), when the neural network achieves zero empirical loss (i.e., L̂n = 0).",3.1 Conditions,[0],[0]
The second condition ensures that the empirical loss L̂n has continuous derivatives with respect to the parameters up to a sufficiently high order.,3.1 Conditions,[0],[0]
The third condition ensures that the loss function is non-decreasing and `′p(z) = 0 is achievable if and only if z ≤ −z0.,3.1 Conditions,[0],[0]
"Here, we provide a simple example of the loss function satisfying all conditions in Assumption 1: the polynomial hinge loss, i.e., `p(z) =",3.1 Conditions,[0],[0]
"[max{z + 1, 0}]p+1.",3.1 Conditions,[0],[0]
"We note that, in this paper, we use L̂n(θ; p) to denote the empirical loss
1We note that, in regression tasks, the empirical loss is usually defined as L̂n(θ)",3.1 Conditions,[0],[0]
= 1 n,3.1 Conditions,[0],[0]
∑n i=1,3.1 Conditions,[0],[0]
"`(yi − f(xi;θ)).
",3.1 Conditions,[0],[0]
when the loss function is `p and the network is parametrized by a set of parameters θ.,3.1 Conditions,[0],[0]
"Further results on the impact of loss functions are presented in Section 4.
",3.1 Conditions,[0],[0]
"Assumption 2 (Data distribution) Assume that for random vectors X1, ...,Xr+ independently drawn from the distribution PX|Y=1 and Z1, ...,Zr− independently drawn from the distribution PX|Y=−1, matrices ( X1, ...,Xr+ )",3.1 Conditions,[0],[0]
"∈ Rr+×d and ( Z1, ...,Zr− ) ∈ Rr−×d are full rank matrices with probability one.
",3.1 Conditions,[0],[0]
Assumption 2 states that support of the conditional distribution PX|Y=1 is sufficiently rich so that r+ samples drawn from it will be linearly independent.,3.1 Conditions,[0],[0]
"In other words, by stating this assumption, we are avoiding trivial cases where all the positively labeled points are located in a very small subset of the linear span of U+.",3.1 Conditions,[0],[0]
"Similarly for the negatively labeled samples.
Assumption 3 (Data distribution) Assume |U+ ∪ U−|",3.1 Conditions,[0],[0]
>,3.1 Conditions,[0],[0]
"max{|U+|, |U−|}, i.e., r > max{r+, r−}.
",3.1 Conditions,[0],[0]
Assumption 3 assumes that the positive and negative samples are not located on the same linear subspace.,3.1 Conditions,[0],[0]
"Previous works [30, 31, 32, 30] have observed that some classes of natural images (e.g., images of faces, handwritten digits, etc) can be reconstructed from lower-dimensional representations.",3.1 Conditions,[0],[0]
"For example, using dimensionality reduction methods such as PCA, one can approximately reconstruct the original image from only a small number of principal components [30, 31].",3.1 Conditions,[0],[0]
"Here, Assumption 3 states that both the positively and negatively labeled samples have lower-dimensional representations, and they do not exist in the same lower-dimensional subspace.",3.1 Conditions,[0],[0]
"We provide additional analysis in Section 4, showing how our main results generalize to other data distributions.
",3.1 Conditions,[0],[0]
"Assumption 4 (Network architecture) Assume that the neural network f is a single-layered neural network, or more generally, has shortcut-like connections shown in Fig 1 (b), where fS is a single layer network and fD is a feedforward network.
",3.1 Conditions,[0],[0]
"Shortcut connections are widely used in the modern network architectures (e.g., Highway Networks [34], ResNet [33], DenseNet [35], etc.), where the skip connections allow the deep layers to have direct access to the outputs of shallow layers.",3.1 Conditions,[0],[0]
"For instance, in the residual network, each residual block has a identity shortcut connection, shown in Fig 1 (a), where the output of each residual block is the vector sum of its input and the output of a network H.
Instead of using the identity shortcut connection, in this paper, we first pass the input through a single layer network fS(x;θS) = a0 + a >σ",3.1 Conditions,[0],[0]
"( W>x ) , where vector a denotes the weight vector, matrix W denotes the weight matrix and vector θS denotes the vector containing all parameters in fS .",3.1 Conditions,[0],[0]
"We next add the output of this network to a network fD and use the addition as the output of the whole network, i.e., f(x;θ) = fS(x;θS) + fD(x;θD), where vector θD and θ denote the vector containing all parameters in the
network fD and the whole network f , respectively.",3.1 Conditions,[0],[0]
"We note here that, in this paper, we do not restrict the number of layers and neurons in the network fD and this means that the network fD can be a feedforward network introduced in Section 2 or a single layer network or even a constant.",3.1 Conditions,[0],[0]
"In fact, when the network fD is a single layer network or a constant, the whole network f becomes a single
layer network.",3.1 Conditions,[0],[0]
"Furthermore, we note that, in Section 4, we will show that if we remove this connection or replace this shortcut-like connection with the identity shortcut connection, the main result does not hold.
",3.1 Conditions,[0],[0]
Assumption 5 (Neuron activation) Assume that neurons σ(z) in the network fS are real analytic and satisfy σ′′(z),3.1 Conditions,[0],[0]
> 0,3.1 Conditions,[0],[0]
"for all z ∈ R. Assume that neurons in the network fD are real functions on R.
In Assumption 5, we assume that neurons in the network fS are infinitely differentiable and have positive second order derivatives on R, while neurons in the network fD are real functions.",3.1 Conditions,[0],[0]
"We make the above assumptions to ensure that the loss function L̂n(θS ,θD; p) is partially differentiable w.r.t.",3.1 Conditions,[0],[0]
the parameters θS in the network fS up to a sufficiently high order and allow us to use Taylor expansion in the analysis.,3.1 Conditions,[0],[0]
"Here, we list a few neurons which can be used in the network fS : softplus neuron, i.e., σ(z) = log2(1+e
z), quadratic neuron, i.e, σ(z) = z2, etc.",3.1 Conditions,[0],[0]
"We note that neurons in the network fS and fD do not need to be of the same type and this means that a more general class of neurons can be used in the network fD, e.g., threshold neuron, i.e., σ(z) = I{z ≥ 0}, rectified linear unit σ(z) = max{z, 0}, sigmoid neuron σ(z) = 1
1+e−z , etc.",3.1 Conditions,[0],[0]
Further discussion on the effects of neurons on the main results are provided in Section 4.,3.1 Conditions,[0],[0]
"Now we present the following theorem to show that when assumptions 1-5 are satisfied, every local minimum of the empirical loss function has zero training error if the number of neurons in the network fS are chosen appropriately.
",3.2 Main Results,[0],[0]
Theorem 1 (Linear subspace data) Suppose that assumptions 1-5 are satisfied.,3.2 Main Results,[0],[0]
Assume,3.2 Main Results,[0],[0]
"that samples in the dataset D = {(xi, yi)}ni=1, n",3.2 Main Results,[0],[0]
≥ 1 are independently drawn from the distribution PX×Y .,3.2 Main Results,[0],[0]
"Assume that the number of neurons M in the network fS satisfies M ≥ 2 max{ n∆r , r+, r−}, where ∆r = r − max{r+, r−}.",3.2 Main Results,[0],[0]
"If θ∗ = (θ∗S ,θ∗D) is a local minimum of the loss function L̂n(θS ,θD; p) and p ≥ 6, then R̂n(θ∗S ,θ∗D) = 0 holds with probability one.
",3.2 Main Results,[0],[0]
"Remark: (i) By setting the network fD to a constant, it directly follows from Theorem 1 that if a single layer network fS(x;θS) consisting of neurons satisfying Assumption 5 and all other conditions in Theorem 1 are satisfied, then every local minimum of the empirical loss L̂n(θS ; p) has zero training error.",3.2 Main Results,[0],[0]
(ii),3.2 Main Results,[0],[0]
The positiveness of ∆r is guaranteed by Assumption 3.,3.2 Main Results,[0],[0]
"In the worst case (e.g., ∆r = 1 and ∆r = 2), the number of neurons needs to be at least greater than the number of samples, i.e., M ≥ n.",3.2 Main Results,[0],[0]
"However, when the two orthonormal basis sets U+ and U− differ significantly (i.e., ∆r 1), the number of neurons required by Theorem 1 can be significantly smaller than the number of samples (i.e., n 2n/∆r).",3.2 Main Results,[0],[0]
"In fact, we can show that, when the neuron has quadratic activation function σ(z) = z2, the assumption M ≥ 2n/∆r can be further relaxed such that the number of neurons is independent of the number of samples.",3.2 Main Results,[0],[0]
"We discuss this in the following proposition.
",3.2 Main Results,[0],[0]
Proposition 1 Assume that assumptions 1-5 are satisfied.,3.2 Main Results,[0],[0]
Assume,3.2 Main Results,[0],[0]
"that samples in the dataset D = {(xi, yi)}ni=1, n",3.2 Main Results,[0],[0]
≥ 1 are independently drawn from the distribution PX×Y .,3.2 Main Results,[0],[0]
Assume that neurons in the network fS satisfy σ(z) =,3.2 Main Results,[0],[0]
"z
2 and the number of neurons in the network fS satisfies M > r.",3.2 Main Results,[0],[0]
"If θ∗ = (θ∗S ,θ ∗ D) is a local minimum of the loss function L̂n(θS ,θD; p) and p ≥ 6, then R̂n(θ∗S ,θD) = 0 holds with probability one.
",3.2 Main Results,[0],[0]
"Remark: Proposition 1 shows that if the number of neuron M is greater than the dimension of the subspace, i.e., M > r, then every local minimum of the empirical loss function has zero training error.",3.2 Main Results,[0],[0]
"We note here that although the result is stronger with quadratic neurons, it does not imply that the
quadratic neuron has advantages over the other types of neurons (e.g., softplus neuron, etc).",3.2 Main Results,[0],[0]
"This is due to the fact that when the neuron has positive derivatives on R, the result in Theorem 1 holds for the dataset where positive and negative samples are linearly separable.",3.2 Main Results,[0],[0]
We provide the formal statement of this result in Theorem 2.,3.2 Main Results,[0],[0]
"However, when the neuron has quadratic activation function, the result in Theorem 1 may not hold for linearly separable dataset and we will illustrate this by providing a counterexample in the next section.
",3.2 Main Results,[0],[0]
"As shown in Theorem 1, when the data distribution satisfies Assumption 2 and 3, every local minimum of the empirical loss has zero training error.",3.2 Main Results,[0],[0]
"However, we can easily see that distributions satisfying these two assumptions may not be linearly separable.",3.2 Main Results,[0],[0]
"Therefore, to provide a complementary result to Theorem 1, we consider the case where the data distribution is linearly separable.",3.2 Main Results,[0],[0]
"Before presenting the result, we first present the following assumption on the data distribution.
",3.2 Main Results,[0],[0]
"Assumption 6 (Linear separability) Assume that there exists a vector w ∈ Rd such that the data distribution satisfies PX×Y (Yw>X > 0) = 1.
",3.2 Main Results,[0],[0]
"In Theorem 2, we will show that when the samples drawn from the data distribution are linearly separable, and the network has a shortcut-like connection shown in Figure 1, all local minima of the empirical loss function have zero training errors if the type of the neuron in the network fS are chosen appropriately.
",3.2 Main Results,[0],[0]
Theorem 2 (Linearly separable data) Suppose that the loss function `p satisfies Assumption 1 and the network architecture satisfies Assumption 4.,3.2 Main Results,[0],[0]
Assume,3.2 Main Results,[0],[0]
"that samples in the dataset D = {(xi, yi)}ni=1, n",3.2 Main Results,[0],[0]
≥ 1 are independently drawn from a distribution satisfying Assumption 6.,3.2 Main Results,[0],[0]
Assume that the single layer network fS has M ≥ 1 neurons and neurons σ in the network fS are twice differentiable and satisfy σ′(z),3.2 Main Results,[0],[0]
> 0,3.2 Main Results,[0],[0]
for all z ∈ R.,3.2 Main Results,[0],[0]
"If θ∗ = (θ∗S ,θ∗D) is a local minimum of the loss function L̂n(θS ,θD; p), p ≥ 3, then R̂n(θ∗S ,θ∗D) = 0 holds with probability one.
",3.2 Main Results,[0],[0]
"Remark: Similar to Proposition 1, Theorem 2 does not require the number of neurons to be in scale with the number of samples.",3.2 Main Results,[0],[0]
"In fact, we make a weaker assumption here: the single layer network fS only needs to have at least one neuron, in contrast to at least r neurons required by Proposition 1.",3.2 Main Results,[0],[0]
"Furthermore, we note here that, in Theorem 2, we assume that neurons in the network fS have positive derivatives on R. This implies that Theorem 2 may not hold for a subset of neurons considered in Theorem 1 (e.g., quadratic neuron, etc).",3.2 Main Results,[0],[0]
"We will provide further discussions on the effects of neurons in the next section.
",3.2 Main Results,[0],[0]
"So far, we have provided results showing that under certain constraints on the (1) neuron activation function, (2) network architecture, (3) loss function and (4) data distribution, every local minimum of the empirical loss function has zero training error.",3.2 Main Results,[0],[0]
"In the next section, we will discuss the implications of these conditions on our main results.",3.2 Main Results,[0],[0]
"In this section, we discuss the effects of the (1) neuron activation, (2) shortcut-like connections, (3) loss function and (4) data distribution on the main results, respectively.",4 Discussions,[0],[0]
We show that the result may not hold if these assumptions are relaxed.,4 Discussions,[0],[0]
"To begin with, we discuss whether the results in Theorem 1 and 2 still hold if we vary the neuron activation function in the single layer network fS .",4.1 Neuron Activations,[0],[0]
"Specifically, we consider the following five classes of
neurons: (1) softplus class, (2) rectified linear unit (ReLU) class, (3) leaky rectified linear unit (Leaky ReLU) class, (4) quadratic class and (5) sigmoid class.",4.1 Neuron Activations,[0],[0]
"In the following, for each class of neurons, we show whether the main results hold and provide counterexamples if certain conditions in the main results are violated.",4.1 Neuron Activations,[0],[0]
We summarize our findings in Table 4.1.,4.1 Neuron Activations,[0],[0]
We visualize some neurons activation functions from these five classes in Fig. 2(a).,4.1 Neuron Activations,[0],[0]
"Softplus class contains neurons with real analytic activation functions σ, where σ′(z)",4.1 Neuron Activations,[0],[0]
"> 0, σ′′(z) > 0",4.1 Neuron Activations,[0],[0]
"for all z ∈ R. A widely used neuron in this class is the softplus neuron, i.e., σ(z) =",4.1 Neuron Activations,[0],[0]
"log2(1 + ez), which is a smooth approximation of ReLU.",4.1 Neuron Activations,[0],[0]
We can see that neurons in this class satisfy assumptions in both Theorem 1 and 2 and this indicates that both theorems hold for the neurons in this class.,4.1 Neuron Activations,[0],[0]
"ReLU class contains neurons with σ(z) = 0 for all z ≤ 0 and σ(z) is piece-wise continuous on R. Some commonly adopted neurons in this class include: threshold units, i.e., I{z ≥ 0}, rectified linear units (ReLU), i.e., max{z, 0} and rectified quadratic units (ReQU),",4.1 Neuron Activations,[0],[0]
"i.e., [max{z, 0}]2.",4.1 Neuron Activations,[0],[0]
We can see that neurons in this class do not satisfy neither assumptions in Theorem 1 nor 2.,4.1 Neuron Activations,[0],[0]
"In proposition 2, we show that when the single layer network fS consists of neurons in the ReLU class, even if all other conditions in Theorem 1 or 2 are satisfied, the empirical loss function can have a local minimum with non-zero training error.
",4.1 Neuron Activations,[0],[0]
Proposition 2 Suppose that assumptions 1 and 4 are satisfed.,4.1 Neuron Activations,[0],[0]
Assume that neurons in the network fS satisfy that σ(z) = 0 for all z ≤ 0 and σ(z) is piece-wise continuous on R.,4.1 Neuron Activations,[0],[0]
"Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 or 2 such that with probability one, the empirical loss L̂n(θ; p), p ≥ 2 has a local minima θ∗ = (θ∗S ,θ∗D) satisfying R̂n(θ ∗)",4.1 Neuron Activations,[0],[0]
"≥ min{n+,n−}n , where n+",4.1 Neuron Activations,[0],[0]
"and n− are the number of positive and negative samples, respectively.
",4.1 Neuron Activations,[0],[0]
Remark: (i),4.1 Neuron Activations,[0],[0]
"We note here that the above result holds in the over-parametrized case, where the number of neurons in the network fS is larger than the number of samples in the dataset.",4.1 Neuron Activations,[0],[0]
"In addition, all counterexamples shown in Section 4.1 hold in the over-parametrized case.",4.1 Neuron Activations,[0],[0]
"(ii) We note here that applying the same analysis, we can generalize the above result to a larger class of neurons satisfying the following condition: there exists a scalar z1 such that σ(z) = constant for all z ≤",4.1 Neuron Activations,[0],[0]
z1 and σ(z) is piece-wise continuous on R. (iii) We note that the training error is strictly non-zero when the dataset has both positive and negative samples and this can happen with probability at least 1− e−Ω(n).,4.1 Neuron Activations,[0],[0]
"Leaky-ReLU class contains neurons with σ(z) = z for all z ≥ 0 and σ(z) is piece-wise continuous on R. Some commonly used neurons in this class include ReLU, i.e., max{z, 0}, leaky rectified linear unit (Leaky-ReLU), i.e., σ(z)",4.1 Neuron Activations,[0],[0]
"= z for z ≥ 0, σ = αz for z ≤ 0 and some constant α ∈ (0, 1), exponential linear unit (ELU), i.e., σ(z) = z for z ≥ 0, σ(z) =",4.1 Neuron Activations,[0],[0]
α(exp(z)− 1) for z ≤ 0 and some constant α < 0.,4.1 Neuron Activations,[0],[0]
"We can see that all neurons in this class do not satisfy assumptions in Theorem 1, while some neurons in this class satisfy the condition in Theorem 2 (e.g., linear neuron, σ(z) = z) and some neurons do not (e.g., ReLU).",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"In Proposition 2, we have provided a counterexample showing that Theorem 2 does not hold for some neurons in this class (e.g., ReLU).",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"Next, we will present the following proposition to show that when the network fS consists of neurons in the Leaky-ReLU class, even if all other conditions in Theorem 1 are satisfied, the empirical loss function is likely to have a local minimum with non-zero training error with high probability.
",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
Proposition 3 Suppose that Assumption 1 and 4 are satisfied.,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
Assume that neurons in the network fS satisfy that σ(z) =,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"z for all z ≥ 0 and σ(z) is piece-wise continuous on R. Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 such that, with probability at least 1− e−Ω(n), the empirical loss L̂n(θ; p), p ≥ 2 has a local minima θ∗ = (θ∗S ,θ∗D) with non-zero training error.
",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"Remark: We note that applying the same proof, we can generalize the above result to a larger class of neurons, i.e., neurons satisfying the condition that there exists two scalars z1 and α such that σ(z) = α(z",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
− z1) for all z ≥ 0,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
and σ is piece-wise continuous on R.,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"In addition, we note that the ReLU neuron (but not all neurons in the ReLU class) satisfies the definition of both ReLU class and Leaky-ReLU class, and therefore both Proposition 2 and 3 hold for the ReLU neuron.",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"Sigmoid class contains neurons with σ(z) + σ(−z) ≡ constant on R. We list a few commonly adopted neurons in this family: sigmoid neuron, i.e., σ(z) = 1
1+e−z , hyperbolic tangent neuron, i.e.,
σ(z) = e z−1 ez+1 , arctangent neuron, i.e., σ(z) = tan −1(z) and softsign neuron, i.e., σ(z) = z1+|z| .",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
We note that all real odd functions2 satisfy the conditions of the sigmoid class.,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"We can see that none of the above neurons satisfy assumptions in Theorem 1, since neurons in this class satisfy either σ′′(z) + σ′′(−z) ≡ 0",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
for all z ∈ R or σ(z) is not twice differentiable.,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"For Theorem 2, we can see that some neurons in this class satisfy the condition in Theorem 2 (e.g., sigmoid neuron) and some neurons do not (e.g., constant neuron σ(z) ≡ 0",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
for all z ∈ R).,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"In Proposition 2, we provided a counterexample showing that Theorem 2 does not hold for some neurons in this class (e.g., constant neuron).",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"Next, we present the following proposition showing that when the network fS consists of neurons in the sigmoid class, then there always exists a data distribution satisfying the assumptions in Theorem 1 such that, with a positive probability, the empirical loss has a local minima with non-zero training error.
",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
Proposition 4 Suppose that assumptions 1 and 4 are satisfed.,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
Assume that there exists a constant c ∈ R such that neurons in the network fS satisfy σ(z)+σ(−z) ≡,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
c for all z ∈ R. Assume that the dataset D has 2n samples.,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"There exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 such that, with a positive probability, the empirical loss function L̂2n(θ; p), p ≥ 2 has a local minimum θ∗ = (θ∗S ,θ ∗ D) satisfying R̂2n(θ
∗)",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"≥ min{n−,n+}2n , where n+ and n− denote the number of positive and negative samples in the dataset, respectively.
",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"Remark: Proposition 4 shows that when the network fS consists of neurons in the sigmoid class, even if all other conditions are satisfied, the results in Theorem 1 does not hold with a positive probability.
",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
2A real function f :,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"R→ R is an odd function, if f(x) + f(−x)",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
≡ 0,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"for all x ∈ R.
Quadratic family contains neurons where σ(z) is real analytic and strongly convex on R and has a global minimum at the point z = 0.",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"A simple example of neuron in this family is the quadratic neuron, i.e., σ(z) = z2.",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
It is easy to check that all neurons in this class satisfy the conditions in Theorem 1 but not in Theorem 2.,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"For Theorem 2, we present a counterexample and show that, when the network fS consists of neurons in the quadratic class, even if positive and negative samples are linearly separable, the empirical loss can have a local minimum with non-zero training error.
",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
Proposition 5 Suppose that Assumption 1 and 4 are satisfied.,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
Assume that neurons in fS satisfy that σ is strongly convex and twice differentiable on R and has a global minimum at z = 0.,Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"There exists a network architecture fD and a distribution satisfying assumptions in Theorem 2 such that with probability one, the empirical loss L̂n(θ; p), p ≥ 2 has a local minima θ∗ = (θ∗S ,θ∗D) satisfying R̂n(θ
∗) ≥ min{n+,n−}n , where n+ and n− denote the number of positive and negative samples in the dataset, respectively.",Theorem Softplus ReLU Leaky-ReLU Sigmoid Quadratic,[0],[0]
"In this subsection, we discuss whether the main results still hold if we remove the shortcut-like connections or replace them with the identity shortcut connections used in the residual network [33].",4.2 Shortcut-like Connections,[0],[0]
"Specifically, we provide two counterexamples and show that the main results do not hold if the shortcut-like connections are removed or replaced with the identity shortcut connections.
",4.2 Shortcut-like Connections,[0],[0]
Feed-forward networks.,4.2 Shortcut-like Connections,[0],[0]
"When the shortcut-like connections (i.e., the network fS in Figure 1(b)) are removed, the network architecture can be viewed as a standard feedforward neural network.",4.2 Shortcut-like Connections,[0],[0]
"We provide a counterexample to show that, for a feedforward network with ReLU neurons, even if the other conditions in Theorem 1 or 2 are satisfied, the empirical loss functions is likely to have a local minimum with non-zero training error.",4.2 Shortcut-like Connections,[0],[0]
"In other words, neither Theorem 1 nor 2 holds when the shortcut-like connections are removed.
",4.2 Shortcut-like Connections,[0],[0]
Proposition 6 Suppose that assumption 1 is satisfied.,4.2 Shortcut-like Connections,[0],[0]
Assume that the feedforward network f(x;θ) has at least one hidden layer and at least one neuron in each hidden layer.,4.2 Shortcut-like Connections,[0],[0]
"If neurons in the network f satisfy that σ(z) = 0 for all z ≤ 0 and σ(z) is continuous on R, then for any dataset D with n samples, the empirical loss L̂n(θ; p), p ≥ 2 has a local minima θ∗ with R̂n(θ∗) ≥ min{n+,n−}n , where n+",4.2 Shortcut-like Connections,[0],[0]
"and n− are the number of positive and negative samples in the dataset, respectively.
",4.2 Shortcut-like Connections,[0],[0]
"Remark: The result holds for ReLUs, since it is easy to check that the ReLU neuron satisfies the above assumptions.
",4.2 Shortcut-like Connections,[0],[0]
Identity shortcut connections.,4.2 Shortcut-like Connections,[0],[0]
"As we stated earlier, adding shortcut-like connections to a network can improve the loss surface.",4.2 Shortcut-like Connections,[0],[0]
"However, the shortcut-like connections shown in Fig 1(b) are different from some popular shortcut connections used in the real-world applications, e.g., the identity shortcut connections in the residual network.",4.2 Shortcut-like Connections,[0],[0]
"Thus, a natural question arises: do the main results still hold if we use the identity shortcut connections?",4.2 Shortcut-like Connections,[0],[0]
"To address the question, we provide the following counterexample to show that, when we replace the shortcut-like connections with the identity shortcut connections, even if the other conditions in Theorem 1 are satisfied, the empirical loss function is likely to have a local minimum with non-zero training error.",4.2 Shortcut-like Connections,[0],[0]
"In other words, Theorem 1 does not hold for the identity shortcut connections.
",4.2 Shortcut-like Connections,[0],[0]
Proposition 7 Assume that H : Rd → Rd is a feedforward neural network parameterized by θ and all neurons in H are ReLUs.,4.2 Shortcut-like Connections,[0],[0]
"Define a network f : Rd → R with identity shortcut connections as f(x;a,θ, b) = a>(x + H(x;θ))",4.2 Shortcut-like Connections,[0],[0]
"+ b, a ∈ Rd, b ∈ R.",4.2 Shortcut-like Connections,[0],[0]
"Then there exists a distribution PX×Y satisfying the assumptions in Theorem 1 such that with probability at least 1 − e−Ω(n), the empirical loss L̂n(a,θ, b; p) = 1 n",4.2 Shortcut-like Connections,[0],[0]
∑n i=1,4.2 Shortcut-like Connections,[0],[0]
"`(−yif(xi;θ); p), p ≥ 2 has a local minimum with non-zero training error.",4.2 Shortcut-like Connections,[0],[0]
"In this subsection, we discuss whether the main results still hold if we change the loss function.",4.3 Loss Functions,[0],[0]
We mainly focus on the following two types of surrogate loss functions: quadratic loss and logistic loss.,4.3 Loss Functions,[0],[0]
"We will show that if the loss function is replaced with the quadratic loss or logistic loss, then neither Theorem 1 nor 2 holds.",4.3 Loss Functions,[0],[0]
"In addition, we show that when the loss function is the logistic loss and the network is a feedforward neural network, there are no local minima with zero training error in the real parameter space.",4.3 Loss Functions,[0],[0]
"In Fig. 2(b), we visualize some surrogate loss functions discussed in this subsection.",4.3 Loss Functions,[0],[0]
Quadratic loss.,4.3 Loss Functions,[0],[0]
The quadratic loss `(z) =,4.3 Loss Functions,[0],[0]
(1 + z)2 has been well-studied in prior works.,4.3 Loss Functions,[0],[0]
"It has been shown that when the loss function is quadratic, under certain assumptions, all local minima of the empirical loss are global minima.",4.3 Loss Functions,[0],[0]
"However, the global minimum of the quadratic loss does not necessarily have zero misclassification error, even in the realizable case (i.e., the case where there exists a set of parameters such that the network achieves zero misclassification error on the dataset or the data distriubtion).",4.3 Loss Functions,[0],[0]
"To illustrate this, we provide a simple example where the network is a simplified linear network and the data distribution is linearly separable.
",4.3 Loss Functions,[0],[0]
"Example 1 Let the distribution PX×Y satisfy that P(Y = 1) = P(Y = −1) = 0.5, P(X = 5/4|Y = 1) = 1 and PX|Y=−1 is a uniform distribution on the interval",4.3 Loss Functions,[0],[0]
"[0, 1].",4.3 Loss Functions,[0],[0]
"For a linear model f(x; a, b) = ax+b, a, b ∈ R, every global minimum (a∗, b∗) of the population loss L(a, b) = EX×Y",4.3 Loss Functions,[0],[0]
"[(1−Y f(X; a, b))2] satisfies PX×Y",4.3 Loss Functions,[0],[0]
"[Y 6= sgn(f(X; a∗, b∗))]",4.3 Loss Functions,[0],[0]
"≥ 1/16.
",4.3 Loss Functions,[0],[0]
Remark: The proof of the above result in Appendix B.7 is very straightforward.,4.3 Loss Functions,[0],[0]
"We have only provided it there since we are unable to find a reference which explicitly states such a result, but we will not be surprised if this result has been known to others.",4.3 Loss Functions,[0],[0]
"This example shows that every global minimum of the quadratic loss has non-zero misclassification error, although the linear model is able to achieve zero misclassification error on this data distribution.",4.3 Loss Functions,[0],[0]
"Similarly, one can easily find datasets under which all global minima of the quadratic loss have non-zero training error.
",4.3 Loss Functions,[0],[0]
"In addition, we provide two examples in Appendix B.8 and show that, when the loss function is replaced with the quadratic loss, even if the other conditions in Theorem 1 or 2 are satisfied, every global minimum of the empirical loss has a training error larger than 1/8 with a positive probability.",4.3 Loss Functions,[0],[0]
"In other words, our main results do hold for the quadratic loss.
",4.3 Loss Functions,[0],[0]
The following observation may be of independent interest.,4.3 Loss Functions,[0],[0]
"Different from the quadratic loss, the loss functions conditioned in Assumption 1 have the following two properties: (i) the minimum empirical loss is zero if and only if there exists a set of parameters achieving zero training error; (ii) every global minimum of the empirical loss has zero training error in the realizable case.
",4.3 Loss Functions,[0],[0]
Proposition 8,4.3 Loss Functions,[0],[0]
Let f :,4.3 Loss Functions,[0],[0]
Rd → R denote a feedforward network parameterized by θ and let the dataset have n samples.,4.3 Loss Functions,[0],[0]
"When the loss function `p satisfies Assumption 1 and p ≥ 1, we have minθ L̂n(θ; p) = 0",4.3 Loss Functions,[0],[0]
if and only if minθ R̂n(θ) = 0.,4.3 Loss Functions,[0],[0]
"Furthermore, if minθ R̂n(θ) = 0, every global minimum θ
∗ of the empirical loss L̂n(θ; p) has zero training error, i.e., R̂n(θ ∗) = 0.
",4.3 Loss Functions,[0],[0]
Remark: We note that the network does not need to be a feedforward network.,4.3 Loss Functions,[0],[0]
"In fact, the same results hold for a large class of network architectures, including both architectures shown in Fig 1.",4.3 Loss Functions,[0],[0]
"We provide additional analysis in Appendix B.9.
Logistic loss.",4.3 Loss Functions,[0],[0]
"The logistic loss `(z) = log2 (1 + e z) is different from the loss functions conditioned in Assumption 1, since the logistic loss does not have a global minimum on R. Here, for the logistic loss function, we show that even if the remaining assumptions in Theorem 1 hold, every critical point is a saddle point.",4.3 Loss Functions,[0],[0]
"In other words, Theorem 1 does not hold for logistic loss.",4.3 Loss Functions,[0],[0]
"Additional analysis on Theorem 2 are provided in Appendix B.11.
",4.3 Loss Functions,[0],[0]
"Proposition 9 Assume that the loss function is the logistic loss, i.e., `(z) = log2(1",4.3 Loss Functions,[0],[0]
+ e z).,4.3 Loss Functions,[0],[0]
Assume that assumptions 2-5 are satisfied.,4.3 Loss Functions,[0],[0]
Assume,4.3 Loss Functions,[0],[0]
"that samples in the dataset D = {(xi, yi)}ni=1, n",4.3 Loss Functions,[0],[0]
≥ 1 are independently drawn from the distribution PX×Y .,4.3 Loss Functions,[0],[0]
"Assume that the number of neurons M in the network fS satisfies M ≥ 2 max{ n∆r , r+, r−}, where ∆r = r − max{r+, r−}.",4.3 Loss Functions,[0],[0]
"If θ∗ denotes a critical point of the empirical loss L̂n(θ), then θ ∗ is a saddle point.",4.3 Loss Functions,[0],[0]
"In particular, there are no local minima.
",4.3 Loss Functions,[0],[0]
"Remark: We note here that the result can be generalized to every loss function ` which is real analytic and has a positive derivative on R. Furthermore, we provide the following result to show that when the dataset contains both positive and negative samples, if the loss is the logistic loss, then every critical point of the empirical loss function has non-zero training error.
",4.3 Loss Functions,[0],[0]
"Proposition 10 Assume the dataset D = {(xi, yi)}ni=1 consists of both positive and negative samples.",4.3 Loss Functions,[0],[0]
Assume that f(x;θ) is a feedforward network parameterized by θ.,4.3 Loss Functions,[0],[0]
"Assume that the loss function is logistic, i.e., `(z) = log2 (1 + e
z).",4.3 Loss Functions,[0],[0]
"If the real parameters θ∗ denote a critical point of the empirical loss L̂n(θ ∗), then R̂n(θ ∗)",4.3 Loss Functions,[0],[0]
"> 0.
",4.3 Loss Functions,[0],[0]
Remark: We provide the proof in Appendix B.12.,4.3 Loss Functions,[0],[0]
The above proposition implies every critical point is either a local minimum with non-zero training error or is a saddle point (also with non-zero training error).,4.3 Loss Functions,[0],[0]
"We note here that, similar to Proposition 9, the result can be generalized to every loss function ` that is differentiable and has a positive derivative on R.",4.3 Loss Functions,[0],[0]
"In this paper, we have mainly considered a class of non-linearly separable distribution where positive and negative samples are located on different subspaces.",4.4 Open Problem: Datasets,[0],[0]
"We show that if the samples are drawn from such a distribution, under certain additional conditions, all local minima of the empirical loss have zero training errors.",4.4 Open Problem: Datasets,[0],[0]
"However, one may ask: how well does the result generalize to other non-linearly separable distributions or datasets?",4.4 Open Problem: Datasets,[0],[0]
"Here, we partially answer this question by presenting the following necessary condition on the dataset so that Theorem 1 can hold.
",4.4 Open Problem: Datasets,[0],[0]
"Proposition 11 Suppose that assumptions 1, 4 and 5 are satisfied.",4.4 Open Problem: Datasets,[0],[0]
"For any feedforward architecture fD(x;θD), every local minimum θ ∗ = (θ∗S ,θ ∗ D) of the empirical loss function L̂n(θS ,θD; p), p ≥ 6 satisfies R̂n(θ ∗)",4.4 Open Problem: Datasets,[0],[0]
= 0,4.4 Open Problem: Datasets,[0],[0]
"only if the matrix ∑n i=1 λiyixix > i is neither positive nor negative definite for all
sequences {λi ≥ 0}ni=1",4.4 Open Problem: Datasets,[0],[0]
satisfying ∑ i:yi=1 λi = ∑ i:yi=−1 λi > 0 and ‖ ∑n i=1,4.4 Open Problem: Datasets,[0],[0]
"λiyixi‖2 = 0.
",4.4 Open Problem: Datasets,[0],[0]
"Remark: The proposition implies that when the dataset does not meet this necessary condition, there exists a feedforward architecture fD",4.4 Open Problem: Datasets,[0],[0]
such that the empirical loss function has a local minimum with a non-zero training error.,4.4 Open Problem: Datasets,[0],[0]
We use this implication to prove the counterexamples provided in Appendix B.14 when Assumption 2 or 3 on the dataset is not satisfied.,4.4 Open Problem: Datasets,[0],[0]
"Therefore, Theorem 1 no longer holds when Assumption 2 or 3 is removed.",4.4 Open Problem: Datasets,[0],[0]
We note that the necessary condition shown here is not equivalent to Assumption 2 and 3.,4.4 Open Problem: Datasets,[0],[0]
"Now we present the following result to show the sufficient and necessary condition that the dataset should satisfy so that Proposition 1 can hold.
",4.4 Open Problem: Datasets,[0],[0]
Proposition 12 Suppose that the loss function `p satisfies Assumption 1 and neurons in the network satisfy Assumption 5.,4.4 Open Problem: Datasets,[0],[0]
"Assume that the single layer network fS(x;θS) has M > d neurons and assume that neurons in fS are quadratic neurons, i.e., σ(z) = z
2.",4.4 Open Problem: Datasets,[0],[0]
"For any network architecture fD(x;θD), every local minimum θ ∗ = (θ∗S ,θ ∗ D) of the empirical loss function L̂n(θS ,θD; p), p ≥ 6 satisfies R̂n(θ ∗)",4.4 Open Problem: Datasets,[0],[0]
= 0,4.4 Open Problem: Datasets,[0],[0]
if and only if the matrix,4.4 Open Problem: Datasets,[0],[0]
∑n i=1 λiyixix,4.4 Open Problem: Datasets,[0],[0]
"> i is indefinite for all sequences {λi ≥ 0}ni=1
satisfying ∑
i:yi=1 λi = ∑",4.4 Open Problem: Datasets,[0],[0]
i:,4.4 Open Problem: Datasets,[0],[0]
"yi=−1 λi > 0.
",4.4 Open Problem: Datasets,[0],[0]
Remark: (i),4.4 Open Problem: Datasets,[0],[0]
"This sufficient and necessary condition implies that for any network architecture fD, there exists a set of parameters θ = (θS ,θD) such that the network f(x;θ) = fS(x;θS) + fD(x;θD) can correctly classify all samples in the dataset.",4.4 Open Problem: Datasets,[0],[0]
"This also indicates the existence of a set of parameters achieving zero training error, regardless of the network architecture of fD. We provide the proof in Appendix B.15.",4.4 Open Problem: Datasets,[0],[0]
(ii) We note that Proposition 12 only holds for the quadratic neuron.,4.4 Open Problem: Datasets,[0],[0]
The problem of finding the sufficient and necessary conditions for the other types of neurons is open.,4.4 Open Problem: Datasets,[0],[0]
"In this paper, we studied the surface of a smooth version of the hinge loss function in binary classification problems.",5 Conclusions,[0],[0]
"We provided conditions under which the neural network has zero misclassification error at all local minima and also provide counterexamples to show that when some of these assumptions are relaxed, the result may not hold.",5 Conclusions,[0],[0]
Further work involves exploiting our results to design efficient training algorithms classification tasks using neural networks.,5 Conclusions,[0],[0]
Lemma 1 (Necessary condition.),A.1 Proof of Lemma 1,[0],[0]
Assume that neurons σ in the network fS are twice differentiable and the loss function ` : R→ R has a continuous derivative on R up to the third order.,A.1 Proof of Lemma 1,[0],[0]
"If n ≥ 1 and parameters θ∗ = (θ∗S ,θ ∗ D) denote a local minimum of the loss function L̂n(θ), then for any j = 1, ...,M ,
n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi = 0d.
",A.1 Proof of Lemma 1,[0],[0]
Proof: We first recall some notations defined in the paper.,A.1 Proof of Lemma 1,[0],[0]
"The output of the neural network is
f(x;θ) = fS(x;θS) + fD(x;θD),
where fS(x;θS) is the single layer neural network parameterized by θS , i.e.,
fS(x;θS)",A.1 Proof of Lemma 1,[0],[0]
"= a0 + M∑ j=1 ajσ ( w>j x ) ,
and fD(x;θD) is a deep neural network parameterized by θD.",A.1 Proof of Lemma 1,[0],[0]
"The empirical loss function is given by
L̂n(θ) = L̂n(θS ,θD) = 1
n n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`(−yif(xi;θ)).
",A.1 Proof of Lemma 1,[0],[0]
"Since the loss function ` has a continuous derivative on R up to the third order, neurons σ in the network fS are twice differentiable, then the gradient vector ∇θS L̂n(θ∗S ,θ∗D) and the Hessian matrix ∇2θS L̂n(θ ∗ S ,θ ∗ D) exists.",A.1 Proof of Lemma 1,[0],[0]
"Furthermore, by the assumption that θ ∗ = (θ∗S ,θ ∗ D) is a local minima of the loss function L̂n(θ), then we should have for j = 1, ...,M ,
0d = ∇wjLn(θ∗) = n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))(−yi∇wjf(xi;θ∗))
= n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))(−yia∗jσ′(w∗j>xi)xi)
= −a∗j n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi.,A.1 Proof of Lemma 1,[0],[0]
"(1)
Now we need to prove that if θ∗ is a local minima, then
∀j ∈",A.1 Proof of Lemma 1,[0],[0]
"[M ], ∥∥∥∥∥ n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi ∥∥∥∥∥
2
= 0.
",A.1 Proof of Lemma 1,[0],[0]
We prove it by contradiction.,A.1 Proof of Lemma 1,[0],[0]
Assume that there exists j ∈,A.1 Proof of Lemma 1,[0],[0]
[M ] such that∥∥∥∥∥ n∑ i=1,A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi ∥∥∥∥∥
2
6= 0.
",A.1 Proof of Lemma 1,[0],[0]
"Then by equation (1), we have a∗j = 0.",A.1 Proof of Lemma 1,[0],[0]
"Now, we consider the following Hessian matrix H(aj ,wj).",A.1 Proof of Lemma 1,[0],[0]
"Since θ∗ is a local minima of the loss function L̂n(θ), then the matrix H(aj ,wj) should be positive semidefinite at (a∗j ,w ∗ j ).",A.1 Proof of Lemma 1,[0],[0]
"By a ∗ j = 0, we have
∇2wjLn(θ∗) = −a∗j∇wj",A.1 Proof of Lemma 1,[0],[0]
[ n∑ i=1,A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi ] = 0d×d,
∂ [ ∇wjLn(θ∗) ]",A.1 Proof of Lemma 1,[0],[0]
∂aj = − n∑ i=1,A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi
",A.1 Proof of Lemma 1,[0],[0]
"− a∗j ∂
∂aj [ n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi ]
",A.1 Proof of Lemma 1,[0],[0]
= − n∑ i=1,A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi.
",A.1 Proof of Lemma 1,[0],[0]
"In addition, we have
∂2Ln(θ ∗)
∂a2j",A.1 Proof of Lemma 1,[0],[0]
"=
∂
∂aj [ n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
`′(−yif(xi;θ∗))(−yiσ(w∗j>xi)),A.1 Proof of Lemma 1,[0],[0]
"]
= n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′′(−yif(xi;θ∗))σ2(w∗j>xi).
",A.1 Proof of Lemma 1,[0],[0]
"Since the matrix H(a∗j ,w ∗ j ) is positive semidefinite, then for any α ∈ R and ω ∈ Rd,
( α ω> )",A.1 Proof of Lemma 1,[0],[0]
"H(a∗j ,w ∗ j )",A.1 Proof of Lemma 1,[0],[0]
( α ω ),A.1 Proof of Lemma 1,[0],[0]
"≥ 0.
",A.1 Proof of Lemma 1,[0],[0]
"Since
( α ω> ) H(a∗j ,w ∗ j )",A.1 Proof of Lemma 1,[0],[0]
( α ω ),A.1 Proof of Lemma 1,[0],[0]
= α2 n∑ i=1,A.1 Proof of Lemma 1,[0],[0]
"`′′(−yif(xi;θ∗))σ2(w∗j>xi)
− αω> n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi,
and by setting
ω = n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi,
then
( α ω> )",A.1 Proof of Lemma 1,[0],[0]
"H(a∗j ,w ∗ j )",A.1 Proof of Lemma 1,[0],[0]
( α ω ),A.1 Proof of Lemma 1,[0],[0]
= α2 n∑ i=1,A.1 Proof of Lemma 1,[0],[0]
"`′′(−yif(xi;θ∗))σ2(w∗j>xi)
− α ∥∥∥∥∥ n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi ∥∥∥∥∥ 2
2
.
",A.1 Proof of Lemma 1,[0],[0]
"Furthermore, since we assume that∥∥∥∥∥ n∑ i=1",A.1 Proof of Lemma 1,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi ∥∥∥∥∥ 2
2
> 0,
then clearly, there exists α such that
( α ω> )",A.1 Proof of Lemma 1,[0],[0]
"H(a∗j ,w ∗ j )",A.1 Proof of Lemma 1,[0],[0]
( α ω ),A.1 Proof of Lemma 1,[0],[0]
"< 0.
and this leads to the contradiction.",A.1 Proof of Lemma 1,[0],[0]
"Thus, we proved the lemma.",A.1 Proof of Lemma 1,[0],[0]
"Theorem 3 Assume that the loss function `p satisfies assumption 1, the distribution PX×Y satisfies assumption 2 and 3, the network architecture satisfies assumption 4 and neurons in the network satisfy assumption 5.",A.2 Proof of Theorem 1,[0],[0]
Assume,A.2 Proof of Theorem 1,[0],[0]
"that samples in the dataset D = {(xi, yi)}ni=1, n",A.2 Proof of Theorem 1,[0],[0]
≥ 1 are independently drawn from the distribution PX×Y .,A.2 Proof of Theorem 1,[0],[0]
"Assume that the number of neurons M in the network fS satisfies M ≥ 2 max{ n∆r , r+, r−}, where ∆r = r −max{r+, r−}.",A.2 Proof of Theorem 1,[0],[0]
"If the real parameters θ∗ = (θ∗S ,θ∗D) denote a local minimum of the loss function L̂n(θS ,θD; p) and p ≥ 6, then R̂n(θ∗S ,θ∗D) = 0 holds with probability one.
",A.2 Proof of Theorem 1,[0],[0]
Proof: We first present some notations used in this proof.,A.2 Proof of Theorem 1,[0],[0]
"The output of the neural network is
f(x;θ) = fS(x;θS) + fD(x;θD),
where fS(x;θS) is the single layer neural network parameterized by θS , i.e.,
fS(x;θS)",A.2 Proof of Theorem 1,[0],[0]
"= a0 + M∑ j=1 ajσ ( w>j x ) ,
and fD(x;θD) is a deep neural network parameterized by θD.",A.2 Proof of Theorem 1,[0],[0]
"The empirical loss function is given by
L̂n(θ; p) = L̂n(θS ,θD; p)",A.2 Proof of Theorem 1,[0],[0]
"= 1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`p(−yif(xi;θ))
",A.2 Proof of Theorem 1,[0],[0]
"We first assume that the real parameters θ∗ = (θ∗S ,θ ∗ D) denote a local minima of the loss function L̂n(θ; p).",A.2 Proof of Theorem 1,[0],[0]
"Next, we prove the following two claims: Claim 1: If θ∗ = (θ∗S ,θ ∗ D) is a local minima and there exists j ∈",A.2 Proof of Theorem 1,[0],[0]
"[M ] such that a∗j = 0, then R̂n(θ ∗) = 0.",A.2 Proof of Theorem 1,[0],[0]
Claim 2:,A.2 Proof of Theorem 1,[0],[0]
"If θ∗ = (θ∗S ,θ ∗ D) is a local minima and a ∗ j 6= 0 for all j ∈",A.2 Proof of Theorem 1,[0],[0]
"[M ], then R̂n(θ∗) = 0.",A.2 Proof of Theorem 1,[0],[0]
(a) Proof of claim 1.,A.2 Proof of Theorem 1,[0],[0]
"We prove that if θ∗ = (θ∗S ,θ ∗ D) is a local minima of the loss function L̂n(θ; p) and there exists j ∈",A.2 Proof of Theorem 1,[0],[0]
"[M ] such that a∗j = 0, then R̂n(θ∗) = 0.",A.2 Proof of Theorem 1,[0],[0]
"Without loss of generality, we assume that a∗1 = 0.",A.2 Proof of Theorem 1,[0],[0]
"Since θ ∗ = (θ∗S ,θ ∗ D) is a local minima, then there exists ε0 > 0",A.2 Proof of Theorem 1,[0],[0]
"such that for all small perturbations ∆a1, ∆w1 on the parameters a ∗ 1 and w ∗ 1, i.e., |∆a1|2 + ‖∆w1‖22 ≤ ε20, we have
L̂n(θ̃S ,θ ∗ D; p) ≥ L̂n(θ∗S ,θ∗D; p),
where θ̃S = (ã0, ã1, ..., ãM , w̃1, ..., w̃M ), ã1 = a ∗ 1 + ∆a1, w̃1 = w ∗ 1 + ∆w1 and ãj = a ∗ j , w̃j",A.2 Proof of Theorem 1,[0],[0]
= w ∗ j for j 6= 1.,A.2 Proof of Theorem 1,[0],[0]
"Now we consider the Taylor expansion of L̂n(θ̃S ,θ∗D; p) at the point θ∗ = (θ∗S ,θ∗D).",A.2 Proof of Theorem 1,[0],[0]
"We note here that the Taylor expansion of L̂(θS ,θ ∗ D; p) on θS always exists, since the empirical loss function L̂n has continuous derivatives with respect to fS up to the p-th order and the output of the neural network f(x;θS) is infinitely differentiable with respect to θS due to the fact that neuron activation function σ is real analytic.",A.2 Proof of Theorem 1,[0],[0]
"We first calculate the first order derivatives at the point θ∗,
dL̂n(θ ∗; p)
da1 =
1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ ( w∗1 >xi ) = 0, θ∗ is a critical point,
∇w1L̂n(θ∗; p) = a∗1 n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ) xi = 0d, θ ∗ is a critical point.
",A.2 Proof of Theorem 1,[0],[0]
"Next, we calculate the second order derivatives at the point θ∗,
d2L̂n(θ ∗; p)
",A.2 Proof of Theorem 1,[0],[0]
"da21 =
1
n",A.2 Proof of Theorem 1,[0],[0]
N∑ i=1,A.2 Proof of Theorem 1,[0],[0]
"`′′p(−yif(xi;θ∗))σ2 ( w∗1 >xi ) ≥ 0,
d
da1 (∇w1L̂n(θ∗; p))",A.2 Proof of Theorem 1,[0],[0]
"=
1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ),A.2 Proof of Theorem 1,[0],[0]
"xi
+ a∗1 n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′′p(−yif(xi;θ∗))σ ( w∗1 >xi ),A.2 Proof of Theorem 1,[0],[0]
σ′,A.2 Proof of Theorem 1,[0],[0]
"( w∗1 >xi ) xi
= 0d,
where the first term equals to the zero vector by the necessary condition for a local minima presented in Lemma 1 and the second term equals to the zero vector by the assumption that a∗1 = 0.",A.2 Proof of Theorem 1,[0],[0]
"Furthermore, by the assumption that a∗1 = 0, we have
∇2w1L̂n(θ∗; p) = a∗1 n ∇w1",A.2 Proof of Theorem 1,[0],[0]
[ n∑ i=1,A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ) xi ] = 0d×d.
",A.2 Proof of Theorem 1,[0],[0]
"Now, we further calculate the third order derivatives
d
da1
[ ∇2w1L̂n(θ∗; p) ]",A.2 Proof of Theorem 1,[0],[0]
"= 1
n
d
da1
[ a∗1∇w1 [ n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ) xi,A.2 Proof of Theorem 1,[0],[0]
"]]
= ∇w1
[ 1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ) xi ] + 0d×d by a ∗ 1 = 0
= 1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′′ ( w∗1 >xi ) xix >,A.2 Proof of Theorem 1,[0],[0]
"i
+ a∗1 n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′′p(−yif(xi;θ∗)),A.2 Proof of Theorem 1,[0],[0]
[ σ′ ( w∗1 >xi )]2 xix >,A.2 Proof of Theorem 1,[0],[0]
"i
= 1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′′ ( w∗1 >xi ) xix,A.2 Proof of Theorem 1,[0],[0]
>,A.2 Proof of Theorem 1,[0],[0]
"i by a ∗ 1 = 0
and
∇3w1L̂n(θ∗; p) = a∗1 n ∇2w1",A.2 Proof of Theorem 1,[0],[0]
[ n∑ i=1,A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ) xi ] = 0d×d×d.
",A.2 Proof of Theorem 1,[0],[0]
"In fact, it is easy to show that for any 2 ≤ k ≤ p,
∇kw1L̂n(θ∗; p) = a∗1 n ∇k−1w1",A.2 Proof of Theorem 1,[0],[0]
[ n∑ i=1,A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ) xi ] = 0d× d× ...× d︸,A.2 Proof of Theorem 1,[0],[0]
"︷︷ ︸
k times
.
",A.2 Proof of Theorem 1,[0],[0]
"Let ε > 0, |∆a1| = ε9/4 and ∆w1 = εu1 for u1 : ‖u1‖2 = 1.",A.2 Proof of Theorem 1,[0],[0]
"Clearly, when ε→ 0, ∆a1 = o(‖∆w1‖2), ∆a1 = o(1) and ‖∆w1‖ = o(1).",A.2 Proof of Theorem 1,[0],[0]
"Then we expand L̂n(θ̃; p) at the point θ∗ up to the sixth order and
thus as ε→ 0,
L̂n(θ̃; p) = L̂n(θ ∗; p) +
1
2!
",A.2 Proof of Theorem 1,[0],[0]
"d2L̂n(θ ∗; p)
d2a1 (∆a1)
2
+ 1
2 ∆a1∆w
> 1
d
da1
[ ∇2w1L̂n(θ∗; p) ]",A.2 Proof of Theorem 1,[0],[0]
"∆w1 + o(|∆a1|2) + o(|∆a1|‖∆w1‖22) + o(‖∆w1‖52)
",A.2 Proof of Theorem 1,[0],[0]
= L̂n(θ ∗),A.2 Proof of Theorem 1,[0],[0]
"+
1
2!
",A.2 Proof of Theorem 1,[0],[0]
"d2L̂n(θ ∗; p)
d2a1 ε9/2
+ 1
2n sgn(∆a1)ε 9/4+2 n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′′ ( w∗1 >xi ) (u>1 xi) 2
+ o(ε9/2) + o(ε9/4+2) + o(ε5)
= L̂n(θ ∗)",A.2 Proof of Theorem 1,[0],[0]
"+
1
2n sgn(∆a1)ε",A.2 Proof of Theorem 1,[0],[0]
17/4 n∑ i=1,A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′′ ( w∗1 >xi ) (u>1 xi) 2 + o(ε17/4).
",A.2 Proof of Theorem 1,[0],[0]
"Since ε > 0 and L̂n(θ̃; p) ≥ L̂n(θ∗; p) holds for any u1 : ‖u1‖2 = 1 and any sgn(∆a1) ∈ {−1, 1}, then n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′′ ( w∗1 >xi ) (u>xi) 2 = 0, for any u ∈ Rd.",A.2 Proof of Theorem 1,[0],[0]
"(2) Therefore, n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′′ ( w∗1 >xi ) xix >,A.2 Proof of Theorem 1,[0],[0]
i = 0d×d.,A.2 Proof of Theorem 1,[0],[0]
"By assumption that there exists a set of orthogonal basis E = {e1, ..., ed} in Rd and a subset U+ ⊆ E such that PX|Y (X ∈ Span(U1)|Y = 1) = 1 and by assumption that r = |U+ ∪ U−| > max{r+, r−} = max{|U+|, |U−|}, then the set U+\U− is not an empty set.",A.2 Proof of Theorem 1,[0],[0]
"It is easy to show that for any vector v ∈ U+\U−, PX×Y (v>X = 0|Y = 1) = 0.",A.2 Proof of Theorem 1,[0],[0]
We prove it by contradiction.,A.2 Proof of Theorem 1,[0],[0]
"If we assume p = PX×Y (v>X = 0|Y = 1) > 0, then for random vectors X1, ...,X|U+| independently drawn from the conditional distribution PX|Y=1,
PX|Y=1 |U+|⋃ i=1",A.2 Proof of Theorem 1,[0],[0]
{ v>Xi = 0 } ∣∣∣∣∣Y,A.2 Proof of Theorem 1,[0],[0]
= 1  = |U+|∏ i=1,A.2 Proof of Theorem 1,[0],[0]
"PX|Y=1 ( v>Xi = 0|Y = 1 ) = p|U+| > 0.
",A.2 Proof of Theorem 1,[0],[0]
"Furthermore, since X1, ...,X|U+| ∈ Span(U+), v>Xi = 0, i = 1, ..., |U+| and v ∈ U+, then the rank of the matrix ( X1, ...,X|U+| ) is at most |U+| − 1 and this indicates that the matrix is not a full rank matrix with probability p|U+| > 0.",A.2 Proof of Theorem 1,[0],[0]
This leads to the contradiction with the Assumption 2.,A.2 Proof of Theorem 1,[0],[0]
"Thus, with probability 1, v>xi 6= 0 for all i : yi = 1 and v>xi = 0 for all i : yi = −1.",A.2 Proof of Theorem 1,[0],[0]
"Therefore, by setting u = v in Equation (2), we have
0 =",A.2 Proof of Theorem 1,[0],[0]
"− ∑ i:yi=1 `′p(−yif(xi;θ∗))σ′′(w∗1>xi)(v>xi)2 ≤ 0,
where the equality holds if and only if ∀i : yi = 1, `′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
= 0,A.2 Proof of Theorem 1,[0],[0]
"and this further indicates that ∀i : yi = 1, yif(xi;θ∗) ≥ z0 > 0.",A.2 Proof of Theorem 1,[0],[0]
"Furthermore, since θ∗ is a critical point and thus
0 = dL̂n(θ
∗; p)
da0 =
1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi) =,A.2 Proof of Theorem 1,[0],[0]
− 1 n ∑,A.2 Proof of Theorem 1,[0],[0]
i:yi=1 `′p(−yif(xi;θ∗)),A.2 Proof of Theorem 1,[0],[0]
+ 1 n ∑,A.2 Proof of Theorem 1,[0],[0]
"i:yi=−1 `′p(−yif(xi;θ∗))
",A.2 Proof of Theorem 1,[0],[0]
"= 1
n",A.2 Proof of Theorem 1,[0],[0]
∑,A.2 Proof of Theorem 1,[0],[0]
"i:yi=−1 `′p(−yif(xi;θ∗)).
",A.2 Proof of Theorem 1,[0],[0]
"Therefore, ∀i : yi = −1, yif(xi;θ∗)",A.2 Proof of Theorem 1,[0],[0]
≥ z0 > 0,A.2 Proof of Theorem 1,[0],[0]
and this indicates that R̂n(θ∗) = 0.,A.2 Proof of Theorem 1,[0],[0]
Proof of Claim 2:,A.2 Proof of Theorem 1,[0],[0]
"First, we define M0 = dM/2e, then
M0 ≥ max{r+, r−}.
",A.2 Proof of Theorem 1,[0],[0]
"In addition, since r = |U+ ∪ U−|, then max{r+, r−}+ min{r+, r−} ≥ r. Therefore,
2M0 ≥ 2 max{r+, r−} > 2r",A.2 Proof of Theorem 1,[0],[0]
− r+ − r− ≥ 2 min{r,A.2 Proof of Theorem 1,[0],[0]
"− r+, r − r−} , 2K,
where we define K = min{r",A.2 Proof of Theorem 1,[0],[0]
"− r+, r − r−}.",A.2 Proof of Theorem 1,[0],[0]
"Since in claim 2, we assume that a∗j 6= 0 for all j ∈",A.2 Proof of Theorem 1,[0],[0]
"[M ], then there exists ai1 , ..., aiM0 , i1 < i2 < ...",A.2 Proof of Theorem 1,[0],[0]
"< iM0 having the same sign, i.e.,
sgn(ai1) = ...",A.2 Proof of Theorem 1,[0],[0]
"= sgn(aiM0 ).
",A.2 Proof of Theorem 1,[0],[0]
"Without loss of generality, we assume that sgn(a1) = ...",A.2 Proof of Theorem 1,[0],[0]
= sgn(aM0) = +1.,A.2 Proof of Theorem 1,[0],[0]
Now we prove the claim 2.,A.2 Proof of Theorem 1,[0],[0]
"First, we consider the Hessian matrix H(w∗1, ...,w ∗ M0 ).",A.2 Proof of Theorem 1,[0],[0]
Since θ∗ is a local minima with R̂n(θ ∗),A.2 Proof of Theorem 1,[0],[0]
"> 0, then the inequality
F (u1, ...,uM0) = M0∑ j=1 M0∑ k=1",A.2 Proof of Theorem 1,[0],[0]
"u>j ∇2wj ,wk L̂n(θ ∗; p)uk ≥ 0
holds for all vectors u1, ...,uM0 ∈ Rd.",A.2 Proof of Theorem 1,[0],[0]
"Since
∇2wj L̂n(θ∗; p) = a∗j n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′′ ( w∗j >xi ) xix,A.2 Proof of Theorem 1,[0],[0]
>,A.2 Proof of Theorem 1,[0],[0]
"i
+ a∗j 2
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′′p(−yif(xi;θ∗)),A.2 Proof of Theorem 1,[0],[0]
[ σ′ ( w∗j >xi )]2 xix >,A.2 Proof of Theorem 1,[0],[0]
"i ,
and
∇2wj ,wk L̂n(θ ∗; p) =
a∗ja ∗ k
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′′p(−yif(xi;θ∗)),A.2 Proof of Theorem 1,[0],[0]
[ σ′ ( w∗j >xi )],A.2 Proof of Theorem 1,[0],[0]
[ σ′ ( w∗k >xi + b ∗ k )],A.2 Proof of Theorem 1,[0],[0]
"xix > i .
",A.2 Proof of Theorem 1,[0],[0]
"Thus, we have for any u1, ...,uM0 ∈ Rd,
F (u1, ...,uM0) =",A.2 Proof of Theorem 1,[0],[0]
"− 1
n M0∑ j=1",A.2 Proof of Theorem 1,[0],[0]
[ a∗j n∑ i=1,A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))yiσ′′ ( w∗j >xi )( u>j xi )2]
+ 1
n M0∑ j=1 M0∑ k=1",A.2 Proof of Theorem 1,[0],[0]
[ a∗ja ∗ k n∑ i=1,A.2 Proof of Theorem 1,[0],[0]
"`′′p(−yif(xi;θ∗))σ′ ( w∗j >xi ) σ′ ( w∗k >xi + b ∗ k )( u>j xi )( u>k xi )]
= − 1 n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))yi M0∑ j=1,A.2 Proof of Theorem 1,[0],[0]
"[ a∗jσ ′′ ( w∗j >xi )( u>j xi )2] + 1
n n∑ i=1 `′′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
M0∑ j=1 a∗jσ ′,A.2 Proof of Theorem 1,[0],[0]
( w∗j >xi )( u>j xi )2 .,A.2 Proof of Theorem 1,[0],[0]
"Now we find some coefficients α1, ..., αM0 , not all zero, and vectors u1, ...,uM0 , not all zero vector, satisfying
M0∑ j=1 αjσ ′",A.2 Proof of Theorem 1,[0],[0]
"( w∗j >xi ) u>j xi = 0, ∀i ∈",A.2 Proof of Theorem 1,[0],[0]
"[n],
and ∀i : yi = −1 and ∀j ∈",A.2 Proof of Theorem 1,[0],[0]
"[M0], u>j xi = 0.
We note here that if sgn(a1) = ...",A.2 Proof of Theorem 1,[0],[0]
"= sgn(aM0) = −1, then we need to find coefficients α1, ..., αM0 , not all zero, and vectors u1, ...,uM0 , not all zero vector, satisfying
M0∑ j=1 αjσ ′",A.2 Proof of Theorem 1,[0],[0]
"( w∗j >xi ) u>j xi = 0, ∀i ∈",A.2 Proof of Theorem 1,[0],[0]
"[n],
and ∀i : yi = 1 and ∀j ∈",A.2 Proof of Theorem 1,[0],[0]
"[M0], u>j xi = 0.",A.2 Proof of Theorem 1,[0],[0]
"Since θ∗ is a local minima, then by Lemma 1, we have
n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))yiσ′(w∗j>xi)xi = 0d.,A.2 Proof of Theorem 1,[0],[0]
"(3)
Furthermore, by the assumption that K = r−max{r+, r−} > 0, then the set U+\U− is not an empty set.",A.2 Proof of Theorem 1,[0],[0]
"Thus, for ∀v ∈ U+\U− ⊂ E , with probability 1, ∀i : yi = −1, v>xi = 0.",A.2 Proof of Theorem 1,[0],[0]
"In addition, by the analysis presented in the proof of claim 1, we have that with probability 1, v>xi 6= 0 for all i : yi = 1.",A.2 Proof of Theorem 1,[0],[0]
"Since
K = r−max{r+, r−} = |U+ ∪U−| −max{|U+|, |U−|} = |U+\U−|+ |U−| −max{|U+|, |U−|} ≤ |U+\U−|,
then without loss of generality, we assume that {e1, ..., eK} ⊆ U+\U− and U+ = {e1, ..., er+}.",A.2 Proof of Theorem 1,[0],[0]
"Thus, with probability 1, ∀j ∈ [K], ∀i : yi = −1, e>j xi = 0 and ∀i : yi = 1, e>j xi 6= 0.",A.2 Proof of Theorem 1,[0],[0]
"Then by Equation (3), now we consider the following set of linear equations
n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))yiσ′(w∗1>xi) ( e>1 xi ) = 0, ..., n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))yiσ′(w∗M0 >xi + b ∗ M0) ( e>1 xi ) = 0,
... n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))yiσ′(w∗1>xi) ( e>Kxi ) = 0, ..., n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−yif(xi;θ∗))yiσ′(w∗M0 >xi + b ∗ M0) ( e>Kxi ) = 0.
",A.2 Proof of Theorem 1,[0],[0]
These equations can be rewritten in a matrix form σ′(w∗1,A.2 Proof of Theorem 1,[0],[0]
>x1) ( e>1 x1 ) ...,A.2 Proof of Theorem 1,[0],[0]
σ′(w∗1,A.2 Proof of Theorem 1,[0],[0]
>xn) ( e>1 xn ) ... ... ...,A.2 Proof of Theorem 1,[0],[0]
σ′(w∗M0,A.2 Proof of Theorem 1,[0],[0]
>x1 + b ∗ M0 ) ( e>1 x1 ) ...,A.2 Proof of Theorem 1,[0],[0]
σ′(w∗M0,A.2 Proof of Theorem 1,[0],[0]
>xn + b ∗ M0 ) ( e>1 xn ) ... ... ...,A.2 Proof of Theorem 1,[0],[0]
σ′(w∗1,A.2 Proof of Theorem 1,[0],[0]
>x1),A.2 Proof of Theorem 1,[0],[0]
( e>Kx1 ) ... σ′(w∗1,A.2 Proof of Theorem 1,[0],[0]
>xn) ( e>Kxn ) ... ...,A.2 Proof of Theorem 1,[0],[0]
"...
σ′(w∗M0",A.2 Proof of Theorem 1,[0],[0]
>x1 + b ∗ M0 ),A.2 Proof of Theorem 1,[0],[0]
( e>Kx1 ) ...,A.2 Proof of Theorem 1,[0],[0]
σ′(w∗M0,A.2 Proof of Theorem 1,[0],[0]
>xn + b ∗ M0 ),A.2 Proof of Theorem 1,[0],[0]
"( e>Kxn )  (KM0×n)︸ ︷︷ ︸
P
 `′p(−y1f(x1;θ∗))y1",A.2 Proof of Theorem 1,[0],[0]
"`′p(−y2f(x2;θ∗))y2 ... ... ... ...
...",A.2 Proof of Theorem 1,[0],[0]
"`′p(−ynf(x1;θ∗))yn  ︸ ︷︷ ︸
q
= 0n
or Pq = 0n.
",A.2 Proof of Theorem 1,[0],[0]
"Since M ≥ 2n∆r = 2nK , then M0K ≥ MK/2",A.2 Proof of Theorem 1,[0],[0]
≥,A.2 Proof of Theorem 1,[0],[0]
n.,A.2 Proof of Theorem 1,[0],[0]
"Clearly, if rank(P )",A.2 Proof of Theorem 1,[0],[0]
"= n, we should have q = 0n and this indicates that `′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
= 0,A.2 Proof of Theorem 1,[0],[0]
for all i ∈,A.2 Proof of Theorem 1,[0],[0]
[n] or R̂n(θ∗) = 0.,A.2 Proof of Theorem 1,[0],[0]
"Thus, we only need to consider
the case where rank(P )",A.2 Proof of Theorem 1,[0],[0]
< n ≤M0K.,A.2 Proof of Theorem 1,[0],[0]
"This means the raw vectors of the matrix P is linearly dependent and thus there exists coefficients vectors (β11, ..., β1K), ..., (βM01, ..., βM0K), not all zero vectors, such that
K∑ s=1 M0∑ j=1 σ′(w∗j >",A.2 Proof of Theorem 1,[0],[0]
"xi)βjs(e > s xi) = 0, ∀i ∈",A.2 Proof of Theorem 1,[0],[0]
"[n],
or M0∑ j=1 a∗jσ ′(w∗j >xi)
( 1
a∗j K∑ s=1 βjses
)>",A.2 Proof of Theorem 1,[0],[0]
xi,A.2 Proof of Theorem 1,[0],[0]
"= 0, ∀i ∈",A.2 Proof of Theorem 1,[0],[0]
"[n],
by assumption that a∗j 6= 0 for all j = 1, ...,M0.",A.2 Proof of Theorem 1,[0],[0]
"Define uj = 1a∗j ∑K s=1 βjses for j = 1, ...,M0, then we have M0∑ j=1 a∗jσ ′(w∗j",A.2 Proof of Theorem 1,[0],[0]
>,A.2 Proof of Theorem 1,[0],[0]
xi)u,A.2 Proof of Theorem 1,[0],[0]
>,A.2 Proof of Theorem 1,[0],[0]
"j xi = 0, ∀i ∈",A.2 Proof of Theorem 1,[0],[0]
[n].,A.2 Proof of Theorem 1,[0],[0]
"(4)
Furthermore, since uj ∈ Span({e1, ..., eK}) and with probability 1, e>j xi = 0, for ∀i : yi = −1, ∀j ∈",A.2 Proof of Theorem 1,[0],[0]
"[K], then ∀j ∈",A.2 Proof of Theorem 1,[0],[0]
"[M ], ∀i : yi = −1, u>j xi = 0.",A.2 Proof of Theorem 1,[0],[0]
"Thus, by setting uj = 1a∗j ∑K s=1 βjses for j = 1, ...,M0, then we have
F (u1, ...,uM0) =",A.2 Proof of Theorem 1,[0],[0]
"− 1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))yi M0∑ j=1,A.2 Proof of Theorem 1,[0],[0]
"[ a∗jσ ′′ ( w∗j >xi )( u>j xi )2] + 1
n n∑ i=1 `′′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
M0∑ j=1 a∗jσ ′,A.2 Proof of Theorem 1,[0],[0]
"( w∗j >xi )( u>j xi )2 = − 1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))yi M0∑ j=1,A.2 Proof of Theorem 1,[0],[0]
[ a∗jσ ′′ ( w∗j >xi )( u>j xi )2] by Eq.,A.2 Proof of Theorem 1,[0],[0]
(4) =,A.2 Proof of Theorem 1,[0],[0]
"− 1
n ∑",A.2 Proof of Theorem 1,[0],[0]
i:yi=1 `′p(−yif(xi;θ∗)) M0∑ j=1,A.2 Proof of Theorem 1,[0],[0]
[ a∗jσ ′′ ( w∗j >xi )( u>j xi )2] ≥ 0.,A.2 Proof of Theorem 1,[0],[0]
(5),A.2 Proof of Theorem 1,[0],[0]
"In addition, since σ′′(z) > 0",A.2 Proof of Theorem 1,[0],[0]
for all z ∈ R and a∗j > 0 for all j ∈,A.2 Proof of Theorem 1,[0],[0]
"[M0], then we have
`′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
M0∑ j=1,A.2 Proof of Theorem 1,[0],[0]
"[ a∗jσ ′′ ( w∗j >xi )( u>j xi )2] ≥ 0, ∀i : yi = 1
and this leads to F (u1, ...,uM0) ≤ 0.
",A.2 Proof of Theorem 1,[0],[0]
Together with Eq.,A.2 Proof of Theorem 1,[0],[0]
"(5), we have F (u1, ...,uM0) = 0,
and thus
`′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
M0∑ j=1,A.2 Proof of Theorem 1,[0],[0]
"[ a∗jσ ′′ ( w∗j >xi )( u>j xi )2] = 0, ∀i : yi = 1. (6)
Now we split the index {1, ..., n} set into two disjoint subset C0, C1:
C0 = {i ∈",A.2 Proof of Theorem 1,[0],[0]
"[n] : yi = 1, and ∃j ∈",A.2 Proof of Theorem 1,[0],[0]
"[M0],u>j xi 6= 0}, C1 = {i ∈",A.2 Proof of Theorem 1,[0],[0]
[n] : yi = 1 and ∀j ∈,A.2 Proof of Theorem 1,[0],[0]
"[M0],u>j xi = 0}.
",A.2 Proof of Theorem 1,[0],[0]
"Clearly, for all i ∈ C0, by the fact that a∗j > 0 for all j ∈",A.2 Proof of Theorem 1,[0],[0]
[M0] and σ′′(z) > 0,A.2 Proof of Theorem 1,[0],[0]
"for all z ∈ R, we have
M0∑ j=1",A.2 Proof of Theorem 1,[0],[0]
"[ a∗jσ ′′ ( w∗j >xi )( u>j xi )2] > 0,
and by Equation (6), we have `′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
"= 0, ∀i ∈ C0.
",A.2 Proof of Theorem 1,[0],[0]
Now we need to consider the index set C1.,A.2 Proof of Theorem 1,[0],[0]
"First, we show that the following inequality holds with probability 1, |C1| < r+ ≤ max{r+, r−}.",A.2 Proof of Theorem 1,[0],[0]
"Since uj =
1 a∗j
∑K i=1 βjses for j = 1, ...,M0 and coefficient vectors (β11, ..., β1K), ..., (βM01, ..., βM0K) are
not all zero vectors, then the there exists a j0 ∈",A.2 Proof of Theorem 1,[0],[0]
[K] such that the non-zero vector uj0 satisfy u>j0xi = 0 for all i ∈ C1 and uj0 ∈,A.2 Proof of Theorem 1,[0],[0]
"Span({e1, ..., eK}).",A.2 Proof of Theorem 1,[0],[0]
"Furthermore, by assumption U+ = {e1, ..., er+}, thus we have
u>j0xi = K∑ s=1",A.2 Proof of Theorem 1,[0],[0]
(u>j0es)(x >,A.2 Proof of Theorem 1,[0],[0]
i es) = r+∑ s=1 (u>j0es)(x,A.2 Proof of Theorem 1,[0],[0]
>,A.2 Proof of Theorem 1,[0],[0]
"i es) = 0 (7)
holds for all i ∈ C1.",A.2 Proof of Theorem 1,[0],[0]
"If |C1| ≥ r+, then without loss of generality, we assume that {1, ..., r+} ⊆ C1.",A.2 Proof of Theorem 1,[0],[0]
"Thus, with probability 1, the matrix e>1 x1 ...",A.2 Proof of Theorem 1,[0],[0]
"e>r+x1... ... ...
",A.2 Proof of Theorem 1,[0],[0]
e>1 xr+ ...,A.2 Proof of Theorem 1,[0],[0]
e > r+xr+  = x>1...,A.2 Proof of Theorem 1,[0],[0]
"x>r+ (e1 ... er+) has a full rank equal to r+, by the fact that {x1, ..., xr+} ⊂ Span(U+) and ( x1, ..., xr+ ) is a full rank matrix with probability 1.",A.2 Proof of Theorem 1,[0],[0]
"Thus, by Equation (7), we have e>1 x1 ...",A.2 Proof of Theorem 1,[0],[0]
e>r+x1... ... ...,A.2 Proof of Theorem 1,[0],[0]
e>1 xr+ ...,A.2 Proof of Theorem 1,[0],[0]
"e > r+xr+  u>j0e1... u>j0er+
 = 0d and this leads to u>j0es = 0 for all s ∈",A.2 Proof of Theorem 1,[0],[0]
[K].,A.2 Proof of Theorem 1,[0],[0]
This contradicts with the fact that uj0 ∈,A.2 Proof of Theorem 1,[0],[0]
"Span({e1, ..., eK})",A.2 Proof of Theorem 1,[0],[0]
and uj0 is not a zero vector.,A.2 Proof of Theorem 1,[0],[0]
"Therefore, |C1| < r+ ≤ M0.",A.2 Proof of Theorem 1,[0],[0]
"Furthermore, since `′(z) = 0",A.2 Proof of Theorem 1,[0],[0]
"if and only if z ≤ −z0 for some positive z0 > 0, then `′′(z) = 0",A.2 Proof of Theorem 1,[0],[0]
when z ≤ −z0.,A.2 Proof of Theorem 1,[0],[0]
"Now we consider the function F , since ∀i ∈ C0 : `′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
= 0 and `′′p(−yif(xi;θ∗)),A.2 Proof of Theorem 1,[0],[0]
"= 0, then
F (u1, ...,uM0) =",A.2 Proof of Theorem 1,[0],[0]
"− 1
n ∑ i∈C1 `′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
M0∑ j=1,A.2 Proof of Theorem 1,[0],[0]
"[ a∗jσ ′′ ( w∗j >xi )( u>j xi )2] + 1
n ∑ i∈C1 `′′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
M0∑ j=1 a∗jσ ′,A.2 Proof of Theorem 1,[0],[0]
"( w∗j >xi )( u>j xi )2 ≥ 0 holds for all u1, ...,uM0 ∈ Span({e1, ..., eK}).",A.2 Proof of Theorem 1,[0],[0]
"Now we set uj = αje1, j = 1, ...,M0 for some scalar αj .",A.2 Proof of Theorem 1,[0],[0]
"We only need to find α1, ..., αM0 such that
M0∑ j=1 αja ∗",A.2 Proof of Theorem 1,[0],[0]
jσ ′,A.2 Proof of Theorem 1,[0],[0]
"( w∗j >xi ) e>1 xi = 0, ∀i ∈ C1.
",A.2 Proof of Theorem 1,[0],[0]
"Since |C1| < r+ ≤M0, then there exists α∗1, ..., α∗M0 , not all zeros, such that
M0∑ j=1 α∗ja ∗",A.2 Proof of Theorem 1,[0],[0]
jσ ′,A.2 Proof of Theorem 1,[0],[0]
"( w∗j >xi ) e>1 xi = 0, ∀i ∈ C1.
",A.2 Proof of Theorem 1,[0],[0]
"Then by setting uj = α ∗ je1, we have
F (u1, ...,uM0) =",A.2 Proof of Theorem 1,[0],[0]
"− 1
n ∑ i∈C1 `′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
M0∑ j=1 [ |α∗j,A.2 Proof of Theorem 1,[0],[0]
"|2a∗jσ′′ ( w∗j >xi )( e>1 xi )2] ≥ 0. .
",A.2 Proof of Theorem 1,[0],[0]
"Similarly, since |α1|, ..., |αM0",A.2 Proof of Theorem 1,[0],[0]
"| are not all zeros, a∗j > 0",A.2 Proof of Theorem 1,[0],[0]
for all j ∈,A.2 Proof of Theorem 1,[0],[0]
"[M0], σ′′(z) > 0 for all z ∈ R and e>1 xi 6= 0 holds for all i with probability 1, then
`′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
"= 0, ∀i ∈ C1.
",A.2 Proof of Theorem 1,[0],[0]
"Therefore, this indicates that
`′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
"= 0, ∀i : yi = 1.
",A.2 Proof of Theorem 1,[0],[0]
"Furthermore, since θ∗ is a local minima and thus
0 = dL̂n(θ
∗; p)
da0 =
1
n n∑ i=1",A.2 Proof of Theorem 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi) =,A.2 Proof of Theorem 1,[0],[0]
− 1 n ∑,A.2 Proof of Theorem 1,[0],[0]
i:yi=1 `′p(−yif(xi;θ∗)),A.2 Proof of Theorem 1,[0],[0]
+ 1 n ∑,A.2 Proof of Theorem 1,[0],[0]
"i:yi=−1 `′p(−yif(xi;θ∗))
",A.2 Proof of Theorem 1,[0],[0]
"= 1
n",A.2 Proof of Theorem 1,[0],[0]
∑,A.2 Proof of Theorem 1,[0],[0]
"i:yi=−1 `′p(−yif(xi;θ∗)).
",A.2 Proof of Theorem 1,[0],[0]
This means when `′p(−yif(xi;θ∗)),A.2 Proof of Theorem 1,[0],[0]
"= 0 holds for all i : yi = 1, we have `′p(−yif(xi;θ∗))",A.2 Proof of Theorem 1,[0],[0]
= 0,A.2 Proof of Theorem 1,[0],[0]
for all i : yi = −1.,A.2 Proof of Theorem 1,[0],[0]
These two together give us R̂n(θ∗) = 0.,A.2 Proof of Theorem 1,[0],[0]
"Similarly, when sgn(a1) = ... = sgn(aM0) = −1, we have the similar the results.",A.2 Proof of Theorem 1,[0],[0]
"Therefore, θ∗ is a local minima with R̂n(θ ∗) = 0.",A.2 Proof of Theorem 1,[0],[0]
"Proposition 13 Assume that the loss function `p satisfies assumption 1, the distribution PX×Y satisfies assumption 2 and 3, the network architecture satisfies assumption 4 and neurons in the network satisfy assumption 5.",A.3 Proof of Proposition 1,[0],[0]
Assume,A.3 Proof of Proposition 1,[0],[0]
"that samples in the dataset D = {(xi, yi)}ni=1, n",A.3 Proof of Proposition 1,[0],[0]
≥ 1 are independently drawn from the distribution PX×Y .,A.3 Proof of Proposition 1,[0],[0]
Assume that the neuron σ(z) = z2 and the number of neurons M > r.,A.3 Proof of Proposition 1,[0],[0]
"If the real parameters θ∗ = (θ∗S ,θ ∗ D) denote a local minimum of the loss function L̂n(θS ,θD; p) and p ≥ 6, then R̂n(θ∗) = L̂n(θ∗; p) = 0 holds with probability one.
",A.3 Proof of Proposition 1,[0],[0]
Proof: We first recall some notations defined in the paper.,A.3 Proof of Proposition 1,[0],[0]
"The output of the neural network is
f(x;θ) = fS(x;θS) + fD(x;θD),
where fS(x;θS) is the single layer neural network parameterized by θS , i.e.,
fS(x;θS)",A.3 Proof of Proposition 1,[0],[0]
"= a0 + M∑ j=1 ajσ ( w>j x ) ,
and fD(x;θD) is a deep neural network parameterized by θD.",A.3 Proof of Proposition 1,[0],[0]
"The empirical loss function is given by
L̂n(θ; p) = L̂n(θS ,θD; p)",A.3 Proof of Proposition 1,[0],[0]
"= 1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`p(−yif(xi;θ)).
",A.3 Proof of Proposition 1,[0],[0]
"We first assume that the θ∗ = (θ∗S ,θ ∗ D) is a local minima.",A.3 Proof of Proposition 1,[0],[0]
"We next prove the following two claims: Claim 1: If θ∗ = (θ∗S ,θ ∗ D) is a local minima and there exists j ∈",A.3 Proof of Proposition 1,[0],[0]
"[M ] such that a∗j = 0, then R̂n(θ ∗) = 0.",A.3 Proof of Proposition 1,[0],[0]
Claim 2:,A.3 Proof of Proposition 1,[0],[0]
"If θ∗ = (θ∗S ,θ ∗ D) is a local minima and a ∗ j 6= 0 for all j ∈",A.3 Proof of Proposition 1,[0],[0]
"[M ], then R̂n(θ∗) = 0.",A.3 Proof of Proposition 1,[0],[0]
(a) Proof of claim 1.,A.3 Proof of Proposition 1,[0],[0]
"We prove that if θ∗ = (θ∗S ,θ ∗ D) is a local minima and there exists j ∈",A.3 Proof of Proposition 1,[0],[0]
"[M ] such that a∗j = 0, then R̂n(θ ∗) = 0.",A.3 Proof of Proposition 1,[0],[0]
"Without loss of generality, we assume that a∗1 = 0.",A.3 Proof of Proposition 1,[0],[0]
"Since θ∗ = (θ∗S ,θ ∗ D) is a local minima, then there exists ε0 > 0",A.3 Proof of Proposition 1,[0],[0]
"such that for any small perturbations ∆a1, ∆w1 on parameters a ∗ 1 and w ∗ 1, i.e., |∆a1|2 + ‖∆w1‖22 ≤",A.3 Proof of Proposition 1,[0],[0]
"ε20, we have
L̂n(θ̃S ,θ ∗ D) ≥ L̃n(θ∗S ,θ∗D),
where θ̃ = (ã0, ã1, ..., ãM , w̃1, ..., w̃M ), ã1 = a ∗ 1 + ∆a1, w̃1 = w ∗ 1 + ∆w1 and ãj = a ∗ j , w̃j",A.3 Proof of Proposition 1,[0],[0]
= w ∗ j for j 6= 1.,A.3 Proof of Proposition 1,[0],[0]
"Now we consider Taylor expansion of L̃n(θ̃S ,θ∗D) at (θ∗S ,θ∗D).",A.3 Proof of Proposition 1,[0],[0]
"We note here that the Taylor expansion of L̂(θS ,θ ∗ D; p) on θS always exists, since the empirical loss function L̂n has continuous derivatives with respect to fS up to the p-th order and the output of the neural network f(x;θS) is infinitely differentiable with respect to θS due to the fact that neuron activation function σ is real analytic.",A.3 Proof of Proposition 1,[0],[0]
"We first calculate the first order derivatives at the point (θ∗S ,θ ∗ D)
dL̂n(θ ∗)
da1 =
1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ ( w∗1 >xi ) = 0, θ∗ is a critical point,
∇w1L̂n(θ∗) = a∗1 n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ) xi = 0d, θ ∗ is a critical point.
",A.3 Proof of Proposition 1,[0],[0]
"Next, we calculate the second order derivatives at the point (θ∗S ,θ ∗ D),
d2L̂(θ∗)
",A.3 Proof of Proposition 1,[0],[0]
"da21 =
1
n",A.3 Proof of Proposition 1,[0],[0]
N∑ i=1,A.3 Proof of Proposition 1,[0],[0]
"`′′p(−yif(xi;θ∗))σ2 ( w∗1 >xi ) ≥ 0,
d
da1 (∇w1L(θ∗))",A.3 Proof of Proposition 1,[0],[0]
"=
1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ),A.3 Proof of Proposition 1,[0],[0]
"xi
+ a∗1 n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗))σ ( w∗1 >xi ),A.3 Proof of Proposition 1,[0],[0]
σ′,A.3 Proof of Proposition 1,[0],[0]
"( w∗1 >xi ) xi
= 0d,
where the first term equals to the zero vector by the necessary condition for a local minima presented in Lemma 1 and the second term equals to the zero vector by the assumption that a∗1 = 0.",A.3 Proof of Proposition 1,[0],[0]
"Furthermore, by the assumption that a∗1 = 0, we have
∇2w1L̂n(θ∗; p) = a∗1 n ∇w1",A.3 Proof of Proposition 1,[0],[0]
[ n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ) xi ] = 0d×d.
",A.3 Proof of Proposition 1,[0],[0]
"We further calculate the third order derivatives
d
da1
[ ∇2w1L̂n(θ∗; p) ] =",A.3 Proof of Proposition 1,[0],[0]
"d
da1
[ a∗1∇w1 [ 1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ),A.3 Proof of Proposition 1,[0],[0]
"xi
]]
= ∇w1
[ 1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ) xi ],A.3 Proof of Proposition 1,[0],[0]
"+ 0d×d by a ∗ 1 = 0
= 1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) xix >,A.3 Proof of Proposition 1,[0],[0]
"i
+ a∗1 n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ)),A.3 Proof of Proposition 1,[0],[0]
[ σ′ ( w∗1 >xi )]2 xix >,A.3 Proof of Proposition 1,[0],[0]
"i
= 1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) xix,A.3 Proof of Proposition 1,[0],[0]
>,A.3 Proof of Proposition 1,[0],[0]
"i by a ∗ 1 = 0
and
∇3w1L̂n(θ∗; p) = a∗1∇2w1
[ 1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ) xi ] = 0d×d×d.
",A.3 Proof of Proposition 1,[0],[0]
"In fact, it is easy to show that for any 2 ≤ k ≤ p,
∇kw1L̂n(θ∗; p) =",A.3 Proof of Proposition 1,[0],[0]
"a∗1∇k−1w1
[ 1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ) xi ] = 0d× d× ...× d︸,A.3 Proof of Proposition 1,[0],[0]
"︷︷ ︸
k times
.
",A.3 Proof of Proposition 1,[0],[0]
"Let ε > 0, ∆a1 = sgn(a1)ε 9/4 and ∆w1 = εu1 for u1 : ‖u1‖2 = 1.",A.3 Proof of Proposition 1,[0],[0]
"Clearly, when ε → 0, ∆a1 = o(‖∆w1‖2), ∆a1 = o(1) and ‖∆w1‖ = o(1).",A.3 Proof of Proposition 1,[0],[0]
"Then we expand L̂n(θ̃S ,θ∗D) at the point θ∗ up to the
sixth order and thus as ε→ 0,
L̂n(θ̃S ,θ ∗ D) = L̂n(θ ∗ S ,θ ∗ D) +
1
2!n
d2L̂n(θ ∗)
d2a1 (∆a1)
2
+ 1
2n ∆a1∆w
> 1
d
da1
[ D2w1L̂n(θ ∗; p) ]",A.3 Proof of Proposition 1,[0],[0]
"∆w1 + o(|a1|2) + o(|a1|‖w1‖22) + o(‖∆w1‖52)
",A.3 Proof of Proposition 1,[0],[0]
"= L̂n(θ ∗ S ,θ ∗ D) +
1
2!n
d2L̂n(θ ∗)
d2a1 ε9/2 +
1
2n sgn(a1)ε 9/4+2 n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))σ′′ ( w∗1 >xi ) (u>1 xi) 2
+ o(ε9/2) + o(ε9/4+2) + o(ε5)
",A.3 Proof of Proposition 1,[0],[0]
"= L̂n(θ ∗ S ,θ ∗ D) +
1
2n sgn(a1)ε",A.3 Proof of Proposition 1,[0],[0]
17/4 n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) (u>1 xi) 2 + o(ε17/4)
",A.3 Proof of Proposition 1,[0],[0]
"Since ε > 0 and L̂n(θ̃S ,θ ∗ D; p) ≥ L̂n(θ∗; p) holds for any u1 : ‖u1‖2 = 1 and any sgn(a1) ∈ {−1, 1}, then n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) (u>xi) 2 = 0, for any u ∈ Rd.",A.3 Proof of Proposition 1,[0],[0]
"(8) Therefore, n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) xix >,A.3 Proof of Proposition 1,[0],[0]
"i = 0d×d.
",A.3 Proof of Proposition 1,[0],[0]
"By assumption that there exists a set of orthogonal basis E = {e1, ..., ed} in Rd and a subset U+ ⊆ E such that PX|Y (X ∈ Span(U1)|Y = 1) = 1 and by assumption that r = |U+ ∪ U−| > max{r+, r−} = max{|U+|, |U−|}, then the set U+\U− is not an empty set.",A.3 Proof of Proposition 1,[0],[0]
"It is easy to show that for any vector v ∈ U+\U−, PX×Y (v>X = 0|Y = 1) = 0.",A.3 Proof of Proposition 1,[0],[0]
"Otherwise, if p = PX×Y (v>X = 0|Y = 1) > 0, then for random vectors X1, ...,X|U+| independently drawn from the conditional distribution PX|Y=1,
PX|Y=1 |U+|⋃ i=1",A.3 Proof of Proposition 1,[0],[0]
{ v>Xi = 0 } ∣∣∣∣∣Y,A.3 Proof of Proposition 1,[0],[0]
= 1  = |U+|∏ i=1,A.3 Proof of Proposition 1,[0],[0]
"PX|Y=1 ( v>Xi = 0|Y = 1 ) = p|U+| > 0.
",A.3 Proof of Proposition 1,[0],[0]
"Furthermore, since X1, ...,X|U+| ∈ Span(U+), v>Xi = 0, i = 1, ..., |U+| and v ∈ U+, then the rank of the matrix ( X1, ...,X|U+| ) is at most |U+| − 1 and this indicates that the matrix is not a full rank matrix with probability p|U+| > 0.",A.3 Proof of Proposition 1,[0],[0]
This leads to the contradiction with the Assumption 2.,A.3 Proof of Proposition 1,[0],[0]
"Thus, with probability 1, v>xi 6= 0 for all i : yi = 1 and v>xi = 0 for all i : yi = −1.",A.3 Proof of Proposition 1,[0],[0]
"Therefore, by setting u = v in Equation (8), we have
0 =",A.3 Proof of Proposition 1,[0],[0]
"− ∑ i:yi=1 `′p(−yif(xi;θ∗))σ′′(w∗1>xi)(v>xi)2 ≤ 0,
where the equality holds if and only if ∀i : yi = 1, `′p(−yif(xi;θ∗))",A.3 Proof of Proposition 1,[0],[0]
= 0,A.3 Proof of Proposition 1,[0],[0]
"and this further indicates that ∀i : yi = 1, yif(xi;θ∗) ≥ z0 > 0.",A.3 Proof of Proposition 1,[0],[0]
"Furthermore, since θ∗ is a critical point and thus
0 = dL̂n(θ
∗; p)
da0 =
1
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi) =,A.3 Proof of Proposition 1,[0],[0]
− 1 n ∑,A.3 Proof of Proposition 1,[0],[0]
i:yi=1 `′p(−yif(xi;θ∗)),A.3 Proof of Proposition 1,[0],[0]
+ 1 n ∑,A.3 Proof of Proposition 1,[0],[0]
"i:yi=−1 `′p(−yif(xi;θ∗))
",A.3 Proof of Proposition 1,[0],[0]
"= 1
n",A.3 Proof of Proposition 1,[0],[0]
∑,A.3 Proof of Proposition 1,[0],[0]
"i:yi=−1 `′p(−yif(xi;θ∗)).
",A.3 Proof of Proposition 1,[0],[0]
"Therefore, ∀i : yi = −1, yif(xi;θ∗)",A.3 Proof of Proposition 1,[0],[0]
≥ z0 > 0,A.3 Proof of Proposition 1,[0],[0]
"and this indicates that R̂n(θ∗) = 0.
(b) Proof of Claim 2: To prove the claim 2, we first prove that if M > r, then there exists coefficients α1, ..., αM , not all zero, such that
(α1w ∗ 1 + ...",A.3 Proof of Proposition 1,[0],[0]
+,A.3 Proof of Proposition 1,[0],[0]
αMw ∗ M ),A.3 Proof of Proposition 1,[0],[0]
> xi,A.3 Proof of Proposition 1,[0],[0]
"= 0, for all i ∈",A.3 Proof of Proposition 1,[0],[0]
"[n].
Since we assume that U+ ⊆ E and U− ⊆ E such that PX|Y (X ∈ Span(U+)|Y = 1) = 1 and PX|Y (X ∈ Span(U−)|Y = −1)",A.3 Proof of Proposition 1,[0],[0]
"= 1, then without loss generality, we assume that xis locate in the linear span of {e1, ..., er} ⊆ {e1, ..., ed} (note that r = |U+ ∪ U−|).",A.3 Proof of Proposition 1,[0],[0]
"Clearly, for any w∗1, ...,w∗M , if M > r, then there exists coefficients α1, ..., αM , not all zero, such that
α1w ∗ 1 + ...",A.3 Proof of Proposition 1,[0],[0]
+,A.3 Proof of Proposition 1,[0],[0]
"αMw ∗ M ∈ Span({er+1, ..., ed}), if r < d, α1w ∗ 1 + ...+ αMw ∗ M = 0d, if r = d.
Therefore, if M > r, then there exists coefficients α1, ..., αM , not all zero, such that
(α1w ∗ 1 + ...+ αMw ∗ M )",A.3 Proof of Proposition 1,[0],[0]
">xi = 0, for all i ∈",A.3 Proof of Proposition 1,[0],[0]
"[n].
Now we prove the claim 2.",A.3 Proof of Proposition 1,[0],[0]
"First, we consider the Hessian matrix H(w∗1, ...,w ∗ M ).",A.3 Proof of Proposition 1,[0],[0]
"Since θ ∗ is a local minima, then
F (u1, ...,uM )",A.3 Proof of Proposition 1,[0],[0]
= M∑ j=1 M∑ k=1,A.3 Proof of Proposition 1,[0],[0]
"u>j ∇2wj ,wk L̂n(θ ∗; p)uk ≥ 0
holds for any vectors u1, ...,uM ∈ Rd.",A.3 Proof of Proposition 1,[0],[0]
"Since σ′′(z) = 2 and σ′(z) = 2z for all z ∈ R, then
∇2wj L̂n(θ∗; p) = a∗j n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′′ ( w∗j >xi ) xix,A.3 Proof of Proposition 1,[0],[0]
>,A.3 Proof of Proposition 1,[0],[0]
"i
+ a∗j 2
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗)),A.3 Proof of Proposition 1,[0],[0]
[ σ′ ( w∗j >xi )]2 xix >,A.3 Proof of Proposition 1,[0],[0]
"i
= − 2a∗j n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))yixix>i + 4a∗j 2 n n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗)) ( w∗j >xi )2 xix >,A.3 Proof of Proposition 1,[0],[0]
"i ,
and
∇2wj ,wk L̂n(θ ∗; p) =
a∗ja ∗ k
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗)),A.3 Proof of Proposition 1,[0],[0]
[ σ′ ( w∗j >xi )],A.3 Proof of Proposition 1,[0],[0]
[ σ′ ( w∗k >xi )],A.3 Proof of Proposition 1,[0],[0]
xix >,A.3 Proof of Proposition 1,[0],[0]
"i
= 4a∗ja ∗ k
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗)) (,A.3 Proof of Proposition 1,[0],[0]
w∗j >xi )( w∗k >xi ) xix,A.3 Proof of Proposition 1,[0],[0]
"> i .
",A.3 Proof of Proposition 1,[0],[0]
"Thus, we have
F (u1, ...,uM )",A.3 Proof of Proposition 1,[0],[0]
= −2,A.3 Proof of Proposition 1,[0],[0]
M∑ j=1,A.3 Proof of Proposition 1,[0],[0]
[ a∗j n n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ∗))yi ( u>j xi )2]
+ 4 M∑ j=1 M∑ k=1
",A.3 Proof of Proposition 1,[0],[0]
"[ a∗ja ∗ k
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗)) (,A.3 Proof of Proposition 1,[0],[0]
w∗j >xi )( w∗k >xi ),A.3 Proof of Proposition 1,[0],[0]
"( u>j xi )( u>k xi
)]
= − 2 n M∑ j=1",A.3 Proof of Proposition 1,[0],[0]
[ a∗j n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ∗))yi ( u>j xi )2]
+ 4
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗))  ,A.3 Proof of Proposition 1,[0],[0]
M∑ j=1 a∗j ( w∗j >xi ),A.3 Proof of Proposition 1,[0],[0]
"( u>j xi )2 .
",A.3 Proof of Proposition 1,[0],[0]
"Since there exists coefficients α1, ..., αM , not all zero, such that (α1w ∗ 1 + ...+ αMw ∗ M )",A.3 Proof of Proposition 1,[0],[0]
">xi = 0, for all i ∈",A.3 Proof of Proposition 1,[0],[0]
"[n], and a∗j 6= 0",A.3 Proof of Proposition 1,[0],[0]
for all j ∈,A.3 Proof of Proposition 1,[0],[0]
[M ] then by setting uj = αju/a∗j for all j ∈,A.3 Proof of Proposition 1,[0],[0]
"[M ], we have that the inequality
F (u1, ...,uM ) =",A.3 Proof of Proposition 1,[0],[0]
"− 2
n M∑ j=1",A.3 Proof of Proposition 1,[0],[0]
[ a∗j n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))yi ( αj/a ∗ j )2,A.3 Proof of Proposition 1,[0],[0]
"( u>xi )2]
+ 4
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`′′p(−yif(xi;θ∗))  M∑ j=1 αj ( w∗j >xi )( u>xi )2 = − 2
n M∑ j=1",A.3 Proof of Proposition 1,[0],[0]
[ a∗j n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))yi ( αj/a ∗ j )2,A.3 Proof of Proposition 1,[0],[0]
"( u>xi )2]
+ 4
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗))   M∑ j=1 αjw ∗,A.3 Proof of Proposition 1,[0],[0]
"j > xi  2 ( u>xi )2 = − 2
n M∑ j=1 ( α2j/a ∗ j ) · n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ∗))yi ( u>xi )2 ≥ 0
holds for any u ∈ Rd.",A.3 Proof of Proposition 1,[0],[0]
"Next we consider the following two cases: (1) ∑M j=1 ( α2j/a ∗ j ) 6= 0; (2) ∑Mj=1 (α2j/a∗j) = 0.
",A.3 Proof of Proposition 1,[0],[0]
"Case 1: If ∑M
j=1 ( α2j/a ∗ j ) 6= 0, then without loss of generality, we assume that ∑Mj=1 (α2j/a∗j) < 0.
",A.3 Proof of Proposition 1,[0],[0]
This indicates that n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ∗))yi ( u>xi )2 ≥ 0, for all u ∈ Rd.
",A.3 Proof of Proposition 1,[0],[0]
"By the assumption that there exists two vectors er, es such that ∀i : yi = 1, e>r xi = 0, e>s xi 6= 0 hold with probability 1 and ∀i : yi = −1, e>s xi = 0, e>r xi 6= 0 hold with probability 1, then by setting u =",A.3 Proof of Proposition 1,[0],[0]
"er, we have that
0 ≤ n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))yi ( e>r xi )2 = − ∑ i:yi=−1 `′p(−yif(xi;θ∗)),A.3 Proof of Proposition 1,[0],[0]
"( e>r xi )2 ≤ 0,
where the equality holds if and only if `′p(−yif(xi;θ∗))",A.3 Proof of Proposition 1,[0],[0]
= 0 or yif(xi;θ∗),A.3 Proof of Proposition 1,[0],[0]
≥ z0 > 0 holds for all i : yi = −1.,A.3 Proof of Proposition 1,[0],[0]
"Furthermore, since θ∗ is a local minima and thus
0 = dL̂n(θ
∗; p)
da0 = n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))(−yi) =,A.3 Proof of Proposition 1,[0],[0]
− ∑ i:yi=1 `′p(−yif(xi;θ∗)),A.3 Proof of Proposition 1,[0],[0]
+ ∑,A.3 Proof of Proposition 1,[0],[0]
"i:yi=−1 `′p(−yif(xi;θ∗))
",A.3 Proof of Proposition 1,[0],[0]
=,A.3 Proof of Proposition 1,[0],[0]
"− ∑ i:yi=1 `′p(−yif(xi;θ∗)).
",A.3 Proof of Proposition 1,[0],[0]
This means when `′p(−yif(xi;θ∗)),A.3 Proof of Proposition 1,[0],[0]
"= 0 holds for all i : yi = −1, we have `′p(−yif(xi;θ∗))",A.3 Proof of Proposition 1,[0],[0]
= 0,A.3 Proof of Proposition 1,[0],[0]
for all i : yi = 1.,A.3 Proof of Proposition 1,[0],[0]
These two together give us R̂n(θ ∗) = 0.,A.3 Proof of Proposition 1,[0],[0]
"When ∑M
j=1 ( α2j/a ∗ j ) > 0, by setting u = es
and following the similar analysis presented above, we can obtain the same result.",A.3 Proof of Proposition 1,[0],[0]
"Therefore, when∑M j=1 ( α2j/a ∗ j ) 6= 0, we have R̂n(θ∗) = 0.
",A.3 Proof of Proposition 1,[0],[0]
"Case 2: If ∑M
j=1 ( α2j/a ∗ j ) = 0, then by setting uj = (αj/a ∗ j +vsgn(αj))u for some scalar v and vector
u ∈ Rd, we have
F (v,u) =",A.3 Proof of Proposition 1,[0],[0]
− 2 n M∑ j=1 [ a∗j n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,A.3 Proof of Proposition 1,[0],[0]
">xi )2]
+ 4
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗))  ,A.3 Proof of Proposition 1,[0],[0]
M∑ j=1 a∗j ( w∗j >xi )( (αj/a ∗ j + vsgn(αj))u,A.3 Proof of Proposition 1,[0],[0]
">xi )2 = − 2
n M∑ j=1",A.3 Proof of Proposition 1,[0],[0]
[ a∗j n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,A.3 Proof of Proposition 1,[0],[0]
">xi )2]
+ 4
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗))   M∑ j=1 (αj + vsgn(αj)a ∗ j )w ∗,A.3 Proof of Proposition 1,[0],[0]
j > xi (u>xi)2  =,A.3 Proof of Proposition 1,[0],[0]
"− 2
n M∑ j=1 [ a∗j n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,A.3 Proof of Proposition 1,[0],[0]
">xi )2]
+ 4v2 n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗))   M∑,A.3 Proof of Proposition 1,[0],[0]
j=1 sgn(αj)a ∗ jw ∗,A.3 Proof of Proposition 1,[0],[0]
j > xi  2,A.3 Proof of Proposition 1,[0],[0]
"( u>xi )2 , − 2
n M∑ j=1",A.3 Proof of Proposition 1,[0],[0]
[ a∗j n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,A.3 Proof of Proposition 1,[0],[0]
">xi )2] + v2R(u),
where we define
R(u) = 4
n n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′′p(−yif(xi;θ∗))   M∑,A.3 Proof of Proposition 1,[0],[0]
j=1 sgn(αj)a ∗ jw ∗,A.3 Proof of Proposition 1,[0],[0]
j > xi  2,A.3 Proof of Proposition 1,[0],[0]
( u>xi )2 .,A.3 Proof of Proposition 1,[0],[0]
"In addition, we have
M∑ j=1",A.3 Proof of Proposition 1,[0],[0]
[ a∗j n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,A.3 Proof of Proposition 1,[0],[0]
">xi )2]
= n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))yi(u>xi)2 ·  M∑ j=1 (α2j/a ∗ j + 2vsgn(αj)αj + v 2a∗j )  ,A.3 Proof of Proposition 1,[0],[0]
"=
n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))yi(u>xi)2 ·  M∑ j=1 (2vsgn(αj)αj + v 2a∗j )  ,A.3 Proof of Proposition 1,[0],[0]
"= 2v
 M∑ j=1 |αj |  n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))yi(u>xi)2 + v2  M∑ j=1 a∗j  n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2.
",A.3 Proof of Proposition 1,[0],[0]
"Therefore, we can rewrite F (v,u) as
F (v,u) = −4v n M∑ j=1 |αj",A.3 Proof of Proposition 1,[0],[0]
| n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))yi(u>xi)2 − 2v2 n M∑ j=1 a∗j · n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2 + v2R(u)
, −4v n M∑ j=1 |αj | n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2 + v2R̂(u)
Since F (v,u) ≥ 0 holds for any scalar v and vector u ∈ Rd, then we should have
M∑ j=1 |αj",A.3 Proof of Proposition 1,[0],[0]
| n∑ i=1,A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2 = 0, for any u ∈ Rd.
",A.3 Proof of Proposition 1,[0],[0]
"Since the coefficient α1, ..., αM are not all zero, then for any u ∈ Rd, we have n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2 = 0.
",A.3 Proof of Proposition 1,[0],[0]
"Since there exists two vectors er, es: ∀i : yi = 1, e>r xi = 0 and e>s xi 6= 0 hold with probability 1 and ∀i : yi = −1, e>s xi = 0 and e>r xi 6= 0 hold with probability 1, then by setting u =",A.3 Proof of Proposition 1,[0],[0]
"er, we have
0 = n∑ i=1",A.3 Proof of Proposition 1,[0],[0]
`′p(−yif(xi;θ))yi(e>r xi)2 =,A.3 Proof of Proposition 1,[0],[0]
"− ∑ i:yi=−1 `′p(−yif(xi;θ))(e>r xi)2 ≤ 0,
where the equality holds if and only if `′p(−yif(xi;θ∗))",A.3 Proof of Proposition 1,[0],[0]
= 0 or yif(xi;θ∗),A.3 Proof of Proposition 1,[0],[0]
≥ z0 > 0 holds for all i : yi = −1.,A.3 Proof of Proposition 1,[0],[0]
"Similar to the case 1, we have that `′p(−yif(xi;θ∗))",A.3 Proof of Proposition 1,[0],[0]
= 0 holds for all i and this leads to R̂n(θ ∗) = 0.,A.3 Proof of Proposition 1,[0],[0]
Theorem 4 Assume that the loss function `p satisfies assumption 1 and the network architecture satisfies assumption 4.,A.4 Proof of Theorem 2,[0],[0]
Assume,A.4 Proof of Theorem 2,[0],[0]
"that samples in the dataset D = {(xi, yi)}ni=1, n",A.4 Proof of Theorem 2,[0],[0]
≥ 1 are independently drawn from a distribution satisfying assumption 6.,A.4 Proof of Theorem 2,[0],[0]
"Assume that the single layer network fS has M ≥ 1 neurons and neurons σ in the network fS are twice differentiable and satisfy σ
′(z)",A.4 Proof of Theorem 2,[0],[0]
> 0,A.4 Proof of Theorem 2,[0],[0]
for all z ∈ R.,A.4 Proof of Theorem 2,[0],[0]
"If a set of real parameters θ∗ = (θ∗S ,θ ∗ D) denotes a local minimum of the loss function L̂n(θS ,θD; p), p ≥ 3, then R̂n(θ∗S ,θ∗D) = 0 holds with probability one.
",A.4 Proof of Theorem 2,[0],[0]
Proof: We first recall some notations defined in the paper.,A.4 Proof of Theorem 2,[0],[0]
"The output of the neural network is
f(x;θ) = fS(x;θS) + fD(x;θD),
where fS(x;θS) is the single layer neural network parameterized by θS , i.e.,
fS(x;θS)",A.4 Proof of Theorem 2,[0],[0]
"= a0 + M∑ j=1 ajσ ( w>j x ) ,
and fD(x;θD) is a deep neural network parameterized by θD.",A.4 Proof of Theorem 2,[0],[0]
"The empirical loss function is given by
L̂n(θ; p) = L̂n(θS ,θD; p)",A.4 Proof of Theorem 2,[0],[0]
"= 1
n n∑ i=1",A.4 Proof of Theorem 2,[0],[0]
"`p(−yif(xi;θ)).
",A.4 Proof of Theorem 2,[0],[0]
"By the assumption that θ∗ = (θ∗S ,θ ∗ D) is a local minima and by the necessary condition presented in Lemma 1, we have
n∑ i=1",A.4 Proof of Theorem 2,[0],[0]
"`′p(−yif(xi;θ∗))yiσ′(w∗j>xi)xi = 0d.
",A.4 Proof of Theorem 2,[0],[0]
Thus,A.4 Proof of Theorem 2,[0],[0]
", for any w ∈ Rd and any j ∈",A.4 Proof of Theorem 2,[0],[0]
"[M ], we have n∑ i=1",A.4 Proof of Theorem 2,[0],[0]
"`′p(−yif(xi;θ∗))σ′(w∗j>xi)yi(w>xi) = 0.
",A.4 Proof of Theorem 2,[0],[0]
"Furthermore, by assumption `′p(z) ≥ 0 and the equality holds if and only if z ≤ −z0.",A.4 Proof of Theorem 2,[0],[0]
"Thus, by assumption that σ′(z) > 0",A.4 Proof of Theorem 2,[0],[0]
"for all z ∈ R and assumption that there exists a vector PX×Y (Yw>X > 0) = 1, then there exists and positive constant c > 0",A.4 Proof of Theorem 2,[0],[0]
"such that
yi(w >xi)",A.4 Proof of Theorem 2,[0],[0]
> c,A.4 Proof of Theorem 2,[0],[0]
"> 0, ∀i ∈",A.4 Proof of Theorem 2,[0],[0]
"[n].
Thus, we have
0 = n∑ i=1",A.4 Proof of Theorem 2,[0],[0]
`′p(−yif(xi;θ∗))σ′(w∗j>xi)yi(w>xi) ≥ c n∑ i=1,A.4 Proof of Theorem 2,[0],[0]
"`′p(−yif(xi;θ∗))σ′(w∗j>xi) ≥ 0,
where the equality holds if and only if `′p(−yif(xi;θ∗))",A.4 Proof of Theorem 2,[0],[0]
= 0,A.4 Proof of Theorem 2,[0],[0]
for all i ∈,A.4 Proof of Theorem 2,[0],[0]
[n].,A.4 Proof of Theorem 2,[0],[0]
"Equivalently, if θ∗ is a local minima, then yif(xi;θ ∗)",A.4 Proof of Theorem 2,[0],[0]
≥ z0 > 0,A.4 Proof of Theorem 2,[0],[0]
for all i ∈,A.4 Proof of Theorem 2,[0],[0]
[n].,A.4 Proof of Theorem 2,[0],[0]
This indicates that Ln(θ∗; p) = R̂n(θ∗) = 0.,A.4 Proof of Theorem 2,[0],[0]
Proposition 14 Assume that assumption 1 and 4 are satisfed.,B.1 Proof of Proposition 2,[0],[0]
Assume that neurons in the network fS satisfy that σ(z) = 0 for all z ≤ 0 and σ(z) is piece-wise continuous on R.,B.1 Proof of Proposition 2,[0],[0]
"Then there exists a feedforward network fD and a distribution satisfying assumptions in Theorem 1 or 2 such that with probability one, the empirical loss L̂n(θ; p), p ≥ 2 has a local minima θ∗ = (θ∗S ,θ∗D) satisfying R̂n(θ ∗)",B.1 Proof of Proposition 2,[0],[0]
"≥ min{n+,n−}n , where n+",B.1 Proof of Proposition 2,[0],[0]
"and n− are the number of positive and negative samples, respectively.
",B.1 Proof of Proposition 2,[0],[0]
Proof: We choose the network architecture fD(x;θD) ≡ 0 for all x ∈ Rd.,B.1 Proof of Proposition 2,[0],[0]
"Then the output of the network is
f(x;θ) = fS(x;θS)",B.1 Proof of Proposition 2,[0],[0]
"= a0 + M∑ j=1 ajσ ( w>j xi ) .
",B.1 Proof of Proposition 2,[0],[0]
"Now we prove the following claim showing that if the dataset contains both positive and negative samples, then the empirical loss has a local minimum with a non-zero training error.
",B.1 Proof of Proposition 2,[0],[0]
Claim 1,B.1 Proof of Proposition 2,[0],[0]
"Under the conditions in proposition 2, if the dataset contains both positive and negative samples and samples in the dataset are drawn in the space Rd−1×{1}×{1,−1}, the empirical loss has a local minimum with a non-zero training error.",B.1 Proof of Proposition 2,[0],[0]
"Furthermore, the training error is no smaller than min{n+,n−}
n .
",B.1 Proof of Proposition 2,[0],[0]
Proof: We construct the local minimum as follows.,B.1 Proof of Proposition 2,[0],[0]
Now we construct a local minimum θ∗ = (θ∗S).,B.1 Proof of Proposition 2,[0],[0]
"The key idea of constructing the local minimum having a training error no smaller than min{n+,n−}n is appropriately choosing wj such that all neurons in the last layer keep inactive on all samples in the dataset.",B.1 Proof of Proposition 2,[0],[0]
This is possible since the number of samples is bounded.,B.1 Proof of Proposition 2,[0],[0]
"Next, for any data set D = {(xi; yi)}ni=1, we define
K = max i∈[n] ‖xi‖2.
",B.1 Proof of Proposition 2,[0],[0]
"Since all samples in the dataset xi ∈ Rd−1 × {1}, then by choosing w∗j = ( w (1) j ∗ , ..., w (d−1) j ∗ , w (d) j ∗) such that
d−1∑",B.1 Proof of Proposition 2,[0],[0]
"k=1 ( w (1) j ∗)2 = 1,
and w (d) j
∗ = −K − 1.",B.1 Proof of Proposition 2,[0],[0]
"Since for all samples in the dataset
w>j xi = d−1∑",B.1 Proof of Proposition 2,[0],[0]
k=1,B.1 Proof of Proposition 2,[0],[0]
w,B.1 Proof of Proposition 2,[0],[0]
(k) j ∗ x (k) i + w (d) j ∗ ≤ K −K,B.1 Proof of Proposition 2,[0],[0]
"− 1 = −1,
then σ(w>j xi) = 0, ∀i ∈",B.1 Proof of Proposition 2,[0],[0]
"[n].
Therefore, the neural network becomes
f(xi;θ ∗)",B.1 Proof of Proposition 2,[0],[0]
"= a∗0, ∀i ∈",B.1 Proof of Proposition 2,[0],[0]
"[n].
Finally, we set a∗0 to the global minimizer of the following convex optimization problem:
min a∈R
1
n n∑ i=1",B.1 Proof of Proposition 2,[0],[0]
"`(−yia).
",B.1 Proof of Proposition 2,[0],[0]
"This indicates that for any a ∈ R,
1
n n∑ i=1",B.1 Proof of Proposition 2,[0],[0]
`(−yia) ≥ 1 n n∑ i=1,B.1 Proof of Proposition 2,[0],[0]
"`(−yia∗0).
",B.1 Proof of Proposition 2,[0],[0]
Now we show that θ∗ is local minimum of the empirical loss function.,B.1 Proof of Proposition 2,[0],[0]
"Now we slightly perturb the parameters a0, ..., aM ,w1, ...,wM by ∆a0, ...,∆aM ,∆w1, ...,∆wM .",B.1 Proof of Proposition 2,[0],[0]
"Define
θ̃ = (a∗0 + ∆a0, ..., a ∗ M + ∆aM ,w ∗ 1 + ∆w1, ...,w ∗ M + ∆wM ).
",B.1 Proof of Proposition 2,[0],[0]
Then,B.1 Proof of Proposition 2,[0],[0]
", if ‖θ − θ̃‖2 ≤ ε and ε is positive and sufficiently small, then for ∀j ∈",B.1 Proof of Proposition 2,[0],[0]
[M ] and ∀ ∈,B.1 Proof of Proposition 2,[0],[0]
"[n], we have
w∗jxi + ∆w",B.1 Proof of Proposition 2,[0],[0]
> j xi ≤ −1,B.1 Proof of Proposition 2,[0],[0]
+ ‖∆wj‖2,B.1 Proof of Proposition 2,[0],[0]
‖xi‖2 ≤ −1,B.1 Proof of Proposition 2,[0],[0]
"+Kε < 0.
",B.1 Proof of Proposition 2,[0],[0]
"This means that if ε is positive and sufficiently small, then
f(xi; θ̃) = a ∗ 0 + ∆a0.
",B.1 Proof of Proposition 2,[0],[0]
"In addition, for all ∆a0 ∈ R,
1
n n∑ i=1",B.1 Proof of Proposition 2,[0],[0]
`(−yia∗ + ∆a0) ≥ 1 n n∑ i=1,B.1 Proof of Proposition 2,[0],[0]
"`(−yia∗0),
therefore for θ̃ : ‖θ̃ − θ∗‖2 ≤ δ(ε) and any a0 ∈ R
L̂n(θ̃) = 1
n n∑ i=1",B.1 Proof of Proposition 2,[0],[0]
`(−yif(xi; θ̃)),B.1 Proof of Proposition 2,[0],[0]
= 1 n n∑ i=1,B.1 Proof of Proposition 2,[0],[0]
"`(−yi(a∗0 + ∆a0))
",B.1 Proof of Proposition 2,[0],[0]
≥ 1 n n∑ i=1,B.1 Proof of Proposition 2,[0],[0]
`(−yia∗0) ≥ 1 n n∑ i=1,B.1 Proof of Proposition 2,[0],[0]
"`(−yif(xi;θ∗)) = L̂n(θ∗).
",B.1 Proof of Proposition 2,[0],[0]
This means that θ∗ is a local minimum of the empirical loss and f(xi;θ ∗),B.1 Proof of Proposition 2,[0],[0]
= a∗0 for all i ∈,B.1 Proof of Proposition 2,[0],[0]
[n].,B.1 Proof of Proposition 2,[0],[0]
"This further indicates that
R̂n(θ ∗) ≥ min{n−, n+}
n .
",B.1 Proof of Proposition 2,[0],[0]
"Now we only need to construct the data distribution satisfying assumptions in Theorem 1 and Theorem 2, respectively, such that with probability at least 1 − e−Ω(n), the dataset drawn from this distribution satisfies the assumption in claim 1.
",B.1 Proof of Proposition 2,[0],[0]
Distribution for Theorem 1:,B.1 Proof of Proposition 2,[0],[0]
"Now we define a distribution as follows, PX|Y=1 is a uniform distribution on the region [−2,−1] ∪ [1, 2] × {0} × {1} × {0}d−3 and PX|Y=−1 is a uniform distribution on the region {0} ×",B.1 Proof of Proposition 2,[0],[0]
"[−2,−1] ∪ [1, 2] × {1} × {0}d−3.",B.1 Proof of Proposition 2,[0],[0]
"In addition, P(Y = 1) = P(Y = −1) = 0.5.",B.1 Proof of Proposition 2,[0],[0]
"It is easy to check that r = 3 > max{r+, r−} = 2 and for any two samples independently drawn from the distribution PX|Y=1 or PX|Y=−1, these two samples are linearly independent.",B.1 Proof of Proposition 2,[0],[0]
This means that this data distribution satisfies the conditions in Theorem 1.,B.1 Proof of Proposition 2,[0],[0]
"In addition, if samples in the dataset are independently drawn from this distribution, then with probability 1− 1
2n−1 , the dataset contains both positive and negative samples.
",B.1 Proof of Proposition 2,[0],[0]
Distribution for Theorem 2:,B.1 Proof of Proposition 2,[0],[0]
"Now we define a distribution as follows, PX|Y=1 is a uniform distribution on the region [−2,−1] × {0} × {1} × {0}d−3 and PX|Y=−1 is a uniform distribution on the region {0}× [−2,−1]×{1}×{0}d−3.",B.1 Proof of Proposition 2,[0],[0]
It is easy to check that This means that this distribution satisfies the conditions in Theorem 2.,B.1 Proof of Proposition 2,[0],[0]
"In addition, if samples in the dataset are independently drawn from this distribution, then with probability 1− 1
2n−1 , the dataset contains both positive and negative samples.",B.1 Proof of Proposition 2,[0],[0]
Proposition 15 Assume that assumption 1 and 4 are satisfed.,B.2 Proof of Proposition 3,[0],[0]
Assume that neurons in the network fS satisfy that σ(z) =,B.2 Proof of Proposition 3,[0],[0]
"z for all z ≥ 0 and σ(z) is piece-wise continuous on R. Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 such that, with probability at least 1− e−Ω(n), the empirical loss L̂n(θ; p), p ≥ 2 has a local minima θ∗ = (θ∗S ,θ∗D) with non-zero training error.
",B.2 Proof of Proposition 3,[0],[0]
Proof: We choose the network architecture fD(x;θD) ≡ 0 for all x ∈ Rd.,B.2 Proof of Proposition 3,[0],[0]
"Then the output of the network is
f(x;θ) = fS(x;θS)",B.2 Proof of Proposition 3,[0],[0]
"= a0 + M∑ j=1 ajσ ( w>j xi ) .
",B.2 Proof of Proposition 3,[0],[0]
"Now we prove the following claim showing that if the dataset contains both positive and negative samples, then the empirical loss has a local minimum with a non-zero training error.
",B.2 Proof of Proposition 3,[0],[0]
Claim 2,B.2 Proof of Proposition 3,[0],[0]
"Under the conditions in proposition 2, if the samples in the dataset are not linearly separable and samples (xi, yi) are drawn in the space Rd−1×{1}×{1,−1}, the empirical loss has a local minimum with a non-zero training error.
",B.2 Proof of Proposition 3,[0],[0]
Proof: We construct the local minimum as follows.,B.2 Proof of Proposition 3,[0],[0]
Now we construct a local minimum θ∗ = (θ∗S).,B.2 Proof of Proposition 3,[0],[0]
"The key idea of constructing the local minimum having a training error no smaller than min{n+,n−}n is appropriately choosing wj such that all neurons in the last layer keep inactive on all samples in the dataset.",B.2 Proof of Proposition 3,[0],[0]
This is possible since the number of samples is bounded.,B.2 Proof of Proposition 3,[0],[0]
"First, let w∗ be a global minimizer of the following convex optimization problem:
min w∈Rd n∑ i=1",B.2 Proof of Proposition 3,[0],[0]
`p(−yi(w>xi)).,B.2 Proof of Proposition 3,[0],[0]
"(9)
Next, for any data set D = {(xi; yi)}ni=1, we define
K = max i∈[n]",B.2 Proof of Proposition 3,[0],[0]
"|w∗>xi| and K1 = max i∈[n] ‖xi‖2.
",B.2 Proof of Proposition 3,[0],[0]
"Since all samples in the dataset xi ∈ Rd−1 × {1}, then by choosing w∗j = ( w (1) j ∗ , ..., w (d−1) j ∗ , w (d) j ∗) such that
w (1) j
∗ = w(1) ∗ , ..., w
(d−1) j
∗ = w(d−1) ∗ , w
(d) j
∗ = w(d) ∗ +K + 1.
",B.2 Proof of Proposition 3,[0],[0]
"Since for all samples in the dataset
w∗j >xi = w ∗>xi +K + 1 ≥ −K +K + 1 = 1, then
σ(w>j xi) = w >xi, ∀i ∈",B.2 Proof of Proposition 3,[0],[0]
"[n].
In addition, let a∗j = 1 M and a ∗ 0 = 0.",B.2 Proof of Proposition 3,[0],[0]
"Therefore, the neural network becomes
f(xi;θ ∗)",B.2 Proof of Proposition 3,[0],[0]
=,B.2 Proof of Proposition 3,[0],[0]
"w>xi, ∀i ∈",B.2 Proof of Proposition 3,[0],[0]
"[n].
Since w∗ is the global optimizer of the convex optimization problem defined in Equation (9), this indicates that for any w ∈ Rd,
1
n n∑ i=1",B.2 Proof of Proposition 3,[0],[0]
`p(−yi(w>xi)),B.2 Proof of Proposition 3,[0],[0]
≥ 1 n n∑ i=1,B.2 Proof of Proposition 3,[0],[0]
"`p(−yi(w∗>xi)).
",B.2 Proof of Proposition 3,[0],[0]
Now we show that θ∗ is local minimum of the empirical loss function.,B.2 Proof of Proposition 3,[0],[0]
"Now we slightly perturb the parameters a0, ..., aM ,w1, ...,wM by ∆a0, ...,∆aM ,∆w1, ...,∆wM .",B.2 Proof of Proposition 3,[0],[0]
"Define
θ̃ = (a∗0 + ∆a0, ..., a ∗ M + ∆aM ,w ∗ 1 + ∆w1, ...,w ∗ M + ∆wM ).
",B.2 Proof of Proposition 3,[0],[0]
Then,B.2 Proof of Proposition 3,[0],[0]
", if ‖θ − θ̃‖2 ≤ ε and ε is positive and sufficiently small, then for ∀j ∈",B.2 Proof of Proposition 3,[0],[0]
[M ] and ∀ ∈,B.2 Proof of Proposition 3,[0],[0]
"[n], we have
w∗jxi + ∆w",B.2 Proof of Proposition 3,[0],[0]
> j xi ≥ 1− ‖∆wj‖2,B.2 Proof of Proposition 3,[0],[0]
‖xi‖2,B.2 Proof of Proposition 3,[0],[0]
"≥ 1−K1ε > 0.
",B.2 Proof of Proposition 3,[0],[0]
"This means that if ε is positive and sufficiently small, then
f(xi; θ̃) = ∆a0 + M∑ j=1 (a∗j + ∆aj)",B.2 Proof of Proposition 3,[0],[0]
"( w>xi + ∆w > j xi ) .
",B.2 Proof of Proposition 3,[0],[0]
This means that f(x; θ̃) behave as a linear model on the dataset.,B.2 Proof of Proposition 3,[0],[0]
"Since w∗ corresponds to the optimal linear model minimizing the empirical loss, then
L̂n(θ̃) = 1
n n∑ i=1",B.2 Proof of Proposition 3,[0],[0]
"`p(−yif(xi; θ̃))
",B.2 Proof of Proposition 3,[0],[0]
≥ 1 n n∑ i=1,B.2 Proof of Proposition 3,[0],[0]
`p(−yi(w>xi)),B.2 Proof of Proposition 3,[0],[0]
≥ 1 n n∑ i=1,B.2 Proof of Proposition 3,[0],[0]
`p(−yif(xi;θ∗)),B.2 Proof of Proposition 3,[0],[0]
"= L̂n(θ∗).
",B.2 Proof of Proposition 3,[0],[0]
This means that θ∗ is a local minimum of the empirical loss and f(xi;θ ∗),B.2 Proof of Proposition 3,[0],[0]
= a∗0 for all i ∈,B.2 Proof of Proposition 3,[0],[0]
[n].,B.2 Proof of Proposition 3,[0],[0]
"This further indicates that
R̂n(θ ∗) ≥ min{n−, n+}
n .
",B.2 Proof of Proposition 3,[0],[0]
"Now we only need to construct the data distribution satisfying assumptions in Theorem 1 such that with probability at least 1− e−Ω(n), the dataset drawn from this distribution satisfies the assumption in claim 2.
",B.2 Proof of Proposition 3,[0],[0]
Distribution for Theorem 1:,B.2 Proof of Proposition 3,[0],[0]
"Now we define a distribution as follows, PX|Y=1 is a uniform distribution on the region [−2,−1] ∪ [1, 2] × {0} × {1} × {0}d−3 and PX|Y=−1 is a uniform distribution on the region {0} ×",B.2 Proof of Proposition 3,[0],[0]
"[−2,−1] ∪ [1, 2] × {1} × {0}d−3.",B.2 Proof of Proposition 3,[0],[0]
"In addition, P(Y = 1) = P(Y = −1) = 0.5.",B.2 Proof of Proposition 3,[0],[0]
"It is easy to check that r = 3 > max{r+, r−} = 2 and for any two samples independently drawn from the distribution PX|Y=1 or PX|Y=−1, these two samples are linearly independent.",B.2 Proof of Proposition 3,[0],[0]
This means that this data distribution satisfies the conditions in Theorem 1.,B.2 Proof of Proposition 3,[0],[0]
"In addition, if samples in the dataset are independently drawn from this distribution, then with probability 1 − e−Ω(n), the dataset contains samples in each of the following four regions: [−2,−1]×{0}×{1}×{0}d−3, [1, 2]×{0}×{1}×{0}d−3, {0}× [1, 2]×{1}× {0}d−3 and {0}× [−2,−1]×{1}× {0}d−3, which makes the samples in the dataset not linearly separable.",B.2 Proof of Proposition 3,[0],[0]
Proposition 16 Assume that assumption 1 and 4 are satisfed.,B.3 Proof of Proposition 4,[0],[0]
Assume that there exists a constant c ∈ R such that neurons in the network fS satisfy σ(z) + σ(−z) ≡,B.3 Proof of Proposition 4,[0],[0]
c for all z ∈ R. Assume that the dataset D has 2n samples.,B.3 Proof of Proposition 4,[0],[0]
"Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 1 such that, with probability at least Ω(1/n2), the empirical loss function L̂2n(θ; p) has a local minimum θ ∗ = (θ∗S ,θ ∗ D) satisfying R̂2n(θ
∗) ≥ min{n−,n+}2n , where n+ and n− denote the number of positive and negative samples in the dataset, respectively.
",B.3 Proof of Proposition 4,[0],[0]
"Proof: We first prove the following claim showing that when the dataset satisfies certain conditions, there exists a local minimum satisfying R̂2n(θ
∗) ≥ min{n−,n+}2n .",B.3 Proof of Proposition 4,[0],[0]
"Next, we construct a data distribution such that the dataset drawn from the distribution satisfies these conditions with probability Ω(1/n2).
",B.3 Proof of Proposition 4,[0],[0]
Claim 3,B.3 Proof of Proposition 4,[0],[0]
"Assume that for each sample (xi, yi) in the dataset D = {(xi, yi)}2ni=1, there exists a sample (xj , yj) ∈ D such that ‖xi + xj‖2 = 0 and yi = yj.",B.3 Proof of Proposition 4,[0],[0]
If the function σ(z),B.3 Proof of Proposition 4,[0],[0]
"+σ(−z) ≡ constant on R, then the empirical loss function L̂2n(θ) has a local minimum θ ∗ satisfying",B.3 Proof of Proposition 4,[0],[0]
"R̂2n(θ ∗) ≥ min{n−,n+}2n .
",B.3 Proof of Proposition 4,[0],[0]
"Proof: Consider a single layer neural network
f(x;θ) = a0 + M∑ j=1",B.3 Proof of Proposition 4,[0],[0]
"ajσ(w > j x).
",B.3 Proof of Proposition 4,[0],[0]
Now we construct a local minimum θ∗. Let a∗1 = ...,B.3 Proof of Proposition 4,[0],[0]
"= a ∗ M = −1, and w∗1 = ...",B.3 Proof of Proposition 4,[0],[0]
= w∗M = 0d.,B.3 Proof of Proposition 4,[0],[0]
Thus f(x;θ∗) = a∗0 −Mσ(0).,B.3 Proof of Proposition 4,[0],[0]
"Let a∗0 be the global optimizer of the following convex optimization problem.
",B.3 Proof of Proposition 4,[0],[0]
min a 2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
"`p(−yi(a−Mσ(0))).
",B.3 Proof of Proposition 4,[0],[0]
"Thus, we have 2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi) = 0, (10) and this indicates that∑ i:yi=1",B.3 Proof of Proposition 4,[0],[0]
`′p(−(a∗0 −Mσ(0))),B.3 Proof of Proposition 4,[0],[0]
= ∑,B.3 Proof of Proposition 4,[0],[0]
i:yi=−1 `′p(a ∗ 0 −Mσ(0)) or `′p(−a∗0 +Mσ(0))n+ =,B.3 Proof of Proposition 4,[0],[0]
`′p(a∗0 −Mσ(0))n−. (11),B.3 Proof of Proposition 4,[0],[0]
"In addition, we have, for ∀j ∈",B.3 Proof of Proposition 4,[0],[0]
"[M ],
∂L̂2n(θ ∗)
",B.3 Proof of Proposition 4,[0],[0]
aj = 2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)σ(0) = 0, by Equation (10)
∇wj L̂2n(θ∗) =",B.3 Proof of Proposition 4,[0],[0]
2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)σ′(0)xi,
= σ′(0) 2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)xi.
",B.3 Proof of Proposition 4,[0],[0]
"By assumption that for each sample (xi, yi) in the dataset, there exists a sample (xj , yj) in the dataset such that xi + xj = 0d",B.3 Proof of Proposition 4,[0],[0]
"and yi = yj , i.e., yixi + yjxj = 0d, thus we have for any j ∈",B.3 Proof of Proposition 4,[0],[0]
"[M ],
∇wj L̂2n(θ∗) = σ′(0)",B.3 Proof of Proposition 4,[0],[0]
2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)xi = 0d.,B.3 Proof of Proposition 4,[0],[0]
"(12)
Furthermore, we have
∂L̂2n(θ ∗)
a0 = 2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi) = 0,
then θ∗ is a critical point.",B.3 Proof of Proposition 4,[0],[0]
Now we only need to show that it is a local minimum.,B.3 Proof of Proposition 4,[0],[0]
We prove it by definition.,B.3 Proof of Proposition 4,[0],[0]
"Consider any perturbation ∆a1, ...,∆aM : |∆aj | < 12 for all j ∈",B.3 Proof of Proposition 4,[0],[0]
"[M ], ∆w1, ...,∆wM ∈ Rd and ∆a0",B.3 Proof of Proposition 4,[0],[0]
"∈ R. Define
θ̃ = (a∗0 + ∆a0, ..., a ∗ M + ∆aM ,w ∗ 1 + ∆w1, ...,w ∗ M + ∆wM ).
",B.3 Proof of Proposition 4,[0],[0]
"Then
2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
`p(−yif(xi; θ̃))−,B.3 Proof of Proposition 4,[0],[0]
2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
`p(−yif(xi;θ∗)),B.3 Proof of Proposition 4,[0],[0]
= 2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
[ `p(−yif(xi; θ̃))− `p(−yif(xi;θ∗)) ],B.3 Proof of Proposition 4,[0],[0]
"≥
",B.3 Proof of Proposition 4,[0],[0]
2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)[f(xi; θ̃)− f(xi;θ∗)]
= 2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)[f(xi; θ̃)− a∗0 +Mσ(0)]
",B.3 Proof of Proposition 4,[0],[0]
= 2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃),
where the inequality follows from the convexity of `p, the second equality follows from the fact that f(x;θ∗) ≡",B.3 Proof of Proposition 4,[0],[0]
a∗0 −Mσ(0) and the third equality follows from Equation (10).,B.3 Proof of Proposition 4,[0],[0]
"In addition, we have
2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) = 2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) + ∆a0
 =
2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) by Eq.,B.3 Proof of Proposition 4,[0],[0]
"(10) =
M∑ j=1 −(a∗j + ∆aj)",B.3 Proof of Proposition 4,[0],[0]
[ 2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>j xi )] .
",B.3 Proof of Proposition 4,[0],[0]
"Now we consider the following term
2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>j xi ) .
",B.3 Proof of Proposition 4,[0],[0]
"By assumption that for each sample (xi, yi) in the dataset, there exists a sample (xk, yk) in the dataset such that xi + xk = 0d, yi = yk by the assumption that there exists a constant c0 such that σ(z) + σ(−z) ≡ c0, thus we have for any ∆wj ∈",B.3 Proof of Proposition 4,[0],[0]
"Rd,
yiσ ( ∆w>j xi ) + ykσ",B.3 Proof of Proposition 4,[0],[0]
"( ∆w>j xk ) = yiσ ( ∆w>j xi ) + yiσ ( −∆w>j xi ) = yic0 =
c0 2 (yi + yk),
where the last equality follows from yi = yk.",B.3 Proof of Proposition 4,[0],[0]
"Therefore, we have for all ∆wj ∈ Rd, 2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
`p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>j xi ) = c0 2 2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
"`p(−yi(a∗0 −Mσ(0)))yi = 0.
Thus, we have 2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) = M∑ j=1 −(a∗j + ∆aj),B.3 Proof of Proposition 4,[0],[0]
[ 2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
`p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>j xi )],B.3 Proof of Proposition 4,[0],[0]
"= 0,
and this further indicates
2n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
`p(−yif(xi; θ̃))−,B.3 Proof of Proposition 4,[0],[0]
2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
`p(−yif(xi;θ∗)),B.3 Proof of Proposition 4,[0],[0]
≥ 2n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) = 0.
",B.3 Proof of Proposition 4,[0],[0]
"Therefore, this means that θ∗ is a local minimum.",B.3 Proof of Proposition 4,[0],[0]
"Since f(x;θ∗) = a∗0 −Mσ(0), then clearly,
R̂2n(θ ∗)",B.3 Proof of Proposition 4,[0],[0]
"≥ min{n+, n−}
n .
",B.3 Proof of Proposition 4,[0],[0]
"Now we construct the data distribution PX×Y as follows
P(X = (1, 0), Y = 1) = P(X = (−1, 0), Y = 1) = P(X = (0, 1), Y = −1) = P(X = (0,−1), Y = −1).
",B.3 Proof of Proposition 4,[0],[0]
"Assume that samples in the dataset D = {(xi, yi)}2ni=1 are independently draw from the data distribution PX×Y .",B.3 Proof of Proposition 4,[0],[0]
"Let n(1,0) and n(−1,0) denote the number of samples at the point (1, 0) and (−1, 0), respectively.",B.3 Proof of Proposition 4,[0],[0]
"Let n(0,1) and n(0,−1) denote the number of samples at the point (0, 1) and (0,−1), respectively.",B.3 Proof of Proposition 4,[0],[0]
"Then the probability that n(1,0) = n(−1,0) and n(0,1) = n(0,−1) is
PX×Y",B.3 Proof of Proposition 4,[0],[0]
"[ n(1,0) = n(−1,0) and n(0,1) = n(0,−1) ]",B.3 Proof of Proposition 4,[0],[0]
= n∑ i=1,B.3 Proof of Proposition 4,[0],[0]
( 2n 2i )( 2i i )( 2(n− i),B.3 Proof of Proposition 4,[0],[0]
n−,B.3 Proof of Proposition 4,[0],[0]
"i )( 1 4 )2n =
n∑ i=1
(2n)!",B.3 Proof of Proposition 4,[0],[0]
(2i)!(2n− 2i)!,B.3 Proof of Proposition 4,[0],[0]
(2i)!,B.3 Proof of Proposition 4,[0],[0]
[i!]2 (2n− 2i)!,B.3 Proof of Proposition 4,[0],[0]
"[(n− i)!]2
( 1
16
)n = n∑ i=1
(2n)!",B.3 Proof of Proposition 4,[0],[0]
"[i!(n− i)!]2 1 16n
= (2n)!
16n(n!)2 n∑ i=1
(n!)2
[i!(n− i)!]2 =
(2n)!
16n(n!)2 n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
"( n i )2 = 1
16n
( 2n
n
)2 >
1
(n+ 1)2
by the equality n∑ i=1",B.3 Proof of Proposition 4,[0],[0]
"( n i )2 = ( 2n n ) and the inequality (
2n
n
) >",B.3 Proof of Proposition 4,[0],[0]
"4n
n+ 1 .
",B.3 Proof of Proposition 4,[0],[0]
Now we only need to check whether the distribution PX×Y satisfies the assumptions shown in Theorem 1.,B.3 Proof of Proposition 4,[0],[0]
"Clearly, r+ = r− = 1 < r = 2 and with probability 1, random vector X drawn from distribution PX|Y=1 and random vector Z drawn from distribution PX|Y=−1 has rank one which equals to r+ and r−. Therefore, the distribution constructed here satisfies the assumptions in Theorem 1.",B.3 Proof of Proposition 4,[0],[0]
Proposition 17 Assume that assumption 1 and 4 are satisfed.,B.4 Proof of Proposition 5,[0],[0]
Assume that neurons in fS satisfy that σ is strongly convex and twice differentiable on R and has a global minimum at z = 0.,B.4 Proof of Proposition 5,[0],[0]
"Then there exists a network architecture fD and a distribution satisfying assumptions in Theorem 2 such that with probability one, the empirical loss L̂n(θ; p), p ≥ 2 has a local minima θ∗ = (θ∗S ,θ∗D) satisfying R̂n(θ
∗) ≥ min{n+,n−}n , where n+ and n− denote the number of positive and negative samples in the dataset, respectively.
",B.4 Proof of Proposition 5,[0],[0]
"Proof: We first prove the following claim showing that if the dataset satisfies certain conditions, then the empirical loss has a local minimum satisfying R̂n(θ
∗) ≥ min{n−,n+}n .",B.4 Proof of Proposition 5,[0],[0]
"Next, we construct a data distribution such that the dataset drawn from the distribution PX×Y satisfies these conditions with probability one.",B.4 Proof of Proposition 5,[0],[0]
Claim 4,B.4 Proof of Proposition 5,[0],[0]
If the matrix 1n+ ∑ i:yi=1 xix >,B.4 Proof of Proposition 5,[0],[0]
i − 1n− ∑,B.4 Proof of Proposition 5,[0],[0]
i:,B.4 Proof of Proposition 5,[0],[0]
yi=−1 xix >,B.4 Proof of Proposition 5,[0],[0]
"i is positive or negative definite, then the empirical loss function L̂n(θ) has a local minimum θ ∗ satisfying R̂n(θ ∗) ≥ min{n−,n+}n .
",B.4 Proof of Proposition 5,[0],[0]
Proof: We prove that,B.4 Proof of Proposition 5,[0],[0]
"if the following matrix
1
n+ ∑ i:yi=1",B.4 Proof of Proposition 5,[0],[0]
xix >,B.4 Proof of Proposition 5,[0],[0]
i,B.4 Proof of Proposition 5,[0],[0]
− 1 n− ∑ i:,B.4 Proof of Proposition 5,[0],[0]
yi=−1 xix >,B.4 Proof of Proposition 5,[0],[0]
"i
is either positive definite or negative definite, then there exists a local minima θ∗ having f(x;θ∗) ≡ constant",B.4 Proof of Proposition 5,[0],[0]
"and this leads to R̂n(θ
∗) ≥ min{n+,n−}n .",B.4 Proof of Proposition 5,[0],[0]
"Without loss of generality, we assume that the matrix is positive definite.",B.4 Proof of Proposition 5,[0],[0]
"Consider a single layer neural network
f(x;θ) = a0 + M∑",B.4 Proof of Proposition 5,[0],[0]
"j=1 ajσ ( w>j x ) .
",B.4 Proof of Proposition 5,[0],[0]
Let a∗1 = ...,B.4 Proof of Proposition 5,[0],[0]
= a ∗ M = −1 and w∗1 = ...,B.4 Proof of Proposition 5,[0],[0]
= w∗M = 0d.,B.4 Proof of Proposition 5,[0],[0]
"Therefore, we have f(x;θ∗)",B.4 Proof of Proposition 5,[0],[0]
= a∗0 −Mσ(0).,B.4 Proof of Proposition 5,[0],[0]
"Let a∗0 be the global optimizer of the following convex optimization problem.
",B.4 Proof of Proposition 5,[0],[0]
min a n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
"`p(−yi(a−Mσ(0))).
",B.4 Proof of Proposition 5,[0],[0]
"Thus, we have n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi) = 0, (13) and this indicates that∑ i:yi=1",B.4 Proof of Proposition 5,[0],[0]
`′p(−(a∗0 −Mσ(0))),B.4 Proof of Proposition 5,[0],[0]
= ∑,B.4 Proof of Proposition 5,[0],[0]
i:yi=−1 `′p(a ∗ 0 −Mσ(0)) or `′p(−a∗0 +Mσ(0))n+ =,B.4 Proof of Proposition 5,[0],[0]
`′p(a∗0 −Mσ(0))n−. (14),B.4 Proof of Proposition 5,[0],[0]
"In addition, since for ∀j ∈ [M ],
∂L̂n(θ ∗)
",B.4 Proof of Proposition 5,[0],[0]
∂aj = n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)σ(0) = 0, by Equation (13),
∇wj L̂n(θ∗) = n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)σ′(0)xi = 0d, by σ′(0) = 0,
and ∂L̂n(θ ∗)
",B.4 Proof of Proposition 5,[0],[0]
∂a0 = n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi) = 0,
then θ∗ is a critical point.",B.4 Proof of Proposition 5,[0],[0]
"Next we show that θ∗ = (a∗0, ..., a ∗ M ,w ∗ 1, ...,w ∗ M ) is a local minima.",B.4 Proof of Proposition 5,[0],[0]
"Consider any perturbation ∆a1, ...,∆aM : |∆aj | < 12 for all j ∈",B.4 Proof of Proposition 5,[0],[0]
"[M ], ∆w1, ...,∆wM ∈ Rd and ∆a0",B.4 Proof of Proposition 5,[0],[0]
"∈ R. Define
θ̃ = (a∗0 + ∆a0, ..., a ∗ M + ∆aM ,w ∗ 1 + ∆w1, ...,w ∗ M + ∆wM ).
",B.4 Proof of Proposition 5,[0],[0]
"Then
n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
`p(−yif(xi; θ̃))− n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
`p(−yif(xi;θ∗)),B.4 Proof of Proposition 5,[0],[0]
= n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
[ `p(−yif(xi; θ̃))− `p(−yif(xi;θ∗)) ],B.4 Proof of Proposition 5,[0],[0]
"≥
n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)[f(xi; θ̃)− f(xi;θ∗)]
= n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)[f(xi; θ̃)− a∗0 +Mσ(0)]
= n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃),
where the inequality follows from the convexity of the loss function `p(z), the second equality follows from the fact that f(x;θ∗) ≡",B.4 Proof of Proposition 5,[0],[0]
a∗0 − Mσ(0) and the third equality follows from Equation (14).,B.4 Proof of Proposition 5,[0],[0]
"In addition, we have
n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) = n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) + ∆a0
 =
n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) by Eq.,B.4 Proof of Proposition 5,[0],[0]
"(14) =
M∑ j=1 −(a∗j + ∆aj)",B.4 Proof of Proposition 5,[0],[0]
[ n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>j xi )] .
",B.4 Proof of Proposition 5,[0],[0]
"Now we define the following function G : Rd → R,
G(u) = n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ ( u>xi ) .
",B.4 Proof of Proposition 5,[0],[0]
"Now we consider the gradient of the function G with respect to the vector u at the point 0d,
∇uG(0d) = n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ′ (0)xi = 0d.
",B.4 Proof of Proposition 5,[0],[0]
"Furthermore, the Hessian matrix ∇2uG(0d) satisfies
∇2uG(0d) = n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))yiσ′′ (0)xix>i = σ′′ (0) n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yixix>i
= σ′′(0)  1",B.4 Proof of Proposition 5,[0],[0]
n+ ∑ i:yi=1 xix,B.4 Proof of Proposition 5,[0],[0]
>,B.4 Proof of Proposition 5,[0],[0]
i,B.4 Proof of Proposition 5,[0],[0]
− 1 n− ∑ i:,B.4 Proof of Proposition 5,[0],[0]
yi=−1 xix >,B.4 Proof of Proposition 5,[0],[0]
"i  0, then the function G(u)",B.4 Proof of Proposition 5,[0],[0]
=,B.4 Proof of Proposition 5,[0],[0]
∑n i=1,B.4 Proof of Proposition 5,[0],[0]
`p(−yi(a∗0 −Mσ(0)))yiσ ( u>xi ) has a local minima at u = 0d.,B.4 Proof of Proposition 5,[0],[0]
"This indicates that there exists ε > 0 such that for all ∆w : ‖∆w‖2 ≤ ε, n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>xi ) ≥ n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
"`p(−yi(a∗0 −Mσ(0)))yiσ (0) = 0.
",B.4 Proof of Proposition 5,[0],[0]
"In addition, since a∗j = −1, |∆aj | < 12 , then for all ∆wj : ‖∆wj‖2 ≤ ε, n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) = M∑ j=1 −(a∗j + ∆aj),B.4 Proof of Proposition 5,[0],[0]
[ n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
`p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>j xi )],B.4 Proof of Proposition 5,[0],[0]
"≥ 0.
",B.4 Proof of Proposition 5,[0],[0]
"Therefore, we have n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) ≥ 0, and this indicates that n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
`p(−yif(xi; θ̃))− n∑ i=1,B.4 Proof of Proposition 5,[0],[0]
`p(−yif(xi;θ∗)),B.4 Proof of Proposition 5,[0],[0]
≥ 0.,B.4 Proof of Proposition 5,[0],[0]
"Thus, θ∗ is a local minima with f(x;θ∗)",B.4 Proof of Proposition 5,[0],[0]
= a∗0 −Mσ(0) = constant.,B.4 Proof of Proposition 5,[0],[0]
"Thus, n∑ i=1",B.4 Proof of Proposition 5,[0],[0]
I{yi 6= sgn(f(xi;θ∗))},B.4 Proof of Proposition 5,[0],[0]
"≥ min{n−, n+} n .
",B.4 Proof of Proposition 5,[0],[0]
Now we define a data distribution as follows.,B.4 Proof of Proposition 5,[0],[0]
Let PY (Y = 1) = P(Y = −1) = 0.5.,B.4 Proof of Proposition 5,[0],[0]
"Let PX|Y=1 be a continuous distribution (e.g., uniform distribution) defined on the interval [2, 3] and PX|Y=−1 be a continuous distribution defined on the interval [−1,−1/2].",B.4 Proof of Proposition 5,[0],[0]
"Then if samples in the dataset D are drawn independently from the this distribution, the scalar 1n+ ∑ i:yi=1 x2i",B.4 Proof of Proposition 5,[0],[0]
− 1n− ∑ i:yi=−1 x 2 i > 0,B.4 Proof of Proposition 5,[0],[0]
if n+ > 0,B.4 Proof of Proposition 5,[0],[0]
"and
the scalar 1n+ ∑ i:yi=1",B.4 Proof of Proposition 5,[0],[0]
x2i,B.4 Proof of Proposition 5,[0],[0]
− 1n− ∑ i:yi=−1 x 2 i < 0,B.4 Proof of Proposition 5,[0],[0]
if n+ = 0.,B.4 Proof of Proposition 5,[0],[0]
This means that the dataset satisfies the conditions in the claim with probability one.,B.4 Proof of Proposition 5,[0],[0]
Proposition 18 Assume that assumption 1 is satisfied.,B.5 Proof of Proposition 6,[0],[0]
Assume that the feedforward neural network f(x;θ) has at least one hidden layer and has at least one neuron in each hidden layer.,B.5 Proof of Proposition 6,[0],[0]
"If neurons in the network f satisfy that σ(z) = 0 for all z ≤ 0 and σ(z) is continuous on R, then the empirical loss L̂n(θ; p), p ≥ 2 has a local minima θ∗ satisfying R̂n(θ∗) ≥ min{n+,n−}n , where n+ and n− denote the number of positive and negative samples in the dataset, respectively.
",B.5 Proof of Proposition 6,[0],[0]
"Proof: Assume that the multilayer neural network f(x;θ) has L ≥ 1 hidden layers, Ml ≥ 1 neurons in the l-th layer.",B.5 Proof of Proposition 6,[0],[0]
Now we let the vector θl contain all parameters in the first l ∈,B.5 Proof of Proposition 6,[0],[0]
[L] layers.,B.5 Proof of Proposition 6,[0],[0]
"Then the output of the neural network can be rewritten as
f(x; a0,θL) = a0 + ML∑",B.5 Proof of Proposition 6,[0],[0]
j=1 ajσ(w > j Φ(x;θL−1),B.5 Proof of Proposition 6,[0],[0]
"+ bj),
where Φ(x;θL−1)",B.5 Proof of Proposition 6,[0],[0]
=,B.5 Proof of Proposition 6,[0],[0]
"(Φ1(x;θL−1), ...,ΦML−1(x;θL−1)) denotes the outputs of the neurons in the layer L − 1.",B.5 Proof of Proposition 6,[0],[0]
"Now we construct a local minimum θ∗ = (a∗0,θ∗L).",B.5 Proof of Proposition 6,[0],[0]
"The key idea of constructing the local minimum having a training error no smaller than min{n+,n−}n is appropriately choosing wj , bj such that all neurons in the last layer keep inactive on all samples in the dataset.",B.5 Proof of Proposition 6,[0],[0]
This is possible since the outputs of the neurons in the layer L− 1 are bounded.,B.5 Proof of Proposition 6,[0],[0]
We first set θL−1 to any unit vector θ ∗,B.5 Proof of Proposition 6,[0],[0]
L−1 : ‖θ∗L−1‖2 = 1.,B.5 Proof of Proposition 6,[0],[0]
"Next, for any data set D = {(xi; yi)}ni=1, we define K = max
i∈[n] ‖Φ(xi;θ∗L−1)‖2.
",B.5 Proof of Proposition 6,[0],[0]
"In addition, it is easy to show that the function ϕij(θ) = Φj(xi;θ) is a continuous function.",B.5 Proof of Proposition 6,[0],[0]
Now we consider the compact set Cδ = {θ : ‖θ,B.5 Proof of Proposition 6,[0],[0]
"− θ∗L−1‖2 ≤ δ}, where δ > 0",B.5 Proof of Proposition 6,[0],[0]
.,B.5 Proof of Proposition 6,[0],[0]
"Since each function ϕij is a continuous function on the compact set C, then by the definition of continuity,
∀ε > 0,∃δij(ε) ∈ (0, 1) : |ϕij(θ)− ϕij(θ∗L−1)| ≤ ε for all θ ∈ Cδij .",B.5 Proof of Proposition 6,[0],[0]
"For a given ε > 0, let
δ(ε) = min i∈[n],j∈[ML−1] δij(ε),
then for all i ∈",B.5 Proof of Proposition 6,[0],[0]
"[n], j ∈ [ML−1] and ∀θ ∈ Cδ, |ϕij(θ)− ϕij(θL−1)| ≤ ε.
",B.5 Proof of Proposition 6,[0],[0]
Now we set wj to some unit vector wj : ‖wj‖2 = 1 for all j ∈,B.5 Proof of Proposition 6,[0],[0]
"[ML−1], and we set bj to a scalar b∗j satisfying
w∗j >Φ(xi;θ ∗ L−1)",B.5 Proof of Proposition 6,[0],[0]
+,B.5 Proof of Proposition 6,[0],[0]
"b ∗ j ≤ −1, for all i ∈",B.5 Proof of Proposition 6,[0],[0]
"[n] and all θ ∈ C.
Therefore, the neural network becomes
f(xi; a0,θ ∗ L) = a0, ∀i ∈",B.5 Proof of Proposition 6,[0],[0]
"[n].
",B.5 Proof of Proposition 6,[0],[0]
"Furthermore, for the δ(ε) defined above and for any parameter vector θ̃L : ‖θ̃L−θ∗L‖2 ≤ δ(ε), we have for all j ∈",B.5 Proof of Proposition 6,[0],[0]
[ML−1] and all i ∈,B.5 Proof of Proposition 6,[0],[0]
"[n],
|w̃>j Φ(xi; θ̃L−1) + b̃j",B.5 Proof of Proposition 6,[0],[0]
−w∗j>Φ(xi;θ∗L−1)− b∗j | ≤,B.5 Proof of Proposition 6,[0],[0]
|w̃>j Φ(xi; θ̃L−1)− w̃>j Φ(xi;θ∗L−1),B.5 Proof of Proposition 6,[0],[0]
+ w̃>j Φ(xi;θ∗L−1)−w∗j>Φ(xi;θ∗L−1)|+ |b̃j,B.5 Proof of Proposition 6,[0],[0]
− bj | ≤ |w̃>j Φ(xi; θ̃L−1)− w̃>j Φ(xi;θ∗L−1)|+,B.5 Proof of Proposition 6,[0],[0]
|w̃>j Φ(xi;θ∗L−1)−w∗j>Φ(xi;θ∗L−1)|+ |b̃j,B.5 Proof of Proposition 6,[0],[0]
− bj | ≤ ‖w̃j‖2‖Φ(xi; θ̃L−1)−Φ(xi;θ∗L−1)‖2 + ‖w̃j −w∗j‖2‖Φ(xi;θ∗L−1)‖2 + |b̃j,B.5 Proof of Proposition 6,[0],[0]
− bj | ≤,B.5 Proof of Proposition 6,[0],[0]
(1 + δ(ε)),B.5 Proof of Proposition 6,[0],[0]
"√ ML−1ε+ εK + ε ≤ (2 √ ML−1 +K + 1)ε.
",B.5 Proof of Proposition 6,[0],[0]
Thus,B.5 Proof of Proposition 6,[0],[0]
", if ε < 1 2(2 √ ML−1+K+1) , then for all θ̃L : ‖θ̃L − θ∗L‖2 ≤ δ(ε), ∀j ∈",B.5 Proof of Proposition 6,[0],[0]
[M ] and ∀i ∈,B.5 Proof of Proposition 6,[0],[0]
"[n]
w̃>j Φ(xi; θ̃L−1) + b̃j ≤ w∗j>Φ(xi;θ∗L−1)",B.5 Proof of Proposition 6,[0],[0]
+ b∗j,B.5 Proof of Proposition 6,[0],[0]
+ 1 2 ≤ −1 2 .,B.5 Proof of Proposition 6,[0],[0]
"(15)
Since σ(z) = 0 for all z ≤ 0, then this indicates that for all θ̃L : ‖θ̃L",B.5 Proof of Proposition 6,[0],[0]
"− θ∗L‖2 ≤ δ(ε),
f(xi; a0, θ̃L−1) = a0, for all",B.5 Proof of Proposition 6,[0],[0]
i ∈,B.5 Proof of Proposition 6,[0],[0]
"[n].
Finally, we set a∗0 to the global minimizer of the following convex optimization problem:
min a∈R
1
n n∑ i=1",B.5 Proof of Proposition 6,[0],[0]
"`(−yia).
",B.5 Proof of Proposition 6,[0],[0]
"This indicates that for any a ∈ R,
1
n n∑ i=1",B.5 Proof of Proposition 6,[0],[0]
`(−yia) ≥ 1 n n∑ i=1,B.5 Proof of Proposition 6,[0],[0]
"`(−yia∗0).
",B.5 Proof of Proposition 6,[0],[0]
"Therefore, for θ̃L : ‖θ̃L − θ∗L‖2 ≤ δ(ε) and any a0 ∈ R
L̂n(a0, θ̃L) = 1
n n∑ i=1",B.5 Proof of Proposition 6,[0],[0]
`(−yif(xi; θ̃L)),B.5 Proof of Proposition 6,[0],[0]
= 1 n n∑ i=1,B.5 Proof of Proposition 6,[0],[0]
"`(−yia0)
≥ 1 n n∑ i=1",B.5 Proof of Proposition 6,[0],[0]
`(−yia∗0) ≥ 1 n n∑ i=1,B.5 Proof of Proposition 6,[0],[0]
"`(−yif(xi; a∗0,θ∗L))",B.5 Proof of Proposition 6,[0],[0]
"= L̂n(a∗0,θ∗L).
",B.5 Proof of Proposition 6,[0],[0]
"This means that (a∗0,θ ∗ L) is a local minima and f(xi; a ∗ 0,θ ∗ L) = a ∗ 0 for all",B.5 Proof of Proposition 6,[0],[0]
i ∈,B.5 Proof of Proposition 6,[0],[0]
[n].,B.5 Proof of Proposition 6,[0],[0]
"This further indicates that
R̂n(θ ∗) ≥ min{n−, n+}
n .",B.5 Proof of Proposition 6,[0],[0]
Proposition 19 Assume that H : Rd → Rd is a feedforward neural network parameterized by θ and all neurons in H are ReLUs.,B.6 Proof of Proposition 7,[0],[0]
"Define a network f : Rd → R with identity shortcut connections as f(x;a,θ, b) = a>(x + H(x;θ))",B.6 Proof of Proposition 7,[0],[0]
"+ b, a ∈ Rd, b ∈ R.",B.6 Proof of Proposition 7,[0],[0]
"Then there exists a distribution PX×Y satisfying the assumptions in Theorem 1 such that with probability at least 1 − e−Ω(n), the empirical loss L̂n(a,θ, b; p) = 1 n",B.6 Proof of Proposition 7,[0],[0]
∑n i=1,B.6 Proof of Proposition 7,[0],[0]
"`(−yif(xi;θ); p), p ≥ 2 has a local minimum with non-zero training error.
",B.6 Proof of Proposition 7,[0],[0]
"Proof: We first show that if the samples in the dataset are not linearly separable, then empirical loss has a local minimum with a non-zero training error.",B.6 Proof of Proposition 7,[0],[0]
"Next, we construct a data distribution such that n samples independently drawn from this data distribution are not linearly separable with probability at least 1− exp(−Ω(n)).
",B.6 Proof of Proposition 7,[0],[0]
Claim 5,B.6 Proof of Proposition 7,[0],[0]
"If the samples in the dataset are not linearly separable, i.e., minw∈Rd,b∈R 1 n",B.6 Proof of Proposition 7,[0],[0]
∑n i=1,B.6 Proof of Proposition 7,[0],[0]
I{yi 6=,B.6 Proof of Proposition 7,[0],[0]
"sgn(w>xi + b)} > 0, then empirical loss has a local minimum with a non-zero training error.
",B.6 Proof of Proposition 7,[0],[0]
"Proof: The proof follows from the proof of Proposition 2 in Appendix B.1 where we show that when the dataset has both positive and negative samples and all neurons in the multilayer network are ReLUs, then the empirical loss has a local minimum with a non-zero training error.",B.6 Proof of Proposition 7,[0],[0]
"Assume that the multilayer neural network H(x;θ) has L ≥ 1 hidden layers, Ml ≥ 1 neurons in the l-th layer in the multilayer neural network H. Clearly,",B.6 Proof of Proposition 7,[0],[0]
ML = d.,B.6 Proof of Proposition 7,[0],[0]
Now we let the vector θl contain all parameters in the first l ∈,B.6 Proof of Proposition 7,[0],[0]
[L] layers.,B.6 Proof of Proposition 7,[0],[0]
"Then the output of the neural network f(x;a,θ, b) can be rewritten as
f(x;a,θ, b) = b+ ML∑ j=1",B.6 Proof of Proposition 7,[0],[0]
ajσ(w > j Φ(x;θL−1),B.6 Proof of Proposition 7,[0],[0]
"+ bj) + a >x,
where Φ(x;θL−1) =",B.6 Proof of Proposition 7,[0],[0]
"(Φ1(x;θL−1), ...,ΦML−1(x;θL−1)) denotes the outputs of the neurons in the layer L − 1.",B.6 Proof of Proposition 7,[0],[0]
"Now we construct a local minimum (a∗,θ∗, b∗).",B.6 Proof of Proposition 7,[0],[0]
The whole idea of constructing the local minimum having a non-zero training error is as follows.,B.6 Proof of Proposition 7,[0],[0]
"We first appropriately choose wj , bj such that all neurons in the last layer of the multilayer network H keep inactive on all samples in the dataset.",B.6 Proof of Proposition 7,[0],[0]
"Then the neural network becomes a linear model
f(x;a∗,θ∗, b∗) = b∗ + a∗>x.
",B.6 Proof of Proposition 7,[0],[0]
"Next we only need to set a∗, b∗ to the global optimizer of the convex optimization problem
min a∈Rd,b∈R
1
n n∑ i=1",B.6 Proof of Proposition 7,[0],[0]
"`p ( −yi(a>xi + b) ) .
",B.6 Proof of Proposition 7,[0],[0]
"Therefore, as we have shown in the proof of Propsition 2, if we slightly perturb the parameter θ∗, the output of the multilayer network H(x; θ̃) on all samples are still zero and this makes f(xi;a
∗, θ̃, b∗) = a∗>xi + b
∗.",B.6 Proof of Proposition 7,[0],[0]
"In addition, if we further perturb the vector a∗ and b∗, the value of the empirical loss will not decrease since a∗ and b∗ are the global optimizer of the empirical loss function.",B.6 Proof of Proposition 7,[0],[0]
Now we present the proof.,B.6 Proof of Proposition 7,[0],[0]
We first set θL−1 to any unit vector θ ∗,B.6 Proof of Proposition 7,[0],[0]
L−1 : ‖θ∗L−1‖2 = 1.,B.6 Proof of Proposition 7,[0],[0]
"Next, for any data set D = {(xi; yi)}ni=1, we define K = max
i∈[n] ‖Φ(xi;θ∗L−1)‖2.
",B.6 Proof of Proposition 7,[0],[0]
"In addition, it is easy to show that the function ϕij(θ) = Φj(xi;θ) is a continuous function.",B.6 Proof of Proposition 7,[0],[0]
Now we consider the compact set Cδ = {θ : ‖θ,B.6 Proof of Proposition 7,[0],[0]
"− θ∗L−1‖2 ≤ δ}, where δ > 0",B.6 Proof of Proposition 7,[0],[0]
.,B.6 Proof of Proposition 7,[0],[0]
"Since each function ϕij is a continuous function on the compact set C, then by the definition of continuity,
∀ε > 0,∃δij(ε) ∈ (0, 1) : |ϕij(θ)− ϕij(θ∗L−1)| ≤ ε for all θ ∈ Cδij .
",B.6 Proof of Proposition 7,[0],[0]
"For a given ε > 0, let δ(ε) =",B.6 Proof of Proposition 7,[0],[0]
"min
i∈[n],j∈[ML−1] δij(ε),
then for all i ∈",B.6 Proof of Proposition 7,[0],[0]
"[n], j ∈ [ML−1] and ∀θ ∈ Cδ,
|ϕij(θ)− ϕij(θL−1)| ≤ ε.
",B.6 Proof of Proposition 7,[0],[0]
Now we set wj to some unit vector wj : ‖wj‖2 = 1 for all j ∈,B.6 Proof of Proposition 7,[0],[0]
"[ML−1], and we set bj to a scalar b∗j satisfying
w∗j >Φ(xi;θ ∗ L−1)",B.6 Proof of Proposition 7,[0],[0]
+,B.6 Proof of Proposition 7,[0],[0]
"b ∗ j ≤ −1, for all i ∈",B.6 Proof of Proposition 7,[0],[0]
"[n] and all θ ∈ C.
Therefore, the neural network becomes
f(xi;a, θ̃, b) = a >xi + b, ∀i ∈",B.6 Proof of Proposition 7,[0],[0]
"[n].
",B.6 Proof of Proposition 7,[0],[0]
"Furthermore, for the δ(ε) defined above and for any parameter vector θ̃L : ‖θ̃L−θ∗L‖2 ≤ δ(ε), we have for all j ∈",B.6 Proof of Proposition 7,[0],[0]
[ML−1] and all i ∈,B.6 Proof of Proposition 7,[0],[0]
"[n],
|w̃>j Φ(xi; θ̃L−1) + b̃j",B.6 Proof of Proposition 7,[0],[0]
−w∗j>Φ(xi;θ∗L−1)− b∗j | ≤,B.6 Proof of Proposition 7,[0],[0]
|w̃>j Φ(xi; θ̃L−1)− w̃>j Φ(xi;θ∗L−1),B.6 Proof of Proposition 7,[0],[0]
+ w̃>j Φ(xi;θ∗L−1)−w∗j>Φ(xi;θ∗L−1)|+ |b̃j,B.6 Proof of Proposition 7,[0],[0]
− bj | ≤ |w̃>j Φ(xi; θ̃L−1)− w̃>j Φ(xi;θ∗L−1)|+,B.6 Proof of Proposition 7,[0],[0]
|w̃>j Φ(xi;θ∗L−1)−w∗j>Φ(xi;θ∗L−1)|+ |b̃j,B.6 Proof of Proposition 7,[0],[0]
− bj | ≤ ‖w̃j‖2‖Φ(xi; θ̃L−1)−Φ(xi;θ∗L−1)‖2 + ‖w̃j −w∗j‖2‖Φ(xi;θ∗L−1)‖2 + |b̃j,B.6 Proof of Proposition 7,[0],[0]
− bj | ≤,B.6 Proof of Proposition 7,[0],[0]
(1 + δ(ε)),B.6 Proof of Proposition 7,[0],[0]
"√ ML−1ε+ εK + ε ≤ (2 √ ML−1 +K + 1)ε.
",B.6 Proof of Proposition 7,[0],[0]
Thus,B.6 Proof of Proposition 7,[0],[0]
", if ε < 1 2(2 √ ML−1+K+1) , then for all θ̃L : ‖θ̃L − θ∗L‖2 ≤ δ(ε), ∀j ∈",B.6 Proof of Proposition 7,[0],[0]
[M ] and ∀i ∈,B.6 Proof of Proposition 7,[0],[0]
"[n]
w̃>j Φ(xi; θ̃L−1) + b̃j ≤ w∗j>Φ(xi;θ∗L−1)",B.6 Proof of Proposition 7,[0],[0]
+ b∗j,B.6 Proof of Proposition 7,[0],[0]
+ 1 2 ≤ −1 2 .,B.6 Proof of Proposition 7,[0],[0]
"(16)
Since σ(z) = 0 for all z ≤ 0, then this indicates that for all θ̃L : ‖θ̃L",B.6 Proof of Proposition 7,[0],[0]
"− θ∗L‖2 ≤ δ(ε),
f(xi;a, θ̃, b) = a >xi + b, for all i ∈",B.6 Proof of Proposition 7,[0],[0]
"[n].
Finally, we set a∗, b∗ to the global minimizer of the following convex optimization problem:
",B.6 Proof of Proposition 7,[0],[0]
"min a∈Rd,b∈R
1
n n∑ i=1",B.6 Proof of Proposition 7,[0],[0]
"`p ( −yi(a>xi + b) ) .
",B.6 Proof of Proposition 7,[0],[0]
"This indicates that for any a ∈ Rd, b ∈ R,
1
n n∑ i=1",B.6 Proof of Proposition 7,[0],[0]
`p(−yi(a>xi + b)),B.6 Proof of Proposition 7,[0],[0]
≥ 1 n n∑ i=1,B.6 Proof of Proposition 7,[0],[0]
"`p(−yi(a∗>xi + b∗)).
",B.6 Proof of Proposition 7,[0],[0]
"Therefore, for θ̃L : ‖θ̃L − θ∗L‖2 ≤ δ(ε) and any a ∈ Rd, b ∈ R
L̂n(a, θ̃L, b; p) = 1
n n∑ i=1",B.6 Proof of Proposition 7,[0],[0]
"`p(−yif(xi;a, θ̃L, b))",B.6 Proof of Proposition 7,[0],[0]
= 1 n n∑ i=1,B.6 Proof of Proposition 7,[0],[0]
"`p(−yi(a>xi + b))
",B.6 Proof of Proposition 7,[0],[0]
≥ 1 n n∑ i=1,B.6 Proof of Proposition 7,[0],[0]
`p(−yi(a∗>xi + b∗)),B.6 Proof of Proposition 7,[0],[0]
≥ 1 n n∑ i=1,B.6 Proof of Proposition 7,[0],[0]
"`p(−yif(xi; a∗0,θ∗L, b∗))",B.6 Proof of Proposition 7,[0],[0]
"= L̂n(a∗,θ∗L, b∗; p).
",B.6 Proof of Proposition 7,[0],[0]
"This means that (a∗,θ∗L, b ∗) is a local minima and f(xi;a ∗,θ∗L, b ∗)",B.6 Proof of Proposition 7,[0],[0]
=,B.6 Proof of Proposition 7,[0],[0]
a∗>xi + b ∗ for all i ∈,B.6 Proof of Proposition 7,[0],[0]
[n].,B.6 Proof of Proposition 7,[0],[0]
"This further indicates that
R̂n(θ ∗) ≥ min
w∈Rd,b∈R
1
n n∑ i=1",B.6 Proof of Proposition 7,[0],[0]
I{yi 6=,B.6 Proof of Proposition 7,[0],[0]
"sgn(w>xi + b)} > 0.
",B.6 Proof of Proposition 7,[0],[0]
Now we consider the following distribution PX×Y defined on the Rd.,B.6 Proof of Proposition 7,[0],[0]
"Let PX|Y=1 is a uniform distribution on the region [1, 2] ∪ [−2,−1] × {0}d−1 and PX|Y=−1 is a uniform distribution on the region {0} ×",B.6 Proof of Proposition 7,[0],[0]
"[1, 2] ∪ [−2,−1] × {0}d−2.",B.6 Proof of Proposition 7,[0],[0]
"In addition, let PY (Y = 1) = PY (Y = −1) = 0.5",B.6 Proof of Proposition 7,[0],[0]
"Clearly, r+ = r− = 1 < r = 2 and this distribution satisfies the assumptions in Theorem 1.",B.6 Proof of Proposition 7,[0],[0]
"Furthermore, with probability at least 1 − 1
4n−1 , there exists at least one sample in the following four regions:",B.6 Proof of Proposition 7,[0],[0]
"[1, 2]×{0}d−1, [−2,−1]×{0}d−1, {0}× [1, 2]×{0}d−2 and {0}× [−2,−1]×{0}d−2 and this makes the samples in the dataset not linearly separable.",B.6 Proof of Proposition 7,[0],[0]
"Example 2 Let the distribution PX×Y satisfy that P(Y = 1) = P(Y = −1) = 0.5, P(X = 5/4|Y = 1) = 1 and P(X|Y = −1) is a uniform distribution on the interval",B.7 Proof of Example 1,[0],[0]
"[0, 1].",B.7 Proof of Example 1,[0],[0]
"For a linear model f(x; a, b) = ax + b, a, b ∈ R, then every global minimum (a∗, b∗) of the population loss L(a, b)",B.7 Proof of Example 1,[0],[0]
= EX×Y,B.7 Proof of Example 1,[0],[0]
"[(1 − Y f(X; a, b))2] satisfies PX×Y",B.7 Proof of Example 1,[0],[0]
"[Y 6= sgn(f(X; a∗, b∗))]",B.7 Proof of Example 1,[0],[0]
"≥ 1/16.
",B.7 Proof of Example 1,[0],[0]
Proof: The proof is simple.,B.7 Proof of Example 1,[0],[0]
We first consider a simpler form of the problem.,B.7 Proof of Example 1,[0],[0]
"Given the distribution PX×Y , the optimal linear estimator Ê[Y |X] is
Ê[Y |X] = E[Y ]",B.7 Proof of Example 1,[0],[0]
"+ Cov(Y,X)V ar−1(X)(X − E[X]).
",B.7 Proof of Example 1,[0],[0]
"Since E[Y ] = 0, Cov(Y,X) =",B.7 Proof of Example 1,[0],[0]
"E[XY ]− E[X]E[Y ] = 1, V ar(X) > 0, E[X] = 7/8, the misclassification rate is 1/16.",B.7 Proof of Example 1,[0],[0]
"In this subsection, we present two counterexamples to show that neither Theorem 1 nor 2 holds if we replace the loss function with the quadratic loss.
",B.8 Proof of Example 3 and 4,[0],[0]
Example 3,B.8 Proof of Example 3 and 4,[0],[0]
"Let the distribution PX×Y defined on R2 × {−1, 1} satisfy that P(Y = 1)",B.8 Proof of Example 3 and 4,[0],[0]
"= P(Y = −1) = 0.5, P(X = (α, 0)|Y = 1) = P(X = (1, 0)|Y = 1) = 0.5 and P(X = (0, α)|Y = −1) = P(X = (0, 1)|Y = −1) = 0.5.",B.8 Proof of Example 3 and 4,[0],[0]
"Assume that samples in the dataset D = {(xi, yi)}4ni=1 are independently drawn from the distribution PX×Y .",B.8 Proof of Example 3 and 4,[0],[0]
"Assume that the network fS has M ≥ 2 neurons and all neurons in the network fS are quadratic neurons, i.e., σ(z) = z
2.",B.8 Proof of Example 3 and 4,[0],[0]
Then there exists an α ∈,B.8 Proof of Example 3 and 4,[0],[0]
"[0, 1] such that every global minimum of the empirical loss function L̂4n(θ)",B.8 Proof of Example 3 and 4,[0],[0]
= 1,B.8 Proof of Example 3 and 4,[0],[0]
4n ∑4n,B.8 Proof of Example 3 and 4,[0],[0]
"i=1(1− yif(xi;θ))2 has a training error greater than 1/8 with probability at least Ω(1/n3).
",B.8 Proof of Example 3 and 4,[0],[0]
Remark: This is a counterexample for Theorem 1.,B.8 Proof of Example 3 and 4,[0],[0]
"It is easy to check that the distribution satisfies assumption 2 and 3, where r = 2 > max{1, 1} = max{r+, r−}.",B.8 Proof of Example 3 and 4,[0],[0]
Proof: Let X =,B.8 Proof of Example 3 and 4,[0],[0]
"(X1, X2).",B.8 Proof of Example 3 and 4,[0],[0]
Set the feedforward network fD to a constant.,B.8 Proof of Example 3 and 4,[0],[0]
"Since the positive and negative samples locate on two orthogonal subspaces, then it is easy to check that under this distribution, for any quadratic function of the form g(X1, X2) = a1X 2 1 + a2X 2 2 + a0, there always exists a neural
network of the form f(X1, X2) = a0 + ∑M j=1 aj(wj1X1 + wj2X2) 2 = a0 + ∑M j=1 aj(w 2 j1X 2 1 + w 2 j2X 2 2 ), M ≥ 2 satisfying PX×Y (f(X) = g(X))",B.8 Proof of Example 3 and 4,[0],[0]
"= 1.
",B.8 Proof of Example 3 and 4,[0],[0]
"In addition, for any neural network f(X1, X2) = a0+ ∑M j=1 aj(wj1X1+wj2X2) 2, there exists a quadratic function of the form g(X1, X2) = a1X 2 1 + a2X 2 2 + a0 satisfying
PX×Y (f(X) = g(X))",B.8 Proof of Example 3 and 4,[0],[0]
"= 1.
",B.8 Proof of Example 3 and 4,[0],[0]
"This indicates that the optimal neural network f(x;θ∗) should be the solution of
min a0∈R,a∈R2
1
4n 4n∑ i=1",B.8 Proof of Example 3 and 4,[0],[0]
( 1− yi ( a0 +,B.8 Proof of Example 3 and 4,[0],[0]
"a1(x (1) i ) 2 + a2(x (2) i ) 2 )) .
",B.8 Proof of Example 3 and 4,[0],[0]
"Let n1, n2, n3 and n4 denote the number of samples at the point (α, 0), (1, 0), (0, α) and (0, 1), respectively.",B.8 Proof of Example 3 and 4,[0],[0]
We only need to focus the case where n1 = n2 = n3 = n4 = n.,B.8 Proof of Example 3 and 4,[0],[0]
"In this case, the optimal linear estimator should be of the form
g(X21 , X 2 2 ; a ∗ 0, a ∗ 1, a ∗ 2) = a ∗",B.8 Proof of Example 3 and 4,[0],[0]
1(X 2 1 − ÊX21 ) +,B.8 Proof of Example 3 and 4,[0],[0]
a∗2(X22,B.8 Proof of Example 3 and 4,[0],[0]
− ÊX22 ),B.8 Proof of Example 3 and 4,[0],[0]
"= a∗1 ( X21 − 1 + α2
4
) +",B.8 Proof of Example 3 and 4,[0],[0]
a∗2 ( X22,B.8 Proof of Example 3 and 4,[0],[0]
"− 1 + α2
4
) .
",B.8 Proof of Example 3 and 4,[0],[0]
"When α = 1/2, then 1+1/44 = 5/16 > 1/4",B.8 Proof of Example 3 and 4,[0],[0]
= α 2 and 1+1/44 = 5/16 < 1.,B.8 Proof of Example 3 and 4,[0],[0]
"Therefore, (1 +α 2)/4 ∈ (α2, 1).",B.8 Proof of Example 3 and 4,[0],[0]
"In this case, for any a∗1, a ∗ 2, the training error cannot be smaller than 1/4.",B.8 Proof of Example 3 and 4,[0],[0]
This can be easily seen by investigating positive and negative samples separately.,B.8 Proof of Example 3 and 4,[0],[0]
"For positive samples at (1, 0), the output of the network is g(1, 0; a∗0, a ∗ 1, a ∗ 2) = a ∗ 1(1− (1 +α2)/4).",B.8 Proof of Example 3 and 4,[0],[0]
"For positive samples at (α, 0), the output of the network is g(α, 0; a∗0, a ∗ 1, a ∗ 2) = a ∗ 1(α
2 − (1 + α2)/4).",B.8 Proof of Example 3 and 4,[0],[0]
Since α2 < 1+α24,B.8 Proof of Example 3 and 4,[0],[0]
"< 1, then if a∗1 6= 0, then the network will misclassify all samples at (α, 0) or (1, 0).",B.8 Proof of Example 3 and 4,[0],[0]
This indicates that a∗1 = 0 or training error is no smaller than 1/4.,B.8 Proof of Example 3 and 4,[0],[0]
"Using the same analysis on the negative samples, we will have a∗2 = 0 or training error is no smaller than 1/4.",B.8 Proof of Example 3 and 4,[0],[0]
"This indicates that the output of the network is a constant equal to zero, which has a training error 1/2.",B.8 Proof of Example 3 and 4,[0],[0]
"In all, the training error is no smaller than 1/4.",B.8 Proof of Example 3 and 4,[0],[0]
"The probability of the case where n1 = n2 = n3 = n4 is(
4n
2n
)( 2n
n )2 1 44n > 42n 2n+ 1",B.8 Proof of Example 3 and 4,[0],[0]
( 4n n+ 1 )2 1 44n = 1 (2n+ 1)(n+,B.8 Proof of Example 3 and 4,[0],[0]
"1)2
Example 4 Let the distribution PX×Y satisfy that P(Y = 1) = P(Y = −1) = 0.5, P(X = 1 + α|Y = 1) = P(X = 1 + 2α|Y = 1) = 0.5 and P(X = 0|Y = −1) = P(X = 1|Y = −1) = 0.5.",B.8 Proof of Example 3 and 4,[0],[0]
"Assume that samples in the dataset D = {(xi, yi)}4ni=1 are independently drawn from the distribution PX×Y .",B.8 Proof of Example 3 and 4,[0],[0]
Assume that the network fS has M ≥ 1 neurons and each neuron is a linear neuron σ(z) = z.,B.8 Proof of Example 3 and 4,[0],[0]
If α ∈,B.8 Proof of Example 3 and 4,[0],[0]
"[0, 1/6], then every global minimum of the empirical loss function L̂4n(θ) = 1",B.8 Proof of Example 3 and 4,[0],[0]
"4n ∑4n i=1(1 − yif(xi; θ))2 has a training error greater than 1/8 with probability at least Ω(1/n3).
",B.8 Proof of Example 3 and 4,[0],[0]
Remark: This is counterexample for Theorem 4.,B.8 Proof of Example 3 and 4,[0],[0]
"It is easy to check that distribution is linearly separable.
",B.8 Proof of Example 3 and 4,[0],[0]
"Proof: Let n−1, n1, n1+α denote the number of samples at the point −1, 1 and 1 + α.",B.8 Proof of Example 3 and 4,[0],[0]
"We only need to focus the case where n−1 = n, n1 = n and n1+α = 2n.",B.8 Proof of Example 3 and 4,[0],[0]
"Since the network is a linear network, then under this distribution, the optimal linear estimator should be of the form
f(x;θ) = a∗ ( x− 3 + 3α
4
) .
",B.8 Proof of Example 3 and 4,[0],[0]
"If a∗ = 0, then the training error is 1/2.",B.8 Proof of Example 3 and 4,[0],[0]
"If a∗ > 0, then the training error is 1/4, due to the misclassification of all points at x = 1.",B.8 Proof of Example 3 and 4,[0],[0]
"If a∗ < 0, then the training error is 3/4, due to the misclassification of all points at x = 1 + α and x = −1.",B.8 Proof of Example 3 and 4,[0],[0]
This means that the training error in this case should be greater or equal to 1/4.,B.8 Proof of Example 3 and 4,[0],[0]
"The probability of this case is(
4n
2n
)( 2n
n )2 1 44n > 42n 2n+ 1 ( 4n n+ 1 )2 1 44n = 1 (2n+ 1)(n+ 1)2",B.8 Proof of Example 3 and 4,[0],[0]
Proposition 20,B.9 Proof of Proposition 8,[0],[0]
Let f :,B.9 Proof of Proposition 8,[0],[0]
Rd → R denote a feedforward network parameterized by θ and let the dataset have n samples.,B.9 Proof of Proposition 8,[0],[0]
"When the loss function `p satisfies assumption 1 and p ≥ 1, we have minθ L̂n(θ; p) = 0",B.9 Proof of Proposition 8,[0],[0]
if and only if minθ R̂n(θ) = 0.,B.9 Proof of Proposition 8,[0],[0]
"Furthermore, if minθ R̂n(θ) = 0, every global minimum θ
∗ of the empirical loss L̂n(θ; p) has zero training error, i.e., R̂n(θ ∗) = 0.
",B.9 Proof of Proposition 8,[0],[0]
"Remark: Using the same proof shown as follows, we can show that Proposition 8 holds for any multilayer network architectures satisfying that for any set of parameters θ and any real numbers a, b ∈ R, there always exists a set of parameters θ̃ such that f(x; θ̃) = a(f(x;θ)− b) holds for all x.",B.9 Proof of Proposition 8,[0],[0]
"It is easy to check that both network architectures in Fig. 1 satisfy this condition.
",B.9 Proof of Proposition 8,[0],[0]
Proof: We first prove the “only if” part.,B.9 Proof of Proposition 8,[0],[0]
"The proof is trivial since, by definition `p(z) ≥ I{z ≥ 0}, then
R̂n(θ) = 1
n n∑ i=1",B.9 Proof of Proposition 8,[0],[0]
I{yi 6= sgn(f(xi;θ))},B.9 Proof of Proposition 8,[0],[0]
≤ 1 n n∑ i=1 I{yif(xi;θ) ≤ 0} ≤ 1 n,B.9 Proof of Proposition 8,[0],[0]
n∑ i=1,B.9 Proof of Proposition 8,[0],[0]
`p(−yif(xi;θ)),B.9 Proof of Proposition 8,[0],[0]
"= L̂n(θ; p).
",B.9 Proof of Proposition 8,[0],[0]
"Therefore, if minθ L̂n(θ; p) = 0 then minθ R̂n(θ) = 0.",B.9 Proof of Proposition 8,[0],[0]
"Next, we prove the “if” part.",B.9 Proof of Proposition 8,[0],[0]
"If minθ R̂n(θ) = 0, then there exists a set of parameter θ ∗ such that I{yi 6= sgn(f(x;θ∗))}",B.9 Proof of Proposition 8,[0],[0]
= 0 holds for all i ∈,B.9 Proof of Proposition 8,[0],[0]
[n].,B.9 Proof of Proposition 8,[0],[0]
This indicates that f(xi;θ∗) ≥ 0,B.9 Proof of Proposition 8,[0],[0]
"for all i : yi = 1 and f(xi;θ
∗)",B.9 Proof of Proposition 8,[0],[0]
< 0,B.9 Proof of Proposition 8,[0],[0]
for all i : yi = −1.,B.9 Proof of Proposition 8,[0],[0]
This means that there exists two real numbers c1 < c2 such that f(xi;θ ∗),B.9 Proof of Proposition 8,[0],[0]
>,B.9 Proof of Proposition 8,[0],[0]
c2 holds for all i : yi = 1 and f(xi;θ ∗),B.9 Proof of Proposition 8,[0],[0]
< c1 holds for all i : yi = −1.,B.9 Proof of Proposition 8,[0],[0]
"Now, we define a new network f(x; θ̃) = α(f(x;θ∗)",B.9 Proof of Proposition 8,[0],[0]
− c1+c22 ).,B.9 Proof of Proposition 8,[0],[0]
"Therefore, for this network f(x; θ̃), we have f(xi; θ̃) > α(c2",B.9 Proof of Proposition 8,[0],[0]
− c1)/2 holds for all i : yi = 1 and f(xi; θ̃) < −α(c2,B.9 Proof of Proposition 8,[0],[0]
− c1)/2 holds for all i : yi = −1.,B.9 Proof of Proposition 8,[0],[0]
"Since `p(z) = 0 iff z ≤ −z0, then by choosing α > 2z0c2−c1 , we have yif(xi; θ̃) > z0 holds for ∀i ∈",B.9 Proof of Proposition 8,[0],[0]
[n].,B.9 Proof of Proposition 8,[0],[0]
This means that L̂n(θ̃; p) = 0.,B.9 Proof of Proposition 8,[0],[0]
"Now we need to show that there exits a set of parameter θ̃ such that
f(x; θ̃) = α",B.9 Proof of Proposition 8,[0],[0]
"( f(x;θ∗)− c1 + c2
2
) .
",B.9 Proof of Proposition 8,[0],[0]
"Since the output of the neural network can be written as
f(x;θ) = a0 + ML∑",B.9 Proof of Proposition 8,[0],[0]
"j=1 ajσ(w > j Φ(x;θ) + bj),
where ML denotes the number of neurons in the last layer and Φ(xi;θ) denotes the outputs from the previous layers.",B.9 Proof of Proposition 8,[0],[0]
"Then by shifting a0 and scaling aj , we have
f(x; θ̃) = α",B.9 Proof of Proposition 8,[0],[0]
"( f(x;θ∗)− c1 + c2
2 )",B.9 Proof of Proposition 8,[0],[0]
= a∗0,B.9 Proof of Proposition 8,[0],[0]
"− α(c1 + c2)
2 + ML∑ j=1 αa∗jσ(w ∗>Φ(x;θ∗) + b∗j )
",B.9 Proof of Proposition 8,[0],[0]
= ã0 + ML∑,B.9 Proof of Proposition 8,[0],[0]
j=1 ãjσ(w ∗>Φ(x;θ∗),B.9 Proof of Proposition 8,[0],[0]
+ b∗j,B.9 Proof of Proposition 8,[0],[0]
").
",B.9 Proof of Proposition 8,[0],[0]
"Therefore, this means that there exists a set of parameters θ̃ such that L̂n(θ̃; p) = 0, i.e., minθ L̂n(θ; p) = 0.",B.9 Proof of Proposition 8,[0],[0]
"This means, the global minimum of the empirical loss L̂n(θ; p) is zero.",B.9 Proof of Proposition 8,[0],[0]
"Furthermore, since R̂n(θ) ≤ L̂n(θ; p) holds for all θ, then every global minimum of the empirical loss has zero training error.",B.9 Proof of Proposition 8,[0],[0]
"Proposition 21 Assume that the loss function is the logistic loss, i.e., `(z) = log2(1",B.10 Proof of Proposition 9,[0],[0]
+ e z).,B.10 Proof of Proposition 9,[0],[0]
Assume that assumptions 2-5 are satisfied.,B.10 Proof of Proposition 9,[0],[0]
Assume,B.10 Proof of Proposition 9,[0],[0]
"that samples in the dataset D = {(xi, yi)}ni=1, n",B.10 Proof of Proposition 9,[0],[0]
≥ 1 are independently drawn from the distribution PX×Y .,B.10 Proof of Proposition 9,[0],[0]
"Assume that the number of neurons M in the network fS satisfies M ≥ 2 max{ n∆r , r+, r−}, where ∆r = r−max{r+, r−}.",B.10 Proof of Proposition 9,[0],[0]
"If a set of real parameters θ∗ denotes a critical point of the empirical loss L̂n(θ), then θ ∗ is a saddle point.
",B.10 Proof of Proposition 9,[0],[0]
Proof: We first recall some notations defined in the paper.,B.10 Proof of Proposition 9,[0],[0]
"The output of the neural network is
f(x;θ) = fS(x;θS) + fD(x;θD),
where fS(x;θS) is the single layer neural network parameterized by θS , i.e.,
fS(x;θS)",B.10 Proof of Proposition 9,[0],[0]
"= a0 + M∑ j=1 ajσ ( w>j x ) ,
and fD(x;θD) is a deep neural network parameterized by θD.",B.10 Proof of Proposition 9,[0],[0]
"The empirical loss function is given by
L̂n(θ) = L̂n(θS ,θD) = 1
n n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
"`(−yif(xi;θ)).
",B.10 Proof of Proposition 9,[0],[0]
"We assume that there exists a local minimum θ∗ = (θ∗S ,θ ∗ D).",B.10 Proof of Proposition 9,[0],[0]
"We next complete the proof by proving the following two claims:
Claim 6",B.10 Proof of Proposition 9,[0],[0]
If there exists j ∈,B.10 Proof of Proposition 9,[0],[0]
"[M ] such that a∗j = 0, then θ∗ is not a local minimum.
",B.10 Proof of Proposition 9,[0],[0]
Claim 7,B.10 Proof of Proposition 9,[0],[0]
If a∗j 6= 0 for all j ∈,B.10 Proof of Proposition 9,[0],[0]
"[M ], then θ∗ is not a local minimum.
",B.10 Proof of Proposition 9,[0],[0]
"Therefore, these two claims contradict with the assumption that θ∗ = (θ∗S ,θ ∗ D) is a local minimum.",B.10 Proof of Proposition 9,[0],[0]
"Therefore, every critical point is not a local minimum.",B.10 Proof of Proposition 9,[0],[0]
"In addition, it is very easy to show that every critical point is not a local maximum, since the loss function is strictly convex with respect to a0.",B.10 Proof of Proposition 9,[0],[0]
"Therefore, every critical point is a saddle point.",B.10 Proof of Proposition 9,[0],[0]
(a) Proof of Claim 6.,B.10 Proof of Proposition 9,[0],[0]
"In this part, we prove that if there exists j ∈",B.10 Proof of Proposition 9,[0],[0]
"[M ] such that a∗j = 0, then θ∗ is not a local minima.",B.10 Proof of Proposition 9,[0],[0]
"Without loss of generality, we assume that a∗1 = 0.",B.10 Proof of Proposition 9,[0],[0]
"Using the same analysis presented in the proof of Theorem 1, we have
n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
`′(−yif(xi;θ))(−yi)σ′′,B.10 Proof of Proposition 9,[0],[0]
( w∗1 >xi ) xix >,B.10 Proof of Proposition 9,[0],[0]
"i = 0d×d.
",B.10 Proof of Proposition 9,[0],[0]
"By assumption that there exists a set of orthogonal basis E = {e1, ..., ed} in Rd and a subset U+ ⊆ E such that PX|Y (X ∈ Span(U1)|Y = 1) = 1 and by assumption that r = |U+ ∪ U−| > max{r+, r−} = max{|U+|, |U−|}, then the set U+\U− is not an empty set.",B.10 Proof of Proposition 9,[0],[0]
"It is easy to show that for any vector v ∈ U+\U−, PX×Y (v>X = 0|Y = 1) = 0.",B.10 Proof of Proposition 9,[0],[0]
We prove it by contradiction.,B.10 Proof of Proposition 9,[0],[0]
"If we assume p = PX×Y (v>X = 0|Y = 1) > 0, then for random vectors X1, ...,X|U+| independently drawn from the conditional distribution PX|Y=1,
PX|Y=1 |U+|⋃ i=1",B.10 Proof of Proposition 9,[0],[0]
{ v>Xi = 0 } ∣∣∣∣∣Y,B.10 Proof of Proposition 9,[0],[0]
= 1  = |U+|∏ i=1,B.10 Proof of Proposition 9,[0],[0]
"PX|Y=1 ( v>Xi = 0|Y = 1 ) = p|U+| > 0.
",B.10 Proof of Proposition 9,[0],[0]
"Furthermore, since X1, ...,X|U+| ∈ Span(U+), v>Xi = 0, i = 1, ..., |U+| and v ∈ U+, then the rank of the matrix ( X1, ...,X|U+| ) is at most |U+| − 1 and this indicates that the matrix is not a full rank matrix with probability p|U+| > 0.",B.10 Proof of Proposition 9,[0],[0]
This leads to the contradiction with the Assumption 2.,B.10 Proof of Proposition 9,[0],[0]
"Thus, with probability 1, v>xi 6= 0 for all i : yi = 1 and v>xi = 0 for all i : yi = −1.",B.10 Proof of Proposition 9,[0],[0]
Proof of Claim 7,B.10 Proof of Proposition 9,[0],[0]
:,B.10 Proof of Proposition 9,[0],[0]
Now we have proved that a∗j 6= 0 for all j ∈,B.10 Proof of Proposition 9,[0],[0]
[M ].,B.10 Proof of Proposition 9,[0],[0]
"Here, we define M0 = dM/2e.",B.10 Proof of Proposition 9,[0],[0]
"Since M0 ≥ max{r+, r−}, and max{r+, r−}+ min{r+, r−} ≥ r, then
2M0 ≥ 2 max{r+, r−} > 2r",B.10 Proof of Proposition 9,[0],[0]
− r+ − r− ≥ 2 min{r,B.10 Proof of Proposition 9,[0],[0]
"− r+, r − r−} , 2K.
Thus, there exists ai1 , ..., aiM0 , i1 < i2 < ...",B.10 Proof of Proposition 9,[0],[0]
"< iM0 such that
sgn(ai1) = ...",B.10 Proof of Proposition 9,[0],[0]
"= sgn(aiM0 ).
",B.10 Proof of Proposition 9,[0],[0]
"Without loss of generality, we assume that sgn(a1) = ...",B.10 Proof of Proposition 9,[0],[0]
= sgn(aM0) = +1.,B.10 Proof of Proposition 9,[0],[0]
Now we prove the claim 7.,B.10 Proof of Proposition 9,[0],[0]
"First, we consider the Hessian matrix H(w∗1, ...,w ∗ M0 ).",B.10 Proof of Proposition 9,[0],[0]
Since θ∗ is a local minima with R̂n(θ ∗),B.10 Proof of Proposition 9,[0],[0]
"> 0, then
F (u1, ...,uM0) = M0∑ j=1 M0∑ k=1",B.10 Proof of Proposition 9,[0],[0]
"u>j ∇2wj ,wk L̂n(θ ∗)uk ≥ 0
holds for any vectors u1, ...,uM0 ∈ Rd.",B.10 Proof of Proposition 9,[0],[0]
"Since
∇2wj L̂n(θ∗) = a∗j n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
`′(−yif(xi;θ∗))(−yi)σ′′ ( w∗j >xi ) xix,B.10 Proof of Proposition 9,[0],[0]
>,B.10 Proof of Proposition 9,[0],[0]
"i
+ a∗j 2 n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
`′′(−yif(xi;θ∗)),B.10 Proof of Proposition 9,[0],[0]
[ σ′ ( w∗j >xi )]2 xix >,B.10 Proof of Proposition 9,[0],[0]
"i ,
and
∇2wj ,wk L̂n(θ ∗; p) =",B.10 Proof of Proposition 9,[0],[0]
a∗ja ∗ k n∑ i=1,B.10 Proof of Proposition 9,[0],[0]
`′′(−yif(xi;θ∗)),B.10 Proof of Proposition 9,[0],[0]
[ σ′ ( w∗j >xi )],B.10 Proof of Proposition 9,[0],[0]
[ σ′ ( w∗k >xi )],B.10 Proof of Proposition 9,[0],[0]
"xix > i .
",B.10 Proof of Proposition 9,[0],[0]
"Thus, we have for any u1, ...,uM0 ∈ Rd,
F (u1, ...,uM0) = −2",B.10 Proof of Proposition 9,[0],[0]
n∑ i=1,B.10 Proof of Proposition 9,[0],[0]
`′(−yif(xi;θ∗))yi M0∑ j=1,B.10 Proof of Proposition 9,[0],[0]
"[ a∗jσ ′′ (w∗jxi) (u>j xi)2] 
+ 4 n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
`′′(−yif(xi;θ∗)) M0∑ j=1 a∗jσ ′,B.10 Proof of Proposition 9,[0],[0]
( w∗j >xi )( u>j xi )2 .,B.10 Proof of Proposition 9,[0],[0]
"Now we find some coefficients α1, ..., αM0 , not all zero and vectors u1, ...,uM0 satisfying
M0∑ j=1 αjσ ′",B.10 Proof of Proposition 9,[0],[0]
"( w∗j >xi ) u>j xi = 0, ∀i ∈",B.10 Proof of Proposition 9,[0],[0]
"[n],
and ∀i : yi = −1 and ∀j ∈",B.10 Proof of Proposition 9,[0],[0]
"[M0], u>j xi = 0.
Since θ∗ is a local minima, then by Lemma 1, we have
n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi = 0d.
Consider the orthogonal vectors e1, ..., eK from the set of orthogonal basis e1, ..., ed satisfying that, with probability 1",B.10 Proof of Proposition 9,[0],[0]
", ∀j ∈ [K], ∀i : yi = −1, e>j xi = 0 and ∀i : yi = 1, e>j xi 6= 0.",B.10 Proof of Proposition 9,[0],[0]
"Then, considering the following set of linear equations
n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗1>xi) ( e>1 xi ) = 0, ..., n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗M0 >xi) ( e>1 xi ) = 0,
... n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗1>xi) ( e>Kxi ) = 0, ..., n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗M0 >xi) ( e>Kxi ) = 0.
",B.10 Proof of Proposition 9,[0],[0]
These equations can be rewritten in a matrix form σ′(w∗1,B.10 Proof of Proposition 9,[0],[0]
>x1) ( e>1 x1 ) ...,B.10 Proof of Proposition 9,[0],[0]
σ′(w∗1,B.10 Proof of Proposition 9,[0],[0]
>xn) ( e>1 xn ) ... ... ...,B.10 Proof of Proposition 9,[0],[0]
σ′(w∗M0 >x1) ( e>1 x1 ) ...,B.10 Proof of Proposition 9,[0],[0]
σ′(w∗M0 >xn) ( e>1 xn ) ... ... ...,B.10 Proof of Proposition 9,[0],[0]
σ′(w∗1,B.10 Proof of Proposition 9,[0],[0]
>x1),B.10 Proof of Proposition 9,[0],[0]
( e>Kx1 ) ... σ′(w∗1,B.10 Proof of Proposition 9,[0],[0]
>xn) ( e>Kxn ) ... ...,B.10 Proof of Proposition 9,[0],[0]
"...
σ′(w∗M0 >x1) ( e>Kx1 ) ...",B.10 Proof of Proposition 9,[0],[0]
"σ′(w∗M0 >xn) ( e>Kxn )  (KM0×n)︸ ︷︷ ︸
P
 `′(−y1f(x1;θ∗))y1 `′(−y2f(x2;θ∗))y2 ... ... ... ...
...",B.10 Proof of Proposition 9,[0],[0]
"`′(−ynf(x1;θ∗))yn  ︸ ︷︷ ︸
q
= 0n
or Pq = 0n.
",B.10 Proof of Proposition 9,[0],[0]
"Since M0K ≥ MK/2 ≥ n, then if rank(P )",B.10 Proof of Proposition 9,[0],[0]
"= n, we should have q = 0n and this indicates that `′(−yif(xi;θ∗))",B.10 Proof of Proposition 9,[0],[0]
= 0,B.10 Proof of Proposition 9,[0],[0]
for all i ∈,B.10 Proof of Proposition 9,[0],[0]
[n] and this contradicts with the fact that `′(z) =,B.10 Proof of Proposition 9,[0],[0]
"11+e−z > 0 for all z ∈ R. Therefore, rank(P )",B.10 Proof of Proposition 9,[0],[0]
"< n ≤ M0K. This means the raw vectors of the matrix P is linearly dependent and thus we have that there exists coefficients vectors (β11, ..., β1K), ..., (βM01, ..., βM0K), not all zero vectors, such that
K∑ s=1 M0∑ j=1 σ′(w∗j >",B.10 Proof of Proposition 9,[0],[0]
"xi)βjs(e > s xi) = 0, ∀i ∈",B.10 Proof of Proposition 9,[0],[0]
"[n],
or M0∑ j=1 a∗jσ ′(w∗j >xi)
( 1
a∗j K∑ s=1 βjses
)>",B.10 Proof of Proposition 9,[0],[0]
xi,B.10 Proof of Proposition 9,[0],[0]
"= 0, ∀i ∈",B.10 Proof of Proposition 9,[0],[0]
"[n],
Define uj = 1 a∗j
∑K s=1 βjses for j = 1, ...,M0, then we have
M0∑ j=1 a∗jσ ′(w∗j",B.10 Proof of Proposition 9,[0],[0]
>,B.10 Proof of Proposition 9,[0],[0]
xi)u,B.10 Proof of Proposition 9,[0],[0]
>,B.10 Proof of Proposition 9,[0],[0]
"j xi = 0, ∀i ∈",B.10 Proof of Proposition 9,[0],[0]
[n].,B.10 Proof of Proposition 9,[0],[0]
"(17)
Furthermore, since uj ∈ Span({e1, ..., eK}), and with probability 1, ∀i : yi = −1 and ∀j ∈ [K], e>j xi = 0, then we have that ∀j ∈",B.10 Proof of Proposition 9,[0],[0]
[M ] and ∀i : yi = −1: u>j xi = 0.,B.10 Proof of Proposition 9,[0],[0]
"Thus,
F (u1, ...,uM0) = −2 n∑ i=1",B.10 Proof of Proposition 9,[0],[0]
`′(−yif(xi;θ∗))yi M0∑ j=1,B.10 Proof of Proposition 9,[0],[0]
"[ a∗jσ ′′ (w∗jxi) (u>j xi)2]  by Eq. (17)
= −2",B.10 Proof of Proposition 9,[0],[0]
∑ i:yi=1 `′(−yif(xi;θ∗)),B.10 Proof of Proposition 9,[0],[0]
M0∑ j=1,B.10 Proof of Proposition 9,[0],[0]
[ a∗jσ ′′ (w∗jxi) (u>j xi)2]  ≥ 0.,B.10 Proof of Proposition 9,[0],[0]
"(18)
Since σ′′(z)",B.10 Proof of Proposition 9,[0],[0]
> 0,B.10 Proof of Proposition 9,[0],[0]
for all z ∈ R and a∗j > 0,B.10 Proof of Proposition 9,[0],[0]
for all j ∈,B.10 Proof of Proposition 9,[0],[0]
"[M0], then we have
`′(−yif(xi;θ∗))",B.10 Proof of Proposition 9,[0],[0]
M0∑ j=1,B.10 Proof of Proposition 9,[0],[0]
"[ a∗jσ ′′ (w∗jxi) (u>j xi)2] ≥ 0, ∀i : yi = 1 and this leads to F (u1, ...,uM0) ≤ 0.",B.10 Proof of Proposition 9,[0],[0]
Together with Eq.,B.10 Proof of Proposition 9,[0],[0]
"(18), we have
F (u1, ...,uM0) = 0
and thus
`′(−yif(xi;θ∗))",B.10 Proof of Proposition 9,[0],[0]
M0∑ j=1,B.10 Proof of Proposition 9,[0],[0]
"[ a∗jσ ′′ (w∗jxi) (u>j xi)2] = 0, ∀i : yi = 1. (19) Now we split the index {i ∈",B.10 Proof of Proposition 9,[0],[0]
"[n] : yi = 1} set into two disjoint subset C0, C1:
C0 = {i ∈",B.10 Proof of Proposition 9,[0],[0]
"[n] : yi = 1, and ∃j ∈",B.10 Proof of Proposition 9,[0],[0]
"[M0],u>j xi 6= 0}, C1 = {i ∈",B.10 Proof of Proposition 9,[0],[0]
[n] : yi = 1 and ∀j ∈,B.10 Proof of Proposition 9,[0],[0]
"[M0],u>j xi = 0}.
",B.10 Proof of Proposition 9,[0],[0]
"Clearly, for all i ∈ C0, by the fact that aj > 0 for all j ∈",B.10 Proof of Proposition 9,[0],[0]
[M0] and σ′′(z) > 0,B.10 Proof of Proposition 9,[0],[0]
"for all z ∈ R, we have
M0∑ j=1",B.10 Proof of Proposition 9,[0],[0]
"[ a∗jσ ′′ (w∗jxi) (u>j xi)2] > 0, and this leads to `′(−yif(xi;θ∗))",B.10 Proof of Proposition 9,[0],[0]
"= 0, ∀i ∈ C0, which contradict with the fact that `′(z) > 0",B.10 Proof of Proposition 9,[0],[0]
"for all z ∈ R. Therefore, C0 = ∅. Now we need to consider the index set C1.",B.10 Proof of Proposition 9,[0],[0]
"First, it is easy to show that with probability 1, |C1| < r+ ≤ M0.",B.10 Proof of Proposition 9,[0],[0]
"This is due to the fact that there exists a non-zero vector uj , such that u > j xi = 0 for all i ∈ C1 and that
uj ∈ Span({e1, ..., eK}).",B.10 Proof of Proposition 9,[0],[0]
"Therefore, u>j xi = ∑K s=1(u > j es)(x >",B.10 Proof of Proposition 9,[0],[0]
i es) = ∑r+ s=1(u >,B.10 Proof of Proposition 9,[0],[0]
j es)(x > i es) = 0 holds for all i ∈ C1.,B.10 Proof of Proposition 9,[0],[0]
"If |C1| ≥ r+, then with probability 1, the matrix e>1 x1 ...",B.10 Proof of Proposition 9,[0],[0]
e>r+x1... ... ...,B.10 Proof of Proposition 9,[0],[0]
e>1 xr+ ...,B.10 Proof of Proposition 9,[0],[0]
"e > r+xr+
 has the full rank equal to r+",B.10 Proof of Proposition 9,[0],[0]
and this makes u > j es = 0 for all s ∈,B.10 Proof of Proposition 9,[0],[0]
[k].,B.10 Proof of Proposition 9,[0],[0]
"This contradicts with the fact that uj ∈ Span({e1, ..., eK}) and uj is not a zero vector.",B.10 Proof of Proposition 9,[0],[0]
"Thus, |C1| < r+ ≤ M0.",B.10 Proof of Proposition 9,[0],[0]
"Now we consider
the function F , since ∀i ∈ C0 : `′(−yif(xi;θ∗))",B.10 Proof of Proposition 9,[0],[0]
"= 0, then for all u1, ...,uM0 ,
F (u1, ...,uM0) = −2",B.10 Proof of Proposition 9,[0],[0]
∑ i∈C1 `′(−yif(xi;θ∗)),B.10 Proof of Proposition 9,[0],[0]
M0∑ j=1,B.10 Proof of Proposition 9,[0],[0]
"[ a∗jσ ′′ (w∗jxi) (u>j xi)2] 
+ 4 ∑ i∈C1 `′′(−yif(xi;θ∗))",B.10 Proof of Proposition 9,[0],[0]
M0∑ j=1 a∗jσ ′,B.10 Proof of Proposition 9,[0],[0]
( w∗j >xi )( u>j xi )2,B.10 Proof of Proposition 9,[0],[0]
"Now we set uj = αje1, j = 1, ...,M0 for some scalar αj .",B.10 Proof of Proposition 9,[0],[0]
"Now we only need find α1, ..., αM0 such that
M0∑ j=1 αja ∗",B.10 Proof of Proposition 9,[0],[0]
jσ ′,B.10 Proof of Proposition 9,[0],[0]
"( w∗j >xi ) e>1 xi = 0, ∀i ∈ C1.
",B.10 Proof of Proposition 9,[0],[0]
"Since |C1| ≤M0 − 1 < M0, then there exists α∗1, ..., α∗M0 , not all zeros, such that
M0∑ j=1 α∗ja ∗",B.10 Proof of Proposition 9,[0],[0]
jσ ′,B.10 Proof of Proposition 9,[0],[0]
"( w∗j >xi ) e>1 xi = 0, ∀i ∈ C1.
",B.10 Proof of Proposition 9,[0],[0]
"Then by setting uj = α ∗ je1, we have
F (u1, ...,uM0) = −2",B.10 Proof of Proposition 9,[0],[0]
∑ i∈C1 `′(−yif(xi;θ∗)),B.10 Proof of Proposition 9,[0],[0]
M0∑ j=1 [ |α∗j,B.10 Proof of Proposition 9,[0],[0]
"|2a∗jσ′′ ( w∗jxi ) ( e>1 xi )2] ≥ 0. .
",B.10 Proof of Proposition 9,[0],[0]
"Similarly, since |α1|, ..., |αM0",B.10 Proof of Proposition 9,[0],[0]
"| are not all zeros, a∗j > 0",B.10 Proof of Proposition 9,[0],[0]
for all j ∈,B.10 Proof of Proposition 9,[0],[0]
"[M0], σ′′(z) > 0 for all z ∈ R and e>1 xi 6= 0 holds for all i with probability 1, then
`′(−yif(xi;θ∗))",B.10 Proof of Proposition 9,[0],[0]
"= 0, ∀i ∈ C1.
",B.10 Proof of Proposition 9,[0],[0]
"Therefore, this indicates that `′(−yif(xi;θ∗))",B.10 Proof of Proposition 9,[0],[0]
"= 0, ∀i : yi = 1.
",B.10 Proof of Proposition 9,[0],[0]
"Since `′(z) > 0 holds for all z ∈ R, then this leads to the contradiction.",B.10 Proof of Proposition 9,[0],[0]
"Therefore, θ∗ is not a local minima.",B.10 Proof of Proposition 9,[0],[0]
"Proposition 13 Assume that the loss function ` is the logistic loss, i.e., `(z) = log2(1 + e z).",B.11 Proof of Proposition 13,[0],[0]
Assume that the network architecture satisfies assumption 4.,B.11 Proof of Proposition 13,[0],[0]
Assume,B.11 Proof of Proposition 13,[0],[0]
"that samples in the dataset D = {(xi, yi)}ni=1, n",B.11 Proof of Proposition 13,[0],[0]
≥ 1 are independently drawn from a distribution satisfying assumption 6.,B.11 Proof of Proposition 13,[0],[0]
Assume that the single layer network fS has M ≥ 1 neurons and neurons σ in the network fS are twice differentiable and satisfy σ′(z),B.11 Proof of Proposition 13,[0],[0]
> 0,B.11 Proof of Proposition 13,[0],[0]
for all z ∈ R.,B.11 Proof of Proposition 13,[0],[0]
"If a set of real parameters θ∗ = (θ∗S ,θ∗D) denotes a local minimum of the loss function L̂n(θS ,θD; p), p ≥ 3, then R̂n(θ∗S ,θ∗D) = 0 holds with probability one.
",B.11 Proof of Proposition 13,[0],[0]
"Proof: We first prove that, if a set of real parameters θ∗ denotes a critical point, then θ∗ is a saddle point.",B.11 Proof of Proposition 13,[0],[0]
We prove it by contradiction.,B.11 Proof of Proposition 13,[0],[0]
We assume that θ∗ denotes a local minima.,B.11 Proof of Proposition 13,[0],[0]
"By assumption that θ∗ = (θ∗1,θ ∗ 2) is a local minima and by the necessary condition presented in Lemma 1, we have
n∑ i=1",B.11 Proof of Proposition 13,[0],[0]
"`′(−yif(xi;θ∗))yiσ′(w∗j>xi)xi = 0d.
",B.11 Proof of Proposition 13,[0],[0]
"Thus, for any w ∈ Rd, we have n∑ i=1",B.11 Proof of Proposition 13,[0],[0]
"`′(−yif(xi;θ∗))σ′(w∗j>xi)yi(w>xi) = 0.
",B.11 Proof of Proposition 13,[0],[0]
"Furthermore, for the cross entropy loss function, we have
`′(z) = 1
1 + exp(−z)",B.11 Proof of Proposition 13,[0],[0]
"> 0, ∀z ∈ R.
",B.11 Proof of Proposition 13,[0],[0]
"Thus, by assumption that σ′(z) > 0 for all z ∈ R and assumption that there exists a vector w ∈ Rd such that PX×Y (Y (w>X) > 0) = 1, then there exists a constant c such that for all samples in the dataset",B.11 Proof of Proposition 13,[0],[0]
i ∈,B.11 Proof of Proposition 13,[0],[0]
"[n],
yiw >xi > c > 0.
",B.11 Proof of Proposition 13,[0],[0]
"Thus, we have
0 = n∑ i=1",B.11 Proof of Proposition 13,[0],[0]
`′(−yif(xi;θ∗))σ′(w∗j>xi)yi(w>xi) ≥ c n∑ i=1,B.11 Proof of Proposition 13,[0],[0]
"`′(−yif(xi;θ∗))σ′(w∗j>xi) > 0,
and this leads to the contradiction.",B.11 Proof of Proposition 13,[0],[0]
"Proposition 10 Assume the dataset D = {(xi, yi)}ni=1 is consisted of both positive and negative samples.",B.12 Proof of Proposition 10,[0],[0]
Assume that f(x;θ) is a feedforward network parameterized by θ.,B.12 Proof of Proposition 10,[0],[0]
"Assume that the loss function is logistic, i.e., `(z) = log2 (1 + e
z).",B.12 Proof of Proposition 10,[0],[0]
"If the real parameters θ∗ denote a critical point of the empirical loss L̂n(θ ∗), then R̂n(θ ∗)",B.12 Proof of Proposition 10,[0],[0]
"> 0.
",B.12 Proof of Proposition 10,[0],[0]
Proof: We prove a general statement claiming that the proposition 10 holds for all differentiable loss functions satisfying `′(z),B.12 Proof of Proposition 10,[0],[0]
> 0,B.12 Proof of Proposition 10,[0],[0]
for all z ∈ R.,B.12 Proof of Proposition 10,[0],[0]
"We note that the following claim holds under the assumptions in Proposition 10.
",B.12 Proof of Proposition 10,[0],[0]
Claim 8,B.12 Proof of Proposition 10,[0],[0]
"If the loss function is differentiable and satisfies `′(z) > 0 for all z ∈ R, then R̂n(θ∗) > 0.
Assume that the multilayer neural network f(x;θ) has L ≥ 1 hidden layers, Ml ≥ 1 neurons in the l-th layer.",B.12 Proof of Proposition 10,[0],[0]
Now we let the vector θl contain all parameters in the first l ∈,B.12 Proof of Proposition 10,[0],[0]
[L] layers.,B.12 Proof of Proposition 10,[0],[0]
"Then the output of the neural network can be rewritten as
f(x; a0,θL) = a0 + ML∑",B.12 Proof of Proposition 10,[0],[0]
j=1 ajσ(w > j Φ(x;θL−1),B.12 Proof of Proposition 10,[0],[0]
"+ bj),
where Φ(x;θL−1)",B.12 Proof of Proposition 10,[0],[0]
=,B.12 Proof of Proposition 10,[0],[0]
"(Φ1(x;θL−1), ...,ΦML−1(x;θL−1)) denotes the outputs of the neurons in the layer L− 1.",B.12 Proof of Proposition 10,[0],[0]
"Then the empirical loss is defined as
L̂n(θ)",B.12 Proof of Proposition 10,[0],[0]
"= 1
n n∑ i=1",B.12 Proof of Proposition 10,[0],[0]
"`(−yif(xi;θ))
",B.12 Proof of Proposition 10,[0],[0]
"If the point θ∗ = (a∗0,θ ∗ L) denotes a critical point of the empirical loss function, then we should have, for ∀j ∈",B.12 Proof of Proposition 10,[0],[0]
"[ML],
∂L̂n(θ ∗)
∂a0 =
1
n n∑ i=1",B.12 Proof of Proposition 10,[0],[0]
"`′(−yif(xi;θ∗))(−yi) = 0, (20)
∂L̂n(θ ∗)
",B.12 Proof of Proposition 10,[0],[0]
"∂aj =
1
n n∑ i=1",B.12 Proof of Proposition 10,[0],[0]
"`′(−yif(xi;θ∗))(−yi)σ ( w∗j >Φ(xi;θ ∗ L−1) + bj ) = 0. (21)
",B.12 Proof of Proposition 10,[0],[0]
"In addition, by adding Equations (20) and (21), we have
0 = a∗0 ∂L̂n(θ
∗)
∂a0",B.12 Proof of Proposition 10,[0],[0]
+ ML∑,B.12 Proof of Proposition 10,[0],[0]
j=1 a∗j ∂L̂n(θ ∗),B.12 Proof of Proposition 10,[0],[0]
∂aj = 1 n n∑ i=1,B.12 Proof of Proposition 10,[0],[0]
`′(−yif(xi;θ∗))(−yi) a∗0 + ML∑ j=1 a∗jσ ( w∗j >Φ(xi;θ ∗ L−1) + bj ),B.12 Proof of Proposition 10,[0],[0]
"= 1
n n∑ i=1",B.12 Proof of Proposition 10,[0],[0]
`′(−yif(xi;θ∗))(−yi)f(xi;θ∗).,B.12 Proof of Proposition 10,[0],[0]
"(22)
",B.12 Proof of Proposition 10,[0],[0]
"This indicates that if θ∗ is a critical point of the empirical loss, then the following equation should hold,
1
n n∑ i=1",B.12 Proof of Proposition 10,[0],[0]
`′(−yif(xi;θ∗))yif(xi;θ∗) = 0.,B.12 Proof of Proposition 10,[0],[0]
"(23)
However, if the dataset contains both positive and the negative samples, `′(z)",B.12 Proof of Proposition 10,[0],[0]
> 0,B.12 Proof of Proposition 10,[0],[0]
"for all z ∈ R, then this means that if R̂n(θ ∗) = 0, then
1
n n∑ i=1",B.12 Proof of Proposition 10,[0],[0]
`′(−yif(xi;θ∗))yif(xi;θ∗),B.12 Proof of Proposition 10,[0],[0]
> 0.,B.12 Proof of Proposition 10,[0],[0]
"(24)
We note here that the assumption that the dataset contains both positive and the negative samples is to ensure that when R̂n(θ ∗) = 0, there is at least one sample in the dataset satisfying
yif(xi;θ ∗)",B.12 Proof of Proposition 10,[0],[0]
"> 0.
",B.12 Proof of Proposition 10,[0],[0]
"Therefore, we have the contradiction.",B.12 Proof of Proposition 10,[0],[0]
This indicates that R̂n(θ ∗),B.12 Proof of Proposition 10,[0],[0]
> 0.,B.12 Proof of Proposition 10,[0],[0]
"Proposition 11 Assume that assumptions 1, 4 and 5 are satisfied.",B.13 Proof of Proposition 11,[0],[0]
"For any feedforward architecture fD(x;θD), every local minimum θ ∗ = (θ∗S ,θ ∗ D) of the empirical loss function L̂n(θS ,θD; p), p ≥ 6 satisfies R̂n(θ ∗)",B.13 Proof of Proposition 11,[0],[0]
= 0,B.13 Proof of Proposition 11,[0],[0]
"only if the matrix ∑n i=1 λiyixix > i is neither positive nor negative definite for all
sequences {λi ≥ 0}ni=1",B.13 Proof of Proposition 11,[0],[0]
satisfying ∑ i:yi=1 λi = ∑ i:yi=−1 λi > 0 and ‖ ∑n i=1,B.13 Proof of Proposition 11,[0],[0]
"λiyixi‖2 = 0.
",B.13 Proof of Proposition 11,[0],[0]
Proof: We prove Proposition 11 by proving the following claim.,B.13 Proof of Proposition 11,[0],[0]
Claim 9,B.13 Proof of Proposition 11,[0],[0]
If there exists a sequence {λi ≥ 0}ni=1 satisfying ∑ i:yi=1 λi = ∑ i:yi=−1 λi > 0 and ‖ ∑n i=1,B.13 Proof of Proposition 11,[0],[0]
"λiyixi‖2 =
0 such that the matrix ∑n
i=1",B.13 Proof of Proposition 11,[0],[0]
λiyixix,B.13 Proof of Proposition 11,[0],[0]
"> i is positive or negative positive definite, then there exists a feed-
forward neural architecture fD",B.13 Proof of Proposition 11,[0],[0]
"such that the empirical loss function L̂n(θS ,θD; p), p ≥ 6 has a local minimum with a non-zero training error.
",B.13 Proof of Proposition 11,[0],[0]
"Proof: Let D = {(xi, yi)}ni=1 denote a dataset consisting of n samples.",B.13 Proof of Proposition 11,[0],[0]
"We rewrite the sample x as x = ( x(1), ..., x(d) ) .",B.13 Proof of Proposition 11,[0],[0]
"Consider the following network,
f(x;θ) = fS(x;θS) + fD(x;θD),
where
fS(x;θS)",B.13 Proof of Proposition 11,[0],[0]
= a0 + M∑ j=1,B.13 Proof of Proposition 11,[0],[0]
"ajσ(w > j xi + bj),
and the multilayer network is defined as follows,
fD(x;θD) = fD(x; θ1, ..., θd) = n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
µi d∏,B.13 Proof of Proposition 11,[0],[0]
k=1 1 { x(k) ∈,B.13 Proof of Proposition 11,[0],[0]
"[ x (k) i − θk, x (k) i + θk ]} .",B.13 Proof of Proposition 11,[0],[0]
"(25)
We note here that µ1, ..., µn are not parameters and later we will show that this function can be implemented by a multilayer network consisted of threshold units.",B.13 Proof of Proposition 11,[0],[0]
"A useful property of the function fD(x;θD) is that if all parameters θis are positive and sufficiently smalls, then for each sample (xi, yi) in the dataset,
fD(xi;θD)",B.13 Proof of Proposition 11,[0],[0]
"= µi.
",B.13 Proof of Proposition 11,[0],[0]
"Furthermore, if we slightly perturb all parameters, the output of the function fD on all samples remain the same.",B.13 Proof of Proposition 11,[0],[0]
"In the proof, we use these two properties to construct the local minimum with a non-zero training error.
",B.13 Proof of Proposition 11,[0],[0]
"By assumption, there exists a sequence {λi ≥ 0}ni=1 satisfying ∑ i:yi=1 λi = ∑ i:yi=−1 λi > 0 and
‖∑ni=1 λiyixi‖2 = 0 such that the matrix ∑ni=1 λiyixix",B.13 Proof of Proposition 11,[0],[0]
>i is positive or negative positive definite.,B.13 Proof of Proposition 11,[0],[0]
"Without loss of generality, we assume that the matrix is positive definite.",B.13 Proof of Proposition 11,[0],[0]
Now we construct a local minimum θ∗. Let a∗0 = a ∗ 1 = ...,B.13 Proof of Proposition 11,[0],[0]
"= a ∗ M = −1, w∗1 = ...",B.13 Proof of Proposition 11,[0],[0]
= w∗M = 0d and b∗1 = ...,B.13 Proof of Proposition 11,[0],[0]
= b∗M = 0.,B.13 Proof of Proposition 11,[0],[0]
"Now we set θ∗1, ..., θ ∗ d to be positive and sufficiently small such that for two different samples in the dataset, e.g.,",B.13 Proof of Proposition 11,[0],[0]
"xi 6= xj , the following equations holds,
d∏ k=1 1",B.13 Proof of Proposition 11,[0],[0]
{,B.13 Proof of Proposition 11,[0],[0]
x (k) j ∈,B.13 Proof of Proposition 11,[0],[0]
"[ x (k) i − 2θ∗k, x (k) i + 2θ ∗",B.13 Proof of Proposition 11,[0],[0]
"k ]} = 0, d∏",B.13 Proof of Proposition 11,[0],[0]
k=1 1 { x (k) i ∈,B.13 Proof of Proposition 11,[0],[0]
"[ x (k) j − 2θ∗k, x (k) j + 2θ ∗",B.13 Proof of Proposition 11,[0],[0]
"k ]} = 0.
",B.13 Proof of Proposition 11,[0],[0]
"Now we choose µ1, ..., µn as follows.",B.13 Proof of Proposition 11,[0],[0]
"The output of the neural network on sample xi in the dataset is f(xi;θ
∗) = µi −Mσ(0).",B.13 Proof of Proposition 11,[0],[0]
"We need to choose µ1, ..., µn to satisfy all conditions shown as follows:
(1) There exists i ∈",B.13 Proof of Proposition 11,[0],[0]
[n] such that yi(µi −Mσ(0)),B.13 Proof of Proposition 11,[0],[0]
"< 0.
(2) For all i : yi = 1 and all k : yk = −1,
`′(−yi(µi −Mσ(0)))∑ j:j=1 ` ′(−yi(µi −Mσ(0)))",B.13 Proof of Proposition 11,[0],[0]
"= λi∑ j:j=1 λj , `′(−yk(µk −Mσ(0)))∑ j:j=−1 ` ′(−yi(µi −Mσ(0)))",B.13 Proof of Proposition 11,[0],[0]
"= λk∑ j:j=−1 λj ,
and ∑ j:j=1 `′(−yi(µi −Mσ(0)))",B.13 Proof of Proposition 11,[0],[0]
= ∑,B.13 Proof of Proposition 11,[0],[0]
"j:j=−1 `′(−yi(µi −Mσ(0))).
",B.13 Proof of Proposition 11,[0],[0]
Now we start from the largest element in the sequence {λi}ni=1.,B.13 Proof of Proposition 11,[0],[0]
"Since ∑n
i=1 λi > 0, the define the index imax as the index of the largest element, i.e.,
imax = arg max i λi.
Let λmax = λimax .",B.13 Proof of Proposition 11,[0],[0]
"Now we choose µimax such that
yimax(µimax −Mσ(0))",B.13 Proof of Proposition 11,[0],[0]
"= −1.
",B.13 Proof of Proposition 11,[0],[0]
"Thus, the index imax satisfy the first condition.",B.13 Proof of Proposition 11,[0],[0]
"Then for i 6= imax, we choose µi such that
`′(−yi(µi −Mσ(0)))",B.13 Proof of Proposition 11,[0],[0]
= λi λmax `(−yimax(µimax −Mσ(0))),B.13 Proof of Proposition 11,[0],[0]
= λi λmax `′(1) ≤,B.13 Proof of Proposition 11,[0],[0]
`′(1).,B.13 Proof of Proposition 11,[0],[0]
"(26)
We note here that for each",B.13 Proof of Proposition 11,[0],[0]
i ∈,B.13 Proof of Proposition 11,[0],[0]
"[n], there always exists a µi solving the above equation.",B.13 Proof of Proposition 11,[0],[0]
"This can be seen by the fact that `′ is continuous, `′p(z) ≥ 0 and `′p(z) = 0 iff z ≤ −z0.",B.13 Proof of Proposition 11,[0],[0]
"This indicates that for ∀z > −z0, `′p(z) > 0, i.e., `′(1) > 0 and that `′(−z0) = 0.",B.13 Proof of Proposition 11,[0],[0]
"Since `′(z) is continuous, then for ∀r ∈",B.13 Proof of Proposition 11,[0],[0]
"[0, `′(1)], there always exists z ∈ R such that `′(z) = r, which further indicates that for ∀i ∈",B.13 Proof of Proposition 11,[0],[0]
"[n], there always exists µi ∈ R solving the Equation (37).",B.13 Proof of Proposition 11,[0],[0]
"Under this construction, it is easy to show that the second condition is satisfied as well.",B.13 Proof of Proposition 11,[0],[0]
Now we only need to show that θ∗ is local minimum.,B.13 Proof of Proposition 11,[0],[0]
We first show that θ∗ is a critical point of the empirical loss function.,B.13 Proof of Proposition 11,[0],[0]
"Since for ∀j ∈ [M ],
∂L̂n(θ ∗)
",B.13 Proof of Proposition 11,[0],[0]
∂aj = n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
"`′(−yi(µi −Mσ(0)))(−yi)σ(0)
= σ(0) n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
λi λmax `′(1)(−yi) =,B.13 Proof of Proposition 11,[0],[0]
"− σ(0)`′(1) λmax n∑ i=1 yiλi
= 0 by ∑ i:yi=1 λi = ∑ i:yi=−1 λi
∇wj L̂n(θ∗) = n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
"`′(−yi(µi −Mσ(0)))(−yi)σ′(0)xi
= −σ′(0) n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
λi λmax `′(1)yixi = − σ′(0)`′(1) λmax n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
"λiyixi
= 0d by ∥∥∥∥∥ n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
"λiyixi ∥∥∥∥∥ 2 = 0
and ∂L̂n(θ ∗)
",B.13 Proof of Proposition 11,[0],[0]
∂a0 = n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
`′(−yi(µi −Mσ(0)))(−yi) =,B.13 Proof of Proposition 11,[0],[0]
− `′(1) λmax n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
"yiλi = 0.
",B.13 Proof of Proposition 11,[0],[0]
"In addition, we have stated earlier, if we slightly perturb the parameter θ∗k in the interval",B.13 Proof of Proposition 11,[0],[0]
"[θ ∗ k/2, 3θ ∗ k/2], the output of the function fD(xi;θD) does not change for all i ∈",B.13 Proof of Proposition 11,[0],[0]
"[n], then θ∗ is a critical point.",B.13 Proof of Proposition 11,[0],[0]
Now we show that θ∗ is local minimum.,B.13 Proof of Proposition 11,[0],[0]
"Consider any perturbation ∆a1, ...,∆aM : |∆aj | < 12 for all j ∈",B.13 Proof of Proposition 11,[0],[0]
"[M ], ∆w1, ...,∆wM ∈ Rd, ∆a0 ∈ R, ∆θk :",B.13 Proof of Proposition 11,[0],[0]
|∆θk| ≤ θk/2 for all k ∈,B.13 Proof of Proposition 11,[0],[0]
[n].,B.13 Proof of Proposition 11,[0],[0]
"Define
θ̃ = (a∗0 + ∆a0, ..., a ∗ M + ∆aM ,w ∗ 1 + ∆w1, ...,w ∗ M + ∆wM , θ ∗ 1 + ∆θ ∗ 1, ..., θ ∗ d + ∆θ ∗ d).
",B.13 Proof of Proposition 11,[0],[0]
"Then
n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
`(−yif(xi; θ̃))− n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
`(−yif(xi;θ∗)) = n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
[ `(−yif(xi; θ̃))− `(−yif(xi;θ∗)) ],B.13 Proof of Proposition 11,[0],[0]
"≥
n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
"`′(−yif(xi;θ∗))(−yi)[f(xi; θ̃)− f(xi;θ∗)].
",B.13 Proof of Proposition 11,[0],[0]
"Since for each sample xi in the dataset,
f(xi; θ̃)− f(xi;θ∗)",B.13 Proof of Proposition 11,[0],[0]
= ∆a0 + M∑ j=1 (a∗j + ∆aj)σ(∆w > j xi) +,B.13 Proof of Proposition 11,[0],[0]
µi,B.13 Proof of Proposition 11,[0],[0]
"− µi
= ∆a0 + M∑ j=1 (a∗j + ∆aj)σ(∆w > j xi),
then
n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
`(−yif(xi; θ̃))− n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
"`(−yif(xi;θ∗))
≥ n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
"`′(−yif(xi;θ∗))(−yi)[f(xi; θ̃)− f(xi;θ∗)]
= n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
"`′(−yi(µi −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) + ∆a0  =
n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
"λi` ′(1) λmax (−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) = `′(1)
λmax M∑ j=1 −(a∗j",B.13 Proof of Proposition 11,[0],[0]
+ ∆aj),B.13 Proof of Proposition 11,[0],[0]
"[ n∑ i=1 λiyiσ ( ∆w>j xi )] .
",B.13 Proof of Proposition 11,[0],[0]
"Now we define the following function G : Rd → R,
G(u) = n∑ i=1 λiyiσ ( u>xi ) .
",B.13 Proof of Proposition 11,[0],[0]
"Now we consider the gradient of the function G with respect to the vector u at the point 0d,
∇uG(0d) = n∑ i=1 λiyiσ ′",B.13 Proof of Proposition 11,[0],[0]
(0)xi = 0d by ∥∥∥∥∥ n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
"λiyixi ∥∥∥∥∥ 2 = 0.
",B.13 Proof of Proposition 11,[0],[0]
"Furthermore, the Hessian matrix ∇2uG(0d) satisfies
∇2uG(0d) = n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
λiyiσ ′′ (0)xix >,B.13 Proof of Proposition 11,[0],[0]
i = σ ′′ (0) n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
λiyixix >,B.13 Proof of Proposition 11,[0],[0]
"i 0,
then the function G(u)",B.13 Proof of Proposition 11,[0],[0]
=,B.13 Proof of Proposition 11,[0],[0]
∑n i=1 λiyiσ ( u>xi ) has a local minima at u = 0d.,B.13 Proof of Proposition 11,[0],[0]
"This indicates that there
exists ε > 0 such that for all (∆w1, ...,∆wM ) : √∑M
j=1 ‖∆wj‖22 ≤ ε, n∑ i=1 λiyiσ ( ∆w>j xi ) ≥ n∑ i=1 λiyiσ (0) = 0,
where the equality holds by the fact that ∑n
i=1 yiλi = 1.",B.13 Proof of Proposition 11,[0],[0]
"In addition, since a ∗ j = −1, |∆aj | < 12 , then
for all ∆wj : ‖∆wj‖2 ≤ ε and ∆bj ∈ R, n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
`(−yif(xi; θ̃))− n∑ i=1,B.13 Proof of Proposition 11,[0],[0]
`(−yif(xi;θ∗)),B.13 Proof of Proposition 11,[0],[0]
"≥ 0.
",B.13 Proof of Proposition 11,[0],[0]
"Thus, θ∗ is a local minima of the empirical loss function with f(xi;θ ∗) = µi −Mσ(0).",B.13 Proof of Proposition 11,[0],[0]
Since there exists a µimax such that yimax(µimax −Mσ(0)),B.13 Proof of Proposition 11,[0],[0]
"= 1, then this means that the neural network makes an incorrect prediction on the sample ximax .",B.13 Proof of Proposition 11,[0],[0]
"This indicates that this local minimum has a non-zero training error.
",B.13 Proof of Proposition 11,[0],[0]
"Finally, we present the way we construct the neural network fD. Since
fD(x;θD) = fD(x; θ1, ..., θd) = n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
µi d∏,B.13 Proof of Proposition 11,[0],[0]
k=1 1 { x(k) ∈,B.13 Proof of Proposition 11,[0],[0]
"[ x (k) i − θk, x (k) i + θk ]} .
",B.13 Proof of Proposition 11,[0],[0]
"Let σth denote the threshold unit, where σth(z) = 1 if z ≥ 0 and σth(z) = 0, otherwise.",B.13 Proof of Proposition 11,[0],[0]
"Therefore, the indicator function can be represented as follows:
1 { x(k) ∈",B.13 Proof of Proposition 11,[0],[0]
"[ x
(k) i − θk, x (k) i + θk",B.13 Proof of Proposition 11,[0],[0]
]} = σth ( x(k),B.13 Proof of Proposition 11,[0],[0]
− x(k)i + θk ),B.13 Proof of Proposition 11,[0],[0]
− σth ( x(k) − x(k)i,B.13 Proof of Proposition 11,[0],[0]
− θk ),B.13 Proof of Proposition 11,[0],[0]
"Therefore,
d∏ k=1 1 { x(k) ∈",B.13 Proof of Proposition 11,[0],[0]
"[ x (k) i − θk, x (k) i + θk",B.13 Proof of Proposition 11,[0],[0]
"]} = σth ( d∑
k=1
[ σth ( x(k) − x(k)i + θk )",B.13 Proof of Proposition 11,[0],[0]
− σth ( x(k) − x(k)i,B.13 Proof of Proposition 11,[0],[0]
− θk )],B.13 Proof of Proposition 11,[0],[0]
"− d+ 1
2
)
Therefore, we have
fD(x;θD) = n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
"µiσth
( d∑
k=1
[ σth ( x(k)",B.13 Proof of Proposition 11,[0],[0]
− x(k)i + θk ),B.13 Proof of Proposition 11,[0],[0]
− σth ( x(k) − x(k)i,B.13 Proof of Proposition 11,[0],[0]
− θk )],B.13 Proof of Proposition 11,[0],[0]
"− d+ 1
2
) .
",B.13 Proof of Proposition 11,[0],[0]
"It is very easy to see that this is a two layer network consisted of threshold units.
",B.13 Proof of Proposition 11,[0],[0]
"Furthermore, we note here that, in the proof shown above, we assume the only parameters in the network fD are θ1, ...,θd.",B.13 Proof of Proposition 11,[0],[0]
"In fact, we can prove a more general statement where the fD is of the form
fD(x;θD) = n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
"µiσth
( d∑
k=1
[ aikσth ( x(k) + uik ) + bikσth ( x(k) + vik )]",B.13 Proof of Proposition 11,[0],[0]
"+ ci ) ,
where aik, bik, uik, vik, ci, i ∈",B.13 Proof of Proposition 11,[0],[0]
"[n], k ∈",B.13 Proof of Proposition 11,[0],[0]
[d] are all parameters.,B.13 Proof of Proposition 11,[0],[0]
"We can show that the neural network
fD(x;θD) = n∑ i=1",B.13 Proof of Proposition 11,[0],[0]
"µiσth
( d∑
k=1
[ σth ( x(k)",B.13 Proof of Proposition 11,[0],[0]
− x(k)i + θk ),B.13 Proof of Proposition 11,[0],[0]
− σth ( x(k) − x(k)i,B.13 Proof of Proposition 11,[0],[0]
− θk )],B.13 Proof of Proposition 11,[0],[0]
"− d+ 1
2
) ,
denotes a local minimum, since any slight perturbations on parameters aik, bik, uik, vik, ci, i ∈",B.13 Proof of Proposition 11,[0],[0]
"[n], k ∈",B.13 Proof of Proposition 11,[0],[0]
[d] do not change the output of the neural network on the samples in the dataset D.,B.13 Proof of Proposition 11,[0],[0]
"In this subsection, we present two examples to show that if either assumption 2 or 3 is not satisfied, even if the other conditions in Theorem 1 are satisfied, Theorem 1 does not hold.
",B.14 Proof of Example 5,[0],[0]
"Example 5 Assume that the distribution PX×Y satisfies that PY (Y = 1) = PY (Y = −1), PX|Y (X = (1, 0)|Y = 1) = PX|Y (X = (−1, 0)|Y = 1) = 0.5 and PX|Y (X = (0, 0)|Y = −1).",B.14 Proof of Example 5,[0],[0]
"Assume that samples in the dataset D = {(xi, yi)}2ni=1 are independently drawn from the distribution PX×Y .",B.14 Proof of Example 5,[0],[0]
"Assume that the network fS has M ≥ 1 neurons and neurons in fS satisfy the condition that σ is analytic and has a positive second order derivative on R. There exists a feedforward network fD such that the empirical loss L̂n(θS ,θD) has a local minimum with non-zero training error with a probability at least Ω(1/n 2).
",B.14 Proof of Example 5,[0],[0]
"Remark: This is a counterexample where Theorem 1 does not hold, when Assumption 3 is satisfied and Assumption 2 is not satisfied.",B.14 Proof of Example 5,[0],[0]
This distribution can be viewed in the following way.,B.14 Proof of Example 5,[0],[0]
"The positive data samples are located on the linear span of the set {(1, 0)}, the negative data samples locate on the linear span of the set {(0, 1)} and all samples are located on the linear span of the set {(1, 0), (0, 1)}.",B.14 Proof of Example 5,[0],[0]
"Therefore, r = 2 > max{r+, r−} = 1.",B.14 Proof of Example 5,[0],[0]
This means that Assumption 3 is satisfied.,B.14 Proof of Example 5,[0],[0]
"In addition, it is easy to check that Assumption 2 is not satisfied, since the matrix (0, 0) has rank zero and thus does not have a full rank.",B.14 Proof of Example 5,[0],[0]
"This means that our main results may not hold when the assumption 2 is not satisfied.
",B.14 Proof of Example 5,[0],[0]
"Proof: Let n1, n0, n−1 denote the number of samples at the point (1, 0), (0, 0), (−1, 0), respectively.",B.14 Proof of Example 5,[0],[0]
It is easy to see that the event that n1 =,B.14 Proof of Example 5,[0],[0]
n−1 > 0 and n0 > 0,B.14 Proof of Example 5,[0],[0]
"happens with probability at least Ω(1/n
2).",B.14 Proof of Example 5,[0],[0]
"We note that this is not a tight bounded, however, we just need to show that this happens with a positive probability.",B.14 Proof of Example 5,[0],[0]
Now we consider the optimization problem under the dataset where n1 =,B.14 Proof of Example 5,[0],[0]
n−1 > 0 and n0 > 0.,B.14 Proof of Example 5,[0],[0]
"We first set the feedforward network fD(x;θD) to constant, i.e., fD(x;θD) ≡ 0",B.14 Proof of Example 5,[0],[0]
for x ∈ R2.,B.14 Proof of Example 5,[0],[0]
"Now the whole network becomes a single layer network,
f(x;θ) = a0 + M∑ j=1 ajσ ( w>j x ) .
",B.14 Proof of Example 5,[0],[0]
Let a∗1 = ...,B.14 Proof of Example 5,[0],[0]
= a ∗ M = −1 and w∗1 = ... = w∗M = 02.,B.14 Proof of Example 5,[0],[0]
"Therefore, we have f(x;θ∗)",B.14 Proof of Example 5,[0],[0]
= a∗0 −Mσ(0).,B.14 Proof of Example 5,[0],[0]
"Let a∗0 be the global optimizer of the following convex optimization problem.
",B.14 Proof of Example 5,[0],[0]
min a 2n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`p(−yi(a−Mσ(0))).
",B.14 Proof of Example 5,[0],[0]
"Thus, we have n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi) = 0, (27)
and this indicates that∑ i:yi=1",B.14 Proof of Example 5,[0],[0]
`′p(−(a∗0 −Mσ(0))),B.14 Proof of Example 5,[0],[0]
= ∑,B.14 Proof of Example 5,[0],[0]
i:yi=−1 `′p(a ∗ 0 −Mσ(0)) or `′p(−a∗0 +Mσ(0))n+ =,B.14 Proof of Example 5,[0],[0]
"`′p(a∗0 −Mσ(0))n−.
(28)
",B.14 Proof of Example 5,[0],[0]
"In addition, since for ∀j ∈ [M ],
∂L̂n(θ ∗)
",B.14 Proof of Example 5,[0],[0]
∂aj = 2n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)σ(0) = 0, by Equation (27),
∇wj L̂n(θ∗) =",B.14 Proof of Example 5,[0],[0]
2n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)σ′(0)xi = 02, by ∑ i:yi=1 xi",B.14 Proof of Example 5,[0],[0]
= ∑,B.14 Proof of Example 5,[0],[0]
i:,B.14 Proof of Example 5,[0],[0]
"yi=−1 xi = 02,
and ∂L̂n(θ ∗)
",B.14 Proof of Example 5,[0],[0]
∂a0 = n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi) = 0,
then θ∗ is a critical point.",B.14 Proof of Example 5,[0],[0]
"Next we show that θ∗ = (a∗0, ..., a ∗ M ,w ∗ 1, ...,w ∗ M ) is a local minima.",B.14 Proof of Example 5,[0],[0]
"Consider any perturbation ∆a1, ...,∆aM : |∆aj | < 12 for all j ∈",B.14 Proof of Example 5,[0],[0]
"[M ], ∆w1, ...,∆wM ∈ R2 and ∆a0 ∈ R. Define
θ̃ = (a∗0 + ∆a0, ..., a ∗ M + ∆aM ,w ∗ 1 + ∆w1, ...,w ∗ M + ∆wM ).
",B.14 Proof of Example 5,[0],[0]
"Then
n∑ i=1",B.14 Proof of Example 5,[0],[0]
`p(−yif(xi; θ̃))− n∑ i=1,B.14 Proof of Example 5,[0],[0]
`p(−yif(xi;θ∗)),B.14 Proof of Example 5,[0],[0]
= n∑ i=1,B.14 Proof of Example 5,[0],[0]
[ `p(−yif(xi; θ̃))− `p(−yif(xi;θ∗)) ],B.14 Proof of Example 5,[0],[0]
"≥
n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)[f(xi; θ̃)− f(xi;θ∗)]
= n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)[f(xi; θ̃)− a∗0 +Mσ(0)]
= n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃),
where the inequality follows from the convexity of the loss function `p(z), the second equality follows from the fact that f(x;θ∗) ≡",B.14 Proof of Example 5,[0],[0]
a∗0 − Mσ(0) and the third equality follows from Equation (28).,B.14 Proof of Example 5,[0],[0]
"In addition, we have
n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃)
= n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) + ∆a0  =
n∑ i=1",B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) by Eq.,B.14 Proof of Example 5,[0],[0]
"(28) =
M∑ j=1 −(a∗j + ∆aj)",B.14 Proof of Example 5,[0],[0]
[ n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>j xi )]
= M∑ j=1 −(a∗j + ∆aj)",B.14 Proof of Example 5,[0],[0]
[ n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w (1) j x (1) i )] by x (2) i = 0,∀i ∈",B.14 Proof of Example 5,[0],[0]
"[n].
Now we define the following function G : R→ R,
G(u)",B.14 Proof of Example 5,[0],[0]
= n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ ( ux (1) i ) .
",B.14 Proof of Example 5,[0],[0]
"Now we consider the gradient of the function G with respect to the variable u at the point u = 0,
∇uG(0) = n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ′ (0)x(1)i = 0.
",B.14 Proof of Example 5,[0],[0]
"Furthermore, the second order derivative ∇2uG(0) satisfies
∇2uG(0) = n∑ i=1",B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))yiσ′′ (0) ( x (1) i )2 = σ′′ (0) n∑ i=1,B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))yi ( x (1) i )2 = σ′′(0)  1 n+ ∑ i:yi=1 ( x (1) i )2 − 1 n− ∑ i:,B.14 Proof of Example 5,[0],[0]
yi=−1 ( x (1),B.14 Proof of Example 5,[0],[0]
"i
)2 > 0, then the function G(u)",B.14 Proof of Example 5,[0],[0]
=,B.14 Proof of Example 5,[0],[0]
∑n i=1,B.14 Proof of Example 5,[0],[0]
`p(−yi(a∗0 −Mσ(0)))yiσ ( ux (1) i ) has a local minima at u = 0.,B.14 Proof of Example 5,[0],[0]
"This indicates that there exists ε > 0 such that for all ∆w : ‖∆w‖2 ≤ ε, n∑ i=1",B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>xi ) ≥ n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`p(−yi(a∗0 −Mσ(0)))yiσ (0) = 0.
",B.14 Proof of Example 5,[0],[0]
"In addition, since a∗j = −1, |∆aj | < 12 , then for all ∆wj : ‖∆wj‖2 ≤ ε, n∑ i=1",B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) = M∑ j=1 −(a∗j + ∆aj),B.14 Proof of Example 5,[0],[0]
[ n∑ i=1,B.14 Proof of Example 5,[0],[0]
`p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>j xi )],B.14 Proof of Example 5,[0],[0]
"≥ 0.
",B.14 Proof of Example 5,[0],[0]
"Therefore, we have n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) ≥ 0, and this indicates that n∑ i=1",B.14 Proof of Example 5,[0],[0]
`p(−yif(xi; θ̃))− n∑ i=1,B.14 Proof of Example 5,[0],[0]
`p(−yif(xi;θ∗)),B.14 Proof of Example 5,[0],[0]
≥ 0.,B.14 Proof of Example 5,[0],[0]
"Thus, θ∗ is a local minima with f(x;θ∗)",B.14 Proof of Example 5,[0],[0]
= a∗0 −Mσ(0) = constant.,B.14 Proof of Example 5,[0],[0]
"Thus,
1
n n∑ i=1",B.14 Proof of Example 5,[0],[0]
I{yi 6= sgn(f(xi;θ∗))},B.14 Proof of Example 5,[0],[0]
"≥ min{n−, n+} n .
",B.14 Proof of Example 5,[0],[0]
"Since the dataset is consisted of both positive and negative examples, then the training error is nonzero.
",B.14 Proof of Example 5,[0],[0]
Example 6 Assume that the distribution PX×Y satisfies that PY (Y = 1) = PY (Y = −1) and PX|Y (X = 2|Y = 1) = PX|Y (X = −1|Y = 1) = 0.5 and PX|Y (X = 0.5|Y = −1) = 1.,B.14 Proof of Example 5,[0],[0]
"Assume that samples in the dataset D = {(xi, yi)}2ni=1 are independently drawn from the distribution PX×Y .",B.14 Proof of Example 5,[0],[0]
Assume that the network fS has M ≥ 1 neurons and neurons in fS satisfy the condition that σ is analytic and has a positive second order derivative on R. There exists a feedforward network fD,B.14 Proof of Example 5,[0],[0]
"such that the empirical loss L̂n(θS ,θD) has a local minimum with non-zero training error with probability at least Ω(1/n2).
",B.14 Proof of Example 5,[0],[0]
"Remark: This is a counterexample where Theorem 1 does not hold, when Assumption 2 is satisfied and Assumption 3 is not satisfied.",B.14 Proof of Example 5,[0],[0]
This distribution can be viewed in the following way.,B.14 Proof of Example 5,[0],[0]
"The positive data samples locate on the linear span of the set {(1)}, the negative data samples locate on the linear span of the set {(1)} and all samples locate on the linear span of the set {(1)}.",B.14 Proof of Example 5,[0],[0]
It is easy to check that assumption 2 is satisfied.,B.14 Proof of Example 5,[0],[0]
"However, r = 1 = max{r+, r−} = 1.",B.14 Proof of Example 5,[0],[0]
"This means the assumption 3 is not satisfied.
",B.14 Proof of Example 5,[0],[0]
"Proof: Let n2, n−1, n0.5 denote the number of samples at the point (2), (−1), (0.5), respectively.",B.14 Proof of Example 5,[0],[0]
"It is easy to see that the event that n2 = n−1 > 0 and n0.5 > 0 happens with probability at least Ω(1/n
2).",B.14 Proof of Example 5,[0],[0]
"We note that this is not a tight bounded, however, we just need to show that this happens with a positive probability.",B.14 Proof of Example 5,[0],[0]
Now we consider the optimization problem under the dataset where n2 = n−1 > 0 and n0.5 > 0.,B.14 Proof of Example 5,[0],[0]
"We first set the feedforward network fD(x;θD) to constant, i.e., fD(x;θD) ≡ 0",B.14 Proof of Example 5,[0],[0]
for x ∈,B.14 Proof of Example 5,[0],[0]
"R. Now the whole network becomes a single layer network,
f(x;θ) = a0 + M∑ j=1 ajσ (wjx) .
",B.14 Proof of Example 5,[0],[0]
Let a∗1 = ...,B.14 Proof of Example 5,[0],[0]
= a ∗ M = −1 and w∗1 = ...,B.14 Proof of Example 5,[0],[0]
= w∗M = 0.,B.14 Proof of Example 5,[0],[0]
"Therefore, we have f(x;θ∗)",B.14 Proof of Example 5,[0],[0]
= a∗0 −Mσ(0).,B.14 Proof of Example 5,[0],[0]
"Let a∗0 be the global optimizer of the following convex optimization problem.
",B.14 Proof of Example 5,[0],[0]
min a 2n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`p(−yi(a−Mσ(0))).
",B.14 Proof of Example 5,[0],[0]
"Thus, we have n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi) = 0, (29) and this indicates that∑ i:yi=1",B.14 Proof of Example 5,[0],[0]
`′p(−(a∗0 −Mσ(0))),B.14 Proof of Example 5,[0],[0]
= ∑,B.14 Proof of Example 5,[0],[0]
i:yi=−1 `′p(a ∗ 0 −Mσ(0)) or `′p(−a∗0 +Mσ(0))n+ =,B.14 Proof of Example 5,[0],[0]
`′p(a∗0 −Mσ(0))n−. (30),B.14 Proof of Example 5,[0],[0]
"In addition, since for ∀j ∈ [M ],
∂L̂n(θ ∗)
",B.14 Proof of Example 5,[0],[0]
∂aj = 2n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)σ(0) = 0, by Equation (29),
∇wj L̂n(θ∗) =",B.14 Proof of Example 5,[0],[0]
2n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)σ′(0)xi = 0, by ∑ i:yi=1 xi",B.14 Proof of Example 5,[0],[0]
= ∑,B.14 Proof of Example 5,[0],[0]
i:,B.14 Proof of Example 5,[0],[0]
"yi=−1 xi = 0,
and ∂L̂n(θ ∗)
∂a0 = n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi) = 0,
then θ∗ is a critical point.",B.14 Proof of Example 5,[0],[0]
"Next we show that θ∗ = (a∗0, ..., a ∗ M , w ∗ 1, ..., w ∗ M ) is a local minima.",B.14 Proof of Example 5,[0],[0]
"Consider any perturbation ∆a1, ...,∆aM : |∆aj | < 12 for all j ∈",B.14 Proof of Example 5,[0],[0]
"[M ], ∆w1, ...,∆wM ∈ R and ∆a0 ∈ R. Define
θ̃ = (a∗0 + ∆a0, ..., a ∗ M + ∆aM , w ∗ 1 + ∆w1, ..., w ∗",B.14 Proof of Example 5,[0],[0]
"M + ∆wM ).
",B.14 Proof of Example 5,[0],[0]
"Then
n∑ i=1",B.14 Proof of Example 5,[0],[0]
`p(−yif(xi; θ̃))− n∑ i=1,B.14 Proof of Example 5,[0],[0]
`p(−yif(xi;θ∗)),B.14 Proof of Example 5,[0],[0]
= n∑ i=1,B.14 Proof of Example 5,[0],[0]
[ `p(−yif(xi; θ̃))− `p(−yif(xi;θ∗)) ],B.14 Proof of Example 5,[0],[0]
"≥
n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)[f(xi; θ̃)− f(xi;θ∗)]
= n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)[f(xi; θ̃)− a∗0 +Mσ(0)]
= n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃),
where the inequality follows from the convexity of the loss function `p(z), the second equality follows from the fact that f(x;θ∗) ≡",B.14 Proof of Example 5,[0],[0]
a∗0 − Mσ(0) and the third equality follows from Equation (30).,B.14 Proof of Example 5,[0],[0]
"In addition, we have
n∑ i=1",B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) = n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ (∆wjxi) + ∆a0
 =
n∑ i=1",B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ (∆wjxi)  by Eq.,B.14 Proof of Example 5,[0],[0]
"(30) =
M∑ j=1 −(a∗j + ∆aj)",B.14 Proof of Example 5,[0],[0]
[ n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ (∆wjxi) ]
= M∑ j=1 −(a∗j + ∆aj)",B.14 Proof of Example 5,[0],[0]
[ n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ (∆wjxi) ] .
",B.14 Proof of Example 5,[0],[0]
"Now we define the following function G : R→ R,
G(u) = n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ (uxi) .
",B.14 Proof of Example 5,[0],[0]
"Now we consider the gradient of the function G with respect to the variable u at the point u = 0,
∇uG(0) = n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yiσ′ (0)xi
= σ′(0)
( 1
2 `′p(−a∗0 +Mσ(0))n+ −
1 2 `′p(a ∗ 0 −Mσ(0))n−
) = 0,
by Equation (30).",B.14 Proof of Example 5,[0],[0]
"Furthermore, the second order derivative ∇2uG(0) satisfies
∇2uG(0) = n∑ i=1",B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))yiσ′′ (0) (xi)2 = σ′′ (0) n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))yi (xi)2
= σ′′(0)  1 n+ ∑ i:yi=1 (xi) 2 − 1 n− ∑ i:yi=−1 (xi) 2  > 0, then the function G(u)",B.14 Proof of Example 5,[0],[0]
=,B.14 Proof of Example 5,[0],[0]
∑n i=1,B.14 Proof of Example 5,[0],[0]
`p(−yi(a∗0,B.14 Proof of Example 5,[0],[0]
− Mσ(0)))yiσ (uxi) has a local minima at u = 0.,B.14 Proof of Example 5,[0],[0]
"This indicates that there exists ε > 0 such that for all ∆w : ‖∆w‖2 ≤ ε, n∑ i=1",B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>xi ) ≥ n∑ i=1,B.14 Proof of Example 5,[0],[0]
"`p(−yi(a∗0 −Mσ(0)))yiσ (0) = 0.
",B.14 Proof of Example 5,[0],[0]
"In addition, since a∗j = −1, |∆aj | < 12 , then for all ∆wj : ‖∆wj‖2 ≤ ε, n∑ i=1",B.14 Proof of Example 5,[0],[0]
`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) = M∑ j=1 −(a∗j + ∆aj),B.14 Proof of Example 5,[0],[0]
[ n∑ i=1,B.14 Proof of Example 5,[0],[0]
`p(−yi(a∗0 −Mσ(0)))yiσ ( ∆w>j xi )],B.14 Proof of Example 5,[0],[0]
"≥ 0.
",B.14 Proof of Example 5,[0],[0]
"Therefore, we have n∑ i=1",B.14 Proof of Example 5,[0],[0]
"`′p(−yi(a∗0 −Mσ(0)))(−yi)f(xi; θ̃) ≥ 0, and this indicates that n∑ i=1",B.14 Proof of Example 5,[0],[0]
`p(−yif(xi; θ̃))− n∑ i=1,B.14 Proof of Example 5,[0],[0]
`p(−yif(xi;θ∗)),B.14 Proof of Example 5,[0],[0]
≥ 0.,B.14 Proof of Example 5,[0],[0]
"Thus, θ∗ is a local minima with f(x;θ∗)",B.14 Proof of Example 5,[0],[0]
= a∗0 −Mσ(0) = constant.,B.14 Proof of Example 5,[0],[0]
"Thus,
1
n n∑ i=1",B.14 Proof of Example 5,[0],[0]
I{yi 6= sgn(f(xi;θ∗))},B.14 Proof of Example 5,[0],[0]
"≥ min{n−, n+} n .
",B.14 Proof of Example 5,[0],[0]
"Since the dataset is consisted of both positive and negative examples, then the training error is nonzero.",B.14 Proof of Example 5,[0],[0]
Lemma 2,B.15 Proof of Lemma 2,[0],[0]
"If samples in the dataset D = {(xi, yi)}ni=1 satisfies that the matrix ∑n i=1 λiyixix",B.15 Proof of Lemma 2,[0],[0]
"> i is
indefinite for all sequences {λi ≥ 0}ni=1",B.15 Proof of Lemma 2,[0],[0]
"satisfying ∑ i:yi=1 λi = ∑ i:yi=−1 λi > 0, then there exists a matrix A ∈ Rd×d and two real numbers c1 > 0",B.15 Proof of Lemma 2,[0],[0]
and c2 ∈ R such that yi(x,B.15 Proof of Lemma 2,[0],[0]
>,B.15 Proof of Lemma 2,[0],[0]
i Axi − c2) > c1 holds for all i ∈,B.15 Proof of Lemma 2,[0],[0]
"[n].
",B.15 Proof of Lemma 2,[0],[0]
"Proof: For each sample xi in the dataset, let",B.15 Proof of Lemma 2,[0],[0]
vec(xix > i ) denote the vectorization of the matrix xix >,B.15 Proof of Lemma 2,[0],[0]
i .,B.15 Proof of Lemma 2,[0],[0]
"Since we assume that for any sequence {λi ≥ 0}ni=1 satisfying ∑ i:yi=1 λi = ∑ i:yi=−1 λi = 1, the vector∑n
i=1",B.15 Proof of Lemma 2,[0],[0]
"yiλivec(xix > i ) does not equal to the zero vector 0d2 , then we have that the convex hull of two vector sets C+ = {vec(xix>i )}i:yi=1 and C− = {vec(xix>i )}",B.15 Proof of Lemma 2,[0],[0]
i:yi=−1 are two disjoint closed compact sets.,B.15 Proof of Lemma 2,[0],[0]
"By the hyperplane separation theorem, this indicates that there exists a vector w ∈ Rd2 and two real numbers c̃1 < c̃2 such that w >u > c̃2 and w >v < c̃1 for all u ∈ C+ and v ∈ C−.",B.15 Proof of Lemma 2,[0],[0]
This further indicates that there exists two real numbers c1 > 0 and c2 ∈ R such that yi(x,B.15 Proof of Lemma 2,[0],[0]
>,B.15 Proof of Lemma 2,[0],[0]
i,B.15 Proof of Lemma 2,[0],[0]
Axi− c2) > c1 holds for all i ∈ R.,B.15 Proof of Lemma 2,[0],[0]
"Proposition 12 Assume that the single layer neural network fS(x;θS) has M > d neurons and assume that the neuron σ is quadratic, i.e., σ(z) = z2.",B.16 Proof of Proposition 12,[0],[0]
"Assume that the dataset D = {(xi, yi)}ni=1 is consisted of both positive and negative samples.",B.16 Proof of Proposition 12,[0],[0]
"For all multilayer neural network fD parameterized by θD, every local minimum θ ∗ = (θ∗S ,θ ∗ D) of the empirical loss function L̂n(θS ,θD; p), p ≥ 6 satisfies R̂n(θ ∗)",B.16 Proof of Proposition 12,[0],[0]
= 0,B.16 Proof of Proposition 12,[0],[0]
if and only if the matrix,B.16 Proof of Proposition 12,[0],[0]
∑n i=1 λiyixix,B.16 Proof of Proposition 12,[0],[0]
"> i is indefinite for all sequences {λi ≥ 0}ni=1
satisfying ∑
i:yi=1 λi = ∑",B.16 Proof of Proposition 12,[0],[0]
i:,B.16 Proof of Proposition 12,[0],[0]
yi=−1 λi > 0.,B.16 Proof of Proposition 12,[0],[0]
"(1) Proof of “if”: It follows from Lemma 2 that if the assumptions on the dataset are satisfied, there exists a set of parameter θS such that fS(x;θS) achieves zero training error and this further indicates that for any neural architecture fD, there exists a set of parameter θ ∗ =",Proof:,[0],[0]
"(θ∗S ,θ ∗ D) such that Ln(θ ∗; p) = 0 for all p ≥ 1.",Proof:,[0],[0]
This means that the empirical loss function has a global minimum with a value equal to zero.,Proof:,[0],[0]
"We first assume that the θ∗ = (θ∗1,θ ∗ 2) is a local minimum.",Proof:,[0],[0]
"We next prove the following two claims: Claim 1: If θ∗ = (θ∗S ,θ ∗ D) is a local minimum and there exists j ∈",Proof:,[0],[0]
"[M ] such that a∗j = 0, then R̂n(θ ∗) = 0.",Proof:,[0],[0]
Claim 2:,Proof:,[0],[0]
"If θ∗ = (θ∗S ,θ ∗ D) is a local minimum and a ∗ j 6= 0 for all j ∈",Proof:,[0],[0]
"[M ], then R̂n(θ∗) = 0.",Proof:,[0],[0]
(a) Proof of claim 1.,Proof:,[0],[0]
"We prove that if θ∗ = (θ∗S ,θ ∗ D) is a local minima and there exists j ∈",Proof:,[0],[0]
"[M ] such that a∗j = 0, then R̂n(θ ∗) = 0.",Proof:,[0],[0]
"Without loss of generality, we assume that a∗1 = 0.",Proof:,[0],[0]
"Since θ∗ = (θ∗S ,θ ∗ D) is a local minima, then there exists ε0 > 0",Proof:,[0],[0]
"such that for any small perturbations ∆a1, ∆w1 on parameters a ∗ 1 and w ∗ 1, i.e., |∆a1|2 + ‖∆w1‖22 ≤",Proof:,[0],[0]
"ε20, we have
L̂n(θ̃S ,θ ∗ D) ≥ L̃n(θ∗S ,θ∗D),
where θ̃ = (ã0, ã1, ..., ãM , w̃1, ..., w̃M ), ã1 = a ∗ 1 + ∆a1, w̃1 = w ∗ 1 + ∆w1 and ãj = a ∗ j , w̃j",Proof:,[0],[0]
= w ∗ j for j 6= 1.,Proof:,[0],[0]
"Now we consider Taylor expansion of L̃n(θ̃S ,θ∗D) at (θ∗S ,θ∗D).",Proof:,[0],[0]
"We note here that the Taylor expansion of L̂(θS ,θ ∗ D; p) on θS always exists, since the empirical loss function L̂n has continuous derivatives with respect to fS up to the p-th order and the output of the neural network f(x;θS) is infinitely differentiable with respect to θS due to the fact that neuron activation function σ is real analytic.",Proof:,[0],[0]
"We first calculate the first order derivatives at the point (θ∗1,θ ∗ 2)
dL̂n(θ ∗)
da1 = n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ ( w∗1 >xi ) = 0, θ∗ is a critical point,
∇w1L̂n(θ∗) = a∗1 n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ) xi = 0d, θ ∗ is a critical point.
",Proof:,[0],[0]
"Next, we calculate the second order derivatives at the point (θ∗1,θ ∗ 2),
d2L̂(θ∗)
",Proof:,[0],[0]
da21 = N∑ i=1,Proof:,[0],[0]
"`′′p(−yif(xi;θ∗))σ2 ( w∗1 >xi ) ≥ 0,
d
da1 (∇w1L(θ∗))",Proof:,[0],[0]
= n∑ i=1,Proof:,[0],[0]
"`′p(−yif(xi;θ∗))(−yi)σ′ ( w∗1 >xi ) xi
+ a∗1 n∑ i=1",Proof:,[0],[0]
`′′p(−yif(xi;θ∗))σ ( w∗1 >xi ),Proof:,[0],[0]
σ′,Proof:,[0],[0]
"( w∗1 >xi ) xi
= 0d,
where the first term equals to the zero vector by the necessary condition for a local minima presented in Lemma 1 and the second term equals to the zero vector by the assumption that a∗1 = 0.",Proof:,[0],[0]
"Furthermore, by the assumption that a∗1 = 0, we have
∇2w1L̂n(θ∗; p) = a∗1∇w1",Proof:,[0],[0]
[ n∑ i=1,Proof:,[0],[0]
"`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ) xi ] = 0d×d.
",Proof:,[0],[0]
"We further calculate the third order derivatives
d
da1
[ ∇2w1L̂n(θ∗; p) ] =",Proof:,[0],[0]
"d
da1
[ a∗1∇w1 [ n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ) xi,Proof:,[0],[0]
"]]
= ∇w1",Proof:,[0],[0]
[ n∑ i=1,Proof:,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ) xi ],Proof:,[0],[0]
"+ 0d×d by a ∗ 1 = 0
= n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) xix >,Proof:,[0],[0]
"i
+ a∗1 n∑ i=1",Proof:,[0],[0]
`′′p(−yif(xi;θ)),Proof:,[0],[0]
[ σ′ ( w∗1 >xi )]2 xix >,Proof:,[0],[0]
"i
= n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) xix,Proof:,[0],[0]
>,Proof:,[0],[0]
"i by a ∗ 1 = 0
and
∇3w1L̂n(θ∗; p) = a∗1∇2w1 [ n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ) xi ] = 0d×d×d.
",Proof:,[0],[0]
"In fact, it is easy to show that for any 2 ≤ k ≤ p,
∇kw1L̂n(θ∗; p) =",Proof:,[0],[0]
a∗1∇k−1w1 [ n∑ i=1,Proof:,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′ ( w∗1 >xi ) xi ] = 0d× d× ...× d︸,Proof:,[0],[0]
"︷︷ ︸
k times
.
",Proof:,[0],[0]
"Let ε > 0, ∆a1 = sgn(a1)ε 9/4 and ∆w1 = εu1 for u1 : ‖u1‖2 = 1.",Proof:,[0],[0]
"Clearly, when ε → 0, ∆a1 = o(‖∆w1‖2), ∆a1 = o(1) and ‖∆w1‖ = o(1).",Proof:,[0],[0]
"Then we expand L̂n(θ̃1,θ∗2) at the point θ∗ up to the sixth order and thus as ε→ 0,
L̂n(θ̃1,θ ∗ 2) = L̂n(θ ∗ 1,θ ∗ 2) +
1
2!
",Proof:,[0],[0]
"d2L̂n(θ ∗)
d2a1 (∆a1)
2
+ 1
2 ∆a1∆w
> 1
d
da1
[ D2w1L̂n(θ ∗; p) ]",Proof:,[0],[0]
"∆w1 + o(|a1|2) + o(|a1|‖w1‖22) + o(‖∆w1‖52)
",Proof:,[0],[0]
"= L̂n(θ ∗ 1,θ ∗ 2) +
1
2!
d2L̂n(θ ∗)
d2a1 ε9/2 +
1 2 sgn(a1)ε 9/4+2 n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ))σ′′ ( w∗1 >xi ) (u>1 xi) 2
+ o(ε9/2) + o(ε9/4+2) + o(ε5)
= L̂n(θ ∗ 1,θ ∗ 2) +
1 2 sgn(a1)ε 17/4 n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) (u>1 xi) 2 + o(ε17/4)
",Proof:,[0],[0]
"Since ε > 0 and L̂n(θ̃1,θ ∗ 2; p) ≥ L̂n(θ∗; p) holds for any u1 : ‖u1‖2 = 1 and any sgn(a1) ∈ {−1, 1}, then n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) (u>xi) 2 = 0, for any u ∈ Rd.",Proof:,[0],[0]
"(31)
Therefore, n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ))(−yi)σ′′ ( w∗1 >xi ) xix >,Proof:,[0],[0]
i = 0d×d.,Proof:,[0],[0]
"Since σ′′(z) = 2 for all z, then
n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ))(−yi)xix>i = 0d×d. (32)
",Proof:,[0],[0]
"Furthermore, since θ∗ is a critical point, then
∂L̂n(θ; p)
∂a0 =
1
n n∑ i=1",Proof:,[0],[0]
`′(−yif(xi;θ∗))(−yi) = 0.,Proof:,[0],[0]
"(33)
Now we assume that R̂n(θ ∗)",Proof:,[0],[0]
> 0.,Proof:,[0],[0]
This means that there exists a index i such that yif(xi;θ ∗),Proof:,[0],[0]
< 0 or `′(−yif(xi;θ∗)),Proof:,[0],[0]
> 0.,Proof:,[0],[0]
"Furthermore, since `′(z) ≥ 0, then by setting λi = `′(−yif(xi;θ∗)), we have that there exists a sequence {λi ≥ 0}ni=1 satisfying ∑ i:yi=1 λi = ∑
i:",Proof:,[0],[0]
"yi=−1 λi > 0, where the equality follows from Equation (33) and the positiveness comes from the assumption that `′(−yif(xi;θ∗))",Proof:,[0],[0]
> 0,Proof:,[0],[0]
"for some i, such that
n∑ i=1 λiyixix >",Proof:,[0],[0]
"i = 0d×d,
where the equality follows from Equation (32).",Proof:,[0],[0]
This leads to the contradiction with our assumption that the matrix,Proof:,[0],[0]
∑n i=1 λiyixix,Proof:,[0],[0]
> i should be indefinite for all sequences {λi ≥ 0}ni=1,Proof:,[0],[0]
"satisfying ∑ i:yi=1
λi =∑ i:",Proof:,[0],[0]
yi=−1 λi > 0.,Proof:,[0],[0]
"Therefore, this indicates that R̂n(θ ∗) = 0.",Proof:,[0],[0]
"(b) Proof of Claim 2: To prove the claim 2, we first show that if M > d, then there exists coefficients α1, ..., αM , not all zero, such that
(α1w ∗ 1 + ...+ αMw ∗ M ) >",Proof:,[0],[0]
xi,Proof:,[0],[0]
"= 0, for all i ∈",Proof:,[0],[0]
"[n].
",Proof:,[0],[0]
"Clearly, if M > r, then there exists coefficients α1, ..., αM , not all zero, such that
(α1w ∗ 1 + ...+ αMw ∗ M ) = 0d, for all i ∈",Proof:,[0],[0]
"[n].
Now we prove the claim 2.",Proof:,[0],[0]
"First, we consider the Hessian matrix H(w∗1, ...,w ∗ M ).",Proof:,[0],[0]
"Since θ ∗ is a local minima, then
F (u1, ...,uM )",Proof:,[0],[0]
= M∑ j=1 M∑ k=1,Proof:,[0],[0]
"u>j ∇2wj ,wk L̂n(θ ∗; p)uk ≥ 0
holds for any vectors u1, ...,uM ∈ Rd.",Proof:,[0],[0]
"Since σ′′(z) = 2 and σ′(z) = 2z for all z ∈ R, then
∇2wj L̂n(θ∗; p) = a∗j n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ∗))(−yi)σ′′ ( w∗j >xi ) xix,Proof:,[0],[0]
>,Proof:,[0],[0]
"i
+ a∗j 2 n∑ i=1",Proof:,[0],[0]
`′′p(−yif(xi;θ∗)),Proof:,[0],[0]
[ σ′ ( w∗j >xi )]2 xix >,Proof:,[0],[0]
"i
= −2a∗j n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ∗))yixix>i + 4a∗j 2 n∑ i=1,Proof:,[0],[0]
`′′p(−yif(xi;θ∗)) (,Proof:,[0],[0]
w∗j >xi )2 xix >,Proof:,[0],[0]
"i ,
and
∇2wj ,wk L̂n(θ ∗; p) =",Proof:,[0],[0]
a∗ja ∗ k n∑ i=1,Proof:,[0],[0]
`′′p(−yif(xi;θ∗)),Proof:,[0],[0]
[ σ′ ( w∗j >xi )],Proof:,[0],[0]
[ σ′ ( w∗k >xi )],Proof:,[0],[0]
xix >,Proof:,[0],[0]
"i
= 4a∗ja ∗ k n∑ i=1",Proof:,[0],[0]
`′′p(−yif(xi;θ∗)) (,Proof:,[0],[0]
w∗j >xi )( w∗k >xi ) xix,Proof:,[0],[0]
"> i .
",Proof:,[0],[0]
"Thus, we have
F (u1, ...,uM )",Proof:,[0],[0]
= −2,Proof:,[0],[0]
M∑ j=1 [ a∗j n∑ i=1,Proof:,[0],[0]
"`′p(−yif(xi;θ∗))yi ( u>j xi )2]
+ 4 M∑ j=1 M∑",Proof:,[0],[0]
k=1,Proof:,[0],[0]
[ a∗ja ∗ k n∑ i=1,Proof:,[0],[0]
`′′p(−yif(xi;θ∗)) (,Proof:,[0],[0]
w∗j >xi )( w∗k >xi ),Proof:,[0],[0]
"( u>j xi )( u>k xi )]
= −2 M∑ j=1",Proof:,[0],[0]
[ a∗j n∑ i=1,Proof:,[0],[0]
"`′p(−yif(xi;θ∗))yi ( u>j xi )2]
+ 4 n∑ i=1",Proof:,[0],[0]
`′′p(−yif(xi;θ∗))  ,Proof:,[0],[0]
M∑ j=1 a∗j ( w∗j >xi ),Proof:,[0],[0]
( u>j xi )2 .,Proof:,[0],[0]
"Since there exists coefficients α1, ..., αM , not all zero, such that (α1w ∗ 1 + ...+ αMw ∗ M )",Proof:,[0],[0]
">xi = 0, for all i ∈",Proof:,[0],[0]
"[n], and a∗j 6= 0",Proof:,[0],[0]
for all j ∈,Proof:,[0],[0]
[M ] then by setting uj = αju/a∗j for all j ∈,Proof:,[0],[0]
"[M ], we have that the inequality
F (u1, ...,uM )",Proof:,[0],[0]
= −2,Proof:,[0],[0]
M∑ j=1 [ a∗j n∑ i=1,Proof:,[0],[0]
`′p(−yif(xi;θ∗))yi ( αj/a ∗ j )2,Proof:,[0],[0]
"( u>xi )2]
",Proof:,[0],[0]
+ 4 n∑ i=1,Proof:,[0],[0]
"`′′p(−yif(xi;θ∗))  M∑ j=1 αj ( w∗j >xi )( u>xi )2 = −2
M∑ j=1",Proof:,[0],[0]
[ a∗j n∑ i=1,Proof:,[0],[0]
`′p(−yif(xi;θ∗))yi ( αj/a ∗ j )2,Proof:,[0],[0]
"( u>xi )2]
",Proof:,[0],[0]
+ 4 n∑ i=1,Proof:,[0],[0]
`′′p(−yif(xi;θ∗))   M∑ j=1 αjw ∗,Proof:,[0],[0]
"j > xi  2 ( u>xi )2 = −2
M∑ j=1 ( α2j/a ∗ j ) · n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ∗))yi ( u>xi )2 ≥ 0
holds for any u ∈ Rd.",Proof:,[0],[0]
"Next we consider the following two cases: (1) ∑M j=1 ( α2j/a ∗ j ) 6= 0; (2) ∑Mj=1 (α2j/a∗j) = 0.
",Proof:,[0],[0]
"Case 1: If ∑M
j=1 ( α2j/a ∗ j ) 6= 0, then without loss of generality, we assume that ∑Mj=1 (α2j/a∗j) < 0.
",Proof:,[0],[0]
This indicates that n∑ i=1,Proof:,[0],[0]
"`′p(−yif(xi;θ∗))yi ( u>xi )2 ≥ 0, for all u ∈ Rd.",Proof:,[0],[0]
"(34)
Since θ∗ is a critical point, then
∂L̂n(θ ∗; p)
∂a0 =
1
n n∑ i=1",Proof:,[0],[0]
`′(−yif(xi;θ∗))(−yi) = 0.,Proof:,[0],[0]
"(35)
Now we assume that R̂n(θ ∗)",Proof:,[0],[0]
> 0.,Proof:,[0],[0]
This means that there exists a index i such that yif(xi;θ ∗),Proof:,[0],[0]
< 0 or `′(−yif(xi;θ∗)),Proof:,[0],[0]
> 0.,Proof:,[0],[0]
"Furthermore, since `′(z) ≥ 0, then by setting λi = `′(−yif(xi;θ∗)), we have that there exists a sequence {λi ≥ 0}ni=1 satisfying ∑ i:yi=1 λi = ∑
i:",Proof:,[0],[0]
"yi=−1 λi > 0, where the equality follows from Equation (33) and the positiveness comes from the assumption that `′(−yif(xi;θ∗))",Proof:,[0],[0]
> 0,Proof:,[0],[0]
"for some i, such that
n∑ i=1 λiyixix >",Proof:,[0],[0]
"i 0,
where the positive semi-definiteness follows from the inequality (34).",Proof:,[0],[0]
This leads to the contradiction with our assumption that the matrix,Proof:,[0],[0]
∑n i=1 λiyixix,Proof:,[0],[0]
"> i should be indefinite for all sequences {λi ≥ 0}ni=1
satisfying ∑
i:yi=1 λi = ∑ i:",Proof:,[0],[0]
yi=−1 λi > 0.,Proof:,[0],[0]
"Therefore, this indicates that R̂n(θ ∗) = 0.
",Proof:,[0],[0]
"Case 2: If ∑M
j=1 ( α2j/a ∗ j ) = 0, then by setting uj = (αj/a ∗ j +vsgn(αj))u for some scalar v and vector
u ∈ Rd, we have
F (v,u) = −2",Proof:,[0],[0]
M∑ j=1 [ a∗j n∑ i=1,Proof:,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,Proof:,[0],[0]
">xi )2]
+ 4 n∑ i=1",Proof:,[0],[0]
`′′p(−yif(xi;θ∗))  ,Proof:,[0],[0]
M∑ j=1 a∗j ( w∗j >xi )( (αj/a ∗ j + vsgn(αj))u,Proof:,[0],[0]
">xi )2 = −2
M∑ j=1 [ a∗j n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,Proof:,[0],[0]
">xi )2]
+ 4 n∑ i=1",Proof:,[0],[0]
`′′p(−yif(xi;θ∗))   M∑ j=1 (αj + vsgn(αj)a ∗ j )w ∗,Proof:,[0],[0]
"j > xi (u>xi)2  = −2
M∑ j=1",Proof:,[0],[0]
[ a∗j n∑ i=1,Proof:,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,Proof:,[0],[0]
">xi )2]
+ 4v2 n∑ i=1",Proof:,[0],[0]
`′′p(−yif(xi;θ∗))   M∑,Proof:,[0],[0]
j=1 sgn(αj)a ∗ jw ∗,Proof:,[0],[0]
j > xi  2,Proof:,[0],[0]
"( u>xi )2 , −2
M∑ j=1",Proof:,[0],[0]
[ a∗j n∑ i=1,Proof:,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,Proof:,[0],[0]
">xi )2] + v2R(u),
where we define
R(u) = 4 n∑ i=1",Proof:,[0],[0]
`′′p(−yif(xi;θ∗))   M∑,Proof:,[0],[0]
j=1 sgn(αj)a ∗ jw ∗,Proof:,[0],[0]
j > xi  2,Proof:,[0],[0]
"( u>xi )2 .
",Proof:,[0],[0]
"In addition, we have
M∑ j=1",Proof:,[0],[0]
[ a∗j n∑ i=1,Proof:,[0],[0]
`′p(−yif(xi;θ∗))yi ( (αj/a ∗ j + vsgn(αj))u,Proof:,[0],[0]
">xi )2]
= n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ))yi(u>xi)2 ·  M∑ j=1 (α2j/a ∗ j + 2vsgn(αj)αj + v 2a∗j )  ,Proof:,[0],[0]
"=
n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ))yi(u>xi)2 ·  M∑ j=1 (2vsgn(αj)αj + v 2a∗j )  ,Proof:,[0],[0]
"= 2v
 M∑ j=1 |αj |  n∑ i=1",Proof:,[0],[0]
`′p(−yif(xi;θ))yi(u>xi)2 + v2  M∑ j=1 a∗j  n∑ i=1,Proof:,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2.
",Proof:,[0],[0]
"Therefore, we can rewrite F (v,u) as
F (v,u) =",Proof:,[0],[0]
2v M∑ j=1 |αj | n∑ i=1,Proof:,[0],[0]
`′p(−yif(xi;θ))yi(u>xi)2 + v2 M∑ j=1 a∗j · n∑ i=1,Proof:,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2 + v2R(u)
, 2v M∑ j=1 |αj | n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2 + v2R̂(u)
Since F (v,u) ≥ 0 holds for any scalar v and vector u ∈ Rd, then we should have
M∑ j=1 |αj",Proof:,[0],[0]
| n∑ i=1,Proof:,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2 = 0, for any u ∈ Rd.
",Proof:,[0],[0]
"Since the coefficient α1, ..., αM are not all zero, then for any u ∈ Rd, we have n∑ i=1",Proof:,[0],[0]
"`′p(−yif(xi;θ))yi(u>xi)2 = 0.
Applying the same analysis shown earlier, we have R̂n(θ ∗) = 0.
",Proof:,[0],[0]
Proof of “only if”: We prove the necessary condition by proving the following claim.,Proof:,[0],[0]
Claim 10,Proof:,[0],[0]
If there exists a sequence {λi ≥ 0}ni=1 satisfying ∑ i:yi=1 λi = ∑ i:,Proof:,[0],[0]
"yi=−1 λi > 0 such that
the matrix ∑n
i=1 λiyixix",Proof:,[0],[0]
"> i is positive or negative positive semi-definite, then there exists a multilayer
neural architecture fD",Proof:,[0],[0]
"such that the empirical loss function L̂n(θS ,θD; p), p ≥ 6 has a local minimum with a non-zero training error.
",Proof:,[0],[0]
"Proof: Let D = {(xi, yi)}ni=1 denote a dataset consisting of n samples.",Proof:,[0],[0]
"We rewrite the sample x as x = ( x(1), ..., x(d) ) .",Proof:,[0],[0]
"Consider the following network,
f(x;θ) = fS(x;θS) + fD(x;θD),
where
fS(x;θS)",Proof:,[0],[0]
= a0 + M∑ j=1,Proof:,[0],[0]
"ajσ(w > j xi + bj),
and the multilayer network is defined as follows,
fD(x;θD) = fD(x; θ1, ..., θd) = n∑ i=1",Proof:,[0],[0]
µi d∏,Proof:,[0],[0]
k=1 1 { x(k) ∈,Proof:,[0],[0]
"[ x (k) i − θk, x (k) i + θk ]} .",Proof:,[0],[0]
"(36)
We note here that µ1, ..., µn are not parameters and later we will show that this function can be implemented by a multilayer network consisted of threshold units.",Proof:,[0],[0]
"A useful property of the function fD(x;θD) is that if all parameters θis are positive and sufficiently smalls, then for each sample (xi, yi) in the dataset,
fD(xi;θD)",Proof:,[0],[0]
"= µi.
",Proof:,[0],[0]
"Furthermore, if we slightly perturb all parameters, the output of the function fD on all samples remain the same.",Proof:,[0],[0]
"In the proof, we use these two properties to construct the local minimum with a non-zero training error.
",Proof:,[0],[0]
"By assumption, there exists a sequence {λi ≥ 0}ni=1 satisfying ∑ i:yi=1 λi = ∑ i:yi=−1 λi > 0",Proof:,[0],[0]
"such that
the matrix ∑n
i=1 λiyixix",Proof:,[0],[0]
>,Proof:,[0],[0]
i is positive or negative semi-definite.,Proof:,[0],[0]
"Without loss of generality, we assume
that the matrix is positive semi-definite.",Proof:,[0],[0]
"Now we construct a local minimum θ∗. Let a∗0 = a ∗ 1 = ... = a∗M = −1, w∗1 = ...",Proof:,[0],[0]
= w∗M = 0d and b∗1 = ...,Proof:,[0],[0]
= b∗M = 0.,Proof:,[0],[0]
"Now we set θ∗1, ..., θ∗d to be positive and sufficiently small such that for two different samples in the dataset, e.g., xi 6= xj , the following equations holds,
d∏",Proof:,[0],[0]
k=1 1 { x (k) j ∈,Proof:,[0],[0]
"[ x (k) i − 2θ∗k, x (k) i + 2θ ∗",Proof:,[0],[0]
"k ]} = 0, d∏",Proof:,[0],[0]
k=1 1 { x (k) i ∈,Proof:,[0],[0]
"[ x (k) j − 2θ∗k, x (k) j + 2θ ∗",Proof:,[0],[0]
"k ]} = 0.
",Proof:,[0],[0]
"Now we choose µ1, ..., µn as follows.",Proof:,[0],[0]
"The output of the neural network on sample xi in the dataset is f(xi;θ
∗) = µi −Mσ(0).",Proof:,[0],[0]
"We need to choose µ1, ..., µn to satisfy all conditions shown as follows:
(1) There exists i ∈",Proof:,[0],[0]
[n] such that yi(µi −Mσ(0)),Proof:,[0],[0]
"< 0.
(2) For all i : yi = 1 and all k : yk = −1,
`′(−yi(µi −Mσ(0)))∑ j:j=1 ` ′(−yi(µi −Mσ(0)))",Proof:,[0],[0]
"= λi∑ j:j=1 λj , `′(−yk(µk −Mσ(0)))∑ j:j=−1 ` ′(−yi(µi −Mσ(0)))",Proof:,[0],[0]
"= λk∑ j:j=−1 λj ,
and ∑ j:j=1 `′(−yi(µi −Mσ(0)))",Proof:,[0],[0]
= ∑,Proof:,[0],[0]
"j:j=−1 `′(−yi(µi −Mσ(0))).
",Proof:,[0],[0]
Now we start from the largest element in the sequence {λi}ni=1.,Proof:,[0],[0]
"Since ∑n
i=1 λi > 0, the define the index imax as the index of the largest element, i.e.,
imax = arg max i λi.
Let λmax = λimax .",Proof:,[0],[0]
"Now we choose µimax such that
yimax(µimax −Mσ(0))",Proof:,[0],[0]
"= −1.
",Proof:,[0],[0]
"Thus, the index imax satisfy the first condition.",Proof:,[0],[0]
"Then for i 6= imax, we choose µi such that
`′(−yi(µi −Mσ(0)))",Proof:,[0],[0]
= λi λmax `(−yimax(µimax −Mσ(0))),Proof:,[0],[0]
= λi λmax `′(1) ≤,Proof:,[0],[0]
`′(1),Proof:,[0],[0]
.,Proof:,[0],[0]
"(37)
",Proof:,[0],[0]
We note here that for each i ∈,Proof:,[0],[0]
"[n], there always exists a µi solving the above equation.",Proof:,[0],[0]
"This can be seen by the fact that `′ is continuous, `′p(z) ≥ 0 and `′p(z) = 0 iff z ≤ −z0.",Proof:,[0],[0]
"This indicates that for ∀z > −z0, `′p(z) > 0, i.e., `′(1) > 0 and that `′(−z0) = 0.",Proof:,[0],[0]
"Since `′(z) is continuous, then for ∀r ∈",Proof:,[0],[0]
"[0, `′(1)], there always exists z ∈ R such that `′(z) = r, which further indicates that for ∀i ∈",Proof:,[0],[0]
"[n], there always exists µi ∈ R solving the Equation (37).",Proof:,[0],[0]
"Under this construction, it is easy to show that the second condition is satisfied as well.",Proof:,[0],[0]
Now we only need to show that θ∗ is local minimum.,Proof:,[0],[0]
We first show that θ∗ is a critical point of the empirical loss function.,Proof:,[0],[0]
"Since for ∀j ∈ [M ],
∂L̂n(θ ∗)
",Proof:,[0],[0]
∂aj = n∑ i=1,Proof:,[0],[0]
"`′(−yi(µi −Mσ(0)))(−yi)σ(0)
= σ(0) n∑ i=1",Proof:,[0],[0]
λi λmax `′(1)(−yi) =,Proof:,[0],[0]
"− σ(0)`′(1) λmax n∑ i=1 yiλi
= 0 by ∑ i:yi=1 λi = ∑ i:yi=−1 λi
∇wj L̂n(θ∗) = n∑ i=1",Proof:,[0],[0]
"`′(−yi(µi −Mσ(0)))(−yi)σ′(0)xi
= −σ′(0) n∑ i=1",Proof:,[0],[0]
λi λmax `′(1)yixi = − σ′(0)`′(1) λmax n∑ i=1,Proof:,[0],[0]
"λiyixi = 0d by σ ′(0) = 0
and ∂L̂n(θ ∗)
",Proof:,[0],[0]
∂a0 = n∑ i=1,Proof:,[0],[0]
`′(−yi(µi −Mσ(0)))(−yi) =,Proof:,[0],[0]
− `′(1) λmax n∑ i=1,Proof:,[0],[0]
"yiλi = 0.
",Proof:,[0],[0]
"In addition, we have stated earlier, if we slightly perturb the parameter θ∗k in the interval",Proof:,[0],[0]
"[θ ∗ k/2, 3θ ∗ k/2], the output of the function fD(xi;θD) does not change for all i ∈",Proof:,[0],[0]
"[n], then θ∗ is a critical point.",Proof:,[0],[0]
Now we show that θ∗ is local minimum.,Proof:,[0],[0]
"Consider any perturbation ∆a1, ...,∆aM : |∆aj | < 12 for all j ∈",Proof:,[0],[0]
"[M ], ∆w1, ...,∆wM ∈ Rd, ∆a0 ∈ R, ∆θk :",Proof:,[0],[0]
|∆θk| ≤ θk/2 for all k ∈,Proof:,[0],[0]
[n].,Proof:,[0],[0]
"Define
θ̃ = (a∗0 + ∆a0, ..., a ∗ M + ∆aM ,w ∗ 1 + ∆w1, ...,w ∗ M + ∆wM , θ ∗ 1 + ∆θ ∗ 1, ..., θ ∗ d + ∆θ ∗ d).
",Proof:,[0],[0]
"Then
n∑ i=1",Proof:,[0],[0]
`(−yif(xi; θ̃))− n∑ i=1,Proof:,[0],[0]
`(−yif(xi;θ∗)) = n∑ i=1,Proof:,[0],[0]
[ `(−yif(xi; θ̃))− `(−yif(xi;θ∗)) ],Proof:,[0],[0]
"≥
n∑ i=1",Proof:,[0],[0]
"`′(−yif(xi;θ∗))(−yi)[f(xi; θ̃)− f(xi;θ∗)].
",Proof:,[0],[0]
"Since for each sample xi in the dataset,
f(xi; θ̃)− f(xi;θ∗)",Proof:,[0],[0]
= ∆a0 + M∑ j=1 (a∗j + ∆aj)σ(∆w > j xi) +,Proof:,[0],[0]
µi,Proof:,[0],[0]
"− µi
= ∆a0 + M∑ j=1 (a∗j + ∆aj)σ(∆w > j xi),
then
n∑ i=1",Proof:,[0],[0]
`(−yif(xi; θ̃))− n∑ i=1,Proof:,[0],[0]
"`(−yif(xi;θ∗))
≥ n∑ i=1",Proof:,[0],[0]
`′(−yif(xi;θ∗))(−yi)[f(xi; θ̃)− f(xi;θ∗)],Proof:,[0],[0]
= n∑ i=1,Proof:,[0],[0]
"`′(−yi(µi −Mσ(0)))(−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) + ∆a0
 =
n∑ i=1",Proof:,[0],[0]
"λi` ′(1) λmax (−yi)  M∑ j=1 (a∗j + ∆aj)σ ( ∆w>j xi ) = `′(1)
λmax M∑ j=1 −(a∗j",Proof:,[0],[0]
+ ∆aj),Proof:,[0],[0]
[ n∑ i=1,Proof:,[0],[0]
"λiyi ( ∆w>j xi )2] .
",Proof:,[0],[0]
"Since by assumption that the matrix ∑n
i=1 λiyixix",Proof:,[0],[0]
"> i is positive semi-definite, then for any ∆w > j ∈ Rd,
n∑ i=1",Proof:,[0],[0]
"λiyi ( ∆w>j xi )2 ≥ 0.
",Proof:,[0],[0]
"In addition, since a∗j = −1, |∆aj | < 12 , then for all ∆wj ∈ Rd, n∑ i=1",Proof:,[0],[0]
`(−yif(xi; θ̃))− n∑ i=1,Proof:,[0],[0]
`(−yif(xi;θ∗)),Proof:,[0],[0]
"≥ 0.
",Proof:,[0],[0]
"Thus, θ∗ is a local minima of the empirical loss function with f(xi;θ ∗) = µi −Mσ(0).",Proof:,[0],[0]
Since there exists a µimax such that yimax(µimax −Mσ(0)),Proof:,[0],[0]
"= 1, then this means that the neural network makes an incorrect prediction on the sample ximax .",Proof:,[0],[0]
"This indicates that this local minimum has a non-zero training error.
",Proof:,[0],[0]
"Finally, we present the way we construct the neural network fD. Since
fD(x;θD) = fD(x; θ1, ..., θd) = n∑ i=1",Proof:,[0],[0]
µi d∏,Proof:,[0],[0]
k=1 1 { x(k) ∈,Proof:,[0],[0]
"[ x (k) i − θk, x (k) i + θk ]} .
",Proof:,[0],[0]
"Let σth denote the threshold unit, where σth(z) = 1 if z ≥ 0 and σth(z) = 0, otherwise.",Proof:,[0],[0]
"Therefore, the indicator function can be represented as follows:
1 { x(k) ∈",Proof:,[0],[0]
"[ x
(k) i − θk, x (k) i + θk",Proof:,[0],[0]
]} = σth ( x(k),Proof:,[0],[0]
− x(k)i + θk ),Proof:,[0],[0]
− σth ( x(k) − x(k)i,Proof:,[0],[0]
− θk ),Proof:,[0],[0]
"Therefore,
d∏ k=1 1 { x(k) ∈",Proof:,[0],[0]
"[ x (k) i − θk, x (k) i + θk",Proof:,[0],[0]
"]} = σth ( d∑
k=1
[ σth ( x(k) − x(k)i + θk )",Proof:,[0],[0]
− σth ( x(k) − x(k)i,Proof:,[0],[0]
− θk )],Proof:,[0],[0]
"− d+ 1
2
)
Therefore, we have
fD(x;θD) = n∑ i=1",Proof:,[0],[0]
"µiσth
( d∑
k=1
[ σth ( x(k)",Proof:,[0],[0]
− x(k)i + θk ),Proof:,[0],[0]
− σth ( x(k) − x(k)i,Proof:,[0],[0]
− θk )],Proof:,[0],[0]
"− d+ 1
2
) .
",Proof:,[0],[0]
"It is very easy to see that this is a two layer network consisted of threshold units.
",Proof:,[0],[0]
"Furthermore, we note here that, in the proof shown above, we assume the only parameters in the network fD are θ1, ...,θd.",Proof:,[0],[0]
"In fact, we can prove a more general statement where the fD is of the form
fD(x;θD) = n∑ i=1",Proof:,[0],[0]
"µiσth
( d∑
k=1
[ aikσth ( x(k) + uik ) + bikσth ( x(k) + vik )]",Proof:,[0],[0]
"+ ci ) ,
where aik, bik, uik, vik, ci, i ∈",Proof:,[0],[0]
"[n], k ∈",Proof:,[0],[0]
[d] are all parameters.,Proof:,[0],[0]
"We can show that the neural network
fD(x;θD) = n∑ i=1",Proof:,[0],[0]
"µiσth
( d∑
k=1
[ σth ( x(k)",Proof:,[0],[0]
− x(k)i + θk ),Proof:,[0],[0]
− σth ( x(k) − x(k)i,Proof:,[0],[0]
− θk )],Proof:,[0],[0]
"− d+ 1
2
) ,
denotes a local minimum, since any slight perturbations on parameters aik, bik, uik, vik, ci, i ∈",Proof:,[0],[0]
"[n], k ∈",Proof:,[0],[0]
[d] do not change the output of the neural network on the samples in the dataset D.,Proof:,[0],[0]
"It is widely conjectured that the reason that training algorithms for neural networks are successful because all local minima lead to similar performance; for example, see [1, 2, 3].",abstractText,[0],[0]
Performance is typically measured in terms of two metrics: training performance and generalization performance.,abstractText,[0],[0]
"Here we focus on the training performance of neural networks for binary classification, and provide conditions under which the training error is zero at all local minima of appropriately chosen surrogate loss functions.",abstractText,[0],[0]
"Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcut-like connection, and the surrogate loss function should be a smooth version of hinge loss.",abstractText,[0],[0]
"We also provide counterexamples to show that, when these conditions are relaxed, the result may not hold.",abstractText,[0],[0]
Understanding the Loss Surface of Neural Networks for Binary Classification,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1946–1956 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).",1 Introduction,[0],[0]
"Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT.
",1 Introduction,[0],[0]
"The decoder makes use of K NMT networks which are either trained independently (Sutskever et al., 2014; Chung et al., 2016; Neubig, 2016; Wu et al., 2016) or share some amount of training iterations (Sennrich et al., 2016b,a; Cromieres et al., 2016; Durrani et al., 2017).",1 Introduction,[0],[0]
"The ensemble decoder computes predictions from each of the individual models which are then combined using the arithmetic average (Sutskever et al., 2014) or the geometric average (Cromieres et al., 2016).
",1 Introduction,[0],[0]
Ensembling consistently outperforms single NMT by a large margin.,1 Introduction,[0],[0]
"However, the decoding speed is significantly worse since the decoder needs to apply K NMT models rather than only one.",1 Introduction,[0],[0]
"Therefore, a recent line of research transfers the idea of knowledge distillation (Bucilu et al., 2006; Hinton et al., 2014) to NMT and trains a smaller network (the student) by minimizing the cross-entropy to the output of the ensemble system (the teacher) (Kim and Rush, 2016; Freitag et al., 2017).",1 Introduction,[0],[0]
This paper presents an alternative to knowledge distillation as we aim to speed up decoding to be comparable to single NMT while retaining the boost in translation accuracy from the ensemble.,1 Introduction,[0],[0]
"In a first step, we describe how to construct a single large neural network which imitates the output of an ensemble of multiple networks with the same topology.",1 Introduction,[0],[0]
We will refer to this process as unfolding.,1 Introduction,[0],[0]
GPU-based decoding with the unfolded network is often much faster than ensemble decoding since more work can be done on the GPU.,1 Introduction,[0],[0]
"In a second step, we explore methods to reduce the size of the unfolded network.",1 Introduction,[0],[0]
"This idea is justified by the fact that ensembled neural networks are often over-parameterized and have a large degree of redundancy (LeCun et al., 1989; Hassibi et al., 1993; Srinivas and Babu, 2015).",1 Introduction,[0],[0]
Shrinking the unfolded network leads to a smaller model which consumes less space on the disk and in the memory; a crucial factor on mobile devices.,1 Introduction,[1.0],['Shrinking the unfolded network leads to a smaller model which consumes less space on the disk and in the memory; a crucial factor on mobile devices.']
"More importantly, the
1946
decoding speed on all platforms benefits greatly from the reduced number of neurons.",1 Introduction,[0],[0]
We find that the dimensionality of linear embedding layers in the NMT network can be reduced heavily by lowrank matrix approximation based on singular value decomposition (SVD).,1 Introduction,[0],[0]
"This suggest that high dimensional embedding layers may be needed for training, but do not play an important role for decoding.",1 Introduction,[0],[0]
"The NMT network, however, also consists of complex layers like gated recurrent units (Cho et al., 2014, GRUs) and attention (Bahdanau et al., 2015).",1 Introduction,[0],[0]
"Therefore, we introduce a novel algorithm based on linear combinations of neurons which can be applied either during training (data-bound) or directly on the weight matrices without using training data (data-free).",1 Introduction,[0],[0]
We report that with a mix of the presented shrinking methods we are able to reduce the size of the unfolded network to the size of the single NMT network while keeping the boost in BLEU score from the ensemble.,1 Introduction,[0],[0]
"Depending on the aggressiveness of shrinking, we report either a gain of 2.2 BLEU at the same decoding speed, or a 3.4× CPU decoding speed up with only a minor drop in BLEU compared to the original single NMT system.",1 Introduction,[1.0],"['Depending on the aggressiveness of shrinking, we report either a gain of 2.2 BLEU at the same decoding speed, or a 3.4× CPU decoding speed up with only a minor drop in BLEU compared to the original single NMT system.']"
"Furthermore, it is often much easier to stage a single NMT system than an ensemble in a commercial MT workflow, and it is crucial to be able to optimize quality at specific speed and memory constraints.",1 Introduction,[0],[0]
Unfolding and shrinking address these problems directly.,1 Introduction,[0],[0]
The first concept of our approach is called unfolding.,2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
Unfolding is an alternative to ensembling of multiple neural networks with the same topology.,2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"Rather than averaging their predictions, unfolding constructs a single large neural net out of the indi-
vidual models which has the same number of input and output neurons but larger inner layers.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"Our main motivation for unfolding is to obtain a single network with ensemble level performance which can be shrunk with the techniques in Sec. 3.
",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
Suppose we ensemble two single layer feedforward neural nets as shown in Fig. 1.,2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"Normally, ensembling is implemented by performing an isolated forward pass through the first network (Fig. 1(a)), another isolated forward pass through the second network (Fig. 1(b)), and averaging the activities in the output layers of both networks.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
This can be simulated by merging both networks into a single large network as shown in Fig. 1(c).,2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"The first neurons in the hidden layer of the combined network correspond to the hidden layer in the first single network, and the others to the hidden layer of the second network.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
A single pass through the combined network yields the same output as the ensemble if the output layer is linear (up to a factor 2).,2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
The weight matrices in the unfolded network can be constructed by stacking the corresponding weight matrices (either horizontally or vertically) in network 1 and 2.,2 Unfolding K Networks into a Single Large Neural Network,[1.0],['The weight matrices in the unfolded network can be constructed by stacking the corresponding weight matrices (either horizontally or vertically) in network 1 and 2.']
"This kind of aggregation of multiple networks with the same topology is not only possible for single-layer feedforward architectures but also for complex networks consisting of multiple GRU layers and attention.
",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"For a formal description of unfolding we address layers with indices d = 0, 1, . . .",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
", D. The special layer 0 has a single neuron for modelling bias vectors.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
Layer 1 holds the input neurons and layer D is the output layer.,2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
We denote the size of a layer in the individual models as s(d).,2 Unfolding K Networks into a Single Large Neural Network,[1.0],['We denote the size of a layer in the individual models as s(d).']
"When combining K networks, the layer size s′(d) in the unfolded network is increased by factor K if d is an inner layer, and equal to s(d) if d is the in-
put or output layer.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"We denote the weight matrix between two layers d1, d2 ∈",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"[0, D] in the k-th individual model (k ∈",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"[1,K]) as Wk(d1, d2) ∈ Rs(d1)×s(d2), and the corresponding weight matrix in the unfolded network as W ′(d1, d2) ∈ Rs′(d1)×s′(d2).",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
We explicitly allow d1 and d2 to be non-consecutive or reversed to be able to model recurrent networks.,2 Unfolding K Networks into a Single Large Neural Network,[1.0],['We explicitly allow d1 and d2 to be non-consecutive or reversed to be able to model recurrent networks.']
We use the zero-matrix if layers d1 and d2 are not connected.,2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"The construction of the unfolded weight matrix W ′(d1, d2) from the individual matrices Wk(d1, d2) depends on whether the connected layers are inner layers or not.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"The complete formula is listed in Fig. 2.
",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
Unfolded NMT networks approximate but do not exactly match the output of the ensemble due to two reasons.,2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"First, the unfolded network synchronizes the attentions of the individual models.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
Each decoding step in the unfolded network computes a single attention weight vector.,2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"In contrast, ensemble decoding would compute one attention weight vector for each of the K input models.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"A second difference is that the ensemble decoder first applies the softmax at the output layer, and then averages the prediction probabilities.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"The unfolded network averages the neuron activities (i.e. the logits) first, and then applies the softmax function.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
"Interestingly, as shown in Sec. 4, these differences do not have any impact on the BLEU score but yield potential speed advantages of unfolding since the computationally expensive softmax layer is only applied once.",2 Unfolding K Networks into a Single Large Neural Network,[0],[0]
After constructing the weight matrices of the unfolded network we reduce the size of it by iteratively shrinking layer sizes.,3 Shrinking the Unfolded Network,[1.0],['After constructing the weight matrices of the unfolded network we reduce the size of it by iteratively shrinking layer sizes.']
"In this section we denote the incoming weight matrix of the layer to
shrink as U ∈ Rmin×m and the outgoing weight matrix as V ∈ Rm×mout .",3 Shrinking the Unfolded Network,[0.9999999307809097],['In this section we denote the incoming weight matrix of the layer to shrink as U ∈ Rmin×m and the outgoing weight matrix as V ∈ Rm×mout .']
Our procedure is inspired by the method of Srinivas and Babu (2015).,3 Shrinking the Unfolded Network,[1.0],['Our procedure is inspired by the method of Srinivas and Babu (2015).']
They propose a criterion for removing neurons in inner layers of the network based on two intuitions.,3 Shrinking the Unfolded Network,[0],[0]
"First, similarly to Hebb’s learning rule, they detect redundancy by the principle neurons which fire together, wire together.",3 Shrinking the Unfolded Network,[0],[0]
"If the incoming weight vectors U:,i and U:,j are exactly the same for two neurons i and j, we can remove the neuron j and add its outgoing connections to neuron i (Vi,: ← Vi,: +",3 Shrinking the Unfolded Network,[0.9711915525234315],"['If the incoming weight vectors U:,i and U:,j are exactly the same for two neurons i and j, we can remove the neuron j and add its outgoing connections to neuron i (Vi,: ← Vi,: + Vj,:) without changing the output.1 This holds since the activity in neuron j will always be equal to the activity in neuron i.']"
"Vj,:) without changing the output.1",3 Shrinking the Unfolded Network,[0],[0]
This holds since the activity in neuron j will always be equal to the activity in neuron i.,3 Shrinking the Unfolded Network,[0],[0]
"In practice, Srinivas and Babu use a distance measure based on the difference of the incoming weight vectors to search for similar neurons as exact matches are very rare.
",3 Shrinking the Unfolded Network,[0],[0]
The second intuition of the criterion used by Srinivas and Babu (2015) is that neurons with small outgoing weights contribute very little overall.,3 Shrinking the Unfolded Network,[0],[0]
"Therefore, they search for a pair of neurons i, j ∈",3 Shrinking the Unfolded Network,[0],[0]
"[1,m] according the following term and remove the j-th neuron.2
arg min i,j∈[1,m]
||U:,i",3 Shrinking the Unfolded Network,[0],[0]
"− U:,j ||22||Vj,:||22 (1)
",3 Shrinking the Unfolded Network,[0],[0]
Neuron j is selected for removal if (1) there is another neuron i which has a very similar set of incoming weights and if (2) j has a small outgoing weight vector.,3 Shrinking the Unfolded Network,[0.9544149330288535],"['Therefore, they search for a pair of neurons i, j ∈ [1,m] according the following term and remove the j-th neuron.2 arg min i,j∈[1,m] ||U:,i − U:,j ||22||Vj,:||22 (1) Neuron j is selected for removal if (1) there is another neuron i which has a very similar set of incoming weights and if (2) j has a small outgoing weight vector.']"
"Their criterion is data-free since
1We denote the i-th row vector of a matrix A with Ai,: and the i-th column vector as A:,i.
",3 Shrinking the Unfolded Network,[0],[0]
2Note that the criterion in Eq. 1 generalizes the criterion of Srinivas and Babu (2015) to multiple outgoing weights.,3 Shrinking the Unfolded Network,[0],[0]
Also note that Srinivas and Babu (2015) propose some heuristic improvements to this criterion.,3 Shrinking the Unfolded Network,[0],[0]
"However, these heuristics did not work well in our NMT experiments.
",3 Shrinking the Unfolded Network,[0],[0]
it does not require any training data.,3 Shrinking the Unfolded Network,[0],[0]
For further details we refer to Srinivas and Babu (2015).,3 Shrinking the Unfolded Network,[0],[0]
"Srinivas and Babu (2015) propose to add the outgoing weights of j to the weights of a similar neuron i to compensate for the removal of j. However, we have found that this approach does not work well on NMT networks.",3.1 Data-Free Neuron Removal,[0],[0]
We propose instead to compensate for the removal of a neuron by a linear combination of the remaining neurons in the layer.,3.1 Data-Free Neuron Removal,[0],[0]
Data-free shrinking assumes for the sake of deriving the update rule that the neuron activation function is linear.,3.1 Data-Free Neuron Removal,[0],[0]
We now ask the following question: How can we compensate as well as possible for the loss of neuron j such that the impact on the output of the whole network is minimized?,3.1 Data-Free Neuron Removal,[0],[0]
"Datafree shrinking represents the incoming weight vector of neuron j (U:,j) as linear combination of the incoming weight vectors of the other neurons.",3.1 Data-Free Neuron Removal,[0],[0]
"The linear factors can be found by satisfying the following linear system:
U:,¬jλ = U:,j (2)
where U:,¬j is matrix U without the j-th column.",3.1 Data-Free Neuron Removal,[0],[0]
"In practice, we use the method of ordinary least squares to find λ because the system may be overdetermined.",3.1 Data-Free Neuron Removal,[0],[0]
"The idea is that if we mix the outputs of all neurons in the layer by the λ-weights, we get the output of the j-th neuron.",3.1 Data-Free Neuron Removal,[0],[0]
"The row vector Vj,: contains the contributions of the j-th neuron to each of the neurons in the next layer.",3.1 Data-Free Neuron Removal,[0],[0]
"Rather than using these connections, we approximate their effect by adding some weight to the outgoing connections of the other neurons.",3.1 Data-Free Neuron Removal,[0],[0]
"How much weight depends on λ and the outgoing weights Vj,:.",3.1 Data-Free Neuron Removal,[0],[0]
"The factor Dk,l which we need to add to the outgoing connection of the k-th neuron to compensate for the loss of the j-th neuron on the l-th neuron in the next layer is:
Dk,l = λkVj,l (3)
",3.1 Data-Free Neuron Removal,[0],[0]
"Therefore, the update rule for V is:
V ← V +D (4) In the remainder we will refer to this method as data-free shrinking.",3.1 Data-Free Neuron Removal,[0],[0]
Note that we recover the update rule of Srinivas and Babu (2015) by setting λ to the i-th unit vector.,3.1 Data-Free Neuron Removal,[0],[0]
"Also note that the error introduced by our shrinking method is due to the
fact that we ignore the non-linearity, and that the solution for λ may not be exact.",3.1 Data-Free Neuron Removal,[0],[0]
"The method is error-free on linear layers as long as the residuals of the least-squares analysis in Eq. 2 are zero.
",3.1 Data-Free Neuron Removal,[0],[0]
"GRU layers The terminology of neurons needs some further elaboration for GRU layers which rather consist of update and reset gates and states (Cho et al., 2014).",3.1 Data-Free Neuron Removal,[0],[0]
"On GRU layers, we treat the states as neurons, i.e. the j-th neuron refers to the j-th entry in the GRU state vector.",3.1 Data-Free Neuron Removal,[0],[0]
Input connections to the gates are included in the incoming weight matrix U for estimating λ in Eq. 2.,3.1 Data-Free Neuron Removal,[0],[0]
Removing neuron j in a GRU layer means deleting the j-th entry in the states and both gate vectors.,3.1 Data-Free Neuron Removal,[0],[0]
"Although we find our data-free approach to be a substantial improvement over the methods of Srinivas and Babu (2015) on NMT networks, it still leads to a non-negligible decline in BLEU score when applied to recurrent GRU layers.",3.2 Data-Bound Neuron Removal,[0],[0]
"Our data-free method uses the incoming weights to identify similar neurons, i.e. neurons expected to have similar activities.",3.2 Data-Bound Neuron Removal,[0],[0]
"This works well enough for simple layers, but the interdependencies between the states and the gates inside gated layers like GRUs or LSTMs are complex enough that redundancies cannot be found simply by looking for similar weights.",3.2 Data-Bound Neuron Removal,[0],[0]
"In the spirit of Babaeizadeh et al. (2016), our data-bound version records neuron activities during training to estimate λ.",3.2 Data-Bound Neuron Removal,[0],[0]
We compensate for the removal of the j-th neuron by using a linear combination of the output of remaining neurons with similar activity patterns.,3.2 Data-Bound Neuron Removal,[0],[0]
"In each layer, we prune 40 neurons each 450 training iterations until the target layer size is reached.",3.2 Data-Bound Neuron Removal,[0],[0]
Let A be the matrix which holds the records of neuron activities in the layer since the last removal.,3.2 Data-Bound Neuron Removal,[0],[0]
"For example, for the decoder GRU layer, a batch size of 80, and target sentence lengths of 20,A has 20 · 80 · 450 = 720K rows and m (the number of neurons in the layer)",3.2 Data-Bound Neuron Removal,[0],[0]
"columns.3 Similarly to Eq. 2 we find interpolation weights λ using the method of least squares on the following linear system.
",3.2 Data-Bound Neuron Removal,[0],[0]
"A:,¬jλ = A:,j (5)
",3.2 Data-Bound Neuron Removal,[0],[0]
"The update rule for the outgoing weight matrix is the same as for our data-free method (Eq. 4).
",3.2 Data-Bound Neuron Removal,[0],[0]
"3In practice, we use a random sample of 50K rows rather than the full matrix to keep the complexity of the leastsquares analysis under control.
",3.2 Data-Bound Neuron Removal,[0],[0]
The key difference between data-free and databound shrinking is the way λ is estimated.,3.2 Data-Bound Neuron Removal,[0],[0]
"Datafree shrinking uses the similarities between incoming weights, and data-bound shrinking uses neuron activities recorded during training.",3.2 Data-Bound Neuron Removal,[1.0],"['Datafree shrinking uses the similarities between incoming weights, and data-bound shrinking uses neuron activities recorded during training.']"
"Once we select a neuron to remove, we estimate λ, compensate for the removal, and proceed with the shrunk network.",3.2 Data-Bound Neuron Removal,[0],[0]
Both methods are prior to any decoding and result in shrunk parameter files which are then loaded to the decoder.,3.2 Data-Bound Neuron Removal,[0],[0]
"Both methods remove neurons rather than single weights.
",3.2 Data-Bound Neuron Removal,[0],[0]
The data-bound algorithm runs gradient-based optimization on the unfolded network.,3.2 Data-Bound Neuron Removal,[0],[0]
"We use the AdaGrad (Duchi et al., 2011) step rule, a small learning rate of 0.0001, and aggressive step clipping at 0.05 to avoid destroying useful weights which were learned in the individual networks prior to the construction of the unfolded network.
",3.2 Data-Bound Neuron Removal,[0],[0]
Our data-bound algorithm uses a data-bound version of the neuron selection criterion in Eq. 1 which operates on the activity matrix A. We search for the pair,3.2 Data-Bound Neuron Removal,[0],[0]
"i, j ∈",3.2 Data-Bound Neuron Removal,[0],[0]
"[1,m] according the following term and remove neuron j.
arg min i,j∈[1,m]
||A:,i −A:,j ||22||A:,j ||22 (6)",3.2 Data-Bound Neuron Removal,[0],[0]
"The standard attention-based NMT network architecture (Bahdanau et al., 2015) includes three linear layers: the embedding layer in the encoder, and the output and feedback embedding layers in the decoder.",3.3 Shrinking Embedding Layers with SVD,[0],[0]
We have found that linear layers are particularly easy to shrink using low-rank matrix approximation.,3.3 Shrinking Embedding Layers with SVD,[0],[0]
As before we denote the incoming weight matrix as U ∈ Rmin×m and the outgoing weight matrix as V ∈ Rm×mout .,3.3 Shrinking Embedding Layers with SVD,[0],[0]
"Since the layer is linear, we could directly connect the previous layer with the next layer using the product of both weight matrices X = U · V .",3.3 Shrinking Embedding Layers with SVD,[0],[0]
"However, X may be very large.",3.3 Shrinking Embedding Layers with SVD,[0],[0]
"Therefore, we approximate X as a product of two low rank matrices Y ∈ Rmin×m′ and Z ∈ Rm′×mout (X ≈ Y Z) where m′ m is the desired layer size.",3.3 Shrinking Embedding Layers with SVD,[0],[0]
A very common way to find such a matrix factorization is using truncated singular value decomposition (SVD).,3.3 Shrinking Embedding Layers with SVD,[0],[0]
The layer is eventually shrunk by replacing U with Y and V with Z.,3.3 Shrinking Embedding Layers with SVD,[0],[0]
"The individual NMT systems we use as source for constructing the unfolded networks are trained us-
ing AdaDelta (Zeiler, 2012) on the Blocks/Theano implementation (van Merriënboer et al., 2015; Bastien et al., 2012) of the standard attentionbased NMT model (Bahdanau et al., 2015) with: 1000 dimensional GRU layers (Cho et al., 2014) in both the decoder and bidrectional encoder; a single maxout output layer (Goodfellow et al., 2013); and 620 dimensional embedding layers.",4 Results,[0],[0]
We follow Sennrich et al. (2016b) and use subword units based on byte pair encoding rather than words as modelling units.,4 Results,[0],[0]
"Our SGNMT decoder (Stahlberg et al., 2017)4 with a beam size of 12 is used in all experiments.",4 Results,[0],[0]
"Our primary corpus is the Japanese-English (Ja-En) ASPEC data set (Nakazawa et al., 2016).",4 Results,[0],[0]
We select a subset of 500K sentence pairs to train our models as suggested by Neubig et al. (2015).,4 Results,[0],[0]
We report cased BLEU scores calculated with Moses’ multi-bleu.pl to be strictly comparable to the evaluation done in the Workshop of Asian Translation (WAT).,4 Results,[0],[0]
"We also apply our method to the WMT data set for English-German (En-De), using the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets.",4 Results,[0],[0]
En-De BLEU scores are computed using mteval-v13a.pl as in the WMT evaluation.,4 Results,[0],[0]
We set the vocabulary sizes to 30K for Ja-En and 50K for En-De.,4 Results,[0],[0]
We also report the size factor for each model which is the total number of model parameters (sum of all weight matrix sizes) divided by the number of parameters in the original NMT network (86M for Ja-En and 120M for EnDe).,4 Results,[0],[0]
"We choose a widely used, simple ensembling method (prediction averaging) as our baseline.",4 Results,[0],[0]
"We feel that the prevalence of this method makes it a reasonable baseline for our experiments.
",4 Results,[0],[0]
"Shrinking the Unfolded Network First, we investigate which shrinking methods are effective for which layers.",4 Results,[0],[0]
"Tab. 1 summarizes our results on a 2-unfold network for Ja-En, i.e. two separate NMT networks are combined in a single large network as described in Sec. 2.",4 Results,[0],[0]
"The layers in the combined network are shrunk to the size of the original networks using the methods discussed in Sec. 3.
",4 Results,[0],[0]
Shrinking the linear embedding layers with SVD (Sec. 3.3) is very effective.,4 Results,[0],[0]
The unfolded model with shrunk embedding layers performs at the same level as the ensemble (compare rows (b) and (c)).,4 Results,[0],[0]
"In our initial experiments, we applied the method of Srinivas and Babu (2015) to
4‘vanilla’ decoding strategy
shrink the other layers, but their approach performed very poorly on this kind of network: the BLEU score dropped down to 15.5 on the development set when shrinking all layers except the decoder maxout and embedding layers, and to 9.9 BLEU when applying their method only to embedding layers.5 Row (e) in Tab. 1 shows that our data-free algorithm from Sec. 3.1 is better suited for shrinking the GRU and attention layers, leading to a drop of only 1 BLEU point compared to the ensemble (b) (i.e. 0.8 BLEU better than the single system (a)).",4 Results,[0],[0]
"However, using the data-bound version of our shrinking algorithm (Sec. 3.2) for the GRU layers performs best.6",4 Results,[0],[0]
The shrunk model yields about the same BLEU score as the ensemble on the test set (25.2 in (b) and 25.3 in (f)).,4 Results,[0],[0]
"Shrinking the maxout layer remains more of a challenge (rows (g) and (h)), but the number of parameters in this layer is small.",4 Results,[0],[0]
"Therefore, shrinking all layers except the maxout layer leads to almost the same number of parameters (factor 1.05 in row (f))",4 Results,[0],[0]
"as the original NMT network (a), and thus to a similar storage size, memory consumption, and decoding speed, but with a 1.8 BLEU gain.",4 Results,[0],[0]
"Based on these results we fix the shrinking method used for each layer for all remaining experiments as follows: We shrink linear embedding layers with our SVD-based method, GRU layers with our databound method, the attention layer with our datafree method, and do not shrink the maxout layer.
",4 Results,[0],[0]
Our data-bound algorithm from Sec. 3.2 has two mechanisms to compensate for the removal of a neuron.,4 Results,[0],[0]
"First, we use a linear combination of the remaining neurons to update the outgoing weight matrix by imitating its activations (Eq. 4).",4 Results,[0],[0]
"Second, stochastic gradient descent (SGD) fine-tunes all
5Results with the original method of Srinivas and Babu (2015) are not included in Tab. 1.
",4 Results,[0],[0]
"6If we apply different methods to different layers of the same network, we first apply SVD-based shrinking, then the data-free method, and finally the data-bound method.
weights during this process.",4 Results,[0],[0]
"Tab. 2 demonstrates that both mechanisms are crucial for minimizing the effect of shrinking on the BLEU score.
",4 Results,[0],[0]
Decoding Speed,4 Results,[0],[0]
"Our testing environment is an Ubuntu 16.04 with Linux 4.4.0 kernel, 32 GB RAM, an Intel R© Core i7-6700 CPU at 3.40 GHz and an Nvidia GeForce GTX Titan X GPU.",4 Results,[0],[0]
CPU decoding uses a single thread.,4 Results,[0],[0]
"We used the first 500 sentences of the Ja-En WAT development set for the time measurements.
",4 Results,[0],[0]
"Our results in Tab. 3 show that decoding with ensembles (rows (b) and (e)) is slow: combining the predictions of the individual models on the CPU is computationally expensive, and ensemble decoding requires K passes through the softmax layer which is also computationally expensive.",4 Results,[0],[0]
Unfolding the ensemble into a single network and shrinking the embedding and attention layers improves the runtimes on the GPU significantly without noticeable impact on BLEU (rows (c) and (f)).,4 Results,[0],[0]
This can be attributed to the fact that unfolding can reduce the communication overhead between CPU and GPU.,4 Results,[0],[0]
Comparing rows (d) and (g) with row (a) reveals that shrinking the unfolded networks even further speeds up CPU and GPU decoding almost to the level of single system decoding.,4 Results,[0],[0]
"However, more aggressive shrinking yields a BLEU score of 25.3 when combining three systems (row (g)) – 1.8 BLEU better than the single system, but 0.6 BLEU worse than the 3-
ensemble.",4 Results,[0.9999999141545102],"['However, more aggressive shrinking yields a BLEU score of 25.3 when combining three systems (row (g)) – 1.8 BLEU better than the single system, but 0.6 BLEU worse than the 3- ensemble.']"
"Therefore, we will investigate the impact of shrinking on the different layers in the next sections more thoroughly.
",4 Results,[0],[0]
Degrees of Redundancy in Different Layers We applied our shrinking methods to isolated layers in the 2-Unfold network of Tab. 1 (f).,4 Results,[0],[0]
Fig. 3 plots the BLEU score when isolated layers are shrunk even below their size in the original NMT network.,4 Results,[0.9928755093095335],['3 plots the BLEU score when isolated layers are shrunk even below their size in the original NMT network.']
The attention layer is very robust against shrinking and can be reduced to 100 neurons (10% of the original size) without impacting the BLEU score.,4 Results,[0],[0]
The embedding layers can be reduced to 60% but are sensitive to more aggressive pruning.,4 Results,[0],[0]
"Shrinking the GRU layers affects the BLEU score the most but still outperforms the single system when the GRU layers are shrunk to 30%.
",4 Results,[0],[0]
Adjusting the Target Sizes of Layers Based on our previous experiments we revise our approach to shrink the 3-Unfold system in Tab. 3.,4 Results,[0.9903454354132061],['Adjusting the Target Sizes of Layers Based on our previous experiments we revise our approach to shrink the 3-Unfold system in Tab.']
"Instead
of shrinking all layers except the maxout layer to the same degree, we adjust the aggressiveness of shrinking for each layer.",4 Results,[0],[0]
"We suggest three different setups (Normal, Small, and Tiny) with the layer sizes specified in Tab. 4.",4 Results,[0],[0]
"3-Unfold-Normal has the same number of parameters as the original NMT networks (size factor: 1.0), 3-UnfoldSmall is only half their size (size factor: 0.5), and 3-Unfold-Tiny reduces the size by two thirds (size factor: 0.33).",4 Results,[1.0],"['3-Unfold-Normal has the same number of parameters as the original NMT networks (size factor: 1.0), 3-UnfoldSmall is only half their size (size factor: 0.5), and 3-Unfold-Tiny reduces the size by two thirds (size factor: 0.33).']"
"When comparing rows (a) and (c) in Tab. 5 we observe that 3-Unfold-Normal yields a gain of 2.2 BLEU with respect to the original single system and a slight improvement in decoding speed at the same time.7 Networks with the size factor 1.0 like 3-Unfold-Normal are very likely to yield about the same decoding speed as the Single network regardless of the decoder implementation, machine learning framework, and hardware.",4 Results,[0],[0]
"Therefore, we think that similar results are possible on other platforms as well.
CPU decoding speed directly benefits even more from smaller setups – 3-Unfold-Tiny is only 0.3 BLEU worse than Single but decoding on a single CPU is 3.4 times faster (row (a) vs. row (e) in Tab. 5).",4 Results,[0.9596142511063696],['CPU decoding speed directly benefits even more from smaller setups – 3-Unfold-Tiny is only 0.3 BLEU worse than Single but decoding on a single CPU is 3.4 times faster (row (a) vs. row (e) in Tab.']
"This is of great practical use: batch decoding with only two CPU threads surpasses production speed which is often set to 2000 words per minute (Beck et al., 2016).",4 Results,[0],[0]
"Our initial experiments in Tab. 6 suggest that the Normal setup is applicable to En-De as well, with substantial improve-
7To validate that the gains come from ensembling and unfolding and not from the layer sizes in 3-Unfold-Normal we trained a network from scratch with the same dimensions.",4 Results,[0],[0]
"This network performed similarly to our Single system.
ments in BLEU compared to Single with about the same decoding speed.",4 Results,[0],[0]
"The idea of pruning neural networks to improve the compactness of the models dates back more than 25 years (LeCun et al., 1989).",5 Related Work,[0],[0]
"The literature is therefore vast (Augasta and Kathirvalavakumar, 2013).",5 Related Work,[0],[0]
One line of research aims to remove unimportant network connections.,5 Related Work,[0],[0]
"The connections can be selected for deletion based on the secondderivative of the training error with respect to the weight (LeCun et al., 1989; Hassibi et al., 1993), or by a threshold criterion on its magnitude (Han et al., 2015).",5 Related Work,[0],[0]
"See et al. (2016) confirmed a high degree of weight redundancy in NMT networks.
",5 Related Work,[0],[0]
In this work we are interested in removing neurons rather than single connections since we strive to shrink the unfolded network such that it resembles the layout of an individual model.,5 Related Work,[0],[0]
We argued in Sec. 4 that removing neurons rather than connections does not only improve the model size but also the memory footprint and decoding speed.,5 Related Work,[0],[0]
"As explained in Sec. 3.1, our data-free method is an extension of the approach by Srinivas and Babu (2015); our extension performs significantly better on NMT networks.",5 Related Work,[0],[0]
"Our data-bound method (Sec. 3.2) is inspired by Babaeizadeh et al. (2016) as we combine neurons with similar activities during training, but we use linear combinations of multiple neurons to compensate for the loss of a neuron rather than merging pairs of neurons.
",5 Related Work,[0],[0]
"Using low rank matrices for neural network compression, particularly approximations via SVD, has been studied widely in the literature (Denil et al., 2013; Denton et al., 2014; Xue et al., 2013; Prabhavalkar et al., 2016; Lu et al., 2016).",5 Related Work,[0],[0]
These approaches often use low rank matrices to approximate a full rank weight matrix in the original network.,5 Related Work,[0],[0]
"In contrast, we shrink an entire linear layer by applying SVD on the product of the incoming and outgoing weight matrices (Sec. 3.3).
",5 Related Work,[0],[0]
"In this paper we mimicked the output of the high performing but cumbersome ensemble by constructing a large unfolded network, and shrank this
network afterwards.",5 Related Work,[0],[0]
"Another approach, known as knowledge distillation, uses the large model (the teacher) to generate soft training labels for the smaller student network (Bucilu et al., 2006; Hinton et al., 2014).",5 Related Work,[0],[0]
The student network is trained by minimizing the cross-entropy to the teacher.,5 Related Work,[0],[0]
"This idea has been applied to sequence modelling tasks such as machine translation and speech recognition (Wong and Gales, 2016; Kim and Rush, 2016; Freitag et al., 2017).",5 Related Work,[0],[0]
"Our approach can be computationally more efficient as the training set does not have to be decoded by the large teacher network.
",5 Related Work,[0],[0]
Junczys-Dowmunt et al. (2016a; 2016b) reported gains from averaging the weight matrices of multiple checkpoints of the same training run.,5 Related Work,[0],[0]
"However, our attempts to replicate their approach were not successful.",5 Related Work,[0],[0]
"Averaging might work well when the behaviour of corresponding units is similar across networks, but that cannot be guaranteed when networks are trained independently.",5 Related Work,[0],[0]
We have described a generic method for improving the decoding speed and BLEU score of single system NMT.,6 Conclusion,[0],[0]
Our approach involves unfolding an ensemble of multiple systems into a single large neural network and shrinking this network by removing redundant neurons.,6 Conclusion,[1.0],['Our approach involves unfolding an ensemble of multiple systems into a single large neural network and shrinking this network by removing redundant neurons.']
"Our best results on Japanese-English either yield a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a 3.4×CPU decoding speed up with only a minor drop in BLEU.
",6 Conclusion,[0],[0]
The current formulation of unfolding works for networks of the same topology as the concatenation of layers is only possible for analogous layers in different networks.,6 Conclusion,[0],[0]
"Unfolding and shrinking diverse networks could be possible, for example by applying the technique only to the input and output layers or by some other scheme of finding associations between units in different models, but we leave this investigation to future work as models in NMT ensembles in current research usually have the same topology (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Durrani et al., 2017).",6 Conclusion,[0],[0]
This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1).,Acknowledgments,[0],[0]
"Data-free and data-bound shrinking can be interpreted as setting the expected difference between network outputs before and after a removal operation to zero under different assumptions.
",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"For simplicity, we focus our probabilistic treatment of shrinking on single layer feedforward networks.",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
Such a network maps an input x ∈ Rmin to an output y ∈ Rmout .,Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"The l-th output yl is computed according the following equation
yl = ∑
k∈[1,m] σ(xuTk )",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"Vk,l (7)
where uk ∈ Rmin is the incoming weight vector of the k-th hidden neuron (denoted as U:,k in the main paper) and V ∈ Rm×mout the outgoing weight matrix of the m-dimensional hidden layer.",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"We now remove the j-th neuron in the hidden layer and modify the outgoing weights to compensate for the removal:
y′l = ∑
k∈[1,m]\{j} σ(xuTk )V
′",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"k,l (8)
where y′l is the output after the removal operation and V ′ ∈ Rm×mout are the modified outgoing weights.",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"Our goal is to choose V ′ such that the expected error introduced by removing neuron j is zero:
Ex(yl − y′l)",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"= 0 (9)
Data-free shrinking Data-free shrinking makes two assumptions to satisfy Eq. 9.",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"First, we assume that the incoming weight vector uj can be represented as linear combination of the other weight vectors.
uj = ∑
k∈[1,m]\{j} λkuk (10)
Second, it assumes that the neuron activation function σ(·) is linear.",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
Starting with Eqs.,Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
7 and 8 we can write Ex(yl − y′l),Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"as
Ex ( σ(xuTj )",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"Vj,l + ∑ k∈[1,m]\{j}
σ(xuTk )(Vk,l − V ′k,l)︸ ︷︷ ︸ :=R
)
Eq. 10 = Ex ( σ(x( ∑ k∈[1,m]\{j} λkuk)T )Vj,l +R )
σ(·) lin. =",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"Ex ( ∑ k∈[1,m]\{j} σ(xuTk )λkVj,l +R )
= ∑ k∈[1,m]\{j} Ex ( σ(xuTk ) )",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"(Vk,l − V ′k,l + λkVj,l)
We set this term to zero (and thus satisfy Eq. 9) by setting each component of the sum to zero.
∀k ∈",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"[1,m] \ {j} : V ′k,l = Vk,l + λkVj,l (11)",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"This condition is directly implemented by the update rule in our shrinking algorithm (Eq. 3 and 4).
",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
Data-bound shrinking Data-bound shrinking does not require linearity in σ(·).,Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"It rather assumes that the expected value of the neuron activity j is a linear combination of the expected values of the other activities:
Ex(σ(xuTj ))",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"= ∑
k∈[1,m]\{j} λkEx(σ(xuTk ))",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"(12)
Ex(·) is estimated using importance sampling:
Êx(σ(xuTk );X )",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"= 1 |X | ∑ x′∈X σ(x′uTk ) (13)
",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"In practice, the samples inX are collected in the activity matrix A from Sec. 3.2.",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"We can satisfy Eq. 9 by using the λ-values from Eq. 12, so that Ex(yl − y′l) becomes
Eqs.",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"7,8 = Ex ( σ(xuTj )",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"Vj,l
+ ∑
k∈[1,m]\{j} σ(xuTk )(Vk,l − V ′k,l) )",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
= Ex(σ(xuTj ),Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"Vj,l)
+ ∑
k∈[1,m]\{j} Ex(σ(xuTk ))",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"(Vk,l − V ′k,l)
Eq. 12 = ∑ k∈[1,m]\{j} Ex(σ(xuTk ))",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
"(Vk,l − V ′k,l + λkVj,l)
Again, we set this to zero using Eq. 11.",Appendix: Probabilistic Interpretation of Data-Free and Data-Bound Shrinking,[0],[0]
Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance.,abstractText,[0],[0]
"Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models.",abstractText,[0],[0]
Ensembling often improves the quality of the generated translations drastically.,abstractText,[0],[0]
"However, it is not suitable for production systems because it is cumbersome and slow.",abstractText,[0],[0]
This work aims to reduce the runtime to be on par with a single system without compromising the translation quality.,abstractText,[0],[0]
"First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system.",abstractText,[0],[0]
We show that unfolding can already improve the runtime in practice since more work can be done on the GPU.,abstractText,[0],[0]
We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers.,abstractText,[0],[0]
On JapaneseEnglish we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.,abstractText,[0],[0]
Unfolding and Shrinking Neural Machine Translation Ensembles,title,[0],[0]
