0	4	Many decision problems can be phrased as coordination problems of many artificial intelligent agents (Boutilier, 1996).
1	104	Examples include robot soccer (Kok et al., 2003), warehouse commissioning (Claes et al., 2017), and traffic light control (Wiering, 2000).
5	15	This means that the total reward to optimise can be decomposed into a sum of local rewards that only depend on (possibly overlapping) subsets of agents.
14	191	In this paper, we formalize multi-agent multi-armed bandits (MAMABs) and investigate how to balance exploration and exploitation in the joint action taken by the agents, such that the loss due to taking suboptimal joint actions during learning is bounded.
15	38	Building on the upper confidence bound (UCB) framework (Auer et al., 2002) for single-agent multi-armed bandits, we formulate a new algorithm that we call multi-agent upper confidence exploration (MAUCE) (Section 4).
16	48	MAUCE balances exploitation and exploration using local estimates and local upper confidence bounds.
17	3	We prove in Section 5 that MAUCE achieves a regret bound that depends on the harmonic mean of the local upper confidence bounds, rather than their sum, as we would get by applying the combinatorial bandit framework (Cesa-Bianchi & Lugosi, 2012; Chen et al., 2013).
18	51	This leads to a regret logarithmic in the number of arm-pulls and linear in the number of agents.
19	15	In contrast, the naive approach of considering the full joint action is exponential in the number of agents.
20	25	In Section 6 we compare empirically the performance of MAUCE to other approaches from the literature, and show that it achieves much less regret in various settings, including wind farm control.
32	38	Before introducing our new algorithm, we first need to define our learning problem.
42	37	A local function fe only depends on the joint action ae of the subset De of agents.
43	39	We refer to the mean reward of a joint action as µa, which in turn is factorized into the same local reward components as F (a): µa = ∑ρ e=1 µ(a e).
44	35	For simplicity, we refer to an agent i by its index.
54	22	This exploits the graphical structure resulting from the factorization, and the size of the local subproblems depends only on the induced width, i.e., how many agents Algorithm 1 MAUCE 1: Input: An MAMAB with a factorized reward function, F (a) = ∑ρ e=1 f e(ae), a time horizon T 2: Initialize µ̂e(ae) and ne(ae) to zero.
56	112	When the coordination graph is sparse, i.e., agents are only involved in a small number of local reward functions, the induced width is typically much smaller than the size of the joint action space, making the maximization problem tractable.
57	19	When we are not simply maximizing over the joint actions to extract the optimal reward, but also need to explore to learn what the values of the mean rewards are, the situation becomes more complex.
58	8	Again, we could ‘flatten’ the MAMAB by treating each joint action as a separate arm in a single-agent MAB, but this quickly leads to too many arms to be able to learn effectively with popular algorithms such as UCB (Auer et al., 2002) of which the regret bounds depend on the number of arms.
60	8	Instead, we propose to treat exploration and exploitation as separate objectives during a VE-like scheme, and taking inspiration from the multiobjective literature (Roijers et al., 2015), define a new VElike subroutine, that allows us to define a MAUCE (Section 4) for which we can prove a much tighter regret-bound.
62	42	MAUCE executes a joint action at every timestep that maximizes the estimated mean reward for a given factorization of the reward function, µ̂(a), plus an exploration bonus, ct(a), that is computed using the same factorization.
66	30	This enables the algorithm to exploit the graphical structure to compute tighter exploration bonuses while guaranteeing a tight regret bound.
71	12	Contrary to single-agent MABs, it is not trivial to maximize over µ̂(a) + ct(a), as we need to maximize over a A efficiently, and ct(a) is a non-linear function in the local counts net (a e).
82	205	This results in an algorithm in which the number of vectors in the intermediate solution sets steeply decreases in the last agent eliminations (in contrast to MOVE, in which the intermediate sets typically continue to grow in size).
84	27	First, we define the input of UCVE.
86	6	Specifically, we define the input to UCVE as a set F of local upper confidence vector set functions (UCVSFs).
87	65	For each fe of F (a), F contains an identically scoped UCVSF ue.
88	8	Each ue initially contains a singleton set, ue(ae) = {ve(ae)}, where ve(ae) is defined as in Equation 1.
89	75	Eliminating an agent i, is performed by replacing all ue(ae) which have i in scope, i.e., i ∈ De, by a new function that incorporates the possibly optimal responses of i.
91	99	Algorithm 2 UCVE(F) Input : A set of local upper confidence vector set functions F and an elimination order q (a queue with all agents) Output : An optimal joint action, a∗ 1: while q is not empty do 2: i← q.dequeue() 3: Fi ← the subset of UCVSFs inF that have i in scope 4: xu, xl ← compute upper and lower bounds on the exploration part of the vectors for the remaining factors in F \ Fi 5: unew(·)← a new UCVSF 6: for all ae−i ∈ ADe\{i} do 7: V ← ⋃ ai∈Ai ⊕ ue∈Fi ue(ae−i × {ai}) 8: unew(ae−i)←prune(V, xu, xl) 9: end for 10: F ← F \ Fi ∪ {unew} 11: end while 12: u← retrieve final factor from F 13: return the optimal joint action from u UCVE is provided in Algorithm 2.
99	51	Note that the resulting actions always include the appropriate actions ai (which is under the union) and the appropriate actions from ae−i.
