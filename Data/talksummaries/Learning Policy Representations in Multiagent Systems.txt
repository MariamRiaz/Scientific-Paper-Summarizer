0	25	Intelligent agents rarely act in isolation in the real world and often seek to achieve their goals through interaction with other agents.
2	41	Depending on the underlying motivations of the agents, interactions could be directed towards achieving a shared goal in a collaborative setting, opposing another agent in a competitive setting, or be a mixture of these in a setting where agents collaborate in teams to compete against other teams.
3	32	Learning useful representations of the policies of agents based on their interactions is an important step towards characterization of the agent behavior and more generally inference and reasoning in multiagent systems.
4	29	In this work, we propose an unsupervised encoder-decoder framework for learning continuous representations of agent policies given access to only a few episodes of interaction.
5	101	For any given agent, the representation function is an encoder that learns a mapping from an interaction (i.e., one or more episodes of observation and action pairs involving the agent) to a continuous embedding vector.
51	16	The representation should be able to distinguish the agent’s policy with the policies of other agents.
69	114	To learn a representation for agent i based on interaction episodes, we use the representation function fθ to compute three sets of embeddings: (i) a positive embedding for an episode e+ ∼ Ei involving agent i, (ii) a negative embedding for an episode e− ∼ Ej involving a random agent j 6= i, and (iii) a reference embedding for an episode e∗ ∼ Ei again involving agent i, but different from e+.
71	51	Intuitively, the loss encourages the positive embedding to be closer to the reference embedding than the negative embedding, which makes the embeddings of the same agent tend to cluster together and be further away from embeddings of other agents.
76	20	(2) to get the final objective used for representation learning: 1 n n∑ i=1 Ee+∼Ei, e∗∼Ei\e+  ∑ 〈o,a〉∼e+ log πφ,θ(a|o, e∗)︸ ︷︷ ︸ imitation − λ ∑ j 6=i Ee−∼Ej [dθ(e+, e−, e∗)]︸ ︷︷ ︸ agent identification  (3) where λ > 0 is a tunable hyperparameter that controls the relative weights of the discriminative and generative terms.
79	15	Generalization is well-understood for supervised learning— models that shows similar train and test performance exhibit good generalization.
80	19	To measure the quality of the learned representations for a multiagent system (MAS), we introduce a graphical formalism for reasoning about agents and their interactions.
84	17	The agent-interaction graph describes interactions between a set of agent policies P and a set of interaction episodes I through a graph G = (P, I).1 An example graph is shown in Figure 1a.
90	26	In particular, we consider the following cases: Weak generalization.2 Here, we are interested in the generalization performance of the representation function on an unseen interaction between existing agents, all of which are observed during training.
91	17	This corresponds to the red edge representing the interaction between Alice and Bob in Figure 1a.
98	16	Since the representation function is learned using an unsupervised auxiliary objective, we test its generalization performance by evaluating the usefulness of these embeddings for various kinds downstream tasks described below.
117	65	The RoboSumo environment For the competitive environment, we use RoboSumo (AlShedivat et al., 2018)—a 3D environment with simulated physics (based on MuJoCo (Todorov et al., 2012)) that allows agents to control multi-legged 3D robots and compete against each other in continuous-time wrestling games (Figure 1b).
118	41	For our analysis, we train a diverse collection of 25 agents, some of which are trained via self-play and others are trained in pairs concurrently using Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017).
134	19	We qualitatively visualize the embeddings learned using Emb-Hyb by projecting them on the leading principal components, as shown in Figures 3a and 3b for 10 test interaction episodes of 5 randomly selected agents in the weak and strong generalization settings respectively.
140	51	Here we ask whether embeddings can be used to improve learned policies in a reinforcement learning setting both in terms of end performance and generalization.
141	25	To this end, we select 5 training, 5 validation, and 5 testing opponents from the pool of 25 pre-trained agents.
146	19	Figure 4 shows the average win rates against the set of training and testing opponents for the baseline and our agents that use different types of embeddings.
170	16	We wish to learn embeddings of listeners based on their interactions with speakers.
174	17	The intra-inter clustering ratios are shown in Table 2, and the projections of the embeddings learned using Emb-Hyb are visualized in Figure 3c and Figure 3d for weak and strong generalization respectively.
182	18	The baseline MADDPG achieves the lowest training error, but fails to generalize well enough and incurs a low average reward for the test listener agents.
199	16	In this work, we presented a framework for learning representations of agent policies in multiagent systems.
201	67	Our learning objective is based on a novel combination of a generative component based on imitation learning and a discriminative component for distinguishing the embeddings of different agent policies.
202	33	Our overall framework is unsupervised, sample-efficient, and domainagnostic, and hence can be readily extended to many environments and downstream tasks.
203	46	Most importantly, we showed the role of these embeddings as privileged information for learning more adaptive agent policies in both collaborative and competitive settings.
204	87	In the future, we would like to explore multiagent systems with more than two agents participating in the interactions.
206	17	Finally, it would be interesting to extend and evaluate the proposed framework to learn representations for history dependent policies such as those parameterized by long short-term memory networks.
