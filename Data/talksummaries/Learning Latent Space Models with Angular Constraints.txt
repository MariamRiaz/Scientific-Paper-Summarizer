0	11	Latent space models (LSMs), such as sparse coding (Olshausen & Field, 1997), topic models (Blei et al., 2003) and neural networks, are widely used in machine learning to extract hidden patterns and learn latent representations of data.
3	16	For instance, in a topic model (Blei et al., 2003), the components are referred to as topics, aiming at discovering the semantics underlying documents.
4	10	Each topic is associated with a weight vector.
6	18	For instance, in the LightLDA (Yuan et al., 2015) topic model, the number of topics is 1 million and the dimension of topic vector is 50000, resulting in a topic matrix with 50 billion parameters.
10	18	Many regularizers have been proposed, including `2 regularization, `1 regularization (Tibshirani, 1996), nuclear norm (Recht et al., 2010), Dropout (Srivastava et al., 2014) and so on.
11	9	Recently, a new type of regularization approaches (Yu et al., 2011; Zou & Adams, 2012a; Xie et al., 2015; 2016a; Rodrı́guez et al., 2016), which aim at encouraging the weight vectors of components in LSMs to be “diverse”, are emerging.
16	22	One intuitive explanation could be: promoting diversity imposes a structural constraint on model parameters, which reduces the model capacity of LSMs and therefore alleviates overfitting.
20	11	Using sparse coding and neural network as study cases, we analyze how ACs affect the generalization performance of these two LSMs.
66	11	Let L(W) denote the objective function of this LSM.
67	10	Similar to (Bao et al., 2013; Xie et al., 2015; Rodrı́guez et al., 2016), we use angle to characterize diversity: the components are considered to be more diverse if they are close to being orthogonal, i.e., their angles are close to π2 .
68	16	To encourage this, we require the absolute value of cosine similarity between each pair of components to be less than a small value τ , which leads to the following angle-constrained LSM (AC-LSM) problem min W L(W) s.t.
70	10	A smaller τ indicates that the vectors are more close to being orthogonality, and hence are more diverse.
72	12	In this section, we apply the ACs to two LSMs.
73	17	Sparse Coding Given a set of data samples {xi}ni=1, where x ∈ Rd, sparse coding (SC) (Olshausen & Field, 1997) aims to use a set of “basis” vectors (referred to as dictionary) W = {wj}mj=1 to reconstruct the data samples.
75	12	The reconstruction error is measured using the squared `2 norm ‖x− ∑m j=1 αjwj‖22.
83	15	Applying ACs to the weight vectors of hidden units, we obtain the following AC-NN problem min W L(W) s.t.
88	13	(1) can be transformed into min W̃,G L(W̃,G) s.t.
104	17	Case 1 First, we assume λ1 = 0, λ2 = 0, then (ρ + 2γ1)v (r) 1 = y (r) 1 + ρw̃p(r) and (ρ + 2γ2)v (r) 2 = y (r) 2 + ρw̃q(r).
106	15	If so, then v(r)1 and v (r) 2 are the optimal solution.
110	10	(9), we get (ρ+2γ2) 2+λ21+2(ρ+2γ2)λ1τ = ‖y (r) 2 +ρw̃q(r)‖22 (11) Taking the inner product of the two vectors on the left hand sides of Eq.
116	18	The corresponding problems can be solved in a similar way as Case 2.
125	19	Following (Vainsencher et al., 2011), we assume the data example x ∈ Rd and basis vector w ∈ Rd are both of unit length, and the linear coefficient vector a ∈ Rm is at most k sparse, i.e., ‖a‖0 ≤ k. The estimation error of dictionary W is defined as L(W) = Ex∼p∗ [min‖a‖0≤k ‖x− ∑m j=1 ajwj‖2].
136	16	For simplicity, we start with a “simple” fully connected network with one hidden layer of m units, used for univariate regression (one output unit) with squared loss.
158	17	This is because a larger number of hidden units bear a larger difficulty in satisfying the pairwise ACs, which causes the function space F to shrink rapidly; accordingly, the approximation power of F decreases quickly.
165	13	For each dataset, five random train/test splits are performed and the results are averaged over the five runs.
175	10	This suggests that AC is effective in reducing overfitting and improving generalization performance.
208	14	Table 3 shows state of the art classification error on the test set.
214	15	The LSTM is used for a question answering (QA) task on two datasets: CNN and DailyMail (Hermann et al., 2015), each containing a training, development and test set with 300k/4k/3k and 879k/65k/53k examples respectively.
224	26	We compare with four diversity promoting regularizers: CS, IC, MA and DC.
242	11	First, it is theoretically analyzable: the generalization error analysis shows that larger diversity leads to smaller estimation error and larger approximation error.
