0	18	The current leading perspective on temporal information extraction regards three phases: (1) a temporal entity recognition phase, extracting events (blue boxes in Fig.
1	38	1) and their attributes, and extracting temporal expressions (green boxes), and normalizing their values to dates or durations, (2) a relation extraction phase, where temporal links (TLinks) among those entities, and between events and the document-creation time (DCT) are found (arrows in Fig.
4	54	Much research concentrated on the first two steps, but very little research looks into step 3, time-line construction, which is the focus of this work.
5	143	In this paper, we propose a new time-line construction paradigm that evades phase 2, the relation extraction phase, because in the classical paradigm temporal relation extraction comes with many difficulties in training and prediction that arise from the fact that for a text with n temporal entities (events or temporal expressions) there are n2 possible entity pairs, which makes it likely for annotators to miss relations, and makes inference slow as n2 pairs need to be considered.
7	11	The ultimate goal of our proposed paradigm is to predict from a text in which entities are already detected, for each entity: (1) a probability distribution on the entity’s starting point, and (2) another distribution on the entity’s duration.
9	8	Constructed time-lines allow for further quantitative reasoning with the temporal information, if this would be needed for certain applications.
10	57	As a first approach towards this goal, in this paper, we propose several initial time-line models in this paradigm, that directly predict - in a linear fashion - start points and durations for each entity, using text with annotated temporal entities as input (shown in Fig.
11	55	The predicted start points and durations constitute a relative time-line, i.e. a total order on entity start and end points.
12	15	The time-line is relative, as start and duration values cannot (yet) be mapped to absolute calender dates or durations expressed in seconds.
48	6	The downside is that S-TLM is limited in its use of contextual information.
49	12	To better exploit the entity context we also propose a contextual time-line model C-TLM (solid edges in Fig 2), that first encodes the full text using two bi-directional recurrent neural networks, one for entity starts (BiRNNs), and one for entity durations (BiRNNd).1 On top of the encoded text we learn two linear mappings, one from the BiRNNd output of the last word of the entity mention to its duration d, and similarly for the start time, from the BiRNNs output to the entity’s start s.
52	22	ei = si +max(di, dmin) (1) Predicting durations rather than end-points makes it easy to control that the end-point lies after the start-point by constraining the duration di by a constant minimum duration value dmin above 0, as shown in Eq.
59	26	Modeling dDCT as a variable allows growth of dDCT and averts this issue.3
64	33	As TLinks relate entities (intervals), we first convert the TLinks to expressions that relate the start and end points of entities.
88	9	To represent this perspective, we also define a ranking loss with a score margin mh in Eq.
90	58	For each batch we (1) perform a forward pass, (2) calculate the total loss (for one of the loss functions), (3) derive gradients using Adam6 (Kingma and Ba, 2014), and (4) update the model parameters θ via back-propagation.
91	9	After each epoch we shuffle the training texts.
96	16	TL2RTL on itself is a method and not a model.
98	34	In detail, for a text t, with annotated entities E(t), we first extract a set of TLinks R(t).
107	13	Neither can we compare directly to existing absolute time-line prediction models such as Reimers et al. (2018) because they are trained on different data with a very different annotation scheme.
108	37	To evaluate the quality of the relative time-line models in a fair way, we use TimeML-style test sets as follows: (1) We predict a time-line for each test-text, and (2) we check for all ground-truth annotated TLinks that are present in the data, what would be the derived relation type based on the predicted time-line, which is the relation type that gives the lowest time-line loss Lr.
131	7	Moreover, we can clearly see that on TE3‡, CTLM performs better than the indirect models, across all loss functions.
135	34	Another result supporting this hypothesis is the fact that the difference between C-TLM and S-TLM is small on the smaller TD‡, indicating that C-TLM does not yet utilize contextual information from this dataset, whereas, in contrast, on TE3‡, the larger dataset, C-TLM clearly outperforms S-TLM across all loss functions, showing that when enough data is available C-TLM learns good LSTM weights that exploit context substantially.
143	34	We also looked at the average token-distance between arguments of correctly satisfied TLinks by the time-lines of each model.
144	18	For TL2RTL (Lτ ) this is 13 tokens, and for C-TLM (Lτ ) 15.
145	10	When looking only at the TLinks that C-TLM (Lτ ) satisfied and TL2RTL (Lτ ) did not, the average distance is 21.
148	21	This consequently prevents TL2RTL to properly position distant events with respect to each other.
149	37	To get more insight in what the model learns we calculated mean durations and mean starts of CTLM (Lτ ) predictions.
151	40	We observe that events that generally have more events included are assigned longer duration and vice versa.
161	22	The proposed models also provide a good starting point for research into probabilistic time-line models, that additionally model the (un)certainty of the predicted positions and durations of the entities.
