2	17	One class of algorithms consists of value function-based methods such as deep Q-learning (Mnih et al., 2013; 2015), which are known to be highly sample efficient but generally assume a Markovian observation space.
3	11	The other class of algorithms consists of Monte Carlo policy gradient methods such as trust region policy optimization (Schulman et al., 2015), which do not need to assume Markovian observations but are less sample efficient than value function-based methods.
4	19	Some policy gradient methods such as advantage actor-critic (Mnih et al., 2016) introduce the Markov assumption through a critic or state-dependent baseline to improve sample efficiency.
10	21	That is, can we develop methods that are sample efficient but are also robust to partial observation spaces?
12	24	Our method learns a policy by estimating an advantage-like function which approximates a quantity called the counterfactual regret.
39	5	Let I be the space of information sets: an information set I ∈ I is a set of states s ∈ I , where s ∈ S, such that only the information set I is directly observable, and the individual states contained in I are hidden.
47	10	(1) What is the interpretation of the overall regret?
51	8	Let Qiσt(I, a) be the counterfactual value of the i-th player, where the i-th player is assumed to reach I and always chooses the action a in the aggregate state I , and otherwise follows the policy πit, while all other players follow the strategy profile σ−it .
53	7	Let (RiT ) (CF)(I, a) be the counterfactual regret of the i-th player, which is the sum of the advantage-like quantities Qiσt(I, a)− V i σt(I) after T learning iterations: (RiT ) (CF)(I, a) = T∑ t=1 Qiσt(I, a)− V i σt(I).
57	19	By naively treating the counterfactual regret (RiT ) (CF)(I, a) for each I as analogous to regret in an online learning setting, then at each learning iteration one may simply plug the counterfactual regret into a regret matching policy update (Hart & Mas-Colell, 2000): (πit+1) RM(a|I) = max(0, (R i t) (CF)(I, a))∑ a′∈Amax(0, (R i t) (CF)(I, a′)) .
59	17	It turns out that updating players’ policies in the extensive game setting by iteratively minimizing the immediate counterfactual regrets according to Equation (4) will also minimize the overall regret (RiT ) (overall) with upper bound O(|I| √ |A|T ).
60	10	The overall regret’s O( √ T ) dependence on the number of iterations T is not impacted by the structure of the information set space I, which is why CFR can be said to be robust to a certain kind of partial observability.
62	43	Since we are interested in the application of CFR to reinforcement learning, we can write down “1-player” versions of the components above: the counterfactual value, reinterpreted as a stationary state-action value function Qπ|I 7→a(I, a), where the action a is always chosen in the aggregate state I , and the policy π is otherwise followed (Bellemare et al., 2016); the counterfactual regret, including its recursive definition: R(CF)T (I, a) = T∑ t=1 Qπt|I 7→a(I, a)− Vπt|I(I) (5) = R(CF)T−1(I, a) +QπT |I 7→a(I, a)− VπT |I(I) (6) where Vπt|I(I) = ∑ a∈A πt(a|I)Qπt|I 7→a(I, a); and the regret matching policy update: πRMt+1(a|I) = max(0, R(CF)t (I, a))∑ a′∈Amax(0, R (CF) t (I, a ′)) .
66	23	One intuition for why the positive clipping of CFR+ can improve upon CFR is that because [R(CF+)T−1 ]+ is nonnegative, it provides a kind of “optimism under uncertainty,” adding a bonus to some transitions while ignoring others.
84	5	The statevalue function Vπt(ok; θt) is fit using n-step returns with a moving target value function V ′(ok+n;ϕ), essentially using the estimator of the deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016).
86	13	The regression targets vk and q+k are defined in terms of the n-step returns gnk = ∑k+n−1 k′=k γ k′−krk′ + γ nV ′(ok+n;ϕ): vk , g n k (11) qk , rk + γg n−1 k+1 (12) φk , Q + t−1(ok, ak;ωt−1)− Vπt−1(ok; θt−1) (13) q+k , max(0, φk) + qk.
87	33	(14) Altogether, each minibatch step of the optimization subproblem consists of the following three parameter updates: θ (`+1) t ← θ (`) t − α 2 ∇ θ (`) t (Vπt(ok; θ (`) t )− vk)2 (15) ω (`+1) t ← ω (`) t − α 2 ∇ ω (`) t (Q+t (ok, ak;ω (`) t )− q+k ) 2 (16) ϕ(`+1) ← ϕ(`) + τ(θ(`+1)t − ϕ(`)).
110	11	On the other hand, for the ARM-like policy gradient (Equation (22)), there is no similar vanishing effect on the equivalent policy entropy term, suggesting that ARM may perform a kind of entropy regularization by default.
113	12	The regret matching policy which is fundamental to ARM can be interpreted as a more nuanced form of exploration compared to the epsilon-greedy policy used with Q-learning.
116	8	The softmax policy typically learned by policy gradient methods is also quite general, but can still put too much probability mass on one action without compensation by an explicit entropy bonus as done in maximum entropy reinforcement learning.
121	16	On the other hand, the constants in the CFR regret bound are constant properties of the environment.
124	39	If partial observability leads to imbalanced exploration due to confounding of states from perceptual aliasing (McCallum, 1997), then Q-learning should be negatively affected in convergence and possibly in absolute performance.
125	57	Because we hypothesize that ARM should perform well in partially observable domains, we conduct our experiments on visual tasks that naturally provide partial observations of state.
126	8	Our evaluations use feedforward convnets with frame history inputs; our hyperparameters are listed in Section A1 of the Supplementary Material.
130	12	For Minecraft, we adopt the teacher-student curriculum learning task of Matiisen et al. (2017), consisting of 5 consecutive “levels” that successively increase the difficulty of completing the simple task of reaching a gold block: the first level (“L1”) consists of a single room; the intermediate levels (“L2”–“L4”) consist of a corridor with lava-bridge and wallgap obstacles; and the final level (“L5”) consists of a 2× 2 arrangement of rooms randomly separated by lava-bridge or wall-gap obstacles.
134	13	Despite this restriction, ARM is still able to quickly learn policies with minimal tuning of hyperparameters and to reach close to the maximum achievable score in under 1 million simulator steps, which is quite sample efficient.
135	23	On MyWayHome, we observed that ARM generally learned a well-performing policy more quickly than other methods.
137	21	We performed our Minecraft experiments using fixed curriculum learning schedules to evaluate the sample efficiency of different algorithms: the agent is initially placed in the first level (“L1”), and the agent is advanced to the next level whenever a preselected number of simulator steps have elapsed, until the agent reaches the last level (“L5”).
139	27	TRPO required a “slow” schedule of 93750 simulator steps between levels to reliably learn.
140	25	ARM was able to consistently learn a well performing policy on all of the levels, whereas double DQN learned more slowly on some of the intermediate levels.
