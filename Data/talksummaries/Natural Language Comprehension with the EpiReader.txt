1	21	The philosopher Epicurus first formalized this idea in his Principle of Multiple Explanations: if several theories are consistent with the observed data, retain them all until more data is observed.
2	50	In this paper, we argue that the same principle can be applied to machine comprehension of natural language.
4	21	Comprehension of natural language by machines, at a near-human level, is a prerequisite for an extremely broad class of useful applications of artificial intelligence.
5	41	Indeed, most human knowledge is collected in the natural language of text.
7	32	Machine comprehension is typically evaluated by posing a set of questions based on a supporting text passage, then scoring a system’s answers to those questions.
13	61	In tandem with these corpora, a host of neural machine comprehension models has been developed (Weston et al., 2015b; Hermann et al., 2015; Hill et al., 2016; Kadlec et al., 2016; Chen et al., 2016).
21	36	The two-stage process is an analogue of structured prediction cascades (Weiss and Taskar, 2010), wherein a sequence of increasingly complex models progressively filters the output space in order to trade off between model complexity and limited computational resources.
23	83	The Extractor follows the form of a pointer network (Vinyals et al., 2015), and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question.
38	24	The articles themselves form the text passages, and questions are generated synthetically from short summary statements that accompany each article.
41	53	All named entities in the articles and questions are replaced with anonymized tokens that are shuffled for each (Q, T ) pair.
44	26	Children’s Book Test This corpus is constructed similarly to CNN, but from children’s books available through Project Gutenberg.
48	23	Like Kadlec et al. (2016), we focus only on the first two classes since Hill et al. (2016) showed that stan- dard LSTM language models already achieve humanlevel performance on the latter two.
92	68	The Reasoner begins by inserting the answer candidates, which are single words or phrases, into the question sequence Q at the placeholder location.
101	65	For each hypothesis and each sentence of the text, Reasoner input consists of two matrices: Si ∈ RD×|Si|, whose columns are the embedding vectors for each word of sentence Si, and Hk ∈ RD×|Hk|, whose columns are the embedding vectors for each word in the hypothesisHk.
124	32	Finally, we combine the evidence from the Reasoner with the probability from the Extractor.
140	32	To get the final loss term LER, minus `2 regularization terms on the model parameters, we take a weighted combination of LE and LR: LER = LE + λLR, (7) where λ is a hyperparameter for weighting the relative contribution of the Extractor and Reasoner losses.
177	30	In Table 5.2, we compare the performance of EpiReader against that of several baselines, on the validation and test sets of the CBT and CNN corpora.
180	20	On CNN, we score 2.2% higher on test than the best previous model of Chen et al. (2016).
194	27	Aside from achieving state-of-the-art results at its final output, the EpiReader framework gives a boost to its Extractor component through the joint training process.
199	35	Although not exactly an ablation, we also tried bypassing the Reasoner’s convolutional encoders altogether, along with the word-match scores and the bilinear similarity.
200	25	This was done as follows: from the Extractor, we pass to the Reasoner’s final GRU (i) the bidirectional hidden representation of the question; (ii) the bidirectional hidden representations of the end of each story sentence (recall that the Reasoner operates on sentence representations).
210	31	The dialogue in the question sentence refers to both: Mr. Toad is its subject, referred to by the pronoun “he”, and Mr. Blacksnake is its object.
211	31	In the preceding sentences, it is clear (to a human) that Jimmy is worried about Mr. Toad and his potential encounter with Mr. Blacksnake.
213	228	The Reasoner corrects this error and selects “Blacksnake” as the answer.
214	78	This relies on a deeper understanding of the text.
216	199	This kind alternation is typical of dialogues, when two actors interact in turns.
218	38	We presented the novel EpiReader framework for machine comprehension and evaluated it on two large, complex datasets: CNN and CBT.
220	32	In future work, we plan to test our framework with alternative models for natural language inference (e.g., Wang and Jiang (2016)), and explore the effect of pretraining such a model specifically on an inference task.
221	47	As a general framework that consists in a twostage cascade, EpiReader can be implemented using a variety of mechanisms in the Extractor and Reasoner stages.
223	40	As more powerful machine comprehension models inevitably emerge, it may be straightforward to boost their performance using EpiReader’s structure.
