6	34	An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus).
8	23	We present methods for extracting frequency and unique context count statistics for mgram queries from CSTs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics.
10	51	Our second method addresses this problem using a single CST backed by a wavelet tree based FM-index (Ferragina et al., 2007), which results in better time complexity and considerably faster runtime performance.
19	38	Figure 1a shows a suffix tree over a sample text.
22	25	The i-th smallest suffix in T corresponds to the path-label of the i-th leaf.
23	37	The starting position of the suffix can be associated its corresponding leaf in the tree as shown in Figure 1a.
27	41	n− 1] such that SA[i] corresponds to the starting position of the i-th smallest suffix in T or the i-th leaf in the suffix tree of T .
29	122	Using only the suffix array and the text, pattern search can be performed using binary search in O(m log n) time.
31	153	In practice, suffix arrays use 4 − 8n bytes of space whereas the most efficient suffix tree implementations require at least 20n bytes of space (Kurtz, 1999) which are both much larger than T and prohibit the use of these structures for all but small data sets.
33	33	The space usage of a suffix array can be reduced significantly by utilizing the compressibility of text combined with succinct data structures.
35	36	For simplicity, we focus on the FM-Index which emulates the functionality of a suffix array over T using nHk(T ) + o(n log σ) bits of space where Hk refers to the k-th order entropy of the text (Ferragina et al., 2007).
36	61	In practice, the FMIndex of T uses roughly space equivalent to the compressed representation of T using a standard compressor such as bzip2.
38	126	The FM-Index relies on the duality between the suffix array and the BWT (Burrows and Wheeler, 1994), a permutation of the text such that T bwt[i] = T [SA[i] − 1] (see Figure 1).
49	38	The time and space complexity of the FMindex thus depends on the cost of storing and pre- processing T bwt to answer RANK efficiently.
59	28	Then, for any SA[i] or SA−1[i], at most O(SAS) RANK operations on T bwt are required to access the value.
67	113	For a more extensive overview of CSTs see Russo et al. (2011).
75	27	In electing to store the corpus directly in a suffix tree, we need to provide mechanisms for computing these counts based on queries into the suffix tree.
96	24	This paper considers an interpolated LM formulation, in which probabilities from higher order contexts are interpolated with lower order estimates.
97	39	This iterative process is apparent in Figure 2 (right) which shows the quantities required for probability scoring for an example mgram.
104	62	depth-first 6: dP ← string-depth(parent(vR)) 7: d← string-depth(vR) 8: for k ← dP + 1 to min (d, dP +m) do 9: s← edge(vR, k) 10: if s is the end of sentence sentinel then 11: skip all children of vR 12: else 13: if k = 2 then 14: N1+(··)← N1+(··) + 1 15: f ← size(vR) 16: if 1 ≤ f ≤ 2 then 17: ck,f ← ck,f + 1 18: if k < d then 19: g← 1 20: else 21: g← degree(vR) 22: if 1 ≤ g ≤ 2 then 23: N1k,g ← N1k,g + 1 24: return c,N1, N1+(··) the number of left or right contexts in which a pattern appears.
105	46	The matching process is illustrated in Figure 2 where the three search nodes are shown on the left, considered bottom to top, and their corresponding count operations are shown to the right.
109	49	In the forward CST, we perform backward search to extend the search pattern to the left, which can be computed very efficiently from the BWT in the CSA.9 Conversely in the reverse CST, we must use forward search as we are effectively extending the reversed pattern to the right; this operation is considerably more costly.
119	44	Instead the critical counts, N1+(·α) and N1+(·α·) are computed directly from a single forward CST.
128	28	match for wkk−i 4: p← 1 5: for i← 1 to m do 6: vallF ← back-search([lb(vallF ), rb(vallF )], wk−i+1) 7: if i > 1 then 8: vF ← back-search([lb(vF), rb(vF)], wk−i+1) 9: Di← discount parameter for igram 10: if i = m then 11: c← size(vallF ) 12: d← size(vF) 13: else 14: c← N1PBACK1(tF, vallF ,·wk−1k−i+1) 15: d← N1PFRONTBACK1(tF, vF,·wk−1k−i+1 ·) 16: if i > 1 then 17: if vF is valid then 18: q← N1P(tF, vF, wk−1k−i+1 ·) 19: p← 1 d (max(c−Di, 0) +Diqp) 20: else 21: p← c/N1+(··) 22: return p Algorithm 5 N1+(·α·), using forward CST Precondition: vF in forward CST tF matches α 1: function N1PFRONTBACK1(tF, vF, α) 2: o← 0 3: if string-depth(vF) > |α| then 4: o← N1PBACK1(tF, vF,·α) 5: else 6: for 〈l, r, s〉 ← int-syms(tF, [lb(vF), rb(vF)]) do 7: l′← Cs + l 8: r′← Cs + r 9: o← o+ N1P(tF, node(l′, r′), sα·) 10: return o by visiting all leaves of the wavelet tree of symbols occurring in T bwt[l, r] (corresponding to α) in O(|P (α)| log σ) time (lines 6-8).
135	35	The first 10k sentences were used as the test data, and the last 80% as the training data, giving rise to training corpora of between 8M and 50M tokens and uncompressed size of up to 200 MiB (see Table 1 for detailed corpus statistics).
149	26	The space usage of D-CST index is comparable to a compact 3-gram SRILM index.
165	24	This paper has demonstrated the massive potential that succinct indexes have for language modelling, by developing efficient algorithms for onthe-fly computing of mgram counts and language model probabilities.
166	37	Although we only considered a Kneser-Ney LM, our approach is portable to the many other LM smoothing method formulated around similar count statistics.
167	59	Our complexity analysis and experimental results show favourable scaling properties with corpus size and Markov order, albeit running between 1-2 orders of magnitude slower than a leading count-based LM.
168	136	Our ongoing work seeks to close this gap: preliminary experiments suggest that with careful tuning of the succinct index parameters and caching expensive computations, query time can be competitive with state-of-the-art toolkits, while using less memory and allowing the use of unlimited context.
