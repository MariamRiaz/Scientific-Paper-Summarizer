19	24	However, we get the benefit of now being able to sample xi and x j in parallel.
23	24	In section 4 we show that the model excels in density estimation and can produce quality high-resolution samples at high speed.
50	47	The main design principle that we follow in building the model is a coarse-to-fine ordering of pixels.
57	24	Also, pixels can depend on other pixels below and to the right, which would have been inaccessible in the standard PixelCNN.
77	112	Given the first group of pixels, the rest of the groups at a given scale can be generated autoregressively.
98	19	All models for ImageNet, CUB, MPII and MS-COCO were trained using RMSprop with hyperparameter ✏ = 1e 8, with batch size 128 for 200K steps.
134	104	We trained two versions of the model, both versions using type-A upscaling networks (See Fig.
135	39	The first is designed to sample in O(T ) time, for T video frames.
136	69	That is, the number of network evaluations per frame is constant with respect to the number of pixels.
137	319	The motivation for training the O(T ) model is that previous frames in a video provide very detailed cues for predicting the next frame, so that our pixel groups could be conditionally independent even without access to a low-resolution image.
138	45	Without the need to upscale from a low-resolution image, we can produce “group 1” pixels - i.e. the upper-left corner group - directly by conditioning on previous frames.
140	41	The second version is our multi-step upscaler used in previous experiments, conditioned on both previous frames and robot arm state and actions.
145	22	Table 2 compares two variants of our model with the original VPN.
146	59	Compared to the O(T ) baseline - a convolutional LSTM model without spatial dependencies - our O(T ) model performs dramatically better.
147	30	On the validation set, in which the model needs to generalize to novel combinations of objects and arm trajectories, the O(T log N) model does much better than our O(T ) model, although not as well as the original O(T N) model.
148	28	On the testing sets, we observed that the O(T ) model performed as well as on the validation set, but the O(T log N) model showed a drop in performance.
149	68	However, this drop does not occur due to the presence of novel objects (in fact this setting actually yields better results), but due to the novel arm and camera configuration used during testing 2.
151	52	It should be possible to overcome this e↵ect with better regularization and perhaps data augmentation such as mirroring and jittering frames, or simply training on data with more diverse camera positions.
152	236	The supplement contains example videos generated on the validation set arm trajectories from our O(T log N) model.
162	52	Interestingly, the model often produced quite realistic bird images from scratch when trained on CUB, and these samples looked more realistic than any animal image generated by our ImageNet models.
167	21	Upscaling starting from 32 ⇥ 32 results in much more realistic images.
168	50	Here the diversity is apparent in the samples (as in the data, conditioned on low-resolution) in the local details such as the dog’s fur patterns or the frog’s eye contours.
169	30	As expected, we observe a very large speedup of our model compared to sampling from a standard PixelCNN at the same resolution (see Table 4).
170	63	Even at 32 ⇥ 32 we observe two orders of magnitude speedup, and the speedup is greater for higher resolution.
173	27	Since our model has a PixelCNN at the lowest resolution, it can also be accelerated by caching PixelCNN hidden unit activations, recently implemented b by Ramachandran et al. (2017).
174	193	This could allow one to use higher-resolution base PixelCNNs without sacrificing speed.
175	62	In this paper, we developed a parallelized, multiscale version of PixelCNN.
176	110	It achieves competitive density estimation results on CUB, MPII, MS-COCO, ImageNet, and Robot Pushing videos, surpassing all other density models that admit fast sampling.
177	406	Qualitatively, it can achieve compelling results in text-to-image synthesis and video generation, as well as diverse super-resolution from very small images all the way to 512⇥ 512.
178	245	Many more samples from all of our models can be found in the appendix and supplementary material.
