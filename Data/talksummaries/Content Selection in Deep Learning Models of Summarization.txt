4	26	In this paper, we seek to better understand how deep learning models of summarization perform content selection across multiple domains (§ 4): news, personal stories, meetings, and medical articles (for which we collect a new corpus).1 We analyze several recent sentence extractive neural network architectures, specifically considering the design choices for sentence encoders (§ 3.1) and sentence extractors (§ 3.2).
6	34	We also question the necessity of auto-regressive sentence extraction (i.e. using previous predictions to inform future predictions), which previous approaches have used (§ 2), and propose two alternative models that extract sentences independently.
10	24	Pre-trained word embeddings are as good, or better than, learned embeddings in five of six datasets.
37	46	Each sentence is itself a sequence of word embeddings si = w (i) 1 , .
44	19	RNN Encoder When using the RNN sentence encoder, a sentence embedding is the concatenation of the final output states of a forward and backward RNN over the sentence’s word embeddings.
55	18	The first, proposed by Cheng and Lapata (2016), is built around a sequence-to-sequence model.
67	27	Proposed Sentence Extractors We propose two sentence extractor models that make a stronger conditional independence assumption p(y|h) = �ni=1 p(yi|h), essentially making independent predictions conditioned on h. RNN Extractor Our first proposed model is a very simple bidirectional RNN based tagging model.
74	20	A separate decoder GRU transforms each sentence into a query vector which attends to the encoder output.
75	21	The attention weighted encoder output and the decoder GRU output are concatenated and fed into a multi-layer perceptron to compute the extraction probability.
78	43	The corpora come from the news domain (CNN-DailyMail, New York Times, DUC), personal narratives domain (Reddit), workplace meetings (AMI), and medical journal articles (PubMed).
79	22	See Table 1 for dataset statistics.
97	47	We also evaluate using METEOR (Denkowski and Lavie, 2014).7 Summaries are generated by extracting the top ranked sentences by model probability p(yi = 1|y<i, h), stopping when the word budget is met or exceeded.
102	29	We trained for a maximum of 50 epochs and the best model was selected with early stopping on the validation set according to ROUGE-2.
109	18	Lead As a baseline we include the lead summary, i.e. taking the first x words of the document as summary, where x is the target summary length for each dataset (see the first paragraph of § 5).
118	20	Overall, on the news and medical journal domains, the differences are quite small with the dif- ferences between worst and best systems on the CNN/DM dataset spanning only .56 of a ROUGE point.
119	44	While there is more performance variability in the Reddit and AMI data, there is less distinction among systems: no differences are significant on Reddit and every extractor has at least one configuration that is indistinguishable from the best system on the AMI corpus.
121	27	Word Embedding Learning Given that learning a sentence encoder (averaging has no learned parameters) does not yield significant improvement, it is natural to consider whether learning word embeddings is also necessary.
124	38	When learning embeddings, words occurring fewer than three times in the training data are mapped to an unknown token (with learned embedding).
129	29	To understand which classes of words were most important we ran an ablation study, selectively removing nouns, verbs (including participles and auxiliaries), adjectives & adverbs, and function words (adpositions, determiners, conjunctions).
140	34	Document Shuffling Sentence position is a well known and powerful feature for news summarization (Hong and Nenkova, 2014), owing to the intentional lead bias in the news article writing10; it also explains the difficulty in beating the lead baseline for single-document summarization (Nenkova, 2005; Brandow et al., 1999).
144	29	We conduct a sentence order experiment where each document’s sentences are randomly shuffled during training.
148	102	By comparison, there is no significant difference between the shuffled and inorder models on the Reddit domain, and shuffling actually improves performance on AMI.
151	19	The summaries generated by all systems described here–the prior work and our proposed simplified models–are highly similar to each other and to the lead baseline.
152	58	The Cheng & Lapata and Seq2Seq extractors (using the averaging encoder) share 87.8% of output sentences on average on the CNN/DM data, with similar numbers for the other news domains (see Table 6 for a typical example).
153	25	Also on CNN/DM, 58% of the Seq2Seq selected sentences also occur in the lead summary, with similar numbers for DUC, NYT, and Reddit.
162	19	This suggests that we need to rethink or find novel forms of sentence representation for the summarization task.
164	67	On the news domain, the models consistently learned to ignore quoted material in the lead, as often the quotes provide color to the story but are unlikely to be included in the summary (e.g. “It was like somebody slugging a punching bag.”).
165	32	This behavior was most likely triggered by the presence of quotes, as the quote attributions, which were often tokenized as separate sentences, would subsequently be included in the summary despite also not containing much information (e.g. Gil Clark of the National Hurricane Center said Thursday).
166	37	We have presented an empirical study of deep learning based content selection algorithms for summarization.
167	39	Our findings suggest such models face stark limitations on their ability to learn robust features for this task and that more work is needed on sentence representation for summarization.
