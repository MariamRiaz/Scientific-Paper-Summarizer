{"sections": [{"text": "Proceedings of NAACL-HLT 2018, pages 1134\u20131145 New Orleans, Louisiana, June 1 - 6, 2018. c\u00a92018 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Word representation learning has become a research area of central importance in NLP, with its usefulness demonstrated across application areas such as parsing (Chen and Manning, 2014), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al., 2011). Standard techniques for inducing word embeddings rely on the distributional hypothesis (Harris, 1954), using co-occurrence information from large textual corpora to learn meaningful word representations (Mikolov et al., 2013; Levy and Goldberg, 2014; Pennington et al., 2014; Bojanowski et al., 2017).\nA major drawback of the distributional hypothesis is that it coalesces different relationships between words, such as synonymy and topical relatedness, into a single vector space. A popular solution\nis to go beyond stand-alone unsupervised learning and fine-tune distributional vector spaces by using external knowledge from human- or automaticallyconstructed knowledge bases. This is often done as a post-processing step, where distributional vectors are gradually refined to satisfy linguistic constraints extracted from lexical resources such as WordNet (Faruqui et al., 2015; Mrk\u0161ic\u0301 et al., 2016), the Paraphrase Database (PPDB) (Wieting et al., 2015), or BabelNet (Mrk\u0161ic\u0301 et al., 2017; Vulic\u0301 et al., 2017a). One advantage of post-processing methods is that they treat the input vector space as a black box, making them applicable to any input space.\nA key property of these methods is their ability to transform the vector space by specialising it for a particular relationship between words.1 Prior work has predominantly focused on distinguishing between semantic similarity and conceptual relatedness (Faruqui et al., 2015; Mrk\u0161ic\u0301 et al., 2017; Vulic\u0301 et al., 2017b). In this paper, we introduce a novel post-processing model which specialises vector spaces for the lexical entailment (LE) relation.\nWord-level lexical entailment is an asymmetric semantic relation (Collins and Quillian, 1972; Beckwith et al., 1991). It is a key principle determining the organisation of semantic networks into hierarchical structures such as semantic ontologies (Fellbaum, 1998). Automatic reasoning about LE supports tasks such as taxonomy creation (Snow et al., 2006; Navigli et al., 2011), natural language inference (Dagan et al., 2013; Bowman et al., 2015), text generation (Biran and McKeown, 2013), and metaphor detection (Mohler et al., 2013).\nOur novel LE specialisation model, termed LEAR (Lexical Entailment Attract-Repel), is inspired by ATTRACT-REPEL, a state-of-the-art general spe-\n1Distinguishing between synonymy and antonymy has a positive impact on real-world language understanding tasks such as Dialogue State Tracking (Mrk\u0161ic\u0301 et al., 2017).\n1134\nand \u2212\u2192 dog or \u2212\u2192 dog and \u2212\u2212\u2212\u2212\u2192 animal); and 2) by imposing an LE ordering using vector norms, adjusting them so that higher-level concepts have larger norms (e.g., |\u2212\u2212\u2212\u2212\u2192animal| > |\u2212\u2192dog| > |\u2212\u2212\u2212\u2212\u2192terrier|).\ncialisation framework (Mrk\u0161ic\u0301 et al., 2017).2 The key idea of LEAR, illustrated by Figure 1, is to pull desirable (ATTRACT) examples described by the constraints closer together, while at the same time pushing undesirable (REPEL) word pairs away from each other. Concurrently, LEAR (re-)arranges vector norms so that norm values in the Euclidean space reflect the hierarchical organisation of concepts according to the given LE constraints: put simply, higher-level concepts are assigned larger norms. Therefore, LEAR simultaneously captures the hierarchy of concepts (through vector norms) and their similarity (through their cosine distance). The two pivotal pieces of information are combined into an asymmetric distance measure which quantifies the LE strength in the specialised space.\nAfter specialising four well-known input vector spaces with LEAR, we test them in three standard word-level LE tasks (Kiela et al., 2015b): 1) hypernymy directionality; 2) hypernymy detection; and 3) combined hypernymy detection/directionality. Our specialised vectors yield notable improvements over the strongest baselines for each task, with each input space, demonstrating the effectiveness and robustness of LEAR specialisation.\n2https://github.com/nmrksic/attract-repel\nThe employed asymmetric distance allows one to make graded assertions about hierarchical relationships between concepts in the specialised space. This property is evaluated using HyperLex, a recent graded LE dataset (Vulic\u0301 et al., 2017). The LEAR-specialised vectors push state-of-the-art Spearman\u2019s correlation from 0.540 to 0.686 on the full dataset (2,616 word pairs), and from 0.512 to 0.705 on its noun subset (2,163 word pairs).\nThe code for the LEAR model is available from: github.com/nmrksic/lear."}, {"heading": "2 Methodology", "text": ""}, {"heading": "2.1 The ATTRACT-REPEL Framework", "text": "Let V be the vocabulary, A the set of ATTRACT word pairs (e.g., intelligent and brilliant), and R the set of REPEL word pairs (e.g., vacant and occupied). The ATTRACT-REPEL procedure operates over mini-batches of such pairs BA and BR. For ease of notation, let each word pair (xl, xr) in these two sets correspond to a vector pair (xl,xr), so that a mini-batch of k1 word pairs is given by BA = [(x1l ,x1r), . . . , (xk1l ,xk1r )] (similarly for BR, which consists of k2 example pairs).\nNext, the sets of pseudo-negative examples TA = [(t 1 l , t 1 r), . . . , (t k1 l , t k1 r )] and TR = [(t1l , t 1 r), . . . , (t k2 l , t k2 r )] are defined as pairs of negative examples for each ATTRACT and REPEL example pair in mini-batches BA and BR. These negative examples are chosen from the word vectors present in BA or BR so that, for each ATTRACT pair (xl,xr), the negative example pair (tl, tr) is chosen so that tl is the vector closest (in terms of cosine distance) to xl and tr is closest to xr. Similarly, for each REPEL pair (xl,xr), the negative example pair (tl, tr) is chosen from the remaining in-batch vectors so that tl is the vector furthest away from xl and tr is furthest from xr.\nThe negative examples are used to: a) force ATTRACT pairs to be closer to each other than to their respective negative examples; and b) to force REPEL pairs to be further away from each other than from their negative examples. The first term of the cost function pulls ATTRACT pairs together:\nAtt(BA, TA) = k1\u2211\ni=1\n[ \u03c4 ( \u03b4att + cos(x i l, t i l)\u2212 cos(xil,xir) )\n+\u03c4 ( \u03b4att + cos(x i r, t i r)\u2212 cos(xil,xir) ) ] (1)\nwhere cos denotes cosine similarity, \u03c4(x) = max(0, x) is the hinge loss function and \u03b4att is the attract margin which determines how much closer these vectors should be to each other than to their respective negative examples. The second part of the cost function pushes REPEL word pairs away from each other:\nRep(BR, TR) = k2\u2211\ni=1\n[ \u03c4 ( \u03b4rep + cos(x i l,x i r)\u2212 cos(xil, til) )\n+\u03c4 ( \u03b4rep + cos(x i l,x i r)\u2212 cos(xir, tir) ) ] (2)\nIn addition to these two terms, an additional regularisation term is used to preserve the abundance of high-quality semantic content present in the distributional vector space, as long as this information does not contradict the injected linguistic constraints. If V (B) is the set of all word vectors present in the given mini-batch, then:\nReg(BA,BR) = \u2211\nxi\u2208V (BA\u222aBR) \u03bbreg \u2016x\u0302i \u2212 xi\u20162\nwhere \u03bbreg is the L2 regularization constant and x\u0302i denotes the original (distributional) word vector for word xi. The full ATTRACT-REPEL cost function is given by the sum of all three terms."}, {"heading": "2.2 LEAR: Encoding Lexical Entailment", "text": "In this section, the ATTRACT-REPEL framework is extended to model lexical entailment jointly with (symmetric) semantic similarity. To do this, the method uses an additional source of external lexical knowledge: let L be the set of directed lexical entailment constraints such as (corgi, dog), (dog, animal), or (corgi, animal), with lower-level concepts on the left and higher-level ones on the right (the source of these constraints will be discussed in Section 3). The optimisation proceeds in the same way as before, considering a mini-batch of LE pairs BL consisting of k3 word pairs standing in the (directed) lexical entailment relation.\nUnlike symmetric similarity, lexical entailment is an asymmetric relation which encodes a hierarchical ordering between concepts. Inferring the direction of the entailment relation between word vectors requires the use of an asymmetric distance function. We define three different ones, all of which use the word vector\u2019s norms to impose an\nordering between high- and low-level concepts:\nD1(x,y) = |x| \u2212 |y| (3) D2(x,y) = |x| \u2212 |y| |x|+ |y| (4) D3(x,y) = |x| \u2212 |y|\nmax(|x|, |y|) (5)\nThe lexical entailment term (for the j-th asymmetric distance, j \u2208 1, 2, 3) is defined as:\nLEj(BL) = k3\u2211\ni=1\nDj(xi,yi) (6)\nThe first distance serves as the baseline: it uses the word vectors\u2019 norms to order the concepts, that is to decide which of the words is likely to be the higher-level concept. In this case, the magnitude of the difference between the two norms determines the \u2018intensity\u2019 of the LE relation. This is potentially problematic, as this distance does not impose a limit on the vectors\u2019 norms. The second and third metric take a more sophisticated approach, using the ratios of the differences between the two norms and either: a) the sum of the two norms; or b) the larger of the two norms. In doing that, these metrics ensure that the cost function only considers the norms\u2019 ratios. This means that the cost function no longer has the incentive to increase word vectors\u2019 norms past a certain point, as the magnitudes of norm ratios grow in size much faster than the linear relation defined by the first distance function.\nTo model the semantic and the LE relations jointly, the LEAR cost function jointly optimises the four terms of the expanded cost function:\nC(BA, TA,BR, TR,BL, TL) = Att(BS , TS) + . . . + Rep(BA, TA) + Reg(BA,BR,BL) + . . . + Att(BL, TL) + LEj(BL)\nLE Pairs as ATTRACT Constraints The combined cost function makes use of the batch of lexical constraints BL twice: once in the defined asymmetric cost function LEj , and once in the symmetric ATTRACT term Att(BL, TL). This means that words standing in the lexical entailment relation are forced to be similar both in terms of cosine distance (via the symmetric ATTRACT term) and in terms of the asymmetric LE distance from Eq. (6).\nDecoding Lexical Entailment The defined cost function serves to encode semantic similarity and\nLE relations in the same vector space. Whereas the similarity can be inferred from the standard cosine distance, the LEAR optimisation embeds lexical entailment as a combination of the symmetric ATTRACT term and the newly defined asymmetric LEj cost function. Consequently, the metric used to determine whether two words stand in the LE relation must combine the two cost terms as well. We define the LE decoding metric as:\nILE(x,y) = dcos(x,y) +Dj(x,y) (7)\nwhere dcos(x,y) denotes the cosine distance. This decoding function combines the symmetric and the asymmetric cost term, in line with the combination of the two used to perform LEAR specialisation. In the evaluation, we show that combining the two cost terms has a synergistic effect, with both terms contributing to stronger performance across all LE tasks used for evaluation."}, {"heading": "3 Experimental Setup", "text": "Starting Distributional Vectors To test the robustness of LEAR specialisation, we experiment with a variety of well-known, publicly available English word vectors: 1) Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013) trained on the Polyglot Wikipedia (Al-Rfou et al., 2013) by Levy and Goldberg (2014); 2) GLOVE Common Crawl (Pennington et al., 2014); 3) CONTEXT2VEC (Melamud et al., 2016), which replaces CBOW contexts with contexts based on bidirectional LSTMs (Hochreiter and Schmidhuber, 1997); and 4) FASTTEXT (Bojanowski et al., 2017), a SGNS variant which builds word vectors as the sum of their constituent character n-gram vectors.3\nLinguistic Constraints We use three groups of linguistic constraints in the LEAR specialisation model, covering three different relation types which are all beneficial to the specialisation process: directed 1) lexical entailment (LE) pairs; 2) synonymy pairs; and 3) antonymy pairs. Synonyms are included as symmetric ATTRACT pairs (i.e., the BA pairs) since they can be seen as defining a trivial symmetric IS-A relation (Rei and Briscoe, 2014; Vulic\u0301 et al., 2017). For a similar reason,\n3All vectors are 300-dimensional except for the 600- dimensional CONTEXT2VEC vectors; for further details regarding the architectures and training setup of the used vector collections, we refer the reader to the original papers. We also experimented with dependency-based SGNS vectors (Levy and Goldberg, 2014), observing similar patterns in the results.\nantonyms are clear REPEL constraints as they anticorrelate with the LE relation.4 Synonymy and antonymy constraints are taken from prior work (Zhang et al., 2014; Ono et al., 2015): they are extracted from WordNet (Fellbaum, 1998) and Roget (Kipfer, 2009). In total, we work with 1,023,082 synonymy pairs (11.7 synonyms per word on average) and 380,873 antonymy pairs (6.5 per word).5\nAs in prior work (Nguyen et al., 2017; Nickel and Kiela, 2017), LE constraints are extracted from the WordNet hierarchy, relying on the transitivity of the LE relation. This means that we include both direct and indirect LE pairs in our set of constraints (e.g., (pangasius, fish), (fish, animal), and (pangasius, animal)). We retained only noun-noun and verb-verb pairs, while the rest were discarded: the final number of LE constraints is 1,545,630.6\nTraining Setup We adopt the original ATTRACTREPEL model setup without any fine-tuning. Hyperparameter values are set to: \u03b4att = 0.6, \u03b4rep = 0.0, \u03bbreg = 10\n\u22129 (Mrk\u0161ic\u0301 et al., 2017). The models are trained for 5 epochs with the AdaGrad algorithm (Duchi et al., 2011), with batch sizes set to k1 = k2 = k3 = 128 for faster convergence."}, {"heading": "4 Results and Discussion", "text": "We test and analyse LEAR-specialised vector spaces in two standard word-level LE tasks used in prior work: hypernymy directionality and detection (Section 4.1) and graded LE (Section 4.2)."}, {"heading": "4.1 LE Directionality and Detection", "text": "The first evaluation uses three classification-style tasks with increased levels of difficulty. The tasks are evaluated on three datasets used extensively in the LE literature (Roller et al., 2014; Santus et al., 2014; Weeds et al., 2014; Shwartz et al., 2017; Nguyen et al., 2017), compiled into an integrated evaluation set by Kiela et al. (2015b).7\n4In short, the question \u201cIs X a type of X?\u201d (synonymy) is trivially true, while the question \u201cIs \u00acX a type of X?\u201d (antonymy) is trivially false.\n5https://github.com/tticoin/AntonymDetection 6We also experimented with additional 30,491 LE constraints from the Paraphrase Database (PPDB) 2.0 (Pavlick et al., 2015). Adding them to the WordNet-based LE pairs makes no significant impact on the final performance. We also used synonymy and antonymy pairs from other sources, such as word pairs from PPDB used previously by Wieting et al. (2015), and BabelNet (Navigli and Ponzetto, 2012) used by Mrk\u0161ic\u0301 et al. (2017), reaching the same conclusions.\n7http://www.cl.cam.ac.uk/\u223cdk427/generality.html\nThe first task, LE directionality, is conducted on 1,337 LE pairs originating from the BLESS evaluation set (Baroni and Lenci, 2011). Given a true LE pair, the task is to predict the correct hypernym. With LEAR-specialised vectors this is achieved by simply comparing the vector norms of each concept in a pair: the one with the larger norm is the hypernym (see Figure 1).\nThe second task, LE detection, involves a binary classification on the WBLESS dataset (Weeds et al., 2014) which comprises 1,668 word pairs standing in a variety of relations (LE, meronymy-holonymy, co-hyponymy, reversed LE, no relation). The model has to detect a true LE pair, that is, to distinguish between the pairs where the statement X is a (type of) Y is true from all other pairs. With LEAR vectors, this classification is based on the asymmetric distance score: if the score is above a certain threshold, we classify the pair as \u201ctrue LE\u201d, otherwise as \u201cother\u201d. While Kiela et al. (2015b) manually define the threshold value, we follow the approach of Nguyen et al. (2017) and cross-validate: in each of the 1,000 iterations, 2% of the pairs are sampled for threshold tuning, and the remaining 98% are used for testing. The reported numbers are therefore average accuracy scores.8\n8We have conducted more LE directionality and detection experiments on other datasets such as EVALution (Santus et al., 2015), the N1 N2 dataset of Baroni et al. (2012), and the dataset of Lenci and Benotto (2012) with similar performances and findings. We do not report all these results for brevity and clarity of presentation.\nThe final task, LE detection and directionality, concerns a three-way classification on BIBLESS, a relabeled version of WBLESS. The task is now to distinguish both LE pairs (\u2192 1) and reversed LE pairs (\u2192 \u22121) from other relations (\u2192 0), and then additionally select the correct hypernym in each detected LE pair. We apply the same test protocol as in the LE detection task.\nResults and Analysis The original paper of Kiela et al. (2015b) reports the following best scores on each task: 0.88 (BLESS), 0.75 (WBLESS), 0.57 (BIBLESS). These scores were recently surpassed by Nguyen et al. (2017), who, instead of post-processing, combine WordNet-based constraints with an SGNS-style objective into a joint model. They report the best scores to date: 0.92 (BLESS), 0.87 (WBLESS), and 0.81 (BIBLESS).\nThe performance of the four LEAR-specialised word vector collections is shown in Figure 2 (together with the strongest baseline scores for each of the three tasks). The comparative analysis confirms the increased complexity of subsequent tasks. LEAR specialisation of each of the starting vector spaces consistently outperformed all baseline scores across all three tasks. The extent of the improvements is correlated with task difficulty: it is lowest for the easiest directionality task (0.92 \u2192 0.96), and highest for the most difficult detection plus directionality task (0.81\u2192 0.88).\nThe results show that the two LEAR variants which do not rely on absolute norm values and\nperform a normalisation step in the asymmetric distance (D2 and D3) have an edge over the D1 variant which operates with unbounded norms. The difference in performance between D2/D3 and D1 is even more pronounced in the graded LE task (see Section 4.2). This shows that the use of unbounded vector norms diminishes the importance of the symmetric cosine distance in the combined asymmetric distance. Conversely, the synergistic combination used in D2/D3 does not suffer from this issue.\nThe high scores achieved with each of the four word vector collections show that LEAR is not dependent on any particular word representation architecture. Moreover, the extent of the performance improvements in each task suggests that LEAR is able to reconstruct the concept hierarchy coded in the input linguistic constraints.\nMoreover, we have conducted a small experiment to verify that the LEAR method can generalise beyond what is directly coded in pairwise external constraints. A simple WordNet lookup baseline yields accuracy scores of 0.82 and 0.80 on the directionality and detection tasks, respectively. This baseline is outperformed by LEAR: its scores are 0.96 and 0.92 on the two tasks when relying on the same set of WordNet constraints.\nImportance of Vector Norms To verify that the knowledge concerning the position in the semantic hierarchy actually arises from vector norms, we also manually inspect the norms after LEAR specialisation. A few examples are provided in Table 1. They indicate a desirable pattern in the norm values which imposes a hierarchical ordering on the concepts. Note that the original distributional SGNS model (Mikolov et al., 2013) does not normalise vectors to unit length after training. However, these norms are not at all correlated with the desired hierarchical ordering, and are therefore useless for LE-related applications: the non-specialised distributional SGNS model scores 0.44, 0.48, and 0.34 on the three tasks, respectively."}, {"heading": "4.2 Graded Lexical Entailment", "text": "Asymmetric distances in the LEAR-specialised space quantify the degree of lexical entailment between any two concepts. This means that they can be used to make fine-grained assertions regarding the hierarchical relationships between concepts. We test this property on HyperLex (Vulic\u0301 et al., 2017), a gold standard dataset for evaluating how well word representation models capture graded LE, grounded in the notions of concept (proto)typicality (Rosch, 1973; Medin et al., 1984) and category vagueness (Kamp and Partee, 1995; Hampton, 2007) from cognitive science. HyperLex contains 2,616 word pairs (2,163 noun pairs and 453 verb pairs) scored by human raters in the [0, 6] interval following the question \u201cTo what degree is X a (type of) Y?\u201d9\nAs shown by the high inter-annotator agreement on HyperLex (0.85), humans are able to consistently reason about graded LE.10 However, current state-of-the-art representation architectures are far from this ceiling. For instance, Vulic\u0301 et al. (2017) evaluate a plethora of architectures and report a high-score of only 0.320 (see the summary table in Figure 3). Two recent representation models (Nickel and Kiela, 2017; Nguyen et al., 2017) focused on the LE relation in particular (and employing the same set of WordNet-based constraints as LEAR) report the highest score of 0.540 (on the entire dataset) and 0.512 (on the noun subset).\nResults and Analysis We scored all HyperLex pairs using the combined asymmetric distance described by Equation (7), and then computed Spearman\u2019s rank correlation with the ground-truth ranking. Our results, together with the strongest baseline scores, are summarised in Figure 3.\nThe summary table in Figure 3(c) shows the HyperLex performance of several prominent LE models. We provide only a quick outline of these models here; further details can be found in the original papers. FREQ-RATIO exploits the fact that more general concepts tend to occur more frequently in textual corpora. SGNS (COS) uses non-specialised\n9From another perspective, one might say that graded LE provides finer-grained human judgements on a continuous scale rather than simplifying the judgements into binary discrete decisions. For instance, the HyperLex score for the pair (girl, person) is 5.91/6, the score for (guest, person) is 4.33, while the score for the reversed pair (person, guest) is 1.73.\n10For further details concerning HyperLex, we refer the reader to the resource paper (Vulic\u0301 et al., 2017). The dataset is available at: http://people.ds.cam.ac. uk/iv250/hyperlex.html\nSGNS vectors and quantifies the LE strength using the symmetric cosine distance between vectors. A comparison of these models to the best-performing LEAR vectors shows the extent of the improvements achieved using the specialisation approach.\nLEAR-specialised vectors also outperform SLQSSIM (Santus et al., 2014) and VISUAL (Kiela et al., 2015b), two LE detection models similar in spirit to LEAR. These models combine symmetric semantic similarity (through cosine distance) with an asymmetric measure of lexical generality obtained either from text (SLQS-SIM) or visual data (VISUAL). The results on HyperLex indicate that the two generality-based measures are too coarsegrained for graded LE judgements. These models were originally constructed to tackle LE directionality and detection tasks (see Section 4.1), but their performance is surpassed by LEAR on those tasks as well. The VISUAL model outperforms SLQS-SIM. However, its numbers on BLESS (0.88), WBLESS (0.75), and BIBLESS (0.57) are far from the topperforming LEAR vectors (0.96, 0.92, 0.88).11\nWN-BEST denotes the best result with asymmetric similarity measures which use the WordNet structure as their starting point (Wu and Palmer, 1994; Pedersen et al., 2004). This model can be observed as a model that directly looks up the full WordNet structure to reason about graded lexical entailment. The reported results from Figure 3(c) suggest it is more effective to quantify the LE re-\n11We note that SLQS and VISUAL do not leverage any external knowledge from WordNet, but the VISUAL model leverages external visual information about concepts.\nlation strength by using WordNet as the source of constraints for specialisation models such as HYPERVEC or LEAR.\nWORD2GAUSS (Vilnis and McCallum, 2015) represents words as multivariate K-dimensional Gaussians rather than points in the embedding space: it is therefore naturally asymmetric and was used in LE tasks before, but its performance on HyperLex indicates that it cannot effectively capture the subtleties required to model graded LE. However, note that the comparison is not strictly fair as WORD2GAUSS does not leverage any external knowledge. An interesting line for future research is to embed external knowledge within this representation framework.\nMost importantly, LEAR outperforms three recent (and conceptually different) architectures: ORDER-EMB (Vendrov et al., 2016), POINCAR\u00c9 (Nickel and Kiela, 2017), and HYPERVEC (Nguyen et al., 2017). Like LEAR, all of these models complement distributional knowledge with external linguistic constraints extracted from WordNet. Each model uses a different strategy to exploit the hierarchical relationships encoded in these constraints (their approaches are discussed in Section 5).12 However, LEAR, as the first LE-oriented post-processor, is able to utilise the constraints more effectively than its competitors. Another advantage of LEAR is its applicability to any input\n12As discussed previously by Vulic\u0301 et al. (2017), the offthe-shelf ORDER-EMB vectors were trained for the binary ungraded LE detection task: this limits their expressiveness in the graded LE task.\nvector space. Figures 3(a) and 3(b) indicate that the two LEAR variants which rely on norm ratios (D2 and D3), rather than on absolute (unbounded) norm differences (D1), achieve stronger performance on HyperLex. The highest correlation scores are again achieved by D2 with all input vector spaces."}, {"heading": "4.3 Further Discussion", "text": "Why Symmetric + Asymmetric? In another experiment, we analyse the contributions of both LErelated terms in the LEAR combined objective function (see Section 2.2). We compare three variants of LEAR: 1) a symmetric variant which does not arrange vector norms using the LEj(BL) term (SYMONLY); 2) a variant which arranges norms, but does not use LE constraints as additional symmetric ATTRACT constraints (ASYM-ONLY); and 3) the full LEAR model, which uses both cost terms (FULL). The results with one input space (similar results are achieved with others) are shown in Table 2. This table shows that, while the stand-alone ASYM-ONLY term seems more beneficial than the SYM-ONLY one, using the two terms jointly yields the strongest performance across all LE tasks.\nLE and Semantic Similarity We also test whether the asymmetric LE term harms the (normindependent) cosine distances used to represent semantic similarity. The LEAR model is compared to the original ATTRACT-REPEL model making use of the same set of linguistic constraints. Two true semantic similarity datasets are used for evaluation: SimLex-999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016). There is no significant difference in performance between the two models, both of which yield similar results on SimLex (Spearman\u2019s rank correlation of \u2248 0.71) and SimVerb (\u2248 0.70). This proves that cosine distances remain preserved during the optimization of the asymmetric objective performed by the joint LEAR model."}, {"heading": "5 Related Work", "text": "Vector Space Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure: they modify the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015a), or use a variant of the SGNSstyle objective (Liu et al., 2015; Osborne et al., 2016; Nguyen et al., 2017). Another class of models, popularly termed retrofitting, fine-tune distributional vector spaces by injecting lexical knowledge from semantic databases such as WordNet or the Paraphrase Database (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrk\u0161ic\u0301 et al., 2016; Mrk\u0161ic\u0301 et al., 2017).\nLEAR falls into the latter category. However, while previous post-processing methods have focused almost exclusively on specialising vector spaces to emphasise semantic similarity (i.e., to distinguish between similarity and relatedness by explicitly pulling synonyms closer and pushing antonyms further apart), this paper proposed a principled methodology for specialising vector spaces for asymmetric hierarchical relations (of which lexical entailment is an instance). Its starting point is the state-of-the-art similarity specialisation framework of Mrk\u0161ic\u0301 et al. (2017), which we extend to support the inclusion of hierarchical asymmetric relationships between words.\nWord Vectors and Lexical Entailment Since the hierarchical LE relation is one of the fundamental building blocks of semantic taxonomies and hierarchical concept categorisations (Beckwith et al., 1991; Fellbaum, 1998), a significant amount of research in semantics has been invested into its automatic detection and classification. Early work relied on asymmetric directional measures (Weeds et al., 2004; Clarke, 2009; Kotlerman et al., 2010; Lenci and Benotto, 2012, i.a.) which were based on the distributional inclusion hypothesis (Geffet and Dagan, 2005) or the distributional informativeness or generality hypothesis (Herbelot and Ganesalingam, 2013; Santus et al., 2014). However, these approaches have recently been superseded by methods based on word embeddings. These methods build dense real-valued vectors for capturing the LE relation, either directly in the LE-focused space (Vilnis and McCallum, 2015; Vendrov et al.,\n2016; Henderson and Popa, 2016; Nickel and Kiela, 2017; Nguyen et al., 2017) or by using the vectors as features for supervised LE detection models (Tuan et al., 2016; Shwartz et al., 2016; Nguyen et al., 2017; Glava\u0161 and Ponzetto, 2017).\nSeveral LE models embed useful hierarchical relations from external resources such as WordNet into LE-focused vector spaces, with solutions coming in different flavours. The model of Yu et al. (2015) is a dynamic distance-margin model optimised for the LE detection task using hierarchical WordNet constraints. This model was extended by Tuan et al. (2016) to make use of contextual sentential information. A major drawback of both models is their inability to make directionality judgements. Further, their performance has recently been surpassed by the HYPERVEC model of Nguyen et al. (2017). This model combines WordNet constraints with the SGNS distributional objective into a joint model. As such, the model is tied to the SGNS objective and any change of the distributional modelling paradigm implies a change of the entire HYPERVEC model. This makes their model less versatile than the proposed LEAR framework. Moreover, the results achieved using LEAR specialisation achieve substantially better performance across all LE tasks used for evaluation.\nAnother model similar in spirit to LEAR is the ORDER-EMB model of Vendrov et al. (2016), which encodes hierarchical structure by imposing a partial order in the embedding space: higher-level concepts get assigned higher per-coordinate values in a d-dimensional vector space. The model minimises the violation of the per-coordinate orderings during training by relying on hierarchical WordNet constraints between word pairs. Finally, the POINCAR\u00c9 model of Nickel and Kiela (2017) makes use of hyperbolic spaces to learn generalpurpose LE embeddings based on n-dimensional Poincar\u00e9 balls which encode both hierarchy and semantic similarity, again using the WordNet constraints. A similar model in hyperbolic spaces was proposed by Chamberlain et al. (2017). In this paper, we demonstrate that LE-specialised word embeddings with stronger performance can be induced using a simpler model operating in more intuitively interpretable Euclidean vector spaces."}, {"heading": "6 Conclusion and Future Work", "text": "This paper proposed LEAR, a vector space specialisation procedure which simultaneously injects sym-\nmetric and asymmetric constraints into existing vector spaces, performing joint specialisation for two properties: lexical entailment and semantic similarity. Since the former is not symmetric, LEAR uses an asymmetric cost function which encodes the hierarchy between concepts by manipulating the norms of word vectors, assigning higher norms to higher-level concepts. Specialising the vector space for both relations has a synergistic effect: LEAR-specialised vectors attain state-of-the-art performance in judging semantic similarity and set new high scores across four different lexical entailment tasks. The code for the LEAR model is available from: github.com/nmrksic/lear.\nIn future work, we plan to apply a similar methodology to other asymmetric relations (e.g., meronymy), as well as to investigate finegrained models which can account for differing path lengths from the WordNet hierarchy. We will also extend the model to reason over words unseen in input lexical resources, similar to the recent post-specialisation model oriented towards specialisation of unseen words for similarity (Vulic\u0301 et al., 2018). We also plan to test the usefulness of LEspecialised vectors in downstream natural language understanding tasks. Porting the model to other languages and enabling cross-lingual applications such as cross-lingual lexical entailment (Upadhyay et al., 2018) is another future research direction."}, {"heading": "Acknowledgments", "text": "We thank the three anonymous reviewers for their insightful comments and suggestions. We are also grateful to the TakeLab research group at the University of Zagreb for offering support to computationally intensive experiments in our hour of need. This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909)."}], "year": 2018, "references": [{"title": "Polyglot: Distributed word representations for multilingual NLP", "authors": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of CoNLL, pages 183\u2013192.", "year": 2013}, {"title": "Entailment above the word level in distributional semantics", "authors": ["Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan."], "venue": "Proceedings of EACL, pages 23\u201332.", "year": 2012}, {"title": "WordNet: A lexical database organized on psycholinguistic principles", "authors": ["Richard Beckwith", "Christiane Fellbaum", "Derek Gross", "George A. Miller."], "venue": "Lexical acquisition: Exploiting on-line resources to build a lexicon, pages 211\u2013231.", "year": 1991}, {"title": "Knowledge-powered deep learning for word embedding", "authors": ["Jiang Bian", "Bin Gao", "Tie-Yan Liu."], "venue": "Proceedings of ECML-PKDD, pages 132\u2013 148.", "year": 2014}, {"title": "Classifying taxonomic relations between pairs of Wikipedia articles", "authors": ["Or Biran", "Kathleen McKeown."], "venue": "Proceedings of IJCNLP, pages 788\u2013794.", "year": 2013}, {"title": "Enriching word vectors with subword information", "authors": ["Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov."], "venue": "Transactions of the ACL, 5:135\u2013146.", "year": 2017}, {"title": "A large annotated corpus for learning natural language inference", "authors": ["Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning."], "venue": "Proceedings of EMNLP, pages 632\u2013642.", "year": 2015}, {"title": "Neural embeddings of graphs in hyperbolic space", "authors": ["Benjamin Paul Chamberlain", "James Clough", "Marc Peter Deisenroth."], "venue": "CoRR, abs/1705.10359.", "year": 2017}, {"title": "A fast and accurate dependency parser using neural networks", "authors": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of EMNLP, pages 740\u2013750.", "year": 2014}, {"title": "Context-theoretic semantics for natural language: An overview", "authors": ["Daoud Clarke."], "venue": "Proceedings of the Workshop on Geometrical Models of Natural Language Semantics (GEMS), pages 112\u2013119.", "year": 2009}, {"title": "Experiments on semantic memory and language comprehension", "authors": ["Allan M. Collins", "Ross M. Quillian."], "venue": "Cognition in Learning and Memory.", "year": 1972}, {"title": "Natural language processing (almost) from scratch", "authors": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "year": 2011}, {"title": "Recognizing textual entailment: Models and applications", "authors": ["Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto."], "venue": "Synthesis Lectures on Human Language Technologies, 6(4):1\u2013220.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "authors": ["John C. Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159.", "year": 2011}, {"title": "Retrofitting word vectors to semantic lexicons", "authors": ["Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith."], "venue": "Proceedings of NAACL-HLT, pages 1606\u20131615.", "year": 2015}, {"title": "The distributional inclusion hypotheses and lexical entailment", "authors": ["Maayan Geffet", "Ido Dagan."], "venue": "Proceedings of ACL, pages 107\u2013114.", "year": 2005}, {"title": "SimVerb-3500: A largescale evaluation set of verb similarity", "authors": ["Daniela Gerz", "Ivan Vuli\u0107", "Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Proceedings of EMNLP, pages 2173\u20132182.", "year": 2016}, {"title": "Dual tensor model for detecting asymmetric lexicosemantic relations", "authors": ["Goran Glava\u0161", "Simone Paolo Ponzetto."], "venue": "Proceedings of EMNLP, pages 1758\u20131768.", "year": 2017}, {"title": "Typicality, graded membership, and vagueness", "authors": ["James A. Hampton."], "venue": "Cognitive Science, 31(3):355\u2013 384.", "year": 2007}, {"title": "Distributional structure", "authors": ["Zellig S. Harris."], "venue": "Word, 10(23):146\u2013162.", "year": 1954}, {"title": "A vector space for distributional semantics for entailment", "authors": ["James Henderson", "Diana Popa."], "venue": "Proceedings of ACL, pages 2052\u20132062.", "year": 2016}, {"title": "Measuring semantic content in distributional vectors", "authors": ["Aur\u00e9lie Herbelot", "Mohan Ganesalingam."], "venue": "Proceedings of ACL, pages 440\u2013445.", "year": 2013}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation", "authors": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics, 41(4):665\u2013695.", "year": 2015}, {"title": "Long Short-Term Memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "year": 1997}, {"title": "Ontologically grounded multi-sense representation learning for semantic vector space models", "authors": ["Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy."], "venue": "Proceedings of NAACL, pages 683\u2013693.", "year": 2015}, {"title": "Prototype theory and compositionality", "authors": ["Hans Kamp", "Barbara Partee."], "venue": "Cognition, 57(2):129\u2013 191.", "year": 1995}, {"title": "Specializing word embeddings for similarity or relatedness", "authors": ["Douwe Kiela", "Felix Hill", "Stephen Clark."], "venue": "Proceedings of EMNLP, pages 2044\u2013 2048.", "year": 2015}, {"title": "Exploiting image generality for lexical entailment detection", "authors": ["Douwe Kiela", "Laura Rimell", "Ivan Vuli\u0107", "Stephen Clark."], "venue": "Proceedings of ACL, pages 119\u2013124.", "year": 2015}, {"title": "Roget\u2019s 21st Century Thesaurus (3rd Edition)", "authors": ["Barbara Ann Kipfer."], "venue": "Philip Lief Group.", "year": 2009}, {"title": "Directional distributional similarity for lexical inference", "authors": ["Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet."], "venue": "Natural Language Engineering, 16(4):359\u2013389.", "year": 2010}, {"title": "Identifying hypernyms in distributional semantic spaces", "authors": ["Alessandro Lenci", "Giulia Benotto."], "venue": "Proceedings of *SEM, pages 75\u201379.", "year": 2012}, {"title": "Dependencybased word embeddings", "authors": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of ACL, pages 302\u2013308.", "year": 2014}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "authors": ["Quan Liu", "Hui Jiang", "Si Wei", "Zhen-Hua Ling", "Yu Hu."], "venue": "Proceedings of ACL, pages 1501\u20131511.", "year": 2015}, {"title": "Given versus induced category representations: Use of prototype and exemplar information in classification", "authors": ["Douglas L. Medin", "Mark W. Altom", "Timothy D. Murphy."], "venue": "Journal of Experimental Psychology, 10(3):333\u2013352.", "year": 1984}, {"title": "Context2vec: Learning generic context embedding with bidirectional LSTM", "authors": ["Oren Melamud", "Jacob Goldberger", "Ido Dagan."], "venue": "Proceedings of CoNLL, pages 51\u201361.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "Proceedings of NIPS, pages 3111\u2013 3119.", "year": 2013}, {"title": "Semantic signatures for example-based linguistic metaphor detection", "authors": ["Michael Mohler", "David Bracewell", "Marc Tomlinson", "David Hinote."], "venue": "Proceedings of the First Workshop on Metaphor in NLP, pages 27\u201335.", "year": 2013}, {"title": "Neural belief tracker: Data-driven dialogue state tracking", "authors": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Tsung-Hsien Wen", "Blaise Thomson", "Steve Young."], "venue": "Proceedings of ACL, pages 1777\u20131788.", "year": 2017}, {"title": "Counter-fitting word vectors to linguistic constraints", "authors": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Maria Rojas-Barahona", "PeiHao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young."], "venue": "Proceedings of NAACL-", "year": 2016}, {"title": "Semantic specialisation of distributional word vector spaces using monolingual and cross-lingual constraints", "authors": ["Nikola Mrk\u0161i\u0107", "Ivan Vuli\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Ira Leviant", "Roi Reichart", "Milica Ga\u0161i\u0107", "Anna Korhonen", "Steve Young."], "venue": "Transactions", "year": 2017}, {"title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "authors": ["Roberto Navigli", "Simone Paolo Ponzetto."], "venue": "Artificial Intelligence, 193:217\u2013250.", "year": 2012}, {"title": "A graph-based algorithm for inducing lexical taxonomies from scratch", "authors": ["Roberto Navigli", "Paola Velardi", "Stefano Faralli."], "venue": "Proceedings of IJCAI, pages 1872\u20131877.", "year": 2011}, {"title": "Hierarchical embeddings for hypernymy detection and directionality", "authors": ["Kim Anh Nguyen", "Maximilian K\u00f6per", "Sabine Schulte im Walde", "Ngoc Thang Vu."], "venue": "Proceedings of EMNLP, pages 233\u2013243.", "year": 2017}, {"title": "Integrating distributional lexical contrast into word embeddings for antonymsynonym distinction", "authors": ["Kim Anh Nguyen", "Sabine Schulte im Walde", "Ngoc Thang Vu."], "venue": "Proceedings of ACL, pages 454\u2013459.", "year": 2016}, {"title": "Poincar\u00e9 embeddings for learning hierarchical representations", "authors": ["Maximilian Nickel", "Douwe Kiela."], "venue": "Proceedings of NIPS.", "year": 2017}, {"title": "Word embedding-based antonym detection using thesauri and distributional information", "authors": ["Masataka Ono", "Makoto Miwa", "Yutaka Sasaki."], "venue": "Proceedings of NAACL-HLT, pages 984\u2013989.", "year": 2015}, {"title": "Encoding prior knowledge with eigenword embeddings", "authors": ["Dominique Osborne", "Shashi Narayan", "Shay Cohen."], "venue": "Transactions of the ACL, 4:417\u2013430.", "year": 2016}, {"title": "PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification", "authors": ["Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In Proceedings of ACL,", "year": 2015}, {"title": "WordNet::Similarity - Measuring the relatedness of oncepts", "authors": ["Ted Pedersen", "Siddharth Patwardhan", "Jason Michelizzi."], "venue": "Proceedings of AAAI, pages 1024\u20131025.", "year": 2004}, {"title": "Glove: Global vectors for word representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of EMNLP, pages 1532\u2013 1543.", "year": 2014}, {"title": "Looking for hyponyms in vector space", "authors": ["Marek Rei", "Ted Briscoe."], "venue": "Proceedings of CoNLL, pages 68\u201377.", "year": 2014}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "authors": ["Stephen Roller", "Katrin Erk", "Gemma Boleda."], "venue": "Proceedings of COLING, pages 1025\u20131036.", "year": 2014}, {"title": "Natural categories", "authors": ["Eleanor H. Rosch."], "venue": "Cognitive Psychology, 4(3):328\u2013350.", "year": 1973}, {"title": "Chasing hypernyms in vector spaces with entropy", "authors": ["Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte im Walde."], "venue": "Proceedings of EACL, pages 38\u201342.", "year": 2014}, {"title": "EVALution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models", "authors": ["Enrico Santus", "Frances Yung", "Alessandro Lenci", "Chu-Ren Huang"], "venue": "In Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources", "year": 2015}, {"title": "Improving hypernymy detection with an integrated path-based and distributional method", "authors": ["Vered Shwartz", "Yoav Goldberg", "Ido Dagan."], "venue": "Proceedings of ACL, pages 2389\u20132398.", "year": 2016}, {"title": "Hypernyms under siege: Linguistically-motivated artillery for hypernymy detection", "authors": ["Vered Shwartz", "Enrico Santus", "Dominik Schlechtweg."], "venue": "Proceedings of EACL, pages 65\u201375.", "year": 2017}, {"title": "Semantic taxonomy induction from heterogenous evidence", "authors": ["Rion Snow", "Daniel Jurafsky", "Andrew Y. Ng."], "venue": "Proceedings of ACL, pages 801\u2013808.", "year": 2006}, {"title": "Learning term embeddings for taxonomic relation identification using dynamic weighting neural network", "authors": ["Luu Anh Tuan", "Yi Tay", "Siu Cheung Hui", "See Kiong Ng."], "venue": "Proceedings of EMNLP, pages 403\u2013 413.", "year": 2016}, {"title": "Word representations: A simple and general method for semi-supervised learning", "authors": ["Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio."], "venue": "Proceedings of ACL, pages 384\u2013394.", "year": 2010}, {"title": "Robust cross-lingual hypernymy detection using dependency context", "authors": ["Shyam Upadhyay", "Yogarshi Vyas", "Marine Carpuat", "Dan Roth."], "venue": "Proceedings of NAACL-HLT.", "year": 2018}, {"title": "Order-embeddings of images and language", "authors": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun."], "venue": "Proceedings of ICLR (Conference Track).", "year": 2016}, {"title": "Word representations via Gaussian embedding", "authors": ["Luke Vilnis", "Andrew McCallum."], "venue": "Proceedings of ICLR (Conference Track).", "year": 2015}, {"title": "Hyperlex: A large-scale evaluation of graded lexical entailment", "authors": ["Ivan Vuli\u0107", "Daniela Gerz", "Douwe Kiela", "Felix Hill", "Anna Korhonen."], "venue": "Computational Linguistics.", "year": 2017}, {"title": "Post-specialisation: Retrofitting vectors of words unseen in lexical resources", "authors": ["Ivan Vuli\u0107", "Goran Glava\u0161", "Nikola Mrk\u0161i\u0107", "Anna Korhonen."], "venue": "Proceedings of NAACL-HLT.", "year": 2018}, {"title": "Cross-lingual induction and transfer of verb classes based on word vector space specialisation", "authors": ["Ivan Vuli\u0107", "Nikola Mrk\u0161i\u0107", "Anna Korhonen."], "venue": "Proceedings of EMNLP, pages 2536\u20132548.", "year": 2017}, {"title": "Morph-fitting: Fine-tuning word vector spaces with simple language-specific rules", "authors": ["Ivan Vuli\u0107", "Nikola Mrk\u0161i\u0107", "Roi Reichart", "Diarmuid \u00d3 S\u00e9aghdha", "Steve Young", "Anna Korhonen."], "venue": "Proceedings of ACL, pages 56\u201368.", "year": 2017}, {"title": "Learning to distinguish hypernyms and co-hyponyms", "authors": ["Julie Weeds", "Daoud Clarke", "Jeremy Reffin", "David Weir", "Bill Keller."], "venue": "Proceedings of COLING, pages 2249\u20132259.", "year": 2014}, {"title": "Characterising measures of lexical distributional similarity", "authors": ["Julie Weeds", "David Weir", "Diana McCarthy."], "venue": "Proceedings of COLING, pages 1015\u2013 1021.", "year": 2004}, {"title": "From paraphrase database to compositional paraphrase model and back", "authors": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Transactions of the ACL, 3:345\u2013358.", "year": 2015}, {"title": "Verb semantics and lexical selection", "authors": ["Zhibiao Wu", "Martha Palmer."], "venue": "Proceedings of ACL, pages 133\u2013138.", "year": 1994}, {"title": "RCNET: A general framework for incorporating knowledge into word representations", "authors": ["Chang Xu", "Yalong Bai", "Jiang Bian", "Bin Gao", "Gang Wang", "Xiaoguang Liu", "Tie-Yan Liu."], "venue": "Proceedings of CIKM, pages 1219\u20131228.", "year": 2014}, {"title": "Improving lexical embeddings with semantic knowledge", "authors": ["Mo Yu", "Mark Dredze."], "venue": "Proceedings of ACL, pages 545\u2013550.", "year": 2014}, {"title": "Learning term embeddings for hypernymy identification", "authors": ["Zheng Yu", "Haixun Wang", "Xuemin Lin", "Min Wang."], "venue": "Proceedings of IJCAI, pages 1390\u2013 1397.", "year": 2015}, {"title": "Word semantic representations using bayesian probabilistic tensor factorization", "authors": ["Jingwei Zhang", "Jeremy Salwen", "Michael Glass", "Alfio Gliozzo."], "venue": "Proceedings of EMNLP, pages 1522\u20131531.", "year": 2014}, {"title": "Bilingual word embeddings for phrase-based machine translation", "authors": ["Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning."], "venue": "Proceedings of EMNLP, pages 1393\u20131398.", "year": 2013}], "id": "SP:b4cccb8ae360b2edc51ce1911f385c362b3eb053", "authors": [{"name": "Ivan Vuli\u0107", "affiliations": []}, {"name": "Nikola Mrk\u0161i\u0107", "affiliations": []}], "abstractText": "We present LEAR (Lexical Entailment AttractRepel), a novel post-processing method that transforms any input word vector space to emphasise the asymmetric relation of lexical entailment (LE), also known as the IS-A or hyponymy-hypernymy relation. By injecting external linguistic constraints (e.g., WordNet links) into the initial vector space, the LE specialisation procedure brings true hyponymyhypernymy pairs closer together in the transformed Euclidean space. The proposed asymmetric distance measure adjusts the norms of word vectors to reflect the actual WordNetstyle hierarchy of concepts. Simultaneously, a joint objective enforces semantic similarity using the symmetric cosine distance, yielding a vector space specialised for both lexical relations at once. LEAR specialisation achieves state-of-the-art performance in the tasks of hypernymy directionality, hypernymy detection, and graded lexical entailment, demonstrating the effectiveness and robustness of the proposed asymmetric specialisation model.", "title": "Specialising Word Vectors for Lexical Entailment"}