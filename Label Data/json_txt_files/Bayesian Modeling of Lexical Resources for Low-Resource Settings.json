{"sections": [{"text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029\u20131039 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1095"}, {"heading": "1 Introduction", "text": "Dictionaries and gazetteers are useful in many natural language processing tasks. These lexical resources may be derived from freely available sources (such as Wikidata and Wiktionary) or constructed for a particular domain. Lexical resources are typically used to complement existing annotations for a given task (Ando and Zhang, 2005; Collobert et al., 2011). In this paper, we focus instead on low-resource settings where task annotations are unavailable or scarce. Specifically, we use lexical resources to guide part-of-speech induction (\u00a74) and to bootstrap named-entity recognizers in low-resource languages (\u00a75).\nGiven their success, it is perhaps surprising that incorporating gazetteers or dictionaries into dis-\ncriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features\u2014which detect whether a name is listed in the dictionary or gazetteer\u2014are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here.\nOur observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon therefore now improves training of the spelling features, rather than competing with the spelling features to help explain the labeled corpus.\nA downside is that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields). In designing our approach\u2014the hierarchical sequence memoizer (HSM)\u2014we aim to be reasonably expressive while retaining practically useful inference algorithms. We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data. The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types (Teh, 2006; Mochihashi et al., 2009). We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling (\u00a73). Our code is available at https://github.com/noa/bayesner.\n1029"}, {"heading": "2 Model", "text": "Our goal is to fit a model that can automatically annotate text. We observe a supervised or unsupervised training corpus. For each label y in the annotation scheme, we also observe a lexicon of strings of type y. For example, in our tagging task (\u00a74), a dictionary provides us with a list of words for each part-of-speech tag y. (These lists need not be disjoint.) For named-entity recognition (NER, \u00a75), we use a list of words or phrases for each named-entity type y (PER, LOC, ORG, etc.).1"}, {"heading": "2.1 Modeling the lexicon", "text": "We may treat the lexicon for type y, of size my, as having been produced by a set of my IID draws from an unknown distribution Py over the words or named entities of type y. It therefore provides some evidence about Py. We will later assume that Py is also used when generating mentions of these words or entities in text. Thanks to this sharing of Py, if x = Washington is listed in the gazetteer of locations (y = LOC), we can draw the same conclusions as if we had seen a LOC-labeled instance of Washington in a supervised corpus.\nGeneralizing this a bit, we may suppose that one observation of string x in the lexicon is equivalent to c labeled tokens of x in a corpus, where the constant c > 0 is known as a pseudocount. In other words, observing a lexicon of my distinct types {x1, . . . , xmy} is equivalent to observing a labeled pseudocorpus of cmy tokens. Notice that given such an observation, the prior probability of any candidate distribution Py is reweighted by the likelihood (cmy)!(c!)my \u00b7 (Py(x1)Py(x2) \u00b7 \u00b7 \u00b7Py(xmy))c. Therefore, this choice of Py can have relatively high posterior probability only to the extent that it assigns high probability to all of the lexicon types."}, {"heading": "2.2 Discussion", "text": "We employ the above model because it has reasonable qualitative behavior and because computationally, it allows us to condition on observed lexicons as easily as we condition on observed corpora. However, we caution that as a generative model of the lexicon, it is deficient, in the sense that it\n1Dictionaries and knowledge bases provide more information than we use in this paper. For instance, Wikidata also provides a wealth of attributes and other metadata for each entity s. In principle, this additional information could also be helpful in estimating Py(s); we leave this intriguing possibility for future work.\nallocates probability mass to events that cannot actually correspond to any lexicon. After all, drawing cmy IID tokens from Py is highly unlikely to result in exactly c tokens of each of my different types, and yet a run of our system will always assume that precisely this happened to produce each observed lexicon! To avoid the deficiency, one could assume that the lexicon was generated by rejection sampling: that is, the gazetteer author repeatedly drew samples of size cmy from Py until one was obtained that had this property, and then returned the set of distinct types in that sample as the lexicon for y. But this is hardly a realistic description of how gazetteers are actually constructed. Rather, one imagines that the gazetteer author simply harvested a lexicon of frequent types from Py or from a corpus of tokens generated from Py. For example, a much better generative story is that the lexicon was constructed as the first my distinct types to appear \u2265 c times in an unbounded sequence of IID draws from Py. When c = 1, this is equivalent to modeling the lexicon as my draws without replacement from Py.2 Unfortunately, draws without replacement are no longer IID or exchangeable: order matters. It would therefore become difficult to condition inference and learning on an observed lexicon, because we would need to explicitly sum or sample over the possibilities for the latent sequence of tokens (or stick segments). We therefore adopt the simpler deficient model.\nA version of our lexicon model (with c = 1) was previously used by Dreyer and Eisner (2011, Appendix C), who observed a list of verb paradigm types rather than word or entity-name types."}, {"heading": "2.3 Prior distribution over Py", "text": "We assume a priori that Py was drawn from a Pitman-Yor process (PYP) (Pitman and Yor, 1997). Both the lexicon and the ordinary corpus are observations that provide information about Py. The PYP is defined by three parameters: a concentration parameter \u03b1, a discount parameter d, and a base distribution Hy. In our case, Hy is a distribution over X = \u03a3\u2217, the set of possible strings over a finite character alphabet \u03a3.\nFor example, HLOC is used to choose new place names, so it describes what place names tend to\n2If we assume that Py was drawn from a Pitman-Yor process prior (as in \u00a72.3) using the stick-breaking method (Pitman, 1996), it is also equivalent to modeling the lexicon as the set of labels of the first my stick segments (which tend to have high probability).\nlook like in the language. The draw PLOC \u223c PYP(d, \u03b1,HLOC) is an \u201cadapted\u201d version of HLOC. It is PLOC that determines how often each name is mentioned in text (and whether it is mentioned in the lexicon). Some names such as Washington that are merely plausible under HLOC are far more frequent under PLOC, presumably because they were chosen as the names of actual, significant places. These place names were randomly drawn from HLOC as part of the procedure for drawing Py.\nThe expected value of Py is H (i.e., H is the mean of the PYP distribution), but if \u03b1 and d are small, then a typical draw of Py will be rather different from H , with much of the probability mass falling on a subset of the strings.\nAt training or test time, when deciding whether to label a corpus token of x = Washington as a place or person, we will be interested in the relative values of PLOC(x) and PPER(x). In practice, we do not have to represent the unknown infinite object Py, but can integrate over its possible values. When Py \u223c PYP(d, \u03b1,Hy), then a sequence of draws X1, X2, . . . \u223c Py is distributed according to a Chinese restaurant process, via\nPy(Xi+1 = x | X1, . . . , Xi) (1)\n= customers(x)\u2212 d \u00b7 tables(x)\n\u03b1+ i\n+ \u03b1+ d \u00b7\u2211x\u2032 tables(x\u2032)\n\u03b1+ i Hy(x)\nwhere customers(x) \u2264 i is the number of times that x appeared among X1, . . . , Xi, and tables(x) \u2264 customers(x) is the number of those times that x was drawn from Hy (where each Py(Xi | \u00b7 \u00b7 \u00b7 ) defined by (1) is interpreted as a mixture distribution that sometimes uses Hy)."}, {"heading": "2.4 Form of the base distribution Hy", "text": "By fitting Hy on corpus and lexicon data, we learn what place names or noun strings tend to look like in the language. By simultaneously fitting Py, we learn which ones are commonly mentioned. Recall that under our model, tokens are drawn from Py but the underlying types are drawn fromHy, e.g.,Hy is responsible for (at least) the first token of each type.\nA simple choice for Hy is a Markov process that emits characters in \u03a3 \u222a {$}, where $ is a distinguished stop symbol that indicates the end of the string. Thus, the probability of producing $ controls the typical string length under Hy.\nWe use a more sophisticated model of strings\u2014a sequence memoizer (SM), which is a (hierarchical) Bayesian treatment of variable-order Markov modeling (Wood et al., 2009). The SM allows dependence on an unbounded history, and the probability of a given sequence (string) can be found efficiently much as in equation (1).\nGiven a string x = a1 \u00b7 \u00b7 \u00b7 aJ \u2208 \u03a3\u2217, the SM assigns a probability to it via\nHy(a1:J) = ( J\u220f\nj=1\nHy(aj | a1:j\u22121) ) Hy($ | a1:J)\n= ( J\u220f\nj=1\nHy,a1:j\u22121(aj) ) Hy,a1:J ($) (2)\nwhere Hy,u(a) denotes the conditional probability of character a given the left context u \u2208 \u03a3\u2217. Each Hy,u is a distribution over \u03a3, defined recursively as\nHy, \u223c PYP(d , \u03b1 ,U\u03a3) (3) Hy,u \u223c PYP(d|u|, \u03b1|u|, Hy,\u03c3(u))\nwhere is the empty sequence, U\u03a3 is the uniform distribution over \u03a3 \u222a {$}, and \u03c3(u) drops the first symbol from u. The discount and concentration parameters (d|u|, \u03b1|u|) are associated with the lengths of the contexts |u|, and should generally be larger for longer (more specific) contexts, implying stronger backoff from those contexts.3\nOur inference procedure is largely indifferent to the form of Hy, so the SM is not the only option. It would be possible to inject more assumptions into Hy, for instance via structured priors for morphology or a grammar of name structure. Another possibility is to use a parametric model such as a neural language model (e.g., Jozefowicz et al. (2016)), although this would require an inner-loop of gradient optimization."}, {"heading": "2.5 Modeling the sequence of tags y", "text": "We now turn to modeling the corpus. We assume that each sentence is generated via a sequence of latent labels y = y1:T \u2208 Y\u2217.4 The observations\n3We fix these hyperparameters using the values suggested in (Wood et al., 2009; Gasthaus and Teh, 2010), which we find to be quite robust in practice. One could also resample their values (Blunsom and Cohn, 2010); we experimented with this but did not observe any consistent advantage to doing so in our setting.\n4The label sequence is terminated by a distinguished endof-sequence label, again written as $.\nx1:T are then generated conditioned on the label sequence via the corresponding Py distribution (defined in \u00a72.3). All observations with the same label y are drawn from the same Py, and thus this subsequence of observations is distributed according to the Chinese restaurant process (1).\nWe model y using another sequence memoizer model. This is similar to other hierarchical Bayesian models of latent sequences (Goldwater and Griffiths, 2007; Blunsom and Cohn, 2010), but again, it does not limit the Markov order (the number of preceding labels that are conditioned on). Thus, the probability of a sequence of latent types is computed in the same way as the base distribution in \u00a72.4, that is,\np(y1:T ) := ( T\u220f\nt=1\nGy1:t\u22121(yt) ) Gy1:T ($) (4)\nwhere Gv(y) denotes the conditional probability of latent label y \u2208 Y given the left context v \u2208 Y\u2217. Each Gv is a distribution over Y , defined recursively as\nG \u223c PYP(d , \u03b1 ,UY) (5) Gv \u223c PYP(d|v|, \u03b1|v|, G\u03c3(v))\nThe probability of transitioning to label yt depends on the assignments of all previous labels y1 . . . yt\u22121.\nFor part-of-speech induction, each label yt is the part-of-speech associated with the corresponding word xt. For named-entity recognition, we say that each word token is labeled with a named entity type (LOC, PER, . . . ),5 or with itself if it is not a named entity but rather a \u201ccontext word.\u201d For example, the word token xt = Washington could have been emitted from the label yt = LOC, or from yt = PER, or from yt = Washington itself (in which case p(xt | yt) = 1). This uses a much larger set of labels Y than in the traditional setup where all context words are emitted from the same latent label type O. Of course, most labels are impossible at most positions (e.g., yt cannot be Washington unless xt = Washington). This scheme makes our generative model sensitive to specific contexts (which is accomplished in discriminative NER systems by contextual features). For example, the SM for y can learn that spoke to PER yesterday is a common 4-gram\n5In \u00a73.2, we will generalize this labeling scheme to allow multi-word named entities such as New York.\nin the label sequence y, and thus we are more likely to label Washington as a person if x = . . .spoke to Washington yesterday . . ..\nWe need one change to make this work, since now Y must include not only the standard NER labels Y \u2032 = {PER, LOC, ORG, GPE} but also words like Washington. Indeed, now Y = Y \u2032 \u222a \u03a3\u2217. But no uniform distribution exists over the infinite set \u03a3\u2217, so how should we replace the base distribution UY over labels in equation (5)? Answer: To draw from the new base distribution, sample y \u223c UY \u2032 \u222a{CONTEXT}. If y = CONTEXT, however, then \u201cexpand\u201d it by resampling y \u223c HCONTEXT. Here HCONTEXT is the base distribution over spellings of context words, and is learned just like the other Hy distributions in \u00a72.4."}, {"heading": "3 Inference via particle Markov chain Monte Carlo", "text": ""}, {"heading": "3.1 Sequential sampler", "text": "Taking Y to be a random variable, we are interested in the posterior distribution p(Y = y | x) over label sequences y given the emitted word sequence x. Our model does not admit an efficient dynamic programming algorithm, owing to the dependencies introduced among the Yt when we marginalize over the unknown G and P distributions that govern transitions and emissions, respectively. In contrast to tagging with a hidden Markov model tagging, the distribution of each label Yt depends on all previous labels y1:t\u22121, for two reasons: \u00ac The transition distribution p(Yt = y | y1:t\u22121) has unbounded dependence because of the PYP prior (4).  The emission distribution p(xt | Yt = y) depends on the emissions observed from any earlier tokens of y, because of the Chinese restaurant process (1). When  is the only complication, block Metropolis-Hastings samplers have proven effective (Johnson et al., 2007). However, this approach uses dynamic programming to sample from a proposal distribution efficiently, which \u00ac precludes in our case. Instead, we use sequential Monte Carlo (SMC)\u2014sometimes called particle filtering\u2014as a proposal distribution. Particle filtering is typically used in online settings, including word segmentation (Borschinger and Johnson, 2011), to make decisions before all of x has been observed. However, we are interested in the inference (or smoothing) problem that conditions on all of x (Dubbin and Blunsom, 2012; Tripuraneni et al., 2015).\nSMC employs a proposal distribution q(y | x)\nwhose definition decomposes as follows:\nq(y1 | x1) T\u220f\nt=2\nq(yt | y1:t\u22121,x1:t) (6)\nfor T = |x|. To sample a sequence of latent labels, first sample an initial label y1 from q1, then proceed incrementally by sampling yt from qt(\u00b7 | y1:t\u22121,x1:t) for t = 2, . . . , T . The final sampled sequence y is called a particle, and is given an unnormalized importance weight of w\u0303 = w\u0303T \u00b7 p($ | y1:T ) where w\u0303T was built up via\nw\u0303t := w\u0303t\u22121 \u00b7 p(y1:t,x1:t)\np(y1:t\u22121,x1:t\u22121) q(yt | y1:t\u22121,x1:t) (7)\nThe SMC procedure consists of generating a system of M weighted particles whose unnormalized importance weights w\u0303(m) : 1 \u2264 m \u2264 M are normalized into w(m) := w\u0303(m)/ \u2211M m=1 w\u0303\n(m). As M \u2192 \u221e, SMC provides a consistent estimate of the marginal likelihood p(x) as 1M \u2211M m=1 w\u0303\n(m), and samples from the weighted particle system are distributed as samples from the desired posterior p(y | x) (Doucet and Johansen, 2009). Particle Gibbs. We employ SMC as a kernel in an MCMC sampler (Andrieu et al., 2010). In particular, we use a block Gibbs sampler in which we iteratively resample the hidden labeling y of a sentence x conditioned on the current labelings for all other sentences in the corpus. In this context, the algorithm is called conditional SMC since one particle is always fixed to the previous sampler state for the sentence being resampled, which ensures that the MCMC procedure is ergodic. At a high level, this procedure is analogous to other Gibbs samplers (e.g. for topic models), except that the conditional SMC (CSMC) kernel uses auxiliary variables (particles) in order to generate the new block variable assignments. The procedure is outlined in Algorithm 1. Given a previous latent state assignment y\u20321:T and observations x1:T , the CSMC kernel produces a new latent state assignment via M auxiliary particles where one particle is fixed to the previous assignment. For ergodicity, M \u2265 2, where larger values of M may improve mixing rate at the expense of increased computation per step.\nProposal distribution. The choice of proposal distribution q is crucial to the performance of SMC methods. In the case of continuous latent variables,\nit is common to propose yt from the transition probability p(Yt | y1:t\u22121) because this distribution usually has a simple form that permits efficient sampling. However, it is possible to do better in the case of discrete latent variables. The optimal proposal distribution is the one which minimizes the variance of the importance weights, and is given by\nq(yt | y1:t\u22121,x1:t) := p(yt | y1:t\u22121,x1:t) (8)\n= p(yt | y1:t\u22121)p(xt | yt)\np(xt | y1:t\u22121)\nwhere\np(xt | y1:t\u22121)= \u2211\nyt\u2208Y p(yt | y1:t\u22121)p(xt | yt) (9)\nSubstituting this expression in equation (7) and simplifying yields the incremental weight update:\nw\u0303t := w\u0303t\u22121 \u00b7 p(xt | y1:t\u22121) (10)\nResampling. In filtering applications, it is common to use resampling operations to prevent weight degeneracy. We do not find resampling necessary here for three reasons. First, note that we resample hidden label sequences that are only as long as the number of words in a given sentence. Second, we use a proposal which minimizes the variance of the weights. Finally, we use SMC as a kernel embedded in an MCMC sampler; asymptotically, this procedure yields samples from the desired posterior regardless of degeneracy (which only affects the mixing rate). Practically speaking, one can diagnose the need for resampling via the effective sample size (ESS) of the particle system:\nESS := 1\n\u2211M m=1(w\u0303 (m))2 =\n( \u2211M\nm=1w (m))2\n\u2211M m=1(w (m))2\nIn our experiments, we find that ESS remains high (a significant fraction of M ) even for long sentences, suggesting that resampling is not necessary to enable mixing of the the Gibbs sampler.\nDecoding. In order to obtain a single latent variable assignment for evaluation purposes, we simply take the state of the Markov chain after a fixed number of iterations of particle Gibbs. In principle, one could collect many samples during particle Gibbs and use them to perform minimum Bayes risk decoding under a given loss function. However, this approach is somewhat slower and did not appear to improve performance in preliminary experiments\nAlgorithm 1 Conditional SMC 1: procedure CSMC(x1:T , y\u20321:T , M ) 2: Draw y(m)1 (eqn. 8) for m \u2208 [1,M \u2212 1] 3: Set y(M)1 = y \u2032 1\n4: Set w\u0303(m)1 (eqn. 10) for m \u2208 [1,M ] 5: for t = 2 to T do 6: Draw y(m)t (eqn. 8) for m \u2208 [1,M \u22121] 7: Set yMt = y \u2032 t 8: Set w\u0303(m)t (eqn. 10) for m \u2208 [1,M ] 9: Set w\u0303(m) = w\u0303(m)T p($|y1:T ) for m \u2208 [1,M ]\n10: Draw index k where p(k = m) \u221d w\u0303(m) 11: return y(k)1:T"}, {"heading": "3.2 Segmental sampler", "text": "We now present an sampler for settings such as NER where each latent label emits a segment consisting of 1 or more words. We make use of the same transition distribution p(yt | y1:t\u22121), which determines the probability of a label in a given context, and an emission distribution p(xt | yt) (namely Pyt); these are assumed to be drawn from hierarchical Pitman-Yor processes described in \u00a72.5 and \u00a72.1, respectively. To allow the xt to be a multi-word string, we simply augment the character set with a distinguished space symbol \u2208 \u03a3 that separates words within a string. For instance, New York would be generated as the 9-symbol sequence New York$.\nAlthough the model emits New York all at once, we still formulate our inference procedure as a particle filter that proposes one tag for each word. Thus, for a given segment label type y, we allow two tag types for its words:\n\u2022 I-y corresponds to a non-final word in a segment of type y (in effect, a word with a following attached). \u2022 E-y corresponds to the final word in a segment\nof type y.\nFor instance, x1:2 = New York would be annotated as a location segment by defining y1:2 = I-LOC E-LOC. This says that y1:2 has jointly emitted x1:2, an event with prior probability PLOC(New York). Each word that is not part of a named entity is considered to be a singleword segment. For example, if the next word were x3 = hosted then it should be tagged with y3 = hosted as in \u00a72.5, in which case x3 was emitted with probability 1.\nTo adapt the sampler described in \u00a73.1 for the segmental case, we need only to define the transition and emission probabilities used in equation (8) and its denominator (9).\nFor the transition probabilities, we want to model the sequence of segment labels. If yt\u22121 is an I- tag, we take p(yt | y1:t\u22121) = 1 , since then yt merely continues an existing segment. Otherwise yt starts a new segment, and we take p(yt | y1:t\u22121) = 1 to be defined by the PYP\u2019s probability Gy1:t\u22121(yt) as usual, but where we interpret the subscript y1:t\u22121 to refer to the possibly shorter sequence of segment labels implied by those t\u2212 1 tags.\nFor the emission probabilities, if yt has the form I-y or E-y, then its associated emission probability no longer has the form p(xt | yt), since the choice of xt also depends on any words emitted earlier in the segment. Let s \u2264 t be the starting position of the segment that contains t. If yt = E-y, then the emission probability is proportional to Py(xs xs+1 . . . xt). If yt = I-y then the emission probability is proportional to the prefix probability \u2211 x Py(x) where x ranges over all strings in \u03a3\u2217 that have xs xs+1 . . . xt as a proper prefix. Prefix probabilities in Hy are easy to compute because Hy has the form of a language model, and prefix probabilities in Py are therefore also easy to compute (using a prefix tree for efficiency).\nThis concludes the description of the segmental sampler. Note that the particle Gibbs procedure is unchanged."}, {"heading": "4 Inducing parts-of-speech with type-level supervision", "text": "Automatically inducing parts-of-speech from raw text is a challenging problem (Goldwater et al., 2005). Our focus here is on the easier problem of type-supervised part-of-speech induction, in which (partial) dictionaries are used to guide inference (Garrette and Baldridge, 2012; Li et al., 2012). Conditioned on the unlabeled corpus and dictionary, we use the MCMC procedure described in \u00a73.1 to impute the latent parts-of-speech.\nSince dictionaries are freely available for hundreds of languages,6 we see this as a mild additional requirement in practice over the purely unsupervised setting.\nIn prior work, dictionaries have been used as constraints on possible parts-of-speech: words appearing in the dictionary take one of their known parts-\n6https://www.wiktionary.org/\nof-speech. In our setting, however, the dictionaries are not constraints but evidence. If monthly is listed in (only) the adjective lexicon, this tells us that PADJ sometimes generates monthly and therefore that HADJ may also tend to generate other words that end with -ly. However, for us, PADV(monthly) > 0 as well, allowing us to still correctly treat monthly as a possible adverb if we later encounter it in a training or test corpus."}, {"heading": "4.1 Experiments", "text": "We follow the experimental procedure described in Li et al. (2012), and use their released code and data to compare to their best model: a second-order maximum entropy Markov model parametrized with log-linear features (SHMM-ME). This model uses hand-crafted features designed to distinguish between different parts-of-speech, and it has special handling for rare words. This approach is surprisingly effective and outperforms alternate approaches such as cross-lingual transfer (Das and Petrov, 2011). However, it also has limitations, since words that do not appear in the dictionary will be unconstrained, and spurious or incorrect lexical entries may lead to propagation of errors.\nThe lexicons are taken from the Wiktionary project; their size and coverage are documented by (Li et al., 2012). We evaluate our model on multi-lingual data released as part of the CoNLL 2007 and CoNLL-X shared tasks. In particular, we use the same set of languages as Li et al. (2012).7 For our method, we impute the parts-of-speech by running particle Gibbs for 100 epochs, where one epoch consists of resampling the states for a each sentence in the corpus. The final sampler state is then taken as a 1-best tagging of the unlabeled data.\nResults. The results are reported in Table 1. We find that our hierarchical sequence memoizer (HSM) matches or exceeds the performance of the baseline (SHMM-ME) for nearly all the tested languages, particularly for morphologically rich languages such as German where the spelling distributions Hy may capture regularities. It is interesting to note that our model performs worse relative to the baseline for English; one possible explanation is that the baseline uses hand-engineered features whereas ours does not, and these features may have been tuned using English data for validation.\n7With the exception of Dutch. Unlike the other CoNLL languages, Dutch includes phrases, and the procedure by which these were split into tokens was not fully documented.\nOur generative model is supposed to exploit lexicons well. To see what is lost from using a generative model, we also compared with Li et al. (2012) on standard supervised tagging without any lexicons. Even here our generative model is very competive, losing only on English and Swedish."}, {"heading": "5 Boostrapping NER with type-level supervision", "text": "Name lists and dictionaries are useful for NER particularly when in-domain annotations are scarce. However, with little annotated data, discriminative training may be unable to reliably estimate lexical feature weights and may overfit. In this section, we are interested in evaluating our proposed Bayesian model in the context of low-resource NER."}, {"heading": "5.1 Data", "text": "Most languages do not have corpora annotated for parts-of-speech, named-entities, syntactic parses, or other linguistic annotations. Therefore, rapidly deploying natural language technologies in a new language may be challenging. In the context of facilitating relief responses in emergencies such as natural disasters, the DARPA LORELEI (Low Resource Languages for Emergent Incidents) program has sponsored the development and release of representative \u201clanguage packs\u201d for Turkish and Uzbek with more languages planned (Strassel and Tracey, 2016). We use the named-entity annotations as part of these language packs which include persons, locations, organizations, and geo-political entities, in order to explore bootstrapping named-entity recognition from small amounts of data. We consider two types of data: \u00ac in-context annotations, where sentences are fully annotated for named-entities, and  lexical resources.\nThe LORELEI language packs lack adequate indomain lexical resources for our purposes. Therefore, we simulate in-domain lexical resources by holding out portions of the annotated development data and deriving dictionaries and name lists from them. For each label y \u2208 {PER, LOC, ORG, GPE, CONTEXT}, our lexicon for y lists all distinct y-labeled strings that appear in the held-out data. This setup ensures that the labels associated with lexicon entries correspond to the annotation guidelines used in the data we use for evaluation. It avoids possible problems that might arise when leveraging noisy out-of-domain knowledge bases, which we may explore in future."}, {"heading": "5.2 Evaluation", "text": "In this section we report supervised NER experiments on two low-resource languages: Turkish and Uzbek. We vary both the amount of supervision as well as the size of the lexical resources. A challenge when evaluating the performance of a model with small amounts of training data is that there may be high-variance in the results. In order to have more confidence in our results, we perform bootstrap resampling experiments in which the training set, evaluation set, and lexical resources are randomized across several replications of the same experiment (for each of the data conditions). We use 10 replications for each of the data conditions reported in Figures 1\u20132, and report both the mean performance and 95% confidence intervals.\nBaseline. We use the Stanford NER system with a standard set of language-independent features (Finkel et al., 2005).8. This model is a conditional random field (CRF) with feature templates which include character n-grams as well as word shape features. Crucially, we also incorporate lexical features. The CRF parameters are regularized using an L1 penalty and optimized via Orthant-wise limited-memory quasi-Newton optimization (Andrew and Gao, 2007). For both our proposed method and the discriminative baseline, we use a fixed set of hyperparameters (i.e. we do not use a separate validation set for tuning each data condition). In order to make a fair comparison to the CRF, we use our sampler for forward inference only, without resampling on the test data.\nResults. We show learning curves as a function of supervised training corpus size. Figure 1 shows that our generative model strongly beats the baseline in this low-data regime. In particular, when there is little annotated training data, our proposed generative model can compensate by exploiting the lexicon, while the discriminative baseline scores terribly. The performance gap decreases with larger\n8We also experimented with neural models, but found that the CRF outperformed them in low-data conditions.\nsupervised corpora, which is consistent with prior results comparing generative and discriminative training (Ng and Jordan, 2002).\nIn Figure 2, we show the effect of the lexicon\u2019s size: as expected, larger lexicons are better. The generative approach significantly outperforms the discriminative baseline at any lexicon size, although its advantage drops for smaller lexicons or larger training corpora.\nIn Figure 1 we found that increasing the pseudocount c consistently decreases performance, so we used c = 1 in our other experiments.9"}, {"heading": "6 Conclusion", "text": "This paper has described a generative model for low-resource sequence labeling and segmentation tasks using lexical resources. Experiments in semisupervised and low-resource settings have demonstrated its applicability to part-of-speech induction and low-resource named-entity recognition. There are many potential avenues for future work. Our model may be useful in the context of active learning where efficient re-estimation and performance in low-data conditions are important. It would also be interesting to explore more expressive parameterizations, such recurrent neural networks for Hy. In the space of neural methods, differentiable memory (Santoro et al., 2016) may be more flexible than the PYP prior, while retaining the ability of the model to cache strings observed in the gazetteer."}, {"heading": "Acknowledgments", "text": "This work was supported by the JHU Human Language Technology Center of Excellence, DARPA LORELEI, and NSF grant IIS-1423276. Thanks to Jay Feldman for early discussions.\n9Why? Even a pseudocount of c = 1 is enough to ensure that Py(s) Hy(s), since the prior probability Hy(s) is rather small for most strings in the lexicon. Indeed, perhaps c < 1 would have increased performance, particularly if the lexicon reflects out-of-domain data. This could be arranged, in effect, by using a hierarchical Bayesian model in which the lexicon and corpus emissions are not drawn from the identical distribution Py but only from similar (coupled) distributions."}], "year": 2017, "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "authors": ["Rie Kubota Ando", "Tong Zhang."], "venue": "Journal of Machine Learning Research 6:1817\u20131853.", "year": 2005}, {"title": "Scalable training of L1-regularized log-linear models", "authors": ["Galen Andrew", "Jianfeng Gao."], "venue": "Proceedings of the 24th International Conference on Machine Learning. pages 33\u201340.", "year": 2007}, {"title": "Particle Markov chain Monte Carlo methods", "authors": ["Christophe Andrieu", "Arnaud Doucet", "Roman Holenstein."], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 72(3):269\u2013342.", "year": 2010}, {"title": "A hierarchical Pitman-Yor process HMM for unsupervised partof-speech induction", "authors": ["Phil Blunsom", "Trevor Cohn."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.", "year": 2010}, {"title": "A particle filter algorithm for Bayesian wordsegmentation", "authors": ["Benjamin Borschinger", "Mark Johnson."], "venue": "Proceedings of the Australasian Language Technology Association Workshop 2011. Canberra, Australia, pages 10\u201318.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "authors": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research 12:2493\u20132537.", "year": 2011}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "authors": ["Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.", "year": 2011}, {"title": "A tutorial on particle filtering and smoothing: Fifteen years later", "authors": ["Arnaud Doucet", "Adam M. Johansen."], "venue": "Handbook of Nonlinear Filtering 12:656\u2013704.", "year": 2009}, {"title": "Discovering morphological paradigms from plain text using a Dirichlet process mixture model", "authors": ["Markus Dreyer", "Jason Eisner."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Edinburgh, pages", "year": 2011}, {"title": "Unsupervised Bayesian part of speech inference with particle Gibbs", "authors": ["Gregory Dubbin", "Phil Blunsom."], "venue": "Proceedings of the 2012 European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I. Springer-", "year": 2012}, {"title": "Incorporating non-local information into information extraction systems", "authors": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning"], "year": 2005}, {"title": "Typesupervised hidden Markov models for part-ofspeech tagging with incomplete tag dictionaries", "authors": ["Dan Garrette", "Jason Baldridge."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing", "year": 2012}, {"title": "Improvements to the sequence memoizer", "authors": ["Jan Gasthaus", "Yee Whye Teh."], "venue": "NIPS. pages 685\u2013693.", "year": 2010}, {"title": "A fully Bayesian approach to unsupervised part-ofspeech tagging", "authors": ["Sharon Goldwater", "Thomas L. Griffiths."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics. Prague, Czech Republic, pages 744\u2013751.", "year": 2007}, {"title": "Interpolating between types and tokens by estimating power-law generators", "authors": ["Sharon Goldwater", "Mark Johnson", "Thomas L. Griffiths."], "venue": "Advances in Neural Information Processing Systems. pages 459\u2013466.", "year": 2005}, {"title": "Bayesian inference for PCFGs via Markov chain Monte Carlo", "authors": ["Mark Johnson", "Thomas L. Griffiths", "Sharon Goldwater."], "venue": "HLT-NAACL. pages 139\u2013146.", "year": 2007}, {"title": "Exploring the limits of language modeling", "authors": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "Computing Research Repository arXiv:1602.02410.", "year": 2016}, {"title": "Wiki-ly supervised part-of-speech tagging", "authors": ["Shen Li", "Joao V Gra\u00e7a", "Ben Taskar."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. pages 1389\u2013", "year": 2012}, {"title": "Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling", "authors": ["Daichi Mochihashi", "Takeshi Yamada", "Naonori Ueda."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-", "year": 2009}, {"title": "On discriminative vs", "authors": ["Andrew Y. Ng", "Michael I. Jordan."], "venue": "generative classifiers: A comparison of logistic regression and naive Bayes. Advances in Neural Information Processing Systems 2:841\u2013848.", "year": 2002}, {"title": "Some developments of the Blackwell-MacQueen urn scheme", "authors": ["Jim Pitman."], "venue": "T. S. Ferguson, L. S. Shapley, and J. B. MacQueen, editors, Statistics, Probability and Game Theory: Papers in Honor of David Blackwell, Institute of Mathe-", "year": 1996}, {"title": "The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator", "authors": ["Jim Pitman", "Marc Yor."], "venue": "The Annals of Probability pages 855\u2013 900.", "year": 1997}, {"title": "One-shot learning with memory-augmented neural networks", "authors": ["Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy P. Lillicrap."], "venue": "Computing Research Repository arXiv:1605.06065.", "year": 2016}, {"title": "Logarithmic opinion pools for conditional random fields", "authors": ["Andrew Smith", "Trevor Cohn", "Miles Osborne."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. pages 18\u201325.", "year": 2005}, {"title": "Using gazetteers in discriminative information extraction", "authors": ["Andrew Smith", "Miles Osborne."], "venue": "Proceedings of the Tenth Conference on Computational Natural Language Learning. pages 133\u2013140.", "year": 2006}, {"title": "Lorelei language packs: Data, tools, and resources for technology development in low resource languages", "authors": ["Stephanie Strassel", "Jennifer Tracey."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC", "year": 2016}, {"title": "Reducing weight undertraining in structured discriminative learning", "authors": ["Charles Sutton", "Michael Sindelar", "Andrew McCallum."], "venue": "Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter", "year": 2006}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "authors": ["Yee Whye Teh."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Lin-", "year": 2006}, {"title": "Particle Gibbs for infinite hidden Markov models", "authors": ["Nilesh Tripuraneni", "Shixiang Gu", "Hong Ge", "Zoubin Ghahramani."], "venue": "Proceedings of the 28th International Conference on Neural Information Processing Systems. MIT Press, Cambridge,", "year": 2015}, {"title": "A stochastic memoizer for sequence data", "authors": ["Frank Wood", "C\u00e9dric Archambeau", "Jan Gasthaus", "Lancelot James", "Yee Whye Teh."], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning. pages 1129\u20131136.", "year": 2009}], "id": "SP:21bc0b4b2a8ec06619e95282feb496742fbcce1f", "authors": [{"name": "Nicholas Andrews", "affiliations": []}, {"name": "Mark Dredze", "affiliations": []}, {"name": "Benjamin Van Durme", "affiliations": []}, {"name": "Jason Eisner", "affiliations": []}], "abstractText": "Lexical resources such as dictionaries and gazetteers are often used as auxiliary data for tasks such as part-of-speech induction and named-entity recognition. However, discriminative training with lexical features requires annotated data to reliably estimate the lexical feature weights and may result in overfitting the lexical features at the expense of features which generalize better. In this paper, we investigate a more robust approach: we stipulate that the lexicon is the result of an assumed generative process. Practically, this means that we may treat the lexical resources as observations under the proposed generative model. The lexical resources provide training data for the generative model without requiring separate data to estimate lexical feature weights. We evaluate the proposed approach in two settings: part-of-speech induction and lowresource named-entity recognition.", "title": "Bayesian Modeling of Lexical Resources for Low-Resource Settings"}