{"sections": [{"heading": "1. Introduction", "text": "Real-world spatio-temporal processes are often poorly modelled by standard inference methods that assume stationarity in time and space. A variety of techniques have been developed for modelling non-stationarity in time via changepoints (CPS), ranging from methods for Gaussian Processes (GPS) (Garnett et al., 2009), the Lasso (Lin et al., 2017) or the Ising model (Fazayeli & Banerjee, 2016) over approaches using density ratio estimation (Liu et al., 2013) and kernelbased methods exploiting M-statistics (Li et al., 2015) to framing CP detection as time series clustering (Khaleghi & Ryabko, 2014). In contrast, CP inference allowing for non-stationarity in space (Herlands et al., 2016) has received comparatively little attention.\nWe offer the first on-line solution to this problem by modeling non-stationarity in both space and time. CPS are used to model non-stationarity in time, and the use of spatially structured Bayesian Vector Autoregressions (SSBVAR) circumvents the assumption of stationarity in space. We unify Adams & MacKay (2007) and Fearnhead & Liu (2007) into\n1Department of Statistics, University of Warwick, UK 2Department of Computer Science, University of Warwick, UK 3The Alan Turing Institute for Data Science & AI, UK. Correspondence to: Jeremias Knoblauch <j.knoblauch@warwick.ac.uk>.\nA version of this paper appeared in the Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.\nan inference procedure for on-line prediction, model selection and CP detection, see Fig. 1. Our construction exploits that both algorithms use Product Partition Models (Barry & Hartigan, 1992), which assume independence of parameters conditional on the CPS and independence of observations conditional on these parameters.\nOur method can be seen as modified on-line version of Xuan & Murphy (2007). In their method, inference is off-line, the model universeM is built during execution and multivariate dependencies are restricted to decomposable graph. In contrast, our procedure specifies M before execution, but runs on-line and does not restrict dependencies. The closest competing on-line procedure in the literature thus far is the work of Saatc\u0327i et al. (2010), which develops Gaussian Process (GP) CP models for Bayesian On-line Changepoint Detection (BOCPD). Though our results suggest that parametric models may be preferable to GP models, the latter can still be integrated into our method as elements of the model universeM without any further modifications.\nIn summary, we make three contributions: Firstly, we substantially augment the existing work on BOCPD by allowing for model uncertainty. Unlike previous extensions of the al-\ngorithm (e.g. Adams & MacKay, 2007; Saatc\u0327i et al., 2010), this avoids having to guess a single best model a priori. Secondly, we introduce SSBVARS as the first class of models for multivariate inference within BOCPD. Thirdly, we demonstrate that using a collection of parametric models can outperform nonparametric GP models in terms of prediction, CP detection and computational efficiency.\nThe structure of this paper is as follows: Section 2 generalizes the BOCPD algorithm of Adams & MacKay (2007), henceforth AM, by integrating it with the approach of Fearnhead & Liu (2007), henceforth FL. In so doing, we arrive at BOCPD with Model Selection, henceforth BOCPDMS. Section 3 proposes VAR models for non-stationary processes within the BOCPD framework. This motivates populating the model universeMwith spatially structured BVAR (SSBVAR) models. Sections 4\u20135 address computational aspects. Section 6 demonstrates the algorithm\u2019s advantages on real world data."}, {"heading": "2. BOCPDMS", "text": "Let {Y t}\u221et=1 be a data stream with an unknown number of CPs. Focusing on univariate data, FL and AM tackled inference by tracking the posterior distribution for the most recent CP. While FL allow the data to be described by different models between CPS, AM only allow for a single model. However, AM perform one-step-ahead predictions, whereas FL do not. Instead, they propose a Maximum A Posteriori (MAP) segmentation for CPS and models. In the remainder of this section, we unify both inference approaches. We call the resulting algorithm BOCPD with model selection (BOCPDMS), as it performs prediction, MAP segmentation and model selection on-line."}, {"heading": "2.1. Run-length & model universe", "text": "The run-length rt at time t is defined as the time since the most recent CP at time t, so rt = 0 corresponds to a CP at time t. Suppose that data between successive CPS can be described by Bayesian models collected in the model universeM. For the process {Y t} on RS , a model m \u2208 M with finite memory of length L \u2208 N0 consists of an observation density fm(Y t = yt|\u03b8m,y(t\u2212L):(t\u22121)) on RS and a parameter prior \u03c0m(\u03b8m) on \u0398m depending on hyperparameters \u03bdm. The notion ofM is due to FL and allows for model uncertainty amongst models developed for BOCPD. For instance, m \u2208M could be a GP (Saatc\u0327i et al., 2010), a time-deterministic regression (Fearnhead, 2005) or a mixture distribution (Caron et al., 2012)."}, {"heading": "2.2. Probabilistic formulation & recursions", "text": "Denote by mt the model describing y(t\u2212rt):t, i.e. the data since the last CP. Given hazard function H : N \u2192 [0, 1],\nBOCPD with Model Selection (BOCPDMS) Input at time 0: model universeM; hazard H; prior q Input at time t: next observation yt Output at time t: y\u0302(t+1):(t+hmax), St, p(mt|y1:t) for next observation yt at time t do\n// STEP I: Compute model-specific quantities for m \u2208M do\nif t\u2212 1 = lag length(m) then I.A Initialize p(y1:t, rt = 0,mt = m) with prior else if t\u2212 1 > lag length(m) then I.B.1 Update p(y1:t, rt,mt = m) via (5a), (5b) I.B.2 Prune model-specific run-length distribution I.B.3 Perform hyperparameter inference via (12)\nend if end for\n// STEP II: Aggregate over models if t >= min(lag length(m)) then\nII.1 Obtain joint distribution overM via (6a)\u2013(6f) II.2 Compute (7)\u2013(9) II.3 Output: y\u0302(t+1):(t+hmax), St, p(mt|y1:t)\nend if end for\nand model prior q :M\u2192 [0, 1], the prior beliefs are\np(rt|rt\u22121) =  1\u2212H(rt\u22121 + 1) if rt = rt\u22121 + 1 H(rt\u22121 + 1) if rt = 0 0 otherwise. (1a)\nq(mt|mt\u22121, rt) = { 1mt\u22121(mt) if rt = rt\u22121 + 1 q(mt) if rt = 0. (1b)\nEq. (1b) implies that the model at time t will be equal to the model at time t\u2212 1 unless a CP occured at t, in which case the next model mt will be a random draw from q. At time t, the algorithm requires for all possible models m and run-lengths rt the computation of the posterior predictives\nfm(yt|y1:(t\u22121), rt)\n= \u222b \u0398m fm(yt|\u03b8m)\u03c0m(\u03b8m|y(t\u2212L\u2212rt):(t\u22121))d\u03b8m. (2)\nTo make the evaluation of this integral efficient, one can use conjugate models (Xuan & Murphy, 2007) or approximations (Turner et al., 2013; Niekum et al., 2014), which make the following recursion efficient, too:\np(y1:t, rt,mt) =\u2211 mt\u22121 \u2211 rt\u22121 { fmt(yt|y1:(t\u22121), rt)q(mt|y1:(t\u22121), rt,mt\u22121)\np(rt|rt\u22121)p(y1:(t\u22121), rt\u22121,mt\u22121) } . (3)\nThe recursion in AM is the special case for |M| = 1. For |M| > 1, q(mt|mt\u22121, rt,y1:(t\u22121)) arises as a new term, which for 1a as the indicator function of a is given by{ 1mt\u22121(mt)q(mt\u22121|y1:(t\u22121), rt\u22121) if rt = rt\u22121 + 1 q(mt) if rt = 0. (4)\nNext, define the growth- and changepoint probabilities as\np(y1:t, rt = rt\u22121 + 1,mt) =\nfmt(yt|y1:(t\u22121), rt)p(y1:(t\u22121), rt\u22121,mt\u22121)\u00d7 (5a) (1\u2212H(rt))q(mt\u22121|y1:(t\u22121), rt),\np(y1:t, rt = 0,mt) =\nfmt(yt|y1:(t\u22121), rt)q(mt)\u00d7 (5b)\u2211 mt\u22121 \u2211 rt\u22121 { H(rt\u22121 + 1)p(y1:(t\u22121), rt\u22121,mt\u22121) } .\nThe evidence can then be calculated via Eq. (6a), which in turn allows calculating the joint model-and-run-length distribution (6b), the model posterior (6c), as well as the model-specific (6d) and global (6e) run-length distributions:\np(y1:t) = \u2211\nmt \u2211 rt p(y1:t,mt, rt) (6a)\np(rt,mt|y1:t) = p(y1:t, rt,mt)/p(y1:t) (6b) p(mt|y1:t) = \u2211 rt p(rt,mt|y1:t) (6c) p(rt|mt,y1:t) = p(rt,mt|y1:t)/p(mt|y1:t) (6d) p(rt|y1:t) = \u2211 mt p(rt,mt|y1:t) (6e)\nq(mt\u22121|y1:(t\u22121), rt\u22121) = p(mt\u22121, rt\u22121|y1:(t\u22121))\np(rt\u22121|y1:(t\u22121)) . (6f)\nEq. (6f) is the conditional model posterior from Eq. (4). Eq. (6e) is arrived at directly in FL and used for on-line MAP segmentation. By framing our derivations in the run-length framework of AM, we additionally obtain (4)\u2013(6d), thus enabling on-line prediction and model selection at the same computational cost."}, {"heading": "2.3. On-line algorithm outputs", "text": "Prediction: Recursive h-step-ahead forecasting uses (6b):\np(Y t+h|y1:t) = \u2211 rt,mt { p(Y t+h|y1:t, y\u0302 h t , rt,mt)p(rt,mt|y1:t) } , (7)\nwhere y\u0302ht = \u2205 if h = 1 and y\u0302 h t = y\u0302(t+1):(t+h\u22121) otherwise, with y\u0302t+h = E(Y t+h|y1:t, y\u0302 h t ) the recursive forecast.\nTracking the model posterior/Bayes Factors: One of the novel capabilites of the algorithm is on-line monitoring of the model posterior via Eq. (6c). This is attractive when structural changes in the data happen slowly and are not captured well by CPS. In this case, P(mt|y1:t) can be used\nto identify periods of change, see Fig. 6. For pairwise comparisons, Bayes Factors can be monitored, too:\nBF(m1, m2)t = p(mt = m1|y1:t) \u00b7 q(m2) p(mt = m2|y1:t) \u00b7 q(m1) . (8)\nMaximum A Posteriori (MAP) segmentation: For MAPt the density of the MAP-estimate of models and CPS before t and MAP0 = 1, FL\u2019s recursive estimator is given by\nMAPt = max r,m\n{ p(y1:t, rt = r,mt = m)MAPt\u2212r\u22121 } . (9)\nFor r\u2217t ,m \u2217 t maximizers for time t, the MAP segmentation is St = St\u2212r\u2217t\u22121 \u222a{(t\u2212 r \u2217 t ,m \u2217 t )}, S0 = \u2205, where (t\u2032,mt\u2032) \u2208 St means a CP at t\u2032 \u2264 t, with mt\u2032 \u2208M the model for yt\u2032:t."}, {"heading": "3. Building a spatio-temporal model universe", "text": "The last section derived BOCPDMS for arbitrary data streams {Y t}. Next, we propose models forM if {Y t} can be mapped into a space S. Let S with |S| = S be a set of spatial locations in S with measurements Y t = (Yt,1, Yt,2, . . . , Yt,S) T recorded at times t = 1, 2, . . ."}, {"heading": "3.1. Bayesian VAR (BVAR)", "text": "Inference on {Y t} can be drawn using conjugate Bayesian Vector Autoregressions (BVAR) with lag length L and E additional variables Zt as elements of model universeM:\n\u03c32 \u223c InverseGamma(a, b) (10a) \u03b5t|\u03c32 \u223c N (0, \u03c32 \u00b7\u2126) (10b) c|\u03c32 \u223c N (0, \u03c32 \u00b7 V c) (10c) Y t = \u03b1+BZt + \u2211L l=1AlY t\u2212l + \u03b5t. (10d)\nHere, Al,B are S \u00d7 S, S \u00d7 E matrices, c = (\u03b1, vec(B), vec(A1), vec(A2), . . . vec(AL))T is a vector of S \u00b7 (LS + 1 + E) model parameters. Scalars a, b > 0, matrix V c, and diagonal matrix \u2126 are hyperparameters."}, {"heading": "3.2. Approximating processes using VARS", "text": "Modelling {Y t} as VAR is attractive, as many complex non-linear processes have VAR representations, including HMMS, time-stationary GPS as well as multivariate GARCH and fractionally integrated VARMA processes (Inoue & Kasahara, 2006; Inoue et al., 2018). Performance guarantees for VAR approximations to such processes are derived using Baxter\u2019s Inequalitiy with multivariate versions of results in Hannan & Kavalieris (1986).\nTheorem 1. Let {Y t} be a time-stationary spatio-temporal process with spectral density satisfying regularity condition A in the Appendix, || \u00b7 || a matrix norm, E(Y t) = 0, E(Y tY Tt ) < \u221e, \u2211\u221e h=\u2212\u221e(1 + |h|)3||E[Y tY \u2032 t+h]|| < \u221e. Then (1)\u2013(3) hold.\n(1) Y t = \u2211\u221e\ni=1AiY t\u2212i + \u03b5t for matrices {Al}l\u2208N and E(\u03b5t) = 0, E(\u03b5t\u03b5\u2032t) = D,D diagonal.\n(2) For Y t = \u2211L l=1A L l Y t\u2212l + et with {A L l }Ll=1 the\nbest linear projection coefficients, \u2203L0 : \u2200L > L0,\u2211L l=1(1 + |l|)3||A L l \u2212 Al|| \u2264 C \u00b7 \u2211\u221e l=L+1(1 +\n|l|)3||Al|| with C constant. (3) Using T observations with L = O([T/ ln(T )]1/6) to\nestimateALl as MAP A\u0302 L l of (10a)\u2013(10d), it holds that L(T )2 \u2211L(T )\nl=1 ||A\u0302 L(T ) l \u2212A L(T ) l || P\u2192 0 as T \u2192\u221e.\nProof. Part (1) is shown in Inoue et al. (2018), part (2) in Lemma 3.1 of Meyer & Kreiss (2015). Part (3) follows by their Remark 3.3 if we can prove that the MAP estimator c\u0302(L(T )) of c equals its Yule-Walker estimator (YWE) as T \u2192\u221e. LetB = 0,\u03b1 = 0 and note that YWE equals OLS as T \u2192\u221e. WithX1:T the regressor matrix of Y t\u2212L(T ):t, c\u0302(L(T )) = (X \u20321:TX1:T + V \u22121 c ) \u22121(X \u20321:TY 1:T ). Then, part (3) holds as OLS P\u2192 E(X \u20321:TX1:T )\u22121E(X \u2032 1:TY 1:T ) and\nc\u0302(L(T )) = (X \u20321:TX1:T + V \u22121 c ) \u22121(X \u20321:TY 1:T )\n= ( 1\nT X \u20321:TX1:T +\n1 T V \u22121c ) \u22121 1 T (X \u20321:TY 1:T )\nP\u2192 E(X \u20321:TX1:T )\u22121E(X \u2032 1:TY 1:T ).\nIn Thm. 1, assuming E(Y t) = 0 is without loss of generality: If E(Y t) = \u03b1+BZt, define Y \u2217t = Y t\u2212 (\u03b1+BZt) and apply the theorem to {Y \u2217t }. Moreover, the results do not require stationarity in space. Lastly, part (3) suggests a principled way of picking lag lengths L for BVAR models based on functions L(T ) = C \u00b7 (T/ ln(T ))1/6, with C a constant: If between T1 and T2 observations are expected between CPS, L = {L \u2208 N : L(T1) \u2264 L \u2264 L(T2)}. In our experiments, we employ this strategy using T1 = 1, T2 = T ."}, {"heading": "3.3. Modeling spatial dependence", "text": "While Thm. 1 motivates approximating spatio-temporal processes between CPS with (10a)\u2013(10d), the matrices {ALl }Ll=1 have S(LS + 1 + E) parameters. This increases model complexity and ignores spatial information. We remedy both issues through neighbourhood systems on S. Definition 1 (Neighbourhood system). For a set of locations S with the sets Ni(s) \u2286 S as the i-th neighbourhoods of s for 0 \u2264 i \u2264 n and all s \u2208 S, let Ni(s) \u2229 Nj(s) = \u2205, s\u2032 \u2208 Ni(s) \u21d0\u21d2 s \u2208 Ni(s\u2032) and N0(s) = {s}. Then, the corresponding neighbourhood system is N(S) = {{Ni(s)}ni=1 : s \u2208 S, 0 \u2264 i \u2264 n}.\nIn the remainder of the paper, smaller indices i imply that the neighbourhoods Ni(s) are closer to s. For a BVAR model of lag length L, the decay of spatial dependence is encapsulated through \u03a0 : {1, . . . , L} \u2192 {0, . . . , n}. In\nparticular, only s\u2032 \u2208 Ni(s) with i \u2264 \u03a0(l) are modeled as affecting s after l time periods."}, {"heading": "3.4. Spatializing BVAR", "text": "In principle, given N(S), sparsification of the BVAR model (10a)\u2013(10d) is possible in two ways: As restriction on the contemporaneous dependence via the covariance matrix of the error term \u03b5t, or as restriction on the conditional dependence via the coefficient matrices {Al}Ll=1. We choose the latter for three reasons: Firstly, linear effects have more interesting interpretations than error covariances. Secondly, using {Al}Ll=1 to encode spatial dependency allows us to work with arbitrary neighbourhoods. In contrast, modelling dependent errors under conjugacy limits dependencies to decomposable graphs (Xuan & Murphy, 2007). Since not even a regular grid is decomposable, this is problematic for spatial data. Thirdly, modelling errors as contemporaneous is attractive for low-frequency data where the resolution of temporal effects is coarse, but the situation reverses for high-frequency data. Since the algorithm runs on-line, we expect {Y t} to be observed with high frequency. Definition 2 (Spatially structured BVAR (SSBVAR)). For process {Y t} on S and (L,N(S),\u03a0(\u00b7)), define the matrices {A\u0303l}Ll=1 by imposing that [A\u0303l](s,s\u2032) = 0 \u21d0\u21d2 s\u2032 /\u2208 Ni(s) for any i \u2264 \u03a0(l). Let A\u0303 6=0 l be the vector of non-zero entries in A\u0303l and c\u0303 = (\u03b1, vec(B), A\u0303 6=0 1 , A\u0303 6=0 2 , . . . A\u0303 6=0 L )\nT . The SSBVAR model on {Y t} induced by (L,N(S),\u03a0(\u00b7)) is obtained by combining (10a)\u2013(10b) with\nc\u0303|\u03c32 \u223c N (0, \u03c32 \u00b7 V c\u0303) (10e) Y t = \u03b1+BZt + \u2211L l=1 A\u0303lY t\u2212l + \u03b5t. (10f)\nFig. 2 illustrates this idea. Further sparsification is possible by modelling neighbourhoods jointly, i.e. [A\u0303l](s,s\u2032) = ai(s),\u2200s\u2032 \u2208 Ni(s), reducing the number of parameters to S \u00b7 \u2211L l=1 \u03a0(l). If one imposes ai(s) = ai(s\n\u2032) = \u00b7 \u00b7 \u00b7 = ai, this number drops to \u2211L l=1 \u03a0(l)."}, {"heading": "3.5. Building SSBVARS: choosing L,N(S),\u03a0(\u00b7)", "text": "For the choice of lag lengths L, part (3) of Thm. 1 suggests L \u2208 {L\u2032 \u2208 N : L(T1) \u2264 L\u2032 \u2264 L(T2)} if one expects T1 to T2 observations between CPS. For any data stream {Y t} on a space S, there are different ways of constructing neighbourhood structures N(S). For example, when analysing pollutants in London\u2019s air in section 6, N(S) could be constructed from Euclidean or Road distances. By fillingM with SSBVARS constructed using competing versions of N(S), BOCPDMS provides a way of dealing with such uncertainty about spatial relations. In fact, it can dynamically discern changing spatial relationships on S. Lastly, \u03a0(\u00b7) should usually be decreasing to reflect that measurements affect each other less when further apart."}, {"heading": "4. Hyperparameter optimization", "text": "Hyperparameter inference on \u03bdm can be addressed either by introducing an additional hierarchical layer (Wilson et al., 2010) or using type-II ML. The latter is obtained by maximizing the model-specific evidence\nlog p(y1:T |\u03bdm) = T\u2211\nt=1\nlog p(yt|\u03bdm,y1:(t\u22121)). (11)\nComputation of the righthand side requires evaluating the gradients \u2207\u03bdmp(y1:t, rt|\u03bdm), which are obtained efficiently and recursively (Turner et al., 2009). Saatc\u0327i et al. (2010) use y1:T \u2032 as a test set, and run BOCPD K times to find \u03bd\u0302m = arg max\u03bdm {p(y1:T \u2032 |\u03bdm)}. Most other on-line GP approaches also require substantial recomputations for hyperparameter learning (e.g., Ranganathan et al., 2011). In contrast, Caron et al. (2012) propose on-line gradient descent updates via\n\u03bdm,t+1 = \u03bdm,t + \u03b1t\u2207\u03bdm,t log p(yt+1|y1:t,\u03bdm1:t). (12)\nThe latter is preferable for two reasons: Firstly, inference and type-II ML are executed simultaneously (rather than sequentially) and thus enable cold-starts of BOCPDMS. Secondly, neither the on-line nature nor the computational complexity of BOCPDMS is affected."}, {"heading": "5. Computation & Complexity", "text": "While tracking |M| models, BOCPDMS has linear time complexity. Step 1 in the pseudocode is the bottleneck, but looping overM can be parallelized: With N threads, it executes in O (d|M|/Ne \u00b7maxM\u2208M CmpTime(M)). Step 2 takes O(|R(t)||M|), for R(t) all run-lengths at time t."}, {"heading": "5.1. Pruning the run-length distribution", "text": "In a naive implementation, all run-lengths are retained and R(t) = {1, 2, . . . , t}. This implies execution time of order\nO(t) for processing yt, but can be made time-constant by pruning: If one discards run-lengths whose posterior probability is \u2264 1/Rmax or only keeps the Rmax most probable ones, |R(t)| \u2264 Rmax (Adams & MacKay, 2007). A third way is Stratified Rejection Control (SRC) (Fearnhead & Liu, 2007), which Caron et al. (2012) and the current paper found to perform as well as the other approaches. In our experiments, we prune by keeping the Rmax most probable model-specific run-lengths p(rt|mt,y1:t) for each model."}, {"heading": "5.2. BVAR updates", "text": "The bottleneck when updating a BVAR model in M is step I.B.1 in the pseudocode of BOCPDMS, when updating the MAP estimate c(r, t) = F (r, t)W (r, t) of the coefficient vector , where F (r, t) = (X \u2032(t\u2212r):tX(t\u2212r):t +V c\u0303) \u22121 and W (r, t) = X \u2032(t\u2212r):tY (t\u2212r):t for all r \u2208 R(t). Since W (r, t) = W (r \u2212 1, t\u2212 1) +X \u2032tY t, updates are O(kS). F (r \u2212 1, t \u2212 1) can be updated to F (r, t) using rank-k updates to its QR-decomposition in O(k3) or using Woodbury\u2019s formula, in O(S3), implying an overall complexity of O(|R(t)|min{k3, S3}) at time t."}, {"heading": "5.3. Comparison with GP-based approaches", "text": "Define kmax as the largest number of regressors of any BVAR model inM. From the previous paragraphs, it follows that if all models in M are BVARS, the overhead C = dN/|M|e \u00b7 min{k3max, S3} is time-constant. Thus, BOCPDMS runs in O(TRmax) on T observations. In contrast, the models of Saatc\u0327i et al. (2010) run in O(TR3max). The experiments in section 6 confirm this: Using the software of Turner (2012) on the Nile data, fitting one ARGPCP model takes 42 seconds compared to 12 seconds when fitting three models in BOCPDMS, so a BVAR model is > 10\u00d7 faster to process. Per inferred parameter, BOCPDMS is > 60\u00d7 faster than ARGPCP; and this factor is much larger for multivariate data (e.g., > 270 for the 30 Portfolio data). More details on the run-times can be found in the Appendix."}, {"heading": "6. Experimental results", "text": "We evaluate performance with code available from https://github.com/alan-turing-institute/bocpdms in two parts. First, we compare to benchmark performances of GP-based models on real world data reported by Saatc\u0327i et al. (2010). This shows that as implied by Thm. 1, VARS are excellent approximations for a large variety of data streams. Next, we showcase BOCPDMS\u2019 novelty in the multivariate setting. We use uniform model priors q, a constant Hazard functions H and gradient descent for hyperparameter optimization as in Section 4. The lag lengths of models inM are chosen based on Thm. 1 (3) and the rates of Hannan & Kavalieris (1986) for BVARS and Bayesian Autoregressions\n(BARS), respectively."}, {"heading": "6.1. Comparison with GP-based approaches", "text": "As in Saatc\u0327i et al. (2010), ARGPCP will refer to the non-linear GP-based AR model, GPTSCP to the timedeterministic model, and NSGP to the non-stationary GP allowing hyper-parameters to change at every CP. Saatc\u0327i et al. (2010) compute the mean squared error (MSE) as well as the negative log predictive likelihood (NLL) of the onestep-ahead predictions for three data sets: The water height of the Nile between 622\u22121284 AD, the snowfall in Whistler (Canada) over a 37 year period and the 3-dimensional time series (x-, y-coordinate and headangle) of a honey bee during a waggle dance sequence. In Turner (2012), all of the models except NSGP were also compared on daily returns for 30 industry portfolios from 1975 \u2212 2008. In Table 1, BOCPDMS is compared to these benchmarks forM consisting of BAR and SSBVAR models.\n6.1.1. DESIGNINGM\nBoth the Nile and the snowfall data are univariate, soM consists of BARS with varying lag lengths. For the 3- dimensional bee data,M additionally contains unrestricted BVARS. Lastly, SSBVARS are used on the 30 Portfolio data. Two neighbourhood systems are constructed from distances in the spaces of pairwise contemporaneous correlations and autocorrelations prior to 1975, a third using the Standard Industrial Classification (SIC), with \u03a0(\u00b7) decreasing linearly."}, {"heading": "6.1.2. FINDINGS", "text": "Predictive performance and fit: In terms of MSE, BOCPDMS clearly outperforms all GP-models on multivariate data. Even on univariate data, the only exception to this is the snowfall data, where NSGP does better. However, NSGP requires grid search or Hamiltonian Monte Carlo sampling for hyperparameter optimization at each obser-\nvation (Saatc\u0327i et al., 2010). Overall, there are three main reasons why BOCPDMS performs better: Firstly, being able to change lag lengths between CPS seems more important to predictive performance than being able to model non-linear dynamics. Secondly, unlike the GP-models, we allow the time series to communicate via {ALl }. Thirdly, the hyperparameters of the GP have a strong influence on inference. In particular, the noise variance \u03c3 is treated as a hyperparameter and optimized via type-II ML. Except for the NSGP, this is only done during a training period. Thus, the GP-models cannot adapt to the observations after training, leading to overconfident predictive distributions that are too narrow (see Turner, 2012, p. 172). This in turn leads them to be more sensitive to outliers, and to mislabel them as CPS. In contrast, (10a)\u2013(10d) models \u03c3 as part of the inferential Bayesian hierarchy, and hyperparameter optimization is instead applied at one level higher. Consequently, our predictive distributions are wider, and the algorithm is less confident about the next observations, making it more robust to outliers. This is also responsible for the overall smaller standard errors of the GP-models in Table 1, since the GPS interpret outliers as CPS and immediately adapt to short-term highs or lows.\nCP Detection: A good demonstration of this finding is the Nile data set, where the MAP segmentation finds a single CP, corresponding to the installation of the nilometer\naround 715 CE, see Fig 5. In contrast, Saatc\u0327i et al. (2010) report 18 additional CPS corresponding to outliers. The same phenomenon is also reflected in the run-length distribution (RLD): While the probability mass in Figs. 3, 4 and 5 are spread across the retained run-lengths, the RLD reported in Saatc\u0327i et al. (2010) is more concentrated and even degenerate for the 30 Portfolio data set. On the other hand, such enhanced sensitivity to change can be advantageous. For instance, in the bee waggle dance, the GP-based techniques are better at identifying the true CPS. The reason is twofold: Firstly, the variance for the bee waggle data is homogeneous across time, so treating it as fixed helps inference. Secondly, the CPS in this data set are subtle, so having narrower predictive distributions is of great help in detecting them. However, it adversely affects performance when changes in the error variance are essential, as for financial data: In particular, BOCPDMS finds the ground truths labelled in Saatc\u0327i et al. (2010), and discovers even more, see Fig. 3. This is especially apparent in times of market turmoil where changes in the variance of returns are significant. We show this using the example of the subprime mortgage financial crisis: While the RLD of Saatc\u0327i et al. (2010) identified only 2 CPS with ground truth and a third unlabelled one during the height of the crisis, BOCPDMS detects a large number of CPS corresponding to ground truths, see Fig. 4.\nLastly, we note that segmentations obtained off-line for both the bee waggle dance and the 30 Portfolios are reported in Xuan & Murphy (2007). Compared to the on-line segmentations produced by BOCPDMS, these are closer to the truth for the bee waggle data, but not for the 30 Portfolio data set.\nModel selection: In most of the experiments where abrupt changes model the non-stationarity well, the model posterior is fairly concentrated and periods of model uncertainty are short. This is different when changes are slower, see Fig. 6. The implicit model complexity penalization Bayesian model selection performs provides BOCPDMS with an Occam\u2019s\nRazor mechanism: Simple models are typically favoured until evidence for more complex dynamics accumulates. For the bee waggle and the 30 Portfolio data set, BVARS are preferred to BARS. For the 30 Portfolio data, the MAP segmentation only selects SSBVARS with neighbourhoods constructed from contemporaneous correlation and autocorrelations. Neighbourhoods using SIC codes are not selected, reflecting that this classification from 1937 is out of date."}, {"heading": "6.2. Performance on spatio-temporal data", "text": "European Temperature: Monthly temperature averages 01/01/1880\u2212 01/01/2010 for the 21 longest-running stations across Europe are taken from http://www.ecad.eu/. We adjust for seasonality by subtracting monthly averages for each station. Station longitudes and latitudes are available, so N(S) is based on concentric rings around the stations using Euclidean distances. Two different decay functions \u03a0(\u00b7),\u03a0+(\u00b7) are used, with \u03a0+(\u00b7) using larger neighbourhoods and slower decaying. Temperature changes are poorly modeled by CPS and more likely to undergo slow transitions. Fig. 6 shows the way in which the model posterior captures such longer periods of change in dynamics. The values on the bottom panel are calculated by considering m\u0302t = arg maxmt\u2208M p(mt|y1:t) as |M|-dimensional multinomial random variable. Its Standardized Generalized Variance (SGV) (Wilks, 1960; SenGupta, 1987) is calculated as |M|-th root of the covariance matrix determinant. We plot the log of the SGV computed using the model posteriors for the last 8 years. This provides an informative summary of the model posterior dispersion.\nAir Pollution: Finally, we analyze Nitrogen Oxide (NOX) observed at 29 locations across London 17/08/2002 \u2212 17/08/2003. The quarterhourly measurements are aver-\naged over 24 hours. Weekly seasonality is accounted for by subtracting week-day averages for each station. M is populated with SSBVAR models whose neighbourhoods are constructed from both road- and Euclidean distances. As 17/02/2003 marks the introduction of London\u2019s first ever congestion charge, we find structural changes in the dynamics around that date. A model with shorter lag length but identical neighbourhood structure is preferred after the congestion charge. In Fig. 7, Bayes Factors (BFS) capture the shift: Kass & Raftery (1995) classify logs of BFS as very strong evidence if their absolute value exceeds 5."}, {"heading": "7. Conclusion", "text": "We have extended Bayesian On-line Changepoint Detection (BOCPD) to multiple models by generalizing Fearnhead & Liu (2007) and Adams & MacKay (2007), arriving at BOCPDMS. For inference in multivariate data streams, we propose BVARS with closed form distributions that have strong theoretical guarantees summarized in Thm. 1. We sparsify BVARS based on neighbourhood systems, thus making BOCPDMS especially amenable to spatio-temporal inference. To demonstrate the power of the resulting framework, we apply it to multivariate real world data, outperforming the state of the art. In future work, we would like to add and remove models from M on-line. This could lower the computational cost for the case where |M| is significantly larger than the number of threads."}, {"heading": "Acknowledgements", "text": "We want to thank N. Karampatziakis for his help with making the method computationally more efficient. JK is funded by EPSRC grant EP/L016710/1. Further, this work was supported by The Alan Turing Institute for Data Science and AI under EPSRC grant EP/N510129/1 and the Lloyds Register Foundation programme on Data Centric Engineering."}], "year": 2018, "references": [{"title": "Bayesian online changepoint detection", "authors": ["R.P. Adams", "D.J. MacKay"], "venue": "arXiv preprint arXiv:0710.3742,", "year": 2007}, {"title": "Product partition models for change point problems", "authors": ["D. Barry", "J.A. Hartigan"], "venue": "The Annals of Statistics,", "year": 1992}, {"title": "On-line changepoint detection and parameter estimation with application to genomic data", "authors": ["F. Caron", "A. Doucet", "R. Gottardo"], "venue": "Statistics and Computing,", "year": 2012}, {"title": "Generalized direct change estimation in ising model structure", "authors": ["F. Fazayeli", "A. Banerjee"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Exact bayesian curve fitting and signal segmentation", "authors": ["P. Fearnhead"], "venue": "IEEE Transactions on Signal Processing,", "year": 2005}, {"title": "On-line inference for multiple changepoint problems", "authors": ["P. Fearnhead", "Z. Liu"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2007}, {"title": "Sequential bayesian prediction in the presence of changepoints", "authors": ["R. Garnett", "M.A. Osborne", "S.J. Roberts"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "year": 2009}, {"title": "The global economic & financial crisis: A timeline", "authors": ["M.F. Guill\u00e9n"], "venue": "The Lauder Institute, University of Pennsylvania,", "year": 2009}, {"title": "Regression, autoregression models", "authors": ["E. Hannan", "L. Kavalieris"], "venue": "Journal of Time Series Analysis,", "year": 1986}, {"title": "Scalable gaussian processes for characterizing multidimensional change surfaces", "authors": ["W. Herlands", "A. Wilson", "H. Nickisch", "S. Flaxman", "D. Neill", "W. Van Panhuis", "E. Xing"], "venue": "In Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Explicit representation of finite predictor coefficients and its applications", "authors": ["A. Inoue", "Y. Kasahara"], "venue": "The Annals of Statistics,", "year": 2006}, {"title": "Baxters inequality for finite predictor coefficients of multivariate long-memory stationary processes", "authors": ["A. Inoue", "Y. Kasahara", "M Pourahmadi"], "year": 2018}, {"title": "Asymptotically consistent estimation of the number of change points in highly dependent time series", "authors": ["A. Khaleghi", "D. Ryabko"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "year": 2014}, {"title": "M-statistic for kernel change-point detection", "authors": ["S. Li", "Y. Xie", "H. Dai", "L. Song"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "A sharp error analysis for the fused lasso, with application to approximate changepoint screening", "authors": ["K. Lin", "J.L. Sharpnack", "A. Rinaldo", "R.J. Tibshirani"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Changepoint detection in time-series data by relative density-ratio estimation", "authors": ["S. Liu", "M. Yamada", "N. Collier", "M. Sugiyama"], "venue": "Neural Networks,", "year": 2013}, {"title": "European seasonal and annual temperature variability, trends, and extremes since 1500", "authors": ["J. Luterbacher", "D. Dietrich", "E. Xoplaki", "M. Grosjean", "H. Wanner"], "venue": "Science,", "year": 2004}, {"title": "On the vector autoregressive sieve bootstrap", "authors": ["M. Meyer", "Kreiss", "J.-P"], "venue": "Journal of Time Series Analysis,", "year": 2015}, {"title": "Changepoint detection using approximate model parameters", "authors": ["S. Niekum", "S. Osentoski", "C.G. Atkeson", "Barto", "A.G. Champ"], "venue": "Technical report, (No. CMU-RI-TR-14-10) Carnegie-Mellon University Pittsburgh PA Robotics Institute,", "year": 2014}, {"title": "Online sparse gaussian process regression and its applications", "authors": ["A. Ranganathan", "Yang", "M.-H", "J. Ho"], "venue": "IEEE Transactions on Image Processing,", "year": 2011}, {"title": "Gaussian process change point models", "authors": ["Y. Saat\u00e7i", "R.D. Turner", "C.E. Rasmussen"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "year": 2010}, {"title": "Generalizations of barlett\u2019s and hartley\u2019s tests of homogeneity using overall variability", "authors": ["A. SenGupta"], "venue": "Communications in Statistics-Theory and Methods,", "year": 1987}, {"title": "Adaptive sequential bayesian change point detection", "authors": ["R. Turner", "Y. Saatci", "C.E. Rasmussen"], "venue": "In Temporal Segmentation Workshop at NIPS,", "year": 2009}, {"title": "Gaussian Processes for State Space Models and Change Point Detection", "authors": ["R.D. Turner"], "venue": "PhD thesis, University of Cambridge,", "year": 2012}, {"title": "Online variational approximations to non-exponential family change point models: with application to radar tracking", "authors": ["R.D. Turner", "S. Bottone", "C.J. Stanek"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Multidimensional statistical scatter", "authors": ["S. Wilks"], "venue": "Contributions to Probability and Statistics, pp", "year": 1960}, {"title": "Bayesian online learning of the hazard rate in change-point problems", "authors": ["R.C. Wilson", "M.R. Nassar", "J.I. Gold"], "venue": "Neural Computation,", "year": 2010}, {"title": "Modeling changing dependency structure in multivariate time series", "authors": ["X. Xuan", "K. Murphy"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "year": 2007}], "id": "SP:bc5b0f4b62026ccbe3b3f9bf22064e40c00652fe", "authors": [{"name": "Jeremias Knoblauch", "affiliations": []}, {"name": "Theodoros Damoulas", "affiliations": []}], "abstractText": "Bayesian On-line Changepoint Detection is extended to on-line model selection and nonstationary spatio-temporal processes. We propose spatially structured Vector Autoregressions (VARS) for modelling the process between changepoints (CPS) and give an upper bound on the approximation error of such models. The resulting algorithm performs prediction, model selection and CP detection on-line. Its time complexity is linear and its space complexity constant, and thus it is two orders of magnitudes faster than its closest competitor. In addition, it outperforms the state of the art for multivariate data.", "title": "Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection "}