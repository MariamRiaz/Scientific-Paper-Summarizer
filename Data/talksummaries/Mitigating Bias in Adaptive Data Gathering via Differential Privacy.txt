0	28	Many modern data sets consist of data that is gathered adaptively: the choice of whether to collect more data points of a given type depends on the data already collected.
1	15	For example, it is common in industry to conduct “A/B” tests to make decisions about many things, including ad targeting, user interface design, and algorithmic modifications, and this A/B testing is often conducted using “bandit learning algorithms” (Bubeck et al., 2012), which adaptively select treatments to show to users in an effort to find the best treatment as quickly as possible.
2	13	Similarly, sequential clinical trials may halt or re-allocate certain treatment groups due to preliminary results, and empirical scientists may initially try and test multiple hypotheses and multiple treatments, but then decide to gather more data in support of certain hypotheses and not others, based on the results of preliminary statistical tests.
5	16	(Nie et al., 2017) give a selective inference approach: in simple stochastic bandit settings, if the data was gathered by a specific stochastic algorithm that they design, they give an MCMC based procedure to perform maximum likelihood estimation to recover de-biased estimates of the underlying distribution means.
8	21	Via elementary techniques, this connection implies the existence of simple stochastic bandit algorithms with nearly optimal worst-case regret bounds, with very strong bias guarantees.
9	48	By leveraging existing connections between differential privacy and adaptive data analysis (Dwork et al., 2015c; Bassily et al., 2016; Rogers et al., 2016), we can extend the generality of our approach to bound not just bias, but to correct for effects of adaptivity on arbitrary statistics of the gathered data.
11	33	This paper has three main contributions: 1.
12	19	Using elementary techniques, we provide explicit bounds on the bias of empirical arm means maintained by bandit algorithms in the simple stochastic setting that make their selection decisions as a differentially private function of their observations.
13	44	Together with existing differentially private algorithms for stochastic bandit problems, this yields an algorithm that obtains an essentially optimal worst-case regret bound, and guarantees minimal bias (on the order of O(1/ √ K · T )) for the empirical mean maintained for every arm.
21	14	We also demonstrate in the linear contextual bandit setting how failing to correct for adaptivity can lead to false discovery when applying t-tests for non-zero regression coefficients on an adaptively gathered dataset.
35	19	, T}, an algorithm A chooses an arm it ∈ [K], and observes a reward yit,t ∼ Pit .
40	18	, PK , and with high probability or in expectation over the randomness of the algorithm and of the reward sampling.
53	32	Above we’ve characterized a bandit algorithmA as gathering data adaptively using a sequence of selection functions ft, which map the observed history Λt ∈ Ht−1 to the index of the next arm pulled.
56	21	In this section, we observe that whether the reward is drawn after the arm is “pulled,” or in advance, is a distinction without a difference.
57	14	We cast this same interaction into the setting where an analyst asks an adaptively chosen sequence of queries to a fixed dataset, representing the arm rewards.
59	25	The formalization consists of observing two things.
61	32	Second, the choice of arm pulled at time t by the bandit algorithm can be viewed as the answer to an adaptively selected query against this fixed dataset of rewards.
65	12	Recall that histories Λ record the choices of the algorithm, in addition to its observations.
96	24	In the contextual setting, two bandit tableau’s (D,C), (D′, C ′) are reward neighbors if C = C ′ and D and D′ differ in at most a single row: i.e. if there exists an index ` such that for all t 6= `, Dt = D′t.
102	15	Together with known differentially private algorithms for stochastic bandit problems, the result is an algorithm that obtains a nearly optimal (worst-case) regret guarantee while also guaranteeing that the collected data is nearly unbiased.
105	14	Note that since µi ∈ [0, 1], and for 1, e ≈ 1+ , this theorem bounds the bias by roughly +Tδ.
108	21	Let 1{ft(Λt)=i} be the indicator for the event that arm i is pulled at time t. We can write the random variable representing the sample mean of arm i at time T as Ŷ Ti = T∑ t=1 1{ft(Λt)=i}∑T t′=1 1{ft′ (Λt′ )=i} yit where we recall that yi,t is the random variable representing the reward for arm i at time t. Note that the numerator (ft(Λt) = i) is by definition independent of yi,t, but the denominator ( ∑T t′=1 1{ft′ (Λt′ )=i}) is not, because for t ′ > tΛt′ depends on yi,t.
109	49	It is this dependence that leads to bias in adaptive data gathering procedures, and that we must argue is mitigated by differential privacy.
110	19	We recall that the random variable NTi represents the number of times arm i is pulled through round T : NTi =∑T t′=1 1{ft′ (Λt′ )=i}.
126	26	(Tossou & Dimitrakakis, 2016) There is an - differentially private algorithm that obtains expected regret bounded by: O ( max ( lnT · (ln ln(T ) + ln(1/ )) , √ kT log T )) Thus, we can take to be as small as = O( ln 1.5 T√ kT ) while still having a regret bound of O( √ kT log T ), which is nearly optimal in the worst case (over instances) (Audibert & Bubeck, 2009).
128	57	There exists a simple stochastic bandit algorithm that simultaneously guarantees that the bias of the empirical average for each arm i is bounded by O(µi · ln 1.5 T√ kT ) and guarantees expected regret bounded by O( √ kT log T ).
130	21	For example, the algorithm of (Tossou & Dimitrakakis, 2016) obtains sub-linear regret so long as = ω( ln 2 T T ).
131	11	Thus, it is possible to obtain non-trivial regret while guaranteeing that the bias of the empirical means remains as low as polylog(T )/T .
135	48	We briefly define max information, state the connection to differential privacy, and illustrate how max information bounds can be used to perform adaptive analyses in the private data gathering framework.
137	14	Let X,Z be jointly distributed random variables over domain (X ,Z).
