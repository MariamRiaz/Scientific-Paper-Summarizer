{"sections": [{"heading": "1. Introduction", "text": "Matrix completion method has been used in a wide range of applications such as collaborative filtering for recommendation (Koren et al., 2009), multi-label learning (Cabral et al., 2011) and clustering (Hsieh et al., 2012). In these applications, every entry is modeled as the inner product between factors corresponding to the row and column variables. For example, in movie recommendation, each row factor represents the latent representation of a user and each column factor represents the latent representation of a movie.\nIn many applications of significant interest, besides the partially observed matrix, side information, in the form of features, is also available. These might correspond to de-\n*Equal contribution 1Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA. 2Machine Learning Department, Carnegie Mellon University, Pittsburgh, Pennsylvania, USA. 3Department of Computer Science, University of California, Los Angeles, CA 90095, USA. Correspondence to: Quanquan Gu <qgu@cs.ucla.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nmographic information (genders, occupation) for users or product information (genre, director) in a movie recommender system for example. With such features at hand, one can model an observation as a specific linear interaction between features to reduce the model complexity. Formally, let L\u2217 \u2208 Rd1\u00d7d2 be the unknown low-rank matrix with rank r, and let XL \u2208 Rd1\u00d7n1 and XR \u2208 Rd2\u00d7n2 be the known feature matrices with d1 \u2265 n1 \u2265 r and d2 \u2265 n2 \u2265 r. We assume the unknown rank-r matrix L\u2217 can be represented by XLM\u2217X>R for some unknown matrix M\n\u2217 \u2208 Rn1\u00d7n2 . Thus instead of learning a large d1 \u00d7 d2 matrix L\u2217, we only need to recover a smaller low-rank matrix M\u2217. This inductive approach has been applied successfully in many applications including collaborative filtering (Abernethy et al., 2009; Menon et al., 2011; Chen et al., 2012), multi-label learning (Xu et al., 2013; Si et al., 2016), semi-supervised clustering (Yi et al., 2013; Si et al., 2016), gene-disease prediction (Natarajan & Dhillon, 2014) and blog recommendation (Shin et al., 2015).\nFrom the theoretical point of view, side information allows us to reduce the overall sample and computational complexities. Xu et al. (2013) and Jain & Dhillon (2013) pioneered the theoretical investigation in this direction. Specifically, Xu et al. (2010) adapted the convex relaxation approach (Cande\u0300s & Recht, 2009; Cande\u0300s & Tao, 2010) and requires only O(rn log n log d)1 samples to recover the underlying matrix, which we believe is tight up to logarithmic factors. However, the computational cost is usually high because they need to solve a nuclear norm minimization problem, which is inherently slow due to its high per-iteration complexity and non-strongly convex objective function (c.f. Equation (2) in Xu et al. (2013)), which does not have linear convergence rate. On the other hand, Jain & Dhillon (2013) (also see Zhong et al. (2015)) proposed an algorithm which first does a spectral initialization to obtain a coarse estimate, then uses alternating minimization to refine the estimate. Their algorithm has a locally linear rate of convergence but requires O(r3n2 log n log(1/ )) samples, which has an unsatisfactory quadratic dependency on n and cannot achieve exact recovery because sample complexity also depends on the target accuracy . A natural and open question is:\n1For the ease of presentation, we assume d1 = d2 = d and n1 = n2 = n when discussing complexities.\nCan we recover the ground truth matrix at a linear rate with sample complexity linear in n?\nIn this paper, we answer this question affirmatively. Specifically, we propose a multi-phase gradient-based algorithm that converges to the underlying true matrix at a linear rate with sample complexity linearly depending on n and logarithmically depending on d. Our algorithm is a novel and highly nontrivial extension of Procrustes Flow (Tu et al., 2015) in which we add an additional phase to reduce the variance of gradient estimate, and therefore we call it MultiPhase Procrustes Flow. The main challenges and technical insights are summarized in the following section."}, {"heading": "1.1. Main Challenges and Technical Insights", "text": "In recent years, a surge of non-convex optimization algorithms for estimating low-rank matrices have been established. A typical procedure is first to do a spectral initialization to obtain a coarse estimate, and then to use BurerMonteiro factorization (Burer & Monteiro, 2003) with projected gradient descent (a.k.a., Procrustes flow) on the partially observed entries to recover the underlying matrix, where the projection is introduced to control the variance of gradient descent (Tu et al., 2015; Zheng & Lafferty, 2016; Yi et al., 2016). Our proposed algorithm also follows this framework. However, direct adaptation does not achieve the desired statistical and computational rates. Statistically, in the classical matrix completion setting, after the initialization phase, the variance of the gradient is at a smaller order than the magnitude of expected gradient for all iterations. However, in our setting, because of limited samples, such uniform bound does not hold. Computationally, the projection step in the inductive setting is more costly than that in the classical setting because we need to solve a convex quadratically-constrained-quadratic-programming (QCQP) problem (c.f. Section 4).\nOur first key observation is that the variance of the gradient converges to 0 at a faster rate than the magnitude of expectation of the gradient. Therefore, if the iterate is close enough to the optimum, say in a ball with radius O(1/n) around the optimum, the desired uniform bound still holds. Further, this observation also indicates when we are close to the optimum, projection step is not needed, i.e., vanilla gradient descent suffices. Nevertheless, with limited samples, the spectral initialization cannot directly achieve this goal.\nOur second key observation is that after a rough spectral initialization, if we use fresh samples to calculate the gradient at each iteration, the variance is still small compared with the expectation of the gradient. In light of this, we add a new phase to the original algorithm where we use fresh samples to estimate the gradient at each iteration and use projected gradient descent to refine our estimation. Though the projection is costly, we only need O(r log n) iterations\nto converge to a ball with radius O(1/n) around the optimum, since gradient descent in our problem enjoys a linear rate of convergence. Putting all these phases together, we propose the first gradient-based algorithm that requires only O ( r2n log n log d ) samples and converges to the ground truth matrix at a linear rate.\nNotation. Capital boldface letters such as A are used for matrices, and [`] is used to denote the index set {1, 2, . . . , `}. Denote the d \u00d7 d identity matrix by Id. Let Ai,\u2217, A\u2217,j and Aij be the i-th row, j-th column and (i, j)-th entry of matrix A, respectively. Denote the `-th largest singular value of A by \u03c3`(A) and its projection onto the index set \u2126 by P\u2126(A), i.e., the (i, j)-th entry of P\u2126(A) is equal to Aij if (i, j) \u2208 \u2126 and zero otherwise. Let \u2016x\u20162 be the `2 norm of a d-dimensional vector x \u2208 Rd. Let \u2016A\u2016F , \u2016A\u20162 be the Frobenius norm and the spectral norm of matrix A respectively. The largest `2 norm of its rows is defined as \u2016A\u20162,\u221e = maxi \u2016Ai,\u2217\u20162. For any two sequences {an} and {bn}, we say an = O(bn) if there exists a positive constant C such that an \u2264 C bn."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. Low-Rank Matrix Completion", "text": "Classical approach for matrix completion relies on convex relaxation (Cande\u0300s & Recht, 2009; Cande\u0300s & Tao, 2010; Recht, 2011; Chen, 2015; Allen-Zhu et al., 2017), which can be solved by nuclear norm minimization. Such methods usually have tight sample complexity (Balcan et al., 2017), but due to the use of nuclear norm and non-strongly convex objective function, they cannot achieve linear convergence rate and often scale cubically with the dimension. Some faster algorithms have been proposed (Jain & Netrapalli, 2015) but they often incur additional sample complexity.\nTo reduce the runtime complexity, various non-convex algorithms have been proposed. Jain et al. (2013); Hardt (2014); Hardt & Wootters (2014); Gu et al. (2016); Gamarnik et al. (2017) showed that with proper initialization, alternating minimization enjoys a linear convergence rate. Proofs of these works often build on a general analytical framework, noisy-power-method (Hardt & Price, 2014; Balcan et al., 2016). Nevertheless, the sample complexity often depends on the inverse of target accuracy. Thus these methods often cannot recover the ground truth matrix exactly.\nAnother line of research studies the landscape of optimization problem and showed that with proper modification of objective function, all local minima are global and all saddle points are strict (Bhojanapalli et al., 2016b; Ge et al., 2016; 2017). Therefore, perturbed gradient descent algorithms can solve this non-convex problem efficiently (Ge et al., 2015; Jin et al., 2017; Du et al., 2017a). However, to guarantee the landscape having nice properties, they all require the\nsample complexity scales with the fourth power of the rank, which is suboptimal.\nLastly, Tu et al. (2015); Zhao et al. (2015); Zheng & Lafferty (2015); Sun & Luo (2015); Bhojanapalli et al. (2016a); Zheng & Lafferty (2016); Yi et al. (2016); Wang et al. (2016; 2017); Ma et al. (2017); Xu et al. (2017); Zhang et al. (2018) proposed first-order algorithms to solve low-rank matrix estimation problems. Similar to Jain et al. (2013); Hardt & Wootters (2014); Hardt (2014), these algorithms first use spectral initialization to find a good starting point, but instead of performing alternating minimization, they use (projected) gradient descent to refine the initial solution, and are guaranteed to converge to the global optimum at a linear rate. Notably, the sample complexity of these algorithms does not depend on the target accuracy and is only slightly larger than that of convex programming approaches. Our proposed algorithm also belongs to this line of research but with significant innovations in both algorithm and theory (c.f. Section 1.1)."}, {"heading": "2.2. Matrix Completion with Side Information", "text": "Matrix completion with side information has drawn much attention for improving the performance of traditional matrix completion methods in various applications. This method dates back to Jain & Dhillon (2013); Xu et al. (2013), where they proposed the so-called Inductive Matrix Completion methods independently. The method is \u201cinductive\u201d, in that it can be generalized to previously unobserved data points, which resolves a major drawback in traditional recommender systems. Extensions to noisy features (Chiang et al., 2015) and non-linear models (Si et al., 2016) have been studied and similar formulation has also been extended to the problem of robust PCA (Chiang et al., 2016; Niranjan et al., 2017; Xue et al., 2017).\nTheoretically, side information allows us to recover the target matrix with sample complexity depending on the intrinsic feature dimension rather than the ambient dimension. Information theoretically speaking, with known features, O(rn) samples are sufficient for exact recovery and this is achieved up to some logarithmic factors by the convex relaxation based algorithm proposed in Xu et al. (2013). However, such formulation requires solving a nuclear norm minimization problem and in general cannot have the linear convergence. Jain & Dhillon (2013) adopted ideas from Jain et al. (2013); Hardt (2014); Hardt & Wootters (2014) to obtain a linear convergent algorithm but it requires O ( r3n2 log n log(1/ ) ) samples. See Table 1 for a detailed comparison between our method and two existing inductive matrix completion algorithms: Maxide (Xu et al., 2013) and AltMin2 (Jain & Dhillon, 2013). It is worth noting that\n2Jain & Dhillon (2013) requires a weaker incoherence condition in that they only assume the features are incoherent. However,\nour approach achieves both linear rate of convergence and sample complexity linear in the feature dimension n."}, {"heading": "3. Problem Setup and Preliminaries", "text": "Recall that our goal is to recover the unknown rank-r matrix L\u2217 \u2208 Rd1\u00d7d2 by learning a lower-dimensional matrix M\u2217 \u2208 Rn1\u00d7n2 given the side information in terms of XL and XR. Denote the rank-r singular value decomposition (SVD) of M\u2217 by M\u2217 = U \u2217 \u03a3\u2217V \u2217> . Let \u03c3\u22171 \u2265 \u03c3\u22172 \u2265 . . . \u2265 \u03c3\u2217r > 0 be the sorted singular values of M\u2217 and \u03ba = \u03c3\u22171/\u03c3 \u2217 r be the condition number. Assume each entry of L\u2217 is observed independently with probability p \u2208 (0, 1). In particular, for any (i, j) \u2208 [d1] \u00d7 [d2], we consider the following Bernoulli observation model\nLij = { L\u2217ij , with probability p; \u2217, otherwise. (3.1)\nLet \u2126 be the index set of observed entries in L\u2217, i.e., \u2126 ={ (i, j) \u2208 [d1]\u00d7 [d2] \u2223\u2223Lij 6= \u2217}. Note that restricting on the observed index set \u2126, we have P\u2126(L) = P\u2126(L\u2217).\nIn order to fully exploit the side information, following Xu et al. (2013); Yi et al. (2013); Chiang et al. (2016), we assume the following standard feasibility condition: col(XL) \u2287 col(L\u2217), col(XR) \u2287 col(L\u2217>), where col(A) represents the column space of matrix A. Intuitively, this condition suggests that the feature matrices are correlated to the underlying true low-rank space, so that we could make use of the feature information to improve our recovery. In other words, we assume L\u2217 can be decomposed as L\u2217 = XLM\u2217X>R. In addition, without loss of generality, we assume both feature matrices XL and XR have orthonormal columns3, i.e., X>LXL = In1 , X > RXR = In2 .\nIt is well-known in matrix completion (Gross, 2011) that if L\u2217 is equal to zero in nearly all of the rows or columns, recovering L\u2217 exactly is impossible unless all of its entries\nwhen additional incoherence condition is imposed, it is unclear whether their algorithm can reduce the sample complexity or not.\n3In practice, one could conduct QR factorization or SVD to acquire the corresponding orthonormal feature matrices.\nare sampled. Therefore, we impose the standard incoherence condition on the unknown low-rank matrix L\u2217 (Cande\u0300s & Recht, 2009; Recht, 2011; Yi et al., 2016). Note that given feature matrices XL,XR, the singular value decomposition of L\u2217 can be formulated as (XLU \u2217 )\u03a3\u2217(XRV)\n>. Assumption 3.1 (Incoherence for L\u2217). The unknown lowrank matrix L\u2217 is \u00b50-incoherent, i.e., \u2016XLU\n\u2217\u20162,\u221e \u2264\u221a \u00b50r/d1, \u2016XRV \u2217\u20162,\u221e \u2264 \u221a \u00b50r/d2.\nFurthermore, following Jain & Dhillon (2013); Xu et al. (2013); Chiang et al. (2016), we impose the following incoherence condition on the feature matrices. Assumption 3.2 (Incoherence for feature matrices). The feature matrices XL and XR are both self-incoherent with parameter \u00b51, i.e, \u2016XL\u20162,\u221e \u2264 \u221a \u00b51n1/d1, \u2016XR\u20162,\u221e \u2264\u221a\n\u00b51n2/d2.\nWith the aid of additional feature information, inductive matrix completion can be formulated as follows\nmin M\u2208Rn1\u00d7n2\n1\n2p \u2225\u2225P\u2126(XLMX>R \u2212 L)\u2225\u22252F , subject to rank(M) \u2264 r,\n(3.2)\nwhere \u2126 is the index set of observed entries and p = |\u2126|/(d1d2) denotes the sampling probability in the observation model. In order to estimate the low-rank matrix M\u2217 more efficiently, following Tu et al. (2015), Zheng & Lafferty (2015) and Yi et al. (2016), we propose to solve the following factorized non-convex optimization problem\nmin U\u2208Rn1\u00d7r V\u2208Rn2\u00d7r\n1\n2p \u2225\u2225P\u2126(XLUV>X>R \u2212 L)\u2225\u22252F . (3.3) Due to the reparameterization M = UV>, the rank constraint in (3.2) is automatically guaranteed in (3.3)."}, {"heading": "4. The Proposed Algorithm", "text": "Let U\u2217 = U \u2217 \u03a3\u22171/2 and V\u2217 = V \u2217 \u03a3\u22171/2 be the true factorized matrices. It is obvious that (U\u2217,V\u2217) is the optimal solution to optimization problem (3.3). However, for any invertible matrix P \u2208 Rr\u00d7r, (U\u2217P,V\u2217(P\u22121)>) is also an optimal solution. In order to deal with this identifiability issue, following Tu et al. (2015); Zheng & Lafferty (2016); Park et al. (2016), we impose an additional regularizer to the objective function in (3.3) to penalize the scale difference between U and V. Specifically, we consider the following regularized optimization problem\nmin U\u2208Rn1\u00d7r V\u2208Rn2\u00d7r\nf\u2126(U,V) := 1\n2p \u2225\u2225P\u2126(XLUV>X>R \u2212 L)\u2225\u22252F + 1\n8\n\u2225\u2225U>U\u2212V>V\u2225\u22252 F ,\n(4.1)\nwhere U \u2208 Rn1\u00d7r, V \u2208 Rn2\u00d7r, and f\u2126 denotes the regularized sample loss function. Intuitively speaking, the regularization term encourages the two factorized matrices U and V to have a similar scale.\nWe propose a multi-phase gradient-based algorithm to solve the proposed estimator (4.1), as shown in Algorithm 1. More specifically, we first randomly split the observed index set \u2126 into S + 1 independent subsets {\u2126s}Ss=0, where \u21260 has cardinality |\u2126|/2 and each of the rest has cardinality |\u2126|/(2S). In Phase 1, we project the observed matrix L onto the first subset \u21260 and perform rank-r SVD on p\u221210 P\u21260(L\u2217) to get an initial estimator (Uinit,Vinit), where p0 = |\u21260|/(d1d2). We use SVDr(\u00b7) to denote the rank-r SVD.\nIn Phase 2, we perform projected gradient descent with resampling (Jain et al., 2013; Jain & Dhillon, 2013) (a.k.a., sample splitting), where we use one fresh subset for each gradient descent update. The projection step guarantees that each intermediate iterate satisfies the similar incoherence condition as that of (U\u2217,V\u2217), while the resampling scheme ensures the independence of the samples used in the current iteration and the previous iterates. As will be clear in the next section and in the proofs, the second phase is crucial in reducing the variance of gradient estimate and ensures the uniform convergence in the third phase. The constraint sets C1 and C2 associated with the projection are defined as\nC1 = { U \u2208 Rn1\u00d7r \u2223\u2223\u2223 \u2016XLU\u20162,\u221e \u2264\u221a\u00b50r\nd1 \u2016Zinit\u20162\n} ,\nC2 = { V \u2208 Rn2\u00d7r \u2223\u2223\u2223 \u2016XRV\u20162,\u221e \u2264\u221a\u00b50r\nd2 \u2016Zinit\u20162\n} ,\n(4.2) where Zinit is specified in Phase 1. Let PC1(U\u0302) be the projection of U\u0302 \u2208 Rn1\u00d7r onto C1, which can be alternatively regarded as the exact solution to the following convex quadratically-constrained-quadratic-programming (QCQP)\nargmin U\u2208Rn1\u00d7r\n1 2 \u2016U\u2212 U\u0302\u20162F ,\nsubject to \u2225\u2225[XLU]i,\u2217\u2225\u222522 \u2264 \u00b50rd1 \u2016Zinit\u201622,\u2200 i \u2208 [d1]. (4.3)\nIt is worth noting that convex QCQP problem can be solved approximately and efficiently using interior point methods (Nemirovskii, 2004). Let PC1(U\u0302, \u03b4) be the \u03b4-approximate solution to optimization problem (4.3), i.e., \u2016PC1(U\u0302, \u03b4)\u2212 PC1(U\u0302)\u2016F \u2264 \u03b4. Similarly, the QCQP problem with respect to V is formulated in a similar way, except that XL (resp. d1) is replaced with XR (resp. d2). Accordingly, we use PC2(V\u0302) to denote the exact projection, and PC2(V\u0302, \u03b4) to be the \u03b4-approximate projection.\nIn addition, the loss function used in the s-th iteration of Phase 2 is based on the subset \u2126s, and it is identical to the loss function in (4.1), except that \u2126 (resp. p) is replaced with \u2126s (resp. ps = |\u2126s|/(d1d2)).\nFinally, in Phase 3, vanilla gradient descent is performed based on the entire observed matrixP\u2126(L\u2217). Provided these three phases, as will be seen in later analysis, Algorithm 1 is guaranteed to converge to the true factorized matrices (U\u2217,V\u2217) with a linear rate of convergence.\nAlgorithm 1 GD for IMC Input: Observed matrix P\u2126(L\u2217); feature matrices XL,\nXR; parameter p0 = |\u2126|/(2d1d2); step size \u03c4, \u03b7; number of iterations S, T , approximation error \u03b4.\nRandomly split \u2126 into subsets \u21260,\u21261, . . . ,\u2126S with |\u21260| = |\u2126|/2 and |\u2126s| = |\u2126|/(2S), for any s \u2208 [S]\n// Phase 1: Initialization [U\u03030,\u03a30, V\u03030] = SVDr ( p\u221210 P\u21260(L\u2217) ) Uinit = X > LU\u03030\u03a3 1/2 0 ; Vinit = X > RV\u03030\u03a3 1/2 0\nZinit = [Uinit; Vinit] // Phase 2: PGD with subsamples\nU0 = PC1(Uinit, \u03b4),V0 = PC2(Vinit, \u03b4) for: s = 1, 2, . . . , S do\nUs = PC1 ( Us\u22121 \u2212 \u03b7\u2207Uf\u2126s(Us\u22121,Vs\u22121), \u03b4 ) Vs = PC2 ( Vs\u22121 \u2212 \u03b7\u2207Vf\u2126s(Us\u22121,Vs\u22121), \u03b4\n) end for\n// Phase 3: Vanilla GD U0 = US , V0 = VS for: t = 0, 1, . . . , T \u2212 1 do\nUt+1 = Ut \u2212 \u03c4\u2207Uf\u2126(Ut,Vt) Vt+1 = Vt \u2212 \u03c4\u2207Vf\u2126(Ut,Vt)\nend for Output: (UT ,VT )"}, {"heading": "5. Main Theory", "text": "Before presenting the main theoretical results, we note that the optimal solution to optimization problem (4.1) is not unique. Therefore, following Tu et al. (2015), we introduce the so-called Procrustes distance. For simplicity, we let Z\u2217 = [U\u2217; V\u2217] be the stacked true parameter matrix. Definition 5.1. For any Z \u2208 R(n1+n2)\u00d7r, let D(Z,Z\u2217) be the minimal distance between Z and Z\u2217 in terms of the optimal rotation, or more precisely, D(Z,Z\u2217) = minR\u2208Qr \u2016Z\u2212 Z\u2217R\u2016F , where Qr denotes the set of r-byr othorgonal matrices.\nIn the following discussions, we use d and n to denote max{d1, d2} and max{n1, n2}, respectively. Our main theoretical result on Algorithm 1 is presented as follows. Theorem 5.2. Assume the observed index set \u2126 follows Bernoulli model (3.1) and incoherence Assumptions 3.1, 3.2 hold. There exist constants c1, c2, c3, c4, c5 such that under condition |\u2126| \u2265 c1 max{\u00b51n, \u00b50r\u03ba}\u00b50r2\u03ba2 log n log d, if step size \u03b7 = c2/(r\u03c3\u22171), \u03c4 = c3/\u03c3 \u2217 1 and approximation\nerror \u03b4 = O ( 1/(r\u03ban2) ) , after S = O(r\u03ba log n) iterations in Phase 2 and T = O (\u03ba log(1/ )) iterations in Phase 3,\nwith probability at least 1 \u2212 c4r\u03ba log n/d, the output of Algorithm 1 satisfies\n\u2016MT \u2212M\u2217\u2016F \u2264 c5 \u221a \u03c3\u22171 ,\nwhere MT = UTVT> and M\u2217 = U\u2217V\u2217>.\nTheorem 5.2 shows that the overall sample complexity of Algorithm 1 is O ( r2\u03ba2n log n log d ) . Here, we explicitly write down the dependency on condition number \u03ba in the O(\u00b7) notation for completeness. It is worth noting that our gradient-based Algorithm 1 achieves both linear rate of convergence and sample complexity linearly depending on n, compared with convex relaxation based approach (Xu et al., 2013) whose convergence rate is sublinear (i.e., O(1/ \u221a )) and alternation minimization (Jain & Dhillon, 2013), which requires at least O ( r3n2 log n log(1/ ) ) samples.\nTheorem 5.2 can be achieved by analyzing the three phases of Algorithm 1. In the sequel, we are going to provide the theoretical guarantees of each phase.\nTheorem 5.3 (Initialization). Assume the observed index set \u2126 follows Bernoulli model (3.1). Suppose Assumptions 3.1, 3.2 hold for the unknown low-rank matrix L\u2217 and the feature matrices XL,XR, respectively. For any \u03b3 \u2208 (0, 1), there exist constants c1, c2 such that under the condition |\u21260| \u2265 c1\u00b50\u00b51r2\u03ba2n log d/\u03b32, with probability at least 1\u2212 c2/d, the output of Phase 1 in Algorithm 1 satisfies\nD(Zinit,Z \u2217) \u2264 4\u03b3 \u221a \u03c3\u2217r .\nTheorem 5.3 suggests that the output of Initialization Phase 1 is already in a small neighbourhood of the optimum with radius O( \u221a \u03c3\u2217r ). Notably, the sample complexity is linear in n, in sharp contrast to that of the classical matrix completion setting which is at least linear in d.\nTheorem 5.4 (PGD with subsamples). Under the same conditions as in Theorem 5.3, suppose the output of Phase 1, Zinit, satisfies D(Zinit,Z\u2217) \u2264 \u03b1 \u221a \u03c3\u2217r/2 with constant \u03b1 \u2264 1/40. There exist constants c1, c2, c3, c4, c5 such that, if the total sample size |\u2126| \u2265 c1S \u00b7max{\u00b50\u00b51r\u03ban, \u00b520r2\u03ba2} log d, with step size \u03b7 = c2/(r\u03c3\u22171) and approximation error \u03b4 \u2264 c3 \u221a \u03c3\u2217r/(r\u03ba), the final iterate (US ,VS) in Phase 2 of Algorithm 1 satisfies\nD2(ZS ,Z \u2217) \u2264 ( 1\u2212 c2\n16r\u03ba\n)S \u03b12\u03c3\u2217r + c4\u03b4r\u03ba \u221a \u03c3\u2217r (5.1)\nwith probability at least 1\u2212 c5S/d, where ZS = [US ; VS ].\nThe last term on the right hand side of (5.1) originates from the approximation error \u03b4 when solving the convex QCQP (4.3) with respect to U (or V). Theorem 5.4 suggests that under proper initialization, the gradient iteration in Phase 2 converges at a linear rate with contraction parameter 1\u2212\nO(1/(r\u03ba)). Note that the step size is chosen asO(1/(r\u03c3\u22171)). In practice, since \u03c3\u22171 is unknown, we can approximate \u03c3 \u2217 1 by C \u00b7 \u2016UinitV>init\u20162 and tune the coefficient C.\nTheorem 5.5 (Vanilla GD). Under the same conditions as in Theorem 5.3, suppose the final iterate (US ,VS) of Phase 2 in Algorithm 1 satisfiesD(ZS ,Z\u2217) \u2264 c0 \u221a \u03c3\u2217r/(\u00b51n) with ZS = [US ; VS ] and constant c0 small enough. Then there exist constants c1, c2, c3 such that if |\u2126| \u2265 c1\u00b50\u00b51rn log d, with step size \u03c4 = c2/\u03c3\u22171 , the output of Phase 3 satisfies\nD2(ZT ,Z\u2217) \u2264 ( 1\u2212 \u03c4\u03c3 \u2217 r\n16\n)T D2(ZS ,Z \u2217)\nwith probability at least 1\u2212 c3/d, where ZT = [UT ; VT ].\nTheorem 5.5 implies that if the final iterate of Phase 2 falls into a even smaller neighbourhood around the optimum with radius O(1/n), vanilla gradient descent suffices to guarantee the linear rate of convergence.\nRemark 5.6 (Computational Complexity). The rank-r SVD in Phase 1 requires O(r|\u21260|) computation. The runtime of the gradient computation for the s-th iteration in the second phase is O(rn|\u2126s| + r2n), while solving the convex QCQP subproblem requiresO(r2n2d3/2 log d) computation if using the path-following interior point method (Nemirovskii, 2004). Thus to perform S = O(r\u03ba log n) iterations, the overall computational complexity of Phase 2 is O(r3n2d3/2 log n log d). The runtime of gradient computation in each iteration of Phase 3 is O(rn|\u2126|) and the total number of iterations required in Phase 3 is T = O(\u03ba log(1/ )), which implies the overall computational cost in Phase 3 is O(r3n2 log n log d log(1/ )). Putting all these pieces together, we conclude the total computational complexity of Algorithm 1 is O(r3n2 log n log d log(1/ ) + r3n2d3/2 log n log d)."}, {"heading": "6. Experiments", "text": "In this section, we compare the proposed gradient-based algorithm with existing inductive matrix completion methods, including the convex relaxation based approach, Maxide (Xu et al., 2013) and alternating minimization based algorithm, AltMin (Jain & Dhillon, 2013) on both synthetic and real datasets. In addition, the standard matrix completion approach based on non-convex projected gradient descent (Zheng & Lafferty, 2016) (MC) is compared as a baseline for simulations and the second real data experiment on genedisease prediction, while the Binary Relevance approach (Boutell et al., 2004) using linear kernel SVM (Chang & Lin, 2011) (BR-linear) is included as a baseline for the first real data experiment on multi-label learning. All algorithms are implemented in Matlab on a machine with Intel 8-core Core i7 3.40 GHz with 8GB RAM."}, {"heading": "6.1. Simulations", "text": "For simplicity, we choose d1 = d2 = d and n1 = n2 = n. Additional experiments regarding the rectangular setting are postponed to the supplemental materials. The unknown low-rank matrix M\u2217 \u2208 Rn\u00d7n is generated such that M\u2217 = U\u2217V\u2217>, and the entries of U\u2217,V\u2217 \u2208 Rn\u00d7r are drawn independently from centered Gaussian distribution with variance 1/n. Let the singular value decomposition of a random matrix F \u2208 Rd\u00d7d be F = YL\u03a3Y>R , where each entry of F is drawn independently from standard normal distribution. The feature matrices XL,XR \u2208 Rd\u00d7n are then generated as the first n columns of the singular matrices YL and YR respectively. The observed data matrix L follows from the Bernoulli model (3.1) with the full data matrix defined by L\u2217 = XLM\u2217X>R.\nTo begin with, we investigate the sample complexity of the proposed gradient-based method. In particular, we consider the following settings: (i) d = 500, n = 50, r = 10; (ii) d = 500, n = 100, r = 5; (iii) d = 1000, n = 50, r = 5; (iv) d = 1000, n = 100, r = 10. We compute the empirical probability of successful recovery after 50 repeated trials, where we regard the trial as successful if the relative error \u2016XLMTX>R \u2212 L\u2217\u2016F /\u2016L\u2217\u2016F is less than 10\u22126. The experimental results are shown in Figure 1(a). Here, m represents the total number of observed entries. Under all of the aforementioned settings, the phase transition happens to be around m/(nr) = 6, which implies that the optimal sample complexity for gradient-based inductive matrix completion approach may be linear in both n and r.\nMoreover, we compare our algorithm with the aforementioned algorithms, including MC, Maxide and AltMin. All the parameters, such as step size and regularization parameters, are tuned by 5-fold cross validation. We measure the performance by the relative reconstruction error \u2016L\u0302 \u2212 L\u2217\u2016F /\u2016L\u2217\u2016F under the setting that d = 1000, n = 100, r = 10 with sampling rate p varied in the range {2%, 5%, 10%}. For the sake of fairness, we use the same initialization procedure as in Algorithm 1 for all the compared algorithms. The results are demonstrated in Figures 1(b), 1(c) and 1(d). Here, each effective data pass evaluates |\u2126| observed entries. It can be seen that inductive methods can recover the unknown low-rank matrix L\u2217 successfully using less observed entries compared with the standard matrix completion approach, which proves the effectiveness of feature information. In addition, our approach achieves the lowest recovery error with respect to the same number of effective data passes, and outperforms existing inductive matrix completion algorithms by a large margin. In addition, we also plot the relative error with respect to CPU time, and similar trend in results can be observed. Due to space limit, these plots are deferred to the supplementary materials. All these comparison results clearly demonstrate the superiority\nof our proposed algorithm in terms of computation and is well aligned with our theory."}, {"heading": "6.2. Multi-Label Learning", "text": "We also apply our proposed algorithm to multi-label learning on the image classification dataset NUS-WIDEOBJECT obtained from Chua et al. (2009), which is one of the prominent applications of inductive matrix completion. Additional experiments on Yahoo datasets (Ueda & Saito, 2003) are deferred to the supplementary materials. The NUS-WIDE-OBJECT dataset consists of d1 = 30000 images classified by d2 = 31 object categories, along with 5 types of low-level features extracted from these images. We construct the feature matrix by further extracting the top-50 principle components from each type of side information, which leads to n1 = 250 features in total. Detailed information regarding the dataset can be found in Chua et al. (2009). Our goal is to predict the labels associated with the unseen instances, based on both the side information as well as the label assignments of the observed instances. By leveraging the low-rankness property of the unknown instance-label matrix (Ji et al., 2008; Goldberg et al., 2010), multi-label learning can be reformulated as an inductive matrix completion problem (3.2), where L\u2217 is the instancelabel matrix, XL represents the feature matrix and XR is set as an identity matrix in this context.\nWe randomly sample p\u00d7 100% instances as the observed (training) data for each dataset, and treat the remaining (1\u2212 p)\u00d7 100% instances as the unobserved (testing) data,\nwith p chosen from {10%, 25%, 50%}. We estimate the unknown matrix of parameters based on the training data, and report the average precision (AP) (Zhang & Zhou, 2014) computed from the testing data. Specifically, the average precision measures the averaged fraction of relevant labels ranked higher than a specific label. We compare our algorithm with the baseline approach, BR-linear, and existing inductive matrix completion algorithms, Maxide and AltMin. All the parameters, including the rank r (we tune it over the grid {5, 10, . . . , 30}), are tuned via 5-fold cross validation based on the training data. Table 2 depicts the detailed experimental results. In detail, for each setting of observed training data, we report the averaged AP over 10 trials and the corresponding standard deviation as well as the total run time. We can observe from Table 2 that the proposed gradient-based algorithm outperforms the BR-linear by a large margin. Compared with existing inductive matrix completion algorithms, our algorithm also achieves significantly better results under all of the experimental settings in terms of both prediction accuracy and running time. This again illustrates the advantage of our algorithm."}, {"heading": "6.3. Gene-Disease Prediction", "text": "We further apply our proposed method for predicting genedisease associations on the OMIM4 data used in Singh-Blom et al. (2013), which is another successful application of inductive matrix completion. In the context of gene-disease\n4OMIM is short for Online Mendelian Inheritance in Man, which is a public database for human gene-disease studies.\nassociation prediction, we let L\u2217 \u2208 Rd1\u00d7d2 be the genedisease association matrix, such that L\u2217ij = 1 if gene i is associated with disease j; L\u2217ij = 0 if the association is unobserved. On this dataset, the association matrix L\u2217 is highly sparse, consisting of d1 = 12331 different genes and d2 = 3209 different diseases with only 3954 discovered gene-disease associations. In addition, we obtain the gene feature matrix XL \u2208 Rd1\u00d7n1 and disease feature matrix XR \u2208 Rd2\u00d7n2 from Natarajan & Dhillon (2014), where n1 = 300 gene features and n2 = 200 disease features are extracted respectively. Our objective is to predict potential genes for certain diseases of interest based on both the observed associations and feature information, which can thus be formulated as an inductive matrix completion problem. Following Natarajan & Dhillon (2014), we include an additional regularization term in (4.1) to take into account the sparsity of the underlying association matrix\nmin U\u2208Rn1\u00d7r V\u2208Rn2\u00d7r\nf\u2126(U,V) + \u03bb \u2016P\u2126c(XLUV>X>R)\u20162F , (6.1)\nwhere r is the supposed rank of L\u2217, \u2126 stands for the (training) index set of gene-disease associations, and \u2126c represents its complement. Note that all the algorithms we studied here including ours can be directly applied to solve (6.1) with slight modification. In our experiment, we tune the regularization parameter \u03bb via cross validation and choose the best value \u03bb = 0.5.\nTo evaluate the performance of our method, we equally split the known gene-disease associations into three groups and perform 3-fold cross validation. Specifically, we treat each group as testing data once and apply our gradient-based method on the remaining two groups to obtain the estimation matrix of L\u2217. For every gene-disease pair (g, d) in the testing group, we order all the genes by the corresponding estimated values associated with disease d, and then record the ranking of gene g in the list. We use the cumulative distribution of the rankings (Singh-Blom et al., 2013; Natarajan\n& Dhillon, 2014) as the performance measure for evaluation, i.e., the probability that the ranking is less than a specific threshold k \u2208 {1, 2, . . . , 100}. The experimental results with rank r varied in the range {10, 30, 50, 100, 200} based on our method are displayed in Figure 2(a), which indicates that the rank plays an important role in gene-disease prediction: higher rank leads to better performance. In later experiments, we choose r = 200 because the performance of inductive matrix completion on this dataset tends to be saturated when r = 200.\nMoreover, we compare our algorithm with the following algorithms: MC, Maxide and AltMin, which are discussed at the beginning of Section 6. The comparison results in terms of the cumulative distribution of the rankings are illustrated in Figure 2(b). It can be seen that our proposed algorithm uniformly outperforms other methods over all threshold values k. In addition, we present the precision-recall curves for all the methods we compared in Figure 2(c). Here the precision is defined as the ratio of true recovered gene-disease associations to the total number of associations we assessed; and the recall is the fraction of the true gene-disease associations that are recovered. Again, the proposed method dominates other relevant approaches, which suggests that our method can better serve for biologists to discover new gene-disease associations."}, {"heading": "7. Conclusions and Future Work", "text": "In this paper we proposed the first gradient-based nonconvex optimization algorithm for inductive matrix completion with sample complexity linear in the number of features and converges to the unknown low-rank matrix at a linear rate. One possible future direction is to extend our algorithm to the case with noisy side information (Chiang et al., 2015) or the agnostic setting, i.e., the underlying matrix has high rank (Du et al., 2017b). Another direction is to generalize our approach to non-linear models (Si et al., 2016)."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers and Yining Wang for their helpful comments. XZ and QG are partially supported by the National Science Foundation IIS-1618948, IIS-1652539, IIS-1717206 and BIGDATA IIS-1741342, SD is partially supported by NSF grant IIS-1563887, AFRL grant FA8750-17-2-0212 and DARPA D17AP00001."}], "year": 2018, "references": [{"title": "A new approach to collaborative filtering: Operator estimation with spectral regularization", "authors": ["J. Abernethy", "F. Bach", "T. Evgeniou", "Vert", "J.-P"], "venue": "Journal of Machine Learning Research,", "year": 2009}, {"title": "Linear convergence of a frank-wolfe type algorithm over trace-norm balls", "authors": ["Z. Allen-Zhu", "E. Hazan", "W. Hu", "Y. Li"], "venue": "arXiv preprint arXiv:1708.02105,", "year": 2017}, {"title": "An improved gapdependency analysis of the noisy power method", "authors": ["Balcan", "M.-F", "S.S. Du", "Y. Wang", "A.W. Yu"], "venue": "In Conference on Learning Theory,", "year": 2016}, {"title": "Optimal sample complexity for matrix completion and related problems via l2-regularization", "authors": ["Balcan", "M.-F", "Y. Liang", "D.P. Woodruff", "H. Zhang"], "venue": "arXiv preprint arXiv:1704.08683,", "year": 2017}, {"title": "Global optimality of local search for low rank matrix recovery", "authors": ["S. Bhojanapalli", "B. Neyshabur", "N. Srebro"], "venue": "arXiv preprint arXiv:1605.07221,", "year": 2016}, {"title": "Learning multi-label scene classification", "authors": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern recognition,", "year": 2004}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "authors": ["S. Burer", "R.D. Monteiro"], "venue": "Mathematical Programming,", "year": 2003}, {"title": "Matrix completion for multi-label image classification", "authors": ["R.S. Cabral", "F. Torre", "J.P. Costeira", "A. Bernardino"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2011}, {"title": "Exact matrix completion via convex optimization", "authors": ["E.J. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics,", "year": 2009}, {"title": "The power of convex relaxation: Nearoptimal matrix completion", "authors": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Transactions on Information Theory,", "year": 2010}, {"title": "Libsvm: a library for support vector machines", "authors": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM transactions on intelligent systems and technology (TIST),", "year": 2011}, {"title": "Svdfeature: a toolkit for feature-based collaborative filtering", "authors": ["T. Chen", "W. Zhang", "Q. Lu", "K. Chen", "Z. Zheng", "Y. Yu"], "venue": "Journal of Machine Learning Research,", "year": 2012}, {"title": "Incoherence-optimal matrix completion", "authors": ["Y. Chen"], "venue": "IEEE Transactions on Information Theory,", "year": 2015}, {"title": "Matrix completion with noisy side information", "authors": ["Chiang", "K.-Y", "Hsieh", "C.-J", "I.S. Dhillon"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Robust principal component analysis with side information", "authors": ["Chiang", "EDU K.-Y", "Hsieh", "C.-J", "E.I.S. Dhillon"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Nus-wide: a real-world web image database from national university of singapore", "authors": ["Chua", "T.-S", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y. Zheng"], "venue": "In Proceedings of the ACM international conference on image and video retrieval,", "year": 2009}, {"title": "Gradient descent can take exponential time to escape saddle points", "authors": ["S.S. Du", "C. Jin", "J.D. Lee", "M.I. Jordan", "B. Poczos", "A. Singh"], "venue": "arXiv preprint arXiv:1705.10412,", "year": 2017}, {"title": "On the power of truncated svd for general high-rank matrix estimation problems", "authors": ["S.S. Du", "Y. Wang", "A. Singh"], "venue": "arXiv preprint arXiv:1702.06861,", "year": 2017}, {"title": "Matrix completion from o(n) samples in linear time", "authors": ["D. Gamarnik", "Q. Li", "H. Zhang"], "venue": "arXiv preprint arXiv:1702.02267,", "year": 2017}, {"title": "Escaping from saddle pointsonline stochastic gradient for tensor decomposition", "authors": ["R. Ge", "F. Huang", "C. Jin", "Y. Yuan"], "venue": "In Conference on Learning Theory, pp", "year": 2015}, {"title": "Matrix completion has no spurious local minimum", "authors": ["R. Ge", "J.D. Lee", "T. Ma"], "venue": "arXiv preprint arXiv:1605.07272,", "year": 2016}, {"title": "No spurious local minima in nonconvex low rank problems: A unified geometric analysis", "authors": ["R. Ge", "C. Jin", "Y. Zheng"], "venue": "arXiv preprint arXiv:1704.00708,", "year": 2017}, {"title": "Transduction with matrix completion: Three birds with one stone", "authors": ["A. Goldberg", "B. Recht", "J. Xu", "R. Nowak", "X. Zhu"], "venue": "In Advances in neural information processing systems,", "year": 2010}, {"title": "Recovering low-rank matrices from few coefficients in any basis", "authors": ["D. Gross"], "venue": "IEEE Transactions on Information Theory,", "year": 2011}, {"title": "Low-rank and sparse structure pursuit via alternating minimization", "authors": ["Q. Gu", "Z.W. Wang", "H. Liu"], "venue": "In Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Understanding alternating minimization for matrix completion", "authors": ["M. Hardt"], "venue": "In Foundations of Computer Science (FOCS),", "year": 2014}, {"title": "The noisy power method: A meta algorithm with applications", "authors": ["M. Hardt", "E. Price"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Fast matrix completion without the condition number", "authors": ["M. Hardt", "M. Wootters"], "venue": "In Conference on Learning Theory,", "year": 2014}, {"title": "Low rank modeling of signed networks", "authors": ["Hsieh", "C.-J", "Chiang", "K.-Y", "I.S. Dhillon"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2012}, {"title": "Provable inductive matrix completion", "authors": ["P. Jain", "I.S. Dhillon"], "venue": "arXiv preprint arXiv:1306.0626,", "year": 2013}, {"title": "Fast exact matrix completion with finite samples", "authors": ["P. Jain", "P. Netrapalli"], "venue": "In COLT, pp", "year": 2015}, {"title": "Low-rank matrix completion using alternating minimization", "authors": ["P. Jain", "P. Netrapalli", "S. Sanghavi"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "year": 2013}, {"title": "Extracting shared subspace for multi-label classification", "authors": ["S. Ji", "L. Tang", "S. Yu", "J. Ye"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2008}, {"title": "How to escape saddle points efficiently", "authors": ["C. Jin", "R. Ge", "P. Netrapalli", "S.M. Kakade", "M.I. Jordan"], "venue": "arXiv preprint arXiv:1703.00887,", "year": 2017}, {"title": "Matrix factorization techniques for recommender systems", "authors": ["Y. Koren", "R. Bell", "C. Volinsky"], "year": 2009}, {"title": "Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion and blind deconvolution", "authors": ["C. Ma", "K. Wang", "Y. Chi", "Y. Chen"], "venue": "arXiv preprint arXiv:1711.10467,", "year": 2017}, {"title": "Response prediction using collaborative filtering with hierarchies and side-information", "authors": ["A.K. Menon", "Chitrapura", "K.-P", "S. Garg", "D. Agarwal", "N. Kota"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2011}, {"title": "Inductive matrix completion for predicting gene\u2013disease associations", "authors": ["N. Natarajan", "I.S. Dhillon"], "venue": "i60\u2013i68,", "year": 2014}, {"title": "Interior point polynomial time methods in convex programming", "authors": ["A. Nemirovskii"], "venue": "Lecture Notes,", "year": 2004}, {"title": "Provable inductive robust pca via iterative hard thresholding", "authors": ["U. Niranjan", "A. Rajkumar", "T. Tulabandhula"], "venue": "arXiv preprint arXiv:1704.00367,", "year": 2017}, {"title": "Finding low-rank solutions to matrix problems, efficiently and provably", "authors": ["D. Park", "A. Kyrillidis", "C. Caramanis", "S. Sanghavi"], "venue": "arXiv preprint arXiv:1606.03168,", "year": 2016}, {"title": "A simpler approach to matrix completion", "authors": ["B. Recht"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "Tumblr blog recommendation with boosted inductive matrix completion", "authors": ["D. Shin", "S. Cetintas", "Lee", "K.-C", "I.S. Dhillon"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "year": 2015}, {"title": "Goaldirected inductive matrix completion", "authors": ["S. Si", "Chiang", "K.-Y", "Hsieh", "C.-J", "N. Rao", "I.S. Dhillon"], "venue": "In KDD,", "year": 2016}, {"title": "Prediction and validation of gene-disease associations using methods inspired by social network analyses", "authors": ["U.M. Singh-Blom", "N. Natarajan", "A. Tewari", "J.O. Woods", "I.S. Dhillon", "E.M. Marcotte"], "venue": "PloS one,", "year": 2013}, {"title": "Guaranteed matrix completion via nonconvex factorization", "authors": ["R. Sun", "Luo", "Z.-Q"], "venue": "In Foundations of Computer Science (FOCS),", "year": 2015}, {"title": "User-friendly tail bounds for sums of random matrices", "authors": ["J.A. Tropp"], "venue": "Foundations of computational mathematics,", "year": 2012}, {"title": "Low-rank solutions of linear matrix equations via procrustes flow", "authors": ["S. Tu", "R. Boczar", "M. Simchowitz", "M. Soltanolkotabi", "B. Recht"], "venue": "arXiv preprint arXiv:1507.03566,", "year": 2015}, {"title": "Parametric mixture models for multilabeled text", "authors": ["N. Ueda", "K. Saito"], "venue": "In Advances in neural information processing systems,", "year": 2003}, {"title": "A unified computational and statistical framework for nonconvex low-rank matrix estimation", "authors": ["L. Wang", "X. Zhang", "Q. Gu"], "venue": "arXiv preprint arXiv:1610.05275,", "year": 2016}, {"title": "A universal variance reductionbased catalyst for nonconvex low-rank matrix recovery", "authors": ["L. Wang", "X. Zhang", "Q. Gu"], "venue": "arXiv preprint arXiv:1701.02301,", "year": 2017}, {"title": "Robust pca via outlier pursuit", "authors": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "Speedup matrix completion with side information: Application to multi-label learning", "authors": ["M. Xu", "R. Jin", "Zhou", "Z.-H"], "venue": "In Advances in NIPS,", "year": 2013}, {"title": "Speeding up latent variable gaussian graphical model estimation via nonconvex optimization", "authors": ["P. Xu", "J. Ma", "Q. Gu"], "venue": "In Advances in Neural Information Processing Systems,", "year": 1941}, {"title": "Informed non-convex robust principal component analysis with features", "authors": ["N. Xue", "J. Deng", "Y. Panagakis", "S. Zafeiriou"], "venue": "arXiv preprint arXiv:1709.04836,", "year": 2017}, {"title": "Semi-supervised clustering by input pattern assisted pairwise similarity matrix completion", "authors": ["J. Yi", "L. Zhang", "R. Jin", "Q. Qian", "A. Jain"], "venue": "In International Conference on Machine Learning,", "year": 2013}, {"title": "Fast algorithms for robust pca via gradient descent", "authors": ["X. Yi", "D. Park", "Y. Chen", "C. Caramanis"], "venue": "In Advances in neural information processing systems,", "year": 2016}, {"title": "A review on multi-label learning algorithms", "authors": ["Zhang", "M.-L", "Zhou", "Z.-H"], "venue": "IEEE transactions on knowledge and data engineering,", "year": 2014}, {"title": "A unified framework for nonconvex low-rank plus sparse matrix recovery", "authors": ["X. Zhang", "L. Wang", "Q. Gu"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "year": 2018}, {"title": "A nonconvex optimization framework for low rank matrix estimation", "authors": ["T. Zhao", "Z. Wang", "H. Liu"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements", "authors": ["Q. Zheng", "J. Lafferty"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Convergence analysis for rectangular matrix completion using burer-monteiro factorization and gradient descent", "authors": ["Q. Zheng", "J. Lafferty"], "venue": "arXiv preprint arXiv:1605.07051,", "year": 2016}, {"title": "Efficient matrix sensing using rank-1 gaussian measurements", "authors": ["K. Zhong", "P. Jain", "I.S. Dhillon"], "venue": "In International Conference on Algorithmic Learning Theory,", "year": 2015}], "id": "SP:3a476be99068fdc6f52059ddd5ef9b4af4454fd5", "authors": [{"name": "Xiao Zhang", "affiliations": []}, {"name": "Simon S. Du", "affiliations": []}, {"name": "Quanquan Gu", "affiliations": []}], "abstractText": "We revisit the inductive matrix completion problem that aims to recover a rank-r matrix with ambient dimension d given n features as the side prior information. The goal is to make use of the known n features to reduce sample and computational complexities. We present and analyze a new gradient-based non-convex optimization algorithm that converges to the true underlying matrix at a linear rate with sample complexity only linearly depending on n and logarithmically depending on d. To the best of our knowledge, all previous algorithms either have a quadratic dependency on the number of features in sample complexity or a sub-linear computational convergence rate. In addition, we provide experiments on both synthetic and real world data to demonstrate the effectiveness of our proposed algorithm.", "title": "Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase Procrustes Flow"}