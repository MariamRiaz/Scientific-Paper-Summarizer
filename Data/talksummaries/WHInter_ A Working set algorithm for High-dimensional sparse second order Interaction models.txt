0	1	In application domains where the number of features exceeds the number of available samples, sparsity-inducing regularisers have a long history of success.
1	29	Genomic prediction of complex phenotypes, biomedical imaging, astronomy or finance are a few examples.
2	11	In particular the least squares with `1 regularisation, known as the LASSO (Tibshirani, 1996), has been extensively studied.
3	64	It enjoys desirable statistical properties, since the number of samples required for exact support recovery of a sparse model scales as the logarithm of the number of features, under some assumptions (Wainwright, 2009).
4	41	It also enjoys practical advantages, notably the interpretability of the learned models and the availability of fast solvers.
5	75	Indeed, a lot of research effort has been devoted to accelerating solvers for sparsity constrained problems in high dimension.
6	95	A central idea is to exploit the sparsity of the solution to develop algorithms that do not spend too much time on optimising coefficients that will end up being 0.
7	55	For example, safe screening rules identify features which are guaranteed to be inactive at the optimum so that their corresponding coefficients can be safely zeroed and set aside from the pool of coefficients to update (El Ghaoui et al., 2012; Xiang et al., 2011; Xiang & Ramadge, 2012; Fercoq et al., 2015; Wang et al., 2013; Raj et al., 2016).
8	23	Dynamic screening rules (Bonnefoy et al., 2015) such as the GAP safe rules (Fercoq et al., 2015) are particularly useful since more and more coefficients can be safely zeroed while the solver approaches the optimal solution.
9	17	In spite of this, safe rules tend to be conservative, thereby limiting the potential speed-up.
10	58	To remedy this drawback, new working set heuristics have been proposed.
11	35	Working set algorithms enjoy great success in practice, as exemplified by the popular GLMNET package (Friedman et al., 2010).
12	19	They iteratively solve subproblems, either problems restricted to a subset of features in the primal or to a subset of constraints in the dual, until convergence.
13	13	Working set methods allow to focus coefficient updates on a set of features which can be significantly smaller than that yielded by safe rules.
14	29	However this comes at a cost, that of checking the optimality conditions for all features at each iteration.
15	23	BLITZ (Johnson & Guestrin, 2015) is a recently proposed working set algorithm that has been shown to have state-of-the-art performance for `1 regularised problems.
16	2	Interestingly, the choice of the working sets in BLITZ can be seen as an aggressive use of the GAP safe rules (as noted in Massias et al., 2017) where the size of the working set is chosen to maximise the progress towards convergence.
19	6	Further developments have also focused on coordinate descent (CD) to avoid wasteful coordinate updates, which represent most of the time spent by the solver (Fujiwara et al., 2016; Johnson & Guestrin, 2017).
26	15	Interestingly, glinternet uses an active set strategy.
31	3	Iterative refinements have been proposed where the LASSO is fit several times, and each time the set of candidate interactions considered is updated either by subsets, with the interactions between the K most relevant main effects selected at the previous fit (Bickel et al., 2010), or in a greedy fashion, where new interactions are included in the model as soon as a new main effect enters the LASSO path (Shah, 2016).
33	2	While these heuristics can deal with higher-dimensional problems than previous methods and enjoy some desirable statistical properties, they do not provide exact solutions and do not enjoy statistical properties as strong as those of the LASSO estimator.
35	1	In the case where variables are binary or with values in [0, 1], they propose an approach called Safe Pattern Pruning (SPP) which is able to provide the optimal solution of the LASSO with two-way interactions (or possibly higher-order interactions) for fairly high-dimensional problems, with no heredity constraint.
36	101	Typically, for a problem with 1,000 samples and 10,000 main effects where two-way interactions are considered, SPP can provide solutions for a grid of regularisation parameters within one or two hours on a laptop with one core.
38	7	More precisely, the authors propose a safe pattern pruning criterion that can safely discard subsets of interactions from the model to speed up convergence.
39	8	The performance of SPP is however hindered by several factors.
40	82	One of them is that safe screening rules can be quite conservative even in the sequential setting.
41	26	This property is inherited and amplified by the SPP criterion which can lead to heavy computations.
