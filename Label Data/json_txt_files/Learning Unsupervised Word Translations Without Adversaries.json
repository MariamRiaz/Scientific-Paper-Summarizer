{"sections": [{"heading": "1 Introduction", "text": "Translating words between languages, or more generally inferring bilingual dictionaries, is a long-studied research direction with applications including machine translation (Lample et al., 2017), multilingual word embeddings (Klementiev et al., 2012), and knowledge transfer to low resource languages (Guo et al., 2016). Research here has a long history under the guise of decipherment (Knight et al., 2006). Current contemporary methods have achieve effective word translation through theme-aligned corpora (Gouws et al., 2015), or seed dictionaries (Mikolov et al., 2013).\nMikolov et al. (2013) showed that monolingual word embeddings exhibit isomorphism across languages, and can be aligned with a simple linear transformation. Given two sets word vectors learned independently from monolingual corpora, and a dictionary of seed pairs to learn a linear transformation for alignment; they were able to\nestimate a complete bilingual lexicon. Many studies have since followed this approach, proposing various improvements such as orthogonal mappings (Artetxe et al., 2016) and improved objectives (Lazaridou et al., 2015).\nObtaining aligned corpora or bilingual seed dictionaries is nevertheless not straightforward for all language pairs. This has motivated a wave of very recent research into unsupervised word translation: inducing bilingual dictionaries given only monolingual word embeddings (Conneau et al., 2018; Zhang et al., 2017b,a; Artetxe et al., 2017). The most successful have leveraged ideas from Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). In this approach the generator provides the cross-modal mapping, taking embeddings of dictionary words in one language and \u2018generating\u2019 their translation in another. The discriminator tries to distinguish between this \u2018fake\u2019 set of translations and the true dictionary of embeddings in the target language. The two play a competitive game, and if the generator learns to fool the discriminator, then its cross-modal mapping should be capable of inducing a complete dictionary, as per Mikolov et al. (2013).\nDespite these successes, such adversarial methods have a number of well-known drawbacks (Arjovsky et al., 2017): Due to the nature of their min-max game, adversarial training is very unstable, and they are prone to divergence. It is extremely hyper-parameter sensitive, requiring problem-specific tuning. Convergence is also hard to diagnose and does not correspond well to efficacy of the generator in downstream tasks (Hoshen and Wolf, 2018).\nIn this paper, we propose an alternative statistical dependency-based approach to unsupervised word translation. Specifically, we propose to search for the cross-lingual word pairing that maximizes statistical dependency in terms of squared\nloss mutual information (SMI) (Yamada et al., 2015; Suzuki and Sugiyama, 2010). Compared to prior statistical dependency-based approaches such as Kernelized Sorting (KS) (Quadrianto et al., 2009) we advance: (i) through use of SMI rather than their Hilbert Schmidt Independence Criterion (HSIC) and (ii) through jointly optimising cross-modal pairing with representation learning within each view. In contrast to prior work that uses a fixed representation, by non-linearly projecting monolingual world vectors before matching, we learn a new embedding where statistical dependency is easier to establish. Our method: (i) achieves similar unsupervised translation performance to recent adversarial methods, while being significantly easier to train and (ii) clearly outperforms prior non-adversarial methods."}, {"heading": "2 Proposed model", "text": ""}, {"heading": "2.1 Deep Distribution Matching", "text": "Let dataset D contain two sets of unpaired monolingual word embeddings from two languages D = ({xi}ni=1, {yj}nj=1) where x,y \u2208 Rd. Let \u03c0 be a permutation function over {1, 2, . . . , n}, and \u03a0 the corresponding permutation indicator matrix: \u03a0 \u2208 {0, 1}n\u00d7n,\u03a01n = 1n, and \u03a0>1n = 1n. Where 1n is the n-dimensional vector with all ones. We aim to optimize for both the permutation \u03a0 (bilingual dictionary), and non-linear transformations gx(\u00b7) and gy(\u00b7) of the respective wordvectors, that maximize statistical dependency between the views. While regularising by requiring the original word embedding information is preserved through reconstruction using decoders fx(\u00b7) and fy(\u00b7). Our overall loss function is:\nmin \u0398x,\u0398y ,\u03a0 \u2126(D; \u0398x,\u0398y)\ufe38 \ufe37\ufe37 \ufe38 Regularizer \u2212\u03bbD\u03a0(D; \u0398x,\u0398y)\ufe38 \ufe37\ufe37 \ufe38 Dependency ,\nD\u03a0(D; \u0398x,\u0398y) = D\u03a0({gx(xi), gy(y\u03c0(i))}ni=1),\n\u2126(D; \u0398x,\u0398y) = n\u2211 i=1 \u2016xi \u2212 fx(gx(xi))\u201622\n+ \u2016yi \u2212 fy(gy(yi))\u201622 +R(\u0398x) +R(\u0398y).\n(1)\nwhere \u0398s parameterize the encoding and reconstruction transformations, R(\u00b7) is a regularizer (e.g., `2-norm and `1-norm), and D\u03a0(\u00b7, \u00b7) is a statistical dependency measure. Crucially compared to prior methods such as matching CCA (Haghighi\net al., 2008), dependency measures such as SMI do not need comparable representations to get started, making the bootstrapping problem less severe."}, {"heading": "2.2 Dependence Estimation", "text": "Squared-Loss Mutual Information (SMI) The squared loss mutual information between two random variables x and y is defined as (Suzuki and Sugiyama, 2010):\nSMI = \u222b\u222b ( p(x,y) p(x)p(y) \u2212 1 )2 p(x)p(y)dxdy,\nwhich is the Pearson divergence (Pearson, 1900) from p(x,y) to p(x)p(y). The SMI is an f - divergence (Ali and Silvey, 1966). That is, it is a non-negative measure and is zero only if the random variables are independent.\nTo measure SMI from a set of samples we take a direct density ratio estimation approach (Suzuki and Sugiyama, 2010), which leads (Yamada et al., 2015) to the estimator:\nS\u0302MI({(xi,yi)}ni=1) = 1 2n tr (diag (\u03b1\u0302)KL)\u2212 1 2 ,\nwhere K \u2208 Rn\u00d7n and L \u2208 Rn\u00d7n are the gram matricies for x and y respectively, and\nH\u0302 = 1\nn2 (KK>) \u25e6 (LL>),\nh\u0302 = 1\nn (K \u25e6L)1n, \u03b1\u0302 =\n( H\u0302 + \u03bbIn )\u22121 h\u0302,\n\u03bb > 0 is a regularizer and In \u2208 Rn\u00d7n is the identity matrix.\nSMI for Matching SMI computes the dependency between two sets of variables, under an assumption of known correspondence. In our application this corresponds to a measure of dependency between two aligned sets of monolingual wordvectors. To exploit SMI for matching, we introduce a permutation variable \u03a0 by replacing L\u2192 \u03a0>L\u03a0 in the estimator:\nS\u0302MI({(xi,y\u03c0(i))}n1 )= 1 2n tr ( diag (\u03b1\u0302\u03a0)K\u03a0 >L\u03a0 ) \u2212 1 2 ,\nthat will enable optimizing \u03a0 to maximize SMI."}, {"heading": "2.3 Optimization of parameters", "text": "To initialize \u0398x and \u0398y, we first independently estimate them using autoencoders. Then we employ an alternative optimization on Eq. (1) for\n(\u0398x,\u0398y) and \u03a0 until convergence. We use 3 layer MLP neural networks for both f and g. Algorithm 1 summarises the steps. Optimization for \u0398x and \u0398y With fixed permutation matrix \u03a0 (or \u03c0), the objective function\nmin \u0398x,\u0398y\n\u2126(D; \u0398x,\u0398y)\u2212 \u03bbD\u03a0(D; \u0398x,\u0398y) (2)\nis an autoencoder optimization with regularizer D\u03a0(\u00b7), and can be solved with backpropagation. Optimization for \u03a0 To find the permutation (word matching) \u03a0 that maximizes SMI given fixed encoding parameters \u0398x,\u0398y, we only need to optimize the dependency term D\u03a0 in Eq. (1). We employ the LSOM algorithm (Yamada et al., 2015). The estimator of SMI for samples {gx(xi), gy(y\u03c0(i))}ni=1 encoded with gx, gy is:\nS\u0302MI = 1 2n tr ( diag (\u03b1\u0302\u0398,\u03a0)K\u0398x\u03a0 >L\u0398y\u03a0 ) \u2212 1 2 .\nWhich leads to the optimization problem:\nmax \u03a0\u2208{0,1}n\u00d7n\ntr (\ndiag (\u03b1\u0302\u0398,\u03a0)K\u0398x\u03a0 >L\u0398y\u03a0 ) s.t. \u03a01n = 1n,\u03a0>1n = 1n. (3)\nSince the optimization problem is NP-hard, we iteratively solve the relaxed problem (Yamada et al., 2015):\n\u03a0new = (1\u2212 \u03b7)\u03a0old+\n\u03b7 argmax \u03a0\ntr ( diag ( \u03b1\u0302\u0398,\u03a0old ) K\u0398x\u03a0 >L\u0398y\u03a0 old ) ,\nwhere 0 < \u03b7 \u2264 1 is a step size. The optimization problem is a linear assignment problem (LAP). Thus, we can efficiently solve the algorithm by using the Hungarian method (Kuhn, 1955). To get discrete \u03a0, we solve the last step by setting \u03b7 = 1.\nIntuitively, this can be seen as searching for the permutation \u03a0 for which the data in the two (initially unsorted views) have a matching withinview affinity (gram) matrix, where matching is defined by maximum SMI."}, {"heading": "3 Experiments", "text": "In this section, we evaluate the efficacy of our proposed method against various state of the art methods for word translation. Implementation Details Our autoencoder consists of two layers with dropout and a tanh nonlinearity. We use polynomial kernel to compute\nAlgorithm 1 SMI-based unsupervised word translation Input: Unpaired word embeddings D = ({xi}ni=1, {yj}nj=1).\n1: Init: weights \u0398x, \u0398y, permutation matrix \u03a0. 2: while not converged do 3: Update \u0398x,\u0398y given \u03a0: Backprop (2). 4: Update \u03a0 given \u0398x,\u0398y: LSOM (3). 5: end while\nOutput: Permutation Matrix \u03a0. Params \u0398x, \u0398y.\nthe gram matrices K and L. For all pairs of languages, we fix the number of training epochs to 20. All the word vectors are `2 unit normalized. For CSLS we set the number of neighbors to 10. For optimizing \u03a0 at each epoch, we set the step size \u03b7 = 0.75 and use 20 iterations. For the regularization R(\u0398), we use the sum of the Frobenius norms of weight matrices. We train \u0398 using full batch gradient-descent, with learning rate 0.05. Datasets We performed experiments on the publicly available English-Italian, EnglishSpanish and English-Chinese datasets released by (Dinu and Baroni, 2015; Zhang et al., 2017b; Vulic and Moens, 2013). We name this collective set of benchmarks BLI. We also conduct further experiments on a much larger recent public benchmark, MUSE (Conneau et al., 2018)1. Setting and Metrics We evaluate all methods in terms of Precision@1, following standard practice. We note that while various methods in the literature were initially presented as fully supervised (Mikolov et al., 2013), semi-supervised (using a seed dictionary) (Haghighi et al., 2008), or unsupervised (Zhang et al., 2017b), most of them can be straightforwardly adapted to run in any of these settings. Therefore we evaluate all methods both in the unsupervised setting in which we are primarily interested, and also the commonly evaluated semi-supervised setting with 500 seed pairs. Competitors: Non-Adversarial In terms of competitors that, like us, do not make use of GANs, we evaluate: Translation Matrix (Mikolov et al., 2013), which alternates between estimating a linear transformation by least squares and matching by nearest neighbour (NN). Multilingual Correlation (Faruqui and Dyer, 2014), and Matching CCA (Haghighi et al., 2008), which alternates between matching and estimat-\n1https://github.com/facebookresearch/MUSE/\ning a joint linear subspace. Kernelized Sorting (Quadrianto et al., 2009), which directly uses HSIC-based statistical dependency to match heterogeneous data points. Self Training (Artetxe et al., 2017) A recent state of the art method that alternate between estimating an orthonormal transformation, and NN matching.\nCompetitors: Adversarial In terms of competitors that do make use of adversarial training, we compare: W-GAN and EMDOT (Zhang et al., 2017b) make use of adversarial learning using Wasserstein GAN and Earth Movers Distance respectively. GAN-NN (Conneau et al., 2018) uses adversarial learning to train an orthogonal transformation, along with some refinement steps and an improvement to the conventional NN matching procedure called \u2018cross-domain similarity lo-\ncal scaling\u2019 (CSLS). Since this is a distinct step, we also evaluate our method with CSLS.\nWe use the provided code for GAN-NN and Self-Train, while re-implementing EDOT/WGAN to avoid dependency on theano."}, {"heading": "3.1 Results", "text": "Fully Unsupervised Table 1 presents comparative results for unsupervised word translation on BLI and MUSE. From these we observe: (i) Our method (bottom) is consistently and significantly better than non-adversarial alternatives (top). (ii) Compared to adversarial alternatives Deep-SMI performs comparably.\nAll methods generally perform better on the MUSE dataset than BLI. These differences are due to a few factors: MUSE is a significantly\nlarger dataset than BLI, benefitting methods that can exploit a large amount of training data. In the ground-truth annotation, BLI contains 1-1 translations while MUSE contains more realistic 1-many translations (if any correct translation is picked, a success is counted), making it easier to reach a higher score.\nSemi-supervised Results using a 500-word bilingual seed dictionary are presented in Table 2. From these we observe: (i) The conventional methods\u2019 performances (top) jump up, showing that they are more competitive if at least some sparse data is available. (ii) Deep-SMI performance also improves, and still outperforms the classic methods significantly overall. (iii) Again, we perform comparably to the GAN methods."}, {"heading": "3.2 Discussion", "text": "Figure 1 shows the convergence process of DeepSMI. From this we see that: (i) Unlike the adversarial methods, our objective (Eq. (1)) improves smoothly over time, making convergence much easier to assess. (ii) Unlike the adversarial methods, our accuracy generally mirrors the model\u2019s loss. In contrast, the various losses of the adversarial approaches do not well reflect translation accuracy, making model selection or early stopping a challenge in itself. Please compare our Figure 1 with Fig 3 in Zhang et al. (2017b), and Fig 2 in Conneau et al. (2018).\nThere are two steps in our optimization: matching permutation \u03a0 and representation weights \u0398. Although this is an alternating optimization, it is analogous to an EM-type algorithm optimizing latent variables (\u03a0) and parameters (\u0398). While local minima are a risk, every optimisation step for either variable reduces our objective Eq. (1).\nThere is no min-max game, so no risk of divergence as in the case of adversarial GAN-type methods.\nOur method can also be understood as providing an unsupervised Deep-CCA type model for relating heterogeneous data across two views. This is in contrast to the recently proposed unsupervised shallow CCA (Hoshen and Wolf, 2018), and conventional supervised Deep-CCA (Chang et al., 2018) that requires paired data for training; and using SMI rather than correlation as the optimisation objective."}, {"heading": "4 Conclusion", "text": "We have presented an effective approach to unsupervised word translation that performs comparably to adversarial approaches while being significantly easier to train and diagnose; as well as outperforming prior non-adversarial approaches."}], "year": 2018, "references": [{"title": "A general class of coefficients of divergence of one distribution from another", "authors": ["Syed M. Ali", "Samuel. D. Silvey."], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), 28(1):131\u2013142.", "year": 1966}, {"title": "Wasserstein generative adversarial networks", "authors": ["Mart\u00edn Arjovsky", "Soumith Chintala", "L\u00e9on Bottou."], "venue": "ICML.", "year": 2017}, {"title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance", "authors": ["Mikel Artetxe", "Gorka Labaka", "Eneko Agirre."], "venue": "EMNLP.", "year": 2016}, {"title": "Learning bilingual word embeddings with (almost) no bilingual data", "authors": ["Mikel Artetxe", "Gorka Labaka", "Eneko Agirre."], "venue": "ACL.", "year": 2017}, {"title": "Scalable and effective deep CCA via soft decorrelation", "authors": ["Xiaobin Chang", "Tao Xiang", "Timothy M. Hospedales."], "venue": "CVPR.", "year": 2018}, {"title": "Word translation without parallel data", "authors": ["Alexis Conneau", "Guillaume Lample", "Marc\u2019Aurelio Ranzato", "Ludovic Denoyer", "Herv\u00e9 J\u00e9gou"], "venue": "In ICLR", "year": 2018}, {"title": "Improving zero-shot learning by mitigating the hubness problem", "authors": ["Georgiana Dinu", "Marco Baroni."], "venue": "ICLR Workshops.", "year": 2015}, {"title": "Improving vector space word representations using multilingual correlation", "authors": ["Manaal Faruqui", "Chris Dyer."], "venue": "EACL.", "year": 2014}, {"title": "Generative adversarial nets", "authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "NIPS.", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "authors": ["Stephan Gouws", "Yoshua Bengio", "Greg Corrado."], "venue": "ICML.", "year": 2015}, {"title": "A distributed representation-based framework for cross-lingual transfer parsing", "authors": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "JAIR, 55(1):995\u20131023.", "year": 2016}, {"title": "Learning bilingual lexicons from monolingual corpora", "authors": ["Aria Haghighi", "Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein."], "venue": "ACL.", "year": 2008}, {"title": "Unsupervised correlation analysis", "authors": ["Yedid Hoshen", "Lior Wolf."], "venue": "CVPR.", "year": 2018}, {"title": "Inducing crosslingual distributed representations of words", "authors": ["Alexandre Klementiev", "Ivan Titov", "Binod Bhattarai."], "venue": "COLING.", "year": 2012}, {"title": "Unsupervised analysis for decipherment problems", "authors": ["K. Knight", "A. Nair", "N. Rathod", "K. Yamada."], "venue": "Proc. ACL-COLING.", "year": 2006}, {"title": "The hungarian method for the assignment problem", "authors": ["Harold W Kuhn."], "venue": "Naval research logistics quarterly, 2(1-2):83\u201397.", "year": 1955}, {"title": "Unsupervised machine translation using monolingual corpora only", "authors": ["Guillaume Lample", "Ludovic Denoyer", "Marc\u2019Aurelio Ranzato"], "venue": "arXiv preprint arXiv:1711.00043", "year": 2017}, {"title": "Hubness and pollution: Delving into cross-space mapping for zero-shot learning", "authors": ["Angeliki Lazaridou", "Georgiana Dinu", "Marco Baroni."], "venue": "ACL.", "year": 2015}, {"title": "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling", "authors": ["Karl Pearson."], "venue": "The London, Edinburgh, and", "year": 1900}, {"title": "Kernelized sorting", "authors": ["Novi Quadrianto", "Le Song", "Alex J Smola."], "venue": "NIPS.", "year": 2009}, {"title": "Sufficient dimension reduction via squared-loss mutual information estimation", "authors": ["Taiji Suzuki", "Masashi Sugiyama."], "venue": "AISTATS.", "year": 2010}, {"title": "Crosslingual semantic similarity of words as the similarity of their semantic word responses", "authors": ["Ivan Vulic", "Marie-Francine Moens."], "venue": "HLT-NAACL.", "year": 2013}, {"title": "Cross-domain matching with squared-loss mutual information", "authors": ["Makoto Yamada", "Leonid Sigal", "Michalis Raptis", "Machiko Toyoda", "Yi Chang", "Masashi Sugiyama."], "venue": "IEEE TPAMI, 37(9):1764\u2013 1776.", "year": 2015}, {"title": "Adversarial training for unsupervised bilingual lexicon induction", "authors": ["Meng Zhang", "Yang Liu", "Huanbo Luan", "Maosong Sun."], "venue": "ACL.", "year": 2017}, {"title": "Earth mover\u2019s distance minimization for unsupervised bilingual lexicon induction", "authors": ["Meng Zhang", "Yang Liu", "Huanbo Luan", "Maosong Sun."], "venue": "EMNLP.", "year": 2017}], "id": "SP:0ebba8e9e3781fe2d6d14220dfc444a796611425", "authors": [{"name": "Tanmoy Mukherjee", "affiliations": []}, {"name": "Makoto Yamada", "affiliations": []}, {"name": "Timothy Hospedales", "affiliations": []}], "abstractText": "Word translation, or bilingual dictionary induction, is an important capability that impacts many multilingual language processing tasks. Recent research has shown that word translation can be achieved in an unsupervised manner, without parallel seed dictionaries or aligned corpora. However, state of the art methods for unsupervised bilingual dictionary induction are based on generative adversarial models, and as such suffer from their well known problems of instability and hyperparameter sensitivity. We present a statistical dependency-based approach to bilingual dictionary induction that is unsupervised \u2013 no seed dictionary or parallel corpora required; and introduces no adversary \u2013 therefore being much easier to train. Our method performs comparably to adversarial alternatives and outperforms prior non-adversarial methods.", "title": "Learning Unsupervised Word Translations Without Adversaries"}