21	20	Furthermore, using the word prediction mechanism on the initial state as a word predictor to reduce the target side vocabulary could greatly improve the decoding efficiency, without a significant loss on the translation quality.
41	29	The gated recurrent unit (GRU) is used as the recurrent unit in each RNN, which is shown to have promising results in speech recognition and machine translation (Cho et al., 2014).
49	24	Currently, the update for the encoder only happens when a translation error occurs in the decoder.
50	21	The error is propagated through multiple time steps in the recurrent structure until it reaches the encoder.
52	51	As a result, the values of initial state may not be exact during the translation process, leading to poor translation performances.
53	17	We propose word prediction as a mechanism to control the values of initial state.
54	78	The intuition is that since the initial state is responsible for the translation of whole target sentence, it should at least contain information of each word in the target sentence.
57	20	Here the word prediction mechanism is a simpler sub-task of translation, where the order of words is not considered.
73	61	NMT models optimize the networks by maximizing the likelihood of the target translation y given source sentence x, denoted by LT. LT = 1 |y| |y|∑ j=1 logP (yj |y<j , x) (22) where P (yj |y<j , x) is defined in Equation 7.
78	32	The previously proposed word prediction mechanism could be used only as a extra training objective, which will not be computed during the translation.
80	52	On the other hand, using a smaller and specific vocabulary for each sentence or batch will improve translation efficiency.
83	53	The prediction could be made from the initial state s0, without using extra resources such as word dictionaries, extracted phrases or frequent word lists, as in Mi et al. (2016).
84	23	We perform experiments on the Chinese-English (CH-EN) and German-English (DE-EN) machine translation tasks.
86	39	We use NIST MT02 as our validation set, and the NIST MT03, MT04 and MT05 as our test sets.
115	33	Using both techniques improves the baseline by 4.53 BLEU.
118	26	We compare our models with systems using dropout and ensemble techniques.
119	35	The results show in Table 3 and 4.
136	16	Specifically, the precision reaches 73% in top 10.
147	16	The comparison is made in both translation quality and decoding time.
148	28	As all our models with fixed vocabulary size have exactly the same number of parameters for decoding (extra mechanism is used only for training), we only plot the decoding time of the WPED for comparison.
150	13	When we start the experiments with top 1k vocabulary (1/30 of the baseline settings), the translation quality of both WPE-V and WPED-V are already higher than the baseNMT; while their decoding time is less than 1/3 of an NMT system with 30k vocabulary.
161	22	It also wrongly translates the company name “time warner inc.” as the redundant information “internet company”; “america online” as “us line”.
163	37	But they still make mistakes about the translation of “online” and the company name “time warner inc.”.
165	21	The encoder-decoder architecture provides a general paradigm for learning machine translation from the source language to the target language.
166	25	However, due to the large amount of parameters and relatively small training data set, the end-toend learning of an NMT model may not be able to learn the best solution.
168	51	Instead of looking for other annotated data, we notice that the words in the target language sentence could be viewed as a natural annotation.
169	31	We propose to use the word prediction mechanism to enhance the initial state generated by the encoder and extend the mechanism to control the hidden states of decoder as well.
172	16	Our attempts demonstrate that the learning of the large scale neural network systems is still not good enough.
173	13	In the future, it might be helpful to analyze the benefits of jointly learning other related tasks together with machine translation, to provide further control of the learning process.
174	15	It is interesting to demonstrate the effectiveness of the proposed mechanism on other sequence to sequence learning tasks as well.
