46	16	We can similarly permute the dependents of nouns (N).2 This permutes about 93% of S’s nodes (Table 2), as UD treats adpositions and conjunctions as childless dependents.
48	57	Note that it still uses English lexical items.
51	33	There may be more adventurous ways to manufacture synthetic languages (see §8 for some options).
53	14	First, we retain the immediate dominance structure and lexical items of the substrate trees, altering only their linear precedence relations.
55	14	These im- portant properties would not be captured by a simple context-free model of dependency trees, which is why we modify real sentences rather than generating new sentences from such a model.
65	13	For example, we do not currently condition the order of the dependent subtrees on their heaviness or on the length of resulting dependencies, and thus we will not faithfully model phenomena like heavy-shift (Hawkins, 1994; Eisner and Smith, 2010).
69	20	All of these problems could be addressed by enriching the features that are described in the next section.
75	54	f extracts a sparse feature vector that describes the ordered pair of nodes πi, πj , where the ordering π would place πi to the left of πj .
87	16	The expensive part of this computation is the gradient of logZ(x), which is an expected feature vector.
89	24	A given language L may not use all of the tags and relations.
93	15	We mitigate this issue in two ways.
94	23	First, our ordering model (1) is designed to be more robust to transfer than, say, a Markov model.
95	25	The position of each node is influenced by all n− 1 other nodes, not just by the two adjacent nodes.
98	35	Second, we actually sample the reordering from a distribution pθ with an interpolated parameter vector θ = θS ′ X = (1− λ)θRXX + λθSX , where λ = 0.05.
106	12	These features detect the relative order of two siblings.
112	15	Thus, a bigram feature such as A.DET.EOS would fire on DET when it falls at the end of the sequence.
116	23	Using the example from Figure 1, for subtree DET ADJ NOUN this particular future det amod the features that fire are Template Features L.ti.ri L.DET.det, L.ADJ.amod L.ti.ri.tj.rj L.DET.det.ADJ.amod d.ti.ri.tj.rj l.DET.det.ADJ.amod A.t1.r1.t2.r2 A.BOS.BOS.DET.det, A.DET.det.ADJ.amod, A.ADJ.amod.NOUN.head, A.NOUN.head.EOS.EOS plus backoff features and H features (not shown).
139	13	An interesting exception in Figure 2 is Latin 1 2 3 4 5 6 7 8 fr pt no de it la_itt ar cs hi es real pos synthetic pos real word synthetic word (la itt), whose poor parsability—at least by a delexicalized parser that does not look at word endings— may be due to its especially free word order (Table 2).
147	67	Figure 4 visualizes a small sample of 110 languages from our collection.6 For each ordered pair of languages (S, T ), we defined the dissimilarity d(S, T ) as the decrease in UAS when we parse the dev data of T using a parser trained on S instead of one trained on T .
148	50	Small dissimilarity (i.e., good parsing transfer) translates to small distance in the figure.
150	26	Some may be unnatural, but others are similar to other real languages, including held-out dev languages.
159	39	We evaluate single-source transfer when the pool of m source languages consists of n real UD languages, plus m− n synthetic GD languages derived by “remixing” just these real languages.7 We try various values of n and m, where n can be as large as 10 (training languages from Table 1) and m can be as large as n× (n+ 1)× (n+ 1) ≤ 1210 (see §5).
161	31	In these experiments (unlike those of §6), we always evaluate fairly on T ’s full dev or test set from UD—not just the sentences we kept for its GD version (cf.
162	97	Table 2).8 The hope is that a large pool will contain at least one language—real or synthetic—that is “close” to T .
163	42	We have two ways of trying to select a source S with this property: Supervised selection selects the S whose parser achieves the highest UAS on 100 training sentences of language T .
164	32	This requires 100 good trees for T , which could be obtained with a modest investment—a single annotator attempting to follow the UD annotation standards in a consistent way on 100 sentences of T , without writing out formal T - specific guidelines.
169	17	More precisely, we choose the S that maximizes pS(tag sequences from T )—in other words, the maximum-likelihood S—where pS is our trigram language model for the tag sequences of S. This approach is loosely inspired by Søgaard (2011).
170	167	Our most complete visualization is Figure 5, which we like to call the “kite graph” for its appearance.
171	24	We plot the UAS on the development treebank of T as a function of n, m, and the selection method.
172	192	As Appendix A details, each point on this graph is actually an average over 10,000 experiments that make random choices of T (from the UD development languages), the n real languages (from the UD training languages), and the m − n synthetic languages (from the GD languages derived from the n real lan- Each point is the mean dev UAS over 10,000 experiments.
