{"sections": [{"heading": "1. Introduction", "text": "Spectral clustering algorithms are designed to solve a relaxation of the graph cut problem based on graph Laplacians that capture pairwise dependencies between vertices, and produce sets with small conductance that represent clusters. Due to their scalability and provable performance guarantees, spectral methods represent one of the most prevalent graph clustering approaches (Chung, 1997; Ng et al., 2002).\nMany relevant problems in clustering, semisupervised learning and MAP inference (Zhou et al., 2007; Hein et al., 2013; Zhang et al., 2017) involve higher-order vertex dependencies that require one to consider hypergraphs instead of graphs. To address spectral hypergraph clustering problems, several approaches have been proposed that typically operate by first projecting the hypergraph onto a graph via clique expansion and then performing spectral clustering on graphs (Zhou et al., 2007). Clique expansion involves transforming a weighted hyperedge into a weighted clique such that the graph cut weights approximately preserve the cut weights of the hyperedge. Almost exclusively, these\n1Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, USA. Correspondence to: Pan Li <panli2@illinois.edu>, Olgica Milenkovic <milenkov@illinois.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\napproximations have been based on the assumption that each hyperedge cut has the same weight, in which case the underlying hypergraph is termed homogeneous.\nHowever, in image segmentation, MAP inference on Markov random fields (Arora et al., 2012; Shanu et al., 2016), network motif studies (Li & Milenkovic, 2017; Benson et al., 2016; Tsourakakis et al., 2017) and rank learning (Li & Milenkovic, 2017), higher order relations between vertices captured by hypergraphs are typically associated with different cut weights. In (Li & Milenkovic, 2017), Li and Milenkovic generalized the notion of hyperedge cut weights by assuming that different hyperedge cuts have different weights, and that consequently, each hyperedge is associated with a vector of weights rather than a single scalar weight. If the weights of the hyperedge cuts are submodular, then one can use a graph with nonnegative edge weights to efficiently approximate the hypergraph, provided that the largest size of a hyperedge is a relatively small constant. This property of the projected hypergraphs allows one to leverage spectral hypergraph clustering algorithms based on clique expansions with provable performance guarantees. Unfortunately, the clique expansion method in general has two drawbacks: The spectral clustering algorithm for graphs used in the second step is merely quadratically optimal, while the projection step can cause a large distortion.\nTo address the quadratic optimality issue in graph clustering, Amghibech (Amghibech, 2003) introduced the notion of p-Laplacians of graphs and derived Cheeger-type inequalities for the second smallest eigenvalue of a p-Laplacian, p > 1, of a graph. These results motivated Bu\u0308hler and Hein\u2019s work (Bu\u0308hler & Hein, 2009) on spectral clustering based on p-Laplacians that provided tighter approximations of the Cheeger constant. Szlam and Bresson (Szlam & Bresson, 2010) showed that the 1-Laplacian allows one to exactly compute the Cheeger constant, but at the cost of computational hardness (Chang, 2016). Very little is known about the use of p-Laplacians for hypergraph clustering and their spectral properties.\nTo address the clique expansion problem, Hein et al. (Hein et al., 2013) introduced a clustering method for homogeneous hypergraphs that avoids expansions and works directly with the total variation of homogeneous hypergraphs,\nwithout investigating the spectral properties of the operator. The only other line of work trying to mitigate the projection problem is due to Louis (Louis, 2015), who used a natural extension of 2-Laplacians for homogeneous hypergraphs, derived quadratically-optimal Cheeger-type inequalities and proposed a semidefinite programing (SDP) based algorithm whose complexity scales with the size of the largest hyperedge in the hypergraph.\nOur contributions are threefold. First, we introduce submodular hypergraphs. Submodular hypergraphs allow one to perform hyperedge partitionings that depend on the subsets of elements involved in each part, thereby respecting higherorder and other constraints in graphs (see (Li & Milenkovic, 2017; Arora et al., 2012; Fix et al., 2013) for applications in food network analysis, learning to rank, subspace clustering and image segmentation). Second, we define p-Laplacians for submodular hypergraphs and generalize the corresponding discrete nodal domain theorems (Tudisco & Hein, 2016; Chang et al., 2017) and higher-order Cheeger inequalities. Even for homogeneous hypergraphs, nodal domain theorems were not known and only one low-order Cheeger inequality for 2-Laplacians was established by Louis (Louis, 2015). An analytical obstacle in the development of such a theory is the fact that p-Laplacians of hypergraphs are operators that act on vectors and produce sets of values. Consequently, operators and eigenvalues have to be defined in a set-theoretic manner. Third, based on the newly established spectral hypergraph theory, we propose two spectral clustering methods that learn the second smallest eigenvalues of 2- and 1-Laplacians. The algorithm for 2-Laplacian eigenvalue computation is based on an SDP framework and can provably achieve quadratic optimality with an O( \u221a \u03b6(E)) approximation constant, where \u03b6(E) denotes the size of the largest hyperedge in the hypergraph. The algorithm for 1-Laplacian eigenvalue computation is based on the inverse power method (IPM) (Hein & Bu\u0308hler, 2010) that only has convergence guarantees. The key novelty of the IPM-based method is that the critical inner-loop optimization problem of the IPM is efficiently solved by algorithms recently developed for decomposable submodular minimization (Jegelka et al., 2013; Ene & Nguyen, 2015; Li & Milenkovic, 2018). Although without performance guarantees, given that the 1-Laplacian provides the tightest approximation guarantees, the IPM-based algorithm \u2013 as opposed to the clique expansion method (Li & Milenkovic, 2017) \u2013 performs very well empirically even when the size of the hyperedges is large. This fact is illustrated on several UC Irvine machine learning datasets available from (Asuncion & Newman, 2007).\nThe paper is organized as follows. Section 2 contains an overview of graph Laplacians and introduces the notion of submodular hypergraphs. The section also contains a description of hypergraph Laplacians, and relevant concepts in submodular function theory. Section 3 presents the funda-\nmental results in the spectral theory of p-Laplacians, while Section 4 introduces two algorithms for evaluating the second largest eigenvalue of p-Laplacians needed for 2-way clustering. Section 5 presents experimental results. All proofs are relegated to the Supplementary Material."}, {"heading": "2. Mathematical Preliminaries", "text": "A weighted graph G = (V,E,w) is an ordered pair of two sets, the vertex set V = [N ] = {1, 2, . . . , N} and the edge set E \u2286 V \u00d7 V , equipped with a weight function w : E \u2192 R+.\nA cut C = (S, S\u0304) is a bipartition of the set V , while the cut-set (boundary) of the cut C is defined as the set of edges that have one endpoint in S and one in the complement of S, S\u0304, i.e., \u2202S = {(u, v) \u2208 E | u \u2208 S, v \u2208 S\u0304}. The weight of the cut induced by S equals vol(\u2202S) = \u2211 u\u2208S, v\u2208S\u0304 wuv , while the conductance of the cut is defined as\nc(S) = vol(\u2202S)\nmin{vol(S), vol(S\u0304)} ,\nwhere vol(S) = \u2211 u\u2208S \u00b5u, and \u00b5u = \u2211 v\u2208V wuv. Whenever clear from the context, for e = (uv), we write we instead of wuv. Note that in this setting, the vertex weight values \u00b5u are determined based on the weights of edges we incident to u. Clearly, one can use a different choice for these weights and make them independent from the edge weights, which is a generalization we pursue in the context of submodular hypergraphs. The smallest conductance of any bipartition of a graph G is denoted by h2 and referred to as the Cheeger constant of the graph.\nA generalization of the Cheeger constant is the k\u2212way Cheeger constant of a graph G. Let Pk denote the set of all partitions of V into k-disjoint nonempty subsets, i.e., Pk = {(S1, S2, ..., Sk)|Si \u2282 V, Si 6= \u2205, Si \u2229 Sj = \u2205,\u2200i, j \u2208 [k], i 6= j}. The k\u2212way Cheeger constant is defined as\nhk = min (S1,S2,...,Sk)\u2208Pk max i\u2208[k] c(Si).\nSpectral graph theory provides a means for bounding the Cheeger constant using the (normalized) Laplacian matrix of the graph, defined as L = D \u2212 A and L = I \u2212D\u22121/2AD\u22121/2, respectively. Here, A stands for the adjacency matrix of the graph, D denotes the diagonal degree matrix, while I stands for the identity matrix. The graph Laplacian is an operator4(g)2 (Chung, 1997) that satisfies\n\u3008x,4(g)2 (x)\u3009 = \u2211\n(uv)\u2208E\nwuv(xu \u2212 xv)2.\nA generalization of the above operator termed the pLaplacian operator of a graph 4(g)p was introduced by\nAmghibech in (Amghibech, 2003), where \u3008x,4(g)p (x)\u3009 = \u2211\n(uv)\u2208E\nwuv|xu \u2212 xv|p.\nThe well known Cheeger inequality asserts the following relationship between h2 and \u03bb, the second smallest eigenvalue of the normalized Laplacian4(g)2 of a graph:\nh2 \u2264 \u221a 2\u03bb \u2264 2 \u221a h2.\nIt can be shown that the cut h\u03022 dictated by the elements of the eigenvector associated with \u03bb satisfies h\u03022 \u2264 \u221a 2\u03bb, which\nimplies h\u03022 \u2264 2 \u221a h2. Hence, spectral clustering provides a quadratically optimal graph partition."}, {"heading": "2.1. Submodular Hypergraphs", "text": "A weighted hypergraph G = (V,E,w) is an ordered pair of two sets, the vertex set V = [N ] and the hyperedge set E \u2286 2V , equipped with a weight function w : E \u2192 R+. The relevant notions of cuts, boundaries and volumes for hypergraphs can be defined in a similar manner as for graphs. If each cut of a hyperedge e has the same weight we, we refer to the cut as a homogeneous cut and the corresponding hypergraph as a homogeneous hypergraph.\nFor a ground set \u2126, a set function f : 2\u2126 \u2192 R is termed submodular if for all S, T \u2286 \u2126, one has f(S) + f(T ) \u2265 f(S \u222a T ) + f(S \u2229 T ).\nA weighted hypergraph G = (V,E,\u00b5,w) is termed a submodular hypergraph with vertex set V , hyperedge set E and positive vertex weight vector \u00b5 , {\u00b5v}v\u2208V , if each hyperedge e \u2208 E is associated with a submodular weight function we(\u00b7) : 2e \u2192 [0, 1]. In addition, we require the weight function we(\u00b7) to be:\n1) Normalized, so that we(\u2205) = 0, and all cut weights corresponding to a hyperedge e are normalized by \u03d1e = maxS\u2286e we(S). In this case, we(\u00b7) \u2208 [0, 1];\n2) Symmetric, so that we(S) = we(e\\S) for any S \u2286 e;\nThe submodular hyperedge weight functions are summarized in the vector w , {(we, \u03d1e)}e\u2208E . If we(S) = 1 for all S \u2208 2e\\{\u2205, e}, submodular hypergraphs reduce to homogeneous hypergraphs. We omit the designation homogeneous whenever there is no context ambiguity.\nClearly, a vertex v is in e if and only if we({v}) > 0: If we({v}) = 0, the submodularity property implies that v is not incident to e, as for any S \u2286 e\\{v}, |we(S \u222a {v}) \u2212 we(S)| \u2264 we({v}) = 0.\nWe define the degree of a vertex v as dv = \u2211 e\u2208E: v\u2208e \u03d1e, i.e., as the sum of the max weights of edges incident to the vertex v. Furthermore, for any vector y \u2208 RN , we define the projection weight of y onto any subset S \u2286 V\nas y(S) = \u2211 v\u2208S yv. The volume of a subset of vertices\nS \u2286 V equals vol(S) = \u2211 v\u2208S \u00b5v.\nFor any S \u2286 V , we generalize the notions of the boundary of S and the volume of the boundary of S according to \u2202S = {e \u2208 E|e \u2229 S 6= \u2205, e \u2229 S\u0304 6= \u2205}, and\nvol(\u2202S) = \u2211 e\u2208\u2202S \u03d1ewe(S) = \u2211 e\u2208E \u03d1ewe(S), (1)\nrespectively. Then, the normalized cut induced by S, the Cheeger constant and the k-way Cheeger constant for hypergraphs are defined in an analogous manner as for graphs."}, {"heading": "2.2. Laplacian Operators for Hypergraphs", "text": "We introduce next p-Laplacians of hypergraphs and a number of relevant notions associated with Laplacian operators.\nHein et al.(Hein et al., 2013) connected p-Laplacians4(h)p for homogeneous hypergraphs with the total variation via\n\u3008x,4(h)p (x)\u3009 = \u2211 e\u2208E we max u,v\u2208e |xu \u2212 xv|p,\nwhere we denotes the weight of a homogeneous hyperedge e. They also introduced the Inverse Power Method (IPM) to evaluate the spectrum of the hypergraph 1-Laplacian 4(h)1 (Hein et al., 2013), but did not establish any performance guarantees. In an independent line of work, Louis (Louis, 2015) introduced a quadratic variant of a hypergraph Laplacian\n\u3008x,4(h)2 (x)\u3009 = \u2211 e\u2208E we max u,v\u2208e (xu \u2212 xv)2.\nHe also derived a Cheeger-type inequality relating the second smallest eigenvalue \u03bb of 4(h)2 and the Cheeger constant of the hypergraph h2 that reads as h\u03022 \u2264 O( \u221a log \u03b6(E)) \u221a \u03bb \u2264 O( \u221a log \u03b6(E)) \u221a h2. Compared to the result of graph (3.12), for homogeneous hypergraphs, log \u03b6(E) plays as some additional difficulty to approximate h2. Learning the spectrum of generalizations of hypergraph Laplacians can be an even more challenging task."}, {"heading": "2.3. Relevant Background on Submodular Functions", "text": "Given an arbitrary set function F : 2V \u2192 R satisfying F (V ) = 0, the Lova\u0301sz extension (Lova\u0301sz, 1983) f : RN \u2192 R of F is defined as follows: For any vector x \u2208 RN , we order its entries in nonincreasing order xi1 \u2265 xi2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 xin while breaking the ties arbitrarily, and set\nf(x) = N\u22121\u2211 j=1 F (Sj)(xij \u2212 xij+1), (2)\nwith Sj = {i1, i2, ..., ij}. For submodular F , the Lova\u0301sz extension is a convex function (Lova\u0301sz, 1983).\nLet 1S \u2208 RN be the indicator vector of the set S. Hence, for any S \u2286 V , one has F (S) = f(1S). For a submodular F , we define a convex set termed the base polytope\nB , {y \u2208 RN |y(S) \u2264 F (S), for all S \u2286 V, and such that y(V ) = F (V ) = 0}.\nAccording to the defining property of submodular functions (Lova\u0301sz, 1983), we may write f(x) = maxy\u2208B\u3008y, x\u3009.\nThe subdifferential \u2207f(x) of f is defined as\n{y \u2208 RN | f(x\u2032)\u2212 f(x) \u2265 \u3008y, x\u2032 \u2212 x\u3009, \u2200x\u2032 \u2208 RN}.\nAn important result from (Bach et al., 2013) characterizes the subdifferentials\u2207f(x): If f(x) is the Lova\u0301sz extension of a submodular function F with base polytope B, then\n\u2207f(x) = arg max y\u2208B \u3008y, x\u3009. (3)\nObserve that\u2207f(x) is a set and that the right hand side of the definition represents a set of maximizers of the objective function. If f(x) is the Lova\u0301sz extension of a submodular function, then \u3008q, x\u3009 = f(x) for all q \u2208 \u2207f(x).\nFor each hyperedge e \u2208 E of a submodular hypergraph, following the above notations, we let Be, E(Be), fe denote the base polytope, the set of extreme points of the base polytope, and the Lova\u0301sz extension of the submodular hyperedge weight function we, respectively. Note that for any S \u2286 V , we(S) = we(S \u2229 e). Consequently, for any y \u2208 Be, yv = 0 for v 6\u2208 e. Since \u2207fe \u2286 Be, it also holds that (\u2207fe)v = 0 for v /\u2208 e. When using formula (2) to explicitly describe the Lova\u0301sz extension fe, we can either use a vector x of dimension N or only those of its components that lie in e. Furthermore, in the later case, |E(Be)| = |e|!.\n3. p-Laplacians of Submodular Hypergraphs We start our discussion by defining the notion of a pLaplacian operator for submodular hypergraphs. We find the following definitions useful for our subsequent exposition.\nLet sgn(\u00b7) be the sign function defined as sgn(a) = 1, for a > 0, sgn(a) = \u22121, for a < 0, and sgn(a) = [\u22121, 1], for a = 0. For all v \u2208 V , define the entries of a vector \u03d5p over RN according to (\u03d5p(x))v = |xv|p\u22121sgn(xv). Furthermore, let U be a N \u00d7N diagonal matrix such that Uvv = \u00b5v for all v \u2208 V .\nLet \u2016x\u2016`p,\u00b5 = ( \u2211 v\u2208V \u00b5v|xv|p)1/p and Sp,\u00b5 , {x \u2208 RN |\u2016x\u2016`p,\u00b5 = 1}. For a function \u03a6 over RN , let \u03a6|Sp,\u00b5 stand for \u03a6 restricted to Sp,\u00b5. Definition 3.1. The p-Laplacian operator of a submodular hypergraph, denoted by4p (p \u2265 1), is defined for all x \u2208\nRN according to \u3008x,4p(x)\u3009 , Qp(x) = \u2211 e\u2208E \u03d1efe(x) p. (4)\nHence,4p(x) may also be specified directly as an operator over RN that reads as\n4p(x) = { \u2211 e\u2208E \u03d1efe(x) p\u22121\u2207fe(x) p > 1,\u2211\ne\u2208E \u03d1e\u2207fe(x) p = 1.\nDefinition 3.2. A pair (\u03bb, x) \u2208 R \u00d7 RN/{0} is called an eigenpair of the p-Laplacian4p if4p(x) \u2229 \u03bbU \u03d5p(x) 6= \u2205.\nAs fe(1) = 0, we have 4p(1) = 0, so that (0,1) is an eigenpair of the operator 4p. A p-Laplacian operates on vectors and produces sets. In addition, since for any t > 0, 4p(tx) = tp\u221214p(x) and \u03d5p(tx) = tp\u22121\u03d5p(x), (tx, \u03bb) is an eigenpair if and only if (x, \u03bb) is an eigenpair. Hence, one only needs to consider normalized eigenpairs: In our setting, we choose eigenpairs that lie in Sp,\u00b5 for a suitable choice for the dimension of the space.\nFor linear operators, the Rayleigh-Ritz method (Gould, 1966) allows for determining approximate solutions to eigenproblems and provides a variational characterization of eigenpairs based on the critical points of functionals. To generalize the method, we introduce two even functions,\nQ\u0303p(x) , Qp(x)|Sp,\u00b5 , Rp(x) , Qp(x)\n\u2016x\u2016p`p,\u00b5 .\nDefinition 3.3. A point x \u2208 Sp,\u00b5 is termed a critical point of Rp(x) if 0 \u2208 \u2207Rp(x). Correspondingly, Rp(x) is termed a critical value of Rp(x). Similarly, x is termed a critical point of Q\u0303p if there exists a \u03c3 \u2208 \u2207Qp(x) such that P (x)\u03c3 = 0, where P (x)\u03c3 stands for the projection of \u03c3 onto the tangent space of Sp,\u00b5 at the point x. Correspondingly, Q\u0303p(x) is termed a critical value of Q\u0303p.\nThe relationships between the critical points of Q\u0303p(x) and Rp(x) and the eigenpairs of4p relevant to our subsequent derivations are listed in Theorem 3.4.\nTheorem 3.4. A pair (\u03bb, x) (x \u2208 Sp,\u00b5) is an eigenpair of the operator4p 1) if and only if x is a critical point of Q\u0303p with critical value \u03bb, and provided that p \u2265 1. 2) if and only if x is a critical point of Rp with critical value \u03bb, and provided that p > 1. 3) if x is a critical point of Rp with critical value \u03bb, and provided that p = 1.\nThe critical points of Q\u0303p bijectively characterize eigenpairs for all choices of p \u2265 1. However, Rp has the same property only if p > 1. This is a consequence of the nonsmoothness of the set S1,\u00b5, which has been observed for graphs as well (See the examples in Section 2.2 of (Chang, 2016))."}, {"heading": "3.1. Discrete Nodal Domain Theorem for p\u2212Laplacians", "text": "Nodal domain theorems are essential for understanding the structure of eigenvectors of operators and they have been the subject of intense study in geometry and graph theory alike (B\u0131y\u0131koglu et al., 2007). The eigenfunctions of a Laplacian operator may take positive and negative values. The signs of the values induce a partition of the vertices in V into maximal connected components on which the sign of the eigenfunction does not change: These components represent the nodal domains of the eigenfunction and approximate the clusters of the graphs.\nDavies et al. (BrianDavies et al., 2001) derived the first discrete nodal domain theorem for the4(g)2 operator. Chang et al. (Chang et al., 2017) and Tudisco et al. (Tudisco & Hein, 2016) generalized these theorem for4(g)1 and4 (g) p (p > 1) of graphs. In what follows, we prove that the discrete nodal domain theorem applies to4p of submodular hypergraphs.\nAs every nodal domain theorem depends on some underlying notion of connectivity, we first define the relevant notion of connectivity for submodular hypergraphs. In a graph or a homogeneous hypergraph, vertices on the same edge or hyperedge are considered to be connected. However, this property does not generalize to submodular hypergraphs, as one can merge two nonoverlapping hyperedges into one without changing the connectivity of the hyperedges. To see why this is the case, consider two hyperedges e1 and e2 that are nonintersecting. One may transform the submodular hypergraph so that it includes a hyperedge e = e1 \u222a e2 with weight we = we1 + we2 . This transformation essentially does not change the submodular hypergraph, but in the newly obtained hypergraph, according to the standard definition of connectivity, the vertices in e1 and e2 are connected. This problem may be avoided by defining connectivity based on the volume of the boundary set.\nDefinition 3.5. Two distinct vertices u, v \u2208 V are said to be connected if for any S such that u \u2208 S and v /\u2208 S, vol(\u2202S) > 0. A submodular hypergraph is connected if for any non-empty S \u2282 V , one has vol(\u2202S) > 0.\nAccording to the following lemma, it is always possible to transform the weight functions of submodular hypergraph in such a way as to preserve connectivity.\nLemma 3.6. Any submodular hypergraph G = (V,E,w,\u00b5) can be reduced to another submodular hypergraph G\u2032 = (V,E\u2032,w\u2032,\u00b5) without changing vol(\u2202S) for any S \u2286 V and ensuring that for any e \u2208 E\u2032, and u, v \u2208 e, u and v are connected.\nDefinition 3.7. Let x \u2208 RN . A positive (respectively, negative) strong nodal domain is the set of vertices of a maximally connected induced subgraph of G such that {v \u2208 V |xv > 0} (respectively, {v \u2208 V |xv < 0}). A positive (respectively, negative) weak nodal domain is defined in\nthe same manner, except for changing the strict inequalities as {v \u2208 V |xv \u2265 0} (respectively, {v \u2208 V |xv \u2264 0}).\nThe following lemma establishes that for a connected submodular hypergraph G, all nonconstant eigenvectors of the operator4p correspond to nonzero eigenvalues. Lemma 3.8. If G is connected, then all eigenvectors associated with the zero eigenvalue have constant entries.\nWe next state new nodal domain theorems for submodular hypergraph p\u2212Laplacians. The results imply the bounds for the numbers of nodal domains induced from eigenvectors of p-Laplacian do not essentially change compared to those for graphs (Tudisco & Hein, 2016). We do not consider the case p = 1, although it is possible to adapt the methods for analyzing the4(g)1 operators of graphs to41 operators of submodular hypergraphs. Such a generalization requires extensions of the critical-point theory to piecewise linear manifolds (Chang, 2016). Theorem 3.9. Let p > 1 and assume that G is a connected submodular hypergraph. Furthermore, let the eigenvalues of 4p be ordered as 0 = \u03bb(p)1 < \u03bb (p) 2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bb (p) k\u22121 < \u03bb (p) k = \u00b7 \u00b7 \u00b7 = \u03bb(p)k+r\u22121 < \u03bb (p) k+r \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bb (p) n , with \u03bb (p) k having multiplicity r. Let x be an arbitrary eigenvector associated with \u03bb(p)k . Then x induces at most k + r \u2212 1 strong and at most k weak nodal domains. Lemma 3.10. Let G be a connected submodular hypergraph. For p > 1, any nonconstant eigenvector has at least two weak (strong) nodal domains. Hence, the eigenvectors associated with the second smallest eigenvalue \u03bb(p)2 have exactly two weak (strong) nodal domains. For p = 1, the eigenvectors associated with the second smallest eigenvalue \u03bb\n(1) 2 may have only one single weak (strong) nodal domain. We define next the following three functions: \u00b5+p (x) ,\u2211 v\u2208V :xv>0 \u00b5v|xv| p\u22121, \u00b50(x) , \u2211 v\u2208V :xv=0 \u00b5v, and\n\u00b5\u2212p (x) , \u2211 v\u2208V :xv<0 \u00b5v|xv|\np\u22121. Lemma 3.11. Let G be a connected submodular hypergraph. Then, for any nonconstant eigenvector x of4p, one has \u00b5+p (x)\u2212\u00b5\u2212p (x) = 0 for p > 1, and |\u00b5+1 (x)\u2212\u00b5 \u2212 1 (x)| \u2264 \u00b50(x) for p = 1. Consequently, 0 \u2208 arg minc\u2208R \u2016x \u2212 c1\u2016p`p,\u00b5 for any p \u2265 1.\nThe nodal domain theorem characterizes the structure of the eigenvectors of the operator, and the number of nodal domains determines the approximation guarantees in Cheegertype inequalities relating the spectra of graphs and hypergraphs and the Cheeger constant. These observations are rigorously formalized in the next section."}, {"heading": "3.2. Higher-Order Cheeger Inequalities", "text": "In what follows, we analytically characterize the relationship between the Cheeger constants and the eigenvalues\n\u03bb (p) k of4p for submodular hypergraphs. Theorem 3.12. Suppose that p \u2265 1 and let (\u03bb(p)k , xk) be the k\u2212th eigenpair of the operator 4p, with mk denoting the number of strong nodal domains of xk. Then,(\n1\n\u03c4 )p\u22121( hmk p )p \u2264 \u03bb(p)k \u2264 (min{\u03b6(E), k}) p\u22121 hk,\nwhere \u03c4 = maxv dv/\u00b5v . For homogeneous hypergraphs, a tighter bound holds that reads as(\n2\n\u03c4 )p\u22121( hmk p )p \u2264 \u03bb(p)k \u2264 2 p\u22121 hk.\nIt is straightforward to see that setting p = 1 produces the tightest bounds on the eigenvalues, while the case p = 2 reduces to the classical Cheeger inequality. This motivates an in depth study of algorithms for evaluating the spectrum of p = 1, 2-Laplacians, described next."}, {"heading": "4. Spectral Clustering Algorithms for Submodular Hypergraphs", "text": "The Cheeger constant is frequently used as an objective function for (balanced) graph and hypergraph partitioning (Zhou et al., 2007; Bu\u0308hler & Hein, 2009; Szlam & Bresson, 2010; Hein & Bu\u0308hler, 2010; Hein et al., 2013; Li & Milenkovic, 2017). Theorem 3.12 implies that \u03bb(p)k is a good approximation for the k-way Cheeger constant of submodular graphs. Hence, to perform accurate hypergraph clustering, one has to be able to efficiently learn \u03bb(p)k (Ng et al., 2002; Von Luxburg, 2007). We outline next how to do so for k = 2.\nIn Theorem 4.1, we describe an objective function that allows us to characterize \u03bb(p)2 in a computationally tractable manner; the choice of the objective function is related to the objective developed for graphs in (Bu\u0308hler & Hein, 2009; Szlam & Bresson, 2010). Minimizing the proposed objective function produces a real-valued output vector x \u2208 RN . Theorem 4.3 describes how to round the vector x and obtain a partition which provably upper bounds c(S). Based on the theorems, we propose two algorithms for evaluating \u03bb\n(2) 2 and \u03bb (1) 2 . Since \u03bb (1) 2 = h2, the corresponding partition corresponds to the tightest approximation of the 2-way Cheeger constant. The eigenvalue \u03bb(2)2 can be evaluated in polynomial time with provable performance guarantees. The problem of devising good approximations for values \u03bb\n(p) k , k 6= 2, is still open.\nLet Zp,\u00b5(x, c) , \u2016x \u2212 c1\u2016p`p,\u00b5 and Zp,\u00b5(x) , minc\u2208R Zp,\u00b5(x, c), and define\nRp(x) , Qp(x)\nZp,\u00b5(x) . (5)\nTheorem 4.1. For p > 1, \u03bb(p)2 = infx\u2208RN Rp(x). Moreover, \u03bb(1)2 = infx\u2208RN R1(x) = h2. Definition 4.2. Given a nonconstant vector x \u2208 RN , and a threshold \u03b8, set \u0398(x, \u03b8) = {v : xv > \u03b8}. The optimal conductance obtained from thresholding vector x equals\nc(x) = inf \u03b8\u2208[xmin,xmax) vol(\u2202\u0398(x, \u03b8)) min{vol(\u0398(x, \u03b8)), vol(V/\u0398(x, \u03b8))} .\nTheorem 4.3. For any x \u2208 RN that satisfies 0 \u2208 arg minc Zp,\u00b5(x, c), i.e., such that Zp,\u00b5(x, 0) = Zp,\u00b5(x), one has c(x) \u2264 p \u03c4 (p\u22121)/pRp(x)1/p, where \u03c4 = maxv\u2208V dv/\u00b5v .\nIn what follows, we present two algorithms. The first algorithm describes how to minimizeR2(x), and hence provides a polynomial-time solution for submodular hypergraph partitioning with provable approximation guarantees, given that the size of the largest hyperedge is a constant. The result is concluded in Theorem 4.5. The algorithm is based on an SDP, and may be computationally too intensive for practical applications involving large hypergrpahs of even moderately large hyperedges. The second algorithm is based on IPM (Hein & Bu\u0308hler, 2010) and aims to minimize R1(x). Although this algorithm does not come with performance guarantees, it provably converges (see Theorem 4.6) and has good heuristic performance. Moreover, the inner loop of the IPM involves solving a version of the proximal-type decomposable submodular minimization problem (see Theorem 4.7), which can be efficiently performed using a number of different algorithms (Kolmogorov, 2012; Jegelka et al., 2013; Nishihara et al., 2014; Ene & Nguyen, 2015; Li & Milenkovic, 2018).\n4.1. An SDP Method for MinimizingR2(x)\nThe R2(x) minimization problem introduced in Equation (5) may be rewritten as\nmin x:Ux\u22a51\nQ2(x)\n\u2016x\u20162`2,\u00b5 , (6)\nwhere we observe that Q2(x) = \u2211 e\u2208E \u03d1ef 2 e (x) =\u2211\ne\u2208E \u03d1e maxy\u2208E(Be)\u3008y, x\u30092. This problem is, in turn, equivalent to the nonconvex optimization problem\nmin x\u2208RN \u2211 e \u03d1e ( max y\u2208E(Be) \u3008y, x\u3009 )2 (7)\ns.t. \u2211 v\u2208V \u00b5vx 2 v = 1, \u2211 v\u2208V \u00b5vxv = 0.\nFollowing an approach proposed for homogeneous hypergraphs (Louis, 2015), one may try to solve an SDP relaxation of (7) instead. To describe the relaxation, let each vertex v of the graph be associated with a vector x\u2032v \u2208 Rn,\nn \u2265 \u03b6(E). The assigned vectors are collected into a matrix of the form X = (x\u20321, .., x \u2032 N ). The SDP relaxation reads as\nmin X\u2208Rn\u00d7N , \u03b7\u2208R|E| \u2211 e \u03d1e\u03b7 2 e (8)\ns.t. \u2016Xy\u201622 \u2264 \u03b72e \u2200y \u2208 E(Be), e \u2208 E\u2211 v\u2208V \u00b5v\u2016x\u2032v\u201622 = 1, \u2211 v\u2208V \u00b5vx \u2032 v = 0.\nNote that E(Be) is of size O(|e|!), and the above problem can be solved efficiently if \u03b6(E) is small.\nAlgorithm 1 lists the steps of an SDP-based algorithm for minimizingR2(x), and it comes with approximation guarantees stated in Lemma 4.4. In contrast to homogeneous hypergraphs (Louis, 2015), for which the approximation factor equals O(log \u03b6(E)), the guarantees for general submodular hypergraphs are O(\u03b6(E)). This is due to the fact that the underlying base polytope Be for a submodular function is significantly more complex than the corresponding polytope for the homogeneous case. We conjecture that this approximation guarantee is optimal for SDP methods.\nAlgorithm 1: Minimization ofR2(x) using SDP Input: A submodular hypergraph G = (V,E,w,\u00b5) 1: Solve the SDP (8). 2: Generate a random Gaussian vector g \u223c N(0, In), where In denotes the identity matrix of order n. 3: Output x = XT g.\nLemma 4.4. Let x be as in Algorithm 1, and let the optimal value of (8) be SDPopt. Then, with high probability, R2(x) \u2264 O(\u03b6(E)) SDPopt \u2264 O(\u03b6(E)) minR2.\nThis result immediately leads to the following theorem.\nTheorem 4.5. Suppose that x is the output of Algorithm 1. Then, c(x) \u2264 O( \u221a \u03b6(E)\u03c4 h2) with high probability.\nWe describe next Algorithm 2 for optimizingR1(x) which has guaranteed convergence properties.\nTheorem 4.6. The sequence {xk} generated by Algorithm 2 satisfiesR1(xk+1) \u2264 R1(xk).\nThe computationally demanding part of Algorithm 2 is the optimization procedure in Step 3. The optimization problem is closely related to the problem of submodular function minimization (SFM) due to the defining properties of the Lova\u0301sz extension. Theorem 4.7 describes different equivalent formulations of the optimization problem in Step 3.\nTheorem 4.7. If the norm of the vector z in Step 3 is \u2016z\u20162, the underlying optimization problem is the dual of the following `2 minimization problem\nmin ye \u2016 \u2211 e\u2208E ye \u2212 \u03bb\u0302kgk\u201622, ye \u2208 \u03d1eBe, \u2200 e \u2208 E, (9)\nAlgorithm 2: IPM-based minimization ofR1(x) Input: A submodular hypergraph G = (V,E,w,\u00b5) Find nonconstant x0 \u2208 RN s.t. 0 \u2208 arg minc \u2016x0 \u2212 c1\u2016`1,\u00b5 initialize \u03bb\u03020 \u2190 R1(x0), k \u2190 0\n1: Repeat:\n2: For v \u2208 V , gkv \u2190\n{ sgn(xkv)\u00b5v, if x k v 6= 0\n\u2212\u00b5 + 1 (x k)\u2212\u00b5\u22121 (x k)\n\u00b50(xk) \u00b5v, if xkv = 0\n3: zk+1 \u2190 arg minz:\u2016z\u2016\u22641Q1(z)\u2212 \u03bb\u0302k\u3008z, gk\u3009 4: ck+1 \u2190 arg minc \u2016zk+1 \u2212 c1\u2016`1,\u00b5 5: xk+1 \u2190 zk+1 \u2212 ck+11 6: \u03bb\u0302k+1 \u2190 R1(xk+1) 7: Until |\u03bb\u0302k+1 \u2212 \u03bb\u0302k|/\u03bb\u0302k < 8. Output xk+1\nwhere the primal and dual variables are related according to z = \u03bb\u0302kgk\u2212 \u2211 e\u2208E ye\n\u2016\u03bb\u0302kgk\u2212 \u2211 e\u2208E ye\u20162 .\nIf the norm of the vector z in Step 3 is \u2016z\u2016\u221e, the underlying optimization problem is equivalent to the following SFM problem\nmin S\u2286V \u2211 e \u03d1ewe(S)\u2212 \u03bb\u0302kgk(S), (10)\nwhere the the primal and dual variables are related according to zv = 1 if v \u2208 S, and zv = \u22121 if v /\u2208 S.\nFor special forms of submodular weights, different algorithms for the optimization problems in Theorem 4.7 may be used instead. For graphs and homogeneous hypergraphs with hyperedges of small size, the min-cut algorithms by Karger et al. and Chekuri et al. (Karger, 1993; Chekuri & Xu, 2017) allow one to efficiently solve the discrete problem (10). Continuous optimization methods such as alternating projections (AP) (Nishihara et al., 2014) and coordinate descend methods (CDM) (Ene & Nguyen, 2015) can be used to solve (9) by \u201ctracking\u201d minimum norm points of base polytopes corresponding to individual hyperedges, where for general submodular weights, the Wolfe\u2019s Algorithm (Wolfe, 1976) can be used. When the submodular weights have some special properties, such as that they depend only on the cardinality of the input, there exist algorithms that operate efficiently even when |e| is extremely large (Jegelka et al., 2013).\nIn our experimental evaluations, we use a random coordinate descent method (RCDM) (Ene & Nguyen, 2015), which ensures an expected (1 + )\u2212approximation by solving an expected number ofO(|V |2|E| log 1 ) min-norm-point problems. Note that when performing continuous optimization, one does not need to solve the inner-loop optimization problem exactly and is allowed to exit the loop as long as the objective function value decreases. The underlying algorithm \u2013 Algorithm 3 \u2013 is described in the Supplement."}, {"heading": "5. Experiments", "text": "In what follows, we compare the algorithms for submodular hypergraph clustering described in the previous section to two methods: The IPM for homogeneous hypergraph clustering (Hein et al., 2013) and the clique expansion method (CEM) for submodular hypergraph clustering (Li & Milenkovic, 2017). We focus on 2-way graph partitioning problems related to the University of California Irvine (UCI) datasets selected for analysis in (Hein et al., 2013), described in Table 1 of the Supplementary Material. The datasets include 20Newsgroups, Mushrooms, Covertype. In all datasets, \u03b6(E) was roughly 103, and each of these datasets describes multiple clusters. Since we are interested in 2-way partitioning, we focused on two pairs of clusters in Covertype, denoted by (4, 5) and (6, 7), and paired the four 20Newsgroups clusters, one of which includes Comp. and Sci, and another one which includes Rec. and Talk. The Mushrooms and 20Newsgroups datasets contain only categorical features, while Covertype also includes numerical features. We adopt the same approach as the one described in (Hein et al., 2013) to construct hyperedges: Each feature corresponds to one hyperedge; hence, each categorical feature is captured by one hyperedge, while numerical features are first quantized into 10 bins of equal size, and then mapped to hyperedges. To describe the submodular weights, we fix \u03d1e = 1 for all hyperedges and parametrize we using a variable \u03b1 \u2208 (0, 0.5]\nwe(S;\u03b1) = 1\n2 +\n1 2 min { 1, |S| d\u03b1|e|e , |e/S| d\u03b1|e|e } , \u2200S \u2286 e.\nThe intuitive explanation behind our choice of weights is that it allows one to accommodate categorization errors and outliers: In contrast to the homogeneous case in which any partition of a hyperedge has weight one, the chosen submodular weights allow a smaller weight to be used when the hyperedge is partitioned into small parts, i.e., when min{|S|, |e/S|} < d\u03b1|e|e. In practice, \u03b1 is chosen to be relatively small \u2013 in all experiments, we set \u03b1 \u2264 0.04, with \u03b1 close to zero producing homogeneous hyperedge weights.\nThe results are shown in Figure 1. As may be observed, both in terms of the Clustering error (i.e., the total number of erroneously classified vertices) and the values of the Cheeger constant, IPM-based methods outperform CEM. This is due to the fact that for large hyperedge sizes, CEM incurs a high distortion when approximating the submodular weights (O(\u03b6(E)) (Li & Milenkovic, 2017)). Moreover, as we(S) depends merely on |S|, the submodular hypergraph CEM reduces to the homogeneous hypergraph CEM (Zhou et al., 2007), which is an issue that the IPM-based method does not face. Comparing the performance of IPM on submodular hypergraphs (IPM-S) with that on homogeneous hypergraphs (IPM-H), we see that IPM-S achieves better clustering performance on both 20Newsgroups and Covertypes, and offers the same performance as IPM-H on the Mushrooms dataset. This indicates that it is practically useful to use submodular hyperedge weights for clustering purposes. A somewhat unexpected finding is that for certain cases, one observes that when \u03b1 increases (and thus, when we decreases), the corresponding Cheeger constant increases. This may be caused by the fact that the IPM algorithm can get trapped in a local optima."}], "year": 2018, "references": [{"title": "Eigenvalues of the discrete p-Laplacian for graphs", "authors": ["S. Amghibech"], "venue": "Ars Combinatoria,", "year": 2003}, {"title": "Generic cuts: An efficient algorithm for optimal inference in higher order MRF-MAP", "authors": ["Arora", "Chetan", "Banerjee", "Subhashis", "Kalra", "Prem", "Maheshwari", "SN"], "venue": "In Proceedings of the European Conference on Computer Vision,", "year": 2012}, {"title": "Higher-order organization of complex", "authors": ["Benson", "Austin R", "Gleich", "David F", "Leskovec", "Jure"], "venue": "networks. Science,", "year": 2016}, {"title": "Laplacian eigenvectors of graphs", "authors": ["B\u0131y\u0131koglu", "T\u00fcrker", "Leydold", "Josef", "Stadler", "Peter F"], "venue": "Lecture notes in mathematics,", "year": 2007}, {"title": "Discrete nodal domain theorems", "authors": ["E BrianDavies", "Gladwell", "GrahamM L", "Leydold", "Josef", "Stadler", "Peter F"], "venue": "Linear Algebra and its Applications,", "year": 2001}, {"title": "Nodal domains of eigenvectors for 1-Laplacian on graphs", "authors": ["KC Chang", "Shao", "Sihong", "Zhang", "Dong"], "venue": "Advances in Mathematics,", "year": 2017}, {"title": "Spectrum of the 1-Laplacian and Cheeger\u2019s constant on graphs", "authors": ["Chang", "Kung Ching"], "venue": "Journal of Graph Theory,", "year": 2016}, {"title": "Computing minimum cuts in hypergraphs", "authors": ["Chekuri", "Chandra", "Xu", "Chao"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms,", "year": 2017}, {"title": "Spectral graph theory", "authors": ["Chung", "Fan RK"], "venue": "Number 92. American Mathematical Soc.,", "year": 1997}, {"title": "Random coordinate descent methods for minimizing decomposable submodular functions", "authors": ["Ene", "Alina", "Nguyen", "Huy"], "venue": "In Proceedings of the International Conference on Machine Learning,", "year": 2015}, {"title": "Variational methods for eigenvalue problems, volume 22", "authors": ["Gould", "Sydney Henry"], "year": 1966}, {"title": "An inverse power method for nonlinear eigenproblems with applications in 1-spectral clustering and sparse pca", "authors": ["Hein", "Matthias", "B\u00fchler", "Thomas"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"title": "The total variation on hypergraphslearning on hypergraphs revisited", "authors": ["Hein", "Matthias", "Setzer", "Simon", "Jost", "Leonardo", "Rangapuram", "Syama Sundar"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Reflection methods for user-friendly submodular optimization", "authors": ["Jegelka", "Stefanie", "Bach", "Francis", "Sra", "Suvrit"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Global min-cuts in RNC, and other ramifications of a simple min-cut algorithm", "authors": ["Karger", "David R"], "venue": "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms,", "year": 1993}, {"title": "Minimizing a sum of submodular functions", "authors": ["Kolmogorov", "Vladimir"], "venue": "Discrete Applied Mathematics,", "year": 2012}, {"title": "Inhomogeneous hypergraph clustering with applications", "authors": ["Li", "Pan", "Milenkovic", "Olgica"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Revisiting decomposable submodular function minimization with incidence relations", "authors": ["Li", "Pan", "Milenkovic", "Olgica"], "venue": "arXiv preprint arXiv:1803.03851,", "year": 2018}, {"title": "Hypergraph markov operators, eigenvalues and approximation algorithms", "authors": ["Louis", "Anand"], "venue": "In Proceedings of the ACM symposium on Theory of computing,", "year": 2015}, {"title": "Submodular functions and convexity", "authors": ["Lov\u00e1sz", "L\u00e1szl\u00f3"], "venue": "In Mathematical Programming The State of the Art,", "year": 1983}, {"title": "On spectral clustering: Analysis and an algorithm", "authors": ["Ng", "Andrew Y", "Jordan", "Michael I", "Weiss", "Yair"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2002}, {"title": "On the convergence rate of decomposable submodular function minimization", "authors": ["Nishihara", "Robert", "Jegelka", "Stefanie", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Min norm point algorithm for higher order MRF-MAP inference", "authors": ["Shanu", "Ishant", "Arora", "Chetan", "Singla", "Parag"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"title": "Total variation and cheeger cuts", "authors": ["Szlam", "Arthur", "Bresson", "Xavier"], "venue": "In Proceedings of the International Conference on Machine Learning,", "year": 2010}, {"title": "Scalable motif-aware graph clustering", "authors": ["Tsourakakis", "Charalampos E", "Pachocki", "Jakub", "Mitzenmacher", "Michael"], "venue": "In Proceedings of the 26th International Conference on World Wide Web, pp. 1451\u20131460. International World Wide Web Conferences Steering Committee,", "year": 2017}, {"title": "A nodal domain theorem and a higher-order cheeger inequality for the graph p-Laplacian", "authors": ["Tudisco", "Francesco", "Hein", "Matthias"], "venue": "arXiv preprint arXiv:1602.05567,", "year": 2016}, {"title": "A tutorial on spectral clustering", "authors": ["Von Luxburg", "Ulrike"], "venue": "Statistics and computing,", "year": 2007}, {"title": "Finding the nearest point in a polytope", "authors": ["Wolfe", "Philip"], "venue": "Mathematical Programming,", "year": 1976}, {"title": "Re-revisiting learning on hypergraphs: Confidence interval and subgradient method", "authors": ["Zhang", "Chenzi", "Hu", "Shuguang", "Tang", "Zhihao Gavin", "Chan", "T-H. Hubert"], "venue": "In Proceedings of the International Conference on Machine Learning,", "year": 2017}, {"title": "Learning with hypergraphs: Clustering, classification, and embedding", "authors": ["Zhou", "Denny", "Huang", "Jiayuan", "Sch\u00f6lkopf", "Bernhard"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2007}], "id": "SP:4ffc25f59c8b8eab903737f783da1d6bcc3fed09", "authors": [{"name": "Pan Li", "affiliations": []}, {"name": "Olgica Milenkovic", "affiliations": []}], "abstractText": "We introduce submodular hypergraphs, a family of hypergraphs that have different submodular weights associated with different cuts of hyperedges. Submodular hypergraphs arise in clustering applications in which higher-order structures carry relevant information. For such hypergraphs, we define the notion of p-Laplacians and derive corresponding nodal domain theorems and k-way Cheeger inequalities. We conclude with the description of algorithms for computing the spectra of 1and 2-Laplacians that constitute the basis of new spectral hypergraph clustering methods.", "title": "Submodular Hypergraphs: p-Laplacians, Cheeger Inequalities  and Spectral Clustering"}