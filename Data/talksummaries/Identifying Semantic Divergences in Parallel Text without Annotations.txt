3	29	This can happen for many reasons: translation lexical choice often involves selecting between near synonyms that introduce language-specific nuances (Hirst, 1995), typological divergences lead to structural mismatches (Dorr, 1994), differences in discourse organization can make it impossible to find oneto-one sentence alignments (Li et al., 2014).
7	19	We propose a semantic model to automatically detect whether a sentence pair is semantically divergent (Section 3).
9	24	Crucially, training this model requires no manual annotation.
26	26	Goutte et al. (2012) show that phrase-based systems are remarkably robust to noise in parallel segments.
34	59	In contrast, we detect semantic divergence with dedicated models that require only 5000 parallel examples (see Section 5).
35	34	This work builds on our initial study of semantic divergences (Carpuat et al., 2017), where we provide a framework for evaluating the impact of meaning mismatches in parallel segments on MT via data selection: we show that filtering out the most divergent segments in a training corpus improves translation quality.
36	31	However, we previously detect mismatches using a cross-lingual entailment classifier, which is based on surface features only, and requires manually annotated training examples (Negri et al., 2012, 2013).
37	25	In this paper, we detect divergences using a semanticallymotivated model that can be trained given any existing parallel corpus without manual intervention.
38	31	We introduce our approach to detecting divergence in parallel sentences, with the goal of (1) detecting differences ranging from large mismatches to subtle nuances, (2) without manual annotation.
39	26	Cross-Lingual Semantic Similarity Model We address the first requirement using a neural model that compares the meaning of sentences using a range of granularities.
40	51	We repurpose the Very Deep Pairwise Interaction (VDPWI) model, which has been previously been used to detect semantic textual similarity (STS) between English sentence pairs (He and Lin, 2016).
41	21	It achieved competitive performance on data from the STS 2014 shared task (Agirre et al., 2014), and outperformed previous approaches on sentence classification tasks (He et al., 2015; Tai et al., 2015), with fewer parameters, faster training, and without requiring expensive external resources such as WordNet.
43	27	We adapt the model to our cross-lingual task by initializing it with bilingual embeddings.
60	23	Specifically, candidate negative examples are generated starting from the positive examples {(ei, fi) ∀i} and taking the Cartesian product of the two sides of the positive examples{(ei, fj)∀i, j s.t.
61	20	This candidate set is filtered to ensure that negative examples are not too easy to identify: we only retain pairs that are close to each other in length (a length ratio of at most 1:2), and have enough words (at least half) which have a translation in the other sentence according to a bilingual dictionary derived from automatic word alignments.
81	27	Using the two test sets obtained above, we can evaluate the accuracy of our cross-lingual semantic divergence detector, and compare it against a diverse set of baselines in controlled settings.
84	39	Finally, we compare our model trained on synthetic examples with a supervised classifier used in prior work to predict finer-grained textual entailment categories based on manually created training examples (Section 5.5).
112	29	Table 2 shows that the semantic similarity model is most successful at distinguishing equivalent from divergent examples.
130	35	Having established the effectiveness of the semantic divergence detector, we now measure the impact of divergences on a downstream task, machine translation.
131	32	As in our prior work (Carpuat et al., 2017), we take a data selection approach, selecting the least divergent examples in a parallel corpus based on a range of divergence detectors, and comparing the translation quality of the resulting neural MT systems.
132	25	English-French We evaluate on 4867 sentences from the Microsoft Spoken Language Translation dataset (Federmann and Lewis, 2016) as well as on 1300 sentences from TED talks (Cettolo et al., 2012), as in past work (Carpuat et al., 2017).
149	55	We train English-French neural MT systems by selecting the least divergent half of the training corpus with the following criteria: • SEMANTIC SIMILARITY (Section 3) • PARALLEL: the non-parallel sentence detec- tor (Section 5.2) • ENTAILMENT: the entailment classifier (Sec- tion 5.5), as in Carpuat et al. (2017) • RANDOM: Randomly downsampling the training corpus Learning curves (Figure 1) show that data selected using SEMANTIC SIMILARITY yields better validation BLEU throughout training compared to all other models.
164	20	SEMAN- TIC SIMILARITY also yields better BLEU than RANDOM with the differences being statistically significant.
168	41	We observe that both the ENTAILMENT and SEMANTIC SIMILARITY based models have similar brevity penalties despite having performances that are at opposite ends of the spectrum in terms of BLEU.
172	58	7 This is partially due to the model’s propensity to generate a sequence of garbage tokens in the beginning of a sentence, especially while translating shorter sentences.
174	76	Only a small fraction (< 0.02%) of the French sentences in our training data begin with these tokens, but the tendency of PARALLEL to promote divergent examples above non-divergent ones, seems to exaggerate the generation of this sequence.
178	39	Importantly, our model does not require manual annotation, and can be trained for any language pair and domain with a parallel corpus.
180	54	New datasets and models introduced in this work are available at http://github.com/ yogarshi/SemDiverge.
181	71	These findings open several avenues for future work: How can we improve divergence detection further?
183	70	How do divergent examples impact other applications, including cross-lingual NLP applications and semantic models induced from parallel corpora, as well as tools for human translators and second language learners?
