{"sections": [{"text": "Proceedings of NAACL-HLT 2018, pages 177\u2013184 New Orleans, Louisiana, June 1 - 6, 2018. c\u00a92017 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Amazon has developed Alexa, a voice assistant that has been deployed across millions of devices and processes voice requests in multiple languages. This paper addresses improvements to the Alexa voice service, whose core capabilities (as measured by the number of supported intents and slots) has expanded more than four-fold over the last two years. In addition more than ten thousand voice skills have been created by third-party developers using the Alexa Skills Kit (ASK). In order to continue this expansion, new voice experiences must be both accurate and capable of supporting complex interactions.\nHowever, as the number of features has expanded, adding new features has become increasingly difficult for four primary reasons. First, requests with a similar surface form may belong to different domains, which makes it challenging to add features without degrading the accuracy of existing domains. For example, similar linguistic phrases such as \u201corder me an echo dot\u201d (e.g., for Shopping) have a similar form to phrases used for a ride-hailing feature such as, \u201cAlexa, order me\na taxi\u201d. The second challenge is that a fixed flat structure is unable to easily support certain features (Gupta et al., 2006b), such as cross-domain queries or complex utterances, which cannot be clearly categorized into a given domain. For example, \u201cFind me a restaurant near the sharks game\u201d contains both local businesses and sporting events and \u201cPlay hunger games and turn the lights down to 3\u201d requires a representation that supports assigning an utterance to two intents. The third challenge is that there is no mechanism to represent ambiguity, forcing the choice of a fixed interpretation for ambiguous utterances. For example, \u201cPlay Hunger Games\u201d could refer to an audiobook, a movie, or a soundtrack. Finally, representations are not reused between skills, leading to the need for each developer to create a custom data and representations for their voice experiences.\nIn order to address these challenges and make Alexa more capable and accurate, we have developed two key components. The first is the Alexa ontology, a large hierarchical ontology that contains fine-grained types, properties, actions and roles. Actions represent a predicate that determines what the agent should do, roles express the arguments to an action, types categorize textual mentions and properties are relations between type mentions. The second component is the Alexa Meaning Representation Language (AMRL), a graph-based domain and language independent meaning representation that can capture the meaning of spoken language utterances to intelligent assistants. AMRL is a rooted graph where action, operators, relations and classes are labeled vertices and properties and roles are labeled edges. Unlike typical representations for spoken language understanding (SLU), which factors language understanding into the prediction of intents (nonoverlapping actions) and slots (e.g., named entities) (Gupta et al., 2006a), our representation is\n177\ngrounded in the Alexa ontology, which provides a common semantic representation for spoken language understanding and can directly represent ambiguity, complex nested utterances and crossdomain queries. Unlike similar meaning representations such as AMR (Banarescu et al., 2013), AMRL is designed to be cross-lingual, explicitly represent fine-grained entity types, logical statements, spatial prepositions and relationships and support type mentions. Examples of AMRL and the SLU representations can be seen in Figure 1.\nThe AMRL has been released via Alexa Skills Kit (ASK) built-in intents and slots in 2016 at a developers conference, offering coverage for eight of the \u223c20 SLU domains 1. In addition to these domains, we have demonstrated that the AMRL can cover a wide range of additional utterances by annotating a sample from all first and thirdparty applications. We have manually annotated data for 20k examples using the Alexa ontology. This data includes the annotation of\u223c100 actions, \u223c500 types, \u223c20 roles and \u223c172 properties."}, {"heading": "2 Approach", "text": "This paper describes a common representation for SLU, consisting of two primary components: \u2022 The Alexa ontology - A large-scale hierarchi-\ncal ontology developed to cover all spoken language usage. \u2022 The Alexa meaning representation language\n(AMRL) - A rooted graph that provides a common semantic representation, is compositional and can support complex user requests.\nThese two components are described in the following sections."}, {"heading": "2.1 The Alexa ontology", "text": "The Alexa ontology provides a common semantics for SLU. The Alexa ontology is developed in RDF and consists of five primary components: \u2022 Classes A hierarchy of Classes, also re-\nferred to as types, is defined in the ontology. This hierarchy is a rooted tree, with finergrained types at deeper levels. Coarse types that are children of THING include PERSON, PLACE, INTANGIBLE, ACTION, PRODUCT, CREATIVEWORK, EVENT and ORGANIZATION. Fine-grained types include MUSICRECORDING and RESTAURANT.\n1https://amzn.to/2qDjNcJ\n\u2022 Properties A given class contains a list of properties, which relate that class to other classes. Properties are defined in a hierarchy, with finer-grained classes inheriting the properties of its parent. There are range restrictions on the available types for both the domain and range of the property. \u2022 Actions A hierarchy of actions are defined as\nclasses within the ontology. ACTIONS cover the core functionality of Alexa. \u2022 Roles ACTIONS operate on entities via roles.\nThe most common role for an ACTION is the .object role, which is defined to be the entity on which the ACTION operates. \u2022 Operators and Relations A hierarchy of op-\nerators and relations represent complex relationships that cannot be expressed easily as properties. Represented as classes, these include ComparativeOperator, Equals and Coordinator (Figure 2).\nThe Alexa ontology utilized schema.org as its base and has been updated to include support for spoken language. In addition, using schema.org as the base of the Alexa Ontology means that it shares a vocabulary used by more than 10 million websites, which can be linked to the Alexa ontology."}, {"heading": "2.2 Alexa meaning representation language", "text": "AMRL leverages classes, properties, actions, roles and operators in the main ontology to create a compositional, graph-based representation of the meaning of an utterance. The graph-based representation conceptualizes each arc as a property and each node as an instance of a type; each type can have multiple parents. Conventions have been developed to annotate the AMRL for an utterance accurately and consistently. These conventions focus primarily on linguistic annotation, and only consider filled pauses, edits, and repairs in limited contexts. The conventions include: \u2022 Fine-grained type mentions When an entity\ntype appears in an utterance, the most finegrained type will be annotated. For \u201cturn on the light\u201d, the mention \u2018light\u2019 could be annotated as a DEVICE. However, there is a more appropriate finer-grained type, LIGHTING which will be selected instead. \u2022 Ambiguous type mentions When more than\none fine-grained type is possible, then the annotator will utilize a more coarse-grained\ntype in the hierarchy. This type should be the finest-grained type that still captures the ambiguity. For example, in the utterance \u201cplay thriller\u2019, \u201cthriller\u201d can either be a MUSICALBUM or a MUSICRECORDING. Instead of selecting one of these a more coarse-grained type of MUSICCREATIVEWORK will be chosen. When the ambiguity would force fallback to the root class of the ontology THING, AMRL annotation chooses a sub-class and marks the usage of it as uncertain. \u2022 Properties Properties are annotated when\nthey are unambiguous. For example, \u201cfind books by truman capote\u201d, the use of the .author property on the BOOK class is unambiguous. Similarly, for \u201cfind books about truman capote\u201d the use of the .about property on the BOOK class is unambiguous. \u2022 Ambiguous property usage When there is\nuncertainty in the property that should be selected for the representation, the annotator may fall back to a more generic property. \u2022 Property inverses When a property can\nbe annotated in two different directions, a canonical property is defined in the ontology and used for all annotations. For example, .parentOrganization has an inverse of .subOrganization. The former is selected as canonical for annotation flexibility and to\neliminate cycles in the graph.\nA few of these properties have special meaning at annotation time. Specifically, for the annotation of textual mentions there exist three primary properties: .name, .value and .type. The conventions for these properties are as follows:\n\u2022 .name This is a nominal mention in the utterance, the .name property links the text to an instance of a class. .name is only used for mentions that are not a numeric quantity or enumeration. An example of .name for a MUSICIAN class would be \u201cmadonna\u201d. \u2022 .value This is defined in the same way as\n.name but is used for mentions that are numeric quantities or enumerations. For instance, \u201ctwo\u201d would be a .value of an INTEGER class. \u2022 .type This is a generic mention of an entity\ntype. For example, \u201cmusician\u201d is a .type mention of the MUSICIAN class.\nOne action (NULLACTION) has a special meaning. This is annotated whenever a SLU query does not have an associated action or the action is unclear. This happens, for example, when someone says, \u201ctemperature\u201d. In contrast, \u201cshow me the temperature\u201d is annotated with the more specific DISPLAYACTION."}, {"heading": "2.3 Expanded Language Support", "text": "AMRL has been used to represent utterances that are either not supported or challenging to support using standard SLU representations. The following section describes support for anaphora, complex and cross-domain utterances, referring expressions for locations and composition."}, {"heading": "2.3.1 Anaphora", "text": "AMRL can natively support pronominal anaphora resolution both within the same utterance or across utterances. For example: \u2022 Within utterance: \u201cFind the highest-rated\ntoaster and show me its reviews\u2019\u2019 \u2022 Across utterances: \u201cWhat is Madonna\u2019s lat-\nest album\u201d \u201cPlay it.\u201d Terminal nodes refer back to the same (unique) entity. An example annotation across multiple utterances can be seen in Figures 3a and 3b. Similar to the above, it can handle bridges within discourse, such as, \u201cfind me an italian restaurant\u201d and \u201cwhat\u2019s on its menu.\u201d"}, {"heading": "2.3.2 Inferred nodes", "text": "AMRL contains nodes that are not grounded in the text. For example, for the utterance, in Figure 2a there are two inferred nodes, one for the address of the restaurant and another for the address of the sports event. Not explicitly representing types has two primary benefits. First, certain linguistic phenomena such as anaphora are easier to support. Second, the representation is aligned to the ontology, which enables direct queries against the knowledge base. Inferred nodes are the AMRL way to perform reification."}, {"heading": "2.3.3 Cross-domain utterances", "text": "Using the common semantics of AMRL means that parses do not need to obey domain boundaries. For example, these utterances would belong to two domains (e.g., sports and local search): \u201cWhere is the nearest restaurant\u201d and \u201cWhat is happening at the Sharks game\u201d. AMRL, as in Figure 2a, can handle utterances that span multiple domains, such as the one shown in Figure 2a."}, {"heading": "2.3.4 Conjunctions, disjunctions and negations", "text": "AMRL can cover logical expressions, where there can be an arbitrary combination of conjunctions, disjunctions, or conditional statements. Some examples of object-level or clause-level conjunctions\ninclude: \u2022 Object-level conjunction: \u201cAdd milk, bread,\nand eggs to my shopping list\u201d \u2022 Clause-level conjunction: \u201cRestart this song\nand turn the volume up to seven\u201d Conjunctions and disjunctions are represented using a Coordinator class. The \u201c.value\u201d property defined which logical operation is to be performed. Examples of the AMRL representation for these is shown in Figure 2b and 2c."}, {"heading": "2.3.5 Conditional statements", "text": "Conditional statements are not usually represented in other formalisms. An example of a conditional statement is, \u201cwhen its raining, turn off the sprinklers\u201d. Time-based conditional statements are special cased due to their frequency in spoken language. For time-based expressions (e.g., \u201cwhen it is three p.m., turn on the lights\u201d), a startTime (or endTime) property is used on the action to denote the condition of when the action should start (or stop). For all other expressions, we use the ConditionalOperator, which has a \u201ccondition\u201d property as well as a \u201cresult\u201d property. When the condition is true, then the result would apply. The constrained properties are defining the arguments of the Equals operator. An example can be seen in Figure 4. A deterministic transformation from the simplified time-based scheme to ConditionalOperator form when greater consistency is desired."}, {"heading": "2.3.6 Referring expressions for locations", "text": "AMRL can represent locations and their relationships. For simpler expressions that are common, such as \u201con\u201d or \u201cin,\u201d properties are used to represent the relationship between two entity mentions. For other spatial relations, such as \u201cbetween\u201d or \u201caround,\u201d an operator is introduced. Two examples of spatial relationships can be seen in Figure 2d. In this example \u201cbeside\u201d grounds to the relation being used (e.g., \u201cbeside\u201d) and uses two properties (e.g., constrained and target), which are the the first and second arguments to the spatial preposition."}, {"heading": "2.3.7 Composition", "text": "AMRL supports composition, which enables reuse of types and subgraphs to represent utterances with similar meanings. For example, Figures 2e and 2f show the ability to create significantly different actions only by changing the type of the object of the utterance. Such substitution can occur\nanywhere in the annotation graph. PlaybackAction is used to denote playing of the entity referred to by the object role."}, {"heading": "2.3.8 Unsupported features", "text": "Although many linguistic phenomena can be supported in AMRL, there are a few that have not been explicitly supported and are left for future work. These include existential and universal quantification and scoping and conventions for agency (most requests are imperative). In addition, there is currently no easy way to convert to first order logic (e.g., lambda calculus), due to conventions that simplify annotation, but lose information about operators such as spatial relationships."}, {"heading": "3 Dataset", "text": "Data has been collected for the AMRL across many spoken language use-cases. The current domains that are supported include music, books, video, local search, weather and calendar. We have prototyped mechanisms to speed up annotation via paraphrasing (Berant and Liang, 2014) and conversion from our current SLU representation, in order to leverage the much larger data available. The primary mechanism we have for data-acquisition is via manual annotation. Tools have been developed in order to acquire the full graph annotated with all the properties, classes, actions and operators.\nAMRL manual annotation is performed by data annotators in four stages. In the first stage an action is selected, for example ACTIVATEACTION in Figure 1b. The second stage defines the text spans in an utterance that link to a class in the ontology (e.g., \u201cmichael jackson\u201d is a Musician type and \u201cthriller\u201d and \u201csong\u201d are MusicRecording types, the first is a .name mention, while the latter is a .type mention. The third stage creates connections between the classes and defines any missing nodes in the graph. In the final stage a skilled annotator reviews the graph for mistakes and and re-annotates it if necessary. There is a visualization of the semantic annotation available, enabling an annotator to verify that they have built the graph in a semantically accurate manner. Manual annotation happens at the rate of 40 per hour. The manually annotated dataset contains\u223c20k annotated utterances and contains 93 unique actions,\n448 types, 172 properties and 23 roles."}, {"heading": "4 Parsing", "text": "Any graph parsing method can be used to predict AMRL given a natural language utterance. One approach is to use hyperedge replacement grammars (Chiang et al., 2013) (Peng et al., 2015), though these require large datasets in order to train accurate parsers. Alternatively, the graph can be linearized, as in (Gildea et al., 2017) and sequence to sequence or sequential models can be used to predict AMRL (Perera and Strubell, 2018). We have shown that AMRL full-parse accuracy is at 78%, though the serialization, use of embeddings from related tasks can improve parser accuracy. More details can be found in (Perera and Strubell, 2018)."}, {"heading": "5 Related Work", "text": "FreeBase (Bollacker et al., 2008) (now WikiData) and schema.org (Guha et al., 2016) are two common ontologies. Schema.org is widely used on the web and contains actions, types and properties. The Alexa ontology expands schema.org to cover types, properties and roles used in spoken language.\nSemantic parsing has been investigated in the content of small domain-specific datasets such as GeoQuery (Wong and Mooney, 2006) and in the context of larger broad-coverage representations such as the Groningen Meaning Bank (GMB) (Bos et al., 2017), the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), PropBank (Kingsbury and Palmer, 2002), Raiment (Baker et al., 1998) and lambda-DCS (Kingsbury and Palmer, 2002). OntoNotes (Hovy et al., 2006), lambdaDCS s (Liang, 2013) (Baker et al., 1998), FrameNet (Baker et al., 1998), combinatory categorial grammars (CCG) (Steedman and\nBaldridge, 2011) (Hockenmaier and Steedman, 2007), universal dependencies (Nivre et al., 2016) are all related representations. A comparison of semantic representations for natural language semantics is described in Abend and Rappoport. Unlike these meaning representations for written language, AMRL covers question answering, imperative actions, and a wide range of new types and properties (e.g., smart home, timers, etc.).\nAMR and AMRL are both rooted, directed, leaf-labeled and edge-labeled graphs. AMRL does not reuse PropBank frame arguments, covers predicate-argument relations, including a wide variety of semantic roles, modifiers, co-reference, named entities and time expressions (Banarescu et al., 2013). There are more than 1000 namedentity types in AMRL (AMR has around 80). Reentrancy is not used in AMRL notation. In addition to the AMR \u201cname\u201d property, AMRL contains a \u201ctype\u201d property for mentions of a type (or class) and a \u201cvalue\u201d property for the mention of numeric values. Anaphora is handled in AMRL for spoken dialog Poesio and Artstein (Gross et al., 1993). Unlike representations used for spoken language understanding (SLU) (Gupta et al., 2006b), AMRL represents both entity spans, complex natural language expressions, and fine-grained named-entity types."}, {"heading": "6 Conclusions and Future Work", "text": "This paper develops AMRL, a meaning representation for spoken language. We have shown how it can be used to expand the set of supported usecases to complex and cross-domain utterances, while leveraging a single compositional semantics. The representation has been released at AWS Re:Invent 2016 2. It is also being used as a representation for expanded support for complex utterances, such as those with sequential composi-\n2https://amzn.to/2qDjNcJ\ntion. Continued development of a common meaning representation for spoken language will enable Alexa to become capable and accurate, expanding the set of functionality for all Alexa users."}], "year": 2018, "references": [{"title": "Freebase: A collab", "authors": ["Sturge", "Jamie Taylor"], "year": 2008}, {"title": "Addressing the data sparsity", "authors": ["Chuan Wang"], "year": 2017}, {"title": "The trains 91 dialogues", "authors": ["R.V. Guha", "Dan Brickley", "Steve Macbeth"], "year": 2016}, {"title": "The at&t spo", "authors": ["G. Riccardi", "M. Gilbert"], "year": 2006}, {"title": "Multi-task learning for parsing the alexa meaning representation language", "authors": ["Vittorio Chung Tagyoung Kollar Thomas Perera", "Emma Strubell."], "venue": "American Association for Artificial Intelligence (AAAI).", "year": 2018}, {"title": "Anaphoric annotation in the arrau corpus", "authors": ["Massimo Poesio", "Ron Artstein."], "venue": "LREC.", "year": 2008}, {"title": "Combinatory Categorial Grammar, Wiley-Blackwell", "authors": ["Mark Steedman", "Jason Baldridge"], "year": 2011}, {"title": "Learning for semantic parsing with statistical machine translation", "authors": ["Yuk Wah Wong", "Raymond J Mooney."], "venue": "Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of", "year": 2006}], "id": "SP:87cbba4bf46b58a0db32b38ca7c53d9bfdd030ed", "authors": [{"name": "Thomas Kollar", "affiliations": []}, {"name": "Danielle Berry", "affiliations": []}, {"name": "Lauren Stuart", "affiliations": []}, {"name": "Karolina Owczarzak", "affiliations": []}, {"name": "Tagyoung Chung", "affiliations": []}, {"name": "Lambert Mathias", "affiliations": []}, {"name": "Michael Kayser", "affiliations": []}, {"name": "Bradford Snow", "affiliations": []}, {"name": "Spyros Matsoukas", "affiliations": []}], "abstractText": "This paper introduces a meaning representation for spoken language understanding. The Alexa meaning representation language (AMRL), unlike previous approaches, which factor spoken utterances into domains, provides a common representation for how people communicate in spoken language. AMRL is a rooted graph, links to a large-scale ontology, supports cross-domain queries, finegrained types, complex utterances and composition. A spoken language dataset has been collected for Alexa, which contains \u223c 20k examples across eight domains. A version of this meaning representation was released to developers at a trade show in 2016.", "title": "The Alexa Meaning Representation Language"}