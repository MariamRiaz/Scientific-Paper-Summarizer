21	54	These unique advantages make minwise hashing arguably the strongest hashing algorithm both in theory and practice.
22	49	Hashing Cost is Bottleneck: The first step of algorithms relying on minwise hashing is to generate, some large enough, k minwise hashes (or fingerprints) of the data vectors.
23	16	In particular, for every data vector x, hi(x) ∀i ∈ {1, 2, ..., k} is repeatedly computed with independent permutations (or hash functions).
30	33	Other Related Fast Sketches are not LSH: Two notable techniques for estimating Jaccard Similarity are: 1) bottom-k sketches and 2) one permutation hashing (Li et al., 2012).
33	33	The Idea of “Densified” One Permutation Hashing: Recently, (Shrivastava & Li, 2014a) showed a technique for densifying sparse sketches from one permutation hashing which provably removes the bias associated with one per- mutation hashing.
44	17	Our Contributions: We show that the existing densification schemes, for fast minwise hashing, are not only suboptimal but, worse, their variances do not go to zero with increasing number of hashes.
50	15	Our proposal makes novel use of 2-universal hashing which could be of independent interest in itself.
57	141	Notations like V ar(h+), will mean the variance of the above estimator when the h+ is used as the hash function.
69	44	(5) An obvious computational advantage of this scheme is that it is likely to generate many hash values, at most k, and only requires one permutation π and only pass over the sets (or binary vectors) S. It was shown that for any two sets S1 and S2 we have a conditional collision probability similar to minwise hashing.
75	31	Unfortunately, whenever the outcome of the random permutation leads to simultaneous empty bins, i.e. event Ei = 1, the LSH Property is not valid.
84	105	Since the positions of empty and non-empty bins were random, it was shown that densification (or reassignment) was equivalent to a stochastic reselection of one hash from a set of existing informative (coming from non-empty bins) hashes which have the LSH property.
85	21	This kind of reassignment restores the LSH property and collision probability for any two hashes, after reassignment, is exactly same as that of minwise hashing.
88	18	O(d+ k) led to an algorithmic improvement over randomized algorithms relying on minwise hashing, as hash computation cost is bottleneck step in all of them.
90	16	In particular, the probability of two empty bins borrowing the information of the same nonempty bin was significantly higher.
100	19	In particular, we have the following theorem about the limiting variances of existing techniques: Theorem 1 Give any two finite sets S1, S2 ∈ Ω, with A = |S1 ∪ S2| > a = |S1 ∩ S2| > 0 and |Ω| = D → ∞.
109	56	For given set S, the densification process reassigns every empty bin with a value from one of the existing non-empty bins.
118	31	Even though there are other informative bins (Bins 1, 6 and 7), their information is never used.
119	36	This local bias increases the probability (p) that two empty bins get tied to the same information, even if there are many other informative non-empty bins.
150	20	The key is to use a 2-universal hashing huniv : [k] × N → [k] which takes two arguments: 1) The current bin id that needs to be reassigned and 2) the number of failed attempt made so far to reach a non-empty bin.
152	15	So even if we reach the same non-empty bin back (cycle), the next time we will visit a new set of bins.
153	31	Also, even if both i and j = huniv(i, attempti) are empty, i and j are not bound to end to the same non-empty bin.
160	37	Note: The variance can be reduced if we allow correlations in the assignment process, for example if we force bin i and bin j to not pick the same bin during reassignments, this will reduce p beyond the perfectly random load balancing value of 1m .
195	21	It should be noted that since all three schemes have the LSH Property, the bias is zero and hence the MSE is the theoretical variance.
199	17	Conclusion 1: The proposed densification is significantly more accurate, irrespective of the choice of sparsity and similarity, than the existing densification schemes especially for large k. Note the y-axis of plots is on log scale, so the accuracy gains are drastic.
204	14	To compute the runtime, we use three publicly available text datasets: 1) RCV1, 2) URL and 3) News20.
207	32	We implemented three methodologies for computing hashes: 1) Densification Scheme h+, 2) The Proposed h∗ (Algorithm 1 and 3) Vanilla Minwise Hashing.
213	82	The time include the end-to-end hash computation of the complete data.
216	60	All the experiments were done on Intel i7-6500U processor laptop with 16GB RAM.
217	19	Also, to get an estimate of the importance of densification, we also show the average number of empty bins generated by only using one permutation hashing and report the numbers in Table 4.
220	30	However, optimal densification is significantly more accurate.
