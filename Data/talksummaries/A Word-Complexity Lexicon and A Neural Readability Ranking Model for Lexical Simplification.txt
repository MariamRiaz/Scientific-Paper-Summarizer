0	51	Lexical simplification is an important subfield that is concerned with the complexity of words or phrases, and particularly how to measure readability and reduce the complexity using alternative paraphrases.
1	38	There are three major lexical simplification tasks which effectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substitution Generation (Glavaš and Štajner, 2015; Coster and Kauchak, 2011) which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity.
2	47	Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013), non-native speakers (Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014), lay readers (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010), or people with reading disabilities (Rello et al., 2013).
6	100	For instance, the word foolishness is simpler than its meaningpreserving substitution folly even though foolishness is longer and less frequent in the Google 1T Ngram corpus (Brants and Franz, 2006).
8	14	To alleviate these inevitable shortcomings of corpus and surface-based methods, we explore a simple but surprisingly unexplored idea – creating an English lexicon of 15,000 words with wordcomplexity ratings by humans.
9	110	We also propose a new neural readability ranking model with a Gaussian-based feature vectorization layer, which can effectively exploit these human ratings as well as other numerical features to measure the complexity of any given word or phrase (including those outside the lexicon and/or with sentential context).
11	23	We also apply the new ranking model to identify lexical simplifications (e.g., commemorate→ celebrate) among the large number of paraphrase rules in PPDB with improved accuracy compared to previous work for Substitution Generation.
13	41	We make our code, the wordcomplexity lexicon, and a lexical resource of over 10 million paraphrase rules with improved readability scores (namely SimplePPDB++) all publicly available.
14	18	We first constructed a lexicon of 15,000 English words with word-complexity scores assessed by human annotators.3 Despite the actual larger English vocabulary size, we found that rating the most frequent 15,000 English words in Google 1T Ngram Corpus4 was effective for simplification purposes (see experiments in §4) as our neural ranking model (§3) can estimate the complexity of any word or phrase even out-of-vocabulary.
15	109	We asked 11 non-native but fluent English speakers to rate words on a 6-point Likert scale.
16	124	We found that an even number 6-point scale worked better than a 5-point scale in a pilot experiment with two annotators, as the 6-point scheme allowed annotators to take a natural two-step approach: first determine whether a word is simple or complex; then decide whether it is ‘very simple’ (or ‘very complex’), ‘simple’ (or ‘complex’), or ‘moderately simple’ (or ‘moderately complex’).
18	14	We also asked the annotators to indicate the words for which they had trouble assessing their complexity due to ambiguity, lack of context or any other reason.
25	67	For the majority of the disagreements, the ratings of one annotator and the mean of the rest were fairly close: the difference is ≤ 0.5 for 47% of the annotations; ≤ 1.0 for 78% of the annotations; and ≤ 1.5 for 93% of the annotations on the 6-point scale.
27	54	In order to predict the complexity of any given word or phrase, within or outside the lexicon, we propose a Neural Readability Ranking model that can leverage the created word-complexity lexicon and take context (if available) into account to further improve performance.
45	17	For an input pair of words/phrases 〈wa, wb〉, we include individual features f(w1), f(w2) and the differences f(wa)−f(wb).
46	25	We also use pairwise features f(〈wa, wb〉) including cosine similarity cos(−→w a,−→w b) and the difference −→w a−−→w b between the word2vec (Mikolov et al., 2013) embedding of the input words.
49	62	Our model relies primarily on numerical features as many previous approaches for lexical simplification.
51	16	We adopt a smooth binning approach and project each numerical feature into a vector representation by applying multiple Gaussian radial basis functions.
54	18	We specify σ as a fraction γ of bin width: σ = 1 k (fmax − fmin) · γ (1) where γ is a tunable hyperparameter in the model.
59	17	The training objective is to minimize the Mean Squared Error (MSE): L(θ) = 1 m m∑ i=1 (yi − ŷi)2 (3) where yi and ŷi are the true and predicted relative complexity scores of 〈wa, wb〉 which can be configured accordingly for different lexical simplification tasks and datasets, m is the number of training examples, and θ is the set of parameters of the NRR model.
61	22	We set the rate to 0.0005 and 0.001 for experiments in (§4.1) and (§4.2) respectively.
65	24	Given an instance consisting of a target complex word in a sentence and a set of candidate substitutions, the goal of the Substitution Ranking task is to rank the candidates in the order of their simplicity.
68	38	The training and test sets consist of 300 and 1,710 instances, respectively, with a total of 201 target words (all single word, mostly polysemous) and each in 10 different sentences.
69	66	One example of such instance contains a target complex word in context: When you think about it, that’s pretty terrible.
70	18	and a set of candidate substitutions {bad, awful, deplorable}.
76	40	Given a test instance with candidate set C, we rank the candidates as follows: for every pair of candidates 〈ca, cb〉, the model predicts the relative complexity score S(ca, cb); we then compute a single score R(ca) = ∑ ca 6=cb∈C S(ca, cb) for each candidate by aggregating pairwise scores and rank the candidates in the increasing order of these scores.
78	46	We compare with the state-of-the-art neural model (Paetzold and Specia, 2017) for substitution ranking with the best reported results on the SemEval 2012 dataset.
89	29	We also conducted ablation experiments to show the effectiveness of the Gaussianbased feature vectorization layer (+binning) and the word-complexity lexicon (+WC).
90	81	We also can apply our NRR model to rank the lexical and phrasal paraphrase rules in the Paraphrase Database (PPDB) (Pavlick et al., 2015), and identify good simplifications (see examples in Table 3).
93	17	In next section, we demonstrate the utility of SimplePPDB++ for the Substitution Generation task.
