0	30	Semantic graphs, such as WordNet (Fellbaum, 1998), encode the structural qualities of language as a representation of human knowledge.
1	122	On the local level, they describe connections between specific semantic concepts, or synsets, through individual edges representing relations such as hypernymy (‘is-a’) or meronymy (‘is-part-of’); on the global level, they encode emergent regular properties in the induced relation graphs.
2	35	Local properties have been subject to extensive study in recent years via the task of relation prediction, where individual edges are found based mostly on distributional methods that embed synsets and relations into a vector space (e.g. Socher et al., 2013; Bordes et al., 2013; Toutanova and Chen, 2015; Neelakantan et al., 2015).
4	26	In this paper, we show how global semantic graph features can facilitate in local tasks such as relation prediction.
9	20	In (c), an impossible situation arises: a cycle in the hypernym graph leads each of the participating synsets to be predicted by transitivity as its own hypernym, contrary to the relation’s definition.
10	28	However, a purely local model has no explicit mechanism for rejecting such an outcome.
52	87	The ERGM scoring function defines a probability over G|V |, the set of all graphs with |V | nodes.
54	17	Features are typically counts of motifs — small subgraph structures — as described in the introduction.
57	13	Semantic graphs are composed of multiple relation types, which the feature space needs to accommodate; their nodes are linguistic constructs (semantic concepts) associated with complex interpretations, which can benefit the graph representation through incorporating their embeddings in Rd into a new scoring model.
59	18	Based on common practice in ERGM feature extraction (e.g., Morris et al., 2008), we select the following graph features as a basis: • Total edge count; • Number of cycles of length k, for k ∈ {2, 3}; • Number of nodes with exactly k outgoing (incoming) edges, for k ∈ {1, 2, 3}; • Number of nodes with at least k outgoing (incoming) edges, for k ∈ {1, 2, 3}; • Number of paths of length 2; • Transitivity: the proportion of length-2 paths u → v → w where an edge u → w also exists.
68	14	• A combinatory ‘2-outgoing’ feature will be extracted for the number of nodes with exactly one derivation and one has part.
79	25	One solution is to approximate probability using a variant of the Monte Carlo Maximum Likelihood Estimation (MCMLE) produce, logP (G) ≈ logψ(G)− log |G|V || M M∑ G̃∼G|V | ψ(G̃), (3) where M is the number of networks G̃ sampled from G|V |, the space of all (multirelational) edge sets on nodes V .
80	55	Each G̃ is referred to as a negative sample, and the goal of estimation is to assign low scores to these samples, in comparison with the score assigned to the observed network G. Network samples can be obtained using edgewise negative sampling.
81	18	For each edge s r−→ t in the training network G, we remove it temporarily and consider T alternative edges, keeping the source s and relation r constant, and sampling a target t̃ from a proposal distribution Q.
88	22	The proposal distribution Q used to sample negative edges is defined to be proportional to the local association scores of edges not present in the training graph: Q(t̃ | s, r,G) ∝ { 0 s r−→ t̃ ∈ G A(r)(s, t̃) s r−→ t̃ /∈ G .
89	14	(7) By preferring edges that have high association scores, the negative sampler helps push the M3GM parameters away from likely false positives.
99	24	We report the following metrics, common in ranking tasks and in relation prediction in particular: MR, the Mean Rank of the desired entity; MRR, Mean Reciprocal Rank, the main evaluation metric; and H@k, the proportion of Hits (true entities) found in the top k of the lists, for k ∈ {1, 10}.
103	95	All other models revert to this baseline for the four symmetric relations.
110	46	We add a variant to this protocol where the graph score and association score are weighted by α and 1 − α, repsectively, before being summed.
115	24	Our default setting backpropagates loss into only the graph weight vector θ.
120	26	A straightforward way of using word embeddings to create synset embeddings is to collect the words representing the synset as surface form within the WordNet dataset and average their embeddings (Socher et al., 2013).
126	16	We do not modify the spelling conventions of WordNet synsets before passing them to Mimick, so e.g. ‘mask.n.02’ (the second synset corresponding to ‘mask’ as a noun) acts as the input character sequence as is.
132	16	M3GM is trained in four epochs using AdaGrad with η = 0.1.
143	37	This could be the result of the greater edge density of the combined training and dev graphs, which enhance the global coherence of the graph structure captured by M3GM features.
144	14	To support this theory, we tested the M3GM model trained on only the training set, and its test set performance was roughly one point worse on all metrics, as compared with the model trained on the training+dev data.
156	17	Lines 3, 6, and 7 hint at deeper interactions between the different relation types.
167	15	This paper presents a novel method for reasoning about semantic graphs like WordNet, combining the distributional coherence between individual entity pairs with the structural coherence of network motifs.
173	67	Our model is capable of scoring bundles of new edges, and in future work, we plan to explore the possibility of combining M3GM with a search algorithm, to automatically extend existing knowledge graphs by linking in one or more new entities.
175	27	To some extent, the structural parameters estimated by M3GM are not specific to English: for example, hypernymy cannot be symmetric in any language.
176	26	If the structural parameters estimated from English WordNet are transferable to other languages, then the combination of M3GM and multilingual word embeddings could facilitate the creation and extension of large-scale semantic resources across many languages (Fellbaum and Vossen, 2012; Bond and Foster, 2013; Lafourcade, 2007).
