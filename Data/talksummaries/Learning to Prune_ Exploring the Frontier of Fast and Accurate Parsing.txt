0	16	Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing (Pauls and Klein, 2009; Caraballo and Charniak, 1998) and machine translation (Petrov et al., 2008; Xu et al., 2013).
1	16	Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference.
2	10	For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features.
3	14	Without heuristics, these parsers take minutes to process a single sentence.
4	14	To speed up inference, we will learn a pruning policy.
6	11	Our approach searches for a policy with maximum end-to-end performance (reward) on training data, where the reward is a linear combination of problemspecific measures of accuracy and runtime, namely reward = accuracy−λ · runtime.
7	56	The parameter λ ≥ 0 specifies the relative importance of runtime and accuracy.
8	8	By adjusting λ, we obtain policies with different speed-accuracy tradeoffs.
16	17	Our experiments show that accounting for end-to-end performance in training leads to better policies along the entire Pareto frontier of accuracy and runtime.
17	10	A simple yet effective approach to speeding up parsing was proposed by Bodenstab et al. (2011), who trained a pruning policy π to classify whether or not spans of the input sentence w1 · · ·wn form plausible 263 Transactions of the Association for Computational Linguistics, vol.
23	9	1 provides pseudocode for weighted CKY with pruning.
24	73	Weighted CKY aims to find the highestscoring derivation (parse tree) of a given sentence, where a given grammar specifies a non-negative score for each derivation rule and a derivation’s score is the product of the scores of the rules it uses.1 CKY uses a dynamic programming strategy to fill in a three-dimensional array β, known as the chart.
28	24	The chart is initialized with lexical grammar rules (lines 3–9), which derive words from grammar symbols.
39	25	To estimate the value of a pruning policy π, we call PARSE(G,w(i), π) on each training sentence w(i), and apply the reward function, r = accuracy−λ · runtime.
40	7	The empirical value of a policy is its average reward on the training set: R(π) = 1 m m∑ i=1 E [ r(PARSE(G,w(i), π)) ] (1) The expectation in the definition may be dropped if PARSE, π, and r are all deterministic, as in our setting.2 Our definition of r depends on the user parameter λ ≥ 0, which specifies the amount of accuracy the user would sacrifice to save one unit of runtime.
42	154	End-to-end training gives us a principled way to decide what to prune.
43	57	Rather than artificially labeling each pruning decision as inherently good or bad, we evaluate its effect in the context of the particular sentence and the other pruning decisions.
44	58	Actions that prune a gold constituent are not equally bad—some cause cascading errors, while others are “worked around” in the sense that the grammar still selects a mostly-gold parse.
45	43	Similarly, actions that prune a non-gold constituent are not equally good—some provide more overall speedup (e.g., pruning narrow constituents prevents wider ones from being built), and some even improve accuracy by suppressing an incorrect but high-scoring parse.
47	47	Yet our approach can still be used in such settings, by evaluating the reward on the downstream task that the latent structure serves.
48	74	Past work on optimizing end-to-end performance is discussed in §8.
49	36	One might try to scale these techniques to learning to prune, but in this work we take a different approach.
51	28	Given a batch of improved action sequences (trajectories), the remaining step is to search for a policy which produces the improved trajectories.
52	13	Conveniently, this can be reduced to a classification problem, much like the asymmetric weighting approach, except that the supervised labels and misclassification costs are not fixed across iterations, but rather are derived from interaction with the environment (i.e., PARSE and the reward function).
53	37	This idea is formalized as a learning algorithm called Locally Optimal Learning to Search (Chang et al., 2015b), described in §4.
54	91	The counterfactual interventions we require— evaluating how reward would change if we changed one action—can be computed more efficiently using our novel algorithms (§5) than by the default strategy of running the parser repeatedly from scratch.
55	15	The key is to reuse work among evaluations, which is possible because LOLS only makes tiny changes.
56	11	Pruned inference is a sequential decision process.
59	78	1 at line 10, after the chart has been initialized from some selected sentence.
61	32	Eventually the parser reaches some state s1 from which it calls the policy to choose action a1 = π(s1), and so on.
