6	21	Yet the literature disagrees about which models are strongest (Weeds et al., 2014; Roller et al., 2014), or even if they work at all (Levy et al., 2015).
10	132	We find the model overwhelmingly learns to identify hypernyms using Hearst patterns available in the distributional space, like “animals such as cats” and “animals including cats.” These patterns have long been used to identify lexical relations (Hearst, 1992; Snow et al., 2004).
19	73	Formally, we consider methods which treat lexical entailment as a supervised classification problem, which take as input the distributional vectors for a pair of words, (H, w), and predict on whether the antecedent w entails the consequent H .2 One of the earliest supervised approaches was Concat (Baroni et al., 2012).
52	33	The data contains 12,600 annotations, but only 945 positive examples encompassing various relations like hypernymy, meronomy, synonymy and contextonymy.3 This makes it one of the most difficult data sets: it is both domain specific and highly unbalanced.
59	28	In all experiments, we use a standard, count-based, syntactic distributional vector space.
62	28	We compute a syntactic distributional space for the 250k most frequent lemmas by counting their dependency neighbors across the corpus.
67	26	As discussed in Section 2, the Concat classifier is a classifier trained on the concatenation of the word vectors, 〈H, w〉.
68	22	As additional background, we first review the findings of Levy et al. (2015), who showed that Concat trained using a linear classifier is only able to capture notions of prototypicality; that is, Concat guesses that (animal, sofa) is a positive example because animal looks like a hypernym.
69	23	Formally, a linear classifier like Logistic Regression or Linear SVM learns a decision hyperplane represented by a vector p̂.
71	30	Crucially, since the input features are the concatenation of the pair vectors 〈H, w〉, the hyperplane p̂ vector can be decomposed into separate H and w components.
74	21	Without any interaction terms, the Concat classifier has no way of estimating the relationship between the two words, and instead only makes predictions based on two independent terms, Ĥ and ŵ, the prototypicality vectors.
89	21	This co-occurrence matrix is factorized using Singular Value Decomposition, producing both W , the ubiquitous word-embedding matrix, and C, the contextembedding matrix (Levy and Goldberg, 2014): M ≈WC> Since the word and context embeddings implicitly live in the same vector space (Melamud et al., 2015), we can also compare Concat’s hyperplane with the context matrix C. Under this interpretation, the Concat model does not learn what words are hypernyms, but rather what contexts or features are indicative of hypernymy.
109	30	We hypothesize the TM14 data set contains too many diverse and mutually exclusive forms of lexical entailment, like instrument-goal (e.g. “honey” → “sweetness”).
113	30	As we saw in the previous section, Concat only acts as a sort of H-feature detector for whether H is a prototypical hypernym, but does not actually infer the relationship between H and w. Nonetheless, this is powerful behavior which should still be used in combination with the insights of other models like Ksim and Asym.
136	57	Fi(〈Hi, wi〉, p̂i) = 〈H>i wi, H>i p̂i, w>i p̂i, (Hi − wi)>p̂i〉 These four “meta”-features capture all the benefits of the H-feature detector (slots 2 and 3), while still addressing Concat’s issues with similarity arguments (slot 1) and distributional inclusion (slot 4).
138	28	The union of all the feature vectors F1, .
140	77	This classifier is trained on the same training data as each of the individual H-feature detectors, so our iterative procedure acts only as a method of feature extraction.
153	46	First, we find there can be considerable variance if the train/test set is regenerated with a different random seed, indicating that multiple trials are necessary.
156	30	Our performance metric is F1 score.
157	29	This is more representative than accuracy, as most of the data sets are heavily unbalanced.
166	26	Namely, we include a baseline Cosine classifier, which only learns a threshold which maximizes F1 score on the training set; three linear models of prior work, Concat, Diff and Asym; and the RBF and Ksim models found to be successful in Kruszewski et al. (2015) and Levy et al. (2015).
179	49	Surprisingly, the detector features are moderately detrimental on the LEDS data set, though this can also be understood in the data set’s construction: since the negative examples are randomly shuffled positive examples, the same detector signal will appear in both positive and negative examples.
184	27	Figure 2 shows these results across all four data sets, with the 0 line set at performance of the n = 1 baseline.
186	71	In the figure, we see that the iterative procedure moderately improves performance LEDS, while greatly improving the scores of BLESS and TM14, but on the medical data set, additional iterations actually hurt performance.
191	21	The first iteration is identical to the one in Ta- ble 2, as expected.
195	79	Nonetheless, we see how multiple iterations of the procedure allows our model to capture many more useful features than a single Concat classifier on its own.
196	47	We considered the task of detecting lexical entailment using distributional vectors of word meaning.
198	40	We found the Concat classifier overwhelmingly acted as a feature detector which automatically identifies Hearst Patterns in the distributional vectors.
200	30	In each iteration of the procedure, an H-feature detector is learned, and then removed from the data, allowing us to identify several different kinds of Hearst Patterns in the data.
202	64	Our model matches or exceeds the performance of prior work, both on hypernymy detection and general lexical entailment.
