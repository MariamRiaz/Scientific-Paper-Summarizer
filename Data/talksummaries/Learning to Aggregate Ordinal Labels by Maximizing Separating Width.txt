40	2	Suppose that there are M workers and N items taken from a total of K classes.
41	6	For item i, define an M ×K matrix Ri by putting Rijk = 1 if worker j labels the item as k, and Rijk = 0 otherwise.
51	2	(4) Thus, the collected labels are aggregated following the rule zi = argk max exp ∑M j=1 ∑K d=1R i jd lnA j kd, where the unknown parameters A can be updated in M-step through maximum likelihood estimation (MLE) principle.
84	2	To evaluate the separating width of samples with the label set z = {zi}Ni=1, with zi ∈ {−1, 1},∀i ∈ [N ], we introduce a linear decision boundary f(Ri) = aTRib with a ∈ RM×1 and b ∈ RK×1.
86	5	(1) and (2), where a denote the worker expertise and b transforms worker’s label into a scale variable.
91	4	One may wonder why confining to rank-1 measurements.
97	2	If Ri lies on the correct side of the margin, ξi = 0.
104	2	The factor 2λ2 is introduced here to simplify the derivations of inference methods later.
106	3	(13), we can obtain the estimated labels with the largest separating width.
115	3	WhenK ≥ 3, the ordinal label should be estimated by considering the predicted results from K − 1 binary problems.
122	2	Moreover, VB often suffers from inaccuracy because of the potentially impractical assumption of independence of variables.
127	2	We employ data augmented technique (Polson & Scott, 2011) to approximate the hinge loss function.
130	2	Note that the right hand side of the inequality is tractable, minimizing which would give an upper bound of the original optimization problem.
139	2	The first term comes from the generative model of crowdsourcing, while the second term maximizes the separating width of the estimated ordinal labels.
148	1	2: Initializing z = {zi}Ni=1 by MV, a, b, α and β 3: while not convergence do 4: for i = 1 : N do 5: Ajkd ∼ D(A j kd|αj + ∑N i=1R i jdI(zi = k)) 6: ωik ∼ D(ωik|βi + I(zi = k)) 7: γit ∼ 1Z γ − 12 it exp[− 12 (γit + λ22ζ 2 it γit )] 8: zi ∼ p(Ri,A, zi, ωi|α,β) ∏K−1 t=1 φ(zi, γit|Rit) 9: end for 10: for t = 1 : K − 1 do 11: Σat = 2λ1‖bt‖22I + ∑N i=1 λ22 〈γit〉R ibtb T t R iT 12: at = Σ−1at ( ∑N i=1(λ2 + λ22 〈γit〉 )〈sgnt(zi)〉R ibt) 13: Σbt = 2λ1‖at‖22I + ∑N i=1 λ22 〈γit〉a T t R iRi T at 14: bt = Σ −1 bt ( ∑N i=1(λ2 + λ22 〈γit〉 )〈sgnt(zi)〉a T t R i) 15: end for 16: αj ← αj − η ∂L(αj)∂αj ,∀j ∈ [M ] 17: βi ← βi − η ∂L(βi)∂βi ,∀i ∈ [N ] 18: end while a Gibbs sampler to generate the random variables to approximate the posterior distribution.
151	1	This algorithm is iteratively implemented to reach a local optimum.
156	1	We first evaluate our method on three binary benchmark datasets shown in Table 1, include labeling bird species (Welinder et al., 2010) (Bird dataset), recognizing textual entailment (Snow et al., 2008) (RTE dataset) and accessing the relevance of topic-document pairs with a binary judgment in TREC 2011 crowdsourcing track (Gabriella & Matthew, 2011) (TREC dataset).
157	1	The competitive methods include the pure majority voting estimator (refereed to as MV), the EM method for DS model initialized by majority voting (refereed to as MV-DS), the EM method for DS model initialized by spectral method (refereed to as Opt-DS) (Zhang et al., 2016), the Gibbs sampler for the Bayesian extension of M3V (Tian & Zhu, 2015) (referred to as G-CrowdSVM), the SVD-based algorithm proposed in (Ghosh et al., 2011) (referred to as Gh-SVD), and the Eigenvalues of Ratio algorithm proposed in (Dalvi et al., 2013) (referred to as Eig-Ratio).
167	2	One is to judge the relevance of query-URL pairs with a 5-level rating score (Web dataset), and the other is to identify the age of each subject with a 7-level rating score (Age dataset).
169	2	Compared with the error rate l0, the measures l1 and l2 take precision into consideration, and may be preferred for aggregating ordinal labels when one cares about the severity of error.
177	2	On this dataset, our method introduces 4 decision boundaries to measure the separating width of generated true labels.
183	2	5 contains three confusion matrices, including the averaged confusion matrix of all workers, the confusion matrices of Table 2. l0 error rate (%) in predicting the latent labels on three binary benchmark datasets.
184	4	Binary Dataset Ours G-CrowdSVM Opt-DS MV-DS MV Gh-SVD Eig-Ratio Bird 9.25±0.17 10.37±0.41 10.09 11.11 24.07 27.78 27.78 RTE 7.00±0.29 7.72±0.22 7.12 7.12 10.31 49.13 9.00 TREC 29.30±0.11 31.32±0.34 29.80 30.02 34.86 42.99 43.96 Table 3.
188	4	6 summarizes the training time and error rates after each iteration for all estimators on the Web dataset.
191	2	To measure the separating width among ordinal labels, we first investigate a binary case, and then extend our achievements to the ordinal one.
192	2	With K − 1 decision boundaries, we define an optimization problem for measuring the separating width among ordinal classes.
194	10	A Gibbs sampler is adopted to approximate the posterior distribution, while the gradient method is used to calculate the separating width and optimize the hyperparameters.
195	15	As demonstrated on the ordinal datasets, which is the main focus of this paper, our method consistently achieves the best performance compared with competitive ones, and the improvements on Web dataset are significant.
196	175	As demonstrated by the experimental results on the binary datasets, our algorithm works slightly better than any previous method.
197	174	Thus, our algorithm provides a uniform method in both binary and ordinal cases, and can be practically useful for real-world applications.
