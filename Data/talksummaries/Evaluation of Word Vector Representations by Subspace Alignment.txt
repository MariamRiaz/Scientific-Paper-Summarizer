1	14	Unsupervised word vectors have been shown to benefit parsing (Lazaridou et al., 2013; Bansal et al., 2014), chunking (Turian et al., 2010), named entity recognition (Guo et al., 2014) and sentiment analysis (Socher et al., 2013), among others.
2	21	Despite their ubiquity, there is no standard scheme for intrinsically evaluating the quality of word vectors: a vector quality is traditionally judged by its utility in downstream NLP tasks.
3	16	This lack of standardized evaluation is due, in part, to word vectors’ major criticism: word vectors are linguistically opaque in a sense that it is still not clear how to interpret individual vector dimensions, and, consequently, it is not clear how to score a non-interpretable representation.
4	55	Nevertheless, to facilitate development of better word vector models and for better error analysis of word vectors, it is desirable (1) to compare word vector models easily, without recourse to multiple extrinsic applications whose implementation and runtime can be costly; and (2) to understand how features in word vectors contribute to downstream tasks.
5	19	We propose a simple intrinsic evaluation measure for word vectors.
11	38	To show that our proposed score is meaningful, we compare our intrinsic evaluation model to the standard (semantic) extrinsic evaluation benchmarks (§4).
12	22	For nine off-the-shelf word vector representation models, our model obtains high correlation (0.34 ≤ r ≤ 0.89) with the extrinsic tasks (§5).
13	24	The crux of our evaluation method lies in quantifying the similarity between a distributional word vector model and a (gold-standard) linguistic re- 2049 source capturing human knowledge.
26	25	Similarly, S ∈ RP×N is the linguistic property matrix with every row as a linguistic property vector s ∈ R1×N .
27	18	P denotes linguistic properties obtained from a manually-annotated linguistic resource.
28	55	We obtain an alignment between the word vector dimensions and the linguistic dimensions which maximizes the correlation between the aligned dimensions of the two matrices.
29	32	This is 1:n alignment: one distributional dimension is aligned to at most one linguistic property, whereas one linguistic property can be aligned to n distributional dimensions; see figure 1.
32	19	The total correlation between two matrices QVEC is our intrinsic evaluation measure of a set of word vectors relative to a set of linguistic properties.
34	12	It is motivated, among others, by the effectiveness of word vectors in linear models implying that linear combinations of features (vector dimensions) produce relevant, salient content.
38	17	Thus, QVEC is a recall-oriented measure: highly- correlated alignments provide evaluation and annotation of vector dimensions, and missing information or noisy dimensions do not significantly affect the score since the correlations are low.
51	25	We use retrofitting (Faruqui et al., 2015) as a post-processing step to enrich GloVe and LSA vectors with semantic information from WordNet and Paraphrase database (PPDB) (Ganitkevitch et al., 2013).6
66	15	We compute the Pearson’s correlation coefficient r to quantify the linear relationship between the scorings.
71	32	The Pearson’s correlation computed on the extended set of comparison points (in the same experimental setup as in table 2) is r = 0.88.
76	24	Table 4 shows, for the same 300-dimensional vectors, that QVEC’s correlation with the downstream text classification tasks is on par with or higher than the correlation between the word similarity and text classification tasks.
77	14	Higher correlating methods—in our experiments, QVEC and MEN—are better predictors of quality in downstream tasks.
80	85	To summarize, we observe high positive correlation between QVEC and the downstream tasks, consistent across the tasks and across different models with vectors of different dimensionalities.
83	30	We aggregate rankings by individual downstream tasks into a global ranking using the Kemeny–Young rank aggregation algorithm, for each dimension separately (Kemeny, 1959).
92	44	Albeit noisy, we find correspondence between the projected labels of distributional columns and the column content.
94	51	This interesting by-product of our method will be addressed in future work.
95	19	While we experiment with linguistic vectors capturing semantic concepts, our methodology is generally applicable to other linguistic resources (Faruqui and Dyer, 2015).
96	72	For example, partof-speech annotations extracted from a treebank would yield linguistic vectors capturing syntactic content of vectors.
97	42	Thus, QVEC can be used as a task-specific evaluator; we will investigate this in future work.
98	45	A useful property of supersenses (features in our linguistic vectors) is that they are stable across languages (Schneider et al., 2013; Tsvetkov et al., 2014b).
99	20	Cross-lingual vector evaluation and evaluation of multilingual word vectors with QVEC is thus an additional promising research avenue.
100	40	We propose a method for intrinsic evaluation of word vectors which shows strong relationship—both linear and monotonic—with the scores/rankings produced by the downstream tasks.
