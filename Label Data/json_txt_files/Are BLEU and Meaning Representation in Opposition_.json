{"sections": [{"text": "1 Are BLEU and Meaning Representation in Opposition?\nOne of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks."}, {"heading": "1 Introduction", "text": "Deep learning has brought the possibility of automatically learning continuous representations of sentences. On the one hand, such representations can be geared towards particular tasks such as classifying the sentence in various aspects (e.g. sentiment, register, question type) or relating the sentence to other sentences (e.g. semantic similarity, paraphrasing, entailment). On the other hand, we can aim at \u201cuniversal\u201d sentence representations, that is representations performing reasonably well in a range of such tasks.\nRegardless the evaluation criterion, the representations can be learned either in an unsupervised way (from simple, unannotated texts) or supervised, relying on manually constructed training sets of sentences equipped with annotations of the appropriate type. A different approach is to obtain sentence representations from training neural machine translation models (Hill et al., 2016).\nSince Hill et al. (2016), NMT has seen substantial advances in translation quality and it is thus\nnatural to ask how these improvements affect the learned representations.\nOne of the key technological changes was the introduction of \u201cattention\u201d (Bahdanau et al., 2014), making it even the very central component in the network (Vaswani et al., 2017). Attention allows the NMT system to dynamically choose which parts of the source are most important when deciding on the current output token. As a consequence, there is no longer a static vector representation of the sentence available in the system.\nIn this paper, we remove this limitation by proposing a novel encoder-decoder architecture with a structured fixed-size representation of the input that still allows the decoder to explicitly focus on different parts of the input. In other words, our NMT system has both the capacity to attend to various parts of the input and to produce static representations of input sentences.\nWe train this architecture on English-to-German and English-to-Czech translation and evaluate the learned representations of English on a wide range of tasks in order to assess its performance in learning \u201cuniversal\u201d meaning representations.\nIn Section 2, we briefly review recent efforts in obtaining sentence representations. In Section 3, we introduce a number of variants of our novel architecture. Section 4 describes some standard and our own methods for evaluating sentence representations. Section 5 then provides experimental results: translation and representation quality. The relation between the two is discussed in Section 6."}, {"heading": "2 Related Work", "text": "The properties of continuous sentence representations have always been of interest to researchers working on neural machine translation. In the first works on RNN sequence-to-sequence models, Cho et al. (2014) and Sutskever et al. (2014)\nar X\niv :1\n80 5.\n06 53\n6v 1\n[ cs\n.C L\n] 1\n6 M\nay 2\n01 8\n2\nprovided visualizations of the phrase and sentence embedding spaces and observed that they reflect semantic and syntactic structure to some extent. Hill et al. (2016) perform a systematic evaluation of sentence representation in different models, including NMT, by applying them to various sentence classification tasks and by relating semantic similarity to closeness in the representation space. Shi et al. (2016) investigate the syntactic properties of representations learned by NMT systems by predicting sentence- and word-level syntactic labels (e.g. tense, part of speech) and by generating syntax trees from these representations. Schwenk and Douze (2017) aim to learn language-independent sentence representations using NMT systems with multiple source and target languages. They do not consider the attention mechanism and evaluate primarily by similarity scores of the learned representations for similar sentences (within or across languages)."}, {"heading": "3 Model Architectures", "text": "Our proposed model architectures differ in (a) which encoder states are considered in subsequent processing, (b) how they are combined, and (c) how they are used in the decoder. Table 1 summarizes all the examined configurations of RNN-based models. The first three (ATTN, FINAL, FINAL-CTX) correspond roughly to the standard sequence-to-sequence models, Bahdanau et al. (2014), Sutskever et al. (2014) and Cho et al. (2014), respectively. The last column (ATTN-ATTN) is our main proposed architecture: compound attention, described here in Section 3.1. In addition to RNN-based models, we experiment with the Transformer model, see Section 3.3."}, {"heading": "3.1 Compound Attention", "text": "Our compound attention model incorporates attention in both the encoder and the decoder. Its architecture is shown in Fig. 1. Encoder with inner attention. First, we process the input sequence x1, x2, . . . , xT using a bidirectional recurrent network with gated recurrent units (GRU, Cho et al., 2014): \u2212\u2192 ht = \u2212\u2212\u2192 GRU(xt, \u2212\u2212\u2192 ht\u22121), \u2190\u2212 ht = \u2190\u2212\u2212 GRU(xt, \u2190\u2212\u2212 ht+1), ht = [ \u2212\u2192 ht , \u2190\u2212 ht ].\n3 We denote by u the combined number of units in the two RNNs, i.e. the dimensionality of ht. Next, our goal is to combine the states (h1, h2, . . . , hT ) = H of the encoder into a vector of fixed dimensionality that represents the entire sentence. Traditional seq2seq models concatenate the final states of both encoder RNNs ( \u2212\u2192 hT and \u2190\u2212 h1) to obtain the sentence representation (denoted as FINAL in Table 1). Another option is to combine all encoder states using the average or maximum over time (Collobert and Weston, 2008; Schwenk and Douze, 2017) (AVGPOOL and MAXPOOL in Table 1 and following). We adopt an alternative approach, which is to use inner attention1 (Liu et al., 2016; Li et al., 2016) to compute several weighted averages of the encoder states (Lin et al., 2017). The main motivation for incorporating these multiple \u201cviews\u201d of the state sequence is that it removes the need for the RNN cell to accumulate the representation of the whole sentence as it processes the input, and therefore it should have more capacity for modeling local dependencies. Specifically, we fix a number r, the number of attention heads, and compute an r\u00d7T matrixA of attention weights \u03b1jt, representing the importance of position t in the input for the jth attention head. We then use this matrix to compute r weighted sums of the encoder states, which become the rows of a new matrix M : M = AH. (1) A vector representation of the source sentence (the \u201csentence embedding\u201d) can be obtained by flattening the matrix M . In our experiments, we project the encoder states h1, h2, . . . , hT down to a given dimensionality before applying Eq. (1), so that we can control the size of the representation. Following Lin et al. (2017), we compute the attention matrix by feeding the encoder states to a two-layer feed-forward network: A = softmax(U tanh(WH>)), (2) where W and U are weight matrices of dimensions d\u00d7 u and r \u00d7 d, respectively (d is the number of hidden units); the softmax function is applied along the second dimension, i.e. across the encoder states. 1Some papers call the same or similar approach selfattention or single-time attention. Attentive decoder. In vanilla seq2seq models with a fixed-size sentence representation, the decoder is usually conditioned on this representation via the initial RNN state. We propose to instead leverage the structured sentence embedding by applying attention to its components. This is no different from the classical attention mechanism used in NMT (Bahdanau et al., 2014), except that it acts on this fixed-size representation instead of the sequence of encoder states. In the ith decoding step, the attention mechanism computes a distribution {\u03b2ij}rj=1 over the r components of the structured representation. This is then used to weight these components to obtain the context vector ci, which in turn is used to update the decoder state. Again, we can write this in matrix form as C = BM, (3) where B = (\u03b2ij) T \u2032,r i=1,j=1 is the attention matrix and C = (ci, c2, . . . , cT \u2032) are the context vectors. Note that by combining Eqs. (1) and (3), we get C = (BA)H. (4) Hence, the composition of the encoder and decoder attentions (the \u201ccompound attention\u201d) defines an implicit alignment between the source and the target sequence. From this viewpoint, our model can be regarded as a restriction of the conventional attention model. The decoder uses a conditional GRU cell (cGRUatt; Sennrich et al., 2017), which consists of two consecutively applied GRU blocks. The first block processes the previous target token yi\u22121, while the second block receives the context vector ci and predicts the next target token yi."}, {"heading": "3.2 Constant Context", "text": "Compared to the FINAL model, the compound attention architecture described in the previous section undoubtedly benefits from the fact that the decoder is presented with information from the encoder (i.e. the context vectors ci) in every decoding step. To investigate this effect, we include baseline models where we replace all context vectors ci with the entire sentence embedding (indicated by the suffix \u201c-CTX\u201d in Table 1). Specifically, we provide either the flattened matrixM (for models with inner attention; ATTN or ATTN-CTX), the final state of the encoder (FINAL-CTX), or the\n4\nresult of mean- or max-pooling (*POOL-CTX) as a constant input to the decoder cell."}, {"heading": "3.3 Transformer with Inner Attention", "text": "The Transformer (Vaswani et al., 2017) is a recently proposed model based entirely on feedforward layers and attention. It consists of an encoder and a decoder, each with 6 layers, consisting of multi-head attention on the previous layer and a position-wise feed-forward network. In order to introduce a fixed-size sentence representation into the model, we modify it by adding inner attention after the last encoder layer. The attention in the decoder then operates on the components of this representation (i.e. the rows of the matrix M ). This variation on the Transformer model corresponds to the ATTN-ATTN column in Table 1 and is therefore denoted TRF-ATTN-ATTN."}, {"heading": "4 Representation Evaluation", "text": "Continuous sentence representations can be evaluated in many ways, see e.g. Kiros et al. (2015), Conneau et al. (2017) or the RepEval workshops.2 We evaluate our learned representations with classification and similarity tasks from SentEval (Section 4.1) and by examining clusters of sentence paraphrase representations (Section 4.2)."}, {"heading": "4.1 SentEval", "text": "We perform evaluation on 10 classification and 7 similarity tasks using the SentEval3 (Conneau et al., 2017) evaluation tool. This is a superset of the tasks from Kiros et al. (2015). 2https://repeval2017.github.io/ 3https://github.com/facebookresearch/ SentEval/\nTable 2 describes the classification tasks (number of classes, data size, task type and an example) and Table 3 lists the similarity tasks. The similarity (relatedness) datasets contain pairs of sentences labeled with a real-valued similarity score. A given sentence representation model is evaluated either by learning to directly predict this score given the respective sentence embeddings (\u201cregression\u201d), or by computing the cosine similarity of the embeddings (\u201csimilarity\u201d) without the need of any training. In both cases, Pearson and Spearman correlation of the predictions with the gold ratings is reported. See Dolan et al. (2004) for details on MRPC and Hill et al. (2016) for the remaining tasks."}, {"heading": "4.2 Paraphrases", "text": "We also evaluate the representation of paraphrases. We use two paraphrase sources for this purpose: COCO and HyTER Networks. COCO (Common Objects in Context; Lin et al., 2014) is an object recognition and image captioning dataset, containing 5 captions for each image. We extracted the captions from its validation set to form a set of 5 \u00d7 5k = 25k sentences grouped by the source image. The average sentence length is 11 tokens and the vocabulary size is 9k types. HyTER Networks (Dreyer and Marcu, 2014)\n5 are large finite-state networks representing a subset of all possible English translations of 102 Arabic and 102 Chinese sentences. The networks were built by manually based on reference sentences in Arabic, Chinese and English. Each network contains up to hundreds of thousands of possible translations of the given source sentence. We randomly generated 500 translations for each source sentence, obtaining a corpus of 102k sentences grouped into 204 clusters, each containing 500 paraphrases. The average length of the 102k English sentences is 28 tokens and the vocabulary size is 11k token types. For every model, we encode each dataset to obtain a set of sentence embeddings with cluster labels. We then compute the following metrics: Cluster classification accuracy (denoted \u201cCl\u201d). We remove 1 point (COCO) or half of the points (HyTER) from each cluster, and fit an LDA classifier on the rest. We then compute the accuracy of the classifier on the removed points. Nearest-neighbor paraphrase retrieval accuracy (NN). For each point, we find its nearest neighbor according to cosine or L2 distance, and count how often the neighbor lies in the same cluster as the original point. Inverse Davies-Bouldin index (iDB). The Davies-Bouldin index (Davies and Bouldin, 1979) measures cluster separation. For every pair of clusters, we compute the ratio Rij of their combined scatter (average L2 distance to the centroid) Si + Sj and the L2 distance of their centroids dij , then average the maximum values for all clusters: Rij = Si + Sj dij (5) DB = 1 N N\u2211 i=1 max j 6=i Rij (6) The lower the DB index, the better the separation. To match with the rest of our metrics, we take its inverse: iDB = 1DB ."}, {"heading": "5 Experimental Results", "text": "We trained English-to-German and English-toCzech NMT models using Neural Monkey4 (Helcl and Libovicky\u0301, 2017a). In the following, we distinguish these models using the code of the target language, i.e. de or cs. 4https://github.com/ufal/neuralmonkey The de models were trained on the Multi30K multilingual image caption dataset (Elliott et al., 2016), extended by Helcl and Libovicky\u0301 (2017b), who acquired additional parallel data using backtranslation (Sennrich et al., 2016) and perplexitybased selection (Yasuda et al., 2008). This extended dataset contains 410k sentence pairs, with the average sentence length of 12 \u00b1 4 tokens in English. We train each model for 20 epochs with the batch size of 32. We truecased the training data as well as all data we evaluate on. For German, we employed Neural Monkey\u2019s reversible pre-processing scheme, which expands contractions and performs morphological segmentation of determiners. We used a vocabulary of at most 30k tokens for each language (no subword units). The cs models were trained on CzEng 1.7 (Bojar et al., 2016).5 We used byte-pair encoding (BPE) with a vocabulary of 30k sub-word units, shared for both languages. For English, the average sentence length is 15\u00b119 BPE tokens and the original vocabulary size is 1.9M. We performed 1 training epoch with the batch size of 128 on the entire training section (57M sentence pairs). The datasets for both de and cs models come with their respective development and test sets of sentence pairs, which we use for the evaluation of translation quality. (We use 1k randomly selected sentence pairs from CzEng 1.7 dtest as a development set. For evaluation, we use the entire etest.) We also evaluate the InferSent model6 (Conneau et al., 2017) as pre-trained on the natural language inference (NLI) task. InferSent has been shown to achieve state-of-the-art results on the SentEval tasks. We also include a bag-ofwords baseline (GloVe-BOW) obtained by averaging GloVe7 word vectors (Pennington et al., 2014)."}, {"heading": "5.1 Translation Quality", "text": "We estimate translation quality of the various models using single-reference case-sensitive BLEU (Papineni et al., 2002) as implemented in Neural Monkey (the reference implementation is mteval-v13a.pl from NIST or Moses). Tables 4 and 5 provide the results on the two datasets. The cs dataset is much larger and the training takes much longer. We were thus able 5http://ufal.mff.cuni.cz/czeng/czeng17 6https://github.com/facebookresearch/ InferSent 7https://nlp.stanford.edu/projects/ glove/\n6\nto experiment with only a subset of the possible model configurations. The columns \u201cSize\u201d and \u201cHeads\u201d specify the total size of sentence representation and the number of heads of encoder inner attention. In both cases, the best performing is the ATTN Bahdanau et al. model, followed by Transformer (de only) and our ATTN-ATTN (compound attention). The non-attentive FINAL Cho et al. is the worst, except cs-MAXPOOL. For 5 selected cs models, we also performed the WMT-style 5-way manual ranking on 200 sentence pairs. The annotations are interpreted as simulated pairwise comparisons. For each model, the final score is the number of times the model was judged better than the other model in the pair. Tied pairs are excluded. The results, shown in Table 5, confirm the automatic evaluation results. We also checked the relation between BLEU\nand the number of heads and representation size. While there are many exceptions, the general tendency is that the larger the representation or the more heads, the higher the BLEU score. The Pearson correlation between BLEU and the number of heads is 0.87 for cs and 0.31 for de."}, {"heading": "5.2 SentEval", "text": "Due to the large number of SentEval tasks, we present the results abridged in two different ways: by reporting averages (Table 6) and by showing only the best models in comparison with other methods (Table 7). The full results can be found in the supplementary material. Table 6 provides averages of the classification and similarity results, along with the results of selected tasks (SNLI, SICK-E). As the baseline for classifications tasks, we assign the most frequent class to all test examples.8 The de models are generally worse, most likely due to the higher OOV rate and overall simplicity of the training sentences. On cs, we see a clear pattern that more heads hurt the performance. The de set has more variations to consider but the results are less conclusive. For the similarity results, it is worth noting that cs-ATTN-ATTN performs very well with 1 attention head but fails miserably with more heads. Otherwise, the relation to the number of heads is less clear. Table 7 compares our strongest models with other approaches on all tasks. Besides InferSent and GloVe-BOW, we include SkipThought as evaluated by Conneau et al. (2017), and the NMTbased embeddings by Hill et al. (2016) trained on the English-French WMT15 dataset (this is the best result reported by Hill et al. for NMT). We see that the supervised InferSent clearly outperforms all other models in all tasks except for MRPC and TREC. Results by Hill et al. are always lower than our best setups, except MRPC. On classification tasks, our models are outperformed even by GloVe-BOW, except for the NLI tasks (SICK-E and SNLI) where cs-FINAL-CTX is better."}, {"heading": "5.3 Paraphrase Scores", "text": "Table 6 also provides our measurements based on sentence paraphrases. For paraphrase retrieval 8For MR, CR, SUBJ, and MPQA, where there is no distinct test set, the class is established on the whole collection. For other tasks, the class is learned from the training set.\n7\n8\nB L\nE U\nM R C R S U\nB J\nM P\nQ A\nS S\nT 2\nS S\nT 5\nT R\nE C\nM R\nP C\nS IC\nK -E\nS N\nL I A v g A cc S IC K -R S T S B S T S 1 2 S T S 1 3 S T S 1 4 S T S 1 5 S T S 1 6 A v g S im H y -C l H y -N N H y -i D B C O -C l C O -N N C O -i D B\nBLEU MR CR\nSUBJ MPQA\nSST2 SST5 TREC MRPC\nSICK-E SNLI AvgAcc SICK-R\nSTSB STS12 STS13 STS14 STS15 STS16 AvgSim Hy-Cl Hy-NN Hy-iDB CO-Cl CO-NN CO-iDB\n\u22121.00 \u22120.75 \u22120.50 \u22120.25 0.00 0.25 0.50 0.75 1.00\nFigure 2: Pearson correlations. Upper triangle: de models, lower triangle: cs models. Positive values shown in shades of green. For similarity tasks, only the Pearson (not Spearman) coefficient is represented.\n(NN), we found cosine distance to work better than L2 distance. We therefore do not list or further consider L2-based results (except in the supplementary material).\nThis evaluation seems less stable and discerning than the previous two, but we can again confirm the victory of InferSent followed by our nonattentive cs models. cs and de models are no longer clearly separated."}, {"heading": "6 Discussion", "text": "To assess the relation between the various measures of sentence representations and translation quality as estimated by BLEU, we plot a heatmap of Pearson correlations in Fig. 2. As one example, Fig. 3 details the cs models\u2019 BLEU scores and AvgAcc (average of SentEval accuracies).\nA good sign is that on the cs dataset, most metrics of representation are positively correlated (the pairwise Pearson correlation is 0.78\u00b1 0.32 on average), the outlier being TREC (\u22120.16\u00b10.16 correlation with the other metrics on average)\nOn the other hand, most representation metrics correlate with BLEU negatively (\u22120.57\u00b10.31) on cs. The pattern is less pronounced but still clear also on the de dataset.\nA detailed understanding of what the learned\nrepresentations contain is difficult. We can only speculate that if the NMT model has some capability for following the source sentence superficially, it will use it and spend its capacity on closely matching the target sentences rather than on deriving some representation of meaning which would reflect e.g. semantic similarity. We assume that this can be a direct consequence of NMT being trained for cross entropy: putting the exact word forms in exact positions as the target sentence requires. Performing well in single-reference BLEU is not an indication that the system understands the meaning but rather that it can maximize the chance of producing the n-grams required by the reference.\nThe negative correlation between the number of attention heads and the representation metrics from Fig. 3 (\u22120.81\u00b10.12 for cs and\u22120.18\u00b10.19 for de, on average) can be partly explained by the following observation. We plotted the induced alignments (e.g. Fig. 4) and noticed that the heads tend to \u201cdivide\u201d the sentence into segments. While one would hope that the segments correspond to some meaningful units of the sentence (e.g. subject, predicate, object), we failed to find any such interpretation for ATTN-ATTN and for cs models in general. Instead, the heads divide the source sentence more or less equidistantly, as documented by Fig. 5. Such a multi-headed sentence representation is then less fit for representing e.g. paraphrases where the subject and object swap their position due to passivization, because their representations are then accessed by different heads, and thus end up in different parts of the sentence embedding vector.\n9\nFor de-ATTN-CTX models, we observed a much flatter distribution of attention weights for each head and, unlike in the other models, we were often able to identify a head focusing on the main verb. This difference between ATTN-ATTN and some ATTN-CTX models could be explained by the fact that in the former, the decoder is oblivious to the ordering of the heads (because of decoder attention), and hence it may not be useful for a given head to look for a specific syntactic or semantic role."}, {"heading": "7 Conclusion", "text": "We presented a novel variation of attentive NMT models (Bahdanau et al., 2014; Vaswani et al., 2017) that again provides a single meeting point with a continuous representation of the source sen-\ntence. We evaluated these representations with a number of measures reflecting how well the meaning of the source sentence is captured. While our proposed \u201ccompound attention\u201d leads to translation quality not much worse than the fully attentive model, it generally does not perform well in the meaning representation. Quite on the contrary, the better the BLEU score, the worse the meaning representation. We believe that this observation is important for representation learning where bilingual MT now seems less likely to provide useful data, but perhaps more so for MT itself, where the struggle towards a high single-reference BLEU score (or even worse, cross entropy) leads to systems that refuse to consider the meaning of the sentence."}, {"heading": "Acknowledgement", "text": "This work has been supported by the grants 18-24210S of the Czech Science Foundation, SVV 260 453 and \u201cProgress\u201d Q18+Q48 of Charles University, and using language resources distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (projects LM2015071 and OP VVV VI CZ.02.1.01/0.0/0.0/16 013/0001781).\n10"}], "year": 2018, "references": [{"title": "Neural machine translation by jointly learning to align and translate", "authors": ["References Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "year": 2014}, {"title": "CzEng 1.6: Enlarged CzechEnglish Parallel Corpus with Processing Tools Dockered", "authors": ["Ond\u0159ej Bojar"], "venue": "In Text, Speech, and Dialogue (TSD),", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "authors": ["Ronan Collobert", "Jason Weston."], "venue": "ICML.", "year": 2008}, {"title": "Supervised learning of universal sentence representations from natural language inference data", "authors": ["Alexis Conneau", "Douwe Kiela", "Holger Schwenk", "Lo\u0131\u0308c Barrault", "Antoine Bordes"], "year": 2017}, {"title": "A cluster separation measure", "authors": ["David L. Davies", "Donald W. Bouldin."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI1:224\u2013227.", "year": 1979}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "authors": ["William B. Dolan", "Chris Quirk", "Chris Brockett."], "venue": "COLING.", "year": 2004}, {"title": "HyTER networks of selected OpenMT08/09 sentences", "authors": ["Markus Dreyer", "Daniel Marcu."], "venue": "Linguistic Data Consortium. LDC2014T09.", "year": 2014}, {"title": "Multi30k: Multilingual englishgerman image descriptions. CoRR, abs/1605.00459", "authors": ["Desmond Elliott", "Stella Frank", "Khalil Sima\u2019an", "Lucia Specia"], "year": 2016}, {"title": "Neural Monkey: An open-source tool for sequence learning", "authors": ["Jind\u0159ich Helcl", "Jind\u0159ich Libovick\u00fd."], "venue": "The Prague Bulletin of Mathematical Linguistics, 107(1):5\u201317.", "year": 2017}, {"title": "Learning distributed representations of sentences from unlabelled data", "authors": ["Felix Hill", "Kyunghyun Cho", "Anna Korhonen."], "venue": "HLT-NAACL.", "year": 2016}, {"title": "Skip-thought vectors", "authors": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "NIPS Vol. 2, NIPS\u201915, pages 3294\u20133302.", "year": 2015}, {"title": "Dataset and neural recurrent sequence labeling model for open-domain factoid question answering", "authors": ["Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu."], "venue": "CoRR, abs/1607.06275.", "year": 2016}, {"title": "Microsoft COCO: common objects in context", "authors": ["Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "Lubomir D. Bourdev", "Ross B. Girshick", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C. Lawrence Zitnick."], "venue": "CoRR, abs/1405.0312.", "year": 2014}, {"title": "A structured self-attentive sentence embedding. CoRR, abs/1703.03130", "authors": ["Zhouhan Lin", "Minwei Feng", "C\u0131\u0301cero Nogueira dos Santos", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Yoshua Bengio"], "year": 2017}, {"title": "Learning natural language inference using bidirectional LSTM model and inner-attention", "authors": ["Yang Liu", "Chengjie Sun", "Lei Lin", "Xiaolong Wang."], "venue": "CoRR, abs/1605.09090.", "year": 2016}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "ACL, pages 311\u2013318.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "EMNLP, pages 1532\u20131543.", "year": 2014}, {"title": "Learning joint multilingual sentence representations with neural machine translation", "authors": ["Holger Schwenk", "Matthijs Douze."], "venue": "volume abs/1704.04154.", "year": 2017}, {"title": "Improving neural machine translation models with monolingual data", "authors": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "CoRR, abs/1511.06709.", "year": 2016}, {"title": "Nematus: a toolkit for neural machine translation", "authors": ["Rico Sennrich"], "venue": "EACL.", "year": 2017}, {"title": "Does string-based neural MT learn source syntax", "authors": ["Xing Shi", "Inkit Padhi", "Kevin Knight"], "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS.", "year": 2014}, {"title": "Attention is all you need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin."], "venue": "NIPS.", "year": 2017}, {"title": "Method of selecting training data to build a compact and efficient translation model", "authors": ["Keiji Yasuda", "Ruiqiang Zhang", "Hirofumi Yamamoto", "Eiichiro Sumita."], "venue": "IJCNLP.", "year": 2008}], "id": "SP:eed8dfdbc1d485cf9c219383b683a5aeaa33e4d2", "authors": [{"name": "Ond\u0159ej C\u0131\u0301fka", "affiliations": []}], "abstractText": "One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.", "title": "Are BLEU and Meaning Representation in Opposition?"}