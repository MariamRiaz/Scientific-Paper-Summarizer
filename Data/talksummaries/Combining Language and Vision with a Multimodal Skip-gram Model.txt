0	22	Distributional semantic models (DSMs) derive vector-based representations of meaning from patterns of word co-occurrence in corpora.
1	18	DSMs have been very effectively applied to a variety of semantic tasks (Clark, 2015; Mikolov et al., 2013b; Turney and Pantel, 2010).
2	30	However, compared to human semantic knowledge, these purely textual models, just like traditional symbolic AI systems (Harnad, 1990; Searle, 1984), are severely impoverished, suffering of lack of grounding in extra-linguistic modalities (Glenberg and Robertson, 2000).
3	18	This observa- tion has led to the development of multimodal distributional semantic models (MDSMs) (Bruni et al., 2014; Feng and Lapata, 2010; Silberer and Lapata, 2014), that enrich linguistic vectors with perceptual information, most often in the form of visual features automatically induced from image collections.
7	125	This is obviously very different from how humans learn about concepts, by hearing words in a situated perceptual context.
11	58	The models build upon the very effective skip-gram approach of Mikolov et al. (2013a), that constructs vector representations by learning, incrementally, to predict the linguistic contexts in which target words occur in a corpus.
12	80	In our extension, for a subset of the target words, relevant visual evidence from 153 natural images is presented together with the corpus contexts (just like humans hear words accompanied by concurrent perceptual stimuli).
13	45	The model must learn to predict these visual representations jointly with the linguistic features.
14	9	The joint objective encourages the propagation of visual information to representations of words for which no direct visual evidence was available in training.
58	39	The parameters of all models are estimated by backpropagation of error via stochastic gradient descent.
69	14	Specifically, we test on general relatedness (MEN, Bruni et al. (2014), 3K pairs), e.g., pickles are related to hamburgers, semantic (≈ taxonomic) similarity (Simlex-999, Hill et al. (2014), 1K pairs; SemSim, Silberer and Lapata (2014), 7.5K pairs), e.g., pickles are similar to onions, as well as visual similarity (VisSim, Silberer and Lapata (2014), same pairs as SemSim with different human ratings), e.g., pickles look like zucchinis.
79	15	We are interested in assessing the models both in terms of how they fuse linguistic and visual evidence when they are both available, and for their robustness in lack of full visual coverage.
82	36	We further report results on the full sets (“100%” columns of Table 1) for models that can propagate visual information and that, consequently, can meaningfully be tested on words without direct visual representations.
83	54	Results The state-of-the-art visual CNN FEATURES alone perform remarkably well, outperforming the purely textual model (SKIP-GRAM) in two tasks, and achieving the best absolute performance on the visual-coverage subset of Simlex-999.
85	19	Their performance is also good on the full data sets, where they consistently outperform SKIP-GRAM and SVD (that is much more strongly affected by lack of complete visual information).
90	36	While we defer to further work a better understanding of the relation between multimodal grounding and different similarity relations, Table 2 provides qualitative insights on how injecting visual information changes the structure of semantic space.
92	42	The owl example shows how multimodal models pick taxonomically closer neighbours of concrete objects, since often closely related things also look similar (Bruni et al., 2014).
96	32	For the concrete mural concept, both multimodal models rank paintings and portraits above less closely related sculptures (they are not a form of painting).
98	57	The last two examples show how the multimodal models turn up the embodiment level in their representation of abstract words.
99	39	For depth, their neighbours suggest a concrete marine setup over the more abstract measurement sense picked by the MMSKIP-GRAM neighbours.
100	47	For chaos, they rank a demon, that is, a concrete agent of chaos at the top, and replace the more abstract notion of despair with equally gloomy but more imageable shadows and destruction (more on abstract words below).
102	17	In particular, given that the quantitative and qualitative results collected so far suggest that the models propagate visual information across words, we apply them to image labeling and retrieval in the challenging zeroshot setup (see Section 2 above).3 3We will refer here, for conciseness’ sake, to image labeling/retrieval, but, as our visual vectors are aggregated representations of images, the tasks we’re modeling consist, more precisely, in labeling a set of pictures denoting the same object and retrieving the corresponding set given the name of the object.
107	16	To perform the vision-tolanguage mapping, we train a Ridge regression by 5- fold cross-validation on the test set (for SKIP-GRAM only, we also add the remaining 75% of word-image vector pairs used in estimating the multimodal models to the Ridge training data).4 In the image retrieval task, given a linguistic/multimodal vector, we map it onto visual space, and retrieve the nearest image.
109	10	For the multimodal models, since maximizing similarity to visual representations is already part of their training objective, we do not fit an extra mapping function.
113	12	The most interesting results however are achieved in image retrieval (Table 4), which is essentially the task the multimodal models have been implicitly optimized for, so that they could be applied to it without any specific training.
116	69	This is especially remarkable because the word vectors we are testing were not matched with visual representations at model training time, and are thus multimodal only by propagation.
121	158	Since the word representations produced by MMSKIP-GRAM-A, including those pertaining to abstract concepts, can be directly used to search for near images in visual space, we decided to verify, experimentally, if these near images (of concrete things) are relevant not only for concrete words, as expected, but also for abstract ones, as predicted by embodied views of meaning.
122	16	More precisely, we focused on the set of 200 words that were sampled across the USF norms concreteness spectrum by Kiela et al. (2014) (2 words had to be excluded for technical reasons).
127	15	Since it is much more common for concrete than abstract words to be directly represented by an image in the picture set, when searching for the nearest neighbour we excluded the picture labeled with the word of interest, if present (e.g., we excluded the picture labeled tree when picking the nearest neighbour of the word tree).
128	12	We ran a CrowdFlower5 survey in which we presented each test word with the two associated images (randomizing presentation order of nearest and random picture), and asked subjects which of the two pictures they found more closely related to the word.
