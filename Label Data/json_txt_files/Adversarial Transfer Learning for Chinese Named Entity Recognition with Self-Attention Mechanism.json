{"sections": [{"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 182\u2013192 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n182"}, {"heading": "1 Introduction", "text": "The task of named entity recognition (NER) is to recognize the named entities in given text. NER is a preliminary and important task in natural language processing (NLP) area and can be used in many downstream NLP tasks, such as relation extraction (Bunescu and Mooney, 2005), event extraction (Chen et al., 2015) and question answering (Yao and Van Durme, 2014). In recent years, numerous methods have been carefully studied for NER task, including Hidden Markov Models (HMMs) (Bikel et al., 1997), Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Currently, with the development\nof deep learning, neural networks (Lample et al., 2016; Peng and Dredze, 2016; Luo and Yang, 2016) have been introduced to NER task. All these methods need to determine entities boundaries and classify them into pre-defined categories.\nAlthough great improvements have been achieved by these methods on Chinese NER task, some issues still have not been well addressed. One significant drawback is that there is only a very small amount of annotated data available. Weibo NER dataset (Peng and Dredze, 2015; He and Sun, 2017a) and Sighan2006 NER dataset (Levow, 2006) are two widely used datasets for Chinese NER task, containing 1.3k and 45k training examples, respectively. On the two datasets, the highest F1 scores are 48.41% and 89.21%, respectively. As a basic task in NLP area, the performance is not satisfactory. Fortunately, Chinese word segmentation (CWS) task is to recognize word boundaries and the amount of supervised training data for CWS is abundant compared with NER. There are many similarities between Chinese NER task and CWS task, which we call task-shared information. As shown in Figure 1, given a sentence \u201c \u00bb \u00af : : (Hilton leaves Houston Airport)\u201d, the two tasks have the same boundaries for some words such as \u201c (Hilton)\u201d and \u201c\u00bb (leaves)\u201d, while Chinese NER has more coarse-grained boundaries\nthan CWS task for certain word such as \u201c \u00af :: (Houston Airport)\u201d in the example of Figure 1, which we call task-specific information. In order to incorporate word boundary information from CWS task into NER task, Peng and Dredze (2016) propose a joint model that performs Chinese NER with CWS task. However, their proposed model only focuses on task-shared information between Chinese NER and CWS, and ignores filtering the specificities of each task, which will bring noise for both of the tasks. For example, the CWS task splits \u201c \u00af :: (Houston Airport)\u201d into \u201c \u00af (Houston)\u201d and \u201c:: (Airport)\u201d, while the NER task takes \u201c \u00af :: (Houston Airport)\u201d as a whole entity. Thus, how to exploit task-shared information and prevent the noise brought by CWS task to Chinese NER task is a challenging problem.\nAnother issue is that most proposed models cannot explicitly model long range dependencies when predicting entity type. Though bidirectional long short term memory (BiLSTM) can learn long-distance dependencies, it cannot conduct direct connections between arbitrary two characters. As shown in Figure 1, if the model only focuses on the word \u201c (Hilton)\u201d, it can be a person or organization. However, when the model explicitly captures the dependencies between \u201c (Hilton)\u201d and \u201c\u00bb (leaves)\u201d, it is easy to classify \u201c (Hilton)\u201d into \u201cperson\u201d category. Context information is very crucial for determining the entity type. While in the sentence \u201c O( (I will be staying at the Hilton)\u201d, the entity type of \u201c (Hilton)\u201d is \u201corganization\u201d. Thus, how to better capture the global dependencies of the whole sentence is another challenging problem.\nTo address the above problems, we propose an adversarial transfer learning framework to integrate the task-shared word boundary information into Chinese NER task in this paper. The adversarial transfer learning is incorporating adversarial training into transfer learning. To better capture long range dependencies and synthesize the information of the sentence, we extend self-attention mechanism into the framework. Specifically, we try to improve Chinese NER task performance by incorporating shared boundary information from CWS task. To prevent the specific information of CWS task from lowering the performance of the Chinese NER task, we introduce adversarial training to ensure that the Chinese NER task on-\nly exploits task-shared word boundary information. Then, for tackling the long range dependency problems, we utilize self-attention to synthesize the hidden representation of BiLSTM. Finally, we evaluate our model on two different widely used Chinese NER datasets. Experimental results show that our proposed model achieves better performance than other state-of-the-art methods and gains new benchmarks.\nIn summary, the contributions of this paper are as follows:\n\u2022 We propose an adversarial transfer learning framework to incorporate task-shared word boundary information from CWS task into Chinese NER task. To our best knowledge, it is the first work to apply adversarial transfer learning method into NER task.\n\u2022 We introduce self-attention mechanism into our model, which aims to capture the global dependencies of the whole sentence and learn inner structure features of sentence.\n\u2022 We conduct our experiment on two different widely used Chinese NER datasets, and the experimental results demonstrate that our proposed model significantly and consistently outperforms previous state-of-the-art methods. We release the source code publicly for further research1."}, {"heading": "2 Related Work", "text": "NER Many methods have been proposed for NER task. Early studies on NER often exploit SVMs (Isozaki and Kazawa, 2002), HMMs (Bikel et al., 1997) and CRFs (Lafferty et al., 2001), heavily relying on feature engineering. Zhou et al. (2013) formulate Chinese NER as a joint identification and categorization task. In recent years, neural network models have been introduced to NER task (Collobert et al., 2011; Huang et al., 2015; Peng and Dredze, 2016). Huang et al. (2015) exploit BiLSTM to extract features and feed them into CRF decoder. After that, the BiLSTM-CRF model is usually exploited as the baseline. Lample et al. (2016) use a character LSTM to represent spelling characteristics. In addition, Wang et al. (2017) propose a gated convolutional neural network (GCNN) model for Chinese NER. Peng and Dredze (2016) propose a joint model for Chinese\n1https://github.com/CPF-NLPR/AT4ChineseNER\nNER, which are jointly trained with CWS task. However, the specific features brought by CWS task can lower the performance of the Chinese NER task.\nAdversarial Training Adversarial networks have achieved great success in computer vision (Goodfellow et al., 2014; Denton et al., 2015). In NLP area, adversarial training has been introduced for domain adaptation (Ganin and Lempitsky, 2014; Zhang et al., 2017; Gui et al., 2017), cross-lingual transfer learning (Chen et al., 2016; Kim et al., 2017), multi-task learning (Chen et al., 2017; Liu et al., 2017) and crowdsourcing learning (Yang et al., 2018). Bousmalis et al. (2016) propose shared-private model in domain separation network. Different from these works, we exploit adversarial network to jointly train Chinese NER task and CWS task, aiming to extract task-shared word boundary information from CWS task. To our knowledge, it is the first work to apply adversarial transfer learning framework to Chinese NER task.\nSelf-Attention Self-attention has been introduced to machine translation by Vaswani et al. (2017) for capturing global dependencies between input and output and achieves state-of-the-art performance. For language understanding task, Shen et al. (2017) exploit self-attention to learn long range dependencies. Tan et al. (2017) apply self-attention to semantic role labelling task and achieve state-of-the-art results. We are the first to\nintroduce self-attention mechanism to Chinese NER task."}, {"heading": "3 Method", "text": "In this paper, we propose a novel adversarial transfer learning framework that will learn task-shared word boundary information from CWS task, filter specific information of CWS and explicitly capture the long range dependencies between arbitrary two characters in sentence. The architecture of our proposed model is illustrated in Figure 2. The model mainly consists of five components: embedding layer, shared-private feature extractor, self-attention, task-specific CRF and task discriminator. In the following sections, we will describe each part of our proposed model in detail."}, {"heading": "3.1 Embedding Layer", "text": "Similar to other neural network models, the first step of our proposed model is to map discrete characters into the distributed representations. For a given Chinese sentence x = {c1, c2, . . . , cN} from Chinese NER dataset or CWS dataset, we lookup embedding vector from pre-trained embedding matrix for each character ci as xi \u2208 Rde ."}, {"heading": "3.2 Shared-Private Feature Extractor", "text": "Long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a variant of recurrent neural network (RNN) (Elman, 1990), which enables to address the gradient vanishing and exploding\nproblems in RNN via introducing gate mechanism and memory cell. The unidirectional LSTM only leverages information from the past, ignoring the future information. In order to incorporate information from both sides of sequence, we adopt BiLSTM to extract features. Specially, the hidden state of BiLSTM could be expressed as follows:\n\u2212\u2192 hi = \u2212\u2212\u2212\u2212\u2192 LSTM( \u2212\u2192 h i\u22121, xi) (1) \u2190\u2212 hi = \u2190\u2212\u2212\u2212\u2212 LSTM( \u2190\u2212 h i+1, xi) (2) hi = \u2212\u2192 hi \u2295 \u2190\u2212 hi (3)\nwhere \u2212\u2192 hi \u2208 Rdh and \u2190\u2212 hi \u2208 Rdh are the hidden states of the forward and backward LSTM at position i, respectively. \u2295 denotes concatenation operation.\nAs shown in Figure 2, we propose a sharedprivate feature extractor, which assigns a private BiLSTM layer and shared BiLSTM layer for task k \u2208 {NER,CWS}. The private BiLSTM layer is used to extract task-specific features, and the shared BiLSTM layer is used to learn task-shared word boundaries. Formally, for any sentence in dataset of task k, the hidden states of shared and private BiLSTM layer can be computed as follows:\nski = BiLSTM(x k i , s k i\u22121; \u03b8s) (4) hki = BiLSTM(x k i ,h k i\u22121; \u03b8k) (5)\nwhere \u03b8s and \u03b8k are the shared BiLSTM parameters and private BiLSTM parameters of task k, respectively."}, {"heading": "3.3 Self-Attention", "text": "Inspired by the self-attention applied to machine translation (Vaswani et al., 2017) and semantic role labelling (Tan et al., 2017), we exploit selfattention to explicitly learn the dependencies between any two characters in sentence and capture the inner structure information of sentence. In this paper, we adopt the multi-head self-attention mechanism. H = {h1,h2, . . . ,hN} denotes the output of private BiLSTM. Correspondingly, S = {s1, s2, . . . , sN} is the output of shared BiLSTM. We will take the self-attention in private space as example to illustrate how it works. The scaled dotproduct attention can be precisely described as follows:\nAttention(Q,K,V) = softmax( QKT\u221a\nd )V (6)\nwhere Q \u2208 RN\u00d72dh , K \u2208 RN\u00d72dh and V \u2208 RN\u00d72dh are query matrix, keys matrix and value matrix, respectively. In our setting, Q = K = V = H. d is the dimension of hidden units of BiLSTM, which equals to 2dh.\nMulti-head attention first linearly projects the queries, keys and values h times by using different linear projections. Then h projections perform the scaled dot-product attention in parallel. Finally, these results of attention are concatenated and once again projected to get the new representation. Formally, the multi-head attention can be expressed as follows:\nheadi = Attention(QWQi ,KW K i ,VW V i ) (7) H \u2032 = (headi \u2295 . . .\u2295 headh)Wo (8)\nwhere WQi \u2208 R2dh\u00d7dk , WKi \u2208 R2dh\u00d7dk and WVi \u2208 R2dh\u00d7dk are trainable projection parameters and dk = 2dh/h. Wo \u2208 R2dh\u00d72dh is also trainable parameter."}, {"heading": "3.4 Task-Specific CRF", "text": "For a sentence in dataset of task k, we compute the final representation via concatenating the representations from private space and shared space after self-attention layer:\nH \u2032\u2032k = H \u2032k \u2295 S\u2032k (9)\nwhere H\u2032k and S \u2032k are the outputs of private selfattention and shared self-attention of task k, respectively.\nConsidering the dependencies between successive labels, we exploit CRF (Lafferty et al., 2001) to inference tags instead of making tagging decisions using h\u2032\u2032i independently. Due to the difference of labels, we introduce a specific CRF layer for each task. Given a sentence x = {c1, c2, . . . , cN} with a predicted tag sequence y = {y1, y2, . . . , yN}, the CRF tagging process can be formalized as follows:\noi = Wsh \u2032\u2032 i + bs (10)\ns(x, y) = N\u2211 i=1 (oi,yi + Tyi\u22121,yi) (11) y\u0304 = arg max y\u2208Yx s(x, y) (12)\nwhere Ws \u2208 R|T |\u00d74dh and bs \u2208 R|T | are trainable parameters. |T | denotes the number of output labels. oi,yi represents the score of the yi-th tag\nof the character ci. T is a transition score matrix which defines the scores of two successive labels. Yx represents all candidate tag sequences for given sentence x. In decoding, we use Viterbi algorithm to get the predicted tag sequence y\u0304.\nFor training, we exploit negative log-likelihood objective as the loss function. The probability of the ground-truth label sequence is computed by:\np(y\u0302|x) = e s(x,y\u0302)\u2211\ny\u0303\u2208Yx e s(x,\u0303y) (13)\nwhere y\u0302 denotes the ground-truth label sequence. Given T training examples (x(i); y\u0302(i)), the loss function LTask can be defined as follows:\nLTask = \u2212 T\u2211 i=1 logp(y\u0302(i)|x(i)) (14)\nWe use gradient back-propagation method to minimize the loss function."}, {"heading": "3.5 Task Discriminator", "text": "Inspired by adversarial networks (Goodfellow et al., 2014), we incorporate adversarial training into shared space to guarantee that specific features of tasks do not exist in shared space. We propose a task discriminator to estimate which task the sentence comes from. Formally, the task discriminator can be expressed as follows:\ns \u2032k = Maxpooling(S \u2032k) (15) D(s \u2032k; \u03b8d) = softmax(Wds \u2032k + bd) (16)\nwhere \u03b8d indicates the parameters of task discriminator. Wd \u2208 RK\u00d72dh and bd \u2208 RK are trainable parameters. K is the number of tasks.\nBesides the task lossLTask, we introduce an adversarial loss LAdv to prevent specific features of CWS task from creeping into shared space. The adversarial loss trains the shared model to produce shared features such that the task discriminator cannot reliably recognize which task the sentence comes from. The adversarial loss can be computed as follows:\nLAdv = min \u03b8s (max \u03b8d K\u2211 k=1 Tk\u2211 i=1 logD(Es(x (i) k )))\n(17) where \u03b8s denotes the trainable parameters of shared BiLSTM.Es denotes the shared feature extractor. Tk is the number of training examples of\ntask k. x(i)k is the i-th example of task k. There is a minimax optimization that the shared BiLSTM generates a representation to mislead the task discriminator and the discriminator tries its best to correctly determine the type of task.\nWe add a gradient reversal layer (Ganin and Lempitsky, 2014) below the softmax layer to address the minimax optimization problem. In the training phrase, we minimize the task discriminator errors, and through gradient reversal layer the gradients will become opposed sign to adversarially encourage the shared feature extractor to learn task-shared word boundary information. After training phrase, the shared feature extractor and task discriminator reach a point where the discriminator cannot differentiate the tasks according to the representations learned from shared feature extractor."}, {"heading": "3.6 Training", "text": "The final loss function of our proposed model can be written as follows:\nL = LNER \u00b7 I(x) + LCWS \u00b7 (1\u2212 I(x)) + \u03bbLAdv (18) where \u03bb is a hyper-parameter. LNER and LCWS can be computed via Eq.14. I(x) is a switching function to identify which task the input comes from. It is defined as follows:\nI(x) = { 1, if x \u2208 DNER 0, if x \u2208 DCWS\n(19)\nwhere DNER and DCWS are Chinese NER training corpora and CWS training corpora, respectively.\nIn the training phrase, at each iteration, we first select a task from {NER,CWS} in turn. Then, we sample a batch of training instances from the given task to update the parameters. We use Adam (Kingma and Ba, 2014) algorithm to optimize the final loss function. Since Chinese NER task and CWS task may have different convergence rate, we repeat the above iterations until early stopping according to the Chinese NER task performance."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Datasets", "text": "To evaluate our proposed model on Chinese NER, we experiment on two different widely used datasets, including Weibo NER dataset (WeiboNER) (Peng and Dredze, 2015; He and Sun,\n2017a) and SIGHAN2006 NER dataset (SighanNER) (Levow, 2006). We use the MSR dataset (from SIGHAN2005) for CWS task.\nThe WeiboNER is annotated with four entity types (person, location, organization and geopolitical entities), including named entities and nominal mentions. The SighanNER is simplified Chinese, which contains three entity types (person, location and organization). For WeiboNER, we use the same training, development and testing splits as Peng and Dredze (2015). Since the SighanNER does not have development set, we sample 10% data of training set as development set. We use MSR dataset to improve the performance of the Chinese NER task. Table 1 gives the details of the three datasets."}, {"heading": "4.2 Settings", "text": "For evaluation, we use the Precision (P), Recall (R) and F1 score as metrics in our experiment.\nFor hyper-parameter configurations, we adjust them according to the performance on development set of Chinese NER task. We set the character embedding size de to 100. The dimensionality of LSTM hidden states dh is 120. The initial learning rate is set to 0.001. The loss weight coefficient \u03bb is set to 0.06. We set the dropout rate to 0.3.\nThe number of projections h is 8. We set the batch size of SighanNER and WeiboNER as 64 and 20, respectively.\nFor trainable parameters initialization, we use xavier initializer (Glorot and Bengio, 2010) to initialize parameters. The character embeddings used in our experiment are pre-trained on Baidu Encyclopedia corpus and Weibo corpus by using word2vec toolkit (Mikolov et al., 2013)."}, {"heading": "4.3 Compared with State-of-the-art Methods", "text": "In this section, we will give the experimental results of our proposed model and previous stateof-the-art methods on WeiboNER dataset and SighanNER dataset, respectively."}, {"heading": "4.3.1 Evaluation on WeiboNER", "text": "We compare our proposed model with the latest models on WeiboNER dataset. Table 2 shows the experimental results for named entities on the original WeiboNER dataset.\nIn the first block of Table 2, we give the performance of the main model and baselines proposed by Peng and Dredze (2015). They propose a CRF-based model to jointly train the embeddings with NER task, which achieves better results than pipeline models. In addition, they consider the po-\nsition of each character in a word to train character and position embeddings.\nIn the second block of Table 2, we report the performance of the main model and baselines proposed by Peng and Dredze (2016). Aiming to incorporate word boundary information into the NER task, they propose an integrated model that can joint training CWS task, improving the F1 score from 46.20% to 48.41% as compared with pipeline model (Pipeline Seg.Repr.+NER).\nIn the last block of Table 2, we give the experimental result of our proposed model (BiLSTM+CRF+adversarial+self-attention). We can observe that our proposed model significantly outperforms other models. Compared with the model proposed by Peng and Dredze (2016), our method gains 4.67% improvement in F1 score. Interestingly, WeiboNER dataset and MSR dataset are different domains. The WeiboNER dataset is social media domain, while the MSR dataset can be regard as news domain. The improvement of performance indicates that our proposed adversarial transfer learning framework may not only learn task-shared word boundary information from CWS task but also tackle the domain adaptation problem.\nWe also conduct an experiment on the updated WeiboNER dataset. Table 3 lists the performance of the latest models and our proposed model on the updated dataset. In the first block of Table 3,\nwe report the performance of the latest models. The model proposed by Peng and Dredze (2015) achieves F1 score of 56.05% on overall performance. He and Sun (2017b) propose an unified model for Chinese NER task to exploit the data from out-of-domain corpus and in-domain unlabelled texts. The unified model improves the F1 score from 54.82% to 58.23% compared with the model proposed by He and Sun (2017a).\nIn the second block of Table 3, we give the result of our proposed model. It can be observed that our proposed model achieves a very competitive performance. Compared with the latest model proposed by He and Sun (2017b), our model improves the F1 score from 58.23% to 58.70% on overall performance. The improvement demonstrates the effectiveness of our proposed model."}, {"heading": "4.3.2 Evaluation on SighanNER", "text": "Table 4 lists the comparisons on SighanNER dataset. We observe that our proposed model achieves new state-of-the-art performance.\nIn the first block, we give the performance of previous methods for Chinese NER task on SighanNER dataset. Chen et al. (2006) propose a character-based CRF model for Chinese NER task. Zhou et al. (2006) introduce a pipeline model, which first segments the text with characterlevel CRF model and then applies word-level CRF to tag. Luo and Yang (2016) first train a word segmenter and then use word segmentation as addi-\ntional features for sequence tagging. Although the model achieves competitive performance, giving the F1 score of 89.21%, it suffers from the error propagation problem.\nIn the second block, we report the result of our proposed model. Compared with the state-ofthe-art model proposed by Luo and Yang (2016), our method improves the F1 score from 89.21% to 90.64% without any additional features, which demonstrates the effectiveness of our proposed model."}, {"heading": "4.4 Effectiveness of Adversarial Transfer Learning and Self-Attention", "text": "Table 5 provides the experimental results of our proposed model and baseline as well as its simplified models on SighanNER dataset and WeiboNER dataset. The simplified models are described as follows:\n\u2022 BiLSTM+CRF: The model is used as strong baseline in our work, which is trained using Chinese NER training data.\n\u2022 BiLSTM+CRF+transfer: We apply transfer learning to BiLSTM+CRF model without adversarial loss and self-attention mechanism.\n\u2022 BiLSTM+CRF+adversarial: Compared with BiLSTM+CRF+transfer model, the BiLST-\nM+CRF+adversarial model incorporates adversarial training.\n\u2022 BiLSTM+CRF+self-attention: The model integrates the self-attention mechanism based on BiLSTM+CRF model.\nFrom the experimental results of Table 5, we have following observations:\n\u2022 Effectiveness of transfer learning. BiLSTM+CRF+transfer improves F1 score from 89.13% to 89.89% as compared with BiLSTM+CRF on SighanNER dataset and achieves 1.08% improvement on WeiboNER dataset, which indicates the word boundary information from CWS is very effective for Chinese NER task.\n\u2022 Effectiveness of adversarial training. By introducing adversarial training, BiLSTM+CRF+adversarial boosts the performance as compared with BiLSTM+CRF+transfer model, showing 0.15% and 0.36% improvement on SighanNER dataset and WeiboNER dataset, respectively. It proves that adversarial training can prevent specific features of CWS task from creeping into shared space.\n\u2022 Effectiveness of self-attention mechanism. When compared with BiLSTM+CRF, the\nBiLSTM+CRF+self-attention significantly improves the performance on the two different datasets with the help of information learned from self-attention, which verifies that the self-attention mechanism is effective for Chinese NER task.\nWe observe that our proposed adversarial transfer learning framework and self-attention lead to noticeable improvements over the baseline, improving F1 score from 51.01% to 53.08% on WeiboNER dataset and giving 1.51% improvement on SighanNER dataset."}, {"heading": "4.5 Detailed Analysis", "text": ""}, {"heading": "4.5.1 Case Study", "text": "Word boundary information from CWS task is very important for Chinese NER task, especially when different entities appear together, . We take a sentence in WeiboNER test set as example for illustrating the effectiveness of our proposed model. As shown in Figure 4(a), when two \u201cperson\u201d entities appearing together, our proposed method exploits word segmentation information to determine the boundary between them and then make correct taggings. In Figure 4(b), when labelling the word \u201c \u00f8 (the boss)\u201d, the self-attention explicitly learns the dependencies with \u201c \u00cd (respect)\u201d, therefore, our model enables to correctly classify the word into \u201cperson\u201d category. It verifies that the self-attention is very effective for Chinese NER task."}, {"heading": "4.5.2 Error Analysis", "text": "According to the results of Table 2 and Table 4, our proposed model achieves 4.67% and 1.43% improvement as compared with previous stateof-the-art methods on WeiboNER dataset and SighanNER dataset, respectively. However, the overall performance on WeiboNER dataset is relatively low. Two reasons can be explained for this issue. One reason is that the number of training examples in WeiboNER dataset is very limited as compared with SighanNER dataset. There are only 1.3k examples in WeiboNER training corpora, which is not enough to train deep neural networks. Another reason is that the expression is informal in social media, lowering the performance on WeiboNER dataset. While the greater improvement on WeiboNER dataset proves that our method is helpful to solve the problem."}, {"heading": "5 Conclusions", "text": "In this paper, we propose a novel adversarial transfer learning framework for Chinese NER task, which can exploit task-shared word boundaries features and prevent the specific information of CWS task. Besides, we introduce self-attention mechanism to capture the dependencies of arbitrary two characters and learn the inner structure information of sentence. Experiments on two different widely used datasets demonstrate that our method significantly and consistently outperforms previous state-of-the-art models."}, {"heading": "Acknowledgments", "text": "The research work is supported by the Natural Science Foundation of China (No.61533018 and No.61702512), and the independent research project of National Laboratory of Pattern Recognition. This work is also supported in part by Beijing Unisound Information Technology Co., Ltd."}], "year": 2018, "references": [{"title": "Nymble: a highperformance learning name-finder", "authors": ["Daniel M Bikel", "Scott Miller", "Richard Schwartz", "Ralph Weischedel."], "venue": "Proceedings of the fifth conference on Applied natural language processing, pages 194\u2013201. Association for Compu-", "year": 1997}, {"title": "Domain separation networks", "authors": ["Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan."], "venue": "Advances in Neural Information Processing Systems, pages 343\u2013 351.", "year": 2016}, {"title": "A shortest path dependency kernel for relation extraction", "authors": ["Razvan C Bunescu", "Raymond J Mooney."], "venue": "Proceedings of the conference on human language technology and empirical methods in natural language processing, pages 724\u2013731. Associa-", "year": 2005}, {"title": "Chinese named entity recognition with conditional probabilistic models", "authors": ["Aitao Chen", "Fuchun Peng", "Roy Shan", "Gordon Sun."], "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 173\u2013176.", "year": 2006}, {"title": "Adversarial deep averaging networks for cross-lingual sentiment classification", "authors": ["Xilun Chen", "Yu Sun", "Ben Athiwaratkun", "Claire Cardie", "Kilian Weinberger."], "venue": "arXiv preprint arXiv:1606.01614.", "year": 2016}, {"title": "Adversarial multi-criteria learning for chinese word segmentation", "authors": ["Xinchi Chen", "Zhan Shi", "Xipeng Qiu", "Xuanjing Huang."], "venue": "arXiv preprint arXiv:1704.07556.", "year": 2017}, {"title": "Event extraction via dynamic multi-pooling convolutional neural networks", "authors": ["Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "authors": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12(Aug):2493\u20132537.", "year": 2011}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "authors": ["Emily L Denton", "Soumith Chintala", "Rob Fergus"], "venue": "In Advances in neural information processing systems,", "year": 2015}, {"title": "Finding structure in time", "authors": ["Jeffrey L Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "year": 1990}, {"title": "Unsupervised domain adaptation by backpropagation", "authors": ["Yaroslav Ganin", "Victor Lempitsky."], "venue": "arXiv preprint arXiv:1409.7495.", "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "authors": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256.", "year": 2010}, {"title": "Generative adversarial nets", "authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio."], "venue": "Advances in neural information processing systems, pages 2672\u20132680.", "year": 2014}, {"title": "Part-of-speech tagging for twitter with adversarial neural networks", "authors": ["Tao Gui", "Qi Zhang", "Haoran Huang", "Minlong Peng", "Xuanjing Huang."], "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2411\u2013", "year": 2017}, {"title": "F-score driven max margin neural network for named entity recognition in chinese social media", "authors": ["Hangfeng He", "Xu Sun."], "venue": "EACL 2017, page 713.", "year": 2017}, {"title": "A unified model for cross-domain and semi-supervised named entity recognition in chinese social media", "authors": ["Hangfeng He", "Xu Sun."], "venue": "AAAI, pages 3216\u20133222.", "year": 2017}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "authors": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "arXiv preprint arXiv:1508.01991.", "year": 2015}, {"title": "Efficient support vector classifiers for named entity recognition", "authors": ["Hideki Isozaki", "Hideto Kazawa."], "venue": "Proceedings of the 19th international", "year": 2002}, {"title": "Cross-lingual transfer learning for pos tagging without cross-lingual resources", "authors": ["Joo-Kyung Kim", "Young-Bum Kim", "Ruhi Sarikaya", "Eric Fosler-Lussier."], "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process-", "year": 2017}, {"title": "Adam: A method for stochastic optimization", "authors": ["Diederik P Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "authors": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira"], "year": 2001}, {"title": "Neural architectures for named entity recognition", "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "arXiv preprint arXiv:1603.01360.", "year": 2016}, {"title": "The third international chinese language processing bakeoff: Word segmentation and named entity recognition", "authors": ["Gina-Anne Levow."], "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108\u2013117.", "year": 2006}, {"title": "Adversarial multi-task learning for text classification", "authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "arXiv preprint arXiv:1704.05742.", "year": 2017}, {"title": "An empirical study of automatic chinese word segmentation for spoken language understanding and named entity recognition", "authors": ["Wencan Luo", "Fan Yang."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "year": 2013}, {"title": "Named entity recognition for chinese social media with jointly trained embeddings", "authors": ["Nanyun Peng", "Mark Dredze."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 548\u2013554.", "year": 2015}, {"title": "Improving named entity recognition for chinese social media with word segmentation representation learning", "authors": ["Nanyun Peng", "Mark Dredze."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, volume 2,", "year": 2016}, {"title": "Disan: Directional self-attention network for rnn/cnnfree language understanding", "authors": ["Tao Shen", "Tianyi Zhou", "Guodong Long", "Jing Jiang", "Shirui Pan", "Chengqi Zhang."], "venue": "arXiv preprint arXiv:1709.04696.", "year": 2017}, {"title": "Deep semantic role labeling with self-attention", "authors": ["Zhixing Tan", "Mingxuan Wang", "Jun Xie", "Yidong Chen", "Xiaodong Shi."], "venue": "arXiv preprint arXiv:1712.01586.", "year": 2017}, {"title": "Attention is all you need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin."], "venue": "Advances in Neural Information Processing Systems, pages 6000\u20136010.", "year": 2017}, {"title": "Named entity recognition with gated convolutional neural networks", "authors": ["Chunqi Wang", "Wei Chen", "Bo Xu."], "venue": "Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, pages 110\u2013121. Springer.", "year": 2017}, {"title": "Adversarial learning for chinese ner from crowd annotations", "authors": ["YaoSheng Yang", "Meishan Zhang", "Wenliang Chen", "Wei Zhang", "Haofen Wang", "Min Zhang."], "venue": "arXiv preprint arXiv:1801.05147.", "year": 2018}, {"title": "Information extraction over structured data: Question answering with freebase", "authors": ["Xuchen Yao", "Benjamin Van Durme."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 956\u2013966.", "year": 2014}, {"title": "Aspect-augmented adversarial networks for domain adaptation", "authors": ["Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "arXiv preprint arXiv:1701.00188.", "year": 2017}, {"title": "Chinese named entity recognition with a multi-phase model", "authors": ["Junsheng Zhou", "Liang He", "Xinyu Dai", "Jiajun Chen."], "venue": "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 213\u2013216.", "year": 2006}, {"title": "Chinese named entity recognition via joint identification and categorization", "authors": ["Junsheng Zhou", "Weiguang Qu", "Fen Zhang."], "venue": "Chinese journal of electronics, 22(2):225\u2013230.", "year": 2013}], "id": "SP:5860a361b9987572405b71a6b78d9bce347d1c40", "authors": [{"name": "Pengfei Cao", "affiliations": []}, {"name": "Yubo Chen", "affiliations": []}, {"name": "Kang Liu", "affiliations": []}, {"name": "Jun Zhao", "affiliations": []}, {"name": "Shengping Liu", "affiliations": []}], "abstractText": "Named entity recognition (NER) is an important task in natural language processing area, which needs to determine entities boundaries and classify them into pre-defined categories. For Chinese NER task, there is only a very small amount of annotated data available. Chinese NER task and Chinese word segmentation (CWS) task have many similar word boundaries. There are also specificities in each task. However, existing methods for Chinese NER either do not exploit word boundary information from CWS or cannot filter the specific information of CWS. In this paper, we propose a novel adversarial transfer learning framework to make full use of task-shared boundaries information and prevent the taskspecific features of CWS. Besides, since arbitrary character can provide important cues when predicting entity type, we exploit selfattention to explicitly capture long range dependencies between two tokens. Experimental results on two different widely used datasets show that our proposed model significantly and consistently outperforms other state-ofthe-art methods.", "title": "Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism"}