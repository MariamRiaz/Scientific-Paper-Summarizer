{"sections": [{"heading": "1. Introduction", "text": "For computational problems of major practical interest (satisfiability, planning, etc.) the computing science community has developed a large number of highly configurable \u201csolvers.\u201d The reason is that while the hardest problem instances take a long time to solve by any of the solvers, the instances that one encounters in practical applications may exhibit specific properties so that the appropriate solver\n1DeepMind, London, UK. 2On leave from Imperial College London, London, UK. 3On leave from University of Alberta, Edmonton, AB, Canada. Correspondence to: Gelle\u0301rt Weisz <gellert@google.com>, Andra\u0301s Gyo\u0308rgy <agyorgy@google.com>, Csaba Szepesva\u0301ri <szepi@google.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwith an appropriate configuration may finish much faster. The plethora of solvers and their configurations, which for simplicity of presentation we will just treat as configurations from this point on, is explained by the diversity of applications. Which configuration to use in a specific application can then be treated as a learning problem, where an application is identified with an unknown distribution over problem instances that one can sample from, the learning algorithm can run any configuration on any sampled instance until a timeout of its choice, and the goal is to find a configuration with nearly optimal expected runtime while using the least amount of time during the search.1 There has been much practical success on designing such blackbox configuration search methods, especially in the context of satisfiability problems. Examples of successful methods include ParamILS (Hutter, 2007; Hutter et al., 2009), SMAC (Hutter et al., 2011; 2013), irace (Birattari et al., 2002; Lo\u0301pez-Iba\u0301nez et al., 2011), and GGA (Anso\u0301tegui et al., 2009; 2015). These methods themselves rely on many heuristics and as such lack theoretical guarantees.\nRecently, Kleinberg et al. (2017) explored this problem, presenting a general-purpose configuration optimizer called Structured Procrastination, with guarantees on both (i) how close to the optimal configuration the algorithm\u2019s result is, and (ii) how long it takes to find such a configuration. For (ii), Kleinberg et al. (2017) prove that the expected runtime of their algorithm is within a logarithmic factor of the optimal runtime in a worst-case sense. Furthermore, they show that the gap between worst-case runtimes of existing algorithms (SMAC, ROAR, ParamILS, GGA, irace) and their solution can be arbitrarily large. Structured Procrastination attempts to refine the runtime guarantee for the empirically fastest solver and solves tasks in increasing order of difficulty, postponing difficult tasks until all simpler tasks have been solved. The main novelty of their work is that it comes with theoretical guarantees (lower and upper bounds on the runtime), but no empirical illustration is provided.\nThis paper builds on the results of Kleinberg et al. (2017), and our problem statement closely follows theirs. Our\n1Related, but different problems are considered, e.g., by Luby et al. (1993); Adam (2001); Mnih et al. (2008); Audibert & Bubeck (2010); Gyo\u0308rgy & Kocsis (2011); Li et al. (2016).\nmain technical contributions are as follows: We present an (arguably simpler) algorithm (LEAPSANDBOUNDS) that finds an approximately optimal configuration with a worstcase runtime bound that improves upon that of Kleinberg et al. (2017), while we consider a broader class of problems (we don not need their global runtime cap). We also present instance-dependent runtime bounds that show that LEAPSANDBOUNDS finishes faster if the runtime of the configurations over different problem instances has small variance. Experiments were carried out to assess practical performance of both Structured Procrastination and LEAPSANDBOUNDS on configuring the open-source minisat solver. LEAPSANDBOUNDS runs every configuration for less time than Structured Procrastination, and returns significantly faster. Finally, to facilitate further research and enable direct comparison to our results, our large-scale measurements on running times of the minisat solver are published together with the paper.2\nThe rest of the paper is organized as follows: The problem is introduced formally in Section 2. For clarity, the most basic version of our algorithm is presented first in Section 3, and its performance is analyzed in Section 4. Improvements to our method, together with their analyses, are presented in Section 5. Experimental results are presented in Section 6, followed by some notes on parallel implementation in Section 7. Finally, conclusions are drawn in Section 8."}, {"heading": "2. Problem Statement", "text": "Following Kleinberg et al. (2017), the algorithm configuration problem is defined by a tuple (N ,\u0393, R, \u03ba0) as follows:3 Here, N is a family of configurations and \u0393 is a distribution over input instances.4 For now, we consider the case when N is a finite set. If we have a benchmark set of instances, we let \u0393 be the uniform distribution over these benchmark instances. For configuration i \u2208 N and instance j, R(i, j) \u2208 [0,\u221e] is the execution time of configuration i on instance j. Finally, \u03ba0 > 0 is the minimum runtime: For all i, j pairs, R(i, j) \u2265 \u03ba0.\nWe let R(i) = EJ\u223c\u0393 [R(i, J)] denote the average runtime of configuration i on instances distributed according to \u0393, and define OPT = miniR(i) as the mean runtime of an optimal configuration. Our goal is to find such an optimal, or at least nearly optimal configuration while spending as little time as possible\u2013proportional to the runtime of the optimal configuration\u2013on this task. For this, a search algorithm can (i) sample instances J at random from \u0393; (ii) enumerate\n2https://github.com/deepmind/ leaps-and-bounds\n3Compared to their problem statement, we removed the global runtime cap from the definition as it is not required for our results.\n4For randomized solvers, input instances can mean (input instance, random seed) pairs.\nthe configurations in N ; (iii) run a configuration i on an instance j until it finishes, or the execution time exceeds a fixed timeout \u03c4 \u2265 0, chosen by the search algorithm. Practically, this means observing R(i, j, \u03c4) .= min(R(i, j), \u03c4) after time R(i, j, \u03c4), and also whether the calculation has finished with a solution or it timed out.\nThe main difficulty in organizing the search is that some configurations may take a long, or even infinite time to execute on some instances. Since an algorithm that claims to find a near-optimal configuration must verify that no other configuration can finish significantly faster than the chosen configuration, the total runtime is at least proportional to n\u00d7OPT, where n = |N | is the number of configurations to be tested. Since knowing the mean runtime up to a multiplicative accuracy of (1 + \u03b5) requires \u2126(1/\u03b52) samples even when the runtime distributions are light-tailed, relaxing the requirement to find a configuration i with runtime R(i) \u2264 (1 + \u03b5)OPT, we get that the total runtime is at least \u2126(n \u00d7 OPT/\u03b52). The situation worsens for heavy-tailed runtime distributions: If the runtime of an algorithm is b > 1 with probability 1/b and 0 otherwise, with b unknown, all sampling methods need to see at least one positive runtime to estimate the expected runtime up to any fixed accuracy. Thus, any sampling method needs to use at least \u2126(b) time, despite that the expected runtime is constant 1 (independently of b). This implies that in the face of heavy-tailed runtime distributions, the runtime of any sound configuration search algorithm would be unbounded in the worst-case, regardless the value of OPT, n and \u03b5. Since heavy tailed runtime distributions are quite common in practice, rather than constraining the problem by ruling these out, following Kleinberg et al. (2017), we relax the search criterion to that of finding an (\u03b5, \u03b4)-optimal configuration. Introducing the \u03c4 -capped version of R(i) as R\u03c4 (i) = EJ\u223c\u0393 [R(i, J, \u03c4)], we have the following definition for (\u03b5, \u03b4)-optimality:\nDefinition 1 ((\u03b5, \u03b4)-optimality). A configuration i\u2217 is (\u03b5, \u03b4)optimal if there exists some threshold \u03c4 such that R\u03c4 (i\u2217) \u2264 (1 + \u03b5)OPT, and PrJ\u223c\u0393 (R(i\u2217, J) > \u03c4) \u2264 \u03b4. Otherwise, we say i\u2217 is (\u03b5, \u03b4)-suboptimal.\nIn words, given (\u03b5, \u03b4), a sound configuration search algorithm must find a configuration i whose \u03c4 -capped mean runtime is at most (1 + \u03b5)OPT, with a \u03c4 larger than the \u03b4quantile of the runtime distribution of configuration i. This is a reasonable criterion when OPT is reasonably small.5\nIn this paper we introduce the algorithm LEAPSANDBOUNDS that identifies an (\u03b5, \u03b4)-optimal configuration with\n5If some problem instances are hopelessly hard, the expected runtime of even an optimal configuration can be infinite, in which case any configuration becomes (\u03b5, \u03b4)-optimal. To alleviate this problem, it would be more meaningful to define (\u03b5, \u03b4)-optimality with respect to the optimal capped runtime; this is left for future work (see Section 8 for more details).\nprobability 1 \u2212 \u03b6 for a failure parameter \u03b6 and has an expected runtime of O ( OPT n\u03b52\u03b4 log ( n log OPT \u03b6 )) . The method of Kleinberg et al. (2017) has an additional assumption that all runtimes of any configuration on any instance sampled from \u0393 are below a maximum \u03ba\u0304 that must also be known by the algorithm. While this renders the runtime distributions light-tailed, a nice feature of their method is that its runtime O ( OPT n\u03b52\u03b4 log ( n log \u03ba\u0304 \u03b6\u03b4\u03b52 )) has only a mild dependence on \u03ba\u0304. LEAPSANDBOUNDS does not require a runtime cap and we shave off a few terms from their bound: We replace the doubly logarithmic dependence on \u03ba\u0304 with an identical dependence on the practically much smaller OPT, and remove logarithmic terms that depend on \u03b4\u22121 and \u03b5\u22122. Kleinberg et al. (2017) also prove that the minimum worstcase runtime for any algorithm is \u2126 ( OPT n\u03b52\u03b4 ) , so both methods are within a logarithmic factor of the optimum.\nThe above results make sense when n = |N | is small enough to allow running each algorithm configuration. Similarly to Kleinberg et al. (2017), LEAPSANDBOUNDS can be extended to the case of an arbitrarily large number of configurations: by sampling n configurations randomly from the set of all configurations, the probability that none of the fastest \u03b3 fraction of configurations have been sampled is at most Ce\u2212n\u03b3 for a universal constant C > 0. Thus, by letting n = \u2308 1 \u03b3 log(C/\u03b6) \u2309 , with probability 1\u2212 2\u03b6, LEAPSANDBOUNDS returns an (\u03b5, \u03b4)-optimal configuration with respect to a configuration from the fastest \u03b3 fraction of configurations from the entire space."}, {"heading": "3. Algorithm", "text": "The main problem in finding a near-optimal solver configuration is that solving some instances may take arbitrarily long. To alleviate the problem, (\u03b5, \u03b4)-optimality only considers the mean of runtimes capped at a timeout, ensuring that at most a \u03b4 fraction of the worst instances run longer than this timeout. This makes estimating the average runtime of a configuration (over random instances) possible through sampling. The main issue with sampling is that computing the average runtime over the samples can be slowed down arbitrarily if we accidentally select a problem instance with a very large running time. This could be avoided if an oracle told us the runtime threshold \u03c4 in the definition of (\u03b5, \u03b4)optimality, but this is not available of course. To solve the problem, we present a configuration optimization algorithm called LEAPSANDBOUNDS (Algorithm 1).\nLEAPSANDBOUNDS attempts to guess a rough value of OPT, starting from a low value. Calling its guess \u03b8, the algorithm then tries to find a configuration with a mean runtime less than \u03b8. If this succeeds, it returns the configuration with the smallest mean found. Otherwise, \u03b8 is doubled and a new phase is started. The simplest way of measuring\nAlgorithm 1 LEAPSANDBOUNDS 1: Inputs:\nSet N of n algorithm configurations Precision parameter \u03b5 \u2208 (0, 13 ) Quantile parameter \u03b4 \u2208 (0, 1) Failure probability parameter \u03b6 \u2208 (0, 1) Lower runtime bound \u03ba0 > 0 Instance distribution \u0393\n2: Initialize: \u03b8 \u2190 167 \u03ba0, k \u2190 0, J \u2190 empty list 3: while True do 4: k \u2190 k + 1 . phase count 5: b\u2190 \u2308 44 log ( 6nk(k+1)\n\u03b6\n) 1 \u03b4\u03b52 \u2309 . instance bound\n6: Add b\u2212 |J | new instances sampled from \u0393 to J 7: for i \u2208 N do 8: Q\u0304i \u2190RUNTIMEEST (i,J , \u03b4, \u03b8) 9: end for\n10: if mini Q\u0304i < \u03b8 then 11: return argmini Q\u0304i 12: end if 13: \u03b8 \u2190 2\u03b8 14: end while\nthe mean runtime while guaranteeing (\u03b5, \u03b4)-optimality is to take runtime samples with timeout \u03b8\u03b4 and reject any algorithm that times out for any instance. Then, a concentration bound on the measurements could be used to ensure that the mean is close to the empirical mean. If the mean is less than \u03b8, Markov\u2019s inequality can be used to bound the tail probability for (\u03b5, \u03b4)-optimality. However, by rejecting any configuration that ever times out, we fail to measure the capped mean\u2013which could be significantly lower\u2013, and thus the algorithm may not stop at the right time. To fix this, we would ideally allow a \u03b4 fraction of runs to time out, but we use 34\u03b4 instead, to achieve a high-confidence tail bound with a Chernoff bound (replacing Markov\u2019s inequality). Still, the measurements could take a long time: if we perform b measurements for a reliable mean estimation with timeout \u03c4 , then we spend up to b\u03c4 time. A key observation is that if we spend more than b\u03b8 time on measurements, the average would have to be above \u03b8, and we would reject the configuration. Thus, we can specify an overall time budget of T = b\u03b8, and reject any configuration early if they run over this limit. These ideas are embodied in our algorithm (RUNTIMEEST)."}, {"heading": "4. Theoretical Analysis", "text": "In this section we explore the theoretical properties of LEAPSANDBOUNDS. We show that the estimates computed by the algorithm are reliable with high probability. Then we prove that if the estimates are reliable, the running time cannot be too large and the algorithm returns an (\u03b5, \u03b4)-optimal\nAlgorithm 2 The RUNTIMEEST subroutine 1: Inputs:\nConfiguration i Instance list J = (J1, . . . , Jb) of length b Quantile parameter \u03b4 \u2208 (0, 1) Average runtime bound \u03b8\n2: Initialize: T \u2190 b\u03b8 . overall runtime budget \u03c4 \u2190 4\u03b83\u03b4 . individual runtime budget j \u2190 1 . instance index 3: while True do 4: Run configuration i on Jj with timeout min{T, \u03c4} 5: Qj \u2190 R(i, Jj ,min{T, \u03c4}) 6: T \u2190 T \u2212Qj 7: // Stopping rules: 8: if T = 0 then . Stop if overall budget zero 9: return \u03b8 10: else if j = b then . Stop after b = |J | samples 11: return Q\u0304 = 1/b \u2211b m=1Qm . Return mean 12: end if 13: j \u2190 j + 1 14: end while\nsolution. We start with a few important observations about RUNTIMEEST.\n4.1. Guarantees for algorithm RUNTIMEEST\nConsider the execution of RUNTIMEEST with the inputs (i,J , \u03b8, b). Noting that the loop is stopped if the budget T0 = b\u03b8 gets exhausted, it follows that the total runtime of the (optimized) algorithm is bounded by b\u03b8:\nLemma 2. The runtime of one call to RUNTIMEEST is O(b\u03b8).\nWith T0 = b\u03b8, for j \u2265 1, define Tj = Tj\u22121 \u2212 Qj = b\u03b8 \u2212 (Q1 + \u00b7 \u00b7 \u00b7 + Qj). If the budget b\u03b8 is not exhausted (i.e., Tb = b\u03b8 \u2212 (Q1 + \u00b7 \u00b7 \u00b7 + Qb) > 0), each instance Jj runs within its min{Tj\u22121, \u03c4} \u2264 \u03c4 individual budget, and so Qj = R(i, Jj) = R(i, Jj , \u03c4) =: Rj . Clearly, Tb > 0 is equivalent to Q\u0304 < \u03b8. Furthermore, in any case, Qj = R(i, Jj ,min(Tj\u22121, \u03c4)) \u2264 Rj . Defining R\u0304 = (R1 + \u00b7 \u00b7 \u00b7 + Rb)/b, we can summarize these findings as follows:\nLemma 3. If RUNTIMEEST returns with Q\u0304<\u03b8, then \u2200j,Qj=Rj and Q\u0304=R\u0304. Otherwise, \u2200j,Qj\u2264Rj and Q\u0304\u2264R\u0304.\nLet us now turn to analyzing Algorithm 1. For this, we need some extra notation."}, {"heading": "4.2. Notation", "text": "Let \u03b8k, \u03c4k and bk denote the respective values of \u03b8, \u03c4 and b in phase k (Line 6 of Algorithm 1), noting that \u03c4k = 4\u03b8k3\u03b4 . Let Jj denote the jth instance (ever) sampled in Line 6 of\nAlgorithm 1. Note that for k large enough so that bk \u2265 j, Jj is the jth instance that is passed on to RUNTIMEEST by Algorithm 1 in phase k (for any configuration i). Let Ri,j,k = R(i, Jj , \u03c4k) be the \u03c4k-capped runtime of configuration i on instance Jj and let R\u0304i,k be the average of these values: R\u0304i,k = 1bk \u2211bk j=1Ri,j,k. Similarly, let Q\u0304i,k be the return value of algorithm RUNTIMEEST in phase k for configuration i, which is also the mean of (Qi,j,k)j , the runtimes observed at Line 5 of RUNTIMEEST. Let \u03c3\u03022i,k be the empirical variance of (Ri,j,k)j : \u03c3\u03022i,k = 1 bk \u2211bk j=1(Ri,j,k \u2212 R\u0304i,k)2."}, {"heading": "4.3. Good events", "text": "Let pi,k = PrJ\u223c\u0393 (R(i, J) > \u03c4k) denote the probability that configuration i does not finish on instance j in time \u03c4k. Next we define two events that ensure that the algorithm works well. First, let\nE1,i,k = {Q\u0304i,k = \u03b8k} \u222a {pi,k \u2264 \u03b4};\nif E1,i,k holds then if Algorithm 1 returns, the probability that the corresponding configuration fails to solve a random task within \u03c4k time is small (note that Q\u0304i,k \u2264 \u03b8k).\nThe next event guarantees that the average capped running time is close to its expectation: let\nE2,i,k = {|R\u0304i,k \u2212R\u03c4k(i)| \u2264 Ci,k} with\nCi,k = \u03c3\u0302i,k\n\u221a 2 log(6nk(k+1)\u03b6 )\nbk +\n3\u03c4k log( 6nk(k+1)\n\u03b6 )\nbk .\nThe main result of this section is to show that E1,i,k and E2,i,k hold with high probability for all i and k simultaneously:\nLemma 4. Let E = \u22c2 i\u2208{1,...,n},k\u2208Z+ (E1,i,k \u2229 E2,i,k). Then, Pr(E) \u2265 1\u2212 \u03b6.\nTo prove the lemma, we individually bound the probabilities that the events do not hold:\nLemma 5. Pr(Ec1,i,k) \u2264 \u03b6 2nk(k+1) .\nProof. If pi,k \u2264 \u03b4, then Pr(Ec1,i,k) = 0 and the statement holds trivially. For the rest of this proof, we assume that pi,k > \u03b4. From the algorithm, we have that bk \u2265 32\u03b4 log( 2nk(k+1) \u03b6 ). Define Bi,j,k as the Bernoulli random variable indicating whether configuration i on input Jj takes more time than \u03c4k to finish (value 1), or not (value 0). For \u03b4\u0302i,k = 1bk \u2211bk j=1Bi,j,k, observe that E(\u03b4\u0302i,k) = pi,k. If the algorithm returns with Q\u0304i,k < \u03b8, as necessary for event Ec1,i,k, then R\u0304i,k = Q\u0304i,k according to Lemma 3. Noting that Bi,j,k = I [Ri,j \u2265 \u03c4k], we have 4\u03b83\u03b4 \u2211 j Bi,j,k \u2264 \u2211 j Ri,j (since \u03c4k = 4\u03b83\u03b4 ). Therefore, 4\u03b8 3\u03b4 \u03b4\u0302i,k \u2264 R\u0304i,k = Q\u0304i,k < \u03b8, so \u03b4\u0302i,k \u2264 34\u03b4.\nApplying a Chernoff bound on the bk independent Bernoulli random variables Bi,j,k, the probability of the latter event can be bounded, giving\nPr(Ec1,i,k) = Pr(Q\u0304i,k < \u03b8) \u2264 Pr ( \u03b4\u0302i,k \u2264 3\n4 \u03b4 ) \u2264 Pr ( \u03b4\u0302i,k \u2264 3\n4 E(\u03b4\u0302i,k)\n) \u2264 exp ( \u2212E(\u03b4\u0302i,k)bk\n32\n)\n< exp ( \u2212 1\n32 \u03b4bk\n) \u2264 \u03b6\n2nk(k + 1) ,\nwhere the second and second to last inequalities follow from E(\u03b4\u0302i,k) > \u03b4.\nLemma 6. Pr(Ec2,i,k) \u2264 \u03b6 2nk(k+1) .\nProof. The samples (Ri,j,k)j are independent and identically distributed with mean R\u0304i,k and expectation R\u03c4k(i). Thus, the lemma holds by the empirical Bernstein bound (cf. Audibert et al., 2009, Theorem 1 and Appendix A).\nNow Lemma 4 follows from Lemmas 5 and 6 and the union bound (details are given in Appendix B)."}, {"heading": "4.4. Bounding the average runtime", "text": "Note that when the algorithm finishes, Q\u0304i,k = R\u0304i,k. Hence, in this section we focus on R\u0304i,k and its deviation from its mean. In particular, we show that |R\u03c4k(i) \u2212 R\u0304i,k| \u2264 3 7\u03b5R\u03c4k(i) holds on event E when phase k is preterm. Here, a phase k is called preterm if miniR\u03c4k(i) \u2265 716\u03b8k. The idea is that if a phase is preterm then the best capped expected runtime is large compared to the guess on the optimal runtime. We then show that on E, any phase executed by the algorithm is preterm.\nSince on E, |R\u03c4k(i) \u2212 R\u0304i,k| \u2264 Ci,k by the definition of E, we need to bound Ci,k. We start with a bound on the empirical variance \u03c3\u03022i,k. Lemma 7. For any preterm phase k, \u03c3\u03022i,k \u2264 3221\u03b4 (R\u0304i,k + R\u03c4k(i)) 2.\nProof. First we show that for any c > 0, \u03c3\u03022i,k \u2264 c ( R\u0304i,k +\n\u03c4k 2c\n)2 . (1)\nNotice that \u03c3\u03022i,k = 1 bk \u2211bk j=1(Ri,j,k \u2212 R\u0304i,k)2 \u2264\n1 bk \u2211bk j=1R 2 i,j,k \u2264 1bk \u2211bk j=1 \u03c4k Ri,j,k = \u03c4k R\u0304i,k because R\u0304i,k \u2264 \u03c4k by definition. Now (1) follows from the obvious R\u0304i,k\u03c4k \u2264 c ( R\u0304i,k + \u03c4k 2c )2 .\nBy the assumption on k, \u03b8k \u2264 167 R\u03c4k(i). Since \u03c4k = 4\u03b8k 3\u03b4 , this means that \u03c4k \u2264 64 21\u03b4R\u03c4k(i). Thus, applying (1) with c = 3221\u03b4 completes the proof as \u03c3\u0302 2 i,k \u2264\n32 21\u03b4 ( R\u0304i,k + 21\u03b4 64 \u03c4k )2 \u2264 3221\u03b4 (R\u0304i,k +R\u03c4k(i))2.\nCombining the above result with the upper bound \u03c4k = 4\u03b8k 3\u03b4 \u2264 64 21\u03b4R\u03c4k(i), which holds for any preterm phase k, simple algebra yields the following bound on Ci,k (the full proof is given in Appendix C):6\nLemma 8. For any preterm phase k, it holds that Ci,k \u2264 \u03b5 3 (R\u0304i,k +R\u03c4k(i)).\nNow we give the promised bound on |R\u03c4k(i)\u2212 R\u0304i,k|. Lemma 9. Assume E holds and Ci,k \u2264 \u03b53 (R\u0304i,k +R\u03c4k(i)). Then, |R\u03c4k(i)\u2212 R\u0304i,k| \u2264 37\u03b5R\u03c4k(i) for all configurations i.\nProof. Let us define x such that R\u0304i,k = (1 + x)R\u03c4k(i). Because E2,i,k holds, |x|R\u03c4k(i) = |R\u03c4k(i)\u2212 R\u0304i,k| \u2264 Ci,k \u2264 \u03b5 3 (R\u0304i,k +R\u03c4k(i)) = \u03b5 3 (1 + 2x)R\u03c4k(i). So |x| \u2264 \u03b5 3 (1 + 2x). If x < 0, then x \u2265 \u2212 \u03b5/31+2\u03b5/3 > \u2212 3 7\u03b5. If x \u2265 0, then x \u2264 \u03b5/31\u22122\u03b5/3 \u2264 3 7\u03b5 because \u03b5 \u2264 1 3 .\nIn the analysis of the correctness and the running time of the algorithm, we only need the slightly weaker corollary of Lemma 8 and Lemma 9 (which also holds for another variant of our algorithm, as opposed to Lemma 8):\nCorollary 10. Assume E holds and phase k is preterm. Then, for each i, if R\u0304i,k < \u03b8k, then |R\u03c4k(i) \u2212 R\u0304i,k| \u2264 3 7\u03b5R\u03c4k(i); otherwise, if R\u0304i,k\u2265\u03b8k, then \u03b8k< (1+ 3 7\u03b5)R\u03c4k(i)."}, {"heading": "4.5. Correctness and runtime", "text": "In this section we show that Algortihm 1 returns an (\u03b5, \u03b4)optimal configuration, and give an upper bound on its running time. First we show the following result promised earlier:\nLemma 11. If E holds then every phase k executed is preterm.\nProof. The first phase is preterm as 716\u03b81 = \u03ba0 \u2264 R\u03c41(i). For a phase k \u2265 2 that is executed, since the algorithm did not return in phase k \u2212 1, by Lemma 3, R\u0304i,k\u22121 \u2265 Q\u0304i,k\u22121 = \u03b8k\u22121. If E holds and phase k \u2212 1 was preterm, by Corollary 10, \u03b8k\u22121 < (1 + 37\u03b5)R\u03c4k\u22121(i). Moreover,\n7 16\u03b8k = 7 8\u03b8k\u22121 \u2264 7 8 (1+ 3 7\u03b5)R\u03c4k\u22121(i) \u2264 R\u03c4k\u22121(i) \u2264 R\u03c4k(i),\nsince \u03b5 \u2264 13 . By induction, any phase executed is preterm.\nLemma 12. If E holds and Algorithm 1 returns with a configuration I in phase K, then I is (\u03b5, \u03b4)-optimal.\nProof. We prove the statement by contradiction. Thus, assume I is (\u03b5, \u03b4)-suboptimal. At stopping, Q\u0304I,K < \u03b8K , hence on E, pI,K = PrJ\u223c\u0393(R(I, J) > \u03c4K) \u2264 \u03b4 must hold.\n6The multiplicative constant in the proof is not optimized carefully to promote simplicity. Nevertheless, in our experiments the empirical effect of this constant is negligible.\nSince I is (\u03b5, \u03b4)-suboptimal, it follows that there exists an instance j such thatR\u03c4K (I) > (1+\u03b5)R(j) > (1+\u03b5)R\u03c4K (j). Take such an index j. Since Algorithm 1 returned I instead of j, Q\u0304I,K \u2264 Q\u0304j,K . By Lemma 3, \u03b8k > Q\u0304I,K = R\u0304I,K and R\u0304j,K \u2265 Q\u0304j,K . Applying Corollary 10 and Lemma 11, if R\u0304j,K < \u03b8k, then (1+ 37\u03b5)R\u03c4K (j) \u2265 R\u0304j,K \u2265 Q\u0304j,K \u2265 Q\u0304I,K . Otherwise, (1 + 37\u03b5)R\u03c4k(i) \u2265 \u03b8k \u2265 Q\u0304I,K . Using this,\nR\u03c4K (j)(1 + 3 7\u03b5) \u2265 Q\u0304I,K = R\u0304I,K > R\u03c4K (I)(1\u2212 3 7\u03b5)\n> R\u03c4K (j)(1 + \u03b5)(1\u2212 37\u03b5).\nTherefore, 1 + 37\u03b5 > (1 + \u03b5)(1 \u2212 3 7\u03b5) which leads to a contradiction since \u03b5 \u2264 13 .\nTheorem 13. Algorithm 1 identifies an (\u03b5, \u03b4)-optimal solution in time O ( OPT n\u03b52\u03b4 log( n log OPT \u03b6 ) ) with probability at least 1\u2212 \u03b6, where OPT = miniR(i).\nProof. By Lemma 4, E holds with probability at least 1\u2212 \u03b6 . The rest of the proof assumes that E holds.\nLet i\u2217 = argminiR(i). If \u03b8k \u2265 (1 + 37\u03b5)OPT \u2265 (1 + 3 7\u03b5)R\u03c4k(i\n\u2217), then only the first case of Corollary 10 can hold. Together with Lemma 11, we have that Q\u0304i\u2217,k \u2264 R\u0304i\u2217,k \u2264 (1 + 37\u03b5)R\u03c4k(i\n\u2217) \u2264 \u03b8k, so Algorithm 1 terminates for \u03b8k \u2265 (1 + 37\u03b5)OPT. Let the total number of phases of the outer loop of Algorithm 1 be L. Then L = O(log OPT).\nThe for loop on Line 7 of Algorithm 1 adds a factor of n to the runtime. By Lemma 2, calling algorithm RUNTIMEEST on Line 8 adds a factor of bk\u03b8k to the runtime. Now bk \u2264 \u2308( 44 log(6nL(L+1)\u03b6 ) 1 \u03b4\u03b52 )\u2309 =\nO (\n1 \u03b52\u03b4 log( 6n log2 OPT \u03b6 )\n) = O ( 1 \u03b52\u03b4 log( 6n log OPT \u03b6 ) ) , so\nsubstituting \u03b8k = 167 \u03ba02 k, the total runtime becomes\nO  \u2308 log2 ( (1+ 37 \u03b5) OPT \u03ba0 )\u2309\u2211 k=1 \u03ba02 k \u00b7 n \u03b52\u03b4 log ( 6n log OPT \u03b6 ) = O ( OPT n\n\u03b52\u03b4 log\n( n log OPT\n\u03b6\n)) .\nBy Lemma 12, when the algorithm returns, it returns with an (\u03b5, \u03b4)-optimal configuration.\n5. Optimizing RUNTIMEEST Our runtime analysis presented in the previous section used a worst-case upper bound for \u03c3\u0302i,k. Some instances may allow faster runtimes if we modify RUNTIMEEST to stop earlier in scenarios where the empirical variance is lower than this worst case bound. To do this, building on the approach of Mnih et al. (2008), we change the stopping rules of algorithm RUNTIMEEST and add two more rules as\nAlgorithm 3 Stopping rules 1: Q\u0304\u2190 1j \u2211j m=1Qm\n2: \u03c3\u03022 \u2190 1j \u2211j m=1 ( Qm \u2212 Q\u0304 )2 3: dj,k \u2190 4nk(k + 1)j(j + 1)/\u03b6 4: c\u2190 \u221a \u03c3\u03022\n2 log(3dj,k) j + 3\u03c4 log(3dj,k) j\n5: LB\u2190 Q\u0304\u2212 c 6: if T = 0 then . Stop if overall budget zero 7: return \u03b8 8: end if 9: if j = b then . Stop after b = |J | samples\n10: return Q\u0304 . Return mean of Q 11: end if 12: if (1 + 37\u03b5)LB \u2265 \u03b8 and Q\u0304 > \u03b8 then . LB too large 13: return \u03b8 14: end if 15: if j \u2265 \u2308 32 \u03b4 log dj,k \u2309 and c \u2264 \u03b53 ( Q\u0304+ LB ) then 16: return Q\u0304 . Return mean of Q 17: end if\ngiven in Algorithm 3. The code shown here should replace Lines 8\u201312 of RUNTIMEEST.\nWe outline a proof sketch that this algorithm is still correct and has the same runtime bound. We define the running averages in iteration j of RUNTIMEEST as Q\u0304i,j,k and R\u0304i,j,k. As before, we define an event, as a union of other events, that guarantees that the empirical estimates behave well. We keep the previously defined events E1,i,k and E2,i,k; note that E1,i,k corresponds to the estimate Q\u0304i,b,k. However, we need a similar event to E1,i,k for all iterations j: E\u2032i,j,k = {Q\u0304i,k \u2265 \u03b8k} \u222a {pi,k \u2264 \u03b4} \u222a {j < \u2308 32 \u03b4 log dj,k \u2309 }.\nLet E = \u22c2 i\u2208{1,...,n},j,k\u2208Z+ ( E1,i,k \u2229 E2,i,k \u2229 E\u2032i,j,k ) . Similarly to the previous section, it is easy to show that Pr(E) \u2265 1 \u2212 3\u03b6/2. If E holds and the algorithm returns with an average runtime less than \u03b8k, then E1,i,k and E\u2032i,j,k guarantee that Pr(R(i, j) > \u03c4k) \u2264 \u03b4 (independently of which stopping condition was activated). Since the original stopping rule is still in place, the runtime of algorithm RUNTIMEEST with the additional stopping rules is still O(bk\u03b8k). Similarly, it is easy to verify that Lemma 3 still holds.\nFurthermore, by a slight modification of Theorem 2 of Mnih (2008), one can show that with probability at least 1\u2212 \u03b6/2, |R\u03c4k(i) \u2212 R\u0304i,j,k| \u2264 ci,j,k holds for all i, j, k, and ci,j,k \u2264 \u03b5 3 ( Q\u0304i,j,k + LBi,j,k ) \u2264 \u03b53 ( R\u0304i,j,k +R\u03c4k(i) ) holds7 for all\nj \u2265 C\u00b7max\n( \u03c32i,k\n\u03b52R2\u03c4k(i) , \u03c4k \u03b5R\u03c4k(i)\n)( log 1\n\u03b6 \u2032 + log\n1\n\u03b5R\u03c4k(i)\n) ,\nwhereC is a universal constant, and Q\u0304i,j,k \u2264 R\u0304i,j,k. Denote 7Here, Lemma 3 was used additionally in the last inequality.\nthis event by E\u2032; then Pr(E\u2032) \u2265 1\u2212 \u03b6/2.8\nApplying Lemma 9, E\u2032 also implies that\n|R\u03c4k(i)\u2212 R\u0304i,j,k| \u2264 37\u03b5R\u03c4k(i).\nThus, if E \u222a E\u2032 holds, then Corollary 10 holds: if Algorithm 3 returns either because it went through all the bk samples or because of Line 12, then |R\u03c4k(i) \u2212 R\u0304i,k| \u2264 3 7\u03b5R\u03c4k(i), which implies the first part of the corollary; otherwise Algorithm 3 returns in line 10, implying that the algorithm returns with \u03b8k and (1 + 37\u03b5)R\u03c4k > \u03b8k. Then the runtime bound and the correctness guarantee of Theorem 13 follows as before.\nOn the other hand, if the variances of the runtimes over instances are low enough, it is possible to prove an improved runtime bound for the whole algorithm. For \u03b6 \u2032 = \u03b64nk(k+1) , there exists a constant C such that if\nj \u2265 C \u00b7 1\u03b4 ( log 1\u03b4 + log 1 \u03b6\u2032 ) , then j \u2265 \u2308 32 \u03b4 log dj,k \u2309 holds. Together with the previous lower bound on j and by upper bounding \u03c4k \u2264 6421\u03b4R\u03c4k(i), by the definition of a preterm phase (see Lemma 8), with probability at least 1\u2212 2\u03b6 , RUNTIMEEST evaluates at most\nC\u00b7max\n( \u03c32i,k\n\u03b52R2\u03c4k(i) ,\n1\n\u03b5\u03b4 ,\n1 \u03b4 log 1 \u03b4\n)( log 1\n\u03b6 \u2032 + log\n1\n\u03b5R\u03c4k(i) ) samples in any phase k for configuration i before the stopping conditions on Line 15 are be satisfied. This bound is usually much lower than the previous bk =\u2308 44 log ( 6nk(k+1)\n\u03b6\n) 1 \u03b4\u03b52 \u2309 : if the variance of runtimes is\nsufficiently low, this scales as \u03b5\u22121 rather than \u03b5\u22122 (the \u03b4\u22121 log \u03b4\u22121 term is negligible).\nMnih et al. (2008) also describe EBGStop, a slightly improved version of empirical Bernstein stopping, which applies Bernstein inequalities to bound the means of an exponentially increasing number of samples. This allows us to effectively replace log 1\u03b5R\u03c4k (i) with log log 1\u03b5R\u03c4k (i) in the bound presented above. We use this version of the algorithm in our experiments. For completeness, the pseudocode of this version is given in Appendix D."}, {"heading": "6. Experiments", "text": "To run experiments, we gathered a benchmark set of runtimes of different configurations on generated SAT problems. We used minisat9 (Sorensson & Een, 2005) as the SAT solver. The SAT problems were generated using CNFuzzDD,10 of which only those 20118 were kept that\n8The original event behind E\u2032 guarantees, via Bernstein\u2019s inequality, that the estimates for the means and variances are accurate enough.\n9We used version 2013/09/25. http://minisat.se/ 10http://fmv.jku.at/cnfuzzdd/\ntook at least about a second to solve for minisat with the default parameters. This was done so that the data reflects what happens when instances are nontrivial to solve. 972 different configurations were tested for minisat, which are described in Appendix E. The solver minisat was run with each configuration and instance combination. The unit of computation, \u03ba0, is one second of CPU time, and the experiments were ran with a timeout of 15 CPU minutes.11 To get a sense of this data, the capped mean runtimes R\u03c4 (i) for each configuration are shown in Fig. 1 in a sorted order. Here, the timeout \u03c4 was set separately for each configuration so that the tail probability PrJ\u223c\u0393(R(i, J) > \u03c4) was approximately \u03b4; the running times are shown for different values of \u03b4 (\u03b4 = 0 corresponds to the mean runtimes). From this, we can see a large difference between configurations. For a particularly \u201cfast\u201d configuration, Fig. 2 shows the distribution of runtimes on different instances. Note that because of the global time limit for executions, the final bucket includes runs that may take arbitrarily long.\nThe benchmark set of runtimes is used to quickly simulate runs of Structured Procrastination and LEAPSANDBOUNDS, as follows. A simulated environment acts as an oracle,\n11Our measurements have been scaled such that the unit of computation roughly corresponds to a second on commodity hardware as of 2018 rather than our machines. In this unit, about 83 CPU years were spent in total to generate this data.\nreturning precomputed values of R(i, j, \u03c4) when queried, accumulating the total time the algorithm under test would have run for.\nBoth LEAPSANDBOUNDS and Structured Procrastination often run the same configuration on the same instance with an increased time limit. Thus, both algorithms can benefit largely when the environment allows pausing and resuming of executions. This can be implemented either by saving the state of the execution when the actual runtime limit is reached, or by reloading the state from automatically saved checkpoints. However, resuming execution comes with an additional memory requirement, and may not always be feasible or preferable to restarts. Thus, we report our experiments for both cases.\nAfter each phase, in Line 13 of LEAPSANDBOUNDS, we double \u03b8. In fact, this multiplier is arbitrary, and changing it only affects the worst-case runtime up to a constant factor. In practice, a smaller multiplier, making smaller steps in \u03b8, typically overshoots the best average runtime less, thereby decreasing the total runtime for environments that allow resuming runs. On the other hand, a smaller multiplier leads to more phases, introducing more overheads in resuming jobs and increasing the total runtime if resuming is not allowed by rerunning portions of jobs more frequently. The value of the multiplier can be optimized by taking these effects into account, e.g., by measuring the overheads related to switching and resuming jobs. For simplicity, and since this information is not included in our benchmark dataset, in the experiments below the value of the multiplier was set to 1.25 (see Appendix F for more details).\nWe simulated LEAPSANDBOUNDS and Structured Procrastination on our benchmark dataset with parameters \u03b5 = 0.2, \u03b4 = 0.2, and \u03b6 = 0.1. Fig. 3 shows that LEAPSANDBOUNDS runs every configuration for a significantly shorter amount of time than Structured Procrastination. The configurations are sorted in the same order as in Fig. 1, for \u03b4 = 0.2. Paradoxically, both algorithms run the faster configurations significantly longer. This is because both algorithms quickly\nreject slow configurations, whereas they both run fast configurations many more times to ensure (\u03b5, \u03b4)-optimality. In total, LEAPSANDBOUNDS runs for 933.50 CPU days in the environment that does not support resuming execution, and 368.50 days in one that does. The corresponding runtime measurements for Structured Procrastination are 1850.46 and 1169.36, respectively. Both algorithms return with configuration 898, which has the best average runtime below a \u03b4 = 0.2 quantile, out of all configurations."}, {"heading": "7. Parallelization", "text": "One benefit of the simplicity of LEAPSANDBOUNDS is that it is embarrassingly parallel. This is due to the fact that there is little dependency between the runtime measurements that need to be carried out. In phase k, when \u03b8k = 167 \u03ba02 k, runs\nof the form R ( i, j, \u03ba02 k+6\n21\u03b4\n) are carried out. The core of\nour argument is that this parallelizes over i, j, and k, but there are three further considerations. First, to implement the overall runtime bound of RUNTIMEEST, for any fixed i and k, the runs of R ( i, j, \u03ba02 k+6\n21\u03b4\n) should be terminated\nonce the summed running times of these reach the overall budget bk\u03b8k. This could be implemented either via interprocess communication or by starting these runs at once on p processors and terminating them after bk\u03b8k/p time. Second, a new phase k of Algorithm 1 should only be started once Q\u0304i,k is available for all i \u2208 N . Thus, runs should be started in increasing order of k, for each i. Third, the optional empirical Bernstein stopping, as described in Section 5, adds a dependency between runs of different j. This could be resolved either by not parallelizing over j, or by running only a small number of parallel runs over j and checking the stopping conditions after they finish."}, {"heading": "8. Conclusions and Future Work", "text": "We have introduced an algorithm applying empirical Bernstein stopping with the goal of finding approximately optimal configurations, and provided guarantees for its worstcase runtime as well as correctness. Our runtime guarantee is tighter than that of Structured Procrastination, which, to our knowledge, is the only other method solving this problem. Empirical evaluations suggest that LEAPSANDBOUNDS outperforms Structured Procrastination in realistic, non-adversarial scenarios too, which depends crucially on leveraging the gap between worst-case and realistic scenarios by using empirical Bernstein stopping.\nThe optimality of the configuration returned by LEAPSANDBOUNDS is, in fact, with respect to configurations with timeout \u03c4K for the final phase K. An important direction of future work is to get guarantees with respect to the best configuration for the fastest (1\u2212 \u03b4\u2032)-proportion of instances for any \u03b4\u2032 < \u03b4."}], "year": 2018, "references": [{"title": "Learning while searching for the best alternative", "authors": ["K. Adam"], "venue": "Journal of Economic Theory,", "year": 2001}, {"title": "A genderbased genetic algorithm for the automatic configuration of algorithms", "authors": ["C. Ans\u00f3tegui", "M. Sellmann", "K. Tierney"], "venue": "In International Conference on Principles and Practice of Constraint Programming,", "year": 2009}, {"title": "Model-based genetic algorithms for algorithm configuration", "authors": ["C. Ans\u00f3tegui", "Y. Malitsky", "H. Samulowitz", "M. Sellmann", "K. Tierney"], "venue": "In International Joint Conference on Artificial Intelligence,", "year": 2015}, {"title": "Best arm identification in multi-armed bandits", "authors": ["Audibert", "J.-Y", "S. Bubeck"], "venue": "In Conference on Learning Theory, pp", "year": 2010}, {"title": "Explorationexploitation tradeoff using variance estimates in multiarmed bandits", "authors": ["Audibert", "J.-Y", "R. Munos", "C. Szepesv\u00e1ri"], "venue": "Theoretical Computer Science,", "year": 1876}, {"title": "A racing algorithm for configuring metaheuristics", "authors": ["M. Birattari", "T. St\u00fctzle", "L. Paquete", "K. Varrentrapp"], "venue": "In Annual Conference on Genetic and Evolutionary Computation,", "year": 2002}, {"title": "Efficient multi-start strategies for local search algorithms", "authors": ["A. Gy\u00f6rgy", "L. Kocsis"], "venue": "Journal of Artificial Intelligence Research,", "year": 2011}, {"title": "On the potential of automatic algorithm configuration", "authors": ["F. Hutter"], "venue": "In SLS-DS2007: Doctoral Symposium on Engineering Stochastic Local Search Algorithms,", "year": 2007}, {"title": "ParamILS: an automatic algorithm configuration framework", "authors": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown", "T. St\u00fctzle"], "venue": "Journal of Artificial Intelligence Research,", "year": 2009}, {"title": "Sequential model-based optimization for general algorithm configuration", "authors": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "In International Conference on Learning and Intelligent Optimization,", "year": 2011}, {"title": "Bayesian optimization with censored response data", "authors": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "venue": "CoRR, abs/1310.1947,", "year": 2013}, {"title": "Efficiency through procrastination: Approximately optimal algorithm configuration with runtime guarantees", "authors": ["R. Kleinberg", "K. Leyton-Brown", "B. Lucier"], "venue": "In International Joint Conference on Artificial Intelligence,", "year": 2017}, {"title": "Efficient hyperparameter optimization and infinitely many armed bandits", "authors": ["L. Li", "K.G. Jamieson", "G. DeSalvo", "A. Rostamizadeh", "A. Talwalkar"], "venue": "CoRR, abs/1603.06560,", "year": 2016}, {"title": "The irace package, iterated race for automatic algorithm configuration", "authors": ["M. L\u00f3pez-Ib\u00e1nez", "J. Dubois-Lacoste", "T. St\u00fctzle", "M. Birattari"], "venue": "Technical report, Technical Report TR/IRIDIA/2011-004, IRIDIA, Universite\u0301 Libre de Bruxelles, Belgium,", "year": 2011}, {"title": "Efficient stopping rules", "authors": ["V. Mnih"], "venue": "Master\u2019s thesis, University of Alberta,", "year": 2008}, {"title": "Empirical Bernstein stopping", "authors": ["V. Mnih", "C. Szepesv\u00e1ri", "Audibert", "J.-Y"], "venue": "In International Conference on Machine Learning,", "year": 2008}, {"title": "13-a sat solver with conflict-clause minimization", "authors": ["N. Sorensson", "Een", "N. Minisat v"], "year": 2005}], "id": "SP:607862ca9a75ee982513be12a90ec05bd01e3641", "authors": [{"name": "Gell\u00e9rt Weisz", "affiliations": []}, {"name": "Andr\u00e1s Gy\u00f6rgy", "affiliations": []}, {"name": "Csaba Szepesv\u00e1ri", "affiliations": []}], "abstractText": "We consider the problem of configuring generalpurpose solvers to run efficiently on problem instances drawn from an unknown distribution. The goal of the configurator is to find a configuration that runs fast on average on most instances, and do so with the least amount of total work. It can run a chosen solver on a random instance until the solver finishes or a timeout is reached. We propose LEAPSANDBOUNDS, an algorithm that tests configurations on randomly selected problem instances for longer and longer time. We prove that the capped expected runtime of the configuration returned by LEAPSANDBOUNDS is close to the optimal expected runtime, while our algorithm\u2019s running time is near-optimal. Our results show that LEAPSANDBOUNDS is more efficient than the recent algorithm of Kleinberg et al. (2017), which, to our knowledge, is the only other algorithm configuration method with non-trivial theoretical guarantees. Experimental results on configuring a public SAT solver on a new benchmark dataset also stand witness to the superiority of our method.", "title": "LEAPSANDBOUNDS: A Method for Approximately Optimal Algorithm Configuration"}