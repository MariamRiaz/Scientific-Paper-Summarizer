{"sections": [{"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2496\u20132506 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n2496"}, {"heading": "1 Introduction", "text": "Coordinated text streams (Wang et al., 2007) refer to the text streams that are topically related and indexed by the same set of time points. Previous studies (Wang et al., 2007; Hu et al., 2012) on coordinated text stream focus on discovering and aligning common topic patterns across languages. Despite their contributions to applications like cross-lingual information retrieval and topic analysis, such a coarse-grained topic-level alignment framework inevitably overlooks many useful fine-grained alignment knowledge. For example, Figure 1 shows typical knowledge that can be derived from fine-grained Chinese-English text stream alignments. In addition to (a) bi-lingual word translations, we can also discover (b) polysemous and multi-referential words if one Chinese word is aligned to multiple English words, (c) synonymous and co-referential word pairs if two Chinese words are aligned to the same English word, and (d) entity phrases (e.g.,\u963f\u5e03\u624e\u6bd4 in Figure 1)\ninject Dezhou Texas ASEAN Abu Dhabi\nChinese\nEnglish\n(a) (b) (c) (d)\nFigure 1: Knowledge derived from fine-grained crosslingual text stream alignments: (a) word translations; (b) polysemy/multi-references; (c) synonym/coreference; (d) entity phrases\nif adjacent Chinese words in text are aligned to the same English named entity.\nIn order to acquire language knowledge for Natural Language Processing (NLP) applications, we study fine-grained cross-lingual text stream alignment. Instead of directly turning massive, unstructured data streams into structured knowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge.\nBased on the motivations, we employ a promising text stream representation \u2013 Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment.\nFor example, in Figure 2, each node in a BINet is a bursty word with one of its burst periods, representing an important information unit in a text stream. To decipher the Chinese BINet, our approach first focuses on the nodes in the English BINet in Figure 2 as the candidates because they co-burst with the Chinese nodes. Then, we decipher some nodes based on prior knowledge (the green node), the pronunciation similarity clue (the orange nodes) or literal translation similarity clue (the blue node). These deciphered nodes will serve as neighbor clues to decipher their adjacent nodes (the red node) which will then be used for further decipherment (e.g., decipher the yellow node) through knowledge propagation across the network, as the dashed arrows in Figure 2 show.\nExperiments on Chinese-English coordinated news streams show our approach can accurately align nodes across the cross-lingual BINets and derive various knowledge, and that with more streaming data provided, we can harvest more high-quality alignments and thus derive more knowledge. By aligning endless text streams, it is promising for never-ending language knowledge mining, which can not only complement language resources but also benefit some NLP applications.\nThe main contributions of this paper are:\n\u2022 We propose a promising framework to mine knowledge from inexhaustible coordinated cross-lingual text streams through finegrained alignment, exploring a paradigm for language knowledge acquisition.\n\u2022 We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches.\n\u2022 We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task."}, {"heading": "2 Burst Information Network", "text": "A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections.\nA BINet is originally defined as G = \u3008V,E,\u03c9\u3009 in (Ge et al., 2016a). Each node v \u2208 V is a burst element defined as a burst word1 during one of its burst periods \u3008w,P\u3009 where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge \u2208 E indicates the connection between two burst elements with the weight \u03c9 which is defined as the number of documents where these two burst elements co-occur in the text stream. In this paper, we extend the BINet definition to G = \u3008V,E,\u03c9,\u03c0\u3009 by adding a binary\n1Burst words and their corresponding burst periods can be detected based on Kleinberg burst detection algorithm (Kleinberg, 2003), as (Ge et al., 2016a) did.\nindicator \u03c0 to indicate if two nodes (i.e., burst elements) are frequently (more than 5 times) adjacent (as a bigram) in text, for mining knowledge such as entity phrases in Figure 1(d)."}, {"heading": "3 Decipherment", "text": "After constructing a BINet from a foreign language (we use Chinese as a foreign language in this paper), we can decipher it by consulting an Engish BINet constructed from its coordinated English text stream. We define Gc = \u3008Vc, Ec,\u03c9c,\u03c0c\u3009 and Ge = \u3008Ve, Ee,\u03c9e,\u03c0e\u3009 as the Chinese BINet and English BINet respectively. For people who do not know Chinese, Gc is a network of ciphers. We design a novel BINet decipherment procedure to decipher Gc by aligning as many nodes inGc as possible toGe. The decipherment process is defined to find e \u2208 Ve for a node c \u2208 Vc so that e is c\u2019s counterpart in the English text stream.2"}, {"heading": "3.1 Starting Point", "text": "To decipher the Chinese BINet, we need a few seeds based on prior knowledge as a starting point. Inspired by previous work on bi-lingual lexicon induction, decipherment and name translation mining, we utilize a few linguistic resources - a bilingual lexicon and language-universal representations such as time/calendar date, number, website URL, currency and emoticons to decipher a subset of Chinese nodes. For the example shown in Figure 2, we can decipher some nodes in the Chinese BINet such as \u201c7-6\u201d (to \u201c7-6\u201d) and \u201c\u79cd\u5b50\u201d (to \u201cseed\u201d)."}, {"heading": "3.2 Candidate Generation", "text": "For the nodes that cannot be deciphered by the prior knowledge, we first need to discover their possible candidates. For a node c in the Chinese BINet, its counterpart e can be any node in the English BINet or does not exist in the English BINet, resulting in an extremely large search space. Fortunately, burst information that refers to a hot topic usually co-bursts across languages. Based on this characteristic, for a node in the Chinese BINet, its counterpart is likely to be a node with the same burst period in the English BINet. For example, the node \u201c\u5a01\u5ec9(Williams)\u201d in the Chinese BINet\n2c and e are burst elements (i.e., nodes in the BINets). Sometimes, we also use c and e to denote the nodes\u2019 word if that does not lead to misunderstanding.\nin Figure 2 bursts between January 25 and January 31, 2010. We only need to look for its counterpart from the nodes in the English BINet whose burst period overlaps with this period. Formally, for a node c \u2208 Vc in the Chinese BINet, its candidate nodes in the English BINet can be derived as: Cand(c) = {e|P(e) \u2229 P(c) 6= \u2205} where e \u2208 Ve, and P(c) and P(e) are the burst periods of c and e respectively."}, {"heading": "3.3 Candidate Verification", "text": "For the candidate list for c (i.e., Cand(c)), we need to verify each node e \u2208 Cand(c) and choose the most probable one as c\u2019s counterpart. Formally, we define Score(c, e) as the credibility score of e being the correct counterpart of c and propose the following novel clues for verification.\nPronunciation Inspired by previous work on name translation mining (e.g., (Schafer III, 2006; Sproat et al., 2006; Ji, 2009)), for a node e \u2208 Cand(c), if its pronunciation is similar to c, then e is likely to be the translation of c. For a Chinese node c and an English node e, we define Sp as its scaled pronunciation score to measure their pronunciation similarity whose range is [0, 1]: Sp \u2208 [0, 1] \u221d 1LD where LD is the normalized (by e\u2019s length) Levenshtein edit distance between c\u2019s pinyin3 string and e\u2019s word string.\nTranslation For a node e \u2208 Cand(c), it is possible that e\u2019s word exists or partially exists in the bi-lingual lexicon. We can exploit the translation clue to verify if e is c\u2019s counterpart. For example, \u201cAustralian Open\u201d is a candidate of \u201c\u6fb3\u6d32\u7f51\u7403\u516c\u5f00 \u8d5b(Australian Open)\u201d as shown in Figure 2. Even though \u201c\u6fb3\u6d32\u7f51\u7403\u516c\u5f00\u8d5b(Australian Open)\u201d is not in the bi-lingual lexicon, \u201cAustralian\u201d and \u201copen\u201d are in the lexicon and their Chinese translations are \u201c\u6fb3\u6d32\u7684(Australian)\u201d and \u201c\u516c\u5f00(open)\u201d respectively. If we literally translate \u201cAustralian Open\u201d word by word, we will get \u201c\u6fb3\u6d32\u7684\u516c \u5f00\u201d which has long common subsequences with the Chinese node \u201c\u6fb3\u6d32\u7f51\u7403\u516c\u5f00\u8d5b(Australian Open)\u201d, inferring that \u201cAustralian Open\u201d is likely to be the translation of \u201c\u6fb3\u6d32\u7f51\u7403\u516c\u5f00\u8d5b\u201d.\n3Pinyin is the official romanization system for Chinese. We use pinyin instead of IPA because romanization is usually more easily available than IPA for a language.\nMotivated by this observation, for a candidate e \u2208 Cand(c), we first extract its possible Chinese translations C(e) from the bilingual lexicon. Note that if e is a multiword, we concatenate translations of its components. Then, for \u3008c, e\u3009, we define St as its scaled translation similarity score whose range is [0, 1]: St \u2208 [0, 1] \u221d maxc\u2032\u2208C(e) LCS(c, c\u2032) where maxc\u2032\u2208C(e) LCS(c, c\u2032) is maximum length of the longest common subsequence between c and c\u2032 \u2208 C(e).\nNeighbor The graph topological structure of a BINet is also an important clue for decipherment. By analyzing a node\u2019s neighbors, we can learn useful topic-level knowledge to decipher the node. For the example in Figure 2, \u201c\u827e\u5b81(Henin)\u201d in the Chinese BINet has neighbors such as \u201c\u5a01\u5ec9(Williams)\u201d, \u201c\u6fb3\u6d32 \u7f51\u7403\u516c\u5f00\u8d5b(Australian Open)\u201d and \u201c\u90d1\u6d01(Zheng Jie)\u201d while \u201cJustine Henin\u201d in the English BINet is connected with \u201cSerena Williams\u201d, \u201cAustralian Open\u201d and \u201cZheng Jie\u201d. If we know \u201cSerena Williams\u201d, \u201cAustralian Open\u201d and \u201cZheng Jie\u201d are the counterpart of \u2018\u5a01\u5ec9\u201d, \u201c\u6fb3\u6d32\u7f51\u7403\u516c\u5f00\u8d5b\u201d and \u201c\u90d1\u6d01\u201d respectively, we can infer \u201cJustine Henin\u201d is likely to be the counterpart of \u201c\u827e\u5b81\u201d, which can be further used as a clue to decipher its neighbors such as \u201c\u5916\u5361(wildcard)\u201d through knowledge propagation.\nWe define N(c) and N(e) as the set of adjacent nodes of c in the Chinese BINet and the adjacent nodes of e in the English BINet respectively. The neighbor clue score Sn of \u3008c, e\u3009 is defined as:\nSn = \u2211\nc\u2032\u2208N(c)\n\u03c9\u0302c,c\u2032 max e\u2032\u2208N(e)\nScore(c\u2032, e\u2032) (1)\nwhere Score(c\u2032, e\u2032) is the overall score of e\u2032 being the counterpart of c\u2032, as defined at the beginning of this section, \u03c9\u0302c,c\u2032 =\n\u03c9c,c\u2032\u2211 c\u2032\u2032\u2208N(c\u2032) \u03c9c\u2032,c\u2032\u2032 is the nor-\nmalized weight of the edge between c and c\u2032.\nCorrelation of burst If the word of e \u2208 Cand(c) frequently co-bursts with the word of c, then e is likely to be the counterpart of c. For example, \u201cSerena Williams\u201d in the English stream usually co-bursts with \u201c\u5c0f\u5a01\u201d in the Chinese stream, as shown in Figure 3, which is a useful clue to infer that \u201cSerena Williams\u201d is the counterpart of \u201c\u5c0f\u5a01\u201d.\nWe define Sb as the burst correlation score:\nSb = sw(c) \u00b7 sw(e)\n\u2016sw(c)\u20161 + \u2016sw(e)\u20161 \u2212 sw(c) \u00b7 sw(e) (2)\nwhere w(v) denotes the word of the node v and sw denotes the burst sequence of the word w in which each entry is a binary variable indicating if w bursts at a moment throughout the time frame. Note that in the above equation, we regard sw as a vector. The numerator is the number of days when w(c) and w(e) co-burst and the denominator is the number of days when either w(c) or w(e) bursts."}, {"heading": "3.4 Graph-based Decipherment", "text": "We define the overall (credibility) score as the linear combination of the clues introduced above:\nScore(c, e) = \u03b7Sp + \u03bbSt + \u03b3Sn + \u03b4Sb (3)\nwhere Sp, St, Sn and Sb are the scores that measure the value/reliability of the pronunciation, translation, neighbor and burst correlation clues respectively, and \u03b7, \u03bb, \u03b3 and \u03b4 are hyperparameters for adjusting their weights.\nBased on Eq (3), we can now compute the score of any candidate pair \u3008c, e\u3009. For pairs that are known to be correct alignments according to prior knowledge, their overall scores will be fixed to 1.0. For other possible candidate pairs, we simply initialize their scores as follows:\nScore(c, e) = 1.0\n|Cand(c)| (4)\nwhere Cand(c) is the set of c\u2019s candidate nodes in the English BINet.\nGiven that Score(c, e) is influenced by other pairs\u2019 scores, we design an iterative algorithm to compute and update the scores to decipher the entire Chinese BINet through propagation. This process is elaborated in Algorithm 1.\nAlgorithm 1 Graph-based Decipherment 1: For the determined pair \u3008c, e\u3009 based on the prior knowl-\nedge, Score(c, e)\u2190 1.0 2: For other undermined pairs \u3008c, e\u3009, initialize Score(c, e)\naccording to Eq (4); 3: while True (until \u2206Conf(Gc, Ge) \u2264 0.0001) do 4: for each undetermined pair \u3008c, e\u3009 do 5: Compute new score according to Eq (3); 6: update(c, e) = min(1.0, new score) 7: end for 8: for each undetermined pair \u3008c, e\u3009 do 9: Score(c, e)\u2190 update(c, e)\n10: end for 11: end while\n\u2206Conf(Gc, Ge) in the 3rd line of Algorithm 1 is the difference between the network decipherment confidence score at the current iteration and that at its previous iteration. Conf(Gc, Ge) is defined as follows, reflecting how much confidence we have in our network decipherment result:\nConf(Gc, Ge) = \u2211 c\u2208Vc max e\u2208Cand(c) Score(c, e) (5)\nIn practice, propagation of prior knowledge and clues makes the confidence score increase because it helps us know more about the network (as illustrated by Figure 2). When the confidence score stops increasing or increases marginally (\u2264 0.0001) after several iterations, the algorithm terminates4."}, {"heading": "4 Experiments", "text": "We first evaluate our approach on aligning nodes in the cross-lingual BINets for fine-grained crosslingual stream alignment in Section 4.1. Then, we show the value of derived alignments for endless language knowledge acquisition in Section 4.2."}, {"heading": "4.1 Stream alignment", "text": ""}, {"heading": "4.1.1 Data", "text": "We used the public 2010 Agence France Presse (AFP) news in Chinese (Graff and Chen, 2005) and English Gigaword (Graff et al., 2003) as our cross-lingual text streams. The Chinese stream has 17,327 while the English one contains 186,737 documents.\nWe removed stopwords, conducted lemmatization and name tagging for the English stream, and did word segmentation and name tagging for the Chinese stream using the Stanford CoreNLP toolkit (Manning et al., 2014).\n4Due to the upper bound ofConf(Gc, Ge), the algorithm must terminate after several iterations.\nWe detected bursts and constructed the BINets5 for the Chinese and English stream based on (Ge et al., 2016a). The constructed Chinese BINet has 7,360 nodes and 33,892 edges while the English one has 8,852 nodes and 85,125 edges. Our seed bi-lingual lexicon is released by (Zens and Ney, 2004), containing 81,990 Chinese word entries, each of which has an English translation. Among the 7,360 nodes in the Chinese BINet, 2,281 nodes need to be deciphered since their words are not in the bi-lingual lexicon."}, {"heading": "4.1.2 Evaluation Setting", "text": "We evaluate our approach in an end-to-end fashion. For a node c in the Chinese BINet, we choose the node e\u2217 which has the highest score as c\u2019s counterpart in the English BINet:\ne\u2217 = arg max e\u2208Cand(c) Score(c, e)\nWe rank the aligned node pairs by the score and manually evaluate the quality of the top K pairs. A pair \u3008c,e\u3009 is annotated as correct if e is a correct translation of c or e refers to an entity that c refers to. The annotation assignment is done by three human judges with 89.4% agreement. The disagreement mainly arises from the ambiguity of some named entities. In the evaluation, we consider \u3008c,e\u3009 correct if more than two judges annotate it as correct.\nWe compare our approach to the following baselines that use various combinations of clues to verify candidates for decipherment as well as the state-of-the-art algorithm for language decipherment from non-parallel corpora:\n\u2022 Pronunciation verification (pv): Use the pronunciation clue only\n\u2022 Translation verification (tv): Use the translation clue only\n\u2022 Neighbor verification (nv): Use the neighbor clue only to decipher the BINet through propagation.\n\u2022 Correlation of burst verification: (cv): Use the burst correlation clue only\n\u2022 pv+tv and pv+tv+nv\n\u2022 Bayesian Inference: Bayesian inference based decipherment approach (Dou and Knight, 2012) based on the alignment of bigram language\n5We discarded the edges whose weight is smaller than a threshold (5 for Chinese and 20 for English BINet given the difference of their data size) for removing trivial connections.\nmodels across languages. We adapt it to our experiment setting by considering adjacent nodes in a BINet as bigrams for decipherment. We used 2009 AFP Chinese/English news in Gigaword as our development set to tune hyperparameters. Since our approach has only 4 parameters (i.e., \u03b7, \u03bb, \u03b3, \u03b4 in Eq (3)), it is easy to tune the parameters using grid search (from 0.0 to 1.0 with a step 0.2) on the development set. For baselines except Bayesian inference, the score computation function is almost identical to Eq (3) except that the weights of the clues which are not used are set to 0."}, {"heading": "4.1.3 Results", "text": "We present the results in Figure 4. Our approach outperforms all the baselines because it considers various clues for decipherment. Among the baselines, accuracy scores of pv and tv drop dramatically with K increasing because a single clue can only decipher a limited number of nodes effectively. pv+tv seems to alleviate the problem to some extent: its accuracy does not drop so drastically as pv or tv because multiple clues allow us to decipher more nodes but its accuracy is still not desirable. Among the clues, cv performs worst, demonstrating that the burst correlation clue alone is far from enough for decipherment. Compared with pv, tv and cv, nv deciphers the nodes in the Chinese BINet through propagation but the neighbor clue alone is not sufficient for accurate decipherment. It is notable that nv achieves comparable performance to the Bayesian inference method which uses similar clues, demonstrating the effectiveness of our decipherment framework despite its simplicity. Moreover, our graph-based decipherment approach is more flexible to incorporate a variety of clues. When it is combined with pv+tv, the performance shows a significant boost\nand achieves approximately 90% accuracy in the top 200 results though it is slightly inferior to our final approach due to the lack of awareness of burst correlation.\nAnother interesting observation from Figure 4 is that our approach clearly know the confidence of its predictions. For top 100 mined pairs with the highest confidence scores (i.e., the score in Eq (3)), the accuracy is 98%. Therefore, it is easy to control the quality of mined pairs, which is important for a text mining algorithm.\nWe also study the effect of language resources on the performance. We first randomly sample different sizes of entries from the original bi-lingual lexicon as new bi-lingual lexicons. The results6 in Figure 5 show the accuracy improves as the size of bi-lingual lexicon grows because more prior knowledge benefits deciphering the BINet. In addition, we test our approach in a low-resource setting where there is no knowledge of the romanization system (i.e., pinyin) and no pre-trained word segmentation and name tagging tools are available. The only available resource is a very small bi-lingual lexicon with 1,000 most common Chinese words7 and their corresponding English translations. In this setting, we use an unsupervised Chinese word segmentation approach combining a Hierarchical Dirichlet Process (HDP) model with a Bayesian HMM model (Chen et al., 2014) to segment Chinese text instead of the preprocessing steps mentioned in Section 4.1.1. According to Figure 5, our approach still performs well in the low-resource setting although its accuracy curve is lower than that in rich-resource settings, demonstrating it can work in both rich- and low-resource settings.\n6The sample processes are repeated for 3 times and the results are the averaged accuracy.\n7We sample these Chinese words based on IDF.\nIn order to test the generalization ability, we evaluate our approach using the same hyperparameters on another coordinated text streams \u2013 AFP Chinese and APW English news stream in 2008. The results in Figure 6 show that our decipherment approach consistently outperforms the other baseline and still deciphers the top 100 nodes in high accuracy even though the curve in 2008 is lower than that in 2010. The performance difference in 2008 and 2010 mainly arises from the difference on topic overlaps. In the streams of 2010, the Chinese and English news are from the same news agency (i.e., AFP). Therefore, the topic overlaps of the cross-lingual streams are larger than 2008, allowing more nodes to be deciphered correctly.\nFinally, we investigated the performance of our approach under various sizes of data provided, as shown in Figure 7. As observed, when the data size is small (e.g., 6-month coordinated text streams), the approach works poorly because there are very few nodes in BINets that can be aligned. As the data size increases, our approach can efficiently8 harvest a growing number of high-quality\n8Efficiency is reported in the supplementary notes.\nalignments, as reflected by the higher curves in Figure 7. Considering massive coordinated text streams generated every day, if the approach can be applied to the endless streams, it is possible to monitor the streaming data and derive countless alignments for never-ending language knowledge acquisition."}, {"heading": "4.2 Endless language knowledge mining", "text": "Table 1 shows the stream alignment result of our approach. As demonstrated above, we can derive a variety of language knowledge from the finegrained cross-lingual alignments.\nWord/entity translations are the main knowledge that can be derived from our alignment results by extracting word pairs from the aligned cross-lingual node pairs. Formally, we find a Chinese word w\u2019s English translation w\u2217 as follows:\nw\u2217 = w(e\u2217)\ne\u2217 = argmaxe\u2208Ve maxc\u2208Vc(w) Score(c, e)\nwhere Vc(w) is the set of Chinese nodes whose word is w, and w(e) denotes the word of node e.\nWe evaluate our approach on mining translations of bursty Chinese words, based on the evaluation criteria of bilingual lexicon extraction. Specifically, we test how many out-of-vocabulary (OOV) words appearing in the Chinese BINet are correctly translated. The datasets used for evaluation are the 2010 and 2008 streams in Figure 6. In total, there are 1,226 and 1,082 distinct Chinese OOV words (excluding incorrectly segmented words) in the corresponding Chinese BINets. Accuracy is used to measure the proportion of the words being correctly translated, as (Tamura et al., 2012) did.\nTable 2 compares our approach to representative bilingual lexicon extraction approaches. CONTEXT is one of the earliest approaches for extracting word translations from comparable cor-\nModel Acc1(2010) Acc1(2008)\npora based on context similarity. COLP and SIMLP are label propagation models on word cooccurrence and similarity graphs for bilingual lexicon extraction. DIVERSE is a variant of CONTEXT by adding various information (e.g., pronunciation and temporality) and DIVERSESP is the approach using phonetic and frequency correlation with a score propagation strategy. BAYESIAN is the Bayesian decipherment approach which has been introduced in the previous section, and it is evaluated in two settings (i.e., based on traditional bigram language models and BINets). According to Table 2, our approach substantially outperforms the other approaches on both datasets, showing its advantages for mining translation of bursty words in coordinated text streams. It is also notable that the BINet-based BAYESIAN improves the LM-based counterpart, demonstrating the advantage of burst-level alignment for this task.\nIn addition to the comparisons to the classical baselines, we also test the latest representative unsupervised bi-lingual lexicon extraction approaches (Zhang et al., 2017a,b) based on word embedding and generative adversarial nets (GANs). Unfortunately, these approaches do not perform well in our setting. For example, the approach in (Zhang et al., 2017a) achieved <1% accuracy9. One reason is that the topic overlap of coordinated cross-lingual text streams is not so significant as the Wikipedia data used for their experiments, and the other reason is that their approaches focus on common fundamental words like \u201c\u57ce\u5e02(city)\u201d while our targets are OOVs like \u201c\u4e1c\u534f(ASEAN)\u201d which do not frequently appear in a corpus. In contrast, our approach is more practical: it not only works well in easily available and endless coordinated text streams without high content overlap requirement, but also can accurately mine translations of many OOVs which do not appear frequently and really need mining their translations.\n9We implement this approach using the codes released by the authors. Their reported accuracy for the common words with over 1,000 occurrences is 2.53% on Gigaword corpus.\nAs illustrated in Figure 1, besides word/entity translations, various types of knowledge can also be derived from the BINet alignment results as by-products. For example, for node 9 in Table 1, deciphering the nickname \u201c\u5c0f\u5a01\u201d into Serena Williams can benefit cross-lingual entity linking. Nodes 10-11 also demonstrate the potential effect on synonym detection, entity linking and coreference resolution, like the case of Figure 1(b). Nodes 12-13 show that the deciphered BINets can detect polysemous/multi-referential word like \u201c\u592e \u884c(Central bank)\u201d which may refer to different entities during different burst periods, like Figure 1(c). Moreover, the deciphered BINets can also help entity phrase extraction based on the idea of Figure 1(d). For example, in nodes 14-15, \u7fc1 \u5c71\u82cf\u59ec(Aung San Suu Kyi) is not recognized as a person name by the Chinese name tagger; instead, it is mistakenly separated into two words \u2013 \u7fc1\u5c71(Aung San) and \u82cf\u59ec(Suu Kyi). However, since \u7fc1\u5c71(Aung San) and \u82cf\u59ec(Suu Kyi) are deciphered into the same English named entity \u2013 Aung San Suu Kyi, we can merge them back to form the correct entity.\nFor evaluating our approach\u2019s performance on language knowledge acquisition, we align the AFP Chinese-English text streams from 2002 to 2010. The Chinese stream has 119,196 documents and the English one contains 1,608,636 documents. Our approach obtained 7,211 node alignments10. Among them, we focus on the top 500 alignments to guarantee their quality and use the aforementioned idea for deriving language knowledge.\nTable 3 shows the result of deriving knowledge from the alignments. Among top 500 alignments, we derived 416 correct word/entity translation pairs with 83% accuracy. Also, we correctly derived 8 polysemous/multi-referential words, 49 synonymous/co-referential word pairs and 84 entity phrases as byproducts. It is notable that the data size of coordinated cross-lingual text streams available on the web is much larger than that used in our experiment and they are endlessly updated.\n10The alignments with a low score (< 0.05) are discarded.\nThat means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging."}, {"heading": "5 Related Work", "text": "Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge.\nIn contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data."}, {"heading": "6 Conclusions and Future Work", "text": "This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsupervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access.\nFor future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach\u2019s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji\u2019s work has been supported by the U.S. DARPA AIDA Program No. FA8750-18-2-0014, Air Force No. FA8650-17-C7715, ARL NS-CTA No. W911NF-09-2-0053 and NSF Awards IIS-0953149 and IIS-1523198. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. The contact author is Zhifang Sui."}], "year": 2018, "references": [{"title": "A distribution-based model to learn bilingual word embeddings", "authors": ["Hailong Cao", "Tiejun Zhao", "Shu Zhang", "Yao Meng."], "venue": "COLING.", "year": 2016}, {"title": "A joint model for unsupervised chinese word segmentation", "authors": ["Miaohong Chen", "Baobao Chang", "Wenzhe Pei."], "venue": "EMNLP.", "year": 2014}, {"title": "Cross-language linking of news stories on the web using interlingual topic modelling", "authors": ["Wim De Smet", "Marie-Francine Moens."], "venue": "ACM workshop on Social web search and mining.", "year": 2009}, {"title": "Large scale decipherment for out-of-domain machine translation", "authors": ["Qing Dou", "Kevin Knight."], "venue": "EMNLP.", "year": 2012}, {"title": "Beyond parallel data: Joint word alignment and decipherment improves machine translation", "authors": ["Qing Dou", "Ashish Vaswani", "Kevin Knight."], "venue": "EMNLP.", "year": 2014}, {"title": "A fast method for parallel document identification", "authors": ["Jessica Enright", "Grzegorz Kondrak."], "venue": "NAACL.", "year": 2007}, {"title": "An ir approach for translating new words from nonparallel and comparable texts", "authors": ["Pascale Fung", "Lo Yuen Yee."], "venue": "COLING-ACL.", "year": 1998}, {"title": "News stream summarization using burst information networks", "authors": ["Tao Ge", "Lei Cui", "Baobao Chang", "Sujian Li", "Ming Zhou", "Zhifang Sui."], "venue": "EMNLP.", "year": 2016}, {"title": "Event detection with burst information networks", "authors": ["Tao Ge", "Lei Cui", "Baobao Chang", "Zhifang Sui", "Ming Zhou."], "venue": "COLING.", "year": 2016}, {"title": "Discovering concept-level event associations from a text stream", "authors": ["Tao Ge", "Lei Cui", "Heng Ji", "Baobao Chang", "Zhifang Sui."], "venue": "NLPCC.", "year": 2016}, {"title": "Chinese gigaword", "authors": ["David Graff", "Ke Chen."], "venue": "LDC Catalog No.: LDC2003T09, ISBN, 1:58563\u2013 58230.", "year": 2005}, {"title": "English gigaword", "authors": ["David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda."], "venue": "Linguistic Data Consortium, Philadelphia.", "year": 2003}, {"title": "Learning bilingual lexicons from monolingual corpora", "authors": ["A. Haghighi", "P. Liang", "T. Berg-Kirkpatrick", "D. Klein."], "venue": "ACL.", "year": 2008}, {"title": "Improving named entity translation by exploiting comparable and parallel corpora", "authors": ["Ahmed Hassan", "Haytham Fahmy", "Hany Hassan."], "venue": "RANLP.", "year": 2007}, {"title": "Cross-lingual topic alignment in time series japanese/chinese news", "authors": ["Shuo Hu", "Takahashi Yusuke", "Liyi Zheng", "Takehito Utsuro", "Masaharu Yoshioka", "Noriko Kando", "Tomohiro Fukuhara", "Hiroshi Nakagawa", "Yoji Kiyota."], "venue": "PACLIC.", "year": 2012}, {"title": "Supervised bilingual lexicon induction with multiple monolingual signals", "authors": ["A. Irvine", "C. Callison-Burch."], "venue": "NAACL.", "year": 2013}, {"title": "A comprehensive analysis of bilingual lexicon induction", "authors": ["Ann Irvine", "Chris Callison-Burch."], "venue": "Computational Linguistics, 1(1).", "year": 2015}, {"title": "Discriminative bilingual lexicon induction", "authors": ["Ann Irvine", "Chris Callison-Burch."], "venue": "Computational Linguistics, 1(1).", "year": 2015}, {"title": "Mining name translations from comparable corpora by creating bilingual information networks", "authors": ["Heng Ji."], "venue": "the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora.", "year": 2009}, {"title": "Visual bilingual lexicon induction with transferred convnet features", "authors": ["Douwe Kiela", "Ivan Vulic", "Stephen Clark."], "venue": "EMNLP.", "year": 2015}, {"title": "Bursty and hierarchical structure in streams", "authors": ["Jon Kleinberg."], "venue": "Data Mining and Knowledge Discovery, 7(4):373\u2013397.", "year": 2003}, {"title": "Bilingual lexicon induction for low-resource languages", "authors": ["Alexandre Klementiev", "Chris Callison-Burch."], "venue": "JHU Technical Report.", "year": 2010}, {"title": "Weakly supervised named entity transliteration and discovery from multilingual comparable corpora", "authors": ["Alexandre Klementiev", "Dan Roth."], "venue": "COLING-ACL.", "year": 2006}, {"title": "Learning a translation lexicon from monolingual corpora", "authors": ["Philipp Koehn", "Kevin Knight."], "venue": "ACL workshop on Unsupervised lexical acquisition.", "year": 2002}, {"title": "Mining named entities with temporally correlated bursts from multilingual web news streams", "authors": ["Alexander Kotov", "ChengXiang Zhai", "Richard Sproat."], "venue": "WSDM.", "year": 2011}, {"title": "A minimally supervised approach for detecting and ranking document translation pairs", "authors": ["Kriste Krstovski", "David A Smith."], "venue": "WMT.", "year": 2011}, {"title": "Bootstrapping translation detection and sentence extraction from comparable corpora", "authors": ["Kriste Krstovski", "David A Smith."], "venue": "NAACL.", "year": 2016}, {"title": "Constructing information networks using one single model", "authors": ["Qi Li", "Heng Ji", "Yu Hong", "Sujian Li."], "venue": "EMNLP.", "year": 2014}, {"title": "Unsupervised language-independent name translation mining from wikipedia infoboxes", "authors": ["Wen-Pin Lin", "Matthew Snover", "Heng Ji."], "venue": "EMNLP Workshop on Unsupervised Learning for NLP.", "year": 2011}, {"title": "The Stanford CoreNLP natural language ssing toolkit", "authors": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "ACL: System Demonstrations.", "year": 2014}, {"title": "Multisource transfer of delexicalized dependency parsers", "authors": ["R. McDonald", "S. Petrov", "K. Hall."], "venue": "EMNLP.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "NIPS.", "year": 2013}, {"title": "Improving machine translation performance by exploiting non-parallel corpora", "authors": ["Dragos Stefan Munteanu", "Daniel Marcu."], "venue": "Computational Linguistics, 31(4):477\u2013504.", "year": 2005}, {"title": "Cross-lingual annotation projection for semantic roles", "authors": ["S. Pado", "M. Lapata."], "venue": "Journal of Artificial Intelligence Research, 36.", "year": 2009}, {"title": "Glove: Global vectors for word representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "EMNLP.", "year": 2014}, {"title": "Deep contextualized word representations", "authors": ["Matthew E Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."], "venue": "arXiv preprint arXiv:1802.05365.", "year": 2018}, {"title": "Automatic identification of word translations from unrelated english and german corpora", "authors": ["Reinhard Rapp."], "venue": "ACL.", "year": 1999}, {"title": "Deciphering foreign language", "authors": ["Sujith Ravi", "Kevin Knight."], "venue": "ACL.", "year": 2011}, {"title": "Inducing translation lexicons via diverse similarity measures and bridge languages", "authors": ["Charles Schafer", "David Yarowsky."], "venue": "CoNLL.", "year": 2002}, {"title": "Translation discovery using diverse similarity measures", "authors": ["Charles F Schafer III."], "venue": "Johns Hopkins University.", "year": 2006}, {"title": "Mining named entity translation from non parallel corpora", "authors": ["Rahma Sellami", "Fatiha Sadat", "Lamia Belguith Hadrich."], "venue": "FLAIRS.", "year": 2014}, {"title": "Mining new word translations from comparable corpora", "authors": ["Li Shao", "Hwee Tou Ng."], "venue": "COLING.", "year": 2004}, {"title": "Extracting parallel sentences from comparable corpora using document level alignment", "authors": ["Jason R Smith", "Chris Quirk", "Kristina Toutanova."], "venue": "NAACL.", "year": 2010}, {"title": "Named entity transliteration with comparable corpora", "authors": ["Richard Sproat", "Tao Tao", "ChengXiang Zhai."], "venue": "ACL.", "year": 2006}, {"title": "Bilingual lexicon extraction from comparable corpora using label propagation", "authors": ["Akihiro Tamura", "Taro Watanabe", "Eiichiro Sumita."], "venue": "EMNLP.", "year": 2012}, {"title": "Mining named entity transliteration equivalents from comparable corpora", "authors": ["Raghavendra Udupa", "K Saravanan", "A Kumaran", "Jagadeesh Jagarlamudi."], "venue": "CIKM.", "year": 2008}, {"title": "Mint: A method for effective and scalable mining of named entity transliterations from large comparable corpora", "authors": ["Raghavendra Udupa", "K. Saravanan", "A. Kumaran", "Jagadeesh Jagarlamudi."], "venue": "EACL.", "year": 2009}, {"title": "Large scale parallel document mining for machine translation", "authors": ["Jakob Uszkoreit", "Jay M Ponte", "Ashok C Popat", "Moshe Dubiner."], "venue": "COLING.", "year": 2010}, {"title": "Bilingual word embeddings from non-parallel documentaligned data applied to bilingual lexicon induction", "authors": ["Ivan Vulic", "Marie-Francine Moens."], "venue": "ACL.", "year": 2015}, {"title": "Mining correlated bursty topic patterns from coordinated text streams", "authors": ["X. Wang", "C. Zhai", "X. Hu", "R. Sproat."], "venue": "KDD.", "year": 2007}, {"title": "Mining common topics from multiple asynchronous text streams", "authors": ["Xiang Wang", "Kai Zhang", "Xiaoming Jin", "Dou Shen."], "venue": "WSDM.", "year": 2009}, {"title": "Mining name translations from entity graph mapping", "authors": ["Gae won You", "Seung won Hwang", "Young-In Song", "Long Jiang", "Zaiqing Nie."], "venue": "EMNLP.", "year": 2010}, {"title": "Improvements in phrase-based statistical machine translation", "authors": ["Richard Zens", "Hermann Ney."], "venue": "HLT-NAACL.", "year": 2004}, {"title": "Cross-lingual latent topic extraction", "authors": ["Duo Zhang", "Qiaozhu Mei", "ChengXiang Zhai."], "venue": "ACL.", "year": 2010}, {"title": "Adversarial training for unsupervised bilingual lexicon induction", "authors": ["Meng Zhang", "Yang Liu", "Huanbo Luan", "Maosong Sun."], "venue": "ACL.", "year": 2017}, {"title": "Earth mover\u2019s distance minimization for unsupervised bilingual lexicon induction", "authors": ["Meng Zhang", "Yang Liu", "Huanbo Luan", "Maosong Sun."], "venue": "EMNLP.", "year": 2017}], "id": "SP:dc90cdf9ce12dae869148a73847d209f577ce7dc", "authors": [{"name": "Tao Ge", "affiliations": []}, {"name": "Qing Dou", "affiliations": []}, {"name": "Heng Ji", "affiliations": []}, {"name": "Lei Cui", "affiliations": []}, {"name": "Baobao Chang", "affiliations": []}, {"name": "Zhifang Sui", "affiliations": []}, {"name": "Furu Wei", "affiliations": []}, {"name": "Ming Zhou", "affiliations": []}], "abstractText": "This paper proposes to study fine-grained coordinated cross-lingual text stream alignment through a novel information network decipherment paradigm. We use Burst Information Networks as media to represent text streams and present a simple yet effective network decipherment algorithm with diverse clues to decipher the networks for accurate text stream alignment. Experiments on Chinese-English news streams show our approach not only outperforms previous approaches on bilingual lexicon extraction from coordinated text streams but also can harvest high-quality alignments from large amounts of streaming data for endless language knowledge mining, which makes it promising to be a new paradigm for automatic language knowledge acquisition.", "title": "Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition"}