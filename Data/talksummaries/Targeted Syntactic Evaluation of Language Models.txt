0	31	A language model (LM) defines a probability distribution over sequences of words.
4	31	Language models are typically evaluated using perplexity: it is considered desirable for an LM to assign a high probability to held-out data from the same corpus as the training data.
6	21	The quality of the syntactic predictions made by the LM is arguably particularly difficult to measure using perplexity: since most sentences are grammatically simple and most words can be predicted from their local context, perplexity rewards LMs primarily for collocational and semantic predictions.
7	17	We propose to supplement perplexity with a metric that assesses whether the probability distribution defined by the model conforms to the grammar of the language.
11	24	In other words, the LSTM learned more robust syntactic representations, but this advantage was not reflected in its average perplexity on the corpus, since syntactically challenging sentences are relatively infrequent.
12	65	Previous work on targeted syntactic evaluation of language models has identified syntactically challenging sentences in corpora (Linzen et al., 2016; Gulordava et al., 2018).
13	35	While evaluation on naturally occurring examples is appealing, this approach has its limitations (see Section 2).
26	35	For example, the following minimal pair illustrates the fact that third- person present English verbs agree with the number of their subject: (1) Simple agreement: a.
36	15	In the current paper, we use the more general setting and compare the probability of the two complete sentences.
37	24	Previous work has used syntactically complex sentences identified from a parsed corpus.
39	18	If the corpus is automatically parsed, the risk of a parse error increases with the complexity of the construction (Bender et al., 2011).
40	58	If the test set is restricted to sentences with gold parses, it can be difficult or impossible to find a sufficient number of examples of syntactically challenging cases.
107	166	Likewise, in the following minimal pair, sentence (16b) is ungrammatical, because the reflexive pronoun themselves, which is part of the main clause, cannot be bound to the noun phrase the architects, which is inside an embedded clause: (16) Reflexive across an object relative clause: a.
108	59	The manager that the architects like doubted himself.
109	39	Negative polarity items, introduced in example (2) above, are words that (to a first approximation) need to occur in the context of negation.
117	16	To show how our challenge set can be used to evaluate the syntactic performance of LMs, we trained three LMs with increasing levels of syntactic sophistication.
122	17	Single-task RNN: The RNN LM had two layers of 650 LSTM units, a batch size of 128, a dropout rate of 0.2, and a learning rate of 20.0, and was trained for 40 epochs (following the hyperparameters of Gulordava et al. 2018).
124	16	We combine language modeling with CCG supertagging, a task that predicts for each word in the sentence its CCG supertag (Bangalore and Joshi, 1999; Lewis et al., 2016).
166	24	Perplexity: The perplexity of the n-gram model on the Wikipedia test data was 157.5, much higher than the perplexity of the single-task RNN (78.65) and the multi-task RNN (61.10).
170	17	As another case study, we examine variation in the results of the simple agreement condition in the single- task RNN.
184	21	Incidentally, the human results with object RCs were also unexpected: while attraction errors when the two subjects differ in number are to be expected (Wagers et al., 2009), our participants made a sizable number of errors even when both subjects were plural.
187	99	The data set consists of pairs of sentences that are matched except for their grammaticality; we consider a language model to capture the relevant aspects of the grammar of the language if it assigns a higher probability to the grammatical sentence than to the ungrammatical one.
188	16	An RNN language model performed very well on local subject-verb agreement dependencies, significantly outperforming an n-gram baseline.
189	16	This suggests that the task is a viable evaluation strategy.
190	16	Even on simple cases, however, the RNN’s accuracy was sensitive to the particular lexical items that occurred in the sentence; this would not be expected if its syntactic representations were fully abstract.
191	53	The RNN’s performance degraded markedly on non-local dependencies, approaching chance levels on agreement across an object relative clause.
194	26	Our results contrast with the results of Gulordava et al. (2018), who obtained a prediction accuracy of 81% on English sentences from their test corpus and 74% on constructed sentences modeled after sentences from the corpus.
196	75	One limitation of our approach is that it is not always clear what constitutes a minimal grammaticality contrast.
199	57	We emphasize that the goal of this article was not to advocate for LSTMs in particular as an effective architecture for modeling syntax; indeed, our results show that LSTM language models are far from matching naive annotators’ performance on this task, let alone performing at 100% accuracy.
200	123	We hope that our data set, and future extensions to other phenomena and languages, will make it possible to measure progress in syntactic language modeling and will lead to better understanding of the syntactic generalizations captured by language models.
