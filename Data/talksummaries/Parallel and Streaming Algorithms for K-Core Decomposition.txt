5	31	k-core is often use a feature in machine learning systems with applications in network analysis, spam detection and biology.
20	22	We then show the first application of our sketching technique in designing a parallel algorithm for computing the k-core decomposition.
21	20	More precisely, we present a MapReducebased algorithm to compute a 1 − approximate k-core decomposition of a graph in O(log n) rounds of computations, where the load of each machine is Õ(n), for any ∈ (0, 1].
35	21	First, by using uniform sampling it is not possible to obtain a (1− ) approximation of the coreness number for nodes of constant degree (unless we do not sample all edges with probability one).
37	24	Hence, the probability that the degree of a vertex in the sampled is not proportional to the sampling rate, is not exponentially small anymore.
41	50	In recent years the k-core decomposition problem received a lot of attention (Bhawalkar et al., 2012; Montresor et al., 2013; Aksu et al., 2014; Sarayuce et al., 2015; Zhang et al., 2017), nevertheless we do not know of any previous distributed algorithms with small bounded memory and number of rounds.
51	19	A k-core is a maximal subgraph H ⊆ G such that ∀v ∈ H we have dH(v) ≥ k. Note that for any k the k-core is unique and it may be possibly disconnected.
55	135	It is wroth noting that this labeling is unique and that it defines a hierarchical decomposition of G. In this paper we are interested in computing a good approxi- mation of the core labeling for a graph G efficiently in the MapReduce and in the streaming model.
57	22	We define a 1 − approximation to the k-core of G to be a subgraph H of G that contains the k-core of G and such that ∀v ∈ H we have dH(v) ≥ (1 − )k. In other words, a 1− approximation to the k-core of G is a subgraph of the (1− )k-core of G and supergraph of the k-core of G. In Figure 1 we present the 3-core for a small graph and a 2 3 -approximate 3-core.
59	24	In the paper we often refer to the classic greedy algorithm (Matula & Beck, 1983)(also known as peeling algorithm) to compute the coreness number.
60	17	The algorithm works as follows: nodes are removed from the graph iteratively.
61	33	In particular, in iteration i of the algorithm all nodes with degree smaller or equal to i are removed iteratively and they are assigned coreness number i.
64	20	In the MapReduce model, the computation happens in parallel in several rounds.
66	65	The model has two main restrictions, one on the total number of machines and another on the memory available on each machine.
73	18	The goal of our algorithm is to obtain a good approximation of the core labelling at the end of the stream using only small memory (Õ(n)).
75	27	Compared with previous sketching for similar problems (Lee et al., 2010; Esfandiari et al., 2015; Bahmani et al., 2012; Epasto et al., 2015; Bhattacharya et al., 2016) our sketching samples different area of the graphs with different, carefully selected, probabilities.
76	19	The main idea behind the sketch is to sample edges more aggressively in denser areas of the graph and less aggressively in sparser areas.
81	51	To compute the coreness numbers of the rest of the node in the graph, we first remove from the graph the nodes for which we have a good estimation and then we iterate the same approach.
83	47	Interestingly, we can show that by sampling edges adaptively, we can iteratively estimate the coreness of all nodes in the graph by analyzing only sparse subgraphs.
85	50	We start by describing a basic subroutine that estimates a modified version of the coreness number.
91	47	2: Initialize Γ = VH \ Λ 3: Initialize l← 0 4: while Γ 6= ∅ do 5: while minv∈Γ(dH(v)) ≤ l do 6: Let v ← argminv∈Γ(dH(v)) 7: Set lv ← l 8: Remove v from Γ 9: Remove v from H 10: end whilel← l + 1 11: end while subset of the nodes in H for which we do not have already a good estimate of the coreness number.
95	30	Let l0(i) be the label of vertex i in this labeling.
96	30	If a vertex i has l0(i) ≥ C log n, for a specific constant C > 0, we can estimate its coreness number in G precisely.
97	61	Intuitively this is true because we are sampling the edges independently so we can use concentration results to bound its coreness number.
105	70	In the remaining of the section we first present pseudocode of our sketching algorithm(Algorithm 2) then we show that at the end of the execution of the algorithm we have a good estimation of the coreness number for all nodes in G. Finally we argue that in every iteration the graphs Hi are sparse so the algorithm uses only small memory at any point in time.
110	26	1: Input: A graph G with n vertices and parameter ∈ (0, 1].
111	89	2: Initialize Λ← ∅ 3: Initialize p0 ← 12 logn 2n 4: for j = 0 to logn do 5: Let Hj be a subgraph of G with the edges sampled independently with probability pj 6: Run Exclusive Core Labeling(Hj ,Λ) and denote the label of vertex i on Hj by lj(i) 7: for i ∈ Hj do 8: if lj(i) ≥ 24 logn 2 ∨ pj = 1 then 9: // Node i has sufficiently high degree to estimate its coreness number.
112	122	10: if lj(i) ≤ 48 logn 2 then 11: Set the label of vertex i to (1− ) lj(i) pj 12: Add i to Λ 13: else 14: Set the label of vertex i to 2(1− )n 2j−1 15: Add i to Λ 16: end if 17: end if 18: end for 19: Remove from G the edges of G induced by Λ 20: pj+1 ← 2pj 21: end for subgraph of G and the sampled subgraph H .
113	31	Let G be a graph and let ∈ (0, 1] and δ ∈ (0, 1) be two arbitrary numbers.
114	30	Let f(n) be a function of n such that f(n) ≥ 6 log n δ 2 and let H be a subgraph of G that contains each edge of G independently with probability p ≥ 6 log n δ 2f(n) .
