24	1	We have also provided theoretic analyses to its algorithmic stability and generalization bound, and conducted comprehensive empirical studies showing the L2T’s superiority over state-of-the-art transfer learning algorithms.
45	1	The number of target labeled examples is much smaller than that of source labeled examples, i.e., ntle nsle.
49	1	Suppose that the transferred knowledge by the algorithm ae can be parameterized asWe.
56	1	In this work, we consider A to contain algorithms transferring single-level latent feature factors, because existing parameter-based and instance-based algorithms cannot address the transfer learning setting we focus on (i.e., X es = X et and Yes = Yet ).
57	1	Though limited parameter-based algorithms (Yang et al., 2007a; Tommasi et al., 2014) can transfer across domains in heterogeneous label spaces, they can only handle binary classification problems.
58	1	Deep neural network based algorithms (Yosinski et al., 2014; Long et al., 2015; Tzeng et al., 2015) transferring latent feature factors in multiple levels are left for our future research.
66	1	We denote by ϕ the function mapping original feature representation into the latent space.
70	1	Consequently, we obtain the similarity metric matrix (Cao et al., 2013) in the latent space, i.e.,G=(Xte) †Zte(Z t e) T [(Xte) T ]†∈Rm×m according to XteG(X t e) T =Zte(Z t e) T , where (Xte) † is the pseudo-inverse ofXte.
102	1	By maximizing the unlabeled discriminant criterion τe, the local scatter covariance matrix guarantees the first principle, while SNe = ∑nte j,j′=1 K(xtej ,xtej′ )−Hjj′ (nte) 2 (x t ej − xtej′)(xtej − xtej′)T , the non-local scatter covariance matrix, enforces the second principle.
103	1	τe also depends on kernels which in this case indicate different neighbour information and different degrees of similarity between neighboured examples.
109	1	λ and μ balance the importance of the three terms in f , and b is the bias term.
111	1	The optimal latent feature factor matrix W∗Ne+1 should maximize the value of f .
117	1	We also provide and prove the algorithmic stability and generalization bound for latent feature factor based transfer learning algorithms without experiences considered in the supplementary.
119	1	Let L(S) be our algorithm that learns meta-cognitive knowledge from Ne transfer learning experiences in S and applies the knowledge to the (Ne+1)-th transfer learning task 〈SNe+1, TNe+1〉.
123	1	Under this assumption, L(S) is uniformly stable.
124	1	Meanwhile, for any e-th transfer learning experience, we assume that the latent feature factor matrix ‖We‖≤ rW .
125	1	To meet the assumption above, we reasonably simplify L(S) so that the latent feature factor matrix for the (Ne+1)-th transfer learning task is a linear combination of all Ne historical latent factor feature matrices plus a noisy latent feature matrix W satisfying ‖W ‖≤r , i.e., WNe+1= ∑Ne e=1 ceWe+W with each coefficient 0≤ce≤1.
126	1	Our algorithm L(S) is uniformly stable.
136	1	Caltech-256, collected from Google Images, contains a total of 30,607 images in 256 categories.
141	1	We characterize each image from both datasets with 4,096-dimensional features extracted by a convolutional neural network pre-trained by ImageNet.
142	1	In this paper we generate transfer learning experiences by ourselves, because we are the first to consider transfer learning experiences and there exists no off-the-shelf datasets.
143	1	In real-world applications, either the number of labeled examples in a target domain or the transfer learning algorithm could vary from experience to experience.
144	1	In order to mimic the real environment, we prepare each transfer learning experience by randomly selecting a transfer learning algorithm from a base set A and randomly setting the number of labeled target examples in the range of [3, 120].
147	1	• Common latent space based transfer learning algorithms: TCA (Pan et al., 2011), ITL (Shi & Sha, 2012), CMF (Long et al., 2014), LSDT (Zhang et al., 2016), STL (Raina et al., 2007), DIP (Baktashmotlagh et al., 2013) and SIE (Baktashmotlagh et al., 2014).
150	1	Based on feature representations obtained by different algorithms, we use the nearestneighbor classifier to perform three-class classification for the target domain.
153	1	The other evaluation metric we adopt is the performance improvement ratio defined in Section 3.1, so as to compare the L2T over different pairs of domains.
193	2	In particular, L2T learns a reflection function mapping a pair of domains and the knowledge transferred between them to the performance improvement ratio.
194	19	When a new pair of domains arrives, L2T optimizes what and how to transfer by maximizing the value of the learned reflection function.
195	87	We believe that L2T opens a new door to improve transfer learning by leveraging transfer learning experiences.
196	83	Many research issues, e.g., incorporating hierarchical latent feature factors as what to transfer and designing online L2T, can be further examined.
