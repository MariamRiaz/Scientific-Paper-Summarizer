2	40	With the opportunity to learn from data collected from different sources other than personal experimentation come new challenges of “transfer” in learning.
3	22	In particular, agents should know that actions that are desirable for populations may not be desirable for all individuals, and as such, should be wary of how observed behavior generalizes (i.e., transfers) to them and how these observations should be combined with the agent’s own experience.
4	8	In this work, we study the conditions under which data collected under heterogeneous conditions (to be defined) can be combined by an online agent to improve performance in a reinforcement learning task.
5	31	This challenge is not without precedent, as recent works have investigated dataset transportability (when source and target differ structurally), though in offline domains (3; 4).
7	91	Recent work from causal analysis has addressed data-fusion for interventional quantities in reinforcement learning tasks (18).
12	32	Our agent’s goal is to quickly learn about its environment by consolidating data collected from observing other agents and data collected through its own experience, so UCs pose a fundamental challenge: the results from seeing another agent performing an action are not always qualitatively the same as doing the action itself.
16	89	Counterfactual data (though traditionally computed from a fully specified model or under specific empirical conditions) represents the rewards associated with actions under a particular (or “personalized”) instantiation of the UCs.
17	45	In the remainder of this work, we demonstrate how these data types can be fused to facilitate learning in a variant of the Multi-Armed Bandit problem with Unobserved Confounders (MABUC), first discussed in (2).
21	13	Though the data-fusion problem is an ongoing exploration in the data sciences (4), this paper is the first to study learning techniques in MABUC settings that combine data sampled under heterogeneous data-collection modes.
26	12	In this section, we consider an expanded version of the Greedy Casino problem introduced in (2).
28	10	After running a battery of preliminary tests, the casino executives discover that two traits in particular predict which of the four machines that a gambler is likely to play: whether or not the machines are all blinking (denotedB ∈ {0, 1}), and whether or not the gambler is drunk (denoted D ∈ {0, 1}).
29	12	After consulting with a team of psychologists and statisticians, the casino learns that any arbitrary gambler’s natural machine choice can be modeled by the structural equation (12): X ← fX(B,D) = B+2 ∗D if the four machines are indexed as X ∈ {0, 1, 2, 3}.
30	38	The casino also learns that its patrons have an equal chance of being drunk or not (i.e., P (D = 1) = 0.5) and decide to program their new machines to blink half of the time (i.e., P (B = 1) = 0.5).
32	52	Wishing to leverage their new discovery about gamblers’ machine choice predilections while conscious of this law, the casino implements a reactive payout strategy for their machines, which are equipped with sensors to determine if their gambler is drunk or not (assume that the sensors are perfect at making this determination).
38	8	Plainly, gamblers are unaware of being manipulated by the UCs B,D, and of the predatory payout policy that the casino has constructed around them.
39	10	The collected data is summarized in Table 1b; the second column (E[y1|X]) represents the observations drawn from the casino’s floor while the third
50	30	(Counterfactual) (12) LetX and Y be two subsets of endogenous variables in V .
56	20	As is well understood in the causal inference literature, this procedure is not the same as first exposing a random unit to condition X = x′ since the ones who initially were inclined to act as X = x are somehow different than the randomly selected subject.
58	14	4) that online learning agents possess the means to estimate counterfactuals directly.
67	36	We can now put these observations together and explicitly define the MABUC problem: Definition 3.3.
72	95	Choice: Xt ∈ {x1, ..., xk} denotes the agent’s final arm choice that is “pulled” at round t, xt = fx(πt).
74	10	2 represents a prototypical MABUC (Def.
75	83	We also add a graphical representation of the agent’s history Ht, a data structure containing the agent’s observations, experiments, and counterfactual experiences up to time step t. The means by which these different data-collections can be used in the agent’s policy are explored at length in the next section.
77	53	Based on this definition, the regret decision criterion can be stated (2): Definition 3.4.
78	18	(Regret Decision Criterion (RDC)) (2) In a MABUC instance with arm choice X , intent I = i, and reward Y , agents should choose the action a that maximizes their intent-specific reward, or formally: argmax a E[YX=a|X = i] (1) In brief, RDC prescribes that the arm X = a that maximizes the expected value of reward Y having conditioned on the intended arm X = i should be selected, even when a 6= i.
84	42	So, the agent can choose to either discard its observations and experiments, and simply gamble by the tenets of RDC, or combine them in an informative way.
96	28	The counterfactual ETT is empirically estimable for arbitrary action-choice dimension (i.e., |X| = k for k ≥ 2) when agents condition on their intent I = i and estimate the response Y to their final action choice X = a.
97	249	For a proof of Theorem 4.1, see supplementary material, Appendix A.
98	108	Because RDC is equivalently an interventional quantity, we have shown that the ETT, a counterfactual expression, can be estimated empirically through counterfactual-based randomization.5 The main advantages of this, now proven, equivalence are threefold: (1) the empirical estimation of previously unidentifiable counterfactual quantities presents opportunities for further exploration in causal analysis, (2) the ETT’s prescription for integrating experimental and observational data (see Eq.
101	80	A second consequence of recording arm-intent-specific payouts in this fashion is that observational data may be substituted directly into cells for which the final arm choice and intent agree (see reference to consistency axiom below Eq.
