0,1,label2,summary_sentences
Neural networks trained through stochastic gradient descent (SGD) can memorize their training data.,1. Introduction,[0],[0]
"Although practitioners have long been aware of this phenomenon, Zhang et al. (2017) recently brought attention to it by showing that standard SGD-based training on AlexNet gets close to zero training error on a modification of the ImageNet dataset even when the labels are randomly permuted.",1. Introduction,[0],[0]
"This leads to an interesting question: If neural nets have sufficient capacity to memorize random training sets why do
1Two Sigma, New York, NY, USA.",1. Introduction,[0],[0]
"Correspondence to: Satrajit Chatterjee <satrajit.chatterjee@twosigma.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
they generalize on real data?,1. Introduction,[0],[0]
A natural hypothesis is that nets behave differently on real data than on random data.,1. Introduction,[0],[0]
Arpit et al. (2017) study this question experimentally and show that there are apparent differences in behavior.,1. Introduction,[0],[0]
"They conclude that generalization and memorization depend not just on the network architecture and optimization procedure but on the dataset itself.
",1. Introduction,[0],[0]
"But what if networks fundamentally do not behave differently on real data than on random data, and, in both cases, are simply memorizing?",1. Introduction,[0],[0]
This is a difficult question to explore for two reasons.,1. Introduction,[0],[0]
"First, it is hard to provide a direct answer.",1. Introduction,[0],[0]
Whereas it is easy to tell when a net is memorizing random data (the training error goes to zero!),1. Introduction,[0],[0]
", there is no easy way to tell when a network is memorizing real data as opposed to “learning”.",1. Introduction,[0],[0]
"Second, and perhaps more importantly, it contradicts the intuitive notion—inherent in the preceding discussion—that memorization and generalization are at odds.",1. Introduction,[0],[0]
This work attempts to shed light on this second difficulty by investigating the following: How much can you learn if memorization is all you can do?,1. Introduction,[0],[0]
"Is generalization even possible in this setting?
",1. Introduction,[0],[0]
"At first, generalization in such a setting of pure memorization may seem hopeless: the simplest way to memorize would be to build a lookup table from the training data.",1. Introduction,[0],[0]
"Although this approach works for special cases where the input population is finite and small, it fails in general since the examples seen during training are unlikely to match test examples.",1. Introduction,[0],[0]
One way to get around this limitation is to use k-Nearest Neighbors (k-NN) or any of its variants at test time.,1. Introduction,[0],[0]
"While k-NNs work well on many problems, they fail on problems where it is not easy to construct a semantically meaningful distance function on the input space.",1. Introduction,[0],[0]
"In such cases, the obvious syntactic distance functions (e.g., say Euclidean distance between images viewed as vectors in Rd) do not work well.",1. Introduction,[0],[0]
"Indeed some of the most interesting results from deep learning have been the discovery—through learning—of semantically meaningful distance functions (via embeddings).
",1. Introduction,[0],[0]
"Therefore, in this work we do not allow ourselves a distance function.",1. Introduction,[0],[0]
"Instead, we get around the problem by applying the notion of depth, which has been wildly successful in improving the performance of neural networks, to direct memorization.",1. Introduction,[0],[0]
"We build a network of lookup tables (also
called “luts”) where the luts are arranged in successive layers much like a neural network.",1. Introduction,[0],[0]
"However, unlike a neural network, training happens through memorization and does not involve backpropagation, gradient descent, or any explicit search.",1. Introduction,[0],[0]
"Now, since in contrast to a neuron, the function implemented by a lut can be arbitrarily complex, without some means to control the complexity, the notion of depth is vacuous.",1. Introduction,[0],[0]
We control the complexity of a function learned by a lut in the simplest possible way: we limit the support (and thereby the size) of the lut.,1. Introduction,[0],[0]
"Each lut in a layer receives inputs from only a few luts in the previous layer, which are picked at random when the network is constructed.",1. Introduction,[0],[0]
This kind of restriction on local function complexity is similar to what is found to work well in deep neural networks.,1. Introduction,[0],[0]
"For example, a convolutional filter is obviously support-limited, and a fully connected layer although not support-limited is nevertheless limited in expressivity.",1. Introduction,[0],[0]
"Furthermore, the learned weight matrices in neural networks are often sparse or can be made so with no loss in accuracy (Han et al., 2015).
",1. Introduction,[0],[0]
We need two restrictions before we can proceed to an algorithm.,1. Introduction,[0],[0]
"First, for simplicity, we focus our attention on binary classification problems.",1. Introduction,[0],[0]
"Second, because lookup tables work naturally with discrete inputs, in this work we limit ourselves to discrete signals.",1. Introduction,[0],[0]
"In fact, the inputs and all intermediate signals in the network of lookup tables are binary.",1. Introduction,[0],[0]
The restriction is not as extreme as it may appear.,1. Introduction,[0],[0]
"There are a number of results in quantized and binary neural networks showing that limited precision is often sufficient (e.g. Rastegari et al., 2016).",1. Introduction,[0],[0]
"Furthermore, even in real-valued neural networks, we need mechanisms such as convolution and pooling to ensure that certain types of small changes in the inputs (e.g., a small displacement) do not lead to large changes in output.",1. Introduction,[0],[0]
"In principle, similar mechanisms could be used in a fully discrete setting to handle real-valued quantities.
",1. Introduction,[0],[0]
"With these restrictions in place, we are now ready to proceed.",1. Introduction,[0],[0]
"Let B = {0, 1} and consider the problem of learning a function f :",2. A Single Lookup Table,[0],[0]
"Bk → B from a list of training examples where each example is an (x, y) pair.",2. A Single Lookup Table,[0],[0]
"Since we want to learn by memorizing, we construct a lookup table with 2k rows (one for each possible bit pattern p ∈",2. A Single Lookup Table,[0],[0]
Bk that can appear at the input) and two columns y0 and y1.,2. A Single Lookup Table,[0],[0]
"The y0 entry for the row corresponding to pattern p (denoted by cp0) counts how many times p is associated with output 0 in the training set, i.e., the number of occurrences of (p, 0) in the training set.",2. A Single Lookup Table,[0],[0]
"Similarly, the y1 entry for row p (denoted by cp1) counts how many times the pattern p is associated with the output 1 in the training set, i.e., the number of occurrences of (p, 1)
in the training set.",2. A Single Lookup Table,[0],[0]
"Note that for a pattern p it is possible for both cp0 and cp1 to be greater than zero since due to Bayes error both (p, 0) and (p, 1) may be present in the training examples.",2. A Single Lookup Table,[0],[0]
It is also possible for both cp0 and cp1 to be zero if the input p never appears in the training examples.,2. A Single Lookup Table,[0],[0]
"We call such a lookup table a k-input lookup table or a k-lut since the inputs are bit vectors of length k.1
Next, we associate a boolean function f̂ : Bk → B with the lookup table in the following manner:
f̂(p) =  1 if cp1 > cp0,0 if cp1 < cp0, b if cp1 = cp0
where b ∈ B is picked uniformly at random when fixing f̂ in order to break ties.",2. A Single Lookup Table,[0],[0]
"In other words, f̂ maps an input p to the output that is most often associated with it in the training set (breaking ties randomly).",2. A Single Lookup Table,[0],[0]
"We say that f̂ is the function learned by the lookup table.
",2. A Single Lookup Table,[0],[0]
Example 1.,2. A Single Lookup Table,[0],[0]
Let k = 3 and consider learning a function f : B3 → B from 7 examples shown on the left below.,2. A Single Lookup Table,[0],[0]
"The lookup table that we learn is shown in the middle, and the truth table of the learned function f̂ is shown on the right.",2. A Single Lookup Table,[0],[0]
"The entries in the truth table which have been picked randomly to break ties are indicated by an asterisk.
",2. A Single Lookup Table,[0],[0]
"x x0x1x2
y
000 0 000 1 000 1 001 1 100 0 110 0 110 1
p x0x1x2
y0 y1
000 1 2 001 0 1 010 0 0 011 0 0 100 1 0 101 0 0 110 1 1 111 0 0
p f̂
000 1 001 1 010 0∗ 011 1∗ 100 0 101 1∗ 110 1∗ 111 0∗
Note that f̂ gets all training examples correct except for the first and sixth.",2. A Single Lookup Table,[0],[0]
"This is the best we can do on this set of training examples because the Bayes error rate is non-zero.
",2. A Single Lookup Table,[0],[0]
"If we measure training error as the average 0–1 loss on the training set, this procedure to learn f̂ has the following properties:
1.",2. A Single Lookup Table,[0],[0]
Optimality.,2. A Single Lookup Table,[0],[0]
"The learned function f̂ is Bayes-optimal on the training set, i.e., there is no function g :",2. A Single Lookup Table,[0],[0]
Bk → B with training error strictly less than that of f̂ .,2. A Single Lookup Table,[0],[0]
"In particular, the training error is zero iff the training set has zero Bayes error.
2.",2. A Single Lookup Table,[0],[0]
Monotonicity.,2. A Single Lookup Table,[0],[0]
"If we have more information for each x in the training set, i.e., we augment each training
1 Typically k is small (less than 10) and so the the table can be stored explicitly.",2. A Single Lookup Table,[0],[0]
"The input bit vector (viewed as an integer) can be used to directly index into the table.
example with m extra bits of information (keeping the labels fixed) and use the above procedure to now learn a new function ĝ : Bk+m → B, then the training error of ĝ is no more than that of f̂ .
",2. A Single Lookup Table,[0],[0]
Proof Sketch.,2. A Single Lookup Table,[0],[0]
Optimality is easy to see since the total training error is the sum of the training error for each possible pattern p which is minimized by choosing the majority class for each p.,2. A Single Lookup Table,[0],[0]
"Monotonicity holds since if not, then we can compose the obvious projection Bk+m",2. A Single Lookup Table,[0],[0]
"→ Bk with f̂ to get a contradiction with the optimality of ĝ.
Note that monotonicity implies in particular that the training accuracy at the output of a lut is no worse than that at any of its inputs.",2. A Single Lookup Table,[0],[0]
"Furthermore, as we make the luts larger, the training error cannot increase but only decrease.",2. A Single Lookup Table,[0],[0]
This is interesting since there are no restrictions on the m extra bits: they could be completely non-informative.,2. A Single Lookup Table,[0],[0]
"These properties will prove useful in the next section as we consider networks of luts.
",2. A Single Lookup Table,[0],[0]
"To summarize, the procedure described to learn a single lookup table in this section is essentially memorization in the presence of Bayes error, where the idea is to simply remember the output that is most commonly associated with an input in the training set.",2. A Single Lookup Table,[0],[0]
"Now consider a binary classification task on MNIST (LeCun & Cortes, 2010) of separating the digits ‘0’ through ‘4’ (we map these to the 0 class) from the digits ‘5’ through ‘9’ (the 1 class) where the pixels are 1-bit quantized.",3. A Network of Lookup Tables,[0],[0]
"Thus the task is to learn a function f : B28×28 → B. We call this the Binary-MNIST task (overloading binary here to mean both binary classification and binary inputs).
",3. A Network of Lookup Tables,[0],[0]
"In principle, we could use the procedure in Section 2 to learn this function.",3. A Network of Lookup Tables,[0],[0]
"However, since we have only 60,000 training examples in MNIST, most of the 228×28 rows in the lookup table would have 0 entries in both columns, and hence the function learned would be mostly random and have very poor generalization to inputs outside the training set.
",3. A Network of Lookup Tables,[0],[0]
"As discussed in the introduction, we get around this problem by introducing depth.",3. A Network of Lookup Tables,[0],[0]
"Instead of learning a giant lookup table with 228×28 entries, we learn a network of (much) smaller lookup tables.",3. A Network of Lookup Tables,[0],[0]
The network consists of d layers with each layer l (1 ≤ l ≤ d) having nl k-input lookup tables.,3. A Network of Lookup Tables,[0],[0]
Each lut in first layer (l = 1) receives its inputs from a krandom subset of the network inputs.,3. A Network of Lookup Tables,[0],[0]
A lut in a layer,3. A Network of Lookup Tables,[0],[0]
l > 1 receives inputs from a k-random subset of the luts in layer l − 1.,3. A Network of Lookup Tables,[0],[0]
The connectivity is fixed at network creation time and does not change during training or inference.,3. A Network of Lookup Tables,[0],[0]
"The final layer of the network has a single lookup table (i.e., nd = 1) which is the output of the network.",3. A Network of Lookup Tables,[0],[0]
"By analogy with neural
networks, we call the final layer the output layer and the other layers hidden layers.
",3. A Network of Lookup Tables,[0],[0]
"We train the lookup tables layer by layer, where the target of each lookup table is the final output.",3. A Network of Lookup Tables,[0],[0]
We start from the first layer and work our way to the output.,3. A Network of Lookup Tables,[0],[0]
"Once a layer has been learned, we use the functions associated with its luts (the f̂s of Section 2) to map its inputs to outputs.",3. A Network of Lookup Tables,[0],[0]
"These outputs serve as the inputs for the next layer, which is learned next.",3. A Network of Lookup Tables,[0],[0]
"Continuing our analogy with neural networks, we call the output values of a layer activations.
",3. A Network of Lookup Tables,[0],[0]
"Inference is similar to training: We start from the inputs and evaluate each layer in order using the functions learned at each lut to map inputs to outputs.
",3. A Network of Lookup Tables,[0],[0]
Example 2.,3. A Network of Lookup Tables,[0],[0]
We modify Example 1.,3. A Network of Lookup Tables,[0],[0]
"Instead of learning a single lut with k = 3 inputs, we learn a network of k = 2 luts.",3. A Network of Lookup Tables,[0],[0]
The network shown in Figure 1 has d = 2 layers.,3. A Network of Lookup Tables,[0],[0]
"The first layer has 2 luts (i.e., n1 = 2) which are connected to inputs x0 and x1 of the network.",3. A Network of Lookup Tables,[0],[0]
"The second layer (which is also the output layer) has 1 lut (i.e., n2 = 1) which is connected to the outputs of the two luts in the first layer.",3. A Network of Lookup Tables,[0],[0]
(The connections were made randomly when the network was created.),3. A Network of Lookup Tables,[0],[0]
"Using the procedure in Section 2, the two lookup tables learned in the first layer (using y as the target) along with their corresponding functions f̂10 and f̂11 are:
p x0x1
y0 y1 f̂10
00 1 3 1 01 0 0 1∗ 10 1 0 0 11 1 1 1∗
p x0x2
y0 y1 f̂11
00 1 2 1 01 0 1 1 10 2 1 0 11 0 0 1∗
Let the output of the luts in the first layer be w10 and w11, i.e., w10 = f̂10(x0x1) and w11 = f̂11(x0x2).",3. A Network of Lookup Tables,[0],[0]
The learning problem for the lut in the second layer is shown in the tables below.,3. A Network of Lookup Tables,[0],[0]
"For convenience, on the left we show the primary inputs x0, x1 and x2, the first layer activations w10 and w11 (which are the inputs of the lut), and the target output y.",3. A Network of Lookup Tables,[0],[0]
"On the right we show the table and the learned function f̂20:
x x0x1x2
w10w11 y
000 11 0 000 11 1 000 11 1 001 11 1 100 00 0 110 10 0 110 10 1
p w10w11
y0 y1 f̂20
00 1 0 0 01 0 0 1∗ 10 1 1 0∗ 11 1 3 1
In this case the function implemented by the network of 2-luts has the same performance on the training set as the function learned by the 3-lut in Example 1.",3. A Network of Lookup Tables,[0],[0]
"Since there are fewer possible patterns in the case of smaller luts, we expect better pattern coverage during training and hence better generalization.
",3. A Network of Lookup Tables,[0],[0]
Implementation.,3. A Network of Lookup Tables,[0],[0]
"The memorization procedure described here is linear in the size of the training data, requiring two passes over the training set.",3. A Network of Lookup Tables,[0],[0]
It is computationally efficient since it only involves counting and dense table lookups and does not require floating point.,3. A Network of Lookup Tables,[0],[0]
"It is also easy to parallelize since each lut in a given layer is independent, and the counts can be computed on disjoint subsets of the training data and then combined (using, for example, a reduction tree).",3. A Network of Lookup Tables,[0],[0]
Note that using this property it is possible to execute the algorithm on extremely large datasets where all the training examples may not fit on a single machine with only the summary statistics of the data (the counts in the lookup tables) being exchanged across machines.,3. A Network of Lookup Tables,[0],[0]
Experiment 1.,4. Experiments,[0],[0]
"In the first experiment, we apply the above procedure to the Binary-MNIST task (as defined in Section 3) to see if this approach to memorization can generalize.",4. Experiments,[0],[0]
"For this experiment, we construct a network with 5 hidden layers of 1024 luts and 1 lut in the output layer.",4. Experiments,[0],[0]
"We set k = 8, i.e., each lut in the network takes 8 inputs.
",4. Experiments,[0],[0]
"The network achieves a training accuracy of 0.89 on this task, which is perhaps not so surprising since we are memorizing the training data after all.",4. Experiments,[0],[0]
"But what is surprising is that the network achieves an accuracy of 0.87 on a heldout set (the 10,000 test images in MNIST) which indicates generalization.
",4. Experiments,[0],[0]
"This result is not state-of-the-art on this variant of MNIST (see Experiment 4), but that is not the point.",4. Experiments,[0],[0]
"It is significantly above the 0.5 accuracy that would be expected by chance, and this is achieved by an algorithm that only memorizes and performs no explicit search.
",4. Experiments,[0],[0]
The training and test accuracies are stable: there is very little variation from run to run.,4. Experiments,[0],[0]
"In other words, very little depends on the actual random choices made when deciding the topology of the network.",4. Experiments,[0],[0]
"To understand why this is
the case, we look at training accuracies of the luts in the network.",4. Experiments,[0],[0]
"Since the target for each lut in the network is the final classification target, we can examine the accuracy of a lut as a function of its layer.
",4. Experiments,[0],[0]
Table 1 shows the summary statistics for the accuracies of luts in each layer.,4. Experiments,[0],[0]
We observe that as depth increases the average accuracy of the luts in a layer goes up.,4. Experiments,[0],[0]
"In other words, depth helps.",4. Experiments,[0],[0]
"Some intuition for this is provided by the monotonicity property of the luts: the output of a lut cannot have lower accuracy than any of its inputs (Section 2).
",4. Experiments,[0],[0]
"Furthermore, we observe in Table 1 the dispersion in accuracy across the luts (measured either by standard deviation (std) or the difference between max and min) goes down.",4. Experiments,[0],[0]
"Therefore, as depth increases the specifics of the connectivity matters less and the network automatically becomes more stable with respect to the random choices made during construction.",4. Experiments,[0],[0]
"Indeed we can say something stronger: we have seen in our experiments (not shown in Table 1) that as depth increases the activations of the luts in a layer become more correlated with each other, and hence become more interchangeable.",4. Experiments,[0],[0]
"While this correlation is good for stability with respect to connectivity, it causes diminishing returns with additional depth.
",4. Experiments,[0],[0]
Remark.,4. Experiments,[0],[0]
The perceptive reader looking at Table 1 will also notice that we are wasting computation: the single output lut in layer 6 receives input from only 8 of the 1024 luts in layer 5 and these in turn can at most receive inputs from 64 luts from layer 4.,4. Experiments,[0],[0]
"Although a different topology would be more computationally efficient, this specific choice allows us to compare the different layers more easily.",4. Experiments,[0],[0]
"We have not optimized this aspect since it typically takes less than 30 seconds using a single threaded unoptimized implementation (Python with NumPy) to run an experiment.
",4. Experiments,[0],[0]
Experiment 2.,4. Experiments,[0],[0]
"As discussed in the introduction and in Section 3, we do not expect unbridled memorization in the form of a large lookup table (say k = 28 × 28 in the case of Binary-MNIST) to generalize at all.",4. Experiments,[0],[0]
"This motivated our
exploration of a network of smaller lookup tables parameterized by k (the number of inputs of each lut).",4. Experiments,[0],[0]
We now vary k to see if we can control the amount of memorization and to see the effect it has on generalization.,4. Experiments,[0],[0]
"To avoid changing too much at once, we keep the number of layers and the number of luts per layer the same as in Experiment 1.
",4. Experiments,[0],[0]
The results are shown in the first 3 columns of Table 2.,4. Experiments,[0],[0]
"With small values of k, the network finds it difficult to memorize the training data.",4. Experiments,[0],[0]
"As intuitively expected (see also the monotonicity property in Section 2), as k increases the training accuracy goes up with perfect memorization at k = 14, i.e., long before 28×28.",4. Experiments,[0],[0]
"However, larger luts generalize less well, and the best test accuracy of 0.90 is achieved at k = 12 though with substantially good memorization of the training data (0.99).",4. Experiments,[0],[0]
"Interestingly, there is a clear monotonic increase in the generalization gap measured as the difference between training and test accuracy with increasing k.
Experiment 3.",4. Experiments,[0],[0]
In this experiment—along the lines of those performed in Zhang et al. (2017)—we randomly permute the labels in the training set and repeat Experiment 2 on this “random” dataset.,4. Experiments,[0],[0]
The results are shown in columns 4 and 5 of Table 2.,4. Experiments,[0],[0]
"As expected, with increasing k the network gets better at memorizing the training data, and the test accuracy hovers around chance (0.5) though with significant variation (± 0.05).",4. Experiments,[0],[0]
"This may be viewed as empirical evidence that the Rademacher complexity goes up with k.
However, and this may be surprising for a pure memorization algorithm, memorizing random data turns out to be harder than memorizing real data (columns 2 and 3 of Table 2) in the sense that a larger k is required to get the same accuracy with random data than with real data.",4. Experiments,[0],[0]
"For example, it takes until k = 12 to get comparable training accuracy on random data as k = 4 gets on real data.",4. Experiments,[0],[0]
"This result corroborates the findings in Arpit et al. (2017, §3 and §4) that real data is easier to fit than random data.",4. Experiments,[0],[0]
But it also means that we cannot conclude that any such difference observed in neural networks is because they do not use brute force memorization on real data.,4. Experiments,[0],[0]
"As this experiment shows, such
differences can appear even with brute force memorization.
",4. Experiments,[0],[0]
"Finally, at k = 12 we have a network that is able to memorize random data (random training accuracy of 0.82) and yet generalizes to test data when trained on real data (real test accuracy of 0.90).",4. Experiments,[0],[0]
This is very similar to findings of Zhang et al. (2017) in the context of neural networks.,4. Experiments,[0],[0]
"Kawaguchi et al. (2017, §3) argue that this phenomenon is universal and our result may be viewed as further empirical evidence for their claim showing that this phenomenon can happen even in the simplified setting of just memorization.
",4. Experiments,[0],[0]
Experiment 4.,4. Experiments,[0],[0]
"For completeness, we compare memorization with several standard methods and the results are shown in Table 3.",4. Experiments,[0],[0]
We have not specifically tuned the other methods since our goal is not to beat the state-of-the-art but to get a sense of how memorization alone does when compared to the standard methods.,4. Experiments,[0],[0]
"The best performance is obtained by a LENET-style convolutional network with 2 convolutions (64 and 32 filters respectively) each followed by a corresponding max pool layer, and 3 fully connected layers (256, 128 and 2 units respectively) with softmax output.",4. Experiments,[0],[0]
"The net is trained for 6 epochs with stochastic gradient descent and dropout.
",4. Experiments,[0],[0]
"Once again, compared to random guessing which has 0.50 test accuracy, memorization does quite well with a test accuracy of 0.90 (using the k = 12 configuration from Experiment 2) and beats logistic regression and naı̈ve Bayes.",4. Experiments,[0],[0]
"Interestingly, 1- and 5-Nearest Neighbors do well too (test accuracy of 0.97) though recall that they are provided with a distance function which memorization does not have access to and must in a sense discover.
",4. Experiments,[0],[0]
Experiment 5.,4. Experiments,[0],[0]
"We now consider the task of separating the i-th digit in MNIST from the j-th digit, which gives us( 10 2 ) = 45 binary classification tasks, which we collectively call Pairwise-MNIST.",4. Experiments,[0],[0]
"The images are binarized as before.
",4. Experiments,[0],[0]
"Figure 2 shows the training accuracy and the test accuracy for each of those 45 experiments for 8 different values of k.
As in Experiment 2, we find that as k increases, the training accuracy increases (reaching 1.0), but the test accuracy falls off.",4. Experiments,[0],[0]
"If we look at the best test accuracies for a given task (across k), on 31 out of the 45 tasks, we do better than 0.98.",4. Experiments,[0],[0]
The worst of these is 0.95 which is the best memorization can do for separating ‘4’ and ‘9’.,4. Experiments,[0],[0]
This is still significantly better than the 0.5 we would expect by chance.,4. Experiments,[0],[0]
"Typically the best test accuracies are achieved at k = 6 and k = 8.
",4. Experiments,[0],[0]
Experiment 6.,4. Experiments,[0],[0]
In Experiment 5 we notice that the variation is quite high for k = 2.,4. Experiments,[0],[0]
This indicates that the depth of the network is insufficient for proper mixing.,4. Experiments,[0],[0]
"To investigate this further, we keep k = 2 and vary the number of hidden layers from 20 to 25.",4. Experiments,[0],[0]
Each hidden layer still has 1024 luts.,4. Experiments,[0],[0]
Figure 3 shows how the training and test accuracies vary with the depth of the network.,4. Experiments,[0],[0]
"It is interesting to note that the test accuracy continues to improve even for relatively deep networks (16 or 32 hidden layers), and we get very high test accuracies even with such small lookup tables.",4. Experiments,[0],[0]
"Furthermore, we note that the variation in the generalization error (difference between training and test accuracies) decreases with increasing depth.
",4. Experiments,[0],[0]
Experiment 7.,4. Experiments,[0],[0]
Next we look at memorization on CIFAR-10 which is a collection of 32 pixel by 32 pixel color images belonging to 10 classes.,4. Experiments,[0],[0]
"As with Binary-MNIST, we quantize each color channel to 1 bit and try to separate the classes 0 through 4 from classes 5 through 9.",4. Experiments,[0],[0]
"This gives us the Binary-CIFAR-10 task where we have to learn a function f : B3×32×32 → B from 50,000 images.",4. Experiments,[0],[0]
"Incidentally, the quantization of each color channel to 1-bit significantly degrades the signal making it a difficult task for humans.
",4. Experiments,[0],[0]
"For this task, we construct a network with 5 hidden layers each with 1024 luts and one output layer with 1 output.",4. Experiments,[0],[0]
We set k = 10 for the luts.,4. Experiments,[0],[0]
This network is able to achieve a training accuracy of 0.79 and a test accuracy of 0.63.,4. Experiments,[0],[0]
"Although not as impressive in absolute terms as the memoriza-
tion result on Binary-MNIST, it is still significantly above chance (0.50).",4. Experiments,[0],[0]
"Furthermore, as before, the result is very stable and does not depend on a specific random topology chosen when the network is constructed.
",4. Experiments,[0],[0]
We compare memorization with several standard methods in Table 4.,4. Experiments,[0],[0]
By comparing Table 4 with Table 3 it is clear that Binary-CIFAR-10 is a harder task than Binary-MNIST since all the methods perform significantly worse on it.,4. Experiments,[0],[0]
"The best test accuracy of 0.71 is again from a LENET-style network similar to the one used in Experiment 4, but with 40 epochs of training.",4. Experiments,[0],[0]
"We believe a ResNet-style architecture (He et al., 2016) may potentially do better here but since our goal is not to achieve state-of-the-art but see how memorization does, we leave this to future work.",4. Experiments,[0],[0]
"For the same reason we don’t explore data augmentation here which is a standard technique for CIFAR-10.
Once again, memorization compares favorably on test accuracy with the other methods, and compared to Binary-MNIST it does relatively better here since it ties with the nearest neighbor searches.
",4. Experiments,[0.9519544579022476],"['Moreover, we can surprisingly do this with a computational cost comparable to that of methods that have access to the closed form kernel k(x1,x2).']"
Experiment 8.,4. Experiments,[0],[0]
"In this experiment, we consider the Pairwise-CIFAR-10 tasks which are defined analogously to Pairwise-MNIST.",4. Experiments,[0],[0]
We use the same network architecture as in Experiment 7 instead of optimizing specifically for these tasks.,4. Experiments,[0],[0]
Training accuracies are generally 0.95 and above whereas the test accuracies range from 0.61 (CAT v/s DOG) to 0.85,4. Experiments,[0],[0]
"(FROG v/s SHIP) with an average test accuracy of 0.76 which is significantly above chance.
",4. Experiments,[0],[0]
Experiment 9.,4. Experiments,[0],[0]
"To get qualitative insight into the decision boundaries learned with different levels of memorization, we classify points in the region [−2, 2] ×",4. Experiments,[0],[0]
"[−2, 2] ∈ R2 as being inside or outside the circle",4. Experiments,[0],[0]
x2,4. Experiments,[0],[0]
+,4. Experiments,[0],[0]
y2 ≤ 1.62.,4. Experiments,[0],[0]
"Our dataset consists of points on a 100 × 100 grid in this region which has been partitioned into equal test and training sets (Figure 4, leftmost column).",4. Experiments,[0],[0]
To make this a hard problem we encode each point as pair of 10-bit fixed-point numbers.,4. Experiments,[0],[0]
"We learn this function f : B20 → B using networks with 32 layers each with 2048 luts and vary k. With k = 10 (rightmost column), the training set is memorized perfectly but (as seen on test)",4. Experiments,[0],[0]
the concept is not learned.,4. Experiments,[0],[0]
"However, memorizing with k = 2, we learn a simpler concept that is not faithful around the “corners” (as can be seen by zooming in) but one that generalizes almost perfectly to test.",4. Experiments,[0],[0]
"Finally, k = 6 provides a satisfactory compromise between the two extremes.",4. Experiments,[0],[0]
"Thus, once again, we see that memorization if done carefully can lead to good generalization.",4. Experiments,[0],[0]
"It is instructive to compare our memorization procedure with a few commonly used procedures for learning.
",5. Comparison with Other Methods,[0],[0]
k-Nearest Neighbors.,5. Comparison with Other Methods,[0],[0]
"The key difference, as noted in the introduction, is that k-NNs require a user-specified distance function which is often syntactic notion of distance such that induced by treating an image as a vector in Rd.",5. Comparison with Other Methods,[0],[0]
These syntactic notions of distance do not work well on more challenging tasks and one may view such a learning problem as essentially that of discovering a semantically meaningful distance function.,5. Comparison with Other Methods,[0],[0]
"We see this in our experiments: the
tr ai
ni ng
−2.0 −1.5",5. Comparison with Other Methods,[0],[0]
−1.0,5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
te st
−2.0 −1.5",5. Comparison with Other Methods,[0],[0]
−1.0,5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
−0.5,5. Comparison with Other Methods,[0],[0]
"0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
ground truth k = 2 k = 6 k = 10
Figure 4.",5. Comparison with Other Methods,[0],[0]
"The decision boundaries learned in Experiment 9.
distance function helps more with Binary-MNIST (Experiment 4) than it does with Binary-CIFAR-10 (Experiment 7).",5. Comparison with Other Methods,[0],[0]
"Furthermore, in a separate experiment we found that augmenting the table lookup with 1-NN search at test time did not significantly improve test accuracy for Binary-CIFAR-10 where memorization was already tied with k-NNs.
Additionally, k-NN requires storing the entire training set and is typically computationally more expensive at test time.",5. Comparison with Other Methods,[0],[0]
"For example, on Binary-MNIST the standard k-NN implementation in scikit-learn (Pedregosa et al., 2011) took more than an hour to evaluate performance on the training and test sets (as opposed to seconds with memorization).",5. Comparison with Other Methods,[0],[0]
"There has been work on speeding up nearest neighbor search by using locality sensitive hashing (Indyk & Motwani, 1998) and, more recently, with random projections (Li & Malik, 2016).",5. Comparison with Other Methods,[0],[0]
"In that context, one may view each lookup table as implementing a trivial locality sensitive hash function where the distance metric arises from exact equality, and the network as an ensemble through cascading of such nearest neighbors classifiers.
",5. Comparison with Other Methods,[0],[0]
Neural Networks.,5. Comparison with Other Methods,[0],[0]
"The initial motivation for this work was to understand neural networks better; particularly to explore with a model the idea that perhaps SGD is a sophisticated way to memorize training data in a manner that generalizes and that perhaps there are simpler ways to memorize data
as well that may yet generalize.",5. Comparison with Other Methods,[0],[0]
"However, a key difference is that gradient descent-based training can learn useful intermediate representations or targets for hidden layers.",5. Comparison with Other Methods,[0],[0]
"In this work we have side stepped that question, by simply setting the intermediate target to be the final output.",5. Comparison with Other Methods,[0],[0]
It is an interesting line of research to see if we can find a way to learn useful intermediate signals in this setting perhaps by purely combinatorial methods.,5. Comparison with Other Methods,[0],[0]
"Practically, that would give us a method to learn purely binary neural networks without using floating point at all, which is useful in resource constrained environments.
",5. Comparison with Other Methods,[0],[0]
Random Forests.,5. Comparison with Other Methods,[0],[0]
"Trees in a random forest are constructed over a subset of the data by iteratively evaluating different input variables to optimize purity after splitting on the variable (Breiman, 2001).",5. Comparison with Other Methods,[0],[0]
"In contrast, memorization uses the whole dataset and does not solve any optimization problem (which makes it more computationally efficient).",5. Comparison with Other Methods,[0],[0]
"Furthermore, random forests combine the tree predictions using voting whereas memorization uses cascading.
",5. Comparison with Other Methods,[0],[0]
Cascading and Stacked Generalization.,5. Comparison with Other Methods,[0],[0]
"A recent extension of random forests are Deep Forests (Zhi-Hua Zhou, 2017) where multiple random forests are constructed at each level and then cascaded using the idea of stacked generalization (Wolpert, 1992) which is a generalization of cross-validation.",5. Comparison with Other Methods,[0],[0]
"In contrast, layers of luts are far simpler, and memorization propagates outputs based on what has been memorized over the entire training data.",5. Comparison with Other Methods,[0],[0]
"Due to the manner in which we construct the lookup tables and the corresponding functions (using the counts of the patterns) it is not clear to us that stacked generalization will help.
",5. Comparison with Other Methods,[0],[0]
Spectral Methods.,5. Comparison with Other Methods,[0],[0]
"There is a rich literature on the theory of learning boolean functions (f : Bk → B in our notation) (Mansour, 1994) which looks at theoretical learning guarantees under assumptions on the input distribution (typically uniform) and on the spectrum of the function (e.g. f can be approximated by a sparse and low degree polynomial in the boolean fourier basis).",5. Comparison with Other Methods,[0],[0]
"Recently, Hazan et al. (2017) have used these techniques in hyperparameter optimization where they find them to be practically useful (the distributional assumption is not fatal for this application).",5. Comparison with Other Methods,[0],[0]
"This line of work does not deal with depth, but only linear combinations of the basis functions.",5. Comparison with Other Methods,[0],[0]
"However, there is similarity in having a low degree in the fourier basis and our notion of support-limited memorization.",5. Comparison with Other Methods,[0],[0]
"These are similar structural priors and our results and those of Hazan et al. may be viewed as evidence that real world functions satisfy these priors.
",5. Comparison with Other Methods,[0],[0]
Learning Boolean Circuits.,5. Comparison with Other Methods,[0],[0]
"There is relatively little prior work in directly learning boolean circuits (Oliveira & Sangiovanni-Vincentelli, 1994; Tapp, 2014).",5. Comparison with Other Methods,[0],[0]
"However, it is interesting to note that the memorization algorithm in Section 3 although developed independently and from different
considerations is similar to the greedy algorithm described by Tapp.2",5. Comparison with Other Methods,[0],[0]
"An important difference is that instead of learning a single tree, we learn a network which makes learning more stable (as seen in Experiment 1).",5. Comparison with Other Methods,[0],[0]
"The experiments of Zhang et al. (2017) and Arpit et al. (2017) on training with random data lead naturally to the question that if neural networks can memorize random data and yet generalize on real data, are they perhaps doing something different in the two cases.",6. Conclusion,[0],[0]
This work started with the opposite thought: What if in both cases they are simply memorizing?,6. Conclusion,[0],[0]
"This, in turn, leads to the question of whether it is even possible to generalize from pure memorization.",6. Conclusion,[0],[0]
Naı̈ve memorization with a lookup table is too simplistic a model,6. Conclusion,[0],[0]
"but, as we saw, a slightly more complex model in the form of a network of support-limited lookup tables does significantly better than chance and is closer to the standard algorithms on a number of binary classification problems from MNIST and CIFAR-10.",6. Conclusion,[0],[0]
"(To investigate if this result holds on other datasets is an important area of future work.)
",6. Conclusion,[0],[0]
"Furthermore, this model replicates some of the key observations with neural networks: the performance of a network improves with depth; it memorizes random data and yet generalizes on real data; and memorizing random data is harder than real data.",6. Conclusion,[0],[0]
"In particular, the last observation implies that we cannot rule out memorization based on differences in the hardness of learning between real and random data.
",6. Conclusion,[0],[0]
"For future work, we would like to understand why memorization generalizes.",6. Conclusion,[0],[0]
"Now, since the size of the hypothesis space is bounded by 2n2 k
(where n is the number of k-luts in the network), we can use results from PAC-learning to bound the generalization gap, but these bounds are typically weak or vacuous.3 Rademacher complexity may be useful for small k (say 2), but for moderate k—where the Rademacher complexity is high yet there is generalization— we would need a different approach, perhaps one based on stability (Bousquet & Elisseeff, 2002).",6. Conclusion,[0],[0]
"In this connection, we expect the results in Devroye & Wagner (1979) to apply to a single lut, but extensions are needed to handle networks of luts, i.e., depth.",6. Conclusion,[0],[0]
"Furthermore, these would have to incorporate details of the construction since not every network of luts generalizes (even for k = 2).
",6. Conclusion,[0],[0]
"Finally, given the computational efficiency of memorization, we would like to extend it to a practically useful algorithm for learning, but that would likely involve introducing some form of explicit optimization or search.
",6. Conclusion,[0],[0]
2We thank David Krueger for noticing the connection.,6. Conclusion,[0],[0]
3,6. Conclusion,[0],[0]
"For example, using Theorem 2.2 in Mohri et al. (2012) for the experiments in Table 2 (with δ = 0.01 for concreteness) bounds the gap to 0.34 for k = 2.",6. Conclusion,[0],[0]
The bound doubles as k increases by 2.,6. Conclusion,[0],[0]
"I thank Ben Rossi, Vinod Valsalam, Rhys Ulerich, and Eric Allen for many useful discussions and Larry Rudolph and Steve Heller for their feedback on the paper.",Acknowledgments,[0],[0]
"In the machine learning research community, it is generally believed that there is a tension between memorization and generalization.",abstractText,[0],[0]
"In this work, we examine to what extent this tension exists, by exploring if it is possible to generalize by memorizing alone.",abstractText,[0],[0]
"Although direct memorization with a lookup table obviously does not generalize, we find that introducing depth in the form of a network of support-limited lookup tables leads to generalization that is significantly above chance and closer to those obtained by standard learning algorithms on several tasks derived from MNIST and CIFAR-10.",abstractText,[0],[0]
"Furthermore, we demonstrate through a series of empirical results that our approach allows for a smooth tradeoff between memorization and generalization and exhibits some of the most salient characteristics of neural networks: depth improves performance; random data can be memorized and yet there is generalization on real data; and memorizing random data is harder in a certain sense than memorizing real data.",abstractText,[0],[0]
The extreme simplicity of the algorithm and potential connections with generalization theory point to several interesting directions for future research.,abstractText,[0],[0]
Learning and Memorization,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 332–344 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1031",text,[0],[0]
"There is a growing interest in automated processing of historical documents, as evidenced by the growing field of digital humanities and the increasing number of digitally available collections of historical documents.",1 Introduction,[0],[0]
"A common approach to deal with the high amount of variance often found in this type of data is to perform spelling normalization (Piotrowski, 2012), which is the mapping of historical spelling variants to standardized/modernized forms (e.g. vnd→ und ‘and’).
",1 Introduction,[0],[0]
"Training data for supervised learning of historical text normalization is typically scarce, making it a challenging task for neural architectures, which typically require large amounts of labeled data.",1 Introduction,[0],[0]
"Nevertheless, we explore framing the
spelling normalization task as a character-based sequence-to-sequence transduction problem, and use encoder–decoder recurrent neural networks (RNNs) to induce our transduction models.",1 Introduction,[0],[0]
"This is similar to models that have been proposed for neural machine translation (e.g., Cho et al. (2014)), so essentially, our approach could also be considered a specific case of character-based neural machine translation.
",1 Introduction,[0],[0]
"By basing our model on individual characters as input, we keep the vocabulary size small, which in turn reduces the model’s complexity and the amount of data required to train it effectively.",1 Introduction,[0],[0]
Using an encoder–decoder architecture removes the need for an explicit character alignment between historical and modern wordforms.,1 Introduction,[0],[0]
"Furthermore, we explore using an auxiliary task for which data is more readily available, namely grapheme-tophoneme mapping (word pronunciation), to regularize the induction of the normalization models.
",1 Introduction,[0.9580564485614198],"['The observation we begin with is that sampling random features (i.e., w above), leads to an unbiased estimate of the kernel in Eq.']"
"We propose several architectures, including multi-task learning architectures taking advantage of the auxiliary data, and evaluate them across 44 small datasets from Early New High German.
",1 Introduction,[0],[0]
Contributions,1 Introduction,[0],[0]
"Our contributions are as follows:
• We are, to the best of our knowledge, the first to propose and evaluate encoder-decoder architectures for historical text normalization.
",1 Introduction,[0],[0]
"• We evaluate several such architectures across 44 datasets of Early New High German.
",1 Introduction,[0],[0]
"• We show that such architectures benefit from bidirectional encoding, beam search, and attention.
",1 Introduction,[0],[0]
"• We also show that MTL with pronunciation as an auxiliary task improves the performance of architectures without attention.
",1 Introduction,[0],[0]
"332
• We analyze the above architectures and show that the MTL architecture learns attention from the auxiliary task, making the attention mechanism largely redundant.
",1 Introduction,[0],[0]
"• We make our implementation publicly available at https://bitbucket.org/ mbollmann/acl2017.
",1 Introduction,[0],[0]
"In sum, we both push the state-of-the-art in historical text normalization and present an analysis that, we believe, brings us a step further in understanding the benefits of multi-task learning.",1 Introduction,[0],[0]
"Normalization For the normalization task, we use a total of 44 texts from the Anselm corpus (Dipper and Schultz-Balluff, 2013) of Early New High German.1",2 Datasets,[0],[0]
"The corpus is a collection of manuscripts and prints of the same core text, a religious treatise.",2 Datasets,[0],[0]
"Although the texts are semi-parallel and share some vocabulary, they were written in different time periods (between the 14th and 16th century) as well as different dialectal regions, and show quite diverse spelling characteristics.",2 Datasets,[0],[0]
"For example, the modern German word Frau ‘woman’ can be spelled as fraw/vraw (Me), frawe (N2), frauwe (St), fraüwe (B2), frow (Stu), vrowe (Ka), vorwe (Sa), or vrouwe (B), among others.2
All texts in the Anselm corpus are manually annotated with gold-standard normalizations following guidelines described in Krasselt et al. (2015).",2 Datasets,[0],[0]
"For our experiments, we excluded texts from the corpus that are shorter than 4,000 tokens, as well as a few for which annotations were not yet available at the time of writing (mostly Low German and Dutch versions).",2 Datasets,[0],[0]
"Nonetheless, the remaining 44 texts are still quite short for machine-learning standards, ranging from about 4,200 to 13,200 tokens, with an average length of 7,350 tokens.
",2 Datasets,[0],[0]
"For all texts, we removed tokens that consisted solely of punctuation characters.",2 Datasets,[0],[0]
"We also lowercase all characters, since it helps keep the size of the vocabulary low, and uppercasing of words is usually not very consistent in historical texts.",2 Datasets,[0],[0]
"Tokenization was not an issue for pre-processing these texts, since modern token boundaries have already been marked by the transcribers.
",2 Datasets,[0],[0]
"1https://www.linguistics.rub.de/ anselm/
2We refer to individual texts using the same internal IDs that are found in the Anselm corpus (cf.",2 Datasets,[0],[0]
"the website).
",2 Datasets,[0],[0]
Grapheme-to-phoneme mappings We use learning to pronounce as our auxiliary task.,2 Datasets,[0],[0]
This task consists of learning mappings from sequences of graphemes to the corresponding sequences of phonemes.,2 Datasets,[0],[0]
"We use the German part of the CELEX lexical database (Baayen et al., 1995), particularly the database of phonetic transcriptions of German wordforms.",2 Datasets,[0],[0]
"The database contains a total of 365,530 wordforms with transcriptions in DISC format, which assigns one character to each distinct phonological segment (including affricates and diphthongs).",2 Datasets,[0],[0]
"For example, the word Jungfrau ‘virgin’ is represented as ’jUN-frB.",2 Datasets,[0],[0]
"We propose several architectures that are extensions of a base neural network architecture, closely following the sequence-to-sequence model proposed by Sutskever et al. (2014).",3.1 Base model,[0],[0]
"It consists of the following:
• an embedding layer that maps one-hot input vectors to dense vectors;
• an encoder RNN that transforms the input sequence to an intermediate vector of fixed dimensionality;
• a decoder RNN whose hidden state is initialized with the intermediate vector, and which is fed the output prediction of one timestep as the input for the next one; and
• a final dense layer with a softmax activation which takes the decoder’s output and generates a probability distribution over the output classes at each timestep.
",3.1 Base model,[0],[0]
"For the encoder/decoder RNNs, we use long short-term memory units (LSTM) (Hochreiter and Schmidhuber, 1997).",3.1 Base model,[0],[0]
"LSTMs are designed to allow recurrent networks to better learn long-term dependencies, and have proven advantageous to standard RNNs on many tasks.",3.1 Base model,[0],[0]
"We found no significant advantage from stacking multiple LSTM layers for our task, so we use the simplest competitive model with only a single LSTM unit for both encoder and decoder.
",3.1 Base model,[0],[0]
"By using this encoder–decoder model, we avoid the need to generate explicit alignments between the input and output sequences, which would bring up the question of how to deal with input/output
pairs of different lengths.",3.1 Base model,[0],[0]
"Another important property is that the model does not start to generate any output until it has seen the full input sequence, which in theory allows it to learn from any part of the input, without being restricted to fixed context windows.",3.1 Base model,[0],[0]
An example illustration of the unrolled network is shown in Fig. 1.,3.1 Base model,[0],[0]
"During training, the encoder inputs are the historical wordforms, while the decoder inputs correspond to the correct modern target wordforms.",3.2 Training,[0],[0]
"We then train each model by minimizing the crossentropy loss across all output characters; i.e., if y = (y1, ..., yn) is the correct output word (as a list of one-hot vectors of output characters) and ŷ",3.2 Training,[0],[0]
"= (ŷ1, ..., ŷn) is the model’s output, we minimize the mean loss−∑ni=1 yi log ŷi over all training samples.",3.2 Training,[0],[0]
"For the optimization, we use the Adam algorithm (Kingma and Ba, 2015) with a learning rate of 0.003.
",3.2 Training,[0],[0]
"To reduce computational complexity, we also set a maximum word length of 14, and filter all training samples where either the input or output word is longer than 14 characters.",3.2 Training,[0],[0]
"This only affects 172 samples across the whole dataset, and is only done during training.",3.2 Training,[0],[0]
"In other words, we evaluate our models across all the test examples.",3.2 Training,[0],[0]
"For prediction, our base model generates output character sequences in a greedy fashion, selecting the character with the highest probability at each timestep.",3.3 Decoding,[0],[0]
"This works fairly well, but the greedy approach can yield suboptimal global picks, in which each individual character is sensibly derived from the input, but the overall word is non-
sensical.",3.3 Decoding,[0],[0]
"We therefore also experiment with beam search decoding, setting the beam size to 5.
",3.3 Decoding,[0],[0]
"Finally, we also experiment with using a lexical filter during the decoding step.",3.3 Decoding,[0],[0]
"Here, before picking the next 5 most likely characters during beam search, we remove all characters that would lead to a string not covered by the lexicon.",3.3 Decoding,[0],[0]
This is again intended to reduce the occurrence of nonsensical outputs.,3.3 Decoding,[0],[0]
"For the lexicon, we use all word forms from CELEX (cf. Sec. 2) plus the target word forms from the training set.3",3.3 Decoding,[0],[0]
"In our base architecture, we assume that we can decode from a single vector encoding of the input sequence.",3.4 Attention,[0],[0]
"This is a strong assumption, especially with long input sequences.",3.4 Attention,[0],[0]
Attention mechanisms give us more flexibility.,3.4 Attention,[0],[0]
"The idea is that instead of encoding the entire input sequence into a fixedlength vector, we allow the decoder to “attend” to different parts of the input character sequence at each time step of the output generation.",3.4 Attention,[0],[0]
"Importantly, we let the model learn what to attend to based on the input sequence and what it has produced so far.
",3.4 Attention,[0],[0]
Our implementation is identical to the decoder with soft attention described by Xu et al. (2015).,3.4 Attention,[0],[0]
"If a = (a1, ..., an) is the encoder’s output and ht is the decoder’s hidden state at timestep t, we first calculate a context vector ẑt as a weighted combination of the output vectors ai:
ẑt =
n∑
i=1
αiai (1)
3We observe that due to this filtering, we cannot reach 2.25% of the targets in our test set, most of which are Latin word forms.
",3.4 Attention,[0],[0]
"The weights αi are derived by feeding the encoder’s output and the decoder’s hidden state from the previous timestep into a multilayer perceptron, called the attention model (fatt):
α = softmax(fatt(a, ht−1))",3.4 Attention,[0],[0]
"(2)
We then modify the decoder by conditioning its internal states not only on the previous hidden state ht−1 and the previously predicted output character yt−1, but also on the context vector ẑt:
it = σ(Wi[ht−1, yt−1, ẑt]",3.4 Attention,[0],[0]
"+ bi)
ft = σ(Wf [ht−1, yt−1, ẑt]",3.4 Attention,[0],[0]
"+ bf )
ot = σ(Wo[ht−1, yt−1, ẑt]",3.4 Attention,[0],[0]
"+ bo)
gt = tanh(Wg[ht−1, yt−1, ẑt] + bg)
ct = ft ct−1",3.4 Attention,[0],[0]
"+ it gt ht = ot tanh(ct)
(3)
",3.4 Attention,[0],[0]
"In Eq. 3, we follow the traditional LSTM description consisting of input gate it, forget gate ft, output gate ot, cell state ct and hidden state ht, where W and b are trainable parameters.
",3.4 Attention,[0],[0]
"For all experiments including an attentional decoder, we use a bi-directional encoder, comprised of one LSTM layer that reads the input sequence normally and another LSTM layer that reads it backwards, and attend over the concatenated outputs of these two layers.
",3.4 Attention,[0],[0]
"While a precise alignment of input and output sequences is sometimes difficult, most of the time the sequences align in a sequential order, which can be exploited by an attentional component.",3.4 Attention,[0],[0]
"Finally, we introduce a variant of the base architecture, with or without beam search, that does multi-task learning (Caruana, 1993).",3.5 Multi-task learning,[0],[0]
"The multitask architecture only differs from the base architecture in having two classifier functions at the outer layer, one for each of our two tasks.",3.5 Multi-task learning,[0],[0]
Our auxiliary task is to predict a sequence of phonemes as the correct pronunciation of an input sequence of graphemes.,3.5 Multi-task learning,[0],[0]
"This choice is motivated by the relationship between phonology and orthography, in particular the observation that spelling variation often stems from phonological variation.
",3.5 Multi-task learning,[0],[0]
"We train our multi-task learning architecture by alternating between the two tasks, sampling one instance of the auxiliary task for each training sample of the main task.",3.5 Multi-task learning,[0],[0]
"We use the encoderdecoder to generate a corresponding output se-
quence, whether a modern word form or a pronunciation.",3.5 Multi-task learning,[0],[0]
"Doing so, we suffer a loss with respect to the true output sequence and update the model parameters.",3.5 Multi-task learning,[0],[0]
"The update for a sample from a specific task affects the parameters of corresponding classifier function, as well as all the parameters of the shared hidden layers.",3.5 Multi-task learning,[0],[0]
We used a single manuscript (B) for manually evaluating and setting the hyperparameters.,3.6 Hyperparameters,[0],[0]
This manuscript is left out of the averages reported below.,3.6 Hyperparameters,[0],[0]
"We believe that using a single manuscript for development, and using the same hyperparameters across all manuscripts, is more realistic, as we often do not have enough data in historical text normalization to reliably tune hyperparameters.
",3.6 Hyperparameters,[0],[0]
"For the final evaluation, we set the size of the embedding and the recurrent LSTM layers to 128, applied a dropout of 0.3 to the input of each recurrent layer, and trained the model on mini-batches with 50 samples each for a total of 50 epochs (in the multi-task learning setup, mini-batches contain 50 samples of each task, and epochs are counted by the size of the training set for the main task only).",3.6 Hyperparameters,[0],[0]
All these parameters were set on the B manuscript alone.,3.6 Hyperparameters,[0],[0]
"We implemented all of the models in Keras (Chollet, 2015).",3.7 Implementation,[0],[0]
Any parameters not explicitly described here were left at their default values in Keras v1.0.8.,3.7 Implementation,[0],[0]
"We split up each text into three parts, using 1,000 tokens each for a test set and a development set (that is not currently used), and the remainder of the text (between 2,000 and 11,000 tokens) for training.",4 Evaluation,[0],[0]
"We then train and evaluate on each of the 43 texts (excluding the B text that was used for hyper-parameter tuning) individually.
",4 Evaluation,[0],[0]
Baselines We compare our architectures to several competitive baselines.,4 Evaluation,[0],[0]
"Our first baseline is an averaged perceptron model trained to predict output character n-grams for each input character, after using Levenshtein alignment with generated segment distances (Wieling et al., 2009, Sec. 3.3) to align input and output characters.",4 Evaluation,[0],[0]
"Our second baseline uses the same alignment, but trains a
deep bi-LSTM sequential tagger, following Bollmann and Søgaard (2016).",4 Evaluation,[0],[0]
We evaluate this tagger using both standard and multi-task learning.,4 Evaluation,[0],[0]
"Finally, we compare our model to the rule-based and Levenshtein-based algorithms provided by the Norma tool (Bollmann, 2012).4",4 Evaluation,[0],[0]
We use word-level accuracy as our evaluation metric.,4.1 Word accuracy,[0],[0]
"While we also measure character-level metrics, minor differences on character level can cause large differences in downstream applications, so we believe that perfectly matching the output sequences is more useful.",4.1 Word accuracy,[0],[0]
"Average scores across all 43 texts are presented in Table 1 (see Appendix A for individual scores).
",4.1 Word accuracy,[0],[0]
We first see that almost all our encoder-decoder architectures perform significantly better than the four state-of-the-art baselines.,4.1 Word accuracy,[0],[0]
"All our architectures perform better than Norma and the averaged perceptron, and all the MTL architectures outperform Bollmann and Søgaard (2016).
",4.1 Word accuracy,[0],[0]
"We also see that beam search, filtering, and attention lead to cumulative gains in the context of the single-task architecture – with the best architecture outperforming the state-of-the-art by almost 3% in absolute terms.
",4.1 Word accuracy,[0],[0]
"For our multi-task architecture, we also observe gains when we add beam search and filtering, but
4https://github.com/comphist/norma
importantly, adding attention does not help.",4.1 Word accuracy,[0],[0]
"In fact, attention hurts the performance of our multitask architecture quite significantly.",4.1 Word accuracy,[0],[0]
"Also note that the multi-task architecture without attention performs on-par with the single-task architecture with attention.
",4.1 Word accuracy,[0],[0]
"We hypothesize that the reason for this pattern, which is not only observed in the average scores in Table 1, but also quite consistent across the individual results in Appendix A, is that our multi-task learning already learns how to focus attention.
",4.1 Word accuracy,[0],[0]
"This is the hypothesis that we will try to validate in Sec. 5: That multi-task learning can induce strategies for focusing attention comparable to attention strategies for recurrent neural networks.
",4.1 Word accuracy,[0],[0]
Sample predictions A small selection of predictions from our models is shown in Table 2.,4.1 Word accuracy,[0],[0]
"They serve to illustrate the effects of the various settings; e.g., the base model with greedy search tends to produce more nonsense words (ters, ünsget) than the others.",4.1 Word accuracy,[0],[0]
"Using a lexical filter helps the most in this regard: the base model with filtering correctly normalizes ergieng to erging ‘(he) fared’, while decoding without a filter produces the non-word erbiggen.",4.1 Word accuracy,[0],[0]
"Even for herczenlichen (modern herzlichen ‘heartfelt’), where no model finds the correct target form, only the model with filtering produces a somewhat reasonable alternative (herzgeliebtes ‘heartily loved’).
",4.1 Word accuracy,[0],[0]
"In some cases (such as gewarnet ‘warned’),
only the models with attention or multi-task learning produce the correct normalization, but even when they are wrong, they often agree on the prediction (e.g. dicke, herzel).",4.1 Word accuracy,[0],[0]
We will investigate this property further in Sec. 5.,4.1 Word accuracy,[0],[0]
"To gain further insights into our model, we created t-SNE projections (Maaten and Hinton, 2008) of vector representations learned on the M4 text.
",4.2 Learned vector representations,[0],[0]
Fig. 2 shows the learned character embeddings.,4.2 Learned vector representations,[0],[0]
"In the representations from the base model (Fig. 2a), characters that are often normalized to the same target character are indeed grouped closely together: e.g., historical <v> and <u> (and, to a smaller extent, <f>) are often used interchangeably in the M4 text.",4.2 Learned vector representations,[0],[0]
Note the wide separation of <n>,4.2 Learned vector representations,[0],[0]
"and <m>, which is a feature of M4 that does not hold true for all of the texts, as these do not always display a clear distinction between nasals.",4.2 Learned vector representations,[0],[0]
"On the other hand, the MTL model shows a better generalization of the training data (Fig. 2b): here, <u> is grouped closer to other vowel characters and far away from <v>/<f>.",4.2 Learned vector representations,[0],[0]
"Also, <n> and <m> are now in close proximity.
",4.2 Learned vector representations,[0],[0]
We can also visualize the internal word representations that are produced by the encoder (Fig. 3).,4.2 Learned vector representations,[0],[0]
"Here, we chose words that demonstrate the interchangeable use of <u> and <v>.",4.2 Learned vector representations,[0],[0]
"Historical vnd, vns, vmb become modern und, uns, um, changing the <v> to <u>.",4.2 Learned vector representations,[0],[0]
"However, the representation of vmb learned by the base model is closer to forms like von, vor, uor, all starting with <v> in the target normalization.",4.2 Learned vector representations,[0],[0]
"In the MTL model, however, these examples are indeed clustered together.",4.2 Learned vector representations,[0],[0]
Table 1 shows that models which employ either an attention mechanism or multi-task learning obtain similar improvements in word accuracy.,5 Analysis: Multi-task learning helps focus attention,[0],[0]
"However, we observe a decline in word accuracy for models that combine multi-task learning with attention.
",5 Analysis: Multi-task learning helps focus attention,[0],[0]
"A possible interpretation of this counterintuitive pattern might be that attention and MTL, to some degree, learn similar functions of the input data, a conjecture by Caruana (1998).",5 Analysis: Multi-task learning helps focus attention,[0],[0]
We put this hypothesis to the test by closely investigating properties of the individual models below.,5 Analysis: Multi-task learning helps focus attention,[0],[0]
"First, we are interested in the weight parameters of the final layer that transforms the decoder output to class probabilities.",5.1 Model parameters,[0],[0]
"We consider these parameters for our standard encoder-decoder model and compare them to the weights that are learned by the attention and multi-task models, respectively.5
Note that hidden layer parameters are not necessarily comparable across models, but with a fixed seed, differences in parameters over a reference model may be (and are, in our case).",5.1 Model parameters,[0],[0]
"With a fixed seed, and iterating over data points in the same order, it is conceivable the two non-baselines end up in roughly the same alternative local optimum (or at least take comparable routes).
",5.1 Model parameters,[0],[0]
"We observe that the weight differences between the standard and the attention model correlate with the differences between the standard and multitask model by a Pearson’s r of 0.346, averaged across datasets, with a standard deviation of 0.315; on individual datasets, correlation coefficient is as
5For the multi-task models, this analysis disregards those dimensions that do not correspond to classes in the main task.
high as 96.",5.1 Model parameters,[0],[0]
Figure 4 illustrates these highly parallel weight changes for the different models when trained on the N4 dataset.,5.1 Model parameters,[0],[0]
"Next, we compare the effect that employing either an attention mechanism or multi-task learning has on the actual output of our system.",5.2 Final output,[0],[0]
"We find that out of the 210.9 word errors that the base model produces on average across all test sets (comprising 1,000 tokens each), attention resolves 47.7, while multi-task learning resolves an average of 45.4 errors.",5.2 Final output,[0],[0]
"Crucially, the overlap of errors that are resolved by both the attention and the MTL model amounts to 27.7 on average.
",5.2 Final output,[0],[0]
"Attention and multi-task also introduce new errors compared to the base model (26.6 and 29.5 per test set, respectively), and again we can observe a relatively high agreement of the models (11.8 word errors are introduced by both models).
",5.2 Final output,[0],[0]
"Finally, the attention and multi-task models display a word-level agreement of κ=0.834 (Cohen’s kappa), while either of these models is less strongly correlated with the base model (κ=0.817 for attention and κ=0.814 for multi-task learning).",5.2 Final output,[0],[0]
Our last analysis regards the saliency of the input timesteps with respect to the predictions of our models.,5.3 Saliency analysis,[0],[0]
We follow Li et al. (2016) in calculating first-derivative saliency for given input/output pairs and compare the scores from the different models.,5.3 Saliency analysis,[0],[0]
"The higher the saliency of an input timestep, the more important it is in determining the model’s prediction at a given output timestep.",5.3 Saliency analysis,[0],[0]
"Therefore, if two models produce similar saliency
matrices for a given input/output pair, they have learned to focus on similar parts of the input during the prediction.",5.3 Saliency analysis,[0],[0]
"Our hypothesis is that the attentional and the multi-task learning model should be more similar in terms of saliency scores than either of them compared to the base model.
",5.3 Saliency analysis,[0],[0]
Figure 5 shows a plot of the saliency matrices generated from the word pair czeychen – zeichen ‘sign’.,5.3 Saliency analysis,[0],[0]
"Here, the scores for the attentional and the MTL model indeed correlate by ρ = 0.615, while those for the base model do not correlate with either of them.",5.3 Saliency analysis,[0],[0]
"A systematic analysis across 19,000 word pairs (where all models agree on the output) shows that this effect only holds for longer input sequences (≥ 7 characters), with a mean ρ = 0.303 (±0.177) for attentional vs. MTL model, while the base model correlates with either of them by ρ < 0.21.",5.3 Saliency analysis,[0],[0]
"Many traditional approaches to spelling normalization of historical texts use edit distances or some form of character-level rewrite rules, handcrafted (Baron and Rayson, 2008) or learned automatically (Bollmann, 2013; Porta et al., 2013).
",6 Related Work,[0],[0]
"A more recent approach is based on characterbased statistical machine translation applied to historical text (Pettersson et al., 2013; SánchezMartínez et al., 2013; Scherrer and Erjavec, 2013; Ljubešić et al., 2016) or dialectal data (Scherrer and Ljubešić, 2016).",6 Related Work,[0],[0]
"This is conceptually very similar to our approach, except that we substitute the classical SMT algorithms for neural networks.",6 Related Work,[0],[0]
"Indeed, our models can be seen as a form of character-based neural MT (Cho et al., 2014).
",6 Related Work,[0],[0]
"Neural networks have rarely been applied to
historical spelling normalization so far.",6 Related Work,[0],[0]
Azawi et al. (2013) normalize old Bible text using bidirectional LSTMs with a layer that performs alignment between input and output wordforms.,6 Related Work,[0],[0]
"Bollmann and Søgaard (2016) also use bi-LSTMs to frame spelling normalization as a characterbased sequence labelling task, performing character alignment as a preprocessing step.
",6 Related Work,[0],[0]
"Multi-task learning was shown to be effective for a variety of NLP tasks, such as POS tagging, chunking, named entity recognition (Collobert et al., 2011) or sentence compression (Klerke et al., 2016).",6 Related Work,[0],[0]
"It has also been used in encoderdecoder architectures, typically for machine translation (Dong et al., 2015; Luong et al., 2016), though so far not with attentional decoders.",6 Related Work,[0],[0]
"We presented an approach to historical spelling normalization using neural networks with an encoder-decoder architecture, and showed that it consistently outperforms several existing baselines.",7 Conclusion and Future Work,[0],[0]
"Encouragingly, our work proves to be fully competitive with the sequence-labeling approach by Bollmann and Søgaard (2016), without requiring a prior character alignment.
",7 Conclusion and Future Work,[0],[0]
"Specifically, we demonstrated the aptitude of multi-task learning to mitigate the shortage of training data for the named task.",7 Conclusion and Future Work,[0],[0]
We included a multifaceted analysis of the effects that MTL introduces to our models and the resemblance that it bears to attention mechanisms.,7 Conclusion and Future Work,[0],[0]
"We believe that this analysis is a valuable contribution to the understanding of MTL approaches also beyond spelling normalization, and we are confident that our observations will stimulate further research into the relationship between MTL and attention.
",7 Conclusion and Future Work,[0],[0]
"Finally, many improvements to the presented approach are conceivable, most notably introducing some form of token context to the model.",7 Conclusion and Future Work,[0],[0]
"Currently, we only consider word forms in isolation, which is problematic for ambiguous cases (such as jn, which can normalize to in ‘in’ or ihn ‘him’) and conceivably makes the task harder for others.",7 Conclusion and Future Work,[0],[0]
Reranking the predictions with a language model could be one possible way to improve on this.,7 Conclusion and Future Work,[0],[0]
Ljubešić,7 Conclusion and Future Work,[0],[0]
"et al. (2016), for example, experiment with segment-based normalization, using a character-based SMT model with character input derived from segments (essentially, token ngrams) instead of single tokens, which also intro-
duces context.",7 Conclusion and Future Work,[0],[0]
"Such an approach could also deal with the issue of tokenization differences between the historical and the modern text, which is another challenge often found in datasets of historical text.",7 Conclusion and Future Work,[0],[0]
"Marcel Bollmann was supported by Deutsche Forschungsgemeinschaft (DFG), Grant DI 1558/4.",Acknowledgments,[0],[0]
This research is further supported by ERC Starting Grant LOWLANDS,Acknowledgments,[0],[0]
"No. 313695, as well as by Trygfonden.",Acknowledgments,[0],[0]
"For interested parties, we provide our full evaluation results for each single text in our dataset.",A Supplementary Material,[0],[0]
"Table 3 shows token counts, a rough classification of each text’s dialectal region, and the results for the baseline methods.",A Supplementary Material,[0],[0]
Table 4 presents the full results for our encoder-decoder models.,A Supplementary Material,[0],[0]
Automated processing of historical texts often relies on pre-normalization to modern word forms.,abstractText,[0],[0]
"Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task.",abstractText,[0],[0]
"We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-theart by an absolute 2% increase in performance.",abstractText,[0],[0]
We analyze the induced models across 44 different texts from Early New High German.,abstractText,[0],[0]
"Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms.",abstractText,[0],[0]
"This, we believe, is an important step toward understanding how MTL works.",abstractText,[0],[0]
Learning attention for historical text normalization by learning to pronounce,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 313–322 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Today, machine learning is centered around algorithms that can be trained on available taskspecific labeled and unlabeled training samples.",1 Introduction,[0],[0]
"Although learning paradigms like Transfer Learning (Pan and Yang, 2010) attempt to incorporate
∗equal contribution †Main work done during internship at Accenture Technol-
ogy Labs
knowledge from one task into another, these techniques are limited in scalability and are specific to the task at hand.",1 Introduction,[0],[0]
"On the other hand, humans have the intrinsic ability to elicit required past knowledge from the world on demand and infuse it with newly learned concepts to solve problems.
",1 Introduction,[0],[0]
"The question that we address in this paper is the following: Is it possible to develop learning models that can be trained in a way that it is able to infuse a general body of world knowledge for prediction apart from learning based on training data?
",1 Introduction,[0],[0]
"By world knowledge, we mean structured general purpose knowledge that need not be domain specific.",1 Introduction,[0],[0]
"Knowledge Graphs (Nickel et al., 2016a) are a popular source of such structured world knowledge.",1 Introduction,[0],[0]
"Knowledge Graphs represent information in the form of fact triplets, consisting of a subject entity, relation and object entity (example: <Italy, capital, Rome>).",1 Introduction,[0],[0]
The entities represent the nodes of the graph and their relations act as edges.,1 Introduction,[0],[0]
"A fact triple (subject entity, relation, object relation) is represented as (h, r, t).",1 Introduction,[0],[0]
"Practical knowledge bases congregate information from secondary databases or extract facts from unstructured text using various statistical learning mechanisms, examples of such systems are NELL (Mitchell et al., 2015) and DeepDive (Niu
313
et al., 2012).",1 Introduction,[0],[0]
"There are human created knowledge bases as well, like Freebase (FB15k) (Bollacker et al., 2008) and WordNet (Miller et al., 1990).",1 Introduction,[0],[0]
"The knowledge present in these knowledge bases includes common knowledge and partially covers common-sense knowledge and domain knowledge (Song and Roth, 2017).",1 Introduction,[0],[0]
"Knowledge Graphs and Knowledge Bases are conceptually equivalent for our purpose and we will use the name interchangeably in this paper.
",1 Introduction,[0],[0]
We illustrate the significance of world knowledge using a few examples.,1 Introduction,[0],[0]
"For the example of a Natural Language Inference (NLI) problem (MacCartney, 2009), consider the two following statements, A: The couple is walking on the sea shore and B: The man and woman are wide awake.",1 Introduction,[0],[0]
"Here, for a learning model to infer B from A, it should have access to the common knowledge that “The man and woman and The couple means the same” since this information may not be specific for a particular inference.",1 Introduction,[0],[0]
"Further, it is not possible for a model to learn all such correlations from just the labeled training data available for the task.
",1 Introduction,[0],[0]
"Consider another example of classifying the news snippet, Donald Trump offered his condolences towards the hurricane victims and their families in Texas.",1 Introduction,[0],[0]
"We cannot classify it as a political news unless we know the facts <Donald Trump, president, United States> and <Texas, state, United States>.",1 Introduction,[0],[0]
"We posit that machine learning models, apart from training them on data with the ground-truth can also be trained to fetch relevant information from structured knowledge bases in order to enhance their performance.
",1 Introduction,[0],[0]
"In this work, we propose a deep learning model that can extract relevant support facts on demand from a knowledge base (Mitchell et al., 2015) and incorporate it in the feature space along with the features learned from the training data (shown in Figure 1).",1 Introduction,[0],[0]
"This is a challenging task, as knowledge bases typically have millions of fact triples.",1 Introduction,[0],[0]
Our proposed model involves a deep learning mechanism to jointly model this look-up scheme along with the task specific training of the model.,1 Introduction,[0],[0]
The look-up mechanism and model is generic enough so that it can be augmented to any task specific learning model to boost the learning performance.,1 Introduction,[0],[0]
"In this paper, we have established superior performance of the proposed KG-augmented models
over vanilla model on text classification and natural language inference.
",1 Introduction,[0],[0]
"Although there is a plethora of work on knowledge graph representation (Nickel et al., 2016a)",1 Introduction,[0],[0]
"(Mitchell et al., 2015) (Niu et al., 2012) from natural language text, no attempt to augment learning models with knowledge graph information have been done.",1 Introduction,[0],[0]
To the best of our knowledge this is the first attempt to incorporate world knowledge from a knowledge base for learning models.,1 Introduction,[0],[0]
Knowledge Graph entities/relations need to be encoded into a numerical representation for processing.,2 Knowledge Graph Representations,[0],[0]
"Before describing the model, we provide a brief overview of graph encoding techniques.",2 Knowledge Graph Representations,[0],[0]
"Various KG embedding techniques can be classified at a high level into: Structure-based embeddings and Semantically-enriched embeddings.
",2 Knowledge Graph Representations,[0],[0]
"Structure-based embeddings: TransE (Bordes et al., 2013) is the introductory work on knowledge graph representation, which translated subject entity to object entity using one-dimensional relation vector (h + r = t).",2 Knowledge Graph Representations,[0],[0]
"Variants of the TransE (Bordes et al., 2013) model uses translation of the entity vectors over relation specific subspaces.",2 Knowledge Graph Representations,[0],[0]
"TransH (Wang et al., 2014b) introduced the relation-specific hyperplane to translate the entities.",2 Knowledge Graph Representations,[0],[0]
Similar work utilizing only the structure of the graph include ManifoldE,2 Knowledge Graph Representations,[0],[0]
"(Xiao et al., 2015b), TransG (Xiao et al., 2015a), TransD",2 Knowledge Graph Representations,[0],[0]
"(Ji et al., 2015), TransM (Fan et al., 2014), HolE (Nickel et al., 2016b) and ProjE (Shi and Weninger, 2017).
",2 Knowledge Graph Representations,[0],[0]
Semantically-enriched embeddings: These embedding techniques learn to represent entities/relations of the KG along with its semantic information.,2 Knowledge Graph Representations,[0],[0]
"Neural Tensor Network(NTN) (Socher et al., 2013) was the pioneering work in this field which initialized entity vectors with the average word embeddings followed by tensor-based operations.",2 Knowledge Graph Representations,[0],[0]
"Recent works involving this idea are “Joint Alignment” (Zhong et al., 2015) and SSP (Xiao et al., 2017).",2 Knowledge Graph Representations,[0],[0]
"DKRL (Xie et al., 2016) is a KG representation technique which also takes into account the descriptive nature of text keeping the simple structure of TransE model.",2 Knowledge Graph Representations,[0],[0]
Pretrained word2vec,2 Knowledge Graph Representations,[0],[0]
"(Mikolov et al., 2013) is used to form the entity representation by passing through a Convolutional Neural Network (CNN) (Kim, 2014) architecture constraining the relationships to hold.
",2 Knowledge Graph Representations,[0],[0]
"In our experiments we have used the DKRL (Xie et al., 2016) encoding scheme as it emphasizes on the semantic description of the text.",2 Knowledge Graph Representations,[0],[0]
"Moreover, DKRL fundamentally uses TransE (Bordes et al., 2013) method for encoding structural information.",2 Knowledge Graph Representations,[0],[0]
"Therefore, we can retrieve relevant entities & relation and obtain the complete the fact using t = h+",2 Knowledge Graph Representations,[0],[0]
r.,2 Knowledge Graph Representations,[0],[0]
"This reduces the complexity of fact retrieval as the number of entities/relations is much less compared to the number of facts, thus making the retrieval process faster.",2 Knowledge Graph Representations,[0],[0]
"Conventional supervised learning models with parameters Θ, given training data x and label y, tries to maximize the following function
max Θ
P (y|x,Θ)
The optimized parameters Θ are given as,
Θ = argmax Θ
logP (y|x,Θ)
",3 The Proposed Model,[0],[0]
"In this work, we propose to augment the supervised learning process by incorporation of world knowledge features xw.",3 The Proposed Model,[0],[0]
"The world knowledge features are retrieved using the data x, using a separate model where, xw = F (x,Θ(2)).",3 The Proposed Model,[0],[0]
"Thus, our modified objective function can be expressed as
max Θ
P (y|x, xw,Θ(1))
where, Θ = {Θ(1),Θ(2)}.",3 The Proposed Model,[0],[0]
"The optimized parameters can be obtained using the equation
Θ = argmax Θ
logP (y|x, F (x,Θ(2)),Θ(1))
",3 The Proposed Model,[0],[0]
The subsequent sections focus on the formulation of the function F which is responsible for fact triple retrieval using the data sample x.,3 The Proposed Model,[0],[0]
"Here it is important to note that, we are not assuming any structural form for P based on F .",3 The Proposed Model,[0],[0]
"So the method is generic and applicable to augment any supervised learning setting with any form for P , only constraint being P should be such that the error gradient can be computed with respect to F .",3 The Proposed Model,[0],[0]
"In the experiments we have used softmax using the LSTM (Greff et al., 2015) encodings of the input as the form for P .",3 The Proposed Model,[0],[0]
"As for F , we use soft attention (Luong et al., 2015; Bahdanau et al., 2014) using the LSTM encodings of the input and appropriate representations of the fact(s).",3 The Proposed Model,[0],[0]
"Based on the
representation used for the facts, we propose two models (a) Vanilla Model (b) Convolution-based entity/relation cluster representation, for fact retrieval in the subsequent sections.",3 The Proposed Model,[0],[0]
"The entities and relationships of KG are encoded using DKRL, explained earlier.",3.1 Vanilla Model,[0],[0]
Let ei ∈ Rm stand for the encoding of the entity i and rj ∈ Rm stands for jth relationship in the KG.,3.1 Vanilla Model,[0],[0]
"The input text in the form of concatenated word vectors, x = (x1, x2, . . .",3.1 Vanilla Model,[0],[0]
", xT ) is first encoded using an LSTM (Greff et al., 2015) module as follows,
ht = f(xt, ht−1)
and
o = 1
T
T∑
t=1
ht,
ht ∈",3.1 Vanilla Model,[0],[0]
"Rn is the hidden state of the LSTM at time t, f is a non-linear function and T is the sequence length.",3.1 Vanilla Model,[0],[0]
"Then a context vector is formed from o as follows,
C = ReLU(oTW ),
where, W ∈ Rn×m represent the weight parameters.",3.1 Vanilla Model,[0],[0]
"The same procedure is duplicated with separate LSTMs to form two seperate context vectors, one for entity retrieval (CE) and one for relationship retrieval (CR).
",3.1 Vanilla Model,[0],[0]
"As the number of fact triples in a KG is in the order of millions in the vanilla model, we resort to generating attention over the entity and relation space separately.",3.1 Vanilla Model,[0],[0]
The fact is then formed using the retrieved entity and relation.,3.1 Vanilla Model,[0],[0]
"The attention for the entity, ei using entity context vector is given by
αei = exp(CTEei)",3.1 Vanilla Model,[0],[0]
|E|∑ j=0,3.1 Vanilla Model,[0],[0]
"exp(CTEej)
where |E| is the number of entities in the KG.",3.1 Vanilla Model,[0],[0]
"Similarly the attention for a relation vector ri is computed as
αri = exp(CTRri) |R|∑ j=0 exp(CTRrj)
where |R| is the number of relations in the KG.",3.1 Vanilla Model,[0],[0]
"The final entity and relation vector retrieval is computed by the weighted sum with the attention
values of individual retrieved entity/relation vectors.
",3.1 Vanilla Model,[0],[0]
"e =
|E|∑
i=0
αeiei r =
|R|∑
i=0
αriri
Figure 2 shows the schematic diagram for entity/relation retrieval.",3.1 Vanilla Model,[0],[0]
"After the final entity and relation vectors are computed, we look forward to completion of the fact triple.",3.1 Vanilla Model,[0],[0]
The KG embedding technique used for the experiment is DKRL which inherently uses the TransE model assumption (h+r ≈ t).,3.1 Vanilla Model,[0],[0]
"Therefore, using the subject entity and relation we form the object entity as t = e+r.",3.1 Vanilla Model,[0],[0]
Thus the fact triplet retrieved is F =,3.1 Vanilla Model,[0],[0]
"[e, r, e + r], where F ∈ R3m. This retrieved fact information is concatenated along with the context vector (C) of input x obtained using LSTM module.",3.1 Vanilla Model,[0],[0]
"The final classification label y is computed as follows,
F ′ = ReLU(FTV )
y = softmax([F ′",3.1 Vanilla Model,[0],[0]
": C]TU) where, V ∈ R3m×u and U ∈ R2u×u are model parameters to be learned.",3.1 Vanilla Model,[0],[0]
y is used to compute the cross entropy loss.,3.1 Vanilla Model,[0],[0]
"We minimize this loss averaged across the training samples, to learn the various model parameters using stochastic gradient descent (Bottou, 2012).",3.1 Vanilla Model,[0],[0]
"The final prediction y, now includes information from both dataset specific samples and world knowledge to aid in en-
hanced performance.",3.1 Vanilla Model,[0],[0]
While jointly training the attention mechanism tunes itself to retrieve relevant facts that are required to do the final classification.,3.1 Vanilla Model,[0],[0]
The vanilla model attends over the entire entity/relation space which is not a good approach as we observe that the gradient for each attention value gets saturated easily.,3.2 Pre-training KG Retrieval,[0],[0]
"While training the classification and retrieval module together, the model tends to ignore the KG part and gradient propagates only through the classification module.",3.2 Pre-training KG Retrieval,[0],[0]
"This is expected to an extent as the most pertinent information for the task at hand comes from the training samples, only background aiding information comes from KG.",3.2 Pre-training KG Retrieval,[0],[0]
"After few epochs of training, the KG retrieved fact always converged to a fixed vector.",3.2 Pre-training KG Retrieval,[0],[0]
"To overcome this problem, we attempted pretraining KG retrieval part separately.",3.2 Pre-training KG Retrieval,[0],[0]
"A pre-trained KG model is used to retrieve the facts and then concatenate with the classification module, while we allow error to be propagate through the pretrained model, at the time of joint training.",3.2 Pre-training KG Retrieval,[0],[0]
We infer that KG doesn’t return noise and has essential information for the task as the separate KG part alone shows significant performance (59% for News20 & 66% for SNLI).,3.2 Pre-training KG Retrieval,[0],[0]
Figure 3 depicts the entire training scheme.,3.2 Pre-training KG Retrieval,[0],[0]
"This procedure solved the issue of gradient saturation in the KG retrieval part
at the time of joint training.",3.2 Pre-training KG Retrieval,[0],[0]
"However, the key problem of attention mechanism having to cover a large span of entities/relation, remained.",3.2 Pre-training KG Retrieval,[0],[0]
"In this section, we propose a mechanism to reduce the large number of entities/relationships over which attention has to be generated in the knowledge graph.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We propose to reduce the attention space by learning the representation of similar entity/relation vectors and attending over them.
",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"In order to cluster similar entity/relation vectors, we used k-means clustering (Bishop, 2006) and formed l clusters with equal number of entity/relation vectors in each cluster.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Each of the clusters were then encoded using convolutional filters.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"The output of the k-means clustering is a sequence of entity/relation vectors {eT1 , eT2 , · · · , eTq }, where ei ∈ Rm.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"For each cluster these vectors were stacked to form E as the 2- D input to the CNN encoder, where E ∈ Rm×q.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"During experimentation for finding a suitable fil-
ter shape, it was observed that using 2-D filters the model failed to converge at all.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Therefore, we inferred that the latent representation of two different indices in the vector ei, should not be tampered using convolution.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We then resorted to use 1-D convolution filters which slide along only the columns of E , as shown Figure 4.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
The stride length along y-axis is the window length k.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"The output of the convolution layer is expressed as,
E ′(i, j) = W T",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"[ei,j , ei+1,j , . . .",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
", ei+k−1,j ]T
where, E ′(i, j) is the (i, j)th element of the output matrix E ′ and W ∈",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Rk is the convolution weight filter.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"A pooling layer followed the convolution layer in order to reduce the parameter space, we used 1-D window only along the y-axis similar to the convolutional kernel mentioned above.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We used a two layered convolution network with the stride length k & max-pool windows n is adjusted to obtain output Ei ∈ Rm, where i is the cluster index.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Similar procedure of clustering followed by the encoding of the cluster entities is done for relations as well.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Thus both the entity and relation space were reduced to contain fewer elements, one each for each cluster.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"After the compact entity space E and relation space R is formed, we followed the same steps as earlier for forming the attention, but now the training was more effective as the gradient was propagating effectively and was not choked by the large space.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"As the convolution architecture is also simultaneously trained, attention mechanism was not burdened as before, to learn over the large space of entities and relations.
",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Another point that needs to be mentioned here is regarding ranking/ordering items in the clusters, we have done experiments to verify the ordering does not affect the final result.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We have verified this by randomly shuffling the entities/relationships in every clusters and the ac-
curacy output remained within an error bound of ±0.5%.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"In various permutations, the representations learned by the convolution operator for clusters varies, but it does not affect the overall results.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Regarding the interpretation of what convolution operator learns, the operator is applied along each dimension of the entity/relationship vector, to learn a representation of the clusters.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"This representation includes information from relevant entities in the cluster, as the relevant entities varies across tasks, the representation learned using convolution also adapts accordingly.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"It is analogous to learning relevant features from an image, in our case the convolution layer learns the features focusing on relevant entities/relations in a cluster pertaining to the task.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Our experiments were designed to analyze whether a deep learning model is being improved when it has access to KG facts from a relevant source.,4 Experiments and Evaluations,[0],[0]
"The selection of knowledge graph has to be pertinent to the task at hand, as currently there is no single knowledge base that contains multiple kinds of information and can cater to all tasks.",4 Experiments and Evaluations,[0],[0]
We illustrate with results that the performance of a deep learning model improves when it has access to relevant facts.,4 Experiments and Evaluations,[0],[0]
"We also illustrate that as the model learns faster with access to knowledge bases, we can train deep learning models with substantially less training data, without compromising on the accuracy.",4 Experiments and Evaluations,[0],[0]
"In the subsequent section we briefly describe the datasets and associated Knowledge Bases used.
Datasets and Relevant Knowledge Graphs
In our experiments, we have mainly used the popular text classification dataset 20Newsgroups (Lichman, 2013) and the Natural Language Inference dataset, Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015).",4 Experiments and Evaluations,[0],[0]
"We have also done experiments on DBPedia ontology classification dataset1, with a very strong baseline.",4 Experiments and Evaluations,[0],[0]
"These datasets are chosen as they share domain knowledge with two most popular knowledge bases, Freebase (FB15k) (Bollacker et al., 2008) and WordNet (WN18) (Bordes et al., 2013).",4 Experiments and Evaluations,[0],[0]
"The training and test size of the datasets are mentioned in Table 1.
1http://wiki.dbpedia.org/ services-resources/dbpedia-data-set-2014
Freebase (FB15k) (Bollacker et al., 2008) contains facts about people, places and things (contains 14904 entities, 1345 relations & 4.9M fact triples), which is useful for text classification in 20Newsgroups (Lichman, 2013) dataset.",4 Experiments and Evaluations,[0],[0]
"On the other hand, WordNet (WN18) (Bordes et al., 2013) (has 40943 entities, 18 relations & 1.5M fact triples) contains facts about common day-to-day things (example: furniture includes bed), which can help in inference tasks like SNLI.",4 Experiments and Evaluations,[0],[0]
"Both the knowledge bases are directed graphs, due to fewer number of relations WN18 the entities are more likely to be connected using the same type of relations.",4 Experiments and Evaluations,[0],[0]
For experiments relating to both the datasets 20Newsgroups & SNLI we used the standard LSTM as the classification module.,4 Experiments and Evaluations,[0],[0]
"As iterated earlier, our KG based fact retrieval is independent of the base model used.",4 Experiments and Evaluations,[0],[0]
We show improvement in performance using the proposed models by KG fact retrieval.,4 Experiments and Evaluations,[0],[0]
We use classification accuracy of the test set as our evaluation metric.,4 Experiments and Evaluations,[0],[0]
All experiments were carried on a Dell Precision Tower 7910 server with Quadro M5000 GPU with 8 GB of memory.,4.1 Experimental Setup,[0],[0]
"The models were trained using the Adam’s Optimizer (Kingma and Ba, 2014) in a stochastic gradient descent (Bottou, 2012) fashion.",4.1 Experimental Setup,[0],[0]
"The models were implemented using TensorFlow (Abadi et al., 2015).",4.1 Experimental Setup,[0],[0]
The relevant hyperparameters are listed in Table 2.,4.1 Experimental Setup,[0],[0]
"The word embeddings for the experiments were obtained using the pre-trained GloVe (Pennington et al., 2014)2 vectors.",4.1 Experimental Setup,[0],[0]
"For words missing in the pre-trained vectors, the local GloVe vectors which was trained on the corresponding dataset was used.",4.1 Experimental Setup,[0],[0]
Table 3 shows the results of test accuracy of the various methods proposed on the datasets News20 & SNLI.,4.2 Results & Discussion,[0],[0]
"We observe that incorporation of KG facts using the basic vanilla model improves the performance slightly, as the retrieval model was
2http://nlp.stanford.edu/data/glove.840B.300d.zip
not getting trained effectively.",4.2 Results & Discussion,[0],[0]
The convolutionbased model shows significant improvement over the normal LSTM classification.,4.2 Results & Discussion,[0],[0]
While tuning the parameters of the convolution for clustered entities/relations it was observed that smaller stride length and longer max-pool window improved performance.,4.2 Results & Discussion,[0],[0]
"For News20 dataset we show an improvement of almost 3% and for SNLI an improvement of almost 5%.
",4.2 Results & Discussion,[0],[0]
The work is motivated more from the perspective of whether incorporation of world knowledge will improve any deep learning model rather than beating the state-of-the-art performance.,4.2 Results & Discussion,[0],[0]
"Although LSTM is used to encode the input for the model as well as the retrieval vector, as mentioned earlier, these two modules need not be same.",4.2 Results & Discussion,[0],[0]
For encoding the input any complex state-of-the-art model can be used.,4.2 Results & Discussion,[0],[0]
LSTM has also been used to generate the retrieval vector.,4.2 Results & Discussion,[0],[0]
"For DBPedia ontology classification dataset, we have used a strong baseline of 98.6%, and after augmenting it with KG (Freebase) using convolution based model we saw an improvement of ∼0.2%.",4.2 Results & Discussion,[0],[0]
"As the baseline is stronger, the improvement quantum has decreased.",4.2 Results & Discussion,[0],[0]
This is quite intuitive as complex models are selfsufficient in learning from the data by itself and therefore the room available for further improvement is relatively less.,4.2 Results & Discussion,[0],[0]
"The improvement as observed in the experiments is significant in weaker learning models, however it is also capable of improving stronger baselines as is evident from the results of DBPedia dataset.",4.2 Results & Discussion,[0],[0]
"We hypothesized that as Knowledge Graph is feeding more information to the model, we can achieve better performance with less training data.
",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
To verify this we have performed experiments on varying dataset fractions for 20Newsgroups dataset as shown in Figure 5.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"From the plot, we observe that KG augmented LSTM with 70% data outperforms the baseline model with full dataset support, thereby reducing the dependency on labeled data by 30%.
",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
We also designed an experiment to compare the accuracy of the baseline model trained on full training data and compared it with the accuracy of the KG augmented model trained with just 70% of the training data for 20Newsgroups and SNLI datasets.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
The accuracy and training loss plots across training epochs is given in Figure 6.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"Even with just 70% of the data, KG augmented model is able to achieve better accuracy compared to the vanilla LSTM model trained on the full data.",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
This clearly indicates that relevant information pertaining to the task is retrieved from the knowledge graph and the training loss reduction is not due to lesser data only.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
Also note that training loss is substantially less for KG LSTM compared to normal LSTM when the dataset size is reduced.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"This result is very promising, to reduce the large labeled training data requirement of large deep learning
models, which is hard to come by.",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"The basic idea of infusing general world knowledge for learning tasks, especially for natural language processing, has not been attempted before.",5 Relevant Previous Work,[0],[0]
"For multi-label image classification, the use of KGs has been pursued recently by (Marino et al., 2016).",5 Relevant Previous Work,[0],[0]
"In this work, they first obtain labels of the input data (using a different model), use these labels to populate features from the KG and in turn use these features back for the final classification.",5 Relevant Previous Work,[0],[0]
"For NLP tasks the information needed may not necessarily depend on the final class, and we are directly using all the information available in the input for populating the relevant information from the knowledge graphs.",5 Relevant Previous Work,[0],[0]
"Our attempt is very different from Transfer Learning (Pan and Yang, 2010).
",5 Relevant Previous Work,[0],[0]
In Transfer Learning the focus is on training the model for one task and tuning the trained model to use it for another task.,5 Relevant Previous Work,[0],[0]
This is heavily dependent on the alignment between source task and destination task and transferred information is in the model.,5 Relevant Previous Work,[0],[0]
"In our case, general world knowledge is being infused into the learning model for any given task.",5 Relevant Previous Work,[0],[0]
"By the same logic, our work is different from domain adaptation (Glorot et al., 2011) as well.",5 Relevant Previous Work,[0],[0]
"There has been attempts to use world knowledge (Song and Roth, 2017) for creating more labeled training data and providing distant supervision etc.",5 Relevant Previous Work,[0],[0]
"Incorporating Inductive Biases (Ridgeway, 2016) based on the known information about a domain onto the structure of the learned models, is an active area of research.",5 Relevant Previous Work,[0],[0]
However our motivation and approach is fundamentally different from these works.,5 Relevant Previous Work,[0],[0]
In this work we have illustrated the need for incorporating world knowledge in training task specific models.,6 Conclusion & Future Work,[0],[0]
We presented a novel convolutionbased architecture to reduce the attention space over entities and relations that outperformed other models.,6 Conclusion & Future Work,[0],[0]
"With significant improvements over the vanilla baselines for two well known datasets, we have illustrated the efficacy of our proposed methods in enhancing the performance of deep learning models.",6 Conclusion & Future Work,[0],[0]
We showcased that the proposed method can be used to reduce labeled training data requirements of deep learning models.,6 Conclusion & Future Work,[0],[0]
"Although in this work we focused only on NLP tasks and using LSTM as the baseline model, the proposed approach is applicable for other domain tasks as well, with more complicated deep learning models as baseline.",6 Conclusion & Future Work,[0],[0]
To the best of our knowledge this is the first attempt at infusing general world knowledge for task specific training of deep learning models.,6 Conclusion & Future Work,[0],[0]
"Being the first work of its kind, there is a lot of scope for improvement.",6 Conclusion & Future Work,[0],[0]
A more sophisticated model which is able to retrieve facts more efficiently from millions of entries can be formulated.,6 Conclusion & Future Work,[0],[0]
"Currently we have focused only on a flat attention structure, a hierarchical attention mechanism would be more suitable.",6 Conclusion & Future Work,[0],[0]
The model uses soft attention to enable training by simple stochastic gradient descent.,6 Conclusion & Future Work,[0],[0]
Hard attention over facts using reinforcement learning can be pursued further.,6 Conclusion & Future Work,[0],[0]
"This will further help in selection of multifacts, that are not of similar type but relevant to the task.",6 Conclusion & Future Work,[0],[0]
"The convolution based model, helped to reduce the space over entities and relationships over which attention had to be generated.",6 Conclusion & Future Work,[0],[0]
"However more sophisticated techniques using similarity based search (Wang et al., 2014a; Mu and Liu, 2017) can be pursued towards this purpose.",6 Conclusion & Future Work,[0],[0]
"The results from the initial experiments illustrates the effectiveness of our proposed approach, advocating further investigations in these directions.",6 Conclusion & Future Work,[0.9504061293888327],['Proofs of the results are provided in Section 5.1 and the appendix.']
"Machine Learning has been the quintessential solution for many AI problems, but learning models are heavily dependent on specific training data.",abstractText,[0],[0]
"Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand.",abstractText,[0],[0]
"In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks.",abstractText,[0],[0]
Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism.,abstractText,[0],[0]
We introduce a convolutionbased model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space.,abstractText,[0],[0]
We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task.,abstractText,[0],[0]
"Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) & DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset.",abstractText,[0],[0]
"We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.",abstractText,[0],[0]
Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 451–462 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1042",text,[0],[0]
Multilingual word embeddings have attracted a lot of attention in recent times.,1 Introduction,[0],[0]
"In addition to having a direct application in inherently crosslingual tasks like machine translation (Zou et al., 2013) and crosslingual entity linking (Tsai and Roth, 2016), they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014) and document classification (Klementiev et al., 2012).
",1 Introduction,[0],[0]
"Most methods to learn these multilingual word embeddings make use of large parallel corpora (Gouws et al., 2015; Luong et al., 2015), but there have been several proposals to relax this requirement, given its scarcity in most language pairs.",1 Introduction,[0],[0]
"A possible relaxation is to use document-aligned or label-aligned comparable corpora (Søgaard et al.,
2015; Vulić and Moens, 2016; Mogadala and Rettinger, 2016), but large amounts of such corpora are not always available for some language pairs.
",1 Introduction,[0],[0]
"An alternative approach that we follow here is to independently train the embeddings for each language on monolingual corpora, and then learn a linear transformation to map the embeddings from one space into the other by minimizing the distances in a bilingual dictionary, usually in the range of a few thousand entries (Mikolov et al., 2013a; Artetxe et al., 2016).",1 Introduction,[0],[0]
"However, dictionaries of that size are not readily available for many language pairs, specially those involving less-resourced languages.
",1 Introduction,[0],[0]
"In this work, we reduce the need of large bilingual dictionaries to much smaller seed dictionaries.",1 Introduction,[0],[0]
"Our method can work with as little as 25 word pairs, which are straightforward to obtain assuming some basic knowledge of the languages involved.",1 Introduction,[0],[0]
"The method can also work with trivially generated seed dictionaries of numerals (i.e. 1-1, 2-2, 3-3, 4-4...) making it possible to learn bilingual word embeddings without any real bilingual data.",1 Introduction,[0],[0]
"In either case, we obtain very competitive results, comparable to other state-of-the-art methods that make use of much richer bilingual resources.
",1 Introduction,[0],[0]
"The proposed method is an extension of existing mapping techniques, where the dictionary is used to learn the embedding mapping and the embedding mapping is used to induce a new dictionary iteratively in a self-learning fashion (see Figure 1).",1 Introduction,[0],[0]
"In spite of its simplicity, our analysis of the implicit optimization objective reveals that the method is exploiting the structural similarity of independently trained embeddings.
",1 Introduction,[0],[0]
We analyze previous work in Section 2.,1 Introduction,[0],[0]
"Section 3 describes the self-learning framework, while Section 4 presents the experiments.",1 Introduction,[0],[0]
"Section 5 analyzes the underlying optimization objective, and Section 6 presents an error analysis.
",1 Introduction,[0],[0]
451,1 Introduction,[0],[0]
"We will first focus on bilingual embedding mappings, which are the basis of our proposals, and then on other unsupervised and weakly supervised methods to learn bilingual word embeddings.",2 Related work,[0],[0]
"Methods to induce bilingual mappings work by independently learning the embeddings in each language using monolingual corpora, and then learning a transformation from one embedding space into the other based on a bilingual dictionary.
",2.1 Bilingual embedding mappings,[0],[0]
"The first of such methods is due to Mikolov et al. (2013a), who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries.",2.1 Bilingual embedding mappings,[0],[0]
"The same optimization objective is used by Zhang et al. (2016), who constrain the transformation matrix to be orthogonal.",2.1 Bilingual embedding mappings,[0],[0]
"Xing et al. (2015) incorporate length normalization in the training of word embeddings and maximize the cosine similarity instead, enforcing the orthogonality constraint to preserve the length normalization after the mapping.",2.1 Bilingual embedding mappings,[0],[0]
"Finally, Lazaridou et al. (2015) use max-margin optimization with intruder negative sampling.
",2.1 Bilingual embedding mappings,[0],[0]
"Instead of learning a single linear transformation from the source language into the target language, Faruqui and Dyer (2014) use canonical correlation analysis to map both languages to a shared vector space.",2.1 Bilingual embedding mappings,[0],[0]
"Lu et al. (2015) extend this work and apply deep canonical correlation analysis to learn non-linear transformations.
",2.1 Bilingual embedding mappings,[0],[0]
"Artetxe et al. (2016) propose a general framework that clarifies the relation between Mikolov et al. (2013a), Xing et al. (2015), Faruqui and Dyer (2014) and Zhang et al. (2016) as variants of the
same core optimization objective, and show that a new variant is able to surpass them all.",2.1 Bilingual embedding mappings,[0],[0]
"While most of the previous methods use gradient descent, Artetxe et al. (2016) propose an efficient analytical implementation for those same methods, recently extended by Smith et al. (2017) to incorporate dimensionality reduction.
",2.1 Bilingual embedding mappings,[0],[0]
"A prominent application of bilingual embedding mappings, with a direct application in machine translation (Zhao et al., 2015), is bilingual lexicon extraction, which is also the main evaluation method.",2.1 Bilingual embedding mappings,[0],[0]
"More specifically, the learned mapping is used to induce the translation of source language words that were missing in the original dictionary, usually by taking their nearest neighbor word in the target language according to cosine similarity, although Dinu et al. (2015) and Smith et al. (2017) propose alternative retrieval methods to address the hubness problem.",2.1 Bilingual embedding mappings,[0],[0]
"As mentioned before, our method works with as little as 25 word pairs, while the methods discussed previously use thousands of pairs.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"The only exception in this regard is the work by Zhang et al. (2016), who only use 10 word pairs with good results on transfer learning for part-of-speech tagging.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Our experiments will show that, although their method captures coarse-grained relations, it fails on finer-grained tasks like bilingual lexicon induction.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Bootstrapping methods similar to ours have been previously proposed for traditional countbased vector space models (Peirsman and Padó, 2010; Vulić and Moens, 2013).",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"However, while previous techniques incrementally build a high-
Algorithm 1 Traditional framework Input: X (source embeddings)",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Input: Z (target embeddings),2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Input: D (seed dictionary)
1: W ← LEARN MAPPING(X , Z, D) 2: D ← LEARN DICTIONARY(X , Z, W ) 3: EVALUATE DICTIONARY(D)
dimensional model where each axis encodes the co-occurrences with a specific word and its equivalent in the other language, our method works with low-dimensional pre-trained word embeddings, which are more widely used nowadays.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
A practical aspect for reducing the need of bilingual supervision is on the design of the seed dictionary.,2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"This is analyzed in depth by Vulić and Korhonen (2016), who propose using documentaligned corpora to extract the training dictionary.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"A more common approach is to rely on shared words and cognates (Peirsman and Padó, 2010; Smith et al., 2017), eliminating the need of bilingual data in practice.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Our use of shared numerals exploits the same underlying idea, but relies on even less bilingual evidence and should thus generalize better to distant language pairs.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Miceli Barone (2016) and Cao et al. (2016) go one step further and attempt to learn bilingual embeddings without any bilingual evidence.,2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"The former uses adversarial autoencoders (Makhzani et al., 2016), combining an encoder that maps the source language embeddings into the target language, a decoder that reconstructs the original embeddings, and a discriminator that distinguishes mapped embeddings from real target language embeddings, whereas the latter adds a regularization term to the training of word embeddings that pushes the mean and variance of each dimension in different languages close to each other.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Although promising, the reported performance in both cases is poor in comparison to other methods.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Finally, the induction of bilingual knowledge from monolingual corpora is closely related to the decipherment scenario, for which models that incorporate word embeddings have also been proposed (Dou et al., 2015).",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"However, decipherment is only concerned with translating text from one language to another and relies on complex statistical models that are designed specifically for that purpose, while our approach is more general and learns task-independent multilingual embeddings.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Algorithm 2 Proposed self-learning framework Input: X (source embeddings),2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Input: Z (target embeddings),2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Input: D (seed dictionary)
1: repeat 2:",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"W ← LEARN MAPPING(X , Z, D) 3: D ← LEARN DICTIONARY(X , Z, W ) 4: until convergence criterion 5: EVALUATE DICTIONARY(D)",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"As discussed in Section 2.1, a common evaluation task (and practical application) of bilingual embedding mappings is to induce bilingual lexicons, that is, to obtain the translation of source words that were missing in the training dictionary, which are then compared to a gold standard test dictionary for evaluation.",3 Proposed self-learning framework,[0],[0]
"This way, one can say that the seed (train) dictionary is used to learn a mapping, which is then used to induce a better dictionary (at least in the sense that it is larger).",3 Proposed self-learning framework,[0],[0]
"Algorithm 1 summarizes this framework.
",3 Proposed self-learning framework,[0],[0]
"Following this observation, we propose to use the output dictionary in Algorithm 1 as the input of the same system in a self-learning fashion which, assuming that the output dictionary was indeed better than the original one, should serve to learn a better mapping and, consequently, an even better dictionary the second time.",3 Proposed self-learning framework,[0],[0]
The process can then be repeated iteratively to obtain a hopefully better mapping and dictionary each time until some convergence criterion is met.,3 Proposed self-learning framework,[0],[0]
"Algorithm 2 summarizes this alternative framework that we propose.
",3 Proposed self-learning framework,[0],[0]
Our method can be combined with any embedding mapping and dictionary induction technique (see Section 2.1).,3 Proposed self-learning framework,[0],[0]
"However, efficiency turns out to be critical for a variety of reasons.",3 Proposed self-learning framework,[0],[0]
"First of all, by enclosing the learning logic in a loop, the total training time is increased by the number of iterations.",3 Proposed self-learning framework,[0],[0]
"Even more importantly, our framework requires to explicitly build the entire dictionary at each iteration, whereas previous work tends to induce the translation of individual words ondemand later at runtime.",3 Proposed self-learning framework,[0],[0]
"Moreover, from the second iteration onwards, it is this induced, full dictionary that has to be used to learn the embedding mapping, and not the considerably smaller seed dictionary as it is typically done.",3 Proposed self-learning framework,[0],[0]
"In the following two subsections, we respectively describe the embedding mapping method and the dictionary in-
duction method that we adopt in our work with these efficiency requirements in mind.",3 Proposed self-learning framework,[0],[0]
"As discussed in Section 2.1, most previous methods to learn embedding mappings use variants of gradient descent.",3.1 Embedding mapping,[0],[0]
"Among the more efficient exact alternatives, we decide to adopt the one by Artetxe et al. (2016) for its simplicity and good results as reported in their paper.",3.1 Embedding mapping,[0],[0]
"We next present their method, adapting the formalization to explicitly incorporate the dictionary as required by our self-learning algorithm.
",3.1 Embedding mapping,[0],[0]
Let X and Z denote the word embedding matrices in two languages so that Xi∗ corresponds to the ith source language word embedding and Zj∗ corresponds to the jth target language embedding.,3.1 Embedding mapping,[0],[0]
"While Artetxe et al. (2016) assume these two matrices are aligned according to the dictionary, we drop this assumption and represent the dictionary explicitly as a binary matrix D, so that Dij = 1 if the ith source language word is aligned with the jth target language word.",3.1 Embedding mapping,[0],[0]
The goal is then to find the optimal mapping matrix W ∗ so that the sum of squared Euclidean distances between the mapped source embeddings Xi∗W and target embeddings Zj∗,3.1 Embedding mapping,[0],[0]
"for the dictionary entries Dij is minimized:
",3.1 Embedding mapping,[0],[0]
"W ∗ = arg min W
∑
i
∑
j
Dij ||Xi∗W",3.1 Embedding mapping,[0],[0]
"− Zj∗||2
Following Artetxe et al. (2016), we length normalize and mean center the embedding matrices X and Z in a preprocessing step, and constrain W to be an orthogonal matrix (i.e. WW T = W TW = I), which serves to enforce monolingual invariance, preventing a degradation in monolingual performance while yielding to better bilingual mappings.",3.1 Embedding mapping,[0],[0]
"Under such orthogonality constraint, minimizing the squared Euclidean distance becomes equivalent to maximizing the dot product, so the above optimization objective can be reformulated as follows:
",3.1 Embedding mapping,[0],[0]
"W ∗ = arg max W
Tr ( XWZTDT )
where Tr (·) denotes the trace operator (the sum of all the elements in the main diagonal).",3.1 Embedding mapping,[0],[0]
"The optimal orthogonal solution for this problem is given by W ∗ = UV T , where",3.1 Embedding mapping,[0],[0]
XTDZ = UΣV T is the singular value decomposition of XTDZ.,3.1 Embedding mapping,[0],[0]
"Since the dictionary matrix D is sparse, this can be efficiently computed in linear time with respect to the number of dictionary entries.",3.1 Embedding mapping,[0],[0]
"As discussed in Section 2.1, practically all previous work uses nearest neighbor retrieval for word translation induction based on embedding mappings.",3.2 Dictionary induction,[0],[0]
"In nearest neighbor retrieval, each source language word is assigned the closest word in the target language.",3.2 Dictionary induction,[0],[0]
"In our work, we use the dot product between the mapped source language embeddings and the target language embeddings as the similarity measure, which is roughly equivalent to cosine similarity given that we apply length normalization followed by mean centering as a preprocessing step (see Section 3.1).",3.2 Dictionary induction,[0],[0]
"This way, following the notation in Section 3.1, we set Dij = 1 if j = argmaxk (Xi∗W )",3.2 Dictionary induction,[0],[0]
·Zk∗ and,3.2 Dictionary induction,[0],[0]
"Dij = 0 otherwise1.
",3.2 Dictionary induction,[0],[0]
"While we find that independently computing the similarity measure between all word pairs is prohibitively slow, the computation of the entire similarity matrix XWZT can be easily vectorized using popular linear algebra libraries, obtaining big performance gains.",3.2 Dictionary induction,[0],[0]
"However, the resulting similarity matrix is often too large to fit in memory when using large vocabularies.",3.2 Dictionary induction,[0],[0]
"For that reason, instead of computing the entire similarity matrix XWZT in a single step, we iteratively compute submatrices of it using vectorized matrix multiplication, find their corresponding maxima each time, and then combine the results.",3.2 Dictionary induction,[0],[0]
"In this section, we experimentally test the proposed method in bilingual lexicon induction and crosslingual word similarity.",4 Experiments and results,[0],[0]
"Subsection 4.1 describes the experimental settings, while Subsections 4.2 and 4.3 present the results obtained in each of the tasks.",4 Experiments and results,[0],[0]
The code and resources necessary to reproduce our experiments are available at https://github.com/artetxem/ vecmap.,4 Experiments and results,[0],[0]
"For easier comparison with related work, we evaluated our mappings on bilingual lexicon induction using the public English-Italian dataset by Dinu et al. (2015), which includes monolingual word embeddings in both languages together with a bilingual dictionary split in a training set and a
1Note that we induce the dictionary entries starting from the source language words.",4.1 Experimental settings,[0],[0]
"We experimented with other alternatives in development, with minor differences.
test set2.",4.1 Experimental settings,[0],[0]
"The embeddings were trained with the word2vec toolkit with CBOW and negative sampling (Mikolov et al., 2013b)3, using a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC) and a 1.6 billion word corpus for Italian (itWaC).",4.1 Experimental settings,[0],[0]
"The training and test sets were derived from a dictionary built form Europarl word alignments and available at OPUS (Tiedemann, 2012), taking 1,500 random entries uniformly distributed in 5 frequency bins as the test set and the 5,000 most frequent of the remaining word pairs as the training set.
",4.1 Experimental settings,[0],[0]
"In addition to English-Italian, we selected two other languages from different language families with publicly available resources.",4.1 Experimental settings,[0],[0]
We thus created analogous datasets for English-German and English-Finnish.,4.1 Experimental settings,[0],[0]
"In the case of German, the embeddings were trained on the 0.9 billion word corpus SdeWaC, which is part of the WaCky collection (Baroni et al., 2009) that was also used for English and Italian.",4.1 Experimental settings,[0],[0]
"Given that Finnish is not included in this collection, we used the 2.8 billion word Common Crawl corpus provided at WMT 20164 instead, which we tokenized using the Stanford Tokenizer (Manning et al., 2014).",4.1 Experimental settings,[0],[0]
"In addition to that, we created training and test sets for both pairs from their respective Europarl dictionaries from OPUS following the exact same procedure used for English-Italian, and the word embeddings were also trained using the same configuration as Dinu et al. (2015).
",4.1 Experimental settings,[0],[0]
"Given that the main focus of our work is on small seed dictionaries, we created random subsets of 2,500, 1,000, 500, 250, 100, 75, 50 and 25 entries from the original training dictionaries of 5,000 entries.",4.1 Experimental settings,[0],[0]
"This was done by shuffling once the training dictionaries and taking their first k entries, so it is guaranteed that each dictionary is a strict subset of the bigger dictionaries.
",4.1 Experimental settings,[0],[0]
"In addition to that, we explored using automatically generated dictionaries as a shortcut to practical unsupervised learning.",4.1 Experimental settings,[0],[0]
"For that purpose, we created numeral dictionaries, consisting of words matching the [0-9]+ regular expression in both vocabularies (e.g. 1-1, 2-2, 3-3, 1992-1992
2http://clic.cimec.unitn.it/ ˜georgiana.dinu/down/
3The context window was set to 5 words, the dimension of the embeddings to 300, the sub-sampling to 1e-05 and the number of negative samples to 10, and the vocabulary was restricted to the 200,000 most frequent words
4http://www.statmt.org/wmt16/ translation-task.html
etc.).",4.1 Experimental settings,[0],[0]
"The resulting dictionary had 2772 entries for English-Italian, 2148 for English-German, and 2345 for English-Finnish.",4.1 Experimental settings,[0],[0]
"While more sophisticated approaches are possible (e.g. involving the edit distance of all words), we believe that this method is general enough that should work with practically any language pair, as Arabic numerals are often used even in languages with a different writing system (e.g. Chinese and Russian).
",4.1 Experimental settings,[0],[0]
"While bilingual lexicon induction is a standard evaluation task for seed dictionary based methods like ours, it is unsuitable for bilingual corpus based methods, as statistical word alignment already provides a reliable way to derive dictionaries from bilingual corpora and, in fact, this is how the test dictionary itself is built in our case.",4.1 Experimental settings,[0],[0]
"For that reason, we carried out some experiments in crosslingual word similarity as a way to test our method in a different task and allowing to compare it to systems that use richer bilingual data.",4.1 Experimental settings,[0],[0]
"There are no many crosslingual word similarity datasets, and we used the RG-65 and WordSim353 crosslingual datasets for English-German and the WordSim-353 crosslingual dataset for EnglishItalian as published by Camacho-Collados et al. (2015) 5.
",4.1 Experimental settings,[0],[0]
"As for the convergence criterion, we decide to stop training when the improvement on the average dot product for the induced dictionary falls below a given threshold from one iteration to the next.",4.1 Experimental settings,[0],[0]
"After length normalization, the dot product ranges from -1 to 1, so we decide to set this threshold at 1e-6, which we find to be a very conservative value yet enough that training takes a reasonable amount of time.",4.1 Experimental settings,[0],[0]
"The curves in the next section confirm that this was a reasonable choice.
",4.1 Experimental settings,[0],[0]
"This convergence criterion is usually met in less than 100 iterations, each of them taking 5 minutes on a modest desktop computer (Intel Core i5-4670 CPU with 8GiB of RAM), including the induction of a dictionary of 200,000 words at each iteration.",4.1 Experimental settings,[0],[0]
"For the experiments on bilingual lexicon induction, we compared our method with those proposed by Mikolov et al. (2013a), Xing et al. (2015), Zhang et al. (2016) and Artetxe et al. (2016), all of them implemented as part of the framework proposed by the latter.",4.2 Bilingual lexicon induction,[0],[0]
"The results ob-
5http://lcl.uniroma1.it/ similarity-datasets/
tained with the 5,000 entry, 25 entry and the numerals dictionaries for all the 3 language pairs are given in Table 1.
",4.2 Bilingual lexicon induction,[0],[0]
"The results for the 5,000 entry dictionaries show that our method is comparable or even better than the other systems.",4.2 Bilingual lexicon induction,[0],[0]
"As another reference, the best published results using nearest-neighbor retrieval are due to Lazaridou et al. (2015), who report an accuracy of 40.20% for the full EnglishItalian dictionary, almost at pair with our system (39.67%).
",4.2 Bilingual lexicon induction,[0],[0]
"In any case, the main focus of our work is on smaller dictionaries, and it is under this setting that our method really stands out.",4.2 Bilingual lexicon induction,[0],[0]
"The 25 entry and numerals columns in Table 1 show the results for this setting, where all previous methods drop dramatically, falling below 1% accuracy in all cases.",4.2 Bilingual lexicon induction,[0],[0]
"The method by Zhang et al. (2016) also obtains poor results with small dictionaries, which reinforces our hypothesis in Section 2.2 that their method can only capture coarse-grain bilingual relations for small dictionaries.",4.2 Bilingual lexicon induction,[0],[0]
"In contrast, our proposed method obtains very competitive results for all dictionaries, with a difference of only 1-2 points between the full dictionary and both the 25 entry dictionary and the numerals dictionary in all three languages.",4.2 Bilingual lexicon induction,[0],[0]
"Figure 2 shows the curve of the English-Italian accuracy for different seed dictionary sizes, confirming this trend.
",4.2 Bilingual lexicon induction,[0],[0]
"Finally, it is worth mentioning that, even if all the three language pairs show the same general behavior, there are clear differences in their absolute accuracy numbers, which can be attributed to the linguistic proximity of the languages involved.",4.2 Bilingual lexicon induction,[0],[0]
"In particular, the results for English-Finnish are about 10 points below the rest, which is explained by the fact that Finnish is a non-indoeuropean agglutinative language, making the task considerably more difficult for this language pair.",4.2 Bilingual lexicon induction,[0],[0]
"In this regard, we believe that the good results with small dictionaries are a strong indication of the robustness of our method, showing that it is able to learn good bilingual mappings from very little bilingual ev-
idence even for distant language pairs where the structural similarity of the embedding spaces is presumably weaker.",4.2 Bilingual lexicon induction,[0],[0]
"In addition to the baseline systems in Section 4.2, in the crosslingual similarity experiments we also tested the method by Luong et al. (2015), which is the state-of-the-art for bilingual word embeddings based on parallel corpora (Upadhyay et al., 2016)6.",4.3 Crosslingual word similarity,[0],[0]
"As this method is an extension of word2vec, we used the same hyperparameters as for the monolingual embeddings when possible (see Section 4.1), and leave the default ones otherwise.",4.3 Crosslingual word similarity,[0],[0]
"We used Europarl as our parallel corpus to train this method as done by the authors, which consists of nearly 2 million parallel sentences.
",4.3 Crosslingual word similarity,[0],[0]
"As shown in the results in Table 2, our method obtains the best results in all cases, surpassing the rest of the dictionary-based methods by 1-3 points depending on the dataset.",4.3 Crosslingual word similarity,[0],[0]
"But, most importantly, it does not suffer from any significant degradation for using smaller dictionaries and, in fact, our method gets better results using the 25 entry dictionary or the numeral list as the only bilingual evidence than any of the baseline systems using much richer resources.
",4.3 Crosslingual word similarity,[0],[0]
"The relatively poor results of Luong et al. (2015) can be attributed to the fact that the dictionary based methods make use of much bigger monolingual corpora, while methods based on parallel corpora are restricted to smaller corpora.",4.3 Crosslingual word similarity,[0],[0]
"However, it is not clear how to introduce monolingual corpora on those methods.",4.3 Crosslingual word similarity,[0],[0]
"We did run some experiments with BilBOWA (Gouws et al., 2015), which supports training in monolingual corpora in addition to bilingual corpora, but obtained very poor results7.",4.3 Crosslingual word similarity,[0],[0]
"All in all, our experiments show
6We also tested English-German pre-trained embeddings from Klementiev et al. (2012) and Chandar A P et al. (2014).",4.3 Crosslingual word similarity,[0],[0]
"They both had coverage problems that made the results hard to compare, and, when considering the correlations for the word pairs in their vocabulary, their performance was poor.
",4.3 Crosslingual word similarity,[0],[0]
"7Upadhyay et al. (2016) report similar problems using
that it is better to use large monolingual corpora in combination with very little bilingual data rather than a bilingual corpus of a standard size alone.",4.3 Crosslingual word similarity,[0],[0]
"It might seem somehow surprising at first that, as seen in the previous section, our simple selflearning approach is able to learn high quality bilingual embeddings from small seed dictionaries instead of falling in degenerated solutions.",5 Global optimization objective,[0],[0]
"In this section, we try to shed light on our approach, and give empirical evidence supporting our claim.
",5 Global optimization objective,[0],[0]
"More concretely, we argue that, for the embedding mapping and dictionary induction methods described in Section 3, the proposed selflearning framework is implicitly solving the following global optimization problem8:
W ∗ = arg max W
∑
i
max j
(Xi∗W ) · Zj∗
s.t. WW T = W TW",5 Global optimization objective,[0],[0]
=,5 Global optimization objective,[0],[0]
"I
Contrary to the optimization objective for W in Section 3.1, the global optimization objective does not refer to any dictionary, and maximizes the similarity between each source language word and its closest target language word.",5 Global optimization objective,[0],[0]
"Intuitively, a random solution would map source language embeddings to seemingly random locations in the target language space, and it would thus be unlikely that
BilBOWA.",5 Global optimization objective,[0],[0]
"8While we restrict our formal analysis to the embedding mapping and dictionary induction method that we use, the general reasoning should be valid for other choices as well.
",5 Global optimization objective,[0],[0]
"they have any target language word nearby, making the optimization value small.",5 Global optimization objective,[0],[0]
"In contrast, a good solution would map source language words close to their translation equivalents in the target language space, and they would thus have their corresponding embeddings nearby, making the optimization value large.",5 Global optimization objective,[0],[0]
"While it is certainly possible to build degenerated solutions that take high optimization values for small subsets of the vocabulary, we think that the structural similarity between independently trained embedding spaces in different languages is strong enough that optimizing this function yields to meaningful bilingual mappings when the size of the vocabulary is much larger than the dimensionality of the embeddings.
",5 Global optimization objective,[0],[0]
The reasoning for how the self-learning framework is optimizing this objective is as follows.,5 Global optimization objective,[0],[0]
"At the end of each iteration, the dictionary D is updated to assign, for the current mapping W , each source language word to its closest target language word.",5 Global optimization objective,[0],[0]
"This way, when we update W to maximize the average similarity of these dictionary entries at the beginning of the next iteration, it is guaranteed that the value of the optimization objective will improve (or at least remain the same).",5 Global optimization objective,[0],[0]
"The reason is that the average similarity between each word and what were previously the closest words will be improved if possible, as this is what the updated W directly optimizes (see Section 3.1).",5 Global optimization objective,[0],[0]
"In addition to that, it is also possible that, for some source words, some other target words get closer after the update.",5 Global optimization objective,[0],[0]
"Thanks to this, our self-learning algorithm is guaranteed to converge to a local optimum of the above global objective, behaving like an alternating optimization algorithm for it.
",5 Global optimization objective,[0],[0]
"It is interesting to note that the above reasoning is valid no matter what the the initial solution is, and, in fact, the global optimization objective does not depend on the seed dictionary nor any other
bilingual resource.",5 Global optimization objective,[0],[0]
"For that reason, it should be possible to use a random initialization instead of a small seed dictionary.",5 Global optimization objective,[0],[0]
"However, we empirically observe that this works poorly in practice, as our algorithm tends to get stuck in poor local optima when the initial solution is not good enough.
",5 Global optimization objective,[0],[0]
"The general behavior of our method is reflected in Figure 3, which shows the learning curve for different seed dictionaries according to both the objective function and the accuracy on bilingual lexicon induction.",5 Global optimization objective,[0],[0]
"As it can be seen, the objective function is improved from iteration to iteration and converges to a local optimum just as expected.",5 Global optimization objective,[0],[0]
"At the same time, the learning curves show a strong correlation between the optimization objective and the accuracy, as it can be clearly observed that improving the former leads to an improvement of the latter, confirming our explanations.",5 Global optimization objective,[0],[0]
"Regarding random initialization, the figure shows that the algorithm gets stuck in a poor local optimum of the objective function, which is the reason of the bad performance (0% accuracy) on bilingual lexicon induction, but the proposed optimization objective itself seems to be adequate.
",5 Global optimization objective,[0],[0]
"Finally, we empirically observe that our algorithm learns similar mappings no matter what the seed dictionary was.",5 Global optimization objective,[0],[0]
"We first repeated our experiments on English-Italian bilingual lexicon induction for 5 different dictionaries of 25 entries, obtaining an average accuracy of 38.15% and a standard deviation of only 0.75%.",5 Global optimization objective,[0],[0]
"In addition to that, we observe that the overlap between the predictions made when starting with the full dictionary and the numerals dictionary is 76.00% (60.00% for the 25 entry dictionary).",5 Global optimization objective,[0],[0]
"At the same time,
37.00% of the test cases are correctly solved by both instances, and it is only 5.07% of the test cases that one of them gets right and the other wrong (34.00% and 8.94% for the 25 entry dictionary).",5 Global optimization objective,[0],[0]
"This suggests that our algorithm tends to converge to similar solutions even for disjoint seed dictionaries, which is in line with our view that we are implicitly optimizing an objective that is independent from the seed dictionary, yet a seed dictionary is necessary to build a good enough initial solution to avoid getting stuck in poor local optima.",5 Global optimization objective,[0],[0]
"For that reason, it is likely that better methods to tackle this optimization problem would allow learning bilingual word embeddings without any bilingual evidence at all and, in this regard, we believe that our work opens exciting opportunities for future research.",5 Global optimization objective,[0.9536005130945381],"['Thus, if for example, we ignore complexity issues and can sample infinitely many w’s, it is not surprising that we can avoid the need for exact computation of the kernel.']"
"So as to better understand the behavior of our system, we performed an error analysis of its output in English-Italian bilingual lexicon induction when starting with the 5,000 entry, the 25 entry and the numeral dictionaries in comparison with the baseline method of Artetxe et al. (2016) with the 5,000 entry dictionary.",6 Error analysis,[0],[0]
"For that purpose, we took 100 random examples from the test set in the [1-5K] frequency bin, another 100 from the [5K20K] frequency bin and 30 from the [100K-200K] frequency bin, and manually analyzed each of the errors made by all the 4 different variants.
",6 Error analysis,[0],[0]
"Our analysis first reveals that, in all the cases, about a third of the translations taken as erroneous according to the gold standard are not so in real-
ity.",6 Error analysis,[0],[0]
This corresponds to both different morphological variants of the gold standard translations (e.g. dichiarato/dichiarò) and other valid translations that were missing in the gold standard (e.g. climb → salita instead of the gold standard scalato).,6 Error analysis,[0],[0]
"This phenomenon is considerably more pronounced in the first frequency bins, which already have a much higher accuracy according to the gold standard.
",6 Error analysis,[0],[0]
"As for the actual errors, we observe that nearly a third of them correspond to named entities for all the different variants.",6 Error analysis,[0],[0]
"Interestingly, the vast majority of the proposed translations in these cases are also named entities (e.g. Ryan→ Jason, John→ Paolo), which are often highly related to the original ones (e.g. Volvo→ BMW, Olympus→ Nikon).",6 Error analysis,[0],[0]
"While these are clear errors, it is understandable that these methods are unable to discriminate between named entities to this degree based solely on the distributional hypothesis, in particular when it comes to common proper names (e.g. John, Andy), and one could design alternative strategies to address this issue like taking the edit distance as an additional signal.
",6 Error analysis,[0],[0]
"For the remaining errors, all systems tend to propose translations that have some degree of relationship with the correct ones, including nearsynonyms (e.g. guidelines → raccomandazioni), antonyms (e.g. sender→ destinatario) and words in the same semantic field (e.g. nominalism→ intuizionismo / innatismo, which are all philosophical doctrines).",6 Error analysis,[0],[0]
"However, there are also a few instances where the relationship is weak or unclear (e.g. loch→ giardini, sweep→ serrare).",6 Error analysis,[0],[0]
"We also observe a few errors that are related to multiwords or collocations (e.g. carrier→ aereo, presumably related to the multiword air carrier / linea aerea), as well as some rare word that is repeated across many translations (Ferruzzi), which could be attributed to the hubness problem (Dinu et al., 2015; Lazaridou et al., 2015).
",6 Error analysis,[0],[0]
"All in all, our error analysis reveals that the baseline method of Artetxe et al. (2016) and the proposed algorithm tend to make the same kind of errors regardless of the seed dictionary used by the latter, which reinforces our interpretation in the previous section regarding an underlying optimization objective that is independent from any training dictionary.",6 Error analysis,[0],[0]
"Moreover, it shows that the quality of the learned mappings is much better than what the raw accuracy numbers might sug-
gest, encouraging the incorporation of these techniques in other applications.",6 Error analysis,[0],[0]
"In this work, we propose a simple self-learning framework to learn bilingual word embedding mappings in combination with any embedding mapping and dictionary induction technique.",7 Conclusions and future work,[0],[0]
"Our experiments on bilingual lexicon induction and crosslingual word similarity show that our method is able to learn high quality bilingual embeddings from as little bilingual evidence as a 25 word dictionary or an automatically generated list of numerals, obtaining results that are competitive with state-of-the-art systems using much richer bilingual resources like larger dictionaries or parallel corpora.",7 Conclusions and future work,[0],[0]
"In spite of its simplicity, a more detailed analysis shows that our method is implicitly optimizing a meaningful objective function that is independent from any bilingual data which, with a better optimization method, might allow to learn bilingual word embeddings in a completely unsupervised manner.
",7 Conclusions and future work,[0],[0]
"In the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all.",7 Conclusions and future work,[0],[0]
"In addition to that, we would like to explore non-linear transformations (Lu et al., 2015) and alternative dictionary induction methods (Dinu et al., 2015; Smith et al., 2017).",7 Conclusions and future work,[0],[0]
"Finally, we would like to apply our model in the decipherment scenario (Dou et al., 2015).",7 Conclusions and future work,[0],[0]
"We thank the anonymous reviewers for their insightful comments and Flavio Merenda for his help with the error analysis.
",Acknowledgements,[0],[0]
"This research was partially supported by a Google Faculty Award, the Spanish MINECO (TUNER TIN2015-65308-C5-1-R, MUSTER PCIN-2015-226 and TADEEP TIN2015-70214-P, cofunded by EU FEDER), the Basque Government (MODELA KK-2016/00082) and the UPV/EHU (excellence research group).",Acknowledgements,[0],[0]
Mikel Artetxe enjoys a doctoral grant from the Spanish MECD.,Acknowledgements,[0],[0]
"Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs.",abstractText,[0],[0]
"This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead.",abstractText,[0],[0]
"In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique.",abstractText,[0],[0]
"Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.",abstractText,[0],[0]
Learning bilingual word embeddings with (almost) no bilingual data,title,[0],[0]
"In this paper we propose a spectral method for learning the following binary latent variable model, shown in Figure 1.",1. Introduction,[0],[0]
"The hidden layer, h = (h1, . . .",1. Introduction,[0],[0]
", hd), consists of d binary random variables with an unknown joint distribution Ph : {0, 1}d",1. Introduction,[0],[0]
→,1. Introduction,[0],[0]
"[0, 1].",1. Introduction,[0],[0]
"The observed vector x ∈ Rm with m ≥ d features is modeled as
x = W>h+ σξ, (1)
whereW ∈ Rd×m is an unknown weight matrix assumed to be full rank d. Here, σ",1. Introduction,[0],[0]
"≥ 0 is the noise level and ξ is an additive noise vector independent of h, whose m coordinates are all i.i.d.",1. Introduction,[0],[0]
"zero mean and unit variance random variables.
",1. Introduction,[0],[0]
1Dept.,1. Introduction,[0],[0]
"of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 7610001, Israel",1. Introduction,[0],[0]
.,1. Introduction,[0],[0]
"2Braun School of Public Health and Community Medicine, The Hebrew University of Jerusalem, Jerusalem 9112102, Israel.",1. Introduction,[0],[0]
"3Program of Applied Mathematics, Yale University, New Haven, CT 06511, USA.",1. Introduction,[0],[0]
"Correspondence to: Ariel Jaffe <ariel.jaffe@weizmann.ac.il>, Roi Weiss <roi.weiss@weizmann.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"For simplicity we assume it is Gaussian, though our method can be modified to handle other noise distributions.
",1. Introduction,[0],[0]
"The model in (1) appears, for example, in overlapping clustering (Banerjee et al., 2005; Baadel et al., 2016), in various problems in bioinformatics (Segal et al., 2002; Becker et al., 2011; Slawski et al., 2013), and in blind source separation (Van der Veen, 1997).",1. Introduction,[0],[0]
"A special instance of model (1) is the Gaussian-Bernoulli restricted Boltzmann machine (GRBM) where the distribution Ph is further assumed to have a parametric energy-based structure (Hinton & Salakhutdinov, 2006; Cho et al., 2011; Wang et al., 2012).",1. Introduction,[0],[0]
"G-RBMs were used, e.g., in modeling human motion (Taylor et al., 2007) and natural image patches (Melchior et al., 2017).
",1. Introduction,[0],[0]
Given n i.i.d.,1. Introduction,[0],[0]
"samples x1, . . .",1. Introduction,[0],[0]
",xn from model (1), the goal is to estimate the weight matrixW .",1. Introduction,[0],[0]
A common approach for learning W is by maximum likelihood.,1. Introduction,[0],[0]
"As this function is non-convex, common optimization schemes include the EM algorithm and alternating least squares (ALS).",1. Introduction,[0],[0]
"In addition, several works developed iterative methods specialized to GRBMs (Hinton, 2010; Cho et al., 2011).",1. Introduction,[0],[0]
"All these methods, however, often lack consistency guarantees and may not be well suited for large datasets due to their potential slow convergence.",1. Introduction,[0],[0]
"This is not surprising, as learning W under model (1) is believed to be computationally hard; see for example Mossel & Roch (2005).
",1. Introduction,[0],[0]
"Over the past years, several works considered variants and specific instances of model (1) under additional assumptions on the distribution Ph or on the weight matrix W .",1. Introduction,[0],[0]
"For example, when Ph is a product distribution, the learning problem becomes that of independent component analysis (ICA) with binary signals (Hyvärinen et al., 2004).",1. Introduction,[0],[0]
"In this case, several methods were derived for estimating W and under suitable non-degeneracy conditions were proven to be both computationally efficient and statistically consistent (Shalvi & Weinstein, 1993; Frieze et al., 1996; Regalia & Kofidis, 2003; Hyvärinen et al., 2004; Anandkumar et al., 2014; Jain & Oh, 2014).",1. Introduction,[0],[0]
"Similarly, when the hidden units are mutually exclusive, namely Ph has support h ∈ {ei}di=1, the model is a Gaussian mixture (GMM) with d spherical components with linearly independent means.",1. Introduction,[0],[0]
"Efficient and consistent algorithms were derived for this case as well (Moitra & Valiant, 2010; Anandkumar et al., 2012a;b; Hsu & Kakade, 2013).",1. Introduction,[0],[0]
"Among those, most relevant to this work
are orthogonal tensor decomposition methods (Anandkumar et al., 2014).",1. Introduction,[0],[0]
"Interestingly, these methods can learn some additional latent models, with hidden units that are not necessarily binary, such as Dirichlet allocation and other correlated topic models (Arabshahi & Anandkumar, 2017).
",1. Introduction,[0],[0]
Learning W given the observed data {xj}nj=1 can also be viewed as a noisy matrix factorization problem.,1. Introduction,[0],[0]
"If W is known to be non-negative, then various non-negative matrix factorization methods can be used.",1. Introduction,[0],[0]
"Moreover, under appropriate conditions, some of these methods were proven to be computationally efficient and consistent (Donoho & Stodden, 2004; Arora et al., 2012).",1. Introduction,[0],[0]
"For general full rank W , the matrix factorization method in Slawski et al. (2013) (SHL) exactly recovers W when σ = 0 with a runtime exponential in d. This method, however, can handle only low levels of noise and has no consistency guarantees when σ > 0.
",1. Introduction,[0],[0]
A tensor eigenpair approach In this paper we propose a novel spectral method for learning W which is based on the eigenvectors of both the second order moment matrix and the third order moment tensor of the observed data.,1. Introduction,[0],[0]
"We prove that our method is consistent under mild non-degeneracy conditions and achieves the parametric rate OP (n − 12 ) for any noise level σ ≥ 0.
",1. Introduction,[0],[0]
The non-degeneracy conditions we pose are significantly weaker than those required by the previous tensor decomposition methods mentioned above.,1. Introduction,[0],[0]
"In particular, their assumptions and resulting methods can be viewed as specific cases of our more general approach.
",1. Introduction,[0],[0]
"Similarly to the matrix factorization method in Slawski et al. (2013), our algorithm has runtime linear in n, polynomial in m, and in general exponential in d. With our current Matlab implementation, most of the runtime is spent on computing the eigenpairs of a d× d× d tensor.",1. Introduction,[0],[0]
"Practically, our method, implemented without any particular optimization, can learn a model with 12 hidden units in less than ten minutes on a standard PC.",1. Introduction,[0],[0]
"Furthermore, the overall runtime can be significantly reduced, since the step of computing the tensor eigenpairs can be embarrassingly parallelized.
",1. Introduction,[0],[0]
Paper outline In the next section we give necessary background on tensor eigenpairs.,1. Introduction,[0],[0]
"In Section 3 we introduce our
method in the case σ = 0.",1. Introduction,[0],[0]
The case σ ≥ 0 is treated in Section 4.,1. Introduction,[0],[0]
Experiments with our method and comparison to other approaches appear in Section 5.,1. Introduction,[0],[0]
All proofs are deferred to the supplementary material.,1. Introduction,[0],[0]
Notation Denote,2. Preliminaries,[0],[0]
"[d] = {1, . . .",2. Preliminaries,[0],[0]
", d} and ei as the i-th unit vector.",2. Preliminaries,[0],[0]
"We slightly abuse notation and view a matrixW also as the set of its columns, namely w ∈ W is some column of W and span(W ) is the span of all its columns.",2. Preliminaries,[0],[0]
"The unit sphere is denoted by Sd−1 = {u ∈ Rd : ‖u‖ = 1}.
",2. Preliminaries,[0],[0]
"A tensor T ∈ Rd×d×d is symmetric if Tijk = Tπ(i,j,k) for all permutations π of i, j, k.",2. Preliminaries,[0],[0]
"Here, we consider only symmetric tensors.",2. Preliminaries,[0],[0]
"T can also be seen as a multi-linear operator: for matrices W 1,W 2,W 3 with W i ∈ Rd×di , the tensor-mode product, denoted T (W 1,W 2,W 3), is a d1 × d2 × d3 tensor whose (i1, i2, i3)-th entry is∑
j1,j2,j3∈[d]
W 1j1i1W 2 j2i2W 3 j3i3Tj1j2j3 .
",2. Preliminaries,[0],[0]
Tensor eigenpairs Several types of eigenpairs of a tensor have been proposed.,2. Preliminaries,[0],[0]
"Here, we consider the following definition, termed Z-eigenpairs by Qi (2005) and l2-eigenpairs by Lim (2005).",2. Preliminaries,[0],[0]
"Henceforth we just call them eigenpairs.
",2. Preliminaries,[0],[0]
Definition 1.,2. Preliminaries,[0],[0]
"(u, λ) ∈ Rd × R is an eigenpair of T if
T (I,u,u) =",2. Preliminaries,[0],[0]
λu and ‖u‖ = 1.,2. Preliminaries,[0],[0]
"(2)
Note that if (u, λ) is an eigenpair then the eigenvalue is simply λ = T (u,u,u).",2. Preliminaries,[0],[0]
"In addition, (−u,−λ) is also an eigenpair.",2. Preliminaries,[0],[0]
"Following common practice we treat these two pairs as one and make the convention that λ ≥ 0.
",2. Preliminaries,[0],[0]
"In contrast to the matrix case, the number of eigenvalues {λ} of a tensor T ∈ Rd×d×d can be much larger than d. As shown by Cartwright & Sturmfels (2013), for a d× d× d tensor, there can be at most 2d",2. Preliminaries,[0],[0]
− 1 of them.,2. Preliminaries,[0],[0]
"With precise definitions appearing in Cartwright & Sturmfels (2013), for a generic tensor, all its eigenvalues have multiplicity one and the number of eigenpairs {(u, λ)} is at most 2d",2. Preliminaries,[0],[0]
"− 1.
",2. Preliminaries,[0],[0]
"In principle, enumerating the set of all eigenpairs of a general symmetric tensor is a #P problem (Hillar & Lim, 2013).",2. Preliminaries,[0],[0]
"Nevertheless, several methods have been proposed for computing at least some eigenpairs, including iterative higherorder power methods (Kolda & Mayo, 2011; 2014), homotopy continuation (Chen et al., 2016), semidefinite programming (Cui et al., 2014), and iterative Newton-based methods (Jaffe et al., 2017; Guo et al., 2017).",2. Preliminaries,[0],[0]
"We conclude this section with the definition of Newton-stable eigenpairs (Jaffe et al., 2017) which are most relevant to our work.
",2. Preliminaries,[0],[0]
"Newton-stable eigenpairs Equivalently to (2), eigenpairs of T can also be characterized by the function g : Rd → Rd,
g(u)",2. Preliminaries,[0],[0]
"= T (I,u,u)− T (u,u,u) · u. (3)
It is easy to verify that a pair (u, λ) with ‖u‖ = 1 is an eigenpair of T if and only if g(u) = 0 and λ = T (u,u,u).",2. Preliminaries,[0],[0]
"The stability of an eigenpair is determined by its Jacobian matrix ∇g(u) ∈ Rd×d, more precisely, by its projection into the d− 1 dimensional subspace orthogonal to u. Formally, let Lu ∈ Rd×(d−1) be a matrix with d− 1 orthonormal columns that span the subspace orthogonal to u and define the (d− 1)× (d− 1) projected Jacobian matrix
Jp(u) = L > u∇g(u)Lu.",2. Preliminaries,[0],[0]
"(4)
Definition 2.",2. Preliminaries,[0],[0]
"An eigenpair (u, λ) of T ∈ Rd×d×d is Newton-stable if the matrix Jp(u) has full rank d− 1.
",2. Preliminaries,[0],[0]
The homotopy continuation method in Chen et al. (2016) is guaranteed to compute all the Newton-stable eigenpairs of a tensor.,2. Preliminaries,[0],[0]
"Alternatively, all the Newton-stable eigenpairs can be computed by the iterative orthogonal Newton correction method (O–NCM) in Jaffe et al. (2017) as these are the attracting fixed points for this algorithm.",2. Preliminaries,[0],[0]
"Moreover, O–NCM converges to any Newton-stable eigenpair at a quadratic rate given a sufficiently close initial guess.",2. Preliminaries,[0],[0]
"Finally, for a generic tensor, all its eigenpairs are Newton-stable.",2. Preliminaries,[0],[0]
To motivate our approach for estimating the matrix W it is instructive to first consider the ideal noiseless case where σ = 0.,3. Learning in the noiseless case,[0],[0]
"In this case, model (1) takes the form x = W>h.",3. Learning in the noiseless case,[0],[0]
Our problem then becomes that of factorizing the observed matrix X =,3. Learning in the noiseless case,[0],[0]
"[x1, . . .",3. Learning in the noiseless case,[0],[0]
",xn] ∈",3. Learning in the noiseless case,[0],[0]
"Rm×n of n samples into a product of real and binary low-rank matrices,1
Find W ∈ Rd×m, H ∈ {0, 1}d×n s.t. X",3. Learning in the noiseless case,[0],[0]
"= W>H. (5)
To be able to recover W we first need conditions under which the decomposition of X into W and H is unique.",3. Learning in the noiseless case,[0],[0]
"Clearly, such a factorization can be unique at most up to a permutation of its components; we henceforth ignore this degeneracy.",3. Learning in the noiseless case,[0],[0]
"A sufficient condition for uniqueness, similar to the one posed in Slawski et al. (2013), is that H is rigid.",3. Learning in the noiseless case,[0],[0]
"Formally, H ∈ {0, 1}d×n is rigid if any non-trivial linear combination of its rows yields a non-binary vector: ∀u 6= 0,
u>H ∈ {0, 1}n ⇔ u ∈ {ei}di=1.",3. Learning in the noiseless case,[0],[0]
"(6)
Condition (6) is satisfied, for example, when the columns of H include ei and ei + ej for all i",3. Learning in the noiseless case,[0],[0]
6= j ∈,3. Learning in the noiseless case,[0],[0]
[d].,3. Learning in the noiseless case,[0],[0]
"If there
1Note that this is different from the problem known as “Boolean matrix factorization”, where X and W are assumed to be binary as well; see Miettinen & Vreeken (2014) and references therein.
exists a positive constant p0 > 0 such that Ph(ei) ≥ p0 and Ph(ei + ej) ≥ p0, then for a sample size n >",3. Learning in the noiseless case,[0],[0]
"2 log(d)/p0 the matrix H is rigid with high probability.
",3. Learning in the noiseless case,[0],[0]
"The following proposition, similar in nature to the (affine constrained) uniqueness guarantee in Slawski et al. (2013), shows that under condition (6) the factorization in (5) is unique and fully characterized by the binary constraints.
",3. Learning in the noiseless case,[0],[0]
Proposition 1.,3. Learning in the noiseless case,[0],[0]
"Let X = W>H with H ∈ {0, 1}d×n rigid and W ∈ Rd×m full rank with m ≥ d.",3. Learning in the noiseless case,[0],[0]
Let W † ∈ Rm×d be the unique right pseudo-inverse of W so WW † = Id.,3. Learning in the noiseless case,[0],[0]
"Then W and H are unique and for all v ∈ span(X) \ {0},
v>X ∈ {0, 1}n ⇔ v ∈W †.",3. Learning in the noiseless case,[0],[0]
"(7)
Hence, under the rigidity condition (6), the matrix factorization problem in (5) is equivalent to the problem of finding the unique set W † = {v∗1 , . .",3. Learning in the noiseless case,[0],[0]
.,3. Learning in the noiseless case,[0],[0]
",v∗d} ⊆ span(X) of d non-zero vectors that satisfy the binary constraints v∗i",3. Learning in the noiseless case,[0],[0]
">X ∈ {0, 1}n.",3. Learning in the noiseless case,[0],[0]
The weight matrix is then W =,3. Learning in the noiseless case,[0],[0]
"(W †)†.
Algorithm outline We recover W † via a two step procedure.",3. Learning in the noiseless case,[0],[0]
"First, a finite set V = {v1,v2, . . .",3. Learning in the noiseless case,[0],[0]
} ⊆ span(X) of candidate vectors is computed with a guarantee that W † ⊆ V .,3. Learning in the noiseless case,[0],[0]
"Specifically, V is computed from the set of eigenpairs of a d× d× d tensor, constructed from the low order moments of X .",3. Learning in the noiseless case,[0],[0]
"Typically, the size of V will be much larger than d, so in the second step V is filtered by selecting all v ∈ V that satisfy v>X ∈ {0, 1}n.
",3. Learning in the noiseless case,[0],[0]
Before describing the two steps in more detail we first state the additional non-degeneracy conditions we pose.,3. Learning in the noiseless case,[0],[0]
"To this end, denote the unknown first, second, and third order moments of the latent binary vector h by
p = E[h] ∈",3. Learning in the noiseless case,[0],[0]
"Rd, C = E[h⊗ h] ∈",3. Learning in the noiseless case,[0],[0]
"Rd×d, C = E[h⊗ h⊗ h] ∈ Rd×d×d.",3. Learning in the noiseless case,[0],[0]
"(8)
Non-degeneracy conditions We assume the following:
(I) H is rigid.
",3. Learning in the noiseless case,[0],[0]
"(II) rank(2C(I, I, ei)− C) =",3. Learning in the noiseless case,[0],[0]
d for all i ∈,3. Learning in the noiseless case,[0],[0]
"[d].
",3. Learning in the noiseless case,[0],[0]
Condition (I) implies that both rank(HH>),3. Learning in the noiseless case,[0],[0]
= d and rank(C) = d. This in turn implies pi = E[hi],3. Learning in the noiseless case,[0],[0]
> 0,3. Learning in the noiseless case,[0],[0]
for all i ∈,3. Learning in the noiseless case,[0],[0]
[d] and that at most one variable hi has pi = 1.,3. Learning in the noiseless case,[0],[0]
"Such an “always on” variable can model a fixed bias to x. As far as we know, condition (II) is new and its nature will become clear shortly.
",3. Learning in the noiseless case,[0],[0]
"We now describe each step of our algorithm in more detail.
",3. Learning in the noiseless case,[0],[0]
"Computing the candidate set To compute a set V that is guaranteed to include the columns of W † we make use of
the second and third order moments of x,
M = E[x⊗ x] ∈ Rm×m, M = E[x⊗ x⊗ x] ∈ Rm×m×m.
(9)
",3. Learning in the noiseless case,[0],[0]
"Given a large number of samples n 1, these can be easily and accurately estimated from the sample X .",3. Learning in the noiseless case,[0],[0]
"For simplicity, in this section we consider the population setting where n → ∞, so M andM are known exactly.",3. Learning in the noiseless case,[0],[0]
"M andM are related to the unknown second and third order moments of h in (8) via (Anandkumar et al., 2014)
M = W>CW, M = C(W,W,W ).",3. Learning in the noiseless case,[0],[0]
"(10)
Since both C and W are full rank, the number of latent units can be deduced by rank(M) =",3. Learning in the noiseless case,[0],[0]
"d. Since C is positive definite, there is a whitening matrix K ∈ Rm×d such that
K>MK = Id. (11)
Such a K can be computed, for example, by an eigendecomposition of M .",3. Learning in the noiseless case,[0],[0]
"Although K is not unique, any K ⊆ span(M) that satisfies (11) suffices for our purpose.",3. Learning in the noiseless case,[0],[0]
"Define the d× d× d lower dimensional whitened tensor
W =M(K,K,K).",3. Learning in the noiseless case,[0],[0]
"(12)
Denote the set of eigenpairs ofW by
U = {(u, λ) ∈",3. Learning in the noiseless case,[0],[0]
"Sd−1 × R+ :W(I,u,u) = λu}.",3. Learning in the noiseless case,[0],[0]
"(13)
Our set of candidates is then
V = {Ku/λ : (u, λ) ∈ U with λ ≥ 1} ⊆ Rm. (14)
",3. Learning in the noiseless case,[0],[0]
The following lemma shows that under condition (I) the set V is guaranteed to contain the d columns of W †.,3. Learning in the noiseless case,[0],[0]
Lemma 1.,3. Learning in the noiseless case,[0],[0]
LetW be the tensor in (12) corresponding to model (1) with σ = 0 and let V be as in (14).,3. Learning in the noiseless case,[0],[0]
If condition (I) holds then W † ⊆ V .,3. Learning in the noiseless case,[0],[0]
"In particular, each (ui, λi) in the set of d relevant eigenpairs
U∗ = {(u, λ) ∈ U : Ku/λ ∈W †} (15)
has the eigenvalue λi = 1/ √ pi ≥ 1 where pi = E[hi] > 0.
",3. Learning in the noiseless case,[0],[0]
"Computing the tensor eigenpairs By Lemma 1, we may construct a candidate set V that contains W † by first calculating the set U of eigenpairs ofW .",3. Learning in the noiseless case,[0],[0]
"Unfortunately, computing the set of all eigenpairs of a general symmetric tensor is computationally hard (Hillar & Lim, 2013).",3. Learning in the noiseless case,[0],[0]
"Moreover, besides the d columns of W †, the set V in (14) may contain many spurious candidates, as the number of eigenpairs of W is typically O(2d)",3. Learning in the noiseless case,[0],[0]
"d (Cartwright & Sturmfels, 2013).
",3. Learning in the noiseless case,[0],[0]
"Nevertheless, as discussed in Section 2, several methods have been proposed for computing some eigenpairs of a tensor under appropriate stability conditions.",3. Learning in the noiseless case,[0],[0]
"The following lemma highlights the importance of condition (II) for the stability of the eigenpairs in U∗. Note that conditions (I)-(II) do not depend on W , but only on the distribution of h.
Lemma 2.",3. Learning in the noiseless case,[0],[0]
Let W be the whitened tensor in (12) corresponding to model (1) with σ = 0.,3. Learning in the noiseless case,[0],[0]
"If conditions (I)-(II) hold, then all (u, λ) ∈ U∗ are Newton-stable eigenpairs ofW .
",3. Learning in the noiseless case,[0],[0]
"Hence, under conditions (I)-(II), the homotopy method in Chen et al. (2016), or alternatively the O–NCM with a sufficiently large number of random initializations (Jaffe et al., 2017), are guaranteed to compute a candidate set V which includes all the columns of W †.",3. Learning in the noiseless case,[0],[0]
"The next step is to extract W † out of V .
",3. Learning in the noiseless case,[0],[0]
Filtering As suggested by Eq.,3. Learning in the noiseless case,[0],[0]
"(7) we select the subset of vectors V̄ ⊆ V that satisfy the binary constraints,
V̄",3. Learning in the noiseless case,[0],[0]
"= {v ∈ V : vTX ∈ {0, 1}n}.",3. Learning in the noiseless case,[0],[0]
"(16)
Indeed, under condition (I), Proposition 1 implies that V̄ = W † and the weight matrix is thus W = V̄ †.
Algorithm 2 in Appendix C summarizes our method for the noiseless case and has the following recovery guarantee.",3. Learning in the noiseless case,[0],[0]
Theorem 1.,3. Learning in the noiseless case,[0],[0]
Let X be a matrix of n samples from model (1) with σ = 0.,3. Learning in the noiseless case,[0],[0]
"If conditions (I)-(II) hold, then the above method recovers W exactly.
",3. Learning in the noiseless case,[0],[0]
"We note that when σ = 0 and conditions (I)-(II) hold for the empirical latent moments Ĉ and Ĉ (rather than C and C), the above procedure exactly recovers W when M and M are replaced by their finite sample estimates.",3. Learning in the noiseless case,[0],[0]
The matrix factorization method SHL in Slawski et al. (2013) also exactly recovers W in the case σ = 0.,3. Learning in the noiseless case,[0],[0]
"While its runtime is also exponential in d, practically it may be much faster than our proposed tensor based approach.",3. Learning in the noiseless case,[0],[0]
"This is because SHL constructs a candidate set of size 2d that can be computed by a suitable linear transformation of the fixed set {0, 1}d, as opposed to our candidate set which is constructed by eigenpairs of a d × d × d tensor.",3. Learning in the noiseless case,[0],[0]
"However, SHL does not take advantage of the large number of samples n, since only m× d sub-matrices of the m×n sample matrix X are used for constructing its candidate set.",3. Learning in the noiseless case,[0],[0]
"Indeed, in the noisy case where σ > 0, SHL has no consistency guarantees and as demonstrated by the simulation results in Section 5 it may fail at high levels of noise.",3. Learning in the noiseless case,[0],[0]
In the next section we derive a modified version of our method that consistently estimates W for any noise level σ ≥ 0.,3. Learning in the noiseless case,[0],[0]
The method in Section 3 to estimateW is clearly inadequate when σ > 0.,4. Learning in the presence of noise,[0],[0]
"However, we now show that by making several adjustments, the two steps of computing the candidate set and its filtering can be both made robust to noise, yielding a consistent estimator of W for any σ ≥ 0.
",4. Learning in the presence of noise,[0],[0]
"Computing the candidate set As in the case σ = 0, our goal in the first step is to compute a finite candidate set
Vσ ⊆ Rm that is guaranteed to contain accurate estimates for the d columns of W †.",4. Learning in the presence of noise,[0],[0]
"To this end, in addition to the second and third order moments M andM in (9), we also consider the first order moment µ = E[x] and define the following noise corrected moments,
Mσ = M − σ2Im,
Mσ = M− σ2 m∑ i=1",4. Learning in the presence of noise,[0],[0]
"( µ⊗ ei ⊗ ei
+ ei ⊗ µ⊗ ei + ei ⊗ ei ⊗ µ ) .
",4. Learning in the presence of noise,[0],[0]
"(17)
By assumption, the noise satisfies E[ξ3i ] = 0.",4. Learning in the presence of noise,[0],[0]
"Thus, similarly to the moment equations in (10), the modified moments in (17) are related to these ofh by (Anandkumar et al., 2014)
Mσ = W >CW, Mσ = C(W,W,W ).",4. Learning in the presence of noise,[0],[0]
"(18)
Hence, if Mσ andMσ were known exactly, a candidate set Vσ that contains W † could be obtained exactly as in the noiseless case, but with M andM replaced with Mσ and Mσ; namely, first calculate the whitening matrix Kσ such that K>σMσKσ =",4. Learning in the presence of noise,[0],[0]
Id,4. Learning in the presence of noise,[0],[0]
"and then compute the eigenpairs of the population whitened tensor
Wσ =Mσ(Kσ,Kσ,Kσ).",4. Learning in the presence of noise,[0],[0]
"(19)
In practice, σ2, d, µ,M andM are all unknown and need to be estimated from the sample matrix X .",4. Learning in the presence of noise,[0],[0]
"Assuming m > d, the parameters σ2 and d can be consistently estimated, for example, by the methods in Kritchman & Nadler (2009).",4. Learning in the presence of noise,[0],[0]
"For simplicity, we assume they are known exactly.",4. Learning in the presence of noise,[0],[0]
"Similarly, µ, M ,M are consistently estimated by their empirical means, µ̂, M̂ , and M̂.",4. Learning in the presence of noise,[0],[0]
"So, after computing the plugin estimates K̂σ such that K̂>σ M̂σK̂σ = Id and Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ), we compute the set Ûσ of eigenpairs of Ŵσ and for some small 0 < τ",4. Learning in the presence of noise,[0],[0]
"= O(n− 1 2 ) take our candidate set as
V̂σ = {K̂σu/λ : (u, λ) ∈ Ûσ with λ ≥ 1−τ}.",4. Learning in the presence of noise,[0],[0]
"(20)
The following lemma shows that under conditions (I)-(II) the above procedure is stable to small perturbations.",4. Learning in the presence of noise,[0],[0]
"Namely, for perturbations of order δ 1 inWσ and Kσ , the method computes a candidate set V̂σ that contains a subset of d vectors that are O(δ) close to the columns of W †.",4. Learning in the presence of noise,[0],[0]
"Furthermore, these d vectors all correspond to Newton-stable eigenpairs of the perturbed tensor and are Ω(1) separated from the other candidates in V̂σ .
",4. Learning in the presence of noise,[0],[0]
Lemma 3.,4. Learning in the presence of noise,[0],[0]
"Let Kσ,Wσ be the population quantities in (19) and let K̂σ, Ŵσ be their perturbed versions, inducing the candidate set V̂σ in (20).",4. Learning in the presence of noise,[0],[0]
"If conditions (I)-(II) hold, then there are c, δ0, δ1 > 0",4. Learning in the presence of noise,[0],[0]
"such that for all 0 ≤ δ ≤ δ0 the following holds: If the perturbed versions satisfy
max{‖Ŵσ −Wσ‖F , ‖K̂σ",4. Learning in the presence of noise,[0],[0]
"−Kσ‖F } ≤ δ, (21)
",4. Learning in the presence of noise,[0],[0]
"then any v∗ ∈W † has a unique v̂ ∈ V̂σ such that
‖v̂ − v∗‖ ≤ cδ.",4. Learning in the presence of noise,[0],[0]
"(22)
",4. Learning in the presence of noise,[0],[0]
"Moreover, v̂ corresponds to a Newton-stable eigenpair of Ŵσ with eigenvalue λ ≥ 1− cδ and for all ṽ ∈ V̂σ \ {v̂},
‖ṽ",4. Learning in the presence of noise,[0],[0]
− v∗‖ ≥ δ1 > 2cδ.,4. Learning in the presence of noise,[0],[0]
"(23)
",4. Learning in the presence of noise,[0],[0]
"The proof is based on the implicit function theorem (Hubbard & Hubbard, 2015); small perturbations to a tensor result in small perturbations to its Newton-stable eigenpairs.
",4. Learning in the presence of noise,[0],[0]
"Now, by the delta method, the plugin estimates K̂σ and Ŵσ are both OP (n− 1 2 ) close to their population quantities,
‖K̂σ",4. Learning in the presence of noise,[0],[0]
"−Kσ‖F = OP (n − 12 ),
‖Ŵσ −Wσ‖F = OP (n − 12 ).
",4. Learning in the presence of noise,[0],[0]
"(24)
By (24), we have that (21) holds with δ = OP (n− 1 2 ).",4. Learning in the presence of noise,[0],[0]
"Hence, by Lemma 3, the eigenpairs of Ŵσ provide a candidate set V̂σ that contains d vectors that are OP (n− 1 2 ) close to the columns of W †.",4. Learning in the presence of noise,[0],[0]
"In addition, any irrelevant candidate is ΩP (1) far away from W †.",4. Learning in the presence of noise,[0],[0]
"As we show next, these properties ensure that with high probability the d relevant candidates can be identified in V̂σ .
",4. Learning in the presence of noise,[0],[0]
"Filtering Given the candidate set V̂σ computed in the first step, our goal now is to find a set V̄σ ⊆ V̂σ of d vectors that accurately estimate the d columns of W †.",4. Learning in the presence of noise,[0],[0]
"To simplify the theoretical analysis, we assume the filtering step is done using a sample X of size n that is independent of V̂σ.",4. Learning in the presence of noise,[0],[0]
"This can be achieved by first splitting a given sample of size 2n into two sets of size n, one for each step.
",4. Learning in the presence of noise,[0],[0]
"Recall that for x from model (1) and any v ∈ Rm,
v>x = v>W>h+",4. Learning in the presence of noise,[0],[0]
"σv>ξ. (25)
",4. Learning in the presence of noise,[0],[0]
"Obviously, when σ > 0, the filtering procedure in (16) for the noiseless case is inadequate, as typically no v∗ ∈ W † will exactly satisfy v∗>X ∈ {0, 1}n.",4. Learning in the presence of noise,[0],[0]
"Nevertheless, we expect that for a sufficiently small noise level σ, any v ∈ V̂σ that is close to some v∗ ∈ W † will result in v>X that is close to being binary, while any v sufficiently far from W † will result in v>X that is far from being binary.",4. Learning in the presence of noise,[0],[0]
"A natural measure for how v>X is “far from being binary”, similar to the one used for filtering in Slawski et al. (2013), is simply its deviation from its binary rounding,
min b∈{0,1}n
‖vTX − b‖2
n‖v‖2 .",4. Learning in the presence of noise,[0],[0]
"(26)
Eq. (26) works extremely well for small σ, but fails for high noise levels.",4. Learning in the presence of noise,[0],[0]
"Here we instead propose a filtering procedure based on the classical Kolmogorov-Smirnov goodness of fit
test (Lehmann & Romano, 2006).",4. Learning in the presence of noise,[0],[0]
"As we show below, this approach gives consistent estimates of W for any σ > 0.
",4. Learning in the presence of noise,[0],[0]
"Before describing the test, we first introduce the probabilistic analogue of the rigidity condition (6).",4. Learning in the presence of noise,[0],[0]
"For any u ∈ Rd, define its corresponding expected binary rounding error,
r(u) = Eh∼Ph",4. Learning in the presence of noise,[0],[0]
"[
min b∈{0,1}
(u>h− b)2 ] .
",4. Learning in the presence of noise,[0],[0]
"Clearly, r(0) = 0 and r(ei) = 0 for all i ∈",4. Learning in the presence of noise,[0],[0]
[d].,4. Learning in the presence of noise,[0],[0]
"We pose the following expected rigidity condition: for all u 6= 0,
r(u)",4. Learning in the presence of noise,[0],[0]
= 0 ⇔ u ∈ {ei}di=1.,4. Learning in the presence of noise,[0],[0]
"(27)
Analogously to the deterministic rigidity condition in (6), condition (27) is satisfied, for example, when Ph(ei) > 0",4. Learning in the presence of noise,[0],[0]
and Ph(ei + ej) > 0,4. Learning in the presence of noise,[0],[0]
for all i 6=,4. Learning in the presence of noise,[0],[0]
j ∈,4. Learning in the presence of noise,[0],[0]
"[d].
To introduce our filtering test, recall that under model (1), ξ ∼ N (0, Im).",4. Learning in the presence of noise,[0],[0]
"Hence, for any fixed v, the random variable v>x in (25) is distributed according to the following univariate Gaussian mixture model (GMM),
v>x ∼ ∑
h∈{0,1}d Ph(h) · N",4. Learning in the presence of noise,[0],[0]
"(v>W>h, σ2‖v‖2).",4. Learning in the presence of noise,[0],[0]
"(28)
",4. Learning in the presence of noise,[0],[0]
Denote the cumulative distribution function of v>x by Fv .,4. Learning in the presence of noise,[0],[0]
"For general v, this mixture may have up to 2d distinct components.",4. Learning in the presence of noise,[0],[0]
"However, for v∗ ∈W †, it reduces to a mixture of two components with means at 0 and 1.",4. Learning in the presence of noise,[0],[0]
"More precisely, for any candidate v with corresponding eigenvalue λ(v)",4. Learning in the presence of noise,[0],[0]
"≥ 1, define the GMM with two components
(1− 1λ(v)2 ) · N (0, σ 2‖v‖2)",4. Learning in the presence of noise,[0],[0]
"+ 1λ(v)2 · N (1, σ 2‖v‖2).",4. Learning in the presence of noise,[0],[0]
"(29)
Denote its cumulative distribution function by Gv.",4. Learning in the presence of noise,[0],[0]
"The following lemma shows that under condition (27), Gv fully characterizes the columns of W †.
Lemma 4.",4. Learning in the presence of noise,[0],[0]
"Let Kσ,Wσ be the population quantities in (19) and let Vσ be the set of population candidates as computed from the eigenpairs of Wσ.",4. Learning in the presence of noise,[0],[0]
"If conditions (I)-(II) and the expected rigidity condition (27) hold, then for any v ∈",4. Learning in the presence of noise,[0],[0]
"Vσ with corresponding eigenvalue λ(v),
Fv = Gv ⇔ v ∈W †.
",4. Learning in the presence of noise,[0],[0]
"Given the empirical candidate set V̂σ, Lemma 4 suggests ranking all v̂ ∈ V̂σ according to their goodness of fit to Gv̂ and taking the d candidates with the best fit.",4. Learning in the presence of noise,[0],[0]
"More precisely, given a sample X =",4. Learning in the presence of noise,[0],[0]
"[x1, . . .",4. Learning in the presence of noise,[0],[0]
",xn] that is independent of V̂σ , for each candidate v̂ ∈ V̂σ we compute the empirical cumulative distribution function, F̂v̂(t) =",4. Learning in the presence of noise,[0],[0]
1n,4. Learning in the presence of noise,[0],[0]
"∑n j=1 1{v̂>xj ≤ t}, t ∈ R, and calculate its Kolmogorov-Smirnov score
∆n(v̂) = sup t∈R |F̂v̂(t)−Gv̂(t)|.",4. Learning in the presence of noise,[0],[0]
"(30)
Algorithm 1 Estimate W when σ > 0 and n <∞",4. Learning in the presence of noise,[0],[0]
"Input: sample matrix X ∈ Rm×n and 0 < τ 1
1: estimate number of hidden units d and noise level σ2 2: compute empirical moments µ̂, M̂ and M̂ and plugin moments M̂σ and M̂σ of (17) 3: compute K̂σ such that K̂>σ M̂σK̂σ = Id 4: construct Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ) 5: compute the set Ûσ of eigenpairs of Ŵσ 6: compute the candidate set V̂σ in (20) 7: for each v̂ ∈ V̂σ compute its KS score ∆n(v̂) in (30) 8: select V̄σ ⊆ V̂σ of d vectors with smallest ∆n(v̂) 9: return the pseudo-inverse Ŵ = V̄ †σ
Our estimator V̄σ ⊆ V̂σ for W † is then the set of d vectors with the smallest scores ∆n(v̂).",4. Learning in the presence of noise,[0],[0]
"The estimator for W is the pseudo-inverse, Ŵ = V̄ †σ .
",4. Learning in the presence of noise,[0],[0]
"The following lemma shows that for sufficiently large n, ∆n(v̂) accurately distinguishes between v̂ ∈ V̂σ that are close to the columns of W † from these that are not.
",4. Learning in the presence of noise,[0],[0]
Lemma 5.,4. Learning in the presence of noise,[0],[0]
"Let v∗ ∈ W † and v̂(1), v̂(2), . . .",4. Learning in the presence of noise,[0],[0]
"a sequence of random vectors such that ‖v̂(n) − v∗‖ = OP (n
− 12 ).",4. Learning in the presence of noise,[0],[0]
"Then, ∆n(v̂(n))",4. Learning in the presence of noise,[0],[0]
= oP (1).,4. Learning in the presence of noise,[0],[0]
"In contrast, if minv∗∈W † ‖v̂(n) − v∗‖ = ΩP (1), then ∆n(v̂(n)) = ΩP (1), provided the expected rigidity condition (27) holds.
",4. Learning in the presence of noise,[0],[0]
"Lemma 5 follows from classical and well studied properties of the Kolmogorov-Smirnov test, see for example Lehmann & Romano (2006); Billingsley (2013).
",4. Learning in the presence of noise,[0],[0]
Algorithm 1 summarizes our method for estimating W in the general case where σ > 0 and n <,4. Learning in the presence of noise,[0],[0]
"∞. The following theorem establishes its consistency.
",4. Learning in the presence of noise,[0],[0]
Theorem 2.,4. Learning in the presence of noise,[0],[0]
"Let x1, . . .",4. Learning in the presence of noise,[0],[0]
",xn be n i.i.d.",4. Learning in the presence of noise,[0],[0]
samples from model (1).,4. Learning in the presence of noise,[0],[0]
"If conditions (I)-(II) and the expected rigidity condition (27) hold, then the estimator Ŵ computed by Algorithm 1 is consistent, achieving the parametric rate,
Ŵ = W +OP (n − 12 ).
",4. Learning in the presence of noise,[0],[0]
Runtime The runtime of Algorithm 1 is composed of three main parts.,4. Learning in the presence of noise,[0],[0]
"First, O(nm3) operations are needed to compute all the relevant moments from the data and to construct the d× d× d whitened tensor Ŵσ .",4. Learning in the presence of noise,[0],[0]
"The most time consuming task is computing the eigenpairs of Ŵσ, which can be done by either the homotopy method or O–NCM.",4. Learning in the presence of noise,[0],[0]
"Currently, no runtime guarantees are available for either of these methods.",4. Learning in the presence of noise,[0],[0]
"In practice, since there are O(2d) eigenpairs, these methods spend O(2d · poly(d)) operations in total.",4. Learning in the presence of noise,[0],[0]
"Finally, since there are O(2d) candidates and each KS test takes O(dn) operations (Gonzalez et al., 1977), the filtering procedure runtime is O(d2dn).
",4. Learning in the presence of noise,[0],[0]
Power-stability and orthogonal decomposition The exponential runtime of our algorithm stems from the fact that the set UN of Newton-stable eigenpairs ofWσ is typically O(2d).,4. Learning in the presence of noise,[0],[0]
"However, in some cases, the set U∗ of d relevant eigenpairs has additional structure so that a smaller candidate set may be computed instead of UN .",4. Learning in the presence of noise,[0],[0]
Consider the subset UP ⊆ UN of power-stable eigenpairs ofWσ:,4. Learning in the presence of noise,[0],[0]
Definition 3.,4. Learning in the presence of noise,[0],[0]
"An eigenpair (u, λ) is power-stable if its projected Jacobian Jp(u) is either positive or negative definite.
",4. Learning in the presence of noise,[0],[0]
"Typically, the number of power-stable eigenpairs is significantly smaller than the number of Newton-stable eigenpairs.2 In addition, UP can be computed by the shifted higher-order power method (Kolda & Mayo, 2011; 2014).
",4. Learning in the presence of noise,[0],[0]
"Similarly to Lemma 2, one can show that UP is guaranteed to contain U∗ whenever the following stronger version of condition (II) holds: for all (ui, λi) ∈ U∗, the matrix
(WKLui) >(2C(I, I, ei)− C)(WKLui) (31)
is either positive-definite or negative-definite.
",4. Learning in the presence of noise,[0],[0]
"As an example, consider the case where Ph has the support h ∈",4. Learning in the presence of noise,[0],[0]
Id.,4. Learning in the presence of noise,[0],[0]
Then model (1) corresponds to a GMM with d spherical components with linearly independent means.,4. Learning in the presence of noise,[0],[0]
"In this case, bothC and C are diagonal with p on their diagonal.",4. Learning in the presence of noise,[0],[0]
"Thus, the matrices in (31) take the form −L>ei diag(p)Lei , which are all negative-definite when p > 0.",4. Learning in the presence of noise,[0],[0]
"In fact, in this case, Wσ has an orthogonal CP decomposition and the d orthogonal eigenpairs in U∗ are the only negative-definite power-stable eigenpairs ofWσ (Anandkumar et al., 2014).",4. Learning in the presence of noise,[0],[0]
"Similarly, when Ph is a product distribution, the same orthogonal structure appears if the centered moments of x are used instead of M andM. As shown in Anandkumar et al. (2014), the power method, accompanied with a deflation procedure, decomposes an orthogonal tensor in polynomial time, thus implying an efficient algorithm in these cases.",4. Learning in the presence of noise,[0],[0]
"However, under the much weaker conditions we pose on Ph, the relevant eigenpairs in U∗ are not necessarily powerstable and the CP decomposition ofWσ does not necessarily include U∗.",4. Learning in the presence of noise,[0],[0]
We demonstrate our method in three scenarios: (I) simulations from the exact binary model (1); (II) learning a common population genetic admixture model; (III) learning the proportion matrix of a cell mixture from DNA methylation levels.,5. Experiments,[0],[0]
"Due to lack of space, (III) is deferred to Appendix N. Code to reproduce the simulation results can be found at https://github.com/arJaffe/ BinaryLatentVariables.
",5. Experiments,[0],[0]
2We currently do not know whether the number of power-stable eigenpairs of a generic tensor is polynomial or exponential in d.,5. Experiments,[0],[0]
"We generated n samples from model (1) with d = 6 hidden units, m = 30 observable features, and Gaussian noise ξ ∼ N (0, Im).",5.1. Simulations,[0],[0]
The m columns of W were drawn uniformly from the unit sphere Sd−1.,5.1. Simulations,[0],[0]
"Fixing a mean vector a ∈ Rd and a covariance matrix R ∈ Rd×d, each hidden vector h was generated independently by first drawing r ∼ N (a, R) and then taking its binary rounding.
",5.1. Simulations,[0],[0]
"Figure 2 shows the error, in Frobenius norm, averaged over 50 independent realizations of X as a function of n (upper panel) and σ (lower panel) for 5 methods: (i) our spectral approach, Algorithm 1 (Spectral); (ii) Algorithm 1 followed by a single weighted least squares step (Appendix K) (Spectral+WLS); (iii) SHL, the matrix decomposition method of Slawski et al. (2013)3; (iv) ALS with a random initialization (Appendix L); and (v) an oracle estimator that is given the exact matrix H and computes W via least squares.
",5.1. Simulations,[0],[0]
"As one can see, as opposed to SHL, our method is consistent for σ > 0 and achieves an error rate O(n−",5.1. Simulations,[0],[0]
1 2 ) corresponding to a slope of −1 in the upper panel of Fig. 2.,5.1. Simulations,[0],[0]
"In addition, as seen in the lower panel of Fig. 2, at low levels of noise our method is comparable to SHL, whereas at high levels it is far more accurate.",5.1. Simulations,[0],[0]
"Finally, adding a weighted least squares step reduces the error for low noise levels, but increases the error
3Code from https://sites.google.com/site/ slawskimartin/code.",5.1. Simulations,[0],[0]
"For each realization X , we made 50 runs of SHL and chose H,W minimizing",5.1. Simulations,[0],[0]
‖X −W>,5.1. Simulations,[0],[0]
"H‖F .
for high noise levels.",5.1. Simulations,[0],[0]
A comparison between the runtime of SHL and the spectral method appears in Appendix I.,5.1. Simulations,[0],[0]
"We present an application of our method to a fundamental problem in population genetics, known as admixture (see Fig. 3).",5.2. Population genetic admixture,[0],[0]
"Admixture refers to the mixing of d ≥ 2 ancestral populations that were long separated, e.g., due to geographical or cultural barriers (Pritchard et al., 2000; Alexander et al., 2009; Li et al., 2008).",5.2. Population genetic admixture,[0],[0]
"The observed data X is an m× n matrix where m is the number of modern “admixed” individuals and n is the number of relevant locations in their DNA, known as SNPs.",5.2. Population genetic admixture,[0],[0]
Each SNP corresponds to two alleles and individuals may have different alleles.,5.2. Population genetic admixture,[0],[0]
"Fixing a reference allele for each location, Xij takes values in {0, 12 , 1} according to the number of reference alleles appearing in the genotype of individual",5.2. Population genetic admixture,[0],[0]
i ∈,5.2. Population genetic admixture,[0],[0]
[m] at locus j ∈,5.2. Population genetic admixture,[0],[0]
"[n].
",5.2. Population genetic admixture,[0],[0]
"Given the genotypes X , an important problem in population genetics is to estimate the following two quantities.",5.2. Population genetic admixture,[0],[0]
The allele frequency matrix H ∈,5.2. Population genetic admixture,[0],[0]
"[0, 1]d×n whose entry Hkj is the frequency of the reference allele at locus j ∈",5.2. Population genetic admixture,[0],[0]
[n] in ancestral population k ∈,5.2. Population genetic admixture,[0],[0]
[d]; and the admixture proportion matrix W ∈,5.2. Population genetic admixture,[0],[0]
"[0, 1]d×m whose columns sum to 1 and its entry Wki is the proportion of individual",5.2. Population genetic admixture,[0],[0]
"i’s genome that was inherited from population k.
A common model for X in terms of W and H is to assume that the number of alleles 2Xij ∈ {0, 1, 2} is the sum of two i.i.d.",5.2. Population genetic admixture,[0],[0]
"Bernoulli random variables with success probability Fij = ∑d k=1WkiHkj , namely, Xij |H ∼ 1 2 · Binomial(2, Fij).",5.2. Population genetic admixture,[0],[0]
"Note that under this model
E[X|H] = F = W>H. (32)
",5.2. Population genetic admixture,[0],[0]
"Although (32) has similar form to model (1), there are two main differences; the noise is not normally distributed and the matrix H is non-binary.",5.2. Population genetic admixture,[0],[0]
"Yet, the binary model (1) serves as a good approximation whenever various alleles are rare in some populations but abundant in others.",5.2. Population genetic admixture,[0],[0]
"Specifically, for ancestral populations that have been long separated, some alleles may become fixed in one population (i.e., reach frequency of 1) while being totally absent in others.
",5.2. Population genetic admixture,[0],[0]
"Simulating genetic admixture We followed a standard simulation scheme apllied, for example, in Xue et al. (2017); Gravel (2012); Price et al. (2009).",5.2. Population genetic admixture,[0],[0]
"First, using SCRM (Staab et al., 2015), we simulated d = 3 ancestral populations separated for 4000 generations and generated the genomes of 40 individuals for each.",5.2. Population genetic admixture,[0],[0]
H was then computed as the frequency of the reference alleles in each population.,5.2. Population genetic admixture,[0],[0]
"Next, the columns of W were sampled from a symmetric Dirichlet distribution with parameter α ≥ 0.",5.2. Population genetic admixture,[0],[0]
"Finally, the genomes of m = 50 admixed individuals were generated as mosaics of genomic segments of individuals from the ancestral populations with proportions W .",5.2. Population genetic admixture,[0],[0]
"The mosaic nature of the admixed genomes is an important realistic detail, due to the linkage (correlation) between SNPs (Xue et al., 2017).",5.2. Population genetic admixture,[0],[0]
"A detailed description is in Appendix M.
We compare our algorithm to two methods.",5.2. Population genetic admixture,[0],[0]
"The first is Admixture (Alexander et al., 2009), one of the most widely used algorithms in population genetics, which aims to maximize the likelihood of X .",5.2. Population genetic admixture,[0],[0]
"The second is the recently proposed spectral method ALStructure (Cabreros & Storey, 2017), where an estimation of span(W>) via Chen & Storey (2015) is followed by constrained ALS iterations of W and H .",5.2. Population genetic admixture,[0],[0]
"For our method, two modification are needed for Algorithm 1.",5.2. Population genetic admixture,[0],[0]
"First, since the distribution ofXij−wTi hj is not Gaussian, the corrected moments M̂σ,M̂σ as calculated by (17) do not satisfy (18).",5.2. Population genetic admixture,[0],[0]
"Instead, we implemented a matrix completion algorithm derived in (Jain & Oh, 2014) for a similar setup, see Appendix J for more details.",5.2. Population genetic admixture,[0],[0]
"In addition, the filtering process described in Section 4 is no longer valid.",5.2. Population genetic admixture,[0],[0]
"However, as d is relatively small, we performed exhaustive search over all candidate subsets of size d and choose the one that maximized the likelihood.
",5.2. Population genetic admixture,[0],[0]
"Figure 4 compares the results of the 3 methods for α = 0.1, 1, 10.",5.2. Population genetic admixture,[0],[0]
"The spectral method outperforms Admixture and ALStructure for α = 1, 10 and performs similarly to Admixture for α = 0.1.
",5.2. Population genetic admixture,[0],[0]
Acknowledgements This research was funded in part by NIH Grant 1R01HG008383-01A1.,5.2. Population genetic admixture,[0],[0]
Uniqueness of the factorization readily follows from (7) so we proceed to prove (7).,A. Proof of Proposition 1,[0],[0]
First note that span(X) = span(W>) =,A. Proof of Proposition 1,[0],[0]
span(W †).,A. Proof of Proposition 1,[0],[0]
"Since W is full rank, we have WW † = Id.",A. Proof of Proposition 1,[0],[0]
"Hence,
(W †)>X = (WW †)>H = H ∈ {0, 1}d×n.
",A. Proof of Proposition 1,[0],[0]
So any v∗ ∈ W † satisfies the binary constraint,A. Proof of Proposition 1,[0],[0]
"v∗>X ∈ {0, 1}n.",A. Proof of Proposition 1,[0],[0]
"For the other direction, let v ∈ span(X)",A. Proof of Proposition 1,[0],[0]
"\ {0} be such that v>X ∈ {0, 1}n.",A. Proof of Proposition 1,[0],[0]
"Since v>X = (Wv)>H , the rigidity condition (6) implies Wv ∈ {ei}di=1.",A. Proof of Proposition 1,[0],[0]
"Since W is full rank and v ∈ span(W †), v must be a column of W †.",A. Proof of Proposition 1,[0],[0]
"Since the vector h is binary, its second and third order moments are related as follows.",B. Proof of Lemma 1,[0],[0]
"For all i, j ∈",B. Proof of Lemma 1,[0],[0]
"[d],
Ciij = Ciji = Cjii = E[h2ihj",B. Proof of Lemma 1,[0],[0]
] = E[hihj ] = Cij .,B. Proof of Lemma 1,[0],[0]
"(33)
",B. Proof of Lemma 1,[0],[0]
"Since W is full rank, WW † = Id.",B. Proof of Lemma 1,[0],[0]
"Hence, applying W † multi-linearly on the moment equations in (10) we obtain
C =",B. Proof of Lemma 1,[0],[0]
"(W †)>MW †,
C = M(W †,W †,W †).
",B. Proof of Lemma 1,[0],[0]
"Thus, the equality in (33) is equivalent to
[M(W †,W †,W †)]iij =",B. Proof of Lemma 1,[0],[0]
[(W †)>MW †]ij .,B. Proof of Lemma 1,[0],[0]
"(34)
Let Y ∗ ∈ Rd×d be the full rank matrix that satisfies W † = KY ∗ where K is the whitening matrix in (11).",B. Proof of Lemma 1,[0],[0]
"Then,
M(W †,W †,W †) = M(KY ∗,KY ∗,KY ∗) = W(Y ∗, Y ∗, Y ∗)
whereW is the whitened tensor in (12).",B. Proof of Lemma 1,[0],[0]
"Similarly, by (11),
(W †)>MW † = (Y ∗)>(KTMK)(Y ∗) =",B. Proof of Lemma 1,[0],[0]
"(Y ∗)>Y ∗.
Inserting these into (34), the matrix Y ∗ must satisfy
[W(Y ∗, Y ∗, Y ∗)]iij =",B. Proof of Lemma 1,[0],[0]
"[(Y ∗)>(Y ∗)]ij , ∀i, j ∈",B. Proof of Lemma 1,[0],[0]
[d].,B. Proof of Lemma 1,[0],[0]
"(35)
The following lemma, proved in Appendix H, shows that Eq. (35) is nothing but a tensor eigen-problem.",B. Proof of Lemma 1,[0],[0]
"Specifically, the columns of Y ∗, up to scaling, are eigenvectors ofW .",B. Proof of Lemma 1,[0],[0]
Lemma 6.,B. Proof of Lemma 1,[0],[0]
Let W ∈ Rd×d×d be an arbitrary symmetric tensor.,B. Proof of Lemma 1,[0],[0]
"Then, a matrix Y =",B. Proof of Lemma 1,[0],[0]
"[y1, . . .",B. Proof of Lemma 1,[0],[0]
",yd] ∈ Rd×d of rank d satisfies (35) if and only if for all k ∈",B. Proof of Lemma 1,[0],[0]
"[d], yk = uk/λk, where (uk, λk)dk=1 are d eigenpairs of W with linearly independent {uk}dk=1.
",B. Proof of Lemma 1,[0],[0]
"By Lemma 6, the set of scaled eigenpairs {y = u/λ} of W is guaranteed to contain the d columns of Y ∗. Since W † = KY ∗, the set {Ky} is guaranteed to contain W †.
To show that each y = u/λ",B. Proof of Lemma 1,[0],[0]
∈,B. Proof of Lemma 1,[0],[0]
"Y ∗ has λ ≥ 1, note that the vector Ky is a column of W †, so WKy = ei for some i ∈",B. Proof of Lemma 1,[0],[0]
[d].,B. Proof of Lemma 1,[0],[0]
"Hence, by the definition of the whitened tensor (12) and the moment equation (10),
W(y,y,y) = M(Ky,Ky,Ky) = C(WKy,WKy,WKy) = C(ei, ei, ei) = Ciii = E[hi] ≤ 1.
",B. Proof of Lemma 1,[0],[0]
"On the other hand, since (u, λ) is an eigenpair ofW with eigenvalue λ =W(u,u,u),
W(y,y,y) = 1 λ3 W(u,u,u) = 1 λ2 .
",B. Proof of Lemma 1,[0],[0]
"By convention, λ ≥ 0.",B. Proof of Lemma 1,[0],[0]
"Hence, λ = 1/ √ E[hi] ≥ 1,
concluding the proof.",B. Proof of Lemma 1,[0],[0]
Algorithm 2 Recover W when σ = 0,C. Recovery algorithm - noiseless case,[0],[0]
"Input: sample matrix X
1: estimate second and third order moments M ,M 2: set d = rank(M) 3: compute K ⊆ span(M) such that K>MK",C. Recovery algorithm - noiseless case,[0],[0]
=,C. Recovery algorithm - noiseless case,[0],[0]
"Id 4: compute whitened tensorW =M(K,K,K) 5: compute the set U of eigenpairs ofW 6: compute the candidate set V in (14) 7: filter V̄ = {v ∈ V : v>X ∈ {0, 1}n} 8: return the pseudo-inverse W = V̄ †",C. Recovery algorithm - noiseless case,[0],[0]
"Let (u, λ) ∈ U∗ be an eigenpair of W such that v∗ = Ku/λ ∈W †.",D. Proof of Lemma 2,[0],[0]
"To show Newton-stability we need to show that under conditions (I)-(II) the projected Jacobian matrix Jp(u) = L > u∇g(u)Lu in (4) is full rank d− 1.
",D. Proof of Lemma 2,[0],[0]
"The Jacobian matrix∇g(u) is
∇g(u) = 2W(I, I,u)− 3uW(I,u,u)>
−W(u,u,u)Id = 2W(I, I,u)− 3λuu>",D. Proof of Lemma 2,[0],[0]
− λId.,D. Proof of Lemma 2,[0],[0]
"(36)
Since L>uu = 0, the second term in (36) does not contribute to Jp(u).",D. Proof of Lemma 2,[0],[0]
"For the first term in (36), by (12) and (10),
W(I, I,u) =M(K,K,Ku) = C(WK,WK,WKu).
",D. Proof of Lemma 2,[0],[0]
"Since v∗ = Ku/λ is a column of W †, WKu =",D. Proof of Lemma 2,[0],[0]
λei for some,D. Proof of Lemma 2,[0],[0]
i ∈,D. Proof of Lemma 2,[0],[0]
[d].,D. Proof of Lemma 2,[0],[0]
"Thus,
W(I, I,u) = λC(WK,WK, ei) = λK>W>C(I, I, ei)WK.
",D. Proof of Lemma 2,[0],[0]
"For the third term in (36), by the definition of K in (11),
Id = K >MK = K>W>CWK.
Putting the last two equalities in (36) and applying the projection Lu we obtain
Jp(u) = L > u∇g(u)Lu
= λL>uK >W>(2C(I, I, ei)− C)WKLu.
",D. Proof of Lemma 2,[0],[0]
"Since λ ≥ 1 and W and K are full rank, condition (II) implies that Jp(u) is full rank as well.",D. Proof of Lemma 2,[0],[0]
"Thus, (u, λ) is a Newton-stable eigenpair ofW .",D. Proof of Lemma 2,[0],[0]
Lemma 3 follows from the following lemma which establishes the stability of Newton-stable eigenpairs of a tensor W to small perturbations W̃ =W + ∆W .,E. Proof of Lemma 3,[0],[0]
Lemma 7.,E. Proof of Lemma 3,[0],[0]
"Let (u, λ) be a Newton-stable eigenpair ofW with λ ≥ 1.",E. Proof of Lemma 3,[0],[0]
"There are c1, c2, ε0 > 0 such that for all sufficiently small ε > 0",E. Proof of Lemma 3,[0],[0]
the following holds.,E. Proof of Lemma 3,[0],[0]
"For any W̃ such that ‖W̃ −W‖F ≤ ε there exists a unique eigenpair (ũ, λ̃) of W̃ such that
‖u− ũ‖ ≤ c1ε and |λ̃− λ| ≤ c2ε.
",E. Proof of Lemma 3,[0],[0]
"In addition, (ũ, λ̃) is Newton-stable and any other eigenvector ṽ of W̃ satisfies ‖ṽ",E. Proof of Lemma 3,[0],[0]
"− u‖ ≥ ε0.
",E. Proof of Lemma 3,[0],[0]
Proof of Lemma 7.,E. Proof of Lemma 3,[0],[0]
For a tensor T ∈ Rd×d×d let t ∈ Rs be the vector of s = d3 entries {Tijk}.,E. Proof of Lemma 3,[0],[0]
"Define the function Q : Rd+s → Rd by
Q(v, t)",E. Proof of Lemma 3,[0],[0]
= T,E. Proof of Lemma 3,[0],[0]
"(I,v,v)− T (v,v,v) ·",E. Proof of Lemma 3,[0],[0]
"v.
Note that for any t ∈ Rs and (v, β) ∈ Rd × R with v 6= 0 and β 6= 0, we have thatQ(v, t) = 0",E. Proof of Lemma 3,[0],[0]
"if and only if (v, β) is an eigenpair of t with eigenvalue β",E. Proof of Lemma 3,[0],[0]
"= T (v,v,v).4 Denote the gradients ofQ with respect to v and t by
A(v, t) = ∇vQ(v, t) ∈ Rd×d, B(v, t) = ∇tQ(v, t) ∈ Rd×s.
",E. Proof of Lemma 3,[0],[0]
"Let w ∈ Rs be the vectorization of W and let (u, λ) ∈",E. Proof of Lemma 3,[0],[0]
Sd−1 × R+ be a Newton-stable eigenpair of w with λ ≥ 1.,E. Proof of Lemma 3,[0],[0]
Since u is Newton-stable and λ,E. Proof of Lemma 3,[0],[0]
"> 0, A(u,w) is invertible.",E. Proof of Lemma 3,[0],[0]
"In addition, the following (d+ s)× (d+ s) matrix is invertible,
D(u,w) =
( A(u,w) B(u,w)
0",E. Proof of Lemma 3,[0],[0]
"Is
) .
",E. Proof of Lemma 3,[0],[0]
"4This does not precisely hold when β = 0 since Q(v, t) = 0 does not imply ‖v‖ = 1 in this case, but only that v is proportional to an eigenvector.
",E. Proof of Lemma 3,[0],[0]
"Let γD = 1/‖D(u,w)−1‖ > 0 be the smallest singular value of D(u,w) and let LD < ∞ be the Lipschitz constant of ∇Q(v, t) =",E. Proof of Lemma 3,[0],[0]
"[A(v, t), B(v, t)] ∈ Rd×(d+s) in a small neighborhood of (u,w), namely, ∀(v, t), (ṽ, t̃) in the neighborhood,
‖∇Q(v, t)−∇Q(ṽ, t̃)‖ ≤ LD‖(v, t)− (ṽ, t̃)‖.
Let Bε(w) ⊂ Rs be the ball of radius ε centered at w.",E. Proof of Lemma 3,[0],[0]
"Then by the implicit function theorem (Hubbard & Hubbard, 2015), for any ε ≤ ε1 := γ2D/(2LD), there exists a unique continuously differentiable mapping ũ : Bε(w)→ B2ε/γD (u) such that Q(ũ(w̃), w̃) = 0 for all w̃ ∈ Bε(w).",E. Proof of Lemma 3,[0],[0]
"In other words, for any w̃ such that ‖w̃ −w‖ ≤ ε, there exist a unique vector ũ in all B2ε/γD (u) that is an eigenpair of w̃. Equivalently, for W̃ such that ‖W̃ −W‖F ≤ ε, there exists a unique eigenvector ũ of W̃ such that
‖ũ− u‖ ≤ 2ε/γD",E. Proof of Lemma 3,[0],[0]
:= c1ε.,E. Proof of Lemma 3,[0],[0]
"(37)
",E. Proof of Lemma 3,[0],[0]
The bound on |λ̃ − λ| readily follows from (37).,E. Proof of Lemma 3,[0],[0]
"Indeed, let q : Rd+s → R be q(v, t) = T (v,v,v) and let Lλ be the Lipschitz constant of q in the neighborhood of (u,w).",E. Proof of Lemma 3,[0],[0]
"Then,
|λ̃− λ| = |q(ũ, w̃)− q(u,w)| ≤ Lλ √ ‖ũ− u‖2 + ‖w̃ −w‖2
≤",E. Proof of Lemma 3,[0],[0]
"Lλ √ 2
γD + 1 · ε",E. Proof of Lemma 3,[0],[0]
":= c2ε.
",E. Proof of Lemma 3,[0],[0]
"As for the Newton-stability of ũ, let r : Rd+s → R+ be r(v, t) = 1/‖A(v, t)−1‖, the minimal singular value of A(v, t).",E. Proof of Lemma 3,[0],[0]
"Since (u, λ) is a Newton-stable eigenpair of w, ∃γA > 0",E. Proof of Lemma 3,[0],[0]
"such that r(u,w) ≥ γA.",E. Proof of Lemma 3,[0],[0]
"Let Lγ be the Lipschitz constant of r(v, t) in the neighborhood (Golub & Van Loan, 2012).",E. Proof of Lemma 3,[0],[0]
"Then, for ε ≤ ε2 := γ/(2Lγ), we have r(ũ, w̃) ≥",E. Proof of Lemma 3,[0],[0]
"γA/2 > 0, so (ũ, λ̃) is a Newton-stable eigenpair of w̃.
Finally, we show that any other eigenvector ṽ of W̃ is apart from u. Since ũ is Newton-stable, there exists ε0 > 0",E. Proof of Lemma 3,[0],[0]
such that ‖ṽ,E. Proof of Lemma 3,[0],[0]
"− ũ‖ ≥ 2ε0 for any other eigenvector ṽ. Hence, for ε ≤ ε0,
‖ṽ",E. Proof of Lemma 3,[0],[0]
− u‖ ≥ ∣∣‖ṽ − ũ‖ − ‖ũ−,E. Proof of Lemma 3,[0],[0]
u‖∣∣ ≥ ‖ṽ,E. Proof of Lemma 3,[0],[0]
"− ũ‖ − ε ≥ ε0.
",E. Proof of Lemma 3,[0],[0]
"Taking ε ≤ min{ε0, ε1, ε2} and c1, c2, ε0 as above concludes the proof of the lemma.
",E. Proof of Lemma 3,[0],[0]
"Lastly, for completeness, we show that γD ≥ γA√ γ2A+d .
γ−1D = ‖D(u,w) −1‖",E. Proof of Lemma 3,[0],[0]
"≤ √ ‖A(u,w)−1‖2(1 + ‖B(u,w)‖2) + ‖Is‖2
≤ √ 1 + 1 + ‖B(v,w)‖2
γ2A .",E. Proof of Lemma 3,[0],[0]
"(38)
",E. Proof of Lemma 3,[0],[0]
"To bound ‖B(u,w)‖, note thatQ(u,w) is linear in w and its i-th entry is given by
[Q(u,w)]i = ∑ k,l wiklukul",E. Proof of Lemma 3,[0],[0]
"− ( ∑ j,k,l wjklujukul)ui.
",E. Proof of Lemma 3,[0],[0]
"Thus, the d×m matrix B(u,w) has entries
[B(u,w)]i,(jkl) =",E. Proof of Lemma 3,[0],[0]
"[∇wQ(u,w)]i,(jkl) = (δij−uiuj)ukul,
which is independent of w. Recalling that ‖u‖ = 1,
‖B(u)‖2 ≤",E. Proof of Lemma 3,[0],[0]
"‖B(u)‖2F = d∑
i,j,k,l=1
(δij",E. Proof of Lemma 3,[0],[0]
"− uiuj)2u2ku2l
= d∑",E. Proof of Lemma 3,[0],[0]
"i,j=1 (δ2ij − 2δijuiuj + u2iu2j ) = d− 1.
Putting this bound in (38), we obtain γD ≥ γA√ γ2A+d .",E. Proof of Lemma 3,[0],[0]
Let v∗ ∈ W †.,F. Proof of Lemma 4,[0],[0]
Then ∃i ∈,F. Proof of Lemma 4,[0],[0]
"[d] such that v∗>W>h = hi ∈ {0, 1}.",F. Proof of Lemma 4,[0],[0]
"Hence, by (28), the c.d.f.",F. Proof of Lemma 4,[0],[0]
"Fv∗ of v∗>x corresponds to the two component GMM
(1− pi) · N (0, σ2‖v∗‖2) + pi · N (1, σ2‖v∗‖2).
",F. Proof of Lemma 4,[0],[0]
By Lemma 1 we have pi = 1/λ(v∗)2.,F. Proof of Lemma 4,[0],[0]
"Thus, Fv∗ = Gv∗ .
",F. Proof of Lemma 4,[0],[0]
"For the other direction, let v ∈",F. Proof of Lemma 4,[0],[0]
Vσ \W †.,F. Proof of Lemma 4,[0],[0]
"Since W is full rank, the d-dimensional vector u> = v>W> /∈",F. Proof of Lemma 4,[0],[0]
{e>i }di=1.,F. Proof of Lemma 4,[0],[0]
"Moreover, by Eq. (23) of Lemma 3,
inf v∈Vσ\W † min v∗∈W †
‖v − v∗‖ ≥ δ1 > 0.
",F. Proof of Lemma 4,[0],[0]
"Hence, there exists ε0 > 0",F. Proof of Lemma 4,[0],[0]
"such that
min i∈[d] ‖u− ei‖ ≥ ε0.
",F. Proof of Lemma 4,[0],[0]
"So by the expected rigidity condition (27), there exists η0 > 0",F. Proof of Lemma 4,[0],[0]
such that r(u) ≥ η0.,F. Proof of Lemma 4,[0],[0]
It follows that Fv has a component with mean that is bounded away from both 0 and 1 and thus Fv 6=,F. Proof of Lemma 4,[0],[0]
Gv .,F. Proof of Lemma 4,[0],[0]
"In particular, there exists η1 > 0 such that
sup t∈R |Fv(t)−Gv(t)| ≥ η1.",F. Proof of Lemma 4,[0],[0]
Recall that our sample of size 2n was split into two separate parts each of size,G. Proof of Lemma 5,[0],[0]
"n. The first n samples were used to estimate the tensor eigenvectors, and the last n samples to estimate the empirical cdf’s of their projections onto the eigenvectors.
",G. Proof of Lemma 5,[0],[0]
"For any v̂ that is close to a vector v, we bound ∆n(v̂) = ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
−Gv̂‖∞,G. Proof of Lemma 5,[0],[0]
"by the triangle inequality,
‖F̂v̂ −Gv̂‖∞ ≤ ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
− Fv̂‖∞ + ‖Fv̂,G. Proof of Lemma 5,[0],[0]
− Fv‖∞ (39) + ‖Fv −Gv‖∞ + ‖Gv,G. Proof of Lemma 5,[0],[0]
"−Gv̂‖∞.
We now consider each of the four terms separately, starting with the first one.",G. Proof of Lemma 5,[0],[0]
"Since σ > 0, the cdf Fv̂ : R → [0, 1] is continuous and the distribution of ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
"− Fv̂‖∞ is independent of v̂. Then, by the Dvoretzky-Kiefer-Wolfowitz inequality, ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
− Fv̂‖∞ is w.h.p.,G. Proof of Lemma 5,[0],[0]
"of order O(1/ √ n) for any v̂, and in particular tends to zero as n→ 0.
",G. Proof of Lemma 5,[0],[0]
"As for the second term, write v̂ = v + η.",G. Proof of Lemma 5,[0],[0]
"Then,
v̂>x = v>x+ η>x.
Recall that x = W>h + σξ.",G. Proof of Lemma 5,[0],[0]
"Hence, |η>x| ≤",G. Proof of Lemma 5,[0],[0]
‖W‖2 √ d‖η‖ + σ|η>ξ|.,G. Proof of Lemma 5,[0],[0]
The term η>ξ is simply a zero mean Gaussian random variable with standard deviation σ‖η‖.,G. Proof of Lemma 5,[0],[0]
"So, there exists",G. Proof of Lemma 5,[0],[0]
Kn > √ d‖W‖2 +,G. Proof of Lemma 5,[0],[0]
"σn1/3 such that with probability tending to one as n→∞, for all n samples xj ∈ X , |η>xj | ≤ Kn‖η‖.",G. Proof of Lemma 5,[0],[0]
"Thus, |v̂>x − v>x| can be bounded by Kn‖v̂ − v‖.",G. Proof of Lemma 5,[0],[0]
"This, in turn, implies that
‖Fv̂ − Fv‖∞ ≤ LKn‖v̂",G. Proof of Lemma 5,[0],[0]
"− v‖,
where L = maxt F ′v(t), which is finite for any σ > 0.",G. Proof of Lemma 5,[0],[0]
"Now, suppose the sequence v̂(n) converges to some v at rate OP (1/ √ n).",G. Proof of Lemma 5,[0],[0]
"Since Kn grows much more slowly with n, this term tends to zero.
",G. Proof of Lemma 5,[0],[0]
"Let us next consider the fourth term, and leave the third term to the end.",G. Proof of Lemma 5,[0],[0]
Here note that Gv is continuous in its parameter v.,G. Proof of Lemma 5,[0],[0]
"So if the sequence v̂(n) converges to some v, then this term tends to zero.
",G. Proof of Lemma 5,[0],[0]
"Finally, consider the third term.",G. Proof of Lemma 5,[0],[0]
"If the limiting vector v belongs to the correct set, namely v∗ ∈W †, then Fv = Gv , and thus overall ‖F̂v̂ −Gv̂‖∞ tends to zero as required.
",G. Proof of Lemma 5,[0],[0]
"In contrast, if v̂ converges to a vector v /∈W †, then instead of Eq.",G. Proof of Lemma 5,[0],[0]
"(39) we invoke the following inequality:
‖F̂v̂ −Gv̂‖∞ ≥ ‖Fv −Gv‖∞",G. Proof of Lemma 5,[0],[0]
− ‖Fv − Fv̂‖∞ −‖Fv̂ − F̂v̂‖∞,G. Proof of Lemma 5,[0],[0]
"− ‖Gv̂ −Gv‖∞.
Here ‖Fv −Gv‖∞ is strictly larger than zero whereas the three other remaining terms tend to zero as n → ∞ as above.",G. Proof of Lemma 5,[0],[0]
Multiplying (35) from the right by the full rank matrix Y −1,H. Proof of Lemma 6,[0],[0]
"we obtain the equations
[W(Y, Y, I)]iij =",H. Proof of Lemma 6,[0],[0]
"[Y >]ij , ∀i, j ∈",H. Proof of Lemma 6,[0],[0]
"[d].
Note that for all i ∈",H. Proof of Lemma 6,[0],[0]
"[d],
[W(Y, Y, I)]iij =",H. Proof of Lemma 6,[0],[0]
"[W(yi,yi, I)]j .
SinceW is symmetric, we thus have
W(I,yi,yi) = yi, ∀i ∈",H. Proof of Lemma 6,[0],[0]
"[d].
Writing yi = ui/λi we obtain the eigenpair equation
W(I,ui,ui) = λiui, ∀i ∈",H. Proof of Lemma 6,[0],[0]
"[d].
",H. Proof of Lemma 6,[0],[0]
The other direction readily follows from the definition of eigenpairs.,H. Proof of Lemma 6,[0],[0]
Figure 5 shows the simulation runtime of the spectral approach and that of SHL vs. the number of samples n.,I. Simulation runtime,[0],[0]
The setup is similar to the one described in section 5.,I. Simulation runtime,[0],[0]
"The runtime of SHL increases linearly with n, as expected.",I. Simulation runtime,[0],[0]
"For our spectral method, for lower values of n the dominant factor is the computation of tensor eigenvectors, which does not depend on n. For large n, the dominant factor is the computation of the correlation tensor, linear in n.",I. Simulation runtime,[0],[0]
"In Algorithm 1, we modify the diagonal elements of M,M by (17).",J. Matrix and tensor denoising,[0],[0]
"This modification is suited for additive Gaussian noise, but is not applicable for the case where X = binomial(2,WTH).",J. Matrix and tensor denoising,[0],[0]
"Instead, we implemented a method derived in (Jain & Oh, 2014) for a similar setup.
",J. Matrix and tensor denoising,[0],[0]
"First, we treat the diagonal elements of Mσ as missing data, and complete them with the following iterative steps.",J. Matrix and tensor denoising,[0],[0]
"(i) compute the first d eigenpairs {vi, λi} of R(k); and (ii) update the diagonal elements by R(k+1)jj =",J. Matrix and tensor denoising,[0],[0]
"( ∑ i λiviv > i )jj .
",J. Matrix and tensor denoising,[0],[0]
"Next, instead of computingMσ via (17) and thenWσ via (19), we compute Wσ directly by solving the following system of linear equations.",J. Matrix and tensor denoising,[0],[0]
"Let K† be the pseudo-inverse
matrix of K, and PΩ(T ) denote a masking operation over the tensor T such that,
PΩ(T ) =",J. Matrix and tensor denoising,[0],[0]
{ Tijk i 6=,J. Matrix and tensor denoising,[0],[0]
j 6= k 0,J. Matrix and tensor denoising,[0],[0]
"o.w
We estimateW by the following minimization problem,
Ŵ = argmin W
‖PΩ ( W(K†,K†,K†) )",J. Matrix and tensor denoising,[0],[0]
"− PΩ(M)‖2F
This method depends only on the off-diagonal elements of M and M and hence is applicable whenever E[X|H] = WTH and the noise has bounded variance.",J. Matrix and tensor denoising,[0],[0]
"In section 5, we compare the results of algorithm 1 with and without an additional single weighted least square step.",K. Adding a weighted least square step to the spectral method,[0],[0]
"Given an estimate Ŵ , for each observed instance xj we calculate the conditional likelihood L(xj |h) for the 2d possible binary vectors h ∈ {0, 1}d,
L(xj |h) = 1√
2πσ2 exp
( − ‖xj − ŴTh‖2 / (2σ2) )",K. Adding a weighted least square step to the spectral method,[0],[0]
"For each instance xj , we keep the top K = 6 vectors h1j , . . .",K. Adding a weighted least square step to the spectral method,[0],[0]
",hKj with the highest likelihood.",K. Adding a weighted least square step to the spectral method,[0],[0]
Let Π ∈,K. Adding a weighted least square step to the spectral method,[0],[0]
"[0, 1]K×n be a weight matrix such that Πkj is proportional to L(xj |hkj), and ∑ k Πkj = 1 for all j. The new estimate Ŵwls is the minimizer of the weighted least square problem,
Ŵwls = argmin W n∑ j=1 K∑ k=1 Πkj‖xj −WThkj‖2.",K. Adding a weighted least square step to the spectral method,[0],[0]
"In section 5, we compare the results of the spectral approach to the following ALS iterations, with a random starting point.
",L. Alternating least squares for W and H,[0],[0]
W (k) = argmin,L. Alternating least squares for W and H,[0],[0]
"W∈Rd×m ‖X −WTH(k−1)‖2F
Ĥ(k) = argmin H∈Rd×n",L. Alternating least squares for W and H,[0],[0]
‖X,L. Alternating least squares for W and H,[0],[0]
"− (W (k))TH‖2F
H(k) = argmin H∈{0,1}d×n ‖H − Ĥ(k)‖2F ,",L. Alternating least squares for W and H,[0],[0]
"The simulated admixture data was generated via the following steps:
1.",M. Genetic admixture simulations,[0],[0]
"We used SCRM (Staab et al., 2015) to simulate a split between d = 3 ancestral populations, with separation
time of 4000 generations.",M. Genetic admixture simulations,[0],[0]
The simulator generated 40 chromosomes of length 250 · 106 for each of the three populations.,M. Genetic admixture simulations,[0],[0]
"The simulation parameters were determined as N0 = 104 effective population size, 10−8 mutation rate (per base pair per generation), and 10−8 recombination rate (per base pair per generation).
2.",M. Genetic admixture simulations,[0],[0]
"We sampled the proportion matrix W from a Dirichlet distribution with parameter α.
3.",M. Genetic admixture simulations,[0],[0]
"Two chromosomes of length 250 · 106 were created for each of the m = 50 admixed individuals with the following steps: (i) An ancestral population was sampled according to W, say, population hA. (ii)",M. Genetic admixture simulations,[0],[0]
"One of the 40 chromosomes was sampled from hA, say hA(k) (iii) A block length l was sampled from an exponential distribution with rate 20 per Morgan corresponding admixture event happening 20 generations ago (in our case, 1 Morgan was 108 base pairs).",M. Genetic admixture simulations,[0],[0]
(iv) A block of length l was copied from chromosome hA(k) to the corresponding locations in the new admixed chromosome.,M. Genetic admixture simulations,[0],[0]
"We repeated steps (i)-(iv) until completion of the chromosome.
",M. Genetic admixture simulations,[0],[0]
"N. Analysis of DNA methylation data The dataset is part of the supplementary material of (Houseman et al., 2012).",M. Genetic admixture simulations,[0],[0]
"The observed matrix X ∈
[0, 1]m×n consists of m = 12 blood samples, each with the DNA methylation measurements in n = 500 sites (called CpGs).
",M. Genetic admixture simulations,[0],[0]
The statistical model for X is similar to that of Eq.,M. Genetic admixture simulations,[0],[0]
(1).,M. Genetic admixture simulations,[0],[0]
"We assume that each blood cell is a mixture of d = 4 cell types, with unknown proportions.",M. Genetic admixture simulations,[0],[0]
The latent variables h correspond to the presence or absence of methylation in each site for the 4 cell types.,M. Genetic admixture simulations,[0],[0]
"Given the DNA methylation array, the task is to estimate the proportion matrix W .
",M. Genetic admixture simulations,[0],[0]
The upper and lower panels of Figure 6 correspond to the real and estimated mixture matrix.,M. Genetic admixture simulations,[0],[0]
"For comparison, we performed the steps described in detail for this dataset in Slawski et al. (2013, Section 4).",M. Genetic admixture simulations,[0],[0]
"For both methods we measured the l1 distance between the real and estimated mixture matrices,
1
mn ∑ ij |Wij",M. Genetic admixture simulations,[0],[0]
"− Ŵij |
The l1 distance were equal to 0.003 for SHL and 0.00056 for the spectral approach.",M. Genetic admixture simulations,[0],[0]
Latent variable models with hidden binary units appear in various applications.,abstractText,[0],[0]
"Learning such models, in particular in the presence of noise, is a challenging computational problem.",abstractText,[0],[0]
"In this paper we propose a novel spectral approach to this problem, based on the eigenvectors of both the second order moment matrix and third order moment tensor of the observed data.",abstractText,[0],[0]
"We prove that under mild non-degeneracy conditions, our method consistently estimates the model parameters at the optimal parametric rate.",abstractText,[0],[0]
"Our tensor-based method generalizes previous orthogonal tensor decomposition approaches, where the hidden units were assumed to be either statistically independent or mutually exclusive.",abstractText,[0],[0]
We illustrate the consistency of our method on simulated data and demonstrate its usefulness in learning a common model for population mixtures in genetics.,abstractText,[0],[0]
Learning Binary Latent Variable Models: A Tensor Eigenpair Approach,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 377–387 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1035",text,[0],[0]
"Detection of sentiment and sarcasm in usergenerated short reviews is of primary importance for social media analysis, recommendation and dialog systems.",1 Introduction,[0],[0]
"Traditional sentiment analyzers and
sarcasm detectors face challenges that arise at lexical, syntactic, semantic and pragmatic levels (Liu and Zhang, 2012; Mishra et al., 2016c).",1 Introduction,[0],[0]
"Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.g. learning that the word deadly conveys a strong positive sentiment in opinions such as Shane Warne is a deadly bowler, as opposed to The high altitude Himalayan roads have deadly turns).",1 Introduction,[0],[0]
"It is, however, extremely difficult to tackle subtleties at semantic and pragmatic levels.",1 Introduction,[0],[0]
"For example, the sentence I really love my job.",1 Introduction,[0],[0]
I work 40 hours a week to be this poor.,1 Introduction,[0],[0]
requires an NLP system to be able to understand that the opinion holder has not expressed a positive sentiment towards her / his job.,1 Introduction,[0],[0]
"In the absence of explicit clues in the text, it is difficult for automatic systems to arrive at a correct classification decision, as they often lack external knowledge about various aspects of the text being classified.
",1 Introduction,[0],[0]
"Mishra et al. (2016b) and Mishra et al. (2016c) show that NLP systems based on cognitive data (or simply, Cognitive NLP systems) , that leverage eye-movement information obtained from human readers, can tackle the semantic and pragmatic challenges better.",1 Introduction,[0],[0]
The hypothesis here is that human gaze activities are related to the cognitive processes in the brain that combine the “external knowledge” that the reader possesses with textual clues that she / he perceives.,1 Introduction,[0],[0]
"While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability of low cost eye-tracking machinery (Wood and Bulling, 2014; Yamamoto et al., 2013), few methods exist for text classification, and they rely on handcrafted features extracted from gaze data (Mishra et al., 2016b,c).",1 Introduction,[0],[0]
"These systems have limited capabilities due to two reasons: (a) Manually designed gaze based features may not adequately
377
capture all forms of textual subtleties (b) Eyemovement data is not as intuitive to analyze as text which makes the task of designing manual features more difficult.",1 Introduction,[0],[0]
"So, in this work, instead of handcrafting the gaze based and textual features, we try to learn feature representations from both gaze and textual data using Convolutional Neural Network (CNN).",1 Introduction,[0],[0]
"We test our technique on two publicly available datasets enriched with eyemovement information, used for binary classification tasks of sentiment polarity and sarcasm detection.",1 Introduction,[0],[0]
Our experiments show that the automatically extracted features often help to achieve significant classification performance improvement over (a) existing systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.,1 Introduction,[0],[0]
"The datasets used in our experiments, resources and other relevant pointers are available at http://www.cfilt.iitb.ac.in/ cognitive-nlp
The rest of the paper is organized as follows.",1 Introduction,[0],[0]
Section 2 discusses the motivation behind using readers’ eye-movement data in a text classification setting.,1 Introduction,[0],[0]
"In Section 3, we argue why CNN is preferred over other available alternatives for feature extraction.",1 Introduction,[0],[0]
The CNN architecture is proposed and discussed in Section 4.,1 Introduction,[0],[0]
Section 5 describes our experimental setup and results are discussed in Section 6.,1 Introduction,[0],[0]
We provide a detailed analysis of the results along with some insightful observations in Section 7.,1 Introduction,[0],[0]
"Section 8 points to relevant literature followed by Section 9 that concludes the paper.
",1 Introduction,[0],[0]
Terminology A fixation is a relatively long stay of gaze on a visual object (such as words in text) where as a sacccade corresponds to quick shifting of gaze between two positions of rest.,1 Introduction,[0],[0]
Forward and backward saccades are called progressions and regressions respectively.,1 Introduction,[0],[0]
A scanpath is a line graph that contains fixations as nodes and saccades as edges.,1 Introduction,[0],[0]
"Presence of linguistic subtleties often induces (a) surprisal (Kutas and Hillyard, 1980; Malsburg et al., 2015), due to the underlying disparity /context incongruity",2 Eye-movement and Linguistic Subtleties,[0],[0]
"or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures.",2 Eye-movement and Linguistic Subtleties,[0],[0]
"While surprisal accounts for irregular saccades (Malsburg et al., 2015), higher cognitive
load results in longer fixation duration (Kliegl et al., 2004).
",2 Eye-movement and Linguistic Subtleties,[0],[0]
Mishra et al. (2016b) find that presence of sarcasm in text triggers either irregular saccadic patterns or unusually high duration fixations than non-sarcastic texts (illustrated through example scanpath representations in Figure 1).,2 Eye-movement and Linguistic Subtleties,[0],[0]
"For sentiment bearing texts, highly subtle eyemovement patterns are observed for semantically/pragmatically complex negative opinions (expressing irony, sarcasm, thwarted expectations, etc.) than the simple ones (Mishra et al., 2016b).",2 Eye-movement and Linguistic Subtleties,[0],[0]
The association between linguistic subtleties and eye-movement patterns could be captured through sophisticated feature engineering that considers both gaze and text inputs.,2 Eye-movement and Linguistic Subtleties,[0],[0]
"In our work, CNN takes the onus of feature engineering.",2 Eye-movement and Linguistic Subtleties,[0],[0]
"CNNs have been quite effective in learning filters for image processing tasks, filters being used to transform the input image into more informative feature space (Krizhevsky et al., 2012).",3 Why Convolutional Neural Network?,[0],[0]
"Filters learned at various CNN layers are quite similar to handcrafted filters used for detection of edges, contours, and removal of redundant backgrounds.",3 Why Convolutional Neural Network?,[0],[0]
"We believe, a similar technique can also be applied to eye-movement data, where the learned filters will, hopefully, extract informative cognitive features.",3 Why Convolutional Neural Network?,[0],[0]
"For instance, for sarcasm, we expect the network to learn filters that detect long distance saccades (refer to Figure 2 for an analogical il-
lustration).",3 Why Convolutional Neural Network?,[0],[0]
"With more number of convolution filters of different dimensions, the network may extract multiple features related to different gaze attributes (such as fixations, progressions, regressions and skips) and will be free from any form of human bias that manually extracted features are susceptible to.",3 Why Convolutional Neural Network?,[0],[0]
Figure 3 shows the CNN architecture with two components for processing and extracting features from text and gaze inputs.,4 Learning Feature Representations: The CNN Architecture,[0],[0]
The components are explained below.,4 Learning Feature Representations: The CNN Architecture,[0],[0]
The text component is quite similar to the one proposed by Kim (2014) for sentence classification.,4.1 Text Component,[0],[0]
Words (in the form of one-hot representation) in the input text are first replaced by their embeddings of dimension K (ith word in the sentence represented by an embedding vector xi ∈ RK).,4.1 Text Component,[0],[0]
"As per Kim (2014), a multi-channel variant of CNN (referred to as MULTICHANNELTEXT) can be implemented by using two channels of embeddingsone that remains static throughout training (referred to as STATICTEXT), and the other one that gets updated during training (referred to as NONSTATICTEXT).",4.1 Text Component,[0],[0]
"We separately experiment with static, non-static and multi-channel variants.
",4.1 Text Component,[0],[0]
"For each possible input channel of the text component, a given text is transformed into a tensor of fixed length N (padded with zero-tensors wherever
necessary to tackle length variations) by concatenating the word embeddings.
",4.1 Text Component,[0],[0]
"x1:N = x1 ⊕ x2 ⊕ x3 ⊕ ...⊕ xN (1)
where ⊕ is the concatenation operator.",4.1 Text Component,[0],[0]
"To extract local features1, convolution operation is applied.",4.1 Text Component,[0],[0]
"Convolution operation involves a filter, W ∈ RHK , which is convolved with a window of H embeddings to produce a local feature for the H words.",4.1 Text Component,[0],[0]
"A local feature, ci is generated from a window of embeddings xi:i+H−1 by applying a non linear function (such as a hyperbolic tangent) over the convoluted output.",4.1 Text Component,[0],[0]
"Mathematically,
ci = f(W.xi:i+H−1 + b) (2)
where b ∈ R is the bias and f is the non-linear function.",4.1 Text Component,[0],[0]
"This operation is applied to each possible window of H words to produce a feature map (c) for the window size H .
",4.1 Text Component,[0],[0]
c =,4.1 Text Component,[0],[0]
"[c1, c2, c3, ..., cN−H+1] (3)
A global feature is then obtained by applying max pooling operation2 (Collobert et al., 2011) over the feature map.",4.1 Text Component,[0],[0]
"The idea behind max-pooling is to capture the most important feature - one with the highest value - for each feature map.
",4.1 Text Component,[0],[0]
We have described the process by which one feature is extracted from one filter (red bordered portions in Figure 3 illustrate the case of H = 2).,4.1 Text Component,[0],[0]
The model uses multiple filters for each filter size to obtain multiple features representing the text.,4.1 Text Component,[0],[0]
"In the MULTICHANNELTEXT variant, for a window of H words, the convolution operation is separately applied on both the embedding channels.",4.1 Text Component,[0],[0]
Local features learned from both the channels are concatenated before applying max-pooling.,4.1 Text Component,[0],[0]
The gaze component deals with scanpaths of multiple participants annotating the same text.,4.2 Gaze Component,[0],[0]
"Scanpaths can be pre-processed to extract two sequences3 of gaze data to form separate channels of input: (1) A sequence of normalized4 durations of fixations (in milliseconds) in the order in which
1features specific to a region in case of images or window of words in case of text
2mean pooling does not perform well.",4.2 Gaze Component,[0],[0]
"3like text-input, gaze sequences are padded where necessary 4scaled across participants using min-max normalization to reduce subjectivity
they appear in the scanpath, and (2) A sequence of position of fixations (in terms of word id) in the order in which they appear in the scanpath.",4.2 Gaze Component,[0],[0]
These channels are related to two fundamental gaze attributes such as fixation and saccade respectively.,4.2 Gaze Component,[0],[0]
"With two channels, we thus have three possible configurations of the gaze component such as (i) FIXATION, where the input is normalized fixation duration sequence, (ii) SACCADE, where the input is fixation position sequence, and (iii) MULTICHANNELGAZE, where both the inputs channels are considered.
",4.2 Gaze Component,[0],[0]
"For each possible input channel, the input is in the form of a P × G matrix (with P → number of participants and G → length of the input sequence).",4.2 Gaze Component,[0],[0]
"Each element of the matrix gij ∈ R, with i ∈ P and j ∈ G, corresponds to the jth gaze attribute (either fixation duration or word id, depending on the channel) of the input sequence of the ith participant.",4.2 Gaze Component,[0],[0]
"Now, unlike the text component, here we apply convolution operation across two dimensions i.e. choosing a two dimensional convolution filter W ∈ RJK (for simplicity, we have kept J = K, thus , making the dimension of W , J2).",4.2 Gaze Component,[0],[0]
"For the dimension size of J2, a local feature cij is computed from the window of gaze elements gij:(i+J−1)(j+J−1) by,
cij = f(W.gij:(i+J−1)(j+J−1) + b) (4)
where b ∈ R is the bias and f is a non-linear func-
tion.",4.2 Gaze Component,[0],[0]
"This operation is applied to each possible window of size J2 to produce a feature map (c),
c =[c11, c12, c13, ..., c1(G−J+1),
c21, c22, c23, ..., c2(G−J+1), ...,
c(P−J+1)1, c(P−J+1)2, ..., c(P−J+1)(G−J+1)]
(5)
",4.2 Gaze Component,[0],[0]
A global feature is then obtained by applying max pooling operation.,4.2 Gaze Component,[0],[0]
"Unlike the text component, max-pooling operator is applied to a 2D window of local features size M × N (for simplicity, we set M = N , denoted henceforth as M2).",4.2 Gaze Component,[0],[0]
"For the window of size M2, the pooling operation on c will result in as set of global features ĉJ = max{cij:(i+M−1)(j+M−1)} for each possible",4.2 Gaze Component,[0],[0]
"i, j.
We have described the process by which one feature is extracted from one filter (of 2D window size J2 and the max-pooling window size of M2).",4.2 Gaze Component,[0],[0]
"In Figure 3, red and blue bordered portions illustrate the cases of J2 =",4.2 Gaze Component,[0],[0]
"[3, 3] and M2 = [2, 2] respectively.",4.2 Gaze Component,[0],[0]
"Like the text component, the gaze component also uses multiple filters for each filter size to obtain multiple features representing the gaze input.",4.2 Gaze Component,[0],[0]
"In the MULTICHANNELGAZE variant, for a 2D window of J2, the convolution operation is separately applied on both fixation duration and saccade channels and local features learned from both the channels are concatenated before maxpooling is applied.
",4.2 Gaze Component,[0],[0]
"Once the global features are learned from both the text and gaze components, they are merged
and passed to a fully connected feed forward layer (with number of units set to 150) followed by a SoftMax layer that outputs the the probabilistic distribution over the class labels.
",4.2 Gaze Component,[0],[0]
"The gaze component of our network is not invariant of the order in which the scanpath data is given as input- i.e., the P rows in the P × G can not be shuffled, even if each row is independent from others.",4.2 Gaze Component,[0],[0]
"The only way we can think of for addressing this issue is by applying convolution operations to all P × G matrices formed with all the permutations of P , capturing every possible ordering.",4.2 Gaze Component,[0],[0]
"Unfortunately, this makes the training process significantly less scalable, as the number of model parameters to be learned becomes huge.",4.2 Gaze Component,[0],[0]
"As of now, training and testing are carried out by keeping the order of the input constant.",4.2 Gaze Component,[0],[0]
We now share several details regarding our experiments below.,5 Experiment Setup,[0],[0]
We conduct experiments for two binaryclassification tasks of sentiment and sarcasm using two publicly available datasets enriched with eye-movement information.,5.1 Dataset,[0],[0]
Dataset 1 has been released by Mishra et al. (2016a).,5.1 Dataset,[0],[0]
It contains 994 text snippets with 383 positive and 611 negative examples.,5.1 Dataset,[0],[0]
"Out of the 994 snippets, 350 are sarcastic.",5.1 Dataset,[0],[0]
"Dataset 2 has been used by Joshi et al. (2014) and it consists of 843 snippets comprising movie reviews and normalized tweets out of which 443 are positive, and 400 are negative.",5.1 Dataset,[0],[0]
Eye-movement data of 7 and 5 readers is available for each snippet for dataset 1 and 2 respectively.,5.1 Dataset,[0],[0]
"With text component alone we have three variants such as STATICTEXT, NONSTATICTEXT and MULTICHANNELTEXT (refer to Section 4.1).",5.2 CNN Variants,[0],[0]
"Similarly, with gaze component we have variants such as FIXATION, SACCADE and MULTICHANNELGAZE (refer to Section 4.2).",5.2 CNN Variants,[0],[0]
"With both text and gaze components, 9 more variants could thus beexperimented with.",5.2 CNN Variants,[0],[0]
"For text component, we experiment with filter widths (H) of [3, 4].",5.3 Hyper-parameters,[0],[0]
"For the gaze component, 2D filters (J2) set to [3× 3], [4× 4] respectively.",5.3 Hyper-parameters,[0],[0]
"The
max pooling 2D window, M2, is set to [2× 2].",5.3 Hyper-parameters,[0],[0]
"In both gaze and text components, number of filters is set to 150, resulting in 150 feature maps for each window.",5.3 Hyper-parameters,[0],[0]
These model hyper-parameters are fixed by trial and error and are possibly good enough to provide a first level insight into our system.,5.3 Hyper-parameters,[0],[0]
"Tuning of hyper-parameters might help in improving the performance of our framework, which is on our future research agenda.",5.3 Hyper-parameters,[0],[0]
"For regularization dropout is employed both on the embedding and the penultimate layers with a constraint on l2-norms of the weight vectors (Hinton et al., 2012).",5.4 Regularization,[0],[0]
"Dropout prevents co-adaptation of hidden units by randomly dropping out - i.e., setting to zero - a proportion p of the hidden units during forward propagation.",5.4 Regularization,[0],[0]
We set p to 0.25.,5.4 Regularization,[0],[0]
"We use ADADELTA optimizer (Zeiler, 2012), with a learning rate of 0.1.",5.5 Training,[0],[0]
The input batch size is set to 32 and number of training iterations (epochs) is set to 200.,5.5 Training,[0],[0]
10% of the training data is used for validation.,5.5 Training,[0],[0]
"Initializing the embedding layer with of pretrained embeddings can be more effective than random initialization (Kim, 2014).",5.6 Use of Pre-trained Embeddings:,[0],[0]
"In our experiments, we have used embeddings learned using the movie reviews with one sentence per review dataset (Pang and Lee, 2005).",5.6 Use of Pre-trained Embeddings:,[0],[0]
"It is worth noting that, for a small dataset like ours, using a small data-set like the one from (Pang and Lee, 2005) helps in reducing the number model parameters resulting in faster learning of embeddings.",5.6 Use of Pre-trained Embeddings:,[0],[0]
The results are also quite close to the ones obtained using word2vec facilitated by Mikolov et al. (2013).,5.6 Use of Pre-trained Embeddings:,[0],[0]
"For sentiment analysis, we compare our systems’s accuracy (for both datasets 1 and 2) with Mishra et al. (2016c)’s systems that rely on handcrafted text and gaze features.",5.7 Comparison with Existing Work,[0],[0]
"For sarcasm detection, we compare Mishra et al. (2016b)’s sarcasm classifier with ours using dataset 1 (with available gold standard labels for sarcasm).",5.7 Comparison with Existing Work,[0],[0]
We follow the same 10-fold train-test configuration as these existing works for consistency.,5.7 Comparison with Existing Work,[0],[0]
"In this section, we discuss the results for different model variants for sentiment polarity and sarcasm detection tasks.",6 Results,[0],[0]
Table 1 presents results for sentiment analysis task.,6.1 Results for Sentiment Analysis Task,[0],[0]
"For dataset 1, different variants of our CNN architecture outperform the best systems reported by Mishra et al. (2016c), with a maximum F-score improvement of 3.8%.",6.1 Results for Sentiment Analysis Task,[0],[0]
This improvement is statistically significant of p < 0.05 as confirmed by McNemar test.,6.1 Results for Sentiment Analysis Task,[0],[0]
"Moreover, we observe an F-score improvement of around 5% for CNNs with both gaze and text components as compared to CNNs with only text components (similar to the system by Kim (2014)), which is also statistically significant (with p < 0.05).
",6.1 Results for Sentiment Analysis Task,[0],[0]
"For dataset 2, CNN based approaches do not perform better than manual feature based approaches.",6.1 Results for Sentiment Analysis Task,[0],[0]
"However, variants with both text and gaze components outperform the ones with only text component (Kim, 2014), with a maximum Fscore improvement of 2.9%.",6.1 Results for Sentiment Analysis Task,[0],[0]
"We observe that for dataset 2, training accuracy reaches 100 within 25 epochs with validation accuracy stable around 50%, indicating the possibility of overfitting.",6.1 Results for Sentiment Analysis Task,[0],[0]
Tuning the regularization parameters specific to dataset 2 may help here.,6.1 Results for Sentiment Analysis Task,[0],[0]
"Even though CNN might
not be proving to be a choice as good as handcrafted features for dataset 2, the bottom line remains that incorporation of gaze data into CNN consistently improves the performance over onlytext-based CNN variants.",6.1 Results for Sentiment Analysis Task,[0],[0]
"For sarcasm detection, our CNN model variants outperform traditional systems by a maximum margin of 11.27% (Table 2).",6.2 Results for Sarcasm Detection Task,[0],[0]
"However, the improvement by adding the gaze component to the CNN network is just 1.34%, which is statistically insignificant over CNN with text component.",6.2 Results for Sarcasm Detection Task,[0],[0]
"While inspecting the sarcasm dataset, we observe a clear difference between the vocabulary of sarcasm and non-sarcasm classes in our dataset.",6.2 Results for Sarcasm Detection Task,[0],[0]
"This, perhaps, was captured well by the text component, especially the variant with only non-static embeddings.",6.2 Results for Sarcasm Detection Task,[0],[0]
"In this section, some important observations from our experiments are discussed.",7 Discussion,[0],[0]
"Embedding dimension has proven to have a deep impact on the performance of neural systems (dos Santos and Gatti, 2014; Collobert et al., 2011).
",7.1 Effect of Embedding Dimension Variation,[0],[0]
We repeated our experiments by varying the embedding dimensions in the range of [50-300]5 and observed that reducing embedding dimension improves the F-scores by a little margin.,7.1 Effect of Embedding Dimension Variation,[0],[0]
Small embedding dimensions are probably reducing the chances of over-fitting when the data size is small.,7.1 Effect of Embedding Dimension Variation,[0],[0]
"We also observe that for different embedding dimensions, performance of CNN with both gaze and text components is consistently better than that with only text component.",7.1 Effect of Embedding Dimension Variation,[0],[0]
"Non-static embedding channel has a major role in tuning embeddings for sentiment analysis by bringing adjectives expressing similar sentiment close to each other (e.g, good and nice), where as static channel seems to prevent over-tuning of embeddings (over-tuning often brings verbs like love closer to the pronoun I in embedding space, purely due to higher co-occurrence of these two words in sarcastic examples).",7.2 Effect of Static / Non-static Text Channels,[0],[0]
"For sentiment detection, saccade channel seems to be handing text having semantic incongruity (due
5a standard range (Liu et al., 2015; Melamud et al., 2016)
to the presence of irony / sarcasm) better.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"Fixation channel does not help much, may be because of higher variance in fixation duration.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"For sarcasm detection, fixation and saccade channels perform with similar accuracy when employed separately.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"Accuracy reduces with gaze multichannel, may be because of higher variation of both fixations and saccades across sarcastic and nonsarcastic classes, as opposed to sentiment classes.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"To examine how good the features learned by the CNN are, we analyzed the features for a few example cases.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
Figure 4 presents some of the example test cases for the task of sarcasm detection.,7.4 Effectiveness of the CNN-learned Features,[0],[0]
"Example 1 contains sarcasm while examples 2, 3 and 4 are non-sarcastic.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"To see if there is any difference in the automatically learned features between text-only and combined text and gaze variants, we examine the feature vector (of dimension 150) for the examples obtained from different model variants.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
Output of the hidden layer after merge layer is considered as features learned by the network.,7.4 Effectiveness of the CNN-learned Features,[0],[0]
"We plot the features, in the form of color-bars, following Li et al. (2016) - denser col-
ors representing feature with higher magnitude.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"In Figure 4, we show only two representative model variants viz., MULTICHANNELTEXT and MULTICHANNELTEXT+ MULTICHANNELGAZE.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"As one can see, addition of gaze information helps to generate features with more subtle differences (marked by blue rectangular boxes) for sarcastic and non-sarcastic texts.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"It is also interesting to note that in the marked region, features for the sarcastic texts exhibit more intensity than the nonsarcastic ones - perhaps capturing the notion that sarcasm typically conveys an intensified negative opinion.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"This difference is not clear in feature vectors learned by text-only systems for instances like example 2, which has been incorrectly classified by MULTICHANNELTEXT.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"Example 4 is incorrectly classified by both the systems, perhaps due to lack of context.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time.,8 Related Work,[0],[0]
"Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc.",8 Related Work,[0],[0]
"(Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011).",8 Related Work,[0],[0]
"For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015)",8 Related Work,[0],[0]
"(b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014).",8 Related Work,[0],[0]
Recent systems are based on variants of deep neural network built on the top of embeddings.,8 Related Work,[0],[0]
"A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi-
tecture (Wang et al., 2016).",8 Related Work,[0],[0]
"Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs.
",8 Related Work,[0],[0]
"Eye-tracking technology is a relatively new NLP, with very few systems directly making use of gaze data in prediction frameworks.",8 Related Work,[0],[0]
"Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features.",8 Related Work,[0],[0]
The closest works to ours are by Mishra et al. (2016b) and Mishra et al. (2016c) that introduce feature engineering based on both gaze and text data for sentiment and sarcasm detection tasks.,8 Related Work,[0],[0]
These recent advancements motivate us to explore the cognitive NLP paradigm.,8 Related Work,[0],[0]
"In this work, we proposed a multimodal ensemble of features, automatically learned using variants of CNNs from text and readers’ eye-movement data, for the tasks of sentiment and sarcasm classification.",9 Conclusion and Future Directions,[0],[0]
"On multiple published datasets for which gaze information is available, our systems could often achieve significant performance improvements over (a) systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.",9 Conclusion and Future Directions,[0],[0]
An analysis of the learned features confirms that the combination of automatically learned features is indeed capable of representing deep linguistic subtleties in text that pose challenges to sentiment and sarcasm classifiers.,9 Conclusion and Future Directions,[0],[0]
"Our future agenda includes: (a) optimizing the CNN framework hyper-parameters (e.g., filter width, dropout, embedding dimensions, etc.) to obtain better results, (b) exploring the applicability of our technique for documentlevel sentiment analysis and (c) applying our framework to related problems, such as emotion analysis, text summarization, and questionanswering, where considering textual clues alone may not prove to be sufficient.",9 Conclusion and Future Directions,[0],[0]
"We thank Anoop Kunchukuttan, Joe Cheri Ross, and Sachin Pawar, research scholars of the Center for Indian Language Technology (CFILT), IIT Bombay for their valuable inputs.",Acknowledgments,[0],[0]
"Cognitive NLP systemsi.e., NLP systems that make use of behavioral data augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc..",abstractText,[0],[0]
Such extraction of features is typically manual.,abstractText,[0],[0]
"We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like sentiment analysis and sarcasm detection, and that even the extraction and choice of features should be delegated to the learning system.",abstractText,[0],[0]
We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection.,abstractText,[0],[0]
Our proposed framework is based on Convolutional Neural Network (CNN).,abstractText,[0],[0]
The CNN learns features from both gaze and text and uses them to classify the input text.,abstractText,[0],[0]
"We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",abstractText,[0],[0]
Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1839–1848 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1839",text,[0],[0]
"In the last few years, convolutional neural networks (CNNs) have demonstrated remarkable progress in various natural language processing applications (Collobert et al., 2011), including sentence/document classification (Kim, 2014; Zhang et al., 2015; Wang et al., 2018), text sequence matching (Hu et al., 2014; Yin et al., 2016; Shen et al., 2017), generic text representations (Gan et al., 2016; Tang et al., 2018), language modeling (Dauphin et al., 2017), machine translation (Gehring et al., 2017) and abstractive sentence summarization (Gehring et al., 2017).",1 Introduction,[0],[0]
"CNNs are typically applied to tasks where feature extrac-
tion and a corresponding supervised task are approached jointly (LeCun et al., 1998).",1 Introduction,[0],[0]
"As an encoder network for text, CNNs typically convolve a set of filters, of window size n, with an inputsentence embedding matrix obtained via word2vec (Mikolov et al., 2013) or Glove (Pennington et al., 2014).",1 Introduction,[0],[0]
"Different filter sizes n may be used within the same model, exploiting meaningful semantic features from different n-gram fragments.
",1 Introduction,[0],[0]
"The learned weights of CNN filters, in most cases, are assumed to be fixed regardless of the input text.",1 Introduction,[0],[0]
"As a result, the rich contextual information inherent in natural language sequences may not be fully captured.",1 Introduction,[0],[0]
"As demonstrated in Cohen and Singer (1999), the context of a word tends to greatly influence its contribution to the final supervised tasks.",1 Introduction,[0],[0]
"This observation is consistent with the following intuition: when reading different types of documents, e.g., academic papers or newspaper articles, people tend to adopt distinct strategies for better and more effective understanding, leveraging the fact that the same words or phrases may have different meaning or imply different things, depending on context.
",1 Introduction,[0],[0]
Several research efforts have sought to incorporate contextual information into CNNs to adaptively extract text representations.,1 Introduction,[0],[0]
"One common strategy is the attention mechanism, which is typically employed on top of a CNN (or Long ShortTerm Memory (LSTM)) layer to guide the extraction of semantic features.",1 Introduction,[0],[0]
"For the embedding of a single sentence, Lin et al. (2017) proposed a selfattentive model that attends to different parts of a sentence and combines them into multiple vector representations.",1 Introduction,[0],[0]
"However, their model needs considerably more parameters to achieve performance gains over traditional CNNs.",1 Introduction,[0],[0]
"To match sentence pairs, Yin et al. (2016) introduced an attentionbased CNN model, which re-weights the convolution inputs or outputs, to extract interdepen-
dent sentence representations.",1 Introduction,[0],[0]
Wang et al. (2016); Wang and Jiang (2017) explore a compare and aggregate framework to directly capture the wordby-word matching between two paired sentences.,1 Introduction,[0],[0]
"However, these approaches suffer from the problem of high matching complexity, since a similarity matrix between pairwise words needs to be computed, and thus it is computationally inefficient or even prohibitive when applied to long sentences (Mou et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we propose a generic approach to learn context-aware convolutional filters for natural language understanding.",1 Introduction,[0],[0]
"In contrast to traditional CNNs, the convolution operation in our framework does not have a fixed set of filters, and thus provides the network with stronger modeling flexibility and capacity.",1 Introduction,[0],[0]
"Specifically, we introduce a meta network to generate a set of contextaware filters, conditioned on specific input sentences; these filters are adaptively applied to either the same (Section 3.2) or different (Section 3.3) text sequences.",1 Introduction,[0],[0]
"In this manner, the learned filters vary from sentence to sentence and allow for more fine-grained feature abstraction.
",1 Introduction,[0],[0]
"Moreover, since the generated filters in our framework can adapt to different conditional information available (labels or paired sentences), they can be naturally generalized to model sentence pairs.",1 Introduction,[0],[0]
"In this regard, we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context-aware representations.
",1 Introduction,[0],[0]
"We investigate the effectiveness of our Adaptive Context-sensitive CNN (ACNN) framework on several text processing tasks: ontology classification, sentiment analysis, answer sentence selection and paraphrase identification.",1 Introduction,[0],[0]
We show that the proposed methods consistently outperforms the standard CNN and attention-based CNN baselines.,1 Introduction,[0],[0]
"Our work provides a new perspective on how to incorporate contextual information into text representations, which can be combined with more sophisticated structures to achieve even better performance in the future.",1 Introduction,[0],[0]
"Learning deep text representations has attracted much attention recently, since they can potentially benefit a wide range of NLP applications (Collobert et al., 2011; Kim, 2014; Wang et al., 2017a; Shen et al., 2018a; Tang and de Sa, 2018; Zhang
et al., 2018).",2 Related Work,[0],[0]
CNNs have been extensively investigated as the encoder networks of natural language.,2 Related Work,[0],[0]
"Our work is in line with previous efforts on improving the adaptivity and flexibility of convolutional neural networks (Jeon and Kim, 2017; De Brabandere et al., 2016).",2 Related Work,[0],[0]
Jeon and Kim (2017) proposed to enhance the transformation modeling capacity of CNNs by adaptively learning the filter shapes through backpropagation.,2 Related Work,[0],[0]
"De Brabandere et al. (2016) introduced an architecture to generate the future frames conditioned on given image(s), by adapting the CNN filter weights to the motion within previous video frames.",2 Related Work,[0],[0]
"Although CNNs have been widely adopted in a large number of NLP applications, improving the adaptivity of vanilla CNN modules has been considerably less studied.",2 Related Work,[0],[0]
"To the best of our knowledge, the work reported in this paper is the first attempt to develop more flexible and adjustable CNN architecture for modeling sentences.
",2 Related Work,[0],[0]
"Our use of a meta network to generate parameters for another network is directly inspired by the recent success of hypernetworks for textgeneration tasks (Ha et al., 2017), and dynamic parameter-prediction for video-frame generation (De Brabandere et al., 2016).",2 Related Work,[0],[0]
"In contrast to these works that focus on generation problems, our model is based on context-aware CNN filters and is aimed at abstracting more informative and predictive sentence features.",2 Related Work,[0],[0]
"Most similar to our work, Liu et al. (2017) designed a meta network to generate compositional functions over tree-structured neural networks for encapsulating sentence features.",2 Related Work,[0],[0]
"However, their model is only suitable for encoding individual sentences, while our framework can be readily generalized to capture the interactions between sentence pairs.",2 Related Work,[0],[0]
"Moreover, our framework is based on CNN models, which is advantageous due to fewer parameters and highly parallelizable computations relative to sequential-based models.",2 Related Work,[0],[0]
"The CNN architectures in (Kim, 2014; Collobert et al., 2011) are typically utilized for extracting sentence representations, by a composition of a convolutional layer and a max-pooling operation over all resulting feature maps.",3.1 Basic CNN for text representations,[0],[0]
"Let the words of a sentence of length T (padded where necessary) be x1, x2, ... , xT .",3.1 Basic CNN for text representations,[0],[0]
"The sentence can be represented
as a matrix X ∈ Rd×T , where each column represents a d-dimensional embedding of the corresponding word.
",3.1 Basic CNN for text representations,[0],[0]
"In the convolutional layer, a set of filters with weights W ∈ RK×h×d is convolved with every window of h words within the sentence, i.e., {x1:h, x2:h+1, . . .",3.1 Basic CNN for text representations,[0],[0]
", xT−h+1:T }, where K is the number of output feature maps (and filters).",3.1 Basic CNN for text representations,[0],[0]
"In this manner, feature maps p for these h-gram text fragments are generated as:
pi = f(W × xi:i+h−1 + b) (1)
where i = 1, 2, ..., T − h + 1 and × denotes the convolution operator at the ith shift location.",3.1 Basic CNN for text representations,[0],[0]
"Parameter b ∈ RK is the bias term and f(·) is a non-linear function, implemented as a rectified linear unit (ReLU) in our experiments.",3.1 Basic CNN for text representations,[0],[0]
"The output feature maps of the convolutional layer, i.e., p ∈ RK×(T−h+1) are then passed to the pooling layer, which takes the maximum value in every row of p, forming a K-dimensional vector, z.",3.1 Basic CNN for text representations,[0],[0]
This operation attempts to keep the most salient feature detected by every filter and discard the information from less fundamental text fragments.,3.1 Basic CNN for text representations,[0],[0]
"Moreover, the max-over-time nature of the pooling operation (Collobert et al., 2011) guarantees that the size of the obtained representation is independent of the sentence length.
",3.1 Basic CNN for text representations,[0],[0]
"Note that in basic CNN sentence encoders, filter weights are the same for different inputs, which may be suboptimal for feature extraction (De Brabandere et al., 2016), especially in the case where conditional information is available.",3.1 Basic CNN for text representations,[0],[0]
"The proposed architecture to learn contextsensitive filters is composed of two principal modules: (i) a filter generation module, which produces a set of filters conditioned on the input sentence; and (ii) an adaptive convolution module, which applies the generated filters to an input sentence (this sentence may be either the same as or different from the first input, as discussed further in Section 3.3).",3.2 Learning context-sensitive filters,[0],[0]
"The two modules are jointly differentiable, and the overall architecture can be trained in an end-to-end manner.",3.2 Learning context-sensitive filters,[0],[0]
"Since the generated filters are sample-specific, our ACNN feature extractor for text tends to have stronger predictive power than a basic CNN encoder.",3.2 Learning context-sensitive filters,[0],[0]
"The general ACNN framework is shown schematically in Figure 1.
Filter generation module Instead of utilizing fixed filter weightsW for different inputs (as (1)), our model generates a set of filters conditioned on the input sentence X .",3.2 Learning context-sensitive filters,[0],[0]
"Given an input X , the filter-generation module can be implemented, in principle, as any deep (differentiable) architecture.",3.2 Learning context-sensitive filters,[0],[0]
"However, in order to handle input sentences of variable length common in natural language, we design a generic filter generation module to produce filters with a predefined size.
",3.2 Learning context-sensitive filters,[0],[0]
"First, the input X is encapsulated into a fixedlength vector (code) z with the dimension of l, via a basic CNN model, where one convolutional layer is employed along with the pooling operation (as described in Section 3.1).",3.2 Learning context-sensitive filters,[0.950850291077094],"['A target function in an ILN class will be of the form: x→ ∫ ψ(x;w)f(w)dµ(w), (1) Here ψ is some function of the input x and parameters w, and dµ(w) is a prior over the parameter space.']"
"On top of this hidden representation z, a deconvolutional layer, which performs transposed operations of convolutions (Radford et al., 2016), is further applied to produce a unique set of filters forX (as illustrated in Figure 1):
z = CNN(X;θe), (2)
f = DCNN(z;θd) , (3)
where θe and θd are the learned parameters in each layer of the filter-generating module, respectively.",3.2 Learning context-sensitive filters,[0],[0]
"Specifically, we convolve z with a filter of size (fs, l, kx, ky), where fs is the number of generated filters and the kernel size is (kx, ky).",3.2 Learning context-sensitive filters,[0],[0]
"The output will be a tensor of shape (fs, kx, ky).",3.2 Learning context-sensitive filters,[0],[0]
"Since the dimension of hidden representation z is independent of input-sentence length, this framework guarantees that the generated filters are of the same shape and size for every sentence.",3.2 Learning context-sensitive filters,[0],[0]
"Intuitively, the encoding part of filter generation module abstracts the information from sentenceX into z. Based on this representation, the deconvolutional up-sampling layer determines a set of fixedsize, fine-grained filters f for the specific input.
",3.2 Learning context-sensitive filters,[0],[0]
Adaptive convolution module The adaptive convolution module takes as inputs the generated filters f and an input sentence.,3.2 Learning context-sensitive filters,[0],[0]
This sentence and the input to the filter-generation module may be identical (as in Figure 1) or different (as in Figure 2).,3.2 Learning context-sensitive filters,[0],[0]
"With the sample-specific filters, the input sentence is adaptively encoded, again, via a basic CNN architecture as in Section 3.1, i.e., one convolutional and one pooling layer.",3.2 Learning context-sensitive filters,[0],[0]
"Notably, there are no additional parameters in the adaptive convolution module (no bias term is employed).
",3.2 Learning context-sensitive filters,[0],[0]
"Our ACNN framework can be seen as a generalization of the basic CNN, which can be represented as an ACNN by setting the outputs of the filter-generation module to a constant, regardless of the contextual information from input sentence(s).",3.2 Learning context-sensitive filters,[0],[0]
"Because of the learning-to-learn (Thrun and Pratt, 2012) nature of the proposed ACNN framework, it tends to have greater representational power than the basic CNN.",3.2 Learning context-sensitive filters,[0],[0]
"Considering the ability of our ACNN framework to generate context-aware filters, it can be naturally generalized to the task of text sequence matching.",3.3 Extension to text sequence matching,[0],[0]
"In this section, we will describe the proposed Adaptive Question Answering (AdaQA) model in the context of answer sentence selection task.",3.3 Extension to text sequence matching,[0],[0]
"Note that the corresponding model can be readily adapted to other sentence matching problems as well (see Section 5.2).
",3.3 Extension to text sequence matching,[0],[0]
"Given a factual question q (associated with a list of candidate answers {a1, a2, . . .",3.3 Extension to text sequence matching,[0],[0]
", am} and their corresponding labels y = {y1, y2, . . .",3.3 Extension to text sequence matching,[0],[0]
", ym}), the goal of the model is to identify the correct answers from the set of candidates.",3.3 Extension to text sequence matching,[0],[0]
"For i = 1, 2, . . .",3.3 Extension to text sequence matching,[0],[0]
",m, if ai correctly answers q, then yi = 1, and otherwise yi = 0.",3.3 Extension to text sequence matching,[0],[0]
"Therefore, the task can be cast as a classification problem where, given an unlabeled question-answer pair (qi, ai), we seek to predict the judgement yi.
",3.3 Extension to text sequence matching,[0],[0]
"Conventionally, a question q and an answer a are independently encoded by two basic CNNs to fixed-length vector representations, denoted hq and ha, respectively.",3.3 Extension to text sequence matching,[0],[0]
They are then directly employed to predict the judgement y.,3.3 Extension to text sequence matching,[0],[0]
"This strategy could be suboptimal, since no communication (information sharing) occurs between the questionanswer pair until the top prediction layer.",3.3 Extension to text sequence matching,[0],[0]
"Intuitively, while the model is inferring the representation for a question, if the meaning of the answer is
taken into account, those features that are relevant for final prediction are more likely to be extracted.",3.3 Extension to text sequence matching,[0],[0]
"So motivated, we propose an adaptive CNN-based question-answer (AdaQA) model for this problem.",3.3 Extension to text sequence matching,[0],[0]
"The AdaQA model can be divided into three modules: filter generation, adaptive convolution, and matching modules, as depicted schematically in Figure 2.",3.3 Extension to text sequence matching,[0],[0]
"Assume there is a question-answer pair to be matched, represented by word-embedding matrices, i.e. Q ∈ RTq×d and A ∈ RTa×d, where d is the embedding dimension and Tq and Ta are respective sentence lengths.",3.3 Extension to text sequence matching,[0],[0]
"First, they are passed to two filter-generation modules, to produce two sets of filters that encapsulate features of the corresponding input sentences.",3.3 Extension to text sequence matching,[0],[0]
"Similar to the setup in Section 3.2, we also employ a two-step process to produce the filters.",3.3 Extension to text sequence matching,[0],[0]
"For a question Q, the generating process is:
zq = CNN(Q;θqe), (4) f q = DCNN(zq;θ q d) (5)
where CNN and DCNN denote the basic CNN unit and deconvolution layer, respectively, as discussed in Section 2.1.",3.3 Extension to text sequence matching,[0],[0]
Parameters θqe and θ q d are to be learned.,3.3 Extension to text sequence matching,[0],[0]
"The same process can be utilized to produce encodings za and filters fa for the answer input,A, with parameters θae and θ a d, respectively.
",3.3 Extension to text sequence matching,[0],[0]
"The two sets of filter weights are then passed to adaptive convolution modules, along with Q and A, to obtain the extracted question and answer embeddings.",3.3 Extension to text sequence matching,[0],[0]
"That is, the question embedding is convolved with the filters produced by the answer and vise versa (ψq and ψa are the bias terms to be learned).",3.3 Extension to text sequence matching,[0],[0]
"The key idea is to abstract information from the answer (or question) that is pertinent to the corresponding question (or answer).
",3.3 Extension to text sequence matching,[0],[0]
"Compared to a Siamese CNN architecture (Bromley et al., 1994), our model selectively encapsulates the most important features for judgement prediction, removing less vital information.",3.3 Extension to text sequence matching,[0],[0]
We then employ the question and answer representations hq ∈,3.3 Extension to text sequence matching,[0],[0]
"Rnh , ha ∈",3.3 Extension to text sequence matching,[0],[0]
Rnh as inputs to the matching module (where nh is the dimension of question/answer embeddings).,3.3 Extension to text sequence matching,[0],[0]
"Following Mou et al. (2016), the matching function is defined as:
t =",3.3 Extension to text sequence matching,[0],[0]
"[hq;ha;hq − ha;hq ha] (6) p(y = 1|hq,ha) = MLP(t;η′) (7)
where − and denote an element-wise subtraction and element-wise product, respectively.",3.3 Extension to text sequence matching,[0],[0]
[ha;hb] indicates that ha and hb are stacked as column vectors.,3.3 Extension to text sequence matching,[0],[0]
"The resulting matching vector t ∈ R4nh is then sent through an MLP layer (with sigmoid activation function and parameters η′ to be learned) to model the desired conditional distribution p(yi = 1|hq,ha).
",3.3 Extension to text sequence matching,[0],[0]
"Notably, we share the weights of filter generating networks for both the question and answer, so that the model adaptivity for answer selection can be improved without an excessive increase in the number of parameters.",3.3 Extension to text sequence matching,[0],[0]
All three modules in AdaQA model are jointly trained end-to-end.,3.3 Extension to text sequence matching,[0],[0]
"Note that the AdaQA model proposed can be readily adapted to other sentence matching tasks, such as paraphrase identification (see Section 5.2).",3.3 Extension to text sequence matching,[0],[0]
"The adaptive context-aware filter generation mechanism proposed here bears close resemblance to attention mechanism (Yin et al., 2016; Bahdanau et al., 2015; Xiong et al., 2017) widely adopted in the NLP community, in the sense that both methods intend to incorporate rich contextual information into text representations.",3.4 Connections to attention mechanism,[0],[0]
"However, attention is typically operated on top of the hidden units preprocessed by CNN or LSTM layers, and assigns different weights to each unit according to a context vector.",3.4 Connections to attention mechanism,[0],[0]
"By contrast, in our context-aware filter generation mechanism, the contextual information is inherently encoded into the convolutional filters, which directly interact with the input sentence during the convolution encoding operation.",3.4 Connections to attention mechanism,[0],[0]
"Notably, according to our experiments, the proposed filter generation module can be readily combined with (standard) attention mechanisms to further enhance the modeling expressiveness of CNN encoder.",3.4 Connections to attention mechanism,[0],[0]
Datasets We investigate the effectiveness of the proposed ACNN framework on both document classification and text sequence matching tasks.,4 Experimental Setup,[0],[0]
"Specifically, we consider two large-scale document classification datasets: Yelp Reviews Polarity, and DBPedia ontology datasets (Zhang et al., 2015).",4 Experimental Setup,[0],[0]
"For Yelp reviews, we seek to predict a binary label (positive or negative) regarding one review about a restaurant.",4 Experimental Setup,[0],[0]
"DBpedia is extracted from Wikipedia by crowd-sourcing and is categorized into 14 non-overlapping ontology classes, including Company, Athlete, Natural Place, etc.",4 Experimental Setup,[0],[0]
"We sample 15% of the training data as the validation set, to select hyperparameters for our models and perform early stopping.",4 Experimental Setup,[0],[0]
"For sentence matching, we evaluate the AdaQA model on two datasets for open-domain question answering: WikiQA (Yang et al., 2015) and SelQA (Jurczyk et al., 2016).",4 Experimental Setup,[0],[0]
"Given a question, the task is to rank the corresponding candidate answers, which, in the case of WikiQA, are sentences extracted from the summary section of a related Wikipedia article.",4 Experimental Setup,[0],[0]
"To facilitate comparison with existing results (Yin et al., 2016; Yang et al., 2015; Shen et al., 2018b), we truncate the candidate answers to a maximum length of 40 tokens for all experiments on the WikiQA dataset.",4 Experimental Setup,[0],[0]
"We also consider the task of paraphrase identification with the Quora Question Pairs dataset, with the same data splits as in (Wang et al., 2017b).",4 Experimental Setup,[0],[0]
"A summary of all datasets is presented in Table 1.
",4 Experimental Setup,[0],[0]
"Training Details For the document classification experiments, we randomly initialize the word embeddings uniformly within [−0.001, 0.001] and update them during training.",4 Experimental Setup,[0],[0]
"For the generated filters, we set the window size as h = 5, with K = 100 feature maps (the dimension of z is set as 100).",4 Experimental Setup,[0],[0]
"For direct comparison, we employ the same filter shape/size settings as in our basic CNN implementation.",4 Experimental Setup,[0],[0]
"A one-layer architecture is utilized for both the CNN baseline and the ACNN model, since we did not observe significant
performance gains with a multilayer architecture.",4 Experimental Setup,[0],[0]
"The minibatch size is set as 128, and a dropout rate of 0.2 is utilized on the embedding layer.",4 Experimental Setup,[0],[0]
"We observed that a larger dropout rate (e.g., 0.5) will hurt performance on document classifications and make training significantly slower.
",4 Experimental Setup,[0],[0]
"For the sentence matching tasks, we initialized the word embeddings with 50-dimensional Glove (Pennington et al., 2014) word vectors pretrained from Wikipedia 2014 and Gigaword 5 (Pennington et al., 2014) for all model variants.",4 Experimental Setup,[0],[0]
"As for the filters, we set the window size as h = 5, with K = 300 feature maps.",4 Experimental Setup,[0],[0]
"As described in Section 3.3, the vector t, output from the matching module, is fed to the prediction layer, implemented as a one-layer MLP followed by the sigmoid function.",4 Experimental Setup,[0],[0]
"We use Adam (Kingma and Ba, 2014) to train the models, with a learning rate of 3 × 10−4.",4 Experimental Setup,[0],[0]
"Dropout (Srivastava et al., 2014), with a rate of 0.5, is employed on the word embedding layer.",4 Experimental Setup,[0],[0]
The hyperparameters are selected by choosing the best model on the validation set.,4 Experimental Setup,[0],[0]
"All models are implemented with TensorFlow (Abadi et al., 2016) and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12GB memory.
",4 Experimental Setup,[0],[0]
"Baselines For document classification, we consider several baseline models: (i) ngrams (Zhang et al., 2015), a bag-of-means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams (up to 5-grams) from the training set and use their corresponding counts as features; (ii) small/large word CNN (Zhang et al., 2015): 6 layer word-based convolutional networks, with 256/1024 features at each layer, denoted as small/large, respectively; (iii) deep CNN (Conneau et al., 2016): deep convolutional neural networks with 9/17/29 layers.",4 Experimental Setup,[0],[0]
"To evaluate the effectiveness of proposed AdaQA model, we compare it with several CNN-based sequence matching baselines, including Vanilla CNN (Jurczyk et al., 2016; Santos et al., 2017), attentive pooling networks (dos Santos et al., 2016), and ABCNN (Yin et al., 2016) (where an attention mechanism is employed over the two sentence representations).
",4 Experimental Setup,[0],[0]
"Evaluation Metrics For document categorization and paraphrase identification tasks, we employ the percentage of correct predictions on the test set to evaluate and compare different models.
",4 Experimental Setup,[0],[0]
"For the answer sentence selection task, mean average precision (MAP) and mean reciprocal rank (MRR) are utilized as the corresponding evaluation metrics.",4 Experimental Setup,[0],[0]
"To explicitly explore whether our ACNN model can leverage the input-aware filter weights for better sentence representation, we perform a comparison between the basic CNN and ACNN models with only a single filter, which are denoted as SCNN, S-ACNN, respectively (this setting may not yield best overall performance, since only a single filter is used, but it allows us to isolate the impact of adaptivity).",5.1 Document Classification,[0],[0]
"As illustrated in Table 2, SACNN significantly outperforms S-CNN on both datasets, demonstrating the advantage of the filtergeneration module in our ACNN framework.",5.1 Document Classification,[0],[0]
"As a result, with only one convolutional filter and thus very limited modeling capacity, our S-ACNN model tends to be much more expressive than the basic CNN model, due to the flexibility of applying different filters to different sentences.
",5.1 Document Classification,[0],[0]
We further experiment on both ACNN and CNN models with multiple filters.,5.1 Document Classification,[0],[0]
The corresponding document categorization accuracies are presented in Table 2.,5.1 Document Classification,[0],[0]
"Although we only use one convolution layer for our ACNN model, it already outperforms other CNN baseline methods with much deeper architectures.",5.1 Document Classification,[0],[0]
"Moreover, our method ex-
hibits higher accuracy than n-grams, which is a very strong baseline as shown in (Zhang et al., 2015).",5.1 Document Classification,[0],[0]
We attribute the superior performance of the ACNN framework to its stronger (adaptive) feature-extraction ability.,5.1 Document Classification,[0],[0]
"Moreover, our MACNN also achieves slightly better performance than self-attentive sentence embeddings proposed in Lin et al. (2017), which requires significant more parameters than our method.
",5.1 Document Classification,[0],[0]
"Effect of number of filters To further demonstrate that the performance gains in document categorization experiments originates from the improved adaptivity of our ACNN framework, we implement the basic CNN model with different numbers of filter sizes, ranging from 1 to 1000.",5.1 Document Classification,[0],[0]
"As illustrated in Figure 3(a), when the filter size is larger than 100, the test accuracy of the standard CNN model does not show any noticeable improvement with more filters.",5.1 Document Classification,[0],[0]
"More importantly, even with a filter size of 1000, the classification accuracy of the CNN is worse than that of the ACNN model with the filter number restricted to 100.",5.1 Document Classification,[0],[0]
"Given these observations, we believe that the boosted categorization accuracy does come from the improved flexibility and thus better feature extraction of our ACNN framework.",5.1 Document Classification,[0],[0]
"To elucidate the role of different parts (modules) in our AdaQA model, we implement several model variants for comparison: (i) a “vanilla” CNN model that independently encodes two sentence representations for matching; (ii) a self-adaptive ACNN-based model where the question/answer sentence generates adaptive filters only to convolve with the input itself; (iii) a one-way ACNN model where only the answer sentence representation is extracted with adaptive filters, which
are generated conditioned on the question; (iv) a two-way AdaQA model as described in Section 2.4, where both sentences are adaptively encoded, with filters generated conditioned on the other sequence; (v) considering that the proposed filter generation mechanism is complementary to the attention layer typically employed in sequence matching tasks (see Section 3.4), we experiment with another model variant that combines the proposed context-aware filter generation mechanism with the multi-perspective attention layer introduced in (Wang et al., 2017b).
",5.2 Answer Sentence Selection,[0],[0]
"Tables 3 and 4 show experimental results of our models on WikiQA and SelQA datasets, along with other state-of-the-art methods.",5.2 Answer Sentence Selection,[0],[0]
"Note that the self-adaptive ACNN model variant, which generates filters only for the input itself (without any interactions before the top matching module), slightly outperforms the vanilla CNN Siamese model.",5.2 Answer Sentence Selection,[0],[0]
"Combined with the results in document categorization experiments, we believe that our ACNN framework, in its simplest form, can be utilized as a powerful feature extractor for transforming natural language sentences into fixed-length vectors.",5.2 Answer Sentence Selection,[0],[0]
"More importantly, our two-way AdaQA model exhibits superior results compared with the one-way variant as well as other CNN-based baseline models on the WikiQA dataset.",5.2 Answer Sentence Selection,[0],[0]
This observation indicates that the bidirectional filter generation mechanism is strongly associated with the performance gains.,5.2 Answer Sentence Selection,[0],[0]
"While combined with the multi-perspective attention layers, adopted after the ACNN encoding layer, our two-way AdaQA model achieves even better performance.",5.2 Answer Sentence Selection,[0],[0]
"This suggests that the proposed strategy is complemen-
tary, in terms of the incorporation of rich contextual information, to the standard attention mechanism.",5.2 Answer Sentence Selection,[0],[0]
"The same trend is also observed on the SelQA dataset (as shown in Table 4), which is a much larger dataset than WikiQA.
",5.2 Answer Sentence Selection,[0],[0]
"Notably, our model yields significantly better results than an attentive pooling network and ABCNN (attention-based CNN) baselines.",5.2 Answer Sentence Selection,[0],[0]
"We attribute the improvement to two potential advantages of our AdaQA model: (i) for the two previous baseline methods, the interaction between question and answer takes place either before or after convolution.",5.2 Answer Sentence Selection,[0],[0]
"However, in our AdaQA model, the communication between two sentences is inherent in the convolution operation, and thus can provide the abstracted features with more flexibility; (ii) the bidirectional filter generation mechanism in our AdaQA model generates co-dependent representations for the question and candidate answer, which could enable the model to recover from initial local maxima corresponding to incorrect predictions (Xiong et al., 2017).
",5.2 Answer Sentence Selection,[0],[0]
"Paragraph Identification Considering that the proposed AdaQA model can be readily generalized to other text sequence matching problems, we further evaluate the proposed framework on the paraphrase identification task with the Quora question pairs dataset.",5.2 Answer Sentence Selection,[0],[0]
"To ensure a fair comparison, we employ the same data splits as in (Wang et al., 2017b).",5.2 Answer Sentence Selection,[0],[0]
"As illustrated in Table 5, our twoway AdaQA model again exhibits superior performances compared with basic CNN models (as reported in (Wang et al., 2017b)).",5.2 Answer Sentence Selection,[0],[0]
"Reasoning ability To associate the improved answer sentence selection results with the reasoning capabilities of our AdaQA model, we further categorize the questions in the WikiQA test set into 5 types containing: ‘What’, ‘Where’, ‘How’, ‘When’ or ‘Who’.",5.3 Discussion,[0],[0]
We then calculate the MAP scores of the basic CNN and our AdaQA model on different question types.,5.3 Discussion,[0],[0]
"Similar to the findings in (Miao et al., 2016), we observe that the ‘How’ question is the hardest to answer, with the lowest MAP scores.",5.3 Discussion,[0],[0]
"However, our AdaQA model improves most over the basic CNN on the ‘How’ type question, see Figure 3(b).",5.3 Discussion,[0],[0]
"Further comparing our results with NASM in (Miao et al., 2016), our AdaQA model (with a MAP score of 0.579) outperforms their reported ‘How’ question MAP scores (0.524) by a large margin, indicating that the adaptive convolutional filter-generation mechanism improves the model’s ability to read and reason over natural language sentences.
",5.3 Discussion,[0],[0]
"Filter visualization To better understand what information has been encoded into our contextaware filters, we visualize one of the filters for sentences within the test set (on the DBpedia dataset) with t-SNE.",5.3 Discussion,[0],[0]
The corresponding results are shown in Figure 3(c).,5.3 Discussion,[0],[0]
"It can be observed that the filters for documents with the same label (ontology) are grouped into clusters, indicating that for different types of document, ACNN has leveraged distinct convolutional filters for better feature extraction.",5.3 Discussion,[0],[0]
"We presented a context-aware convolutional filtergeneration mechanism, introducing a meta network to adaptively produce a set of input-aware filters.",6 Conclusions,[0],[0]
"In this manner, the filter weights vary from sample to sample, providing the CNN encoder network with more modeling flexibility and capacity.
",6 Conclusions,[0],[0]
"This framework is further generalized to model question-answer sentence pairs, leveraging a twoway feature abstraction process.",6 Conclusions,[0],[0]
"We evaluate our models on several document-categorization and sentence matching benchmarks, and they consistently outperform the standard CNN and attentionbased CNN baselines, demonstrating the effectiveness of our framework.
",6 Conclusions,[0],[0]
"Acknowledgments This research was supported in part by DARPA, DOE, NIH, ONR and NSF.",6 Conclusions,[0],[0]
Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP).,abstractText,[0],[0]
"Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences.",abstractText,[0],[0]
"In this paper, we consider an approach of using a small meta network to learn contextaware convolutional filters for text processing.",abstractText,[0],[0]
The role of meta network is to abstract the contextual information of a sentence or document into a set of input-aware filters.,abstractText,[0],[0]
"We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations.",abstractText,[0],[0]
"In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-aware filters, consistently outperforms the standard CNN and attentionbased CNN baselines.",abstractText,[0],[0]
"By visualizing the learned context-aware filters, we further validate and rationalize the effectiveness of proposed framework.",abstractText,[0],[0]
Learning Context-Aware Convolutional Filters for Text Processing,title,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",1. Introduction,[0],[0]
This is particularly important while dealing with exponentially large domains such as source code and logical expressions.,1. Introduction,[0],[0]
Symbolic notation allows us to abstractly represent a large set of states that may be perceptually very different.,1. Introduction,[0],[0]
"Although symbolic reasoning is very powerful, it also tends to be hard.",1. Introduction,[0],[0]
"For example, problems such as the satisfiablity of boolean expressions and automated formal proofs tend to be NP-hard or worse.",1. Introduction,[0],[0]
"This raises the exciting opportunity of using pattern recognition within symbolic reasoning, that is, to learn patterns from datasets of symbolic expressions that approximately represent se-
Work started when M. Allamanis was at Edinburgh.",1. Introduction,[0],[0]
This work was done while P. Kohli was at Microsoft.,1. Introduction,[0],[0]
"1Microsoft Research, Cambridge, UK 2University of Edinburgh, UK 3DeepMind, London, UK 4The Alan Turing Institute, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Miltiadis Allamanis <t-mialla@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
mantic relationships.",1. Introduction,[0],[0]
"However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.",1. Introduction,[0],[0]
"In this work, we explore the direction of learning continuous semantic representations of symbolic expressions.",1. Introduction,[0],[0]
"The goal is for expressions with similar semantics to have similar continuous representations, even if their syntactic representation is very different.",1. Introduction,[0],[0]
"Such representations have the potential to allow a new class of symbolic reasoning methods based on heuristics that depend on the continuous representations, for example, by guiding a search procedure in a symbolic solver based on a distance metric in the continuous space.
",1. Introduction,[0],[0]
"In this paper, we make a first essential step of addressing the problem of learning continuous semantic representations (SEMVECs) for symbolic expressions.",1. Introduction,[0],[0]
"Our aim is, given access to a training set of pairs of expressions for which semantic equivalence is known, to assign continuous vectors to symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.",1. Introduction,[0],[0]
"This is an important but hard problem; learning composable SEMVECs of symbolic expressions requires that we learn about the semantics of symbolic elements and operators and how they map to the continuous representation space, thus encapsulating implicit knowledge about symbolic semantics and its recursive abstractive nature.",1. Introduction,[0],[0]
"As we show in our evaluation, relatively simple logical and polynomial expressions present significant challenges and their semantics cannot be sufficiently represented by existing neural network architectures.
",1. Introduction,[0],[0]
"Our work in similar in spirit to the work of Zaremba et al. (2014), who focus on learning expression representations to aid the search for computationally efficient identities.",1. Introduction,[0],[0]
"They use recursive neural networks (TREENN)1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.",1. Introduction,[0],[0]
"While they present impressive results, we find that the TREENN model fails when applied to more complex symbolic polynomial and boolean expressions.",1. Introduction,[0],[0]
"In particular, in our experiments we find that TREENNs tend to assign similar representations to syntactically similar expressions, even when they are semantically very different.",1. Introduction,[0],[0]
"The underlying conceptual problem is how to develop a con-
1To avoid confusion, we use TREENN for recursive neural networks and RNN for recurrent neural networks.
",1. Introduction,[0],[0]
"tinuous representation that follows syntax but not too much, that respects compositionality while also representing the fact that a small syntactic change can be a large semantic one.
",1. Introduction,[0],[0]
"To tackle this problem, we propose a new architecture, called neural equivalence networks (EQNET).",1. Introduction,[0],[0]
"EQNETs learn how syntactic composition recursively composes SEMVECs, like a TREENN, but are also designed to model large changes in semantics as the network progresses up the syntax tree.",1. Introduction,[0],[0]
"As equivalence is transitive, we formulate an objective function for training based on equivalence classes rather than pairwise decisions.",1. Introduction,[0],[0]
"The network architecture is based on composing residual-like multi-layer networks, which allows more flexibility in modeling the semantic mapping up the syntax tree.",1. Introduction,[0],[0]
"To encourage representations within an equivalence class to be tightly clustered, we also introduce a training method that we call subexpression autoencoding, which uses an autoencoder to force the representation of each subexpression to be predictable and reversible from its syntactic neighbors.",1. Introduction,[0],[0]
"Experimental evaluation on a highly diverse class of symbolic algebraic and boolean expression types shows that EQNETs dramatically outperform existing architectures like TREENNs and RNNs.
",1. Introduction,[0],[0]
"To summarize, the main contributions of our work are: (a) We formulate the problem of learning continuous semantic representations (SEMVECs) of symbolic expressions and develop benchmarks for this task.",1. Introduction,[0],[0]
"(b) We present neural equivalence networks (EQNETs), a neural network architecture that learns to represent expression semantics onto a continuous semantic representation space and how to perform symbolic operations in this space.",1. Introduction,[0],[0]
"(c) We provide an extensive evaluation on boolean and polynomial expressions, showing that EQNETs perform dramatically better than state-of-the-art alternatives.",1. Introduction,[0],[0]
Code and data are available at groups.inf.ed.ac.uk/cup/semvec.,1. Introduction,[0],[0]
"In this work, we are interested in learning semantic, compositional representations of mathematical expressions, which we call SEMVECs, and in learning to generate identical representations for expressions that are semantically equivalent, i.e. they belong to the same equivalence class.",2. Model,[0],[0]
"Equivalence is a stronger property than similarity, which has been the focus of previous work in neural network learning (Chopra et al., 2005), since equivalence is additionally a transitive relationship.
",2. Model,[0],[0]
Problem Hardness.,2. Model,[0],[0]
Finding the equivalence of arbitrary symbolic expressions is a NP-hard problem or worse.,2. Model,[0],[0]
"For example, if we focus on boolean expressions, reducing an expression to the representation of the false equivalence class amounts to proving its non-satisfiability — an NPcomplete problem.",2. Model,[0],[0]
"Of course, we do not expect to circum-
vent an NP-complete problem with neural networks.",2. Model,[0],[0]
A network for solving boolean equivalence would require an exponential number of nodes in the size of the expression if P 6= NP .,2. Model,[0],[0]
"Instead, our goal is to develop architectures that efficiently learn to solve the equivalence problems for expressions that are similar to a smaller number of expressions in a given training set.",2. Model,[0],[0]
"The supplementary material shows a sample of such expressions that illustrate the hardness of this problem.
",2. Model,[0],[0]
Notation and Framework.,2. Model,[0],[0]
"To allow our representations to be compositional, we employ the general framework of recursive neural networks (TREENN)",2. Model,[0],[0]
"(Socher et al., 2012; 2013), in our case operating on tree structures of the syntactic parse of a formula.",2. Model,[0],[0]
"Given a tree T , TREENNs learn distributed representations for each node in the tree by recursively combining the representations of its subtrees using a neural network.",2. Model,[0],[0]
We denote the children of a node n as ch(n) which is a (possibly empty) ordered tuple of nodes.,2. Model,[0],[0]
"We also use par(n) to refer to the parent node of n. Each node in our tree has a type, e.g. a terminal node could be of type “a” referring to the variable a or of type “and” referring to a node of the logical AND (∧) operation.",2. Model,[0],[0]
We refer to the type of a node n as τn.,2. Model,[0],[0]
"In pseudocode, TREENNs retrieve the representation of a tree T rooted at node ρ, by invoking the function TREENN(ρ) that returns a vector representation rρ ∈ RD, i.e., a SEMVEC.",2. Model,[0],[0]
"The function is defined as TREENN (current node n)
if n is not a leaf then rn ← COMBINE(TREENN(c0), . . .",2. Model,[0],[0]
", TREENN(ck), τn), where (c0, . . .",2. Model,[0],[0]
", ck) = ch(n) else rn ← LOOKUPLEAFEMBEDDING(τn)
return rn The general framework of TREENN allows two points of variation, the implementation of LOOKUPLEAFEMBEDDING and COMBINE.",2. Model,[0],[0]
"Traditional TREENNs (Socher et al., 2013) define LOOKUPLEAFEMBEDDING as a simple lookup operation within a matrix of embeddings and COMBINE as a single-layer neural network.",2. Model,[0],[0]
"As discussed next, these will both prove to be serious limitations in our setting.",2. Model,[0],[0]
"To train these networks to learn SEMVECs, we will use a supervised objective based on a set of known equivalence relations (see Section 2.2).",2. Model,[0],[0]
"Our domain requires that the network learns to abstract away syntax, assigning identical representations to expressions that may be syntactically different but semantically equivalent, and also assigning different representations to expressions that may be syntactically very similar but nonequivalent.",2.1. Neural Equivalence Networks,[0],[0]
"In this work, we find that standard neural architectures do not handle well this challenge.",2.1. Neural Equivalence Networks,[0],[0]
"To represent semantics from syntax, we need to learn to recursively
compose and decompose semantic representations and remove syntactic “noise”.",2.1. Neural Equivalence Networks,[0],[0]
"Any syntactic operation may significantly change semantics (e.g. negation, or appending ∧FALSE) while we may reach the same semantic state through many possible operations.",2.1. Neural Equivalence Networks,[0],[0]
This necessitates using high-curvature operations over the semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Furthermore, some operations are semantically reversible and thus we need to learn reversible semantic representations (e.g. ¬¬A and A should have an identical SEMVECs).",2.1. Neural Equivalence Networks,[0],[0]
"Based on these, we define neural equivalence networks (EQNET), which learn to compose representations of equivalence classes into new equivalence classes (Figure 1a).",2.1. Neural Equivalence Networks,[0],[0]
"Our network follows the TREENN architecture, i.e. is implemented using TREENN to model the compositional nature of symbolic expressions but is adapted based on the domain requirements.",2.1. Neural Equivalence Networks,[0],[0]
"The extensions we introduce have two aims: first, to improve the network training; and second, and more interestingly, to encourage the learned representations to abstract away surface level information while retaining semantic content.
",2.1. Neural Equivalence Networks,[0],[0]
The first extension that we introduce is to the network structure at each layer in the tree.,2.1. Neural Equivalence Networks,[0],[0]
"Traditional TREENNs (Socher et al., 2013) use a single-layer neural network at each tree node.",2.1. Neural Equivalence Networks,[0],[0]
"During our preliminary investigations and in Section 3, we found that single layer networks are not adequately expressive to capture all operations that transform the input SEMVECs to the output SEMVEC and maintain semantic equivalences, requiring high-curvature operations.",2.1. Neural Equivalence Networks,[0],[0]
Part of the problem stems from the fact that within the Euclidean space of SEMVECs some operations need to be non-linear.,2.1. Neural Equivalence Networks,[0],[0]
For example a simple XOR boolean operator requires high-curvature operations in the continuous semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Instead, we turn to multi-layer neural
networks.",2.1. Neural Equivalence Networks,[0],[0]
"In particular, we define the network as shown in the function COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
This uses a twolayer MLP with a residual-like connection to compute the SEMVEC of each parent node in that syntax tree given that of its children.,2.1. Neural Equivalence Networks,[0],[0]
"Each node type τn, e.g., each logical operator, has a different set of weights.",2.1. Neural Equivalence Networks,[0],[0]
"We experimented with deeper networks but this did not yield any improvements.
",2.1. Neural Equivalence Networks,[0],[0]
"However, as TREENNs become deeper, they suffer from optimization issues, such as diminishing and exploding gradients.",2.1. Neural Equivalence Networks,[0],[0]
"This is essentially because of the highly compositional nature of tree structures, where the same network (i.e. the COMBINE non-linear function) is used recursively, causing it to “echo” its own errors and producing unstable feedback loops.",2.1. Neural Equivalence Networks,[0],[0]
"We observe this problem even with only two-layer MLPs, as the overall network can become quite deep when using two layers for each node in the syntax tree.",2.1. Neural Equivalence Networks,[0],[0]
We resolve this issue in the training procedure by constraining each SEMVEC to have unit norm.,2.1. Neural Equivalence Networks,[0],[0]
"That is, we set LOOKUPLEAFEMBEDDING(τn) = Cτn/ ‖Cτn‖2 , and we normalize the output of the final layer of COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
"The normalization step of l̄out and Cτn is somewhat similar to weight normalization (Salimans & Kingma, 2016) and vaguely resembles layer normalization (Ba et al., 2016).",2.1. Neural Equivalence Networks,[0],[0]
"Normalizing the SEMVECs partially resolves issues with diminishing and exploding gradients, and removes a spurious degree of freedom in the semantic representation.",2.1. Neural Equivalence Networks,[0],[0]
"As simple as this modification may seem, we found it vital for obtaining good performance, and all of our multi-layer TREENNs converged to low-performing settings without it.
",2.1. Neural Equivalence Networks,[0],[0]
"Although these modifications seem to improve the representation capacity of the network and its ability to be trained, we found that they were not on their own sufficient for good
performance.",2.1. Neural Equivalence Networks,[0],[0]
"In our early experiments, we noticed that the networks were primarily focusing on syntax instead of semantics, i.e., expressions that were nearby in the continuous space were primarily ones that were syntactically similar.",2.1. Neural Equivalence Networks,[0],[0]
"At the same time, we observed that the networks did not learn to unify representations of the same equivalence class, observing multiple syntactically distinct but semantically equivalent expressions to have distant SEMVECs.
",2.1. Neural Equivalence Networks,[0],[0]
"Therefore we modify the training objective in order to encourage the representations to become more abstract, reducing their dependence on surface-level syntactic information.",2.1. Neural Equivalence Networks,[0],[0]
We add a regularization term on the SEMVECs that we call a subexpression autoencoder (SUBEXPAE).,2.1. Neural Equivalence Networks,[0],[0]
We design this regularization to encourage the SEMVECs to have two properties: abstraction and reversibility.,2.1. Neural Equivalence Networks,[0],[0]
"Because abstraction arguably means removing irrelevant information, a network with a bottleneck layer seems natural, but we want the training objective to encourage the bottleneck to discard syntactic information rather than semantic information.",2.1. Neural Equivalence Networks,[0],[0]
"To achieve this, we introduce a component that aims to encourage reversibility, which we explain by an example.",2.1. Neural Equivalence Networks,[0],[0]
"Observe that given the semantic representation of any two of the three nodes of a subexpression (by which we mean the parent, left child, right child of an expression tree)",2.1. Neural Equivalence Networks,[0],[0]
it is often possible to completely determine or at least place strong constraints on the semantics of the third.,2.1. Neural Equivalence Networks,[0],[0]
"For example, consider a boolean formula F (a, b) = F1(a, b) ∨ F2(a, b) where F1 and F2 are arbitrary propositional formulae over the variables a, b.",2.1. Neural Equivalence Networks,[0],[0]
"Then clearly if we know that F implies that a is true but F1 does not, then F2 must imply that a is true.",2.1. Neural Equivalence Networks,[0],[0]
"More generally, if F belongs to some equivalence class e0 and F1 belongs to a different class e1, we want the continuous representation of F2 to reflect that there are strong constraints on the equivalence class of F2.
",2.1. Neural Equivalence Networks,[0],[0]
"Subexpression autoencoding encourages abstraction by employing an autoencoder with a bottleneck, thereby removing irrelevant information from the representations, and encourages reversibility by autoencoding the parent and child representations together, to encourage dependence in the representations of parents and children.",2.1. Neural Equivalence Networks,[0],[0]
"More specifically, given any node p in the tree with children c0 . . .",2.1. Neural Equivalence Networks,[0],[0]
"ck, we can define a parent-children tuple",2.1. Neural Equivalence Networks,[0],[0]
"[rc0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", rck , rp] containing the (computed) SEMVECs of the children and parent nodes.",2.1. Neural Equivalence Networks,[0],[0]
What SUBEXPAE does is to autoencode this representation tuple into a low-dimensional space with a denoising autoencoder.,2.1. Neural Equivalence Networks,[0],[0]
"We then seek to minimize the reconstruction error of the child representations (r̃c0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", r̃ck ) as well as the reconstructed parent representation r̃p that can be computed from the reconstructed children.",2.1. Neural Equivalence Networks,[0],[0]
"More formally, we minimize the return value of SUBEXPAE in Figure 1c where n is a binary noise vector with κ percent of its elements set to zero.",2.1. Neural Equivalence Networks,[0],[0]
Note that the encoder is specific to the parent node type τp.,2.1. Neural Equivalence Networks,[0],[0]
"Although our SUBEXPAE may seem similar to the recursive autoencoders of Socher et al. (2011), it differs
in two major ways.",2.1. Neural Equivalence Networks,[0],[0]
"First, SUBEXPAE autoencodes on the entire parent-children representation tuple, rather than the child representations alone.",2.1. Neural Equivalence Networks,[0],[0]
"Second, the encoding is not used to compute the parent representation, but only serves as a regularizer.
",2.1. Neural Equivalence Networks,[0],[0]
Subexpression autoencoding has several desirable effects.,2.1. Neural Equivalence Networks,[0],[0]
"First, it forces each parent-children tuple to lie in a lowdimensional space, requiring the network to compress information from the individual subexpressions.",2.1. Neural Equivalence Networks,[0],[0]
"Second, because the denoising autoencoder is reconstructing parent and child representations together, this encourages child representations to be predictable from parents and siblings.",2.1. Neural Equivalence Networks,[0],[0]
"Putting these two together, the goal is that the information discarded by the autoencoder bottleneck will be more syntactic than semantic, assuming that the semantics of child node is more predictable from its parent and sibling than its syntactic realization.",2.1. Neural Equivalence Networks,[0],[0]
"The goal is to nudge the network to learn consistent, reversible semantics.",2.1. Neural Equivalence Networks,[0],[0]
"Additionally, subexpression autoencoding has the potential to gradually unify distant representations that belong to the same equivalence class.",2.1. Neural Equivalence Networks,[0],[0]
"To illustrate this point, imagine two semantically equivalent c′0 and c ′′ 0 child nodes of different expressions that
have distant SEMVECs, i.e. ∥∥rc′0 − rc′′0 ∥∥2 although COMBINE(rc′0 , . . . )",2.1. Neural Equivalence Networks,[0],[0]
"≈ COMBINE(rc′′0 , . . . ).",2.1. Neural Equivalence Networks,[0],[0]
"In some cases due to the autoencoder noise, the differences between the input tuple x′,x′′ that contain rc′0 and rc′′0 will be non-existent and the decoder will predict a single location r̃c0 (possibly different from rc′0 and rc′′0 ).",2.1. Neural Equivalence Networks,[0],[0]
"Then, when minimizing the reconstruction error, both rc′0 and rc′′0 will be attracted to r̃c0 and eventually should merge.",2.1. Neural Equivalence Networks,[0],[0]
We train EQNETs from a dataset of expressions whose semantic equivalence is known.,2.2. Training,[0],[0]
Given a training set T = {T1 . . .,2.2. Training,[0],[0]
"TN} of parse trees of expressions, we assume that the training set is partitioned into equivalence classes E = {e1 . . .",2.2. Training,[0],[0]
eJ}.,2.2. Training,[0],[0]
"We use a supervised objective similar to classification; the difference between classification and our setting is that whereas standard classification problems consider a fixed set of class labels, in our setting the number of equivalence classes in the training set will vary with N .",2.2. Training,[0],[0]
"Given an expression tree T that belongs to the equivalence class ei ∈ E , we compute the probability
P (ei|T ) =",2.2. Training,[0],[0]
"exp
( TREENN(T )>qei + bi )∑ j exp ( TREENN(T )>",2.2. Training,[0],[0]
qej,2.2. Training,[0],[0]
"+ bj
) (1) where qei are model parameters that we can interpret as representations of each equivalence class that appears in the training class, and bi are scalar bias terms.",2.2. Training,[0],[0]
"Note that in this work, we only use information about the equivalence class of the whole expression T , ignoring available information about subexpressions.",2.2. Training,[0],[0]
"This is without loss of generality, because if we do know the equivalence class of a subexpression of T , we can simply add that subexpression to
the training set.",2.2. Training,[0],[0]
"To train the model, we use a max-margin objective that maximizes classification accuracy, i.e.
LACC(T, ei) = max (
0, arg max ej 6=ei,ej∈E log P (ej |T ) P (ei|T ) +m ) (2)
where m > 0 is a scalar margin.",2.2. Training,[0],[0]
"And therefore the optimized loss function for a single expression tree T that belongs to equivalence class ei ∈ E is
L(T, ei) = LACC(T, ei) + µ |Q| ∑ n∈Q SUBEXPAE(ch(n), n)
(3)
",2.2. Training,[0],[0]
"where Q = {n ∈ T : | ch(n)| > 0}, i.e. contains the nonleaf nodes of T and µ ∈ (0, 1] a scalar weight.",2.2. Training,[0],[0]
"We found that subexpression autoencoding is counterproductive early in training, before the SEMVECs begin to represent aspects of semantics.",2.2. Training,[0],[0]
"So, for each epoch t, we set µ = 1− 10−νt with ν ≥ 0.",2.2. Training,[0],[0]
"Instead of the supervised objective that we propose, an alternative option for training EQNET would be a Siamese objective (Chopra et al., 2005) that learns about similarities (rather than equivalence) between expressions.",2.2. Training,[0],[0]
"In practice, we found the optimization to be very unstable, yielding suboptimal performance.",2.2. Training,[0],[0]
We believe that this has to do with the compositional and recursive nature of the task that creates unstable dynamics and the fact that equivalence is a stronger property than similarity.,2.2. Training,[0],[0]
Datasets.,3. Evaluation,[0],[0]
We generate datasets of expressions grouped into equivalence classes from two domains.,3. Evaluation,[0],[0]
The datasets from the BOOL domain contain boolean expressions and the POLY datasets contain polynomial expressions.,3. Evaluation,[0],[0]
"In both domains, an expression is either a variable, a binary operator that combines two expressions, or a unary operator applied to a single expression.",3. Evaluation,[0],[0]
"When defining equivalence, we interpret distinct variables as referring to different entities in the domain, so that, e.g., the polynomials c · (a · a+ b) and f ·(d·d+e) are not equivalent.",3. Evaluation,[0],[0]
"For each domain, we generate “simple” datasets which use a smaller set of possible operators and “standard” datasets which use a larger set of more complex operators.",3. Evaluation,[0],[0]
We generate each dataset by exhaustively generating all parse trees up to a maximum tree size.,3. Evaluation,[0],[0]
All expressions are symbolically simplified into a canonical from in order to determine their equivalence class and are grouped accordingly.,3. Evaluation,[0],[0]
Table 1 shows the datasets we generated.,3. Evaluation,[0],[0]
In the supplementary material we present some sample expressions.,3. Evaluation,[0],[0]
"For the polynomial domain, we also generated ONEV-POLY datasets, which are polynomials over a single variable, since they are similar to the setting considered by Zaremba et al. (2014) — although ONEV-POLY is still a little more general because it is not restricted to homogeneous polynomials.",3. Evaluation,[0],[0]
"Learning SEMVECs for boolean expressions
is already a hard problem; with n boolean variables, there are 22 n
equivalence classes (i.e. one for each possible truth table).",3. Evaluation,[0],[0]
"We split the datasets into training, validation and test sets.",3. Evaluation,[0],[0]
"We create two test sets, one to measure generalization performance on equivalence classes that were seen in the training data (SEENEQCLASS), and one to measure generalization to unseen equivalence classes (UNSEENEQCLASS).",3. Evaluation,[0],[0]
It is easiest to describe UNSEENEQCLASS first.,3. Evaluation,[0],[0]
"To create the UNSEENEQCLASS, we randomly select 20% of all the equivalence classes, and place all of their expressions in the test set.",3. Evaluation,[0],[0]
We select equivalence classes only if they contain at least two expressions but less than three times the average number of expressions per equivalence class.,3. Evaluation,[0],[0]
We thus avoid selecting very common (and hence trivial to learn) equivalence classes in the testset.,3. Evaluation,[0],[0]
"Then, to create SEENEQCLASS, we take the remaining 80% of the equivalence classes, and randomly split the expressions in each class into training, validation, SEENEQCLASS test in the proportions 60%–15%–25%.",3. Evaluation,[0],[0]
"We provide the datasets online at groups.inf.ed.ac.uk/cup/semvec.
Baselines.",3. Evaluation,[0],[0]
"To compare the performance of our model, we train the following baselines.",3. Evaluation,[0],[0]
"TF-IDF: learns a representation given the expression tokens (variables, operators and parentheses).",3. Evaluation,[0],[0]
This captures topical/declarative knowledge but is unable to capture procedural knowledge.,3. Evaluation,[0],[0]
GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation.,3. Evaluation,[0],[0]
Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens.,3. Evaluation,[0],[0]
We also include two recursive neural networks (TREENN).,3. Evaluation,[0],[0]
The 1- layer TREENN which is the original TREENN also used by Zaremba et al. (2014).,3. Evaluation,[0],[0]
"We also include a 2-layer TREENN, where COMBINE is a classic two-layer MLP without residual connections.",3. Evaluation,[0],[0]
"This shows the effect of SEMVEC normalization and subexpression autoencoder.
Hyperparameters.",3. Evaluation,[0],[0]
"We tune the hyperparameters of all models using Bayesian optimization (Snoek et al., 2012) on a boolean dataset with 5 variables and maximum tree size of 7 (not shown in Table 1) using the average k-NN (k = 1, . . .",3. Evaluation,[0],[0]
", 15) statistics (described next).",3. Evaluation,[0],[0]
The selected hyperparameters are detailed in the supplementary material.,3. Evaluation,[0],[0]
Metrics.,3.1. Quantitative Evaluation,[0],[0]
To evaluate the quality of the learned representations we count the proportion of k nearest neighbors of each expression (using cosine similarity) that belong to the same equivalence class.,3.1. Quantitative Evaluation,[0],[0]
"More formally, given a test query expression q in an equivalence class c we find the k nearest neighbors Nk(q) of q across all expressions, and define the
score as
scorek(q) = |Nk(q) ∩ c| min(k, |c|) .",3.1. Quantitative Evaluation,[0],[0]
"(4)
To report results for a given testset, we simply average scorek(q) for all expressions q in the testset.",3.1. Quantitative Evaluation,[0],[0]
"We also report the precision-recall curves for the problem of clustering the SEMVECs into their appropriate equivalence classes.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation.,3.1. Quantitative Evaluation,[0],[0]
Figure 2 presents the average per-model precision-recall curves across the datasets.,3.1. Quantitative Evaluation,[0],[0]
Table 1 shows score5 of UNSEENEQCLASS.,3.1. Quantitative Evaluation,[0],[0]
Detailed plots are found in the supplementary material.,3.1. Quantitative Evaluation,[0],[0]
"EQNET performs better for all datasets, by a large margin.",3.1. Quantitative Evaluation,[0],[0]
"The only exception is POLY5, where the 2-L TREENN performs better.",3.1. Quantitative Evaluation,[0],[0]
"However, this may have to do with the small size of the dataset.",3.1. Quantitative Evaluation,[0],[0]
The reader may observe that the simple datasets (containing fewer operations and variables) are easier to learn.,3.1. Quantitative Evaluation,[0],[0]
"Understandably, introducing more variables increases the size of the represented space reducing performance.",3.1. Quantitative Evaluation,[0],[0]
"The tf-idf method performs better in settings with more variables, because it captures well the variables and operations used.",3.1. Quantitative Evaluation,[0],[0]
Similar observations can be made for sequence models.,3.1. Quantitative Evaluation,[0],[0]
The one and two layer TREENNs have mixed performance; we believe that this has to do with exploding and diminishing gradients due to the deep and highly compositional nature of TREENNs.,3.1. Quantitative Evaluation,[0],[0]
"Although Zaremba et al. (2014) consider a different problem to us, they use data similar to the ONEV-POLY datasets with a traditional TREENN architecture.",3.1. Quantitative Evaluation,[0],[0]
"Our evaluation suggests that EQNETs perform much better within the ONEV-POLY setting.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation of Compositionality.,3.1. Quantitative Evaluation,[0],[0]
"We evaluate whether EQNETs successfully learn to compute compositional representations, rather than overfitting to expression trees of
a small size.",3.1. Quantitative Evaluation,[0],[0]
"To do this we consider a type of transfer setting, in which we train on simpler datasets, but test on more complex ones; for example, training on the training set of BOOL5 but testing on the testset of BOOL8.",3.1. Quantitative Evaluation,[0],[0]
We average over 11 different train-test pairs (full list in supplementary material) and show the results in Figure 3a and Figure 3b.,3.1. Quantitative Evaluation,[0],[0]
"These graphs again show that EQNETs are better than any of the other methods, and indeed, performance is only a bit worse than in the non-transfer setting.
",3.1. Quantitative Evaluation,[0],[0]
"Impact of EQNET Components EQNETs differ from traditional TREENNs in two major ways, which we analyze here.",3.1. Quantitative Evaluation,[0],[0]
"First, SUBEXPAE improves performance.",3.1. Quantitative Evaluation,[0],[0]
"When training the network with and without SUBEXPAE, on average, the area under the curve (AUC) of scorek decreases by 16.8% on the SEENEQCLASS and 19.7% on the UNSEENEQCLASS.",3.1. Quantitative Evaluation,[0],[0]
"This difference is smaller in the transfer setting, where AUC decreases by 8.8% on average.",3.1. Quantitative Evaluation,[0],[0]
"However, even in this setting we observe that SUBEXPAE helps more in large and diverse datasets.",3.1. Quantitative Evaluation,[0],[0]
The second key difference to traditional TREENNs is the output normalization and the residual connections.,3.1. Quantitative Evaluation,[0],[0]
"Comparing our model to the one-layer and two-layer TREENNs again, we find that output normalization results in important improvements (the two-layer TREENNs have on average 60.9% smaller AUC).",3.1. Quantitative Evaluation,[0],[0]
"We note that only the combination of the residual connections and the output normalization improve the performance, whereas when used separately, there are no significant improvements over the two-layer TREENNs.",3.1. Quantitative Evaluation,[0],[0]
Table 2 shows expressions whose SEMVEC nearest neighbor is of an expression of another equivalence class.,3.2. Qualitative Evaluation,[0],[0]
"Manually inspecting boolean expressions, we find that EQNET confusions happen more when a XOR or implication operator is
Table 2.",3.2. Qualitative Evaluation,[0],[0]
Non semantically equivalent first nearest-neighbors from BOOL8 and POLY8.,3.2. Qualitative Evaluation,[0],[0]
"A checkmark indicates that the method correctly results in the nearest neighbor being from the same equivalence class.
",3.2. Qualitative Evaluation,[0],[0]
Expr a ∧ (a ∧ (a ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (a+ c)) ((a+ c) ·,3.2. Qualitative Evaluation,[0],[0]
"c) + a (b · b)− b
tfidf c",3.2. Qualitative Evaluation,[0],[0]
∧,3.2. Qualitative Evaluation,[0],[0]
((a ∧ a) ∧ (¬a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
a+ (c+ a) ·,3.2. Qualitative Evaluation,[0],[0]
c (c · a) + (a+ c) b · (b− b) GRU X a ∧ (a ∧ (c ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
b+ (c · (a+ c)) ((b+ c) · c) + a (b+ b) · b− b 1L-TREENN a ∧ (a ∧ (a ∧ (¬b))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬b))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬b)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (b+ c)) ((b+ c) ·,3.2. Qualitative Evaluation,[0],[0]
c) + a (a− c) · b− b EQNET X X (¬(b⇒ (b ∨ c))),3.2. Qualitative Evaluation,[0],[0]
"∧ a X X (b · b) · b− b
involved.",3.2. Qualitative Evaluation,[0],[0]
"In fact, we fail to find any confused expressions for EQNET not involving these operations in BOOL5 and in the top 100 expressions in BOOL10.",3.2. Qualitative Evaluation,[0],[0]
"As expected, tf-idf confuses expressions with others that contain the same operators and variables ignoring order.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, GRU and TREENN tend to confuse expressions with very similar symbolic representations, i.e. that differ in one or two deeply nested variables or operators.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, EQNET tends to confuse fewer expressions (as we previously showed) and the confused expressions tend to be more syntactically diverse and semantically related.
",3.2. Qualitative Evaluation,[0],[0]
Figure 4 shows a visualization of score5 for each node in the expression tree.,3.2. Qualitative Evaluation,[0],[0]
"One may see that as EQNET knows how
¬(c ⊕ (a ∧ ((a ⊕ c) ∧ b)))",3.2. Qualitative Evaluation,[0],[0]
"((c ∨ (¬b))⇒ a) ∧ (a ⇒ a)
((b ⊕ (¬c))",3.2. Qualitative Evaluation,[0],[0]
"∧ b)⊕ (a ∨ b) ((b · a)− a) · b
a − ((a + b) · a) ((c · b) · c) · a b + ((b · b) · b)
Figure 4.",3.2. Qualitative Evaluation,[0],[0]
Visualization of score5 for all expression nodes for three BOOL10 and four POLY8 test sample expressions using EQNET.,3.2. Qualitative Evaluation,[0],[0]
"The darker the color, the lower the score, i.e. white implies a score of 1 and dark red a score of 0.
to compose expressions that achieve good score, even if the subexpressions achieve a worse score.",3.2. Qualitative Evaluation,[0],[0]
"This suggests that for common expressions, (e.g. single variables and monomials) the network tends to select a unique location, without merging the equivalence classes or affecting the upstream performance of the network.",3.2. Qualitative Evaluation,[0],[0]
"Larger scale interactive t-SNE visualizations can be found online.
",3.2. Qualitative Evaluation,[0],[0]
Figure 5 presents two PCA visualizations of the SEMVECs of simple expressions and their negations/negatives.,3.2. Qualitative Evaluation,[0],[0]
It can be discerned that the black dots and their negations (in red) are discriminated in the semantic representation space.,3.2. Qualitative Evaluation,[0],[0]
"Figure 5b shows this property in a clear manner: left-right discriminates between polynomials with 1 and −a, topbottom between polynomials with−b and b and the diagonal parellelt to y = −x between c and−c.",3.2. Qualitative Evaluation,[0],[0]
We observe a similar behavior in Figure 5a for boolean expressions.,3.2. Qualitative Evaluation,[0],[0]
"Researchers have proposed compilation schemes that can transform any given program or expression to an equivalent neural network (Gruau et al., 1995; Neto et al., 2003; Siegel-
mann, 1994).",4. Related Work,[0],[0]
One can consider a serialized version of the resulting neural network as a representation of the expression.,4. Related Work,[0],[0]
"However, it is not clear how we could compare the serialized representations corresponding to two expressions and whether this mapping preserves semantic distances.
",4. Related Work,[0],[0]
"Recursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications.",4. Related Work,[0],[0]
Socher et al. (2012) show that TREENNs can learn to compute the values of some simple propositional statements.,4. Related Work,[0],[0]
"EQNET’s SUBEXPAE may resemble recursive autoencoders (Socher et al., 2011) but differs in form and function, encoding the whole parent-children tuple to force a clustering behavior.",4. Related Work,[0],[0]
"In addition, when encoding each expression our architecture does not use a pooling layer but directly produces a single representation for the expression.
",4. Related Work,[0],[0]
Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks.,4. Related Work,[0],[0]
"Although they learn representations of the student tasks, these representations capture task-specific syntactic features rather than code semantics.",4. Related Work,[0],[0]
Piech et al. (2015) also learn distributed matrix representations of student code submissions.,4. Related Work,[0],[0]
"However, to learn the representations, they use input and output program states and do not test for program equivalence.",4. Related Work,[0],[0]
"Additionally, these representations do not necessarily represent program equivalence, since they do not learn the representations over all possible input-outputs.",4. Related Work,[0],[0]
"Allamanis et al. (2016) learn variable-sized representations of source code snippets to summarize them with a short function-like name but aim learn summarization features in code rather than representations of symbolic expression equivalence.
",4. Related Work,[0],[0]
"More closely related is the work of Zaremba et al. (2014) who use a TREENN to guide the search for more efficient mathematical identities, limited to homogeneous singlevariable polynomial expressions.",4. Related Work,[0],[0]
"In contrast, EQNETs consider at a much wider set of expressions, employ subexpression autoencoding to guide the learned SEMVECs to better
represent equivalence, and do not use search when looking for equivalent expressions.",4. Related Work,[0],[0]
Alemi et al. (2016) use RNNs and convolutional neural networks to detect features within mathematical expressions to speed the search for premise selection in automated theorem proving but do not explicitly account for semantic equivalence.,4. Related Work,[0],[0]
"In the future, SEMVECs may be useful within this area.
",4. Related Work,[0],[0]
"Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).",4. Related Work,[0],[0]
"In contrast to this work, we do not aim to learn how to evaluate expressions or execute programs with neural network architectures but to learn continuous semantic representations (SEMVECs) of expression semantics irrespectively of how they are syntactically expressed or evaluated.",4. Related Work,[0],[0]
"In this work, we presented EQNETs, a first step in learning continuous semantic representations (SEMVECs) of procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"SEMVECs have the potential of bridging continuous representations with symbolic representations, useful in multiple applications in artificial intelligence, machine learning and programming languages.
",5. Discussion & Conclusions,[0],[0]
We show that EQNETs perform significantly better than state-of-the-art alternatives.,5. Discussion & Conclusions,[0],[0]
"But further improvements are needed, especially for more robust training of compositional models.",5. Discussion & Conclusions,[0],[0]
"In addition, even for relatively small symbolic expressions, we have an exponential explosion of the semantic space to be represented.",5. Discussion & Conclusions,[0],[0]
"Fixed-sized SEMVECs, like the ones used in EQNET, eventually limit the capacity that is available to represent procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"In the future, to represent more complex procedures, variable-sized representations would seem to be required.",5. Discussion & Conclusions,[0],[0]
This work was supported by Microsoft Research through its PhD Scholarship Programme and the Engineering and Physical Sciences Research Council [grant number EP/K024043/1].,Acknowledgments,[0],[0]
We thank the University of Edinburgh Data Science EPSRC Centre for Doctoral Training for providing additional computational resources.,Acknowledgments,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",abstractText,[0],[0]
"As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions.",abstractText,[0],[0]
"These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different.",abstractText,[0],[0]
"The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures.",abstractText,[0],[0]
"We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.",abstractText,[0],[0]
Learning Continuous Semantic Representations of Symbolic Expressions,title,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",1. Introduction,[0],[0]
This is particularly important while dealing with exponentially large domains such as source code and logical expressions.,1. Introduction,[0],[0]
Symbolic notation allows us to abstractly represent a large set of states that may be perceptually very different.,1. Introduction,[0],[0]
"Although symbolic reasoning is very powerful, it also tends to be hard.",1. Introduction,[0],[0]
"For example, problems such as the satisfiablity of boolean expressions and automated formal proofs tend to be NP-hard or worse.",1. Introduction,[0],[0]
"This raises the exciting opportunity of using pattern recognition within symbolic reasoning, that is, to learn patterns from datasets of symbolic expressions that approximately represent se-
Work started when M. Allamanis was at Edinburgh.",1. Introduction,[0],[0]
This work was done while P. Kohli was at Microsoft.,1. Introduction,[0],[0]
"1Microsoft Research, Cambridge, UK 2University of Edinburgh, UK 3DeepMind, London, UK 4The Alan Turing Institute, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Miltiadis Allamanis <t-mialla@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
mantic relationships.",1. Introduction,[0],[0]
"However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.",1. Introduction,[0],[0]
"In this work, we explore the direction of learning continuous semantic representations of symbolic expressions.",1. Introduction,[0],[0]
"The goal is for expressions with similar semantics to have similar continuous representations, even if their syntactic representation is very different.",1. Introduction,[0],[0]
"Such representations have the potential to allow a new class of symbolic reasoning methods based on heuristics that depend on the continuous representations, for example, by guiding a search procedure in a symbolic solver based on a distance metric in the continuous space.
",1. Introduction,[0],[0]
"In this paper, we make a first essential step of addressing the problem of learning continuous semantic representations (SEMVECs) for symbolic expressions.",1. Introduction,[0],[0]
"Our aim is, given access to a training set of pairs of expressions for which semantic equivalence is known, to assign continuous vectors to symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.",1. Introduction,[0],[0]
"This is an important but hard problem; learning composable SEMVECs of symbolic expressions requires that we learn about the semantics of symbolic elements and operators and how they map to the continuous representation space, thus encapsulating implicit knowledge about symbolic semantics and its recursive abstractive nature.",1. Introduction,[0],[0]
"As we show in our evaluation, relatively simple logical and polynomial expressions present significant challenges and their semantics cannot be sufficiently represented by existing neural network architectures.
",1. Introduction,[0],[0]
"Our work in similar in spirit to the work of Zaremba et al. (2014), who focus on learning expression representations to aid the search for computationally efficient identities.",1. Introduction,[0],[0]
"They use recursive neural networks (TREENN)1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.",1. Introduction,[0],[0]
"While they present impressive results, we find that the TREENN model fails when applied to more complex symbolic polynomial and boolean expressions.",1. Introduction,[0],[0]
"In particular, in our experiments we find that TREENNs tend to assign similar representations to syntactically similar expressions, even when they are semantically very different.",1. Introduction,[0],[0]
"The underlying conceptual problem is how to develop a con-
1To avoid confusion, we use TREENN for recursive neural networks and RNN for recurrent neural networks.
",1. Introduction,[0],[0]
"tinuous representation that follows syntax but not too much, that respects compositionality while also representing the fact that a small syntactic change can be a large semantic one.
",1. Introduction,[0],[0]
"To tackle this problem, we propose a new architecture, called neural equivalence networks (EQNET).",1. Introduction,[0],[0]
"EQNETs learn how syntactic composition recursively composes SEMVECs, like a TREENN, but are also designed to model large changes in semantics as the network progresses up the syntax tree.",1. Introduction,[0],[0]
"As equivalence is transitive, we formulate an objective function for training based on equivalence classes rather than pairwise decisions.",1. Introduction,[0],[0]
"The network architecture is based on composing residual-like multi-layer networks, which allows more flexibility in modeling the semantic mapping up the syntax tree.",1. Introduction,[0],[0]
"To encourage representations within an equivalence class to be tightly clustered, we also introduce a training method that we call subexpression autoencoding, which uses an autoencoder to force the representation of each subexpression to be predictable and reversible from its syntactic neighbors.",1. Introduction,[0],[0]
"Experimental evaluation on a highly diverse class of symbolic algebraic and boolean expression types shows that EQNETs dramatically outperform existing architectures like TREENNs and RNNs.
",1. Introduction,[0],[0]
"To summarize, the main contributions of our work are: (a) We formulate the problem of learning continuous semantic representations (SEMVECs) of symbolic expressions and develop benchmarks for this task.",1. Introduction,[0],[0]
"(b) We present neural equivalence networks (EQNETs), a neural network architecture that learns to represent expression semantics onto a continuous semantic representation space and how to perform symbolic operations in this space.",1. Introduction,[0],[0]
"(c) We provide an extensive evaluation on boolean and polynomial expressions, showing that EQNETs perform dramatically better than state-of-the-art alternatives.",1. Introduction,[0],[0]
Code and data are available at groups.inf.ed.ac.uk/cup/semvec.,1. Introduction,[0],[0]
"In this work, we are interested in learning semantic, compositional representations of mathematical expressions, which we call SEMVECs, and in learning to generate identical representations for expressions that are semantically equivalent, i.e. they belong to the same equivalence class.",2. Model,[0],[0]
"Equivalence is a stronger property than similarity, which has been the focus of previous work in neural network learning (Chopra et al., 2005), since equivalence is additionally a transitive relationship.
",2. Model,[0],[0]
Problem Hardness.,2. Model,[0],[0]
Finding the equivalence of arbitrary symbolic expressions is a NP-hard problem or worse.,2. Model,[0],[0]
"For example, if we focus on boolean expressions, reducing an expression to the representation of the false equivalence class amounts to proving its non-satisfiability — an NPcomplete problem.",2. Model,[0],[0]
"Of course, we do not expect to circum-
vent an NP-complete problem with neural networks.",2. Model,[0],[0]
A network for solving boolean equivalence would require an exponential number of nodes in the size of the expression if P 6= NP .,2. Model,[0],[0]
"Instead, our goal is to develop architectures that efficiently learn to solve the equivalence problems for expressions that are similar to a smaller number of expressions in a given training set.",2. Model,[0],[0]
"The supplementary material shows a sample of such expressions that illustrate the hardness of this problem.
",2. Model,[0],[0]
Notation and Framework.,2. Model,[0],[0]
"To allow our representations to be compositional, we employ the general framework of recursive neural networks (TREENN)",2. Model,[0],[0]
"(Socher et al., 2012; 2013), in our case operating on tree structures of the syntactic parse of a formula.",2. Model,[0],[0]
"Given a tree T , TREENNs learn distributed representations for each node in the tree by recursively combining the representations of its subtrees using a neural network.",2. Model,[0],[0]
We denote the children of a node n as ch(n) which is a (possibly empty) ordered tuple of nodes.,2. Model,[0],[0]
"We also use par(n) to refer to the parent node of n. Each node in our tree has a type, e.g. a terminal node could be of type “a” referring to the variable a or of type “and” referring to a node of the logical AND (∧) operation.",2. Model,[0],[0]
We refer to the type of a node n as τn.,2. Model,[0],[0]
"In pseudocode, TREENNs retrieve the representation of a tree T rooted at node ρ, by invoking the function TREENN(ρ) that returns a vector representation rρ ∈ RD, i.e., a SEMVEC.",2. Model,[0],[0]
"The function is defined as TREENN (current node n)
if n is not a leaf then rn ← COMBINE(TREENN(c0), . . .",2. Model,[0],[0]
", TREENN(ck), τn), where (c0, . . .",2. Model,[0],[0]
", ck) = ch(n) else rn ← LOOKUPLEAFEMBEDDING(τn)
return rn The general framework of TREENN allows two points of variation, the implementation of LOOKUPLEAFEMBEDDING and COMBINE.",2. Model,[0],[0]
"Traditional TREENNs (Socher et al., 2013) define LOOKUPLEAFEMBEDDING as a simple lookup operation within a matrix of embeddings and COMBINE as a single-layer neural network.",2. Model,[0],[0]
"As discussed next, these will both prove to be serious limitations in our setting.",2. Model,[0],[0]
"To train these networks to learn SEMVECs, we will use a supervised objective based on a set of known equivalence relations (see Section 2.2).",2. Model,[0],[0]
"Our domain requires that the network learns to abstract away syntax, assigning identical representations to expressions that may be syntactically different but semantically equivalent, and also assigning different representations to expressions that may be syntactically very similar but nonequivalent.",2.1. Neural Equivalence Networks,[0],[0]
"In this work, we find that standard neural architectures do not handle well this challenge.",2.1. Neural Equivalence Networks,[0],[0]
"To represent semantics from syntax, we need to learn to recursively
compose and decompose semantic representations and remove syntactic “noise”.",2.1. Neural Equivalence Networks,[0],[0]
"Any syntactic operation may significantly change semantics (e.g. negation, or appending ∧FALSE) while we may reach the same semantic state through many possible operations.",2.1. Neural Equivalence Networks,[0],[0]
This necessitates using high-curvature operations over the semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Furthermore, some operations are semantically reversible and thus we need to learn reversible semantic representations (e.g. ¬¬A and A should have an identical SEMVECs).",2.1. Neural Equivalence Networks,[0],[0]
"Based on these, we define neural equivalence networks (EQNET), which learn to compose representations of equivalence classes into new equivalence classes (Figure 1a).",2.1. Neural Equivalence Networks,[0],[0]
"Our network follows the TREENN architecture, i.e. is implemented using TREENN to model the compositional nature of symbolic expressions but is adapted based on the domain requirements.",2.1. Neural Equivalence Networks,[0],[0]
"The extensions we introduce have two aims: first, to improve the network training; and second, and more interestingly, to encourage the learned representations to abstract away surface level information while retaining semantic content.
",2.1. Neural Equivalence Networks,[0],[0]
The first extension that we introduce is to the network structure at each layer in the tree.,2.1. Neural Equivalence Networks,[0],[0]
"Traditional TREENNs (Socher et al., 2013) use a single-layer neural network at each tree node.",2.1. Neural Equivalence Networks,[0],[0]
"During our preliminary investigations and in Section 3, we found that single layer networks are not adequately expressive to capture all operations that transform the input SEMVECs to the output SEMVEC and maintain semantic equivalences, requiring high-curvature operations.",2.1. Neural Equivalence Networks,[0],[0]
Part of the problem stems from the fact that within the Euclidean space of SEMVECs some operations need to be non-linear.,2.1. Neural Equivalence Networks,[0],[0]
For example a simple XOR boolean operator requires high-curvature operations in the continuous semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Instead, we turn to multi-layer neural
networks.",2.1. Neural Equivalence Networks,[0],[0]
"In particular, we define the network as shown in the function COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
This uses a twolayer MLP with a residual-like connection to compute the SEMVEC of each parent node in that syntax tree given that of its children.,2.1. Neural Equivalence Networks,[0],[0]
"Each node type τn, e.g., each logical operator, has a different set of weights.",2.1. Neural Equivalence Networks,[0],[0]
"We experimented with deeper networks but this did not yield any improvements.
",2.1. Neural Equivalence Networks,[0],[0]
"However, as TREENNs become deeper, they suffer from optimization issues, such as diminishing and exploding gradients.",2.1. Neural Equivalence Networks,[0],[0]
"This is essentially because of the highly compositional nature of tree structures, where the same network (i.e. the COMBINE non-linear function) is used recursively, causing it to “echo” its own errors and producing unstable feedback loops.",2.1. Neural Equivalence Networks,[0],[0]
"We observe this problem even with only two-layer MLPs, as the overall network can become quite deep when using two layers for each node in the syntax tree.",2.1. Neural Equivalence Networks,[0],[0]
We resolve this issue in the training procedure by constraining each SEMVEC to have unit norm.,2.1. Neural Equivalence Networks,[0],[0]
"That is, we set LOOKUPLEAFEMBEDDING(τn) = Cτn/ ‖Cτn‖2 , and we normalize the output of the final layer of COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
"The normalization step of l̄out and Cτn is somewhat similar to weight normalization (Salimans & Kingma, 2016) and vaguely resembles layer normalization (Ba et al., 2016).",2.1. Neural Equivalence Networks,[0],[0]
"Normalizing the SEMVECs partially resolves issues with diminishing and exploding gradients, and removes a spurious degree of freedom in the semantic representation.",2.1. Neural Equivalence Networks,[0],[0]
"As simple as this modification may seem, we found it vital for obtaining good performance, and all of our multi-layer TREENNs converged to low-performing settings without it.
",2.1. Neural Equivalence Networks,[0],[0]
"Although these modifications seem to improve the representation capacity of the network and its ability to be trained, we found that they were not on their own sufficient for good
performance.",2.1. Neural Equivalence Networks,[0],[0]
"In our early experiments, we noticed that the networks were primarily focusing on syntax instead of semantics, i.e., expressions that were nearby in the continuous space were primarily ones that were syntactically similar.",2.1. Neural Equivalence Networks,[0],[0]
"At the same time, we observed that the networks did not learn to unify representations of the same equivalence class, observing multiple syntactically distinct but semantically equivalent expressions to have distant SEMVECs.
",2.1. Neural Equivalence Networks,[0],[0]
"Therefore we modify the training objective in order to encourage the representations to become more abstract, reducing their dependence on surface-level syntactic information.",2.1. Neural Equivalence Networks,[0],[0]
We add a regularization term on the SEMVECs that we call a subexpression autoencoder (SUBEXPAE).,2.1. Neural Equivalence Networks,[0],[0]
We design this regularization to encourage the SEMVECs to have two properties: abstraction and reversibility.,2.1. Neural Equivalence Networks,[0],[0]
"Because abstraction arguably means removing irrelevant information, a network with a bottleneck layer seems natural, but we want the training objective to encourage the bottleneck to discard syntactic information rather than semantic information.",2.1. Neural Equivalence Networks,[0],[0]
"To achieve this, we introduce a component that aims to encourage reversibility, which we explain by an example.",2.1. Neural Equivalence Networks,[0],[0]
"Observe that given the semantic representation of any two of the three nodes of a subexpression (by which we mean the parent, left child, right child of an expression tree)",2.1. Neural Equivalence Networks,[0],[0]
it is often possible to completely determine or at least place strong constraints on the semantics of the third.,2.1. Neural Equivalence Networks,[0],[0]
"For example, consider a boolean formula F (a, b) = F1(a, b) ∨ F2(a, b) where F1 and F2 are arbitrary propositional formulae over the variables a, b.",2.1. Neural Equivalence Networks,[0],[0]
"Then clearly if we know that F implies that a is true but F1 does not, then F2 must imply that a is true.",2.1. Neural Equivalence Networks,[0],[0]
"More generally, if F belongs to some equivalence class e0 and F1 belongs to a different class e1, we want the continuous representation of F2 to reflect that there are strong constraints on the equivalence class of F2.
",2.1. Neural Equivalence Networks,[0],[0]
"Subexpression autoencoding encourages abstraction by employing an autoencoder with a bottleneck, thereby removing irrelevant information from the representations, and encourages reversibility by autoencoding the parent and child representations together, to encourage dependence in the representations of parents and children.",2.1. Neural Equivalence Networks,[0],[0]
"More specifically, given any node p in the tree with children c0 . . .",2.1. Neural Equivalence Networks,[0],[0]
"ck, we can define a parent-children tuple",2.1. Neural Equivalence Networks,[0],[0]
"[rc0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", rck , rp] containing the (computed) SEMVECs of the children and parent nodes.",2.1. Neural Equivalence Networks,[0],[0]
What SUBEXPAE does is to autoencode this representation tuple into a low-dimensional space with a denoising autoencoder.,2.1. Neural Equivalence Networks,[0],[0]
"We then seek to minimize the reconstruction error of the child representations (r̃c0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", r̃ck ) as well as the reconstructed parent representation r̃p that can be computed from the reconstructed children.",2.1. Neural Equivalence Networks,[0],[0]
"More formally, we minimize the return value of SUBEXPAE in Figure 1c where n is a binary noise vector with κ percent of its elements set to zero.",2.1. Neural Equivalence Networks,[0],[0]
Note that the encoder is specific to the parent node type τp.,2.1. Neural Equivalence Networks,[0],[0]
"Although our SUBEXPAE may seem similar to the recursive autoencoders of Socher et al. (2011), it differs
in two major ways.",2.1. Neural Equivalence Networks,[0],[0]
"First, SUBEXPAE autoencodes on the entire parent-children representation tuple, rather than the child representations alone.",2.1. Neural Equivalence Networks,[0],[0]
"Second, the encoding is not used to compute the parent representation, but only serves as a regularizer.
",2.1. Neural Equivalence Networks,[0],[0]
Subexpression autoencoding has several desirable effects.,2.1. Neural Equivalence Networks,[0],[0]
"First, it forces each parent-children tuple to lie in a lowdimensional space, requiring the network to compress information from the individual subexpressions.",2.1. Neural Equivalence Networks,[0],[0]
"Second, because the denoising autoencoder is reconstructing parent and child representations together, this encourages child representations to be predictable from parents and siblings.",2.1. Neural Equivalence Networks,[0],[0]
"Putting these two together, the goal is that the information discarded by the autoencoder bottleneck will be more syntactic than semantic, assuming that the semantics of child node is more predictable from its parent and sibling than its syntactic realization.",2.1. Neural Equivalence Networks,[0],[0]
"The goal is to nudge the network to learn consistent, reversible semantics.",2.1. Neural Equivalence Networks,[0],[0]
"Additionally, subexpression autoencoding has the potential to gradually unify distant representations that belong to the same equivalence class.",2.1. Neural Equivalence Networks,[0],[0]
"To illustrate this point, imagine two semantically equivalent c′0 and c ′′ 0 child nodes of different expressions that
have distant SEMVECs, i.e. ∥∥rc′0 − rc′′0 ∥∥2 although COMBINE(rc′0 , . . . )",2.1. Neural Equivalence Networks,[0],[0]
"≈ COMBINE(rc′′0 , . . . ).",2.1. Neural Equivalence Networks,[0],[0]
"In some cases due to the autoencoder noise, the differences between the input tuple x′,x′′ that contain rc′0 and rc′′0 will be non-existent and the decoder will predict a single location r̃c0 (possibly different from rc′0 and rc′′0 ).",2.1. Neural Equivalence Networks,[0],[0]
"Then, when minimizing the reconstruction error, both rc′0 and rc′′0 will be attracted to r̃c0 and eventually should merge.",2.1. Neural Equivalence Networks,[0],[0]
We train EQNETs from a dataset of expressions whose semantic equivalence is known.,2.2. Training,[0],[0]
Given a training set T = {T1 . . .,2.2. Training,[0],[0]
"TN} of parse trees of expressions, we assume that the training set is partitioned into equivalence classes E = {e1 . . .",2.2. Training,[0],[0]
eJ}.,2.2. Training,[0],[0]
"We use a supervised objective similar to classification; the difference between classification and our setting is that whereas standard classification problems consider a fixed set of class labels, in our setting the number of equivalence classes in the training set will vary with N .",2.2. Training,[0],[0]
"Given an expression tree T that belongs to the equivalence class ei ∈ E , we compute the probability
P (ei|T ) =",2.2. Training,[0],[0]
"exp
( TREENN(T )>qei + bi )∑ j exp ( TREENN(T )>",2.2. Training,[0],[0]
qej,2.2. Training,[0],[0]
"+ bj
) (1) where qei are model parameters that we can interpret as representations of each equivalence class that appears in the training class, and bi are scalar bias terms.",2.2. Training,[0],[0]
"Note that in this work, we only use information about the equivalence class of the whole expression T , ignoring available information about subexpressions.",2.2. Training,[0],[0]
"This is without loss of generality, because if we do know the equivalence class of a subexpression of T , we can simply add that subexpression to
the training set.",2.2. Training,[0],[0]
"To train the model, we use a max-margin objective that maximizes classification accuracy, i.e.
LACC(T, ei) = max (
0, arg max ej 6=ei,ej∈E log P (ej |T ) P (ei|T ) +m ) (2)
where m > 0 is a scalar margin.",2.2. Training,[0],[0]
"And therefore the optimized loss function for a single expression tree T that belongs to equivalence class ei ∈ E is
L(T, ei) = LACC(T, ei) + µ |Q| ∑ n∈Q SUBEXPAE(ch(n), n)
(3)
",2.2. Training,[0],[0]
"where Q = {n ∈ T : | ch(n)| > 0}, i.e. contains the nonleaf nodes of T and µ ∈ (0, 1] a scalar weight.",2.2. Training,[0],[0]
"We found that subexpression autoencoding is counterproductive early in training, before the SEMVECs begin to represent aspects of semantics.",2.2. Training,[0],[0]
"So, for each epoch t, we set µ = 1− 10−νt with ν ≥ 0.",2.2. Training,[0],[0]
"Instead of the supervised objective that we propose, an alternative option for training EQNET would be a Siamese objective (Chopra et al., 2005) that learns about similarities (rather than equivalence) between expressions.",2.2. Training,[0],[0]
"In practice, we found the optimization to be very unstable, yielding suboptimal performance.",2.2. Training,[0],[0]
We believe that this has to do with the compositional and recursive nature of the task that creates unstable dynamics and the fact that equivalence is a stronger property than similarity.,2.2. Training,[0],[0]
Datasets.,3. Evaluation,[0],[0]
We generate datasets of expressions grouped into equivalence classes from two domains.,3. Evaluation,[0],[0]
The datasets from the BOOL domain contain boolean expressions and the POLY datasets contain polynomial expressions.,3. Evaluation,[0],[0]
"In both domains, an expression is either a variable, a binary operator that combines two expressions, or a unary operator applied to a single expression.",3. Evaluation,[0],[0]
"When defining equivalence, we interpret distinct variables as referring to different entities in the domain, so that, e.g., the polynomials c · (a · a+ b) and f ·(d·d+e) are not equivalent.",3. Evaluation,[0],[0]
"For each domain, we generate “simple” datasets which use a smaller set of possible operators and “standard” datasets which use a larger set of more complex operators.",3. Evaluation,[0],[0]
We generate each dataset by exhaustively generating all parse trees up to a maximum tree size.,3. Evaluation,[0],[0]
All expressions are symbolically simplified into a canonical from in order to determine their equivalence class and are grouped accordingly.,3. Evaluation,[0],[0]
Table 1 shows the datasets we generated.,3. Evaluation,[0],[0]
In the supplementary material we present some sample expressions.,3. Evaluation,[0],[0]
"For the polynomial domain, we also generated ONEV-POLY datasets, which are polynomials over a single variable, since they are similar to the setting considered by Zaremba et al. (2014) — although ONEV-POLY is still a little more general because it is not restricted to homogeneous polynomials.",3. Evaluation,[0],[0]
"Learning SEMVECs for boolean expressions
is already a hard problem; with n boolean variables, there are 22 n
equivalence classes (i.e. one for each possible truth table).",3. Evaluation,[0],[0]
"We split the datasets into training, validation and test sets.",3. Evaluation,[0],[0]
"We create two test sets, one to measure generalization performance on equivalence classes that were seen in the training data (SEENEQCLASS), and one to measure generalization to unseen equivalence classes (UNSEENEQCLASS).",3. Evaluation,[0],[0]
It is easiest to describe UNSEENEQCLASS first.,3. Evaluation,[0],[0]
"To create the UNSEENEQCLASS, we randomly select 20% of all the equivalence classes, and place all of their expressions in the test set.",3. Evaluation,[0],[0]
We select equivalence classes only if they contain at least two expressions but less than three times the average number of expressions per equivalence class.,3. Evaluation,[0],[0]
We thus avoid selecting very common (and hence trivial to learn) equivalence classes in the testset.,3. Evaluation,[0],[0]
"Then, to create SEENEQCLASS, we take the remaining 80% of the equivalence classes, and randomly split the expressions in each class into training, validation, SEENEQCLASS test in the proportions 60%–15%–25%.",3. Evaluation,[0],[0]
"We provide the datasets online at groups.inf.ed.ac.uk/cup/semvec.
Baselines.",3. Evaluation,[0],[0]
"To compare the performance of our model, we train the following baselines.",3. Evaluation,[0],[0]
"TF-IDF: learns a representation given the expression tokens (variables, operators and parentheses).",3. Evaluation,[0],[0]
This captures topical/declarative knowledge but is unable to capture procedural knowledge.,3. Evaluation,[0],[0]
GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation.,3. Evaluation,[0],[0]
Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens.,3. Evaluation,[0],[0]
We also include two recursive neural networks (TREENN).,3. Evaluation,[0],[0]
The 1- layer TREENN which is the original TREENN also used by Zaremba et al. (2014).,3. Evaluation,[0],[0]
"We also include a 2-layer TREENN, where COMBINE is a classic two-layer MLP without residual connections.",3. Evaluation,[0],[0]
"This shows the effect of SEMVEC normalization and subexpression autoencoder.
Hyperparameters.",3. Evaluation,[0],[0]
"We tune the hyperparameters of all models using Bayesian optimization (Snoek et al., 2012) on a boolean dataset with 5 variables and maximum tree size of 7 (not shown in Table 1) using the average k-NN (k = 1, . . .",3. Evaluation,[0],[0]
", 15) statistics (described next).",3. Evaluation,[0],[0]
The selected hyperparameters are detailed in the supplementary material.,3. Evaluation,[0],[0]
Metrics.,3.1. Quantitative Evaluation,[0],[0]
To evaluate the quality of the learned representations we count the proportion of k nearest neighbors of each expression (using cosine similarity) that belong to the same equivalence class.,3.1. Quantitative Evaluation,[0],[0]
"More formally, given a test query expression q in an equivalence class c we find the k nearest neighbors Nk(q) of q across all expressions, and define the
score as
scorek(q) = |Nk(q) ∩ c| min(k, |c|) .",3.1. Quantitative Evaluation,[0],[0]
"(4)
To report results for a given testset, we simply average scorek(q) for all expressions q in the testset.",3.1. Quantitative Evaluation,[0],[0]
"We also report the precision-recall curves for the problem of clustering the SEMVECs into their appropriate equivalence classes.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation.,3.1. Quantitative Evaluation,[0],[0]
Figure 2 presents the average per-model precision-recall curves across the datasets.,3.1. Quantitative Evaluation,[0],[0]
Table 1 shows score5 of UNSEENEQCLASS.,3.1. Quantitative Evaluation,[0],[0]
Detailed plots are found in the supplementary material.,3.1. Quantitative Evaluation,[0],[0]
"EQNET performs better for all datasets, by a large margin.",3.1. Quantitative Evaluation,[0],[0]
"The only exception is POLY5, where the 2-L TREENN performs better.",3.1. Quantitative Evaluation,[0],[0]
"However, this may have to do with the small size of the dataset.",3.1. Quantitative Evaluation,[0],[0]
The reader may observe that the simple datasets (containing fewer operations and variables) are easier to learn.,3.1. Quantitative Evaluation,[0],[0]
"Understandably, introducing more variables increases the size of the represented space reducing performance.",3.1. Quantitative Evaluation,[0],[0]
"The tf-idf method performs better in settings with more variables, because it captures well the variables and operations used.",3.1. Quantitative Evaluation,[0],[0]
Similar observations can be made for sequence models.,3.1. Quantitative Evaluation,[0],[0]
The one and two layer TREENNs have mixed performance; we believe that this has to do with exploding and diminishing gradients due to the deep and highly compositional nature of TREENNs.,3.1. Quantitative Evaluation,[0],[0]
"Although Zaremba et al. (2014) consider a different problem to us, they use data similar to the ONEV-POLY datasets with a traditional TREENN architecture.",3.1. Quantitative Evaluation,[0],[0]
"Our evaluation suggests that EQNETs perform much better within the ONEV-POLY setting.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation of Compositionality.,3.1. Quantitative Evaluation,[0],[0]
"We evaluate whether EQNETs successfully learn to compute compositional representations, rather than overfitting to expression trees of
a small size.",3.1. Quantitative Evaluation,[0],[0]
"To do this we consider a type of transfer setting, in which we train on simpler datasets, but test on more complex ones; for example, training on the training set of BOOL5 but testing on the testset of BOOL8.",3.1. Quantitative Evaluation,[0],[0]
We average over 11 different train-test pairs (full list in supplementary material) and show the results in Figure 3a and Figure 3b.,3.1. Quantitative Evaluation,[0],[0]
"These graphs again show that EQNETs are better than any of the other methods, and indeed, performance is only a bit worse than in the non-transfer setting.
",3.1. Quantitative Evaluation,[0],[0]
"Impact of EQNET Components EQNETs differ from traditional TREENNs in two major ways, which we analyze here.",3.1. Quantitative Evaluation,[0],[0]
"First, SUBEXPAE improves performance.",3.1. Quantitative Evaluation,[0],[0]
"When training the network with and without SUBEXPAE, on average, the area under the curve (AUC) of scorek decreases by 16.8% on the SEENEQCLASS and 19.7% on the UNSEENEQCLASS.",3.1. Quantitative Evaluation,[0],[0]
"This difference is smaller in the transfer setting, where AUC decreases by 8.8% on average.",3.1. Quantitative Evaluation,[0],[0]
"However, even in this setting we observe that SUBEXPAE helps more in large and diverse datasets.",3.1. Quantitative Evaluation,[0],[0]
The second key difference to traditional TREENNs is the output normalization and the residual connections.,3.1. Quantitative Evaluation,[0],[0]
"Comparing our model to the one-layer and two-layer TREENNs again, we find that output normalization results in important improvements (the two-layer TREENNs have on average 60.9% smaller AUC).",3.1. Quantitative Evaluation,[0],[0]
"We note that only the combination of the residual connections and the output normalization improve the performance, whereas when used separately, there are no significant improvements over the two-layer TREENNs.",3.1. Quantitative Evaluation,[0],[0]
Table 2 shows expressions whose SEMVEC nearest neighbor is of an expression of another equivalence class.,3.2. Qualitative Evaluation,[0],[0]
"Manually inspecting boolean expressions, we find that EQNET confusions happen more when a XOR or implication operator is
Table 2.",3.2. Qualitative Evaluation,[0],[0]
Non semantically equivalent first nearest-neighbors from BOOL8 and POLY8.,3.2. Qualitative Evaluation,[0],[0]
"A checkmark indicates that the method correctly results in the nearest neighbor being from the same equivalence class.
",3.2. Qualitative Evaluation,[0],[0]
Expr a ∧ (a ∧ (a ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (a+ c)) ((a+ c) ·,3.2. Qualitative Evaluation,[0],[0]
"c) + a (b · b)− b
tfidf c",3.2. Qualitative Evaluation,[0],[0]
∧,3.2. Qualitative Evaluation,[0],[0]
((a ∧ a) ∧ (¬a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
a+ (c+ a) ·,3.2. Qualitative Evaluation,[0],[0]
c (c · a) + (a+ c) b · (b− b) GRU X a ∧ (a ∧ (c ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
b+ (c · (a+ c)) ((b+ c) · c) + a (b+ b) · b− b 1L-TREENN a ∧ (a ∧ (a ∧ (¬b))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬b))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬b)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (b+ c)) ((b+ c) ·,3.2. Qualitative Evaluation,[0],[0]
c) + a (a− c) · b− b EQNET X X (¬(b⇒ (b ∨ c))),3.2. Qualitative Evaluation,[0],[0]
"∧ a X X (b · b) · b− b
involved.",3.2. Qualitative Evaluation,[0],[0]
"In fact, we fail to find any confused expressions for EQNET not involving these operations in BOOL5 and in the top 100 expressions in BOOL10.",3.2. Qualitative Evaluation,[0],[0]
"As expected, tf-idf confuses expressions with others that contain the same operators and variables ignoring order.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, GRU and TREENN tend to confuse expressions with very similar symbolic representations, i.e. that differ in one or two deeply nested variables or operators.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, EQNET tends to confuse fewer expressions (as we previously showed) and the confused expressions tend to be more syntactically diverse and semantically related.
",3.2. Qualitative Evaluation,[0],[0]
Figure 4 shows a visualization of score5 for each node in the expression tree.,3.2. Qualitative Evaluation,[0],[0]
"One may see that as EQNET knows how
¬(c ⊕ (a ∧ ((a ⊕ c) ∧ b)))",3.2. Qualitative Evaluation,[0],[0]
"((c ∨ (¬b))⇒ a) ∧ (a ⇒ a)
((b ⊕ (¬c))",3.2. Qualitative Evaluation,[0],[0]
"∧ b)⊕ (a ∨ b) ((b · a)− a) · b
a − ((a + b) · a) ((c · b) · c) · a b + ((b · b) · b)
Figure 4.",3.2. Qualitative Evaluation,[0],[0]
Visualization of score5 for all expression nodes for three BOOL10 and four POLY8 test sample expressions using EQNET.,3.2. Qualitative Evaluation,[0],[0]
"The darker the color, the lower the score, i.e. white implies a score of 1 and dark red a score of 0.
to compose expressions that achieve good score, even if the subexpressions achieve a worse score.",3.2. Qualitative Evaluation,[0],[0]
"This suggests that for common expressions, (e.g. single variables and monomials) the network tends to select a unique location, without merging the equivalence classes or affecting the upstream performance of the network.",3.2. Qualitative Evaluation,[0],[0]
"Larger scale interactive t-SNE visualizations can be found online.
",3.2. Qualitative Evaluation,[0],[0]
Figure 5 presents two PCA visualizations of the SEMVECs of simple expressions and their negations/negatives.,3.2. Qualitative Evaluation,[0],[0]
It can be discerned that the black dots and their negations (in red) are discriminated in the semantic representation space.,3.2. Qualitative Evaluation,[0],[0]
"Figure 5b shows this property in a clear manner: left-right discriminates between polynomials with 1 and −a, topbottom between polynomials with−b and b and the diagonal parellelt to y = −x between c and−c.",3.2. Qualitative Evaluation,[0],[0]
We observe a similar behavior in Figure 5a for boolean expressions.,3.2. Qualitative Evaluation,[0],[0]
"Researchers have proposed compilation schemes that can transform any given program or expression to an equivalent neural network (Gruau et al., 1995; Neto et al., 2003; Siegel-
mann, 1994).",4. Related Work,[0],[0]
One can consider a serialized version of the resulting neural network as a representation of the expression.,4. Related Work,[0],[0]
"However, it is not clear how we could compare the serialized representations corresponding to two expressions and whether this mapping preserves semantic distances.
",4. Related Work,[0],[0]
"Recursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications.",4. Related Work,[0],[0]
Socher et al. (2012) show that TREENNs can learn to compute the values of some simple propositional statements.,4. Related Work,[0],[0]
"EQNET’s SUBEXPAE may resemble recursive autoencoders (Socher et al., 2011) but differs in form and function, encoding the whole parent-children tuple to force a clustering behavior.",4. Related Work,[0],[0]
"In addition, when encoding each expression our architecture does not use a pooling layer but directly produces a single representation for the expression.
",4. Related Work,[0],[0]
Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks.,4. Related Work,[0],[0]
"Although they learn representations of the student tasks, these representations capture task-specific syntactic features rather than code semantics.",4. Related Work,[0],[0]
Piech et al. (2015) also learn distributed matrix representations of student code submissions.,4. Related Work,[0],[0]
"However, to learn the representations, they use input and output program states and do not test for program equivalence.",4. Related Work,[0],[0]
"Additionally, these representations do not necessarily represent program equivalence, since they do not learn the representations over all possible input-outputs.",4. Related Work,[0],[0]
"Allamanis et al. (2016) learn variable-sized representations of source code snippets to summarize them with a short function-like name but aim learn summarization features in code rather than representations of symbolic expression equivalence.
",4. Related Work,[0],[0]
"More closely related is the work of Zaremba et al. (2014) who use a TREENN to guide the search for more efficient mathematical identities, limited to homogeneous singlevariable polynomial expressions.",4. Related Work,[0],[0]
"In contrast, EQNETs consider at a much wider set of expressions, employ subexpression autoencoding to guide the learned SEMVECs to better
represent equivalence, and do not use search when looking for equivalent expressions.",4. Related Work,[0],[0]
Alemi et al. (2016) use RNNs and convolutional neural networks to detect features within mathematical expressions to speed the search for premise selection in automated theorem proving but do not explicitly account for semantic equivalence.,4. Related Work,[0],[0]
"In the future, SEMVECs may be useful within this area.
",4. Related Work,[0],[0]
"Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).",4. Related Work,[0],[0]
"In contrast to this work, we do not aim to learn how to evaluate expressions or execute programs with neural network architectures but to learn continuous semantic representations (SEMVECs) of expression semantics irrespectively of how they are syntactically expressed or evaluated.",4. Related Work,[0],[0]
"In this work, we presented EQNETs, a first step in learning continuous semantic representations (SEMVECs) of procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"SEMVECs have the potential of bridging continuous representations with symbolic representations, useful in multiple applications in artificial intelligence, machine learning and programming languages.
",5. Discussion & Conclusions,[0],[0]
We show that EQNETs perform significantly better than state-of-the-art alternatives.,5. Discussion & Conclusions,[0],[0]
"But further improvements are needed, especially for more robust training of compositional models.",5. Discussion & Conclusions,[0],[0]
"In addition, even for relatively small symbolic expressions, we have an exponential explosion of the semantic space to be represented.",5. Discussion & Conclusions,[0],[0]
"Fixed-sized SEMVECs, like the ones used in EQNET, eventually limit the capacity that is available to represent procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"In the future, to represent more complex procedures, variable-sized representations would seem to be required.",5. Discussion & Conclusions,[0],[0]
This work was supported by Microsoft Research through its PhD Scholarship Programme and the Engineering and Physical Sciences Research Council [grant number EP/K024043/1].,Acknowledgments,[0],[0]
We thank the University of Edinburgh Data Science EPSRC Centre for Doctoral Training for providing additional computational resources.,Acknowledgments,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",abstractText,[0],[0]
"As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions.",abstractText,[0],[0]
"These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different.",abstractText,[0],[0]
"The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures.",abstractText,[0],[0]
"We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.",abstractText,[0],[0]
Learning Continuous Semantic Representations of Symbolic Expressions,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1285–1295, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014).",1 Introduction,[0],[0]
"Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning.",1 Introduction,[0],[0]
"A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Täckström",1 Introduction,[0],[0]
"et al., 2012; Duong et al., 2015).",1 Introduction,[0],[0]
A key barrier for crosslingual transfer is lexical matching between the source and the target language.,1 Introduction,[0],[0]
"Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012).
",1 Introduction,[0],[0]
Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing.,1 Introduction,[0],[0]
We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluation criterion through monolingual word similarity and bilingual lexicon induction tasks.,1 Introduction,[0],[0]
"Moreover, many prior work (Chandar A P et al., 2014; Kočiský et al., 2014) used bilingual or comparable corpus which is also expensive for many low-resource languages.",1 Introduction,[0],[0]
"Søgaard et al. (2015) impose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods.",1 Introduction,[0],[0]
"To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (Luong et al., 2015).",1 Introduction,[0],[0]
"However, many previous approaches are not capable of scaling up either because of the complicated objective functions or the nature of the algorithm.",1 Introduction,[0],[0]
"Other methods use a dictionary as the bridge between languages (Mikolov et al., 2013a; Xiao and Guo, 2014), however they do not adequately handle translation ambiguity.
",1 Introduction,[0],[0]
"Our model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal.",1 Introduction,[0],[0]
"Panlex covers more than a thousand languages and therefore our approach applies to many languages, including low-resource languages.",1 Introduction,[0],[0]
"Our method selects the translation based on the context in an Expectation-Maximization style training algorithm which explicitly handles polysemy through incorporating multiple dictionary translations (word sense and translation are closely linked (Resnik and Yarowsky, 1999)).",1 Introduction,[0],[0]
"In addition to the dictionary,
1285
our method only requires monolingual data.",1 Introduction,[0],[0]
"Our approach is an extension of the continuous bag-ofwords (CBOW) model (Mikolov et al., 2013b) to inject multilingual training signal based on dictionary translations.",1 Introduction,[0],[0]
"We experiment with several variations of our model, whereby we predict only the translation or both word and its translation and consider different ways of using the different learned center-word versus context embeddings in application tasks.",1 Introduction,[0],[0]
We also propose a regularisation method to combine the two embedding matrices during training.,1 Introduction,[0],[0]
"Together, these modifications substantially improve the performance across several tasks.",1 Introduction,[0],[0]
"Our final model achieves state-of-the-art performance on bilingual lexicon induction task, large improvement over word similarity task compared with previous published crosslingual word embeddings, and competitive result on cross-lingual document classification task.",1 Introduction,[0],[0]
"Notably, our embedding combining techniques are general, yielding improvements also for monolingual word embedding.
",1 Introduction,[0],[0]
"This paper makes the following contributions:
• Proposing a new crosslingual training method for learning vector embeddings, based only on monolingual corpora and a bilingual dictionary;
• Evaluating several methods for combining embeddings, which are shown to help in both crosslingual and monolingual evaluations; and
• Achieving consistent results which are competitive in monolingual, bilingual and crosslingual transfer settings.",1 Introduction,[0],[0]
"There is a wealth of prior work on crosslingual word embeddings, which all exploit some kind of bilingual resource.",2 Related work,[0],[0]
"This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012).",2 Related work,[0],[0]
"These approaches are affected by errors from automatic word alignments, motivating other approaches which operate at the sentence level (Chandar A P et al., 2014; Hermann and Blunsom, 2014; Gouws et al., 2015) through learning compositional vector representations of sentences,
in order that sentences and their translations representations closely match.",2 Related work,[0],[0]
"The word embeddings learned this way capture translational equivalence, despite not using explicit word alignments.",2 Related work,[0],[0]
"Nevertheless, these approaches demand large parallel corpora, which are not available for many language pairs.
Vulić and Moens (2015) use bilingual comparable text, sourced from Wikipedia.",2 Related work,[0],[0]
Their approach creates a psuedo-document by forming a bag-ofwords from the lemmatized nouns in each comparable document concatenated over both languages.,2 Related work,[0],[0]
These pseudo-documents are then used for learning vector representations using Word2Vec.,2 Related work,[0],[0]
"Their system, despite its simplicity, performed surprisingly well on a bilingual lexicon induction task (we compare our method with theirs on this task.)",2 Related work,[0],[0]
"Their approach is compelling due to its lesser resource requirements, although comparable bilingual data is scarce for many languages.",2 Related work,[0],[0]
"Related, Søgaard et al. (2015) exploit the comparable part of Wikipedia.",2 Related work,[0],[0]
"They represent word using Wikipedia entries which are shared for many languages.
",2 Related work,[0],[0]
A bilingual dictionary is an alternative source of bilingual information.,2 Related work,[0],[0]
"Gouws and Søgaard (2015) randomly replace the text in a monolingual corpus with a random translation, using this corpus for learning word embeddings.",2 Related work,[0],[0]
"Their approach doesn’t handle polysemy, as very few of the translations for each word will be valid in context.",2 Related work,[0],[0]
For this reason a high coverage or noisy dictionary with many translations might lead to poor outcomes.,2 Related work,[0],[0]
"Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary.",2 Related work,[0],[0]
"Our approach also uses a dictionary, however we use all the translations and explicitly disambiguate translations during training.
",2 Related work,[0],[0]
Another distinguishing feature on the above-cited research is the method for training embeddings.,2 Related work,[0],[0]
Mikolov et al. (2013a) and Faruqui and Dyer (2014) use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary.,2 Related work,[0],[0]
"Most of the other works train multlingual models jointly, which appears to have better performance over cascade training (Gouws et al., 2015).
",2 Related work,[0],[0]
For this reason we also use a form of joint training in our work.,2 Related work,[0],[0]
"Our model is an extension of the contextual bag of words (CBOW) model of Mikolov et al. (2013b), a method for learning vector representations of words based on their distributional contexts.",3 Word2Vec,[0],[0]
"Specifically, their model describes the probability of a token wi at position i using logistic regression with a factored parameterisation,
p(wi|wi±k\i) = exp(u>wihi)∑ w∈W exp(u > whi) , (1)
where hi = 12k ∑k
j=−k;j",3 Word2Vec,[0],[0]
6=0 vwi+j is a vector encoding the context over a window of size k centred around position,3 Word2Vec,[0],[0]
"i, W is the vocabulary and the parameters V and U ∈ R|W |×d are matrices referred to as the context and word embeddings.",3 Word2Vec,[0],[0]
"The model is trained to maximise the log-pseudo likelihood of a training corpus, however due to the high complexity of computing the denominator of equation (1), Mikolov et al. (2013b) propose negative sampling as an approximation, by instead learning to differentiate data from noise (negative examples).",3 Word2Vec,[0],[0]
"This gives rise to the following optimisation objective
∑
i∈D
( log σ(u>wihi)+",3 Word2Vec,[0],[0]
"p∑
j=1
Ewj∼Pn(w) log σ(−u>wjhi) ) ,
(2) where D is the training data and p is the number of negative examples randomly drawn from a noise distribution Pn(w).",3 Word2Vec,[0],[0]
"Our approach extends CBOW to model bilingual text, using two monolingual corpora and a bilingual dictionary.",4 Our Approach,[0],[0]
We believe this data condition to be less stringent than requiring parallel or comparable texts as the source of the bilingual signal.,4 Our Approach,[0],[0]
"It is common for field linguists to construct a bilingual dictionary when studying a new language, as one of the first steps in the language documentation process.",4 Our Approach,[0],[0]
"Translation dictionaries are a rich information source, capturing much of the lexical ambiguity in a language through translation.",4 Our Approach,[0],[0]
"For example, the word bank in English might mean the river bank
Algorithm 1 EM algorithm for selecting translation during training, where θ = (U,V) are the model parameters and η is the learning rate.
",4 Our Approach,[0],[0]
"1: randomly initialize V, U 2: for i < Iter do 3: for i ∈ De ∪Df do 4: s← vwi",4 Our Approach,[0],[0]
+,4 Our Approach,[0],[0]
"hi 5: w̄i = argmaxw∈dict(wi) cos(s,vw) 6: θ ← θ + η ∂O(w̄i,wi,hi)∂θ {see (3) or (5)} 7: end for 8: end for
or financial bank which corresponds to two different translations sponda and banca in Italian.",4 Our Approach,[0],[0]
"If we are able to learn to select good translations, then this implicitly resolves much of the semantic ambiguity in the language, and accordingly we seek to use this idea to learn better semantic vector representations of words.",4 Our Approach,[0],[0]
"To learn bilingual relations, we use the context in one language to predict the translation of the centre word in another language.",4.1 Dictionary replacement,[0],[0]
This is motivated by the fact that the context is an excellent means of disambiguating the translation for a word.,4.1 Dictionary replacement,[0],[0]
"Our method is closely related to Gouws and Søgaard (2015), however we only replace the middle word wi with a translation w̄i while keeping the context fixed.",4.1 Dictionary replacement,[0],[0]
"We replace each centre word with a translation on the fly during training, predicting instead p(w̄i|wi±k\i) but using the same formulation as equation (1) albeit with an augmented U matrix to cover word types in both languages.
",4.1 Dictionary replacement,[0],[0]
The translation w̄i is selected from the possible translations of wi listed in the dictionary.,4.1 Dictionary replacement,[0],[0]
"The problem of selecting the correct translation from the many options is reminiscent of the problem faced in expectation maximisation (EM), in that crosslingual word embeddings will allow for accurate translation, however to learn these embeddings we need to know the translations.",4.1 Dictionary replacement,[0],[0]
"We propose an EMinspired algorithm, as shown in Algorithm 1, which operates over both monolingual corpora, De and Df .",4.1 Dictionary replacement,[0],[0]
"The vector s is the semantic representation combining both the centre word, wi, and the con-
text,1 which is used to choose the best translation into the other language from the bilingual dictionary dict(wi).2 After selecting the translation, we use w̄i together with the context vector h to make a stochastic gradient update of the CBOW log-likelihood.",4.1 Dictionary replacement,[0],[0]
Words and their translations should appear in very similar contexts.,4.2 Joint Training,[0],[0]
One way to enforce this is to jointly learn to predict both the word and its translation from its monolingual context.,4.2 Joint Training,[0],[0]
"This gives rise to the following joint objective function,
O = ∑
i∈De∪Df
( α log σ(u>wihi)+(1−α) log σ(u>w̄ihi)
+
p∑
j=1
Ewj∼Pn(w) log σ(−u>wjhi) ) , (3)
where α controls the contribution of the two terms.",4.2 Joint Training,[0],[0]
"For our experiments, we set α = 0.5.",4.2 Joint Training,[0],[0]
The negative examples are drawn from combined vocabulary unigram distribution calculated from combined data De ∪Df .,4.2 Joint Training,[0],[0]
Many vector learning methods learn two embedding spaces V and U. Usually only V is used in application.,4.3 Combining Embeddings,[0],[0]
"The use of U, on the other hand, is understudied (Levy and Goldberg, 2014) with the exception of Pennington et al. (2014) who use a linear combination U + V, with minor improvement over V alone.
",4.3 Combining Embeddings,[0],[0]
"We argue that with our model, V is better at capturing the monolingual regularities and U is better at capturing bilingual signal.",4.3 Combining Embeddings,[0],[0]
The intuition for this is as follows.,4.3 Combining Embeddings,[0],[0]
"Assuming that we are predicting the word finance and its Italian translation finanze from the context (money, loan, bank, debt, credit) as shown in figure 1.",4.3 Combining Embeddings,[0],[0]
"In V only the context word representations are updated and in U only the representations of finance, finanze and negative samples such as tree and dog are updated.",4.3 Combining Embeddings,[0],[0]
"CBOW learns good embeddings because each time it updates the parameters, the words in the contexts are pushed closer to each
1Using both embeddings gives a small improvement compared to just using context vector h alone.
",4.3 Combining Embeddings,[0],[0]
"2We also experimented with using expectations over translations, as per standard EM, with slight degredation in results.
",4.3 Combining Embeddings,[0],[0]
other in the V space.,4.3 Combining Embeddings,[0],[0]
"Similarly, the target word wi and the translation w̄i are also pushed closer in the U space.",4.3 Combining Embeddings,[0],[0]
This is directly related to poitwise mutual information values of each pair of word and context explained in Levy and Goldberg (2014).,4.3 Combining Embeddings,[0],[0]
"Thus, U is bound to better at bilingual lexicon induction task and V is better at monolingual word similarity task.
",4.3 Combining Embeddings,[0],[0]
"The simple question is, how to combine both V and U to produce a better representation.",4.3 Combining Embeddings,[0],[0]
"We experiment with several ways to combine V and U. First, we can follow Pennington et al. (2014) to interpolate V and U in the post-processing step.",4.3 Combining Embeddings,[0],[0]
"i.e.
γV +",4.3 Combining Embeddings,[0],[0]
"(1− γ)U (4)
where γ controls the contribution of each embedding space.",4.3 Combining Embeddings,[0],[0]
"Second, we can also concatenate V and U instead of interpolation such that C =",4.3 Combining Embeddings,[0],[0]
"[V : U] where C ∈ R|W |×2d and W is the combined vocabulary from De ∪Df .
",4.3 Combining Embeddings,[0],[0]
"Moreover, we can also fuse V and U during training.",4.3 Combining Embeddings,[0],[0]
"For each word in the combined dictionary Ve ∪ Vf , we encourage the model to learn similar representation in both V and U by adding a regularization term to the objective function in equation (3) during training.
O′ =",4.3 Combining Embeddings,[0],[0]
"O + δ ∑
w∈Ve∪Vf ‖uw − vw‖22 (5)
where δ controls to what degree we should bind two spaces together.3",4.3 Combining Embeddings,[0],[0]
"Our experimental evaluation seeks to determine how well lexical distances in the learned embedding
3In the stochastic gradient update for a given word in context, we only compute the gradient of the regularisation term in (5) with respect to the words in the set of positive and negative examples.
",5 Experimental Setup,[0],[0]
spaces match with known lexical similarity judgements from bilingual and monolingual lexical resources.,5 Experimental Setup,[0],[0]
"To this end, in §6 we test crosslingual distances using a bilingual lexicon induction task in which we evaluate the embeddings in terms of how well nearby pairs of words from two languages in the embedding space match with human judgements.",5 Experimental Setup,[0],[0]
"Next, to evaluate the monolingual embeddings we evaluate word similarities in a single language against standard similarity datasets (§7).",5 Experimental Setup,[0],[0]
"Lastly, to demonstrate the usefulness of our embeddings in a task-based setting, we evaluate on crosslingual document classification (§9).
",5 Experimental Setup,[0],[0]
Monolingual Data,5 Experimental Setup,[0],[0]
The monolingual data is taken from the pre-processed Wikipedia dump from AlRfou et al. (2013).,5 Experimental Setup,[0],[0]
The data is already cleaned and tokenized.,5 Experimental Setup,[0],[0]
We additionally lower-case all words.,5 Experimental Setup,[0],[0]
Normally monolingual word embeddings are trained on billions of words.,5 Experimental Setup,[0],[0]
"However, obtaining that much monolingual data for a low-resource language is infeasible.",5 Experimental Setup,[0],[0]
"Therefore, we only select the first 5 million sentences (around 100 million words) for each language.
",5 Experimental Setup,[0],[0]
Dictionary A bilingual dictionary is the only source of bilingual correspondence in our technique.,5 Experimental Setup,[0],[0]
"We prefer a dictionary that covers many languages, such that our approach can be applied widely to many low-resource languages.",5 Experimental Setup,[0],[0]
"We use Panlex, a dictionary which currently covers around 1300 language varieties with about 12 million expressions.",5 Experimental Setup,[0],[0]
"The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc.",5 Experimental Setup,[0],[0]
"Accordingly, Panlex has high language coverage but often noisy translations.4 Table 1 summarizes the sizes of monolingual corpora and dictionaries for each pair of language in our experiments.
",5 Experimental Setup,[0],[0]
4We also experimented with a crowd-sourced dictionary from Wiktionary.,5 Experimental Setup,[0],[0]
Our initial observation was that the translation quality was better but with a lower-coverage.,5 Experimental Setup,[0],[0]
"For example, for en-it dictionary, Panlex and Wiktionary have a coverage of 42.1% and 16.8% respectively for the top 100k most frequent English words from Wikipedia.",5 Experimental Setup,[0],[0]
The average number of translations are 5.2 and 1.9 respectively.,5 Experimental Setup,[0],[0]
We observed similar trend using Panlex and Wiktionary dictionary in our model.,5 Experimental Setup,[0],[0]
"However, using Panlex results in much better performance.",5 Experimental Setup,[0],[0]
We can run the model on the combined dictionary from both Panlex and Wiktionary but we leave it for future work.,5 Experimental Setup,[0.9523770240826346],"['We next turn to the statistical setting, where we provide bounds on the expected performance.']"
"Given a word in a source language, the bilingual lexicon induction (BLI) task is to predict its translation in the target language.",6 Bilingual Lexicon Induction,[0],[0]
Vulić and Moens (2015) proposed this task to test crosslingual word embeddings.,6 Bilingual Lexicon Induction,[0],[0]
The difficulty of this is that it is evaluated using the recall of the top ranked word.,6 Bilingual Lexicon Induction,[0],[0]
"The model must be very discriminative in order to score well.
",6 Bilingual Lexicon Induction,[0],[0]
"We build the CLWE for 3 language pairs: it-en, es-en and nl-en, using similar parameters setting with Vulić and Moens (2015).5 The remaining tunable parameters in our system are δ from Equation (5), and the choice of algorithm for combining embeddings.",6 Bilingual Lexicon Induction,[0],[0]
"We use the regularization technique from §4.3 for combining context and word embeddings with δ = 0.01, and word embeddings U are used as the output for all experiments (but see comparative experiments in §8.)
",6 Bilingual Lexicon Induction,[0],[0]
"Qualitative evaluation We jointly train the model to predict both wi and the translation w̄i, combine V and U during training for each language pair.",6 Bilingual Lexicon Induction,[0],[0]
Table 2 shows the top 10 closest words in both source and target languages according to cosine similarity.,6 Bilingual Lexicon Induction,[0],[0]
"Note that the model correctly identifies the translation in en as the top candidate, and the top 10 words in both source and target languages are highly related.",6 Bilingual Lexicon Induction,[0],[0]
"This qualitative evaluation initially demonstrates the ability of our CLWE to capture both the bilingual and monolingual relationship.
",6 Bilingual Lexicon Induction,[0],[0]
Quantitative evaluation Table 3 shows our results compared with prior work.,6 Bilingual Lexicon Induction,[0],[0]
"We reimple-
5Default learning rate of 0.025, negative sampling with 25 samples, subsampling rate of value 1e−4, embedding dimension d = 200, window size cs = 48 and run for 15 epochs.
",6 Bilingual Lexicon Induction,[0],[0]
ment Gouws and Søgaard (2015) using Panlex and Wiktionary dictionaries.,6 Bilingual Lexicon Induction,[0],[0]
The result with Panlex is substantially worse than with Wiktionary.,6 Bilingual Lexicon Induction,[0],[0]
This confirms our hypothesis in §2.,6 Bilingual Lexicon Induction,[0],[0]
"That is the context might be corrupted if we just randomly replace the training data with the translation from noisy dictionary such as Panlex.
",6 Bilingual Lexicon Induction,[0],[0]
"Our model when randomly picking the translation is similar to Gouws and Søgaard (2015), using the Panlex dictionary.",6 Bilingual Lexicon Induction,[0],[0]
The biggest difference is that they replace the training data (both context and middle word) while we fix the context and only replace the middle word.,6 Bilingual Lexicon Induction,[0],[0]
"For a high coverage yet noisy dictionary such as Panlex, our approach gives better average score.",6 Bilingual Lexicon Induction,[0],[0]
"Comparing our two most basic models (EM selection and random selection), it is clear that the model using EM to select the translation outperforms random selection by a significant margin.
",6 Bilingual Lexicon Induction,[0],[0]
"Our joint model, as described in equation (3) which predicts both target word and the translation, further improves the performance, especially for nl-en.",6 Bilingual Lexicon Induction,[0],[0]
We use equation (5) to combine both context embeddings V and word embeddings U for all three language pairs.,6 Bilingual Lexicon Induction,[0],[0]
This modification during training substantially improves the performance.,6 Bilingual Lexicon Induction,[0],[0]
"More importantly, all our improvements are consistent for all three language pairs and both evaluation metrics, showing the robustness of our models.
",6 Bilingual Lexicon Induction,[0],[0]
Our combined model out-performed previous approaches by a large margin.,6 Bilingual Lexicon Induction,[0],[0]
"Vulić and Moens (2015)
used bilingual comparable data, but this might be hard to obtain for some language pairs.",6 Bilingual Lexicon Induction,[0],[0]
Their performance on nl-en is poor because their comparable data between en and nl is small.,6 Bilingual Lexicon Induction,[0],[0]
"Besides, they also use POS tagger and lemmatizer to filter only Noun and reduce the morphology complexity during training.",6 Bilingual Lexicon Induction,[0],[0]
These tools might not be available for many languages.,6 Bilingual Lexicon Induction,[0],[0]
"For a fairer comparison to their work, we also use the same Treetagger (Schmid, 1995) to lemmatize the output of our combined model before evaluation.",6 Bilingual Lexicon Induction,[0],[0]
Table 3 (+lemmatization) shows some improvements but minor.,6 Bilingual Lexicon Induction,[0],[0]
It demonstrates that our model is already good at disambiguating morphology.,6 Bilingual Lexicon Induction,[0],[0]
"For example, the top 2 translations for es word lenguas in en are languages and language which correctly prefer the plural translation.",6 Bilingual Lexicon Induction,[0],[0]
Now we consider the efficacy of our CLWE on monolingual word similarity.,7 Monolingual Word Similarity,[0],[0]
"We evaluate on English monolingual similarity on WordSim353 (WSen), RareWord (RW-en) and German version of WordSim353 (WS-de) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015).",7 Monolingual Word Similarity,[0],[0]
"Each of those datasets contain many tuples (w1, w2,s) where s is a scalar denoting the semantic similarity between w1 and w2 given by human annotators.",7 Monolingual Word Similarity,[0],[0]
"Good system should produce the score correlated with human judgement.
",7 Monolingual Word Similarity,[0],[0]
"We train the model as described in §4, which is the combine embeddings setting from Table 3.",7 Monolingual Word Similarity,[0],[0]
"Since the evaluation involves de and en word similarity, we train the CLWE for en-de pair.",7 Monolingual Word Similarity,[0],[0]
Table 4 shows the performance of our combined model compared with several baselines.,7 Monolingual Word Similarity,[0],[0]
"Our combined model out-performed both Luong et al. (2015) and Gouws and Søgaard (2015)6 which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively.
",7 Monolingual Word Similarity,[0],[0]
"We also compare our system with the monolingual CBOW model trained on the monolingual data for each language, using the same parameter settings from earlier (§6).",7 Monolingual Word Similarity,[0],[0]
"Surprisingly, our combined model performs better than the monolingual CBOW baseline which makes our result close to the monolingual state-of-the-art on each different dataset.",7 Monolingual Word Similarity,[0],[0]
"However, the best monolingual methods use much larger
6trained using the Panlex dictionary
monolingual corpora (Shazeer et al., 2016), WordNet or the output of commercial search engines (Yih and Qazvinian, 2012).
",7 Monolingual Word Similarity,[0],[0]
Next we explain the gain of our combined model compared with the monolingual CBOW model.,7 Monolingual Word Similarity,[0],[0]
"First, we compare the combined model with the joint-model with respect to monolingual CBOW model (Table 4).",7 Monolingual Word Similarity,[0],[0]
"It shows that the improvement seems mostly come from combining V and U. If we apply the combining algorithm to the monolingual CBOW model (CBOW + combine), we also ob-
serve an improvement.",7 Monolingual Word Similarity,[0],[0]
"Clearly most of the improvement is from combining V and U, however our V and U are more complementary as the gain is more marked.",7 Monolingual Word Similarity,[0],[0]
"Other improvements can be explained by the observation that a dictionary can improve monolingual accuracy through linking synonyms (Faruqui and Dyer, 2014).",7 Monolingual Word Similarity,[0],[0]
"For example, since plane, airplane and aircraft have the same Italian translation aereo, the model will encourage those words to be closer in the embedding space.",7 Monolingual Word Similarity,[0],[0]
Combining context embeddings and word embeddings results in an improvement in both monolingual similarity and bilingual lexicon induction.,8 Model selection,[0],[0]
"In §4.3, we introduce several combination methods including post-processing (interpolation and concatenation) and during training (regularization).",8 Model selection,[0],[0]
"In this section, we justify our parameter and model choices.
",8 Model selection,[0],[0]
"We use en-it pair for tuning purposes, considering the value of γ in equation 4.",8 Model selection,[0],[0]
Figure 2 shows the performances using different values of γ.,8 Model selection,[0],[0]
The two extremes where γ = 0 and γ = 1 corresponds to no interpolation where we just use U or V respectively.,8 Model selection,[0],[0]
"As γ increases, the performance on WSen increases yet BLI decreases.",8 Model selection,[0],[0]
These results confirm our hypothesis in §4.3 that U is better at capturing bilingual relations and V is better at capturing monolingual relations.,8 Model selection,[0],[0]
"As a compromise, we choose γ = 0.5 in our experiments.",8 Model selection,[0],[0]
"Similarly, we tune the regularization sensitivity δ in equation (5) which combines embeddings space during training.",8 Model selection,[0],[0]
"We test δ = 10−n with n = {0, 1, 2, 3, 4} and us-
ing V, U or the interpolation of both V+U2 as the learned embeddings, evaluated on the same BLI and WS-en.",8 Model selection,[0],[0]
"We select δ = 0.01.
",8 Model selection,[0],[0]
Table 5 shows the performance with and without using combining algorithms mentioned in §4.3.,8 Model selection,[0],[0]
"As the compromise between both monolingual and crosslingual tasks, we choose regularization + U as the combination algorithm.",8 Model selection,[0],[0]
"All in all, we apply the regularization algorithm for combining V and U with δ = 0.01 and U as the output for all language pairs without further tuning.",8 Model selection,[0],[0]
"In this section, we evaluate our CLWE on a downstream crosslingual document classification (CLDC)
task.",9 Crosslingual Document Classification,[0],[0]
"In this task, the document classifier is trained on a source language and then applied directly to classify a document in the target language.",9 Crosslingual Document Classification,[0],[0]
This is convenient for a target low-resource language where we do not have document annotations.,9 Crosslingual Document Classification,[0],[0]
"The experimental setup is the same as Klementiev et al. (2012)7 with the training and testing data sourced from Reuter RCV1/RCV2 corpus (Lewis et al., 2004).
",9 Crosslingual Document Classification,[0],[0]
The documents are represented as the bag of word embeddings weighted by tf.idf.,9 Crosslingual Document Classification,[0],[0]
A multi-class classifier is trained using the average perceptron algorithm on 1000 documents in the source language and tested on 5000 documents in the target language.,9 Crosslingual Document Classification,[0],[0]
"We use the CLWE, such that the document representation in the target language embeddings is in the same space with the source language.
",9 Crosslingual Document Classification,[0],[0]
We build the en-de CLWE using combined models as described in section §4.,9 Crosslingual Document Classification,[0],[0]
"Following prior work, we also use monolingual data8 from the RCV1/RCV2 corpus (Klementiev et al., 2012; Gouws et al., 2015; Chandar A P et al., 2014).
",9 Crosslingual Document Classification,[0],[0]
Table 6 shows the CLDC results for various CLWE.,9 Crosslingual Document Classification,[0],[0]
"Despite its simplicity, our model achieves competitive performance.",9 Crosslingual Document Classification,[0],[0]
"Note that aside from our model, all other models in Table 6 use a large bitext (Europarl) which may not exist for many lowresource languages, limiting their applicability.
7The data split and code are kindly provided by the authors.",9 Crosslingual Document Classification,[0],[0]
8We randomly sample documents in RCV1 and RCV2 corpora and selected around 85k documents to form 400k monolingual sentences for both en and de.,9 Crosslingual Document Classification,[0],[0]
"For each document, we perform basic pre-processing including: lower-casing, remove html tags and tokenization.",9 Crosslingual Document Classification,[0],[0]
These monolingual data are then concatenated with the monolingual data from Wikipedia to form the final training data.,9 Crosslingual Document Classification,[0],[0]
"Our model exploits dictionaries, which are more widely available than parallel corpora.",10 Low-resource languages,[0],[0]
"However the question remains as to how well this performs of a real low-resource language, rather than a simulated condition like above, whereupon the quality of the dictionary is likely to be worse.",10 Low-resource languages,[0],[0]
"To test this, we evaluation on Serbian, a language with few annotated language resources.",10 Low-resource languages,[0],[0]
Table 1 shows the relative size of monolingual data and dictionary for en-sr compared with other language pairs.,10 Low-resource languages,[0],[0]
Both the Serbian monolingual data and the dictionary size is more than 10 times smaller than other language pairs.,10 Low-resource languages,[0],[0]
We build the en-sr CLWE using our best model (joint + combine) and evaluate on the bilingual word induction task using 939 gold translation pairs.9 We achieved recall score of 35.8% and 45.5% at 1 and 5 respectively.,10 Low-resource languages,[0],[0]
"Although worse than the earlier results, these numbers are still well above chance.
",10 Low-resource languages,[0],[0]
We can also simulate low-resource setting using our earlier datasets.,10 Low-resource languages,[0],[0]
"For estimating the performance loss on all three tasks, we down sample the dictionary for en-it and en-de based on en word frequency.",10 Low-resource languages,[0],[0]
Figure 3 shows the performance with different dictionary sizes for all three tasks.,10 Low-resource languages,[0],[0]
The monolingual similarity performance is very similar across various sizes.,10 Low-resource languages,[0],[0]
"For BLI and CLDC, dictionary size is more important, although performance levels off at around 80k dictionary pairs.",10 Low-resource languages,[0],[0]
"We conclude that this size is sufficient for decent performance.
",10 Low-resource languages,[0],[0]
"9The sr→en translations are sourced from Google Translate by translating one word at a time, followed by manually verification, after which 61 translation pairs were ruled out as being bad or questionable.",10 Low-resource languages,[0],[0]
Previous CLWE methods often impose high resource requirements yet have low accuracy.,11 Conclusion,[0],[0]
We introduce a simple framework based on a large noisy dictionary.,11 Conclusion,[0],[0]
We model polysemy using EM translation selection during training to learn bilingual correspondences from monolingual corpora.,11 Conclusion,[0],[0]
"Our algorithm allows to train on massive amount of monolingual data efficiently, representing monolingual and bilingual properties of language.",11 Conclusion,[0],[0]
"This allows us to achieve state-of-the-art performance on bilingual lexicon induction task, competitive result on monolingual word similarity and crosslingual document classification task.",11 Conclusion,[0],[0]
"Our combination techniques during training, especially using regularization, are highly effective and could be used to improve monolingual word embeddings.",11 Conclusion,[0],[0]
This work was conducted during Duong’s internship at IBM Research – Tokyo and partially supported by the University of Melbourne and National ICT Australia (NICTA).,Acknowledgments,[0],[0]
"We are grateful for support from NSF Award 1464553 and the DARPA/I2O, Contract No. HR0011-15-C-0114.",Acknowledgments,[0],[0]
"We thank Yuta Tsuboi and Alvin Grissom II for helpful discussions, Jan Šnajder for helping with sr-en evaluation.",Acknowledgments,[0],[0]
"Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools.",abstractText,[0],[0]
"However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy.",abstractText,[0],[0]
We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages.,abstractText,[0],[0]
"Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.",abstractText,[0],[0]
Learning Crosslingual Word Embeddings without Bilingual Corpora,title,[0],[0]
"Deep neural networks (DNNs) have improved performances of many applications, as the non-linearity of DNNs provides expressive modeling capacity, but it also makes DNNs difficult to train and easy to overfit the training data.
",1. Introduction,[0],[0]
"Whitened neural network (WNN) (Desjardins et al., 2015), a recent advanced deep architecture, is ideally to solve the above difficulties.",1. Introduction,[0],[0]
"WNN extends batch normalization (BN) (Ioffe & Szegedy, 2015) by normalizing the internal hidden representation using whitening transformation instead of standardization.",1. Introduction,[0],[0]
"Whitening helps regularize each diagonal block of the Fisher Information Matrix (FIM) to be an
1Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China 2Multimedia Laboratory, The Chinese University of Hong Kong, Hong Kong.",1. Introduction,[0],[0]
"Correspondence to: Ping Luo <pluo@ie.cuhk.edu.hk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
approximation of the identity matrix.,1. Introduction,[0],[0]
"This is an appealing property, as training WNN using stochastic gradient descent (SGD) mimics the fast convergence of natural gradient descent (NGD) (Amari & Nagaoka, 2000).",1. Introduction,[0],[0]
The whitening transformation also improves generalization.,1. Introduction,[0],[0]
"As demonstrated in (Desjardins et al., 2015), WNN exhibited superiority when being applied to various network architectures, such as autoencoder and convolutional neural network, outperforming many previous works including SGD, RMSprop (Tieleman & Hinton, 2012), and BN.
",1. Introduction,[0],[0]
"Although WNN is able to reduce the number of training iterations and improve generalization, it comes with a price of increasing training time, because eigen-decomposition occupies large computations.",1. Introduction,[0],[0]
The runtime scales up when the number of hidden layers that require whitening transformation increases.,1. Introduction,[0],[0]
"We revisit WNN by breaking down its performance and show that its main runtime comes from two aspects, 1) computing full covariance matrix for whitening and 2) solving singular value decomposition (SVD).",1. Introduction,[0],[0]
"Previous work (Desjardins et al., 2015) suggests to overcome these problems by a) using a subset of training data to estimate the full covariance matrix and b) solving the SVD every hundreds or thousands of training iterations.",1. Introduction,[0],[0]
"Both of them rely on the assumption that the SVD holds in this period, but it is generally not true.",1. Introduction,[0],[0]
"When this period becomes large, WNN degenerates to canonical SGD due to ill conditioning of FIM.
",1. Introduction,[0],[0]
"We propose generalized WNN (GWNN), which possesses the beneficial properties of WNN, but significantly reduces its runtime and improves its generalization.",1. Introduction,[0],[0]
"We introduce two variants of GWNN, including pre-whitening and postwhitening GWNNs.",1. Introduction,[0],[0]
"The former one whitens a hidden layer’s input values, whilst the latter one whitens the preactivation values (hidden features).",1. Introduction,[0],[0]
GWNN has three appealing characteristics.,1. Introduction,[0],[0]
"First, compared to WNN, GWNN is capable of learning more compact hidden representation, such that the SVD can be approximated by a few top eigenvectors to reduce computation.",1. Introduction,[0],[0]
This compact representation also improves generalization.,1. Introduction,[0],[0]
"Second, it enables the whitening transformation to be performed in a short period, maintaining conditioning of FIM.",1. Introduction,[0],[0]
"Third, by exploiting knowledge of the distribution of the hidden features, we calculate the covariance matrix in an analytical form to further improve computational efficiency.",1. Introduction,[0],[0]
We begin by defining the basic notation for feed-forward neural network.,2. Notation and Background,[0],[0]
A neural network transforms an input vector o0 to an output vector o` through a series of ` hidden layers {oi}`i=1.,2. Notation and Background,[0],[0]
We assume each layer has identical dimension for the simplicity of notation i.e. ∀oi ∈ Rd×1.,2. Notation and Background,[0],[0]
"In this case, all vectors and matrixes in the following should have d rows unless otherwise stated.",2. Notation and Background,[0],[0]
"As shown in Fig.1 (a), each fully-connected (fc) layer consists of a weight matrix, W i, and a set of hidden neurons, hi, each of which receives as input a weighted sum of outputs from the previous layer.",2. Notation and Background,[0],[0]
We have hi = W ioi−1.,2. Notation and Background,[0],[0]
"In this work, we take fully-connected network as an example.",2. Notation and Background,[0],[0]
"Note that the above computation can be also applied to a convolutional network, where an image patch is vectorized as a column vector and represented by oi−1 and each row of W i represents a filter.
",2. Notation and Background,[0],[0]
"As the recent deep architectures typically stack a batch normalization (BN) layer before the pre-activation values, we do not explicitly include a bias term when computing hi, because it is normalized in BN, such that φi = hi−E[hi]√
Var[hi] , where the expectation and variance are computed over a minibatch of samples.",2. Notation and Background,[0],[0]
LeCun et al. (2002) showed that such normalization speeds up convergence even when the hidden features are not decorrelated.,2. Notation and Background,[0],[0]
"Furthermore, output of each layer is calculated by a nonlinear activation function.",2. Notation and Background,[0],[0]
"A popular choice is the rectified linear unit, relu(x) = max(0, x).",2. Notation and Background,[0],[0]
"The precise computation for an output is oi = max(0,diag(αi)φi + βi), where diag(x) represents a matrix whose diagonal entries are x. αi and βi are two vectors that scale and shift the normalized features, in order to maintain the network’s representation capacity.",2. Notation and Background,[0],[0]
This section revisits whitened neural networks (WNN).,2.1. Whitened Neural Networks,[0],[0]
Any neural architecture can be adapted to a WNN by stacking a whitening transformation layer after the layer’s input.,2.1. Whitened Neural Networks,[0],[0]
"For example, Fig.1 (b) adapts a fc layer as shown in
(a) into a whitened fc layer.",2.1. Whitened Neural Networks,[0],[0]
"Its information flow becomes
õi−1 = P i−1(oi−1 − µi−1), ĥi = Ŵ iõi−1, (1)
",2.1. Whitened Neural Networks,[0],[0]
"φi = ĥ i√
Var[ĥi] , oi = max(0,diag(αi)φi + βi),
where µi−1 represents a centering variable, µi−1 = E[oi−1].",2.1. Whitened Neural Networks,[0],[0]
"P i−1 is a whitening matrix whose rows are obtained from eigen-decomposition of Σi−1, which is the covariance matrix of the input, Σi−1 = E[(oi−1 − µi−1)(oi−1 − µi−1)T].",2.1. Whitened Neural Networks,[0],[0]
"The input is decorrelated by P i−1 in the sense that its covariance matrix becomes an identity matrix, i.e. E[õi−1õi−1T ] = I .",2.1. Whitened Neural Networks,[0],[0]
"To avoid ambiguity, we use ‘ˆ’ to distinguish the variables in WNN and the canonical fc layer whenever necessary.",2.1. Whitened Neural Networks,[0],[0]
"For instance, Ŵ i represents a whitened weight matrix.",2.1. Whitened Neural Networks,[0],[0]
"In Eqn.(1), computation of the BN layer has been simplified because we have E[ĥi] = Ŵ iP i−1(E[oi−1]− µi−1) = 0.
",2.1. Whitened Neural Networks,[0],[0]
"We define θ to be a vector consisting of all the whitened weight matrixes concatenated together, θ = {vec(Ŵ 1)T, vec(Ŵ 2)T, ..., vec(Ŵ `)T}, where vec(·) is an operator that vectorizes a matrix by stacking its columns.",2.1. Whitened Neural Networks,[0],[0]
"Let L(o`, y; θ) denote a loss function of WNN, which measures the disagreement between a prediction o` made by the network, and a target y. WNN is trained by minimizing the loss function with respect to the parameter vector θ and two constraints
min θ
L(o`, y; θ) (2)
s.t. E[õi−1õi−1T ]",2.1. Whitened Neural Networks,[0],[0]
"= I, hi − E[hi] = ĥi, i = 1...`.
To satisfy the first constraint, P i−1 is obtained by decomposing the covariance matrix, Σi−1 = U i−1Si−1U i−1T. We choose P i−1 =",2.1. Whitened Neural Networks,[0],[0]
"(Si−1)− 1 2U i−1
T, where Si−1 is a diagonal matrix whose diagonal elements are eigenvalues and U i−1 is an orthogonal matrix of eigenvectors.",2.1. Whitened Neural Networks,[0],[0]
"The first constraint holds under the construction of eigendecomposition.
",2.1. Whitened Neural Networks,[0],[0]
"The second constraint, hi − E[hi] = ĥi, enforces that the centered hidden features are the same, before and after
adapting a fc layer to WNN, as shown in Fig.1 (a) and (b).",2.1. Whitened Neural Networks,[0],[0]
"In other words, it ensures that their representation powers are identical.",2.1. Whitened Neural Networks,[0],[0]
"By combing the computations in Fig.1 (a) and Eqn.(1), the second constraint implies that ‖(hi−E[hi])− ĥi‖22",2.1. Whitened Neural Networks,[0],[0]
= ‖(W ioi−1,2.1. Whitened Neural Networks,[0],[0]
− W iµi−1),2.1. Whitened Neural Networks,[0],[0]
"− Ŵ iõi−1‖22 = 0, which has a closed-form solution, Ŵ i = W i(P i−1)−1.",2.1. Whitened Neural Networks,[0],[0]
"To see this, we have ĥi = W i(P i−1)−1P",2.1. Whitened Neural Networks,[0],[0]
i−1(oi−1 − µi−1) = W i(oi−1−µi−1) = hi−E[hi].,2.1. Whitened Neural Networks,[0],[0]
"The representation capacity can be preserved by mapping the whitened weight matrix from the ordinary weight matrix.
",2.1. Whitened Neural Networks,[0],[0]
Conditioning of the FIM.,2.1. Whitened Neural Networks,[0],[0]
"Here we show that WNN improves training efficiency by conditioning the Fisher information matrix (FIM) (Amari & Nagaoka, 2000).",2.1. Whitened Neural Networks,[0],[0]
"A FIM, denoted as F , consists of `× ` blocks.",2.1. Whitened Neural Networks,[0],[0]
"Each block is indexed by Fij , representing the covariance (co-adaptation) between the whitened weight matrixes of the i-th and j-th layers.",2.1. Whitened Neural Networks,[0],[0]
"We have Fij = E[vec(δŴ i)vec(δŴ j)T], where δŴ i indicates the gradient of the i-th whitened weight matrix.",2.1. Whitened Neural Networks,[0],[0]
"For example, the gradient of Ŵ i is achieved by õi−1(δĥi)T, as illustrated in Eqn.(1).",2.1. Whitened Neural Networks,[0],[0]
"We have vec(δŴ i) = vec(õi−1(δĥi)T) = δĥi ⊗ õi−1, where ⊗ denotes the Kronecker product.",2.1. Whitened Neural Networks,[0],[0]
"In this case, Fij can be rewritten as E[(δĥi ⊗ õi−1)(δĥj ⊗ õj−1)T] = E[δĥi(δĥj)T ⊗ õi−1(õj−1)T].",2.1. Whitened Neural Networks,[0],[0]
"By assuming δĥ and õ are independent as demonstrated in (Raiko et al., 2012), Fij can be approximated by E[δĥi(δĥj)T] ⊗ E[õi−1(õj−1)T].",2.1. Whitened Neural Networks,[0],[0]
"As a result, when i = j, each diagonal block of F , Fii, has a block diagonal structure because we have E[õi−1(õi−1)T] = I as shown in Eqn.(2), which improves conditioning of FIM and thus speeds up training.",2.1. Whitened Neural Networks,[0],[0]
"In general, WNN regularizes the diagonal blocks of FIM and achieves stronger conditioning than those methods (LeCun et al., 2002; Tieleman & Hinton, 2012) that regularized the diagonal entries.
",2.1. Whitened Neural Networks,[0.9512531902136055],"['One such line of work consists of learning classifiers that are linear functions of a very large or infinite collection of non-linear functions (Bach, 2014; Daniely et al., 2016; Cho & Saul, 2009; Heinemann et al., 2016; Williams, 1997).']"
Training WNN.,2.1. Whitened Neural Networks,[0],[0]
Alg.1 summarizes training of WNN.,2.1. Whitened Neural Networks,[0],[0]
"At the 1st line, the whitened weight matrix Ŵ i0 is initialized by W i of the ordinary fc layer, which can be pretrained or sampled from a Gaussian distribution.",2.1. Whitened Neural Networks,[0],[0]
The 4th line shows that Ŵ it is updated in each iteration t using SGD.,2.1. Whitened Neural Networks,[0],[0]
The first and second constraints are achieved in the 7th and 8th lines respectively.,2.1. Whitened Neural Networks,[0],[0]
"For example, the 8th line ensures that the hidden features are the same before and after updating the whitening matrix.",2.1. Whitened Neural Networks,[0],[0]
"As the distribution of the hidden representation changes after every update of the whitened weight matrix, to maintain good conditioning of FIM, the whitening matrix, P i−1, needs to be reconstructed frequently by performing eigen-decomposition on Σi−1, which is estimated using N samples.",2.1. Whitened Neural Networks,[0],[0]
N is typically 104 in experiments.,2.1. Whitened Neural Networks,[0],[0]
"However, this raw strategy increases computation time.",2.1. Whitened Neural Networks,[0],[0]
"Desjardins et al. (2015) performed whitening in every τ iterations as shown in the 5th line of Alg.1 to reduce computations, e.g. τ = 103.
",2.1. Whitened Neural Networks,[0],[0]
"How good is the conditioning of the FIM by using Al-
Algorithm 1 Training WNN 1: Init: initial network parameters θ, αi, βi; whitening matrix P i−1 = I; iteration t = 0; Ŵ it = W i; ∀i ∈ {1...`}.
g.1?",2.1. Whitened Neural Networks,[0],[0]
"We measure the similarity of the covariance matrix, E[õi−1(õi−1)T], with the identity matrix I .",2.1. Whitened Neural Networks,[0],[0]
This is called the orthogonality.,2.1. Whitened Neural Networks,[0],[0]
We employ Pearson’s correlation1 as the similarity between two matrixes.,2.1. Whitened Neural Networks,[0],[0]
"Intuitively, this measure has a value between −1 and +1, representing negative and positive correlations.",2.1. Whitened Neural Networks,[0],[0]
Larger values indicate higher orthogonality.,2.1. Whitened Neural Networks,[0],[0]
"Fig.2 visualizes four randomly generated covariance matrixes, where (a,b) are sampled from a uniform distribution between 0 and 1.",2.1. Whitened Neural Networks,[0],[0]
"Fig.2 (c,d) are generated by truncating different numbers of columns of a randomly generated orthogonal matrix.",2.1. Whitened Neural Networks,[0],[0]
"For instance, (a,b) have small similarity with respect to the identity matrix.",2.1. Whitened Neural Networks,[0],[0]
"In contrast, when the correlation equals 0.65 as shown in (c), all entries in the diagonal are larger than 0.9 and more than 80% off-diagonal entries have values smaller than 0.1.",2.1. Whitened Neural Networks,[0],[0]
"Furthermore, Pearson’s correlation is insensitive to the size of matrix, such that orthogonality of different layers can be compared together.",2.1. Whitened Neural Networks,[0.9501337315212945],"['For comparison, note that O(T 2) is the size of the kernel matrix, and is thus likely to be the cost of any algorithm that uses an explicit kernel matrix, where one is available.']"
"For example, although matrixes in
1Given an identity matrix, I , and a covariance matrix, Σ, the Pearson’s correlation between them is defined as corr(Σ, I) = vec(Σ)Tvec(I)√ vec(Σ)Tvec(Σ)·vec(I)Tvec(I) , where vec(Σ) is a normalized vector by subtracting mean of all entries.
",2.1. Whitened Neural Networks,[0],[0]
"(a) and (b) have different sizes, they have similar value of orthogonality when they are sampled from the same distribution.
",2.1. Whitened Neural Networks,[0],[0]
"As shown in Fig.3 (a), we adopt network-in-network (NIN) (Lin et al., 2014) that is trained on CIFAR-10 (Krizhevsky, 2009), and plot the orthogonalities of three different convolutional layers, which are whitened every τ",2.1. Whitened Neural Networks,[0],[0]
= 103 iterations by using Alg.1.,2.1. Whitened Neural Networks,[0],[0]
"We see that orthogonality values during training have large fluctuations except those of the first convolutional layer, abbreviated as ‘conv1’.",2.1. Whitened Neural Networks,[0],[0]
"This is because the distributions of deeper layers’ inputs change after the whitened weight matrixes have been updated, leading to ill-conditions of the whitening matrixes, which are estimated in a large interval.",2.1. Whitened Neural Networks,[0],[0]
"In fact, large τ will degenerate WNN to canonical SGD.",2.1. Whitened Neural Networks,[0],[0]
"However, ‘conv1’ uses image data as inputs, whose distribution is typically stable during training.",2.1. Whitened Neural Networks,[0],[0]
"Its whitening matrix can be estimated once at the beginning and fixed in the entire training stage.
",2.1. Whitened Neural Networks,[0],[0]
"In the section below, we present generalized whitened neural networks to improve conditioning of FIM while reducing computation time.",2.1. Whitened Neural Networks,[0],[0]
"We present two types of generalized WNN (GWNN), including pre-whitening and post-whitening GWNNs.",3. Generalized Whitened Neural Networks,[0],[0]
"Both models share beneficial properties of WNN, but have lower computation time.",3. Generalized Whitened Neural Networks,[0],[0]
"This section introduces pre-whitening GWNN, abbreviated as pre-GWNN, which performs whitening transformation before applying the weight matrix (i.e. whiten the input),
as illustrated in Fig.1 (c).",3.1. Pre-whitening GWNN,[0],[0]
"When adapting a fc layer to pre-GWNN, the whitening matrix is truncated by removing those eigenvectors that have small eigenvalues, in order to learn compact representation.",3.1. Pre-whitening GWNN,[0],[0]
"This allows the input vector to vary its length, so as to gradually adapt the learned representation to informative patterns with high variations, but not noises.",3.1. Pre-whitening GWNN,[0],[0]
"Learning pre-GWNN is formulated analogously to learning WNN in Eqn.(2), but with one additional constraint truncated the rank of the whitening matrix,
min θ
L(o`, y; θ) (3)
s.t. rank(P i−1) ≤ d′, E[õi−1d′ õ i−1T d′ ]",3.1. Pre-whitening GWNN,[0],[0]
"= I,
hi − E[hi] = ĥi, i = 1...`.
Let d be the dimension of the original fc layer.",3.1. Pre-whitening GWNN,[0],[0]
"By combining Eqn.(2), we have P i−1 =",3.1. Pre-whitening GWNN,[0],[0]
"(Si−1)− 1 2U i−1
T ∈ Rd×d, which is truncated by using P i−1d′ = (S i−1 d′ )",3.1. Pre-whitening GWNN,[0],[0]
"− 12U i−1d′ T ∈ Rd′×d, where Si−1d′ is achieved by keeping rows and columns associated with the first d′ large eigenvalues, whilst U i−1d′ contains the corresponding d
′ eigenvectors.",3.1. Pre-whitening GWNN,[0],[0]
The value of d′ can be tuned using a validation set.,3.1. Pre-whitening GWNN,[0],[0]
"For simplicity, we choose d′ = d2 , which works well throughout our experiments.",3.1. Pre-whitening GWNN,[0],[0]
"This is inspired by the finding in (Zhang et al., 2015), who disclosed that the first 50% eigenvectors contribute over 95% energy in a deep model.
",3.1. Pre-whitening GWNN,[0],[0]
"More specifically, pre-GWNN first projects an input vector to a d′ low-dimensional space, õi−1d′ = P i−1 d′",3.1. Pre-whitening GWNN,[0],[0]
"(o
i−1 − µi−1) ∈ Rd′×1.",3.1. Pre-whitening GWNN,[0],[0]
"The whitened weight matrix then produces a hidden feature vector of d dimensions, which has the same length as the ordinary fc layer, i.e. ĥi = Ŵ iõi−1d′ ∈ Rd×1, where Ŵ i = W i(P i−1 d′ )
−1",3.1. Pre-whitening GWNN,[0],[0]
∈ Rd×d′ .,3.1. Pre-whitening GWNN,[0],[0]
"The computations of BN and the nonlinear activation are identical to Eqn.(1).
",3.1. Pre-whitening GWNN,[0],[0]
Training pre-GWNN is similar to Alg.1.,3.1. Pre-whitening GWNN,[0],[0]
The main modification is produced at the 7th line in order to reduce runtime.,3.1. Pre-whitening GWNN,[0],[0]
"Although Alg.1 decreases number of iterations when training converged, each iteration has additional computation time for eigen-decomposition.",3.1. Pre-whitening GWNN,[0],[0]
"For example, in WNN, the required computation of full singular value decomposition (SVD) is typically O(Nd2), where N represents the number of samples employed to estimate the covariance matrix.",3.1. Pre-whitening GWNN,[0],[0]
"In particular, when we have ` whitened layers and T is the number of iterations, all whitening transformations occupy O(Nd
2T` τ ) runtime in
the entire training stage.",3.1. Pre-whitening GWNN,[0],[0]
"In contrast, pre-GWNN performs the popular online estimation for the top d′ eigenvectors in P i−1d′ such as online SVD (Shamir, 2015; Povey et al., 2015), instead of using full SVD as WNN did.",3.1. Pre-whitening GWNN,[0],[0]
"This difference reduces runtime to O( (N+M)d
′T` τ ′ ), where τ ′
represents the whitening interval in GWNN and M is the number of samples used to estimate the top eigenvectors.",3.1. Pre-whitening GWNN,[0],[0]
"We have M = N as employed in previous works.
",3.1. Pre-whitening GWNN,[0],[0]
"For pre-GWNN, reducing runtime and improving conditioning is a tradeoff, since the former requires to increase τ ′",3.1. Pre-whitening GWNN,[0],[0]
but the latter requires to decrease it.,3.1. Pre-whitening GWNN,[0],[0]
"When M = N and d′ = d2 , we compare the runtime complexity of preGWNN to that of WNN, and obtain a ratio of dτ ′
τ , which tells us that whitening can be performed in a short interval without increasing runtime.",3.1. Pre-whitening GWNN,[0],[0]
"For instance, as shown in Fig.3 (b) when τ ′",3.1. Pre-whitening GWNN,[0],[0]
"= 20, orthogonalities are well preserved and more stable than those in (a).",3.1. Pre-whitening GWNN,[0],[0]
"In this case, preGWNN reduces computations of WNN by at least 20× when d > τ , which is a typical choice in recent deep architectures (Krizhevsky et al., 2012; Lin et al., 2014) where d ∈ {1024, 2048, 4096}.",3.1. Pre-whitening GWNN,[0],[0]
"Another variant we propose is post-whitening GWNN, abbreviated as post-GWNN.",3.2. Post-whitening GWNN,[0],[0]
"Unlike WNN and pre-GWNN, post-GWNN performs whitening transformation after applying the weight matrix (i.e. whiten the feature), as illustrated in Fig.1 (d).",3.2. Post-whitening GWNN,[0],[0]
"In general, post-GWNN reduces runtime to O( (N
′+M)d′T` τ ′ ), where N ′ N .
",3.2. Post-whitening GWNN,[0],[0]
Fig.1 (d) shows how to adapt a fc layer to post-GWNN.,3.2. Post-whitening GWNN,[0],[0]
"Suppose oi−1d′ has been whitened by P i−1 d′ in the previous layer, at the i-th layer we have
ĥi = Ŵ i(oi−1d′ − µ i−1 d′ ), h i d′ = P i d′ ĥ i, (4)
φid′ =",3.2. Post-whitening GWNN,[0],[0]
"hi d′√
Var[hi d′",3.2. Post-whitening GWNN,[0],[0]
],3.2. Post-whitening GWNN,[0],[0]
", oid′ = max(0,diag(α",3.2. Post-whitening GWNN,[0],[0]
i d′)φ i d′,3.2. Post-whitening GWNN,[0],[0]
+ β,3.2. Post-whitening GWNN,[0],[0]
"i d′),
where µi−1d′ = E[o i−1 d′ ].",3.2. Post-whitening GWNN,[0],[0]
"In Eqn.(4), a feature vector ĥi ∈ Rd×1 is first produced by applying a whitened weight matrix on the input, in order to recover the original feature length as the fc layer.",3.2. Post-whitening GWNN,[0],[0]
A whitening matrix then projects ĥi to a decorrelated feature vector hid′ ∈,3.2. Post-whitening GWNN,[0],[0]
"Rd
′×1.",3.2. Post-whitening GWNN,[0],[0]
"We have Ŵ i = W i(P i−1d′ )
−1",3.2. Post-whitening GWNN,[0],[0]
"∈ Rd×d′ , where P i−1d′ = (Si−1d′ ) − 12U i−1d′ T ∈ Rd′×d, and U i−1 and Si−1 contain eigenvectors and eigenvalues of the hidden features at the i− 1-th layer.
",3.2. Post-whitening GWNN,[0],[0]
Conditioning.,3.2. Post-whitening GWNN,[0],[0]
Here we disclose that whitening hidden features also enforces good conditioning of FIM.,3.2. Post-whitening GWNN,[0],[0]
"At this point, we have decorrelated the hidden features by satisfying E[hid′hi T d′ ] = I .",3.2. Post-whitening GWNN,[0],[0]
"Then h i d′ follows a standard multivariate Gaussian distribution, hid′ ∼ N (0, I).",3.2. Post-whitening GWNN,[0],[0]
"As a result, the layer’s output follows a rectified Gaussian distribution, which is uncorrelated as presented in remark 1.",3.2. Post-whitening GWNN,[0],[0]
"In post-GWNN, whitening hidden features of the i−",3.2. Post-whitening GWNN,[0],[0]
1- th layer improves conditioning for the i-th layer.,3.2. Post-whitening GWNN,[0],[0]
"To see this, by following the description in Sec.2.1, the diagonal block of FIM associated with the i-th layer can be written as Fii ≈ E[δĥi(δĥi)T]⊗E[(oi−1d′ −µ i−1 d′ )",3.2. Post-whitening GWNN,[0],[0]
"(o i−1 d′ −µ i−1 d′ )
T], where the parameters have low correlations since Fii has a block diagonal structure.
",3.2. Post-whitening GWNN,[0],[0]
"Algorithm 2 Training post-GWNN 1: Init: initial θ, αi, βi; and t = 0, tw = k, λ = twk ; P
i−1 = I , Ŵ it = W
i, ∀i ∈ {1...`}.",3.2. Post-whitening GWNN,[0],[0]
"2: repeat 3: for i = 1 to ` do 4: update Ŵ it , αit, and βit by SGD. 5: if mod(t, τ) = 0",3.2. Post-whitening GWNN,[0],[0]
then 6: store old P i−1o = P i−1 d′ .,3.2. Post-whitening GWNN,[0],[0]
7: estimate mean and variance of ĥi by a minibatch of N ′ samples or following remark 2 when N ′,3.2. Post-whitening GWNN,[0],[0]
= 1. 8: update P i−1d′ by online SVD.,3.2. Post-whitening GWNN,[0],[0]
"9: transform Ŵ it = Ŵ itP i−1o (P i−1 d′ )
−1.",3.2. Post-whitening GWNN,[0],[0]
10: tw = 1 and λ = twk .,3.2. Post-whitening GWNN,[0],[0]
"11: end if 12: end for 13: t = t+ 1. 14: if tw < k then tw = tw + 1 end if 15: until convergence
Remark 1.",3.2. Post-whitening GWNN,[0],[0]
"Let h ∼ N (0, I) and o = max(0, Ah + b).",3.2. Post-whitening GWNN,[0],[0]
Then E[(oj − E[oj ])(ok − E[ok])],3.2. Post-whitening GWNN,[0],[0]
≈ 0,3.2. Post-whitening GWNN,[0],[0]
"if A is a diagonal matrix, where j, k index any two entries of o and j 6= k.
For remark 1, we have A = diag(αid′) and b = β i d′ .",3.2. Post-whitening GWNN,[0],[0]
It tells us three things.,3.2. Post-whitening GWNN,[0],[0]
"First, by using whitening and BN, covariance of any two different entries of oid′ approaches zero.",3.2. Post-whitening GWNN,[0],[0]
"Second, at the iteration when we construct P id′ , we can estimate the full covariance matrix of ĥi using the mean and variance of oi−1d′ , E[ĥiĥi T
] = Ŵ iE[(oi−1d′ − µi−1d′ )(o i−1 d′",3.2. Post-whitening GWNN,[0],[0]
− µ i−1 d′ ),3.2. Post-whitening GWNN,[0],[0]
"T]Ŵ i T
.",3.2. Post-whitening GWNN,[0],[0]
The mean and variance can be estimated with a minibatch of samples i.e. N ′ N .,3.2. Post-whitening GWNN,[0],[0]
"Third, to the extreme, when N ′",3.2. Post-whitening GWNN,[0],[0]
"= 1, these statistics can still be computed in analytical forms leveraging remark 2.
",3.2. Post-whitening GWNN,[0],[0]
Remark 2.,3.2. Post-whitening GWNN,[0],[0]
"Let a random variable x ∼ N (0, 1) and y = max(0, ax + b).",3.2. Post-whitening GWNN,[0],[0]
"Then E[y] = a√
2π e−
b2
2a2 + b2Ψ(− b√ 2a )
and E[y2] = ab√ 2π e−
b2
2a2 + a 2+b2
2 Ψ(− b√ 2a ), where Ψ(x) = 1− erf(x) and erf(x) is the error function.
",3.2. Post-whitening GWNN,[0],[0]
The above remark derives the mean and variance of a rectified output unit that has shift and scale parameters.,3.2. Post-whitening GWNN,[0],[0]
"It generalizes (Arpit et al., 2016) that presented a special case when a = 1 and b = 0.",3.2. Post-whitening GWNN,[0],[0]
"In that case, we have E[y] = 1√
2π
and Var[y] = E[y2]−E[y]2 = 12− 1 2π , which are consistent with previous work.
Extensions.",3.2. Post-whitening GWNN,[0],[0]
"Remark 1 and 2 can be extended to other nonlinear activation functions, such as leaky rectified unit defined as leakyrelu(x) = max(0, x)+amin(0, x), where the slope of the negative part is controlled by the coefficient a, which is fixed in (Maas et al., 2013) and is learned in (He et al., 2015).",3.2. Post-whitening GWNN,[0],[0]
"Similar to pre-GWNN, the learning problem can be formulated as
min θ
λL(o`, y; θ) + (1− λ) ∑` i=1",3.3. Training post-GWNN,[0],[0]
"L feat(hi, ĥi; θ) (5)
s.t. rank(P i) ≤",3.3. Training post-GWNN,[0],[0]
"d′, E[hid′hi T d′ ]",3.3. Training post-GWNN,[0],[0]
"= I, i = 1...`.
Eqn.(5) has two loss functions.",3.3. Training post-GWNN,[0],[0]
"Different from WNN and pre-GWNN where the feature equality constraint can be satisfied in a closed form, this constraint is treated as an auxiliary loss function in post-GWNN, defined as Lfeat(hi, ĥi) = 12‖(h
i − E[hi])",3.3. Training post-GWNN,[0],[0]
− ĥi‖22 and minimized in the training stage.,3.3. Training post-GWNN,[0],[0]
It does not have an analytical solution because there is a nonlinear activation function between the weight matrix and the whitening matrix (i.e. in the previous layer).,3.3. Training post-GWNN,[0],[0]
"In Eqn.(5), λ is a coefficient that balances the contribution of two loss functions, and 1− λ is linearly decayed as 1− λ = k−twk , where tw = 1, 2, ...,",3.3. Training post-GWNN,[0],[0]
k.,3.3. Training post-GWNN,[0],[0]
"At each time after we update the whitening matrix, we start decay by setting tw = 1",3.3. Training post-GWNN,[0],[0]
"and k indicates the iterations at which we stop annealing.
",3.3. Training post-GWNN,[0],[0]
Alg.2 summarizes the training procedure.,3.3. Training post-GWNN,[0],[0]
It preforms online update of the top d′ eigenvectors of the whitening matrix similar to pre-GWNN.,3.3. Training post-GWNN,[0],[0]
"In comparison, it decreases the runtime of whitening transformation to O( (N
′+M)d′T` τ ′ ),
which is N+MN ′+M fold reduction with respect to pre-GWNN.",3.3. Training post-GWNN,[0],[0]
"For example, when N = M and N ′",3.3. Training post-GWNN,[0],[0]
"= 1, post-GWNN is capable of reducing computations of pre-GWNN and WNN by 2× and (2τ ′)× respectively, while maintaining better conditioning than these alternatives by choosing small τ ′.",3.3. Training post-GWNN,[0],[0]
"We compare WNN, pre-GWNN, and post-GWNN in the following aspects, including a) number of iterations when training converged, b) computation times for training, and c) generalization capacities on various datasets.",4. Empirical Studies,[0],[0]
We also conduct ablation studies with respect to 1) effect of the number of samples N to estimate the covariance matrix for pre-GWNN and 2) effect of N ′ for post-GWNN.,4. Empirical Studies,[0],[0]
"Finally, we try to tune the value of d′.
Datasets.",4. Empirical Studies,[0],[0]
We employ the following datasets.,4. Empirical Studies,[0],[0]
"a) MNIST (Lecun et al., 1998) has 60, 000 28× 28 images of 10 handwritten digits (0-9) for training and another 10, 000 test images.",4. Empirical Studies,[0],[0]
"5, 000 images from the training set are randomly selected as a validation set.",4. Empirical Studies,[0],[0]
"b) CIFAR-10 (Krizhevsky, 2009) consists of 50, 000 32 × 32 color images for training and 10, 000 images for testing.",4. Empirical Studies,[0],[0]
Each image is categorized into one of the 10 object labels.,4. Empirical Studies,[0],[0]
"For CIFAR-10, 5, 000 images are chosen for validation.",4. Empirical Studies,[0],[0]
c) CIFAR-100,4. Empirical Studies,[0],[0]
"(Krizhevsky, 2009) has the same number of
images as CIFAR-10, but each image is classified into 100 categories.",4. Empirical Studies,[0],[0]
"For CIFAR-100, we select 5, 000 images from training set for validation.",4. Empirical Studies,[0],[0]
"d) SVHN (Netzer et al., 2011) consists of color images of house numbers collected by Google Street View.",4. Empirical Studies,[0],[0]
"The task is to predict the center digit (0-9) of each image, which is of size 32×32.",4. Empirical Studies,[0],[0]
"There are 73, 257 images in the training set, 26, 032 images for test, and 531, 131 additional examples.",4. Empirical Studies,[0],[0]
"We follow (Sermanet et al., 2012) to build a validation set by selecting 400 samples per class from the training set and 200 samples per class from the additional set.",4. Empirical Studies,[0],[0]
"We didn’t train on validation, which is for tuning hyperparameters.
",4. Empirical Studies,[0],[0]
Experimental Settings.,4. Empirical Studies,[0],[0]
"We have two settings, an unsupervised and a supervised learning settings.",4. Empirical Studies,[0],[0]
"First, following (Desjardins et al., 2015), we compare the above three approaches on the task of minimizing reconstruction error of an autoencoder on MNIST.",4. Empirical Studies,[0],[0]
"The encoder consists of 4 fc sigmoidal layers, which have 1000, 500, 256, and 30 hidden neurons respectively.",4. Empirical Studies,[0],[0]
The decoder is symmetric and untied with respect to the encoder.,4. Empirical Studies,[0],[0]
"Second, for the task of image classification on CIFAR-10, -100, and SVHN, we employ the same network-in-network (NIN) (Lin et al., 2014) architecture, which has 9 convolutional layers and 3 pooling layers defined as2: conv(192, 5)-conv(160, 1)-maxpool(3, 2)-conv(96, 1)- conv(192, 5)-conv(192, 1)-avgpool(3, 2)-conv(192, 1)- conv(192, 5)-conv(192, 1)-conv(l, 1)-avgpool(8, 8), where l = 10 for CIFAR-10 and SVHN and l = 100 for CIFAR-100.",4. Empirical Studies,[0],[0]
"For all models, we use SGD with momentum of 0.9.",4. Empirical Studies,[0],[0]
"We record the number of epochs and computation time, when training WNN, pre-, and post-GWNN on MNIST and CIFAR-100, respectively.",4.1. Comparisons of Convergence and Computations,[0],[0]
We employ the first setting above for MNIST and the second setting for CIFAR100.,4.1. Comparisons of Convergence and Computations,[0],[0]
"For both settings, hyperparamters are chosen by grid search on the validation sets.",4.1. Comparisons of Convergence and Computations,[0],[0]
"The search specifications of minibatch size, learning rate, and whitening interval τ are {64, 128, 256}, {0.1, 0.01, 0.001}, and {20, 50, 100, 103}, respectively.",4.1. Comparisons of Convergence and Computations,[0],[0]
"In particular, for WNN and pre-GWNN, the number of samples used to estimate the covariance matrix, N , is picked up from {103, 10 4
2 , 10 4}.",4.1. Comparisons of Convergence and Computations,[0],[0]
"For post-GWNN,
N ′ is chosen to be the same as the minibatch size and the decay period k = 0.1τ .",4.1. Comparisons of Convergence and Computations,[0],[0]
"For a fair comparison, we report the best performance on validation set for each approach, and didn’t employ any data augmentation such as random image cropping and flipping.
2The ‘conv’, ‘maxpool’, and ‘avgpool’ represent convolution, max pooling, and average pooling respectively.",4.1. Comparisons of Convergence and Computations,[0],[0]
"Each convolutional layer is defined as conv(number of filters, filter size).",4.1. Comparisons of Convergence and Computations,[0],[0]
"For each pooling layer, we have pool(kernel size, stride).",4.1. Comparisons of Convergence and Computations,[0],[0]
"All convolutions have stride 1.
",4.1. Comparisons of Convergence and Computations,[0],[0]
The convergence and computation time are reported in Fig.4 (a-d).,4.1. Comparisons of Convergence and Computations,[0],[0]
We have several important observations.,4.1. Comparisons of Convergence and Computations,[0],[0]
"First, all three approaches converge much faster than the canonical network trained by SGD.",4.1. Comparisons of Convergence and Computations,[0],[0]
"Second, pre- and postGWNN achieve better convergence than WNN on both datasets as shown in (a) and (c).",4.1. Comparisons of Convergence and Computations,[0],[0]
"Moreover, post-GWNN outperforms pre-GWNN.",4.1. Comparisons of Convergence and Computations,[0],[0]
"Third, post-GWNN significantly reduces computation time compared to all the other methods, as illustrated in (b) and (d).",4.1. Comparisons of Convergence and Computations,[0],[0]
"We see that although WNN reduces the number of epochs, it takes long time to train because its whitening transformation occupies large computations.",4.1. Comparisons of Convergence and Computations,[0],[0]
"We evaluate WNN, pre-, and post-GWNN on CIFAR-10, -100, and SVHN datasets, and compare their classification accuracies to existing state-of-the-art methods.",4.2. Performances on various Datasets,[0],[0]
"For all the datasets and approaches, we utilize the same network structure as mentioned in the second setting above.",4.2. Performances on various Datasets,[0],[0]
"For two CIFAR datasets, we adopt minibatch size 64 and initial learning rate 0.1, which is reduced by half after every 25 epochs.",4.2. Performances on various Datasets,[0],[0]
We train for 250 epochs.,4.2. Performances on various Datasets,[0],[0]
"As SVHN is a large dataset, we train for 100 epochs with minibatch size 128 and initial learning rate 0.05, which is reduced by half after every 10 epochs.",4.2. Performances on various Datasets,[0],[0]
"We train on CIFAR-10 and -100 using both without and with data augmentation, which includes random cropping and horizontal flipping.",4.2. Performances on various Datasets,[0],[0]
"For SVHN, we didn’t augment data following (Sermanet et al., 2012).",4.2. Performances on various Datasets,[0],[0]
"For all the methods, we shuffle samples at the beginning of every epoch.",4.2. Performances on various Datasets,[0],[0]
"We use N = 10 4
2 for WNN and preGWNN and N ′",4.2. Performances on various Datasets,[0],[0]
= 64 for post-GWNN.,4.2. Performances on various Datasets,[0],[0]
"For both preand post-GWNN, we have M = N and d′ = d2 .",4.2. Performances on various Datasets,[0],[0]
The other experimental settings are similar to Sec.4.1.,4.2. Performances on various Datasets,[0],[0]
Table 1 shows the results.,4.2. Performances on various Datasets,[0],[0]
"We see that pre- and post-GWNN consistently achieve better results than those of WNN, and also outperform previous state-of-the-art approaches.",4.2. Performances on various Datasets,[0],[0]
The following experiments are conducted on CIFAR-100 using pre- or post-GWNN.,4.3. Ablation Studies,[0],[0]
The first two experiments follow the setting as mentioned in Sec.4.1.,4.3. Ablation Studies,[0],[0]
"First, we evaluate the effect of the number of samples, N , used to estimate the covariance matrix in pre-GWNN.",4.3. Ablation Studies,[0],[0]
"We compare performances of using different values of N picked up from {102, 103, 3 × 103, 5 × 103, 104}.",4.3. Ablation Studies,[0],[0]
Fig.5 (a) plots the results.,4.3. Ablation Studies,[0],[0]
We see that performance can drop because of ill conditioning when N is small e.g. N = 100.,4.3. Ablation Studies,[0],[0]
"When it is too large e.g. N = 104, we observe slightly overfitting.",4.3. Ablation Studies,[0],[0]
"Second, Fig.5 (b) highlights the effect of N ′",4.3. Ablation Studies,[0],[0]
in postGWNN.,4.3. Ablation Studies,[0],[0]
"We see that post-GWNN can work reasonably well when N ′ is small.
",4.3. Ablation Studies,[0],[0]
"Finally, instead of treating d′ = d2 as a constant in training, we study the effect of tuning its value on the validation set using a simple heuristic strategy.",4.3. Ablation Studies,[0],[0]
"If the validation error reduces more than 2% over 4 consecutive evaluations, we have d′ = d′",4.3. Ablation Studies,[0],[0]
"− rate × d′.
If the error has no reduction over this period, d′ is increased by the same rate as above.",4.3. Ablation Studies,[0],[0]
We use post-GWNN and follow experimental setting in Sec.4.2.,4.3. Ablation Studies,[0],[0]
"We take two different rates {0.1, 0.2} as examples.",4.3. Ablation Studies,[0],[0]
Fig.6 plots the variations of dimensions when d = 192 and shows their test errors.,4.3. Ablation Studies,[0],[0]
"We find
that keeping d′ as a constant generally produces better result than those obtained by the above strategy, but this strategy yields less runtime because more dimensions are pruned.",4.3. Ablation Studies,[0],[0]
We presented generalized WNN (GWNN) to reduce runtime and improve generalization of WNN.,5. Conclusion,[0],[0]
"Different from WNN that reduces computation time by whitening with a large period, leading to ill conditioning of FIM, GWNN learns compact internal representation, such that SVD is approximated by the top eigenvectors in an online manner, making GWNN not only reduces computations but also improves generalization.",5. Conclusion,[0],[0]
"By exploiting the knowledge of the hidden representation’s distribution, we showed that post-GWNN is able to compute the covariance matrix in a closed form, which can be also extended to the other activation function.",5. Conclusion,[0],[0]
Extensive experiments demonstrated the effectiveness of GWNN.,5. Conclusion,[0],[0]
"This work is partially supported by the National Natural Science Foundation of China (61503366, 61472410, U1613211), the National Key Research and Development Program of China (No.2016YFC1400700), the External Cooperation Program of BIC, Chinese Academy of Sciences (No.172644KYSB20160033), and the Science and Technology Planning Project of Guangdong Province (2015B010129013, 2014B050505017).",Acknowledgements,[0],[0]
"Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves convergence and generalization of canonical neural networks by whitening their internal hidden representation.",abstractText,[0],[0]
"However, the whitening transformation increases computation time.",abstractText,[0],[0]
"Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates convergence due to the ill conditioning, we present generalized WNN (GWNN), which has three appealing properties.",abstractText,[0],[0]
"First, GWNN is able to learn compact representation to reduce computations.",abstractText,[0],[0]
"Second, it enables whitening transformation to be performed in a short period, preserving good conditioning.",abstractText,[0],[0]
"Third, we propose a data-independent estimation of the covariance matrix to further improve computational efficiency.",abstractText,[0],[0]
Extensive experiments on various datasets demonstrate the benefits of GWNN.,abstractText,[0],[0]
Learning Deep Architectures via Generalized Whitened Neural Networks,title,[0],[0]
"ar X
iv :1
70 6.
04 96
4v 4
[ cs
.L G
] 1
4 Ju
n 20
18
ing theory for the ResNet architectures which simultaneously creates a new technique for boosting over features (in contrast to labels) and provides a new algorithm for ResNet-style architectures. Our proposed training algorithm, BoostResNet, is particularly suitable in nondifferentiable architectures. Our method only requires the relatively inexpensive sequential training of T “shallow ResNets”. We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l1 norm bounded weights.",text,[0],[0]
"Why do residual neural networks (ResNets) (He et al., 2016) and the related highway networks (Srivastava et al., 2015) work?",1. Introduction,[0],[0]
"And if we study closely why they work, can we come up with new understandings of how to train them and how to define working algorithms?
",1. Introduction,[0],[0]
"Deep neural networks have elicited breakthrough successes in machine learning, especially in image classification and object recognition (Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Zeiler & Fergus, 2014) in recent years.",1. Introduction,[0],[0]
"As the number of layers increases, the nonlinear network becomes more powerful, deriving richer features from input data.",1. Introduction,[0],[0]
"Em-
1Department of Computer Science, University of Maryland; 2Department of Computer Science, Princeton University; 3Microsoft Research.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Furong Huang <furongh@cs.umd.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"pirical studies suggest that challenging tasks in image classification (He et al., 2015; Ioffe & Szegedy, 2015; Simonyan & Zisserman, 2014; Szegedy et al., 2015) and object recognition (Girshick, 2015; Girshick et al., 2014; He et al., 2014; Long et al., 2015; Ren et al., 2015) often require “deep” networks, consisting of tens or hundreds of layers.",1. Introduction,[0],[0]
"Theoretical analyses have further justified the power of deep networks (Mhaskar & Poggio, 2016) compared to shallow networks.
",1. Introduction,[0],[0]
"However, deep neural networks are difficult to train despite their intrinsic representational power.",1. Introduction,[0],[0]
"Stochastic gradient descent with back-propagation (BP) (LeCun et al., 1989) and its variants are commonly used to solve the non-convex optimization problems.",1. Introduction,[0],[0]
"A major challenge that exists for training both shallow and deep networks is vanishing or exploding gradients (Bengio et al., 1994; Glorot & Bengio, 2010).",1. Introduction,[0],[0]
"Recent works have proposed normalization techniques (Glorot & Bengio, 2010; LeCun et al., 2012; Ioffe & Szegedy, 2015; Saxe et al., 2013) to effectively ease the problem and achieve convergence.",1. Introduction,[0],[0]
"In training deep networks, however, a surprising training performance degradation is observed (He & Sun, 2015; Srivastava et al., 2015; He et al., 2016): the training performance degrades rapidly with increased network depth after some saturation point.",1. Introduction,[0],[0]
This training performance degradation is representationally surprising as one can easily construct a deep network identical to a shallow network by forcing any part of the deep network to be the same as the shallow network with the remaining layers functioning as identity maps.,1. Introduction,[0],[0]
He et al.,1. Introduction,[0],[0]
"(He et al., 2016) presented a residual network (ResNet) learning framework to ease the training of networks that are substantially deeper than those used previously.",1. Introduction,[0],[0]
And they explicitly reformulate the layers as learning residual functions with reference to the layer inputs by adding identity loops to the layers.,1. Introduction,[0],[0]
"It is shown in (Hardt & Ma, 2016) that identity loops ease the problem of spurious local optima in shallow networks.",1. Introduction,[0],[0]
"Srivastava et al. (Srivastava et al., 2015) introduce a novel architecture that enables the optimization of networks with virtually arbitrary depth through the use of a learned gating mechanism for regulating information flow.
",1. Introduction,[0],[0]
Empirical evidence overwhelmingly shows that these deep residual networks are easier to optimize than non-residual ones.,1. Introduction,[0],[0]
"Can we develop a theoretical justification for this
observation?",1. Introduction,[0],[0]
And does that justification point us towards new algorithms with better characteristics?,1. Introduction,[0],[0]
"We propose a new framework, multi-channel telescoping sum boosting (defined in Section 4), to characterize a feed forward ResNet in Section 3.",1.1. Summary of Results,[0],[0]
We show that the top level (final) output of a ResNet can be thought of as a layer-bylayer boosting method (defined in Section 2).,1.1. Summary of Results,[0],[0]
"Traditional boosting, which ensembles “estimated score functions” or “estimated labels” from weak learners, does not work in the ResNet setting because of two reasons: (1) ResNet is a telescoping sum boosting of weak learners, not a naive (weighted) ensemble; (2) ResNet boosts over “representations”, not “estimated labels”.",1.1. Summary of Results,[0],[0]
We provide the first error bound for telescoping sum boosting over features.,1.1. Summary of Results,[0],[0]
Boosting over features and boosting over labels are different.,1.1. Summary of Results,[0],[0]
There is no existing work that proves a boosting theory (guaranteed 0 training error) for boosting features.,1.1. Summary of Results,[0],[0]
"Moreover, the special structure of a ResNet entails more complicated analysis: telescoping sum boosting, which has never been introduced before in the existing literature.
",1.1. Summary of Results,[0],[0]
We introduce a learning algorithm (BoostResNet) guaranteed to reduce error exponentially as depth increases so long as a weak learning assumption is obeyed.,1.1. Summary of Results,[0],[0]
BoostResNet adaptively selects training samples or changes the cost function (Section 4 Theorem 4.2).,1.1. Summary of Results,[0],[0]
"In Section 4.4, we analyze the generalization error of BoostResNet and provide advice to avoid overfitting.",1.1. Summary of Results,[0],[0]
"The procedure trains each residual block sequentially, only requiring that each provides a better-than-a-weak-baseline in predicting labels.
",1.1. Summary of Results,[0],[0]
BoostResNet requires radically lower computational complexity for training than end-to-end back propagation (e2eBP).,1.1. Summary of Results,[0],[0]
The number of gradient updates required by BoostResNet is much smaller than e2eBP as discussed in Section 4.3.,1.1. Summary of Results,[0],[0]
"Memorywise, BoostResNet requires only individual layers of the network to be in the graphics processing unit (GPU) while e2eBP inevitably keeps all layers in the GPU.",1.1. Summary of Results,[0],[0]
"For example, in a state-of-the-art deep ResNet, this might reduce the RAM requirements for GPU by a factor of the depth of the network.",1.1. Summary of Results,[0],[0]
"Similar improvements in computation are observed since each e2eBP step involves back propagating through the entire deep network.
",1.1. Summary of Results,[0],[0]
"Experimentally, we compare BoostResNet with e2eBP over two types of feed-forward ResNets, multilayer perceptron residual network (MLP-ResNet) and convolutional neural network residual network (CNN-ResNet), on multiple datasets.",1.1. Summary of Results,[0],[0]
BoostResNet shows substantial computational performance improvements and accuracy improvement under the MLP-ResNet architecture.,1.1. Summary of Results,[0],[0]
"Under CNN-ResNet, a faster convergence for BoostResNet is observed.
",1.1. Summary of Results,[0],[0]
One of the hallmarks of our approach is to make an explicit distinction between the classes of the multiclass learning problem and channels that are constructed by the learning procedure.,1.1. Summary of Results,[0],[0]
A channel here is essentially a scalar value modified by the rounds of boosting so as to implicitly minimize the multiclass error rate.,1.1. Summary of Results,[0],[0]
"Our multi-channel telescoping sum boosting learning framework is not limited to ResNet and can be extended to other, even non-differentiable, nonlinear hypothesis units, such as decision trees or tensor decompositions.",1.1. Summary of Results,[0],[0]
"Our contribution does not limit to explaining ResNet in the boosting framework, we have also developed a new boosting framework for other relevant tasks that require multi-channel telescoping sum structure.",1.1. Summary of Results,[0],[0]
Training deep neural networks has been an active research area in the past few years.,1.2. Related Works,[0],[0]
The main optimization challenge lies in the highly non-convex nature of the loss function.,1.2. Related Works,[0],[0]
"There are two main ways to address this optimization problem: one is to select a loss function and network architecture that have better geometric properties (details refer to appendix A.1), and the other is to improve the network’s learning procedure (details refer to appendix A.2).
",1.2. Related Works,[0],[0]
"Many authors have previously looked into neural networks and boosting, each in a different way.",1.2. Related Works,[0],[0]
"Bengio et al. (2006) introduce single hidden layer convex neural networks, and propose a gradient boosting algorithm to learn the weights of the linear classifier.",1.2. Related Works,[0],[0]
The approach has not been generalized to deep networks with more than one hidden layer.,1.2. Related Works,[0],[0]
Shalev-Shwartz (2014) proposes a selfieBoost algorithm which boosts the accuracy of an entire network.,1.2. Related Works,[0],[0]
Our algorithm is different as we instead construct ensembles of classifiers.,1.2. Related Works,[0],[0]
Veit et al. (2016) interpret residual networks as a collection of many paths of differing length.,1.2. Related Works,[0],[0]
"Their empirical study shows that residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.
",1.2. Related Works,[0],[0]
Comparison with AdaNet,1.2. Related Works,[0],[0]
"The authors of AdaNet (Cortes et al., 2016) consider ensembles of neural layers with a boosting-style algorithm and provide a method for structural learning of neural networks by optimizing over the generalization bound, which consists of the training error and the complexity of the AdaNet architecture.",1.2. Related Works,[0],[0]
AdaNet uses the traditional boosting framework where weak classifiers are being boosted.,1.2. Related Works,[0],[0]
"Therefore, to obtain low training error guarantee, AdaNet maps the feature vectors (hidden layer representations) to a classifier space and boosts the weak classifiers.",1.2. Related Works,[0],[0]
"In AdaNet, features (representations) from each lower layer have to be fed into a classifier (in other words, be transferred to score function in the label space).",1.2. Related Works,[0],[0]
"This is because AdaNet uses traditional boosting, which ensembles score functions or labels.",1.2. Related Works,[0],[0]
"As
a result, the top classifier in AdaNet has to be connected to all lower layers, making the structure bushy.",1.2. Related Works,[0],[0]
"Therefore AdaNet chooses its own structure during learning, and its boosting theory does not necessarily work for a ResNet structure.
",1.2. Related Works,[0],[0]
"Our BoostResNet, instead, boosts features (representations) over multiple channels, and thus produces a less “bushy” architecture.",1.2. Related Works,[0],[0]
"We are able to boost features by developing this new “telescoping-sum boosting” framework, one of our main contributions.",1.2. Related Works,[0],[0]
We come up with the new weak learning condition for the telescoping-sum boosting framework.,1.2. Related Works,[0],[0]
"The algorithm is also very different from AdaNet and is explained in details in section 3 and 4.
",1.2. Related Works,[0],[0]
"BoostResNet focuses on a ResNet architecture, provides a new training algorithm for ResNet, and proves a training error guarantee for deep ResNet architecture.",1.2. Related Works,[0],[0]
"A ResNetstyle architecture is a special case of AdaNet, so AdaNet generalization guarantee applies here and our generalization analysis is built upon their work.",1.2. Related Works,[0],[0]
A residual neural network (ResNet) is composed of stacked entities referred to as residual blocks.,2. Preliminaries,[0],[0]
Each residual block consists of a neural network module and an identity loop (shortcut).,2. Preliminaries,[0],[0]
Commonly used modules include MLP and CNN.,2. Preliminaries,[0],[0]
"Throughout this paper, we consider training and test examples generated i.i.d.",2. Preliminaries,[0],[0]
"from some distribution D over X ×Y , where X is the input space and Y is the label space.",2. Preliminaries,[0],[0]
"We denote by S = ((x1, y1), (x2, y2), . . .",2. Preliminaries,[0],[0]
", (xm, ym))",2. Preliminaries,[0],[0]
"a training set of m examples drawn according to Dm.
",2. Preliminaries,[0],[0]
A Residual Block of ResNet ResNet consists of residual blocks.,2. Preliminaries,[0],[0]
Each residual block contains a module and an identity loop.,2. Preliminaries,[0],[0]
"Let each module map its input x̃ to ft(x̃) where
t denotes the level of the modules.",2. Preliminaries,[0],[0]
"Each module ft is a nonlinear unit with n channels, i.e., ft(·) ∈ Rn.",2. Preliminaries,[0],[0]
"In multilayer perceptron residual network (MLP-ResNet), ft is a shallow MLP, for instance, ft(x̃) = Ṽ ⊤",2. Preliminaries,[0],[0]
t σ(W̃ ⊤ t x̃),2. Preliminaries,[0],[0]
where W̃t ∈,2. Preliminaries,[0],[0]
"Rn×k, Ṽt ∈ Rk×n and σ is a nonlinear operator such as sigmoidal function or relu function.",2. Preliminaries,[0],[0]
"Similarly, in convolutional neural network residual network (CNN-ResNet), ft(·) represents the t-th convolutional module.",2. Preliminaries,[0],[0]
"Then the t-th residual block outputs gt+1(x)
",2. Preliminaries,[0],[0]
gt+1(x) = ft(gt(x)),2. Preliminaries,[0],[0]
"+ gt(x), (1)
where x is the input fed to the ResNet.",2. Preliminaries,[0],[0]
"See Figure 1 for an illustration of a ResNet, which consists of stacked residual blocks (each residual block contains a nonlinear module and an identity loop).
",2. Preliminaries,[0],[0]
"Output of ResNet Due to the recursive relation specified in Equation (1), the output of the T -th residual block is equal to the summation over lower module outputs, i.e., gT+1(x) = ∑T
t=0 ft(gt(x)), where g0(x) = 0 and f0(g0(x))",2. Preliminaries,[0],[0]
= x.,2. Preliminaries,[0],[0]
"For binary classification tasks, the final output of a ResNet given input x is rendered after a linear classifier w ∈ Rn on representation gT+1(x) (In the multiclass setting, let C be the number of classes; the linear classifier W ∈ Rn×C is a matrix instead of a vector.):
ŷ",2. Preliminaries,[0],[0]
= σ̃ (F (x)),2. Preliminaries,[0],[0]
"= σ̃(w⊤gT+1(x)) = σ̃ ( w⊤ T∑
t=0
ft(gt(x))
)
(2)
where F (x) = w⊤gT+1(x) and σ̃(·) denotes a map from classifier outputs (scores) to labels.",2. Preliminaries,[0],[0]
For instance σ̃(z) = sign(z) for binary classification (σ̃(z) =,2. Preliminaries,[0],[0]
"argmax
i zi
for multiclass classification).",2. Preliminaries,[0],[0]
"The parameters of a depthT ResNet are {w, {ft(·), ∀t ∈ T }}.",2. Preliminaries,[0],[0]
A ResNet training involves training the classifier w and the weights of modules ft(·) ∀t ∈,2. Preliminaries,[0],[0]
"[T ] when training examples (x1, y1), (x2, y2), . . .",2. Preliminaries,[0],[0]
", (xm, ym) are available.
",2. Preliminaries,[0],[0]
"Boosting Boosting (Freund & Schapire, 1995) assumes the availability of a weak learning algorithm which, given labeled training examples, produces a weak classifier (a.k.a. base classifier).",2. Preliminaries,[0],[0]
The goal of boosting is to improve the performance of the weak learning algorithm.,2. Preliminaries,[0],[0]
The key idea behind boosting is to choose training sets for the weak classifier in such a fashion as to force it to infer something new about the data each time it is called.,2. Preliminaries,[0],[0]
"The weak learning algorithm will finally combine many weak classifiers into a single strong classifier whose prediction power is strong.
",2. Preliminaries,[0],[0]
"From empirical experience, ResNet remedies the problem of training error degradation (instability of solving nonconvex optimization problem using SGD) in deeper neural networks.",2. Preliminaries,[0],[0]
"We are curious about whether there is a
theoretical justification that identity loops help in training.",2. Preliminaries,[0],[0]
"More importantly, we are interested in proposing a new algorithm that avoids end-to-end back-propagation (e2eBP) through the deep network and thus is immune to the instability of SGD for non-convex optimization of deep neural networks.",2. Preliminaries,[0],[0]
"As we recall from Equation (2), ResNet indeed has a similar form as the strong classifier in boosting.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"The key difference is that boosting is an ensemble of estimated hypotheses whereas ResNet is an ensemble of estimated feature representations ∑T
t=0 ft(gt(x)).",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"To solve this problem, we introduce an auxiliary linear classifier wt on top of each residual block to construct a hypothesis module.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Formally, a hypothesis module is defined as
ot(x) def = w⊤t gt(x) ∈ R (3)
in the binary classification setting.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
Therefore ot+1(x) = w⊤t+1[ft(gt(x)),3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
+ gt(x)] as gt+1(x) = ft(gt(x)) + gt(x).,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"We emphasize that given gt(x), we only need to train ft and wt+1 to train ot+1(x).",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"In other words, we feed the output of previous residual block (gt(x)) to the current module and train the weights of current module ft(·) and the auxiliary classifier wt+1.
",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Now the input, gt+1(x), of the t + 1-th residual block is the output, ft(gt(x))+gt(x), of the t-th residual block.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"As a result, ot(x) = ∑t−1 t′=0 w ⊤ t ft′(gt′(x))",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"In other words, the auxiliary linear classifier is common for all modules underneath.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"It would not be realistic to assume a common auxiliary linear classifier, as such an assumption prevents us from training the T hypothesis module sequentially.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"We design a weak module classifier using the idea of telescoping sum as follows.
",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
Definition 3.1.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"A weak module classifier is defined as
ht(x) def = αt+1ot+1(x) − αtot(x) (4)
where ot(x) def = w⊤t gt(x) is a hypothesis module, and αt is a scalar.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"We call it a “telescoping sum boosting” framework if the weak learners are restricted to the form of the weak module classifier.
",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
ResNet:,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Ensemble of Weak Module Classifiers Recall that the T -th residual block of a ResNet outputs gT+1(x), which is fed to the top/final linear classifier for the final classification.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
We show that an ensemble of the weak module classifiers is equivalent to a ResNet’s final output.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
We state it formally in Lemma 3.2.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"For purposes of exposition,
we will call F (x) the output of ResNet although a σ̃ function is applied on top of F (x), mapping the output to the label space Y .",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
Lemma 3.2.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Let the input gt(x) of the t-th module be the output of the previous module, i.e., gt+1(x) = ft(gt(x))",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
+ gt(x).,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Then the summation of T weak module classifiers divided by αT+1 is identical to the output, F (x), of the depth-T ResNet,
F (x) = w⊤gT+1(x) ≡",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"1
αT+1
T∑
t=0
ht(x),",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"(5)
where the weak module classifier ht(x) is defined in Equation (4).
See Appendix B for the proof.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Overall, our proposed ensemble of weak module classifiers is a new framework that allows for sequential training of ResNet.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
Note that traditional boosting algorithm results do not apply here.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
We now analyze our telescoping sum boosting framework in Section 4.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Our analysis applies to both binary and multiclass, but we will focus on the binary class for simplicity in the main text and defer the multiclass analysis to the Appendix F.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Below, we propose a learning algorithm whose training error decays exponentially with the number of weak module classifiers T under a weak learning condition.",4. Telescoping Sum Boosting for Binary Classification,[0],[0]
"We restrict to bounded hypothesis modules, i.e., |ot(x)| ≤ 1.",4. Telescoping Sum Boosting for Binary Classification,[0],[0]
The weak module classifier involves the difference between (scaled version of) ot+1(x) and ot(x).,4.1. Weak Learning Condition,[0],[0]
Let γ̃t def = Ei∼Dt−1,4.1. Weak Learning Condition,[0],[0]
[yiot(xi)],4.1. Weak Learning Condition,[0],[0]
"> 0 be the edge of the hypothesis module ot(x), where Dt−1 is the weight of the examples.",4.1. Weak Learning Condition,[0],[0]
"As the hypothesis module ot(x) is bounded by 1, we obtain |γ̃t| ≤ 1.",4.1. Weak Learning Condition,[0],[0]
So γ̃t characterizes the performance of the hypothesis module ot(x).,4.1. Weak Learning Condition,[0],[0]
"A natural requirement would be that ot+1(x) improves slightly upon ot(x), and thus γ̃t+1 − γ̃t ≥ γ′ > 0 could serve as a weak learning condition.",4.1. Weak Learning Condition,[0],[0]
"However this weak learning condition is too strong: even when current hypothesis module is performing almost ideally (γ̃t is close to 1), we still seek a hypothesis module which performs consistently better than the previous one by γ′. Instead, we consider a much weaker learning condition, inspired by training error analysis, as follows.
",4.1. Weak Learning Condition,[0],[0]
Definition 4.1 (γ-Weak Learning Condition).,4.1. Weak Learning Condition,[0],[0]
"A weak module classifier ht(x) = αt+1ot+1 − αtot satisfies the γweak learning condition if γ̃2t+1−γ̃ 2 t
1−γ̃2t ≥ γ2 > 0",4.1. Weak Learning Condition,[0],[0]
"and the
Algorithm 1 BoostResNet: telescoping sum boosting for binary-class classification Input: m labeled samples",4.1. Weak Learning Condition,[0],[0]
"[(xi, yi)]m",4.1. Weak Learning Condition,[0],[0]
"where yi ∈ {−1,+1} and a threshold γ Output: {ft(·), ∀t} and wT+1 ⊲ Discard wt+1, ∀t 6= T
1: Initialize t← 0, γ̃0",4.1. Weak Learning Condition,[0],[0]
"← 0, α0 ← 0, o0(x)← 0 2: Initialize sample weights at round 0: D0(i)← 1/m, ∀i ∈",4.1. Weak Learning Condition,[0],[0]
"[m] 3: while γt > γ do 4: ft(·), αt+1,wt+1, ot+1(x)← Algorithm 2(gt(x), Dt, ot(x), αt)
5: Compute γt ← √
γ̃2t+1−γ̃2t 1−γ̃2t
⊲ where γ̃t+1 ← Ei∼Dt",4.1. Weak Learning Condition,[0],[0]
"[yiot+1(xi)]
6: Update Dt+1(i)← Dt(i) exp(−yiht(xi))m∑ i=1",4.1. Weak Learning Condition,[0],[0]
Dt(i) exp[−yiht(xi)],4.1. Weak Learning Condition,[0],[0]
"⊲ where ht(x) = αt+1ot+1(x) − αtot(x) 7: t← t+ 1 8: end while 9: T ← t− 1
Algorithm 2 BoostResNet:",4.1. Weak Learning Condition,[0],[0]
"oracle implementation for training a ResNet block
Input: gt(x),Dt,ot(x) and αt Output: ft(·), αt+1, wt+1 and ot+1(x)
1: (ft, αt+1,wt+1)← arg min (f,α,v) m∑ i=1 Dt(i) exp ( −yiαv⊤ [f(gt(xi))",4.1. Weak Learning Condition,[0],[0]
+ gt(xi)],4.1. Weak Learning Condition,[0],[0]
+ yiαtot(xi) ),4.1. Weak Learning Condition,[0],[0]
2: ot+1(x)← w⊤t+1 [ft(gt(x)),4.1. Weak Learning Condition,[0],[0]
"+ gt(x)]
covariance between exp(−yot+1(x)) and exp(yot(x)) is non-positive.
",4.1. Weak Learning Condition,[0],[0]
"The weak learning condition is motivated by the learning theory and it is met in practice (refer to Figure 4).
",4.1. Weak Learning Condition,[0],[0]
"Interpretation of weak learning condition For each weak
module classifier ht(x), γt def = √ γ̃2t+1−γ̃2t 1−γ̃2t characterizes the normalized improvement of the correlation between the true labels y and the hypothesis modules ot+1(x) over the correlation between the true labels y and the hypothesis modules ot(x).",4.1. Weak Learning Condition,[0],[0]
The condition specified in Definition 4.1 is mild as it requires the hypothesis module ot+1(x) to perform only slightly better than the previous hypothesis module ot(x).,4.1. Weak Learning Condition,[0],[0]
"In residual network, since ot+1(x) represents a depth-(t + 1) residual network which is a deeper counterpart of the depth-t residual network ot(x), it is natural to assume that the deeper residual network improves slightly upon the shallower residual network.",4.1. Weak Learning Condition,[0],[0]
"When γ̃t is close to 1, γ̃2t+1 only needs to be slightly better than γ̃ 2 t as the denominator 1 − γ̃2t is small.",4.1. Weak Learning Condition,[0],[0]
"The assumption of the covariance between exp(−yot+1(x)) and exp(yot(x)) being non-positive is suggesting that the weak module classifiers should not be adversarial, which may be a reasonable assumption for ResNet.",4.1. Weak Learning Condition,[0],[0]
"We now propose a novel training algorithm for telescoping sum boosting under binary-class classification as in Algo-
rithm 1.",4.2. BoostResNet,[0],[0]
"In particular, we introduce a training procedure for deep ResNet in Algorithm 1 & 2, BoostResNet, which only requires sequential training of shallow ResNets.
",4.2. BoostResNet,[0],[0]
The training algorithm is a module-by-module procedure following a bottom-up fashion as the outputs of the t-th module gt+1(x) are fed as the training examples to the next t + 1-th module.,4.2. BoostResNet,[0],[0]
Each of the shallow ResNet ft(gt(x)),4.2. BoostResNet,[0],[0]
+ gt(x) is combined with an auxiliary linear classifier wt+1 to form a hypothesis module ot+1(x).,4.2. BoostResNet,[0],[0]
The weights of the ResNet are trained on these shallow ResNets.,4.2. BoostResNet,[0],[0]
The telescoping sum construction is the key for successful interpretation of ResNet as ensembles of weak module classifiers.,4.2. BoostResNet,[0],[0]
The innovative introduction of the auxiliary linear classifiers (wt+1) is the key solution for successful multi-channel representation boosting with theoretical guarantees.,4.2. BoostResNet,[0],[0]
"Auxiliary linear classifiers are only used to guide training, and they are not included in the model (proved in Lemma 3.2).",4.2. BoostResNet,[0],[0]
This is the fundamental difference between BoostResNet and AdaNet.,4.2. BoostResNet,[0],[0]
"AdaNet (Cortes et al., 2016) maps the feature vectors (hidden layer representations) to a classifier space and boosts the weak classifiers.",4.2. BoostResNet,[0],[0]
Our framework is a multi-channel representation (or information) boosting rather than a traditional classifier boosting.,4.2. BoostResNet,[0],[0]
"Traditional boosting theory does not apply in our setting.
",4.2. BoostResNet,[0],[0]
Theorem 4.2.,4.2. BoostResNet,[0],[0]
[ Training error bound ],4.2. BoostResNet,[0],[0]
"The training error of a T -module telescoping sum boosting framework using Algorithms 1 and 2 decays exponentially with the number
of modules T ,
Pr i∼S
( σ̃ ( ∑
t
ht (xi) )",4.2. BoostResNet,[0],[0]
"6= yi ) ≤ e− 12Tγ2
if ∀t ∈",4.2. BoostResNet,[0],[0]
"[T ] the weak module classifier ht(x) satisfies the γ-weak learning condition defined in Definition 4.1.
",4.2. BoostResNet,[0],[0]
"The training error of Algorithms 1 and 2 is guaranteed to decay exponentially with the ResNet depth even when each hypothesis module ot+1(x) performs slightly better than its previous hypothesis module ot(x) (i.e., γ > 0).",4.2. BoostResNet,[0],[0]
Refer to Appendix F for the algorithm and theoretical guarantees for multiclass classification.,4.2. BoostResNet,[0],[0]
"In Algorithm 2, the implementation of the oracle at line 1 is equivalent to
(ft, αt+1,wt+1) =
arg min (f,α,v)
1 m
m ∑
i=1
exp ( −yiαv ⊤",4.3. Oracle Implementation for ResNet,[0],[0]
"[f(gt(xi)) + gt(xi)] ) (6)
",4.3. Oracle Implementation for ResNet,[0],[0]
The minimization problem over f corresponds to finding the weights of the t-th nonlinear module of the residual network.,4.3. Oracle Implementation for ResNet,[0],[0]
Auxiliary classifier wt+1 is used to help solve this minimization problem with the guidance of training labels yi.,4.3. Oracle Implementation for ResNet,[0],[0]
"However, the final neural network model includes none of the auxiliary classifiers, and still follows a standard ResNet structure (proved in Lemma 3.2).",4.3. Oracle Implementation for ResNet,[0],[0]
"In practice, there are various ways to implement Equation (6).",4.3. Oracle Implementation for ResNet,[0],[0]
"For instance, Janzamin et. al. (Janzamin et al., 2015) propose a tensor decomposition technique which decomposes a tensor formed by some transformation of the features x combined with labels y and recovers the weights of a one-hidden layer neural network with guarantees.",4.3. Oracle Implementation for ResNet,[0],[0]
"One can also use backpropagation as numerous works have shown that gradient based training are relatively stable on shallow networks with identity loops (Hardt & Ma, 2016; He et al., 2016).
",4.3. Oracle Implementation for ResNet,[0],[0]
Computational & Memory Efficiency BoostResNet training is memory efficient as the training process only requires parameters of two consecutive residual blocks to be in memory.,4.3. Oracle Implementation for ResNet,[0],[0]
"Given that the limited GPU memory being one of the main bottlenecks for computational efficiency, BoostResNet requires significantly less training time than e2eBP in deep networks as a result of reduced communication overhead and the speed-up in shallow gradient forwarding and back-propagation.",4.3. Oracle Implementation for ResNet,[0],[0]
"Let M1 be the memory required for one module, and M2 be the memory required for one linear classifier, the memory consumption is M1 +M2 by BoostResNet and M1T +M2 by e2eBP.",4.3. Oracle Implementation for ResNet,[0],[0]
"Let the flops needed for gradient update over one module and one linear classifier be C1 and C2 respectively, the computation cost is C1+C2 by BoostResNet and C1T + C2 by e2eBP.",4.3. Oracle Implementation for ResNet,[0],[0]
"In this section, we analyze the generalization error to understand the possibility of overfitting under Algorithm 1.",4.4. Generalization Error Analysis,[0],[0]
"The strong classifier or the ResNet is F (x) = ∑ t ht(x)
αT+1 .",4.4. Generalization Error Analysis,[0],[0]
"Now
we define the margin for example (x, y) as yF (x).",4.4. Generalization Error Analysis,[0],[0]
"For simplicity, we consider MLP-ResNet with n multiple channels and assume that the weight vector connecting a neuron at layer t with its preceding layer neurons is l1 norm bounded by Λt,t−1.",4.4. Generalization Error Analysis,[0],[0]
"Recall that there exists a linear classifier w on top, and we restrict to l1 norm bounded classifiers, i.e., ‖w‖1 ≤ C0 <∞. The expected training examples are l∞ norm bounded r∞ def = ES∼D",4.4. Generalization Error Analysis,[0],[0]
[ maxi∈[m]‖xi‖∞ ],4.4. Generalization Error Analysis,[0],[0]
"< ∞. We introduce Corollary 4.3 which follows directly from Lemma 2 of (Cortes et al., 2016).",4.4. Generalization Error Analysis,[0],[0]
Corollary 4.3.,4.4. Generalization Error Analysis,[0],[0]
"(Cortes et al., 2016) Let D be a distribution over X × Y and S be a sample of m examples chosen independently at random according to D. With probability at least 1−δ, for θ > 0, the strong classifier F (x) (ResNet) satisfies that
Pr D (yF (x) ≤ 0) ≤",4.4. Generalization Error Analysis,[0],[0]
"Pr S (yF (x) ≤ θ)+
4C0r∞ θ
√
log(2n)
2m
T ∑
t=0
Λt + 2
θ
√
log T
m + β(θ,m, T, δ) (7)
where Λt def = ∏t t′=0 2Λt′,t′−1 and β(θ,m, T, δ)
def =√⌈
4 θ2
log (
θ2m log T )⌉ log T m + log 2 δ 2m .
",4.4. Generalization Error Analysis,[0],[0]
"From Corollary 4.3, we obtain a generalization error bound in terms of margin bound PrS (yF (x) ≤ θ) and network complexity 4C0r∞
θ
√ log(2n)
2m ∑T t=0",4.4. Generalization Error Analysis,[0],[0]
"Λt + 2 θ √ log T m +
β(θ,m, T, δ).",4.4. Generalization Error Analysis,[0],[0]
"Larger margin bound (larger θ) contributes positively to generalization accuracy, and l1 norm bounded weights (smaller ∑T t=0",4.4. Generalization Error Analysis,[0],[0]
Λt ) are beneficial to control network complexity and to avoid overfitting.,4.4. Generalization Error Analysis,[0],[0]
"The dominant term in the network complexity is
4C0r∞ θ
√ log(2n)
2m ∑T t=0 Λt which scales as least linearly
with the depth T .",4.4. Generalization Error Analysis,[0],[0]
"See Appendix D for the proof.
",4.4. Generalization Error Analysis,[0],[0]
"This corollary suggests that stronger weak module classifiers which produce higher accuracy predictions and larger edges, will yield larger margins and suffer less from overfitting.",4.4. Generalization Error Analysis,[0],[0]
"The larger the value of θ, the smaller the term 4C0r∞
θ
√ log(2n)
2m ∑T t=0",4.4. Generalization Error Analysis,[0],[0]
"Λt + 2 θ √ log T m + β(θ,m, T, δ) is.
",4.4. Generalization Error Analysis,[0],[0]
"With larger edges on the training set and when γ̃T+1 < 1, we are able to choose larger values of θ while keeping the error term zero or close to zero.",4.4. Generalization Error Analysis,[0],[0]
"We compare our proposed BoostResNet algorithm with e2eBP training a ResNet on the MNIST (LeCun et al.,
0 5 10 15 20 25 30 0
0.2
0.4
0.6
0.8 1 T ra in in g A cc u ra cy PSfrag replacements Training Acc
Number of Residual Blocks
BoostResNet e2eBP
(a) depth vs training accuracy
0 5 10 15 20 25 30 0
0.2
0.4
0.6
0.8
1
T e
A cc
u ra
cy
PSfrag replacements
Test Acc
BoostResNet
Number of Residual Blocks
e2eBP
(b) depth vs test accuracy
Figure 2:",5. Experiments,[0],[0]
"Comparison of BoostResNet (ours, blue) and e2eBP (baseline, red) on multilayer perceptron residual network on MNIST dataset.
10 7
10 8
10 9
Number of Gradient Updates
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
A cc
u ra
cy
BoostResNet Training BoostResNet Test e2eBP Training e2eBP Test PSfrag replacements
BoostResNet Training
BoostResNet Test
e2eBP Training (a) SVHN
10 7
10 8
10 9
10 10
Number of Gradient Updates
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
A cc
u ra
cy
BoostResNet Training BoostResNet Test e2eBP Training e2eBP Test
(b) CIFAR-10
Figure 3: Convergence performance comparison between e2eBP and BoostResNet on convolutional neural network residual network on the SVHN and CIFAR-10 dataset.",5. Experiments,[0],[0]
"The vertical dotted line shows when BoostResNet training stopped, and we began refining the network with standard e2eBP training.
0",5. Experiments,[0],[0]
"10 20 30 40 50 0
0.1
0.2
PSfrag replacements γt
depth T (a) γt
0 10 20 30 40 50 0.7
0.8
0.9
",5. Experiments,[0],[0]
"PSfrag replacementsγ̃t
depth T (b) γ̃t
Figure 4: Visualization of edge γt and edge for each residual block γ̃t.",5. Experiments,[0],[0]
"The x-axis represents depth, and the y-axis represents γt or γ̃t values.",5. Experiments,[0],[0]
"The plots are for a convolutional network composed of 50 residual blocks and trained on the SVHN dataset.
1998), street view house numbers (SVHN) (Netzer et al., 2011), and CIFAR-10 (Krizhevsky & Hinton, 2009) benchmark datasets.",5. Experiments,[0],[0]
"Two different types of architectures are tested: a ResNet where each module is a fully-connected multi-layer perceptron (MLP-ResNet) and a more common, convolutional neural network residual network (CNNResNet).",5. Experiments,[0],[0]
"In each experiment the architecture of both algorithms is identical, and they are both initialized with the same random seed.",5. Experiments,[0],[0]
"As a baseline, we also experiment with standard boosting (AdaBoost.",5. Experiments,[0],[0]
"MM (Mukherjee & Schapire, 2013)) of convolutional modules for SVHN and CIFAR-10 datasets.",5. Experiments,[0],[0]
Our experiments are programmed in the Torch deep learning framework for Lua and executed on NVIDIA Tesla P100 GPUs.,5. Experiments,[0],[0]
"All models are trained using the Adam variant of SGD (Kingma & Ba, 2014).
",5. Experiments,[0],[0]
"Hyperparameters are selected via random search for high-
est accuracy on a validation set.",5. Experiments,[0],[0]
"They are specified in Appendix H. In BoostResNet, the most important hyperparameters, according to our experiments, are those that govern when the algorithm stops training the current module and begins training its successor.
",5. Experiments,[0],[0]
"MLP-ResNet on MNISTThe MNIST database (LeCun et al., 1998) of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples.",5. Experiments,[0],[0]
The data contains ten classes.,5. Experiments,[0],[0]
"We test the performance of BoostResNet on MLP-ResNet using MNIST dataset, and compare it with e2eBP baseline.",5. Experiments,[0],[0]
"Each residual block is composed of an MLP with a single, 1024-dimensional hidden layer.",5. Experiments,[0],[0]
The training and test error between BoostResNet and e2eBP is in Figure 2 as a function of depth.,5. Experiments,[0],[0]
"Surprisingly, we find that training error degrades for e2eBP, although the ResNet’s identity
loop is supposed to alleviate this problem.",5. Experiments,[0],[0]
"Our proposed sequential training procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth increases.
",5. Experiments,[0],[0]
"CNN-ResNet on SVHN SVHN (Netzer et al., 2011) is a real-world image dataset, obtained from house numbers in Google Street View images.",5. Experiments,[0],[0]
"The dataset contains over 600,000 training images, and about 20,000 test images.",5. Experiments,[0],[0]
"We fit a 50-layer, 25-residual-block CNN-ResNet using both BoostResNet and e2eBP (figure 3a).",5. Experiments,[0],[0]
Each residual block is composed of a CNN using 15 3 × 3 filters.,5. Experiments,[0],[0]
We refine the result of BoostResNet by initializing the weights using the result of BoostResNet and run end-to-end back propagation (e2eBP).,5. Experiments,[0],[0]
"From figure 3a, our BoostResNet converges much faster (requires much fewer gradient updates) than e2eBP.",5. Experiments,[0],[0]
"The test accuracy of BoostResNet is comparable with e2eBP.
",5. Experiments,[0],[0]
"CNN-ResNet on CIFAR-10 The CIFAR-10 dataset is a benchmark dataset composed of 10 classes of small images, such as animals and vehicles.",5. Experiments,[0],[0]
"It consists of 50,000 training images and 10,000 test images.",5. Experiments,[0],[0]
"We again fit a 50-layer, 25- residual-block CNN-ResNet using both BoostResNet and e2eBP (figure 3b).",5. Experiments,[0],[0]
BoostResNet training converges to the optimal solution faster than e2eBP.,5. Experiments,[0],[0]
"Unlike in the previous two datasets, the efficiency of BoostResNet comes at a cost when training with CIFAR-10.",5. Experiments,[0],[0]
"We find that the test accuracy of the e2eBP refined BoostResNet to be slightly lower than that produced by e2eBP.
",5. Experiments,[0],[0]
Weak Learning Condition Check The weak learning condition (Definition 4.1) inspired by learning theory is checked in Figure 4.,5. Experiments,[0],[0]
"The required better than random guessing edge γt is depicted in Figure 4a, it is always greater than 0 and our weak learning condition is thus nonvacuous.",5. Experiments,[0],[0]
"In Figure 4b, the representations we learned using BoostResNet is increasingly better (for this classification task) as the depth increases.
",5. Experiments,[0],[0]
"Comparison of BoostResNet, e2eBP and AdaBoost Besides e2eBP, we also experiment with standard boosting (AdaBoost.",5. Experiments,[0],[0]
"MM (Mukherjee & Schapire, 2013)), as another baseline, of convolutional modules.",5. Experiments,[0],[0]
"In this experiment, each weak learner is a residual block of the ResNet,
paired with a classification layer.",5. Experiments,[0],[0]
We do 25 rounds of AdaBoost.,5. Experiments,[0],[0]
MM and train each weak learner to convergence.,5. Experiments,[0],[0]
"Table 1 and table 2 exhibit a comparison of BoostResNet, e2eBP and AdaBoost performance on SVHN and CIFAR10 dataset respectively.
",5. Experiments,[0],[0]
"On SVHN dataset, the advantage of BoostResNet over e2eBP is obvious.",5. Experiments,[0],[0]
"Using 3 × 108 number of gradient updates, BoostResNet achieves 93.8% test accuracy whereas e2eBP obtains a test accuracy of 83%.",5. Experiments,[0],[0]
The training and test accuracies of SVHN are listed in Table 1.,5. Experiments,[0],[0]
"BoostResNet training allows the model to train much faster than end-to-end training, and still achieves the same test accuracy when refined with e2eBP.",5. Experiments,[0],[0]
"To list the hyperparameters we use in our BoostResNet training after searching over candidate hyperparamters, we optimize learning rate to be 0.004 with a 9 × 10−5 learning rate decay.",5. Experiments,[0],[0]
The gamma threshold is optimized to be 0.001 and the initial gamma value on SVHN is 0.75.,5. Experiments,[0],[0]
"On CIFAR-10 dataset, the main advantage of BoostResNet over e2eBP is the speed of training.",5. Experiments,[0],[0]
BoostResNet refined with e2eBP obtains comparable results with e2eBP.,5. Experiments,[0],[0]
This is because we are using a suboptimal architecture of ResNet which overfits the CIFAR-10 dataset.,5. Experiments,[0],[0]
"AdaBoost, on the other hand, is known to be resistant to overfitting.",5. Experiments,[0],[0]
"In BoostResNet training, we optimize learning rate to be 0.014 with a 3.46 × 10−5 learning rate decay.",5. Experiments,[0],[0]
The gamma threshold is optimized to be 0.007 and the initial gamma value on CIFAR-10 is 0.93.,5. Experiments,[0],[0]
"We find that a standard ResNet, to its credit, is quite robust to hyperparameters, namely learning rate and learning rate decay, provided that we use an optimization procedure that automatically modulates these values.",5. Experiments,[0],[0]
Our proposed BoostResNet algorithm achieves exponentially decaying (with the depth T ) training error under the weak learning condition.,6. Conclusions and Future Works,[0],[0]
BoostResNet is much more computationally efficient compared to end-to-end backpropagation in deep ResNet.,6. Conclusions and Future Works,[0],[0]
"More importantly, the memory required by BoostResNet is trivial compared to end-toend back-propagation.",6. Conclusions and Future Works,[0],[0]
It is particularly beneficial given the limited GPU memory and large network depth.,6. Conclusions and Future Works,[0],[0]
"Our learn-
ing framework is natural for non-differentiable data.",6. Conclusions and Future Works,[0],[0]
"For instance, our learning framework is amenable to take weak learning oracles using tensor decomposition techniques.",6. Conclusions and Future Works,[0],[0]
"Tensor decomposition, a spectral learning framework with theoretical guarantees, is applied to learning one layer MLP in (Janzamin et al., 2015).",6. Conclusions and Future Works,[0],[0]
We plan to extend our learning framework to non-differentiable data using general weak learning oracles.,6. Conclusions and Future Works,[0],[0]
A.1.,A. Related Works,[0],[0]
"Loss function and architecture selection
In neural network optimization, there are many commonly-used loss functions and criteria, e.g., mean squared error, negative log likelihood, margin criterion, etc.",A. Related Works,[0],[0]
"There are extensive works (Girshick, 2015; Rubinstein & Kroese, 2013; Tygert et al., 2015) on selecting or modifying loss functions to prevent empirical difficulties such as exploding/vanishing gradients or slow learning (Balduzzi et al., 2017).",A. Related Works,[0],[0]
"However, there are no rigorous principles for selecting a loss function in general.",A. Related Works,[0],[0]
"Other works consider variations of the multilayer perceptron (MLP) or convolutional neural network (CNN) by adding identity skip connections (He et al., 2016), allowing information to bypass particular layers.",A. Related Works,[0],[0]
"However, no theoretical guarantees on the training error are provided despite breakthrough empirical successes.",A. Related Works,[0],[0]
"Hardt et al. (Hardt & Ma, 2016) have shown the advantage of identity loops in linear neural networks with theoretical justifications; however the linear setting is unrealistic in practice.
",A. Related Works,[0],[0]
A.2.,A. Related Works,[0],[0]
"Learning algorithm design
There have been extensive works on improving BP (LeCun et al., 1989).",A. Related Works,[0],[0]
"For instance, momentum (Qian, 1999), Nesterov accelerated gradient (Nesterov, 1983), Adagrad (Duchi et al., 2011) and its extension Adadelta (Zeiler, 2012).",A. Related Works,[0],[0]
"Most recently, Adaptive Moment Estimation (Adam) (Kingma & Ba, 2014), a combination of momentum and Adagrad, has received substantial success in practice.",A. Related Works,[0],[0]
"All these methods are modifications of stochastic gradient descent (SGD), but our method only requires an arbitrary oracle, which does not necessarily need to be an SGD solver, that solves a relatively simple shallow neural network.",A. Related Works,[0],[0]
Proof.,B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"In our algorithm, the input of the next module is the output of the current module
gt+1(x) = ft(gt(x))",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"+ gt(x), (8)
we thus obtain that each weak learning module is
ht(x) =",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
αt+1w ⊤ t+1(ft(gt(x)),B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"+ gt(x))− αtw⊤t gt(x) (9)
= αt+1w ⊤ t+1gt+1(x)− αtw⊤t gt(x), (10)
and similarly
ht+1 = αt+2w ⊤ t+2gt+2(x)− αt+1w⊤t+1gt+1(x).",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"(11)
Therefore the sum over ht(x) and ht+1(x) is
ht(x) + ht+1(x) =",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
αt+2w ⊤ t+2gt+2(x),B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"− αtw⊤t gt(x) (12)
",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"And we further see that the weighted summation over all ht(x) is a telescoping sum (note that g0(x) = 0):
T∑
t=0
ht(x) =",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
αT+1w ⊤ T+1gT+1(x),B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
− α0w⊤0 g0(x) = αT+1w⊤T+1gT+1(x).,B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
(13),B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
Proof.,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
We will use a 0-1 loss to measure the training error.,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"In our analysis, the 0-1 loss is bounded by exponential loss.
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"The training error is therefore bounded by
Pr i∼D1
(p(αT+1w ⊤ T+1gT+1(xi))",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"6= yi) (14)
=
m∑
i=1
D1(i)1{σ̃(αT+1w⊤T+1gT+1(xi))",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
6=,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"yi} (15)
=
m∑
i=1
D1(i)1
{ σ̃ ( T∑
t=0
ht(xi) )",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"6= yi } (16)
≤",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
D1(i)",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"exp { −yi T∑
t=0
ht(xi)
} (17)
=
m∑
i=1
DT+1(i)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"T∏
t=0
Zt (18)
=
T∏
t=0
Zt (19)
where Zt = m∑ i=1 Dt(i) exp (−yiht(xi)).
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"We choose αt+1 to minimize Zt.
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"∂Zt ∂αt+1
=",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"− m∑
i=1
Dt(i)yiot+1 exp (−yiht(xi)) (20)
= −Zt m∑
i=1
Dt+1(i)yiot+1(i) = 0 (21)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
Furthermore each learning module is bounded as we see in the following analysis.,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"We obtain
Zt =
m∑
i=1
Dt(i)e −yiht(xi) (22)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"=
m∑
i=1
Dt(i)e −αt+1yiot+1(xi)+αtyiot(xi) (23)
≤",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)e −αt+1yiot+1(xi)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)e αtyiot(xi) (24)
=",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)e −αt+1
1+yiot+1(xi) 2 +αt+1 1−yiot+1(xi) 2
m∑
i=1
Dt(i)e αt
1+yiot(xi) 2 −αt 1−yiot(xi)
2 (25)
≤",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)
( 1 + yiot+1(xi)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"2 e−αt+1 + 1− yiot+1(xi) 2
eαt+1 ) ·",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)
( 1 + yiot(xi)
2 eαt + 1− yiot(xi) 2
e−αt )
(26)
=
m∑
i=1
Dt(i)
( 1 + yiot+1(xi)
2 e−αt+1 + 1− yiot+1(xi) 2
eαt+1 ) eαt + e−αt
2 (27)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"=
m∑
i=1
Dt(i)
( e−αt+1 + eαt+1
2 +",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
e−αt+1,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"− eαt+1 2 yiot+1(xi)
)",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"eαt + e−αt
2 (28)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"=
( e−αt+1 + eαt+1
2 + e−αt+1",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"− eαt+1 2 γ̃t
) eαt + e−αt
2 (29)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
Equation (24) is due to the non-positive correlation between exp(−yot+1(x)) and exp(yot(x)).,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Jensen’s inequality in Equation (26) holds only when |yiot+1(xi)| ≤ 1 which is satisfied by the definition of the weak learning module.
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
The algorithm chooses αt+1 to minimize Zt.,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"We achieve an upper bound on Zt,
√ 1−γ̃2t
1−γ̃2t−1 by minimizing the bound in
Equation (29)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Zt|αt+1=argminZt ≤ Zt|αt+1= 12 ln( 1+γ̃t1−γ̃t ) (30)
≤ ( e−αt+1 + eαt+1
2 + e−αt+1",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"− eαt+1 2 γ̃t
) eαt + e−αt
2
∣∣∣∣ αt+1= 1 2 ln( 1+γ̃t 1−γ̃t ) (31)
= √ 1− γ̃2t 1− γ̃2t−1 = √ 1− γ2t (32)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Therefore over the T modules, the training error is upper bounded as follows
Pr i∼D
(p(αT+1w ⊤ T+1gT+1(xi)))",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"6= yi) ≤
T∏
t=0
√ 1− γ2t ≤",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"T∏
t=0
√ 1− γ2 = exp ( −1 2 Tγ2 ) (33)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Overall, Algorithm 1 leads us to consistent learning of ResNet.",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Rademacher complexity technique is powerful for measuring the complexity of H any family of functions h : X → R, based on easiness of fitting any dataset using classifiers inH (whereX is any space).",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Let S =< x1, . . .",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
", xm > be a sample of m points in X .",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"The empirical Rademacher complexity ofH with respect to S is defined to be
RS(H) def=",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Eσ [ sup h∈H 1 m m∑
i=1
σih(xi)
]",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"(34)
where σ is the Rademacher variable.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
The Rademacher complexity on m data points drawn from distribution D is defined by Rm(H) =,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
ES∼D,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
[RS(H)] .,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
(35) Proposition D.1.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"(Theorem 1 (Cortes et al., 2014))",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
Let H be a hypothesis set admitting a decomposition H = ∪li=1Hi for some l > 1.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
Hi are distinct hypothesis sets.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Let S be a random sequence of m points chosen independently from X according to some distribution D. For θ > 0 and any H =∑Tt=0 ht, with probability at least 1− δ,
Pr D (yH(x) ≤ 0) ≤",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Pr S (yH(x) ≤ θ) + 4 θ
T∑
t=0
Rm(Hkt)",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"+ 2
θ
√ log l
m
+
√
⌈ 4 θ2 log
( θ2m
log l
) ⌉",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"log l
m +
log 2 δ
2m (36)
for all ht ∈ Hkt .",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
Lemma D.2.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Let h̃ = w̃⊤f̃ , where w̃ ∈ Rn, f̃ ∈ Rn.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Let H̃ and F̃ be two hypothesis sets, and h̃ ∈ H̃ , f̃j ∈ F̃ , ∀j ∈",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
[n].,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"The Rademacher complexity of H̃ and F̃ with respect to m points from D are related as follows
Rm(H̃) = ‖w̃‖1Rm(F̃).",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"(37)
D.1.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"ResNet Module Hypothesis Space
Let n be the number of channels in ResNet, i.e., the number of input or output neurons in a module ft(gt(x)).",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"We have proved that ResNet is equivalent as
F (x) =",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"w⊤ T∑
t=0
f(gt(x))",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"(38)
We define the family of functions that each neuron ft,j , ∀j ∈",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"[n] belong to as
Ft = {x→ ut−1,j(σ ◦ ft−1)(x) : ut−1,j ∈ Rn, ‖ut−1,j‖1 ≤",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Λt,t−1, ft−1,i ∈ Ft−1} (39)
where ut−1,j denotes the vector of weights for connections from unit j to a lower layer t−1, σ◦ ft−1 denotes element-wise nonlinear transformation on ft−1.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
The output layer of each module is connected to the output layer of previous module.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"We consider 1-layer modules for convenience of analysis.
",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Therefore in ResNet with probability at least 1− δ,
Pr D (yF (x) ≤ 0)",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
≤,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Pr S (yF (x) ≤ θ) + 4 θ
T∑
t=0
‖w‖1Rm(Ft) + 2
θ
√ logT
m
+
√
⌈ 4 θ2 log
( θ2m
logT
) ⌉ logT
m +
log 2 δ
2m (40)
for all ft ∈ Ft.
Define the maximum infinity norm over samples as r∞ def = ES∼D",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"[ maxi∈[m]‖xi‖∞ ] and the product of l1 norm bound on weights as Λt def = ∏t
t′=0 2Λt′,t′−1.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"According to lemma 2 of (Cortes et al., 2016), the empirical Rademacher complexity is bounded as a function of r∞, Λt and n:
Rm(Ft) ≤",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"r∞Λt √ log(2n)
2m (41)
",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Overall, with probability at least 1− δ,
Pr D (yF (x) ≤ 0) ≤",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Pr S (yF (x) ≤ θ) +
4‖w‖1r∞ √ log(2n) 2m
θ
T∑
t=0
Λt
+ 2
θ
√ logT
m +
√
⌈ 4 θ2 log
( θ2m
logT
) ⌉ logT
m +
log 2 δ
2m (42)
for all ft ∈ Ft.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
Theorem E.1.,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
[ Generalization error bound ],E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Given algorithm 1, the fraction of training examples with margin at most θ is at most (1 + 21√ γ̃T+1 −1 )",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
θ 2 exp(− 12γ2T ).,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"And the generalization error PrD(yF (x) ≤ 0) satisfies
Pr D (yF (x) ≤ 0) ≤",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(1 + 21
γ̃T+1 − 1)
θ 2 exp(−1
2 γ2T )
+ 4C0r∞
θ
√ log(2n)
2m
T∑
t=0
Λt + 2
θ
√ logT
m + β(θ,m, T, δ) (43)
with probability at least 1− δ for β(θ,m, T, δ) def= √⌈
4 θ2
log (
θ2m log T )⌉ log T m + log 2 δ 2m .
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Now the proof for Theorem E is the following.
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
Proof.,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"The fraction of examples in sample set S being smaller than θ is bounded
Pr S (yF (x) ≤ θ) ≤ 1 m
m∑
i=1
1{yiF (xi) ≤ θ} (44)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"= 1
m
m∑
i=1
1{yi T∑
t=0
ht(xi) ≤ θαT+1} (45)
≤ 1 m
m∑
i=1
exp(−yi T∑
t=0
ht(xi)",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"+ θαT+1) (46)
= exp(θαT+1) 1
m
m∑
i=1
exp(−yi T∑
t=0
ht(xi))",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(47)
= exp(θαT+1)
T∏
t=0
Zt (48)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
To bound exp(θαT+1) = √ (1+γ̃T+11−γ̃T+1 ),E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"θ , we first bound γ̃T+1: We know that ∏T
t′=t+1(1− γ2t′)γ2t ≤ (1− γ2)T−tγ2 for all ∀γt ≥ γ2 + ǫ",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
if γ2 ≥ 1−ǫ2 .,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Therefore ∀ γt ≥ γ2 + ǫ and γ2 ≥ 1−ǫ2
γ̃2T+1 =",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(1− γ2T )γ̃2T + γ2T (49)
=
T∑
t=1
T∏
t′=t+1
(1 − γ2t′)γ2t + T∏
t=1
(1− γ2t )γ̃21",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(50)
≤ T∑
t=1
(1− γ2)T−tγ2 + (1− γ2)T γ̃21",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(51)
=
T−1∑
t=0
(1− γ2)tγ2 + (1− γ2)T γ̃21 (52)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"= 1− (1− γ2)T + (1− γ2)T γ̃21 (53) = 1− (1− γ̃21)(1− γ2)T (54)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Therefore
Pr S (yF (x) ≤ θ) ≤ exp(θαT+1)
T∏
t=1
Zt (55)
= ( 1 + γ̃T+1 1− γ̃T+1 ) θ 2
T∏
t=1
Zt (56)
= ( 1 + γ̃T+1 1− γ̃T+1 ) θ 2
T∏
t=1
√ 1− γ2t (57)
= (1 + 2
1 γ̃T+1
− 1) θ 2 exp(−1 2 γ2T ) (58)
≤ (1 + 21√ 1−(1−γ̃21)(1−γ2)T",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"− 1) θ 2 exp(−1 2 γ2T ) (59)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"As T →∞, PrS(yF (x) ≤ θ) ≤ 0",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"as exp(− 12γ2T ) decays faster than (1 + 21√ 1−(1−γ̃2
1 )(1−γ2)T
−1 )",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
θ 2 .,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Recall that the weak module classifier is defined as
ht(x) = αt+1ot+1(x) − αtot(x) ∈ RC , (60)
where ot(x) ∈ ∆C−1.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"The weak learning condition for multi-class classification is different from the binary classification stated in the previous section, although minimal demands placed on the weak module classifier require prediction better than random on any distribution over the training set intuitively.
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
We now define the weak learning condition.,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"It is again inspired by the slightly better than random idea, but requires a more sophisticated analysis in the multi-class setting.
F.1.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Cost Matrix
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"In order to characterize the training error, we introduce the cost matrix C ∈ Rm×C where each row denote the cost incurred by classifying that example into one of the C categories.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"We will bound the training error using exponential loss, and under the exponential loss function defined as in Definition G.1, the optimal cost function used for best possible training error is therefore determined.
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
Lemma F.1.,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"The optimal cost function under the exponential loss is
Ct(i, l) = { exp (st(xi, l)− st(xi, yi))",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
if l 6= yi,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"− ∑
l′ 6=yi exp (st(xi, l
′)− st(xi, yi))",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"if l = yi (61)
where st(x) = t∑
τ=1 hτ (x).
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
F.2.,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Weak Learning Condition
Definition F.2.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Let γ̃t+1 = −
m∑
i=1
<Ct(i,:),ot+1(xi)>
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"m∑
i=1
∑
l 6=yi
Ct(i,l) and γ̃t =
− m∑
i=1
<Ct−1(i,:),ot(xi)>
m∑
i=1
∑
l 6=yi
Ct−1(i,l) .",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"A multi-class weak module classifier
ht(x)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"= αt+1ot+1(x) − αtot(x) satisfies the γ-weak learning condition if γ̃ 2 t+1−γ̃ 2 t
1−γ̃2t ≥ γ2 > 0, and Cov(< Ct(i, :
), ot+1(xi)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
">,< Ct(i, :), ot+1(xi) >)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"≥ 0.
We propose a novel learning algorithm using the optimal edge-over-random cost function for training ResNet under multiclass classification task as in Algorithm 3.
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
Theorem F.3.,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"The training error of a T -module ResNet using Algorithm 3and 4 decays exponentially with the depth of the ResNet T ,
C",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"− 1 m
m∑
i=1
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
Lexpη (sT (xi)) ≤,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"(C − 1)e− 1 2Tγ 2
(62)
if the weak module classifier ht(x) satisfies the γ-weak learning condition ∀t ∈",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"[T ].
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"The exponential loss function defined as in Definition G.1
Algorithm 3 BoostResNet: telescoping sum boosting for multi-class classification Input:",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Given (x1, y1), . . .",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"(xm, ym) where yi ∈ Y = {1, . . .",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
", C} and a threshold γ Output: {ft(·),∀t} and WT+1 ⊲",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Discard wt+1, ∀t 6=",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"T
1: Initialize t← 0, γ̃0",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"← 1, α0 ← 0, o0 ← 0 ∈ RC , s0(xi, l) = 0, ∀i ∈",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"[m], l ∈ Y 2: Initialize cost function C0(i, l)← { 1",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
if l 6= yi 1− C,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"if l = yi 3: while γt > γ do 4: ft(·), αt+1,Wt+1, ot+1(x)← Algorithm 4(gt(x),Ct, ot(x), αt) 5: Compute γt ← √
γ̃2t+1−γ̃2t 1−γ̃2t
⊲ where γ̃t+1 ← −
m∑
i=1 Ct(i,:)·ot+1(xi)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"m∑
i=1
∑
l 6=yi
Ct(i,l)
6: Update st+1(xi, l)←",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"st(xi, l) +",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"ht(xi, l) ⊲",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"where ht(xi, l) = αt+1ot+1(xi, l)− αtot(xi, l) 7: Update cost function Ct+1(i, l)← { est+1(xi,l)−st+1(xi,yi)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
if l 6= yi,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"− ∑
l′ 6=yi est+1(xi,l ′)−st+1(xi,yi) if l = yi
8: t← t+ 1 9: end while
10: T ← t− 1
Algorithm 4 BoostResNet:",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"oracle implementation for training a ResNet module (multi-class)
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Input: gt(x),st,ot(x) and αt Output: ft(·), αt+1, Wt+1 and ot+1(x)
1: (ft, αt+1,Wt+1)← arg min (f,α,V ) m∑ i=1",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
∑,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
l 6=yi eαV,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"⊤[f(gt(xi),l)−f(gt(xi),yi)+gt(xi,l)−gt(xi,yi)]",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
2: ot+1(x)←W⊤t+1,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
[ft(gt(x)),F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"+ gt(x)]
F.3.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Oracle Implementation
We implement an oracle to minimize Zt def = m∑ i=1 ∑",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"l 6=yi est(xi,l)−st(xi,yi)eht(xi,l)−ht(xi,yi) given current state st and hypothesis module ot(x).",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Therefore minimizing Zt is equivalent to the following.
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"min (f,α,V )
m∑
i=1
∑
l 6=yi
est(xi,l)−st(xi,yi)e−αt(ot(xi,l)−ot(xi,yi))eαV",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"⊤[f(gt(xi),l)−f(gt(xi),yi)+gt(xi,l)−gt(xi,yi)] (63)
≡ min (f,α,V )
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"m∑
i=1
∑
l 6=yi
eαV",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"⊤[f(gt(xi),l)−f(gt(xi),yi)+gt(xi,l)−gt(xi,yi)]",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"(64)
≡ min α,f,v
m∑
i=1
e−αv ⊤[f(xi,yi)+gt(xi,yi)]
∑
l 6=yi
eαv ⊤[f(xi,l)+gt(xi,l)] (65)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
Proof.,G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"To characterize the training error, we use the exponential loss function
Definition G.1.",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Define loss function for a multiclass hypothesis H(xi) on a sample (xi, yi) as
Lexpη (H(xi), yi) = ∑
l 6=yi
exp ((H(xi, l)−H(xi, yi))) .",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"(66)
Define the accumulated weak learner st(xi, l) = t∑
t′=0 ht′(xi, l) and the loss Zt = m∑ i=1",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
∑,G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"l 6=yi exp(st(xi, l)",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"−
st(xi, yi))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"exp(ht(xi, l)− ht(xi, yi)).
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Recall that st(xi, l) = t∑
t′=0
ht′(xi, l) =",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"αt+1W ⊤ t+1gt+1(xi), the loss for a T -module multiclass ResNet is thus
Pr i∼D1
(p(αT+1W ⊤ T+1gT+1(xi))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"6= yi) ≤
1
m
m∑
i=1
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Lexpη (sT (xi)) (67)
≤ 1 m
m∑
i=1
∑
l 6=yi
exp (η(sT (xi, l)− sT (xi, yi)))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"(68)
≤ 1 m ZT (69) =",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"T∏
t=0
Zt Zt−1
(70)
Note that Z0 = 1 m as the initial accumulated weak learners s0(xi, l) = 0.
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"The loss fraction between module t and t− 1, Zt Zt−1 , is related to Zt − Zt−1 as ZtZt−1 = Zt−Zt−1 Zt−1 + 1.
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"The Zt is bounded
Zt =
m∑
i=1
∑
l 6=yi
exp(st(xi, l)− st(i, yi) + ht(xi, l)− ht(xi, yi))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"(71)
≤",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"m∑
i=1
∑
l 6=yi
est(xi,l)−st(xi,yi)eαt+1ot+1(xi,l)−αt+1ot+1(xi,yi)",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"m∑
i=1
∑
l 6=yi
est(xi,l)−st(xi,yi)e−αtot(xi,l)+αtot(xi,yi) (72)
≤",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"m∑
i=1
∑
l 6=yi
est(xi,l)−st(xi,yi) ( e−αt+1 + eαt+1
2 + e−αt+1",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"− eαt+1 2
(ot+1(xi, yi)− ot+1(xi, l)) )
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"m∑
i=1
∑
l 6=yi
est−1(xi,l)−st−1(xi,yi) ( eαt + e−αt
2
) (73)
=(",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"e−αt+1 + eαt+1 − 2
2 Zt−1 + eαt+1 − e−αt+1 2
m∑
i=1
<",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Ct(xi, :), ot+1(xi, :) >)
( eαt + e−αt
2
)
≤(e",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"−αt+1 + eαt+1 − 2
2 Zt−1 + eαt+1 − e−αt+1 2 m∑
i=1
<",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Ct(xi, :), Uγ̃t(xi, :) >)
( eαt + e−αt
2
) (74)
=( e−αt+1 + eαt+1 − 2
2 Zt−1 + eαt+1 − e−αt+1 2
(−γ̃t)Zt−1) ( eαt + e−αt
2
) (75)
Therefore Zt Zt−1 ≤",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
( e−αt+1 + eαt+1 2 + e−αt+1,G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"− eαt+1 2 γ̃t )( eαt + e−αt 2 ) (76)
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
The algorithm chooses αt+1 to minimize Zt.,G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"We achieve an upper bound on Zt,
√ 1−γ̃2t
1−γ̃t−12
by minimizing the bound in Equation (76)
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Zt|αt+1=argminZt ≤ Zt|αt+1= 12 ln( 1+γ̃t1−γ̃t ) (77)
≤ ( e−αt+1 + eαt+1
2 + e−αt+1",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"− eαt+1 2 γ̃t
) eαt + e−αt
2
∣∣∣∣ αt+1= 1 2 ln( 1+γ̃t 1−γ̃t ) (78)
= √ 1− γ̃2t 1− γ̃2t−1 = √ 1− γ2t (79)
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Therefore over the T modules, the training error is upper bounded as follows
Pr i∼D
(p(αT+1w ⊤ T+1gT+1(xi)))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"6= yi) ≤
T∏
t=0
√ 1− γ2t ≤",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"T∏
t=0
√ 1− γ2 = exp ( −1 2 Tγ2 ) (80)
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Overall, Algorithm 3 and 4 leads us to consistent learning of ResNet.",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
H.1.,H. Experiments,[0],[0]
"Training error degradation of e2eBP on ResNet
We investigate e2eBP training performance on various depth ResNet.",H. Experiments,[0],[0]
"Surprisingly, we observe a training error degradation for e2eBP although the ResNet’s identity loop is supposed to alleviate this problem.",H. Experiments,[0],[0]
"Despite the presence of identity loops, the e2eBP eventually is susceptible to spurious local optima.",H. Experiments,[0],[0]
"This phenomenon is explored further in Figures 5a and 5b, which respectively show how training and test accuracies vary throughout the fitting process.",H. Experiments,[0],[0]
"Our proposed sequential training procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth increases.",H. Experiments,[0],[0]
We prove a multi-channel telescoping sum boosting theory for the ResNet architectures which simultaneously creates a new technique for boosting over features (in contrast to labels) and provides a new algorithm for ResNet-style architectures.,abstractText,[0],[0]
"Our proposed training algorithm, BoostResNet, is particularly suitable in nondifferentiable architectures.",abstractText,[0],[0]
Our method only requires the relatively inexpensive sequential training of T “shallow ResNets”.,abstractText,[0],[0]
We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.,abstractText,[0],[0]
"In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.",abstractText,[0],[0]
A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l1 norm bounded weights.,abstractText,[0],[0]
Learning Deep ResNet Blocks Sequentially using Boosting Theory,title,[0],[0]
"Determinantal Point Processes (DPPs) are a family of probabilistic models that arose from the study of quantum mechanics (Macchi, 1975) and random matrix theory (Dyson, 1962).",1. Introduction,[0],[0]
"Following the seminal work of Kulesza and Taskar (Kulesza & Taskar, 2012), discrete DPPs have found numerous applications in machine learning, including in document and timeline summarization (Lin & Bilmes, 2012; Yao et al., 2016), image search (Kulesza & Taskar, 2011; Affandi et al., 2014) and segmentation (Lee et al., 2016), audio signal processing (Xu & Ou, 2016), bioinformatics (Batmanghelich et al., 2014) and neuroscience (Snoek et al., 2013).",1. Introduction,[0],[0]
"What makes such models appealing is that they exhibit repulsive behavior and lend themselves naturally to tasks where returning a diverse set of objects is important.
",1. Introduction,[0],[0]
"One way to define a DPP is through an N × N symmetric positive semidefinite matrix K, called a kernel, whose
1Department of Mathematics, MIT, USA.",1. Introduction,[0],[0]
"Correspondence to: John Urschel <urschel@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"eigenvalues are bounded in the range [0, 1].",1. Introduction,[0],[0]
"Then the DPP associated with K, which we denote by DPP(K), is the distribution on Y ⊆",1. Introduction,[0],[0]
"[N ] = {1, . . .",1. Introduction,[0],[0]
", N} that satisfies, for any J ⊆",1. Introduction,[0],[0]
"[N ],
P[J ⊆ Y ] = det(KJ),
where KJ is the principal submatrix of K indexed by the set J .",1. Introduction,[0],[0]
"The graph induced by K is the graph G = ([N ], E) on the vertex set [N ] that connects i, j ∈",1. Introduction,[0],[0]
"[N ] if and only if Ki,j 6= 0.
",1. Introduction,[0],[0]
"There are fast algorithms for sampling (or approximately sampling) from DPP(K) (Deshpande & Rademacher, 2010; Rebeschini & Karbasi, 2015; Li et al., 2016b;a).",1. Introduction,[0],[0]
Marginalizing the distribution on a subset I ⊆,1. Introduction,[0],[0]
"[N ] and conditioning on the event that J ⊆ Y both result in new DPPs and closed form expressions for their kernels are known (Borodin & Rains, 2005).
",1. Introduction,[0],[0]
There has been much less work on the problem of learning the parameters of a DPP.,1. Introduction,[0],[0]
"A variety of heuristics have been proposed, including Expectation-Maximization (Gillenwater et al., 2014), MCMC (Affandi et al., 2014), and fixed point algorithms (Mariet & Sra, 2015).",1. Introduction,[0],[0]
"All of these attempt to solve a nonconvex optimization problem, and no guarantees on their statistical performance are known.",1. Introduction,[0],[0]
"Recently, Brunel et al. (Brunel et al., 2017) studied the rate of estimation achieved by the maximum likelihood estimator, but the question of efficient computation remains open.
",1. Introduction,[0],[0]
"Apart from positive results on sampling, marginalization and conditioning, most provable results about DPPs are actually negative.",1. Introduction,[0],[0]
"It is conjectured that the maximum likelihood estimator is NP-hard to compute (Kulesza, 2012).",1. Introduction,[0],[0]
"Actually, approximating the mode of size k of a DPP to within a ck factor is known to be NP-hard for some c > 1 (Çivril & Magdon-Ismail, 2009; Summa et al., 2015).",1. Introduction,[0],[0]
"The best known algorithms currently obtain a ek+o(k) approximation factor (Nikolov, 2015; Nikolov & Singh, 2016).
",1. Introduction,[0],[0]
"In this work, we bypass the difficulties associated with maximum likelihood estimation by using the method of moments to achieve optimal sample complexity.",1. Introduction,[0],[0]
"We introduce a parameter `, which we call the cycle sparsity of the graph induced by the kernelK, which governs the number of moments that need to be considered and, thus, the sample complexity.",1. Introduction,[0],[0]
"Moreover, we use a refined version of Horton’s al-
gorithm (Horton, 1987; Amaldi et al., 2010) to implement the method of moments in polynomial time.
",1. Introduction,[0],[0]
The cycle sparsity of a graph is the smallest integer ` so that the cycles of length at most ` yield a basis for the cycle space of the graph.,1. Introduction,[0],[0]
"Even though there are in general exponentially many cycles in a graph to consider, Horton’s algorithm constructs a minimum weight cycle basis and, in doing so, also reveals the parameter ` together with a collection of at most ` induced cycles spanning the cycle space.
",1. Introduction,[0],[0]
We use such cycles in order to construct our method of moments estimator.,1. Introduction,[0],[0]
"For any fixed ` ≥ 2, our overall algorithm has sample complexity
n =",1. Introduction,[0],[0]
"O ((C α )2` + logN α2ε2 ) for some constant C > 1 and runs in time polynomial in n and N , and learns the parameters up to an additive ε with high probability.",1. Introduction,[0],[0]
The (C/α)2` term corresponds to the number of samples needed to recover the signs of the entries in K. We complement this result with a minimax lower bound (Theorem 2) to show that this sample complexity is in fact near optimal.,1. Introduction,[0],[0]
"In particular, we show that there is an infinite family of graphs with cycle sparsity ` (namely length ` cycles) on which any algorithm requires at least (C ′α)−2` samples to recover the signs of the entries of K for some constant C ′",1. Introduction,[0],[0]
> 1.,1. Introduction,[0],[0]
"Finally, we show experimental results that confirm many quantitative aspects of our theoretical predictions.",1. Introduction,[0],[0]
"Together, our upper bounds, lower bounds, and experiments present a nuanced understanding of which DPPs can be learned provably and efficiently.",1. Introduction,[0],[0]
"Let Y1, . . .",2.1. Model and definitions,[0],[0]
", Yn be n independent copies of Y ∼ DPP(K), for some unknown kernel K",2.1. Model and definitions,[0],[0]
such that 0 K IN .,2.1. Model and definitions,[0],[0]
It is well known that K is identified by DPP(K) only up to flips of the signs of its rows and columns: If K ′ is another symmetric matrix with 0 K ′,2.1. Model and definitions,[0],[0]
"IN , then DPP(K ′)=DPP(K) if and only if K ′ = DKD for some D ∈ DN , where DN denotes the class of all N × N diagonal matrices with only 1 and −1 on their diagonals (Kulesza, 2012, Theorem 4.1).",2.1. Model and definitions,[0],[0]
"We call such a transform a DN -similarity of K.
In view of this equivalence class, we define the following pseudo-distance between kernels K and K ′:
ρ(K,K ′) = inf D∈DN |DKD −K ′|∞ ,
where for any matrix K, |K|∞ = maxi,j∈[N ] |Ki,j | denotes the entrywise sup-norm.
",2.1. Model and definitions,[0],[0]
For any S ⊂,2.1. Model and definitions,[0],[0]
"[N ], we write ∆S = det(KS), where KS denotes the |S| × |S| submatrix of K obtained by keeping rows and colums with indices in S. Note that for 1 ≤",2.1. Model and definitions,[0],[0]
i 6=,2.1. Model and definitions,[0],[0]
"j ≤ N , we have the following relations:
Ki,i = P[i ∈ Y ], ∆{i,j} = P[{i, j} ⊆ Y ],
and |Ki,j | = √ Ki,iKj,j −∆{i,j}.",2.1. Model and definitions,[0],[0]
"Therefore, the principal minors of size one and two of K determine K up to the sign of its off-diagonal entries.",2.1. Model and definitions,[0],[0]
"In fact, for any K, there exists an ` depending only on the graph GK induced by K, such that K can be recovered up to a DN -similarity with only the knowledge of its principal minors of size at most `.",2.1. Model and definitions,[0],[0]
We will show that this ` is exactly the cycle sparsity.,2.1. Model and definitions,[0],[0]
"In this section, we review some of the interplay between graphs and DPPs that plays a key role in the definition of our estimator.
",2.2. DPPs and graphs,[0],[0]
We begin by recalling some standard graph theoretic notions.,2.2. DPPs and graphs,[0],[0]
Let G =,2.2. DPPs and graphs,[0],[0]
(,2.2. DPPs and graphs,[0],[0]
"[N ], E), |E| = m. A cycle C of G is any connected subgraph in which each vertex has even degree.",2.2. DPPs and graphs,[0],[0]
Each cycle C is associated with an incidence vector x ∈ GF (2)m such that xe = 1 if e is an edge in C and xe = 0 otherwise.,2.2. DPPs and graphs,[0],[0]
The cycle space C of G is the subspace of GF (2)m spanned by the incidence vectors of the cycles in G. The dimension νG,2.2. DPPs and graphs,[0],[0]
"of the cycle space is called cyclomatic number, and it is well known that νG",2.2. DPPs and graphs,[0],[0]
":= m−N + κ(G), where κ(G) denotes the number of connected components of G.
Recall that a simple cycle is a graph where every vertex has either degree two or zero and the set of vertices with degree two form a connected set.",2.2. DPPs and graphs,[0],[0]
A cycle basis is a basis of C ⊂ GF (2)m such that every element is a simple cycle.,2.2. DPPs and graphs,[0],[0]
"It is well known that every cycle space has a cycle basis of induced cycles.
",2.2. DPPs and graphs,[0],[0]
Definition 1.,2.2. DPPs and graphs,[0],[0]
"The cycle sparsity of a graph G is the minimal ` for which G admits a cycle basis of induced cycles of length at most `, with the convention that ` = 2 whenever the cycle space is empty.",2.2. DPPs and graphs,[0],[0]
"A corresponding cycle basis is called a shortest maximal cycle basis.
",2.2. DPPs and graphs,[0],[0]
"A shortest maximal cycle basis of the cycle space was also studied for other reasons by (Chickering et al., 1995).",2.2. DPPs and graphs,[0],[0]
"We defer a discussion of computing such a basis to Section 4.
",2.2. DPPs and graphs,[0],[0]
For any subset S ⊆,2.2. DPPs and graphs,[0],[0]
"[N ], denote by GK(S) =",2.2. DPPs and graphs,[0],[0]
"(S,E(S))",2.2. DPPs and graphs,[0],[0]
the subgraph of GK induced by S. A matching of GK(S) is a subset M ⊆ E(S) such that any two distinct edges in M are not adjacent in G(S).,2.2. DPPs and graphs,[0],[0]
The set of vertices incident to some edge in M is denoted by V (M).,2.2. DPPs and graphs,[0],[0]
We denote by M(S) the collection of all matchings of GK(S).,2.2. DPPs and graphs,[0],[0]
"Then, if GK(S) is an induced cycle, we can write the principal
minor ∆S = det(KS) as follows: ∆S = ∑
M∈M(S)
(−1)|M | ∏
{i,j}∈M
K2i,j ∏
i 6∈V",2.2. DPPs and graphs,[0],[0]
"(M)
Ki,i
+ 2× (−1)|S|+1 ∏
{i,j}∈E(S)
Ki,j .",2.2. DPPs and graphs,[0],[0]
"(1)
Others have considered the relationship between the principal minors of K and recovery of DPP(K).",2.2. DPPs and graphs,[0],[0]
"There has been work regarding the symmetric principal minor assignment problem, namely the problem of computing a matrix given an oracle that gives any principal minor in constant time (Rising et al., 2015).
",2.2. DPPs and graphs,[0],[0]
"In our setting, we can approximate the principal minors of K by empirical averages.",2.2. DPPs and graphs,[0],[0]
"However the accuracy of our estimator deteriorates with the size of the principal minor, and we must therefore estimate the smallest possible principal minors in order to achieve optimal sample complexity.",2.2. DPPs and graphs,[0],[0]
"Here, we prove a new result, namely, that the smallest ` such that all the principal minors of K are uniquely determined by those of size at most ` is exactly the cycle sparsity of the graph induced by K.
Proposition 1.",2.2. DPPs and graphs,[0],[0]
Let K ∈,2.2. DPPs and graphs,[0],[0]
"RN×N be a symmetric matrix, GK be the graph induced byK, and ` ≥ 3 be some integer.",2.2. DPPs and graphs,[0],[0]
"The kernelK is completely determined up toDN -similarity by its principal minors of size at most ` if and only if the cycle sparsity of GK is at most `.
Proof.",2.2. DPPs and graphs,[0],[0]
"Note first that all the principal minors of K completely determine K up to a DN -similarity (Rising et al., 2015, Theorem 3.14).",2.2. DPPs and graphs,[0],[0]
"Moreover, recall that principal minors of degree at most 2 determine the diagonal entries of K as well as the magnitude of its off-diagonal entries.",2.2. DPPs and graphs,[0],[0]
"In particular, given these principal minors, one only needs to recover the signs of the off-diagonal entries of K. Let the sign of cycle C in K be the product of the signs of the entries of K corresponding to the edges of C.
Suppose GK has cycle sparsity ` and let (C1, . . .",2.2. DPPs and graphs,[0],[0]
", Cν) be a cycle basis of GK where each Ci, i ∈",2.2. DPPs and graphs,[0],[0]
[ν] is an induced cycle of length at most `.,2.2. DPPs and graphs,[0],[0]
"By (1), the sign of any Ci, i ∈",2.2. DPPs and graphs,[0],[0]
"[ν] is completely determined by the principal minor ∆S , where S is the set of vertices of Ci and is such that |S| ≤",2.2. DPPs and graphs,[0],[0]
`.,2.2. DPPs and graphs,[0],[0]
"Moreover, for i ∈",2.2. DPPs and graphs,[0],[0]
"[ν], let xi ∈ GF",2.2. DPPs and graphs,[0],[0]
(2)m denote the incidence vector of Ci.,2.2. DPPs and graphs,[0],[0]
"By definition, the incidence vector x of any cycle C is given by ∑ i∈I xi for some subset",2.2. DPPs and graphs,[0],[0]
I ⊂,2.2. DPPs and graphs,[0],[0]
[ν].,2.2. DPPs and graphs,[0],[0]
"The sign of C is then given by the product of the signs of Ci, i ∈",2.2. DPPs and graphs,[0],[0]
I and thus by corresponding principal minors.,2.2. DPPs and graphs,[0],[0]
"In particular, the signs of all cycles are determined by the principal minors ∆S with |S| ≤",2.2. DPPs and graphs,[0],[0]
`.,2.2. DPPs and graphs,[0],[0]
"In turn, by Theorem 3.12 in (Rising et al., 2015), the signs of all cycles completely determine K, up to a DN -similarity.
",2.2. DPPs and graphs,[0],[0]
"Next, suppose the cycle sparsity of GK is at least ` + 1,
and let C` be the subspace of GF (2)m spanned by the induced cycles of length at most ` in GK .",2.2. DPPs and graphs,[0],[0]
"Let x1, . . .",2.2. DPPs and graphs,[0],[0]
", xν be a basis of C` made of the incidence column vectors of induced cycles of length at most ` inGK and form the matrix A ∈ GF (2)m×ν by concatenating the xi’s.",2.2. DPPs and graphs,[0],[0]
"Since C` does not span the cycle space of GK , ν < νGK ≤ m.",2.2. DPPs and graphs,[0],[0]
"Hence, the rank of A is less than m, so the null space of A> is non trivial.",2.2. DPPs and graphs,[0],[0]
Let x̄ be the incidence column vector of an induced cycle C̄,2.2. DPPs and graphs,[0],[0]
"that is not in C`, and let h ∈ GL(2)m with A>h = 0, h 6= 0 and x̄>h = 1.",2.2. DPPs and graphs,[0],[0]
These three conditions are compatible because C̄ /∈,2.2. DPPs and graphs,[0],[0]
C`.,2.2. DPPs and graphs,[0],[0]
We are now in a position to define an alternate kernel K ′,2.2. DPPs and graphs,[0],[0]
"as follows: Let K ′i,i = Ki,i and |K ′i,j | = |Ki,j | for all i, j ∈",2.2. DPPs and graphs,[0],[0]
[N ].,2.2. DPPs and graphs,[0],[0]
We define the signs of the off-diagonal entries of K ′,2.2. DPPs and graphs,[0],[0]
"as follows: For all edges e = {i, j}, i 6= j, sgn(K ′e)",2.2. DPPs and graphs,[0],[0]
= sgn(Ke) if he = 0 and sgn(K ′e) =,2.2. DPPs and graphs,[0],[0]
− sgn(Ke) otherwise.,2.2. DPPs and graphs,[0],[0]
We now check that K and K ′ have the same principal minors of size at most ` but differ on a principal minor of size larger than `.,2.2. DPPs and graphs,[0],[0]
"To that end, let x be the incidence vector of a cycle C in C` so that x",2.2. DPPs and graphs,[0],[0]
=,2.2. DPPs and graphs,[0],[0]
Aw for some w ∈ GL(2)ν .,2.2. DPPs and graphs,[0],[0]
"Thus the sign of C in K is given by∏
e : xe=1
Ke = (−1)x >h ∏ e : xe=1 K ′e
= (−1)w >A>h ∏ e : xe=1 K ′e = ∏ e : xe=1 K ′e
because A>h = 0.",2.2. DPPs and graphs,[0],[0]
"Therefore, the sign of any C ∈ C` is the same in K and K ′. Now, let S ⊆",2.2. DPPs and graphs,[0],[0]
[N ] with |S| ≤,2.2. DPPs and graphs,[0],[0]
"`, and let",2.2. DPPs and graphs,[0],[0]
"G = GKS = GK′S be the graph corresponding to KS (or, equivalently, to K ′S).",2.2. DPPs and graphs,[0],[0]
"For any induced cycle C in G, C is also an induced cycle in GK and its length is at most `.",2.2. DPPs and graphs,[0],[0]
"Hence, C ∈ C` and the sign of C is the same in K and K ′. By (Rising et al., 2015, Theorem 3.12), det(KS) = det(K ′S).",2.2. DPPs and graphs,[0],[0]
"Next observe that the sign of C̄ in K is given by∏
e : x̄e=1
Ke = (−1)x̄ >h ∏ e : x̄e=1 K ′e = − ∏ e : xe=1 K ′e.
",2.2. DPPs and graphs,[0],[0]
"Note also that since C̄ is an induced cycle of GK = GK′ , the above quantity is nonzero.",2.2. DPPs and graphs,[0],[0]
"Let S̄ be the set of vertices in C̄. By (1) and the above display, we have det(KS̄) 6=",2.2. DPPs and graphs,[0],[0]
"det(K ′
S̄ )",2.2. DPPs and graphs,[0],[0]
.,2.2. DPPs and graphs,[0],[0]
"Together with (Rising et al., 2015, Theorem
3.14), it yields K 6=",2.2. DPPs and graphs,[0],[0]
DK ′D for all D ∈ DN .,2.2. DPPs and graphs,[0],[0]
Our procedure is based on the previous result and can be summarized as follows.,2.3. Definition of the Estimator,[0],[0]
"We first estimate the diagonal entries (i.e., the principal minors of size one) of K by the method of moments.",2.3. Definition of the Estimator,[0],[0]
"By the same method, we estimate the principal minors of size two ofK, and we deduce estimates of the magnitude of the off-diagonal entries.",2.3. Definition of the Estimator,[0],[0]
"To use these estimates to deduce an estimate Ĝ of GK , we make the following assumption on the kernel K.
Assumption 1.",2.3. Definition of the Estimator,[0],[0]
"Fix α ∈ (0, 1).",2.3. Definition of the Estimator,[0],[0]
"For all 1 ≤ i < j ≤ N , either Ki,j = 0, or |Ki,j | ≥ α.
",2.3. Definition of the Estimator,[0],[0]
"Finally, we find a shortest maximal cycle basis of Ĝ, and we set the signs of our non-zero off-diagonal entry estimates by using estimators of the principal minors induced by the elements of the basis, again obtained by the method of moments.
",2.3. Definition of the Estimator,[0],[0]
For S ⊆,2.3. Definition of the Estimator,[0],[0]
"[N ], set ∆̂S = 1
n n∑ p=1 1S⊆Yp , and define
K̂i,i = ∆̂{i} and B̂i,j = K̂i,iK̂j,j − ∆̂{i,j},
where K̂i,i and B̂i,j are our estimators of Ki,i and K2i,j , respectively.
",2.3. Definition of the Estimator,[0],[0]
Define Ĝ =,2.3. Definition of the Estimator,[0],[0]
"([N ], Ê),",2.3. Definition of the Estimator,[0],[0]
"where, for i 6= j, {i, j} ∈ Ê if and only if B̂i,j ≥ 12α
2.",2.3. Definition of the Estimator,[0],[0]
The graph Ĝ is our estimator of GK .,2.3. Definition of the Estimator,[0],[0]
"Let {Ĉ1, ..., ĈνĜ} be a shortest maximal cycle basis of the cycle space of Ĝ.",2.3. Definition of the Estimator,[0],[0]
Let Ŝi ⊆,2.3. Definition of the Estimator,[0],[0]
"[N ] be the subset of vertices of Ĉi, for 1 ≤ i ≤",2.3. Definition of the Estimator,[0],[0]
νĜ.,2.3. Definition of the Estimator,[0],[0]
"We define
Ĥi = ∆̂Ŝi − ∑
M∈M(Ŝi)
(−1)|M | ∏
{i,j}∈M
B̂i,j ∏
i 6∈V (M)
K̂i,i,
for 1 ≤",2.3. Definition of the Estimator,[0],[0]
i ≤ νĜ.,2.3. Definition of the Estimator,[0],[0]
"In light of (1), for large enough n, this quantity should be close to
Hi = 2× (−1)|Ŝi|+1 ∏
{i,j}∈E(Ŝi)
Ki,j .
",2.3. Definition of the Estimator,[0],[0]
"We note that this definition is only symbolic in nature, and computing Ĥi in this fashion is extremely inefficient.",2.3. Definition of the Estimator,[0],[0]
"Instead, to compute it in practice, we will use the determinant of an auxiliary matrix, computed via a matrix factorization.",2.3. Definition of the Estimator,[0],[0]
"Namely, let us define the matrix K̃ ∈",2.3. Definition of the Estimator,[0],[0]
"RN×N such that K̃i,i = K̂i,i for 1 ≤",2.3. Definition of the Estimator,[0],[0]
"i ≤ N , and K̃i,j = B̂1/2i,j .",2.3. Definition of the Estimator,[0],[0]
"We have
det K̃Ŝi = ∑ M∈M (−1)|M | ∏ {i,j}∈M B̂i,j ∏ i 6∈V (M) K̂i,i
+ 2× (−1)|Ŝi|+1 ∏
{i,j}∈Ê(Ŝi)
B̂ 1/2 i,j ,
so that we may equivalently write
Ĥi = ∆̂Ŝi − det(K̃Ŝi) + 2× (−1) |Ŝi|+1",2.3. Definition of the Estimator,[0],[0]
"∏ {i,j}∈Ê(Ŝi) B̂ 1/2 i,j .
Finally, let m̂ = |Ê|.",2.3. Definition of the Estimator,[0],[0]
"Set the matrix A ∈ GF (2)νĜ×m̂ with i-th row representing Ĉi in GF (2)m, 1 ≤ i ≤ νĜ, b = (b1, . . .",2.3. Definition of the Estimator,[0],[0]
", bνĜ) ∈ GF (2)
νĜ with bi = 12",2.3. Definition of the Estimator,[0],[0]
"[sgn(Ĥi) + 1], 1 ≤ i ≤ νĜ, and let x ∈ GF (2)m be a solution to the linear system",2.3. Definition of the Estimator,[0],[0]
"Ax = b if a solution exists, x = 1m otherwise.
",2.3. Definition of the Estimator,[0],[0]
"We define K̂i,j = 0",2.3. Definition of the Estimator,[0],[0]
"if {i, j} /∈ Ê and K̂i,j = K̂j,i = (2x{i,j} − 1)B̂ 1/2",2.3. Definition of the Estimator,[0],[0]
"i,j for all {i, j} ∈ Ê.",2.3. Definition of the Estimator,[0],[0]
The main result of this subsection is the following lemma which relates the quality of estimation of K in terms of ρ to the quality of estimation of the principal minors ∆S .,2.4. Geometry,[0],[0]
Lemma 1.,2.4. Geometry,[0],[0]
"Let K satisfy Assumption 1, and let ` be the cycle sparsity of GK .",2.4. Geometry,[0],[0]
Let ε > 0.,2.4. Geometry,[0],[0]
If |∆̂S −∆S | ≤ ε for all S ⊆,2.4. Geometry,[0],[0]
[N ] with |S| ≤ 2 and if |∆̂S −∆S | ≤ (α/4)|S| for all,2.4. Geometry,[0],[0]
S ⊆,2.4. Geometry,[0],[0]
[N ] with 3 ≤ |S| ≤,2.4. Geometry,[0],[0]
"`, then
ρ(K̂,K) <",2.4. Geometry,[0],[0]
"4ε/α .
",2.4. Geometry,[0],[0]
Proof.,2.4. Geometry,[0],[0]
"We can bound |B̂i,j −K2i,j |, namely,
B̂i,j ≤",2.4. Geometry,[0],[0]
"(Ki,i + α2/16)(Kj,j + α2/16)− (∆{i,j} − α2/16) ≤",2.4. Geometry,[0],[0]
"K2i,j + α2/4
and
B̂i,j ≥ (Ki,i − α2/16)(Kj,j − α2/16)− (∆{i,j} + α2/16) ≥",2.4. Geometry,[0],[0]
"K2i,j − 3α2/16,
giving |B̂i,j −K2i,j | < α2/4.",2.4. Geometry,[0],[0]
"Thus, we can correctly determine whether Ki,j = 0 or |Ki,j | ≥ α, yielding Ĝ = GK .",2.4. Geometry,[0],[0]
"In particular, the cycle basis Ĉ1, . . .",2.4. Geometry,[0],[0]
", ĈνĜ of Ĝ is a cycle basis of GK .",2.4. Geometry,[0],[0]
Let 1 ≤ i ≤ νĜ.,2.4. Geometry,[0],[0]
Denote by t = (α/4)|Si|.,2.4. Geometry,[0],[0]
We have∣∣∣Ĥi −Hi∣∣∣ ≤,2.4. Geometry,[0],[0]
|∆̂Ŝi −∆Ŝi |+ |M(Ŝi)|maxx∈±1 [ (1 + 4tx)|Ŝi|,2.4. Geometry,[0],[0]
"− 1
] ≤ (α/4)|Ŝi| + |M(Ŝi)|",2.4. Geometry,[0],[0]
"[ (1 + 4t)|Ŝi| − 1
] ≤ (α/4)|Ŝi| + T ( |Ŝi|, ⌊ |Ŝi| 2 ⌋) 4t T (|Ŝi|, |Ŝi|)
≤ (α/4)|Ŝi| + 4t (2 |Ŝi| 2 − 1)(2|Ŝi|",2.4. Geometry,[0],[0]
"− 1)
≤ (α/4)|Ŝi| + t22|Ŝi|
< 2α|Ŝi| ≤ |Hi|,
where, for positive integers p < q, we denote by T (q, p) = ∑p i=1",2.4. Geometry,[0],[0]
( q i ),2.4. Geometry,[0],[0]
.,2.4. Geometry,[0],[0]
"Therefore, we can deter-
mine the sign of the product ∏ {i,j}∈E(Ŝi)Ki,j for every element in the cycle basis and recover the signs of the non-zero off-diagonal entries of Ki,j .",2.4. Geometry,[0],[0]
"Hence,
ρ(K̂,K) = max1≤i,j≤N ∣∣∣|K̂i,j | − |Ki,j |∣∣∣.",2.4. Geometry,[0],[0]
"For i = j,∣∣∣|K̂i,j | − |Ki,j |∣∣∣ = |K̂i,i −Ki,i| ≤ ε.",2.4. Geometry,[0],[0]
"For i 6= j with {i, j} ∈ Ê = E, one can easily show that∣∣∣B̂i,j −K2i,j∣∣∣ ≤ 4ε, yielding
|B̂1/2i,j − |Ki,j || ≤ 4ε∣∣B̂1/2i,j + |Ki,j |∣∣ ≤",2.4. Geometry,[0],[0]
"4ε α ,
which completes the proof.
",2.4. Geometry,[0],[0]
We are now in a position to establish a sufficient sample size to estimate K within distance ε.,2.4. Geometry,[0],[0]
Theorem 1.,2.4. Geometry,[0],[0]
"Let K satisfy Assumption 1, and let ` be the cycle sparsity of GK .",2.4. Geometry,[0],[0]
Let ε > 0.,2.4. Geometry,[0],[0]
"For any A > 0, there exists C > 0",2.4. Geometry,[0],[0]
"such that
n ≥ C",2.4. Geometry,[0],[0]
"( 1 α2ε2 + ` ( 4 α )2`) logN ,
yields ρ(K̂,K) ≤ ε",2.4. Geometry,[0],[0]
"with probability at least 1−N−A.
Proof.",2.4. Geometry,[0],[0]
"Using the previous lemma, and applying a union bound,
P [ ρ(K̂,K) > ε ] ≤ ∑ |S|≤2 P [ |∆̂S −∆S | > αε/4 ]",2.4. Geometry,[0],[0]
"+
∑ 2≤|S|≤` P [ |∆̂S −∆S | > (α/4)|S| ] ≤ 2N2e−nα 2ε2/8 + 2N `+1e−2n(α/4) 2`
, (2)
where we used Hoeffding’s inequality.",2.4. Geometry,[0],[0]
We prove an information-theoretic lower bound that holds already if GK is an `-cycle.,3. Information theoretic lower bound,[0],[0]
"Let D(K‖K ′) and H(K,K ′) denote respectively the Kullback-Leibler divergence and the Hellinger distance between DPP(K) and DPP(K ′).",3. Information theoretic lower bound,[0],[0]
Lemma 2.,3. Information theoretic lower bound,[0],[0]
"For η ∈ {−,+}, letKη be the `×`matrix with elements given by
Ki,j =  1/2 if j = i α if j = i± 1 ηα",3. Information theoretic lower bound,[0],[0]
"if (i, j) ∈ {(1, `), (`, 1)} 0 otherwise .
",3. Information theoretic lower bound,[0],[0]
"Then, for any α ≤ 1/8, it holds
D(K‖K ′) ≤ 4(6α)`, and H(K,K ′) ≤ (8α2)` .
",3. Information theoretic lower bound,[0],[0]
Proof.,3. Information theoretic lower bound,[0],[0]
"It is straightforward to see that
det(K+J )",3. Information theoretic lower bound,[0],[0]
− det(K,3. Information theoretic lower bound,[0],[0]
− J ) =,3. Information theoretic lower bound,[0],[0]
{ 2α` if J =,3. Information theoretic lower bound,[0],[0]
"[`] 0 else .
",3. Information theoretic lower bound,[0],[0]
"If Y is sampled from DPP(Kη), we denote by pη(S) = P[Y = S], for S ⊆",3. Information theoretic lower bound,[0],[0]
[`].,3. Information theoretic lower bound,[0],[0]
It follows from the inclusionexclusion principle that for all S ⊆,3. Information theoretic lower bound,[0],[0]
"[`],
p+(S)− p−(S) =",3. Information theoretic lower bound,[0],[0]
"∑
J⊆[`]\S
(−1)|J|(detK+S∪J",3. Information theoretic lower bound,[0],[0]
"− detK − S∪J)
=",3. Information theoretic lower bound,[0],[0]
"(−1)`−|S|(detK+ − detK−) = ±2α` , (3)
where |J | denotes the cardinality of J .",3. Information theoretic lower bound,[0],[0]
The inclusionexclusion principle also yields that pη(S) = |det(Kη,3. Information theoretic lower bound,[0],[0]
− IS̄)| for all S ⊆,3. Information theoretic lower bound,[0],[0]
"[l], where IS̄ stands for the ` × ` diagonal matrix with ones on its entries (i, i) for i /∈ S, zeros elsewhere.
",3. Information theoretic lower bound,[0],[0]
"Denote by D(K+‖K−) the Kullback Leibler divergence between DPP(K+) and DPP(K−):
D(K+‖K−) = ∑ S⊆[`] p+(S) log ( p+(S) p−(S) )
≤ ∑ S⊆[`] p+(S) p−(S) (p+(S)− p−(S))
≤",3. Information theoretic lower bound,[0],[0]
2α` ∑ S⊆[`] |det(K+ − IS̄)| |det(K−,3. Information theoretic lower bound,[0],[0]
"− IS̄)| , (4)
by (3).",3. Information theoretic lower bound,[0],[0]
"Using the fact that 0 < α ≤ 1/8 and the Gershgorin circle theorem, we conclude that the absolute value of all eigenvalues of Kη− IS̄ are between 1/4 and 3/4.",3. Information theoretic lower bound,[0],[0]
"Thus we obtain from (4) the bound D(K+‖K−) ≤ 4(6α)`.
",3. Information theoretic lower bound,[0],[0]
"Using the same arguments as above, the Hellinger distance H(K+,K−) between DPP(K+) and DPP(K−) satisfies
H(K+,K−) =",3. Information theoretic lower bound,[0],[0]
∑ J⊆,3. Information theoretic lower bound,[0],[0]
[`] ( p+(J)− p−(J)√ p+(J),3. Information theoretic lower bound,[0],[0]
+,3. Information theoretic lower bound,[0],[0]
"√ p−(J) )2
≤ ∑ J⊆[`] 4α2` 2 · 4−` = (8α2)`
which completes the proof.
",3. Information theoretic lower bound,[0],[0]
"The sample complexity lower bound now follows from standard arguments.
",3. Information theoretic lower bound,[0],[0]
Theorem 2.,3. Information theoretic lower bound,[0],[0]
Let 0,3. Information theoretic lower bound,[0],[0]
< ε ≤ α ≤ 1/8 and 3 ≤ ` ≤ N .,3. Information theoretic lower bound,[0],[0]
There exists a constant C > 0,3. Information theoretic lower bound,[0],[0]
"such that if
n ≤ C ( 8` α2` + log(N/`) (6α)` + logN ε2 ) ,
then the following holds: for any estimator K̂ based on n samples, there exists a kernelK that satisfies Assumption 1 and such that the cycle sparsity of GK is ` and for which ρ(K̂,K) ≥ ε with probability at least 1/3.
Proof.",3. Information theoretic lower bound,[0],[0]
Recall the notation of Lemma 2.,3. Information theoretic lower bound,[0],[0]
First consider the N ×N block diagonal matrix K (resp.,3. Information theoretic lower bound,[0],[0]
K ′) where its first block isK+ (resp. K−) and its second block is IN−`.,3. Information theoretic lower bound,[0],[0]
"By a standard argument, the Hellinger distance Hn(K,K ′) between the product measures DPP(K)⊗n and DPP(K ′)⊗n satisfies
1− H 2 n(K,K ′) 2 =",3. Information theoretic lower bound,[0],[0]
"( 1− H 2(K,K ′) 2 )n",3. Information theoretic lower bound,[0],[0]
"≥ (1− α2` 2× 8` )n ,
which yields the first term in the desired lower bound.
",3. Information theoretic lower bound,[0],[0]
"Next, by padding with zeros, we can assume that L = N/` is an integer.",3. Information theoretic lower bound,[0],[0]
Let K(0) be a block diagonal matrix where each block is K+ (using the notation of Lemma 2).,3. Information theoretic lower bound,[0],[0]
"For j = 1, . . .",3. Information theoretic lower bound,[0],[0]
", L, define the N × N block diagonal matrix K(j) as the matrix obtained from K(0) by replacing its jth block with K− (again using the notation of Lemma 2).
",3. Information theoretic lower bound,[0],[0]
"Since DPP(K(j)) is the product measure of L lower dimensional DPPs that are each independent of each other, using Lemma 2 we have D(K(j)‖K(0)) ≤",3. Information theoretic lower bound,[0],[0]
4(6α)`.,3. Information theoretic lower bound,[0],[0]
"Hence, by Fano’s lemma (see, e.g., Corollary 2.6 in (Tsybakov, 2009)), the sample complexity to learn the kernel of a DPP within a distance ε ≤ α is
Ω
( log(N/`)
(6α)` ) which yields the second term.
",3. Information theoretic lower bound,[0],[0]
The third term follows from considering K0 = (1/2)IN and letting Kj be obtained from K0 by adding ε to the jth entry along the diagonal.,3. Information theoretic lower bound,[0],[0]
It is easy to see that D(Kj‖K0) ≤ 8ε2.,3. Information theoretic lower bound,[0],[0]
"Hence, a second application of Fano’s lemma yields that the sample complexity to learn the kernel of a DPP within a distance ε is Ω( logNε2 ).
",3. Information theoretic lower bound,[0],[0]
The third term in the lower bound is the standard parametric term and is unavoidable in order to estimate the magnitude of the coefficients of K.,3. Information theoretic lower bound,[0],[0]
The other terms are more interesting.,3. Information theoretic lower bound,[0],[0]
"They reveal that the cycle sparsity of GK , namely, `, plays a key role in the task of recovering the sign pattern of K.",3. Information theoretic lower bound,[0],[0]
Moreover the theorem shows that the sample complexity of our method of moments estimator is near optimal.,3. Information theoretic lower bound,[0],[0]
We first give an algorithm to compute the estimator K̂ defined in Section 2.,4.1. Horton’s algorithm,[0],[0]
"A well-known algorithm of Horton (Horton, 1987) computes a cycle basis of minimum total length in time O(m3N).",4.1. Horton’s algorithm,[0],[0]
"Subsequently, the running time was improved toO(m2N/ logN) time (Amaldi et al., 2010).",4.1. Horton’s algorithm,[0],[0]
"Also, it is known that a cycle basis of minimum total length is a shortest maximal cycle basis (Chickering et al., 1995).",4.1. Horton’s algorithm,[0],[0]
"Together, these results imply the following.
",4.1. Horton’s algorithm,[0],[0]
Lemma 3.,4.1. Horton’s algorithm,[0],[0]
Let G =,4.1. Horton’s algorithm,[0],[0]
(,4.1. Horton’s algorithm,[0],[0]
"[N ], E), |E| = m. There is an algorithm to compute a shortest maximal cycle basis in O(m2N/ logN) time.
",4.1. Horton’s algorithm,[0],[0]
"In addition, we recall the following standard result regarding the complexity of Gaussian elimination (Golub & Van Loan, 2012).
",4.1. Horton’s algorithm,[0],[0]
"Algorithm 1 Compute Estimator K̂ Input: samples Y1, ..., Yn, parameter α > 0.
",4.1. Horton’s algorithm,[0],[0]
Compute ∆̂S for all |S| ≤ 2.,4.1. Horton’s algorithm,[0],[0]
"Set K̂i,i = ∆̂{i} for 1 ≤",4.1. Horton’s algorithm,[0],[0]
i ≤ N .,4.1. Horton’s algorithm,[0],[0]
"Compute B̂i,j for 1 ≤",4.1. Horton’s algorithm,[0],[0]
i < j ≤ N .,4.1. Horton’s algorithm,[0],[0]
Form K̃ ∈,4.1. Horton’s algorithm,[0],[0]
RN×N and Ĝ,4.1. Horton’s algorithm,[0],[0]
=,4.1. Horton’s algorithm,[0],[0]
"([N ], Ê).",4.1. Horton’s algorithm,[0],[0]
"Compute a shortest maximal cycle basis {v̂1, ..., v̂νĜ}.",4.1. Horton’s algorithm,[0],[0]
Compute ∆̂Ŝi for 1 ≤,4.1. Horton’s algorithm,[0],[0]
i ≤ νĜ.,4.1. Horton’s algorithm,[0],[0]
Compute ĈŜi using det K̃Ŝi for 1 ≤,4.1. Horton’s algorithm,[0],[0]
i ≤ νĜ.,4.1. Horton’s algorithm,[0],[0]
"Construct A ∈ GF (2)νĜ×m, b ∈ GF (2)νĜ .",4.1. Horton’s algorithm,[0],[0]
Solve Ax = b using Gaussian elimination.,4.1. Horton’s algorithm,[0],[0]
"Set K̂i,j = K̂j,i = (2x{i,j}−1)B̂ 1/2",4.1. Horton’s algorithm,[0],[0]
"i,j , for all {i, j} ∈",4.1. Horton’s algorithm,[0],[0]
"Ê.
Lemma 4.",4.1. Horton’s algorithm,[0],[0]
"LetA ∈ GF (2)ν×m, b ∈ GF",4.1. Horton’s algorithm,[0],[0]
(2)ν .,4.1. Horton’s algorithm,[0],[0]
"Then Gaussian elimination will find a vector x ∈ GF (2)m such that Ax = b or conclude that none exists in O(ν2m) time.
",4.1. Horton’s algorithm,[0],[0]
We give our procedure for computing the estimator K̂ in Algorithm 1.,4.1. Horton’s algorithm,[0],[0]
"In the following theorem, we bound the running time of Algorithm 1 and establish an upper bound on the sample complexity needed to solve the recovery problem as well as the sample complexity needed to compute an estimate K̂ that is close to K.
Theorem 3.",4.1. Horton’s algorithm,[0],[0]
Let K ∈,4.1. Horton’s algorithm,[0],[0]
"RN×N be a symmetric matrix satisfying 0 K I , and satisfying Assumption 1.",4.1. Horton’s algorithm,[0],[0]
Let GK be the graph induced by K and ` be the cycle sparsity of GK .,4.1. Horton’s algorithm,[0],[0]
"Let Y1, ..., Yn be samples from DPP(K) and δ ∈ (0, 1).",4.1. Horton’s algorithm,[0],[0]
"If
n > log(N",4.1. Horton’s algorithm,[0],[0]
"`+1/δ)
(α/4) 2`
,
then with probability at least 1 − δ, Algorithm 1 computes an estimator K̂ which recovers the signs of K up to a DN - similarity and satisfies
ρ(K, K̂)",4.1. Horton’s algorithm,[0],[0]
"< 1
α
( 8 log(4N `+1/δ)
n
)1/2 (5)
in O(m3 + nN2) time.
",4.1. Horton’s algorithm,[0],[0]
Proof.,4.1. Horton’s algorithm,[0],[0]
(5) follows directly from (2) in the proof of Theorem 1.,4.1. Horton’s algorithm,[0],[0]
"That same proof also shows that with probability at least 1 − δ, the support of GK and the signs of K are recovered up to a DN -similarity.",4.1. Horton’s algorithm,[0],[0]
What remains is to upper bound the worst case run time of Algorithm 1.,4.1. Horton’s algorithm,[0],[0]
We will perform this analysis line by line.,4.1. Horton’s algorithm,[0],[0]
Initializing K̂ requires O(N2) operations.,4.1. Horton’s algorithm,[0],[0]
Computing ∆S for all subsets |S| ≤ 2 requires O(nN2) operations.,4.1. Horton’s algorithm,[0],[0]
"Setting K̂i,i requires O(N) operations.",4.1. Horton’s algorithm,[0],[0]
"Computing B̂i,j for 1 ≤",4.1. Horton’s algorithm,[0],[0]
i < j ≤ N requires O(N2) operations.,4.1. Horton’s algorithm,[0],[0]
Forming K̃ requires O(N2) operations.,4.1. Horton’s algorithm,[0],[0]
Forming GK requires O(N2) operations.,4.1. Horton’s algorithm,[0],[0]
"By
Lemma 3, computing a shortest maximal cycle basis requires O(mN) operations.",4.1. Horton’s algorithm,[0],[0]
"Constructing the subsets Si, 1 ≤ i ≤ νĜ, requires O(mN) operations.",4.1. Horton’s algorithm,[0],[0]
Computing ∆̂Si for 1 ≤,4.1. Horton’s algorithm,[0],[0]
i ≤ νĜ requires O(nm) operations.,4.1. Horton’s algorithm,[0],[0]
Computing ĈSi using det(K̃[Si]) for 1 ≤,4.1. Horton’s algorithm,[0],[0]
"i ≤ νĜ requires O(m`3) operations, where a factorization of each K̃[Si] is used to compute each determinant in O(`3) operations.",4.1. Horton’s algorithm,[0],[0]
Constructing A and b requires O(m`) operations.,4.1. Horton’s algorithm,[0],[0]
"By Lemma 4, finding a solution x using Gaussian elimination requires O(m3) operations.",4.1. Horton’s algorithm,[0],[0]
"Setting K̂i,j for all edges {i, j} ∈ E requires O(m) operations.",4.1. Horton’s algorithm,[0],[0]
"Put this all together, Algorithm 1 runs in O(m3 + nN2) time.",4.1. Horton’s algorithm,[0],[0]
Here we show that it is possible to obtain faster algorithms by exploiting the structure of GK .,4.2. Chordal Graphs,[0],[0]
"Specifically, in the case where GK chordal, we give an O(m) time algorithm to determine the signs of the off-diagonal entries of the estimator K̂, resulting in an improved overall runtime of O(m + nN2).",4.2. Chordal Graphs,[0],[0]
Recall that a graph G =,4.2. Chordal Graphs,[0],[0]
(,4.2. Chordal Graphs,[0],[0]
"[N ], E) is said to be chordal if every induced cycle in G is of length three.",4.2. Chordal Graphs,[0],[0]
"Moreover, a graph G = ([N ], E) has a perfect elimination ordering (PEO) if there exists an ordering of the vertex set {v1, ..., vN} such that, for all i, the graph induced by {vi} ∪ {vj |{i, j} ∈ E, j > i} is a clique.",4.2. Chordal Graphs,[0],[0]
It is well known that a graph is chordal if and only if it has a PEO.,4.2. Chordal Graphs,[0],[0]
"A PEO of a chordal graph with m edges can be computed in O(m) operations using lexicographic breadth-first search (Rose et al., 1976).
",4.2. Chordal Graphs,[0],[0]
Lemma 5.,4.2. Chordal Graphs,[0],[0]
Let G =,4.2. Chordal Graphs,[0],[0]
(,4.2. Chordal Graphs,[0],[0]
"[N ], E), be a chordal graph and {v1, ..., vn} be a PEO.",4.2. Chordal Graphs,[0],[0]
"Given i, let i∗",4.2. Chordal Graphs,[0],[0]
:,4.2. Chordal Graphs,[0],[0]
= min{j|j >,4.2. Chordal Graphs,[0],[0]
"i, {vi, vj} ∈ E}.",4.2. Chordal Graphs,[0],[0]
Then the graph G′ =,4.2. Chordal Graphs,[0],[0]
(,4.2. Chordal Graphs,[0],[0]
"[N ], E′), where E′ = {{vi, vi∗}}N−κ(G)i=1 , is a spanning forest of G.
Proof.",4.2. Chordal Graphs,[0],[0]
"We first show that there are no cycles in G′. Suppose to the contrary, that there is an induced cycle C of length k on the vertices {vj1 , ..., vjk}.",4.2. Chordal Graphs,[0],[0]
Let v be the vertex of smallest index.,4.2. Chordal Graphs,[0],[0]
Then v is connected to two other vertices in the cycle of larger index.,4.2. Chordal Graphs,[0],[0]
"This is a contradiction to the construction.
",4.2. Chordal Graphs,[0],[0]
What remains is to show that |E′| = N − κ(G).,4.2. Chordal Graphs,[0],[0]
It suffices to prove the case κ(G) = 1.,4.2. Chordal Graphs,[0],[0]
"Suppose to the contrary, that there exists a vertex vi, i < N , with no neighbors of larger index.",4.2. Chordal Graphs,[0],[0]
Let P be the shortest path in G from vi to vN .,4.2. Chordal Graphs,[0],[0]
"By connectivity, such a path exists.",4.2. Chordal Graphs,[0],[0]
Let vk be the vertex of smallest index in the path.,4.2. Chordal Graphs,[0],[0]
"However, it has two neighbors in the path of larger index, which must be adjacent to each other.",4.2. Chordal Graphs,[0],[0]
"Therefore, there is a shorter path.
",4.2. Chordal Graphs,[0],[0]
"Now, given the chordal graph GK induced by K and the estimates of principal minors of size at most three, we provide an algorithm to determine the signs of the edges of
Algorithm 2 Compute Signs of Edges in Chordal Graph
Input: GK =",4.2. Chordal Graphs,[0],[0]
"([N ], E) chordal, ∆̂S for |S| ≤ 3.
",4.2. Chordal Graphs,[0],[0]
"Compute a PEO {v1, ..., vN}.",4.2. Chordal Graphs,[0],[0]
Compute the spanning forest G′ =,4.2. Chordal Graphs,[0],[0]
(,4.2. Chordal Graphs,[0],[0]
"[N ], E′).",4.2. Chordal Graphs,[0],[0]
Set all edges in E′ to have positive sign.,4.2. Chordal Graphs,[0],[0]
"Compute Ĉ{i,j,i∗} for all {i, j} ∈ E \ E′, j < i. Order edges E \",4.2. Chordal Graphs,[0],[0]
E′,4.2. Chordal Graphs,[0],[0]
"= {e1, ..., eν} such that",4.2. Chordal Graphs,[0],[0]
i,4.2. Chordal Graphs,[0],[0]
> j,4.2. Chordal Graphs,[0],[0]
if max ei < max ej .,4.2. Chordal Graphs,[0],[0]
"Visit edges in sorted order and for e = {i, j}, j >",4.2. Chordal Graphs,[0],[0]
"i, set
sgn({i, j}) = sgn(Ĉ{i,j,i∗}) sgn({i, i∗}) sgn({j, i∗}).
",4.2. Chordal Graphs,[0],[0]
"GK , or, equivalently, the off-diagonal entries of K.
Theorem 4.",4.2. Chordal Graphs,[0],[0]
"IfGK is chordal, Algorithm 2 correctly determines the signs of the edges of GK in O(m) time.
",4.2. Chordal Graphs,[0],[0]
Proof.,4.2. Chordal Graphs,[0],[0]
We will simultaneously perform a count of the operations and a proof of the correctness of the algorithm.,4.2. Chordal Graphs,[0],[0]
Computing a PEO requires O(m) operations.,4.2. Chordal Graphs,[0],[0]
Computing the spanning forest requires O(m) operations.,4.2. Chordal Graphs,[0],[0]
"The edges of the spanning tree can be given arbitrary sign, because it is a cycle-free graph.",4.2. Chordal Graphs,[0],[0]
This assigns a sign to two edges of each 3-cycle.,4.2. Chordal Graphs,[0],[0]
"Computing each Ĉ{i,j,i∗} requires a constant number of operations because ` = 3, requiring a total of O(m −N) operations.",4.2. Chordal Graphs,[0],[0]
Ordering the edges requires O(m) operations.,4.2. Chordal Graphs,[0],[0]
"Setting the signs of each remaining edge requires O(m) operations.
",4.2. Chordal Graphs,[0],[0]
"Therefore, when GK is chordal, the overall complexity required by our algorithm to compute K̂ is reduced to O(m+ nN2).",4.2. Chordal Graphs,[0],[0]
Here we present experiments to supplement the theoretical results of the paper.,5. Experiments,[0],[0]
We test our algorithm on two types of random matrices.,5. Experiments,[0],[0]
"First, we consider the matrix K ∈",5. Experiments,[0],[0]
"RN×N corresponding to the cycle on N vertices,
K = 1
2 I +
1 4 A,
where A is symmetric and has non-zero entries only on the edges of the cycle, either +1 or −1, each with probability 1/2.",5. Experiments,[0],[0]
"By the Gershgorin circle theorem, 0 K I .",5. Experiments,[0],[0]
"Next, we consider the matrix K ∈",5. Experiments,[0],[0]
"RN×N corresponding to the clique on N vertices,
K = 1
2 I +
1
4 √ N A,
where A is symmetric and has all entries either +1 or −1, each with probability 1/2.",5. Experiments,[0],[0]
It is well known that −2,5. Experiments,[0],[0]
"√ N A 2 √ N with high probability, implying 0 K I .
",5. Experiments,[0],[0]
"For both cases and for a range of values of matrix dimension N and samples n, we run our algorithm on 50 randomly generated instances.",5. Experiments,[0],[0]
"We record the proportion of trials where we recover the graph induced by K, and the proportion of the trials where we recover both the graph and correctly determine the signs of the entries.
",5. Experiments,[0],[0]
"In Figure 1, the shade of each box represents the proportion of trials where recovery was successful for a given pair N,n. A completely white box corresponds to zero success rate, black to a perfect success rate.
",5. Experiments,[0],[0]
The plots corresponding to the cycle and the clique are telling.,5. Experiments,[0],[0]
"We note that for the clique, recovering the sparsity pattern and recovering the signs of the off-diagonal entries come hand-in-hand.",5. Experiments,[0],[0]
"However, for the cycle, there is a noticeable gap between the number of samples required to recover the sparsity pattern and the number of samples required to recover the signs of the off-diagonal entries.",5. Experiments,[0],[0]
"This empirically confirms the central role that cycle sparsity plays in parameter estimation, and further corroborates our theoretical results.",5. Experiments,[0],[0]
"In this paper, we gave the first provable guarantees for learning the parameters of a DPP.",6. Conclusion and open questions,[0],[0]
"Our upper and lower bounds reveal the key role played by the parameter `, which is the cycle sparsity of graph induced by the kernel of the DPP.",6. Conclusion and open questions,[0],[0]
"Our estimator does not need to know ` beforehand, but can adapt to the instance.",6. Conclusion and open questions,[0],[0]
"Moreover, our procedure outputs an estimate of `, which could potentially be used for further inference questions such as testing and confidence intervals.",6. Conclusion and open questions,[0],[0]
"An interesting open question is whether on a graph by graph basis, the parameter ` exactly determines the optimal sample complexity.",6. Conclusion and open questions,[0],[0]
"Moreover when the number of samples is too small, can we exactly characterize which signs can be learned correctly and which cannot (up to a similarity transformation by D)?",6. Conclusion and open questions,[0],[0]
"Such results would lend new theoretical insights into the output of algorithms for learning DPPs, and which individual parameters in the estimate we can be confident about and which we cannot.
Acknowledgements.",6. Conclusion and open questions,[0],[0]
"A.M. is supported in part by NSF CAREER Award CCF-1453261, NSF Large CCF1565235, a David and Lucile Packard Fellowship and an Alfred P. Sloan Fellowship.",6. Conclusion and open questions,[0],[0]
"P.R. is supported in part by NSF CAREER DMS-1541099, NSF DMS-1541100, DARPA W911NF-16-1-0551, ONR N00014-17-1-2147 and a grant from the MIT NEC Corporation.",6. Conclusion and open questions,[0],[0]
"Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important.",abstractText,[0],[0]
"While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP.",abstractText,[0],[0]
"Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the cycle sparsity; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity.",abstractText,[0],[0]
"Finally, we give experimental results that confirm our theoretical findings.",abstractText,[0],[0]
Learning Determinantal Point Processes with Moments and Cycles,title,[0],[0]
"For well over a decade there has been extensive work on learning social network influence models (Liben-Nowell & Kleinberg, 2003; Netrapalli & Sanghavi, 2012; Abrahao et al., 2013; Friggeri et al., 2014; Anderson et al., 2015; Subbian et al., 2017), and the independent cascade model in particular (Saito et al., 2008; Gomez Rodriguez et al., 2010; Goyal et al., 2010; Gomez Rodriguez et al., 2011; Du et al., 2014; Lemonnier et al., 2014; Bourigault et al.,
*Equal contribution 1Department of Computer Science, Harvard University 2Facebook, Menlo Park.",1. Introduction,[0],[0]
"Correspondence to: Dimitris Kalimeris <kalimeris@g.harvard.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
2014; Narasimhan et al., 2015).",1. Introduction,[0],[0]
"Independent cascade (IC) was popularized by the seminal work of Kempe, Kleinberg, and Tardos (Kempe et al., 2003) and is a stochastic model that predicts the likelihood of information diffusing from one individual to another in a social network.",1. Introduction,[0],[0]
"In this model for every pair of individuals connected in the network u, v there is a probability pu,v that v adopts the behavior of u (i.e. information is diffused from u to v).
",1. Introduction,[0],[0]
The main challenge with learning the IC model is that the sample complexity is often overwhelmingly large or simply infeasible.,1. Introduction,[0],[0]
"To illustrate this point, Figure 1 shows the cumulative distribution of edge interactions for millions of public events on Facebook over varying periods of time, ranging from one week to two months (see detailed description of the dataset in Section 5).",1. Introduction,[0],[0]
The vertical line marks the minimal number of observations per edge required to infer the likelihood of influence with error 0.1 and confidence 95%.,1. Introduction,[0],[0]
"In this data set, more than 90% of the edges do not have enough observations to learn the respective diffusion probabilities accurately, even over a period of two months.",1. Introduction,[0],[0]
"Furthermore, in a single week (the timeframe in which inference about an event is often most relevant), none of the edges in the data set have sufficiently many observations.
",1. Introduction,[0],[0]
"Given that even with the data available on Facebook there are not enough observations to learn the model, one needs to impose additional assumptions.",1. Introduction,[0],[0]
"A natural approach is to assume that the diffusion probabilities are a function of network and individuals’ characteristics and some underlying
global hyperparameter θ.",1. Introduction,[0],[0]
"In the case of events for example, it seems reasonable that influence could be estimated as a function of some global unknown multidimensional parameter θ and individuals’ characteristics such as location, gender, and age, and topological features like the ratio between the intersection and the union of the individuals’ neighborhoods.",1. Introduction,[0],[0]
"Using xu,v to denote the characteristics of u and v, the hyperparametric approach assumes that the probability of u to influence v denoted pu,v is not arbitrary and can be faithfully estimated via some function p that maps θ and xu,v to [0, 1], i.e. pu,v = p(θ,xu,v).",1. Introduction,[0],[0]
"Given a set of characteristics, learning the IC model then reduces to recovering the underlying hyperparameter θ.
Intuitively, learning a hyperparametric model necessitates far fewer samples than a general diffusion model for two main reasons.",1. Introduction,[0],[0]
"First, since the diffusion probabilities are correlated, each observation provides information about all edges in the network.",1. Introduction,[0],[0]
"Second, it seems reasonable that the sample complexity of learning the hyperparameter should largely depend on the dimension of the hyperparameter rather than the number of edges the network.
",1. Introduction,[0],[0]
A simple example.,1. Introduction,[0],[0]
"To solidify our intuition, consider a simple bipartite network G = (U, V,E) where nodes in U attempt to activate nodes from V as depicted in Figure 1.1 and each activation attempt together with its outcome (label) constitutes one sample.",1. Introduction,[0],[0]
"Our goal is to find a p̂u,v for every edge (u, v) ∈",1. Introduction,[0],[0]
"E, s.t. with prob.",1. Introduction,[0],[0]
"at least 1− δ for all edges:
|pu,",1. Introduction,[0],[0]
"v − p̂u,v| ≤
Hoeffding’s inequality and a union bound imply that Θ ( |E| 2 log |E| δ ) samples are necessary and sufficient to learn the diffusion probabilities on all the edges in the graph.",1. Introduction,[0],[0]
"In comparison, suppose that the diffusion probability of each edge is a function of a hyperparameter θ ∈",1. Introduction,[0],[0]
"[0, 1]d and some known features of the edge xu,v ∈",1. Introduction,[0],[0]
"[0, 1]d as follows:
pu,v = 1
1 + e−〈θ,xu,v〉
Then, learning the diffusion probabilities becomes a logistic regression problem and thus only O ( d 2 log d δ )",1. Introduction,[0],[0]
"samples are required, independent of the number of edges.",1. Introduction,[0],[0]
"This reduces the sample complexity by a factor of |E|d which is quite dramatic when the number of edges in the network |E| is large and the dimension of the hyperparameter d is small.
",1. Introduction,[0],[0]
"Beyond potential improvements in sample complexity, a hyperparametric model is convenient due to the structure it imposes.",1. Introduction,[0],[0]
"A recent line of work on influence maximization in bandit models (Wen et al., 2015; Vaswani et al., 2017), assumes that the diffusion probabilities are a linear function of edge features, i.e. pu,v = 〈θ, xu,v〉 and this structure is leveraged in order to develop faster algorithms.",1. Introduction,[0],[0]
Our goal in this paper is to explore a hyperparametric approach for learning the independent cascade diffusion model.,1.1. A Hyperparametric Approach,[0],[0]
"Doing so requires addressing three open questions:
• Does restriction to a low-dimensional hyperparameter substantially decrease the sample complexity?",1.1. A Hyperparametric Approach,[0],[0]
"As discussed above, the motivation for a hyperparametric approach is that intuitively its sample complexity should depend on the dimension of the hyperparameter rather than the number of edges.",1.1. A Hyperparametric Approach,[0],[0]
"While intuitive, when the indegree of nodes is greater than 1, minimizing empirical risk becomes a non-convex optimization problem and analyzing sample complexity is not trivial; • Can a hyperparametric model be learned efficiently?",1.1. A Hyperparametric Approach,[0],[0]
"As learning a hyperparametric model requires solving a non-convex optimization problem, it is not clear it can be learned efficiently, in theory or in practice; 1 • Are low-dimensional hyperparametric models predictive?",1.1. A Hyperparametric Approach,[0],[0]
"Assuming that sample complexity heavily depends on its dimension, our approach is relevant only if reasonable estimates of the IC model are achievable with a low-dimensional hypothesis class.
",1.1. A Hyperparametric Approach,[0],[0]
In this paper we address the above questions.,1.1. A Hyperparametric Approach,[0],[0]
We first show that the sample complexity can indeed be dramatically reduced when restricting the hypothesis class to a lowdimensional hyperparameter.,1.1. A Hyperparametric Approach,[0],[0]
"Specifically, when comparing with the state-of-the-art bound for learning the independent cascade model (without the hyperparametric assumption) we show that the sample complexity can be reduced by a factor of |E|/d, as foreshadowed by the example above.
",1.1. A Hyperparametric Approach,[0],[0]
Despite being a non-concave optimization problem we show that the problem has a great deal of structure.,1.1. A Hyperparametric Approach,[0],[0]
"Under mild assumptions about the distribution generating the samples, we show how this structure can be leveraged to efficiently train a model with arbitrarily small generalization error.
",1.1. A Hyperparametric Approach,[0],[0]
"Lastly, we show that the hyperparametric approach does work in practice.",1.1. A Hyperparametric Approach,[0],[0]
"To do so, we ran experiments on large scale cascades recorded on the Facebook social network.",1.1. A Hyperparametric Approach,[0],[0]
"We show that with a hyperparameter of dimension 40 one
1We note that even without the hyperparametric assumption PAC learning the IC model is a non-convex optimization problem (Narasimhan et al., 2015).
can estimate the diffusion probabilities with remarkably high accuracy.",1.1. A Hyperparametric Approach,[0],[0]
"Naturally, there are data sets that do not contain individuals’ characteristics.",1.1. A Hyperparametric Approach,[0],[0]
"Nonetheless, most social networking services include useful information about its members that help predict diffusion, and the topology of the network alone often may serve as a good proxy.",1.1. A Hyperparametric Approach,[0],[0]
"A social network is a finite directed graph G = (V,E), where the set of nodes V represents the individuals in the network and the set of edges E represents their social links.
",2. The Hyperparametric Model,[0],[0]
Independent Cascade (IC) model.,2. The Hyperparametric Model,[0],[0]
The IC model assumes that every node v ∈ V can either be active or inactive.,2. The Hyperparametric Model,[0],[0]
All nodes begin as inactive.,2. The Hyperparametric Model,[0],[0]
"At time step t = 0 a subset of the nodes X called the seed becomes active, and activations in the network continue according to the following stochastic process: Each node u that became active at time step t = τ attempts to influence every one of its neighbors v only once at time step t = τ + 1 independently, and succeeds with some probability pu,v. A node v that became active during the process will never go back to being inactive.",2. The Hyperparametric Model,[0],[0]
"Our work generalizes the standard IC model by assuming that the probabilities pu,v are not arbitrary but correlated and specifically consequences of nodes’ features.",2. The Hyperparametric Model,[0],[0]
"This is the hyperparametric assumption, as described below.
Hyperparametrization.",2. The Hyperparametric Model,[0],[0]
Every node u ∈ V is associated with a vector of features containing information about it.,2. The Hyperparametric Model,[0],[0]
"Every edge (u, v) ∈",2. The Hyperparametric Model,[0],[0]
"E, is also associated with a feature vector, the concatenation of the feature vectors of its endpoints, denoted by xu,v. The diffusion probability of each edge is a function of a global hyperparameter θ and its feature vector.",2. The Hyperparametric Model,[0],[0]
"Formally, we assume that there exists a function p :",2. The Hyperparametric Model,[0],[0]
Rd × Rd,2. The Hyperparametric Model,[0],[0]
"→ [0, 1] s.t. pu,v = p(θ, xu,v) for any edge (u, v) ∈",2. The Hyperparametric Model,[0],[0]
E.,2. The Hyperparametric Model,[0],[0]
"In this work we define p as the sigmoid function:
pu,v = σ(θ, xu,v) = 1
1 + e−〈θ,xu,v〉 .
",2. The Hyperparametric Model,[0],[0]
We restrict the hyperparameter θ to lie in a hypothesis class H =,2. The Hyperparametric Model,[0],[0]
"[−B,B]d for some constant B > 0, and w.l.o.g.",2. The Hyperparametric Model,[0],[0]
"we assume that the feature vector of every edge lies in [0, 1]d.",2. The Hyperparametric Model,[0],[0]
"Additionally, we assume that pu,v is bounded away from 0 and 1 for all edges, i.e. pu,v ∈",2. The Hyperparametric Model,[0],[0]
"[λ, 1− λ], for some λ > 0.
",2. The Hyperparametric Model,[0],[0]
"Further discussion about the hyperparametric model and the choice of the sigmoid function can be found in Appendix A.
Samples.",2. The Hyperparametric Model,[0],[0]
The input to a learning algorithm is a collection of labeled samples.,2. The Hyperparametric Model,[0],[0]
"We assume that there is some unknown distribution D0 over subset of nodes, that activates the initial seed of the cascade, V0.",2. The Hyperparametric Model,[0],[0]
"Subsequently, as we discussed before, we can partition V \ V0 into subsets of nodes V1, V2, . . .",2. The Hyperparametric Model,[0],[0]
", Vn−1 that become activated at steps
τ = 1, 2, . . .",2. The Hyperparametric Model,[0],[0]
", n− 1, respectively 2.",2. The Hyperparametric Model,[0],[0]
"Notice that the cascade can be further decomposed into a sequence of simpler samples as follows: for every τ ∈ {0, 1, . . .",2. The Hyperparametric Model,[0],[0]
", n− 1} consider all the nodes v /∈",2. The Hyperparametric Model,[0],[0]
∪τ−1t=0 Vt that are within distance of 1 from Vτ .,2. The Hyperparametric Model,[0],[0]
"For every v that became activated by Vτ (i.e. v ∈ Vτ+1) create the sample ((Vτ , v), 1), and for every v that remained inactive create the sample ((Vτ , v), 0).",2. The Hyperparametric Model,[0],[0]
"Throughout this paper we assume that the input to our learning algorithm is of the form {(Xi, vi), yi}mi=1 where Xi ⊆ V is a subset of active nodes, vi is a node in distance 1 from Xi and yi ∈ {0, 1} is its label.",2. The Hyperparametric Model,[0],[0]
"In Appendix C we map every seed-generating distribution D0 to a sample-generating distribution D.
Log-likelihood of a sample.",2. The Hyperparametric Model,[0],[0]
"For every node v, the event “v becomes influenced by X , when the hyperparameter has value θ” is a Bernoulli random variable with probability of success fθv (X) = 1− ∏ u∈X∩N(v)(1−pu,v(θ))",2. The Hyperparametric Model,[0],[0]
"whereN(v) is the set of in-neighbors of node v. Hence, the likelihood of a sample s = ((X, v), y), where v /∈",2. The Hyperparametric Model,[0],[0]
"X is fθv (X)y · ( 1−
fθv (X) )1−y , and the respective log-likelihood of s is:
L(s, θ)",2. The Hyperparametric Model,[0],[0]
= y ln(fθv (X)),2. The Hyperparametric Model,[0],[0]
+ (1− y) ln(1− fθv (X)),2. The Hyperparametric Model,[0],[0]
"(1)
We want to recover a hyperparameter θ that yields accurate estimates.",2. The Hyperparametric Model,[0],[0]
"To do so, given a training set S = {si}mi=1, we seek the most probable hyperparameter generating S by maximizing the cumulative log-likelihood function:
θ̂ = arg max θ∈H
1
m m∑ i=1 L(si, θ) (2)
",2. The Hyperparametric Model,[0],[0]
Learning a diffusion model.,2. The Hyperparametric Model,[0],[0]
"Our goal is to bound the sample complexity, i.e. the number of i.i.d.",2. The Hyperparametric Model,[0],[0]
"samples generated by a distribution D that we need to observe to PAC learn H. That is, guarantee that supθ∈H Es∼D[L(s, θ)]",2. The Hyperparametric Model,[0],[0]
"− Es∼D[L(s, θ̂)] ≤ , with probability at least 1 − δ (see definition of PAC learnability in Appendix B).
",2. The Hyperparametric Model,[0],[0]
"Notice that while there are |E| edges in the network, which translates to |E| diffusion probabilities, in the optimization problem (2) there are only d parameters to be learned.
",2. The Hyperparametric Model,[0],[0]
We would like to note that PAC learning guarantees are required to hold for any distribution D that generates the data.,2. The Hyperparametric Model,[0],[0]
"Hence, it is easy to see that the diffusion probabilities or the hyperparameter θ are not learnable without extra assumptions on D. For details refer to Appendix D.",2. The Hyperparametric Model,[0],[0]
In this section we prove Theorem 2 which is the main technical result of the paper.,3. Learning a Hyperparametric Model,[0],[0]
The main takeaway is that a hyperparameteric approach makes learning an influence model feasible.,3. Learning a Hyperparametric Model,[0],[0]
"Informally, the theorem states that the number of samples required to ( , δ)-PAC learn the model is
2The influence process terminates after at most |V",3. Learning a Hyperparametric Model,[0],[0]
"| − 1 steps.
",3. Learning a Hyperparametric Model,[0],[0]
"Õ ( ∆2 ( ∆·d+log 1δ 2 )) , where ∆ is the maximum degree in
the network and d is the dimension of the hyperparameter.",3. Learning a Hyperparametric Model,[0],[0]
"As we later show in the experiments section, very small constant values of d suffice to learn an influence model with almost no error on real data.",3. Learning a Hyperparametric Model,[0],[0]
"This is in sharp contrast to the best sample complexity guarantees due to (Narasimhan et al., 2015) for learning the model without the hyperparametric assumption which is Õ ( ∆2 ( ∆·|E|+log 1δ 2 )) .
",3. Learning a Hyperparametric Model,[0],[0]
"Furthermore, imposing assumptions on the distribution D, can reduce the dependence on ∆ making the sample complexity (almost) independent of the size of the network.",3. Learning a Hyperparametric Model,[0],[0]
The main technical challenge is due to the fact that the MLE objective in (2) is non-concave,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
and we cannot immediately derive sample complexity bounds from convergence guarantees of Stochastic Gradient Descent for example.,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Instead, to analyze the sample complexity we will argue about the Rademacher complexity of our hypothesis class by using covering numbers.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Informally, the Rademacher complexity measures the expressive power of a hypothesis classH with respect to a probability distribution and the covering number of a set is the number of balls of a certain radius whose union contains the set (see Definitions 2 and 3 in Appendix B).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Recall that the sample complexity of a hypothesis class can be derived from its Rademacher complexity.
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Theorem 1 ((Shalev-Shwartz & Ben-David, 2014)).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Assume that for every sample s ∼ D and every θ ∈ H we have that: |L(s, θ)| ≤ C. Let S ∼ Dm and θ̂ = arg maxθ∈H { 1 m ∑m i=1",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"L (si, θ) } .",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Then with probability at least 1− δ over the choice of S we have that:
Es∼D[L(s, θ̂)]",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"≥ sup θ∈H Es∼D[L(s, θ)]
−R(S,H)−O C √
log 1δ m  whereR(S,H) is the Rademacher complexity of the class H with respect to S.
Hence, our goal reduces to boundingR(S,H) for a training set S of size m. We do so by discretizingH by , and prove that if the discretization is dense enough, then we do not sacrifice a lot by searching for the most likely hyperparameter in the discrete spaceH instead of the continuousH.
To this end, we construct an -cover of the hypothesis class H =",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"[−B,B]d.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Proving that the log-likelihood of any fixed sample s, is bounded and Lipschitz3 in θ with respect to
3intuitively for a Lipschitz function a small change in the argument cannot lead to a large change in the value of the function, see Definition 4, Appendix B.
the `1-norm, where the Lipschitz parameter depends on λ (Lemma 3 in Appendix E), allows us to translate the cover of the space of the hyperparameter, into a cover of the space of the log-likelihood functions, by slightly increasing the number of points we include in it, as stated in Lemma 1.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"The proof is deferred to Appendix E.
Lemma 1.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Let S = {((Xi, vi), yi)}mi=1 be a non-empty set of samples and let ∆S = maxs∈S |X∩N(v)| (maximum indegree of a node that was activated, across all samples).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"The covering number of the class of all log-likelihood functions for S is O (( Bρd λ∆S )d) , i.e. we can choose a discrete cover
H ⊆ H of size",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"O ((
Bρd λ∆S
)d) , such that for all θ ∈ H,
there exists a θ ∈ H with sup s∈S |L(s, θ)− L(s, θ )| ≤ .
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Given the above lemma, we can invoke Massart’s lemma (Lemma 5 in Appendix E) onH which upper bounds the Rademacher complexity of finite hypothesis classes.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Subsequently, we use Lemma 4 (Appendix E) to upper bound the Rademacher complexity ofH from that ofH .",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"We are now ready to prove the main theorem of the section.
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
Theorem 2 (Sample Complexity of MLE).,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Let G = (V,E) be a directed graph and D be a distribution that generates samples of the form s = ((X, v), y).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
Let ∆ = maxs∼D |X ∩N(v)|.,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Then, for any , δ ∈ (0, 1) , if we use Maximum Likelihood Estimation on a training set of size m ≥ m( , δ) =",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
O ( ∆2 log2(1/λ)d log(Bρd/λ ∆ ),3.1. Sample Complexity via Radamacher Complexity,[0],[0]
+log(1/δ),3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"2
) samples drawn i.i.d.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"from D, with probability at least 1− δ (over the draw of the training set) it holds:
sup θ∈H
Es∼D[L ( s, θ ) ]",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"− Es∼D[L ( s, θ̂ ) ] ≤ .
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
Proof.,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Define ∆ := maxs∼D |X ∩N(v)|, i.e. the maximum active in-degree that any sample generated by D can have.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Then, applying Lemma 1 we can create a discrete -cover of the space of the log-likelihoods, H ⊆ H, of size |H",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
| =,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
O (( Bρd λ∆ )d) for any training set S of any size.,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Invoking Lemma 4 (Appendix E), we can associate the Rademacher complexity ofH with that of its coverH , for any S and any > 0, as follows:
R(S,H) ≤",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"R(S,H ) + 2 .
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Hence, we can focus on bounding the Rademacher complexity of H instead of that of H. Since H is finite, the well-known Massart’s lemma apply yielding:
R(S,H) ≤",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"R(S,H ) + 2
≤ 2 max θ∈H ∣∣∣∣∣∣(L(si, θ))mi=1∣∣∣∣∣∣2",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"√ 2 log(|H |) m + 2
≤ 2 √ m∆",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"ln 1 λ · √ 2 log(|H |) m + 2
= 2∆ ln 1
λ
√ 2 log(|H |)
m + 2
= O ( ∆ log 1
λ
√ d log(Bρd/λ∆ )
m
)",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"+ 2
where the first inequality holds because of Lemma 4 (discretization), the second because of Massart’s lemma (finite hypothesis class), the third because of Lemma 3 (bounded L), and the last one because of Lemma 1 (covering number).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Setting = 1/m yields: R(S,H) =",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"O ( ∆ log(1/λ) √
d log(Bρdm/λ∆) m
) .",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Now using the
generalization bound of Theorem 1, one can see that in order to achieve Es∼D[L(s, θ̂)]",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"≥ supθ∈H Es∼D[L(s, θ)]− , with probability at least 1 − δ, we need S to be of size m = O",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
( ∆2 log2(1/λ)d log(Bρd/λ ∆ ),3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"+log(1/δ) 2 ) , which
concludes the proof.
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Given that B and λ are constants the above bound simplifies to Õ ( ∆2 ( ∆·d+log 1δ 2 )) , which allows immediate compar-
ison with the bounds derived in (Narasimhan et al., 2015).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Additionally, when the degree of every node is constant (which is the case for real social networks like Facebook) or when the distribution D activates only seeds of constant size, ∆ is a constant and the sample complexity becomes Õ ( d+log 1δ 2 ) , independent of the size of the network.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"As we mentioned in the previous section, the maximization problem (2) is non-concave and it cannot be solved efficiently.",4. Algorithms,[0],[0]
"However, the cumulative log-likelihood function we aim to optimize has a great deal of structure we can utilize.
",4. Algorithms,[0],[0]
"To understand this structure, note that there are only three distinct cases for a sample s = ((X, v), y) in the training set S: (i) node v was not influenced, (ii) node v was influenced and there is only one neighbor of v in X and (iii) node v was influenced and there is more than one neighbor of v in X .",4. Algorithms,[0],[0]
The only case that makes the respective log-likelihood non-concave is (iii) since we are unable to identify which of the parents of v actually influenced it and how to update the hyperparameter (equation (1) yields that formally).,4. Algorithms,[0],[0]
"We refer to such samples as obfuscated.
",4. Algorithms,[0],[0]
One can partition S into So and S \,4. Algorithms,[0],[0]
So where So contains the obfuscated samples.,4. Algorithms,[0],[0]
"We can then write f̃(θ) := 1m ∑ s∈S L(s, θ) = 1 m ∑ s∈S\So L(s, θ) + 1 m ∑ s∈So L(s, θ) =: f(θ) + ξ(θ).",4. Algorithms,[0],[0]
Optimizing f̃ can be perceived as optimizing a concave function f under noise ξ.,4. Algorithms,[0],[0]
"The magnitude of the noise depends on the probability of seeing an obfuscated sample, which characterizes the difficulty of the optimization problem and can be computed in simple cases (see e.g. Lemma 8 in Appendix F).
",4. Algorithms,[0],[0]
"There are three distinct approaches that we can follow:
1.",4. Algorithms,[0],[0]
"Ignore the obfuscated samples and optimize f instead of f̃ , using standard methods like Gradient Descent.",4. Algorithms,[0],[0]
The fact that the likelihood of each sample is bounded (Lemma 3 in Appendix E) will assure that the recovered solution will approximately optimize f̃ as well.,4. Algorithms,[0],[0]
2.,4. Algorithms,[0],[0]
"Optimize f̃ directly by applying techniques from (Belloni et al., 2015) for concave optimization under noise.",4. Algorithms,[0],[0]
3.,4. Algorithms,[0],[0]
"Attempt to optimize f̃ using standard concave optimization techniques (for example Stochastic Gradient Descent (SGD) which is widely used in the training of deep networks, a non-convex optimization problem).
",4. Algorithms,[0],[0]
"The first two methods provide theoretical guarantees for noise of small magnitude, if the noise is large however, they can lead to large error.",4. Algorithms,[0],[0]
See Appendix F for a detailed description of these two approaches.,4. Algorithms,[0],[0]
"The third heuristic approach works remarkably well in practice, even when the noise is large, as the experiments of Section 5.1 demonstrate.",4. Algorithms,[0],[0]
"Additionally, in Section 5.2 we include experiments indicating that, even if the noise is small, it is still in our best interest to utilize all the available samples since the shortage of the training set hurts us more than non-concavity.",4. Algorithms,[0],[0]
We conduct two sets of experiments.,5. Experiments,[0],[0]
"First, using synthetic datasets we show that if the hyperparametric assumption holds in a network, we can accurately learn the edge probabilities despite the non-concavity of (2), and significantly outperform methods that do not include information about the node features, for small training sets4.",5. Experiments,[0],[0]
We also investigate which properties of the network and the model affect the convergence rate.,5. Experiments,[0],[0]
"Secondly, we validate our approach using real Facebook data, by showing that low-dimensional hyperparametric models are predictive in practice.",5. Experiments,[0],[0]
"Real Graphs: We also use the “ego-facebook”, “wiki-Vote”, “bitcoin-otc” and “bitcoin-alpha” datasets from (Leskovec & Krevl, 2014), which are publicly available real-world social networks, enabling the reproducibility of our experiments.
",5.1. Learning the Diffusion Probabilities,[0],[0]
Graphs.,5.1. Learning the Diffusion Probabilities,[0],[0]
"We synthetically generate the social graph and the hyperparameter that determines the diffusion probabilities.
",5.1. Learning the Diffusion Probabilities,[0],[0]
Synthetic Graphs: We simulate a social network using standard graph models.,5.1. Learning the Diffusion Probabilities,[0],[0]
"Since different models yield graphs with different topology, we selected four of the widely used ones: Barabási-Albert, Kronecker, Erdös-Rényi and the con-
4Notice that learning the diffusion probabilities allows us to compute other quantities of interest as well, like the probability of a node becoming influenced, the final size of a cascade initiated from a given set, or the influence function.
figuration model.",5.1. Learning the Diffusion Probabilities,[0],[0]
"For a more detailed description of these models and the construction process refer to Appendix G.
Experimental setup.",5.1. Learning the Diffusion Probabilities,[0],[0]
"We generate 15 random features in [0, 1] for every node (we found consistent results across a large range of features).",5.1. Learning the Diffusion Probabilities,[0],[0]
"We define the first 10 of them to be the ones in which the hyperparametric assumption is based, and the rest is redundant information (30 features for each edge, where only 20 of them are important).",5.1. Learning the Diffusion Probabilities,[0],[0]
"Additionally, we generate a random hyperparameter in [−1, 1]d (d = 20).",5.1. Learning the Diffusion Probabilities,[0],[0]
"We use the sigmoid function over the important features of each edge e and the hyperparameter to compute pe, as described in Section 2, imposing correlation between the probabilities of different edges by construction.
",5.1. Learning the Diffusion Probabilities,[0],[0]
"Subsequently, we generate 100,000 samples, and attempt to solve the optimization problem (2) using SGD, initializing the hyperparameter to 0 and using a learning rate of 1/ √ T , where T is the size of the training set.",5.1. Learning the Diffusion Probabilities,[0],[0]
"Details on how we create the training set can be found in Appendix G.
Benchmarks.",5.1. Learning the Diffusion Probabilities,[0],[0]
"We tested the hyperparametric model against the following benchmarks: • Omniscient MLE: The true diffusion probability of an
edge is approximated by p̂e = n+e /ne, where n + e is the number of activations of edge e, while ne is the total number of exposures of e (activation attempts).",5.1. Learning the Diffusion Probabilities,[0],[0]
"Here, we assume that for every sample ((X, v), y) we observe the activation or not of all the edges e = (u, v), where u is an active neighbor of v.",5.1. Learning the Diffusion Probabilities,[0],[0]
"This is a strong benchmark since in practice, we can observe whether v became active but not which node activated it.",5.1. Learning the Diffusion Probabilities,[0],[0]
"• Non-hyperparametric MLE: We implemented the algorithm of (Narasimhan et al., 2015), that allows one
to learn the diffusion probabilities only by observing whether a node v was influenced by the seed X or not.",5.1. Learning the Diffusion Probabilities,[0],[0]
"• Hyperparametric MLE, reduced information: We com-
pare ourselves against a hyperparametric model that is unaware of the exact features that are important, and selects only a subset of them.",5.1. Learning the Diffusion Probabilities,[0],[0]
Here we select only 5 out of 10 important features of every node.,5.1. Learning the Diffusion Probabilities,[0],[0]
"• Hyperparametric MLE, augmented information: Similarly, we compare ourselves against a hyperparametric model that is unaware of the important features, thus it selects all the available ones (15 features per node).
Results.",5.1. Learning the Diffusion Probabilities,[0],[0]
"We repeat each experiment 10 times, and provide the mean and the standard deviation in Figure 3.",5.1. Learning the Diffusion Probabilities,[0],[0]
"The y-axis corresponds to 1|E| ∑ e∈E |pe − p̂e|, the average absolute error between the real probability (known by construction) and the empirical one across the network.",5.1. Learning the Diffusion Probabilities,[0],[0]
"In all networks, the hyperparametric approach greatly outperforms the nonhyperparametric benchmarks, even assuming omniscience.
",5.1. Learning the Diffusion Probabilities,[0],[0]
"Note that in the non-hyperparametric benchmarks, since samples do not carry global information, there exist edges that have no exposures given the samples that we have seen so far.",5.1. Learning the Diffusion Probabilities,[0],[0]
"In that case, we define p̂e = 0.",5.1. Learning the Diffusion Probabilities,[0],[0]
"This explains the initial increase in the error in the omniscient MLE since, if pe is small and the first exposure of edge e is an activation, the error on e increases from pe to 1 − pe.",5.1. Learning the Diffusion Probabilities,[0],[0]
"Once we see enough samples, p̂e converges to pe.",5.1. Learning the Diffusion Probabilities,[0],[0]
"One can also notice, the effect of omniscience since it leads to faster convergence than actual implementable non-hyperparametric methods.
",5.1. Learning the Diffusion Probabilities,[0],[0]
"Regarding the two benchmarks that involve the hyperparametric assumption it is worth noting that reduced information does not allow convergence to 0 error, while augmented
information does (see also Figure 4d for a more detailed investigation).",5.1. Learning the Diffusion Probabilities,[0],[0]
"Finally, the initial difference in the errors of different networks is related to how good predictor the initialization of SGD is (i.e. θ = 0), as well as how large the average diffusion probability in the network is.",5.1. Learning the Diffusion Probabilities,[0],[0]
"The learning effect is evident and universal though, since the error converges to 0 independently of the underlying network.",5.1. Learning the Diffusion Probabilities,[0],[0]
"In these experiments we use Erdös-Rényi graphs with 1000 nodes and 20000 edges (unless otherwise stated).
",5.2. Convergence Rate Investigation,[0],[0]
Samples vs Concavity.,5.2. Convergence Rate Investigation,[0],[0]
In Section 4 we classified the samples into categories based on whether they yield concave log-likelihood or not.,5.2. Convergence Rate Investigation,[0],[0]
"Recall that if we ignore the obfuscated samples, the optimization problem becomes concave.",5.2. Convergence Rate Investigation,[0],[0]
A natural question is whether sacrificing samples for concavity leads to faster convergence.,5.2. Convergence Rate Investigation,[0],[0]
"To this end, we generate samples and if a sample is obfuscated, we discard it with probability p ∈ {0.0, 0.25, 0.5, 0.75, 1.0}.",5.2. Convergence Rate Investigation,[0],[0]
The results can be found in Figure 4a.,5.2. Convergence Rate Investigation,[0],[0]
"It is evident that even though our optimization problem becomes concave and hence theoretically easier to solve, the price due to data shortage is huge.
",5.2. Convergence Rate Investigation,[0],[0]
Approximate models.,5.2. Convergence Rate Investigation,[0],[0]
Here we investigate how properties of the graph or the model affect the convergence rate.,5.2. Convergence Rate Investigation,[0],[0]
"• Graph Density: We create an Erdös-Rényi graph with
1000 nodes and varying number of edges, exploring how does graph density affect the convergence of the error.",5.2. Convergence Rate Investigation,[0],[0]
"As the density increases, so does the average degree in the network and, as a result, the number of obfuscated samples.",5.2. Convergence Rate Investigation,[0],[0]
"Hence, the information obtained becomes “noisier” and convergence is slower.",5.2. Convergence Rate Investigation,[0],[0]
• Noisy model: Until now we assumed that the hyperparametric model is the ground truth.,5.2. Convergence Rate Investigation,[0],[0]
Here we relax this assumption.,5.2. Convergence Rate Investigation,[0],[0]
"We generate each edge probability as before, and subsequently add noise uniform in [−N,N ], for increasing values ofN .",5.2. Convergence Rate Investigation,[0],[0]
"Now, the average error does not converge to 0 and increases with N .",5.2. Convergence Rate Investigation,[0],[0]
•,5.2. Convergence Rate Investigation,[0],[0]
"Features Effect: In many cases we might not know the exact features that support the hyperparametric model.
",5.2. Convergence Rate Investigation,[0],[0]
We explore the effect of this lack of information by including varying number of significant features in our model.,5.2. Convergence Rate Investigation,[0],[0]
"Our results show that in terms of convergence more information does not hurt, despite being more costly computationally.",5.2. Convergence Rate Investigation,[0],[0]
"However, if we fail to include all the significant features, we do not converge to 0 error, and the error grows with the removal of features.
",5.2. Convergence Rate Investigation,[0],[0]
"The results can be found in Figure 4b-d. In all the cases the way that we enforced the hyperparametric assumption, created the samples and ran SGD is the same as in Section 5.1.",5.2. Convergence Rate Investigation,[0],[0]
"Importantly, we evaluate the validity of the hyperparametric assumption on real cascade data.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"To this end, we use the following aggregated and public Facebook data sets containing only de-identified data (i.e. they don’t include personally identifying information about individuals in the dataset).
Events.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"In Facebook a user can invite a set of other users for an event, who can then forward the invite to their friends to join.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Moreover, when friends join an event a user may be notified in their Facebook feed and join in turn.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
The cascade in this scenario is an event and an exposure is either a direct invite or a feed notification.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
A user is influenced if she marked herself as “going” to the event.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
This dataset is a random sample of events that happened over a twomonth period in late 2017.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
We have included only public events that are visible to everyone and also excluded users who created the event or joined without being invited.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"The number of cascades in our dataset is roughly 3 million with 90 million users participating and 130 million exposures.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
Video and Photo Reshares.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
A cascade in this dataset is a video or photo content.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
Whenever a user watches a video (photo) posted from a friend we consider it as an exposure and when the user shares that video (photo) after watching we consider it as an adoption (we consider only videos that were explicitly seen and not auto-played).,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"We collected a random sample of photo/video data on a random day in January 2018, and included only public photo and video
posts.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Our data sets contain roughly 10 million cascades with more than 100 million users and 500 million exposures.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
Experimental setup.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"The features that we include in our model in both cases are user attributes such as Facebook age in days, friend count, number of initiated friendship requests, subscriber count and subscription count, city, country, and language, number of days active in last 7 days, and 28 days.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
The categorical features were binarized in the model.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
An important difference with the experiments of Sections 5.1 and 5.2 is that here we don’t know the true diffusion probability of every edge by construction.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Instead, we estimate it from samples as p̂e = n+e ne
.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"In order to estimate p̂e accurately, we need enough samples for edge e. Hence, we restrict our evaluation set (the set of edges where we measure the error) only to edges that have at least 67 interactions, meaning that |pe − p̂e| ≤ 0.15 with probability at least 90%.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Each experiment is repeated 50 times and the averages together with the standard deviations are reported in Figures 5 and 6.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
Results.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
Our first set of experiments is to validate the hyperparametric assumption in real data.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"We observe that using the optimization problem (2), with very few samples, the hyperparametric model achieves significant reduction in average error (up to 60%) over methods that don’t utilize node features.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"The results, reported in Fig. 5, are consistent with our synthetic experiments, where the hyperparametric assumption holds by construction.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Note that the non-hyperparametric methods will eventually converge to zero error as they correspond to the ground truth while the hyperparametric model is only a good approximation of it.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
We also included reduced and augmented hyperparametric models for comparison as in the synthetic experiments.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
In the case of the reduced model we used only 20% of the most important features of each edge (measured using Mutual Information).,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"For the augmented version on the other hand, we augment the feature vector of each node with redundant information (increase its dimension by 50% and fill the extra coordinates with random noise) and investigate whether convergence still occurs.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"As in the experiments of Section 5.1, the reduced model converges to higher average error than the models that use more information, while the augmented model successfully ignores all the redundant features.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
We also evaluated the sensitivity of the hyperparametric model when we include all versus few selected features.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"The picture that we see matches the synthetic experiments (Figure 4d), i.e. the hyperparametric model is supported on several features and if we fail to include all of them our error won’t converge to 0.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"However, an important difference with the synthetic experiments is that here not all the features are equally important, hence by applying feature-selection algorithms we can collect a small subset that performs almost as well as using the entire feature vector (see e.g. the difference in the error between 20% and 90% of the features).",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"This research was supported by NSF grant CAREER CCF1452961, BSF grant 2014389, NSF USICCS proposal 1540428, and a Facebook research award.",6. Acknowledgements,[0],[0]
In this paper we advocate for a hyperparametric approach to learn diffusion in the independent cascade (IC) model.,abstractText,[0],[0]
The sample complexity of this model is a function of the number of edges in the network and consequently learning becomes infeasible when the network is large.,abstractText,[0],[0]
We study a natural restriction of the hypothesis class using additional information available in order to dramatically reduce the sample complexity of the learning process.,abstractText,[0],[0]
In particular we assume that diffusion probabilities can be described as a function of a global hyperparameter and features of the individuals in the network.,abstractText,[0],[0]
One of the main challenges with this approach is that training a model reduces to optimizing a non-convex objective.,abstractText,[0],[0]
"Despite this obstacle, we can shrink the best-known sample complexity bound for learning IC by a factor of |E|/d where |E| is the number of edges in the graph and d is the dimension of the hyperparameter.",abstractText,[0],[0]
We show that under mild assumptions about the distribution generating the samples one can provably train a model with low generalization error.,abstractText,[0],[0]
"Finally, we use large-scale diffusion data from Facebook to show that a hyperparametric model using approximately 20 features per node achieves remarkably high accuracy.",abstractText,[0],[0]
Learning Diffusion using Hyperparameters,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 654–664 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1061",text,[0],[0]
"The dialog manager is one of the key components of dialog systems, which is responsible for modeling the decision-making process.",1 Introduction,[0],[0]
"Specifically, it typically takes a new utterance and the dialog context as input, and generates discourse-level decisions (Bohus and Rudnicky, 2003; Williams and Young, 2007).",1 Introduction,[0],[0]
"Advanced dialog managers usually have a list of potential actions that enable them to have diverse behavior during a conversation, e.g. different strategies to recover from non-understanding (Yu et al., 2016).",1 Introduction,[0],[0]
"However, the conventional approach of designing a dialog manager (Williams and Young, 2007) does not
scale well to open-domain conversation models because of the vast quantity of possible decisions.",1 Introduction,[0],[0]
"Thus, there has been a growing interest in applying encoder-decoder models (Sutskever et al., 2014) for modeling open-domain conversation (Vinyals and Le, 2015; Serban et al., 2016a).",1 Introduction,[0],[0]
"The basic approach treats a conversation as a transduction task, in which the dialog history is the source sequence and the next response is the target sequence.",1 Introduction,[0],[0]
"The model is then trained end-to-end on large conversation corpora using the maximum-likelihood estimation (MLE) objective without the need for manual crafting.
",1 Introduction,[0],[0]
"However recent research has found that encoder-decoder models tend to generate generic and dull responses (e.g., I don’t know), rather than meaningful and specific answers (Li et al., 2015; Serban et al., 2016b).",1 Introduction,[0],[0]
"There have been many attempts to explain and solve this limitation, and they can be broadly divided into two categories (see Section 2 for details): (1) the first category argues that the dialog history is only one of the factors that decide the next response.",1 Introduction,[0],[0]
"Other features should be extracted and provided to the models as conditionals in order to generate more specific responses (Xing et al., 2016; Li et al., 2016a); (2) the second category aims to improve the encoder-decoder model itself, including decoding with beam search and its variations (Wiseman and Rush, 2016), encouraging responses that have long-term payoff (Li et al., 2016b), etc.
",1 Introduction,[0],[0]
"Building upon the past work in dialog managers and encoder-decoder models, the key idea of this paper is to model dialogs as a one-to-many problem at the discourse level.",1 Introduction,[0],[0]
"Previous studies indicate that there are many factors in open-domain dialogs that decide the next response, and it is nontrivial to extract all of them.",1 Introduction,[0],[0]
"Intuitively, given a similar dialog history (and other observed inputs), there may exist many valid responses (at the
654
discourse-level), each corresponding to a certain configuration of the latent variables that are not presented in the input.",1 Introduction,[0],[0]
"To uncover the potential responses, we strive to model a probabilistic distribution over the distributed utterance embeddings of the potential responses using a latent variable (Figure 1).",1 Introduction,[0],[0]
"This allows us to generate diverse responses by drawing samples from the learned distribution and reconstruct their words via a decoder neural network.
",1 Introduction,[0],[0]
"Specifically, our contributions are three-fold: 1.",1 Introduction,[0],[0]
"We present a novel neural dialog model adapted from conditional variational autoencoders (CVAE) (Yan et al., 2015; Sohn et al., 2015), which introduces a latent variable that can capture discourse-level variations as described above 2.",1 Introduction,[0],[0]
"We propose Knowledge-Guided CVAE (kgCVAE), which enables easy integration of expert knowledge and results in performance improvement and model interpretability.",1 Introduction,[0],[0]
3.,1 Introduction,[0],[0]
"We develop a training method in addressing the difficulty of optimizing CVAE for natural language generation (Bowman et al., 2015).",1 Introduction,[0],[0]
"We evaluate our models on human-human conversation data and yield promising results in: (a) generating appropriate and discourse-level diverse responses, and (b) showing that the proposed training method is more effective than the previous techniques.",1 Introduction,[0],[0]
Our work is related to both recent advancement in encoder-decoder dialog models and generative models based on CVAE.,2 Related Work,[0],[0]
"Since the emergence of the neural dialog model, the problem of output diversity has received much attention in the research community.",2.1 Encoder-decoder Dialog Models,[0],[0]
Ideal output responses should be both coherent and diverse.,2.1 Encoder-decoder Dialog Models,[0],[0]
"However, most models end up with generic and dull responses.",2.1 Encoder-decoder Dialog Models,[0],[0]
"To tackle this problem, one line of research has focused on augmenting the input of encoder-decoder models with richer context information, in order to generate more spe-
cific responses.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Li et al., (2016a) captured speakers’ characteristics by encoding background information and speaking style into the distributed embeddings, which are used to re-rank the generated response from an encoder-decoder model.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Xing et al., (2016) maintain topic encoding based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) of the conversation to encourage the model to output more topic coherent responses.
",2.1 Encoder-decoder Dialog Models,[0],[0]
"On the other hand, many attempts have also been made to improve the architecture of encoderdecoder models.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Li et al,.",2.1 Encoder-decoder Dialog Models,[0],[0]
"(2015) proposed to optimize the standard encoder-decoder by maximizing the mutual information between input and output, which in turn reduces generic responses.",2.1 Encoder-decoder Dialog Models,[0],[0]
"This approach penalized unconditionally high frequency responses, and favored responses that have high conditional probability given the input.",2.1 Encoder-decoder Dialog Models,[0],[0]
Wiseman and Rush (2016) focused on improving the decoder network by alleviating the biases between training and testing.,2.1 Encoder-decoder Dialog Models,[0],[0]
They introduced a searchbased loss that directly optimizes the networks for beam search decoding.,2.1 Encoder-decoder Dialog Models,[0],[0]
"The resulting model achieves better performance on word ordering, parsing and machine translation.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Besides improving beam search, Li et al., (2016b) pointed out that the MLE objective of an encoder-decoder model is unable to approximate the real-world goal of the conversation.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Thus, they initialized a encoderdecoder model with MLE objective and leveraged reinforcement learning to fine tune the model by optimizing three heuristic rewards functions: informativity, coherence, and ease of answering.",2.1 Encoder-decoder Dialog Models,[0],[0]
"The variational autoencoder (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) is one of the most popular frameworks for image generation.",2.2 Conditional Variational Autoencoder,[0],[0]
The basic idea of VAE is to encode the input x into a probability distribution z instead of a point encoding in the autoencoder.,2.2 Conditional Variational Autoencoder,[0],[0]
"Then VAE applies a decoder network to reconstruct the original input using samples from z. To generate images, VAE first obtains a sample of z from the prior distribution, e.g. N (0, I), and then produces an image via the decoder network.",2.2 Conditional Variational Autoencoder,[0],[0]
"A more advanced model, the conditional VAE (CVAE), is a recent modification of VAE to generate diverse images conditioned on certain attributes, e.g. generating different human faces given skin color (Yan et al., 2015; Sohn et al., 2015).",2.2 Conditional Variational Autoencoder,[0],[0]
"Inspired by CVAE, we view the dialog contexts as the conditional attributes and adapt CVAE
to generate diverse responses instead of images.",2.2 Conditional Variational Autoencoder,[0],[0]
"Although VAE/CVAE has achieved impressive results in image generation, adapting this to natural language generators is non-trivial.",2.2 Conditional Variational Autoencoder,[0],[0]
"Bowman et al., (2015) have used VAE with Long-Short Term Memory (LSTM)-based recognition and decoder networks to generate sentences from a latent Gaussian variable.",2.2 Conditional Variational Autoencoder,[0],[0]
They showed that their model is able to generate diverse sentences with even a greedy LSTM decoder.,2.2 Conditional Variational Autoencoder,[0],[0]
They also reported the difficulty of training because the LSTM decoder tends to ignore the latent variable.,2.2 Conditional Variational Autoencoder,[0],[0]
We refer to this issue as the vanishing latent variable problem.,2.2 Conditional Variational Autoencoder,[0],[0]
"Serban et al., (2016b) have applied a latent variable hierarchical encoder-decoder dialog model to introduce utterance-level variations and facilitate longer responses.",2.2 Conditional Variational Autoencoder,[0],[0]
"To improve upon the past models, we firstly introduce a novel mechanism to leverage linguistic knowledge in training end-to-end neural dialog models, and we also propose a novel training technique that mitigates the vanishing latent variable problem.",2.2 Conditional Variational Autoencoder,[0],[0]
"Each dyadic conversation can be represented via three random variables: the dialog context c (context window size k − 1), the response utterance x (the kth utterance) and a latent variable z, which is used to capture the latent distribution over the valid responses.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Further, c is composed of the dialog history: the preceding k-1 utterances; conversational floor (1 if the utterance is from the same speaker of x, otherwise 0) and meta features m (e.g. the topic).",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"We then define the conditional distribution p(x, z|c)",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"= p(x|z, c)p(z|c) and our goal is to use deep neural networks (parametrized by θ) to approximate p(z|c) and p(x|z, c).",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"We refer to pθ(z|c) as the prior network and pθ(x, |z, c) as the
response decoder.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Then the generative process of x is (Figure 2 (a)):
1.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Sample a latent variable z from the prior network pθ(z|c).
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
2.,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Generate x through the response decoder pθ(x|z, c).
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"CVAE is trained to maximize the conditional log likelihood of x given c, which involves an intractable marginalization over the latent variable z. As proposed in (Sohn et al., 2015; Yan et al., 2015), CVAE can be efficiently trained with the Stochastic Gradient Variational Bayes (SGVB) framework (Kingma and Welling, 2013) by maximizing the variational lower bound of the conditional log likelihood.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"We assume the z follows multivariate Gaussian distribution with a diagonal covariance matrix and introduce a recognition network qφ(z|x, c) to approximate the true posterior distribution p(z|x, c).",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Sohn and et al,.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"(2015) have shown that the variational lower bound can be written as:
L(θ, φ;x, c) = −KL(qφ(z|x, c)‖pθ(z|c))",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"+Eqφ(z|c,x)[log pθ(x|z, c)] (1) ≤ log p(x|c)
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
Figure 3 demonstrates an overview of our model.,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"The utterance encoder is a bidirectional recurrent neural network (BRNN) (Schuster and Paliwal, 1997) with a gated recurrent unit (GRU) (Chung et al., 2014) to encode each utterance into fixedsize vectors by concatenating the last hidden states of the forward and backward RNN ui =",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"[~hi, ~hi]. x is simply uk.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
The context encoder is a 1-layer GRU network that encodes the preceding k-1 utterances by taking u1:k−1 and the corresponding conversation floor as inputs.,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
The last hidden state hc of the context encoder is concatenated with meta features and c =,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"[hc,m].",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Since we assume z follows isotropic Gaussian distribution, the recognition network qφ(z|x, c) ∼ N (µ, σ2I) and the prior network pθ(z|c) ∼ N (µ′, σ′2I), and then we have:
[ µ
log(σ2)
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"] =Wr [ x c ] + br (2)
[ µ′
log(σ′2)
] = MLPp(c) (3)
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"We then use the reparametrization trick (Kingma and Welling, 2013) to obtain samples of z either
from N (z;µ, σ2I) predicted by the recognition network (training) or N (z;µ′, σ′2I) predicted by the prior network (testing).",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Finally, the response decoder is a 1-layer GRU network with initial state s0 =Wi[z, c]+bi.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
The response decoder then predicts the words in x sequentially.,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"In practice, training CVAE is a challenging optimization problem and often requires large amount of data.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"On the other hand, past research in spoken dialog systems and discourse analysis has suggested that many linguistic cues capture crucial features in representing natural conversation.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"For example, dialog acts (Poesio and Traum, 1998) have been widely used in the dialog managers (Litman and Allen, 1987; Raux et al., 2005; Zhao and Eskenazi, 2016) to represent the propositional function of the system.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"Therefore, we conjecture that it will be beneficial for the model to learn meaningful latent z",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"if it is provided with explicitly extracted discourse features during the training.
",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"In order to incorporate the linguistic features into the basic CVAE model, we first denote the set of linguistic features as y. Then we assume that the generation of x depends on c, z and y. y relies on z and c as shown in Figure 2.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"Specifically, during training the initial state of the response decoder is s0 = Wi[z, c, y] + bi and the input at every step is [et, y] where et is the word embedding of tth word in x.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"In addition, there is an MLP to predict y′ = MLPy(z, c) based on z and c.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"In the testing stage, the predicted y′ is used by the response decoder instead of the oracle decoders.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"We denote the modified model as knowledge-guided
CVAE (kgCVAE) and developers can add desired discourse features that they wish the latent variable z to capture.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"KgCVAE model is trained by maximizing:
L(θ, φ;x, c, y) = −KL(qφ(z|x, c, y)‖Pθ(z|c))",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
+,3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"Eqφ(z|c,x,y)[log p(x|z, c, y)]",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"+Eqφ(z|c,x,y)[log p(y|z, c)] (4)
Since now the reconstruction of y is a part of the loss function, kgCVAE can more efficiently encode y-related information into z than discovering it only based on the surface-level x and c. Another advantage of kgCVAE is that it can output a highlevel label (e.g. dialog act) along with the wordlevel responses, which allows easier interpretation of the model’s outputs.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"A straightforward VAE with RNN decoder fails to encode meaningful information in z due to the vanishing latent variable problem (Bowman et al., 2015).",3.3 Optimization Challenges,[0],[0]
"Bowman et al., (2015) proposed two solutions: (1) KL annealing: gradually increasing the weight of the KL term from 0 to 1 during training; (2) word drop decoding: setting a certain percentage of the target words to 0.",3.3 Optimization Challenges,[0],[0]
We found that CVAE suffers from the same issue when the decoder is an RNN.,3.3 Optimization Challenges,[0],[0]
"Also we did not consider word drop decoding because Bowman et al,.",3.3 Optimization Challenges,[0],[0]
"(2015) have shown that it may hurt the performance when the drop rate is too high.
",3.3 Optimization Challenges,[0],[0]
"As a result, we propose a simple yet novel technique to tackle the vanishing latent variable problem: bag-of-word loss.",3.3 Optimization Challenges,[0],[0]
"The idea is to introduce
an auxiliary loss that requires the decoder network to predict the bag-of-words in the response x as shown in Figure 3(b).",3.3 Optimization Challenges,[0],[0]
"We decompose x into two variables: xo with word order and xbow without order, and assume that xo and xbow are conditionally independent given z and c: p(x, z|c)",3.3 Optimization Challenges,[0],[0]
"= p(xo|z, c)p(xbow|z, c)p(z|c).",3.3 Optimization Challenges,[0],[0]
"Due to the conditional independence assumption, the latent variable is forced to capture global information about the target response.",3.3 Optimization Challenges,[0],[0]
"Let f = MLPb(z, x) ∈ RV where V is vocabulary size, and we have:
log p(xbow|z, c) = log |x|∏
t=1
efxt ∑V
j e fj
(5)
where |x| is the length of x and xt is the word index of tth word in x.",3.3 Optimization Challenges,[0],[0]
"The modified variational lower bound for CVAE with bag-of-word loss is (see Appendix A for kgCVAE):
L′(θ, φ;x, c) = L(θ, φ;x, c) +",3.3 Optimization Challenges,[0],[0]
"Eqφ(z|c,x,y)[log p(xbow|z, c)]",3.3 Optimization Challenges,[0],[0]
"(6)
We will show that the bag-of-word loss in Equation 6 is very effective against the vanishing latent variable and it is also complementary to the KL annealing technique.",3.3 Optimization Challenges,[0],[0]
We chose the Switchboard (SW) 1,4.1 Dataset,[0],[0]
"Release 2 Corpus (Godfrey and Holliman, 1997) to evaluate the proposed models.",4.1 Dataset,[0],[0]
SW has 2400 two-sided telephone conversations with manually transcribed speech and alignment.,4.1 Dataset,[0],[0]
"In the beginning of the call, a computer operator gave the callers recorded prompts that define the desired topic of discussion.",4.1 Dataset,[0],[0]
There are 70 available topics.,4.1 Dataset,[0],[0]
We randomly split the data into 2316/60/62 dialogs for train/validate/test.,4.1 Dataset,[0],[0]
"The pre-processing includes (1) tokenize using the NLTK tokenizer (Bird et al., 2009); (2) remove non-verbal symbols and repeated words due to false starts; (3) keep the top 10K frequent word types as the vocabulary.",4.1 Dataset,[0],[0]
"The final data have 207, 833/5, 225/5, 481 (c, x) pairs for train/validate/test.",4.1 Dataset,[0],[0]
"Furthermore, a subset of SW was manually labeled with dialog acts (Stolcke et al., 2000).",4.1 Dataset,[0],[0]
"We extracted dialog act labels based on the dialog act recognizer proposed in (Ribeiro et al., 2015).",4.1 Dataset,[0],[0]
"The features include the uni-gram and bi-gram of the utterance, and the contextual features of the last 3 utterances.",4.1 Dataset,[0],[0]
"We trained a Support Vector Machine
(SVM) (Suykens and Vandewalle, 1999) with linear kernel on the subset of SW with human annotations.",4.1 Dataset,[0],[0]
There are 42 types of dialog acts and the SVM achieved 77.3% accuracy on held-out data.,4.1 Dataset,[0],[0]
Then the rest of SW data are labelled with dialog acts using the trained SVM dialog act recognizer.,4.1 Dataset,[0],[0]
We trained with the following hyperparameters (according to the loss on the validate dataset): word embedding has size 200 and is shared across everywhere.,4.2 Training,[0],[0]
"We initialize the word embedding from Glove embedding pre-trained on Twitter (Pennington et al., 2014).",4.2 Training,[0],[0]
The utterance encoder has a hidden size of 300 for each direction.,4.2 Training,[0],[0]
The context encoder has a hidden size of 600 and the response decoder has a hidden size of 400.,4.2 Training,[0],[0]
The prior network and the MLP for predicting y both have 1 hidden layer of size 400 and tanh non-linearity.,4.2 Training,[0],[0]
The latent variable z has a size of 200.,4.2 Training,[0],[0]
The context window k is 10.,4.2 Training,[0],[0]
"All the initial weights are sampled from a uniform distribution [-0.08, 0.08].",4.2 Training,[0],[0]
The mini-batch size is 30.,4.2 Training,[0],[0]
"The models are trained end-to-end using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.001 and gradient clipping at 5.",4.2 Training,[0],[0]
We selected the best models based on the variational lower bound on the validate data.,4.2 Training,[0],[0]
"Finally, we use the BOW loss along with KL annealing of 10,000 batches to achieve the best performance.",4.2 Training,[0],[0]
Section 5.4 gives a detailed argument for the importance of the BOW loss.,4.2 Training,[0],[0]
"We compared three neural dialog models: a strong baseline model, CVAE, and kgCVAE.",5.1 Experiments Setup,[0],[0]
"The baseline model is an encoder-decoder neural dialog model without latent variables similar to (Serban et al., 2016a).",5.1 Experiments Setup,[0],[0]
The baseline model’s encoder uses the same context encoder to encode the dialog history and the meta features as shown in Figure 3.,5.1 Experiments Setup,[0],[0]
The encoded context c is directly fed into the decoder networks as the initial state.,5.1 Experiments Setup,[0],[0]
"The hyperparameters of the baseline are the same as the ones reported in Section 4.2 and the baseline is trained to minimize the standard cross entropy loss of the decoder RNN model without any auxiliary loss.
",5.1 Experiments Setup,[0],[0]
"Also, to compare the diversity introduced by the stochasticity in the proposed latent variable versus the softmax of RNN at each decoding step, we generate N responses from the baseline by sam-
pling from the softmax.",5.1 Experiments Setup,[0],[0]
"For CVAE/kgCVAE, we sample N times from the latent z and only use greedy decoders so that the randomness comes entirely from the latent variable z.",5.1 Experiments Setup,[0],[0]
"Automatically evaluating an open-domain generative dialog model is an open research challenge (Liu et al., 2016).",5.2 Quantitative Analysis,[0],[0]
"Following our one-tomany hypothesis, we propose the following metrics.",5.2 Quantitative Analysis,[0],[0]
"We assume that for a given dialog context c, there existMc reference responses rj , j ∈",5.2 Quantitative Analysis,[0],[0]
"[1,Mc].",5.2 Quantitative Analysis,[0],[0]
Meanwhile a model can generateN hypothesis responses,5.2 Quantitative Analysis,[0],[0]
"hi, i ∈",5.2 Quantitative Analysis,[0],[0]
"[1, N ].",5.2 Quantitative Analysis,[0],[0]
"The generalized responselevel precision/recall for a given dialog context is:
precision(c) =
∑N i=1maxj∈[1,Mc]d(rj , hi)
N
recall(c) =
∑Mc j=1maxi∈[1,N ]d(rj , hi))
",5.2 Quantitative Analysis,[0],[0]
"Mc
where d(rj , hi) is a distance function which lies between 0 to 1 and measures the similarities between rj and hi.",5.2 Quantitative Analysis,[0],[0]
"The final score is averaged over the entire test dataset and we report the performance with 3 types of distance functions in order to evaluate the systems from various linguistic points of view:
1.",5.2 Quantitative Analysis,[0],[0]
"Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified ngram precision with a length penalty (Papineni et al., 2002; Li et al., 2015).",5.2 Quantitative Analysis,[0],[0]
"We use BLEU-1 to 4 as our lexical similarity metric and normalize the score to 0 to 1 scale.
2.",5.2 Quantitative Analysis,[0],[0]
"Cosine Distance of Bag-of-word Embedding: a simple method to obtain sentence embeddings is to take the average or extrema of all the word embeddings in the sentences (Forgues et al., 2014; Adi et al., 2016).",5.2 Quantitative Analysis,[0],[0]
"The d(rj , hi) is the cosine distance of the two embedding vectors.",5.2 Quantitative Analysis,[0],[0]
"We used Glove embedding described in Section 4 and denote the average method as A-bow and extrema method as E-bow.
3.",5.2 Quantitative Analysis,[0],[0]
"Dialog Act Match: to measure the similarity at the discourse level, the same dialogact tagger from 4.1 is applied to label all the generated responses of each model.",5.2 Quantitative Analysis,[0],[0]
"We set d(rj , hi) = 1 if rj and hi have the same dialog acts, otherwise d(rj , hi) = 0.
",5.2 Quantitative Analysis,[0],[0]
"One challenge of using the above metrics is that there is only one, rather than multiple reference responses/contexts.",5.2 Quantitative Analysis,[0],[0]
This impacts reliability of our measures.,5.2 Quantitative Analysis,[0],[0]
"Inspired by (Sordoni et al., 2015), we utilized information retrieval techniques (see Appendix A) to gather 10 extra candidate reference responses/context from other conversations with the same topics.",5.2 Quantitative Analysis,[0],[0]
"Then the 10 candidate references are filtered by two experts, which serve as the ground truth to train the reference response classifier.",5.2 Quantitative Analysis,[0],[0]
The result is 6.69 extra references in average per context.,5.2 Quantitative Analysis,[0],[0]
The average number of distinct reference dialog acts is 4.2.,5.2 Quantitative Analysis,[0],[0]
"Table 1 shows the results.
",5.2 Quantitative Analysis,[0],[0]
The proposed models outperform the baseline in terms of recall in all the metrics with statistical significance.,5.2 Quantitative Analysis,[0],[0]
This confirms our hypothesis that generating responses with discourse-level diversity can lead to a more comprehensive coverage of the potential responses than promoting only word-level diversity.,5.2 Quantitative Analysis,[0],[0]
"As for precision, we observed that the baseline has higher or similar scores than CVAE in all metrics, which is expected since the baseline tends to generate the mostly likely and safe responses repeatedly in the N hypotheses.",5.2 Quantitative Analysis,[0],[0]
"However, kgCVAE is able to achieve the highest precision and recall in the 4 metrics at the same time (BLEU1-4, E-BOW).",5.2 Quantitative Analysis,[0],[0]
"One reason
for kgCVAE’s good performance is that the predicted dialog act label in kgCVAE can regularize the generation process of its RNN decoder by forcing it to generate more coherent and precise words.",5.2 Quantitative Analysis,[0],[0]
We further analyze the precision/recall of BLEU4 by looking at the average score versus the number of distinct reference dialog acts.,5.2 Quantitative Analysis,[0],[0]
"A low number of distinct dialog acts represents the situation where the dialog context has a strong constraint on the range of the next response (low entropy), while a high number indicates the opposite (highentropy).",5.2 Quantitative Analysis,[0],[0]
Figure 4 shows that CVAE/kgCVAE achieves significantly higher recall than the baseline in higher entropy contexts.,5.2 Quantitative Analysis,[0],[0]
"Also it shows that CVAE suffers from lower precision, especially in low entropy contexts.",5.2 Quantitative Analysis,[0],[0]
"Finally, kgCVAE gets higher precision than both the baseline and CVAE in the full spectrum of context entropy.",5.2 Quantitative Analysis,[0],[0]
Table 2 shows the outputs generated from the baseline and kgCVAE.,5.3 Qualitative Analysis,[0],[0]
"In example 1, caller A begins with an open-ended question.",5.3 Qualitative Analysis,[0],[0]
The kgCVAE model generated highly diverse answers that cover multiple plausible dialog acts.,5.3 Qualitative Analysis,[0],[0]
"Further, we notice that the generated text exhibits similar dialog acts compared to the ones predicted separately by the model, implying the consistency of natural language generation based on y.",5.3 Qualitative Analysis,[0],[0]
"On the contrary, the responses from the baseline model are limited to local n-gram variations and share a similar prefix, i.e. ”I’m”.",5.3 Qualitative Analysis,[0],[0]
Example 2 is a situation where caller A is telling B stories.,5.3 Qualitative Analysis,[0],[0]
The ground truth response is a back-channel and the range of valid answers is more constrained than example 1 since B is playing the role of a listener.,5.3 Qualitative Analysis,[0],[0]
The baseline successfully predicts ”uh-huh”.,5.3 Qualitative Analysis,[0],[0]
The kgCVAE model is also able to generate various ways of back-channeling.,5.3 Qualitative Analysis,[0],[0]
"This implies that the latent z is able to capture context-sensitive variations, i.e. in low-entropy dialog contexts modeling lexical diversity while in high-entropy ones modeling discourse-level diversity.",5.3 Qualitative Analysis,[0],[0]
"Moreover, kgCVAE is occasionally able to
generate more sophisticated grounding (sample 4) beyond a simple back-channel, which is also an acceptable response given the dialog context.
",5.3 Qualitative Analysis,[0],[0]
"In addition, past work (Kingma and Welling, 2013) has shown that the recognition network is able to learn to cluster high-dimension data, so we conjecture that posterior z outputted from the recognition network should cluster the responses into meaningful groups.",5.3 Qualitative Analysis,[0],[0]
"Figure 5 visualizes the posterior z of responses in the test dataset in 2D space using t-SNE (Maaten and Hinton, 2008).",5.3 Qualitative Analysis,[0],[0]
"We found that the learned latent space is highly correlated with the dialog act and length of responses, which confirms our assumption.",5.3 Qualitative Analysis,[0],[0]
"Finally, we evaluate the effectiveness of bag-ofword (BOW) loss for training VAE/CVAE with the RNN decoder.",5.4 Results for Bag-of-Word Loss,[0],[0]
"To compare with past work (Bowman et al., 2015), we conducted the same language modelling (LM) task on Penn Treebank using VAE.",5.4 Results for Bag-of-Word Loss,[0],[0]
The network architecture is same except we use GRU instead of LSTM.,5.4 Results for Bag-of-Word Loss,[0],[0]
We compared four different training setups: (1) standard VAE without any heuristics; (2) VAE with KL annealing (KLA); (3) VAE with BOW loss; (4) VAE with both BOW loss and KLA.,5.4 Results for Bag-of-Word Loss,[0],[0]
"Intuitively, a well trained model should lead to a low reconstruction loss and small but non-trivial KL cost.",5.4 Results for Bag-of-Word Loss,[0],[0]
"For all models with KLA, the KL weight increases linearly from 0 to 1 in the first 5000 batches.
",5.4 Results for Bag-of-Word Loss,[0],[0]
Table 3 shows the reconstruction perplexity and the KL cost on the test dataset.,5.4 Results for Bag-of-Word Loss,[0],[0]
"The standard VAE fails to learn a meaningful latent variable by hav-
ing a KL cost close to 0 and a reconstruction perplexity similar to a small LSTM LM (Zaremba et al., 2014).",5.4 Results for Bag-of-Word Loss,[0],[0]
"KLA helps to improve the reconstruction loss, but it requires early stopping since the models will fall back to the standard VAE after the KL weight becomes 1.",5.4 Results for Bag-of-Word Loss,[0],[0]
"At last, the models with BOW loss achieved significantly lower perplexity and larger KL cost.
",5.4 Results for Bag-of-Word Loss,[0],[0]
Figure 6 visualizes the evolution of the KL cost.,5.4 Results for Bag-of-Word Loss,[0],[0]
"We can see that for the standard model, the KL cost crashes to 0 at the beginning of training and never recovers.",5.4 Results for Bag-of-Word Loss,[0],[0]
"On the contrary, the model with only KLA learns to encode substantial information in latent z when the KL cost weight is small.",5.4 Results for Bag-of-Word Loss,[0],[0]
"However, after the KL weight is increased to 1 (after 5000 batch), the model once again decides to ignore the latent z and falls back to the naive implementation.",5.4 Results for Bag-of-Word Loss,[0],[0]
"The model with BOW loss, however, consistently converges to a non-trivial KL cost even without KLA, which confirms the importance of BOW loss for training latent variable models with the RNN decoder.",5.4 Results for Bag-of-Word Loss,[0],[0]
"Last but not least, our experiments showed that the conclusions drawn from LM using VAE also apply to training CVAE/kgCVAE, so we used BOW loss together with KLA for all previous experiments.",5.4 Results for Bag-of-Word Loss,[0],[0]
"In conclusion, we identified the one-to-many nature of open-domain conversation and proposed two novel models that show superior performance in generating diverse and appropriate responses at the discourse level.",6 Conclusion and Future Work,[0],[0]
"While the current paper addresses diversifying responses in respect to dialogue acts, this work is part of a larger research direction that targets leveraging both past linguistic findings and the learning power of deep neural networks to learn better representation of the latent factors in dialog.",6 Conclusion and Future Work,[0],[0]
"In turn, the output of this novel neural dialog model will be easier to explain and control by humans.",6 Conclusion and Future Work,[0],[0]
"In addition to dialog acts, we plan to apply our kgCVAE model to capture other different linguistic phenomena including sentiment, named entities,etc.",6 Conclusion and Future Work,[0],[0]
"Last but not least, the recognition network in our model will serve as the foundation for designing a datadriven dialog manager, which automatically discovers useful high-level intents.",6 Conclusion and Future Work,[0],[0]
All of the above suggest a promising research direction.,6 Conclusion and Future Work,[0],[0]
"Variational Lower Bound for kgCVAE We assume that even with the presence of linguistic feature y regarding x, the prediction of xbow still only depends on the z and c. Therefore, we have:
L(θ, φ;x, c, y) = −KL(qφ(z|x, c, y)‖Pθ(z|c))",A Supplemental Material,[0],[0]
+,A Supplemental Material,[0],[0]
"Eqφ(z|c,x,y)[log p(x|z, c, y)]",A Supplemental Material,[0],[0]
"+Eqφ(z|c,x,y)[log p(y|z, c)]",A Supplemental Material,[0],[0]
"+Eqφ(z|c,x,y)[log p(xbow|z, c)]
(7)
Collection of Multiple Reference Responses We collected multiple reference responses for each dialog context in the test set by information retrieval techniques combining with traditional a machine learning method.",A Supplemental Material,[0],[0]
"First, we encode the dialog history using Term Frequency-Inverse Document Frequency (TFIDF) (Salton and Buckley, 1988) weighted bag-of-words into vector representation h.",A Supplemental Material,[0],[0]
"Then we denote the topic of the conversation as t and denote f as the conversation floor, i.e. if the speakers of the last utterance in the dialog history and response utterance are the same f = 1 otherwise f = 0.",A Supplemental Material,[0],[0]
"Then we computed the similarity d(ci, cj) between two dialog contexts using:
d(ci, cj) =",A Supplemental Material,[0],[0]
"1(ti = tj)1(ti = tj) hi · hj ||hi||||hj || (8)
Unlike past work (Sordoni et al., 2015), this similarity function only cares about the distance in the context and imposes no constraints on the response, therefore is suitbale for finding diverse responses regarding to the same dialog context.",A Supplemental Material,[0],[0]
"Secondly, for each dialog context in the test set, we retrieved the 10 nearest neighbors from the training set and treated the responses from the training set as candidate reference responses.",A Supplemental Material,[0],[0]
"Thirdly, we further sampled 240 context-responses pairs from 5481 pairs in the total test set and post-processed the selected candidate responses by two human computational linguistic experts who were told to give a binary label for each candidate response about whether the response is appropriate regarding its dialog context.",A Supplemental Material,[0],[0]
The filtered lists then served as the ground truth to train our reference response classifier.,A Supplemental Material,[0],[0]
"For the next step, we extracted bigrams, part-of-speech bigrams and word part-of-speech
pairs from both dialogue contexts and candidate reference responses with rare threshold for feature extraction being set to 20.",A Supplemental Material,[0],[0]
Then L2-regularized logistic regression with 10-fold cross validation was applied as the machine learning algorithm.,A Supplemental Material,[0],[0]
Cross validation accuracy on the human-labelled data was 71%.,A Supplemental Material,[0],[0]
"Finally, we automatically annotated the rest of test set with this trained classifier and the resulting data were used for model evaluation.",A Supplemental Material,[0],[0]
"While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses.",abstractText,[0],[0]
"Unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder.",abstractText,[0],[0]
Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders.,abstractText,[0],[0]
We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance.,abstractText,[0],[0]
"Finally, the training procedure is improved by introducing a bag-of-word loss.",abstractText,[0],[0]
Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making.,abstractText,[0],[0]
Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,title,[0],[0]
"The task of unsupervised discrete representation learning is to obtain a function that maps similar (resp. dissimilar) data into similar (resp. dissimilar) discrete representations, where the similarity of data is defined according to applications of interest.",1. Introduction,[0],[0]
"It is a central machine learning task because of the compactness of the representations and ease
1University of Tokyo, Japan 2RIKEN AIP, Japan 3Preferred Networks, Inc., Japan 4ATR Cognitive Mechanism Laboratories, Japan.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Weihua Hu <hu@ms.k.utokyo.ac.jp>, Takeru Miyato <takeru.miyato@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
of interpretation.",1. Introduction,[0],[0]
The task includes two important machine learning tasks as special cases: clustering and unsupervised hash learning.,1. Introduction,[0],[0]
"Clustering is widely applied to data-driven application domains (Berkhin, 2006), while hash learning is popular for an approximate nearest neighbor search for large scale information retrieval (Wang et al., 2016).
",1. Introduction,[0],[0]
"Deep neural networks are promising to be used thanks to their scalability and flexibility of representing complicated, non-linear decision boundaries.",1. Introduction,[0],[0]
"However, their model complexity is huge, and therefore, regularization of the networks is crucial to learn meaningful representations of data.",1. Introduction,[0],[0]
"Particularly, in unsupervised representation learning, target representations are not provided and hence, are unconstrained.",1. Introduction,[0],[0]
"Therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest (e.g., invariance to small perturbations or affine transformation).",1. Introduction,[0],[0]
"Naı̈ve regularization to use is a weight decay (Erin Liong et al., 2015).",1. Introduction,[0],[0]
"Such regularization, however, encourages global smoothness of the function prediction; thus, may not necessarily impose the intended invariance on the predicted discrete representations.
",1. Introduction,[0],[0]
"Instead, in this paper, we use data augmentation to model the invariance of learned data representations.",1. Introduction,[0],[0]
"More specifically, we map data points into their discrete representations by a deep neural network and regularize it by encouraging its prediction to be invariant to data augmentation.",1. Introduction,[0],[0]
The predicted discrete representations then exhibit the invariance specified by the augmentation.,1. Introduction,[0],[0]
Our proposed regularization method is illustrated as red arrows in Figure 1.,1. Introduction,[0],[0]
"As depicted, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion.",1. Introduction,[0],[0]
We term such regularization Self-Augmented Training (SAT).,1. Introduction,[0],[0]
"SAT is inspired by the recent success in regularization of neural networks in semi-supervised learning (Bachman et al., 2014; Miyato et al., 2016; Sajjadi et al., 2016).",1. Introduction,[0],[0]
SAT is flexible to impose various types of invariances on the representations predicted by neural networks.,1. Introduction,[0],[0]
"For example, it is generally preferred for data representations to be locally invariant, i.e., remain unchanged under local perturbations on data points.",1. Introduction,[0],[0]
"Using SAT, we can impose the local invariance on the representations by pushing the predictions of perturbed data points to be close to those of the original data points.",1. Introduction,[0],[0]
"For image data, it may also be preferred for data representations to be invariant under affine distortion, e.g., rotation, scaling and parallel movement.",1. Introduction,[0],[0]
"We can similarly impose the invariance via SAT by using the affine distortion for the data augmentation.
",1. Introduction,[0],[0]
"We then combine the SAT with the Regularized Information Maximization (RIM) for clustering (Gomes et al., 2010; Bridle et al., 1991), and arrive at our Information Maximizing Self-Augmented Training (IMSAT), an information-theoretic method for learning discrete representations using deep neural networks.",1. Introduction,[0],[0]
We illustrate the basic idea of IMSAT in Figure 1.,1. Introduction,[0],[0]
"Following the RIM, we maximize information theoretic dependency between inputs and their mapped outputs, while regularizing the mapping function.",1. Introduction,[0],[0]
IMSAT differs from the original RIM in two ways.,1. Introduction,[0],[0]
"First, IMSAT deals with a more general setting of learning discrete representations; thus, is also applicable to hash learning.",1. Introduction,[0],[0]
"Second, it uses a deep neural network for the mapping function and regularizes it in an end-to-end fashion via SAT.",1. Introduction,[0],[0]
"Learning with our method can be performed by stochastic gradient descent (SGD); thus, scales well to large datasets.
",1. Introduction,[0],[0]
"In summary, our contributions are: 1) an informationtheoretic method for unsupervised discrete representation learning using deep neural networks with the end-to-end regularization, and 2) adaptations of the method to clustering and hash learning to achieve the state-of-the-art performance on several benchmark datasets.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
"Related work is summarized in Section 2, while our method, IMSAT, is
presented in Section 3.",1. Introduction,[0],[0]
Experiments are provided in Section 4 and conclusions are drawn in Section 5.,1. Introduction,[0],[0]
Various methods have been proposed for clustering and hash learning.,2. Related Work,[0],[0]
"The representative ones include K-means clustering and hashing (He et al., 2013), Gaussian mixture model clustering, iterative quantization (Gong et al., 2013), and minimal-loss hashing (Norouzi & Blei, 2011).",2. Related Work,[0],[0]
"However, these methods can only model linear boundaries between different representations; thus, cannot fit to non-linear structures of data.",2. Related Work,[0],[0]
"Kernel-based (Xu et al., 2004; Kulis & Darrell, 2009) and spectral (Ng et al., 2001; Weiss et al., 2009) methods can model the non-linearity of data, but they are difficult to scale to large datasets.
",2. Related Work,[0],[0]
"Recently, clustering and hash learning using deep neural networks have attracted much attention.",2. Related Work,[0],[0]
"In clustering, Xie et al. (2016) proposed to use deep neural networks to simultaneously learn feature representations and cluster assignments, while Dilokthanakul et al. (2016) and Zheng et al. (2016) proposed to model the data generation process by using deep generative models with Gaussian mixture models as prior distributions.
",2. Related Work,[0],[0]
"Regarding hashing learning, a number of studies have used deep neural networks for supervised hash learning and achieved state-of-the-art results on image and text retrievals (Xia et al., 2014; Lai et al., 2015; Zhang et al., 2015; Xu et al., 2015; Li et al., 2015).",2. Related Work,[0],[0]
Relatively few studies have focused on unsupervised hash learning using deep neural networks.,2. Related Work,[0],[0]
"The pioneering work is semantic hashing, which uses stacked RBM models to learn compact binary representations (Salakhutdinov & Hinton, 2009).",2. Related Work,[0],[0]
Erin Liong et al. (2015) recently proposed to use deep neural networks for the mapping function and achieved stateof-the-art results.,2. Related Work,[0],[0]
"These unsupervised methods, however, did not explicitly intended impose the invariance on the learned representations.",2. Related Work,[0],[0]
"Consequently, the predicted representations may not be useful for applications of interest.
",2. Related Work,[0],[0]
"In supervised and semi-supervised learning scenarios, data augmentation has been widely used to regularize neural networks.",2. Related Work,[0],[0]
Leen (1995) showed that applying data augmentation to a supervised learning problem is equivalent to adding a regularization to the original cost function.,2. Related Work,[0],[0]
"Bachman et al. (2014); Miyato et al. (2016); Sajjadi et al. (2016) showed that such regularization can be adapted to semi-supervised learning settings to achieve state-of-theart performance.
",2. Related Work,[0],[0]
"In unsupervised representation learning scenarios, Dosovitskiy et al. (2014) proposed to use data augmentation to model the invariance of learned representations.",2. Related Work,[0],[0]
"Our IMSAT is different from Dosovitskiy et al. (2014)
in two important aspects: 1) IMSAT directly imposes the invariance on the learned representations, while Dosovitskiy et al. (2014) imposes invariance on surrogate classes, not directly on the learned representations.",2. Related Work,[0],[0]
"2) IMSAT focuses on learning discrete representations that are directly usable for clustering and hash learning, while Dosovitskiy et al. (2014) focused on learning continuous representations that are then used for other tasks such as classification and clustering.",2. Related Work,[0],[0]
"Relation of our work to denoising and contractive auto-encoders (Vincent et al., 2008; Rifai et al., 2011) is discussed in Appendix A.",2. Related Work,[0],[0]
"Let X and Y denote the domains of inputs and discrete representations, respectively.",3. Method,[0],[0]
"Given training samples, {x1, x2, . . .",3. Method,[0],[0]
", xN}, the task of discrete representation learning is to obtain a function, f :",3. Method,[0],[0]
"X → Y , that maps similar inputs into similar discrete representations.",3. Method,[0],[0]
"The similarity of data is defined according to applications of interest.
",3. Method,[0],[0]
We organize Section 3 as follows.,3. Method,[0],[0]
"In Section 3.1, we review the RIM for clustering (Gomes et al., 2010).",3. Method,[0],[0]
"In Section 3.2, we present our proposed method, IMSAT, for discrete representation learning.",3. Method,[0],[0]
"In Sections 3.3 and 3.4, we adapt IMSAT to the tasks of clustering and hash learning, respectively.",3. Method,[0],[0]
"In Section 3.5, we discuss an approximation technique for scaling up our method.",3. Method,[0],[0]
"The RIM (Gomes et al., 2010) learns a probabilistic classifier pθ(y|x) such that mutual information (Cover & Thomas, 2012) between inputs and cluster assignments is maximized.",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"At the same time, it regularizes the complexity of the classifier.",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
Let X ∈ X and Y ∈,3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"Y ≡ {0, . . .",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
",K − 1} denote random variables for data and cluster assignments, respectively, where K is the number of clusters.",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"The RIM minimizes the objective:
R(θ)− λI(X;Y ), (1)
where R(θ) is the regularization penalty, and I(X;Y ) is mutual information between X and Y , which depends on θ through the classifier, pθ(y|x).",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"Mutual information measures the statistical dependency between X and Y , and is 0 iff they are independent.",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
Hyper-parameter λ ∈ R trades off the two terms.,3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"Here, we present two components that make up our IMSAT.",3.2. Information Maximizing Self-Augmented Training,[0],[0]
We present the Information Maximization part in Section 3.2.1 and the SAT part in Section 3.2.2 .,3.2. Information Maximizing Self-Augmented Training,[0],[0]
We extend the RIM and consider learning M -dimensional discrete representations of data.,3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"Let the output domain be Y = Y1× · · ·×YM , where Ym ≡ {0, 1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", Vm−1}, 1 ≤ m ≤ M .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"Let Y = (Y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", YM ) ∈ Y be a random variable for the discrete representation.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"Our goal is to learn a multioutput probabilistic classifier pθ(y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", yM |x) that maps similar inputs into similar representations.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"For simplicity, we model the conditional probability pθ(y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", yM |x) by using the deep neural network depicted in Figure 1.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"Under the model, {y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", yM} are conditionally independent given x:
pθ(y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", yM |x)",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"= M∏
m=1
pθ(ym|x).",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"(2)
Following the RIM (Gomes et al., 2010), we maximize the mutual information between inputs and their discrete representations, while regularizing the multi-output probabilistic classifier.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
The resulting objective to minimize looks exactly the same as Eq.,3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"(1), except that Y is multi-dimensional in our setting.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"VIA SELF-AUGMENTED TRAINING
We present an intuitive and flexible regularization objective, termed Self-Augmented Training (SAT).",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
SAT uses data augmentation to impose the intended invariance on the data representations.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"Essentially, SAT penalizes representation dissimilarity between the original data points and augmented ones.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
Let T : X → X denote a pre-defined data augmentation under which the data representations should be invariant.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"The regularization of SAT made on data point x is
RSAT(θ;x, T (x))
",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"= − M∑
m=1
Vm−1∑
ym=0
pθ̂(ym|x) log pθ(ym|T (x)), (3)
where pθ̂(ym|x) is the prediction of original data point x, and θ̂ is the current parameter of the network.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
In Eq.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"(3), the representations of the augmented data are pushed to be close to those of the original data.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"Since probabilistic classifier pθ(y|x) is modeled using a deep neural network, it is flexible enough to capture a wide range of invariances specified by the augmentation function T .",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"The regularization by SAT is then the average of RSAT(θ;x, T (x)) over all the training data points:
RSAT(θ;T )",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"= 1
N
N∑
n=1
RSAT(θ;xn, T (xn)).",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"(4)
The augmentation function T can either be stochastic or deterministic.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
It can be designed specifically for the applications of interest.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"For example, for image data, affine distortion such as rotation, scaling and parallel movement can be used for the augmentation function.
",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"Alternatively, more general augmentation functions that do not depend on specific applications can be considered.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"A representative example is local perturbations, in which the augmentation function is
T (x) = x+ r, (5)
where r is a small perturbation that does not alter the meaning of the data point.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
The use of local perturbations in SAT encourages the data representations to be locally invariant.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
The resulting decision boundaries between different representations tend to lie in low density regions of a data distribution.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"Such boundaries are generally preferred and follow the low-density separation principle (Grandvalet et al., 2004).
",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"The two representative regulariztion methods based on local perturbations are: Random Perturbation Training (RPT) (Bachman et al., 2014) and Virtual Adversarial Training (VAT) (Miyato et al., 2016).",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"In RPT, perturbation r is sampled randomly from hyper-sphere ||r||2 = ϵ, where ϵ is a hyper-parameter that controls the range of the local perturbation.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"On the other hand, in VAT, perturbation r is chosen to be an adversarial direction:
r = argmax r′
{RSAT(θ̂;x, x+ r′); ||r′||2 ≤ ϵ}.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"(6)
The solution of Eq.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
(6) can be approximated efficiently by a pair of forward and backward passes.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"For further details, refer to Miyato et al. (2016).",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"In clustering, we can directly apply the RIM (Gomes et al., 2010) reviewed in Section 3.1.",3.3. IMSAT for Clustering,[0],[0]
"Unlike the original RIM, however, our method, IMSAT, uses deep neural networks for the classifiers and regularizes them via SAT.",3.3. IMSAT for Clustering,[0],[0]
"By representing mutual information as the difference between marginal entropy and conditional entropy (Cover & Thomas, 2012), we have the objective to minimize:
RSAT(θ;T )",3.3. IMSAT for Clustering,[0],[0]
− λ,3.3. IMSAT for Clustering,[0],[0]
"[H(Y )−H(Y |X)] , (7)
where H(·) and H(·|·) are entropy and conditional entropy, respectively.",3.3. IMSAT for Clustering,[0],[0]
"The two entropy terms can be calculated as
H(Y ) ≡ h(pθ(y))",3.3. IMSAT for Clustering,[0],[0]
"= h ( 1
N
N∑
i=1
pθ(y|x) ) , (8)
H(Y |X) ≡",3.3. IMSAT for Clustering,[0],[0]
"1 N
N∑
i=1
h(pθ(y|xi)), (9)
where h(p(y)) ≡",3.3. IMSAT for Clustering,[0],[0]
"− ∑
y′ p(y ′) log p(y′) is the entropy
function.",3.3. IMSAT for Clustering,[0],[0]
"Increasing the marginal entropy H(Y ) encourages the cluster sizes to be uniform, while decreasing the conditional entropy H(Y |X) encourages unambiguous cluster assignments (Bridle et al., 1991).
",3.3. IMSAT for Clustering,[0],[0]
"In practice, we can incorporate our prior knowledge on cluster sizes by modifying H(Y )",3.3. IMSAT for Clustering,[0],[0]
"(Gomes et al., 2010).",3.3. IMSAT for Clustering,[0],[0]
"Note that H(Y ) = logK − KL[pθ(y)|| U ], where K is the number of clusters, KL[·||·] is the Kullback-Leibler divergence, and U is a uniform distribution.",3.3. IMSAT for Clustering,[0],[0]
"Hence, maximization of H(Y ) is equivalent to minimization of KL[pθ(y)|| U ], which encourages predicted cluster distribution pθ(y) to be close to U .",3.3. IMSAT for Clustering,[0],[0]
Gomes et al. (2010) replaced U in KL[pθ(y)|| U ] with any specified class prior q(y) so that pθ(y) is encouraged to be close to q(y).,3.3. IMSAT for Clustering,[0],[0]
"In our preliminary experiments, we found that the resulting pθ(y) could still be far apart from pre-specified q(y).",3.3. IMSAT for Clustering,[0],[0]
"To ensure that pθ(y) is actually close to q(y), we consider the following constrained optimization problem:
min θ RSAT(θ;T ) +",3.3. IMSAT for Clustering,[0],[0]
"λH(Y |X),
subject to KL[pθ(y)|| q(y)]",3.3. IMSAT for Clustering,[0],[0]
"≤ δ, (10)
where δ > 0 is a tolerance hyper-parameter that is set sufficiently small so that predicted cluster distribution pθ(y) is the same as class prior q(y) up to δ-tolerance.",3.3. IMSAT for Clustering,[0],[0]
"Eq. (10) can be solved by using the penalty method (Bertsekas, 1999), which turns the original constrained optimization problem into a series of unconstrained optimization problems.",3.3. IMSAT for Clustering,[0],[0]
Refer to Appendix B for the detail.,3.3. IMSAT for Clustering,[0],[0]
"In hash learning, each data point is mapped into a D-bitbinary code.",3.4. IMSAT for Hash Learning,[0],[0]
"Hence, the original RIM is not directly applicable.",3.4. IMSAT for Hash Learning,[0],[0]
"Instead, we apply our method for discrete representation learning presented in Section 3.2.1.
",3.4. IMSAT for Hash Learning,[0],[0]
"The computation of mutual information I(Y1, . . .",3.4. IMSAT for Hash Learning,[0],[0]
", YD;X), however, is intractable for large D because it involves a summation over an exponential number of terms, each of which corresponds to a different configuration of hash bits.
",3.4. IMSAT for Hash Learning,[0],[0]
"Brown (2009) showed that mutual information I(Y1, . . .",3.4. IMSAT for Hash Learning,[0],[0]
", YD;X) can be expanded as the sum of interaction information (McGill, 1954):
I(Y1, . . .",3.4. IMSAT for Hash Learning,[0],[0]
", YD;X) =",3.4. IMSAT for Hash Learning,[0],[0]
"∑
C⊆SY
I(C ∪X), |C| ≥ 1, (11)
where SY ≡ {Y1, . .",3.4. IMSAT for Hash Learning,[0],[0]
.,3.4. IMSAT for Hash Learning,[0],[0]
", YD}.",3.4. IMSAT for Hash Learning,[0],[0]
Note that I denotes interaction information when its argument is a set of random variables.,3.4. IMSAT for Hash Learning,[0],[0]
Interaction information is a generalization of mutual information and can take a negative value.,3.4. IMSAT for Hash Learning,[0],[0]
"When the argument is a set of two random variables, the interaction information reduces to mutual information between the two
random variables.",3.4. IMSAT for Hash Learning,[0],[0]
"Following Brown (2009), we only retain terms involving pairs of output dimensions in Eq.",3.4. IMSAT for Hash Learning,[0],[0]
"(11), i.e., all terms where |C| ≤ 2.",3.4. IMSAT for Hash Learning,[0],[0]
"This gives us
D∑
d=1
I(Yd;X)",3.4. IMSAT for Hash Learning,[0],[0]
"+ ∑
1≤d ̸=d′≤D I({Yd, Yd′ , X}).",3.4. IMSAT for Hash Learning,[0],[0]
"(12)
",3.4. IMSAT for Hash Learning,[0],[0]
This approximation ignores the interactions among hash bits beyond the pairwise interactions.,3.4. IMSAT for Hash Learning,[0],[0]
"It is related to the orthogonality constraint that is widely used in the literature to remove redundancy among hash bits (Wang et al., 2016).",3.4. IMSAT for Hash Learning,[0],[0]
"In fact, the orthogonality constraint encourages the covariance between a pair of hash bits to 0.",3.4. IMSAT for Hash Learning,[0],[0]
"Thus, it also takes into account the pairwise interactions.
",3.4. IMSAT for Hash Learning,[0],[0]
It follows from the definition of interaction information and the conditional independence in Eq.,3.4. IMSAT for Hash Learning,[0],[0]
"(2) that
I({Yd, Yd′ , X}) ≡ I(Yd;Yd′ |X)− I(Yd;Yd′) = −I(Yd;Yd′).",3.4. IMSAT for Hash Learning,[0],[0]
"(13)
In summary, our approximated objective to minimize is
RSAT(θ;T )− λ
⎛ ⎝ D∑
d=1
I(X;Yd)− ∑
1≤d",3.4. IMSAT for Hash Learning,[0],[0]
"̸=d′≤D I(Yd;Yd′)
⎞
⎠ .
",3.4. IMSAT for Hash Learning,[0],[0]
"(14)
The first term regularizes the neural network.",3.4. IMSAT for Hash Learning,[0],[0]
"The second term maximizes the mutual information between data and each hash bit, and the third term removes the redundancy among the hash bits.",3.4. IMSAT for Hash Learning,[0],[0]
"To scale up our method to large datasets, we would like the objective in Eq.",3.5. Approximation of the Marginal Distribution,[0],[0]
(1) to be amenable to optimization based on mini-batch SGD.,3.5. Approximation of the Marginal Distribution,[0],[0]
"For the regularization term, we use the SAT in Eq.",3.5. Approximation of the Marginal Distribution,[0],[0]
"(4), which is the sum of per sample penalties and can be readily adapted to mini-batch computation.",3.5. Approximation of the Marginal Distribution,[0],[0]
"For the approximated mutual information in Eq. (14), we can decompose it into three parts: (i) conditional entropy H(Yd|X), (ii) marginal entropy H(Yd), and (iii) mutual information between a pair of output dimensions I(Yd;Yd′).",3.5. Approximation of the Marginal Distribution,[0],[0]
"The conditional entropy only consists of a sum over per example entropies (see Eq. (9)); thus, can be easily adapted to mini-batch computation.",3.5. Approximation of the Marginal Distribution,[0],[0]
"However, the marginal entropy (see Eq. (8)) and the mutual information involve the marginal distribution over a subset of target dimensions, i.e., pθ(c) ≡",3.5. Approximation of the Marginal Distribution,[0],[0]
"1N ∑N n=1 pθ(c|xn), where c ⊆ {y1, . .",3.5. Approximation of the Marginal Distribution,[0],[0]
.,3.5. Approximation of the Marginal Distribution,[0],[0]
", yM}.",3.5. Approximation of the Marginal Distribution,[0],[0]
"Hence, the marginal distribution can only be calculated using the entire dataset and is not amenable to the mini-batch setting.",3.5. Approximation of the Marginal Distribution,[0],[0]
"Following Springenberg (2015), we approximate the marginal distributions using mini-batch data:
pθ(c)",3.5. Approximation of the Marginal Distribution,[0],[0]
"≈ 1 |B| ∑
x∈B pθ(c|x) ≡ p̂θ(B)(c), (15)
where B is a set of data in the mini-batch.",3.5. Approximation of the Marginal Distribution,[0],[0]
"In the case of clustering, the approximated objective that we actually minimize is an upper bound of the exact objective that we try to minimize.",3.5. Approximation of the Marginal Distribution,[0],[0]
Refer to Appendix C of the supplementary material for the detailed discussion.,3.5. Approximation of the Marginal Distribution,[0],[0]
"In this section, we evaluate IMSAT for clustering and hash learning using benchmark datasets.",4. Experiments,[0],[0]
"In unsupervised learning, it is not straightforward to determine hyper-parameters by cross-validation.",4.1. Implementation,[0],[0]
"Therefore, in all the experiments with benchmark datasets, we used commonly reported parameter values for deep neural networks and avoided dataset-specific tuning as much as possible.",4.1. Implementation,[0],[0]
"Specifically, inspired by Hinton et al. (2012), we set the network dimensionality to d-1200-1200-M for clustering across all the datasets, where d and M are input and output dimensionality, respectively.",4.1. Implementation,[0],[0]
"For hash learning, we used smaller network sizes to ensure fast computation of mapping data into hash codes.",4.1. Implementation,[0],[0]
"We used rectified linear units (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011) for all the hidden activations and applied batch normalization (Ioffe & Szegedy, 2015) to each layer to accelerate training.",4.1. Implementation,[0],[0]
"For the output layer, we used the softmax for clustering and the sigmoids for hash learning.",4.1. Implementation,[0],[0]
"Regarding optimization, we used Adam (Kingma & Ba, 2015) with the step size 0.002.",4.1. Implementation,[0],[0]
Refer to Appendix D for further details.,4.1. Implementation,[0],[0]
"Our implementation based on Chainer (Tokui et al., 2015) is available at https://github.com/weihua916/imsat.",4.1. Implementation,[0],[0]
We evaluated our method for clustering presented in Section 3.3 on eight benchmark datasets.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"We performed experiments with two variants of the RIM and three variants of IMSAT, each of which uses different classifiers and regularization.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
Table 1 summarizes these variants.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"We also compared our IMSAT with existing clustering methods including K-means, DEC (Xie et al., 2016), denoising AutoEncoder (dAE)+K-means (Xie et al., 2016).
",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
A brief summary of dataset statistics is given in Table 2.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"In the experiments, our goal was to discover clusters that correspond well with the ground-truth categories.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"For the STL, CIFAR10 and CIFAR100 datasets, raw pixels are not suited for our goal because color information is dominant.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"We therefore applied 50-layer pre-trained deep residual networks (He et al., 2016) to extract features and used them for clustering.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"Note that since the residual network was trained on ImageNet, each class of the STL dataset (which is a subset of ImageNet) was expected to be well-separated in the feature space.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"For Omniglot, 100 types of characters were sampled, each containing 20 data points.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"Each data point was augmented 20 times by the stochastic affine distortion described in Appendix F. For SVHN, each image was represented as a 960-dimensional GIST feature (Oliva & Torralba, 2001).",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"For Reuters and 20news, we removed stop words and retained the 2000 most frequent words.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
We then used tf-idf features.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
Refer to Appendix E of the supplementary material for further details.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"Following Xie et al. (2016), we set the number of clusters to the number of ground-truth categories and evaluated clustering performance with unsupervised clustering accuracy (ACC):
ACC = max m
∑N n=1 1{ln = m(cn)}
N , (16)
where ln and cn are the ground-truth label and cluster assignment produced using the algorithm for xn, respectively.",4.2.2. EVALUATION METRIC,[0],[0]
The m ranges over all possible one-to-one mappings between clusters and labels.,4.2.2. EVALUATION METRIC,[0],[0]
"The best mapping can be efficiently computed using the Hungarian algorithm (Kuhn, 1955).",4.2.2. EVALUATION METRIC,[0],[0]
"In unsupervised learning, it is not straightforward to determine hyper-parameters by cross-validation.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"Hence, we fixed hyper-parameters across all the datasets unless there was an objective way to select them.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For K-means, we tried 12 different initializations and reported the results with the best objectives.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For dAE+K-means and DEC (Xie et al., 2016), we used the recommended hyperparameters for the network dimensionality and annealing speed.
",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"Inspired by the automatic kernel width selection in spectral clustering (Zelnik-Manor & Perona, 2004), we set the perturbation range, ϵ, on data point x in VAT and RPT as
ϵ(x) = α · σt(x), (17)
where α is a scalar and σt(x) is the Euclidian distance to the t-th neighbor of x.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"In our experiments, we fixed t = 10.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For Linear IMSAT (VAT), IMSAT (RPT) and IMSAT (VAT), we fixed α = 0.4, 2.5 and 0.25, respectively, which performed well across the datasets.
",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For the methods shown in Table 1, we varied one hyperparameter and chose the best one that performed well across the datasets.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"More specifically, for Linear RIM and Deep RIM, we varied the decay rate over 0.0025 ·",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"2i, i = 0, 1, . . .",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
", 7.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For the three variants of IMSAT, we varied λ in Eq.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"(19) for 0.025 · 2i, i = 0, 1, . . .",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
", 7.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
We set q to be the uniform distribution and let δ = 0.01 · h(q(y)) in Eq.,4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"(10) for the all experiments.
",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"Consequently, we chose 0.005 for decay rates in both Linear RIM and Deep RIM.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"Also, we set λ = 1.6, 0.05 and 0.1 for Linear IMSAT (VAT), IMSAT (RPT) and IMSAT (VAT), respectively.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
We hereforth fixed these hyperparameters throughout the experiments for both clustering and hash learning.,4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"In Appendix G, we report all the experimental results and the criteria to choose the parameters.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"In Table 3, we compare clustering performance across eight benchmark datasets.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We see that IMSAT (VAT) performed well across the datasets.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"The fact that our IMSAT outperformed Linear RIM, Deep RIM and Linear IMSAT (VAT) for most datasets suggests the effectiveness of using deep neural networks with an end-to-end regularization via SAT.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
Linear IMSAT (VAT) did not perform well even with the end-to-end regularization probably because the linear classifier was not flexible enough to model the intended invariance of the representations.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We also see from Table 3 that IMSAT (VAT) consistently outperformed IMSAT (RPT) in our experiments.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"This suggests that VAT is an effective regularization method in unsupervised learning scenarios.
",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We further conducted experiments on the Omniglot dataset to demonstrate that clustering performance can be improved by incorporating domain-specific knowledge in the augmentation function of SAT.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"Specifically, we used the affine distortion in addition to VAT for the augmented function of SAT.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"We compared the clustering accuracy of IMSAT with three different augmentation functions: VAT, affine distortion, and the combination of VAT & affine distortion, in which we simply set the regularization to be
1 2 · RSAT(θ;TVAT)",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"+ 1 2 · RSAT(θ;Taffine), (18)
where TVAT and Taffine are augmentation functions of VAT and affine distortion, respectively.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"For Taffine, we used the stochastic affine distortion function defined in Appendix F.
We report the clustering accuracy of Omniglot in Table 4.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We see that including affine distortion in data augmentation significantly improved clustering accuracy.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
Figure 2 shows ten randomly selected clusters of the Omniglot dataset that were found using IMSAT (VAT) and IMSAT (VAT & affine distortion).,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We observe that IMSAT (VAT & affine distortion) was able to discover cluster assignments that are invariant to affine distortion as we intended.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
These results suggest that our method successfully captured the invariance in the hand-written character recognition in an unsupervised way.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We evaluated our method for hash learning presented in Section 3.4 on two benchmark datasets: MNIST and CIFAR10 datasets.,4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
"Each data sample of CIFAR10 is represented as a 512-dimensional GIST feature (Oliva & Torralba, 2001).",4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
"Our method was compared against several unsupervised hash learning methods: spectral hashing (Weiss et al., 2009), PCA-ITQ (Gong et al., 2013), and Deep Hash (Erin Liong et al., 2015).",4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
We also compared our method to the hash versions of Linear RIM and Deep RIM.,4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
"For our IMSAT, we used VAT for the regularization.",4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
We used the same hyper-parameters as in Section 4.2.3.,4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
"Following Erin Liong et al. (2015), we used three evaluation metrics to measure the performance of the different methods: 1) mean average precision (mAP); 2) precision at N = 500 samples; and 3) Hamming look-up result where the hamming radius is set as r = 2.",4.3.2. EVALUATION METRIC,[0],[0]
We used the class labels to define the neighbors.,4.3.2. EVALUATION METRIC,[0],[0]
We repeated the experiments ten times and took the average as the final result.,4.3.2. EVALUATION METRIC,[0],[0]
"The MNIST and CIFAR10 datasets both have 10 classes, and contain 70000 and 60000 data points, respectively.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Following Erin Liong et al. (2015), we randomly sampled 1000 samples, 100 per class, as the query data and used the remaining data as the gallery set.
",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
We tested performance for 16 and 32-bit hash codes.,4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"In practice, fast computation of hash codes is crucial for fast information retrieval.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Hence, small networks are preferable.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"We therefore tested our method on three different network sizes: the same ones as Deep Hash (Erin Liong et al., 2015), d-200-200-M , and d-400-400-M .",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Note that Deep Hash used d-60-30-M and d-80-50-M for learning 16 and 32-bit hash codes, respectively.
",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
Table 5 lists the results for 16-bit hash.,4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Due to the space
constraint, we report the results for 32-bit hash codes in Appendix H, but the results showed a similar tendency as that of 16-bit hash codes.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
We see from Table 5 that IMSAT with the largest network sizes (400-400) achieved competitive performance in both datasets.,4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"The performance of IMSAT improved significantly when slightly bigger networks (200-200) were used, while the performance of Deep RIM did not improve much with the larger networks.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
We deduce that this is because we can better model the local invariance by using more flexible networks.,4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Deep RIM, on the other hand, did not significantly benefit from the larger networks, because the additional flexibility of the networks was not used by the global function regularization via weight-decay.1 In Appendix I, our deduction is supported using a toy dataset.
",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"1Hence, we deduce that Deep Hash, which is only regularized by weight-decay, would not benefit much by using larger networks.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"In this paper, we presented IMSAT, an informationtheoretic method for unsupervised discrete representation learning using deep neural networks.",5. Conclusion & Future Work,[0],[0]
"Through extensive experiments, we showed that intended discrete representations can be obtained by directly imposing the invariance to data augmentation on the prediction of neural networks in an end-to-end fashion.",5. Conclusion & Future Work,[0],[0]
"For future work, it is interesting to apply our method to structured data, i.e., graph or sequential data, by considering appropriate data augmentation.",5. Conclusion & Future Work,[0],[0]
We thank Brian Vogel for helpful discussions and insightful reviews on the paper.,Acknowledgements,[0],[0]
"This paper is based on results obtained from Hu’s internship at Preferred Networks, Inc.",Acknowledgements,[0],[0]
Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation.,abstractText,[0],[0]
The task includes clustering and hash learning as special cases.,abstractText,[0],[0]
Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets.,abstractText,[0],[0]
"However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest.",abstractText,[0],[0]
"To this end, we propose a method called Information Maximizing Self-Augmented Training (IMSAT).",abstractText,[0],[0]
"In IMSAT, we use data augmentation to impose the invariance on discrete representations.",abstractText,[0],[0]
"More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion.",abstractText,[0],[0]
"At the same time, we maximize the informationtheoretic dependency between data and their predicted discrete representations.",abstractText,[0],[0]
Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning.,abstractText,[0],[0]
Learning Discrete Representations via Information Maximizing Self-Augmented Training,title,[0],[0]
"The goal of unsupervised learning is to uncover hidden structure in unlabelled data, often in the form of latent feature representations.",1. Introduction,[0],[0]
"One popular type of model, an autoencoder, does this by trying to reconstruct its input (Bengio et al., 2007).",1. Introduction,[0],[0]
"Autoencoders have been used in various forms to address problems in machine translation (Chandar et al., 2014; Tu et al., 2017), speech processing (Elman & Zipser, 1987; Zeiler et al., 2013), and computer vision (Rifai et al., 2011;
1Computer Science Division, Stellenbosch University, South Africa 2CSIR/SU Centre for Artificial Intelligence Research, 3Department of Electrical and Electronic Engineering, Stellenbosch University, South Africa.",1. Introduction,[0],[0]
"Correspondence to: Steve Kroon <kroon@sun.ac.za>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"*Code to reproduce all the results in this paper is available at: https://github.com/arnupretorius/lindaedynamics icml2018
Larsson, 2017), to name just a few areas.",1. Introduction,[0],[0]
"Denoising autoencoders (DAEs) are an extension of autoencoders which learn latent features by reconstructing data from corrupted versions of the inputs (Vincent et al., 2008).",1. Introduction,[0],[0]
"Although this corruption step typically leads to improved performance over standard autoencoders, a theoretical understanding of its effects remains incomplete.",1. Introduction,[0],[0]
"In this paper, we provide new insights into the inner workings of DAEs by analysing the learning dynamics of linear DAEs.
",1. Introduction,[0],[0]
"We specifically build on the work of Saxe et al. (2013a;b), who studied the learning dynamics of deep linear networks in a supervised regression setting.",1. Introduction,[0],[0]
"By analysing the gradient descent weight update steps as time-dependent differential equations (in the limit as the learning rate approaches a small value), Saxe et al. (2013a) were able to derive exact solutions for the learning trajectory of these networks as a function of training time.",1. Introduction,[0],[0]
Here we extend their approach to linear DAEs.,1. Introduction,[0],[0]
"To do this, we use the expected reconstruction loss over the noise distribution as an objective (requiring a different decomposition of the input covariance) as a tractable way to incorporate noise into our analytic solutions.",1. Introduction,[0],[0]
"This approach yields exact equations which can predict the learning trajectory of a linear DAE.
",1. Introduction,[0],[0]
"Our work here shares the motivation of many recent studies (Advani & Saxe, 2017; Pennington & Worah, 2017; Pennington & Bahri, 2017; Nguyen & Hein, 2017; Dinh et al., 2017; Louart et al., 2017; Swirszcz et al., 2017; Lin et al., 2017; Neyshabur et al., 2017; Soudry & Hoffer, 2017; Pennington et al., 2017) working towards a better theoretical understanding of neural networks and their behaviour.",1. Introduction,[0],[0]
"Although we focus here on a theory for linear networks, such networks have learning dynamics that are in fact nonlinear.",1. Introduction,[0],[0]
"Furthermore, analyses of linear networks have also proven useful in understanding the behaviour of nonlinear neural networks (Saxe et al., 2013a; Advani & Saxe, 2017).
",1. Introduction,[0],[0]
First we introduce linear DAEs (§2).,1. Introduction,[0],[0]
"We then derive analytic expressions for their nonlinear learning dynamics (§3), and verify our solutions in simulations (§4) which show how noise can influence the shape of the loss surface and change the rate of convergence for gradient descent optimisation.",1. Introduction,[0],[0]
We also find that an appropriate amount of noise can help DAEs ignore low variance directions in the input while learning the reconstruction mapping.,1. Introduction,[0],[0]
"In the remainder of
the paper, we compare DAEs to standard regularised autoencoders and show that our theoretical predictions match both simulations (§5) and experimental results on MNIST and CIFAR-10 (§6).",1. Introduction,[0],[0]
"We specifically find that while the noise in a DAE has an equivalent effect to standard weight decay, the DAE exhibits faster learning dynamics.",1. Introduction,[0],[0]
We also show that our observations hold qualitatively for nonlinear DAEs.,1. Introduction,[0],[0]
We first give the background of linear DAEs.,2. Linear Denoising Autoencoders,[0],[0]
"Given training data consisting of pairs {(x̃i,xi), i = 1, ..., N}, where x̃ represents a corrupted version of the training data x ∈ RD, the reconstruction loss for a single hidden layer DAE with activation function φ is given by
L = 1 2N N∑ i=1",2. Linear Denoising Autoencoders,[0],[0]
"||xi −W2φ(W1x̃i)||2.
",2. Linear Denoising Autoencoders,[0],[0]
"Here, W1 ∈ RH×D and W2 ∈ RD×H are the weights of the network with hidden dimensionality H .",2. Linear Denoising Autoencoders,[0],[0]
"The learned feature representations correspond to the latent variable z = φ(W1x̃).
",2. Linear Denoising Autoencoders,[0],[0]
"To corrupt an input x, we sample a noise vector , where each component is drawn i.i.d.",2. Linear Denoising Autoencoders,[0],[0]
from a pre-specified noise distribution with mean zero and variance s2.,2. Linear Denoising Autoencoders,[0],[0]
We define the corrupted version of the input as x̃ = x + .,2. Linear Denoising Autoencoders,[0],[0]
"This ensures that the expectation over the noise remains unbiased, i.e. E (x̃) =",2. Linear Denoising Autoencoders,[0],[0]
"x.
Restricting our scope to linear neural networks, with φ(a) = a, the loss in expectation over the noise distribution is
E",2. Linear Denoising Autoencoders,[0],[0]
"[L] = 1
2N N∑ i=1",2. Linear Denoising Autoencoders,[0],[0]
"||xi −W2W1xi||2
whitece+ s2
2 tr(W2W1WT1 W T 2 ),",2. Linear Denoising Autoencoders,[0],[0]
"(1)
See the supplementary material for the full derivation.",2. Linear Denoising Autoencoders,[0],[0]
"Here we derive the learning dynamics of linear DAEs, beginning with a brief outline to build some intuition.
",3. Learning Dynamics of Linear DAEs,[0],[0]
"The weight update equations for a linear DAE can be formulated as time-dependent differential equations in the limit as the gradient descent learning rate becomes small (Saxe et al., 2013a).",3. Learning Dynamics of Linear DAEs,[0],[0]
The task of an ordinary (undercomplete) linear autoencoder is to learn the identity mapping that reconstructs the original input data.,3. Learning Dynamics of Linear DAEs,[0],[0]
The matrix corresponding to this learned map will essentially be an approximation of the full identity matrix that is of rank equal to the input dimension.,3. Learning Dynamics of Linear DAEs,[0],[0]
"It turns out that tracking the temporal updates of this mapping represents a difficult problem that involves dealing with
coupled differential equations, since both the on-diagonal and off-diagonal elements of the weight matrices need to be considered in the approximation dynamics at each time step.
",3. Learning Dynamics of Linear DAEs,[0],[0]
"To circumvent this issue and make the analysis tractable, we follow the methodology introduced in Saxe et al. (2013a), which is to: (1) decompose the input covariance matrix using an eigenvalue decomposition; (2) rotate the weight matrices to align with these computed directions of variation; and (3) use an orthogonal initialisation strategy to diagonalise the composite weight matrix W = W2W1.",3. Learning Dynamics of Linear DAEs,[0],[0]
"The important difference in our setting, is that additional constraints are brought about through the injection of noise.
",3. Learning Dynamics of Linear DAEs,[0],[0]
The remainder of this section outlines this derivation for the exact solutions to the learning dynamics of linear DAEs.,3. Learning Dynamics of Linear DAEs,[0],[0]
Consider a continuous time limit approach to studying the learning dynamics of linear DAEs.,3.1. Gradient descent update,[0],[0]
This is achieved by choosing a sufficiently small learning rate α for optimising the loss in (1) using gradient descent.,3.1. Gradient descent update,[0],[0]
"The update for W1 in a single gradient descent step then takes the form of a time-dependent differential equation
τ",3.1. Gradient descent update,[0],[0]
"d
dt W1 = N∑ i=1",3.1. Gradient descent update,[0],[0]
WT2 ( xix T,3.1. Gradient descent update,[0],[0]
i −W2W1xixTi ) whitesp−,3.1. Gradient descent update,[0],[0]
"εWT2 W2W1 = WT2 (Σxx −W2W1Σxx)− εWT2 W2W1.
",3.1. Gradient descent update,[0],[0]
"Here t is the time measured in epochs, τ =",3.1. Gradient descent update,[0],[0]
"Nα , ε = Ns 2 and Σxx = ∑N i=1",3.1. Gradient descent update,[0],[0]
"xix T i , represents the input covariance matrix.",3.1. Gradient descent update,[0],[0]
"Let the eigenvalue decomposition of the input covariance be Σxx = V ΛV
T , where V is an orthogonal matrix and denote the eigenvalues",3.1. Gradient descent update,[0],[0]
λj =,3.1. Gradient descent update,[0],[0]
"[Λ]jj , with λ1 ≥ λ2 ≥ · · · ≥ λD. The update can then be rewritten as
τ",3.1. Gradient descent update,[0],[0]
"d
dt W1 = W
T 2 V ( Λ− V TW2W1V Λ ) V T
morewhitespace− εWT2 W2W1.
",3.1. Gradient descent update,[0],[0]
The weight matrices can be rotated to align with the directions of variation in the input by performing the rotations W 1 = W1V andW 2 = V TW2.,3.1. Gradient descent update,[0],[0]
"Following a similar derivation for W2, the weight updates become
τ",3.1. Gradient descent update,[0],[0]
"d
dt W 1 = W
T 2 ( Λ−W 2W 1Λ )",3.1. Gradient descent update,[0],[0]
"− εWT2W 2W 1
τ d
dt",3.1. Gradient descent update,[0],[0]
"W 2 =
( Λ−W 2W 1Λ )",3.1. Gradient descent update,[0],[0]
W T 1 − εW 2W 1W T 1 .,3.1. Gradient descent update,[0],[0]
"To decouple the dynamics, we can set W2 = V D2RT and W1 = RD1V T , where R is an arbitrary orthogonal matrix
and D2 and D1 are diagonal matrices.",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"This results in the product of the realigned weight matrices
W 2W 1 = V TV D2R TRD1V TV = D2D1
to become diagonal.",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"The updates now reduce to the following scalar dynamics that apply independently to each pair of diagonal elements w1j and w2j of D1 and D2 respectively:
τ",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"d
dt w1j = w2jλj (1− w2jw1j)− εw22jw1j (2)
τ",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"d
dt w2j = w1jλj (1− w2jw1j)− εw2jw21j .",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"(3)
Note that the same dynamics stem from gradient descent on the loss given by
` = D∑ j=1 λj 2τ (1− w2jw1j)2 + D∑ j=1 ε 2τ (w2jw1j) 2. (4)
By examining (4), it is evident that the degree to which the first term will be reduced will depend on the magnitude of the associated eigenvalue λj .",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"However, for directions in the input covariance Σxx with relatively little variation the decrease in the loss from learning the identity map will be negligible and is likely to result in overfitting (since little to no signal is being captured by these eigenvalues).",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
The second term in (4) is the result of the input corruption and acts as a suppressant on the magnitude of the weights in the learned mapping.,3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
Our interest is to better understand the interplay between these two terms during learning by studying their scalar learning dynamics.,3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"As noted above, the dynamics of learning are dictated by the value ofw = w2w1",3.3. Exact solutions to the dynamics of learning,[0],[0]
over time.,3.3. Exact solutions to the dynamics of learning,[0],[0]
"An expression can be derived for w(t) by using a hyperbolic change of coordinates in (2) and (3), letting θ parameterise points along a dynamics trajectory represented by the conserved quantity w22−w21 = ±c0.",3.3. Exact solutions to the dynamics of learning,[0],[0]
This relies on the fact that ` is invariant under a scaling of the weights such that w = (w1/c)(cw2) =,3.3. Exact solutions to the dynamics of learning,[0],[0]
"w2w1 for any constant c (Saxe et al., 2013a).",3.3. Exact solutions to the dynamics of learning,[0],[0]
"Starting at any initial point (w1, w2) the dynamics are
w(t) = c0 2 sinh (θt) , (5)
with θt = 2tanh−1",3.3. Exact solutions to the dynamics of learning,[0],[0]
[ (1− E) ( ζ2 − β2 − 2βδ ),3.3. Exact solutions to the dynamics of learning,[0],[0]
"− 2(1 + E)ζδ
(1− E) (2β + 4δ)− 2(1 + E)ζ
]
where β = c0 ( 1 + ελ ) , ζ = √ β2 + 4, δ = tanh ( θ0 2 ) and E = eζλt/τ .",3.3. Exact solutions to the dynamics of learning,[0],[0]
Here θ0 depends on the initial weights w1 and w2 through the relationship θ0 = sinh−1(2w/c0).,3.3. Exact solutions to the dynamics of learning,[0],[0]
"The
derivation for θt involves rewriting τ ddtw in terms of θ, integrating over the interval θ0 to θt, and finally rearranging terms to get an expression for θ(t) ≡ θt (see the supplementary material for full details).",3.3. Exact solutions to the dynamics of learning,[0],[0]
"To derive the learning dynamics for different noise distributions, the corresponding ε must be computed and used to determine β and ζ .",3.3. Exact solutions to the dynamics of learning,[0],[0]
"For example, sampling noise from a Gaussian distribution such that ∼ N (0, σ2I), gives ε = Nσ2.",3.3. Exact solutions to the dynamics of learning,[0],[0]
"Alternatively, if is distributed according to a zero-mean Laplace distribution with scale parameter b, then ε = 2Nb2.",3.3. Exact solutions to the dynamics of learning,[0],[0]
"Since the expression for the learning dynamics of a linear DAE in (5) evolve independently for each direction of variation in the input, it is enough to study the effect that noise has on learning for a single eigenvalue λ.",4. The Effects of Noise: a Simulation Study,[0],[0]
To do this we trained a scalar linear DAE to minimise the loss `λ = λ 2 (1−w2w1) 2+ ε2 (w2w1) 2 with λ = 1 using gradient descent.,4. The Effects of Noise: a Simulation Study,[0],[0]
"Starting from several different randomly initialised weights w1 and w2, we compare the simulated dynamics with those predicted by equation (5).",4. The Effects of Noise: a Simulation Study,[0],[0]
"The top row in Figure 1 shows the exact fit between the predictions and numerical simulations for different noise levels, ε = 0, 1, 5.
",4. The Effects of Noise: a Simulation Study,[0],[0]
The trajectories in the top row of Figure 1 converge to the optimal solution at different rates depending on the amount of injected noise.,4. The Effects of Noise: a Simulation Study,[0],[0]
"Specifically, adding more noise results in faster convergence.",4. The Effects of Noise: a Simulation Study,[0],[0]
"However, the trade-off in (4) ensures that the fixed point solution also diminishes in magnitude.
",4. The Effects of Noise: a Simulation Study,[0],[0]
"To gain further insight, we also visualise the associated loss surfaces for each experiment in the bottom row of Figure 1.",4. The Effects of Noise: a Simulation Study,[0],[0]
"Note that even though the scalar product w2w1 defines a linear mapping, the minimisation of `λ with respect to w1 and w2 is a non-convex optimisation problem.",4. The Effects of Noise: a Simulation Study,[0],[0]
The loss surfaces in Figure 1 each have an unstable saddle point at w2 = w1 = 0,4. The Effects of Noise: a Simulation Study,[0],[0]
(red star) with all remaining fixed points lying on a minimum loss manifold (cyan curve).,4. The Effects of Noise: a Simulation Study,[0],[0]
This manifold corresponds to the different possible combinations ofw2 and w1 that minimise `λ.,4. The Effects of Noise: a Simulation Study,[0],[0]
"The paths that gradient descent follow from various initial starting weights down to points situated on the manifold are represented by dashed orange lines.
",4. The Effects of Noise: a Simulation Study,[0],[0]
"For a fixed value of λ, adding noise warps the loss surface making steeper slopes and pulling the minimum loss manifold in towards the saddle point.",4. The Effects of Noise: a Simulation Study,[0],[0]
"Therefore, steeper descent directions cause learning to converge at a faster rate to fixed points that are smaller in magnitude.",4. The Effects of Noise: a Simulation Study,[0],[0]
"This is the result of a sharper curving loss surface and the minimum loss manifold lying closer to the origin.
",4. The Effects of Noise: a Simulation Study,[0],[0]
"We can compute the fixed point solution for any pair of initial starting weights (not on the saddle point) by taking
the derivative
d`λ dw = −λ τ (1− w) + ε τ w,
and setting it equal to zero to find w∗ = λλ+ε .",4. The Effects of Noise: a Simulation Study,[0],[0]
This solution reveals the interaction between the input variance associated with λ and the noise ε.,4. The Effects of Noise: a Simulation Study,[0],[0]
"For large eigenvalues for which λ ε, the fixed point will remain relatively unaffected by adding noise, i.e., w∗ ≈ 1.",4. The Effects of Noise: a Simulation Study,[0],[0]
"In contrast, if λ ε, the noise will result in w∗ ≈ 0.",4. The Effects of Noise: a Simulation Study,[0],[0]
"This means that over a distribution of eigenvalues, an appropriate amount of noise can help a DAE to ignore low variance directions in the input data while learning the reconstruction.",4. The Effects of Noise: a Simulation Study,[0],[0]
"In a practical setting, this motivates the tuning of noise levels on a development set to prevent overfitting.",4. The Effects of Noise: a Simulation Study,[0],[0]
"It is well known that adding noise to the inputs of a neural network is equivalent to a form of regularisation (Bishop, 1995).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Therefore, to further understand the role of noise in linear DAEs we compare the dynamics of noise to those of explicit regularisation in the form of weight decay (Krogh & Hertz, 1992).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The reconstruction loss for a linear weight
decayed autoencoder (WDAE) is given by
1
2N N∑ i=1",5. The Relationship Between Noise and Weight Decay,[0],[0]
||xi,5. The Relationship Between Noise and Weight Decay,[0],[0]
"−W2W1xi||2 + γ 2 ( ||W1||2 + ||W2||2 ) (6)
where γ is the penalty parameter that controls the amount of regularisation applied during learning.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Provided that the weights of the network are initialised to be small, it is also possible (see supplementary material) to derive scalar dynamics of learning from (6) as
wγ(t) =",5. The Relationship Between Noise and Weight Decay,[0],[0]
"ξEγ
Eγ − 1 + ξ/w0 , (7)
where ξ = (1−Nγ/λ) and Eγ = e2ξt/τ .
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Figure 2 compares the learning trajectories of linear DAEs and WDAEs over time (as measured in training epochs) for λ = 2.5, 1, 0.5 and 0.1.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The dynamics for both noise and weight decay exhibit a sigmoidal shape with an initial period of inactivity followed by rapid learning, finally reaching a plateau at the fixed point solution.",5. The Relationship Between Noise and Weight Decay,[0],[0]
Figure 2 illustrates that the learning time associated with an eigenvalue is negatively correlated with its magnitude.,5. The Relationship Between Noise and Weight Decay,[0],[0]
"Thus, the eigenvalue corresponding to the largest amount of variation explained is the quickest to escape inactivity during learning.
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The colour intensity of the lines in Figure 2 correspond to the amount of noise or regularisation applied in each run,
with darker lines indicating larger amounts.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"In the continuous time limit with equal learning rates, when compared with noise dynamics, weight decay experiences a delay in learning such that the initial inactive period becomes extended for every eigenvalue, whereas adding noise has no effect on learning time.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"In other words, starting from small weights, noise injected learning is capable of providing an equivalent regularisation mechanism to that of weight decay in terms of a constrained fixed point mapping, but with zero time delay.
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"However, this analysis does not take into account the practice of using well-tuned stable learning rates for discrete optimisation steps.",5. The Relationship Between Noise and Weight Decay,[0],[0]
We therefore consider the impact on training time when using optimised learning rates for each approach.,5. The Relationship Between Noise and Weight Decay,[0],[0]
"By using second order information from the Hessian as in Saxe et al. (2013a), (here of the expected reconstruction loss with respect to the scalar weights), we relate the optimal learning rates for linear DAEs and WDAEs,
where each optimal rate is inversely related to the amount of noise/regularisation applied during training (see supplementary material).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The ratio of the optimal DAE rate to that for the WDAE is
R = 2λ+ γ
2λ+ 3ε .",5. The Relationship Between Noise and Weight Decay,[0],[0]
"(8)
Note that the ratio in (8) will essentially be equal to one for eigenvalues that are significantly larger than both ε and γ, with deviations from unity only manifesting for smaller values of λ.
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Furthermore, weight decay and noise injected learning result in equivalent scalar solutions when their parameters are related by γ = λελ+ε (see supplementary material).",5. The Relationship Between Noise and Weight Decay,[0],[0]
This leads to the following two observations.,5. The Relationship Between Noise and Weight Decay,[0],[0]
"First, it shows that adding noise during learning can be interpreted as a form of weight decay where the penalty parameter γ adapts to each direction of variation in the data.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"In other words, noise essentially makes use of the statistical structure of
the input data to influence the amount of shrinkage that is being applied in various directions during learning.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Second, together with (8), we can theoretically compare the learning dynamics of DAEs and WDAEs, when both equivalent regularisation and the relative differences in optimal learning rates are taken into account.
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The effects of optimal learning rates (for λ = 1), are shown in Figure 3.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"DAEs still exhibit faster dynamics (left panel), even when taking into account the difference in the learning rate as a function of noise, or equivalent weight decay (middle panel).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"In addition, for equivalent regularisation effects, the ratio of the optimal rates R can be shown to be a monotonically decreasing function of the noise level, where the rate of decay depends on the size of λ.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"This means that for any amount of added noise, the DAE will require a slower learning rate than that of the WDAE.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Even so, a faster rate for the WDAE does not seem to compensate for its slower dynamics and the difference in learning time is also shown to grow as more noise (regularisation) is applied during training (right panel).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"A primary motivation for weight decay as a regulariser is that it provides solutions with smaller weight norms, producing smoother models that have better generalisation performance.",5.1. Exploiting invariance in the loss function,[0],[0]
Figure 4 shows the effect of noise (top row) compared to weight decay (bottom row) on the norm of the weights during learning.,5.1. Exploiting invariance in the loss function,[0],[0]
"Looking at the loss surface for weight decay (bottom left panel), the penalty on the size of the weights acts by shrinking the minimum loss manifold down from a long curving valley to a single point (associ-
ated with a small norm solution).",5.1. Exploiting invariance in the loss function,[0],[0]
"Interestingly, this results in gradient descent following a trajectory towards an “invisible” minimum loss manifold similar to the one associated with noise.",5.1. Exploiting invariance in the loss function,[0],[0]
"However, once on this manifold, weight decay begins to exploit invariances in the loss function to changes in the weights, so as to move along the manifold down towards smaller norm solutions.",5.1. Exploiting invariance in the loss function,[0],[0]
"This means that even when the two approaches learn the exact same mapping over time (as shown by the learning dynamics in the middle column of Figure 4), additional epochs will cause weight decay to further reduce the size of the weights (bottom right panel).",5.1. Exploiting invariance in the loss function,[0],[0]
This happens in a stage-like manner where the optimisation first focuses on reducing the reconstruction loss by learning the optimal mapping and then reduces the regularisation loss through invariance.,5.1. Exploiting invariance in the loss function,[0],[0]
It is common practice to initialise the weights of a network with small values.,5.2. Small weight initialisation and early stopping,[0],[0]
"In fact, this strategy has recently been theoretically shown to help, along with early stopping, to ensure good generalisation performance for neural networks in certain high-dimensional settings (Advani & Saxe, 2017).",5.2. Small weight initialisation and early stopping,[0],[0]
"In our analysis however, what we find interesting about small weight initialisation is that it removes some of the differences in the learning behaviour of DAEs compared to regularised autoencoders that use weight decay.
",5.2. Small weight initialisation and early stopping,[0],[0]
"To see this, the magenta lines in Figure 4 show the learning dynamics for the two approaches where the weights of both the networks were initialised to small random starting values.",5.2. Small weight initialisation and early stopping,[0],[0]
"The learning dynamics are almost identical in terms of their temporal trajectories and have equal fixed
points.",5.2. Small weight initialisation and early stopping,[0],[0]
"However, what is interesting is the implicit regularisation that is brought about through the small initialisation.",5.2. Small weight initialisation and early stopping,[0],[0]
"By starting small and making incremental updates to the weights, the scalar solution in both cases end up being equal to the minimum norm solution.",5.2. Small weight initialisation and early stopping,[0],[0]
"In other words, the path that gradient descent takes from the initialisation to the minimum loss manifold, reaches the manifold where the norm of the weights happen to also be small.",5.2. Small weight initialisation and early stopping,[0],[0]
"This means that the second phase of weight decay (where the invariance of the loss function would be exploited to reduce the regularisation penalty), is not only no longer necessary, but also does not result in a norm that is appreciably smaller than that obtained by learning with added noise.",5.2. Small weight initialisation and early stopping,[0],[0]
"Therefore in this case, learning with explicit regularisation provides no additional benefit over that of learning with noise in terms of reducing the norm of the weights during training.
",5.2. Small weight initialisation and early stopping,[0],[0]
"When initialising small, early stopping can also serve as a form of implicit regularisation by ensuring that the weights do not change past the point where the validation loss starts to increase (Bengio et al., 2007).",5.2. Small weight initialisation and early stopping,[0],[0]
"In the context of learning dynamics, early stopping for DAEs can be viewed as a method that effectively selects only the directions of variation deemed useful for generalisation during reconstruction, considering the remaining eigenvalues to carry no additional signal.",5.2. Small weight initialisation and early stopping,[0],[0]
To verify the dynamics of learning on real-world data sets we compared theoretical predictions with actual learning on MNIST and CIFAR-10.,6. Experimental Results,[0],[0]
"In our experiments we considered the following linear autoencoder networks: a regular AE, a WDAE and a DAE.
",6. Experimental Results,[0],[0]
"For MNIST, we trained each autoencoder with small randomly initialised weights, using N = 50000 training samples for 5000 epochs, with a learning rate α = 0.01 and a hidden layer width ofH = 256.",6. Experimental Results,[0],[0]
"For the WDAE, the penalty parameter was set at γ = 0.5 and for the DAE, σ2 = 0.5.",6. Experimental Results,[0],[0]
"The results are shown in Figure 5 (left column).
",6. Experimental Results,[0],[0]
The theoretical predictions (solid lines) in Figure 5 show good agreement with the actual learning dynamics (points).,6. Experimental Results,[0],[0]
"As predicted, both regularisation (orange) and noise (green) suppress the fixed point value associated with the different eigenvalues and, whereas regularisation delays learning (fewer fixed points are reached by the WDAE during training when compared to the DAE), the use of noise has no effect on training time.
",6. Experimental Results,[0],[0]
Similar agreement is shown for CIFAR-10 in the right column of Figure 5.,6. Experimental Results,[0],[0]
"Here, we trained each network with small randomly initialised weights using N = 30000 training samples for 5000 epochs, with a learning rate α = 0.001 and a hidden dimension H = 512.",6. Experimental Results,[0],[0]
"For the WDAE, the
penalty parameter was set at γ = 0.5 and for the DAE, σ2 = 0.5.
",6. Experimental Results,[0],[0]
"Next, we investigated whether these dynamics are at least also qualitatively present in nonlinear autoencoder networks.",6. Experimental Results,[0],[0]
"Figure 6 shows the dynamics of learning for nonlinear AEs, WDAEs and DAEs, using ReLU activations, trained on MNIST (N = 50000) and CIFAR-10 (N = 30000) with equal learning rates.",6. Experimental Results,[0],[0]
"For the DAE, the input was corrupted using sampled Gaussian noise with mean zero and σ2 = 3.",6. Experimental Results,[0],[0]
"For the WDAE, the amount of weight decay was manually tuned to γ = 0.0045, to ensure that both autoencoders displayed roughly the same degree of regularisation in terms of the fixed points reached.",6. Experimental Results,[0],[0]
"During the course of training, the identity mapping associated with each eigenvalue was estimated (see supplementary material), at equally spaced intervals of size 10 epochs.
",6. Experimental Results,[0],[0]
"The learning dynamics are qualitatively similar to the dy-
namics observed in the linear case.",6. Experimental Results,[0],[0]
Both noise and weight decay result in a shrinkage of the identity mapping associated with each eigenvalue.,6. Experimental Results,[0],[0]
"Furthermore, in terms of the number of training epochs, the DAE is seen to learn as quickly as a regular AE, whereas the WDAE incurs a delay in learning time.",6. Experimental Results,[0],[0]
"Although these experimental results stem from a single training run for each autoencoder, we note that wall-clock times for training may still differ because DAEs require some additional time for sampling noise.",6. Experimental Results,[0],[0]
Similar results were observed when using a tanh nonlinearity and are provided in the supplementary material.,6. Experimental Results,[0],[0]
There have been many studies aiming to provide a better theoretical understanding of DAEs.,7. Related Work,[0],[0]
"Vincent et al. (2008) analysed DAEs from several different perspectives, including manifold learning and information filtering, by establishing an equivalence between different criteria for learning and the original training criterion that seeks to minimise the reconstruction loss.",7. Related Work,[0],[0]
"Subsequently, Vincent (2011) showed that under a particular set of conditions, the training of DAEs can also be interpreted as a type of score matching.",7. Related Work,[0],[0]
This connection provided a probabilistic basis for DAEs.,7. Related Work,[0],[0]
"Following this, a more in-depth analysis of DAEs as a possible generative model suitable for arbitrary loss functions and multiple types of data was given by Bengio et al. (2013).
",7. Related Work,[0],[0]
"In contrast to a probabilistic understanding of DAEs, we present here an analysis of the learning process.",7. Related Work,[0],[0]
"Specifically inspired by Saxe et al. (2013a), as well as by earlier work on supervised neural networks (Opper, 1988; Sanger, 1989; Baldi & Hornik, 1989; Saad & Solla, 1995), we provide a theoretical investigation of the temporal behaviour of linear DAEs using derived equations that exactly describe their dynamics of learning.",7. Related Work,[0],[0]
"Specifically for the linear case, the squared error loss for the reconstruction contractive autoencoder (RCAE) introduced in Alain & Bengio (2014) is equivalent to the expected loss (over the noise) for the DAE.",7. Related Work,[0],[0]
"Therefore, the learning dynamics described in this paper also apply to linear RCAEs.
",7. Related Work,[0],[0]
For our analysis to be tractable we used a marginalised reconstruction loss where the gradient descent dynamics are viewed in expectation over the noise distribution.,7. Related Work,[0],[0]
"Whereas our motivation is analytical in nature, marginalising the reconstruction loss tends to be more commonly motivated from the point of view of learning useful and robust feature representations at a significantly lower computational cost (Chen et al., 2014; 2015).",7. Related Work,[0],[0]
"This approach has also been investigated in the context of supervised learning (van der Maaten et al., 2013; Wang & Manning, 2013; Wager et al., 2013).",7. Related Work,[0],[0]
"Also related to our work is the analysis by Poole et al. (2014), who showed that training autoencoders with noise (added at different levels of the network architecture),
is closely connected to training with explicit regularisation and proposed a marginalised noise framework for noisy autoencoders.",7. Related Work,[0],[0]
This paper analysed the learning dynamics of linear denoising autoencoders (DAEs) with the aim of providing a better understanding of the role of noise during training.,8. Conclusion and Future Work,[0],[0]
"By deriving exact time-dependent equations for learning, we showed how noise influences the shape of the loss surface as well as the rate of convergence to fixed point solutions.",8. Conclusion and Future Work,[0],[0]
"We also compared the learning behaviour of added input noise to that of weight decay, an explicit form of regularisation.",8. Conclusion and Future Work,[0],[0]
"We found that while the two have similar regularisation effects, the use of noise for regularisation results in faster training.",8. Conclusion and Future Work,[0],[0]
"We compared our theoretical predictions with actual learning dynamics on real-world data sets, observing good agreement.",8. Conclusion and Future Work,[0],[0]
"In addition, we also provided evidence (on both MNIST and CIFAR-10) that our predictions hold qualitatively for nonlinear DAEs.
",8. Conclusion and Future Work,[0],[0]
This work provides a solid basis for further investigation.,8. Conclusion and Future Work,[0],[0]
"Our analysis could be extended to nonlinear DAEs, potentially using the recent work on nonlinear random matrix theory for neural networks (Pennington & Worah, 2017; Louart et al., 2017).",8. Conclusion and Future Work,[0],[0]
Our findings indicate that appropriate noise levels help DAEs ignore low variance directions in the input; we also obtained new insights into the training time of DAEs.,8. Conclusion and Future Work,[0],[0]
"Therefore, future work might consider how these insights could actually be used for tuning noise levels and predicting the training time of DAEs.",8. Conclusion and Future Work,[0],[0]
"This would require further validation and empirical experiments, also on other datasets.",8. Conclusion and Future Work,[0],[0]
"Finally, our analysis only considers the training dynamics, while a better understanding of generalisation and what influences the quality of feature representations during testing, are also of prime importance.",8. Conclusion and Future Work,[0],[0]
"We would like to thank Andrew Saxe for early discussions that got us interested in this work, as well as the reviewers for insightful comments and suggestions.",Acknowledgements,[0],[0]
"We would like to thank the CSIR/SU Centre for Artificial Intelligence Research (CAIR), South Africa, for financial support.",Acknowledgements,[0],[0]
AP would also like to thank the MIH Media Lab at Stellenbosch University and Praelexis (Pty) Ltd for providing stimulating working environments for a portion of this work.,Acknowledgements,[0],[0]
"Denoising autoencoders (DAEs) have proven useful for unsupervised representation learning, but a thorough theoretical understanding is still lacking of how the input noise influences learning.",abstractText,[0],[0]
Here we develop theory for how noise influences learning in DAEs.,abstractText,[0],[0]
"By focusing on linear DAEs, we are able to derive analytic expressions that exactly describe their learning dynamics.",abstractText,[0],[0]
We verify our theoretical predictions with simulations as well as experiments on MNIST and CIFAR-10.,abstractText,[0],[0]
"The theory illustrates how, when tuned correctly, noise allows DAEs to ignore low variance directions in the inputs while learning to reconstruct them.",abstractText,[0],[0]
"Furthermore, in a comparison of the learning dynamics of DAEs to standard regularised autoencoders, we show that noise has a similar regularisation effect to weight decay, but with faster training dynamics.",abstractText,[0],[0]
We also show that our theoretical predictions approximate learning dynamics on real-world data and qualitatively match observed dynamics in nonlinear DAEs.,abstractText,[0],[0]
*,abstractText,[0],[0]
Learning Dynamics of Linear Denoising Autoencoders,title,[0],[0]
"Hospitalized patients are vulnerable to a wide range of adverse events, including cardiopulmonary arrests (Kause et al., 2004; Hogan et al., 2012; Yoon et al., 2016), acute respiratory failures (Mokart, 2013), septic shocks (Henry et al., 2015), and post-operative complications (Clifton
*Equal contribution 1University of California, Los Angeles, US.",1. Introduction,[0],[0]
"2University of Oxford, UK.",1. Introduction,[0],[0]
"3Alan Turing Institute, UK..",1. Introduction,[0],[0]
"Correspondence to: Ahmed M. Alaa <ahmedmalaa@ucla.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"et al., 2012).",1. Introduction,[0],[0]
"For a patient in a regular ward, the occurrence of any such event entails an unplanned transfer to an intensive care unit (ICU), the timing of which is a major determinant of the eventual outcome.",1. Introduction,[0],[0]
"Indeed, recent medical studies have confirmed that delayed transfer to the ICU is strongly correlated with morbidity and mortality (Mardini L, 2012; Mokart, 2013).",1. Introduction,[0],[0]
"The problem of delayed ICU transfer is enormous and acute: over 750,000 septic shocks and 200,000 cardiac arrests occur in the U.S. each year with mortality rates of 28.6% and 75% respectively (Merchant et al., 2011; Kumar et al., 2011).",1. Introduction,[0],[0]
"Fortunately, experts believe that much of these events could be prevented with accurate prognosis and early warning (Nguyen et al., 2007).
",1. Introduction,[0],[0]
"Motivated by the proliferation of electronic health records (EHRs) (currently available in more than 75% of hospitals in the U.S. (Charles et al., 2016))",1. Introduction,[0],[0]
we develop a datadriven real-time risk score that can promptly assess a hospitalized patient’s risk of clinical deterioration.,1. Introduction,[0],[0]
"Our risk score hinges on a novel continuous-time semi-Markovmodulated marked Hawkes process model for a monitored patient’s episode, i.e. the patient’s evolving (latent) clinical states and her corresponding (observed) physiological data.",1. Introduction,[0],[0]
"With the guidance of critical care experts, we conducted experiments on a dataset for a cohort of criticallyill patients admitted to a major academic medical center.",1. Introduction,[0],[0]
Results show that our risk score offers significant gains in the accuracy (and timeliness) of predicting clinical deterioration; our risk score attains a 23% improvement in the Area Under Receiver Operating Characteristic (AUROC) as compared to the technology currently deployed in our medical center.,1. Introduction,[0],[0]
"Since it confers a significant prognostic value in subacute care in wards, the proposed risk score is currently being installed in our medical center as a replacement for the current technology.
",1. Introduction,[0],[0]
The proposed probabilistic model (based on which our risk score is computed) captures a hospitalized patient’s entire episode as recorded in the EHR.,1. Introduction,[0],[0]
"A typical (critical care) patient episode comprises the time of her admission to the ward, the time of her admission to the ICU or discharge from the ward, and a temporal sequence of irregularly sampled physiological data that are collected during her stay in the ward (Johnson, 2016; Ghassemi et al., 2015).",1. Introduction,[0],[0]
"We model
a patient’s episode as being driven by a latent clinical state process, which we represent as a semi-Markov jump process (Yu, 2010), describing the evolution of the patient’s “severity of illness” over time.",1. Introduction,[0],[0]
"All the observable physiological variables are modulated by this process: the times at which clinicians decide to observe the patient’s physiological data are drawn from a Hawkes point process (Hawkes & Oakes, 1974), the intensity of which is modulated by the patient’s clinical state process, whereas the observed physiological data is drawn from a switching multi-task Gaussian process with hyper-parameters that depend on the patient’s clinical state.",1. Introduction,[0],[0]
The patient episode is thus a marked Hawkes process –with the physiological data serving as the marks– that is modulated by the patient’s clinical states.,1. Introduction,[0],[0]
"We provide a detailed description of the model in Section 3, and then propose an EM-based algorithm for learning its parameters from the EHR data in Section 4.
",1. Introduction,[0],[0]
A distinctive feature of our model is its ability to incorporate informative clinical judgments into the generative process for the patient’s episode.,1. Introduction,[0],[0]
The manifestation of informative clinical judgments in the EHR episodes is doublefaceted: the patients’ episodes are both “informatively sampled” and “informatively censored”.,1. Introduction,[0],[0]
"Informative sampling results from the fact that clinicians decide to observe the patient’s physiological data more intensely if they believe that the patient is in a “bad” clinical state (Moskovitch et al., 2015; Qin & Shelton, 2015)– a belief that is based on either the clinician’s own assessment of the patient’s state, or the communication between the patient and the ward staff (Kyriacos et al., 2014).",1. Introduction,[0],[0]
"Informative censoring results from the clinicians’ decision on when to send the patient to the ICU or discharge her from the ward, which is indeed informative of the “ clinical deterioration” or “clinical stability” onsets.",1. Introduction,[0],[0]
"In our model, informative censoring is taken into account by adopting an absorbing semi-Markov chain as a model for the patient’s latent clinical states; a patient’s risk score at any point of time is thus defined as the probability of eventual absorption in a “clinical deterioration” state.
",1. Introduction,[0],[0]
Related work:,1. Introduction,[0],[0]
"Marked point processes have been recently used in a very different context to model check-in data (Du et al., 2016; Pan et al., 2016), but we are not aware of any attempts for their deployment in the medical context.",1. Introduction,[0],[0]
"Most of the previous works on risk prognosis for critical care patients viewed informative censoring as a “surrogate label” for a patient’s clinical deterioration, and hence used those labels to train a supervised (regression) model using the physiological data in a fixed-size time window before censoring.",1. Introduction,[0],[0]
"The supervised models used in the literature included logistic regression (Ho et al., 2012; Saria et al., 2010), SVMs (Wiens et al., 2012), Gaussian processes (Ghassemi et al., 2015; Yoon et al., 2016) and recurrent neural networks (Che et al., 2016).",1. Introduction,[0],[0]
"The main limitation of this approach is that, in addition to the fact that it gener-
ally does not deal with informatively sampled episodes, it does not model the entire patient’s physiological trajectory, and hence it does not accurately capture intermediate (subtle) deterioration stages that are indicative of future severe deterioration stages, which leads to a sluggish risk assessment and delayed ICU alarms.
",1. Introduction,[0],[0]
"Another strand of literature has focused on building probabilistic models, usually variants of Hidden Markov Models (HMMs), for the entire patient’s physiological trajectories; applications have ranged from disease progression modeling to neonatal sepsis prediction (Wang et al., 2014; Stanculescu et al., 2014).",1. Introduction,[0],[0]
"These models are not capable of dealing with irregularly-sampled data, do not deal with informatively sampled episodes, and are restricted to the Markovianity assumption which entails unrealistically memoryless clinical state transitions.",1. Introduction,[0],[0]
"In (Henry et al., 2015), a ranking algorithm was used to construct a risk score for sepsis shocks; however, the approach therein requires the clinicians to provide assessments that order the disease severity at different time instances– we typically do not have such data in the EHR for ward patients.
",1. Introduction,[0],[0]
"Various works in the medical literature have proposed “expert-based” medical risk scores for prognosis in hospital wards (Morgan et al., 1997; Parshuram et al., 2009), some of which are currently used in practice.",1. Introduction,[0],[0]
"The medical literature has also suggested the use of mortality risk scores that are normally used in the ICU, such as APACHE-II and SOFA, as risk scores for ward patients (Yu et al., 2014).",1. Introduction,[0],[0]
"However, recent systematic reviews have demonstrated the modest net clinical utility of all these scores (Cvach, 2012).",1. Introduction,[0],[0]
"More recently, a data-driven medical risk score based on a simple regression model, known as the Rothman index, has been developed and commercialized (Finlay et al., 2014; Rothman et al., 2013).",1. Introduction,[0],[0]
"The Rothman index is currently deployed in various major hospitals in the U.S. including our medical center; in Section 5, we show that our risk score significantly outperforms the Rothman index in terms of AUROC and timeliness of ICU admission alarms.",1. Introduction,[0],[0]
The subacute care data in an EHR typically comprises a set of episodes; each episode is a sequence of vital signs and lab tests (physiological data) that have been gathered (by clinicians) for a hospitalized patient at irregularly spaced time instances during her stay in a ward.,2. Structure of the EHR Data,[0],[0]
"The episode starts at the time of admission to the ward, and is concluded by either an unplanned admission to the ICU, which means that the patient was clinically deteriorating, or a discharge from the ward, which means that the patient was clinically stable.",2. Structure of the EHR Data,[0],[0]
"We denote an EHR dataset D that comprises the episodes for D patients as D = {Ed}Dd=1, where Ed is the episode for the dth patient, and is defined as
Ed = ({ydm, tdm} Md m=1, T d c , l d), with ydm being the m th Qdimensional physiological variable (vital signs and lab test outcomes) for patient d observed at time tdm, and the total number of samples observed for that patient during her episode is Md.",2. Structure of the EHR Data,[0],[0]
"The duration of patient d’s stay in the ward is denoted by T dc , whereas her endpoint outcome (clinical deterioration and ICU admission, or clinical stability and discharge) is declared via a binary variable ld; the realization ld = 1 means that patient d was admitted to the ICU, and ld = 0 means that the patient was discharged home.",2. Structure of the EHR Data,[0],[0]
"We stress that the labels ld associated with every episode are neither noisy nor entirely subjective: we assign a label ld = 1 for patients who actually needed a therapeutic intervention in the ICU after an unplanned admission, and assign a label ld = 0 for patients who were discharged and not re-admitted shortly after.",2. Structure of the EHR Data,[0],[0]
We excluded all post-surgical ward patients for whom an ICU admission was preordained since for those patients the prognosis problem is not relevant.,2. Structure of the EHR Data,[0],[0]
"Our dataset comprises thousands of episodes for patients admitted to a large medical center over a 3-year period; all the episodes display the structure described above.
",2. Structure of the EHR Data,[0],[0]
The clinicians were right!,2. Structure of the EHR Data,[0],[0]
"Clinical judgments manifest in the dth episode of D through informative sampling (encoded in the observation times {tdm} Md m=1), and informative censoring (encoded in the episode duration T dc and the endpoint outcome l d).",2. Structure of the EHR Data,[0],[0]
Figure 1 is a depiction for both informative sampling and informative censoring.,2. Structure of the EHR Data,[0],[0]
"In Figure 1, we estimate the physiological data (time-varying) sampling rate using all the episodes in our dataset over a time horizon of 35 hours before ICU admission for deteriorating patients (i.e. patients with ld = 1, with t = 0 being the ICU admission time), and we compute the same estimates for stable patients (i.e. patients with ld = 0, with t = 0 being the discharge time).
",2. Structure of the EHR Data,[0],[0]
"We can see from the trends in Figure 1 that as the deteriorating patient approaches the ICU admission time, the clinicians tend to sample her physiological data more intensely, whereas as the stable patient approaches the discharge time, the clinicians tend to have a more relaxed schedule for observing her vital signs and lab tests.",2. Structure of the EHR Data,[0],[0]
The divergence between the sampling rates for deteriorating and stable patient groups increases as the patients approach their ICU admission and discharge onsets– that is because the clinicians become less uncertain about the patient’s state as time progresses.,2. Structure of the EHR Data,[0],[0]
"We have tested the hypothesis that the sampling rate of deteriorating patients is –on average– larger than that for stable patients in the last 24 hours before ICU admission or discharge via a two-sample t-test with a significance level of 0.05, and the hypothesis was accepted.
",2. Structure of the EHR Data,[0],[0]
"The take-away from Figure 1 is that the clinician’s judgment of the patient’s clinical state –manifesting in the vital signs and lab tests sampling rates– is very predictive of the
endpoint outcomes.",2. Structure of the EHR Data,[0],[0]
"This implies that there is a room for “learning from the informative clinical judgments”; that is, one can infer the patients’ latent states over time by using the clinicians’ observable sampling patterns as proximal noisy labels for those latent states.",2. Structure of the EHR Data,[0],[0]
Now we present a probabilistic model for the episodes {Ed}Dd=1 that captures the informative sampling and censoring effects discussed in Section 2.,3. The Semi-Markov-Modulated Marked Hawkes Process Model,[0],[0]
"We start by modeling the patient’s latent clinical state process in Subsection 3.1, before modeling the observable variables in Subsection 3.2.",3. The Semi-Markov-Modulated Marked Hawkes Process Model,[0],[0]
We assume that each patient’s episode is governed by an underlying latent clinical state process X(t) that represents the evolution of her “clinical well-being” over time.,3.1. Latent Clinical States,[0],[0]
The patient’s latent clinical state X(t) at any point of time t ∈ R+,3.1. Latent Clinical States,[0],[0]
"(t = 0 corresponds to the time of admission to the ward) belongs to a finite space X comprising N states, i.e. X = {1, 2, . .",3.1. Latent Clinical States,[0],[0]
", ., N}.",3.1. Latent Clinical States,[0],[0]
"We model informative censoring by assuming that states 1 and N are absorbing states; state 1 is the state of clinically stability, at which the patient can be safely discharged home, whereas state N is the state of clinical deterioration, at which the patient needs to be admitted to the ICU.",3.1. Latent Clinical States,[0],[0]
"Whenever the patient is in state 1 or N , her episode is terminated by the clinicians shortly after.",3.1. Latent Clinical States,[0],[0]
"All other states in X/{1, N} are transient states in which the patient needs vigilant monitoring by the ward staff.
",3.1. Latent Clinical States,[0],[0]
"The clinical state process X(t) is a semi-Markov jump process (Yu, 2010), i.e. X(t) =",3.1. Latent Clinical States,[0],[0]
∑K n=1 Xn · 1{τn≤t<τn+1} is a semi-Markov process for which every new state realization,3.1. Latent Clinical States,[0],[0]
"Xn starts at a jump time τn, where τ1 = 0, and lasts for a random sojourn time Sn = τn+1",3.1. Latent Clinical States,[0],[0]
− τn.,3.1. Latent Clinical States,[0],[0]
"A total number of K states are realized in the path X(t), where K is indeed random, and XK ∈ {1, N}, i.e. the patient’s episode is concluded by either clinical deterioration or stability.",3.1. Latent Clinical States,[0],[0]
"The advantage of adopting a semi-Markovian model for the clinical state process is that unlike Marko-
vian models, semi-Markovianity does not imply memoryless transitions– the transition probability from one clinical state to another at any time depends on the time spent in the current state, a property that has been recently validated in various clinical state models (Taghipour et al., 2013).",3.1. Latent Clinical States,[0],[0]
"We adopt an explicit-duration model for the state sojourn times (Johnson & Willsky, 2013): the nth state sojourn time Sn is drawn from a Gamma distribution1 with state-specific parameters as follows
Sn|(Xn = i) ∼ Gamma(γi), ∀i ∈ X .",3.1. Latent Clinical States,[0],[0]
"(1)
Since the model includes two absorbing states (1 and N ) for which the notion of sojourn time is inapplicable, we define the variables Sn of such states as the clinicians’ “response times” for admitting patients to the ICU or discharging them upon the clinical deterioration/stability onset.",3.1. Latent Clinical States,[0],[0]
"The transitions among clinical states are governed by a semiMarkov transition kernel matrix P = (pij)i,j , i.e.
P(Xn+1 = j|Xn = i) =",3.1. Latent Clinical States,[0],[0]
"pij , (2)
where self-transitions are eliminated for all transient states (Yu, 2010; Johnson & Willsky, 2013), i.e. pii = 0, ∀i ∈ X/{1, N}, and enforced for the two absorbing states pii = 1, ∀i ∈ {1, N}.",3.1. Latent Clinical States,[0],[0]
"The initial state distribution is given by π = (πi) N i=1, where πi = P(X(0) = i).",3.1. Latent Clinical States,[0],[0]
"Every episode Ed in an EHR dataset D is associated with a latent clinical state trajectory {Xdn, Sdn}K d
n=1, but we can only observe the absorbing state realization XKd",3.1. Latent Clinical States,[0],[0]
= ld in the EHR data.,3.1. Latent Clinical States,[0],[0]
"The patient’s latent clinical state process X(t) manifests in two ways: (1) it modulates the intensity of sampling the patient’s physiological variables (informative sampling), and (2) it modulates the distributional properties of the observed physiological variables.",3.2. Observable Physiological Data,[0],[0]
"We capture these two effects via a marked point process model for the patient’s
1We model the sojourn time via a Gamma distribution since it encompasses memoryless exponential distributions of Markov models as a special case (Liu et al., 2015).
episode E : the marked point process {(ym, tm)}m∈N+ comprises an observation process {tm}m∈N+ , which represents the physiological variables’ sampling times, and a mark process {ym}m∈N+ , which represents the realized physiological variables at these sampling times.",3.2. Observable Physiological Data,[0],[0]
The distributional specifications of our marked point process are given in the following Subsections.,3.2. Observable Physiological Data,[0],[0]
"We model the observation process generating the physiological variables’ observation times {tm}m∈N+ as a doubly stochastic point process whose intensity, λ(t), is a stochastic process modulated by the latent clinical state process X(t).",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"In particular, the observation process {tm}m∈N+ is modeled as a one-dimensional Hawkes process with a linear self-exciting intensity function λ(t,X(t))",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"(Lee et al., 2016), i.e.
λ(t,X(t) = i) = λoi + αi ∑
τ<tm<t
e−βi(t−tm), (3)
∀",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"i ∈ X ,, where λoi , αi and βi are the state-dependent intensity parameters, e−βi(t−tm) is an exponential triggering kernel, and τ <",3.2.1. THE OBSERVATION PROCESS,[0],[0]
t is the time of the most recent jump in X(t).,3.2.1. THE OBSERVATION PROCESS,[0],[0]
"In order to ensure the local stationarity of the Hawkes process within the sojourn time of every latent state, we assume that αiβi < 1, ∀i ∈ X (Roueff et al., 2016); the expected value of the intensity function is therefore given by E[λ(t,X(t) = i)]",3.2.1. THE OBSERVATION PROCESS,[0],[0]
= λ,3.2.1. THE OBSERVATION PROCESS,[0],[0]
o,3.2.1. THE OBSERVATION PROCESS,[0],[0]
"i
",3.2.1. THE OBSERVATION PROCESS,[0],[0]
1−αiβi .,3.2.1. THE OBSERVATION PROCESS,[0],[0]
"For βi = ∞
or αi = 0, we recover a modulated Poisson process as a special case (Pan et al., 2016).
",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"In Figure 2, we depict the observation times in two patients’ episodes: patient A being a clinically deteriorating patient, and patient B being a clinically stable patient.",3.2.1. THE OBSERVATION PROCESS,[0],[0]
We can see that patient A’s episode had its sampling rate escalating as her condition was worsening; the sampling rate remained intense after she was admitted to the ICU.,3.2.1. THE OBSERVATION PROCESS,[0],[0]
"On the other hand, patient B’s episode exhibited a decelerating sampling rate on her path to clinical stability.",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"We can also notice that
the observation times display a subtle “clustered” pattern that point out to their temporal dependencies– indeed, the clinicians are not memoryless, and the times at which they observe the physiological data are dependent.",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"In the light of the above, the modulated Hawkes process described above appears to be a sensible model for the observation process {tm}m∈N+ as it captures both the time-varying intensity and the temporal dependencies illustrated in Figure 2.",3.2.1. THE OBSERVATION PROCESS,[0],[0]
Now we provide the distributional specification of the mark process {ym}m∈N+ .,3.2.2. THE MARK PROCESS,[0],[0]
"Since the physiological data are irregularly sampled from an underlying continuous-time physiological process Y (t) at the sampling times determined by the observation process {tm}m∈N+ , a convenient model for Y (t) is a switching Gaussian Process defined as follows: Y (t) = ∑K n=1 Yn(t)1{τn≤t<τn+1}, with
Yn(t)|(Xn = i) ∼ GP(mi(t), ki(t, t′)), (4)
where mi(t) and ki(t, t′) are the state-dependent mean function and covariance kernel, respectively.",3.2.2. THE MARK PROCESS,[0],[0]
"We use a constant mean function mi(t) = mi and a Matérn kernel given by
ki(t, t ′)",3.2.2. THE MARK PROCESS,[0],[0]
"=
(√ 2vi−1 |t−t′|
ℓi
)vi− 12 Kvi− 12 (√ 2vi−1 |t−t′| ℓi ) 2vi−
3 2 Γ(vi − 12 )
,
(5) where vi ∈ N+, ℓi ∈ R+, Γ(.) is the Gamma function and Kvi− 12 (.) is a modified Bessel function (Rasmussen, 2006).
",3.2.2. THE MARK PROCESS,[0],[0]
"Our choice for the Matérn kernel is motivated by its ability to represent various commonly used stochastic processes; for instance, when vi = 1, then Yn(t)|(Xn = i) is an Ornstein-Uhlenbeck process (Rasmussen, 2006), whereas for a general integer value of vi, Yn(t)|(Xn = i) is a continuous-time analogue of the Auto-regressive process AR(vi)– a process that has been widely used to model physiological time-series data (Stanculescu et al., 2014).",3.2.2. THE MARK PROCESS,[0],[0]
"By constructing Yn(t) as a continuous-time analog of the AR model, the process Y (t) = ∑K n=1 Yn(t)1{τn≤t≤τn+1} becomes a continuous-time switching AR model that is modulated by the patient’s latent clinical state process X(t).",3.2.2. THE MARK PROCESS,[0],[0]
"We observe the continuous-time process Y (t) only at the sampling times dictated by the observation process (tm)m∈N+ , and the resulting process {ym}m∈N+ defines the mark process.",3.2.2. THE MARK PROCESS,[0],[0]
"The observation process together with the mark process, both modulated by the latent clinical state process X(t), constitute a marked Hawkes process, which completely describes a patient’s episode.
",3.2.2. THE MARK PROCESS,[0],[0]
"The mark process defined above is one-dimensional, and hence we need to extend the definition to handle a multidimensional process that represents multiple lab tests and
vital signs.",3.2.2. THE MARK PROCESS,[0],[0]
"In other words, we seek a continuous-time analog of the switching Vector Auto-regressive (VAR) model rather than an AR model2.",3.2.2. THE MARK PROCESS,[0],[0]
"This is achieved by adopting a multi-task Gaussian Process as a model for the Qdimensional physiological process Y (t) ∈ RQ (Durichen et al., 2015).",3.2.2. THE MARK PROCESS,[0],[0]
"That is, we assume that Yn(t)|(Xn = i) ∼ GP(mi(t),Ki(t, t′)), where the covariance kernel Ki(t, t
′) = {ki(r, g, t, t′)}Qr,g=1 is based on the intrinsic correlation model (Bonilla et al., 2007), i.e. Ki(t, t′) can be written in the following separable form
ki(r, g, t, t ′)",3.2.2. THE MARK PROCESS,[0],[0]
"= Σi(r, g) · ki(t, t′), (6)
where ki(r, g, t, t′) is the covariance between the rth physiological variable at time t and the gth physiological variable at time t′,",3.2.2. THE MARK PROCESS,[0],[0]
"Σi is a Q × Q intrinsic correlation matrix, and ki(t, t′) is the Matérn kernel in (5).",3.2.2. THE MARK PROCESS,[0],[0]
"For every state i, we denote the multi-task GP parameter set as Θi = (mi(t),Ki(t, t
′)), and the Hawkes process parameter set as Λi.",3.2.2. THE MARK PROCESS,[0],[0]
"The entire model parameters can be bundled in the parameter set Ω as follows
Ω = {P, (πi, γi,Λi,Θi)i∈X }.
",3.2.2. THE MARK PROCESS,[0],[0]
"Given a parameter set Ω, we can easily generate sample patient episodes from our model by first sampling a state sequence {X1, . .",3.2.2. THE MARK PROCESS,[0],[0]
.,3.2.2. THE MARK PROCESS,[0],[0]
", XK} using the semi-Markov transition kernel, then sampling a corresponding sequence of sojourn times {S1, . .",3.2.2. THE MARK PROCESS,[0],[0]
.,3.2.2. THE MARK PROCESS,[0],[0]
", SK} from the state-dependent Gamma distributions, then sampling a set of multi-task Gaussian process {Y1(t), . .",3.2.2. THE MARK PROCESS,[0],[0]
"., YK(t)}, and finally sampling a sequence of observation times {tm}Mm=1 using Ogata’s modified thinning algorithm (Ogata, 1981).",3.2.2. THE MARK PROCESS,[0],[0]
Figure 3 depicts one patient episode sampled from our model.,3.2.2. THE MARK PROCESS,[0],[0]
(An algorithm for sampling episodes from our model is provided in Appendix B in the supplementary material.),3.2.2. THE MARK PROCESS,[0],[0]
"In this Section, we develop an offline learning algorithm that learns the model parameter Ω using the offline training episodes in D, and a real-time risk scoring algorithm that computes a hospitalized patient’s risk over time.",4. Learning and Inference,[0],[0]
"The learning algorithm operates by first detecting change-points in the physiological data and the observation process; using the detected change-points, the algorithm segments each episode into a sequence of states, and uses an EM algorithm to estimate the model parameters.",4. Learning and Inference,[0],[0]
"The real-time risk scoring algorithm operates by inferring the patient’s current state via forward-filtering, and then computing the probability of eventual absorption in the deteriorating state.
",4. Learning and Inference,[0],[0]
"2In Appendix A of the supplementary material, we establish the connection between the switching multi-task Gaussian Process model described herein and the conventional VAR model, showing that the former is the continuous-time analog of the latter.
0",4. Learning and Inference,[0],[0]
"5 10 15 20 25 0
2
4
6
8
10
Time t
L a te n t S ta te s a n d",4. Learning and Inference,[0],[0]
In te n si ty F u,4. Learning and Inference,[0],[0]
"n ct io n
Clinical state space is X = {1,2,3,4} (state 4 is the “clinical deterioration” state)
0 5 10 15 20 25 0
5
10
15
20
25
Time t
P",4. Learning and Inference,[0],[0]
h y si o lo,4. Learning and Inference,[0],[0]
"g ic a l V a ri a b le s
Y (t) {ym} M m=1
Intensity function λ(t,X(t))",4. Learning and Inference,[0],[0]
"Clinical state process X(t) Observation process {tm}
Informative censoring l = 1
X1 = 2 X2 = 3
X3 = 4
Figure 3.",4. Learning and Inference,[0],[0]
An episode sampled from the proposed model.,4. Learning and Inference,[0],[0]
"Estimating the model parameters Ω from the dataset D = {Ed}Dd=1 is a daunting task due to the hiddenness of the patients’ clinical state trajectories {Xdn, Sdn}Dd=1; an application of MCMC-based inference methods, such as the method in (Qin & Shelton, 2015), will incur an excessive computational cost for a complex model like ours.",4.1. The Offline Learning Algorithm,[0],[0]
We therefore developed an efficient three-step learning algorithm that capitalizes on the structure of the patients’ episodes in order to find a point estimate Ω̂ for Ω.,4.1. The Offline Learning Algorithm,[0],[0]
"The three steps of the our learning algorithm are listed hereunder3.
",4.1. The Offline Learning Algorithm,[0],[0]
"Step 1: Change-point detection We first estimate the jump times {τd1 , . .",4.1. The Offline Learning Algorithm,[0],[0]
"., τdKd} for every episode Ed in D.",4.1. The Offline Learning Algorithm,[0],[0]
"This is achieved by using the E-divisive change-point detection algorithm (Matteson & James, 2014).",4.1. The Offline Learning Algorithm,[0],[0]
"Since the E-divisive algorithm is nonparametric, we are able to estimate the onsets of all clinical states, i.e. the jump times of Xd(t), prior to finding the estimate Ω̂.",4.1. The Offline Learning Algorithm,[0],[0]
"We let the E-divisive algorithm jointly detect changes in the distributions of the observable variables {ydm} Md m=1 and the observation process {tdm} Md m=1 by creating an augmented vector of observables that comprises both the physiological observations and a “differential” observation process, i.e.
{τ̂d1 , . .",4.1. The Offline Learning Algorithm,[0],[0]
"., τ̂dKd}",4.1. The Offline Learning Algorithm,[0],[0]
"= E-divisive((y d 1 ,∆t d 1), . . .",4.1. The Offline Learning Algorithm,[0],[0]
", (y d Md ,∆t d Md)),
where ∆tdm = t d m − tdm−1, with ∆td1 = 0.",4.1. The Offline Learning Algorithm,[0],[0]
"By the end of this step, we obtain an estimate for the start and end times of all clinical state realizations for every episode Ed.
",4.1. The Offline Learning Algorithm,[0],[0]
"Step 2: Maximum Likelihood Estimation (MLE) of the absorbing states’ parameters By virtue of informative censoring, we know the identities of all the absorbing states, i.e. XdKd = l
d, ∀d.",4.1. The Offline Learning Algorithm,[0],[0]
"Now that we have estimates for the onsets of the absorbing states, obtained from step 1, then we can estimate the response
3The details for all the algorithms in this Section are provided in Appendix C in the supplementary material.
times as ŜdKd = T d c",4.1. The Offline Learning Algorithm,[0],[0]
"− τ̂dKd , ∀1",4.1. The Offline Learning Algorithm,[0],[0]
"≤ d ≤ D. Define the (fully observable) sub-dataset Di as follows
Di = { (ym, tm){tm≥τ̂d
Kd }, Ŝ
d",4.1. The Offline Learning Algorithm,[0],[0]
"Kd : l
d = i, Ed ∈ D } ,
∀i ∈ {0, 1}.",4.1. The Offline Learning Algorithm,[0],[0]
"Given such a fully fledged specification of the absorbing states’ onsets, identities, and the corresponding physiological variables, we can directly apply MLE to estimate the parameters Θ1,ΘN , γ1, γN ,Λ1 and ΛN .",4.1. The Offline Learning Algorithm,[0],[0]
"Using the dataset Di, the parameter Θi is estimated using the gradient method for Gaussian processes as in (Bonilla et al., 2007), γi is estimated using the standard MLE estimating equations, and Λi is estimated by maximizing the recursive likelihood formula in (Ogata, 1981) using the Nelder-Mead simplex method (Nelder & Mead, 1965).
",4.1. The Offline Learning Algorithm,[0],[0]
"Step 3: Estimation of the transient states’ parameters using the EM algorithm While the absorbing states are observable, the transient states are all hidden.",4.1. The Offline Learning Algorithm,[0],[0]
"In order to estimate the parameters P, {Θi}N−1i=2 , {γi} N−1 i=2 , and {Λi} N−1 i=2 , we use the jump times’ estimates {τ̂d1 , . .",4.1. The Offline Learning Algorithm,[0],[0]
".,",4.1. The Offline Learning Algorithm,[0],[0]
"τ̂dKd−1} (obtained from step 1) in order to segment every episode d into a set of finite transition,",4.1. The Offline Learning Algorithm,[0],[0]
and hence we obtain a discrete-time HMM-like process.,4.1. The Offline Learning Algorithm,[0],[0]
"We truncate all the episodes by removing the data belonging to the absorbing state, and run the EM algorithm in order to estimate the transient state parameters.",4.1. The Offline Learning Algorithm,[0],[0]
The EM algorithm takes advantage of informative censoring in the forward-backward message passing stage by computing the backward messages conditioned on the identity of the endpoint absorbing state ld for every episode d.,4.1. The Offline Learning Algorithm,[0],[0]
"Having learned the model parameters Ω̂ from an offline dataset D, we now explain how risk scoring is conducted in real-time for a newly hospitalized patient.",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"The patient risk score at time t is denoted by R(t), and is defined as R(t) = P(X(∞)",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
= N,4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"| {ym, tm}, tm ≤ t, Ω̂).",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"That is, the risk score R(t) is the probability of being eventually absorbed in the deteriorating state N given the observable physiological data up to time t. Using Bayes’ rule, the risk score R(t) is given by∑ i∈X P(X(t) =",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"i | {ym, tm})︸ ︷︷ ︸",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
Current state · P(X(∞) = N |X(t) = i)︸ ︷︷ ︸,4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"Future transition ,
where the “current state” term is computed in real-time using the efficient (dynamic programming) forward-filtering algorithm, whereas the “future transition” term is computed offline using the estimated model parameters.",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"In order to evaluate the prognostic utility of our model, we conducted experiments on a dataset D comprising informa-
tion on patient admissions to a major medical center over a 3-year period, and compared the proposed risk score defined in Subsection 4.2 with other competing baselines.",5. Experiments,[0],[0]
"We briefly describe our dataset in the next Subsection, and then present the experimental results.",5. Experiments,[0],[0]
A very detailed description for our dataset and the implementation of the baselines is provided in Appendix D in the supplementary material.,5. Experiments,[0],[0]
Each patient record in D is an episode that is formatted as described in Section 2.,5.1. Data Description,[0],[0]
"The patients’ cohort in D is very heterogeneous; diagnoses included sepsis, hypertension, renal failure, leukemia, septicemia and pneumonia.",5.1. Data Description,[0],[0]
"Each patient’s episode in D comprises 21 vital signs and lab tests that are collected for the patient over time, along with the time instances at which they where collected.",5.1. Data Description,[0],[0]
"The vital signs include diastolic and systolic blood pressure, Glasgow coma scale score, heart rate, eye opening, respiratory rate, temperature, O2 saturation and device assistance, best motor and verbal responses.",5.1. Data Description,[0],[0]
"The lab tests included measurements of chloride, glucose, urea nitrogen, white blood cell count, creatinine, hemoglobin, platelet count, potassium, sodium and CO2.",5.1. Data Description,[0],[0]
"In all the experiments conducted in this Section, we split D into a training set with admissions over a 2.5-year period (5,000 episodes) and a testing set with admissions over a 6-month period (1,094 episodes).",5.1. Data Description,[0],[0]
"We ran the offline learning algorithm in Subsection 4.1 (with 1000 EM iterations) on the 5,000 training episodes in D, and obtained an estimate Ω̂ for the semi-Markovmodulated marked Hawkes process that describes the patient cohort.",5.2. Results,[0],[0]
"Using the Bayesian information criterion, we selected an instantiation of our model with 4 clinical states, where state 1 is the clinical stability (absorbing) state, and state 4 is the clinical deterioration (absorbing) state.",5.2. Results,[0],[0]
"The learned Hawkes process intensity functions for these two states are given by
λ(t, 1) = 0.55︸︷︷︸ Baseline intensity ↓",5.2. Results,[0],[0]
"+ 0.2 ∑ tm
e−8.46(t−tm)︸ ︷︷ ︸",5.2. Results,[0],[0]
"Temporal dependencies ↓ ,
λ(t, 4) = 0.82︸︷︷︸ Baseline intensity ↑ + 0.16 ∑ tm
e−1.36(t−tm)︸ ︷︷ ︸",5.2. Results,[0],[0]
"Temporal dependencies ↑ ,
where λ(t,X(t)) is measured in samples per hour.",5.2. Results,[0],[0]
"We note that the estimated Hawkes process parameters accurately describe the clinicians’ judgments; when the patient is in the deteriorating state (state 4), the clinicians tend to observe her physiological measurements more frequently.",5.2. Results,[0],[0]
"This manifests in the baseline intensity of λ(t, 4) being
50% higher than that of λ(t, 1).",5.2. Results,[0],[0]
"Moreover, we note that when the patient is clinically stable (state 1), the temporal dependencies between the observation times almost disappear as the exponential triggering kernel plays little role in determining the observation times, i.e. λ(t, 1) ≈ 0.55, which renders the observation process closer to a Poisson process.",5.2. Results,[0],[0]
"Contrarily, when the patient is deteriorating, strong temporal dependencies are displayed in the observation times– this is intuitive since for a deteriorating patient, the follow-up times decided by the clinicians strongly depend on what have been observed in the past.",5.2. Results,[0],[0]
"These distinguishing state-specific features of the clinical judgments are the essence of informative sampling, which allows us to integrate physiological data together with clinical experience while learning the patient’s physiological model.
",5.2. Results,[0],[0]
We illustrate the value of informative sampling in Figure 4 through an episode for a cardiac patient who was admitted in the ward for 1 week before being sent to the ICU upon a cardiac arrest.,5.2. Results,[0],[0]
"When running the offline learning algorithm in Section 4.1 while assuming that the observation process {tdm}m is uninformative (i.e. λ(t, i) = λ(t, j), ∀i, j ∈ X ), the detected clinical deterioration onset is only 10 hours ahead of the cardiac arrest event (onset (2) in Figure 4): this is because the states are estimated solely based on the physiological data, and hence the clinical deterioration state is detected only when the patient’s heart rate fell below 60 beats per minute (BPM).",5.2. Results,[0],[0]
"When running the learning again but with informative sampling taken into account, the detected clinical deterioration onset is 40 hours ahead of the cardiac arrest event (onset (1) in Figure 4); this is the time instance at which the clinicians decided to monitor the patient more vigilantly (i.e. more intense sampling rate) even though the evidence for an upcoming cardiac arrest in her heart rate trajectory was rather subtle.",5.2. Results,[0],[0]
"The clinicians’ decision to intensely observe the patient’s physiological trajectory is based either on their experience, or on apparent symptoms or complains from the patient that were not recorded in the EHR.",5.2. Results,[0],[0]
"By integrating the clinicians’ judgments into our model, we are able to capture the subtleties in the patients’ temporal physiological parameters, and hence learn more accurate representations for the clinical states.
",5.2. Results,[0],[0]
"We then computed the real-time risk score (as described in Subsection 4.2) for the testing episodes, and compared its sensitivity-precision AUCROC with that of the baseline risk scoring methods listed hereunder.
",5.2. Results,[0],[0]
"Medical Risk Scores: we considered the two most commonly used medical risk scores in regular wards– the MEWS score (Morgan et al., 1997) and the Rothman index4 (Rothman et al., 2013).",5.2. Results,[0],[0]
"We implemented the MEWS score and the Rothman index as described in (Kyriacos et al., 2014) and (Rothman et al., 2013), respectively.",5.2. Results,[0],[0]
"We also compared with the APACHE-II and SOFA scores.
",5.2. Results,[0],[0]
"Machine Learning Algorithms: we considered the traditional approach for real-time risk scoring, which treats informative censoring as a surrogate label based on which a supervised regression model is learned offline, and then risk scoring is applied in real-time using the temporal data within a sliding window– we call these methods “slidingwindow methods”.",5.2. Results,[0],[0]
"We implemented sliding-window methods based on logistic regression (Ho et al., 2012; Saria et al., 2010), random forests, and Gaussian process regression (Ghassemi et al., 2015; Yoon et al., 2016).",5.2. Results,[0],[0]
"In addition, we compared our risk score with a standard Hidden Markov Model with Gaussian emissions.",5.2. Results,[0],[0]
"The relevant physiological measurements for every baseline were selected through the correlated feature selection method (Yu & Liu, 2003).",5.2. Results,[0],[0]
"The hyper-parameters of all the baselines, including the size of the sliding window for the supervised learning methods, were optimized via cross-validation.",5.2. Results,[0],[0]
"To handle the irregularly sampled data, we discretized the time horizon into 1-hour steps and fed the baselines with interpolated, discrete-time episodes.
",5.2. Results,[0],[0]
"In Table 1, we compare the performance of our risk score with the baselines in terms of the sensitivity-precision AU-
4At the time of conducting these experiments, the Rothman index was deployed in more than 60 major hospitals in the US.
ROC.",5.2. Results,[0],[0]
"As we can see, the proposed risk score offers a 23% AUROC improvement as compared to the best performing medical risk score –the Rothman index– which was the score deployed in our medical center at the time of conducting this experiment.",5.2. Results,[0],[0]
"Moreover, our risk score also provides significant gains over discriminative sliding-window regression models; the proposed risk score achieves a 11.5% AUROC improvement as compared to the best performing ML algorithm (random forest).",5.2. Results,[0],[0]
"In addition, the proposed risk score achieves a 16% AUROC improvement as compared to a standard HMM with Gaussian emission variables5.",5.2. Results,[0],[0]
"On average, our risk score prompts ICU alarms 8 hours before the censoring time at a sensitivity of 50% and precision of 35%.",5.2. Results,[0],[0]
"We stress that while computing our risk score for the testing episodes, we did not use the information conveyed in the observation process {tdm}m to infer the patients’ clinical states.",6. Discussion: Chicken-and-egg,[0],[0]
"This is because in practice, the value of the realtime risk score R(t) itself influences the clinician’s behavior and hence impacts the observation process, creating a chicken-and-egg dilemma in which one cannot clearly conceptualize the causal relation between the risk score and the clinicians’ judgments.",6. Discussion: Chicken-and-egg,[0],[0]
"A very interesting research direction is to consider an observation process that is modulated by both the patient’s state and the real-time risk score through an intensity function λ(t,X(t), R(t)), where the algorithm learns the clinical state representation online by “sharing experience” with the clinicians.",6. Discussion: Chicken-and-egg,[0],[0]
"That is, the algorithm uses the clinician’s judgments to refine its clinical state model, which leads to a refined risk score R(t) that would in turn allow the clinician to exhibit more accurate judgments; an online learning process that would ideally converge to a state of “shared knowledge” between the clinician and the system.
",6. Discussion: Chicken-and-egg,[0],[0]
The significant prognostic value offered by our risk score promises a great improvement in the quality of subacute care in wards.,6. Discussion: Chicken-and-egg,[0],[0]
"By utilizing the proposed score instead of the current technology, clinicians in a crowded ward can better focus their attention on patients at real risk of deterioration, and can also plan for timely ICU admissions and effective therapeutic interventions.",6. Discussion: Chicken-and-egg,[0],[0]
"With the high in-hospital mortality rates in wards, deploying our risk score may help save thousands of lives annually– we are currently working towards installing the proposed risk score in our medical center.
",6. Discussion: Chicken-and-egg,[0],[0]
"5The adoption of a semi-Markovian model for the clinical state process protects our model from the overtly rapid state switching behavior that is introduced by memoryless HMMs (Matteson & James, 2014).",6. Discussion: Chicken-and-egg,[0],[0]
Critically ill patients in regular wards are vulnerable to unanticipated adverse events which require prompt transfer to the intensive care unit (ICU).,abstractText,[0],[0]
"To allow for accurate prognosis of deteriorating patients, we develop a novel continuoustime probabilistic model for a monitored patient’s temporal sequence of physiological data.",abstractText,[0],[0]
"Our model captures “informatively sampled” patient episodes: the clinicians’ decisions on when to observe a hospitalized patient’s vital signs and lab tests over time are represented by a marked Hawkes process, with intensity parameters that are modulated by the patient’s latent clinical states, and with observable physiological data (mark process) modeled as a switching multi-task Gaussian process.",abstractText,[0],[0]
"In addition, our model captures “informatively censored” patient episodes by representing the patient’s latent clinical states as an absorbing semi-Markov jump process.",abstractText,[0],[0]
The model parameters are learned from offline patient episodes in the electronic health records via an EM-based algorithm.,abstractText,[0],[0]
Experiments conducted on a cohort of patients admitted to a major medical center over a 3-year period show that risk prognosis based on our model significantly outperforms the currently deployed medical risk scores and other baseline machine learning algorithms.,abstractText,[0],[0]
Learning from Clinical Judgments: Semi-Markov-Modulated Marked  Hawkes Processes for Risk Prognosis,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2390–2400 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
Learning sentence representations is central to many natural language modeling applications.,1 Introduction,[0],[0]
The aim of a model for this task is to learn fixedlength feature vectors that encode the semantic and syntactic properties of sentences.,1 Introduction,[0],[0]
"Deep learning techniques have shown promising performance on sentence modeling, via feedforward neural networks (Huang et al., 2013), recurrent neural networks (RNNs) (Hochreiter and Schmidhuber, 1997), convolutional neural networks (CNNs) (Kalchbrenner et al., 2014; Kim, 2014; Shen et al., 2014), and recursive neural networks (Socher et al., 2013).",1 Introduction,[0],[0]
Most of these models are task-dependent: they are trained specifically for a certain task.,1 Introduction,[0],[0]
"However, these methods may be-
come inefficient when we need to repeatedly learn sentence representations for a large number of different tasks, because they may require retraining a new model for each individual task.",1 Introduction,[0],[0]
"In this paper, in contrast, we are primarily interested in learning generic sentence representations that can be used across domains.
",1 Introduction,[0],[0]
Several approaches have been proposed for learning generic sentence embeddings.,1 Introduction,[0],[0]
"The paragraphvector model of Le and Mikolov (2014) incorporates a global context vector into the log-linear neural language model (Mikolov et al., 2013) to learn the sentence representation; however, at prediction time, one needs to perform gradient descent to compute a new vector.",1 Introduction,[0],[0]
"The sequence autoencoder of Dai and Le (2015) describes an encoder-decoder model to reconstruct the input sentence, while the skip-thought model of Kiros et al. (2015) extends the encoder-decoder model to reconstruct the surrounding sentences of an input sentence.",1 Introduction,[0],[0]
"Both the encoder and decoder of the methods above are modeled as RNNs.
",1 Introduction,[0],[0]
"CNNs have recently achieved excellent results in various task-dependent natural language applications as the sentence encoder (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).",1 Introduction,[0],[0]
This motivates us to propose a CNN encoder for learning generic sentence representations within the framework of encoder-decoder models proposed by Sutskever et al. (2014); Cho et al. (2014).,1 Introduction,[0],[0]
"Specifically, a CNN encoder performs convolution and pooling operations on an input sentence, then uses a fullyconnected layer to produce a fixed-length encoding of the sentence.",1 Introduction,[0],[0]
This encoding vector is then fed into a long short-term memory (LSTM) recurrent network to produce a target sentence.,1 Introduction,[0],[0]
"Depending on the task, we propose three models: (i)",1 Introduction,[0],[0]
"CNNLSTM autoencoder: this model seeks to reconstruct the original input sentence, by capturing the intra-sentence information; (ii) CNN-LSTM future
2390
predictor: this model aims to predict a future sentence, by leveraging inter-sentence information; (iii) CNN-LSTM composite model: in this case, there are two LSTMs, decoding the representation to the input sentence itself and a future sentence.",1 Introduction,[0],[0]
"This composite model aims to learn a sentence encoder that captures both intra- and inter-sentence information.
",1 Introduction,[0],[0]
The proposed CNN-LSTM future predictor model only considers the immediately subsequent sentence as context.,1 Introduction,[0],[0]
"In order to capture longerterm dependencies between sentences, we further introduce a hierarchical encoder-decoder model.",1 Introduction,[0],[0]
This model abstracts the RNN language model of Mikolov et al. (2010) to the sentence level.,1 Introduction,[0],[0]
"That is, instead of using the current word in a sentence to predict future words (sentence continuation), we encode a sentence to predict multiple future sentences (paragraph continuation).",1 Introduction,[0],[0]
"This model is termed hierarchical CNN-LSTM model.
",1 Introduction,[0],[0]
"As in Kiros et al. (2015), we first train our proposed models on a large collection of novels.",1 Introduction,[0],[0]
"We then evaluate the CNN sentence encoder as a generic feature extractor for 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks.",1 Introduction,[0],[0]
"In these experiments, we train a linear classifier on top of the extracted sentence features, without additional fine-tuning of the CNN.",1 Introduction,[0],[0]
"We show that our trained sentence encoder yields generic repre-
sentations that perform as well as, or better, than those of Kiros et al. (2015); Hill et al. (2016), in all the tasks considered.
",1 Introduction,[0],[0]
"Summarizing, the main contribution of this paper is a new class of CNN-LSTM encoder-decoder models that is able to leverage the vast quantity of unlabeled text for learning generic sentence representations.",1 Introduction,[0],[0]
"Inspired by the skip-thought model (Kiros et al., 2015), we have further explored different variants: (i) CNN is used as the sentence encoder rather than RNN; (ii) larger context windows are considered: we propose the hierarchical CNN-LSTM model to encode a sentence for predicting multiple future sentences.",1 Introduction,[0],[0]
"Consider the sentence pair (sx, sy).",2.1 CNN-LSTM model,[0],[0]
"The encoder, a CNN, encodes the first sentence sx into a feature vector z, which is then fed into an LSTM decoder that predicts the second sentence sy.",2.1 CNN-LSTM model,[0],[0]
Let wtx ∈,2.1 CNN-LSTM model,[0],[0]
"{1, . . .",2.1 CNN-LSTM model,[0],[0]
", V } represent the t-th word in sentences sx, where wtx indexes one element in a V - dimensional set (vocabulary); wty is defined similarly w.r.t.",2.1 CNN-LSTM model,[0],[0]
sy.,2.1 CNN-LSTM model,[0],[0]
"Each word wtx is embedded into a k-dimensional vector xt = We[wtx], where We ∈ Rk×V is a word embedding matrix (learned), and notation We[v] denotes the v-th column of matrix We.",2.1 CNN-LSTM model,[0],[0]
"Similarly, we let yt = We[wty].
CNN encoder The CNN architecture in Kim (2014); Collobert et al. (2011) is used for sentence encoding, which consists of a convolution layer and a max-pooling operation over the entire sentence for each feature map.",2.1 CNN-LSTM model,[0],[0]
"A sentence of length T (padded where necessary) is represented as a matrix X ∈ Rk×T , by concatenating its word embeddings as columns, i.e., the t-th column of X is xt.
",2.1 CNN-LSTM model,[0],[0]
A convolution operation involves a filter Wc ∈,2.1 CNN-LSTM model,[0],[0]
"Rk×h, applied to a window of h words to produce a new feature.",2.1 CNN-LSTM model,[0],[0]
"According to Collobert et al. (2011), we can induce one feature map c = f(X ∗Wc + b) ∈ RT−h+1, where f(·) is a nonlinear activation function such as the hyperbolic tangent used in our experiments, b ∈ RT−h+1 is a bias vector, and ∗ denotes the convolutional operator.",2.1 CNN-LSTM model,[0],[0]
Convolving the same filter with the h-gram at every position in the sentence allows the features to be extracted independently of their position in the sentence.,2.1 CNN-LSTM model,[0],[0]
"We then apply a max-over-time pooling operation (Collobert et al., 2011) to the feature map and take its maximum value, i.e., ĉ = max{c}, as the feature corresponding to this particular filter.",2.1 CNN-LSTM model,[0],[0]
"This pooling scheme tries to capture the most important feature, i.e., the one with the highest value, for each feature map, effectively filtering out less informative compositions of words.",2.1 CNN-LSTM model,[0],[0]
"Further, pooling also guarantees that the extracted features are independent of the length of the input sentence.
",2.1 CNN-LSTM model,[0],[0]
The above process describes how one feature is extracted from one filter.,2.1 CNN-LSTM model,[0],[0]
"In practice, the model uses multiple filters with varying window sizes (Kim, 2014).",2.1 CNN-LSTM model,[0],[0]
"Each filter can be considered as a linguistic feature detector that learns to recognize a specific class of n-grams (or h-grams, in the above notation).",2.1 CNN-LSTM model,[0],[0]
"However, since the h-grams are computed in the embedding space, the model naturally handles similar h-grams composed of synonyms.",2.1 CNN-LSTM model,[0],[0]
"Assume we have m window sizes, and for each window size, we use d filters; then we obtain a md-dimensional vector to represent a sentence.
",2.1 CNN-LSTM model,[0],[0]
"Compared with the LSTM encoders used in Kiros et al. (2015); Dai and Le (2015); Hill et al. (2016), a CNN encoder may have the following advantages.",2.1 CNN-LSTM model,[0],[0]
"First, the sparse connectivity of a CNN, which indicates fewer parameters are required, typically improves its statistical efficiency as well as reduces memory requirements (Goodfellow et al., 2016).",2.1 CNN-LSTM model,[0],[0]
"For example, excluding the number of parameters used in the word embeddings, our trained CNN sentence encoder has 3 million parameters,
while the skip-thought vector of Kiros et al. (2015) contains 40 million parameters.",2.1 CNN-LSTM model,[0],[0]
"Second, a CNN is easy to implement in parallel over the whole sentence, while an LSTM needs sequential computation.
",2.1 CNN-LSTM model,[0],[0]
LSTM decoder The CNN encoder maps sentence sx into a vector z.,2.1 CNN-LSTM model,[0],[0]
"The probability of a length-T sentence sy given the encoded feature vector z is defined as
p(sy|z) =",2.1 CNN-LSTM model,[0],[0]
"T∏
t=1
p(wty|w0y, . . .",2.1 CNN-LSTM model,[0],[0]
", wt−1y , z) (1)
where w0y is defined as a special start-of-thesentence token.",2.1 CNN-LSTM model,[0],[0]
"All the words in the sentence are sequentially generated using the RNN, until the end-of-the-sentence symbol is generated.",2.1 CNN-LSTM model,[0],[0]
"Specifically, each conditional p(wty|w<ty , z), where < t = {0, . . .",2.1 CNN-LSTM model,[0],[0]
", t− 1}, is specified as softmax(Vht), where ht, the hidden units, are recursively updated through ht = H(yt−1,ht−1, z), and h0 is defined as a zero vector (h0 is not updated during training).",2.1 CNN-LSTM model,[0],[0]
V is a weight matrix used for computing a distribution over words.,2.1 CNN-LSTM model,[0],[0]
Bias terms are omitted for simplicity throughout the paper.,2.1 CNN-LSTM model,[0],[0]
"The transition function H(·) is implemented with an LSTM (Hochreiter and Schmidhuber, 1997).
",2.1 CNN-LSTM model,[0],[0]
"Given the sentence pair (sx, sy), the objective function is the sum of the log-probabilities of the target sentence conditioned on the encoder representation in (1): ∑T t=1 log p(w t y|w<ty , z).",2.1 CNN-LSTM model,[0],[0]
"The total objective is the above objective summed over all the sentence pairs.
",2.1 CNN-LSTM model,[0],[0]
"Applications Inspired by Srivastava et al. (2015), we propose three models: (i) an autoencoder, (ii) a future predictor, and (iii) the composite model.",2.1 CNN-LSTM model,[0],[0]
"These models share the same CNN-LSTM model architecture, but are different in terms of the choices of the target sentence.",2.1 CNN-LSTM model,[0],[0]
"An illustration of the proposed encoder-decoder models is shown in Figure 1(left).
",2.1 CNN-LSTM model,[0],[0]
The autoencoder (i) aims to reconstruct the same sentence as the input.,2.1 CNN-LSTM model,[0],[0]
"The intuition behind this is that an autoencoder learns to represent the data using features that explain its own important factors of variation, and hence model the internal structure of sentences, effectively capturing the intrasentence information.",2.1 CNN-LSTM model,[0],[0]
Another natural task is encoding an input sentence to predict the subsequent sentence.,2.1 CNN-LSTM model,[0],[0]
"The future predictor (ii) achieves this, effectively capturing the inter-sentence information,
which has been shown to be useful to learn the semantics of a sentence (Kiros et al., 2015).",2.1 CNN-LSTM model,[0],[0]
"These two tasks can be combined to create a composite model (iii), where the CNN encoder is asked to learn a feature vector that is useful to simultaneously reconstruct the input sentence and predict a future sentence.",2.1 CNN-LSTM model,[0],[0]
This composite model encourages the sentence encoder to incorporate contextual information both within and beyond the sentence.,2.1 CNN-LSTM model,[0],[0]
The future predictor described in Section 2.1 only considers the immediately subsequent sentence as context.,2.2 Hierarchical CNN-LSTM model,[0],[0]
"By utilizing a larger surrounding context, it is likely that we can learn even higher-quality sentence representations.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"Inspired by the standard RNN-based language model (Mikolov et al., 2010) that uses the current word to predict future words, we propose a hierarchical encoder-decoder model that encodes the current sentence to predict multiple future sentences.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"An illustration of the hierarchical model is shown in Figure 1(right), with details provided in Figure 2.
",2.2 Hierarchical CNN-LSTM model,[0],[0]
Our proposed hierarchical model characterizes the hierarchy word-sentence-paragraph.,2.2 Hierarchical CNN-LSTM model,[0],[0]
"A paragraph is modeled as a sequence of sentences, and each sentence is modeled as a sequence of words.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"Specifically, assume we are given a paragraph D = (s1, . . .",2.2 Hierarchical CNN-LSTM model,[0],[0]
", sL), that consists of L sentences.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"The probability for paragraph D is then defined as
p(D) = L∏
`=1
p(s`|s<`) (2)
where s0 is defined as a special start-of-theparagraph token.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"As shown in Figure 2(left), each p(s`|s<`) in (2) is calculated as
p(s`|s<`) = p(s`|h(p)` ) (3) h
(p) ` = LSTMp(h (p) `−1, z`−1) (4)
z`−1 = CNN(s`−1) (5)
where h(p)` denotes the `-th hidden state of the LSTM paragraph generator, and h(p)0 is fixed as a zero vector.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"The CNN in (5) is as described in Section 2.1, encoding the sentence s`−1 into a vector representation z`−1.
",2.2 Hierarchical CNN-LSTM model,[0],[0]
"Equation (4) serves as the paragraph-level language model (Mikolov et al., 2010), which encodes all the previous sentence representations z<` into a vector representation h(p)` .",2.2 Hierarchical CNN-LSTM model,[0],[0]
"This hidden state h (p) `
is used to guide the generation of the `-th sentence through the decoder (3), which is defined as
p(s`|h(p)` )",2.2 Hierarchical CNN-LSTM model,[0],[0]
"= T∏̀
t=1
p(w`,t|w`,<t,h(p)` ) (6)
where w`,0 is defined as a special start-of-thesentence token.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"T` is the length of sentence `, and w`,t denotes the t-th word in sentence `.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"As shown in Figure 2(right), each p(w`,t|w`,<t,h(p)` ) in (6) is calculated as
p(w`,t|w`,<t,h(p)` )",2.2 Hierarchical CNN-LSTM model,[0],[0]
"= softmax(Vh(s)`,t ) (7) h
(s) `,t = LSTMs(h (s) `,t−1,x`,t−1,h (p) ` ) (8)
where h(s)`,t denotes the t-th hidden state of the LSTM decoder for sentence `, x`,t−1 denotes the word embedding for w`,t−1, and h (s) `,0 is fixed as a zero vector for all ` = 1, . . .",2.2 Hierarchical CNN-LSTM model,[0],[0]
", L. V is a weight matrix used for computing distribution over words.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"Various methods have been proposed for sentence modeling, which generally fall into two categories.",3 Related work,[0],[0]
"The first consists of models trained specifically for a certain task, typically combined with downstream applications.",3 Related work,[0],[0]
"Several models have been proposed along this line, ranging from simple additional composition of the word vectors (Mitchell and Lapata, 2010; Yu and Dredze, 2015; Iyyer et al., 2015) to those based on complex nonlinear functions like recursive neural networks (Socher et al., 2011, 2013), convolutional neural networks (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015; Gan et al., 2017), and recurrent neural networks (Tai et al., 2015; Lin et al., 2017).
",3 Related work,[0],[0]
The other category consists of methods aiming to learn generic sentence representations that can be used across domains.,3 Related work,[0],[0]
"This includes the paragraph vector (Le and Mikolov, 2014), skip-thought vector (Kiros et al., 2015), and the sequential denoising autoencoders (Hill et al., 2016).",3 Related work,[0],[0]
"Hill et al. (2016) also proposed a sentence-level log-linear bag-of-words (BoW) model, where a BoW representation of an input sentence is used to predict adjacent sentences that are also represented as BoW. Most recently, Wieting et al. (2016); Arora et al. (2017); Pagliardini et al. (2017) proposed methods in which sentences are represented as a weighted average of fixed (pre-trained) word vectors.",3 Related work,[0],[0]
"Our model falls into this category, and is most related to Kiros et al. (2015).
",3 Related work,[0],[0]
"However, there are two key aspects that make our model different from Kiros et al. (2015).",3 Related work,[0],[0]
"First, we use CNN as the sentence encoder.",3 Related work,[0],[0]
"The combination of CNN and LSTM has been considered in image captioning (Karpathy and Fei-Fei, 2015), and in some recent work on machine translation (Kalchbrenner and Blunsom, 2013; Meng et al., 2015; Gehring et al., 2016).",3 Related work,[0],[0]
"Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different.",3 Related work,[0],[0]
"Our work aims to use a CNN to learn generic sentence embeddings.
",3 Related work,[0],[0]
"Second, we use the hierarchical CNN-LSTM model to predict multiple future sentences, rather than the surrounding two sentences as in Kiros et al. (2015).",3 Related work,[0],[0]
"Utilizing a larger context window aids our model to learn better sentence representations, capturing longer-term dependencies between sentences.",3 Related work,[0],[0]
Similar work to this hierarchical language modeling can be found in Li et al. (2015); Sordoni et al. (2015); Lin et al. (2015); Wang and Cho (2016).,3 Related work,[0],[0]
"Specifically, Li et al. (2015); Sordoni et al. (2015) uses an LSTM for the sentence encoder, while Lin et al. (2015) uses a bag-of-words to represent sentences.",3 Related work,[0],[0]
"We first provide qualitative analysis of our CNN encoder, and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase detection, semantic relatedness and image-sentence ranking.",4 Experiments,[0],[0]
"As in Kiros et al. (2015), we evaluate the capabilities of our encoder as a generic feature extractor.",4 Experiments,[0],[0]
"To further demonstrate the advantage of our learned generic sentence representations, we also fine-tune our trained sentence encoder on the 5 clas-
sification benchmarks.",4 Experiments,[0],[0]
"All the CNN-LSTM models are trained using the BookCorpus dataset (Zhu et al., 2015), which consists of 70 million sentences from over 7000 books.
",4 Experiments,[0],[0]
"We train four models in total: (i) an autoencoder, (ii) a future predictor, (iii) the composite model, and (iv) the hierarchical model.",4 Experiments,[0],[0]
"For the CNN encoder, we employ filter windows (h) of sizes {3,4,5} with 800 feature maps each, hence each sentence is represented as a 2400-dimensional vector.",4 Experiments,[0],[0]
"For both, the LSTM sentence decoder and paragraph generator, we use one hidden layer of 600 units.
",4 Experiments,[0],[0]
"The CNN-LSTM models are trained with a vocabulary size of 22,154 words.",4 Experiments,[0],[0]
"In order to learn a generic sentence encoder that can encode a large number of possible words, we use two methods of considering words not in the training set.",4 Experiments,[0],[0]
"Suppose we have a large pretrained word embedding matrix, such as the publicly available word2vec vectors (Mikolov et al., 2013), in which all test words are assumed to reside.
",4 Experiments,[0],[0]
"The first method learns a linear mapping between the word2vec embedding space Vw2v and the learned word embedding space Vcnn by solving a linear regression problem (Kiros et al., 2015).",4 Experiments,[0],[0]
"Thus, any word from Vw2v can be mapped into Vcnn for encoding sentences.",4 Experiments,[0],[0]
"The second method fixes the word vectors in Vcnn as the corresponding word vectors in Vw2v , and we do not update the word embedding parameters during training.",4 Experiments,[0],[0]
"Thus, any word vector from Vw2v can be naturally used to encode sentences.",4 Experiments,[0],[0]
"By doing this, our trained sentence encoder can successfully encode 931,331 words.
",4 Experiments,[0],[0]
"For training, all weights in the CNN and nonrecurrent weights in the LSTM are initialized from a uniform distribution in [-0.01,0.01].",4 Experiments,[0],[0]
Orthogonal initialization is employed on the recurrent matrices in the LSTM.,4 Experiments,[0],[0]
All bias terms are initialized to zero.,4 Experiments,[0],[0]
The initial forget gate bias for LSTM is set to 3.,4 Experiments,[0],[0]
"Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014).",4 Experiments,[0],[0]
"The Adam algorithm (Kingma and Ba, 2015) with learning rate 2× 10−4 is utilized for optimization.",4 Experiments,[0],[0]
"For all the CNN-LSTM models, we use mini-batches of size 64.",4 Experiments,[0],[0]
"For the hierarchical CNN-LSTM model, we use mini-batches of size 8, and each paragraph is composed of 8 sentences.",4 Experiments,[0],[0]
"We do not perform any regularization other than dropout (Srivastava et al., 2014).",4 Experiments,[0],[0]
"All experiments are implemented
in Theano (Bastien et al., 2012), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory.",4 Experiments,[0],[0]
"We first demonstrate that the sentence representation learned by our model exhibits a structure that makes it possible to perform analogical reasoning using simple vector arithmetics, as illustrated in Table 1.",4.1 Qualitative analysis,[0],[0]
It demonstrates that the arithmetic operations on the sentence representations correspond to wordlevel addition and subtractions.,4.1 Qualitative analysis,[0],[0]
"For instance, in the 3rd example, our encoder captures that the difference between sentence B and C is “you"" and “him"", so that the former word in sentence A is replaced by the latter (i.e., “you”-“you”+“him”=“him”), resulting in sentence D.
Table 2 shows nearest neighbors of sentences from a CNN-LSTM autoencoder trained on the BookCorpus dataset.",4.1 Qualitative analysis,[0],[0]
Nearest neighbors are scored by cosine similarity from a random sample of 1 million sentences from the BookCorpus dataset.,4.1 Qualitative analysis,[0],[0]
"As can be seen, our encoder learns to accurately
capture semantic and syntax of the sentences.",4.1 Qualitative analysis,[0],[0]
"Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005) and TREC (Li and Roth, 2002).",4.2 Quantitative evaluations,[0],[0]
"On all the datasets, we separately train a logistic regression model on top of the extracted sentence features.",4.2 Quantitative evaluations,[0],[0]
We restrict our comparison to methods that also aims to learn generic sentence embeddings for fair comparison.,4.2 Quantitative evaluations,[0],[0]
We also provide the state-of-the-art results using task-dependent learning methods for reference.,4.2 Quantitative evaluations,[0],[0]
Results are summarized in Table 3.,4.2 Quantitative evaluations,[0],[0]
"Our CNN encoder provides better results than the combine-skip model of Kiros et al. (2015) on all the 5 datasets.
",4.2 Quantitative evaluations,[0],[0]
We highlight some observations.,4.2 Quantitative evaluations,[0],[0]
"First, the autoencoder performs better than the future predictor, indicating that the intra-sentence information may be more important for classification than the inter-sentence information.",4.2 Quantitative evaluations,[0],[0]
"Second, the hierarchi-
cal model performs better than the future predictor, demonstrating the importance of capturing longterm dependencies across multiple sentences.",4.2 Quantitative evaluations,[0],[0]
"Our combined model, which concatenates the feature vectors learned from both the hierarchical model and the composite model, performs the best.",4.2 Quantitative evaluations,[0],[0]
This may be due to that: (i) both intra- and long-term inter-sentence information are leveraged; (ii) it is easier to linearly separate the feature vectors in higher dimensional spaces.,4.2 Quantitative evaluations,[0],[0]
"Further, using (fixed) pre-trained word embeddings consistently provides better performance than using the learned word embeddings.",4.2 Quantitative evaluations,[0],[0]
"This may be due to that word2vec provides more generic word representations, since it is trained on the large Google News dataset (containing 100 billion words) (Mikolov et al., 2013).
",4.2 Quantitative evaluations,[0],[0]
"To further demonstrate the advantage of the learned generic representations, we train a CNN classifier (i.e., a CNN encoder with a logistic regression model on top) with two different initialization strategies: random initialization and initialization with trained parameters from the CNN-LSTM composite model.",4.2 Quantitative evaluations,[0],[0]
Results are shown in Figure 3(left).,4.2 Quantitative evaluations,[0],[0]
"The pretraining provides substantial improvements
(3.52% on average) over random initialization of CNN parameters.",4.2 Quantitative evaluations,[0],[0]
Figure 3(right) shows the effect of pretraining as the number of labeled sentences is varied.,4.2 Quantitative evaluations,[0],[0]
"For the TREC dataset, the performance improves from 79.7% to 84.1% when only 10% sentences are labeled.",4.2 Quantitative evaluations,[0],[0]
"As the size of the set of labeled sentences grows, the improvement becomes smaller, as expected.",4.2 Quantitative evaluations,[0],[0]
"For future work, our CNNLSTM model can be also used for semi-supervised
learning, with the autoencoder on all the data (labeled and unlabled), and the classifier only on the labeled data.
",4.2 Quantitative evaluations,[0],[0]
"Paraphrase detection Now we consider paraphrase detection on the MSRP dataset (Dolan et al., 2004).",4.2 Quantitative evaluations,[0],[0]
"On this task, one needs to predict whether or not two sentences are paraphrases.",4.2 Quantitative evaluations,[0],[0]
"The training set consists of 4076 sentence pairs, and the test set has 1725 pairs.",4.2 Quantitative evaluations,[0],[0]
"As in Tai et al. (2015), given two sentence representations zx and zy, we first compute their element-wise product zx zy and their absolute difference |zx",4.2 Quantitative evaluations,[0],[0]
"− zy|, and then concatenate them together.",4.2 Quantitative evaluations,[0],[0]
A logistic regression model is trained on top of the concatenated features to predict whether two sentences are paraphrases.,4.2 Quantitative evaluations,[0],[0]
We present our results on the last column of Table 3.,4.2 Quantitative evaluations,[0],[0]
"Our best result is better than the other results that use task-independent methods.
",4.2 Quantitative evaluations,[0],[0]
"Image-sentence ranking We consider the task of image-sentence ranking, which aims to retrieve items in one modality given a query from the other.",4.2 Quantitative evaluations,[0],[0]
"We use the COCO dataset (Lin et al., 2014), which contains 123,287 images each with 5 captions.",4.2 Quantitative evaluations,[0],[0]
For development and testing we use the same splits as Karpathy and Fei-Fei (2015).,4.2 Quantitative evaluations,[0],[0]
The development and test sets each contain 5000 images.,4.2 Quantitative evaluations,[0],[0]
"We further split them into 5 random sets of 1000 images, and report the average performance over the 5 splits.",4.2 Quantitative evaluations,[0],[0]
"Performance is evaluated using Recall@K, which measures the average times a correct item is found within the top-K retrieved results.",4.2 Quantitative evaluations,[0],[0]
"We also report the median rank of the closest ground truth result
in the ranked list.",4.2 Quantitative evaluations,[0],[0]
"We represent images using 4096-dimensional feature vectors from VggNet (Simonyan and Zisserman, 2015).",4.2 Quantitative evaluations,[0],[0]
Each caption is encoded using our trained CNN encoder.,4.2 Quantitative evaluations,[0],[0]
"The training objective is the same pairwise ranking loss as used in Kiros et al. (2015), which takes the form of max(0, α− f(xn, yn) +",4.2 Quantitative evaluations,[0],[0]
"f(xn, ym)), where f(·, ·) is the image-sentence score.",4.2 Quantitative evaluations,[0],[0]
"(xn, yn) denotes the related image-sentence pair, and (xn, ym) is the randomly sampled unrelated image-sentence pair with n 6= m. For image retrieval from sentences, x denotes the caption, y denotes the image, and vice versa.",4.2 Quantitative evaluations,[0],[0]
"The objective is to force the matching score of the related pair (xn, yn) to be greater than the unrelated pair (xn, ym) by a margin α, which is set to 0.1 in our experiments.
",4.2 Quantitative evaluations,[0],[0]
Table 4 shows our results.,4.2 Quantitative evaluations,[0],[0]
"Consistent with previous experiments, we empirically found that the encoder trained using the fixed word embedding performed better on this task, hence only results using this method are reported.",4.2 Quantitative evaluations,[0],[0]
"As can be seen, we obtain the same median rank as in Kiros et al. (2015), indicating that our encoder is as competitive as the skip-thought vectors (Kiros et al., 2015).",4.2 Quantitative evaluations,[0],[0]
"The performance gain between our encoder and the combine-skip model of Kiros et al. (2015) on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on re-
trieving the most correct item than the skip-thought vector.
",4.2 Quantitative evaluations,[0],[0]
"Semantic relatedness For our final experiment, we consider the task of semantic relatedness on the SICK dataset (Marelli et al., 2014), consisting of 9927 sentence pairs.",4.2 Quantitative evaluations,[0],[0]
"Given two sentences, our goal is to produce a real-valued score between [1, 5] to indicate how semantically related two sentences are, based on human generated scores.",4.2 Quantitative evaluations,[0],[0]
We compute a feature vector representing the pair of sentences in the same way as on the MSRP dataset.,4.2 Quantitative evaluations,[0],[0]
"We follow the method in Tai et al. (2015), and use the crossentropy loss for training.",4.2 Quantitative evaluations,[0],[0]
Results are summarized in Table 5.,4.2 Quantitative evaluations,[0],[0]
Our result is better than the combineskip model of Kiros et al. (2015).,4.2 Quantitative evaluations,[0],[0]
This suggests that CNN also provides competitive performance at matching human relatedness judgements.,4.2 Quantitative evaluations,[0],[0]
We presented a new class of CNN-LSTM encoderdecoder models to learn sentence representations from unlabeled text.,5 Conclusion,[0],[0]
"Our trained convolutional encoder is highly generic, and can be an alternative to the skip-thought vectors of Kiros et al. (2015).",5 Conclusion,[0],[0]
Compelling experimental results on several tasks demonstrated the advantages of our approach.,5 Conclusion,[0],[0]
"In future work, we aim to use more advanced CNN architectures (Conneau et al., 2016) for learning generic sentence embeddings.",5 Conclusion,[0],[0]
"This research was supported by ARO, DARPA, DOE, NGA, ONR and NSF.",Acknowledgments,[0],[0]
We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes.,abstractText,[0],[0]
"The model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector, and using a long short-term memory recurrent neural network as a decoder.",abstractText,[0],[0]
"Several tasks are considered, including sentence reconstruction and future sentence prediction.",abstractText,[0],[0]
"Further, a hierarchical encoderdecoder model is proposed to encode a sentence to predict multiple future sentences.",abstractText,[0],[0]
"By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice.",abstractText,[0],[0]
"Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.",abstractText,[0],[0]
Learning Generic Sentence Representations Using Convolutional Neural Networks,title,[0],[0]
"Real-world interactions among multiple entities are often recorded as asynchronous event sequences, such as user behaviors in social networks, job hunting and hopping among companies, and diseases and their complications.",1. Introduction,[0],[0]
The entities or event types in the sequences often exhibit selftriggering and mutually-triggering patterns.,1. Introduction,[0],[0]
"For example, a tweet of a twitter user may trigger further responses from her friends (Zhao et al., 2015).",1. Introduction,[0],[0]
"A disease of a patient may trigger other complications (Choi et al., 2015).",1. Introduction,[0],[0]
"Hawkes processes, an important kind of temporal point process model (Hawkes & Oakes, 1974), have capability to describe the triggering patterns quantitatively and capture the infectivity network of the entities.
",1. Introduction,[0],[0]
"1Georgia Institute of Technology, Atlanta, Georgia, USA 2University of Toronto, Toronto, Ontario, Canada.",1. Introduction,[0],[0]
"Correspondence to: Hongteng Xu <hxu42@gatech.edu>, Dixin Luo <dixin.luo@utoronto.ca>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Despite the usefulness of Hawkes processes, robust learning of Hawkes processes often needs many event sequences with events occurring over a long observation window.",1. Introduction,[0],[0]
"Unfortunately, the observation window is likely to be very short and sequence-specific in many important practical applications, i.e., within an imagined universal window, each sequence is only observed with a corresponding short subinterval of it, and the events outside this sub-interval are not observed — we call them short doubly-censored (SDC) event sequences.",1. Introduction,[0],[0]
"Existing learning algorithms of Hawkes processes directly applied to SDCs may suffer from overfitting, and what is worse, the triggering patterns between historical events and current ones are lost, so that the triggering patterns learned from SDC event sequences are often unreliable.",1. Introduction,[0],[0]
"This problem is a thorny issue in several practical applications, especially in those having timevarying triggering patterns.",1. Introduction,[0],[0]
"For example, the disease networks of patients should evolve with the increase of age.",1. Introduction,[0],[0]
"However, it is very hard to track and record people’s diseases on a life-time scale.",1. Introduction,[0],[0]
"Instead, we can only obtain their several admissions (even only one admission) in a hospital during one or two years, which are just SDC event sequences.",1. Introduction,[0],[0]
"Therefore, it is highly desirable to propose a method to learn Hawkes processes having a longtime support from a collection of SDC event sequences
In this paper, we propose a novel and simple data synthesis method to enhance the robustness of learning algorithms for Hawkes processes.",1. Introduction,[0],[0]
Fig. 1 illustrates the principle of our method.,1. Introduction,[0],[0]
"Given a set of SDC event sequences, we sample predecessor for each event sequence from potential candidates and stitch them together as new training data.",1. Introduction,[0],[0]
"In the sampling step, the distribution of predecessor (and successor) is estimated according to the similarities between
current sequence and its candidates, and the similarity is defined based on the information of time stamps and (optional) features of event sequences.",1. Introduction,[0],[0]
We analyze the rationality and the feasibility of our data synthesis method and discuss the necessary condition for using the method.,1. Introduction,[0],[0]
Experimental results show that our data synthesis method indeed helps to improve the robustness of various learning algorithms for Hawkes processes.,1. Introduction,[0],[0]
"Especially in the case of time-varying Hawkes processes, applying our method in the learning phase achieves much better results than learning directly from SDC event sequences, which is meaningful for many practical applications, e.g., constructing dynamical disease network, and learning long-term infectivity among different IT companies.",1. Introduction,[0],[0]
"An event sequence can be represented as s = {(ti, ci)}Mi=1, where time stamps ti’s are in an observation window",2. Related Work,[0],[0]
"[Tb, Te] and events ci’s are in a set of event types C = {1, ..., C}.",2. Related Work,[0],[0]
"A point process {Nc}c∈C is a random process model taking event sequences as instances, where Nc = {Nc(t)|t ∈",2. Related Work,[0],[0]
"[Tb, Te]} and Nc(t) is the number of type-c events occurring at or before time t. A point process can be characterized via its conditional intensity function λc(t) = E[dNc(t)|HCt ]/dt, where c ∈ C and HCt",2. Related Work,[0],[0]
"= {(ti, ci)|ti < t, ci ∈ C} is the set of history.",2. Related Work,[0],[0]
"It represents the expected instantaneous happening rate of events given historical record (Daley & Vere-Jones, 2007).",2. Related Work,[0],[0]
"The intensity is often modeled with certain parameters Θ to capture the phenomena of interests, i.e., self-triggering (Hawkes & Oakes, 1974) or self-correcting (Xu et al., 2015).",2. Related Work,[0],[0]
"Based on {λc(t)}c∈C , the likelihood of an event sequence s is
L(s; Θ) =",2. Related Work,[0],[0]
"∏
i λci(ti) exp
( − ∑
c ∫ Te Tb λc(s)ds ) .",2. Related Work,[0],[0]
"(1)
Hawkes Processes.",2. Related Work,[0],[0]
"Hawkes processes (Hawkes & Oakes, 1974) have a particular form of intensity:
λc(t) = µc + ∑C
c′=1 ∫ t 0 φcc′(t, s)dNc′(s), (2)
where µc is the exogenous base intensity independent of the history while ∫ t 0 φcc′(t, s)dNc′(s) is the endogenous intensity capturing the influence of historical events on type-c ones at time t (Xu et al., 2016a).",2. Related Work,[0],[0]
"Here, φcc′(t, s) ≥ 0 is called impact function.",2. Related Work,[0],[0]
"It quantifies the influence of the type-c′ event at time s to the type-c event at time t. Hawkes processes provide us with a physically-meaningful model to capture the infectivity among various events, which are used in social network analysis (Zhou et al., 2013b; Zhao et al., 2015), behavior analysis (Yang & Zha, 2013; Luo et al., 2015) and financial analysis (Bacry et al., 2013).",2. Related Work,[0],[0]
"However, the methods in these references assume
that the impact function is shift-invariant (i.e., φcc′(t, s) = φcc′(t− s), t ≥ s), which limits their applications on longtime scale.",2. Related Work,[0],[0]
"Recently, the time-dependent Hawkes process (TiDeH) in (Kobayashi & Lambiotte, 2016) and the neural network-based Hawkes process in (Mei & Eisner, 2016) learn very flexible Hawkes processes with complicated intensity functions.",2. Related Work,[0],[0]
"Because they highly depend on the size and the quality of data, they may fail in the case of SDC event sequences.
",2. Related Work,[0],[0]
Learning from Imperfect Observations.,2. Related Work,[0],[0]
"In practice, we need to learn sequential models from imperfect observations (e.g., interleaved (Xu et al., 2016b), aggregated (Luo et al., 2016) and extremely-short sequences (Xu et al., 2016c)).",2. Related Work,[0],[0]
"Multiple imputation (MI) (Rubin, 2009) is a general framework to build surrogate observations from the current model.",2. Related Work,[0],[0]
"For time series, bootstrap method (Efron, 1982; Politis & Romano, 1994; Gonçalves & Kilian, 2004) and its variants (Paparoditis & Politis, 2001; Guan & Loh, 2007) have been used to improve learning results when observations are insufficient.",2. Related Work,[0],[0]
"In survival analysis, many techniques have been made to deal with truncated and censored data (Turnbull, 1974; De Gruttola & Lagakos, 1989; Klein & Moeschberger, 2005; Van den Berg & Drepper, 2016).",2. Related Work,[0],[0]
"For point processes, the global (Streit, 2010) or local (Fan, 2009) likelihood maximization estimators (MLE) are used to learn Poisson processes.",2. Related Work,[0],[0]
"Nonparametric approaches for non-homogeneous Poisson processes use the pseudo MLE (Sun & Kalbfleisch, 1995) or full MLE (Wellner & Zhang, 2000).",2. Related Work,[0],[0]
"The bootstrap methods above are also used to learn point processes (Cowling et al., 1996; Guan & Loh, 2007; Kirk & Stumpf, 2009).",2. Related Work,[0],[0]
"To learn Hawkes processes robustly, structural constraints, e.g., low-rank (Luo et al., 2015) and group-sparse regularizers (Xu et al., 2016a), are introduced.",2. Related Work,[0],[0]
"However, all of these methods do not consider the case of SDC event sequences for Hawkes processes.",2. Related Work,[0],[0]
Suppose that the original complete event sequences are in a long observation window.,3. Learning from SDC Event Sequences,[0],[0]
"However, the observation window in practice might be segmented into several intervals {Tnb , Tne }Nn=1, and we can only observe Kn SDC sequences {snk} Kn k=1 in the n-th interval, n = 1, ..., N .",3. Learning from SDC Event Sequences,[0],[0]
"Although we can still apply maximum likelihood estimator to learn Hawkes processes, i.e.,
minΘ− ∑
n,k logL(snk ; Θ), (3)
the SDC event sequences would lead to over-fitting problem and the loss of triggering patterns.",3. Learning from SDC Event Sequences,[0],[0]
Can we do better in such a situation?,3. Learning from SDC Event Sequences,[0],[0]
"In this work, we propose a data synthesis method based on a sampling-stitching mechanism, which extends SDC event sequences to longer ones and enhances the robustness of learning algorithms.",3. Learning from SDC Event Sequences,[0],[0]
Denote the k-th SDC event sequence in the n-th interval as snk .,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Because its predecessor is unavailable, if we learn the parameters of our model via (3) directly, we actually impose a strong assumption on our data that there is no event happening before snk (or previous events are too far away from snk to have influences on s n k ).",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Obviously, this assumption is questionable — it is likely that there are influential events happening before snk .",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"A more reasonable strategy is enumerating potential predecessors and maximizing the expected log-likelihood over the whole observation window:
minΘ− ∑
n,k Es∼HC
Tn b
[logL([s, snk ]; Θ)].",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"(4)
Here Ex∼D[f(x)] represents the expectation of function f(x) with random variable x yielding to a distribution D. s ∼ HCTnb means all possible history before T n b , and L([s, snk ]; Θ) is the likelihood of stitched sequence [s, snk ].
",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"The stitched sequence [s, snk ] can be generated via sampling SDC sequence s from previous 1st, ..., (k− 1)-th intervals and stitching s to snk .",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
The sampling process yields to the probabilistic distribution of the stitched sequences.,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Given snk , we can compute its similarity between its potential predecessor sn ′
k′ in [T n′ b , T n′ e ] as
w(sn ′
k′ , s n k )",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"=
{ 0, when Tn ′
e ≥",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Tnb S(Tnb , T n′ e )",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"S(f n k , f n′ k′ ), o.w. (5)
Here, S(a, b) = exp(−‖b",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
− a‖22/σs) is a predefined similarity function with parameter σs.,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"fnk is the feature of snk , which is available in some applications.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Note that the availability of feature is optional — even if the feature of sequence is unavailable, we can still define the similarity measurement purely based on time stamps.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"The normalized {w(sn′k′ , snk )} provides us with the probability that sn ′ k′ appears before snk , i.e., p(s n′ k′ |snk ) ∝",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"w(sn ′ k′ , s n k ).",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Then, we can sample sn ′
k′ according to the categorical distribution, i.e., sn ′
k′ ∼ Category(w(·, snk )).
",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
We can apply such a sampling-stitching mechanism L times iteratively to the SDC sequences in both backward and forward directions and get long stitched event sequences.,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Specifically, we represent a stitched event sequence as sstitch =",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"[s1, ..., s2L+1], sl ∈ {snk}, l = 1, ..., 2L+ 1, whose probability is
p(sstitch) ∝",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"∏2L
l=1 w(sl, sl+1).",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"(6)
Note that our data synthesis method naturally contains two variants.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"When the starting (the ending) point of time window is unavailable, we use the time stamp of the first (the last) event of SDC sequence instead.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Additionally, we can
relax the constraint Tn ′
e ≤",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
Tnb in (5) and allow a SDC sequence to have an overlap with its predecessor/successor.,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"In this case, we preserve the overlap part randomly either from itself or its predecessor/successor before applying our sampling-stitching method.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"These two variants ensure that our data synthesis method is doable in practice, which are used in the following experiments on real-world data.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"After applying our data synthesis method, we obtain many stitched event sequences, which can be used as instances for estimating Es∼HC
Tn b",3.2. Justification,[0],[0]
"[logL([s, snk ]; Θ)].",3.2. Justification,[0],[0]
"Specifically, taking advantage of stitched sequences, we can rewrite the learning problem in (4) approximately as
minΘ− ∑
sstitch∈S p(sstitch) logL(sstitch; Θ), (7)
which is actually the minimum cross-entropy estimation.",3.2. Justification,[0],[0]
"p(sstitch) represents the “true” probability that the stitched sequence happens, which is estimated via the predefined similarity measurement and the sampling mechanism.",3.2. Justification,[0],[0]
"The likelihood L(sstitch; Θ) represents the “unnatural” probability that the stitched sequence happens, which is estimated based on the definition in (1).",3.2. Justification,[0],[0]
Our data synthesis method takes advantage of the information of time stamps and (optional) features and makes p(sstitch) suitable for practical situations.,3.2. Justification,[0],[0]
"For example, the likelihood of a sequence generally reduces with the increase of observation time window.",3.2. Justification,[0],[0]
"The proposed probability p(sstitch) yields to the same pattern — according to (6), the longer a stitched sequence is, the smaller its probability becomes.
",3.2. Justification,[0],[0]
"The set of all possible stitched sequences, i.e., the S in (7), is very large, whose cardinality is |S| =",3.2. Justification,[0],[0]
O( ∏N n=1Kn).,3.2. Justification,[0],[0]
"In practice, we cannot and do not need to enumerate all possible combinations.",3.2. Justification,[0],[0]
"An empirical setting is making the number of stitched sequences comparable to that of original SDC event sequences, i.e., generating O( ∑N n=1Kn) stitched sequences.",3.2. Justification,[0],[0]
"In the following experiments, we just apply 5(= U ) trials and generate 5 stitched sequences for each original SDC event sequence, which achieves a tradeoff between computational complexity and performance.",3.2. Justification,[0],[0]
It should be noted that our data synthesis method is only suitable for those complicated point processes whose historical events have influences on current and future ones.,3.3. Feasibility,[0],[0]
"Specifically, we analyze the feasibility of our method for several typical point processes.
",3.3. Feasibility,[0],[0]
Poisson Processes.,3.3. Feasibility,[0],[0]
Our data synthesis method cannot improve learning results if the event sequences are generated via Poisson processes.,3.3. Feasibility,[0],[0]
"For Poisson processes, the happening rate of future events is independent of historical
events.",3.3. Feasibility,[0],[0]
"In other words, the intensity function of each interval can be learned independently based on the SDC event sequences.",3.3. Feasibility,[0],[0]
"The stitched sequences do not provide us with any additional information.
",3.3. Feasibility,[0],[0]
Hawkes Processes.,3.3. Feasibility,[0],[0]
"For Hawkes processes, whose intensity function is defined as (2), our data synthesis method can enhance the robustness of learning algorithm generally.",3.3. Feasibility,[0],[0]
"In particular, consider a “long” event sequence generated via a Hawkes process in the time window",3.3. Feasibility,[0],[0]
"[Tb, Te].",3.3. Feasibility,[0],[0]
"If we divide the time window into 2 intervals, i.e., [Tb, T ] and (T, Te], the intensity function corresponding to the second interval can be written as
λc(t) = µc + ∑ ti≤T φcci(t, ti) + ∑ T<ti≤Te φcci(t, ti).",3.3. Feasibility,[0],[0]
"(8)
If the events in the first interval are unobserved, we just have a SDC event sequence, and the second term in (8) is unavailable.",3.3. Feasibility,[0],[0]
"Learning Hawkes processes directly from the SDC event sequence ignores the information of the second term, which has a negative influence on learning results.",3.3. Feasibility,[0],[0]
Our data synthesis method leverages the information from other potential predecessors and generates multiple candidate long sequences.,3.3. Feasibility,[0],[0]
"As a result, we obtain multiple intensity functions sharing the second interval and maximize the weighted sum of their log-likelihood functions (i.e., an estimated expectation of the log-likelihood of the real long sequence), as (7) does.
",3.3. Feasibility,[0],[0]
"Compared with learning from SDC event sequences directly, applying our data synthesis method can improve learning results in general, unless the term∑ ti≤T φcci(t, ti) is ignorable.",3.3. Feasibility,[0],[0]
"Specifically, we can model the impact functions {φcc′(t, s)}c,c′∈C of Hawkes processes based on basis representation:
φcc′(t, s) = ψcc′(t)︸ ︷︷ ︸",3.3. Feasibility,[0],[0]
Infectivity × g(t−,3.3. Feasibility,[0],[0]
s)︸ ︷︷ ︸,3.3. Feasibility,[0],[0]
"Triggering kernel
= ∑M
m=1 acc′mκm(t)g(t− s).
",3.3. Feasibility,[0],[0]
"(9)
Here, we decompose impact functions into two parts: 1)",3.3. Feasibility,[0],[0]
Infectivity ψcc′(t) = ∑M m=1 acc′mκm(t) represents the infectivity of event type c′ to c at time t.1 2),3.3. Feasibility,[0],[0]
Triggering kernel g(t) = exp(−βt) measures the time decay of infectivity.,3.3. Feasibility,[0],[0]
It means that the infectivity of a historical event to current one reduces exponentially with the increase of temporal distance between them.,3.3. Feasibility,[0],[0]
"When β is very large, φcc′(t, s) decays rapidly with the increase of t − s, and the events happening long ago can be ignored.",3.3. Feasibility,[0],[0]
"In such a situation, our data synthesis method is unable to improve learning results.
",3.3. Feasibility,[0],[0]
"1When M = 1 and κm(t) ≡ 1, we obtain the simplest timeinvariant Hawkes process.",3.3. Feasibility,[0],[0]
"Relaxing the shift-invariant assumption, i.e., M > 1 and κm(t) is Gaussian, we obtain a flexible time-varying Hawkes process model.",3.3. Feasibility,[0],[0]
Hawkes process is a kind of physically-interpretable model for many natural and social phenomena.,4. Implementation for Hawkes Processes,[0],[0]
The proposed model in (9) reflects many common properties of realworld event sequences.,4. Implementation for Hawkes Processes,[0],[0]
"First, the infectivity among various event types often changes smoothly in practice: in social networks, the interaction between two users changes smoothly, which is not established or blocked suddenly; in disease networks, the infectivity among diseases should change smoothly with the increase of patient’s age.",4. Implementation for Hawkes Processes,[0],[0]
Applying Gaussian basis representation guarantees the smoothness of infectivity function.,4. Implementation for Hawkes Processes,[0],[0]
"Second, the triggering kernel measures the decay of infectivity over time.",4. Implementation for Hawkes Processes,[0],[0]
"According to existing work, the decay of infectivity is exponential approximately, which has been verified in many real-world data (Zhou et al., 2013a; Kobayashi & Lambiotte, 2016; Choi et al., 2015).",4. Implementation for Hawkes Processes,[0],[0]
"For learning Hawkes processes from SDC event sequences, we combine our data synthesis method with an EM-based learning algorithm of Hawkes processes.",4. Implementation for Hawkes Processes,[0],[0]
"Applying our data synthesis method, we obtain a set of stitched event sequences S = {sn} and their appearance probabilities {pn}, where sn = {(tni , cni )",4. Implementation for Hawkes Processes,[0],[0]
In i=1|tni ∈,4. Implementation for Hawkes Processes,[0],[0]
"[Tnb , Tne ], cni ∈ C} and pn is calculated based on (5).",4. Implementation for Hawkes Processes,[0],[0]
"According to (7, 9), we can learn target Hawkes process via
min µ≥0, A≥0
− ∑|S|
n=1 pn logL(sn; Θ) + γR(A).",4. Implementation for Hawkes Processes,[0],[0]
"(10)
",4. Implementation for Hawkes Processes,[0],[0]
"Here Θ = {µ, A} represents the parameters of our model.",4. Implementation for Hawkes Processes,[0],[0]
The vector µ =,4. Implementation for Hawkes Processes,[0],[0]
[µc] and the tensor A = [acc′m] are nonnegative.,4. Implementation for Hawkes Processes,[0],[0]
"Based on (1, 9), the log-likelihood function is
logL(sn; Θ)
= ∑In
i=1",4. Implementation for Hawkes Processes,[0],[0]
log λci(ti)− ∑C c=1 ∫,4. Implementation for Hawkes Processes,[0],[0]
"Tne Tnb λc(s)ds
= ∑In
i=1",4. Implementation for Hawkes Processes,[0],[0]
"log
[ µcni + ∑",4. Implementation for Hawkes Processes,[0],[0]
j<i g(τnij) ∑ m acni cnjmκm(t n,4. Implementation for Hawkes Processes,[0],[0]
i ) ],4. Implementation for Hawkes Processes,[0],[0]
"− ∑C
c=1
( ∆nµc − ∑ m ∑In i=1",4. Implementation for Hawkes Processes,[0],[0]
∑,4. Implementation for Hawkes Processes,[0],[0]
"j≤i accnjmGij ) ,
where τnij = t n",4. Implementation for Hawkes Processes,[0],[0]
i,4. Implementation for Hawkes Processes,[0],[0]
"− tnj , Gij = ∫ tni+1 tni
κm(s)g(s",4. Implementation for Hawkes Processes,[0],[0]
"− tnj )ds, and ∆n = Tne",4. Implementation for Hawkes Processes,[0],[0]
− Tnb .,4. Implementation for Hawkes Processes,[0],[0]
"R(A) represents the regularizer of parameters, whose weight is γ.",4. Implementation for Hawkes Processes,[0],[0]
"Following existing work in (Luo et al., 2015; Zhou et al., 2013a; Xu et al., 2016a), we assume the infectivity connections among different event types to be sparse and impose a `1-norm regularizer on the coefficient tensor A, i.e., R(A) = ‖A‖1 =∑ c,c′,m |acc′m|.
",4. Implementation for Hawkes Processes,[0],[0]
We can solve the problem via an EM algorithm.,4. Implementation for Hawkes Processes,[0],[0]
"Specifically, when sparse regularizer is applied, we take advantage of ADMM method, introducing auxiliary variable Z =",4. Implementation for Hawkes Processes,[0],[0]
[zcc′m] and dual variable U =,4. Implementation for Hawkes Processes,[0],[0]
"[ucc′m] for A and
rewriting the objective function in (10) as − ∑
n pn logL(sn; Θ) + 0.5ρ‖A−Z‖2F
+ ρtr(U>(A−Z))",4. Implementation for Hawkes Processes,[0],[0]
"+ γ‖Z‖1.
",4. Implementation for Hawkes Processes,[0],[0]
"Here ρ controls the weights of regularization terms, which increases with the number of EM iterations.",4. Implementation for Hawkes Processes,[0],[0]
tr(·) computes the trace of matrix.,4. Implementation for Hawkes Processes,[0],[0]
"Then, we can update {µ,A}, Z, and U alternatively.
",4. Implementation for Hawkes Processes,[0],[0]
Update µ,4. Implementation for Hawkes Processes,[0],[0]
"and A: Given the parameters in the k-th iteration, we apply Jensen’s inequality to − ∑ n logL(sn; Θ) and obtain a surrogate objective function for µ andA:
Q(µ,A; µk,Ak,Zk,Uk)
",4. Implementation for Hawkes Processes,[0],[0]
=− N∑ n=1 pn { In∑ i=1,4. Implementation for Hawkes Processes,[0],[0]
"[∑ j<i M∑ m=1 qijm log g(τnij)acni cnjmκm(t n i ) qijm
+ qi log µcni qi",4. Implementation for Hawkes Processes,[0],[0]
− C∑ c=1 M∑ m=1,4. Implementation for Hawkes Processes,[0],[0]
∑ j≤i accnjmGij ] −∆n C∑ c=1 µc } + 0.5ρ‖A−Zk,4. Implementation for Hawkes Processes,[0],[0]
"+Uk‖2F ,
where qi = µkcn i
λk cn",4. Implementation for Hawkes Processes,[0],[0]
i,4. Implementation for Hawkes Processes,[0],[0]
(,4. Implementation for Hawkes Processes,[0],[0]
"tni )
and qijm = g(τnij)a k cn",4. Implementation for Hawkes Processes,[0],[0]
i cn j mκm(t n,4. Implementation for Hawkes Processes,[0],[0]
"i )
",4. Implementation for Hawkes Processes,[0],[0]
λk cn,4. Implementation for Hawkes Processes,[0],[0]
i,4. Implementation for Hawkes Processes,[0],[0]
(,4. Implementation for Hawkes Processes,[0],[0]
"tni )
, and
λkcni (t n i ) is calculated based on µ k and Ak.",4. Implementation for Hawkes Processes,[0],[0]
"Then, we can update µ andA via solving ∂Q∂µ = 0 and ∂Q ∂A = 0.",4. Implementation for Hawkes Processes,[0],[0]
"Both of these two equations have closed-form solution:
µk+1c =
∑ n pn ∑ cni =c
qi∑ n pn∆n ,
ak+1cc′m =
√ B2 − 4ρC −B
2ρ ,
(11)
where B = ρ(ukcc′m − zkcc′m) +",4. Implementation for Hawkes Processes,[0],[0]
∑,4. Implementation for Hawkes Processes,[0],[0]
n,4. Implementation for Hawkes Processes,[0],[0]
"pn ∑
cni =c, c n",4. Implementation for Hawkes Processes,[0],[0]
j,4. Implementation for Hawkes Processes,[0],[0]
"=c
′, j≤i Gij , C =",4. Implementation for Hawkes Processes,[0],[0]
"− ∑ n pn ∑
cni =c, c n j =c
′, j≤i qijm.
",4. Implementation for Hawkes Processes,[0],[0]
Update Z:,4. Implementation for Hawkes Processes,[0],[0]
"Given Ak+1 and Uk, we can update Z via solving the following optimization problem:
minZ γ‖Z‖1 + 0.5ρ‖Ak+1 −Z +Uk‖2F .
",4. Implementation for Hawkes Processes,[0],[0]
"Applying soft-thresholding method, we have
Zk+1 =",4. Implementation for Hawkes Processes,[0],[0]
"F γ ρ (Ak+1 +Uk), (12)
where Fη(x) = sign(x) min{|x|",4. Implementation for Hawkes Processes,[0],[0]
"− η, 0} is the softthresholding function.
",4. Implementation for Hawkes Processes,[0],[0]
Update U :,4. Implementation for Hawkes Processes,[0],[0]
"Given Ak+1 and Zk+1, we can further update dual variable as
Uk+1 = Uk + (Ak+1 −Zk+1).",4. Implementation for Hawkes Processes,[0],[0]
"(13)
Algorithm 1 Learning Algorithm of Hawkes Processes 1: Input: Event sequences S.",4. Implementation for Hawkes Processes,[0],[0]
"The threshold V . Prede-
fined parameters β, σκ, and γ. 2: Output: ParametersA and µ. 3: Initialize Ak and µk randomly.",4. Implementation for Hawkes Processes,[0],[0]
"Zk = Ak, Uk = 0.",4. Implementation for Hawkes Processes,[0],[0]
k,4. Implementation for Hawkes Processes,[0],[0]
"= 0, ρ = 1. 4: repeat 5: ObtainAk+1 and µk+1 via (11).",4. Implementation for Hawkes Processes,[0],[0]
6: Obtain Zk+1 via (12).,4. Implementation for Hawkes Processes,[0],[0]
7: Obtain Uk+1 via (13).,4. Implementation for Hawkes Processes,[0],[0]
"8: k = k + 1, ρ = 1.5ρ.",4. Implementation for Hawkes Processes,[0],[0]
"9: until ‖Ak −Ak−1‖F < V
10: A = Ak, µ = µk.
",4. Implementation for Hawkes Processes,[0],[0]
"In summary, Algorithm 1 shows the scheme of our learning method.",4. Implementation for Hawkes Processes,[0],[0]
Note that the algorithm can be applied to SDC event sequences directly via ignoring pn’s.,4. Implementation for Hawkes Processes,[0],[0]
"To demonstrate the usefulness of our data synthesis method, we combine it with various learning algorithms of Hawkes processes and learn different models accordingly from SDC event sequences.",5.1. Implementation Details,[0],[0]
"For time-invariant Hawkes processes, we consider two learning algorithms — our EMbased learning algorithm and the least squares (LS) algorithm in (Eichler et al., 2016).",5.1. Implementation Details,[0],[0]
"For time-varying Hawkes processes, we apply our EM-based learning algorithm.",5.1. Implementation Details,[0],[0]
"In the following experiments, we use Gaussian basis functions: κm(t) =",5.1. Implementation Details,[0],[0]
exp((t − tm)2/σκ) with center tm and bandwidth σκ.,5.1. Implementation Details,[0],[0]
"The number and the bandwidth of basis can be set according to the basis selection method proposed in (Xu et al., 2016a).",5.1. Implementation Details,[0],[0]
"Additionally, we set V = 10−4, γ = 1, and σs = 1 in our algorithm.",5.1. Implementation Details,[0],[0]
"Given SDC event sequences, we learn Hawkes processes in three ways: 1) learning directly from SDC event sequences; 2) applying the stationary bootstrap method in (Politis & Romano, 1994) to generate more synthetic SDC event sequences and learning from these sequences accordingly; 3) learning from stitched sequences generated via our data synthesis method.",5.1. Implementation Details,[0],[0]
"For real-world data, whose SDC sequences do not have predefined starting and ending time stamps, we applied the variants of our method mentioned in the end of Section 3.1.",5.1. Implementation Details,[0],[0]
The synthetic SDC event sequences are generated via the following method: 2000 complete event sequences are simulated in the time window,5.2. Synthetic Data,[0],[0]
"[0, 50] based on a 2-dimensional Hawkes process.",5.2. Synthetic Data,[0],[0]
"The base intensity {µc}2c=1 are randomly generated in the range [0.1, 0.2].",5.2. Synthetic Data,[0],[0]
"The parameter of trigger-
ing kernel, β, is set to be 0.2.",5.2. Synthetic Data,[0],[0]
"For time-invariant Hawkes processes, we set the infectivity {ψcc′(t)} to be 4 constants randomly generated in the range [0, 0.2].",5.2. Synthetic Data,[0],[0]
"For time-varying Hawkes processes, we set ψcc′(t) = 0.2 cos(2π ωcc′ 50 t), where {ωcc′} are randomly generated in the range [1, 4].",5.2. Synthetic Data,[0],[0]
"Given these complete event sequences, we select 1000 sequences as testing set while the remaining 1000 sequences as training set.",5.2. Synthetic Data,[0],[0]
"To generate SDC event sequences, we segment time window into 10 intervals, and just randomly preserve the data in one interval for each training sequences.",5.2. Synthetic Data,[0],[0]
"We test all methods in 10 trials and compare them on the relative error between real parameters Θ and their estimation results Θ̂, i.e., ‖Θ−Θ̂‖2‖Θ‖2 , and the log-likelihood of testing sequences.
",5.2. Synthetic Data,[0],[0]
Time-invariant Hawkes Processes.,5.2. Synthetic Data,[0],[0]
Fig. 2 shows the comparisons on log-likelihood and relative error for various methods.,5.2. Synthetic Data,[0],[0]
"In Fig. 2(a) we can find that compared with the learning results based on complete event sequences, the results based on SDC event sequences degrade a lot (lower log-likelihood and higher relative error) because of the loss of information.",5.2. Synthetic Data,[0],[0]
"Our data synthesis method improves the learning results consistently with the increase of training sequences, which outperforms its bootstrap-based competitor (Politis & Romano, 1994) as well.",5.2. Synthetic Data,[0],[0]
"To demonstrate the universality of our method, besides our EM-based algorithm, we apply our method to the Least Squares (LS) algo-
rithm (Eichler et al., 2016).",5.2. Synthetic Data,[0],[0]
Fig. 2(b) shows that our method also improves the learning results of the LS algorithm in the case of SDC event sequences.,5.2. Synthetic Data,[0],[0]
"Both the log-likelihood and the relative error obtained from the stitched sequences approach to the results learned from complete sequences.
",5.2. Synthetic Data,[0],[0]
Time-varying Hawkes Processes.,5.2. Synthetic Data,[0],[0]
Fig. 3 shows the comparisons on log-likelihood and relative error for various methods.,5.2. Synthetic Data,[0],[0]
"Similarly, the learning results are improved because of applying our method — higher log-likelihood and lower relative error are obtained and their standard deviation (the error bars associated with curves) is shrunk.",5.2. Synthetic Data,[0],[0]
"In this case, applying our method twice achieves better results than applying once, which verifies the usefulness of the iterative framework in our sampling-stitching algorithm.",5.2. Synthetic Data,[0],[0]
"Besides objective measurements, in Fig. 4 we visualize the infectivity functions {ψcc′(t)}.",5.2. Synthetic Data,[0],[0]
"It is easy to find that the infectivity functions learned from stitched sequences (red curves) are comparable to those learned from complete event sequences (yellow curves), which have small estimation errors of the ground truth (black curves).
",5.2. Synthetic Data,[0],[0]
"Note that our iterative framework is useful, especially for time-varying Hawkes processes, when the number of stitches is not very large.",5.2. Synthetic Data,[0],[0]
"In our experiments, we fixed the maximum number of synthetic sequences.",5.2. Synthetic Data,[0],[0]
"As a result, Figs. 2 and 3 show that the likelihoods first increase (i.e., stitching once or twice) and then decrease (i.e., stitching more than three times) while the relative errors have opponent changes w.r.t.",5.2. Synthetic Data,[0],[0]
the number of stitches.,5.2. Synthetic Data,[0],[0]
These phenomena imply that too many stitches introduce too much unreliable interdependency among events.,5.2. Synthetic Data,[0],[0]
"Therefore, we fix
the number of stitches to 2 in the following experiments.",5.2. Synthetic Data,[0],[0]
"Besides synthetic data, we also test our method on realworld data, including the LinkedIn data collected by ourselves and the MIMIC III data set (Johnson et al., 2016).
",5.3. Real-World Data,[0],[0]
LinkedIn Data.,5.3. Real-World Data,[0],[0]
"The LinkedIn data we collected online contain job hopping records of 3, 000 LinkedIn users in 82 IT companies.",5.3. Real-World Data,[0],[0]
"For each user, her/his check-in time stamps corresponding to different companies are recorded as an event sequence, and her/his profile (e.g., education background, skill list, etc.) is treated as the feature associated with the sequence.",5.3. Real-World Data,[0],[0]
"For each person, the attractiveness of a company is always time-varying.",5.3. Real-World Data,[0],[0]
"For example, a young man may be willing to join in startup companies and increase his income via jumping between different companies.",5.3. Real-World Data,[0],[0]
"With the increase of age, he would more like to stay in the same company and achieve internal promotions.",5.3. Real-World Data,[0],[0]
"In other words, the infectivity network among different companies should be dynamical w.r.t.",5.3. Real-World Data,[0],[0]
the age of employees.,5.3. Real-World Data,[0],[0]
"Unfortunately, most of the records in LinkedIn are short and doubly-censored — only the job hopping events in recent years are recorded.",5.3. Real-World Data,[0],[0]
"How to construct the dynamical infectivity network among different companies from the SDC event sequences is still an open problem.
",5.3. Real-World Data,[0],[0]
"Applying our data synthesis method, we can stitch different users’ job hopping sequences based on their ages (time stamps) and their profile (feature) and learn the dynamical network of company over time.",5.3. Real-World Data,[0],[0]
"In particular, we select 100 users with relatively complete job hopping history (i.e., the range of their working experience is over 25 years) as testing set.",5.3. Real-World Data,[0],[0]
"The remaining 2, 900 users are randomly selected as training set.",5.3. Real-World Data,[0],[0]
The log-likelihood of testing set in 10 trials is shown in Fig. 5(a).,5.3. Real-World Data,[0],[0]
"We can find that the log-likelihood obtained from stitched sequences is higher than that obtained from original SDC sequences or that obtained from the sequences generated via the bootstrap method (Politis & Romano, 1994), and its standard deviation is bounded
stably.",5.3. Real-World Data,[0],[0]
Fig. 6(a) visualizes the adjacent matrix of infectivity network.,5.3. Real-World Data,[0],[0]
"The properties of the network verifies the rationality of our results: 1) the diagonal elements of the adjacent matrix are larger than other elements in general, which reflects the fact that most employees would like to stay in the same company and achieve a series of internal promotions; 2) with the increase of age, the infectivity network becomes sparse, which reflects the fact that users are more likely to try different companies in the early stages of their careers.
",5.3. Real-World Data,[0],[0]
MIMIC III Data.,5.3. Real-World Data,[0],[0]
"The MIMIC III data contain admission records of over 40, 000 patients in the Beth Israel Deaconess Medical Center between 2001 and 2012.",5.3. Real-World Data,[0],[0]
"For each patient, her/his admission time stamps and diseases (represented via the ICD-9 codes (Deyo et al., 1992)) are recorded as an event sequence, and her/his profile (including gender, race and chronic history) is represented as binary feature of the sequence.",5.3. Real-World Data,[0],[0]
"As aforementioned, some work (Choi et al., 2015) has been done to extract timeinvariant disease network from admission records.",5.3. Real-World Data,[0],[0]
"However, the real disease network should be time-varying w.r.t.",5.3. Real-World Data,[0],[0]
the age of patient.,5.3. Real-World Data,[0],[0]
"Similar to the LinkedIn data, here we only obtain SDC event sequences — the ranges of most admission records are just 1 or 2 years.
",5.3. Real-World Data,[0],[0]
"Applying our data synthesis method, we can leverage the information from different patients and stitch their sequences based on their ages and their profile.",5.3. Real-World Data,[0],[0]
"Focusing on 600 common diseases in 12 categories, we select 15, 000 patients’ admission records randomly as training set and 1, 000 patients with relatively complete records as testing set.",5.3. Real-World Data,[0],[0]
Fig.,5.3. Real-World Data,[0],[0]
5(b) shows that applying our data synthesis method indeed helps to improve log-likelihood of testing data.,5.3. Real-World Data,[0],[0]
"Compared with our bootstrap-based competitor, our data synthesis method gets more obvious improvements.
",5.3. Real-World Data,[0],[0]
"Furthermore, we visualize the adjacent matrix of dynamical network of disease categories in Fig. 6(b).",5.3. Real-World Data,[0],[0]
"We can find that: 1) with the increase of age the disease network becomes dense, which reflects the fact that the complications of diseases are more and more common when people become old; 2) the networks show that neoplasms and the diseases of circulatory, respiratory, and digestive systems have strong self-triggering patterns because the treatments of these diseases often include several phases and require patients to make admission multiple times; 3) for kids and teenagers, their disease networks (i.e., “Age 0” and “Age 10” networks) are very sparse, and their common diseases mainly include neoplasms and the diseases of circulatory, respiratory, and digestive systems; 4) for middle-aged people, the reasons for their admissions are diverse and complicated so that their disease networks are dense and include many mutually-triggering patterns; 5) for longevity people, their disease networks (i.e., “Age 80” and “Age 90” networks) are relatively sparser than those of middle-aged people, because their admissions are generally caused by elderly chronic diseases.
",5.3. Real-World Data,[0],[0]
"Additionally, we visualize the dynamical networks of the diseases of circulatory systems in Fig. 7, and find some interesting triggering patterns.",5.3. Real-World Data,[0],[0]
"For example, for kids (“Age 0” network), the typical circulatory diseases are “diseases of mitral and aortic valves” (ICD-9 396) and “cardiac dysrhythmias” (ICD-9 427), which are common for premature babies and the kids having congenital heart disease.",5.3. Real-World Data,[0],[0]
"For the old (“Age 80” network), the network becomes dense.",5.3. Real-World Data,[0],[0]
"We can find that 1) as a main cause of death, “heart failure” (ICD-9 428) is triggered via multiple other diseases, especially “secondary hypertension” (ICD-9 405); 2) “sec-
ondary hypertension” is also likely to cause “other and illdefined cerebrovascular disease” (ICD-9 437); 3) “Hemorrhoids” (ICD-9 455), as a common disease with strong self-triggering pattern, will cause frequent admissions of patients.",5.3. Real-World Data,[0],[0]
"In summary, the analysis above verifies the rationality of our result — the dynamical disease networks we learned indeed reflect the properties of human’s health trajectory.",5.3. Real-World Data,[0],[0]
The list of ICD-9 codes and the complete enlarged network over age are shown in the supplementary file.,5.3. Real-World Data,[0],[0]
"In this paper, we propose a novel data synthesis method to learn Hawkes processes from SDC event sequences.",6. Conclusion,[0],[0]
"With the help of temporal information and optional features, we measure the similarities among different SDC event sequences and estimate the distribution of potential long event sequences.",6. Conclusion,[0],[0]
"Applying a sampling-stitching mechanism, we successfully synthesize a large amount of long event sequences and learn point processes robustly.",6. Conclusion,[0],[0]
We test our method for both time-invariant and time-varying Hawkes processes.,6. Conclusion,[0],[0]
Experiments show that our data synthesis method improves the robustness of learning algorithms for various models.,6. Conclusion,[0],[0]
"In the future, we plan to provide more theoretical and quantitative analysis to our data synthesis method and apply it to more applications.",6. Conclusion,[0],[0]
"This work is supported in part via NSF IIS-1639792, DMS-1317424, NIH R01 GM108341, NSFC 61628203, U1609220 and the Key Program of Shanghai Science and Technology Commission 15JC1401700.",Acknowledgements,[0],[0]
Many real-world applications require robust algorithms to learn point processes based on a type of incomplete data — the so-called short doublycensored (SDC) event sequences.,abstractText,[0],[0]
We study this critical problem of quantitative asynchronous event sequence analysis under the framework of Hawkes processes by leveraging the idea of data synthesis.,abstractText,[0],[0]
"Given SDC event sequences observed in a variety of time intervals, we propose a sampling-stitching data synthesis method, sampling predecessors and successors for each SDC event sequence from potential candidates and stitching them together to synthesize long training sequences.",abstractText,[0],[0]
The rationality and the feasibility of our method are discussed in terms of arguments based on likelihood.,abstractText,[0],[0]
Experiments on both synthetic and real-world data demonstrate that the proposed data synthesis method improves learning results indeed for both timeinvariant and time-varying Hawkes processes.,abstractText,[0],[0]
Learning Hawkes Processes from Short Doubly-Censored Event Sequences,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 595–605 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"For most Natural Language Processing (NLP) tasks, obtaining sufficient annotated text for training accurate models is a critical bottleneck.",1 Introduction,[0],[0]
"Thus active learning has been applied to NLP tasks to minimise the expense of annotating data (Thompson et al., 1999; Tong and Koller, 2001; Settles and Craven, 2008).",1 Introduction,[0],[0]
"Active learning aims to reduce cost by identifying a subset of unlabelled data for annotation, which is selected to maximise the accuracy of a supervised model trained on the data (Settles, 2010).",1 Introduction,[0],[0]
"There have been many successful applications to NLP, e.g., Tomanek et al. (2007) used an active learning algorithm for CoNLL corpus to get an F1 score 84% with a reduction of annotation cost of about 48%.",1 Introduction,[0],[0]
"In prior work most active learning algorithms are designed for English based on
heuristics, such as using uncertainty or informativeness.",1 Introduction,[0],[0]
"There has been comparatively little work done about how to learn the active learning strategy itself.
",1 Introduction,[0],[0]
"It is no doubt that active learning is extremely important for other languages, particularly lowresource languages, where annotation is typically difficult to obtain, and annotation budgets more modest (Garrette and Baldridge, 2013).",1 Introduction,[0],[0]
"Such settings are a natural application for active learning, however there is little work to this end.",1 Introduction,[0],[0]
"A potential reason is that most active learning algorithms require a substantial ‘seed set’ of data for learning a basic classifier, which can then be used for active data selection.",1 Introduction,[0],[0]
"However, given the dearth of data in the low-resource setting, this assumption can make standard approaches infeasible.
",1 Introduction,[0],[0]
"In this paper,1 we propose PAL, short for Policy based Active Learning, a novel approach for learning a dynamic active learning strategy from data.",1 Introduction,[0],[0]
"This allows for the strategy to be applied in other data settings, such as cross-lingual applications.",1 Introduction,[0],[0]
"Our algorithm does not use a fixed heuristic, but instead learns how to actively select data, formalised as a reinforcement learning (RL) problem.",1 Introduction,[0],[0]
"An intelligent agent must decide whether or not to select data for annotation in a streaming setting, where the decision policy is learned using a deep Q-network (Mnih et al., 2015).",1 Introduction,[0],[0]
"The policy is informed by observations including sentences’ content information, the supervised model’s classifications and its confidence.",1 Introduction,[0],[0]
"Accordingly, a rich and dynamic policy can be learned for annotating new data based on the past sequence of annotation decisions.
",1 Introduction,[0],[0]
"Furthermore, in order to reduce the dependence on the data in the target language, which may be low resource, we first learn the policy of active
1Source code available at https://github.com/ mengf1/PAL
595
learning on another language and then transfer it to the target language.",1 Introduction,[0],[0]
"It is easy to learn a policy on a high resource language, where there is plentiful data, such as English.",1 Introduction,[0],[0]
"We use cross-lingual word embeddings to learn compatible data representations for both languages, such that the learned policy can be easily ported into the other language.
",1 Introduction,[0],[0]
Our work is different for prior work in active learning for NLP.,1 Introduction,[0],[0]
Most previous active learning algorithms developed for NER tasks is based on one language and then applied to the language itself.,1 Introduction,[0],[0]
"Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016).",1 Introduction,[0],[0]
"However, in our algorithm, we implicitly use uncertainty information as one kind of observations to the RL agent.
",1 Introduction,[0],[0]
The remainder of this paper is organised as follows.,1 Introduction,[0],[0]
"In Section 2, we briefly review some related work.",1 Introduction,[0],[0]
"In Section 3, we present active learning algorithms, which cross multiple languages.",1 Introduction,[0],[0]
The experimental results are presented in Section 4.,1 Introduction,[0],[0]
We conclude our work in Section 5.,1 Introduction,[0],[0]
"As supervised learning methods often require a lot of training data, active learning is a technique that selects a subset of data to annotate for training the best classifier.",2 Related work,[0],[0]
"Existing active learning (AL) algorithms can be generally considered as three categories: 1) uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2001), which selects the data about which the current classifier is the most uncertain; 2) query by committee (Seung et al., 1992), which selects the data about which the “committee” disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled.",2 Related work,[0],[0]
"Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al., 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).",2 Related work,[0],[0]
Qian et al. used uncertainty sampling to jointly perform on English and Chinese.,2 Related work,[0],[0]
"Stratos and Collins and Zhang et al. deployed uncertainty-based AL algorithms for languages with the minimal supervision.
",2 Related work,[0],[0]
Deep reinforcement learning (DRL) is a general-purpose framework for decision making based on representation learning.,2 Related work,[0],[0]
"Recently, there are some notable examples include deep Qlearning (Mnih et al., 2015), deep visuomotor policies (Levine et al., 2016), attention with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al., 2015).",2 Related work,[0],[0]
"Other important works include massively parallel frameworks (Nair et al., 2015), dueling architecture (Wang et al., 2016) and expert move prediction in the game of Go (Maddison et al., 2015), which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search (Silver et al., 2016).",2 Related work,[0],[0]
DRL has been also studied in NLP tasks.,2 Related work,[0],[0]
"For example, recently, DRL has been studied for information extraction problem (Narasimhan et al., 2016).",2 Related work,[0],[0]
They designed a framework that can decide to acquire external evidence and the framework is under the reinforcement learning method.,2 Related work,[0],[0]
"However, there has been fairly little work on using DRL to learn active learning strategies for language processing tasks, especially in cross-lingual settings.
",2 Related work,[0],[0]
"Recent deep learning work has also looked at transfer learning (Bengio, 2012).",2 Related work,[0],[0]
"More recent work in deep learning has also considered transferring policies by reusing policy parameters between environments (Parisotto et al., 2016; Rusu et al., 2016), using either regularization or novel neural network architectures, though this work has not looked at transfer active learning strategies between languages with shared feature space in state.",2 Related work,[0],[0]
"We now show how active learning can be formalised as as a decision process, and then show how this allows for the active learning selection policy to be learned from data using deep reinforcement learning.",3 Methodology,[0],[0]
Later we introduce a method for transferring the policy between languages.,3 Methodology,[0],[0]
"Active learning is a simple technique for labelling data, which involves first selecting some instances from an unlabelled dataset, which are then annotated by a human oracle, which is then repeated many times until a termination criterion is satisfied, e.g., the annotation budget is exhausted.",3.1 Active learning as a decision process,[0],[0]
"Most often the selection function is based on the pre-
dictions of a trained model, which has been fit to the labelled dataset at each stage in the algorithm, where datapoints are selected based on the model’s predictive uncertainty (Lewis and Gale, 1994), or divergence in predictions over an ensemble (Seung et al., 1992).",3.1 Active learning as a decision process,[0],[0]
"The key idea of these methods is to find the instances on which the model is most likely to make errors, such that after their labelling and inclusion in the training set, the model becomes more robust to these types of errors on unseen data.
",3.1 Active learning as a decision process,[0],[0]
"The steps in active learning can be viewed as a decision process, a means of formalising the active learning algorithm as a sequence of decisions, where the stages of active learning correspond to the state of the system.",3.1 Active learning as a decision process,[0],[0]
"Accordingly, the state corresponds to the selected data for labelling and their labels, and each step in the active learning algorithm corresponds to a selection action, wherein the heuristic selects the next items from a pool.",3.1 Active learning as a decision process,[0],[0]
"This process terminates when the budget is exhausted.
",3.1 Active learning as a decision process,[0],[0]
"Effectively the active learning heuristic is operating as a decision policy, a form of function taking as input the current state — comprising the labelled data, from which a model is trained — and a candidate unlabelled data point — e.g., the model uncertainty.",3.1 Active learning as a decision process,[0],[0]
"This raises the opportunity to consider general policy functions, based on the state and data point inputs, and resulting in a labelling decision, and, accordingly a mechanism for learning such functions from data.",3.1 Active learning as a decision process,[0],[0]
"We now elaborate on the components of this process, namely the formulation of the decision process, architecture of the policy function, and means of learning the decision policy automatically from data.",3.1 Active learning as a decision process,[0],[0]
"For simplicity, we make a streaming assumption, whereby unlabelled data (sentences) arrive in a stream (Lewis and Gale, 1994).2 As each instance arrives, an agent must decide the action to take, namely whether or not the instance should be manually annotated.",3.2 Stream-based learning,[0],[0]
"This process is illustrated in Figure 1, which illustrates the space of decision sequences for a small corpus.",3.2 Stream-based learning,[0],[0]
"As part of this process, a separate model, pφ, is trained on the labelled data, and updated accordingly as the labelled dataset is expanded as new annotations ar-
2This is different to pool-based active learning, where one of several options is chosen for annotation.",3.2 Stream-based learning,[0],[0]
"Our setup permits simpler learning, while remaining sufficiently general.
rive.",3.2 Stream-based learning,[0],[0]
"This model is central to the policy for choosing the labelling actions at each stage, and for determining the reward for a sequence of actions.
",3.2 Stream-based learning,[0],[0]
"This is a form of Markov Decision Process (MDP), which allows the learning of a policy that can dynamically select instances that are most informative.",3.2 Stream-based learning,[0],[0]
"As illustrated in Figure 1 at each time, the agent observes the current state si which includes the sentence xi, and the learned model φ.",3.2 Stream-based learning,[0],[0]
"The agent selects a binary action ai, denoting whether to label xi, according to the policy π.",3.2 Stream-based learning,[0],[0]
"For ai = 1, the corresponding sentence is labelled and added to the labelled data, and the model pφ updated to include this new training point.",3.2 Stream-based learning,[0],[0]
"The process then repeats, terminating when either the dataset is exhausted or a fixed annotation budget is reached.",3.2 Stream-based learning,[0],[0]
"After termination a reward is computed based on the accuracy of the final model, φ.",3.2 Stream-based learning,[0],[0]
"We represent the MDP framework as a tuple 〈S,A, Pr(si+1|si, a), R〉, where S = {s} is the space of all possible states, A = {0, 1} is the set of actions, R(s, a) is the reward function, and Pr(si+1|si, a) is the transition function.",3.2 Stream-based learning,[0],[0]
The state at time i comprises the candidate instance being considered for annotation and the labelled dataset constructed in steps 1 . . .,3.2.1 State,[0],[0]
"i. We represent the state using a continuous vector, using the concatenation of the vector representation of xi, and outputs of the model pφ trained over the labelled data.",3.2.1 State,[0],[0]
"These outputs use both the predictive marginal distributions of the model on the instance, and a representation of the model’s confidence.",3.2.1 State,[0],[0]
"We now elaborate on each component.
",3.2.1 State,[0],[0]
"Content representation A key input to the agent is the content of the sentence, xi, which we encode using a convolutional neural network to arrive at a fixed sized vector representation, following Kim (2014).",3.2.1 State,[0],[0]
"This involves embedding each of the n words in the sentence to produce a matrix Xi = {xi,1, xi,2, · · · , xi,n}, after which a series of wide convolutional filters is applied, using multiple filters with different gram sizes.",3.2.1 State,[0],[0]
Each filter uses a linear transformation with a rectified linear unit activation function.,3.2.1 State,[0],[0]
"Finally the filter outputs are merged using a max-pooling operation to yield a hidden state hc, which is used to represent the sentence.
",3.2.1 State,[0],[0]
"Representation of marginals The prediction outputs of the training model, pφ(y|xi), are central to all active learning heuristics, and accordingly, we include this in our approach.",3.2.1 State,[0],[0]
"In order to generalise existing techniques, we elect to use the predictive marginals directly, rather than only using statistics thereof, e.g., entropy.",3.2.1 State,[0],[0]
"This generality allows for different and more nuanced concepts to be learned, including patterns of probabilities that span several adjacent positions in the sentence (e.g., the uncertainty about the boundary of a named entity).
",3.2.1 State,[0],[0]
"We use another convolutional neural network to process the predictive marginals, as shown in Figure 2.",3.2.1 State,[0],[0]
"The convolutional layer contains j filters with ReLU activation, based on a window of width 3 and height equal to the number of classes, and with a stride of one token.",3.2.1 State,[0],[0]
"We use a wide convolution, by padding the input matrix to either size with vectors of zeros.",3.2.1 State,[0],[0]
"These j feature maps are then subsampled with mean pooling, such that the network is easily able to capture the average uncertainty in each window.",3.2.1 State,[0],[0]
"The final hidden layer he is used to represent the predictive marginals.
",3.2.1 State,[0],[0]
Confidence of sequential prediction The last component is a score C which indicates the confidence of the model prediction.,3.2.1 State,[0],[0]
"This is defined based on the most probable label sequence under the model, e.g., using Viterbi algorithm with a CRF, and the probability of this sequence is used to represent the confidence, C = n √
maxy pφ(y|xi), where n = |xi| is the length of the sentence.",3.2.1 State,[0],[0]
"We now turn to the action, which denotes whether the human oracle must annotate the current sentence.",3.2.2 Action,[0],[0]
"The agent selects either to annotate xi, in which case ai = 1, or not, with ai = 0, after which the agent proceeds to consider the next instance, xi+1.",3.2.2 Action,[0],[0]
"When action ai = 1 is chosen, an oracle is requested to annotate the sentence, and the newly annotated sentence is added to the training data, and φ updated accordingly.",3.2.2 Action,[0],[0]
"A special ‘terminate’ option applies when no further data remains or the annotation budget is exhausted, which concludes the active learning run (referred to as an ‘episode’ or ‘game’ herein).",3.2.2 Action,[0],[0]
"The training signal for learning the policy takes the form of a scalar ‘reward’, which provides feedback on the quality of the actions made by the agent.",3.2.3 Reward,[0],[0]
"The most obvious reward is to wait for a game to conclude, then measure the held-out performance of the model, which has been trained on the labelled data.",3.2.3 Reward,[0],[0]
"However, this reward is delayed, and is difficult to related to individual actions after a long game.",3.2.3 Reward,[0],[0]
"To compensate for this,
we use reward shaping, whereby small intermediate rewards are assigned which speeds up the learning process (Ng, 2003; Lample and Chaplot, 2016).",3.2.3 Reward,[0],[0]
"At each step, the intermediate reward is defined as the change in held-out performance, i.e., R(si−1, a) = Acc(φi)",3.2.3 Reward,[0],[0]
"− Acc(φi−1), where Acc denotes predictive accuracy (here F1 score), and φi is the trained model after action a has take place, which may include an additional training instance.",3.2.3 Reward,[0],[0]
"Accordingly, when considering the aggregate reward over a game, the intermediate terms cancel, such that the total reward measures the performance improvement over the whole game.",3.2.3 Reward,[0],[0]
"Note that the value of R(s, a) can be positive or negative, indicating a beneficial or detrimental effect on the performance.",3.2.3 Reward,[0],[0]
"There is a fixed budget B for the total number of instances annotated, which corresponds to the terminal state in the MDP.",3.2.4 Budget,[0],[0]
It is a predefined number and chosen according to time and cost constraints.,3.2.4 Budget,[0],[0]
"A game is finished when the data is exhausted or the budget reached, and with the final result being the dataset thus created, upon which the final model is trained.",3.2.4 Budget,[0],[0]
The remaining question is how the above components can be used to learn a good policy.,3.2.5 Reinforcement learning,[0],[0]
"Different policies make different data selections, and thus result in models with different performance.",3.2.5 Reinforcement learning,[0],[0]
"We adopt a reinforcement learning (RL) approach to learn a policy resulting a highly accurate model.
",3.2.5 Reinforcement learning,[0],[0]
"Having represented the problem as a MDP, episode as a sequence of transitions (si, a, r, si+1).",3.2.5 Reinforcement learning,[0],[0]
"One episode of active learning produces a finite sequence of states, actions and rewards.",3.2.5 Reinforcement learning,[0],[0]
"We use a deep Q-learning approach (Mnih et al., 2015), which formalises the policy using function Qπ(s, a)→ Rwhich determines the utility of taking a from state s according to a policy π.",3.2.5 Reinforcement learning,[0],[0]
"In Qlearning, the agent iteratively updates Q(s, a) using rewards obtained from each episode, with updates based on the recursive Bellman equation for the optimal Q:
Qπ(s, a) =",3.2.5 Reinforcement learning,[0],[0]
"E[Ri|si = s, ai = a, π].",3.2.5 Reinforcement learning,[0],[0]
"(1)
Here, Ri = ∑T t=i γ t−irt is the discounted future reward and γ ∈",3.2.5 Reinforcement learning,[0],[0]
"[0, 1] is a factor discounting the value of future rewards and the expectation is
Algorithm 1 Learn an active learning policy Input: data D, budget B Output: π
1: for episode = 1, 2, . . .",3.2.5 Reinforcement learning,[0],[0]
", N do 2:",3.2.5 Reinforcement learning,[0],[0]
Dl ← ∅ and shuffle D 3: φ←,3.2.5 Reinforcement learning,[0],[0]
"Random 4: for i ∈ {0, 1, 2, . . .",3.2.5 Reinforcement learning,[0],[0]
", |D|} do 5: Construct the state si using xi 6:",3.2.5 Reinforcement learning,[0],[0]
"The agent makes a decision according to ai = arg maxQπ(si, a) 7: if ai = 1 then 8: Obtain the annotation yi 9:",3.2.5 Reinforcement learning,[0],[0]
"Dl ← Dl + (xi,yi) 10: Update model φ based on Dl 11: end if 12: Receive a reward ri using held-out set 13: if |Dl| = B then 14:",3.2.5 Reinforcement learning,[0],[0]
"Store (si, ai, ri,Terminate) inM 15:",3.2.5 Reinforcement learning,[0],[0]
"Break 16: end if 17: Construct the new state si+1 18: Store transition (si, ai, ri, si+1) inM 19: Sample random minibatch of transitions
{(sj , aj , rj , sj+1)} from M, and perform gradient descent step on L(θ)
20: Update policy π with θ 21: end for 22: end for 23: return the latest policy π
taken over all transitions involving state s and action a.
Following Deep Q-learning (Mnih et al., 2015), we make use of a deep neural network to compute the expected Q-value, in order to update the parameters.",3.2.5 Reinforcement learning,[0],[0]
"We implement the Q-function using a single hidden layer neural network, taking as input the state representation (hc,he, C) (defined in §3.2.1), and outputting two scalar values corresponding to the values Q(s, a) for a ∈ {0, 1}.",3.2.5 Reinforcement learning,[0],[0]
"This network uses a rectified linear unit (ReLU) activation function in its hidden layer.
",3.2.5 Reinforcement learning,[0],[0]
"The parameters in the DQN are learnt using stochastic gradient descent, based on a regression objective to match the Q-values predicted by the DQN and the expected Q-values from the Bellman equation, ri + γmaxaQ(si+1, a; θ).",3.2.5 Reinforcement learning,[0],[0]
"Following (Mnih et al., 2015), we use an experience replay memory M to store each transition (s, a, r, s′) as it is used in an episode, after which
Algorithm 2 Active learning by policy transfer Input: unlabelled data D, budget B, policy π Output: Dl
1: Dl ← ∅ 2: φ←",3.2.5 Reinforcement learning,[0],[0]
"Random 3: for |Dl| 6= B and D not empty do 4: Randomly sample xi from the data pool D and construct the state si 5: The agent chooses an action ai according to ai = arg maxQπ(si, a) 6: if ai = 1 then 7: Obtain the annotation yi 8:",3.2.5 Reinforcement learning,[0],[0]
"Dl ← Dl + (xi,yi) 9: Update model φ based on Dl
10: end if 11: D ← D\xi 12: Receive a reward ri using held-out set 13:",3.2.5 Reinforcement learning,[0],[0]
Update policy π 14: end for 15: return,3.2.5 Reinforcement learning,[0],[0]
"Dl
we sample a mini-batch of transitions from the memory and then minimize the loss function:
L(θ) = Es,a,r,s′",3.2.5 Reinforcement learning,[0],[0]
"[( yi(r, s′)−Q(s, a; θ) )2] , (2)
where yi(r, s′) =",3.2.5 Reinforcement learning,[0],[0]
r + γmaxa′,3.2.5 Reinforcement learning,[0],[0]
"Q(s′, a′; θi−1) is the target Q-value, based on the current parameters θi−1, and the expectation is over the minibatch.",3.2.5 Reinforcement learning,[0],[0]
"Learning updates are made every training step, based on stochastic gradient descent to minimise Eq. 2 w.r.t.",3.2.5 Reinforcement learning,[0],[0]
"parameters θ.
",3.2.5 Reinforcement learning,[0],[0]
The algorithm for learning is summarised in Algorithm 1.,3.2.5 Reinforcement learning,[0],[0]
"We train the policy by running multiple active learning episodes over the training data, where each episode is a simulated active learning run.",3.2.5 Reinforcement learning,[0],[0]
"For each episode, we shuffle the data, and hide the known labels, which are revealed as requested during the run.",3.2.5 Reinforcement learning,[0],[0]
"A disjoint held-out set is used to compute the reward, i.e., model accuracy, which is fixed over the episodes.",3.2.5 Reinforcement learning,[0],[0]
"Between each episode the model is reset to its initialisation condition, with the main changes being the different (random) data ordering and the evolving policy function.",3.2.5 Reinforcement learning,[0],[0]
We now turn to the question of how the learned policy can be applied to another dataset.,3.3 Cross-lingual policy transfer,[0],[0]
"Given the extensive use of the training dataset, the policy application only makes sense when employed in a
Algorithm 3 Active learning by policy and model transfer, for ‘cold-start’ scenario Input: unlabelled data D, budget B, policy π,
model φ Output: Dl
1: Dl ← ∅ 2: for |Dl| 6= B and D not empty do 3: Randomly sample xi from the data pool D and construct the state si 4: The agent chooses an action ai according to ai = arg maxQπ(si, a) 5: if ai = 1 then 6:",3.3 Cross-lingual policy transfer,[0],[0]
"Dl ← Dl + (xi,−) 7: end if 8: D ← D\xi 9: end for
10:",3.3 Cross-lingual policy transfer,[0],[0]
"Obtain all the annotations for Dl 11: return Dl
different data setting, e.g., where the domain, task or language is different.",3.3 Cross-lingual policy transfer,[0],[0]
"For this paper, we consider a cross-lingual application of the same task (NER), where we train a policy on a source language (e.g., English), and then transfer the learned policy to a different target language.",3.3 Cross-lingual policy transfer,[0],[0]
"Cross-lingual word embeddings provide a common shared representation to facilitate application of the policy to other languages.
",3.3 Cross-lingual policy transfer,[0],[0]
We illustrate the policy transfer algorithm in Algorithm 2.,3.3 Cross-lingual policy transfer,[0],[0]
"This algorithm is broadly similar to Algorithm 1, but has two key differences.",3.3 Cross-lingual policy transfer,[0],[0]
"Firstly, Algorithm 2 makes only one pass over the data, rather than several passes, as befits an application to a low-resource language where oracle labelling is costly.",3.3 Cross-lingual policy transfer,[0],[0]
"Secondly, the algorithm also assumes an initial policy, π, which is fine tuned during the episode based on held-out performance such that the policy can adapt to the test scenario.3",3.3 Cross-lingual policy transfer,[0],[0]
"The above transfer algorithm has some limitations, which may not be realistic for low-resource settings: the requirement for held-out evaluation data and the embedding of the oracle annotator inside the learning loop.",3.4 Cold-start transfer,[0],[0]
"The former implies more supervision than is ideal in a low-resource setting,
3Moreover, the algorithm can be extended to a traditional batch setting by evaluating a batch of data instances and selectinag the best k instances for labelling under the policy.",3.4 Cold-start transfer,[0],[0]
"This could be applied in either the transfer step (Algorithm 2) or initial policy training (Algorithm 1), or both.
while the latter places limitations on the communication with annotator as well as a necessity for real-time processing, both which are unlikely in a field linguistics setting.
",3.4 Cold-start transfer,[0],[0]
"For this data and- communication-impoverished setting, denoted as cold-start, we allow only one chance to request labels for the target data, and, having no held-out data, do not allow policy updates.",3.4 Cold-start transfer,[0],[0]
"The agent needs to select a batch of unlabelled target instances for annotations, but cannot use these resulting annotations or any other feedback to refine the selection.",3.4 Cold-start transfer,[0],[0]
"In this, more difficult cold-start setting, we bootstrap the process with an initial model, such that the agent can make informative decisions in the absence of feedback.
",3.4 Cold-start transfer,[0],[0]
The procedure is outlined in Algorithm 3.,3.4 Cold-start transfer,[0],[0]
"Using the cross-lingual word embeddings, we transfer both a policy and a model into the target language.",3.4 Cold-start transfer,[0],[0]
"The model, φ, is trained on one source language, and the policy is learned on a different source language.",3.4 Cold-start transfer,[0],[0]
"Policy learning uses Alg 1, with the small change that in step 3 the model is initialised using φ.",3.4 Cold-start transfer,[0],[0]
"Consequently the learned policy can exploit the knowledge from cross-lingual initialisation, such that it can figure out which aspects that need to be corrected using target annotated data.",3.4 Cold-start transfer,[0],[0]
"Overall this allows for estimates and confidence values to be produced by the model, thus providing the agent with sufficient information for data selection.",3.4 Cold-start transfer,[0],[0]
"We conduct experiments to validate the proposed active learning method in a cross-lingual setting, whereby an active learning policy trained on a source language is transferred to a target language.",4 Experiments,[0],[0]
"We allow repeated active learning simulations on the source language, where annotated corpora are plentiful, to learn a policy, while for target languages we only permit a single episode, to mimic a language without existing resources.
",4 Experiments,[0],[0]
"We use NER corpora from CoNLL2002/2003 shared tasks,4 which comprise NER annotated text in English (en), German (de), Spanish (es), and Dutch (nl), each annotated using the IOB1 labelling scheme, which we convert to the IO labeling scheme.",4 Experiments,[0],[0]
"We use the existing corpus partions, with train used for policy training, testb used
4 http://www.cnts.ua.ac.be/conll2002/ ner/, http://www.cnts.ua.ac.be/conll2003/ ner/
as held-out for computing rewards, and final results are reported on testa.
",4 Experiments,[0],[0]
"We consider three experimental conditions, as illustrated in Table 1:
bilingual where English is the source (used for policy learning) and we vary the target language;
multilingual where several source languages are the used in joint learning of the policy, and a separate language is used as target; and
cold-start where a pretrained English NER tagger is used to initialise policy learning on a source language, and in cold-start application to a separate target language.
",4 Experiments,[0],[0]
Configuration We now outline the parameter settings for the experimental runs.,4 Experiments,[0],[0]
"For learning an active learning policy, we run N = 10, 000 episodes with budget B = 200 sentences using Alg. 1.",4 Experiments,[0],[0]
"Content representations use three convolutional filters of size 3, 4 and 5, using 128 filters for each size, while for predictive marginals, the convolutional filters are of width 3, using 20 filters.",4 Experiments,[0],[0]
The size of the last hidden layer is 256.,4 Experiments,[0],[0]
The discount factor is set to γ = 0.99.,4 Experiments,[0],[0]
We used the ADAM algorithm with mini-batches of size 32 for training the neural network.,4 Experiments,[0],[0]
"To report performance, we apply the learned policy to the target training set (using Alg. 2 or 3, again with budget 200),5 after which we use the final trained model for which we report F1 score.
",4 Experiments,[0],[0]
"For word embeddings, we use off the shelf CCA trained multilingual embeddings (Ammar et al.,
5Although it is possible the policy may learn not to use the full budget, this does not occur in practise.
",4 Experiments,[0],[0]
"2016),6 using a 40 dimensional embedding and fixing these during training of both the policy and model.",4 Experiments,[0],[0]
"As the model, we use a standard linear chain CRF (Lafferty et al., 2001) for the first two sets of experiments, while for cold-start case we use a basic RNN classifier with the same multilingual embeddings as before, and a 128 dimensional hidden layer.
",4 Experiments,[0],[0]
"The proposed method is referred to as PAL, as shorthand Policy based Active Learning.",4 Experiments,[0],[0]
"Subscripts b,m, c are used to denote the bilingual, multilingual and cold-start experimental configurations.",4 Experiments,[0],[0]
"For comparative baselines, we use the following methods:
Uncertainty sampling we use the total token entropy measure (Settles and Craven, 2008), which takes the instance x maximising∑|x|
t=1H(yt|x, φ), where H is the token entropy.",4 Experiments,[0],[0]
"We use the whole training set as the data pool, and select a single instance for labelling in each active learning step.",4 Experiments,[0],[0]
"This method was shown to achieve the best result among model-independent active learning methods on the CoNLL data.
",4 Experiments,[0],[0]
"Random sampling which randomly selects examples from the unlabelled pool.
",4 Experiments,[0],[0]
"Results Figure 3 shows results the bilingual case, where PALb consistently outperforms the Random and Uncertainty baselines across the three target languages.",4 Experiments,[0],[0]
"Uncertainty sampling is ineffective, particularly towards the start of the run,
6http://128.2.220.95/multilingual
as a consequence of its dependence on a high quality model.",4 Experiments,[0],[0]
"The use of content information allows PALb to make a stronger start, despite the poor initial model.
",4 Experiments,[0],[0]
"Also shown in Figure 3 are results for multilingual policy learning, PALm, which outperform all other approaches including PALb.",4 Experiments,[0],[0]
"This illustrates that the additional training over several languages gives rise to a better policy, than only using one source language.",4 Experiments,[0],[0]
"The superior performance is particularly marked in the early stages of the runs for Spanish and Dutch, which may indicate that the approach was better able to learn to exploit the sentence content information.
",4 Experiments,[0],[0]
We evaluate the cold-start setting in Figure 4.,4 Experiments,[0],[0]
"Recall that in this setting there are no policy or model updates, as no heldout data is used, and all annotations arrive in a batch.",4 Experiments,[0],[0]
"The model, however, is initialised with a NER tagger trained on a different language, which explains why the performance for all methods starts from around 40% rather than 0%.",4 Experiments,[0],[0]
"Even in this challenging evaluation setting, our algorithm PALc outperforms both baseline methods, showing that deep Q learning allows for better exploitation of the pretrained classifier, alongside the sentence content.
",4 Experiments,[0],[0]
"Lastly, we report the results for all approaches in Table 2, based on training on the full 200 labelled sentences as selected under the different methods.",4 Experiments,[0],[0]
"It is clear that the PAL methods all outperform the baselines, and among these the multilingual training of PALm outperforms the bilingual setting in PALb.",4 Experiments,[0],[0]
"Surprisingly, PALc gives the overall best results, despite using a static policy and model during target application, underscoring the importance of model pretraining.",4 Experiments,[0],[0]
"Table 2 also re-
ports the cost reduction versus random sampling, showing that the PAL methods can reduce the annotation burden to as low as 10%.",4 Experiments,[0],[0]
"In this paper, we have proposed a new active learning algorithm capable of learning active learning strategies from data.",5 Conclusion,[0],[0]
"We formalise active learning under a Markov decision framework, whereby active learning corresponds to a sequence of binary annotation decisions applied to a stream of data.",5 Conclusion,[0],[0]
"Based on this, we design an active learning algorithm as a policy based on deep reinforcement learning.",5 Conclusion,[0],[0]
"We show how these learned active learning policies can be transferred between languages, which we empirically show provides consistent and sizeable improvements over baseline methods, including traditional uncertainty sampling.",5 Conclusion,[0],[0]
"This
holds true even in a very difficult cold-start setting, where no evaluation data is available, and there is no ability to react to annotations.",5 Conclusion,[0],[0]
This work was sponsored by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114.,Acknowledgments,[0],[0]
The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.,Acknowledgments,[0],[0]
Trevor Cohn was supported by an Australian Research Council Future Fellowship.,Acknowledgments,[0],[0]
Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate.,abstractText,[0],[0]
"This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets.",abstractText,[0],[0]
"To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic.",abstractText,[0],[0]
"Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages.",abstractText,[0],[0]
"We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.",abstractText,[0],[0]
Learning how to Active Learn: A Deep Reinforcement Learning Approach,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1874–1883 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1874",text,[0],[0]
"For many real-world NLP tasks, labeled data is rare while unlabelled data is abundant.",1 Introduction,[0],[0]
Active learning (AL) seeks to learn an accurate model with minimum amount of annotation cost.,1 Introduction,[0],[0]
It is inspired by the observation that a model can get better performance if it is allowed to choose the data points on which it is trained.,1 Introduction,[0],[0]
"For example, the learner can identify the areas of the space where it does not have enough knowledge, and query those data points which bridge its knowledge gap.
",1 Introduction,[0],[0]
"Traditionally, AL is performed using engineered heuristics in order to estimate the usefulness of unlabeled data points as queries to an annotator.",1 Introduction,[0],[0]
"Recent work (Fang et al., 2017; Bachman et al., 2017; Woodward and Finn, 2017) have focused on learning the AL querying strategy, as engineered heuristics are not flexible to exploit char-
acteristics inherent to a given problem.",1 Introduction,[0],[0]
"The basic idea is to cast AL as a decision process, where the most informative unlabeled data point needs to be selected based on the history of previous queries.",1 Introduction,[0],[0]
"However, previous works train for the AL policy by a reinforcement learning (RL) formulation, where the rewards are provided at the end of sequences of queries.",1 Introduction,[0],[0]
"This makes learning the AL policy difficult, as the policy learner needs to deal with the credit assignment problem.",1 Introduction,[0],[0]
"Intuitively, the learner needs to observe many pairs of query sequences and the resulting end-rewards to be able to associate single queries with their utility scores.
",1 Introduction,[0],[0]
"In this work, we formulate learning AL strategies as an imitation learning problem.",1 Introduction,[0],[0]
"In particular, we consider the popular pool-based AL scenario, where an AL agent is presented with a pool of unlabelled data.",1 Introduction,[0],[0]
"Inspired by the Dataset Aggregation (DAGGER) algorithm (Ross et al., 2011), we develop an effective AL policy learning method by designing an efficient and effective algorithmic expert, which provides the AL agent with good decisions in the encountered states.",1 Introduction,[0],[0]
We then use a deep feedforward network to learn the AL policy to associate states to actions.,1 Introduction,[0],[0]
"Unlike the RL approach, our method can get observations and actions directly from the expert’s trajectory.",1 Introduction,[0],[0]
"Therefore, our trained policy can make better rankings of unlabelled datapoints in the pool, leading to more effective AL strategies.
",1 Introduction,[0],[0]
We evaluate our method on text classification and named entity recognition.,1 Introduction,[0],[0]
"The results show our method performs better than strong AL methods using heuristics and reinforcement learning, in that it boosts the performance of the underlying model with fewer labelling queries.",1 Introduction,[0],[0]
An open source implementation of our model is available at: https://github.com/Grayming/ ALIL.,1 Introduction,[0],[0]
"We consider the popular pool-based AL setting where we are given a small set of initial labeled data and a large pool of unlabelled data, and a budget for getting the annotation of some unlabelled data by querying an oracle, e.g. a human annotator.",2 Pool-based AL as a Decision Process,[0],[0]
"The goal is to intelligently pick those unlabelled data for which if the annotations were available, the performance of the underlying re-trained model would be improved the most.
",2 Pool-based AL as a Decision Process,[0],[0]
The main challenge in AL is how to identify and select the most beneficial unlabelled data points.,2 Pool-based AL as a Decision Process,[0],[0]
"Various heuristics have been proposed to guide the unlabelled data selection (Settles, 2010).",2 Pool-based AL as a Decision Process,[0],[0]
"However, there is no one AL heuristic which performs best for all problems.",2 Pool-based AL as a Decision Process,[0],[0]
"The goal of this paper is to provide an approach to learn an AL strategy which is best suited for the problem at hand, instead of resorting to ad-hoc heuristics.
",2 Pool-based AL as a Decision Process,[0],[0]
"The AL strategy can be learned by attempting to actively learn on tasks sampled from a distribution over the tasks (Bachman et al., 2017).",2 Pool-based AL as a Decision Process,[0],[0]
"The idea is to simulate the AL scenario on instances of the problem created using available labeled data, where the label of some part of the data is kept hidden.",2 Pool-based AL as a Decision Process,[0],[0]
"This allows to have an automatic oracle to reveal the labels of the queried data, resulting in an efficient way to quickly evaluate a hypothesised AL strategy.",2 Pool-based AL as a Decision Process,[0],[0]
"Once the AL strategy is learned on simulations, it is then applied to real AL scenarios.",2 Pool-based AL as a Decision Process,[0],[0]
"The more related are the tasks in the real scenario to those used to train the AL strategy, the more effective the AL strategy would be.
",2 Pool-based AL as a Decision Process,[0],[0]
We are interested to train a model mφ which maps an inputx ∈ X to its label y ∈,2 Pool-based AL as a Decision Process,[0],[0]
"Yx , where Yx is the set of labels for the input x and φ is the parameter vector of the underling model.",2 Pool-based AL as a Decision Process,[0],[0]
"For example, in the named entity recognition (NER) task, the input is a sentence and the output is its label sequence, e.g. in the IBO format.",2 Pool-based AL as a Decision Process,[0],[0]
"Let D = {(x,y)} be a support set of labeled data, which is randomly partitioned into labeledDlab, unlabelledDunl, and evaluation Devl datasets.",2 Pool-based AL as a Decision Process,[0],[0]
Repeated random partitioning creates multiple instances of the AL problem.,2 Pool-based AL as a Decision Process,[0],[0]
"At each time step t of an AL problem, the algorithm interacts with the oracle and queries the label of a datapoint xt ∈ Dunlt .",2 Pool-based AL as a Decision Process,[0],[0]
"As the result of this action, the followings happen:
• The automatic oracle reveals the label yt;
• The labeled and unlabelled datasets are up-
dated to include and exclude the recently queried data point, respectively;
• The underlying model is re-trained based on the enlarged labeled data to update φ; and
• The AL algorithm receives a reward −loss(mφ, Devl), which is the negative loss of the current trained model on the evaluation set, defined as
loss(mφ, D evl) := ∑ (x,y)∈Devl loss(mφ(x), y)
where loss(y ′, y) is the loss incurred due to predicting y ′ instead of the ground truth y .
",2 Pool-based AL as a Decision Process,[0],[0]
"More formally, a pool-based AL problem is a Markov decision process (MDP), denoted by (S,A, Pr(st+1|st, at), R) where S is the state space, A is the set of actions, Pr(st+1|st, at) is the transition function, and R is the reward function.",2 Pool-based AL as a Decision Process,[0],[0]
The state st ∈ S at time t consists of the labeled Dlabt and unlabelled D unl t datasets paired with the parameters of the currently trained model φt.,2 Pool-based AL as a Decision Process,[0],[0]
An action at ∈,2 Pool-based AL as a Decision Process,[0],[0]
"A corresponds to the selection of a query datapoint, and the reward function R(st, at, st+1) := −loss(mφt , Devl).
",2 Pool-based AL as a Decision Process,[0],[0]
We aim to find the optimal AL policy prescribing which datapoint needs to be queried in a given state to get the most benefit.,2 Pool-based AL as a Decision Process,[0],[0]
"The optimal policy is found by maximising the following objective over the parameterised policies:
E(Dlab,Dunl,Devl)∼D [ Eπθ [ B∑ t=1 R(st, at, st+1) ]] (1)
where πθ is the policy network parameterised by θ,D is a distribution over possible AL problem instances, and B is the maximum number of queries made in an AL run, a.k.a. an episode.",2 Pool-based AL as a Decision Process,[0],[0]
"Following (Bachman et al., 2017), we maximise the sum of the rewards after each time step to encourage the anytime behaviour, i.e. the model should perform well after each label query.",2 Pool-based AL as a Decision Process,[0],[0]
"AL Policy
The question remains as how can we train the policy network to maximise the training objective in eqn 1.",3 Deep Imitation Learning to Train the,[0],[0]
"Typical learning approaches resort to deep reinforcement learning (RL) and provide training signal at the end of each episode to learn the optimal policy (Fang et al., 2017; Bachman
et al., 2017)",3 Deep Imitation Learning to Train the,[0],[0]
"e.g., using policy gradient methods.",3 Deep Imitation Learning to Train the,[0],[0]
"These approaches, however, need a large number of training episodes to learn a reasonable policy as they need to deal with the credit assignment problem, i.e. discovery of the utility of individual actions in the sequence based on the achieved reward at the end of the episode.",3 Deep Imitation Learning to Train the,[0.952150467451319],"['We will elaborate on the structure of the Algorithm later, but first provide the main result.']"
"This exacerbates the difficulty of finding a good AL policy.
",3 Deep Imitation Learning to Train the,[0],[0]
We formulate learning for the AL policy as an imitation learning problem.,3 Deep Imitation Learning to Train the,[0],[0]
"At each state, we provide the AL agent with a correct action which is computed by an algorithmic expert.",3 Deep Imitation Learning to Train the,[0],[0]
The AL agent uses the sequence of states observed in an episode paired with the expert’s sequence of actions to update its policy.,3 Deep Imitation Learning to Train the,[0],[0]
"This directly addresses the credit assignment problem, and reduces the complexity of the problem compared to the RL approaches.",3 Deep Imitation Learning to Train the,[0],[0]
"In what follows, we describe the ingredients of our deep imitation learning (IL) approach, which is summarised in Algorithm 1.
Algorithmic Expert.",3 Deep Imitation Learning to Train the,[0],[0]
"At a given AL state st, our algorithmic expert computes an action by evaluating the current pool of unlabeled data.",3 Deep Imitation Learning to Train the,[0],[0]
"More concretely, for each x′ ∈ Dpoolrnd and its correct label y ′, the underlying model mφt is re-trained to get mx ′
φt , where Dpoolrnd ⊂ D unl t is a small subset of the current large pool of unlabeled data.",3 Deep Imitation Learning to Train the,[0],[0]
"The expert action is then computed as:
arg min x′∈Dpoolrnd
loss(mx ′
φt (x), Devl).",3 Deep Imitation Learning to Train the,[0],[0]
"(2)
In other words, our algorithmic expert tries a subset of actions to roll-out one step from the current state, in order to efficiently compute a reasonable action.",3 Deep Imitation Learning to Train the,[0],[0]
"Searching for the optimal action would be O(|Dunl|B), which is computationally challenging due to (i) the large action set, and (ii) the exponential dependence on the length of the roll out.",3 Deep Imitation Learning to Train the,[0],[0]
"We will see in the experiments that our method efficiently learns effective AL policies.
",3 Deep Imitation Learning to Train the,[0],[0]
Policy Network.,3 Deep Imitation Learning to Train the,[0],[0]
Our policy network is a feedforward network with two fully-connected hidden layers.,3 Deep Imitation Learning to Train the,[0],[0]
"It receives the current AL state, and provides a preference score for a given unlabeled data point, allowing to select the most beneficial one corresponding to the highest score.",3 Deep Imitation Learning to Train the,[0],[0]
"The input to our policy network consists of three parts: (i) a fixed dimensional representation of the content and the predicted label of the unlabeled data point under consideration, (ii) a fixed-dimensional rep-
resentation of the content and the labels of the labeled dataset, and (iii) a fixed-dimensional representation of the content of the unlabeled dataset.
",3 Deep Imitation Learning to Train the,[0],[0]
Imitation Learning Algorithm.,3 Deep Imitation Learning to Train the,[0],[0]
A typical approach to imitation learning (IL) is to train the policy network so that it mimics the expert’s behaviour given training data of the encountered states (input) and actions (output) performed by the expert.,3 Deep Imitation Learning to Train the,[0],[0]
The policy network’s prediction affects future inputs during the execution of the policy.,3 Deep Imitation Learning to Train the,[0],[0]
"This violates the crucial independent and identically distributed (iid) assumption, inherent to most statistical supervised learning approaches for learning a mapping from states to actions.
",3 Deep Imitation Learning to Train the,[0],[0]
"We make use of Dataset Aggregation (DAGGER) (Ross et al., 2011), an iterative algorithm for IL which addresses the non-iid nature of the encountered states during the AL process (see Algorithm 1).",3 Deep Imitation Learning to Train the,[0],[0]
"In round τ of DAGGER, the learned policy network π̂τ is applied to the AL problem to collect a sequence of states which are paired with the expert actions.",3 Deep Imitation Learning to Train the,[0],[0]
"The collected pair of states and actions are aggregated to the dataset of such pairs M , collected from the previous iterations of the algorithm.",3 Deep Imitation Learning to Train the,[0],[0]
"The policy network is then re-trained on the aggregated set, resulting in π̂τ+1 for the next iteration of the algorithm.",3 Deep Imitation Learning to Train the,[0],[0]
"The intuition is to build up the set of states that the algorithm is likely to encounter during its execution, in order to increase the generalization of the policy network.",3 Deep Imitation Learning to Train the,[0],[0]
"To better leverage the training signal from the algorithmic expert, we allow the algorithm to collect state-action pairs according to a modified policy which is a mixture of π̂τ and the expert policy π̃∗τ , i.e.
πτ = βτ π̃ ∗ +",3 Deep Imitation Learning to Train the,[0],[0]
"(1− βτ )π̂τ
where βτ ∈",3 Deep Imitation Learning to Train the,[0],[0]
"[0, 1] is a mixing coefficient.",3 Deep Imitation Learning to Train the,[0],[0]
"This amounts to tossing a coin with parameter βτ in
each iteration of the algorithm to decide one of these two policies for data collection.
",3 Deep Imitation Learning to Train the,[0],[0]
Re-training the Policy Network.,3 Deep Imitation Learning to Train the,[0],[0]
"To train our policy network, we turn the preference scores to probabilities, and optimise the parameters such that the probability of the action prescribed by the expert is maximized.",3 Deep Imitation Learning to Train the,[0],[0]
"More specifically, let M := {(si, ai)}Ii=1 be the collected states paired with their expert’s prescribed actions.",3 Deep Imitation Learning to Train the,[0],[0]
"LetDpooli be the set of unlabelled datapoints in the pool within the state, and ai denote the datapoint selected by the expert in the set.",3 Deep Imitation Learning to Train the,[0],[0]
"Our training objective is∑I
i=1",3 Deep Imitation Learning to Train the,[0],[0]
"logPr(ai|D pool i ) where
Pr(ai|Dpooli ) := exp π̂(ai;si)∑
x∈Dpooli exp π̂(x;si)
.
",3 Deep Imitation Learning to Train the,[0],[0]
The above can be interpreted as the probability of ai being the best action among all possible actions in the state.,3 Deep Imitation Learning to Train the,[0],[0]
"Following (Mnih et al., 2015), we randomly sample multiple1 mini-batches from the replay memoryM, in addition to the current round’s stat-action pair, in order to retrain the policy network.",3 Deep Imitation Learning to Train the,[0],[0]
"For each mini-batch, we make one SGD step to update the policy, where the gradients of the network parameters are calculated using the backpropagation algorithm.
",3 Deep Imitation Learning to Train the,[0],[0]
Transferring the Policy.,3 Deep Imitation Learning to Train the,[0],[0]
We now apply the policy learned on the source task to AL in the target task.,3 Deep Imitation Learning to Train the,[0],[0]
We expect the learned policy to be effective for target tasks which are related to the source task in terms of the data distribution and characteristics.,3 Deep Imitation Learning to Train the,[0],[0]
Algorithm 2 illustrates the policy transfer.,3 Deep Imitation Learning to Train the,[0],[0]
"The pool-based AL scenario in Algorithm 2 is cold-start; however, extending to incorporate initially available labeled data is straightforward.",3 Deep Imitation Learning to Train the,[0],[0]
We conduct experiments on text classification and named entity recognition (NER).,4 Experiments,[0],[0]
"The AL scenarios include cross-domain sentiment classification, cross-lingual authorship profiling, and crosslingual named entity recognition (NER), whereby an AL policy trained on a source domain/language is transferred to the target domain/language.
",4 Experiments,[0],[0]
"We compare our proposed AL method using imitation learning (ALIL) with the followings:
• Random sampling:",4 Experiments,[0],[0]
"The query datapoint is chosen randomly.
",4 Experiments,[0],[0]
"1In our experiments, we use 10 mini-bathes, each of which of size 100.
",4 Experiments,[0],[0]
"Algorithm 1 Learn active learning policy via imitation learning Input: large labeled data D, max episodes T , budget B,
sample size K, the coin parameter β Output: The learned policy 1: M ← ∅ .",4 Experiments,[0],[0]
"the aggregated dataset 2: initialise π̂1 with a random policy 3: for τ=1, . . .",4 Experiments,[0],[0]
", T do 4: Dlab, Dunl, Devl ← dataPartition(D) 5: φ1 ← trainModel(Dlab) 6: c← coinToss(β) 7: for t ∈ 1, . . .",4 Experiments,[0],[0]
",B do 8:",4 Experiments,[0],[0]
"Dpoolrnd ← sampleUniform(D unl,K)
9: st ← (Dlab, Dpoolrnd ,φt) 10: at ← argminx′∈Dpool
rnd loss(mx
′ φt , Devl)
11: if c is head then .",4 Experiments,[0],[0]
the expert 12: xt ← at 13: else .,4 Experiments,[0],[0]
"the policy 14: xt ← argmaxx′∈Dpool
rnd π̂τ (x
′;st)
15: end if 16: Dlab ← Dlab + {(xt, yt)} 17:",4 Experiments,[0],[0]
"Dunl ← Dunl − {xt} 18: M ←M + {(st, at)} 19: φt+1 ← retrainModel(φt, Dlab) 20: end for 21: π̂τ+1 ← retrainPolicy(π̂τ ,M) 22: end for 23: return π̂T+1
Algorithm 2 Active learning by policy transfer Input: unlabeled pool Dunl, budget B, policy π̂",4 Experiments,[0],[0]
"Output: labeled dataset and trained model 1: Dlab ← ∅ 2: initialise φ randomly 3: for t ∈ 1, . . .",4 Experiments,[0],[0]
",B do 4: st ← (Dlab, Dunl,φ) 5: xt ← argmaxx′∈Dunl π̂(x′;st) 6: yt ← askAnnotation(xt) 7: Dlab ← Dlab + {(xt, yt)} 8:",4 Experiments,[0],[0]
Dunl ← Dunl − {xt} 9: φ←,4 Experiments,[0],[0]
"retrainModel(φ,Dlab) 10: end for 11: return Dlab and φ
• Diversity sampling: The query datapoint is argminx ∑ x′∈Dlab Jaccard(x,x
′), where the Jaccard coefficient between the unigram features of the two given texts is used as the similarity measure.
",4 Experiments,[0],[0]
"• Uncertainty-based sampling: For text classification, we use the datapoint with the highest predictive entropy, argmaxx − ∑ y p(y|x,D
lab) log p(y|x,Dlab) where p(y|x,Dlab) comes from the underlying model.",4 Experiments,[0],[0]
"We further use a state-of-the-art extension of this method, called uncertainty with rationals (Sharma et al., 2015), which not only considers uncertainty but also looks whether
the unlabelled document contains sentiment words or phrases that were returned as rationales for any of the existing labeled documents.",4 Experiments,[0],[0]
"For NER, we use the Total Token Entropy (TTE) as the uncertainty sampling method, argmaxx − ∑|x| i=1",4 Experiments,[0],[0]
"∑ yi p(yi|x,Dlab) log p(yi|x,Dlab) which has been shown to be the best heuristic for this task among 17 different heuristics (Settles and Craven, 2008).
",4 Experiments,[0],[0]
"• PAL: A reinforcement learning based approach (Fang et al., 2017), which makes use a deep Q-network to make the selection decision for stream-based active learning.",4 Experiments,[0],[0]
Datasets and Setup.,4.1 Text Classification,[0],[0]
"The first task is sentiment classification, in which product reviews express either positive or negative sentiment.",4.1 Text Classification,[0],[0]
"The data comes from the Amazon product reviews (McAuley and Yang, 2016); see Table 1 for data statistics.
",4.1 Text Classification,[0],[0]
"The second task is Authorship Profiling, in which we aim to predict the gender of the text author.",4.1 Text Classification,[0],[0]
"The data comes from the gender profiling task in PAN 2017 (Rangel et al., 2017), which consists of a large Twitter corpus in multiple languages: English (en), Spanish (es) and Portuguese (pt).",4.1 Text Classification,[0],[0]
"For each language, all tweets collected from a user constitute one document; Table 1 shows data statistics.",4.1 Text Classification,[0],[0]
"The multilingual embeddings for this task come from off-the-shelf CCA-trained embeddings (Ammar et al., 2016) for twelve languages, including English, Spanish and Portuguese.",4.1 Text Classification,[0],[0]
"We fix these word embeddings during training of both the policy and the underlying classification model.
",4.1 Text Classification,[0],[0]
"For training, 10% of the source data is used as the evaluation set for computing the best action in imitation learning.",4.1 Text Classification,[0],[0]
"We run T = 100 episodes with the budget B = 100 documents in each episode, set the sample size K = 5, and fix the mixing coefficient βτ = 0.5.",4.1 Text Classification,[0],[0]
"For testing, we take 90% of the target data as the unlabeled pool, and the
remaining 10% as the test set.",4.1 Text Classification,[0],[0]
We show the test accuracy w.r.t.,4.1 Text Classification,[0],[0]
"the number of labelled documents selected in the AL process.
",4.1 Text Classification,[0],[0]
"As the underlying model mφ , we use a fast and efficient text classifier based on convolutional neural networks.",4.1 Text Classification,[0],[0]
"More specifically, we apply 50 convolutional filters with ReLU activation on the embedding of all words in a document x, where the width of the filters is 3.",4.1 Text Classification,[0],[0]
"The filter outputs are averaged to produce a 50-dimensional document representation h(x), which is then fed into a softmax to predict the class.
",4.1 Text Classification,[0],[0]
Representing state-action.,4.1 Text Classification,[0],[0]
"The input to the policy network, i.e. the feature vector representing a state-action pair, includes: the candidate document represented by the convolutional net h(x), the distribution over the document’s class labels mφ(x), the sum of all document vector representations in the labeled set ∑ x′∈Dlab h(x
′), the sum of all document vectors in the random pool of unlabelled data
∑ x′∈Dpoolrnd
h(x′), and the empirical distribution of class labels in the labeled dataset.
Results.",4.1 Text Classification,[0],[0]
"Fig 2 shows the results on product sentiment prediction and authorship profiling, in cross-domain and cross-lingual AL scenarios2.",4.1 Text Classification,[0],[0]
"Our ALIL method consistently outperforms both heuristic-based and RL-based (PAL) (Fang et al., 2017) approaches across all tasks.",4.1 Text Classification,[0],[0]
"ALIL tends to convergence faster than other methods, which indicates its policy can quickly select the most informative datapoints.",4.1 Text Classification,[0],[0]
"Interestingly, the uncertainty and diversity sampling heuristics perform worse than random sampling on sentiment classification.",4.1 Text Classification,[0],[0]
We speculate this may be due to these two heuristics not being able to capture the polarity information during the data selection process.,4.1 Text Classification,[0],[0]
"PAL performs on-par with uncertainty with rationals on musical device, both of which outperform the traditional diversity and uncertainty sampling heuristics.",4.1 Text Classification,[0],[0]
"Interestingly, PAL is outperformed by random sampling on movie reviews, and by the traditional uncertainty sampling heuristic on authorship profiling tasks.",4.1 Text Classification,[0],[0]
"We attribute this to ineffectiveness of the RL-based approach for learning a reasonable AL query strategy.
",4.1 Text Classification,[0],[0]
We further investigate combining the transfer of the policy network with the transfer of the underlying classifier.,4.1 Text Classification,[0],[0]
"That is, we first train a classi-
2Uncertainty with rationale cannot be done for authorship profiling as the rationales come from a sentiment dictionary.
fier on all of the annotated data from the source domain/language.",4.1 Text Classification,[0],[0]
"Then, this classifier is ported to the target domain/language; for cross-language transfer, we make use of multilingual word embeddings.",4.1 Text Classification,[0],[0]
"We start the AL process starting from the transferred classifier, referred to as the warmstart AL.",4.1 Text Classification,[0],[0]
We compare the performance of the directly transferred classifier with those obtained after the AL process in the warm-start and cold-start scenarios.,4.1 Text Classification,[0],[0]
The results are shown in Table 2.,4.1 Text Classification,[0],[0]
"We have run the cold-start and warm-start AL for 25 times, and reported the average accuracy in Table 2.",4.1 Text Classification,[0],[0]
"As seen from the results, both the cold and warm start AL settings outperform the direct transfer significantly, and the warm start consistently gets higher accuracy than the cold start.",4.1 Text Classification,[0],[0]
"The difference between the results are statistically significant, with a p-value of .001, according to McNemar test3 (Dietterich, 1998).",4.1 Text Classification,[0],[0]
"Data and setup We use NER corpora from the CONLL2002/2003 shared tasks, which include annotated text in English (en), German (de), Spanish (es), and Dutch (nl).",4.2 Named Entity Recognition,[0],[0]
"The original annotation is based on IOB1, which we convert to the IO
3As the contingency table needed for the McNemar test, we have used the average counts across the 25 runs.
labelling scheme.",4.2 Named Entity Recognition,[0],[0]
"Following Fang et al. (2017), we consider two experimental conditions: (i) the bilingual scenario where English is the source (used for policy training) and other languages are the target, and (ii) the multilingual scenario where one of the languages (except English) is the target and the remaining ones are the source used in joint training of the AL policy.",4.2 Named Entity Recognition,[0],[0]
The underlying model mφ is a conditional random field (CRF) treating NER as a sequence labelling task.,4.2 Named Entity Recognition,[0],[0]
"The prediction is made using the Viterbi algorithm.
",4.2 Named Entity Recognition,[0],[0]
"In the existing corpus partitions from CoNLL, each language has three subsets: train, testa and testb.",4.2 Named Entity Recognition,[0],[0]
"During policy training with the source language(s), we combine these three subsets, shuffle, and re-split them into simulated training, unlabelled pool, and evaluation sets in every episode.",4.2 Named Entity Recognition,[0],[0]
"We run N = 100 episodes with the budget B = 200, and set the sample size k = 5.",4.2 Named Entity Recognition,[0],[0]
"When we transfer the policy to the target language, we do one episode and select B datapoints from train (treated as the pool of unlabeled data) and report F1 scores on testa.
",4.2 Named Entity Recognition,[0],[0]
Representing state-action.,4.2 Named Entity Recognition,[0],[0]
"The input to the policy network includes the representation of the candidate sentence using the sum of its words’ embeddings h(x), the representation of the labelling marginals using the label-level convolutional network cnnlab(Emφ(y|x)[y]) (Fang et al., 2017), the representation of sentences in the labeled data ∑ (x′,y ′)∈Dlab h(x
′), the representation of sentences in the random pool of unlabelled data
∑ x′∈Dpoolrnd
h(x′), the representation of ground-truth labels in the labeled data∑
(x′,y ′)∈Dlab cnnlab(y ′) using the empirical distributions, and the confidence of the sequential pre-
diction |x|",4.2 Named Entity Recognition,[0],[0]
"√
maxy mφ(y|x), where |x| denotes the length of the sentence x.",4.2 Named Entity Recognition,[0],[0]
"For the word embeddings, we use off-the-shelf CCA trained multilingual embeddings (Ammar et al., 2016) with 40 dimensions; we fix these during policy training.
Results.",4.2 Named Entity Recognition,[0],[0]
Fig. 3 shows the results for three target languages.,4.2 Named Entity Recognition,[0],[0]
"In addition to the strong heuristicbased methods, we compare our imitation learning approach (ALIL) with the reinforcement learning approach (PAL) (Fang et al., 2017), in both bilingual (bi) and multilingual (mul) transfer settings.",4.2 Named Entity Recognition,[0],[0]
"Across all three languages, ALIL.bi and ALIL.mul outperform the heuristic methods, including Uncertainty Sampling based on TTE.",4.2 Named Entity Recognition,[0],[0]
"This is expected as the uncertainty sampling largely relies on a high quality underlying model, and diversity sampling ignores the labelling information.",4.2 Named Entity Recognition,[0],[0]
"In the bilingual case, ALIL.bi outperforms PAL.bi on Spanish (es) and Dutch (nl), and performs similarly on German (de).",4.2 Named Entity Recognition,[0],[0]
"In the multilingual case, ALIL.mul achieves the best performance on Spanish, and performs competitively with PAL.mul on German and Dutch.",4.2 Named Entity Recognition,[0],[0]
Insight on the selected data.,4.3 Analysis,[0],[0]
We compare the data selected by ALIL to other methods.,4.3 Analysis,[0],[0]
"This will confirm that ALIL learns policies which are suitable for the problem at hand, without resorting to a fixed engineered heuristics.",4.3 Analysis,[0],[0]
"For this analysis, we report the mean reciprocal rank (MRR) of the data points selected by the ALIL policy under rankings of the unlabelled pool generated by the uncertainty and diversity sampling.",4.3 Analysis,[0],[0]
"Furthermore, we measure the fraction of times the decisions made by the ALIL policy agrees with those which would
have been made by the heuristic methods, which is measured by the accuracy (acc).",4.3 Analysis,[0],[0]
Table 3 report these measures.,4.3 Analysis,[0],[0]
"As we can see, for sentiment classification since uncertainty and diversity sampling perform badly, ALIL has a big disagreement with them on the selected data points.",4.3 Analysis,[0],[0]
"While for gender classification on Portuguese and NER on Spanish, ALIL shows much more agreement with other three heuristics.
",4.3 Analysis,[0],[0]
"Lastly, we compare chosen queries by ALIL to those by PAL, to investigate the extent of the agreement between these two methods.",4.3 Analysis,[0],[0]
This is simply measure by the fraction of identical query data points among the total number of queries (i.e. accuracy).,4.3 Analysis,[0],[0]
"Since PAL is stream-based and sensitive to the order in which it receives the data points, we report the average accuracy taken over multiple runs with random input streams.",4.3 Analysis,[0],[0]
The expected accuracy numbers are reported in Table 3.,4.3 Analysis,[0],[0]
"As seen, ALIL has higher overlap with PAL than the heuristic-based methods, in terms of the selected queries.
",4.3 Analysis,[0],[0]
Sensitivity toK.,4.3 Analysis,[0],[0]
"As seen in Algorithm 1, we resort to an approximate algorithmic expert, which selects the best action in a random subset of the
pool of unlabelled data with size K, in order to make the policy training efficient.",4.3 Analysis,[0],[0]
"Note that, in policy training, settingK to one and the size of the unlabelled data pool correspond to stream-based and pool-based AL scenarios, respectively.",4.3 Analysis,[0],[0]
"By changingK to values between these two extremes, we can analyse the effect of the quality of the algorithmic expert on the trained policy; Figure 4 shows the results.",4.3 Analysis,[0],[0]
"A larger candidate set may correspond to a better learned policy, needed to be traded off with the training time growing linearly with K. Interestingly, even small candidate sets lead to strong AL policies as increasing K beyond 10 does not change the performance significantly.
",4.3 Analysis,[0],[0]
Dynamically changing β.,4.3 Analysis,[0],[0]
"In our algorithm, β plays an important role as it trades off exploration versus exploitation.",4.3 Analysis,[0],[0]
"In the above experiments, we fix it to 0.5; however, we can change its value throughout trajectory collection as a function of τ (see Algorithm 1).",4.3 Analysis,[0],[0]
"We investigate schedules which tend to put more emphasis on exploration and exploitation towards the beginning and end of data collection, respectively.",4.3 Analysis,[0],[0]
"We investigate the following schedules: (i) linear βτ = max(0.5, 1− 0.01τ), (ii) exponential βτ = 0.9τ , and (iii) and inverse sigmoid βτ = 55+exp(τ/5) , as a function of iterations.",4.3 Analysis,[0],[0]
Fig. 5 shows the comparisons of these schedules.,4.3 Analysis,[0],[0]
The learned policy seems to perform competitively with either a fixed or an exponential schedule.,4.3 Analysis,[0],[0]
"We have also investigated tossing the coin in each step within the trajectory roll out, but found that it is more effective to have it before the full trajectory roll out (as currently done in Algorithm 1).",4.3 Analysis,[0],[0]
"Traditional active learning algorithms rely on various heuristics (Settles, 2010), such as uncertainty sampling (Settles and Craven, 2008; Houlsby et al., 2011), query-by-committee (GiladBachrach et al., 2006), and diversity sampling (Brinker, 2003; Joshi et al., 2009; Yang et al., 2015).",5 Related Work,[0],[0]
"Apart from these, different heuristics can be combined, thus creating integrated strategy which consider one or more heuristics at the same time.",5 Related Work,[0],[0]
"Combined with transfer learning, pre-existing labeled data from related tasks can help improve the performance of an active learner (Xiao and Guo, 2013; Kale and Liu, 2013; Huang and Chen, 2016; Konyushkova et al., 2017).",5 Related Work,[0],[0]
"More recently, deep reinforcement learning is used as the framework for learning active learning algorithms, where the active learning cycle is considered as a decision process.",5 Related Work,[0],[0]
"(Woodward and Finn, 2017) extended one shot learning to active learning and combined reinforcement learning with a deep recurrent model to make labeling decisions.",5 Related Work,[0],[0]
"(Bachman et al., 2017) introduced a policy gradient based method which jointly learns data representation, selection heuristic as well as the model prediction function.",5 Related Work,[0],[0]
"(Fang et al., 2017) designed an active learning algorithm based on a deep Qnetwork, in which the action corresponds to binary annotation decisions applied to a stream of data.",5 Related Work,[0],[0]
"The learned policy can then be transferred between languages or domains.
",5 Related Work,[0],[0]
Imitation learning (IL) refers to an agent’s acquisition of skills or behaviours by observing an expert’s trajectory in a given task.,5 Related Work,[0],[0]
It helps reduce sequential prediction tasks into supervised learning by employing a (near) optimal oracle at training time.,5 Related Work,[0],[0]
"Several IL algorithms has been proposed in sequential prediction tasks, including SEARA (Daumé et al., 2009), AggreVaTe (Ross and Bagnell, 2014), DaD (Venkatraman et al., 2015), LOLS(Chang et al., 2015), DeeplyAggreVaTe (Sun et al., 2017).",5 Related Work,[0],[0]
"Our work is closely related to Dagger (Ross et al., 2011), which can guarantee to find a good policy by addressing the dependency nature of encountered states in a trajectory.",5 Related Work,[0],[0]
"In this paper, we have proposed a new method for learning active learning algorithms using deep imitation learning.",6 Conclusion,[0],[0]
"We formalize pool-based active
learning as a Markov decision process, in which active learning corresponds to the selection decision of the most informative data points from the pool.",6 Conclusion,[0],[0]
Our efficient algorithmic expert provides state-action pairs from which effective active learning policies can be learned.,6 Conclusion,[0],[0]
"We show that the algorithmic expert allows direct policy learning, while at the same time, the learned policies transfer successfully between domains and languages, demonstrating improvement over previous heuristic and reinforcement learning approaches.",6 Conclusion,[0],[0]
We would like to thank the feedback from anonymous reviewers.,Acknowledgments,[0],[0]
G. H. is grateful to Trevor Cohn for interesting discussions.,Acknowledgments,[0],[0]
This work was supported by computational resources from the Multimodal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) at Monash University.,Acknowledgments,[0],[0]
Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary.,abstractText,[0],[0]
We introduce a method that learns an AL policy using imitation learning (IL).,abstractText,[0],[0]
"Our IL-based approach makes use of an efficient and effective algorithmic expert, which provides the policy learner with good actions in the encountered AL situations.",abstractText,[0],[0]
"The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints.",abstractText,[0],[0]
We evaluate our method on two different tasks: text classification and named entity recognition.,abstractText,[0],[0]
Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning.,abstractText,[0],[0]
Learning How to Actively Learn: A Deep Imitation Learning Approach,title,[0],[0]
"As neural networks become increasingly popular, their black box reputation is a barrier to adoption when interpretability is paramount.",1. Introduction,[0],[0]
"Here, we present DeepLIFT (Deep Learning Important FeaTures), a novel algorithm to assign importance score to the inputs for a given output.",1. Introduction,[0],[0]
Our approach is unique in two regards.,1. Introduction,[0],[0]
"First, it frames the question of importance in terms of differences from a ‘reference’ state, where the ‘reference’ is chosen according to the problem at hand.",1. Introduction,[0],[0]
"In contrast to most gradient-based methods, using a difference-from-reference allows DeepLIFT to propagate an importance signal even in situations where the gradient is zero and avoids artifacts caused by discontinuities in the gradient.",1. Introduction,[0],[0]
"Second, by op-
1Stanford University, Stanford, California, USA.",1. Introduction,[0],[0]
"Correspondence to: A Kundaje <akundaje@stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"tionally giving separate consideration to the effects of positive and negative contributions at nonlinearities, DeepLIFT can reveal dependencies missed by other approaches.",1. Introduction,[0],[0]
"As DeepLIFT scores are computed using a backpropagationlike algorithm, they can be obtained efficiently in a single backward pass after a prediction has been made.",1. Introduction,[0],[0]
This section provides a review of existing approaches to assign importance scores for a given task and input example.,2. Previous Work,[0],[0]
These approaches make perturbations to individual inputs or neurons and observe the impact on later neurons in the network.,2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"Zeiler & Fergus (Zeiler & Fergus, 2013) occluded different segments of an input image and visualized the change in the activations of later layers.",2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"“In-silico mutagenesis” (Zhou & Troyanskaya, 2015) introduced virtual mutations at individual positions in a genomic sequence and quantified the their impact on the output.",2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"Zintgraf et al. (Zintgraf et al., 2017) proposed a clever strategy for analyzing the difference in a prediction after marginalizing over each input patch.",2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"However, such methods can be computationally inefficient as each perturbation requires a separate forward propagation through the network.",2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
They may also underestimate the importance of features that have saturated their contribution to the output (Fig. 1).,2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"Unlike perturbation methods, backpropagation approaches propagate an importance signal from an output neuron backwards through the layers to the input in one pass, making them efficient.",2.2. Backpropagation-Based Approaches,[0],[0]
DeepLIFT is one such approach.,2.2. Backpropagation-Based Approaches,[0],[0]
"Simonyan et al. (Simonyan et al., 2013) proposed using the gradient of the output w.r.t.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
pixels of an input image to compute a “saliency map” of the image in the context of image classification tasks.,"2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"The authors showed that this was similar to deconvolutional networks (Zeiler & Fergus,
2013) except for the handling of the nonlinearity at rectified linear units (ReLUs).","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"When backpropagating importance using gradients, the gradient coming into a ReLU during the backward pass is zero’d out if the input to the ReLU during the forward pass is negative.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"By contrast, when backpropagating an importance signal in deconvolutional networks, the importance signal coming into a ReLU during the backward pass is zero’d out if and only if it is negative, with no regard to sign of the input to the ReLU during the forward pass.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Springenberg et al., (Springenberg et al., 2014) combined these two approaches into Guided Backpropagation, which zero’s out the importance signal at a ReLU if either the input to the ReLU during the forward pass is negative or the importance signal during the backward pass is negative.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Guided Backpropagation can be thought of as equivalent to computing gradients, with the caveat that any gradients that become negative during the backward pass are discarded at ReLUs.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Due to the zero-ing out of negative gradients, both guided backpropagation and deconvolutional networks can fail to highlight inputs that contribute negatively to the output.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Additionally, none of the three approaches would address the saturation problem illustrated in Fig. 1, as the gradient of y w.r.t.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"h is negative (causing Guided Backprop and deconvolutional networks to assign zero importance), and the gradient of h w.r.t both i1 and i2 is zero when i1 + i2 > 1 (causing both gradients and Guided Backprop to be zero).","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
Discontinuities in the gradients can also cause undesirable artifacts (Fig. 2).,"2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Bach et al. (Bach et al., 2015) proposed an approach for propagating importance scores called Layerwise Relevance Propagation (LRP).",2.2.2. LAYERWISE RELEVANCE PROPAGATION AND GRADIENT × INPUT,[0],[0]
"Shrikumar et al. and Kindermans et al. (Shrikumar et al., 2016; Kindermans et al., 2016) showed that absent modifications to deal with numerical stability, the original LRP rules were equivalent within a scaling fac-
tor to an elementwise product between the saliency maps of Simonyan et al. and the input (in other words, gradient × input).",2.2.2. LAYERWISE RELEVANCE PROPAGATION AND GRADIENT × INPUT,[0],[0]
"In our experiments, we compare DeepLIFT to gradient × input as the latter is easily implemented on a GPU, whereas LRP does not currently have GPU implementations available to our knowledge.
",2.2.2. LAYERWISE RELEVANCE PROPAGATION AND GRADIENT × INPUT,[0],[0]
"While gradient × input is often preferable to gradients alone as it leverages the sign and strength of the input, it still does not address the saturation problem in Fig. 1 or the thresholding artifact in Fig. 2.",2.2.2. LAYERWISE RELEVANCE PROPAGATION AND GRADIENT × INPUT,[0],[0]
"Instead of computing the gradients at only the current value of the input, one can integrate the gradients as the inputs are scaled up from some starting value (eg: all zeros) to their current value (Sundararajan et al., 2016).",2.2.3. INTEGRATED GRADIENTS,[0],[0]
"This addressess the saturation and thresholding problems of Fig. 1 and Fig. 2, but numerically obtaining high-quality integrals adds computational overhead.",2.2.3. INTEGRATED GRADIENTS,[0],[0]
"Further, this approach can still give misleading results (see Section 3.4.3).",2.2.3. INTEGRATED GRADIENTS,[0],[0]
"Grad-CAM (Selvaraju et al., 2016) computes a coarsegrained feature-importance map by associating the feature maps in the final convolutional layer with particular classes based on the gradients of each class w.r.t.",2.3. Grad-CAM and Guided CAM,[0],[0]
"each feature map, and then using the weighted activations of the feature maps as an indication of which inputs are most important.",2.3. Grad-CAM and Guided CAM,[0],[0]
"To obtain more fine-grained feature importance, the authors proposed performing an elementwise product between the scores obtained from Grad-CAM and the scores obtained from Guided Backpropagation, termed Guided Grad-CAM.",2.3. Grad-CAM and Guided CAM,[0],[0]
"However, this strategy inherits the limitations of Guided Backpropagation caused by zero-ing out negative gradients during backpropagation.",2.3. Grad-CAM and Guided CAM,[0],[0]
It is also specific to convolutional neural networks.,2.3. Grad-CAM and Guided CAM,[0],[0]
DeepLIFT explains the difference in output from some ‘reference’ output in terms of the difference of the input from some ‘reference’ input.,3.1. The DeepLIFT Philosophy,[0],[0]
The ‘reference’ input represents some default or ‘neutral’ input that is chosen according to what is appropriate for the problem at hand (see Section 3.3 for more details).,3.1. The DeepLIFT Philosophy,[0],[0]
"Formally, let t represent some target output neuron of interest and let x1, x2, ..., xn represent some neurons in some intermediate layer or set of layers that are necessary and sufficient to compute t. Let t0 represent the reference activation of t. We define the quantity ∆t to be the difference-from-reference, that is ∆t = t− t0.",3.1. The DeepLIFT Philosophy,[0],[0]
"DeepLIFT assigns contribution scores C∆xi∆t to ∆xi s.t.:
n∑ i=1",3.1. The DeepLIFT Philosophy,[0],[0]
"C∆xi∆t = ∆t (1)
",3.1. The DeepLIFT Philosophy,[0],[0]
We call Eq. 1 the summation-to-delta property.,3.1. The DeepLIFT Philosophy,[0],[0]
C∆xi∆t can be thought of as the amount of difference-fromreference in t that is attributed to or ‘blamed’ on the difference-from-reference of xi.,3.1. The DeepLIFT Philosophy,[0],[0]
"Note that when a neuron’s transfer function is well-behaved, the output is locally linear in its inputs, providing additional motivation for Eq. 1.
C∆xi∆t can be non-zero even when ∂t ∂xi is zero.",3.1. The DeepLIFT Philosophy,[0],[0]
"This allows DeepLIFT to address a fundamental limitation of gradients because, as illustrated in Fig. 1, a neuron can be signaling meaningful information even in the regime where its gradient is zero.",3.1. The DeepLIFT Philosophy,[0],[0]
"Another drawback of gradients addressed by DeepLIFT is illustrated in Fig. 2, where the discontinuous nature of gradients causes sudden jumps in the importance score over infinitesimal changes in the input.",3.1. The DeepLIFT Philosophy,[0],[0]
"By contrast, the difference-from-reference is continuous, allowing DeepLIFT to avoid discontinuities caused by bias terms.",3.1. The DeepLIFT Philosophy,[0],[0]
"For a given input neuron x with difference-from-reference ∆x, and target neuron t with difference-from-reference ∆t that we wish to compute the contribution to, we define the multiplier m∆x∆t as:
m∆x∆t = C∆x∆t
∆x (2)
",3.2.1. DEFINITION OF MULTIPLIERS,[0],[0]
"In other words, the multiplier m∆x∆t is the contribution of ∆x to ∆t divided by ∆x.",3.2.1. DEFINITION OF MULTIPLIERS,[0],[0]
"Note the close analogy to the
idea of partial derivatives: the partial derivative ∂t∂x is the infinitesimal change in t caused by an infinitesimal change in x, divided by the infinitesimal change in x.",3.2.1. DEFINITION OF MULTIPLIERS,[0],[0]
"The multiplier is similar in spirit to a partial derivative, but over finite differences instead of infinitesimal ones.",3.2.1. DEFINITION OF MULTIPLIERS,[0],[0]
"Assume we have an input layer with neurons x1, ..., xn, a hidden layer with neurons y1, ..., yn, and some target output neuron t. Given values for m∆xi∆yj and m∆yj∆t, the following definition of m∆xi∆t is consistent with the summation-to-delta property in Eq. 1 (see Appendix A for the proof):
m∆xi∆t = ∑ j m∆xi∆yjm∆yj∆t",3.2.2. THE CHAIN RULE FOR MULTIPLIERS,[0],[0]
"(3)
We refer to Eq. 3 as the chain rule for multipliers.",3.2.2. THE CHAIN RULE FOR MULTIPLIERS,[0],[0]
"Given the multipliers for each neuron to its immediate successors, we can compute the multipliers for any neuron to a given target neuron efficiently via backpropagation - analogous to how the chain rule for partial derivatives allows us to compute the gradient w.r.t.",3.2.2. THE CHAIN RULE FOR MULTIPLIERS,[0],[0]
the output via backpropagation.,3.2.2. THE CHAIN RULE FOR MULTIPLIERS,[0],[0]
"When formulating the DeepLIFT rules described in Section 3.5, we assume that the reference of a neuron is its activation on the reference input.",3.3. Defining the Reference,[0],[0]
"Formally, say we have a neuron y with inputs x1, x2, ... such that y = f(x1, x2, ...).",3.3. Defining the Reference,[0],[0]
"Given the reference activations x01, x 0 2, ... of the inputs, we can calculate the reference activation y0 of the output as:
y0 = f(x01, x 0 2, ...)",3.3. Defining the Reference,[0],[0]
"(4)
i.e. references for all neurons can be found by choosing a reference input and propagating activations through the net.
",3.3. Defining the Reference,[0],[0]
The choice of a reference input is critical for obtaining insightful results from DeepLIFT.,3.3. Defining the Reference,[0],[0]
"In practice, choosing a good reference would rely on domain-specific knowledge, and in some cases it may be best to compute DeepLIFT scores against multiple different references.",3.3. Defining the Reference,[0],[0]
"As a guiding principle, we can ask ourselves “what am I interested in measuring differences against?”.",3.3. Defining the Reference,[0],[0]
"For MNIST, we use a reference input of all-zeros as this is the background of the images.",3.3. Defining the Reference,[0],[0]
"For the binary classification tasks on DNA sequence inputs (strings over the alphabet {A,C,G,T}), we obtained sensible results using either a reference input containing the expected frequencies of ACGT in the background (Fig. 5), or by averaging the results over multiple reference inputs for each sequence that are generated by shuffling each original sequence (Appendix J).",3.3. Defining the Reference,[0],[0]
"For CIFAR10 data, we found that using a blurred version of the original image as the
reference highlighted outlines of key objects, while an allzeros reference highlighted hard-to-interpret pixels in the background (Appendix L).
",3.3. Defining the Reference,[0],[0]
It is important to note that gradient×input implicitly uses a reference of all-zeros (it is equivalent to a first-order Taylor approximation of gradient×∆input where ∆ is measured w.r.t.,3.3. Defining the Reference,[0],[0]
an input of zeros).,3.3. Defining the Reference,[0],[0]
"Similary, integrated gradients (Section 2.2.3) requires the user to specify a starting point for the integral, which is conceptually similar to specifying a reference for DeepLIFT.",3.3. Defining the Reference,[0],[0]
"While Guided Backprop and pure gradients don’t use a reference, we argue that this is a limitation as these methods only describe the local behaviour of the output at the specific input value, without considering how the output behaves over a range of inputs.",3.3. Defining the Reference,[0],[0]
"We will see in Section 3.5.3 that, in some situations, it is essential to treat positive and negative contributions differently.",3.4. Separating Positive and Negative Contributions,[0],[0]
"To do this, for every neuron y, we will introduce ∆y+ and ∆y− to represent the positive and negative components of ∆y, such that:
∆y = ∆y+ + ∆y−
C∆y∆t = C∆y+∆t + C∆y−∆t
For linear neurons, ∆y+ and ∆y− are found by writing ∆y as a sum of terms involving its inputs ∆xi and grouping positive and negative terms together.",3.4. Separating Positive and Negative Contributions,[0],[0]
"The importance of this will become apparent when applying the RevealCancel rule (Section 3.5.3), where for a given target neuron t we may find thatm∆y+∆t andm∆y−∆t differ.",3.4. Separating Positive and Negative Contributions,[0],[0]
"However, when applying only the Linear or Rescale rules (Section 3.5.1 and Section 3.5.2), m∆y∆t = m∆y+∆t = m∆y−∆t.",3.4. Separating Positive and Negative Contributions,[0],[0]
We present the rules for assigning contribution scores for each neuron to its immediate inputs.,3.5. Rules for Assigning Contribution Scores,[0],[0]
"In conjunction with the chain rule for multipliers (Section 3.2), these rules can be used to find the contributions of any input (not just the immediate inputs) to a target output via backpropagation.",3.5. Rules for Assigning Contribution Scores,[0],[0]
This applies to Dense and Convolutional layers (excluding nonlinearities).,3.5.1. THE LINEAR RULE,[0],[0]
Let y be a linear function of its inputs xi such that y = b + ∑ i wixi.,3.5.1. THE LINEAR RULE,[0],[0]
We have ∆y = ∑ i wi∆xi.,3.5.1. THE LINEAR RULE,[0],[0]
"We define the positive and negative parts of ∆y as:
∆y+ = ∑ i 1{wi∆xi >",3.5.1. THE LINEAR RULE,[0],[0]
"0}wi∆xi
= ∑ i 1{wi∆xi > 0}wi(∆x+i",3.5.1. THE LINEAR RULE,[0],[0]
+ ∆x,3.5.1. THE LINEAR RULE,[0],[0]
"− i )
",3.5.1. THE LINEAR RULE,[0],[0]
∆y−,3.5.1. THE LINEAR RULE,[0],[0]
"= ∑ i 1{wi∆xi < 0}wi∆xi
= ∑ i 1{wi∆xi < 0}wi(∆x+i",3.5.1. THE LINEAR RULE,[0],[0]
+ ∆x,3.5.1. THE LINEAR RULE,[0],[0]
"− i )
Which leads to the following choice for the contributions: C∆x+i ∆y+
= 1{wi∆xi > 0}wi∆x+i C∆x−i ∆y+
= 1{wi∆xi > 0}wi∆x−i",3.5.1. THE LINEAR RULE,[0],[0]
"C∆x+i ∆y−
",3.5.1. THE LINEAR RULE,[0],[0]
= 1{wi∆xi < 0}wi∆x+i C∆x−i,3.5.1. THE LINEAR RULE,[0],[0]
"∆y− = 1{wi∆xi < 0}wi∆x−i
We can then find multipliers using the definition in Section 3.2.1, which givesm∆x+i ∆y+ = m∆x−i ∆y+",3.5.1. THE LINEAR RULE,[0],[0]
= 1{wi∆xi > 0}wi and m∆x+i ∆y− = m∆x−i,3.5.1. THE LINEAR RULE,[0],[0]
∆y−,3.5.1. THE LINEAR RULE,[0],[0]
"= 1{wi∆xi < 0}wi.
",3.5.1. THE LINEAR RULE,[0],[0]
What about when ∆xi = 0?,3.5.1. THE LINEAR RULE,[0],[0]
"While setting multipliers to 0 in this case would be consistent with summation-to-delta, it is possible that ∆x+i and ∆x",3.5.1. THE LINEAR RULE,[0],[0]
"− i are nonzero (and cancel each other out), in which case setting the multiplier to 0 would fail to propagate importance to them.",3.5.1. THE LINEAR RULE,[0],[0]
"To avoid this, we set m∆x+i ∆y+ = m∆x+i",3.5.1. THE LINEAR RULE,[0],[0]
∆y− = 0.5wi when ∆xi is 0 (similarly for ∆x−).,3.5.1. THE LINEAR RULE,[0],[0]
See Appendix B for how to compute these multipliers using standard neural network ops.,3.5.1. THE LINEAR RULE,[0],[0]
"This rule applies to nonlinear transformations that take a single input, such as the ReLU, tanh or sigmoid operations.",3.5.2. THE RESCALE RULE,[0],[0]
Let neuron y be a nonlinear transformation of its input x such that y = f(x).,3.5.2. THE RESCALE RULE,[0],[0]
"Because y has only one input, we have by summation-to-delta that C∆x∆y = ∆y, and consequently m∆x∆y = ∆y∆x .",3.5.2. THE RESCALE RULE,[0],[0]
"For the Rescale rule, we set ∆y + and ∆y− proportional to ∆x+ and ∆x− as follows:
∆y+ = ∆y
∆x ∆x+ = C∆x+∆y+
∆y− = ∆y
∆x",3.5.2. THE RESCALE RULE,[0],[0]
"∆x− = C∆x−∆y−
Based on this, we get:
m∆x+∆y+ = m∆x−∆y− = m∆x∆y = ∆y
∆x
In the case where x→ x0, we have ∆x→ 0 and ∆y → 0.",3.5.2. THE RESCALE RULE,[0],[0]
"The definition of the multiplier approaches the derivative, i.e. m∆x∆y → dydx , where the dy dx is evaluated at x = x
0.",3.5.2. THE RESCALE RULE,[0],[0]
"We can thus use the gradient instead of the multiplier when x is close to its reference to avoid numerical instability issues caused by having a small denominator.
",3.5.2. THE RESCALE RULE,[0],[0]
Note that the Rescale rule addresses both the saturation and the thresholding problems illustrated in Fig. 1 and Fig. 2.,3.5.2. THE RESCALE RULE,[0],[0]
"In the case of Fig. 1, if i01 = i02 = 0, then at i1 + i2 > 1 we have ∆h = −1 and ∆y = 1, giving
m∆h∆y = ∆y ∆h = −1",3.5.2. THE RESCALE RULE,[0],[0]
"even though dy dh = 0 (in other words, using difference-from-reference allows information to flow even when the gradient is zero).",3.5.2. THE RESCALE RULE,[0],[0]
"In the case of Fig. 2, assuming x0 = y0 = 0, at x = 10 + we have ∆y = , giving m∆x∆y = 10+ and C∆x∆y = ∆x ×m∆x∆y = .",3.5.2. THE RESCALE RULE,[0],[0]
"By contrast, gradient×input assigns a contribution of 10+ to x and−10 to the bias term (DeepLIFT never assigns importance to bias terms).
",3.5.2. THE RESCALE RULE,[0],[0]
"As revealed in previous work (Lundberg & Lee, 2016), there is a connection between DeepLIFT and Shapely values.",3.5.2. THE RESCALE RULE,[0],[0]
"Briefly, the Shapely values measure the average marginal effect of including an input over all possible orderings in which inputs can be included.",3.5.2. THE RESCALE RULE,[0],[0]
"If we define “including” an input as setting it to its actual value instead of its reference value, DeepLIFT can be thought of as a fast approximation of the Shapely values.",3.5.2. THE RESCALE RULE,[0],[0]
"At the time, Lundberg & Lee cited a preprint of DeepLIFT which described only the Linear and Rescale rules with no separate treatment of positive and negative contributions.",3.5.2. THE RESCALE RULE,[0],[0]
"SHAPELY VALUES: THE REVEALCANCEL RULE
While the Rescale rule improves upon simply using gradients, there are still some situations where it can provide misleading results.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Consider the min(i1, i2) operation depicted in Fig. 3, with reference values of i1 = 0 and i2 = 0.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Using the Rescale rule, all importance would be assigned either to i1 or to i2 (whichever is smaller).",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"This can obscure the fact that both inputs are relevant for the min operation.
",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"To understand why this occurs, consider the case when i1 > i2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"We have h1 = (i1 − i2) > 0 and h2 = max(0, h1) = h1.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"By the Linear rule, we calculate that C∆i1∆h1 = i1 and C∆i2∆h1 = −i2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"By the Rescale rule, the multiplier m∆h1∆h2 is ∆h2 ∆h1
= 1, and thus C∆i1∆h2 = m∆h1∆h2C∆i1∆h1 = i1 and C∆i2∆h2 = m∆h1∆h2C∆i2∆h1 = −i2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
The total contribution of i1 to the output o becomes (i1 − C∆i1∆h2) =,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"(i1 − i1) = 0, and the total contribution of i2 to o is −C∆i2∆h2 = i2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"This calculation is misleading as it discounts the fact that C∆i2∆h2 would be 0 if i1 were 0 - in other words, it ignores a dependency induced between i1 and i2 that comes from i2 canceling out i1 in the nonlinear neuron h2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
A similar failure occurs when i1 < i2; the Rescale rule results in C∆i1∆o = i1 and C∆i2∆o = 0.,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Note that gradients, gradient×input, Guided Backpropagation and integrated gradients would also assign all importance to either i1 or i2, because for any given input the gradient is zero for one of i1 or i2 (see Appendix C for a detailed calculation).
",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
One way to address this is by treating the positive and negative contributions separately.,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
We again consider the nonlinear neuron y = f(x).,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Instead of assuming that ∆y+ and ∆y− are proportional to ∆x+ and ∆x− and that
m∆x+∆y+ = m∆x−∆y− = m∆x∆y (as is done for the Rescale rule), we define them as follows:
∆y+",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"= 1
2
( f(x0 + ∆x+)− f(x0) )",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"+ 1
2
( f(x0 + ∆x− + ∆x+)−",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
f(x0 + ∆x−) ),3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"∆y− = 1
2
( f(x0 + ∆x−)− f(x0) )",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"+ 1
2
( f(x0 + ∆x+",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
+ ∆x−)−,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
f(x0 + ∆x+) ),3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"m∆x+∆y+ = C∆x+y+
∆x+ =
∆y+ ∆x+",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"; m∆x−∆y− = ∆y− ∆x−
In other words, we set ∆y+ to the average impact of ∆x+ after no terms have been added and after ∆x− has been added, and we set ∆y− to the average impact of ∆x− after no terms have been added and after ∆x+ has been added.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"This can be thought of as the Shapely values of ∆x+ and ∆x− contributing to y.
By considering the impact of the positive terms in the absence of negative terms, and the impact of negative terms in the absence of positive terms, we alleviate some of the issues that arise from positive and negative terms canceling each other out.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"In the case of Fig. 3, RevealCancel would assign a contribution of 0.5 min(i1, i2) to both inputs (see Appendix C for a detailed calculation).
",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"While the RevealCancel rule also avoids the saturation and thresholding pitfalls illustrated in Fig. 1 and Fig. 2, there are some circumstances where we might prefer to use the Rescale rule.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Specifically, consider a thresholded ReLU where ∆y > 0",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
iff ∆x,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
≥ b.,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"If ∆x < b merely indicates noise, we would want to assign contributions of 0 to both ∆x+ and ∆x− (as done by the Rescale rule) to mitigate the noise.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
RevealCancel may assign nonzero contributions by considering ∆x+ in the absence of ∆x− and vice versa.,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"In the case of softmax or sigmoid outputs, we may prefer to compute contributions to the linear layer preceding the final nonlinearity rather than the final nonlinearity itself.",3.6. Choice of Target Layer,[0],[0]
"This would be to avoid an attentuation caused by the
summation-to-delta property described in Section 3.1.",3.6. Choice of Target Layer,[0],[0]
"For example, consider a sigmoid output o = σ(y), where y is the logit of the sigmoid function.",3.6. Choice of Target Layer,[0],[0]
Assume y,3.6. Choice of Target Layer,[0],[0]
"= x1 + x2, where x01 = x 0 2 = 0.",3.6. Choice of Target Layer,[0],[0]
"When x1 = 50 and x2 = 0, the output o saturates at very close to 1 and the contributions of x1 and x2 are 0.5 and 0 respectively.",3.6. Choice of Target Layer,[0],[0]
"However, when x1 = 100 and x2 = 100, the output o is still very close to 0, but the contributions of x1 and x2 are now both 0.25.",3.6. Choice of Target Layer,[0],[0]
This can be misleading when comparing scores across different inputs because a stronger contribution to the logit would not always translate into a higher DeepLIFT score.,3.6. Choice of Target Layer,[0],[0]
"To avoid this, we compute contributions to y rather than o.
Adjustments for Softmax Layers
If we compute contributions to the linear layer preceding the softmax rather than the softmax output, an issue that could arise is that the final softmax output involves a normalization over all classes, but the linear layer before the softmax does not.",3.6. Choice of Target Layer,[0],[0]
"To address this, we can normalize the contributions to the linear layer by subtracting the mean contribution to all classes.",3.6. Choice of Target Layer,[0],[0]
"Formally, if n is the number of classes, C∆x∆ci represents the unnormalized contribution to class ci in the linear layer and C ′∆x∆ci represents the normalized contribution, we have:
C ′∆x∆ci = C∆x∆ci − 1
n n∑ j=1 C∆x∆cj (5)
As a justification for this normalization, we note that subtracting a fixed value from all the inputs to the softmax leaves the output of the softmax unchanged.",3.6. Choice of Target Layer,[0],[0]
"We train a convolutional neural network on MNIST (LeCun et al., 1999) using Keras (Chollet, 2015) to perform digit classification and obtain 99.2% test-set accuracy.",4.1. Digit Classification (MNIST),[0],[0]
"The architecture consists of two convolutional layers, followed by a fully connected layer, followed by the softmax output layer (see Appendix D for full details on model architecture and training).",4.1. Digit Classification (MNIST),[0],[0]
"We used convolutions with stride > 1 instead of pooling layers, which did not result in a drop in performance as is consistent with previous work (Springenberg et al., 2014).",4.1. Digit Classification (MNIST),[0],[0]
"For DeepLIFT and integrated gradients, we used a reference input of all zeros.
",4.1. Digit Classification (MNIST),[0],[0]
"To evaluate importance scores obtained by different methods, we design the following task: given an image that originally belongs to class co, we identify which pixels to erase to convert the image to some target class ct.",4.1. Digit Classification (MNIST),[0],[0]
"We do this by finding Sxidiff = Sxico − Sxict (where Sxic is the score for pixel xi and class c) and erasing up to 157 pixels (20% of the image) ranked in descending order of Sxidiff for which
Sxidiff > 0.",4.1. Digit Classification (MNIST),[0],[0]
"We then evaluate the change in the log-odds score between classes co and ct for the original image and the image with the pixels erased.
",4.1. Digit Classification (MNIST),[0],[0]
"As shown in Fig. 4, DeepLIFT with the RevealCancel rule outperformed the other backpropagation-based methods.",4.1. Digit Classification (MNIST),[0],[0]
"Integrated gradients (Section 2.2.3) computed numerically over either 5 or 10 intervals produced results comparable to each other, suggesting that adding more intervals would not change the result.",4.1. Digit Classification (MNIST),[0],[0]
"Integrated gradients also performed comparably to gradient*input, suggesting that saturation and thresholding failure modes are not common on MNIST data.",4.1. Digit Classification (MNIST),[0],[0]
"Guided Backprop discards negative gradients during backpropagation, perhaps explaining its poor performance at discriminating between classes.",4.1. Digit Classification (MNIST),[0],[0]
We also explored using the Rescale rule instead of RevealCancel on various layers and found that it degraded performance (Appendix E).,4.1. Digit Classification (MNIST),[0],[0]
"Next, we compared the importance scoring methods when applied to classification tasks on DNA sequence inputs (strings over the alphabet {A,C,G,T}).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
The human genome has millions of DNA sequence elements ( 200-1000 in length) containing specific combinations of short functional words to which regulatory proteins (RPs) bind to regulate gene activity.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
Each RP (e.g. GATA1) has binding affinity to specific collections of short DNA words (motifs) (e.g. GATAA and GATTA).,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
A key problem in computational genomics is the discovery of motifs in regulatory DNA elements that give rise to distinct molecular signatures (labels) which can be measured experimentally.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Here, in order to benchmark DeepLIFT and competing methods to uncover predictive patterns in DNA sequences, we design a simple simulation that captures the essence of the motif discovery problem described above.
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Background DNA sequences of length 200 were generated by sampling the letters ACGT at each position with
probabilities 0.3, 0.2, 0.2 and 0.3 respectively.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
Motif instances were randomly sampled from previously known probabilistic motif models (See Appendix F) of two RPs named GATA1 and TAL1 (Fig.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"6a)(Kheradpour & Kellis, 2014), and 0-3 instances of a given motif were inserted at random non-overlapping positions in the DNA sequences.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"We trained a multi-task neural network with two convolutional layers, global average pooling and one fullyconnected layer on 3 binary classification tasks.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Positive labeled sequences in task 1 represented “both GATA1 and TAL1 present”, task 2 represented “GATA1 present” and in task 3 represented “TAL1 present”.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"14 of sequences had both GATA1 and TAL1 motifs (labeled 111), 14 had only GATA1 (labeled 010), 14 had only TAL1 (labeled 001), and 1 4 had no motifs (labeled 000).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Details of the simulation, network architecture and predictive performance are given in Appendix F. For DeepLIFT and integrated gradients, we used a reference input that had the expected frequencies of ACGT at each position (i.e. we set the ACGT channel axis to 0.3, 0.2, 0.2, 0.3; see Appendix J for results using
shuffled sequences as a reference).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"For fair comparison, this reference was also used for gradient×input and Guided Backprop×input (“input” is more accurately called ∆input where ∆ measured w.r.t the reference).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"For DNA sequence inputs, we found Guided Backprop×input performed better than vanilla Guided Backprop; thus, we used the former.
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Given a particular subsequence, it is possible to compute the log-odds score that the subsequence was sampled from a particular motif vs. originating from the background distribution of ACGT.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"To evaluate different importancescoring methods, we found the top 5 matches (as ranked by their log-odds score) to each motif for each sequence from the test set, as well as the total importance allocated to the match by different importance-scoring methods for each task.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
The results are shown in Fig. 5 (for TAL1) and Appendix E (for GATA1).,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Ideally, we expect an importance scoring method to show the following properties: (1) high scores for TAL1 motifs on task 2 and (2) low scores for TAL1 on task 1, with (3) higher scores corresponding to stronger log-odds matches; analogous pattern for GATA1 motifs (high for task 1, low for task 2); (4) high scores for both TAL1 and GATA1 motifs for task 0, with (5) higher scores on sequences containing both kinds of motifs vs. sequences containing only one kind (revealing cooperativity; corresponds to red dots lying above green dots in Fig. 5).
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
We observe Guided Backprop×input fails (2) by assigning positive importance to TAL1 on task 1 (see Appendix H for an example sequence).,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
It fails property (4) by failing to identify cooperativity in task 0,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
(red dots overlay green dots).,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Both Guided Backprop×input and gradient×input show suboptimal behavior regarding property (3), in that there is a sudden increase in importance when the log-odds score is around 7, but little differentiation at higher logodds scores (by contrast, the other methods show a more gradual increase).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"As a result, Guided Backprop×input and gradient×input can assign unduly high importance to weak motif matches (Fig. 6).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
This is a practical consequence of the thresholding problem from Fig. 2.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"The large discontinuous jumps in gradient also result in inflated scores (note the scale on the y-axes) relative to other methods.
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"We explored three versions of DeepLIFT: Rescale at all nonlinearities (DeepLIFT-Rescale), RevealCancel at all nonlinearities (DeepLIFT-RevealCancel), and Rescale at convolutional layers with RevealCancel at the fully connected layer (DeepLIFT-fc-RC-conv-RS).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"In contrast to the results on MNIST, we found that DeepLIFT-fc-RC-convRS reduced noise relative to pure RevealCancel.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"We think this is because of the noise-suppression property discussed in Section 3.5.3; if the convolutional layers act like motif detectors, the input to convolutional neurons that do not fire may just represent noise and importance should not be propagated to them (see Fig. 6 for an example sequence).
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Gradient×inp, integrated gradients and DeepLIFT-Rescale occasionally miss relevance of TAL1 for Task 0",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"(Fig. 5b), which is corrected by using RevealCancel on the fully connected layer (see example sequence in Fig. 6).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
Note that the RevealCancel scores seem to be tiered.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"As illustrated in Appendix I, this is related to having multiple instances of a given motif in a sequence (eg: when there are multiple TAL1 motifs, the importance assigned to the presence of TAL1 is distributed across all the motifs).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"We have presented DeepLIFT, a novel approach for computing importance scores based on explaining the difference of the output from some ‘reference’ output in terms of differences of the inputs from their ‘reference’ inputs.",5. Conclusion,[0],[0]
"Using the difference-from-reference allows information to propagate even when the gradient is zero (Fig. 1), which could prove especially useful in Recurrent Neural Networks where saturating activations like sigmoid or tanh are popular.",5. Conclusion,[0],[0]
DeepLIFT avoids placing potentially misleading importance on bias terms (in contrast to gradient*input - see Fig. 2).,5. Conclusion,[0],[0]
"By allowing separate treatment of positive and negative contributions, the DeepLIFT-RevealCancel rule can identify dependencies missed by other methods (Fig. 3).",5. Conclusion,[0],[0]
"Open questions include how to apply DeepLIFT to RNNs, how to compute a good reference empirically from the data, and how best to propagate importance through ‘max’ operations (as in Maxout or Maxpooling neurons) beyond simply using the gradients.",5. Conclusion,[0],[0]
We thank Anna Shcherbina for early experiments applying DeepLIFT to image data and beta-testing.,6. Acknowledgements,[0],[0]
AS is supported by a Howard Hughes Medical Institute International Student Research Fellowship and a Bio-X Bowes Fellowship.,7. Funding,[0],[0]
PG is supported by a Bio-X Stanford Interdisciplinary Graduate Fellowship.,7. Funding,[0],[0]
AK was supported by NIH grants DP2-GM-123485 and 1R01ES025009-02.,7. Funding,[0],[0]
AS & PG conceptualized DeepLIFT.,8. Author Contributions,[0],[0]
AS implemented DeepLIFT.,8. Author Contributions,[0],[0]
AS ran experiments on MNIST.,8. Author Contributions,[0],[0]
AS & PG ran experiments on genomic data.,8. Author Contributions,[0],[0]
AK provided guidance and feedback.,8. Author Contributions,[0],[0]
"AS, PG and AK wrote the manuscript.",8. Author Contributions,[0],[0]
The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential.,abstractText,[0],[0]
"Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input.",abstractText,[0],[0]
DeepLIFT compares the activation of each neuron to its ‘reference activation’ and assigns contribution scores according to the difference.,abstractText,[0],[0]
"By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches.",abstractText,[0],[0]
Scores can be computed efficiently in a single backward pass.,abstractText,[0],[0]
"We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods.",abstractText,[0],[0]
"Video tutorial: http://goo.gl/ qKb7pL, code: http://goo.gl/RM8jvH.",abstractText,[0],[0]
Learning Important Features Through Propagating Activation Differences,title,[0],[0]
"The Partially Observable Markov Decision Processes (POMDP) [8] is a general model for sequential decision making in stochastic and partially observable environments, which are ubiquitous in real-world problems.",1 Introduction,[0],[0]
A key shortcoming of POMDP methods is the assumption that the dynamics of the environment are known a priori.,1 Introduction,[0],[0]
"In real-world applications, however, it may be impossible to obtain a complete and accurate description of the system.",1 Introduction,[0],[0]
"Instead, we may have uncertain prior knowledge about the model.",1 Introduction,[0],[0]
"When lacking a model, a prior can be incorporated into the POMDP problem in a principled way, as demonstrated by the Bayes-Adaptive POMDP framework [13].
",1 Introduction,[0],[0]
"The BA-POMDP framework provides a Bayesian approach to decision making by maintaining a probability distribution over possible models as the agent acts in an online reinforcement learning setting [5, 19].",1 Introduction,[0],[0]
This method casts the Bayesian reinforcement learning problem into a POMDP planning problem where the hidden model of the environment is part of the state space.,1 Introduction,[0],[0]
"Unfortunately, this planning problem becomes very large, with a countably infinite state space over all possible models, and as such, current solution methods are not scalable or perform poorly [13].
",1 Introduction,[0],[0]
Online and sample-based planning has shown promising performance on non-trivial POMDP problems [12].,1 Introduction,[0],[0]
"Online methods reduce the complexity by considering the relevant (i.e., reachable) states only, and sample-based approaches tackle the complexity issues through approximations in the form of simulated interactions with the environment.",1 Introduction,[0],[0]
"Here we modify one of those methods, Partial Observable Monte-Carlo Planning (POMCP)",1 Introduction,[0],[0]
"[15] and extend it to the Bayes-Adaptive case, leading to a novel approach: BA-POMCP.
",1 Introduction,[0],[0]
"In particular, we improve the sampling approach by exploiting the structure of the BA-POMDP resulting in root sampling and expected models methods.",1 Introduction,[0],[0]
"We also present an approach for more efficient model representation, which we call linking states.",1 Introduction,[0],[0]
"Lastly, we prove the correctness of our improvements, showing that they converge to the true BA-POMDP solution.",1 Introduction,[0],[0]
"As a result, we present methods that significantly improve the scalability of learning in BAPOMDPs, making them practical for larger problems.",1 Introduction,[0],[0]
"First, we discuss POMDPs and BA-POMDPs in respectively Section 2.1 and 2.2.",2 Background,[0],[0]
"Formally, a POMDP is described by a tuple (S, A, Z, D, R, γ, h), where S is the set of states of the environment; A is the set of actions; Z is the set of observations; D is the ‘dynamics function’ that describes the dynamics of the ∗This is an extended version of a paper that was published at ICML’2017.
",2.1 POMDPs,[0],[0]
"ar X
iv :1
80 6.
",2.1 POMDPs,[0],[0]
"05 63
1v 1
[ cs
.A",2.1 POMDPs,[0],[0]
"I]
1 4
Ju n
20 18
system in the form of transition probabilities D(s′,z|s,a);1 R is the immediate reward function R(s, a) that describes the reward of selecting a in s; γ ∈",2.1 POMDPs,[0],[0]
"[0,1) is the discount factor; and h is the horizon of an episode in the system.
",2.1 POMDPs,[0],[0]
"The goal of the agent in a POMDP is to maximize the expected cumulative (discounted) reward, also called the expected return.",2.1 POMDPs,[0],[0]
"The agent has no direct access to the system’s state, so it can only rely on the action-observation history ht = 〈a0,z1, . . .",2.1 POMDPs,[0],[0]
",at−1,zt〉 up to the current step t. It can use this history to maintain a probability distribution over the state, also called a belief, b(s).",2.1 POMDPs,[0],[0]
"A solution to a POMDP is then a mapping from a belief b to an action a, which is called a policy π.",2.1 POMDPs,[0],[0]
"Solution methods aim to find an optimal policy, a mapping from a belief to an action with the highest possible expected return.
",2.1 POMDPs,[0],[0]
The agent maintains its belief during execution through belief updates.,2.1 POMDPs,[0],[0]
"A belief update calculates the posterior probability of the state s′ given the previous belief over s and action-observation pair 〈a,z〉: b′(s) = P (s′|b(s), a, z).",2.1 POMDPs,[0],[0]
This operation is infeasible for large spaces because it enumerates over the entire state space.,2.1 POMDPs,[0],[0]
A common approximation method is to represent the belief with a (unweighted) particle filter [17].,2.1 POMDPs,[0],[0]
A particle filter is a collection of K particles (states).,2.1 POMDPs,[0],[0]
"Each particle represents a probability of 1K ; if a specific state x occurs n times in a particle filter, then P (x) = nK .",2.1 POMDPs,[0],[0]
"The precision of the filter is determined by the number of particles K. To update such a belief after execution of action a and observation z, a standard approach is to utilize rejection sampling: the agent repeatedly samples a state s from its belief, then simulates the execution of a on s through D, and receives a (simulated) new state s′sim and observation zsim.",2.1 POMDPs,[0],[0]
"s
′ is added to the new belief only when zsim equals z, and rejected otherwise.",2.1 POMDPs,[0],[0]
"This process repeats until the new belief contains K particles.
",2.1 POMDPs,[0],[0]
Partially Observable Monte-Carlo Planning (POMCP),2.1 POMDPs,[0],[0]
"[15], is a scalable method which extends Monte Carlo tree search (MCTS) to solve POMDPs.",2.1 POMDPs,[0],[0]
POMCP is one of the leading algorithms for solving general POMDPs.,2.1 POMDPs,[0],[0]
"At each time step, the algorithm performs online planning by incrementally building a lookahead tree that contains Q(h,a), where h is the action-observation history-path to reach that node.",2.1 POMDPs,[0],[0]
It samples hidden states s at the root node (called ‘root sampling’) and uses that state to sample a trajectory that first traverses the lookahead tree and then performs a (random) rollout.,2.1 POMDPs,[0],[0]
The return of this trajectory is used to update the statistics for all visited nodes.,2.1 POMDPs,[0],[0]
"These statistics include the number of times an action has been taken at a history (N(h,a)) and estimated value of being in that node (Q(h,a)), based on an average over the returns.
",2.1 POMDPs,[0],[0]
"Because this lookahead tree can be very large, the search is directed to the relevant parts by selecting the actions inside the tree that maximize the ‘upper confidence bounds’",2.1 POMDPs,[0],[0]
"[2]: U(h,a) = Q(h, a) + c √ log(N(h) + 1)/N(h,a).",2.1 POMDPs,[0],[0]
"Here, N(h) is the number of times the history has been visited.",2.1 POMDPs,[0],[0]
"At the end of each simulation, the discounted accumulated return is used to update the estimated value of all the nodes in the tree that have been visited during that simulation.",2.1 POMDPs,[0],[0]
"POMCP terminates after some criteria has been met, typically defined by a maximum number of simulations or allocated time.",2.1 POMDPs,[0],[0]
"The agent then picks the action with the highest estimated value (maxaQ(b,a)).",2.1 POMDPs,[0],[0]
POMCP can be shown to converge to an -optimal value function.,2.1 POMDPs,[0],[0]
"Moreover, the method has demonstrated good performance in large domains with a limited number of simulations.",2.1 POMDPs,[0],[0]
The extension of POMCP that is used in this work is discussed in Section 3.,2.1 POMDPs,[0],[0]
"Most research concerning POMDPs has considered the task of planning: given a full specification of the model, determine an optimal policy (e.g., [8, 14]).",2.2 BA-POMDPs,[0],[0]
"However, in many real-world applications, the model is not (perfectly) known in advance, which means that the agent has to learn about its environment during execution.",2.2 BA-POMDPs,[0],[0]
This is the task considered in reinforcement learning (RL),2.2 BA-POMDPs,[0],[0]
"[16].
",2.2 BA-POMDPs,[0],[0]
"A fundamental RL problem is the difficulty of deciding whether to select actions in order to learn a better model of the environment, or to exploit the current knowledge about the rewards and effects of actions.",2.2 BA-POMDPs,[0],[0]
"In recent years, Bayesian RL methods have become popular because they can provide a principled solution to this exploration/exploitation tradeoff",2.2 BA-POMDPs,[0],[0]
"[19, 5, 6, 10, 18].
",2.2 BA-POMDPs,[0],[0]
"In particular, we consider the framework of Bayes-Adaptive POMDPs [11, 13].",2.2 BA-POMDPs,[0],[0]
BA-POMDPs use Dirichlet distributions to model uncertainty over transitions and observations2 (typically assuming the reward function is chosen by the designer and is known).,2.2 BA-POMDPs,[0],[0]
"In particular, if the agent could observe both states and observations, it could maintain a vector χ with the counts of the occurrences for all 〈s, a, s′, z〉 tuples.",2.2 BA-POMDPs,[0],[0]
"We write χs′zsa for the number of times that 〈s,a〉 is followed by 〈s′,z〉.
",2.2 BA-POMDPs,[0],[0]
"While the agent cannot observe the states and has uncertainty about the actual count vector, this uncertainty can be represented using regular POMDP formalisms.",2.2 BA-POMDPs,[0],[0]
"That is, the count vector is included as part of the hidden state of a specific POMDP, called a BA-POMDP.",2.2 BA-POMDPs,[0],[0]
"Formally, a BA-POMDP is a tuple 〈S̄, A, D̄, R̄, Z, γ, h〉 with some modified
1 This formulation allows for easier notation and generalizes the typical formulation with separate transition T and observation functions",2.2 BA-POMDPs,[0],[0]
"O: D = 〈T,O〉.",2.2 BA-POMDPs,[0],[0]
"In our experiments, we do employ this typical factorization.
2 [11, 13] follow the standard T & O POMDP representations, but we use our combined D formalism.
",2.2 BA-POMDPs,[0],[0]
"Algorithm 1 BA-POMCP(b̄,num sims)
1: //b̄ is an augmented belief (e.g., particle filter) 2: h0 ← () .",2.2 BA-POMDPs,[0],[0]
"The empty history (i.e., now) 3: for i← 1 . . .",2.2 BA-POMDPs,[0],[0]
"num sims do 4: //First, we root sample an (augmented) state: 5: s̄← SAMPLE(b̄) .",2.2 BA-POMDPs,[0],[0]
"reference to a particle 6: s̄′ ← COPY(s̄) 7: SIMULATE(s̄′, 0, h0) 8: end for 9: a← GREEDYACTIONSELECTION(h0)
10: return a
components in comparison to the POMDP.",2.2 BA-POMDPs,[0],[0]
"While the observation and action space remain unchanged, the state (space) of the BA-POMDP now includes Dirichlet parameters: s̄ = 〈s, χ〉, which we will refer to as augmented states.",2.2 BA-POMDPs,[0],[0]
"The reward model remains the same, since it is assumed to be known, R̄(〈s′,χ′),a) =",2.2 BA-POMDPs,[0],[0]
"R(s,a).",2.2 BA-POMDPs,[0],[0]
"The dynamics functions, D̄, however, is described in terms of the counts in s̄, and is defined as follows
Dχ(s ′,z|s, a) , E[D(s′,z|s, a)|χ] = χ s′z sa∑
s′z χ s′z sa
.",2.2 BA-POMDPs,[0],[0]
"(1)
These expectations can now be used to define the transitions for the BA-POMDP.",2.2 BA-POMDPs,[0],[0]
If we let δs ′z,2.2 BA-POMDPs,[0],[0]
"sa denote a vector of the length of χ containing all zeros except for the position corresponding to 〈s,a,s′,z〉 (where it has a one), and if we let Ia,b denote the Kronecker delta that indicates (is 1 when) a = b, then we can define D̄ as D̄(s′,χ′,z|s,χ, a) = Dχ(s
′,z|s, a)Iχ′,χ+δs′zsa .",2.2 BA-POMDPs,[0],[0]
"Remember that these counts are not observed by the agent, since that would require observations of the state.",2.2 BA-POMDPs,[0],[0]
The agent can only maintain belief over these count vectors.,2.2 BA-POMDPs,[0],[0]
"Still, when interacting with the environment, the ratio of the true—but unknown—count vectors will converge to coincide with the true transition and observation probabilities in expectation.",2.2 BA-POMDPs,[0],[0]
"It is important to realize, however, that this convergence of count vector ratios does not directly imply learnability by the agent: even though the ratio of the count vectors of the true hidden state will converge, the agent’s belief over count vectors might not.
",2.2 BA-POMDPs,[0],[0]
BA-POMDPs are infinite state POMDP models and thus extremely difficult to solve.,2.2 BA-POMDPs,[0],[0]
"Ross et al. [13] introduced a technique to convert such models to finite models, but these are still very large.",2.2 BA-POMDPs,[0],[0]
"Therefore, Ross et al. propose a simple lookahead planner to solve BA-POMDPs in an online manner.",2.2 BA-POMDPs,[0],[0]
"This approach approximates the expected values associated with each action at the belief by applying a lookahead search of depth d. This method will function as the comparison baseline in our experiments, as no other BA-POMDP solution methods have been proposed.",2.2 BA-POMDPs,[0],[0]
"Powerful methods, such as POMCP [15], have significantly improved the scalability of POMDP solution methods.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"At the same time the most practical solution method for BA-POMDPs, the aforementioned lookahead algorithm, is quite limited in dealing with larger problems.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"POMDP methods have rarely been applied to BA-POMDPs [1], and no systematic investigation of their performance has been conducted.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In this paper, we aim to address this void, by extending POMCP to BA-POMDPs, in an algorithm that we refer to as BA-POMCP.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Moreover, we propose a number of novel adaptations to BA-POMCP that exploit the structure of the BA-POMDP.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In this section, we first lay out the basic adaptation of POMCP to BA-POMDPs and then describe the proposed modifications that improve its efficiency.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"BA-POMCP BA-POMCP, just like POMCP, constructs a lookahead tree through simulated experiences (Algorithm 1).",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In BA-POMDPs, however, the dynamics of the system are inaccessible during simulations, and the belief is a probability distribution over augmented states.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"BA-POMCP, as a result, must sample augmented states from the belief b̄, and use copies of those states (s̄ = 〈s,χ〉) for each simulation (Algorithm 2).",3 BA-POMDPs via Sample-based Planning,[0],[0]
We will refer to this as root sampling of the state (line 6).,3 BA-POMDPs via Sample-based Planning,[0],[0]
"The copy is necessary, as otherwise the STEP function in Algorithm 2 would alter the belief b̄.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"It is also expensive, for χ grows with the state, action and observation space, to |S|2 × |A| × |Ω| parameters.",3 BA-POMDPs via Sample-based Planning,[0],[0]
3,3 BA-POMDPs via Sample-based Planning,[0],[0]
"In practice, this operation becomes a bottleneck to the runtime of BA-POMCP in larger domains.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
3It is |S|2 × |A|+ |S| ×,3 BA-POMDPs via Sample-based Planning,[0],[0]
"|A| × |Ω| when assuming D is factored in T & O
Algorithm 2 SIMULATE(s̄, d, h) 1: if ISTERMINAL(h) ||",3 BA-POMDPs via Sample-based Planning,[0],[0]
"d == max depth then 2: return 0 3: end if 4: //Action selection uses statistics stored at node h: 5: a← UCBACTIONSELECTION(h) 6: R← R(s̄,a) 7: z ← STEP(s̄, a) //modifies",3 BA-POMDPs via Sample-based Planning,[0],[0]
"s̄ to sampled next state 8: h′ ← (h,a,z) 9: if h′ ∈ Tree then 10: r ← R+ γ SIMULATE(s̄, d+ 1, h′) 11: else 12: CONSTRUCTNODE(h′) //Initializes statistics 13:",3 BA-POMDPs via Sample-based Planning,[0],[0]
"r ← R+ γ ROLLOUT(s̄, d+ 1, h′) 14: end if 15: //Update statistics: 16: N(h,a)← N(h,a) + 1 17: Q(h,a)← N(h,a)−1
N(h,a) Q(h,a) + 1 N(h,a) r
18: return r
Algorithm 3 BA-POMCP-STEP(s̄ = 〈s, χ〉, a) 1: Dsa ∼ χsa 2: 〈s′,z〉 ∼ Dsa 3: //In place updating of s̄ = 〈s, χ〉 4: χs ′z",3 BA-POMDPs via Sample-based Planning,[0],[0]
sa ← χs ′z,3 BA-POMDPs via Sample-based Planning,[0],[0]
"sa + 1
5: s← s′ 6: return z
To apply POMCP on BA-POMDPs, where the dynamics are unknown, we modify the STEP function, proposing several variants.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The most straightforward one, BA-POMCP-STEP is employed in what we refer to as ‘BA-POMCP’.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"This method, shown in Algorithm 3, is similar to BA-MCP [7]: essentially, it samples a dynamic model Dsa which specifies probabilities Pr(s′,z|s,a) and subsequently samples an actual next state and observation from that distribution.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Note that the underlying states and observations are all represented simply as an index, and hence the assignment on line 5 is not problematic.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"However, the cost of the model sampling operation in line 1 is.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Root Sampling of the Model BA-MCP [7] addresses the fully observable BRL problem by using POMCP on an augmented state s̄ = 〈s, T 〉, consisting of the observable state, as well as the hidden true transition function T .",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Application of POMCP’s root sampling of state in this case leads to ‘root sampling of a transition function’: Since the true transition model T does not change during the simulation, one is sampled at the root and used during the entire simulation.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In the BA-POMCP case, root sampling of a state s̄ = 〈s, χ〉 does not lead to a same interpretation: no model, but counts are root sampled and they do change over time.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"We use this as inspiration to introduce a similar, but clearly different, (since this is not root sampling of state) technique called root sampling of the model (which we will refer to as just ‘root sampling’).",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The idea is simple: every time we root sample a state s̄ = 〈s, χ〉 ∼ b̄ at the beginning of a simulation (line 5 in Algorithm 1), we directly sample a Ḋ ∼ Dir(χ), which we will refer to as the root-sampled model Ḋ and it is used for the rest of the simulation.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
We denote this root sampling in BA-POMCP as ‘R-BA-POMCP’.,3 BA-POMDPs via Sample-based Planning,[0],[0]
The approach is formalized by R-BA-POMCPSTEP (Algorithm 4).,3 BA-POMDPs via Sample-based Planning,[0],[0]
Note that no count updates take place (cf. line 4 in Algorithm 3).,3 BA-POMDPs via Sample-based Planning,[0],[0]
"This highlights an important advantage of this technique: since the counts are not used in the remainder of the simulation, the copy of counts (as part of line 6 of Algorithm 1) can be avoided altogether.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Since this copy operation is costly, especially in larger domains, where the number of states, action and observations and the number of counts is large, this can lead to significant savings.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Finally, we point out that, similar to what Guez et al. [7] propose, Ḋ can be constructed lazily: the part of the model Ḋ is only sampled when it becomes necessary.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The transition probabilities during R-BA-POMCP differ from those in BA-POMCP, and it is not obvious that a policy based on R-BA-POMCP maintains the same guarantees.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"We prove in Section 4 that the solution of R-BAPOMCP in the limit converges to that of BA-POMCP.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Algorithm 4 R-BA-POMCP-STEP (s̄ = 〈s, χ〉, a) 1: //Sample from the root sampled model 2: s′,z ∼ Ḋs,a 3: s← s′ 4: return z
Algorithm 5 E-BA-POMCP-STEP(s̄ = 〈s, χ〉, a) 1: //Sample from the Expected model 2:",3 BA-POMDPs via Sample-based Planning,[0],[0]
"s′,z ∼ Dχ(·, ·|s,a) 3: χs ′z",3 BA-POMDPs via Sample-based Planning,[0],[0]
sa ← χs ′z,3 BA-POMDPs via Sample-based Planning,[0],[0]
"sa + 1
4: s← s′ 5: return z
Expected models during simulations The second, complementary, adaptation modifies the way models are sampled from the root-sampled counts in STEP.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"This version samples the transitions from the expected dynamics Dχ given in (1), rather than from a sampled dynamics function D ∼ Dir(χ).",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The latter operation is relatively costly, while constructing Dχ is very cheap.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In fact, this operation is so cheap, that it is more efficient to (re-)calculate it on the fly rather than to actually store Dχ.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"This approach is shown in Algorithm 5.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
Linking States,3 BA-POMDPs via Sample-based Planning,[0],[0]
"Lastly, we propose a specialized data structure to encode the augmented BA-POMDP states.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The structure aims to optimize for the complexity of the count-copy operation in line 6 of Algorithm 1 while allowing modifications to s̄. The linking state sl is a tuple of a system state, a pointer (or link) to an unmodifiable set of counts χ and a set of updated counts 〈s, l, δ〉.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"l is a pointer to some set of counts χ, which remain unchanged during count updates (such as in the STEP function), and instead are stored in the set of updated counts, δ, as shown in Algorithm 6.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The consequence is that the linking state copy-operation can safely perform a shallow copy of the counts χ, and must only consider δ, which is assumed to be much smaller.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
Linking states can be used during the (rejection-sample-based) belief update at the beginning of each real time step.,3 BA-POMDPs via Sample-based Planning,[0],[0]
"While the root-sampled augmented states (including δ in linking states) are typically deleted at the end of each simulation during L-BA-POMCP, each belief update potentially increases the size of δ of each particle.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Theoretically, the number of updated counts represented in δ increases and the size of δ may (eventually) grow similar to the size of χ.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Therefore, at some point, it is necessary to construct a new χ′ that combines χ and δ (after which δ can be safely emptied).",3 BA-POMDPs via Sample-based Planning,[0],[0]
"We define a new parameter for the maximum size of δ, λ, and condition to merge only if the size of δ exceeds λ.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"We noticed that, in practice, the number of merges is much smaller than the amount of copies in BA-POMCP.",3 BA-POMDPs via Sample-based Planning,[0],[0]
We also observed in our experiments that it is often the case that a specific (small) set of transitions are notably more popular than others and that δ grows quite slowly.,3 BA-POMDPs via Sample-based Planning,[0],[0]
"Here, we analyze the proposed root sampling of the dynamics function and expected transition techniques, and demonstrate they converge to the solution of the BA-POMDP.",4 Theoretical Analysis,[0],[0]
These main steps of this proof are similar to those in [15].,4 Theoretical Analysis,[0],[0]
"We point out however, that the technicalities of proving the components are far more involved.
",4 Theoretical Analysis,[0],[0]
"The convergence guarantees of the original POMCP method are based on showing that, for an arbitrary rollout policy π, the POMDP rollout distribution (the distribution over full histories when performing root sampling of state) is equal to the derived MDP rollout distribution (the distribution over full histories when sampling in the belief MDP).",4 Theoretical Analysis,[0],[0]
Given that these are identical it is easy to see that the statistics maintained in the search tree will converge to the same number in expectation.,4 Theoretical Analysis,[0],[0]
"As such, we will show a similar result here for expected transitions (‘expected’ for short) and root sampling of the dynamics function (‘root sampling’ below).
",4 Theoretical Analysis,[0],[0]
"We define H0 as the full history (also including states) at the root of simulation, Hd as the full history of a node at depth d in the simulation tree, and χ(Hd) as the counts induced by Hd.",4 Theoretical Analysis,[0],[0]
"We then define the rollout distributions:
Definition 1.",4 Theoretical Analysis,[0],[0]
"The expected full-history expected transition BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π.",4 Theoretical Analysis,[0],[0]
"It is given by
Pπ(Hd+1) = Dχ(Hd)(sd+1,zd+1|as,sd)π(ad|hd)P π(Hd) (2)
with Pπ(H0) = b0(〈s0,χ0〉)",4 Theoretical Analysis,[0],[0]
"the belief ‘now’ (at the root of the online planning).
",4 Theoretical Analysis,[0],[0]
"Algorithm 6 L-BA-POMCP-STEP(sl = 〈s, l, δ〉, a) 1: D ∼ 〈l, δ〉 2: s′,z ∼ Ds,a 3: s← s′ 4: δs ′z",4 Theoretical Analysis,[0],[0]
sa ← δs ′z,4 Theoretical Analysis,[0],[0]
"sa + 1
5: return z
Note that there are two expectations in the above definition: ‘expected transitions’ mean that transitions for a history Hd are sampled from Dχ(Hd).",4 Theoretical Analysis,[0],[0]
"The other ‘expected’ is the expectation of those samples (and it is easy to see that this will converge to the expected transition probabilities Dχ(Hd)(sd+1,zd+1|as,sd)).",4 Theoretical Analysis,[0],[0]
"For root sampling of the dynamics model, this is less straightforward, and we give the definition in terms of the empirical distribution:
Definition 2.",4 Theoretical Analysis,[0],[0]
"The empirical full-history root-sampling (RS) BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π in combination with root sampling of the dynamics model D. This distribution, for a particular stage d, is given by
P̃πK(Hd) , 1
Kd Kd∑ i=1",4 Theoretical Analysis,[0],[0]
"I{ Hd=H (i) d
}, where
• K is the number of simulations that comprise the empirical distribution, • Kd is the number of simulations that reach depth d (not all simulations might be equally long), • H(i)d is the history specified by the i-th particle at stage d.
Now, our main theoretical result is that these distributions are the same in the limit of the number of simulations:
Theorem 3.",4 Theoretical Analysis,[0],[0]
"The full-history RS-BA-POMDP rollout distribution (Def. 2) converges in probability to the quantity of Def. 1:
∀Hd P̃πKd(Hd) p→ Pπ(Hd).",4 Theoretical Analysis,[0],[0]
"(3)
Proof.",4 Theoretical Analysis,[0],[0]
"The proof is listed in appendix A.
Corollary 4.",4 Theoretical Analysis,[0],[0]
"Given suitably chosen exploration constant (e.g., c > Rmax1−γ ), BA-POMCP with root-sampling of dynamics function converges in probability to the expected transition solution.
",4 Theoretical Analysis,[0],[0]
Proof.,4 Theoretical Analysis,[0],[0]
"Since Theorem 3 guarantees the distributions over histories are the same in the limit, they will converge to the same values maintained in the tree.
",4 Theoretical Analysis,[0],[0]
"Finally, we see that these are solutions for the BA-POMDP:
Corollary 5.",4 Theoretical Analysis,[0],[0]
"BA-POMCP with expected transitions sampling, as well as with root sampling of dynamics function converge to an -optimal value function of a BA-POMDP: V (〈s,χ〉 ,h) p→ V ∗ (〈s,χ〉 ,h), where =
precision 1−γ .
",4 Theoretical Analysis,[0],[0]
Proof.,4 Theoretical Analysis,[0],[0]
"A BA-POMDP is a POMDP, so the analysis from Silver and Veness [15] applies to the BA-POMDP, which means that the stated guarantees hold for BA-POMCP.",4 Theoretical Analysis,[0],[0]
"The BA-POMDP is stated in terms of expected transitions, so the theoretical guarantees extend to the expected transition BA-POMCP, which in turn via corollary 4 implies that the theoretical guarantees extend to RS-BA-POMCP.
",4 Theoretical Analysis,[0],[0]
"Finally, we note that linking states does not affect they way that sampling is performed at all:
Proposition 6.",4 Theoretical Analysis,[0],[0]
Linking states does not affect convergence of BA-POMCP.,4 Theoretical Analysis,[0],[0]
"Experimental setup In this section, we evaluate our algorithms on a small toy problem, the well-known Tiger problem [3] and test scalability on a larger domain: the Partially Observable Sysadmin (POSysadmin) problem.",5 Empirical Evaluation,[0],[0]
"In POSysadmin, the agent acts as a system administrator with the task of maintaining a network of n computers.",5 Empirical Evaluation,[0],[0]
"Computers are either ‘working’ or ‘failing’, which can be deterministically resolved by ‘rebooting’ the computer.",5 Empirical Evaluation,[0],[0]
"The agent does not know the state of any computer, but can ‘ping’ any individual computer.",5 Empirical Evaluation,[0],[0]
"At each step, any of the computers can ‘fail’ with some probability f .",5 Empirical Evaluation,[0],[0]
"This leads to a state space of size 2n, an action space of 2n+ 1, where the agent can ‘ping’ or ‘reboot’ any of the computers, or ‘do nothing’, and an observation space of 3 ({NULL, failing, working}).",5 Empirical Evaluation,[0],[0]
"The ‘ping’ action has a cost of 1 associated with it, while rebooting a computer costs 20 and switches the computer to ‘working’.",5 Empirical Evaluation,[0],[0]
"Lastly, each ‘failing’ computer has a cost of 10 at each time step.
",5 Empirical Evaluation,[0],[0]
We conducted an empirical evaluation with aimed for 3 goals: The first goal attempts to support the claims made in Section 4 and show that the adaptations to BA-POMCP do not decrease the quality of the resulting policies.,5 Empirical Evaluation,[0],[0]
"Second, we investigate the runtime of those modifications to demonstrate their contribution to the efficiency of BA-POMCP.",5 Empirical Evaluation,[0],[0]
The last part contains experiments that directly compare the performance per action selection time with the baseline approach of Ross et al. [13].,5 Empirical Evaluation,[0],[0]
"For brevity, Table 1 describes the default parameters for the following experiments.",5 Empirical Evaluation,[0],[0]
"It will be explicitly mentioned whenever different values are used.
",5 Empirical Evaluation,[0],[0]
"BA-POMCP variants Section 4 proves that the solutions of the proposed modifications (root-sampling (R-), expected models (E-) and linking states (L-)) in the limit converge to the solution of BA-POMCP.",5 Empirical Evaluation,[0],[0]
"Here, we investigate the behaviour of these methods in practice.",5 Empirical Evaluation,[0],[0]
"For the Tiger problem, the agent’s initial belief over the transition model is correct (i.e., counts that correspond to the true probabilities with high confidence), but it provides an uncertain belief that underestimates the reliability of the observations.",5 Empirical Evaluation,[0],[0]
"Specifically, it assigns 5 counts to hearing the correct observation and 3 counts to incorrect: the agent initially beliefs it will hear correctly with a probability of 62.5%.",5 Empirical Evaluation,[0],[0]
"The experiment is run for with 100, 1000 & 1000 simulations and all combinations of BA-POMCP adaptations.
",5 Empirical Evaluation,[0],[0]
Figure 1a plots the average return over 10000 runs for a learning period of 100 episodes for Tiger.,5 Empirical Evaluation,[0],[0]
"The key
observation here is two-fold.",5 Empirical Evaluation,[0],[0]
"First, all methods improve over time through refining their knowledge about D. Second, there are three distinct clusters of lines, each grouped by the number of simulations.",5 Empirical Evaluation,[0],[0]
"This shows that all 3 variants (R/L/E-BA-POMCP) lead to the same results.
",5 Empirical Evaluation,[0],[0]
We repeat this investigation with the (3-computer),5 Empirical Evaluation,[0],[0]
"POSysadmin problems, where we allow 100 simulations per time step.",5 Empirical Evaluation,[0],[0]
"In this configuration, the network was fully connected with a failure probability f = 0.1.",5 Empirical Evaluation,[0],[0]
"The (deterministic) observation function is assumed known a priori, but the prior over the transition function is noisy as follows: for each count c, we take the true probability of that transition (called p) and (randomly) either subtract or add .15.",5 Empirical Evaluation,[0],[0]
Note that we do not allow transitions with non-zero probability to fall below 0 by setting those counts to 0.001.,5 Empirical Evaluation,[0],[0]
Each Dirichlet distribution is then normalized the counts to sum to 20.,5 Empirical Evaluation,[0],[0]
"With 3 computers, this results in |S| × |A| = 8× 7 = 56 noisy Dirichlet distributions of |S| = 8 parameters.
",5 Empirical Evaluation,[0],[0]
Figure 1b shows how each method is able to increase its performance over time for POSysadmin.,5 Empirical Evaluation,[0],[0]
"Again, the proposed modifications do not seem to alter the solution quality for a specific number of simulations.
",5 Empirical Evaluation,[0],[0]
BA-POMCP scalability,5 Empirical Evaluation,[0],[0]
"While the previous experiments indicate that the three adaptations produce equally good policies, they do not support any of the efficiency claims made in Section 3.",5 Empirical Evaluation,[0],[0]
"Here, we compare the scalability of BA-POMCP on the POSysadmin problem.",5 Empirical Evaluation,[0],[0]
"The proposed BA-POMCP variants are repeatedly run for 100 episodes on instances of POSysadmin of increasing network size (3 to 10 computers), and we measure the average action selection time required for 1000 simulations.",5 Empirical Evaluation,[0],[0]
"Note that the experiments are capped to allow up to 5 seconds per action selection, demonstrating the problem size that a specific method can perform 1000 simulations in under 5 seconds.
",5 Empirical Evaluation,[0],[0]
"Figure 2 shows that BA-POMCP takes less than 0.5 seconds to perform 1000 simulations on an augmented state with approximately 150 parameters (3 computers), but is quickly unable to solve larger problems, as it requires more than 4 seconds to plan for a BA-POMDP with 200000 counts.",5 Empirical Evaluation,[0],[0]
"BA-POMCP versions with a single adaptation are able to solve the same problems twice as fast, while combinations are able to solve much larger problems with up to 5 million parameters (10 computers).",5 Empirical Evaluation,[0],[0]
"This implies not only that each individual adaptation is able to speed up BA-POMCP, but also that they complement one another.
",5 Empirical Evaluation,[0],[0]
Performance The previous experiments first show that the adaptations do not decrease the policy quality of BAPOMCP and second that the modified BA-POMCP methods improve scalability.,5 Empirical Evaluation,[0],[0]
Here we put those thoughts together and directly consider the performance relative to the action selection time.,5 Empirical Evaluation,[0],[0]
In these experiments we take the average return over multiple repeats of 100 episodes and plot them according to the time required to reach such performance.,5 Empirical Evaluation,[0],[0]
"Here BA-POMCP is also directly compared to the baseline lookahead planner by Ross et al. [13].
",5 Empirical Evaluation,[0],[0]
"First, we apply lookahead with depth 1&2 on the Tiger problem under the same circumstance as the first experiment for increasing number of particles (25, 50, 100, 200 & 500), which determines the runtime.",5 Empirical Evaluation,[0],[0]
"The resulting average episode return is plotted against the action selection time in Figure 3a.
",5 Empirical Evaluation,[0],[0]
The results show that most methods reach near optimal performance after 0.5 seconds action selection time.,5 Empirical Evaluation,[0],[0]
"R-BAPOMCP and E-R-BA-POMCP perform worse than their counterparts BA-POMCP and E-BAPOMCP, which suggests that root sampling of the dynamics actually slows down BA-POMCP slightly.",5 Empirical Evaluation,[0],[0]
"This phenomenon is due to the fact that the Tiger problem is so small, that the overhead of copying the augmented state and re-sampling of dynamics (during
(a) The average return over 100 episodes per action selection time of on the Tiger problem
(b) The average return over 100 episodes per action selection time of BA-POMCP on the POSysadmin problem
STEP function) that root sampling avoids is negligible and does overcome the additional complexity of root sampling.",5 Empirical Evaluation,[0],[0]
"Also note that, even though the Tiger problem is so trivial that a lookahead of depth 1 suffices to solve the POMDP problem optimally, BA-POMCP still consistently outperforms this baseline.
",5 Empirical Evaluation,[0],[0]
The last experiment shows BA-POMCP and lookahead on the POSysadmin domain with 6 computers (which contains 55744 counts) with a failure rate of 0.05.,5 Empirical Evaluation,[0],[0]
The agent was provided with an accurate belief χ.4,5 Empirical Evaluation,[0],[0]
"The results are shown in Figure 3b.
",5 Empirical Evaluation,[0],[0]
We were unable to get lookahead search to solve this problem: the single instance which returned results in a reasonable amount of time (the single dot in the lower right corner) was with a lookahead depth of 1 (which is insufficient for this domain) with just 50 particles.,5 Empirical Evaluation,[0],[0]
"BA-POMCP, however, was able to perform up to 4096 simulations within 5 seconds and reach an average return of approximately −198, utilizing a belief of 1000 particles.",5 Empirical Evaluation,[0],[0]
"The best performing method, L-R-E-BA-POMCP requires less than 2 seconds for similar results, and is able to reach approximately −190 in less than 3 seconds.",5 Empirical Evaluation,[0],[0]
"Finally, we see that each of the individual modifications outperform the original BA-POMCP, where Expected models seems to be the biggest contributor.",5 Empirical Evaluation,[0],[0]
This paper provides a scalable framework for learning in Bayes-Adaptive POMDPs.,6 Conclusion,[0],[0]
"BA-POMDPs give a principled way of balancing exploration and exploiting in RL for POMDPs, but previous solution methods have not scaled to non-trivial domains.",6 Conclusion,[0],[0]
"We extended the Monte Carlo Tree Search method POMCP to BA-POMDPs and described three modifications—Root Sampling, Linking States and Expected Dynamics models— to take advantage of BA-POMDP structure.",6 Conclusion,[0],[0]
We proved convergence of the techniques and demonstrated that our methods can generate high-quality solutions on significantly larger problems than previous methods in the literature.,6 Conclusion,[0],[0]
"Research supported by NSF grant #1664923 and NWO Innovational Research Incentives Scheme Veni #639.021.336.
4 We do not use the same prior as in the first BA-POMCP variants experiments since this gives uninformative results due to the fact that solution methods convergence to the optimal policy with respect to the (noisy) belief, which is different from the one with respect to the true model.",Acknowledgements,[0],[0]
"While RS-BA-POMCP is potentially more efficient, it is not directly clear whether it still converges to an -optimal value function.",A Proof of Theorem 3,[0],[0]
"Here we show that the method is sound by showing that, when using root sampling of the model, the distribution over full histories (including states, actions and observations) will converge in probability to the same distribution when not using this additional root sampling step.",A Proof of Theorem 3,[0],[0]
We will give an concise itemized description of the used notation.,Notation,[0],[0]
• hd is an action-observation history at depth d of a simulation.,Action-observation histories.,[0],[0]
"• hd = (a0,z1, . . .",Action-observation histories.,[0],[0]
",ad−1,zd).
",Action-observation histories.,[0],[0]
‘Full’ histories.,Action-observation histories.,[0],[0]
"In addition to actions and observations, full histories also include the states.",Action-observation histories.,[0],[0]
"• H0 is the (unknown) full history (of real experience) at the root of the simulation: i.e., if there have been k steps of
‘real’ experience H0 = (s−k,a−k,s−k+1,z−k−1, . . .",Action-observation histories.,[0],[0]
",a−1,s0,z0).",Action-observation histories.,[0],[0]
•,Action-observation histories.,[0],[0]
"Hd is a full history (of simulated experience) at depth d in the lookahead tree: Hd = (H0,a0,s1,z1,a1,s2,z2, . . .",Action-observation histories.,[0],[0]
",ad−1,sd,zd) =
(Hd−1,ad−1,sd,zd) = 〈H0,s0:d,hd〉.",Action-observation histories.,[0],[0]
• H(i)d is the full history at depth d corresponding to simulation i. •,Action-observation histories.,[0],[0]
"In our proof, we will also need to indicate if a particular full history",Action-observation histories.,[0],[0]
"Hd is consistent with a full history at the root
of simulation:
Cons(H0,Hd)",Action-observation histories.,[0],[0]
"= { 1 if Hd is consistent with the full history at the root H0 , 0 otherwise.
",Action-observation histories.,[0],[0]
Dynamics Function.,Action-observation histories.,[0],[0]
We fold transition and observations function into one: • D denotes the dynamics model.,Action-observation histories.,[0],[0]
• Dstztst−1at−1,Action-observation histories.,[0],[0]
"= D s′z sa = Dst−1,at−1(st,zt) = D(st,zt|st−1,at−1) =",Action-observation histories.,[0],[0]
"Pr(st,zt|st−1,at−1).
",Action-observation histories.,[0],[0]
"• Dsa denotes the vector: 〈 Ds 1z1 sa , . . .",Action-observation histories.,[0],[0]
",D s|S|z|Z|",Action-observation histories.,[0],[0]
sa 〉 .,Action-observation histories.,[0],[0]
"• χs′zsa denotes how often 〈s′,z〉 occurred after 〈s,a〉.",Counts.,[0],[0]
"• χsa is the vector of counts for 〈s,a〉.",Counts.,[0],[0]
"• χ = 〈χs1a1 , . . .",Counts.,[0],[0]
",χs|S|a|A|〉 is the total collection of all such count vectors.",Counts.,[0],[0]
• χ(Hd) denotes the vector of counts at simulated full history Hd. •,Counts.,[0],[0]
"If χ0 = χ(H0) is the count vector at the root of simulation, we have that χ(Hd) = χ0 + ∆(Hd), with ∆(Hd) the
vector of counts of all (s,a,s′,z) quadruples occurring in Hd since the root of simulation (after H0).",Counts.,[0],[0]
• Let x = 〈x1 . . .,Dirichlet distributions.,[0],[0]
xK〉 ∈ ∆K and α = 〈α1 . .,Dirichlet distributions.,[0],[0]
.,Dirichlet distributions.,[0],[0]
"αK〉 be a count vector, then we write Dir(x|α) = Pr(x;α) = B(α) ∏K i=1",Dirichlet distributions.,[0],[0]
x,Dirichlet distributions.,[0],[0]
"αi−1 i , with B(α) =",Dirichlet distributions.,[0],[0]
"Γ( ∑ i αi)∏
i Γ(αi) the Dirichlet normalization constant, with Γ the gamma function.
",Dirichlet distributions.,[0],[0]
"• So, in translated in terms of dynamics function and counts, we have:
– for a particular s,a: Dir(Dsa|χsa) = Pr(Dsa;χsa) = B(χsa) ∏",Dirichlet distributions.,[0],[0]
"〈s′,z〉∈S×Z ( Ds ′z sa )χs′zsa −1 .
– we will also abuse notation and write Dir(D|χ) =",Dirichlet distributions.,[0],[0]
"∏ 〈s,a〉Dir(Dsa|χsa).",Dirichlet distributions.,[0],[0]
• ẋ denotes a root sampled quantity x. • I{condition} is the indicator function which is 1 iff condition is true and 0 otherwise.,Var.,[0],[0]
Definition 7.,Definitions,[0],[0]
"The expected full-history expected transition BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π.",Definitions,[0],[0]
"It is given by
Pπ(Hd+1) = Dχ(Hd)(sd+1,zd+1|as,sd)π(ad|hd)P π(Hd) (4)
with Pπ(H0) = b0(〈s0,χ0〉)",Definitions,[0],[0]
"the belief ‘now’ (at the root of the online planning).
",Definitions,[0],[0]
Definition 8.,Definitions,[0],[0]
"The empirical full-history root-sampling (RS) BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π in combination with root sampling of the dynamics model D. This distribution, for a particular stage d, is given by
P̃πK(Hd) , 1
Kd Kd∑ i=1",Definitions,[0],[0]
"I{ Hd=H (i) d
}, where
• K is the number of simulations that comprise the empirical distribution.",Definitions,[0],[0]
•,Definitions,[0],[0]
Kd is the number of simulations that reach depth d (not all simulations might be equally long).,Definitions,[0],[0]
"• H(i)d is the history specified by the i-th particle at stage d.
Remark: throughout this proof we assume that there is only 1 initial count vector at the root.",Definitions,[0],[0]
Or put better: we assume that there is one unique H0 at which all simulations start.,Definitions,[0],[0]
"However, for ‘real’ steps t > 0 we could be in different Hrealt all corresponding to the same observed real history h real t .",Definitions,[0],[0]
"In this case, root sampling from the belief can be thought of root sampling the initial full history H0 ∼ b(Hrealt ).",Definitions,[0],[0]
"As such, our proof shows convergence in probability of
∀H0∀Hd P̃πKd(Hd|H0) p→ Pπ(Hd|H0).
",Definitions,[0],[0]
for each such sampled H0.,Definitions,[0],[0]
It is clear that that directly implies that ∀Hd P̃πKd(Hd) = EH0 [ P̃πKd(Hd|H0) ],Definitions,[0],[0]
p→ EH0,Definitions,[0],[0]
[Pπ(Hd|H0)],Definitions,[0],[0]
"= Pπ(Hd).
",Definitions,[0],[0]
"In the below, we omit the explicit conditioning on H0.",Definitions,[0],[0]
"The proof depends on a lemma that follows below.
",Proof of Main Theorem,[0],[0]
Theorem 9.,Proof of Main Theorem,[0],[0]
"The full-history RS-BA-POMDP rollout distribution (Def. 8) converges in probability to full-history BAPOMDP rollout distribution (Def. 7):
∀Hd P̃πKd(Hd) p→ Pπ(Hd).",Proof of Main Theorem,[0],[0]
"(5)
Proof.",Proof of Main Theorem,[0],[0]
For ease of notation we prove this for stage d+ 1.,Proof of Main Theorem,[0],[0]
Note that a history Hd+1 =,Proof of Main Theorem,[0],[0]
"(Hd,ad,sd+1,zd+1), only differs from Hd in that it has one extra transition for the (sd,ad,sd+1,zd+1) quadruple, implying that χ(Hd+1) only differs from χ(Hd) in the counts χsdad for sdad.",Proof of Main Theorem,[0],[0]
"Therefore, the expression for P̃ π",Proof of Main Theorem,[0],[0]
"Kd
(Hd) derived in Lemma 10 below (cf. equation (23)) can be written in recursive form as
P̃π(Hd+1) = Cons(H0,Hd) d∏ t=0 π(at|ht) ∏",Proof of Main Theorem,[0],[0]
"〈s,a〉 B(χsa(H0)) B(χsa(Hd+1))
",Proof of Main Theorem,[0],[0]
"= Cons(H0,Hd) d−1∏ t=0 π(at|ht)π(ad|hd)",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s,a〉 B(χsa(H0)) B(χsa(Hd)) B(χsa(Hd)) B(χsa(Hd+1))",Proof of Main Theorem,[0],[0]
"= Cons(H0,Hd) d−1∏ t=0 π(at|ht)π(ad|hd) ∏",Proof of Main Theorem,[0],[0]
"〈s,a〉 B(χsa(H0)) B(χsa(Hd))",Proof of Main Theorem,[0],[0]
"∏ 〈s,a〉 B(χsa(Hd)) B(χsa(Hd+1))
 =
Cons(H0,Hd) d−1∏ t=0 π(at|ht) ∏",Proof of Main Theorem,[0],[0]
"〈s,a〉 B(χsa(H0)) B(χsa(Hd)) π(ad|hd) B(χsdad(Hd)) B(χsdad(Hd+1))
",Proof of Main Theorem,[0],[0]
"= P̃π(Hd)π(ad|hd) B(χsdad(Hd))
B(χsdad(Hd+1))
",Proof of Main Theorem,[0],[0]
"with base case P̃π(H0) = 1, and
B(χsdad(Hd))
B(χsdad(Hd+1))",Proof of Main Theorem,[0],[0]
"=
B(χsdad(H0))",Proof of Main Theorem,[0],[0]
B(χsdad(Hd+1)) ·,Proof of Main Theorem,[0],[0]
B(χsdad(Hd)),Proof of Main Theorem,[0],[0]
B(χsdad(H0)),Proof of Main Theorem,[0],[0]
= B(χsdad(H0))/B(χsdad(Hd+1)) B(χsdad(H0))/B(χsdad(Hd)),Proof of Main Theorem,[0],[0]
"(6)
the result of dividing out the contribution of the old counts for sdad and multiplying in the new contribution.",Proof of Main Theorem,[0],[0]
"Now, we investigate these terms more closely.
",Proof of Main Theorem,[0],[0]
Again remember that the sole difference between Hd+1 =,Proof of Main Theorem,[0],[0]
"(Hd,ad,sd+1,zd+1) and Hd is that it has one extra transition for the (sd,ad,sd+1,zd+1) quadruple.",Proof of Main Theorem,[0],[0]
"Let us write T = ∑ (s′,z) χ",Proof of Main Theorem,[0],[0]
"s′z sdad
(Hd) for the total of the counts for sd,ad and N = χsd+1zd+1sdad",Proof of Main Theorem,[0],[0]
(Hd) for the number of counts for that such a transition was to (sd+1zd+1).,Proof of Main Theorem,[0],[0]
"Because Hd+1 only has 1 extra transition, we also know that for this history, the total counts is one higher: ∑ (s′,z) χ s′z sdad (Hd+1)",Proof of Main Theorem,[0],[0]
= T,Proof of Main Theorem,[0],[0]
+ 1 and since that transition was to (sd+1zd+1) the counts χ sd+1zd+1 sdad (Hd+1),Proof of Main Theorem,[0],[0]
= N + 1.,Proof of Main Theorem,[0],[0]
"Now let us expand the term from (6):
B(χsdad(Hd))
B(χsdad(Hd+1))",Proof of Main Theorem,[0],[0]
"=
Γ(T )/",Proof of Main Theorem,[0],[0]
∏ s′z Γ(χ s′z sdad,Proof of Main Theorem,[0],[0]
"(Hd))
Γ(T + 1)/ ∏",Proof of Main Theorem,[0],[0]
"s′z Γ(χ s′z sdad (Hd+1))
= Γ(T )
Γ(T",Proof of Main Theorem,[0],[0]
"+ 1)
",Proof of Main Theorem,[0],[0]
"∏ s′z Γ(χ s′z sdad
(Hd+1))∏ s′z Γ(χ s′ sdad (Hd))
= Γ(T )
Γ(T",Proof of Main Theorem,[0],[0]
"+ 1)
Γ(χ sd+1zd+1 sdad (Hd+1))",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
s′z 6=(sd+1zd+1) Γ(χ,Proof of Main Theorem,[0],[0]
"s′z sdad (Hd+1))
Γ(χ sd+1zd+1 sdad (Hd)) ∏",Proof of Main Theorem,[0],[0]
s′z 6=(sd+1zd+1) Γ(χ,Proof of Main Theorem,[0],[0]
"s′z sdad (Hd))
",Proof of Main Theorem,[0],[0]
"= Γ(T )
Γ(T",Proof of Main Theorem,[0],[0]
"+ 1)
Γ(χ sd+1zd+1 sdad (Hd+1))
Γ(χ sd+1zd+1 sdad (Hd))
= Γ(T )
Γ(T",Proof of Main Theorem,[0],[0]
"+ 1)
Γ(N + 1)
Γ(N)
",Proof of Main Theorem,[0],[0]
"Now, the gamma function has the property that Γ(x+ 1) = xΓ(x)",Proof of Main Theorem,[0],[0]
"[4], which means that we get
= Γ(T )
TΓ(T )
NΓ(N) Γ(N) = N T .
",Proof of Main Theorem,[0],[0]
"Therefore we get B(χsdad(Hd))
B(χsdad(Hd+1))",Proof of Main Theorem,[0],[0]
"=
χ sd+1zd+1 sdad (Hd)∑
(s′,z) χ s′z sdad (Hd)
and thus
P̃π(Hd+1) = P̃ π(Hd)π(ad|hd)
χ sd+1zd+1 sdad (Hd)∑
(s′,z) χ s′z sdad (Hd) .",Proof of Main Theorem,[0],[0]
"(7)
the r.h.s.",Proof of Main Theorem,[0],[0]
of this equation is identical to (4) except for the difference in between P̃π(Hd) and Pπ(Hd).,Proof of Main Theorem,[0],[0]
"This can be resolved by forward induction with base step: P̃π(H0) = b0(〈s0,χ0,ψ0〉) = Pπ(H0), and the induction step (show P̃π(Hd+1) = P
π(Hd+1) given P̃π(Hd) = Pπ(Hd)) directly following from (4) and (7).",Proof of Main Theorem,[0],[0]
"Therefore we can conclude that ∀d P̃π(Hd) = Pπ(Hd).
",Proof of Main Theorem,[0],[0]
"Since Lemma 10 establishes that ∀Hd P̃πKd(Hd) p→ P̃π(Hd), we directly have
∀Hd P̃πKd(Hd) p→ Pπ(Hd),
thus proving the result.
",Proof of Main Theorem,[0],[0]
"The proof depends on the following lemma:
Lemma 10.",Proof of Main Theorem,[0],[0]
"The full-history RS-BA-POMDP rollout distribution converges in probability to the following quantity:
∀Hd P̃πKd(Hd) p→ b0(s0) [ d∏ t=1 π(at−1|ht−0) ]∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd)  (8) with B(α) = Γ(α1+...·+αk)Γ(α1)·...·Γ(αk) the normalization term of a Dirichlet distribution with parametric vector α.
",Proof of Main Theorem,[0],[0]
Proof.,Proof of Main Theorem,[0],[0]
"Via the weak law of large numbers, we have that the empirical mean of a random variable converges in probability to its expectation.
",Proof of Main Theorem,[0],[0]
∀Hd P̃πKd(Hd) p→ 1 Kd Kd∑ i=1,Proof of Main Theorem,[0],[0]
I{ Hd=H (i) d } p→ E [ I{ Hd=H (i) d }],Proof of Main Theorem,[0],[0]
"This expectation can be rewritten as follows
E",Proof of Main Theorem,[0],[0]
"[ I{ Hd=H (i) d }] = ∑ H
(i) d
P̃π ( H
(i) d )",Proof of Main Theorem,[0],[0]
"I{ Hd=H (i) d } = P̃π (Hd) (9)
where P̃π(Hd) denotes the (true, non-empirical) probability that the RS-BA-POMDP rollout generates full historyHd.",Proof of Main Theorem,[0],[0]
"This is an expectation over the root sampled model Ḋ:
P̃π(Hd) = ∫",Proof of Main Theorem,[0],[0]
"P̃π ( Hd|Ḋ ) Dir(Ḋ|χ̇)dḊ (10)
= ∫ [ Cons(H0,Hd)
d∏ t=1",Proof of Main Theorem,[0],[0]
"Ḋ(st,zt|st−1,at−1)π(at−1|ht−1)
] Dir(Ḋ|χ̇)dḊ (11)
= Cons(H0,Hd)",Proof of Main Theorem,[0],[0]
"[ d∏ t=1 π(at−1|ht−1) ](∫ [ d∏ t=1 Ḋ(st,zt|st−1,at−1) ]",Proof of Main Theorem,[0],[0]
"Dir(Ḋ|χ̇)dḊ ) (12)
Where Cons(H0,Hd) is a term that indicates whether (takes value 1 if)",Proof of Main Theorem,[0],[0]
"Hd is consistent with the full history at the root H0.5
5An earlier version of this proof ([9]) contained a term b0(s0) instead of Cons(H0,Hd), which fails to recognize that this proof assumes H0 to be fixed.",Proof of Main Theorem,[0],[0]
"See also the remark on page 11.
",Proof of Main Theorem,[0],[0]
"Now we can exploit the fact that only the Dirichlet for the transitions specified by Hd matter.∫ [ d∏ t=1 Ḋ(st,zt|st−1,at−1) ] Dir(Ḋ|χ0)dḊ (13) ={split up the integral over one big vector into integrals over smaller vectors}∫ · · · ∫",Proof of Main Theorem,[0],[0]
"[ d∏
t=1
Ḋst,ztst−1,at−1 ]∏ 〈s,a〉 Dir(Ḋsa|χsa(H0))  dḊs1a1 . . .",Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (14) ={reorder the transition probabilities: ∆sas ′z χ,Proof of Main Theorem,[0],[0]
"(Hd)is the number of occurences of (s,a,s
′,z)in Hd}∫ · · · ∫ ∏
〈s,a〉 ∏ 〈s′,z〉 ( Ḋs ′z sa )",Proof of Main Theorem,[0],[0]
"∆sas′zχ (Hd)∏ 〈s,a〉 Dir(Ḋsa|χsa(H0))  ",Proof of Main Theorem,[0],[0]
dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (15) = ∫ · · · ∫,Proof of Main Theorem,[0],[0]
"∏
〈s,a〉 ∏ 〈s′,z〉 ( Ḋs ′z sa )",Proof of Main Theorem,[0],[0]
"∆sas′zχ (Hd)∏ 〈s,a〉 B(χ̇sa)",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1",Proof of Main Theorem,[0],[0]
dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (16) = ∫ · · · ∫,Proof of Main Theorem,[0],[0]
"∏
〈s,a〉  ∏ 〈s′,z〉 ( Ḋs ′z sa )",Proof of Main Theorem,[0],[0]
∆sas′zχ (Hd)B(χ̇sa) ∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1",Proof of Main Theorem,[0],[0]
dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (17) = ∫ · · · ∫,Proof of Main Theorem,[0],[0]
"∏
〈s,a〉 B(χ̇sa)  ",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )",Proof of Main Theorem,[0],[0]
∆sas′zχ (Hd) ∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1",Proof of Main Theorem,[0],[0]
dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (18) = ∫ · · · ∫,Proof of Main Theorem,[0],[0]
"∏
〈s,a〉 B(χ̇sa)",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1+∆sas′zχ",Proof of Main Theorem,[0],[0]
(Hd) dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (19),Proof of Main Theorem,[0],[0]
"Now we reverse the order of integration and multiplication, which is possible since the different s,a pairs over which we integrate are disjoint.6 We obtain:
= ∏ 〈s,a〉 B(χsa(H0)) ∫",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋsa(s ′,z) )",Proof of Main Theorem,[0],[0]
χsas′z0,Proof of Main Theorem,[0],[0]
+∆sas′zχ,Proof of Main Theorem,[0],[0]
"(Hd)−1 dḊsa (20)
={since we integrate over the entire vector Ḋsa, the integral equals 1/B(χsa(H0) +",Proof of Main Theorem,[0],[0]
"∆saχ (Hd))}∏ 〈s,a〉 B(χsa(H0)) 1 B(χsa(H0) + ∆saχ (Hd))",Proof of Main Theorem,[0],[0]
"(21)
= ∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd)) (22)
Therefore
P̃π(Hd) = Cons(H0,Hd) [ d−1∏ t=0 π(at|ht) ]∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd))  , (23) proving (8).
",Proof of Main Theorem,[0],[0]
"6E.g, consider two sets A1 = { a (1) 1 ,a (2) 1 } and A2 = { a (1) 2 ,a (2) 2 ,a (3) 2 } .",Proof of Main Theorem,[0],[0]
"Equation (19) is of the same form as
∑ a1∈A1 ∑ a2∈A2 2∏ i=1",Proof of Main Theorem,[0],[0]
"ai = ∑ a1∈A1 ∑ a2∈A2 a1a2 = a (1) 1 a (1) 2 + a (1) 1 a (2) 2 + a (1) 1 a (3) 2 + a (2) 1 a (1) 2 + a (2) 1 a (2) 2 + a (2) 1 a (3) 2
= a (1) 1 ( a (1) 2 + a (2) 2 + a (3) 2 ) + a (2) 1 ( a (1) 2 + a (2) 2 + a (3) 2 ) =",Proof of Main Theorem,[0],[0]
( a (1) 1 + a (2) 1 )( a (1) 2 + a (2) 2 + a (3) 2 ),Proof of Main Theorem,[0],[0]
"=
 ∑ a1∈A1",Proof of Main Theorem,[0],[0]
a1  ∑ a2∈A2 a2  = 2∏ i=1 ∑ ai∈Ai ai,Proof of Main Theorem,[0],[0]
"The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult.",abstractText,[0],[0]
Bayes-Adaptive Partially Observable Markov Decision Processes (BAPOMDPs) extend POMDPs to allow the model to be learned during execution.,abstractText,[0],[0]
"BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration.",abstractText,[0],[0]
"Unfortunately, BAPOMDPs are currently impractical to solve for any non-trivial domain.",abstractText,[0],[0]
"In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve.",abstractText,[0],[0]
"Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.",abstractText,[0],[0]
Learning in POMDPs with Monte Carlo Tree Search∗,title,[0],[0]
"With the increasing success of highly non-convex and complex learning architectures such as neural networks, there is an increasing effort to further understand and explain the limits of training such hierarchical structures.
",1. Introduction,[0],[0]
"Recently there have been attempts to draw mathematical insight from kernel methods in order to better understand deep learning, as well as come up with new computationally learnable architectures.",1. Introduction,[0],[0]
"One such line of work consists of learning classifiers that are linear functions of a very large or infinite collection of non-linear functions (Bach,
1University of Princeton, Princeton, New Jersey, USA 2Tel-Aviv University, Tel-Aviv, Israel.",1. Introduction,[0],[0]
"Correspondence to: Roi Livni <rlivni@cs.princeton.edu>, Daniel Carmon <carmonda@mail.tau.ac.il>, Amir Globerson <gamir@mail.tau.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
2014; Daniely et al., 2016; Cho & Saul, 2009; Heinemann et al., 2016; Williams, 1997).",1. Introduction,[0],[0]
"Such models can be interpreted as a neural network with infinitely many nodes in a hidden layer, and we thus refer to them as “Infinite Layer Networks” (ILN).",1. Introduction,[0],[0]
"They are of course also related to kernel based classifiers, as will be discussed later.
",1. Introduction,[0.9999999705909574],"['They are of course also related to kernel based classifiers, as will be discussed later.']"
"A target function in an ILN class will be of the form: x→ ∫ ψ(x;w)f(w)dµ(w), (1)
",1. Introduction,[0],[0]
"Here ψ is some function of the input x and parameters w, and dµ(w) is a prior over the parameter space.",1. Introduction,[0],[0]
"For example, ψ(x;w) can be a single sigmoidal neuron or a complete convolutional network.",1. Introduction,[1.0],"['For example, ψ(x;w) can be a single sigmoidal neuron or a complete convolutional network.']"
"The integral can be thought of as an infinite sum over all such possible networks, and f(w) can be thought of as an infinite output weight vector to be trained.
",1. Introduction,[0.9999999487099299],"['The integral can be thought of as an infinite sum over all such possible networks, and f(w) can be thought of as an infinite output weight vector to be trained.']"
A Standard 1–hidden layer network with a finite set of units can be obtained from the above formalism as follows.,1. Introduction,[1.0],['A Standard 1–hidden layer network with a finite set of units can be obtained from the above formalism as follows.']
"First, choose ψ(x;w) = σ(x ·w) where σ is an activation function (e.g., sigmoid or relu).",1. Introduction,[1.0],"['First, choose ψ(x;w) = σ(x ·w) where σ is an activation function (e.g., sigmoid or relu).']"
"Next, set dµ(w) to be a discrete measure over a finite set w1, . . .",1. Introduction,[0],[0]
",wd.1 In this case, the integral results in a network with d hidden units, and the function f is the linear weights of the output layer.",1. Introduction,[0],[0]
"Namely:
x→ 1 d d∑ i=1 f(wi) · σ(x ·wi).
",1. Introduction,[0],[0]
"The main challenge when training 1–hidden layer networks is of course to find the w1, . . .",1. Introduction,[0],[0]
",wd on which we wish to support our distribution.",1. Introduction,[1.0],"[',wd on which we wish to support our distribution.']"
"It is known (Livni et al., 2014), that due to hardness of learning intersection of halfspaces (Klivans & Sherstov, 2006; Daniely et al., 2014), 1–hidden layer neural networks are computationally hard for a wide class of activation functions.",1. Introduction,[0],[0]
"Therefore, as the last example illustrates, the choice of µ is indeed crucial for performance.
",1. Introduction,[0],[0]
"For a fixed prior µ, the class of ILN functions is highly expressive, since f can be chosen to approximate any 1- hidden layer architecture to arbitrary precision (by setting f to delta functions around the weights of the network, as
1In δ function notation dµ(w)",1. Introduction,[0],[0]
= 1 d ∑d i=1,1. Introduction,[0],[0]
"δ(w −wi)dw
we did above for µ).",1. Introduction,[0],[0]
"However, this expressiveness comes at a cost.",1. Introduction,[0],[0]
"As argued in Heinemann et al. (2016), ILN will generalize well when there is a large probability mass of w parameters that attain a small loss.
",1. Introduction,[0],[0]
The key observation that makes certain ILN tractable to learn is that Eq. 1 is a linear functional in f .,1. Introduction,[0],[0]
In that sense it is a linear classifier and enjoys the rich theory and algorithmic toolbox for such classifiers.,1. Introduction,[0],[0]
"In particular, one can use the fact that linear classifiers can be learned via the kernel trick in a batch (Cortes & Vapnik, 1995) as well as online settings (Kivinen et al., 2004).",1. Introduction,[0],[0]
"In other words, we can reduce learning ILN to the problem of computing the kernel function between two examples.",1. Introduction,[0],[0]
"Specifically the problem reduces to computing integrals of the following form:
k(x1,x2) = ∫ ψ(x1;w) · ψ(x2;w)dµ(w) (2)
= E w̄∼µ",1. Introduction,[0],[0]
[ψ(x1; w̄) · ψ(x2; w̄)] .,1. Introduction,[0],[0]
"(3)
In this work we extend this result to the case where no closed form kernel is available, and thus the kernel trick is not directly applicable.",1. Introduction,[0],[0]
"We thus turn our attention to the setting where features (i.e., w vectors) can be randomly sampled.",1. Introduction,[1.0],"['We thus turn our attention to the setting where features (i.e., w vectors) can be randomly sampled.']"
"In this setting, our main result shows that for the squared loss, we can efficiently learn the above class.",1. Introduction,[1.0],"['In this setting, our main result shows that for the squared loss, we can efficiently learn the above class.']"
"Moreover, we can surprisingly do this with a computational cost comparable to that of methods that have access to the closed form kernel k(x1,x2).
",1. Introduction,[0],[0]
"The observation we begin with is that sampling random features (i.e., w above), leads to an unbiased estimate of the kernel in Eq. 2.",1. Introduction,[0],[0]
"Thus, if for example, we ignore complexity issues and can sample infinitely many w’s, it is not surprising that we can avoid the need for exact computation of the kernel.",1. Introduction,[0],[0]
"However, our results provide a much stronger and practical result.",1. Introduction,[1.0],"['However, our results provide a much stronger and practical result.']"
"Given T training samples, the lower bound on achievable accuracy is O(1/ √ T ) (see Shamir, 2015).",1. Introduction,[1.0],"['Given T training samples, the lower bound on achievable accuracy is O(1/ √ T ) (see Shamir, 2015).']"
"We show that we can in fact achieve this rate, using Õ(T 2) calls2 to the random feature generator.",1. Introduction,[1.0],"['We show that we can in fact achieve this rate, using Õ(T 2) calls2 to the random feature generator.']"
"For comparison, note that O(T 2) is the size of the kernel matrix, and is thus likely to be the cost of any algorithm that uses an explicit kernel matrix, where one is available.",1. Introduction,[0],[0]
"As we discuss later, our approach improves on previous random features based learning (Dai et al., 2014; Rahimi & Recht, 2009) in terms of sample/computational complexity, and expressiveness.",1. Introduction,[0],[0]
We consider algorithms that learn a mapping from input instances x ∈ X to labels y ∈ Y .,2. Problem Setup,[0],[0]
"We focus on the regression case where Y is the interval [−1, 1].",2. Problem Setup,[0],[0]
Our starting point is a class of feature functions ψ(w;x) :,2. Problem Setup,[0],[0]
"Ω×X → R,
2We use Õ notation to suppress logarithmic factors
parametrized by vectors w ∈",2. Problem Setup,[0],[0]
"Ω. The functions ψ(w;x) may contain highly complex non linearities, such as multilayer networks consisting of convolution and pooling layers.",2. Problem Setup,[0],[0]
"Our only assumption on ψ(w;x) is that for all w ∈ Ω and x ∈ X it holds that |ψ(w;x)| < 1.
",2. Problem Setup,[0],[0]
"Given a distribution µ on Ω, we denote by L2(Ω, µ) the class of square integrable functions over Ω.
L2(Ω, µ) =
{ f : ∫ f2(w)dµ(w)",2. Problem Setup,[0],[0]
"<∞ } .
",2. Problem Setup,[0],[0]
We will use functions f ∈,2. Problem Setup,[0],[0]
"L2(Ω, µ) as mixture weights over the class Ω, where each f naturally defines a new regression function from x to R as follows:
x→ ∫ ψ(w;x)f(w)dµ(w).",2. Problem Setup,[0],[0]
"(4)
Our key algorithmic assumption is that the learner can efficiently sample random w",2. Problem Setup,[0],[0]
"according to the distribution µ. Denote the time to generate one such sample by ρ.
",2. Problem Setup,[0],[0]
In what follows it will be simpler to express the integrals as scalar products.,2. Problem Setup,[0],[0]
Define the following scalar product on functions f ∈,2. Problem Setup,[0],[0]
"L2(Ω, µ).
",2. Problem Setup,[0],[0]
"〈f, g〉 = ∫ f(w)g(w)dµ(w) (5)
We denote the corresponding `2 norm by ‖f‖ = √ 〈f, f〉.",2. Problem Setup,[0],[0]
"Also, given features x denote by Φ(x) the function in L2(Ω, µ) given by Φ(x)[w] = ψ(w;x).",2. Problem Setup,[0],[0]
The regression functions we are considering are then of the form x→,2. Problem Setup,[0],[0]
"〈f,Φ(x)〉.
",2. Problem Setup,[0],[0]
"A subclass of norm bounded elements in L2(Ω, µ) induces a natural subclass of regression functions.",2. Problem Setup,[0],[0]
"Namely, we consider the following class:
HBµ = {x→ 〈f,Φ(x)〉 : ‖f‖ < B} .
",2. Problem Setup,[0],[0]
"Our ultimate goal is to output a predictor f ∈ L2(Ω, µ) that is competitive, in terms of prediction, with the best target function in the classHBµ .
",2. Problem Setup,[0],[0]
"We will consider an online setting, and use it to derive generalization bounds via standard online to batch conversion.",2. Problem Setup,[0],[0]
"In our setting, at each round a learner chooses a target function ft ∈ L2(Ω, µ) and an adversary then reveals a sample xt and label yt.",2. Problem Setup,[0],[0]
"The learner then incurs a loss of
`t(ft)",2. Problem Setup,[0],[0]
"= 1
2 (〈ft,Φ(xt)〉 − yt)2 .",2. Problem Setup,[0],[0]
"(6)
The use of squared loss might seem restrictive if one is interested in classification.",2. Problem Setup,[0],[0]
"However, L2 loss is common by now in classification with support vector machines and kernel methods since (Suykens & Vandewalle, 1999; Suykens
et al., 2002).",2. Problem Setup,[0],[0]
"More recently Zhang et al. (2016) showed that when using a large number of features regression achieves performance comparable to the corresponding linear classifiers (see Section 5 therein).
",2. Problem Setup,[0],[0]
"The objective of the learner is to minimize her T round regret w.r.t norm bounded elements in L2(Ω, µ).",2. Problem Setup,[1.0],"['The objective of the learner is to minimize her T round regret w.r.t norm bounded elements in L2(Ω, µ).']"
"Namely:
T∑ t=1",2. Problem Setup,[0],[0]
`t(ft)− min f∗∈HBµ T∑ t=1 `t(f ∗).,2. Problem Setup,[0],[0]
"(7)
In the statistical setting we assume that the sequence S = {(xi, yi)}Ti=1 is generated IID according to some unknown distribution P. We then define the expected loss of a predictor as
L(f) = E (x,y)∼P
[ 1
2 (〈f,Φ(x)〉 − y)2
] .",2. Problem Setup,[0],[0]
(8),2. Problem Setup,[0],[0]
Theorem 1 states our result for the online model.,3. Main Results,[0],[0]
The corresponding result for the statistical setting is given in Corollary 1.,3. Main Results,[0],[0]
"We will elaborate on the structure of the Algorithm later, but first provide the main result.
",3. Main Results,[0],[0]
"Algorithm 1: The SHRINKING GRADIENT algorithm.
",3. Main Results,[0],[0]
"Data: T, B > 1, η,m Result: Weights α(1), . . .",3. Main Results,[0],[0]
", α(T+1) ∈ RT .",3. Main Results,[0],[0]
"Functions
ft ∈ L2(Ω, µ) defined as ft = ∑t i=1",3. Main Results,[0],[0]
"α (t) i Φ(xi);
Initialize α(1) = 0̄ ∈ RT ; for t = 1, . . .",3. Main Results,[0.9549741736999482],"['Functions ft ∈ L2(Ω, µ) defined as ft = ∑t i=1 α (t) i Φ(xi); Initialize α(1) = 0̄ ∈ RT ; for t = 1, .']"
", T do
Observe xt, yt; Set Et = EST SCALAR PROD(α(t),x1:t−1,xt,m); if |Et| < 16B then
α(t+1) = α(t); α
(t+1) t = −η(yt − Et);
else α(t+1) = 14α (t);
Theorem 1.",3. Main Results,[0],[0]
"Run Algorithm 1 with parameters T , B ≥ 1, η = B√
T and m = O
( B4T log (BT ) ) .",3. Main Results,[0],[0]
"Then:
1.",3. Main Results,[0],[0]
"For every sequence of squared losses `1, . . .",3. Main Results,[0],[0]
", `T observed by the algorithm we have for f1, . .",3. Main Results,[0],[0]
.,3. Main Results,[0],[0]
", fT :
E",3. Main Results,[0],[0]
[ T∑ t=1 `t(ft)− min f∗∈HBµ T∑ t=1 `t(f ∗) ],3. Main Results,[0],[0]
"= O(B √ T )
Algorithm 2: EST SCALAR PROD Data: α, x1:t−1, x, m Result:",3. Main Results,[0],[0]
"Estimated scalar product E if α = 0̄ then
Set E = 0 else
for k=1.. . .",3. Main Results,[0],[0]
",m do Sample i from the distribution q(i) =",3. Main Results,[0],[0]
"|αi|∑ |αi| ; Sample parameter w̄ from µ. Set E(k) = sgn(αi)ψ(xi; w̄)ψ(x; w̄);
",3. Main Results,[0],[0]
"Set E = ‖α‖1m ∑m k=1E (k)
2.",3. Main Results,[0],[0]
"The run-time of the algorithm is Õ ( ρB4T 2 ) .3
3.",3. Main Results,[0],[0]
For each t = 1 . . .,3. Main Results,[0],[0]
"T and a new test example x, we can with probability ≥ 1 − δ",3. Main Results,[0],[0]
"estimate 〈ft,Φ(x)〉 within accuracy 0 by running Algorithm 2 with parameters α(t), {xi}ti=1, ,x and m = O(B
4T 20
log 1/δ).",3. Main Results,[0],[0]
"The resulting running time for a test point is then O(ρm).
",3. Main Results,[0],[0]
"We next turn to the statistical setting, where we provide bounds on the expected performance.",3. Main Results,[0],[0]
"Following standard online to batch conversion and Theorem 1 we can obtain the following Corollary (e.g., see Shalev-Shwartz, 2011):
Corollary 1 (Statistical Setting).",3. Main Results,[0],[0]
The following holds for any > 0.,3. Main Results,[0],[0]
"Run Algorithm 1 as in Theorem 1, with T = O(B 2
2 ).",3. Main Results,[0],[0]
"Let S = {(xt, yt)} T t=1, be an IID sample drawn from some unknown distribution P. Let fS = 1T ∑ ft.",3. Main Results,[0],[0]
"Then the expected loss satisfies:
E S∼P",3. Main Results,[0],[0]
[L(fS)],3. Main Results,[0],[0]
"< inf f∗∈HBµ
L(f∗) + .
",3. Main Results,[0],[0]
"The runtime of the algorithm, as well as estimation time on a test example are as defined in Theorem 1.
",3. Main Results,[0],[0]
Proofs of the results are provided in Section 5.1 and the appendix.,3. Main Results,[0],[0]
"Learning with random features can be traced to the early days of learning (Minsky & Papert, 1988), and infinite networks have also been introduced more than 20 years ago (Williams, 1997; Hornik, 1993).",4. Related Work,[0],[0]
"More recent works have considered learning neural nets (also multi-layer) with infinite hidden units using the kernel trick (Cho & Saul, 2009; Deng et al., 2012; Hazan & Jaakkola, 2015; Heinemann et al., 2016).",4. Related Work,[0],[0]
"These works take a similar approach
3Ignoring logarithmic factors in B and T .
to ours but focus on computing the kernel for certain feature classes in order to invoke the kernel trick.",4. Related Work,[0],[0]
Our work in contrast avoids using the kernel trick and applies to any feature class that can be randomly generated.,4. Related Work,[0],[0]
"All the above works are part of a broader effort of trying to circumvent hardness in deep learning by mimicking deep nets through kernels (Mairal et al., 2014; Bouvrie et al., 2009; Bo et al., 2011; 2010), and developing general duality between neural networks and kernels (Daniely et al., 2016).
",4. Related Work,[0],[0]
From a different perspective the relation between random features and kernels has been noted by Rahimi & Recht (2007) who show how to represent translation invariant kernels in terms of random features.,4. Related Work,[0],[0]
"This idea has been further studied (Bach, 2015; Kar & Karnick, 2012) for other kernels as well.",4. Related Work,[0],[0]
"The focus of these works is mainly to allow scaling down of the feature space and representation of the final output classifier.
",4. Related Work,[0],[0]
"Dai et al. (2014) focus on tractability of large scale kernel methods, and their proposed doubly stochastic algorithm can also be used for learning with random features as we have here.",4. Related Work,[0],[0]
"In Dai et al. (2014) the objective considered is of the regularized form:γ2 ‖f‖
2 +R(f), with a corresponding sample complexity of O(1/(γ2 2)) samples needed to achieve approximation with respect to the risk of the optimum of the regularized objective.
",4. Related Work,[0],[0]
"To relate the above results to ours, we begin by emphasizing that the bound in (Dai et al., 2014) holds for fixed γ, and refers to optimization of the regularized objective.",4. Related Work,[0],[0]
"Our objective is to minimize the risk R(f) which is the expected squared loss, for which we need to choose γ = O( B2 ) in order to attain accuracy (Sridharan et al., 2009).",4. Related Work,[0],[0]
"Plugging this γ into the generalization bound in Dai et al. (2014) we obtain that the algorithm in Dai et al. (2014) needs O(B 4
4 ) samples to compete with the optimal target function in the B-ball.",4. Related Work,[0],[0]
"Our algorithm needs O(B 2
2 ) examples which is considerably better.",4. Related Work,[0],[0]
"We note that their method does extend to a larger class of losses, whereas our is restricted to the quadratic loss.
",4. Related Work,[0],[0]
"In Rahimi & Recht (2009), the authors consider embedding the domain into the feature space x →",4. Related Work,[0],[0]
"[ψ(w1;x), . . .",4. Related Work,[0],[0]
", ψ(wm;x)], where wi are IID random variables sampled according to some prior µ(w).",4. Related Work,[0],[0]
"They show that with O(B 2 log 1/δ 2 ) random features estimated on O(B 2 log 1/δ 2 ) samples they can compete with the class:
",4. Related Work,[0],[0]
"HBµ max = { x→ ∫ ψ(w;x)f(w)dµ(w) : |f(w)| ≤ B } Our algorithm relates to the mean square error cost function which does not meet the condition in Rahimi & Recht (2009), and is hence formally incomparable.",4. Related Work,[0],[0]
"Yet we can invoke our algorithm to compete against a larger class
of target functions.",4. Related Work,[0],[0]
"Our main result shows that Algorithm 1, using Õ(B 8 4 ) estimated features and using O( B2
2 ) samples will, in expectation, output a predictor that is close to the best in HBµ .",4. Related Work,[0],[0]
Note that |f(w)| < B implies Ew∼µ(f2(w)),4. Related Work,[0],[0]
< B2.,4. Related Work,[0],[0]
Hence HBµ max ⊆ H B µ .,4. Related Work,[0],[0]
"Note however, that the number of estimated features (as a function of B) is worse in our case.
",4. Related Work,[0],[0]
Our approach to the problem is to consider learning with a noisy estimate of the kernel.,4. Related Work,[0.9531783789300736],['A classic approach to the problem is to exploit the OGD algorithm.']
"A related setting was studied in Cesa-Bianchi et al. (2011b), where the authors considered learning with kernels when the data is corrupted.",4. Related Work,[0],[0]
Noise in the data and noise in the scalar product estimation are not equivalent when there is non-linearity in the kernel space embedding.,4. Related Work,[0],[0]
"There is also extensive research on linear regression with actively chosen attributes (CesaBianchi et al., 2011a; Hazan & Koren, 2012).",4. Related Work,[0],[0]
The convergence rates and complexity of the algorithms are dimension dependent.,4. Related Work,[0],[0]
It would be interesting to see if their method can be extended from finite set of attributes to a continuum set of attributes.,4. Related Work,[0],[0]
"We next turn to present Algorithm 1, from which our main result is derived.",5. Algorithm,[0],[0]
"The algorithm is similar in spirit to Online Gradient Descent (OGD) (Zinkevich, 2003), but with some important modifications that are necessary for our analysis.
",5. Algorithm,[0],[0]
"We first introduce the problem in the terminology of online convex optimization, as in Zinkevich (2003).",5. Algorithm,[1.0],"['We first introduce the problem in the terminology of online convex optimization, as in Zinkevich (2003).']"
At iteration t our algorithm outputs a hypothesis ft.,5. Algorithm,[1.0],['At iteration t our algorithm outputs a hypothesis ft.']
"It then receives as feedback (xt, yt), and suffers a loss `t(ft) as in Eq. 6.",5. Algorithm,[0],[0]
"The objective of the algorithm is to minimize the regret against a benchmark of B-bounded functions, as in Eq. 7.
",5. Algorithm,[0],[0]
A classic approach to the problem is to exploit the OGD algorithm.,5. Algorithm,[0],[0]
"Its simplest version would be to update ft+1 → ft − η∇t where η is a step size, and ∇t is the gradient of the loss w.r.t.",5. Algorithm,[0],[0]
f at ft.,5. Algorithm,[0],[0]
"In our case,∇t is given by:
∇t = (〈ft,Φ(xt)〉 − yt) Φ(xt) (9)
",5. Algorithm,[0],[0]
Applying this update would also result in a function ft =∑t i=1 αiΦ(xt) as we have in Algorithm 1 (but with different αi from ours).,5. Algorithm,[0],[0]
"However, in our setting this update is not applicable since the scalar product 〈ft,Φ(xt)〉 is not available.",5. Algorithm,[0],[0]
One alternative is to use a stochastic unbiased estimate of the gradient that we denote by ∇̄t.,5. Algorithm,[0],[0]
This induces an update step ft+1 → ft − η∇̄t.,5. Algorithm,[0],[0]
"One can show that OGD with such an estimated gradient enjoys the following upper bound on the regret E [ ∑ `t(ft)− `t(f∗)] for every ‖f∗‖ ≤ B (e.g., see Shalev-Shwartz, 2011):
B2
η + η T∑ i=1",5. Algorithm,[0],[0]
E,5. Algorithm,[0],[0]
[ ‖∇t‖2 ] + η T∑ i=1,5. Algorithm,[0],[0]
"V [ ∇̄t ] , (10)
where V [ ∇̄t ]",5. Algorithm,[0],[0]
= E [ ‖∇̄t −∇t‖2 ] .,5. Algorithm,[0],[0]
"We can bound the first two terms using standard techniques applicable for the squared loss (e.g., see Zhang, 2004; Srebro et al., 2010).",5. Algorithm,[0],[0]
The third term depends on our choice of gradient estimate.,5. Algorithm,[0],[0]
"There are various choices for such an estimate, and we use a version which facilitates our analysis, as explained below.",5. Algorithm,[0],[0]
"Assume that at iteration t, our function ft is given by ft =∑t i=1",5. Algorithm,[0],[0]
α (t) i Φ(xt).,5. Algorithm,[0],[0]
"We now want to use sampling to obtain an unbiased estimate of 〈ft,Φ(xt)〉.",5. Algorithm,[0],[0]
"This will be done via a two step sampling procedure, as described in Algorithm 2.",5. Algorithm,[0],[0]
"First, sample an index",5. Algorithm,[0],[0]
i ∈,5. Algorithm,[0],[0]
"[1, . . .",5. Algorithm,[0],[0]
", t] by sampling according to the distribution q(i) ∝ |α(t)i",5. Algorithm,[0],[0]
|.,5. Algorithm,[0],[0]
"Next, for the chosen i, sample w̄ according to µ, and use ψ(x; w̄)ψ(xi; w̄) to construct an estimate of 〈Φ(xi),Φ(xt)〉.",5. Algorithm,[1.0],"['Next, for the chosen i, sample w̄ according to µ, and use ψ(x; w̄)ψ(xi; w̄) to construct an estimate of 〈Φ(xi),Φ(xt)〉.']"
"The resulting unbiased estimate of 〈Φ(xi),Φ(xt)〉 is denoted by Et and given by:
Et = ‖α(t)‖1 m m∑ i=1",5. Algorithm,[0],[0]
"sgn(α(t)i )ψ(xi; w̄)ψ(xt; w̄) (11)
",5. Algorithm,[0],[0]
"The corresponding unbiased gradient estimate is:
∇̄t = (Et − yt)xt (12)
",5. Algorithm,[0],[0]
The variance of ∇̄ affects the convergence rate and depends on both ‖α‖1 and the number of estimationsm.,5. Algorithm,[0],[0]
"We wish to maintain m = O(T ) estimations per round, while achieving O( √ T ) regret.
",5. Algorithm,[0],[0]
"To effectively regularize ‖α‖1, we modify the OGD algorithm so that whenever Et is larger then 16B, we do not perform the usual update.",5. Algorithm,[1.0],"['To effectively regularize ‖α‖1, we modify the OGD algorithm so that whenever Et is larger then 16B, we do not perform the usual update.']"
"Instead, we perform a shrinking step that divides α(t) (and hence ft) by 4.",5. Algorithm,[0],[0]
"Treating B as constant, this guarantees that ‖α‖1 = O(ηT ), and in turn Var(∇̄t) = O(η 2T 2 m ).",5. Algorithm,[1.0],"['Treating B as constant, this guarantees that ‖α‖1 = O(ηT ), and in turn Var(∇̄t) = O(η 2T 2 m ).']"
"Setting η = O(1/ √ T ), we have that m = O(T ) estimations are sufficient.
",5. Algorithm,[0],[0]
"The rationale for the shrinkage is that wheneverEt is large, it indicates that ft is “far away” from the B-ball, and a shrinkage step, similar to projection, brings ft closer to the optimal element in the B-ball.",5. Algorithm,[0],[0]
"However, due to stochasticity, the shrinkage step does add a further term to the regret bound that we would need to take care of.",5. Algorithm,[0],[0]
"In what follows we analyze the regret for Algorithm 1, and provide a high level proof of Theorem 1.",5.1. Analysis,[1.0],"['In what follows we analyze the regret for Algorithm 1, and provide a high level proof of Theorem 1.']"
The appendix provides the necessary lemmas and a more detailed proof.,5.1. Analysis,[0],[0]
"We begin by modifying the regret bound for OGD in Eq. 10 to accommodate for steps that differ from the standard gradient update, such as shrinkage.",5.1. Analysis,[0],[0]
"We use the following notation for the regret at iteration t:
Rt(f ∗)",5.1. Analysis,[0],[0]
= E,5.1. Analysis,[0],[0]
[ T∑ t=1 `t(ft)− `t(f∗) ],5.1. Analysis,[0],[0]
"(13)
Lemma 1.",5.1. Analysis,[0],[0]
"Let `1, . . .",5.1. Analysis,[0],[0]
", `T be an arbitrary sequence of convex loss functions, and let f1, . . .",5.1. Analysis,[0],[0]
", fT be random vectors, produced by an online algorithm.",5.1. Analysis,[0],[0]
Assume ‖fi‖ ≤,5.1. Analysis,[0],[0]
BT for all i ≤ T .,5.1. Analysis,[0],[0]
For each t let ∇̄t be an unbiased estimator of ∇`t(ft).,5.1. Analysis,[1.0],['For each t let ∇̄t be an unbiased estimator of ∇`t(ft).']
"Denote f̂t = ft−1 − η∇̄t−1 and let
Pt(f ∗)",5.1. Analysis,[0],[0]
= P [ ‖ft,5.1. Analysis,[0],[0]
− f∗‖ > ‖f̂t − f∗‖ ] .,5.1. Analysis,[0],[0]
"(14)
",5.1. Analysis,[0],[0]
"For every ‖f∗‖ ≤ B it holds that :
Rt(f ∗) ≤",5.1. Analysis,[0],[0]
"B
2
η + η T∑ t=1",5.1. Analysis,[0],[0]
E,5.1. Analysis,[0],[0]
[ ‖∇t‖2 ],5.1. Analysis,[0],[0]
+ η T∑ t=1 V [ ∇̄t ],5.1. Analysis,[0],[0]
"+
T∑ t=1",5.1. Analysis,[0],[0]
(BT +B),5.1. Analysis,[0],[0]
2 η E,5.1. Analysis,[0],[0]
[Pt(f∗)],5.1. Analysis,[0],[0]
"(15)
See Appendix B.1 for proof of the lemma.",5.1. Analysis,[0],[0]
"As discussed earlier, the first three terms on the RHS are the standard bound for OGD from Eq. 10.",5.1. Analysis,[0],[0]
"Note that in the standard OGD it holds that ft = f̂t, and therefore Pt(f∗) = 0",5.1. Analysis,[0],[0]
"and the last term disappears.
",5.1. Analysis,[0],[0]
The third term will be bounded by controlling ‖α‖1.,5.1. Analysis,[0],[0]
The last term Pt(f∗) is a penalty that results from updates that stir ft away from the standard update step f̂t.,5.1. Analysis,[0],[0]
This will indeed happen for the shrinkage step.,5.1. Analysis,[0],[0]
The next lemma bounds this term.,5.1. Analysis,[0],[0]
"See Appendix B.2 for proof.
",5.1. Analysis,[0],[0]
Lemma 2.,5.1. Analysis,[0],[0]
"Run Algorithm 1 with parameters T , B ≥ 1 and η < 1/8.",5.1. Analysis,[0],[0]
Let ∇̄t be the unbiased estimator of ∇`t(ft) of the form ∇̄t =,5.1. Analysis,[0],[0]
(Et − yt)Φ(xt).,5.1. Analysis,[0],[0]
Denote f̂t = ft − η∇̄t and define Pt(f∗) as in Eq. 14.,5.1. Analysis,[0],[0]
"Then:
Pt(f ∗)",5.1. Analysis,[0],[0]
"≤ 2 exp ( − m
(3ηt)2
)
",5.1. Analysis,[0],[0]
The following lemma (see Appendix B.3 for proof) bounds the second and third terms of Eq. 15.,5.1. Analysis,[0],[0]
Lemma 3.,5.1. Analysis,[0],[0]
Consider the setting as in Lemma 2.,5.1. Analysis,[0],[0]
Then V [ ∇̄t ] ≤ ((16B+1)ηt) 2 m and E,5.1. Analysis,[0],[0]
[ ‖∇t‖2 ] ≤ 2E,5.1. Analysis,[0],[0]
"[`t(ft)].
",5.1. Analysis,[0],[0]
"Proof of Theorem 1 Combining Lemmas 1, 2 and 3 and rearranging we get:
(1− 2η)E [Rt(f∗)]",5.1. Analysis,[0],[0]
"≤ B2
η + 2η T∑ t=1 `t(f ∗)",5.1. Analysis,[0],[0]
"+ (16)
η ((16B + 1)ηT )2T
m +
(BT +B) 2
η
T∑ t=1 Pt(f ∗)
To bound the second term in Eq. 16",5.1. Analysis,[0],[0]
"we note that:
min ‖f∗‖<B T∑ t=1 `t(f ∗) ≤",5.1. Analysis,[0],[0]
T∑ t=1 `t(0) ≤,5.1. Analysis,[0],[0]
"T. (17)
We next set η and m as in the statement of the theorem.",5.1. Analysis,[0],[0]
"Namely: η = B
2 √ T , andm = ((16B+1)B)2T log γ, where γ = max (
((16B+1)ηT+B)2) η2 , e
) .",5.1. Analysis,[0],[0]
"This choice of m implies
that m > ((16B + 1)ηT )2, and hence the third term in Eq.",5.1. Analysis,[0],[0]
"16 is upper bounded by T .
",5.1. Analysis,[0],[0]
"Next we have that m > (3ηt)2 log γ for every t, and by the bound on BT we have that γ > (B+BT ) 2
η2 .",5.1. Analysis,[0],[0]
"Taken together with Lemma 2 we have that:
(BT +B) 2
η
T∑ t=1",5.1. Analysis,[0],[0]
"Pt(f ∗) ≤ ηT. (18)
",5.1. Analysis,[0],[0]
"The above bounds imply that:
(1− 2η)E [Rt(f∗)]",5.1. Analysis,[0],[0]
"≤ B2
η + 2ηT + ηT + ηT
Finally by choice of η, and dividing both sides by (1− 2η) we obtain the desired result.",5.1. Analysis,[0],[0]
In this section we provide a toy experiment to compare our Shrinking Gradient algorithm to other random feature based methods.,6. Experiments,[0],[0]
"In particular, we consider the following three algorithms: Fixed-Random: Sample a set of r features w1, . . .",6. Experiments,[0],[0]
",wr and evaluate these on all the train and test points.",6. Experiments,[0],[0]
"Namely, all x points will be evaluated on the same features.",6. Experiments,[0],[0]
This is the standard random features approach proposed in Rahimi & Recht (2007; 2009).,6. Experiments,[0],[0]
"Doubly Stochastic Gradient Descent (Dai et al., 2014):",6. Experiments,[0],[0]
"Here each training point x samples k features w1, . . .",6. Experiments,[0],[0]
",wk.",6. Experiments,[0],[0]
These features will from that point on be used for evaluating dot products with x.,6. Experiments,[0],[0]
"Thus, different x points will use different features.",6. Experiments,[0],[0]
Shrinking Gradient:,6. Experiments,[0],[0]
This is the approach proposed here in Section 3.,6. Experiments,[0],[0]
"Namely, each training point x samples m features in order to calculate the dot product with the current regression function.
",6. Experiments,[0],[0]
"In comparing the algorithms we choose r, k,m so that the same overall number of features is calculated.",6. Experiments,[0],[0]
"For all methods we explored different initial step sizes and schedules for changing the step size.
",6. Experiments,[0],[0]
The key question in comparing the three algorithms is how well they use a given budget of random features.,6. Experiments,[0],[0]
To explore this we perform an experiments to simulate the high dimensional feature case.,6. Experiments,[0],[0]
"We consider vectors x ∈ RD, where a random feature w corresponds to a uniform choice of coordinate w in x.",6. Experiments,[0],[0]
"We work in the regime where D is large in the sense that D > T , where T is the size of the training data.",6. Experiments,[0],[0]
Thus random sampling of T features will not reveal all coordinates of x.,6. Experiments,[0],[0]
The training set is generated as follows.,6. Experiments,[0],[0]
"First, a training set x1, . . .",6. Experiments,[0],[0]
",xT ∈",6. Experiments,[0],[0]
"RD is
sampled from a standard Gaussian.",6. Experiments,[0],[0]
"We furthermore clip negative values to zero, in order to make the data sparser and more challenging for feature sampling.",6. Experiments,[0],[0]
Next a weight vector a ∈,6. Experiments,[0],[0]
RD is chosen as a random sparse linear combination of the training points.,6. Experiments,[0],[0]
This is done in order for the true function to be in the corresponding RKHS.,6. Experiments,[0],[0]
"Finally, the training set is labeled using yi = a · xi.
",6. Experiments,[0],[0]
During training we do not assume that the algorithms have access to x.,6. Experiments,[0],[0]
"Rather they can uniformly sample coordinates from it, which mimics our setting of random features.",6. Experiments,[0],[0]
"For the experiment we take D = 550, 600, . . .",6. Experiments,[0],[0]
", 800 and T = 200.",6. Experiments,[0],[0]
"All algorithms perform one pass over the data, to emulate the online regret setting.",6. Experiments,[0],[0]
The results shown in Figure 1 show that our method indeed achieves a lower loss while working with the same feature budget.,6. Experiments,[0],[0]
We presented a new online algorithm that employs kernels implicitly but avoids the kernel trick assumption.,7. Discussion,[0],[0]
"Namely, the algorithm can be invoked even when one has access to only estimations of the scalar product.",7. Discussion,[0],[0]
"The problem was motivated by kernels resulting from neural nets, but it can of course be applied to any scalar product of the form we described.",7. Discussion,[0],[0]
"As an example of an interesting extension, consider a setting where a learner can observe an unbiased estimate of a coordinate in a kernel matrix, or alternatively the scalar product between any two observations.",7. Discussion,[0],[0]
"Our results imply that in this setting the above rates are applicable, and at least for the square loss, having no access to the true values in the kernel matrix is not necessarily prohibitive during training.
",7. Discussion,[0],[0]
"The results show that with sample size T we can achieve error of O( B√
T ).",7. Discussion,[0],[0]
"As demonstrated in Shamir (2015) these
rates are optimal, even when the scalar product is computable.",7. Discussion,[0],[0]
To achieve this rate our algorithm needs to perform Õ(B4T 2) scalar product estimations.,7. Discussion,[0],[0]
"When the scalar product can be computed, existing kernelized algorithms need to observe a fixed proportion of the kernel matrix, hence they observe order of Ω(T 2) scalar products.",7. Discussion,[0],[0]
"In Cesa-Bianchi et al. (2015) it was shown that when the scalar product can be computed exactly, one would need access to at least Ω(T ) entries to the kernel matrix.",7. Discussion,[0],[0]
It is still an open problem whether one has to access Ω(T 2) entries when the kernel can be computed exactly.,7. Discussion,[0],[0]
"However, as we show here, for fixed B even if the kernel can only be estimated Õ(T 2) estimations are enough.",7. Discussion,[0],[0]
"It would be interesting to further investigate and improve the performance of our algorithm in terms of the norm bound B.
To summarize, we have shown that the kernel trick is not strictly necessary in terms of sample complexity.",7. Discussion,[0],[0]
"Instead, simply sampling random features via our proposed algorithm results in a similar sample complexity.",7. Discussion,[0],[0]
"Recent empirical results by Zhang et al. (2016) show that using a large number of random features and regression comes close to the performance of the first successful multilayer CNNs (Krizhevsky et al., 2012) on CIFAR-10.",7. Discussion,[0],[0]
"Although deep learning architectures still substantially outperform random features, it is conceivable that with the right choice of random features, and scalable learning algorithms like we present here, considerable improvement in performance is possible.",7. Discussion,[0],[0]
In this section we provide concentration bounds for the estimation procedure in Algorithm 2.,A. Estimation Concentration Bounds,[0],[0]
Lemma 4.,A. Estimation Concentration Bounds,[0],[0]
"Run Algorithm 2 with α and, {xi}Ti=1, x, and m. Let f = ∑ αiΦ(xi).",A. Estimation Concentration Bounds,[0],[0]
Assume that |ψ(x;w)| < 1 for all w and x.,A. Estimation Concentration Bounds,[0],[0]
Let E be the output of Algorithm 2.,A. Estimation Concentration Bounds,[0],[0]
"Then E is an unbiased estimator for 〈f,Φ(x)〉 and:
P [|E − 〈f,Φ(x)〉| > ] ≤ exp ( − m 2
‖α‖21
) (19)
Proof.",A. Estimation Concentration Bounds,[0],[0]
Consider the random variables ‖α‖1E(k),A. Estimation Concentration Bounds,[0],[0]
(where E(k) is as defined in Algorithm 2) and note that they are IID.,A. Estimation Concentration Bounds,[0],[0]
One can show that E [ ‖α‖1E(k) ],A. Estimation Concentration Bounds,[0],[0]
"=∑
αiE",A. Estimation Concentration Bounds,[0],[0]
"[ψ(xi;w)ψ(x;w)] = 〈f,Φ(x)〉.",A. Estimation Concentration Bounds,[0],[0]
By the bound on ψ(x;w),A. Estimation Concentration Bounds,[0],[0]
we have that ∣∣‖α‖1E(k)∣∣ < ‖α‖1 with probability 1.,A. Estimation Concentration Bounds,[0],[0]
"Since E = 1m ∑ E(k) the result follows directly from Hoeffding’s inequality.
",A. Estimation Concentration Bounds,[0],[0]
"Next, we bound the α(t) coeffcients and obtain a concentration bound for the estimated dot product Et.",A. Estimation Concentration Bounds,[0],[0]
Lemma 5.,A. Estimation Concentration Bounds,[0],[0]
"The α(t) obtained in Algorithm 1 satisfies:
‖α(t)‖1 ≤ (16B + 1)ηt.
",A. Estimation Concentration Bounds,[0],[0]
"As a corollary of this and Lemma 4 we have that the function ft satisfies:
",A. Estimation Concentration Bounds,[0],[0]
P,A. Estimation Concentration Bounds,[0],[0]
"[|Et − 〈ft,Φ(xt)〉| >",A. Estimation Concentration Bounds,[0],[0]
"] ≤ exp ( −
2m
((16B + 1)ηt)2 ) (20)
Proof.",A. Estimation Concentration Bounds,[0],[0]
We prove the statement by induction.,A. Estimation Concentration Bounds,[0],[0]
"We separate into two cases, depending on whether the shrinkage step was performed or not.
",A. Estimation Concentration Bounds,[0],[0]
"If |Et| ≥ 16B the algorithm sets α(t+1) = 14α (t), and:
‖α(t+1)‖1 = 1
4 ‖α(t)‖1 ≤ (16B + 1)η(t+ 1)
",A. Estimation Concentration Bounds,[0],[0]
If |Et| < 16B the gradient update is performed.,A. Estimation Concentration Bounds,[0],[0]
Since |yt| ≤ 1 we have that |Et,A. Estimation Concentration Bounds,[0],[0]
−,A. Estimation Concentration Bounds,[0],[0]
"yt| < 16B + 1 and:
‖α(t+1)‖1 ≤ ‖α(t)‖1 + η|Et",A. Estimation Concentration Bounds,[0],[0]
− yi| ≤ (16B + 1)η(t+ 1).,A. Estimation Concentration Bounds,[0],[0]
B.1.,B. Proofs of Lemmas,[0],[0]
"Proof of Lemma 1
First, by convexity we have that
2(`t(ft)− `t(f∗))",B. Proofs of Lemmas,[0],[0]
"≤ 2 〈∇t, ft − f∗〉 .",B. Proofs of Lemmas,[0],[0]
"(21)
",B. Proofs of Lemmas,[0],[0]
"Next we upper bound 〈∇t, ft − f∗〉.",B. Proofs of Lemmas,[0],[0]
Denote by E the event ‖ft+1,B. Proofs of Lemmas,[0],[0]
− f∗‖ > ‖f̂t+1,B. Proofs of Lemmas,[0],[0]
"− f∗‖. Note that:
E [ ‖ft+1",B. Proofs of Lemmas,[0],[0]
− f∗‖2 ] ≤ E [ ‖f̂t+1,B. Proofs of Lemmas,[0],[0]
"− f∗‖2 ] +
E [ ‖ft+1",B. Proofs of Lemmas,[0],[0]
− f∗‖2 ∣∣E] · Pt+1(f∗) ≤ E,B. Proofs of Lemmas,[0],[0]
"[ ‖f̂t+1 − f∗‖2 ] + (B +BT ) 2Pt+1(f ∗)
",B. Proofs of Lemmas,[0],[0]
"Plugging in f̂t+1 = ft − η∇̄t, summing over t and using Eq. 21 and E",B. Proofs of Lemmas,[0],[0]
"[ ‖∇̄t‖2 ] = E [ ‖∇t‖2 ] + V [ ∇̄t ] , we obtain the desired result.
",B. Proofs of Lemmas,[0],[0]
B.2.,B. Proofs of Lemmas,[0],[0]
"Proof for Lemma 2
To prove the bound in the lemma, we first bound the event Pt(f ∗)",B. Proofs of Lemmas,[0],[0]
"w.r.t to two possible events:
Lemma 6.",B. Proofs of Lemmas,[0],[0]
Consider the setting as in Lemma 2.,B. Proofs of Lemmas,[0],[0]
"Run Algorithm 1 and for each t consider the following two events:
• Et1 : |Et| > 16B and |Et| > 14η‖ft‖.
• Et2 : |Et| > 16B and ‖ft‖ < 8B.
For every ‖f∗‖",B. Proofs of Lemmas,[0],[0]
<,B. Proofs of Lemmas,[0],[0]
B,B. Proofs of Lemmas,[0],[0]
we have that Pt(f∗),B. Proofs of Lemmas,[0],[0]
< P,B. Proofs of Lemmas,[0],[0]
"[Et1 ∪ Et2].
",B. Proofs of Lemmas,[0],[0]
Proof.,B. Proofs of Lemmas,[0],[0]
Denote the event |Et| > 16B by Et0.,B. Proofs of Lemmas,[0],[0]
"Note that if Et0 does not happen, then ft = f̂t.",B. Proofs of Lemmas,[0],[0]
"Hence trivially
Pt(f ∗)",B. Proofs of Lemmas,[0],[0]
= P [ ‖ft,B. Proofs of Lemmas,[0],[0]
− f∗‖ > ‖f̂t − f∗‖ ∧ Et0 ],B. Proofs of Lemmas,[0],[0]
"We will assume that: (1) |Et| > 16B., (2) |Et| < 14η‖ft‖., (3) ‖ft‖",B. Proofs of Lemmas,[0],[0]
>,B. Proofs of Lemmas,[0],[0]
8B.,B. Proofs of Lemmas,[0],[0]
"We then show ‖ft+1−f∗‖ ≤ ‖f̂t+1−f∗‖.
",B. Proofs of Lemmas,[0],[0]
"In other words, we will show that if Et0 happens and ‖ft+1− f∗‖ > ‖f̂t+1",B. Proofs of Lemmas,[0],[0]
"− f∗‖, then either Et2 or Et1 happened.",B. Proofs of Lemmas,[0],[0]
"This will conclude the proof.
",B. Proofs of Lemmas,[0],[0]
"Fix t, note that since |ψ(x;w)| < 1 we have that ‖Φ(x)‖ < 1.",B. Proofs of Lemmas,[0],[0]
"We then have:
‖f̂t+1‖ = ‖ft",B. Proofs of Lemmas,[0],[0]
− η(Et,B. Proofs of Lemmas,[0],[0]
"− y)Φ(xt)‖ (22)
≥ ‖ft‖ −",B. Proofs of Lemmas,[0],[0]
"η|Et| − η ≥ 3
4 ‖ft‖",B. Proofs of Lemmas,[0],[0]
"− η
where the last inequality is due to assumption (2) above.",B. Proofs of Lemmas,[0],[0]
"We therefore have the following for every ‖f∗‖ < B:
‖f̂t+1",B. Proofs of Lemmas,[0],[0]
"− f∗‖ ≥ 3
4 ‖ft‖",B. Proofs of Lemmas,[0],[0]
"− η −B
On the other hand, if ft+1 6= f̂t+1 then by construction of the algorithm ft+1 = 14ft:
‖ft+1",B. Proofs of Lemmas,[0],[0]
− f∗‖ ≤ ‖ft+1‖+ ‖f∗‖ ≤,B. Proofs of Lemmas,[0],[0]
"‖ft‖
4 +B.
",B. Proofs of Lemmas,[0],[0]
Next note that η < 2B and assumption (3) states ‖ft‖ >,B. Proofs of Lemmas,[0],[0]
"8B. Therefore: 12‖ft‖ > 4B > η + 2B, and:
‖f̂t+1",B. Proofs of Lemmas,[0],[0]
"− f∗‖ ≥ 3
4 ‖ft‖",B. Proofs of Lemmas,[0],[0]
"− η −B
= 1
4 ‖ft‖+
( 1
2 ‖ft‖",B. Proofs of Lemmas,[0],[0]
− η,B. Proofs of Lemmas,[0],[0]
"− 2B
)",B. Proofs of Lemmas,[0],[0]
"+B
≥ 1 4 ‖ft‖+B ≥ ‖ft+1",B. Proofs of Lemmas,[0],[0]
"− f∗‖
Next we upper bound P [Et1 ∪ Et2].",B. Proofs of Lemmas,[0],[0]
In what follows the superscript t is dropped.,B. Proofs of Lemmas,[0],[0]
"A bound for P [E1 ∩ Ec2 ]: Assume that
|Et − 〈ft,Φ(xt)〉|",B. Proofs of Lemmas,[0],[0]
<,B. Proofs of Lemmas,[0],[0]
"( 1
4η − 1)8B.
We assume T is sufficiently large and η < 18 .",B. Proofs of Lemmas,[0],[0]
We have 1 4η − 1 > 1.,B. Proofs of Lemmas,[0],[0]
"Since we assume E2 did not happen we must have ‖ft‖ > 8B and |Et − 〈ft,Φ(xt)〉| <",B. Proofs of Lemmas,[0],[0]
"( 14η − 1)‖f‖, and therefore:
Et − ‖f‖ <",B. Proofs of Lemmas,[0],[0]
"|Et − 〈ft,Φ(xt)〉| <",B. Proofs of Lemmas,[0],[0]
"( 1
4η − 1)‖f‖.
Which implies Et < 14η‖f‖, and we get that E1 did not happen.",B. Proofs of Lemmas,[0],[0]
"We conclude that if E1 and not E2 then:
|Et − 〈ft,Φ(xt)〉| ≥ ( 1
4η − 1)8B.
Since 14η − 1 > 1 we have that: |Et − 〈ft,Φ(xt)〉| ≥ 8B, leading to:
P [E1 ∩ Ec2 ]",B. Proofs of Lemmas,[0],[0]
≤ P,B. Proofs of Lemmas,[0],[0]
"[|Et − 〈ft,Φ(xt)〉| ≥ 8B] .",B. Proofs of Lemmas,[0],[0]
"(23)
A bound for P [E2]: If |Et| > 16B and ‖ft‖ < 8B then by normalization of Φ(xt)",B. Proofs of Lemmas,[0],[0]
"we have that 〈ft,Φ(xt)〉 < 8B and trivially we have that |Et − 〈ft,Φ(xt)〉| ≥ 8B, and therefore:
",B. Proofs of Lemmas,[0],[0]
P [E2] ≤ P,B. Proofs of Lemmas,[0],[0]
"[|Et − 〈ft,Φ(xt)〉| ≥ 8B] .",B. Proofs of Lemmas,[0],[0]
"(24)
Taking Eq. 23 and Eq. 24 we have that
P [E2 ∪ E1] ≤",B. Proofs of Lemmas,[0],[0]
2P,B. Proofs of Lemmas,[0],[0]
"[|Et − 〈ft,Φ(xt)〉| ≥",B. Proofs of Lemmas,[0],[0]
8B] .,B. Proofs of Lemmas,[0],[0]
"(25)
By Lemma 5 we have that:
P (|Et − 〈ft,Φ(xt)〉|) > 8B)",B. Proofs of Lemmas,[0],[0]
"<
exp(− m(8B) 2
((16B + 1)ηt)2 ) < exp
( − m
(3ηt)2 )",B. Proofs of Lemmas,[0],[0]
"Taking the above upper bounds together with Lemma 6 we can prove Lemma 2.
B.3.",B. Proofs of Lemmas,[0],[0]
"Proof of Lemma 3
Begin by noting that since ‖Φ(x)‖ < 1, it follows from the definitions of ∇, ∇̄ that V [ ∇̄t ]",B. Proofs of Lemmas,[0],[0]
"= E [ ‖∇̄t −∇t‖2 ] and therefore
V [ ∇̄t ] ≤",B. Proofs of Lemmas,[0],[0]
"E [ (Et − 〈ft,Φ(xt)〉)2 ] = V [Et]
By construction (see Algorithm 2) we have that:
V [Et] = 1 m V [ ‖α(t)‖21ψ(xi;w)ψ(xt;w) ] where the index i is sampled as in Algorithm 2, and ψ(xi;w)ψ(xt;w) is bounded by 1.",B. Proofs of Lemmas,[0],[0]
"By Lemma 5 we have that
V [Et] ≤ ((16B + 1)ηt)2
m .",B. Proofs of Lemmas,[0],[0]
This provides the required bound on V [ ∇̄t ] .,B. Proofs of Lemmas,[0],[0]
"Additionally, we have that
‖∇t‖2 = (〈ft,Φ(xt)〉 − yt)2‖Φ(xt)‖2 ≤ 2`t(ft)
and the result follows by taking expectation.
",B. Proofs of Lemmas,[0],[0]
Acknowledgements The authors would like to thank Tomer Koren for helpful discussions.,B. Proofs of Lemmas,[0],[0]
Roi Livni was supported by funding from Eric and Wendy Schmidt Fund for Strategic Innovation.,B. Proofs of Lemmas,[0],[0]
"This work was supported by the Blavatnik Computer Science Research Fund, the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), and an ISF Centers of Excellence grant.",B. Proofs of Lemmas,[0],[0]
Infinite Layer Networks (ILN) have been proposed as an architecture that mimics neural networks while enjoying some of the advantages of kernel methods.,abstractText,[0],[0]
ILN are networks that integrate over infinitely many nodes within a single hidden layer.,abstractText,[0],[0]
"It has been demonstrated by several authors that the problem of learning ILN can be reduced to the kernel trick, implying that whenever a certain integral can be computed analytically they are efficiently learnable.",abstractText,[0],[0]
"In this work we give an online algorithm for ILN, which avoids the kernel trick assumption.",abstractText,[0],[0]
"More generally and of independent interest, we show that kernel methods in general can be exploited even when the kernel cannot be efficiently computed but can only be estimated via sampling.",abstractText,[0],[0]
"We provide a regret analysis for our algorithm, showing that it matches the sample complexity of methods which have access to kernel values.",abstractText,[0],[0]
"Thus, our method is the first to demonstrate that the kernel trick is not necessary, as such, and random features suffice to obtain comparable performance.",abstractText,[0],[0]
Learning Infinite Layer Networks Without the Kernel Trick,title,[0],[0]
