0	28	Relation extraction is a core task in information extraction and natural language understanding.
1	45	The goal of relation extraction is to predict relations for entities in a sentence (Zelenko et al., 2003; Bunescu and Mooney, 2005; GuoDong et al., 2005).
2	139	For example, given a sentence “Barack Obama is married to Michelle Obama.”, a relation classifier aims at predicting the relation of “spouse”.
4	22	A major issue encountered in the early development of relation extraction algorithms is the data sparsity issue—It is extremely expensive, and almost impossible for human annotators to go through a large corpus of millions of sentences to provide a large amount of labeled training instances.
6	14	In recent years, neural network approaches (Zeng et al., 2014, 2015) have been proposed to train the relation extractor under these noisy conditions.
7	30	To suppress the noisy(Roth et al., 2013), recent stud- ies (Lin et al., 2016) have proposed the use of attention mechanisms to place soft weights on a set of noisy sentences, and select samples.
9	15	In this paper, we investigate the possibility of using dynamic selection strategies for robust distant supervision.
11	14	Intuitively, our agent would like to remove false positives, and reconstruct a cleaned set of distantly supervised instances to maximize the reward based on the classification accuracy.
14	20	Our contributions are three-fold: • We propose a novel deep reinforcement learning framework for robust distant supervision relation extraction.
15	16	• Our method is model-independent, meaning that it could be applied to any state-of-the-art relation extractors.
46	54	First, the prerequisite of reinforcement learning is that the external environment should be modeled as a Markov decision process (MDP).
48	17	In other words, we cannot merely use the information of the sentence being processed as the state.
50	14	The other component, RL agent, is parameterized with a policy network πθ(s, a) = p(a|s; θ).
71	32	With this relation classifier, F1 score is calculated from the new validation set {P iv, N iv}, where P iv is also filtered by the current agent.
96	24	Cross-entropy cost function is used to train this binary classifier, where the negative label corresponds to the removing action, and the positive label corresponds to the retaining action.
110	35	First, this validation set is filtered and redistributed by the current agent as {Pv, Nv}; the F1 score of the current relation classifier is calculated from it.
121	20	Therefore, we definite two sets: Ωi−1 = Ψi−1 − (Ψi ∩Ψi−1) (3) Ωi = Ψi − (Ψi ∩Ψi−1) (4) where Ψi is the removed part of epoch i. Ωi−1 and Ωi are represented with the different colors in Figure 2.
126	22	Through the above reinforcement learning procedure, for each relation type, we obtain a agent as the false-positive indicator.
129	20	For one entity pair, if all the sentence aligned from corpus are classified as false positive, then this entity pair is redistributed into the negative set.
132	21	We evaluate the proposed method on a commonlyused dataset2, which is first presented in Riedel et al. (2010).
142	13	We adopt a single-window CNN as this policy network.
148	97	When one relation type has too many distantsupervised positive sentences (for example, /lo- cation/location/contains has 75768 sentences), we sample a subset of size 7,500 sentences to train the agent.
164	15	We adopt our RL agents to redistribute Riedel dataset by moving false positive samples into the negative sample set.
166	14	As shown in Figure 3 and Figure 4, under the assistant of our RL agent, the same model can achieve obvious improvement with more reasonable training dataset.
171	55	At the same time, we analyze the correlation between the false positive phenomenon and the number of sentences of entity pairs : With this the number ranging from 1 to 5, the corresponding percentages are [55.9%, 32.0%, 3.7%, 4.4%, 0.7%].
173	46	Because Freebase is, to some extent, not completely aligned with the NYT corpus, entity pairs with fewer sentences are more likely to be false positive, which is the major factor hindering the performance of the previous systems.
178	128	This phenomenon also increases the probability of the appearance of false positive samples.
180	37	The intuition is that, in contrast to prior works that utilize only one instance per entity pair and use soft attention weights to select plausible distantly supervised examples, we describe a policy-based framework to systematically learn to relocate the false positive samples, and better utilize the unlabeled data.
181	54	More specifically, our goal is to teach the reinforcement agent to optimize the selection/redistribution strategy that maximizes the reward of boosting the performance of relation classification.
183	37	In experiments, we show that our framework boosts the performance of distant supervision relation extraction of various strong deep learning baselines on the widely used New York Times - Freebase dataset.
