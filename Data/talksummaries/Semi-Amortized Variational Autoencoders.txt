4	39	In stochastic variational inference (SVI), the variational parameters for each data point are randomly initialized and then optimized to maximize the evidence lower bound (ELBO) with, for example, gradient ascent.
7	71	Variational autoencoders (VAEs) are deep generative models that utilize AVI for inference and jointly train the generative model alongside the inference network.
8	16	SVI gives good local (i.e. instance-specific) distributions within the variational family but requires performing optimization for each data point.
9	36	AVI has fast inference, but having the variational parameters be a parametric function of the input may be too strict of a restriction.
12	43	Recent work has targeted this amortization gap by combining amortized inference with iterative refinement during training (Hjelm et al., 2016; Krishnan et al., 2018).
17	56	We propose an approach that leverages differentiable optimization (Domke, 2012; Maclaurin et al., 2015; Belanger et al., 2017) and differentiates through SVI while training the inference network/generative model.
18	45	We find that this method is able to both improve estimation of variational parameters and produce better generative models.
20	49	We also find that under our framework, we are able to utilize a powerful generative model without experiencing the “posterior-collapse” phenomenon often observed in VAEs, wherein the variational posterior collapses to the prior and the generative model ignores the latent variable (Bowman et al., 2016; Chen et al., 2017; Zhao et al., 2017).
21	27	This problem has particularly made it very difficult to utilize VAEs for text, an important open issue in the field.
22	62	With SA-VAE, we are able to outperform an LSTM language model by utilizing an LSTM generative model that maintains non-trivial latent representations.
27	20	, ûm], and further use dfdv to denote the total derivative of f with respect to v, which exists if u is a differentiable function of v. Note that in general ∇uif(û) 6= df dui since other components of u could be a function of ui.1 We also let Hui,ujf(û) ∈ Rdim(ui)×dim(uj) be the matrix formed by taking the i-th group of rows and the j-th group of columns of the Hessian of f evaluated at û.
38	32	SVI optimizes directly for instance-specific variational distributions, but may require running iterative inference for a large number of steps.
47	48	Semi-amortized variational autoencoders (SA-VAE) utilize an inference network over the input to give the initial variational parameters, and subsequently run SVI to refine them.
51	35	Update φ based on dELBO(λK ,θ,x)dφ Note that for training we need to compute the total derivative of the final ELBO with respect to θ, φ (i.e. steps 4 and 5 above).
58	34	4 The full forward/backward step, which uses gradient descent with momentum on the negative ELBO, is shown in Algorithm 1.
89	26	The first approach is from Krishnan et al. (2018), where the generative model takes a gradient step based on the final variational parameters and the inference network takes a gradient step based on the initial variational parameters, i.e. we update θ based on ∇θ ELBO(λK , θ,x) and update φ based on dλ0 dφ ∇λ ELBO(λ0, θ,x).
106	17	Interestingly, SA-VAE without any refinement steps at test time has a substantially nonzero KL term (KL = 6.65, PPL = 62.0).
108	17	Finally, while Yang et al. (2017) found that initializing the encoder with a pretrained language model improved performance (+ INIT in Table 2), we did not observe this on our baseline VAE model when we trained with SGD and hence did not pursue this further.
110	60	We use a three-layer ResNet (He et al., 2016) as our inference network.
111	18	The generative model first transforms the 32-dimensional latent vector to the image spatial resolution, which is concatenated with the original image and fed to a 12-layer Gated PixelCNN (van den Oord et al., 2016) with varying filter sizes, followed by a final sigmoid layer.
115	52	Our findings are largely consistent with results from text: the semi-amortized approaches outperform VAE/SVI baselines, and further they learn generative models that make more use of the latent representations (i.e. KL portion of the loss is higher).
117	20	In Appendix C we further investigate the performance of VAE and SA-VAE as we vary the training set size and the capacity of the inference network/generative model.
120	23	However these additions are largely orthogonal to our approach and we hypothesize they will also benefit from combining amortized inference with iterative refinement.11
123	22	Saliency is therefore a measure of how much the latent variable is being used to predict a particular token.
124	38	We visualize the saliency of a few examples from the test set in Figure 3 (top).
125	23	Each example consists of a question followed by an answer from the Yahoo corpus.
126	48	From a qualitative analysis several things are apparent: the latent variable seems to encode question type (i.e. if, what, how, why, etc.)
128	19	); saliency of the </s> token is quite high, indicating that the length information is also encoded in the latent space.
131	15	We can also roughly measure the influence of the input xt on the latent representation z, which we refer to as input saliency:∥∥∥Eq(z;λ)[d‖z‖2 dwt ] ∥∥∥ 2 Here wt is the encoder word embedding for xt.12 We visualize the input saliency for a test example (Figure 3, middle) and a made-up example (Figure 3, bottom).
132	64	Under each input example we also visualize a two samples from the variational posterior, and find that the generated examples are often meaningfully related to the input example.13 We quantitatively analyze output saliency across part-ofspeech, token position, word frequency, and log-likelihood in Figure 4: nouns (NN), adjectives (JJ), verbs (VB), numbers (CD), and the </s> token have higher saliency than conjunctions (CC), determiners (DT), prepositions (IN), and the TO token—the latter are relatively easier to predict by conditioning on previous tokens; similarly, on average, tokens occurring earlier have much higher saliency than those dwt .
