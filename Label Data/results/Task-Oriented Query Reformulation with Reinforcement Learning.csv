0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1.",text,[0],[0]
"Distributed representations of words (or word embeddings) (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) have shown to provide useful features for various tasks in natural language processing and computer vision.",1 Introduction,[0],[0]
"While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence.",1 Introduction,[0],[0]
"That is, how to capture the
1https://www.github.com/ facebookresearch/InferSent
relationships among multiple words and phrases in a single vector remains an question to be solved.
",1 Introduction,[0.9540130165032429],['The most important implication of this framework is that a search engine is treated as a black box that an agent learns to use in order to retrieve more relevant items.']
"In this paper, we study the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks.",1 Introduction,[0],[0]
"Two questions need to be solved in order to build such an encoder, namely: what is the preferable neural network architecture; and how and on what task should such a network be trained.",1 Introduction,[0],[0]
"Following existing work on learning word embeddings, most current approaches consider learning sentence encoders in an unsupervised manner like SkipThought (Kiros et al., 2015) or FastSent (Hill et al., 2016).",1 Introduction,[0],[0]
"Here, we investigate whether supervised learning can be leveraged instead, taking inspiration from previous results in computer vision, where many models are pretrained on the ImageNet (Deng et al., 2009) before being transferred.",1 Introduction,[0],[0]
"We compare sentence embeddings trained on various supervised tasks, and show that sentence embeddings generated from models trained on a natural language inference (NLI) task reach the best results in terms of transfer accuracy.",1 Introduction,[0],[0]
"We hypothesize that the suitability of NLI as a training task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences.
",1 Introduction,[0],[0]
"Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks.",1 Introduction,[0],[0]
"Hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes.",1 Introduction,[0],[0]
"Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling, trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), yields state-of-the-art sentence embeddings com-
670
pared to all existing alternative unsupervised approaches like SkipThought or FastSent, while being much faster to train.",1 Introduction,[0],[0]
We establish this finding on a broad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information.,1 Introduction,[0],[0]
"Transfer learning using supervised features has been successful in several computer vision applications (Razavian et al., 2014).",2 Related work,[0],[0]
"Striking examples include face recognition (Taigman et al., 2014) and visual question answering (Antol et al., 2015), where image features trained on ImageNet (Deng et al., 2009) and word embeddings trained on large unsupervised corpora are combined.
",2 Related work,[0],[0]
"In contrast, most approaches for sentence representation learning are unsupervised, arguably because the NLP community has not yet found the best supervised task for embedding the semantics of a whole sentence.",2 Related work,[0],[0]
"Another reason is that neural networks are very good at capturing the biases of the task on which they are trained, but can easily forget the overall information or semantics of the input data by specializing too much on these biases.",2 Related work,[0],[0]
Learning models on large unsupervised task makes it harder for the model to specialize.,2 Related work,[0],[0]
"Littwin and Wolf (2016) showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder.",2 Related work,[0],[0]
"They propose a loss that incorporates multiple orthogonal classifiers to counteract this effect.
",2 Related work,[0],[0]
"Recent work on generating sentence embeddings range from models that compose word embeddings (Le and Mikolov, 2014; Arora et al., 2017; Wieting et al., 2016b) to more complex neural network architectures.",2 Related work,[0],[0]
"SkipThought vectors (Kiros et al., 2015) propose an objective function that adapts the skip-gram model for words (Mikolov et al., 2013) to the sentence level.",2 Related work,[0],[0]
"By encoding a sentence to predict the sentences around it, and using the features in a linear model, they were able to demonstrate good performance on 8 transfer tasks.",2 Related work,[0],[0]
"They further obtained better results using layer-norm regularization of their model in (Ba et al., 2016).",2 Related work,[0],[0]
Hill et al. (2016) showed that the task on which sentence embeddings are trained significantly impacts their quality.,2 Related work,[0],[0]
"In addition to unsupervised methods, they included supervised training in their comparison—namely, on
machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014).",2 Related work,[0],[0]
"These models obtained significantly lower results compared to the unsupervised Skip-Thought approach.
",2 Related work,[0],[0]
"Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015).",2 Related work,[0],[0]
"The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017).",2 Related work,[0],[0]
"To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders.",2 Related work,[0],[0]
"As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data.",2 Related work,[0],[0]
"This work combines two research directions, which we describe in what follows.",3 Approach,[0],[0]
"First, we explain how the NLI task can be used to train universal sentence encoding models using the SNLI task.",3 Approach,[0],[0]
"We subsequently describe the architectures that we investigated for the sentence encoder, which, in our opinion, covers a suitable range of sentence encoders currently in use.",3 Approach,[0],[0]
"Specifically, we examine standard recurrent models such as LSTMs and GRUs, for which we investigate mean and maxpooling over the hidden representations; a selfattentive network that incorporates different views of the sentence; and a hierarchical convolutional network that can be seen as a tree-based method that blends different levels of abstraction.",3 Approach,[0],[0]
"The SNLI dataset consists of 570k humangenerated English sentence pairs, manually labeled with one of three categories: entailment, contradiction and neutral.",3.1 The Natural Language Inference task,[0],[0]
"It captures natural language inference, also known in previous incarnations as Recognizing Textual Entailment (RTE), and constitutes one of the largest high-quality labeled resources explicitly constructed in order to require understanding sentence semantics.",3.1 The Natural Language Inference task,[0],[0]
"We hypothesize that the semantic nature of NLI makes it a good candidate for learning universal sentence
embeddings in a supervised way.",3.1 The Natural Language Inference task,[0],[0]
"That is, we aim to demonstrate that sentence encoders trained on natural language inference are able to learn sentence representations that capture universally useful features.
",3.1 The Natural Language Inference task,[0],[0]
"Models can be trained on SNLI in two different ways: (i) sentence encoding-based models that explicitly separate the encoding of the individual sentences and (ii) joint methods that allow to use encoding of both sentences (to use cross-features or attention from one sentence to the other).
",3.1 The Natural Language Inference task,[0],[0]
"Since our goal is to train a generic sentence encoder, we adopt the first setting.",3.1 The Natural Language Inference task,[0],[0]
"As illustrated in Figure 1, a typical architecture of this kind uses a shared sentence encoder that outputs a representation for the premise u and the hypothesis v.",3.1 The Natural Language Inference task,[0],[0]
"Once the sentence vectors are generated, 3 matching methods are applied to extract relations between u and v : (i) concatenation of the two representations (u, v); (ii) element-wise product u ∗ v; and (iii) absolute element-wise difference |u− v|.",3.1 The Natural Language Inference task,[0],[0]
"The resulting vector, which captures information from both the premise and the hypothesis, is fed into a 3-class classifier consisting of multiple fullyconnected layers culminating in a softmax layer.",3.1 The Natural Language Inference task,[0],[0]
"A wide variety of neural networks for encoding sentences into fixed-size representations exists, and it is not yet clear which one best captures generically useful information.",3.2 Sentence encoder architectures,[0],[0]
"We compare 7 different architectures: standard recurrent encoders with either Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU), concatenation of last hidden states of forward and backward GRU, Bi-directional LSTMs (BiLSTM)
with either mean or max pooling, self-attentive network and hierarchical convolutional networks.",3.2 Sentence encoder architectures,[0],[0]
"Our first, and simplest, encoders apply recurrent neural networks using either LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) modules, as in sequence to sequence encoders (Sutskever et al., 2014).",3.2.1 LSTM and GRU,[0],[0]
"For a sequence of T words (w1, . . .",3.2.1 LSTM and GRU,[0],[0]
", wT ), the network computes a set of T hidden representations h1, . . .",3.2.1 LSTM and GRU,[0],[0]
", hT , with ht = −−−−→ LSTM(w1, . . .",3.2.1 LSTM and GRU,[0],[0]
", wT ) (or using GRU units instead).",3.2.1 LSTM and GRU,[0],[0]
"A sentence is represented by the last hidden vector, hT .
",3.2.1 LSTM and GRU,[0],[0]
"We also consider a model BiGRU-last that concatenates the last hidden state of a forward GRU, and the last hidden state of a backward GRU to have the same architecture as for SkipThought vectors.",3.2.1 LSTM and GRU,[0],[0]
"For a sequence of T words {wt}t=1,...,T , a bidirectional LSTM computes a set of T vectors {ht}t.",3.2.2 BiLSTM with mean/max pooling,[0],[0]
For t ∈,3.2.2 BiLSTM with mean/max pooling,[0],[0]
"[1, . . .",3.2.2 BiLSTM with mean/max pooling,[0],[0]
", T ], ht, is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions:
−→ ht = −−−−→ LSTMt(w1, . . .",3.2.2 BiLSTM with mean/max pooling,[0],[0]
", wT )←−
",3.2.2 BiLSTM with mean/max pooling,[0],[0]
"ht = ←−−−− LSTMt(w1, . . .",3.2.2 BiLSTM with mean/max pooling,[0],[0]
", wT )",3.2.2 BiLSTM with mean/max pooling,[0],[0]
ht =,3.2.2 BiLSTM with mean/max pooling,[0],[0]
"[ −→ ht , ←− ht ]
We experiment with two ways of combining the varying number of {ht}t to form a fixed-size vector, either by selecting the maximum value over each dimension of the hidden units (max pooling) (Collobert and Weston, 2008) or by considering the average of the representations (mean pooling).",3.2.2 BiLSTM with mean/max pooling,[0],[0]
"The self-attentive sentence encoder (Liu et al., 2016; Lin et al., 2017) uses an attention mechanism over the hidden states of a BiLSTM to generate a representation u of an input sentence.",3.2.3 Self-attentive network,[0],[0]
"The attention mechanism is defined as :
h̄i = tanh(Whi + bw)
αi = eh̄",3.2.3 Self-attentive network,[0],[0]
T,3.2.3 Self-attentive network,[0],[0]
"i uw∑
",3.2.3 Self-attentive network,[0],[0]
"i e h̄Ti uw u = ∑
t
αihi
where {h1, . . .",3.2.3 Self-attentive network,[0],[0]
", hT } are the output hidden vectors of a BiLSTM.",3.2.3 Self-attentive network,[0],[0]
"These are fed to an affine transformation (W , bw) which outputs a set of keys (h̄1, . . .",3.2.3 Self-attentive network,[0],[0]
", h̄T ).",3.2.3 Self-attentive network,[0],[0]
The {αi} represent the score of similarity between the keys and a learned context query vector uw.,3.2.3 Self-attentive network,[0],[0]
"These weights are used to produce the final representation u, which is a weighted linear combination of the hidden vectors.
",3.2.3 Self-attentive network,[0],[0]
"Following Lin et al. (2017) we use a selfattentive network with multiple views of the input sentence, so that the model can learn which part of the sentence is important for the given task.",3.2.3 Self-attentive network,[0],[0]
"Concretely, we have 4 context vectors u1w, u 2 w, u 3 w, u 4 w which generate 4 representations that are then concatenated to obtain the sentence representation u. Figure 3 illustrates this architecture.",3.2.3 Self-attentive network,[0],[0]
"One of the currently best performing models on classification tasks is a convolutional architecture termed AdaSent (Zhao et al., 2015), which concatenates different representations of the sentences
at different level of abstractions.",3.2.4 Hierarchical ConvNet,[0],[0]
"Inspired by this architecture, we introduce a faster version consisting of 4 convolutional layers.",3.2.4 Hierarchical ConvNet,[0],[0]
"At every layer, a representation ui is computed by a max-pooling operation over the feature maps (see Figure 4).
",3.2.4 Hierarchical ConvNet,[0],[0]
The final representation u =,3.2.4 Hierarchical ConvNet,[0],[0]
"[u1, u2, u3, u4] concatenates representations at different levels of the input sentence.",3.2.4 Hierarchical ConvNet,[0],[0]
The model thus captures hierarchical abstractions of an input sentence in a fixed-size representation.,3.2.4 Hierarchical ConvNet,[0],[0]
"For all our models trained on SNLI, we use SGD with a learning rate of 0.1 and a weight decay of 0.99.",3.3 Training details,[0],[0]
"At each epoch, we divide the learning rate by 5 if the dev accuracy decreases.",3.3 Training details,[0],[0]
We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10−5.,3.3 Training details,[0],[0]
"For the classifier, we use a multi-layer perceptron with 1 hidden-layer of 512 hidden units.",3.3 Training details,[0],[0]
We use opensource GloVe vectors trained on Common Crawl 840B2 with 300 dimensions as fixed word embeddings.,3.3 Training details,[0],[0]
"Our aim is to obtain general-purpose sentence embeddings that capture generic information that is
2https://nlp.stanford.edu/projects/ glove/
useful for a broad set of tasks.",4 Evaluation of sentence representations,[0],[0]
"To evaluate the quality of these representations, we use them as features in 12 transfer tasks.",4 Evaluation of sentence representations,[0],[0]
We present our sentence-embedding evaluation procedure in this section.,4 Evaluation of sentence representations,[0],[0]
We constructed a sentence evaluation tool3 to automate evaluation on all the tasks mentioned in this paper.,4 Evaluation of sentence representations,[0],[0]
"The tool uses Adam (Kingma and Ba, 2014) to fit a logistic regression classifier, with batch size 64.
",4 Evaluation of sentence representations,[0],[0]
"Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR, SST), question-type (TREC), product reviews (CR), subjectivity/objectivity (SUBJ) and opinion polarity (MPQA).",4 Evaluation of sentence representations,[0],[0]
We generate sentence vectors and train a logistic regression on top.,4 Evaluation of sentence representations,[0],[0]
"A linear classifier requires fewer parameters than an MLP and is thus suitable for small datasets, where transfer learning is especially well-suited.",4 Evaluation of sentence representations,[0],[0]
"We tune the L2 penalty of the logistic regression with grid-search on the validation set.
",4 Evaluation of sentence representations,[0],[0]
Entailment and semantic relatedness We also evaluate on the SICK dataset for both entailment (SICK-E) and semantic relatedness (SICK-R).,4 Evaluation of sentence representations,[0],[0]
We use the same matching methods as in SNLI and learn a Logistic Regression on top of the joint representation.,4 Evaluation of sentence representations,[0],[0]
"For semantic relatedness evaluation, we follow the approach of (Tai et al., 2015) and learn to predict the probability distribution of relatedness scores.",4 Evaluation of sentence representations,[0],[0]
"We report Pearson correlation.
",4 Evaluation of sentence representations,[0],[0]
"STS14 - Semantic Textual Similarity While semantic relatedness is supervised in the case of SICK-R, we also evaluate our embeddings on the 6 unsupervised SemEval tasks of STS14 (Agirre et al., 2014).",4 Evaluation of sentence representations,[0],[0]
"This dataset includes subsets of news articles, forum discussions, image descriptions and headlines from news articles containing pairs of sentences (lower-cased), labeled with
3https://www.github.com/ facebookresearch/SentEval
a similarity score between 0 and 5.",4 Evaluation of sentence representations,[0],[0]
"These tasks evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations.
",4 Evaluation of sentence representations,[0],[0]
Paraphrase detection The Microsoft Research Paraphrase Corpus is composed of pairs of sentences which have been extracted from news sources on the Web.,4 Evaluation of sentence representations,[0],[0]
Sentence pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence relationship.,4 Evaluation of sentence representations,[0],[0]
"We use the same approach as with SICK-E, except that our classifier has only 2 classes.
",4 Evaluation of sentence representations,[0],[0]
"Caption-Image retrieval The caption-image retrieval task evaluates joint image and language feature models (Hodosh et al., 2013; Lin et al., 2014).",4 Evaluation of sentence representations,[0],[0]
"The goal is either to rank a large collection of images by their relevance with respect to a given query caption (Image Retrieval), or ranking captions by their relevance for a given query image (Caption Retrieval).",4 Evaluation of sentence representations,[0],[0]
"We use a pairwise rankingloss Lcir(x, y): ∑ y ∑ k
max(0, α− s(V y, Ux) + s(V y, Uxk))",4 Evaluation of sentence representations,[0],[0]
"+∑ x ∑ k′ max(0, α− s(Ux, V y) + s(Ux, V yk′))
",4 Evaluation of sentence representations,[0],[0]
"where (x, y) consists of an image y with one of its associated captions x, (yk)k and (yk′)k′ are negative examples of the ranking loss, α is the margin and s corresponds to the cosine similarity.",4 Evaluation of sentence representations,[0],[0]
U and V are learned linear transformations that project the caption x and the image y to the same embedding space.,4 Evaluation of sentence representations,[0],[0]
We use a margin α = 0.2 and 30 contrastive terms.,4 Evaluation of sentence representations,[0],[0]
"We use the same splits as in (Karpathy and Fei-Fei, 2015), i.e., we use 113k images from the COCO dataset (each containing 5 captions) for training, 5k images for validation and 5k images for test.",4 Evaluation of sentence representations,[0],[0]
"For evaluation, we split the 5k images in 5 random sets of 1k images on which we compute Recall@K, with K ∈ {1, 5, 10} and
median (Med r) over the 5 splits.",4 Evaluation of sentence representations,[0],[0]
"For fair comparison, we also report SkipThought results in our setting, using 2048-dimensional pretrained ResNet101 (He et al., 2016) with 113k training images.",4 Evaluation of sentence representations,[0],[0]
"In this section, we refer to ”micro” and ”macro” averages of development set (dev) results on transfer tasks whose metrics is accuracy: we compute a ”macro” aggregated score that corresponds to the classical average of dev accuracies, and the ”micro” score that is a sum of the dev accuracies, weighted by the number of dev samples.",5 Empirical results,[0.9518717342532704],"['The model trained on MSA selects terms that cover different aspects of the entity being queried, such as “arts center” and “library”, since retrieving a diverse set of documents is necessary for the task the of citation recommendation.']"
Model We observe in Table 3 that different models trained on the same NLI corpus lead to different transfer tasks results.,5.1 Architecture impact,[0],[0]
The BiLSTM-4096 with the max-pooling operation performs best on both SNLI and transfer tasks.,5.1 Architecture impact,[0],[0]
"Looking at the micro and macro averages, we see that it performs significantly better than the other models LSTM, GRU, BiGRU-last, BiLSTM-Mean, inner-attention and the hierarchical-ConvNet.
",5.1 Architecture impact,[0],[0]
"Table 3 also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM-Mean for instance.
",5.1 Architecture impact,[0],[0]
We hypothesize that some models are likely to over-specialize and adapt too well to the biases of a dataset without capturing general-purpose information of the input sentence.,5.1 Architecture impact,[0],[0]
"For example, the inner-attention model has the ability to focus only on certain parts of a sentence that are useful for the SNLI task, but not necessarily for the transfer tasks.",5.1 Architecture impact,[0],[0]
"On the other hand, BiLSTM-Mean does not make sharp choices on which part of the sentence is more important than others.",5.1 Architecture impact,[0],[0]
"The difference between the results seems to come from the different abilities of the models to incorporate general information while not focusing too much on specific features useful for the task at hand.
",5.1 Architecture impact,[0],[0]
"For a given model, the transfer quality is also sensitive to the optimization algorithm: when training with Adam instead of SGD, we observed that the BiLSTM-max converged faster on SNLI (5 epochs instead of 10), but obtained worse results on the transfer tasks, most likely because of the model and classifier’s increased capability to over-specialize on the training task.
",5.1 Architecture impact,[0],[0]
"Embedding size Figure 5 compares the overall performance of different architectures, showing the evolution of micro averaged performance with
Model MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E STS14 Unsupervised representation training (unordered sentences) Unigram-TFIDF 73.7 79.2 90.3 82.4 - 85.0 73.6/81.7 - - .58/.57",5.1 Architecture impact,[0],[0]
ParagraphVec (DBOW) 60.2 66.9 76.3 70.7 - 59.4 72.9/81.1 - - .42/.43 SDAE 74.6 78.0,5.1 Architecture impact,[0],[0]
90.8 86.9 - 78.4 73.7/80.7 - - .37/.38 SIF (GloVe + WR) - - - - 82.2 - - - 84.6 .69/ - word2vec BOW† 77.7 79.8 90.9 88.3 79.7 83.6 72.5/81.4 0.803 78.7,5.1 Architecture impact,[0],[0]
.65/.64 fastText BOW†,5.1 Architecture impact,[0],[0]
76.5 78.9 91.6 87.4 78.8 81.8 72.4/81.2 0.800 77.9 .63/.62,5.1 Architecture impact,[0],[0]
GloVe BOW†,5.1 Architecture impact,[0],[0]
78.7 78.5 91.6 87.6 79.8 83.6 72.1/80.9 0.800 78.6 .54/.56 GloVe Positional Encoding† 78.3 77.4 91.1 87.1 80.6 83.3 72.5/81.2 0.799 77.9,5.1 Architecture impact,[0],[0]
.51/.54,5.1 Architecture impact,[0],[0]
"BiLSTM-Max (untrained)† 77.5 81.3 89.6 88.7 80.7 85.8 73.2/81.6 0.860 83.4 .39/.48
Unsupervised representation training (ordered sentences) FastSent 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3 - - .63/.64 FastSent+AE 71.8 76.7 88.8 81.5 - 80.4 71.2/79.1 - - .62/.62 SkipThought 76.5 80.1 93.6 87.1 82.0 92.2 73.0/82.0 0.858 82.3 .29/.35",5.1 Architecture impact,[0],[0]
"SkipThought-LN 79.4 83.1 93.7 89.3 82.9 88.4 - 0.858 79.5 .44/.45
Supervised representation training CaptionRep (bow) 61.9 69.3 77.4 70.8 - 72.2 - - - .46/.42 DictRep (bow) 76.7 78.7 90.7 87.2 - 81.0 68.4/76.8 - - .67/.70",5.1 Architecture impact,[0],[0]
NMT En-to-Fr 64.7 70.1 84.9 81.5 - 82.8 - - .43/.42 Paragram-phrase - - - - 79.7 - - 0.849 83.1 .71/ - BiLSTM-Max (on SST)† (*) 83.7 90.2 89.5 (*) 86.0 72.7/80.9 0.863 83.1 .55/.54 BiLSTM-Max (on SNLI)† 79.9 84.6 92.1 89.8 83.3 88.7 75.1/82.3 0.885 86.3 .68/.65,5.1 Architecture impact,[0],[0]
"BiLSTM-Max (on AllNLI)† 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 0.884 86.3 .70/.67
Supervised methods (directly trained for each task – no transfer) Naive Bayes - SVM 79.4 81.8 93.2 86.3 83.1 - - - - - AdaSent 83.1 86.3 95.5 93.3 - 92.4 - - - - TF-KLD - - - - - - 80.4/85.9 - - - Illinois-LH - - - - - - - - 84.5 - Dependency Tree-LSTM - - - - - - - 0.868 - -
Table 4: Transfer test results for various architectures trained in different ways.",5.1 Architecture impact,[0],[0]
"Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way.",5.1 Architecture impact,[0],[0]
"† indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016).",5.1 Architecture impact,[0],[0]
"For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014).",5.1 Architecture impact,[0],[0]
(*),5.1 Architecture impact,[0],[0]
"Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods.
regard to the embedding size.
",5.1 Architecture impact,[0],[0]
"Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models.",5.1 Architecture impact,[0],[0]
"However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows.",5.1 Architecture impact,[0],[0]
We hypothesize that such networks are able to incorporate information that is not directly relevant to the objective task (results on SNLI are relatively stable with regard to embedding size) but that can nevertheless be useful as features for transfer tasks.,5.1 Architecture impact,[0],[0]
We report in Table 4 transfer tasks results for different architectures trained in different ways.,5.2 Task transfer,[0],[0]
We group models by the nature of the data on which they were trained.,5.2 Task transfer,[0],[0]
The first group corresponds to models trained with unsupervised unordered sentences.,5.2 Task transfer,[0],[0]
"This includes bag-of-words models such as word2vec-SkipGram, the UnigramTFIDF model, the Paragraph Vector model (Le and Mikolov, 2014), the Sequential Denoising Auto-Encoder (SDAE) (Hill et al., 2016) and the SIF model (Arora et al., 2017), all trained on the Toronto book corpus (Zhu et al., 2015).",5.2 Task transfer,[0],[0]
"The second group consists of models trained with unsu-
pervised ordered sentences such as FastSent and SkipThought (also trained on the Toronto book corpus).",5.2 Task transfer,[0],[0]
We also include the FastSent variant “FastSent+AE” and the SkipThought-LN version that uses layer normalization.,5.2 Task transfer,[0],[0]
"We report results from models trained on supervised data in the third group, and also report some results of supervised methods trained directly on each task for comparison with transfer learning approaches.
",5.2 Task transfer,[0],[0]
"Comparison with SkipThought The best performing sentence encoder to date is the SkipThought-LN model, which was trained on a very large corpora of ordered sentences.",5.2 Task transfer,[0],[0]
"With much less data (570k compared to 64M sentences) but with high-quality supervision from the SNLI dataset, we are able to consistently outperform the results obtained by SkipThought vectors.",5.2 Task transfer,[0],[0]
We train our model in less than a day on a single GPU compared to the best SkipThought-LN network trained for a month.,5.2 Task transfer,[0],[0]
"Our BiLSTM-max trained on SNLI performs much better than released SkipThought vectors on MR, CR, MPQA, SST, MRPC-accuracy, SICK-R, SICK-E and STS14 (see Table 4).",5.2 Task transfer,[0],[0]
"Except for the SUBJ dataset, it also performs better than SkipThought-LN on MR, CR and MPQA.",5.2 Task transfer,[0],[0]
We also observe by looking at the STS14 results that the cosine metrics in our embedding space is much more semantically informative than in SkipThought embedding space (pearson score of 0.68 compared to 0.29 and 0.44 for ST and ST-LN).,5.2 Task transfer,[0],[0]
"We hypothesize that this is namely linked to the matching method of SNLI models which incorporates a notion of distance (element-wise product and absolute difference) during training.
",5.2 Task transfer,[0],[0]
"NLI as a supervised training set Our findings indicate that our model trained on SNLI obtains much better overall results than models trained on other supervised tasks such as COCO, dictionary definitions, NMT, PPDB (Ganitkevitch et al., 2013) and SST.",5.2 Task transfer,[0],[0]
"For SST, we tried exactly the same models as for SNLI; it is worth noting that SST is smaller than NLI.",5.2 Task transfer,[0],[0]
Our representations constitute higher-quality features for both classification and similarity tasks.,5.2 Task transfer,[0],[0]
"One explanation is that the natural language inference task constrains the model to encode the semantic information of the input sentence, and that the information required to perform NLI is generally discriminative and informative.
",5.2 Task transfer,[0],[0]
Domain adaptation on SICK tasks Our transfer learning approach obtains better results than previous state-of-the-art on the SICK task - can be seen as an out-domain version of SNLI - for both entailment and relatedness.,5.2 Task transfer,[0],[0]
"We obtain a pearson score of 0.885 on SICK-R while (Tai et al., 2015) obtained 0.868, and we obtain 86.3% test accuracy on SICK-E while previous best handengineered models (Lai and Hockenmaier, 2014) obtained 84.5%.",5.2 Task transfer,[0],[0]
"We also significantly outperformed previous transfer learning approaches on SICK-E (Bowman et al., 2015) that used the parameters of an LSTM model trained on SNLI to fine-tune on SICK (80.8% accuracy).",5.2 Task transfer,[0],[0]
"We hypothesize that our embeddings already contain the information learned from the in-domain task, and that learning only the classifier limits the number of parameters learned on the small out-domain task.
",5.2 Task transfer,[0],[0]
"Image-caption retrieval results In Table 5, we report results for the COCO image-caption retrieval task.",5.2 Task transfer,[0],[0]
We report the mean recalls of 5 random splits of 1K test images.,5.2 Task transfer,[0],[0]
"When trained with
ResNet features and 30k more training data, the SkipThought vectors perform significantly better than the original setting, going from 33.8 to 37.9 for caption retrieval R@1, and from 25.9 to 30.6 on image retrieval R@1.",5.2 Task transfer,[0],[0]
"Our approach pushes the results even further, from 37.9 to 42.4 on caption retrieval, and 30.6 to 33.2 on image retrieval.",5.2 Task transfer,[0],[0]
"These results are comparable to previous approach of (Ma et al., 2015) that did not do transfer but directly learned the sentence encoding on the imagecaption retrieval task.",5.2 Task transfer,[0],[0]
"This supports the claim that pre-trained representations such as ResNet image features and our sentence embeddings can achieve competitive results compared to features learned directly on the objective task.
",5.2 Task transfer,[0],[0]
MultiGenre NLI,5.2 Task transfer,[0],[0]
"The MultiNLI corpus (Williams et al., 2017) was recently released as a multi-genre version of SNLI.",5.2 Task transfer,[0],[0]
"With 433K sentence pairs, MultiNLI improves upon SNLI in its coverage: it contains ten distinct genres of written and spoken English, covering most of the complexity of the language.",5.2 Task transfer,[0],[0]
We augment Table 4 with our model trained on both SNLI and MultiNLI (AllNLI).,5.2 Task transfer,[0],[0]
We observe a significant boost in performance overall compared to the model trained only on SLNI.,5.2 Task transfer,[0],[0]
"Our model even reaches AdaSent performance on CR, suggesting that having a larger coverage for the training task helps learn even better general representations.",5.2 Task transfer,[0],[0]
"On semantic textual similarity STS14, we are also competitive with PPDB based paragramphrase embeddings with a pearson score of 0.70.",5.2 Task transfer,[0],[0]
"Interestingly, on caption-related transfer tasks such as the COCO image caption retrieval task, training our sentence encoder on other genres from MultiNLI does not degrade the performance compared to the model trained only SNLI (which contains mostly captions), which confirms the generalization power of our embeddings.",5.2 Task transfer,[0],[0]
This paper studies the effects of training sentence embeddings with supervised data by testing on 12 different transfer tasks.,6 Conclusion,[0],[0]
We showed that models learned on NLI can perform better than models trained in unsupervised conditions or on other supervised tasks.,6 Conclusion,[0],[0]
"By exploring various architectures, we showed that a BiLSTM network with max pooling makes the best current universal sentence encoding methods, outperforming existing approaches like SkipThought vectors.
",6 Conclusion,[0],[0]
We believe that this work only scratches the surface of possible combinations of models and tasks for learning generic sentence embeddings.,6 Conclusion,[0],[0]
Larger datasets that rely on natural language understanding for sentences could bring sentence embedding quality to the next level.,6 Conclusion,[0],[0]
"Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features.",abstractText,[0],[0]
"Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful.",abstractText,[0],[0]
Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted.,abstractText,[0],[0]
"In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks.",abstractText,[0],[0]
"Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks.",abstractText,[0],[0]
Our encoder is publicly available1.,abstractText,[0],[0]
Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,title,[0],[0]
"Proceedings of the SIGDIAL 2016 Conference, pages 242–251, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Current virtual personal assistants (PAs) require users to either formulate complex intents in one utterance (e.g., “call Peter Miller on his mobile phone”) or go through tedious sub-dialogues (e.g., “phone call” – who would you like to call? – “Peter Miller” – I have a mobile number and a work number.",1 Introduction,[0],[0]
Which one do you want?).,1 Introduction,[0],[0]
"This is not how one would interact with a human assistant, where the request would be naturally structured into smaller chunks that individually get acknowledged (e.g., “Can you make a connection for me?” – sure – “with Peter Miller” - uh huh",1 Introduction,[0],[0]
- “on his mobile” - dialling now).,1 Introduction,[0],[0]
"Current PAs signal ongoing understanding by displaying the state of
the recognised speech (ASR) to the user, but not their semantic interpretation of it.",1 Introduction,[0],[0]
Another type of assistant system forgoes enquiring user intent altogether and infers likely intents from context.,1 Introduction,[0],[0]
"GoogleNow, for example, might present traffic information to a user picking up their mobile phone at their typical commute time.",1 Introduction,[0],[0]
"These systems display their “understanding” state, but do not allow any type of interaction with it apart from dismissing the provided information.
",1 Introduction,[0],[0]
"In this work, we explore adding a graphical user interface (GUI) modality that makes it possible to see these interaction styles as extremes on a continuum, and to realise positions between these extremes and present a mixed graphical/voice enabled PA that can provide feedback of understanding to the user incrementally as the user’s utterance unfolds–allowing users to make requests in instalments instead of fully thought-out requests.",1 Introduction,[0],[0]
It does this by signalling ongoing understanding in an intuitive tree-like GUI that can be displayed on a mobile device.,1 Introduction,[0],[0]
"We evaluate our system by directing users to perform tasks using it under nonincremental (i.e., ASR endpointing) and incremental conditions and then compare the two conditions.",1 Introduction,[0],[0]
"We further compare a non-adaptive with an adaptive (i.e., infers likely events) version of our system.",1 Introduction,[0],[0]
"We report that the users found the interface intuitive and easy to use, and that users were able to perform tasks more efficiently with incremental as well as adaptive variants of the system.",1 Introduction,[0],[0]
This work builds upon several threads of previous research: Chai et al. (2014),2 Related Work,[0],[0]
"addressed misalignments in understanding (i.e., common ground (Clark and Schaefer, 1989)) between robots and humans by informing the human of the internal system state via speech.",2 Related Work,[0],[0]
"We take this idea and ap-
242
ply it to a PA by displaying the internal state of the system to the user via a GUI (explained in Section 3.5), allowing the user to determine if system understanding has taken place–a way of providing feedback and backchannels to the user.",2 Related Work,[0],[0]
"Dethlefs et al. (2016) provide a good review of work that show how backchannels facilitate grounding, feedback, and clarifications in human spoken dialogue, and apply an information density approach to determine when to backchannel using speech.",2 Related Work,[0],[0]
"Because we don’t backchannel using speech here, there is no potential overlap between the user and the system; rather, our system can display backchannels and ask clarifications without frustrating the user through inadvertent overlaps.
",2 Related Work,[0],[0]
"Though different in many ways, our work is similar in some regards to Larsson et al. (2011), which displays information to the user and allows the user to navigate the display itself (e.g., by saying up or down in a menu list)–functionality that we intend to apply to our GUI in future work.",2 Related Work,[0],[0]
"Our work is also comparable to SDS toolkits such as IrisTK (Skantze and Moubayed, 2012) and OpenDial (Lison, 2015) which enable SDS designers to visualise the internal state of their systems, though not for end user interpretability.
",2 Related Work,[0],[0]
"Some of the work here is inspired by the Microsoft Language Understanding Intelligent Service (LUIS) project (Williams et al., 2015).",2 Related Work,[0],[0]
"While our system by no means achieves the scale that LUIS does, we offer here an additional contribution of an open source LUIS-like system (with the important addition of the graphical interface) that is authorable (using JSON files; we leave authoring using a web interface like that of LUIS to future work), extensible (affordances can be easily added), incremental (in that respect going beyond LUIS), trainable (i.e., can learn from examples, but can still function well without examples), and can learn through interacting (here we apply a user model that learns during interaction).",2 Related Work,[0],[0]
"This section introduces and describes our SDS, which is modularised into four main components: ASR, natural language understanding (NLU), dialogue management (DM), and the graphical user interface (GUI) which, as explained below, is visualised as a right-branching tree.",3 System Description,[0],[0]
The overall system is represented in Figure 1.,3 System Description,[0],[0]
"For the remainder of this section, each module is explained in
turn.",3 System Description,[0],[0]
"As each module processes input incrementally (i.e., word for word), we first explain our framework for incremental processing.",3 System Description,[0],[0]
An aspect of our SDS that sets it apart from others is the requirement that it process incrementally.,3.1 Incremental Dialogue,[0],[0]
"One potential concern with incremental processing is regarding informativeness: why act early when waiting might provide additional information, resulting in better-informed decisions?",3.1 Incremental Dialogue,[0],[0]
The trade off is naturalness as perceived by the user who is interacting with the SDS.,3.1 Incremental Dialogue,[0],[0]
"Indeed, it has been shown that human users perceive incremental systems as being more natural than traditional, turn-based systems (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991; Asri et al., 2014), offer a more human-like experience (Edlund et al., 2008) and are more satisfying to interact with than non-incremental systems (Aist et al., 2007).",3.1 Incremental Dialogue,[0],[0]
"Psycholinguistic research has also shown that humans comprehend utterances as they unfold and do not wait until the end of an utterance to begin the comprehension process (Tanenhaus et al., 1995; Spivey et al., 2002).
",3.1 Incremental Dialogue,[0],[0]
The trade-off between informativeness and naturalness can be reconciled when mechanisms are in place that allow earlier decisions to be repaired.,3.1 Incremental Dialogue,[0],[0]
"Such mechanisms are offered by the incremental unit (IU) framework for SDS (Schlangen and Skantze, 2011), which we apply here.",3.1 Incremental Dialogue,[0],[0]
"Following Kennington et al. (2014), the IU framework consists of a network of processing modules.",3.1 Incremental Dialogue,[0],[0]
"A typical module takes input, performs some kind of processing on that data, and produces output.
",3.1 Incremental Dialogue,[0],[0]
The data are packaged as the payload of incremental units (IUs) which are passed between modules.,3.1 Incremental Dialogue,[0],[0]
"The IUs themselves are interconnected via so-called same level links (SLL) and groundedin links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that sequence to convey what IUs directly affect it (see Figure 2 for an example of incremental ASR).",3.1 Incremental Dialogue,[0],[0]
"Thus IUs can be added, but can be later revoked and replaced in light of new information.",3.1 Incremental Dialogue,[0],[0]
"The IU framework can take advantage of up-to-date information, but have the potential to function in such a way that users perceive as more natural.
",3.1 Incremental Dialogue,[0],[0]
The modules explained in the remainder of this section are implemented as IU-modules and process incrementally.,3.1 Incremental Dialogue,[0],[0]
Each will now be explained.,3.1 Incremental Dialogue,[0],[0]
The module that takes speech input from the user in our SDS is the ASR component.,3.2 Speech Recognition,[0],[0]
"Incremental ASR must transcribe uttered speech into words which must be forthcoming from the ASR as early as possible (i.e., the ASR must not wait for endpointing to produce output).",3.2 Speech Recognition,[0],[0]
"Each module that follows must also process incrementally, acting in lock-step upon input as it is received.",3.2 Speech Recognition,[0],[0]
"Incremental ASR is not new (Baumann et al., 2009) and many of the current freely-accessible ASR systems can produce output (semi-) incrementally.",3.2 Speech Recognition,[0],[0]
We opt for Google ASR for its vocabulary coverage of our evaluation language (German).,3.2 Speech Recognition,[0],[0]
"Following, Baumann et al. (2016), we package output from the Google service into IUs which are passed to the NLU module, which we now explain.",3.2 Speech Recognition,[0],[0]
We approach the task of NLU as a slot-filling task (a very common approach; see Tur et al. (2012)) where an intent is complete when all slots of a frame are filled.,3.3 Language Understanding,[0],[0]
"The main driver of the NLU in
our SDS is the SIUM model of NLU introduced in Kennington et al. (2013).",3.3 Language Understanding,[0],[0]
"SIUM has been used in several systems which have reported substantial results in various domains, languages, and tasks (Han et al., 2015; Kennington et al., 2015; Kennington and Schlangen, 2017)",3.3 Language Understanding,[0],[0]
"Though originally a model of reference resolution, it was always intended to be used for general NLU, which we do here.",3.3 Language Understanding,[0],[0]
"The model is formalised as follows:
",3.3 Language Understanding,[0],[0]
P (I|U) = 1 P (U) P (I) ∑ r∈R P (U |R = r)P,3.3 Language Understanding,[0],[0]
"(R = r|I) (1)
That is, P (I|U) is the probability of the intent",3.3 Language Understanding,[0.9520826509800199],"['The perexample stochastic objective is defined as Ca = (R− R̄) ∑ t∈T − logP (t|q0), (4) where R is the reward and R̄ is the baseline, computed by the value network as: R̄ = σ(ST tanh(V (φa(v)‖ē) + b)), (5) where ē = 1N ∑N i=1 φb(ei), N = |q0 ∪ D0|, V ∈ Rd×2d and S ∈ Rd are weights and b ∈ R is a bias.']"
"I (i.e., a frame slot) behind the speaker’s (ongoing) utterance U .",3.3 Language Understanding,[0],[0]
"This is recovered using the mediating variable R, a set of properties which map between aspects of U and aspects of I .",3.3 Language Understanding,[0],[0]
"We opt for abstract properties here (e.g., the frame for restaurant might be filled by a certain type of cuisine intent such as italian which has properties like pasta, mediterranean, vegetarian, etc.).",3.3 Language Understanding,[0],[0]
Properties are pre-defined by a system designer and can match words that might be uttered to describe the intent in question.,3.3 Language Understanding,[0],[0]
"For P (R|I), probability is distributed uniformly over all properties that a given intent is specified to have.",3.3 Language Understanding,[0],[0]
"(If other information is available, more informative priors could be used as well.)",3.3 Language Understanding,[0],[0]
The mapping between properties and aspects of U can be learned from data.,3.3 Language Understanding,[0],[0]
"During application, R is marginalised over, resulting in a distribution over possible intents.1",3.3 Language Understanding,[0],[0]
"This occurs at each word increment, where the distribution from the previous increment is combined via P (I), keeping track of the distribution over time.
",3.3 Language Understanding,[0],[0]
We further apply a simple rule to add in apriori knowledge: if some r ∈ R and w ∈ U are such that r,3.3 Language Understanding,[0],[0]
".= w (where .= is string equality; e.g., an intent has the property of pasta and the word pasta is uttered), then we set C(U=w|R=r)=1.",3.3 Language Understanding,[0],[0]
"To allow for possible ASR confusions, we also apply C(U=w|R=r)= 1",3.3 Language Understanding,[0],[0]
"− ld(w, r)/max(len(w), len(r)), where ld is the Levenshtein distance (but we only apply this if the calculated value is above a threshold of 0.6; i.e., the two strings are mostly similar).",3.3 Language Understanding,[0],[0]
"For all otherw, C(w|r)=0.",3.3 Language Understanding,[0],[0]
"This results in a distribution C, which we renormalise and blend with learned distribution to yield P (U |R).
",3.3 Language Understanding,[0],[0]
"1In Kennington et al. (2013) the authors apply Bayes’ Rule to allow P (U |R) to produce a distribution over properties, which we adopt here.
",3.3 Language Understanding,[0],[0]
We apply an instantiation of SIUM for each slot.,3.3 Language Understanding,[0],[0]
"The candidate slots which are processed depends on the state of the dialogue; only slots represented by visible nodes are considered, thereby reducing the possible frames that could be predicted.",3.3 Language Understanding,[0],[0]
"At each word increment, the updated slots (and their corresponding) distributions are given to the DM, which will now be explained.",3.3 Language Understanding,[0],[0]
"The DM plays a crucial role in our SDS: as well as determining how to act, the DM is called upon to decide when to act, effectively giving the DM the control over timing of actions rather than relying on ASR endpointing–further separating our SDS from other systems.",3.4 Dialogue Manager,[0],[0]
"The DM policy is based on a confidence score derived from the NLU (in this case, we used the distribution’s argmax value) using thresholds for the actions (see below), set by hand (i.e., trial and error).",3.4 Dialogue Manager,[0],[0]
"At each word and resulting distribution from NLU, the DM needs to choose one of the following:
• wait – wait for more information (i.e., for the next word)
• select – as the NLU is confident enough, fill the slot can with the argmax from NLU
• request – signal a (yes/no) clarification request on the current slot and the proposed filler
• confirm – act on the confirmation of the user; in effect, select the proposed slot value
Though the thresholds are statically set, we applied OpenDial (Lison, 2015) as an IU-module to perform the task of the DM with the future goal that these values could be adjusted through reinforcement learning (which OpenDial could provide).",3.4 Dialogue Manager,[0],[0]
"The DM processes and makes a decision for each slot, with the assumption that only one slot out of all that are processed will result in an non-wait action (though this is not enforced).",3.4 Dialogue Manager,[0],[0]
The goal of the GUI is to intuitively inform the user about the internal state of the ongoing understanding.,3.5 Graphical User Interface,[0],[0]
"One motivation for this is that the user can determine if the system understood the user’s intent before providing the user with a response
(e.g., a list of restaurants of a certain type); i.e., if any misunderstanding takes place, it happens before the system commits to an action and is potentially more easily repaired.
",3.5 Graphical User Interface,[0],[0]
"The display is a rightbranching tree, where the branches directly off the root node display the affordances of the system (i.e., what domains of things it can understand and do something about).",3.5 Graphical User Interface,[0],[0]
"When the first tree is displayed, it represents a state of the NLU where none of the slots are filled, as in Figure 3.
",3.5 Graphical User Interface,[0],[0]
"When a user verbally selects a domain to ask about, the tree is adjusted to make that domain the only one displayed and
the slots that are required for that domain are shown as branches.",3.5 Graphical User Interface,[0],[0]
"The user can then fill those slots (i.e., branches) by uttering the displayed name, or, alternatively, by uttering the item to fill the slot directly.",3.5 Graphical User Interface,[0],[0]
"For example, at a minimum, the user could utter the name of the domain then an item for each slot (e.g., food Thai downtown) or the speech could be more natural (e.g., I’m quite hungry, I am looking for some Thai food maybe in the downtown area).",3.5 Graphical User Interface,[0],[0]
"Crucially, the user can also hesitate within and between chunks, as advancement is not triggered by silence thresholding, but rather semantically.",3.5 Graphical User Interface,[0],[0]
"When something is uttered that falls into the request state of the DM as explained above, the display expands the subtree under question and marks the item with a question mark (see Figure 4).",3.5 Graphical User Interface,[0],[0]
"At this point, the user can utter any kind of confirmation.",3.5 Graphical User Interface,[0],[0]
A positive confirmation fills the slot with the item in question.,3.5 Graphical User Interface,[0],[0]
"A negative confirmation retracts the question, but leaves the branch expanded.",3.5 Graphical User Interface,[0],[0]
The expanded branches are displayed according to their rank as given by the NLU’s probability distribution.,3.5 Graphical User Interface,[0],[0]
"Though a branch in the display can theoretically display an unlimited number of children, we opted to only show 7 children; if a branch had more, the final child displayed as an ellipsis.
",3.5 Graphical User Interface,[0],[0]
"A completed branch is collapsed, visually marking its corresponding slot as filled.",3.5 Graphical User Interface,[0],[0]
"At any
time, a user can backtrack by saying no (or equivalent) or start the entire interaction over from the beginning with a keyword, e.g., restart.",3.5 Graphical User Interface,[0],[0]
"To aid the user’s attention, the node under question is marked in red, where completed slots are represented by outlined nodes, and filled nodes represent candidates for the current slot in question (see examples of all three in Figure 4).",3.5 Graphical User Interface,[0],[0]
"For cases where the system is in the wait state for several words (during which there is no change in the tree), the system signals activity at each word by causing the red node in question to temporarily change to white, then back to red (i.e., appearing as a blinking node to the user).",3.5 Graphical User Interface,[0],[0]
"Figure 5 shows a filled frame, represented as tree with one branch for each filled slot.
",3.5 Graphical User Interface,[0],[0]
Figure 5: Example tree where all of the slots are filled.,3.5 Graphical User Interface,[0],[0]
"(i.e., domain:food, location:university, type:thai)
",3.5 Graphical User Interface,[0],[0]
Such an interface clearly shows the internal state of the SDS and whether or not it has understood the request so far.,3.5 Graphical User Interface,[0],[0]
"It is designed to aid the user’s attention to the slot in question, and clearly indicates the affordances that the system has.",3.5 Graphical User Interface,[0],[0]
"The interface is currently a read-only display that is purely speech-driven, but it could be augmented with additional functionalities, such as tapping a node for expansion or typing input that the system might not yet display.",3.5 Graphical User Interface,[0],[0]
"It is currently implemented as a web-based interface (using the JavaScript D3 library), allowing it to be usable as a web application on any machine or mobile device.
",3.5 Graphical User Interface,[0],[0]
"Adaptive Branching The GUI as explained affords an additional straight-forward extension: in order to move our system towards adaptivity on the above-mentioned continuum, the GUI can be used to signal what the system thinks the user might say next.",3.5 Graphical User Interface,[0],[0]
"This is done by expanding a branch and displaying a confirmation on that branch, signalling that the system predicts that the user will choose that particular branch.",3.5 Graphical User Interface,[0],[0]
"Alternatively, if the system is confident that a user will fill a slot with a particular value, that particular slot can be filled without confirmation.",3.5 Graphical User Interface,[0],[0]
This is displayed as a collapsed tree branch.,3.5 Graphical User Interface,[0],[0]
"A system that perfectly predicts a user’s intent would fill an entire tree (i.e., all slots) only requiring the user to confirm once.",3.5 Graphical User Interface,[0],[0]
A more careful system would confirm at each step (such an interaction would only require the user to utter confirmations and nothing else).,3.5 Graphical User Interface,[0],[0]
We applied this adaptive variant of the tree in one of our experiments explained below.,3.5 Graphical User Interface,[0],[0]
"In this section, we describe two experiments where we evaluated our system.",4 Experiments,[0],[0]
It is our primary goal to show that our GUI is useful and signals understanding to the user.,4 Experiments,[0],[0]
We also wish to show that incremental presentation of such a GUI is more effective than an endpointed system.,4 Experiments,[0],[0]
We further want to show that an adaptive system is more effective than a non-adaptive system (though both would process incrementally).,4 Experiments,[0],[0]
"In order to best evaluate our system, we recruited participants to interact with our system in varied settings to compare endpointed (i.e., non-incremental) and nonadaptive as well as adaptive versions.",4 Experiments,[0],[0]
"We describe how the data were collected from the participants, then explain each experiment and give results.",4 Experiments,[0],[0]
The participants were seated at a desk and given written instructions indicating that they were to use the system to perform as many tasks as possible in the allotted time.,4.1 Task & Procedure,[0],[0]
Figure 6 shows some example tasks as they would be displayed (one at a time) to the user.,4.1 Task & Procedure,[0],[0]
"A screen, tablet, and keyboard were on the desk in front of the user (see Figure",4.1 Task & Procedure,[0],[0]
7).2,4.1 Task & Procedure,[0],[0]
"The user was instructed to convey the task presented on the screen to the system such
2We used a Samsung 8.4 Pro tablet turned to its side to show a larger width for the tree to grow to the right.",4.1 Task & Procedure,[0],[0]
"The tablet only showed the GUI; the SDS ran on a separate computer.
that the GUI on the tablet would have a completed tree (e.g., as in Figure 5).",4.1 Task & Procedure,[0],[0]
"When the participant was satisfied that the system understood her intent, she was to press space bar on the keyboard which triggered a new task to be displayed on the screen and reset the tree to its start state on the tablet (as in Figure 3).
",4.1 Task & Procedure,[0],[0]
"The possible task domains were call, which had a single slot for name to be filled (i.e., one out of the 22 most common German given names); message which had a slot for name and a slot for the message (which, when invoked, would simply fill in directly from the
ASR until 1 second of silence was detected); eat which had slots for type (in this case, 6 possible types) and location (in this case, 6 locations based around the city of Bielefeld); route which had slots for source city and the destination city (which shared the same list of the top 100 most populous German cities); and reminder which had a slot for message.
",4.1 Task & Procedure,[0],[0]
"For each task, the domain was first randomly chosen from the 5 possible domains, and then each slot value to be filled was randomly chosen (the message slot for the name and message domains was randomly selected from a list of 6 possible “messages”, each with 2-3 words; e.g., feed the cat, visit grandma, etc.).",4.1 Task & Procedure,[0],[0]
The system kept track of which tasks were already presented to the participant.,4.1 Task & Procedure,[0],[0]
"At any time after the first task, the system could choose a task that was previously presented and present it again to the participant (with a 50% chance) so the user would often see tasks that she had seen before (with the assumption that humans who use PAs often do perform similar, if not the same, tasks more than once).
",4.1 Task & Procedure,[0],[0]
"The participant was told that she would interact with the system in three different phases, each for 4 minutes, and to accomplish as many tasks as possible in that time allotment.",4.1 Task & Procedure,[0],[0]
The participant was not told what the different phases were.,4.1 Task & Procedure,[0],[0]
"The experiments described in Sections 4.2 and
4.3 respectively describe and report a comparison first between the Phase 1 and 2 (denoted as the endpointed and incremental variants of the system) in order to establish whether or not the incremental variant produced better results than the endpointed variant.",4.1 Task & Procedure,[0],[0]
We also report a comparison between Phase 2 and 3 (incremental and incremental-adaptive phases).,4.1 Task & Procedure,[0],[0]
Phase 1 and Phase 3 are not directly comparable to each other as Phase 3 is really a variant of Phase 2.,4.1 Task & Procedure,[0],[0]
"Because of this, we fixed the order of the phase presentation for all participants.",4.1 Task & Procedure,[0],[0]
Each of these phases are described below.,4.1 Task & Procedure,[0],[0]
"Before the participant began Phase 1, they were able to try it out for up to 4 minutes (in Phase 1 settings) and ask for help from the experimenter, allowing them to get used to the Phase 1 interface before the actual experiment began.",4.1 Task & Procedure,[0],[0]
"After this trial phase, the experiment began with Phase 1.
",4.1 Task & Procedure,[0],[0]
"Phase 1: Non-incremental In this phase, the system did not appear to work incrementally; i.e., the system displayed tree updates after ASR endpointing (of 1.2 seconds–a reasonable amount of time to expect a response from a commercial spoken PA).",4.1 Task & Procedure,[0],[0]
The system displayed the ongoing ASR on the tablet as it was recognised (as is often done in commercial PAs).,4.1 Task & Procedure,[0],[0]
"At the end of Phase 1, a pop up window notified the user that the phase was complete.",4.1 Task & Procedure,[0],[0]
"They then moved onto Phase 2.
",4.1 Task & Procedure,[0],[0]
"Phase 2: Incremental In this phase, the system displayed the tree information incrementally without endpointing.",4.1 Task & Procedure,[0],[0]
"The ASR was no longer displayed; only the tree provided feedback in understanding, as explained in Section 3.5.
",4.1 Task & Procedure,[0],[0]
"After Phase 2, a 10-question questionnaire was displayed on the screen for the participant to fill out comparing Phase 1 and Phase 2.",4.1 Task & Procedure,[0],[0]
"For each question, they had the choice of Phase 1, Phase
2, Both, and Neither.",4.1 Task & Procedure,[0],[0]
(See Appendix for full list of questions.),4.1 Task & Procedure,[0],[0]
"After completing the questionnaire, they moved onto Phase 3.
",4.1 Task & Procedure,[0],[0]
"Phase 3: Incremental-adaptive In this phase, the incremental system was again presented to the participant with an added user model that “learned” about the user.",4.1 Task & Procedure,[0],[0]
"If the user saw a task more than once, the user model would predict that, if the user chose that task domain again (e.g., route) then the system would automatically ask a clarification using the previously filled values (except for the message slot, which the user always had to fill).",4.1 Task & Procedure,[0],[0]
"If the user saw a task more than 3 times, the system skipped asking for clarifications and filled in the domain slots completely, requiring the user only to press the space bar to confirm it was the correct one (i.e., to complete the task).",4.1 Task & Procedure,[0],[0]
"An example progression might be as follows: a participant is presented with the task route from Bielefeld to Berlin, then the user would attempt to get the system to fill in the tree (i.e., slots) with those values.",4.1 Task & Procedure,[0],[0]
"After some interaction in other domains, the user sees the same task again, and now after indicating the intent type route, the user must only say “yes” for each slot to confirm the system’s prediction.",4.1 Task & Procedure,[0],[0]
"Later, if the task is presented a third time, when entering that domain (i.e, route), the two slots would already be filled.",4.1 Task & Procedure,[0],[0]
"If later a different route task was presented, e.g., route from Bielefeld to Hamburg, the system would already have the two slots filled, but the user could backtrack by saying “no, to Hamburg” which would trigger the system to fill the appropriate slot with the corrected value.",4.1 Task & Procedure,[0],[0]
"Later interactions within the route domain would ask for a clarification on the destination slot since it has had several possible values given by the participant, but continue to fill the from slot with Bielefeld.
",4.1 Task & Procedure,[0],[0]
"After Phase 3, the participants were presented with another questionnaire on the screen to fill out with the same questions (plus two additional questions), this time comparing Phase 2 and Phase 3.",4.1 Task & Procedure,[0],[0]
"For each item, they had the choice of Phase 2, Phase 3, Both, and Neither.",4.1 Task & Procedure,[0],[0]
"At the end of the three phases and questionnaires, the participants were given a final questionnaire to fill out by hand on their general impressions of the systems.
",4.1 Task & Procedure,[0],[0]
We recruited 14 participants for the evaluation.,4.1 Task & Procedure,[0],[0]
"We used the Mint tools data collection framework (Kousidis et al., 2012) to log the interactions.",4.1 Task & Procedure,[0],[0]
"Due to some technical issues, one of the participants
did not log interactions.",4.1 Task & Procedure,[0],[0]
"We collected data from 13 participants, post-Phase 2 questionnaires from 12 participants, post-Phase 3 questionnaires from all 14 participants, and general questionnaires from all 14 participants.",4.1 Task & Procedure,[0],[0]
"In the experiments that follow, we report objective and subjective measures to determine the settings that produced superior results.
",4.1 Task & Procedure,[0],[0]
Metrics We report the subjective results of the participant questionnaires.,4.1 Task & Procedure,[0],[0]
We only report those items that were statistically significant (see Appendix for a full list of the questions).,4.1 Task & Procedure,[0],[0]
"We further report objective measures for each system variant: total number of completed tasks, fully correct frames, average frame f-score, and average time elapsed (averages are taken over all participants for each variant; we only used the 10 participants who fully interacted with all three phases).",4.1 Task & Procedure,[0],[0]
Discussion is left to the end of this section.,4.1 Task & Procedure,[0],[0]
"In this section we report the results of the evaluation between the endpointed (i.e., nonincremental; Phase 1) variant vs the incremental (Phase 2) variant of our system.
",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"Subjective Results We applied a multinomial test of significance to the results, treating all four possible answers as equally likely (with Bonferroni correction of 10).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"The item The interface was useful and easy to understand with the answer of Both was significant (χ2 (4, N = 12)",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"= 9.0, p < .005), as was The assistant was easy and intuitive to use also with the answer Both (χ2 (4, N = 12) = 9.0, p < .005).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"The item I always understood what the system wanted from me was also answered Both significantly more times than other answers (χ2 (4, N = 14) = 9.0, p< .005), similarly for It was sometimes unclear to me if the assistant understood me with the answer of Both (χ2 (4, N = 12) = 10.0, p < .005).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"These responses tell us that though the participants did not report preference for either system variant, they reported a general positive impression of the GUI (in both variants).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"This is a nice result; the GUI could be used in either system with benefit to the users.
",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
Objective Results The endpointed (Phase 1) and incremental (Phase 2) columns in Table 1 show the results of the objective evaluation.,4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"Though the average time per task and fscore for the endpointed variant are better than those of the
incremental variant, the total number of tasks for the incremental variant was higher.
",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"Manual inspection of logs indicate that participants took advantage of the system’s flexibility of understanding instalments (i.e., filling frames incrementally).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"This is evidenced in that participants often uttered words understood by the system as being negative (e.g., nein/no), either as a result of an explicit confirmation request by the system (e.g., Thai?) or after a slot was incorrectly filled (something very easily determined through the GUI).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"This is a desired outcome of using our system; participants were able to repair local areas of misunderstanding as they took place instead of needing to correct an entire intent (i.e., frame).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"However, we cannot fully empirically measure these tendencies given our data.",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"In this section we report results for the evaluation between the incremental (Phase 2) and incremental-adaptive (henceforth just adaptive; Phase 3) systems.
",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
Subjective Results We applied the same significance test as Experiment 1 (with Bonferroni correction of 12).,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"The item The interface was useful and easy to understand was answered with Both significantly (χ2 (4, N = 14)",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"= 10.0, p < .0042), The item I had the feeling that the assistant attempted to learn about me was answered with Neither (χ2 (4, N = 14) = 8.0, p < .0042), though Phase 3 was also marked (6 times).",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
All other items were not significant.,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
Here again we see that there is a general positive impression of the GUI under all conditions.,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"If anyone noticed that a system variant was attempting to learn a user model at all, they noticed that it was in Phase 3, as expected.
",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"Objective Results The incremental (Phase 2) and adaptive (Phase 3) columns in Table 1 show
the results for the objective evaluation for this experiment.",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"There is a clear difference between the two variants, with the adaptive showing more completed tasks, more fully correct frames, and a higher average fscore (all three likely due to the fact that frames were potentially pre-filled).",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"While the responses don’t express any preference for a particular system variant, the overall impression of the GUI was positive.",4.4 Discussion,[0],[0]
"The objective measures show that there are gains to be made when the system signals understanding at a more finegrained interval than at the utterance level, due to the higher number of completed tasks and locallymade repairs.",4.4 Discussion,[0],[0]
"There are further gains to be made when the system applies simple user modelling (i.e., adaptivity) by attempting to predict what the user might want to do in a chosen domain, decreasing the possibility of user error and allowing the system to accurately and quickly complete more tasks.",4.4 Discussion,[0],[0]
"Participants also didn’t just get used to the system over time, as the average time per episode was fairly similar in all three phases.
",4.4 Discussion,[0],[0]
The open-ended questionnaire sheds additional light.,4.4 Discussion,[0],[0]
"Most of the suggestions for improvement related to ASR misrecognition and speed (i.e., not about the system itself).",4.4 Discussion,[0],[0]
Two participants suggested an ability to add “free input” or select alternatives from the tree.,4.4 Discussion,[0],[0]
"Two participants suggested that the system be more responsive (i.e., in wait states), and give more feedback (i.e., backchannels) more often.",4.4 Discussion,[0],[0]
"For those participants that expressed preference to the non-incremental system (Phase 1), none of them had used a speech-based PA before, whereas those that expressed preference to the incremental versions (Phases 2 and 3) use them regularly.",4.4 Discussion,[0],[0]
"We conjecture that people without SDS experience equate understanding with ASR, whereas those that are more familiar with PAs know that perfect ASR doesn’t translate to perfect understanding–hence the need for a GUI.",4.4 Discussion,[0],[0]
"A potential remedy would be to display ASR with the tree, signalling understanding despite ASR errors.",4.4 Discussion,[0],[0]
"Given the results and analysis, we conclude that an intuitive presentation that signals a system’s ongoing understanding benefits end users who perform simple tasks which might be performed by a PA.",5 Conclusion & Future Work,[0],[0]
"The GUI that we provided, using a right-branching
tree, worked well; indeed, the participants who used it found it intuitive and easy to understand.",5 Conclusion & Future Work,[0],[0]
There are gains to be made when the system signals understanding at finer-grained levels than just at the end of a pre-formulated utterance.,5 Conclusion & Future Work,[0],[0]
There are further gains to be made when a PA attempts to learn (even a rudimentary) user model to predict what the user might want to do next.,5 Conclusion & Future Work,[0],[0]
"The adaptivity moves our system from one extreme of the continuum–simple slot filling–closer towards the extreme that is fully predictive, with the additional benefit of being able to easily correct mistakes in the predictions.
",5 Conclusion & Future Work,[0],[0]
"For future work, we intend to provide simple authoring tools for the system to make building simple PAs using our GUI easy.",5 Conclusion & Future Work,[0],[0]
We want to improve the NLU and scale to larger domains.3,5 Conclusion & Future Work,[0],[0]
"We also plan on implementing this as a standalone application that could be run on a mobile device, which could actually perform the tasks.",5 Conclusion & Future Work,[0],[0]
"It would further be beneficial to compare the GUI with a system that responds with speech (i.e., without a GUI).",5 Conclusion & Future Work,[0],[0]
"Lastly, we will investigate using touch as an additional input modality to select between possible alternatives that are offered by the system.
",5 Conclusion & Future Work,[0],[0]
Acknowledgements Thanks to the anonymous reviewers who provided useful comments and suggestions.,5 Conclusion & Future Work,[0],[0]
Thanks also to Julian Hough for helping with experiments.,5 Conclusion & Future Work,[0],[0]
"We acknowledge support by the Cluster of Excellence “Cognitive Interaction Technology” (CITEC; EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG), and the BMBF KogniHome project.
",5 Conclusion & Future Work,[0],[0]
"Appendix The following questions were asked on both questionnaires following Phase 2 and Phase 3 (comparing the two most latest used system versions; as translated into English):
•",5 Conclusion & Future Work,[0],[0]
The interface was useful and easy to understand.,5 Conclusion & Future Work,[0],[0]
• The assistant was easy and intuitive to use.,5 Conclusion & Future Work,[0],[0]
• The assistant understood what I wanted to say.,5 Conclusion & Future Work,[0],[0]
• I always understood what the system wanted from me.,5 Conclusion & Future Work,[0],[0]
•,5 Conclusion & Future Work,[0],[0]
The assistant made many mistakes.,5 Conclusion & Future Work,[0],[0]
• The assistant did not respond while I spoke.,5 Conclusion & Future Work,[0],[0]
"3Kennington and Schlangen (2017) showed that our chosen NLU approach can scale fairly well, but the GUI has some limits when applied to larger domains with thousands of items.",5 Conclusion & Future Work,[0],[0]
"We leave improved scaling to future work.
",5 Conclusion & Future Work,[0],[0]
"• It was sometimes unclear to me if the assistant understood me.
",5 Conclusion & Future Work,[0],[0]
• The assistant responded while I spoke.,5 Conclusion & Future Work,[0],[0]
• The assistant sometimes did things that I did not expect.,5 Conclusion & Future Work,[0],[0]
"• When the assistant made mistakes, it was easy for me
to correct them.
",5 Conclusion & Future Work,[0],[0]
"In addition to the above 10 questions, the following were also asked on the questionnaire following Phase 3: • I had the feeling that the assistant attempted to learn
about me.
",5 Conclusion & Future Work,[0],[0]
"• I had the feeling that the assistant made incorrect guesses.
",5 Conclusion & Future Work,[0],[0]
The following questions were used on the general questionnaire:,5 Conclusion & Future Work,[0],[0]
"• I regularly use personal assistants such as Siri, Cortana,
Google now or Amazon Echo:",5 Conclusion & Future Work,[0],[0]
"Yes/No
• I have never used a speech-based personal assistant: Yes/No
• What was your general impression of our personal assistants?
",5 Conclusion & Future Work,[0],[0]
• Would you use one of these assistants on a smart phone or tablet if it were available?,5 Conclusion & Future Work,[0],[0]
"If yes, which one?
• Do you have suggestions that you think would help us improve our assistants?
",5 Conclusion & Future Work,[0],[0]
"• If you have used other speech-based interfaces before, do you prefer this interface?",5 Conclusion & Future Work,[0],[0]
"Arguably, spoken dialogue systems are most often used not in hands/eyes-busy situations, but rather in settings where a graphical display is also available, such as a mobile phone.",abstractText,[0],[0]
We explore the use of a graphical output modality for signalling incremental understanding and prediction state of the dialogue system.,abstractText,[0],[0]
"By visualising the current dialogue state and possible continuations of it as a simple tree, and allowing interaction with that visualisation (e.g., for confirmations or corrections), the system provides both feedback on past user actions and guidance on possible future ones, and it can span the continuum from slot filling to full prediction of user intent (such as GoogleNow).",abstractText,[0],[0]
"We evaluate our system with real users and report that they found the system intuitive and easy to use, and that incremental and adaptive settings enable users to accomplish more tasks.",abstractText,[0],[0]
Supporting Spoken Assistant Systems with a Graphical User Interface that Signals Incremental Understanding and Prediction State,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 640–645 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
640",text,[0],[0]
"In structured input-output models as used in tasks like translation and image captioning, the attention variable decides which part of the input aligns to the current output.",1 Introduction,[0],[0]
"Many attention mechanisms have been proposed (Xu et al., 2015; Bahdanau et al., 2014; Luong et al., 2015; Martins and Astudillo, 2016) but the de facto standard is a soft attention mechanism that first assigns attention weights to input encoder states, then computes an attention weighted ’soft’ aligned input state, which finally derives the output distribution.",1 Introduction,[0],[0]
"This method is end to end differentiable and easy to implement.
",1 Introduction,[0],[0]
Another less popular variant is hard attention that aligns each output to exactly one input state but requires intricate training to teach the network to choose that state.,1 Introduction,[0],[0]
"When successfully trained, hard attention is often found to be more accurate (Xu et al., 2015; Zaremba and Sutskever, 2015).",1 Introduction,[0],[0]
"In NLP, a recent success has been in a monotonic hard attention setting in morphological inflection tasks (Yu et al., 2016; Aharoni and Goldberg, 2017).",1 Introduction,[0],[0]
"For general seq2seq learning, methods like SparseMax (Martins and Astudillo, 2016) and local attention (Luong et al., 2015) were proposed to bridge the gap between soft and hard attention.
",1 Introduction,[0],[0]
"∗Both authors contributed equally to this work
In this paper we propose a surprisingly simpler alternative based on the original joint distribution between output and attention, of which existing soft and hard attention mechanisms are approximations.",1 Introduction,[0],[0]
"The joint model couples input states individually to the output like in hard attention, but it combines the advantage of end-to-end trainability of soft attention.",1 Introduction,[0],[0]
"When the number of input states is large, we propose to use a simple approximation of the full joint distribution called Beam-joint.",1 Introduction,[0],[0]
"This approximation is also easily trainable and does not suffer from the high variance of Monte-Carlo sampling gradients of hard attention.
",1 Introduction,[0],[0]
"We evaluated our model on five translation tasks and increased BLEU by 0.8 to 1.7 over soft attention, which in turn was better than hard and the recent Sparsemax (Martins and Astudillo, 2016) attention.",1 Introduction,[0],[0]
"More importantly, the training process was as easy as soft attention.",1 Introduction,[0],[0]
"For further support, we also evaluate on two morphological inflection tasks and got gains over soft and hard attention.",1 Introduction,[0],[0]
For sequence to sequence (seq2seq) learning the encoder-decoder model is the standard and we review it here.,2 Background and Related Work,[0],[0]
We then review related work on attention mechanisms on these models.,2 Background and Related Work,[0],[0]
"Let x1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", xm denote the tokens in the input sequence that have been transformed by an encoder network to state vectors x1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", xm, which we jointly denote as x1...m. Let y1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", yn denote the output tokens in the target sequence.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"The Encoder-Decoder (ED) network factorizes Pr(y1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", yn|x1...m) as ∏n t=1 Pr(yt|x1...m, st) where st is a decoder state summarizing y1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
yt−1.,2.1 Attention-based Encoder Decoder Model,[0],[0]
"For each t, a hidden attention variable at is used to denote which part of x1...m aligns with yt.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Let P (at = j|x1...m, st) denote the
probability that encoder state xj is relevant for output yt.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Typically this is estimated using a softmax function over attention scores computed from xj and decoder state st as follows.
",2.1 Attention-based Encoder Decoder Model,[0],[0]
"P (at = j|x1...m, st) =",2.1 Attention-based Encoder Decoder Model,[0],[0]
"eAθ(xj ,st)∑m r=1",2.1 Attention-based Encoder Decoder Model,[0],[0]
"e Aθ(xr,st) (1)
where Aθ(., .) is the attention unit that scores each input state xj as per the decoder state st.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Thereafter, in the popular soft-attention mechanism, the attention weighted sum of the input states is used to model log likelihood for each yt as
log Pr(yt|x1...m) = log Pr(yt| ∑ a Pt(a)xa) (2)
where Pt(at = j) is the short form for P (at = j|x1...m, st).",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Also, here and in the rest of the paper we drop st from P (yt) and Pt(a) for ease of notation.",2.1 Attention-based Encoder Decoder Model,[0],[0]
The weighted sum ∑ a Pt(a)xa is called an input context ct which is fed to the decoder RNN along with yt for computing the next state st+1.,2.1 Attention-based Encoder Decoder Model,[0],[0]
"We next review existing attention types.
",2.2 Related Work,[0],[0]
"Soft Attention is the attention method described in the previous section and is the current standard for seq2seq learning (Xu Chen, 2018; Koehn, 2017).",2.2 Related Work,[0],[0]
"It was proposed for translation in (Bahdanau et al., 2014) and refined further in (Luong et al., 2015).",2.2 Related Work,[0],[0]
"As shown in Eq 2, here each output is derived from an attention averaged input.",2.2 Related Work,[0],[0]
This diffuses the coupling between the input and output.,2.2 Related Work,[0],[0]
"The advantage of soft attention is end to end differentiability, and fast training and inference.
",2.2 Related Work,[0],[0]
"Hard Attention was proposed in its current form in (Xu et al., 2015) and attends to exactly one input state for an output1.",2.2 Related Work,[0],[0]
"During training, log-likelihood is an expectation over sampled attentions:
logPt(yt|x1...m) = M∑ l=1 logPt(yt|xãl) (3)
where ã1, . . .",2.2 Related Work,[0],[0]
", ãM are sampled from the multinomial Pt(a).",2.2 Related Work,[0],[0]
"Because of the sampling, the gradient has to be computed by Monte Carlo gradient/REINFORCE (Williams, 1992) and is subject to high variance.",2.2 Related Work,[0],[0]
"Many tricks are required to train
1Note, attention on a single input encoder state does not imply attention on a single input token because RNNs or selfattention capture the context around the token.
hard attention and there is little standardization across implementations.",2.2 Related Work,[0],[0]
Xu et al (2015) use a combination of REINFORCE and soft attention.,2.2 Related Work,[0],[0]
Zaremba et al(2015) uses curriculum learning that starts as soft-attention and gradually becomes discrete.,2.2 Related Work,[0],[0]
"Ling& Rush (2017) aggregates multiple samples during training, and a single sampled attention while testing.",2.2 Related Work,[0],[0]
"However, once trained well the sharp focus on memory provided by hard-attention has been found to yield superior performance (Xu et al., 2015; Shankar and Sarawagi, 2018).
",2.2 Related Work,[0],[0]
Sparse/Local Attention Many attempts have been made to bridge the gap between soft and hard attention.,2.2 Related Work,[0],[0]
Luong et al (2015) proposes local attention that averages a window of input.,2.2 Related Work,[0],[0]
"This has been refined later to include syntax (Chen et al., 2017; Sennrich and Haddow, 2016; Chen et al., 2018).",2.2 Related Work,[0],[0]
"Another idea is to replace the softmax in soft attention with sparsity inducing operators (Martins and Astudillo, 2016; Niculae and Blondel, 2017).",2.2 Related Work,[0],[0]
"However, all sparse/local attention methods continue to compute P (y) from an attention weighted sum of inputs (Eq: 2) unlike hard attention.",2.2 Related Work,[0],[0]
"We start from an explicit joint representation of the uncertainty of the attention and output variables.
",3 Joint Attention-Output Models,[0],[0]
logPt(yt|x1...m) = log ∑ a Pt(a)Pt(yt|xa),3 Joint Attention-Output Models,[0],[0]
"(4)
The joint model directly couples individual input states to the output, and thus is a type of hard attention.",3 Joint Attention-Output Models,[0],[0]
"Also, by taking an expectation, instead of a single hard attention, it enjoys differentiability as in soft-attention.",3 Joint Attention-Output Models,[0],[0]
"We call this the full-joint method.
",3 Joint Attention-Output Models,[0],[0]
"Unfortunately, either when the vocabulary or the number of encoder states (m) is large, full-joint is not practical.",3 Joint Attention-Output Models,[0],[0]
Existing hard and soft attentions can be viewed as its approximations that either marginalize early or hard select attention.,3 Joint Attention-Output Models,[0],[0]
We show a surprisingly simple alternative approximation that provides hard attention without its training complexity.,3 Joint Attention-Output Models,[0],[0]
"Our method called Beam-joint deterministically selects the top-k highest attention values and approximates the full joint log probability as
logPt(yt|x1...m)",3 Joint Attention-Output Models,[0],[0]
"≈ log ∑
a∈TopK(Pt(a))
Pt(a)Pt(yt|xa)",3 Joint Attention-Output Models,[0],[0]
"(5)
Thus, in beam-joint, we first compute the multinomial attention distribution in O(m) time using
Eq 1, select the Top-K input positions from the multinomial, next with hard attention on each position compute K output softmax, and finally compute the attention weighted output mixture distribution.",3 Joint Attention-Output Models,[0],[0]
The number of output softmax is K times in normal soft-attention but the actual running time overhead is only 20–30% for translation tasks.,3 Joint Attention-Output Models,[0],[0]
We used the default pass-through TopK operator (which is not differentiable) and optimize the beamapproximation directly.,3 Joint Attention-Output Models,[0],[0]
"We also experimented with a version which smoothly shifts from soft-attention to beam-attention, but found that training the beamapproximation directly leads to best results.
",3 Joint Attention-Output Models,[0],[0]
We show empirically that this very simple scheme is surprisingly effective compared to existing hard and soft attention over several translation tasks.,3 Joint Attention-Output Models,[0],[0]
"Unlike sampling and variational methods that require careful tuning and exotic tricks during training, this simple scheme trains as easily as softattention, without significant increase in training time because even K = 6 works well enough.
",3 Joint Attention-Output Models,[0],[0]
"Another reason why our ’sum of probabilities’ form performs better could be the softmax barrier effect highlighted in (Yang et al., 2018).",3 Joint Attention-Output Models,[0],[0]
The authors argue that the richness of natural language cannot be captured in normal softmax due to the low rank constraint it imposes on input-to-output matrix.,3 Joint Attention-Output Models,[0],[0]
They improve performance using a Mixture of Softmax model.,3 Joint Attention-Output Models,[0],[0]
Our beam-joint also is a mixture of softmax and possibly achieves higher rank than a single softmax.,3 Joint Attention-Output Models,[0],[0]
"However their mixture requires learning multiple softmax matrices, whereas ours are due to varying attention and we do not learn any extra parameters than soft attention.",3 Joint Attention-Output Models,[0],[0]
We compare attention models on two NLP tasks: machine translation and morphological inflection.,4 Experiments,[0],[0]
We experiment on five language pairs from three datasets:,4.1 Machine translation,[0],[0]
"IWSLT15 English↔Vietnamese (Cettolo et al., 2015) which contains 133k train, 1.5k validation(tst2012) and 1.2k test(tst2013) sentence pairs respectively; IWSLT14 German↔English (Cettolo et al., 2014) which contains 160k train, 7.2k validation and 6.7k test sentence pairs respectively ; Workshop on Asian Translation 2017 Japanese→English",4.1 Machine translation,[0],[0]
"(Nakazawa et al., 2016) which contains 2M train, 1.8k validation and 1.8k test sentence pairs respectively.",4.1 Machine translation,[0],[0]
"We use a 2 layer bi-
directional encoder and a 2 layer unidirectional decoder with 512 hidden LSTM units and 0.2 dropout rate with vanilla SGD optimizer.",4.1 Machine translation,[0],[0]
We base our implementation2 on the NMT code3 in Tensorflow.,4.1 Machine translation,[0],[0]
"We did no special hyper-parameter tuning and used standard-softmax tuned parameters on a batch size of 64.
",4.1 Machine translation,[0],[0]
Comparing attention models We compare beam-joint (default K = 6) with standard soft and hard attention.,4.1 Machine translation,[0],[0]
"To further dissect the reasons behind beam-joint’s gains, we compare beam-joint with a sampling based approximation of full-joint called Sample-Joint that replaces the TopK in Eq 5 with K attention weighted samples.",4.1 Machine translation,[0],[0]
We train samplejoint as well as hard-attention with REINFORCE with 6-samples.,4.1 Machine translation,[0],[0]
"Also to ascertain that our gains are not explained by sparsity alone, we compare with Sparsemax (Martins and Astudillo, 2016).
",4.1 Machine translation,[0],[0]
In Table 1 we show perplexity and BLEU with three beam sizes (B).,4.1 Machine translation,[0],[0]
"Beam-joint significantly outperforms all other variants, including the standard soft attention by 0.8 to 1.7 BLEU points.",4.1 Machine translation,[0],[0]
The perplexity shows even a more impressive drop in all five datasets.,4.1 Machine translation,[0],[0]
"Also we observe training times for beam-joint to be only 20–30% higher than softattention, establishing that beam-joint is both practical and more accurate.
",4.1 Machine translation,[0],[0]
Sample-joint is much worse than beam-joint.,4.1 Machine translation,[0],[0]
"Apart from the problem of high variance of gradients in the reinforce step, another problem is that sampling repeats states whereas TopK in beamjoint gets distinct states.",4.1 Machine translation,[0],[0]
"Hard attention too faces training issues and performs worse than soft attention, explaining why it is not commonly used in NMT.",4.1 Machine translation,[0],[0]
"Sample-joint is better than Hard attention, further highlighting the merits of the joint distribution.",4.1 Machine translation,[0],[0]
Sparsemax is competitive but marginally worse than soft attention.,4.1 Machine translation,[0],[0]
"This is concordant with the recent experiments of (Niculae and Blondel, 2017).
",4.1 Machine translation,[0],[0]
Comparison with Full Joint Next we evaluate the impact of our beam-joint approximation against full-joint and soft attention.,4.1 Machine translation,[0],[0]
"Full-joint cannot scale to large vocabularies, therefore we only compare on En-Vi with a batch size of 32.",4.1 Machine translation,[0],[0]
Figure 1a shows final BLEU of these methods as well as BLEU against increasing training steps.,4.1 Machine translation,[0],[0]
"Beam-joint both converges faster and to a higher score than soft-
2https://github.com/sid7954/beam-joint-attention 3https://github.com/tensorflow/nmt
attention.",4.1 Machine translation,[0],[0]
"For example by 10000 steps ( 5 epochs), beam-joint has surpassed soft-attention by almost 2 BLEU points (20 vs 22).",4.1 Machine translation,[0],[0]
"Moreover beam-joint tracks full-joint well, and both converge finally to similar BLEUs near 27 against 26 for soft attention.",4.1 Machine translation,[0],[0]
"This shows that an attention-beam of size 6 suffices to approximate full joint almost perfectly.
",4.1 Machine translation,[0],[0]
"Next, in Figure 1b, we compare beam-joint (solid lines) and soft attention (dotted lines) for convergence rates on three other datasets.",4.1 Machine translation,[0],[0]
"For each dataset beam-joint trains faster with a consistent improvement of more than 1 BLEU.
",4.1 Machine translation,[0],[0]
Effect of K in Beam-joint We show the effect of K used in TopK of beam-joint in Figure 2 on the En-Vi and De-En tasks.,4.1 Machine translation,[0],[0]
On En-Vi BLEU increases from 16.0 to 25.7 to 26.5 as K increases from 1 to 2 to 3; and then saturates quickly.,4.1 Machine translation,[0],[0]
Similar behavior is observed in the other dataset.,4.1 Machine translation,[0],[0]
"This shows that small K values like 6 suffice for translation.
",4.1 Machine translation,[0],[0]
We further evaluate whether the performance gain of beam-joint is due to the softmax barrier alone in Table 2.,4.1 Machine translation,[0],[0]
"We used our models trained with K=6, and deployed them for test-time greedy decoding with K set to 1.",4.1 Machine translation,[0],[0]
"Since the output now has only a single softmax component, this model faces the same bottleneck as soft-attention.",4.1 Machine translation,[0],[0]
"One can observe that as expected these results are worse than beam-joint with K=6, however they still exceed soft-attention by a significant margin, demonstrating that the performance gain is not solely due to the effect of ensembling or softmax-barrier.",4.1 Machine translation,[0],[0]
"To demonstrate the use of this approach beyond translation, we next consider two morphological
inflection tasks.",4.2 Morphological Inflection,[0],[0]
"We use (Durrett and DeNero, 2013)’s dataset containing 8 inflection forms for German Nouns (de-N) and 27 forms for German Verbs (de-V).",4.2 Morphological Inflection,[0],[0]
The number of training words is 2364 and 1627 respectively while the validation and test words are 200 each.,4.2 Morphological Inflection,[0],[0]
"We train a one layer encoder and decoder with 128 hidden LSTM units each with a dropout rate of 0.2 using Adam(Kingma and Ba, 2014) and measure 0/1 accuracy for soft, hard and full-joint attention models.",4.2 Morphological Inflection,[0],[0]
"Due to limited input length and vocabulary, we were able to run directly the full-joint model.",4.2 Morphological Inflection,[0],[0]
"We also ran the 100 units wide two layer LSTM with hard-monotonic attention provided by (Aharoni and Goldberg, 2017) labeled Hard-Mono4.",4.2 Morphological Inflection,[0],[0]
The table below shows that even for this task full-joint scores over existing attention models5.,4.2 Morphological Inflection,[0],[0]
"The generic full-joint attention provides slight gains even over the task specific hard-monotonic attention.
",4.2 Morphological Inflection,[0],[0]
"Dataset Soft Hard HardMono
FullJoint
de-N 85.50 85.13 85.65 85.81 de-V 94.91 95.04 95.31 95.52
Conclusion
",4.2 Morphological Inflection,[0],[0]
In this paper we showed a simple yet effective approximation of the joint attention-output distribution in sequence to sequence learning.,4.2 Morphological Inflection,[0],[0]
Our joint model consistently provides higher accuracy without significant running time overheads in five translation and two morphological inflection tasks.,4.2 Morphological Inflection,[0],[0]
"An interesting direction for future work is to extend beam-joint to multi-head attention architectures as in (Vaswani et al., 2017; Xu Chen, 2018).
",4.2 Morphological Inflection,[0],[0]
"Acknowledgements We thank NVIDIA Corporation for supporting this research by the donation of Titan X GPU.
4https://github.com/roeeaharoni/morphologicalreinflection
5Our numbers are lower than earlier reported because ours use a single model whereas (Aharoni and Goldberg, 2017) and others report from an ensemble of five models.",4.2 Morphological Inflection,[0],[0]
"In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning.",abstractText,[0],[0]
The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention.,abstractText,[0],[0]
On five translation and two morphological inflection tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.,abstractText,[0],[0]
Surprisingly Easy Hard-Attention for Sequence to Sequence Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93–104 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
93
We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",text,[0],[0]
"When we read a story, we bring to it a large body of implicit knowledge about the physical world.",1 Introduction,[0],[0]
"For instance, given the context “on stage, a woman takes a seat at the piano,” shown in Table 1, we can easily infer what the situation might look like: a woman is giving a piano performance, with a crowd watching her.",1 Introduction,[0],[0]
"We can furthermore infer her likely next action: she will most likely set her fingers on the piano keys and start playing.
",1 Introduction,[0],[0]
"This type of natural language inference requires commonsense reasoning, substantially broadening the scope of prior work that focused primarily on
linguistic entailment (Chierchia and McConnellGinet, 2000).",1 Introduction,[0],[0]
"Whereas the dominant entailment paradigm asks if two natural language sentences (the ‘premise’ and the ‘hypothesis’) describe the same set of possible worlds (Dagan et al., 2006; Bowman et al., 2015), here we focus on whether a (multiple-choice) ending describes a possible (future) world that can be anticipated from the situation described in the premise, even when it is not strictly entailed.",1 Introduction,[0],[0]
"Making such inference necessitates a rich understanding about everyday physical situations, including object affordances (Gibson, 1979) and frame semantics (Baker et al., 1998).
",1 Introduction,[0],[0]
A first step toward grounded commonsense inference with today’s deep learning machinery is to create a large-scale dataset.,1 Introduction,[0],[0]
"However, recent work has shown that human-written datasets are susceptible to annotation artifacts: unintended stylistic patterns that give out clues for the gold labels (Gururangan et al., 2018; Poliak et al., 2018).",1 Introduction,[0],[0]
"As a result, models trained on such datasets with hu-
man biases run the risk of over-estimating the actual performance on the underlying task, and are vulnerable to adversarial or out-of-domain examples (Wang et al., 2018; Glockner et al., 2018).
",1 Introduction,[0],[0]
"In this paper, we introduce Adversarial Filtering (AF), a new method to automatically detect and reduce stylistic artifacts.",1 Introduction,[0],[0]
We use this method to construct Swag: an adversarial dataset with 113k multiple-choice questions.,1 Introduction,[0],[0]
"We start with pairs of temporally adjacent video captions, each with a context and a follow-up event that we know is physically possible.",1 Introduction,[0],[0]
We then use a state-of-theart language model fine-tuned on this data to massively oversample a diverse set of possible negative sentence endings (or counterfactuals).,1 Introduction,[0],[0]
"Next, we filter these candidate endings aggressively and adversarially using a committee of trained models to obtain a population of de-biased endings with similar stylistic features to the real ones.",1 Introduction,[0],[0]
"Finally, these filtered counterfactuals are validated by crowd workers to further ensure data quality.
",1 Introduction,[0],[0]
"Extensive empirical results demonstrate unique contributions of our dataset, complementing existing datasets for natural langauge inference (NLI) (Bowman et al., 2015; Williams et al., 2018) and commonsense reasoning (Roemmele et al., 2011; Mostafazadeh et al., 2016; Zhang et al., 2017).",1 Introduction,[0],[0]
"First, our dataset poses a new challenge of grounded commonsense inference that is easy for humans (88%) while hard for current state-ofthe-art NLI models (<60%).",1 Introduction,[0],[0]
"Second, our proposed adversarial filtering methodology allows for cost-effective construction of a large-scale dataset while substantially reducing known annotation artifacts.",1 Introduction,[0],[0]
"The generality of adversarial filtering allows it to be applied to build future datasets, ensuring that they serve as reliable benchmarks.
",1 Introduction,[0],[0]
"2 Swag: Our new dataset
We introduce a new dataset for studying physically grounded commonsense inference, called Swag.1",1 Introduction,[0],[0]
Our task is to predict which event is most likely to occur next in a video.,1 Introduction,[0],[0]
"More formally, a model is given a context c = (s,n): a complete sentence s and a noun phrase n that begins a second sentence, as well as a list of possible verb phrase sentence endings V = {v1, . . .",1 Introduction,[0],[0]
",v4}.",1 Introduction,[0],[0]
"See Figure 1 for an example triple (s,n,vi).",1 Introduction,[0],[0]
"The model must then select the most appropriate verb phrase vî ∈ V .
1Short for Situations With Adversarial Generations.
",1 Introduction,[0],[0]
"Overview Our corpus consists of 113k multiple choice questions (73k training, 20k validation, 20k test) and is derived from pairs of consecutive video captions from ActivityNet Captions (Krishna et al., 2017; Heilbron et al., 2015) and the Large Scale Movie Description Challenge (LSMDC; Rohrbach et al., 2017).",1 Introduction,[0],[0]
The two datasets are slightly different in nature and allow us to achieve broader coverage: ActivityNet contains 20k YouTube clips containing one of 203 activity types (such as doing gymnastics or playing guitar); LSMDC consists of 128k movie captions (audio descriptions and scripts).,1 Introduction,[0],[0]
"For each pair of captions, we use a constituency parser (Stern et al., 2017) to split the second sentence into noun and verb phrases (Figure 1).2 Each question has a human-verified gold ending and 3 distractors.",1 Introduction,[0],[0]
"In this section, we outline the construction of Swag.",3 A solution to annotation artifacts,[0],[0]
"We seek dataset diversity while minimizing annotation artifacts, conditional stylistic patterns such as length and word-preference biases.",3 A solution to annotation artifacts,[0],[0]
"For many NLI datasets, these biases have been shown to allow shallow models (e.g. bag-of-words) obtain artificially high performance.
",3 A solution to annotation artifacts,[0],[0]
"To avoid introducing easily “gamed” patterns, we present Adversarial Filtering (AF), a generallyapplicable treatment involving the iterative refinement of a set of assignments to increase the entropy under a chosen model family.",3 A solution to annotation artifacts,[0],[0]
"We then discuss how we generate counterfactual endings, and
2We filter out sentences with rare tokens (≤3 occurrences), that are short (l ≤ 5), or that lack a verb phrase.
",3 A solution to annotation artifacts,[0],[0]
Algorithm 1 Adversarial filtering (AF) of negative samples.,3 A solution to annotation artifacts,[0],[0]
"During our experiments, we set Neasy = 2 for refining a population ofN− = 1023 negative examples to k = 9, and used a 80%/20% train/test split.
while convergence not reached do • Split the dataset D randomly up into training and testing portions Dtr and Dte. •",3 A solution to annotation artifacts,[0],[0]
Optimize a model fθ on Dtr. for index i in Dte do •,3 A solution to annotation artifacts,[0],[0]
Identify easy indices:,3 A solution to annotation artifacts,[0],[0]
Aeasyi = {j ∈ Ai : fθ(x,3 A solution to annotation artifacts,[0],[0]
+ i ) > fθ(x,3 A solution to annotation artifacts,[0],[0]
"− i,j)}
• Replace N easy easy indices j ∈ Aeasyi with adversarial indices k 6∈",3 A solution to annotation artifacts,[0],[0]
Ai satisfying fθ(x,3 A solution to annotation artifacts,[0],[0]
"− i,k) > fθ(x",3 A solution to annotation artifacts,[0],[0]
"− i,j).
end for end while
finally, the models used for filtering.",3 A solution to annotation artifacts,[0],[0]
"In this section, we formalize what it means for a dataset to be adversarial.",3.1 Formal definition,[0],[0]
"Intuitively, we say that an adversarial dataset for a model f is one on which f will not generalize, even if evaluated on test data from the same distribution.",3.1 Formal definition,[0],[0]
"More formally, let our input space be X and the label space be Y .",3.1 Formal definition,[0],[0]
"Our trainable classifier f , taking parameters θ is defined as fθ : X → R|Y|.",3.1 Formal definition,[0],[0]
"Let our dataset of size N be defined as D = {(xi, yi)}1≤i≤N , and let the loss function over the dataset be L(fθ,D).",3.1 Formal definition,[0],[0]
"We say that a dataset is adversarial with respect to f if we expect high empirical error I over all leave-one-out train/test splits (Vapnik, 2000):
I(D, f)",3.1 Formal definition,[0],[0]
"= 1 N N∑ i=1 L(fθ?i , {(xi, yi)}), (1)
where θ?i = argmin θ L(fθ,D \ {(xi, yi)}), (2)
",3.1 Formal definition,[0],[0]
with regularization terms omitted for simplicity.,3.1 Formal definition,[0],[0]
"In this section, we outline an approach for generating an adversarial dataset D, effectively maximizing empirical error I with respect to a family of trainable classifiers f .",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"Without loss of generality, we consider the situation where we have N contexts, each associated with a single positive example (x+i , 1)∈X ×Y , and a large population of context-specific negative examples (x−i,j , 0)∈X ×Y , where 1≤j≤N− for each i. For instance, the negative examples could be incorrect relations in knowledge-base completion (Socher et al., 2013), or all words in a dictionary for a
single-word cloze task (Zweig and Burges, 2011).",3.2 Adversarial filtering (AF) algorithm,[0],[0]
Our goal will be to filter the population of negative examples for each instance i to a size of k N−.,3.2 Adversarial filtering (AF) algorithm,[0],[0]
"This will be captured by returning a set of assignments A, where for each instance the assignment will be a k-subset Ai = [1 . . .",3.2 Adversarial filtering (AF) algorithm,[0],[0]
N−]k.,3.2 Adversarial filtering (AF) algorithm,[0],[0]
"The filtered dataset will then be:
DAF = {(xi, 1), {(x−i,j , 0)}j∈Ai}1≤i≤N (3)
Unfortunately, optimizing I(DAF , f) is difficult as A is global and non-differentiable.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"To address this, we present Algorithm 1.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"On each iteration, we split the data into dummy ‘train’ and ‘test’ splits.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"We train a model f on the training portion and obtain parameters θ, then use the remaining test portion to reassign the indices of A.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"For each context, we replace some number of ‘easy’ negatives in A that fθ classifies correctly with ‘adversarial’ negatives outside ofA that fθ misclassifies.
",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"This process can be thought of as increasing the overall entropy of the dataset: given a strong model fθ that is compatible with a random subset of the data, we aim to ensure it cannot generalize to the held-out set.",3.2 Adversarial filtering (AF) algorithm,[0.9522163362291831],"['As we conjectured earlier, we see that a search engine tends to return a document simply with the largest overlap in the text, necessitating the reformulation of a query to retrieve semantically relevant documents.']"
We repeat this for several iterations to reduce the generalization ability of the model family f over arbitrary train/test splits.,3.2 Adversarial filtering (AF) algorithm,[0],[0]
"To generate counterfactuals for Swag, we use an LSTM (Hochreiter and Schmidhuber, 1997) language model (LM), conditioned on contexts from video captions.",3.3 Generating candidate endings,[0],[0]
"We first pretrain on BookCorpus (Zhu et al., 2015), then finetune on the video caption datasets.",3.3 Generating candidate endings,[0],[0]
The architecture uses standard best practices and was validated on held-out perplexity of the video caption datasets; details are in the appendix.,3.3 Generating candidate endings,[0],[0]
"We use the LM to sample N−=1023 unique endings for a partial caption.3
Importantly, we greedily sample the endings, since beam search decoding biases the generated endings to be of lower perplexity (and thus easily distinguishable from found endings).",3.3 Generating candidate endings,[0],[0]
"We find this process gives good counterfactuals: the generated endings tend to use topical words, but often make little sense physically, making them perfect for our task.",3.3 Generating candidate endings,[0],[0]
"Further, the generated endings are marked as “gibberish” by humans only 9.1% of the time (Sec 3.5); in that case the ending is filtered out.
",3.3 Generating candidate endings,[0],[0]
"3To ensure that the LM generates unique endings, we split the data into five validation folds and train five separate LMs, one for each set of training folds.",3.3 Generating candidate endings,[0],[0]
This means that each LM never sees the found endings during training.,3.3 Generating candidate endings,[0],[0]
"In creating Swag, we designed the model family f to pick up on low-level stylistic features that we posit should not be predictive of whether an event happens next in a video.",3.4 Stylistic models for adversarial filtering,[0],[0]
"These stylistic features are an obvious case of annotation artifacts (Cai et al., 2017; Schwartz et al.,",3.4 Stylistic models for adversarial filtering,[0],[0]
2017).4 Our final classifier is an ensemble of four stylistic models:,3.4 Stylistic models for adversarial filtering,[0],[0]
1.,3.4 Stylistic models for adversarial filtering,[0],[0]
A multilayer perceptron (MLP) given LM perplexity features and context/ending lengths.,3.4 Stylistic models for adversarial filtering,[0],[0]
2.,3.4 Stylistic models for adversarial filtering,[0],[0]
A bag-of-words model that averages the word embeddings of the second sentence as features.,3.4 Stylistic models for adversarial filtering,[0],[0]
3.,3.4 Stylistic models for adversarial filtering,[0],[0]
"A one-layer CNN, with filter sizes ranging from 2-5, over the second sentence.",3.4 Stylistic models for adversarial filtering,[0],[0]
4.,3.4 Stylistic models for adversarial filtering,[0],[0]
A bidirectional LSTM over the 100 most common words in the second sentence; uncommon words are replaced by their POS tags.,3.4 Stylistic models for adversarial filtering,[0],[0]
We ensemble the models by concatenating their final representations and passing it through an MLP.,3.4 Stylistic models for adversarial filtering,[0],[0]
"On every adversarial iteration, the ensemble is trained jointly to minimize cross-entropy.
",3.4 Stylistic models for adversarial filtering,[0],[0]
"The accuracies of these models (at each iteration, evaluated on a 20% split of the test dataset before indices of A get remapped) are shown in Figure 2.",3.4 Stylistic models for adversarial filtering,[0],[0]
"Performance decreases from 60% to close to random chance; moreover, confusing the perplexity-based MLP is not sufficient to lower performance of the ensemble.",3.4 Stylistic models for adversarial filtering,[0],[0]
"Only once the other stylistic models are added does the ensemble accuracy drop substantially, suggesting that our approach is effective at reducing stylistic artifacts.
",3.4 Stylistic models for adversarial filtering,[0],[0]
"4A broad definition of annotation artifacts might include aspects besides lexical/stylistic features: for instance, certain events are less likely semantically regardless of the context (e.g. riding a horse using a hose).",3.4 Stylistic models for adversarial filtering,[0],[0]
"For this work, we erred more conservatively and only filtered based on style.",3.4 Stylistic models for adversarial filtering,[0],[0]
The final data-collection step is to have humans verify the data.,3.5 Human verification,[0],[0]
"Workers on Amazon Mechanical Turk were given the caption context, as well as six candidate endings: one found ending and five adversarially-sampled endings.",3.5 Human verification,[0],[0]
The task was twofold:,3.5 Human verification,[0],[0]
"Turkers ranked the endings independently as likely, unlikely, or gibberish, and selected the best and second best endings (Fig 3).
",3.5 Human verification,[0],[0]
We obtained the correct answers to each context in two ways.,3.5 Human verification,[0],[0]
"If a Turker ranks the found ending as either best or second best (73.7% of the time), we add the found ending as a gold example, with negatives from the generations not labelled best or gibberish.",3.5 Human verification,[0],[0]
"Further, if a Turker ranks a generated ending as best, and the found ending as second best, then we have reason to believe that the generation is good.",3.5 Human verification,[0],[0]
"This lets us add an additional training example, consisting of the generated best ending as the gold, and remaining generations as negatives.5 Examples with ≤3 nongibberish endings were filtered out.6
We found after 1000 examples that the annotators tended to have high agreement, also generally choosing found endings over generations (see Table 2).",3.5 Human verification,[0],[0]
"Thus, we collected the remaining 112k examples with one annotator each, periodically verifying that annotators preferred the found endings.",3.5 Human verification,[0],[0]
"In this section, we evaluate the performance of various NLI models on Swag.",4 Experiments,[0],[0]
"Recall that models
5These two examples share contexts.",4 Experiments,[0],[0]
"To prevent biasing the test and validation sets, we didn’t perform this procedure on answers from the evaluation sets’ context.
",4 Experiments,[0],[0]
"6To be data-efficient, we reannotated filtered-out examples by replacing gibberish endings, as well as generations that outranked the found ending, with candidates from A.
for our dataset take the following form: given a sentence and a noun phrase as context c = (s,n), as well as a list of possible verb phrase endings V = {v1, . . .",4 Experiments,[0],[0]
",v4}, a model fθ must select a verb î that hopefully matches igold:
î =",4 Experiments,[0],[0]
"argmax i fθ(s,n,vi) (4)
To study the amount of bias in our dataset, we also consider models that take as input just the ending verb phrase vi, or the entire second sentence (n,vi).",4 Experiments,[0],[0]
"For our learned models, we train f by minimizing multi-class cross-entropy.",4 Experiments,[0],[0]
"We consider three different types of word representations: 300d GloVe vectors from Common Crawl (Pennington et al., 2014), 300d Numberbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018).",4 Experiments,[0],[0]
"We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data.",4 Experiments,[0],[0]
See the appendix for more details.,4 Experiments,[0],[0]
"The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage.",4.1 Unary models,[0],[0]
a.,4.1 Unary models,[0],[0]
"fastText (Joulin et al., 2017):",4.1 Unary models,[0],[0]
"This library models a single span of text as a bag of n-grams, and tries to predict the probability of an ending being correct or incorrect independently.7",4.1 Unary models,[0],[0]
"b. Pretrained sentence encoders We consider two types of pretrained RNN sentence encoders, SkipThoughts (Kiros et al., 2015) and InferSent
7The fastText model is trained using binary cross-entropy; at test time we extract the prediction by selecting the ending with the highest positive likelihood under the model.
",4.1 Unary models,[0],[0]
"(Conneau et al., 2017).",4.1 Unary models,[0],[0]
"SkipThoughts was trained by predicting adjacent sentences in book data, whereas InferSent was trained on supervised NLI data.",4.1 Unary models,[0],[0]
"For each second sentence (or just the ending), we feed the encoding into an MLP.",4.1 Unary models,[0],[0]
c. LSTM sentence encoder,4.1 Unary models,[0],[0]
"Given an arbitrary span of text, we run a two-layer BiLSTM over it.",4.1 Unary models,[0],[0]
"The final hidden states are then max-pooled to obtain a fixed-size representation, which is then used to predict the potential for that ending.",4.1 Unary models,[0],[0]
The following models predict labels from two spans of text.,4.2 Binary models,[0],[0]
"We consider two possibilties for these models: using just the second sentence, where the two text spans are n,vi, or using the context and the second sentence, in which case the spans are s, (n,vi).",4.2 Binary models,[0],[0]
The latter case includes many models developed for the NLI task.,4.2 Binary models,[0],[0]
"d. Dual Bag-of-Words For this baseline, we treat each sentence as a bag-of-embeddings (c,vi).",4.2 Binary models,[0],[0]
We model the probability of picking an ending i using a bilinear model: softmaxi(cWvTi ).,4.2 Binary models,[0],[0]
"8 e. Dual pretrained sentence encoders Here, we obtain representations from SkipThoughts or InferSent for each span, and compute their pairwise compatibility using either 1) a bilinear model or 2) an MLP from their concatenated representations.",4.2 Binary models,[0],[0]
"f. SNLI inference Here, we consider two models that do well on SNLI (Bowman et al., 2015): Decomposable Attention (Parikh et al., 2016) and ESIM (Chen et al., 2017).",4.2 Binary models,[0],[0]
"We use pretrained versions of these models (with ELMo embeddings) on SNLI to obtain 3-way entailment, neutral, and contradiction probabilities for each example.",4.2 Binary models,[0],[0]
We then train a log-linear model using these 3-way probabilities as features.,4.2 Binary models,[0],[0]
g. SNLI models (retrained),4.2 Binary models,[0],[0]
"Here, we train ESIM and Decomposable Attention on our dataset: we simply change the output layer size to 1 (the potential of an ending vi) with a softmax over i.",4.2 Binary models,[0],[0]
We also considered the following models:,4.3 Other models,[0],[0]
h. Length:,4.3 Other models,[0],[0]
"Although length was used by the adversarial classifier, we want to verify that human validation didn’t reintroduce a length bias.",4.3 Other models,[0],[0]
"For this baseline, we always choose the shortest ending.",4.3 Other models,[0],[0]
i. ConceptNet,4.3 Other models,[0],[0]
"As our task requires world knowledge, we tried a rule-based system on top of the
8We also tried using an MLP, but got worse results.
",4.3 Other models,[0],[0]
"ConceptNet knowledge base (Speer et al., 2017).",4.3 Other models,[0],[0]
"For an ending sentence, we use the spaCy dependency parser to extract the head verb and its dependent object.",4.3 Other models,[0],[0]
The ending score is given by the number of ConceptNet causal relations9 between synonyms of the verb and synonyms of the object.,4.3 Other models,[0],[0]
"j. Human performance To benchmark human performance, five Mechanical Turk workers were asked to answer 100 dataset questions, as did an ‘expert’ annotator (the first author of this paper).",4.3 Other models,[0],[0]
Predictions were combined using a majority vote.,4.3 Other models,[0],[0]
We present our results in Table 3.,4.4 Results,[0],[0]
"The best model that only uses the ending is the LSTM sequence model with ELMo embeddings, which obtains 43.6%.",4.4 Results,[0],[0]
"This model, as with most models studied, greatly improves with more context: by 3.1% when given the initial noun phrase, and by an ad-
9We used the relations ‘Causes’, ‘CapableOf’, ‘ReceivesAction’, ‘UsedFor’, and ‘HasSubevent’.",4.4 Results,[0],[0]
"Though their coverage is low (30.4% of questions have an answer with≥1 causal relation), the more frequent relations in ConceptNet, such as ‘IsA’, at best only indirectly relate to our task.
",4.4 Results,[0],[0]
ditional 4% when also given the first sentence.,4.4 Results,[0],[0]
Further improvement is gained from models that compute pairwise representations of the inputs.,4.4 Results,[0],[0]
"While the simplest such model, DualBoW, obtains only 35.1% accuracy, combining InferSent sentence representations gives 40.5% accuracy (InferSent-Bilinear).",4.4 Results,[0],[0]
"The best results come from pairwise NLI models: when fully trained on Swag, ESIM+ELMo obtains 59.2% accuracy.
",4.4 Results,[0],[0]
"When comparing machine results to human results, we see there exists a lot of headroom.",4.4 Results,[0],[0]
"Though there likely is some noise in the task, our results suggest that humans (even untrained) converge to a consensus.",4.4 Results,[0],[0]
"Our in-house “expert” annotator is outperformed by an ensemble of 5 Turk workers (with 88% accuracy); thus, the effective upper bound on our dataset is likely even higher.",4.4 Results,[0],[0]
"5.1 Swag versus existing NLI datasets The past few years have yielded great advances in NLI and representation learning, due to the availability of large datasets like SNLI and MultiNLI
(Bowman et al., 2015; Williams et al., 2018).",5 Analysis,[0],[0]
"With the release of Swag, we hope to continue this trend, particularly as our dataset largely has the same input/output format as other NLI datasets.",5 Analysis,[0],[0]
"We observe three key differences between our dataset and others in this space:
First, as noted in Section 1, Swag requires a unique type of temporal reasoning.",5 Analysis,[0],[0]
"A state-of-theart NLI model such as ESIM, when bottlenecked through the SNLI notion of entailment (SNLIESIM), only obtains 36.1% accuracy.10 This implies that these datasets necessitate different (and complementary) forms of reasoning.
",5 Analysis,[0],[0]
"Second, our use of videos results in wide coverage of dynamic and temporal situations Compared with SNLI, with contexts from Flickr30K (Plummer et al., 2017) image captions, Swag has more active verbs like ‘pull’ and ‘hit,’ and fewer static verbs like ‘sit’ and ‘wear’ (Figure 4).11
Third, our dataset suffers from few lexical biases.",5 Analysis,[0],[0]
"Whereas fastText, a bag of n-gram model, obtains 67.0% accuracy on SNLI versus a 34.3% baseline (Gururangan et al., 2018), fastText obtains only 29.0% accuracy on Swag.12",5 Analysis,[0],[0]
"We sought to quantify how human judgments differ from the best studied model, ESIM+ELMo.",5.2 Error analysis,[0],[0]
"We randomly sampled 100 validation questions
10The weights of SNLI-ESIM pick up primarily on entailment probability (0.59), as with neutral (0.46), while contradiction is negatively correlated (-.42).
",5.2 Error analysis,[0],[0]
"11Video data has other language differences; notably, character names in LSMDC were replaced by ‘someone’
12The most predictive individual words on SWAG are infrequent in number: ‘dotted‘ with P(+|dotted) = 77% with 10.3 counts, and P(−|similar)",5.2 Error analysis,[0],[0]
= 81% with 16.3 counts.,5.2 Error analysis,[0],[0]
"(Counts from negative endings were discounted 3x, as there are 3 times as many negative endings as positive endings).
",5.2 Error analysis,[0],[0]
"that ESIM+ELMo answered incorrectly, for each extracting both the gold ending and the model’s preferred ending.",5.2 Error analysis,[0],[0]
"We asked 5 Amazon Mechanical Turk workers to pick the better ending (of which they preferred the gold endings 94% of the time) and to select one (or more) multiple choice reasons explaining why the chosen answer was better.
",5.2 Error analysis,[0],[0]
"The options, and the frequencies, are outlined in Table 4.",5.2 Error analysis,[0],[0]
"The most common reason for the turkers preferring the correct answer is situational (52.3% of the time), followed by weirdness (17.5%) and plausibility (14.4%).",5.2 Error analysis,[0],[0]
"This suggests that ESIM+ELMo already does a good job at filtering out weird and implausible answers, with the main bottleneck being grounded physical understanding.",5.2 Error analysis,[0],[0]
"The ambiguous percentage is also relatively low (12.0%), implying significant headroom.",5.2 Error analysis,[0],[0]
"Last, we show several qualitative examples in Table 5.",5.3 Qualitative examples,[0],[0]
"Though models can do decently well by identifying complex alignment patterns between the two sentences (e.g. being “up a tree” implies that “tree” is the end phrase), the incorrect model predictions suggest this strategy is insuffi-
cient.",5.3 Qualitative examples,[0],[0]
"For instance, answering “An old man rides a small bumper car” requires knowledge about bumper cars and how they differ from regular cars: bumper cars are tiny, don’t drive on roads, and don’t work in parking lots, eliminating the alternatives.",5.3 Qualitative examples,[0],[0]
"However, this knowledge is difficult to extract from existing corpora: for instance, the ConceptNet entry for Bumper Car has only a single relation: bumper cars are a type of vehicle.",5.3 Qualitative examples,[0],[0]
"Other questions require intuitive physical reasoning: e.g, for “he pours the raw egg batter into the pan,” about what happens next in making an omelet.",5.3 Qualitative examples,[0],[0]
Our results suggest that Swag is a challenging testbed for NLI models.,5.4 Where to go next?,[0],[0]
"However, the adversarial models used to filter the dataset are purely stylistic and focus on the second sentence; thus, subtle artifacts still likely remain in our dataset.",5.4 Where to go next?,[0],[0]
"These patterns are ostensibly picked up by the NLI models (particularly when using ELMo features), but the large gap between machine and human performance suggests that more is required to solve the dataset.",5.4 Where to go next?,[0],[0]
"As models are developed for commonsense inference, and more broadly as the field of NLP advances, we note that AF can be used again to create a more adversarial version of Swag using better language models and AF models.",5.4 Where to go next?,[0],[0]
"Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018).",6 Related Work,[0],[0]
"Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components
for performing better video captioning (Pasunuru and Bansal, 2017), summarization (Pasunuru and Bansal, 2018), and generation (Holtzman et al., 2018), confirming the importance of NLI research.",6 Related Work,[0],[0]
"The NLI task requires a variety of commonsense knowledge (LoBue and Yates, 2011), which our work complements.",6 Related Work,[0],[0]
"However, previous datasets for NLI have been challenged by unwanted annotation artifacts, (Gururangan et al., 2018; Poliak et al., 2018) or scale issues.",6 Related Work,[0],[0]
"Our work addresses these challenges by constructing a new NLI benchmark focused on grounded commonsense reasoning, and by introducing an adversarial filtering mechanism that substantially reduces known and easily detectable annotation artifacts.
",6 Related Work,[0],[0]
"Commonsense NLI Several datasets have been introduced to study NLI beyond linguistic entailment: for inferring likely causes and endings given a sentence (COPA; Roemmele et al., 2011), for choosing the most sensible ending to a short story (RocStories; Mostafazadeh et al., 2016; Sharma et al., 2018), and for predicting likelihood of a hypothesis by regressing to an ordinal label (JOCI; (Zhang et al., 2017)).",6 Related Work,[0],[0]
These datasets are relatively small: 1k examples for COPA and 10k cloze examples for RocStories.13 JOCI increases the scale by generating the hypotheses using a knowledge graph or a neural model.,6 Related Work,[0],[0]
"In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans.",6 Related Work,[0],[0]
"In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality.
",6 Related Work,[0],[0]
"13For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories.
",6 Related Work,[0],[0]
"Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about.
",6 Related Work,[0],[0]
Vision datasets Several resources have been introduced to study temporal inference in vision.,6 Related Work,[0],[0]
"The Visual Madlibs dataset has 20k image captions about hypothetical next/previous events (Yu et al., 2015); similar to our work, the test portion is multiple-choice, with counterfactual answers retrieved from similar images and verified by humans.",6 Related Work,[0],[0]
"The question of ‘what will happen next?’ has also been studied in photo albums (Huang et al., 2016), videos of team sports, (Felsen et al., 2017) and egocentric dog videos (Ehsani et al., 2018).",6 Related Work,[0],[0]
"Last, annotation artifacts are also a recurring problem for vision datasets such as Visual Genome (Zellers et al., 2018) and Visual QA (Jabri et al., 2016); recent work was done to create a more challenging VQA dataset by annotating complementary image pairs (Goyal et al., 2016).
",6 Related Work,[0],[0]
"Reducing gender/racial bias Prior work has sought to reduce demographic biases in word embeddings (Zhang et al., 2018) as well as in image recognition models (Zhao et al., 2017).",6 Related Work,[0],[0]
"Our work has focused on producing a dataset with minimal annotation artifacts, which in turn helps to avoid some gender and racial biases that stem from elicitation (Rudinger et al., 2017).",6 Related Work,[0],[0]
"However, it is not perfect in this regard, particularly due to biases in movies (Schofield and Mehr, 2016; Sap et al., 2017).",6 Related Work,[0],[0]
"Our methodology could potentially be extended to construct datasets free of (possibly intersectional) gender or racial bias.
",6 Related Work,[0],[0]
"Physical knowledge Prior work has studied learning grounded knowledge about objects and verbs: from knowledge bases (Li et al., 2016), syntax parses (Forbes and Choi, 2017), word embeddings (Lucy and Gauthier, 2017), and images and dictionary definitions (Zellers and Choi, 2017).",6 Related Work,[0],[0]
"An alternate thread of work has been to learn scripts: high-level representations of event chains (Schank and Abelson, 1975; Chambers and Jurafsky, 2009).",6 Related Work,[0],[0]
"Swag evaluates both of these strands.
",6 Related Work,[0],[0]
14Prior work on sentence completion filtered negatives with heuristics based on LM perplexities.,6 Related Work,[0],[0]
"We initially tried something similar, but found the result to still be gameable.",6 Related Work,[0],[0]
We propose a new challenge of physically situated commonsense inference that broadens the scope of natural language inference (NLI) with commonsense reasoning.,7 Conclusion,[0],[0]
"To support research toward commonsense NLI, we create a large-scale dataset Swag with 113k multiple-choice questions.",7 Conclusion,[0],[0]
"Our dataset is constructed using Adversarial Filtering (AF), a new paradigm for robust and cost-effective dataset construction that allows datasets to be constructed at scale while automatically reducing annotation artifacts that can be easily detected by a committee of strong baseline models.",7 Conclusion,[0],[0]
"Our adversarial filtering paradigm is general, allowing potential applications to other datasets that require human composition of question answer pairs.",7 Conclusion,[0],[0]
"We thank the anonymous reviewers, members of the ARK and xlab at the University of Washington, researchers at the Allen Institute for AI, and Luke Zettlemoyer for their helpful feedback.",Acknowledgements,[0],[0]
We also thank the Mechanical Turk workers for doing a fantastic job with the human validation.,Acknowledgements,[0],[0]
"This work was supported by the National Science Foundation Graduate Research Fellowship (DGE-1256082), the NSF grant (IIS1524371, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the IARPA DIVA program through D17PC00343, and gifts by Google and Facebook.",Acknowledgements,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as representing endorsements of IARPA, DOI/IBC, or the U.S. Government.",Acknowledgements,[0],[0]
"Given a partial description like “she opened the hood of the car,” humans can reason about the situation and anticipate what might come next (“then, she examined the engine”).",abstractText,[0],[0]
"In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.",abstractText,[0],[0]
"We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations.",abstractText,[0],[0]
"To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data.",abstractText,[0],[0]
"To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals.",abstractText,[0],[0]
"Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task.",abstractText,[0],[0]
We provide comprehensive analysis that indicates significant opportunities for future research.,abstractText,[0],[0]
Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 856–861 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
856
In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix.",text,[0],[0]
Data augmentation algorithms generate extra data points from the empirically observed training set to train subsequent machine learning algorithms.,1 Introduction and Related Work,[0],[0]
"While these extra data points may be of lower quality than those in the training set, their quantity and diversity have proven to benefit various learning algorithms (DeVries and Taylor, 2017; Amodei et al., 2016).",1 Introduction and Related Work,[0],[0]
"In image processing, simple augmentation techniques such as flipping, cropping, or increasing and decreasing the contrast of the image are both widely utilized and highly effective (Huang et al., 2016; Zagoruyko and Komodakis, 2016).
",1 Introduction and Related Work,[0],[0]
"However, it is nontrivial to find simple equivalences for NLP tasks like machine translation, because even slight modifications of sentences can result in significant changes in their semantics, or
*: Equal contributions.
require corresponding changes in the translations in order to keep the data consistent.",1 Introduction and Related Work,[0],[0]
"In fact, indiscriminate modifications of data in NMT can introduce noise that makes NMT systems brittle (Belinkov and Bisk, 2018).
",1 Introduction and Related Work,[0],[0]
"Due to such difficulties, the literature in data augmentation for NMT is relatively scarce.",1 Introduction and Related Work,[0],[0]
"To our knowledge, data augmentation techniques for NMT fall into two categories.",1 Introduction and Related Work,[0],[0]
"The first category is based on back-translation (Sennrich et al., 2016b; Poncelas et al., 2018), which utilizes monolingual data to augment a parallel training corpus.",1 Introduction and Related Work,[0],[0]
"While effective, back-translation is often vulnerable to errors in initial models, a common problem of self-training algorithms (Chapelle et al., 2009).",1 Introduction and Related Work,[0],[0]
The second category is based on word replacements.,1 Introduction and Related Work,[0],[0]
"For instance, Fadaee et al. (2017) propose to replace words in the target sentences with rare words in the target vocabulary according to a language model, and then modify the aligned source words accordingly.",1 Introduction and Related Work,[0],[0]
"While this method generates augmented data with relatively high quality, it requires several complicated preprocessing steps, and is only shown to be effective for low-resource datasets.",1 Introduction and Related Work,[0],[0]
"Other generic word replacement methods include word dropout (Sennrich et al., 2016a; Gal and Ghahramani, 2016), which uniformly set some word embeddings to 0 at random, and Reward Augmented Maximum Likelihood (RAML; Norouzi et al. (2016)), whose implementation essentially replaces some words in the target sentences with other words from the target vocabulary.
",1 Introduction and Related Work,[0],[0]
"In this paper, we derive an extremely simple and efficient data augmentation technique for NMT.",1 Introduction and Related Work,[0],[0]
"First, we formulate the design of a data augmentation algorithm as an optimization problem, where we seek the data augmentation policy that maximizes an objective that encourages two desired properties: smoothness and diversity.",1 Introduction and Related Work,[0],[0]
"This optimization problem has a tractable analytic solution,
which describes a generic framework of which both word dropout and RAML are instances.",1 Introduction and Related Work,[0],[0]
"Second, we interpret the aforementioned solution and propose a novel method: independently replacing words in both the source sentence and the target sentence by other words uniformly sampled from the source and the target vocabularies, respectively.",1 Introduction and Related Work,[0],[0]
"Experiments show that this method, which we name SwitchOut, consistently improves over strong baselines on datasets of different scales, including the large-scale WMT 15 English-German dataset, and two medium-scale datasets: IWSLT 2016 German-English and IWSLT 2015 EnglishVietnamese.",1 Introduction and Related Work,[0],[0]
"We use uppercase letters, such as X , Y , etc., to denote random variables and lowercase letters such as x, y, etc., to denote the corresponding actual values.",2.1 Notations,[0],[0]
"Additionally, since we will discuss a data augmentation algorithm, we will use a hat to denote augmented variables and their values, e.g. bX , bY , bx, by, etc.",2.1 Notations,[0],[0]
"We will also use boldfaced characters, such as p, q, etc., to denote probability distributions.",2.1 Notations,[0],[0]
We facilitate our discussion with a probabilistic framework that motivates data augmentation algorithms.,2.2 Data Augmentation,[0],[0]
"With X , Y being the sequences of words in the source and target languages (e.g. in machine translation), the canonical MLE framework maximizes the objective
JMLE(✓) =",2.2 Data Augmentation,[0],[0]
"E x,y⇠bp(X,Y )",2.2 Data Augmentation,[0],[0]
"[logp✓(y|x)] .
",2.2 Data Augmentation,[0],[0]
"Here bp(X,Y ) is the empirical distribution over all training data pairs (x, y) and p
✓ (y|x) is a parameterized distribution that we aim to learn, e.g. a neural network.",2.2 Data Augmentation,[0],[0]
"A potential weakness of MLE is the mismatch between bp(X,Y ) and the true data distribution p(X,Y ).",2.2 Data Augmentation,[0],[0]
"Specifically, bp(X,Y ) is usually a bootstrap distribution defined only on the observed training pairs, while p(X,Y ) has a much larger support, i.e. the entire space of valid pairs.",2.2 Data Augmentation,[0],[0]
"This issue can be dramatic when the empirical observations are insufficient to cover the data space.
",2.2 Data Augmentation,[0],[0]
"In practice, data augmentation is often used to remedy this support discrepancy by supplying additional training pairs.",2.2 Data Augmentation,[0],[0]
"Formally, let q( bX, bY ) be the augmented distribution defined on a larger support than the empirical distribution bp(X,Y ).",2.2 Data Augmentation,[0],[0]
"Then,
MLE training with data augmentation maximizes
JAUG(✓) = Ebx,by⇠q( bX,bY )",2.2 Data Augmentation,[0],[0]
"[logp✓(by|bx)] .
",2.2 Data Augmentation,[0],[0]
"In this work, we focus on a specific family of q, which depends on the empirical observations by
q( bX, bY ) =",2.2 Data Augmentation,[0],[0]
"E x,y⇠bp(x,y)
h q( bX, bY |x, y) i .
",2.2 Data Augmentation,[0],[0]
"This particular choice follows the intuition that an augmented pair (bx, by) that diverges too far from any observed data is more likely to be invalid and thus harmful for training.",2.2 Data Augmentation,[0],[0]
The reason will be more evident later.,2.2 Data Augmentation,[0],[0]
"Certainly, not all q are equally good, and the more similar q is to p, the more desirable q will be.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Unfortunately, we only have access to limited observations captured by bp.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Hence, in order to use q to bridge the gap between bp and p, it is necessary to utilize some assumptions about p. Here, we exploit two highly generic assumptions, namely:
• Diversity: p(X,Y ) has a wider support set, which includes samples that are more diverse than those in the empirical observation set.
",2.3 Diverse and Smooth Augmentation,[0],[0]
"• Smoothness: p(X,Y ) is smooth, and similar (x, y) pairs will have similar probabilities.
",2.3 Diverse and Smooth Augmentation,[0],[0]
"To formalize both assumptions, let s(bx, by;x, y) be a similarity function that measures how similar an augmented pair (bx, by) is to an observed data pair (x, y).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Then, an ideal augmentation policy q( bX, bY |x, y) should have two properties.",2.3 Diverse and Smooth Augmentation,[0],[0]
"First, based on the smoothness assumption, if an augmented pair (bx, by) is more similar to an empirical pair (x, y), it is more likely that (bx, by) is sampled under the true data distribution p(X,Y ), and thus q( bX, bY |x, y) should assign a significant amount of probability mass to (bx, by).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Second, to quantify the diversity assumption, we propose that the entropy H[q( bX, bY |x, y)] should be large, so that the support of q( bX, bY ) is larger than the support of bp and thus is closer to the support p(X,Y ).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Combining these assumptions implies that q( bX, bY |x, y) should maximize the objective
J(q;x, y) = Ebx,by⇠q( bX,bY |x,y) ⇥",2.3 Diverse and Smooth Augmentation,[0],[0]
"s(bx, by;x, y) ⇤
+ ⌧H(q( bX, bY |x, y)), (1)
where ⌧ controls the strength of the diversity objective.",2.3 Diverse and Smooth Augmentation,[0],[0]
"The first term in (1) instantiates the smoothness assumption, which encourages q to draw samples that are similar to (x, y).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Meanwhile, the second term in (1) encourages more diverse samples from q. Together, the objective J(q;x, y) extends the information in the “pivotal” empirical sample (x, y) to a diverse set of similar cases.",2.3 Diverse and Smooth Augmentation,[0],[0]
"This echoes our particular parameterization of q in Section 2.2.
",2.3 Diverse and Smooth Augmentation,[0],[0]
"The objective J(q;x, y) in (1) is the canonical maximum entropy problem that one often encounters in deriving a max-ent model (Berger et al., 1996), which has the analytic solution:
q⇤(bx, by|x, y) = exp {s(bx, by;x, y)/⌧}P bx0,by0 exp {s(bx0, by0;x, y)/⌧}
(2) Note that (2) is a fairly generic solution which is agnostic to the choice of the similarity measure s. Obviously, not all similarity measures are equally good.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Next, we will show that some existing algorithms can be seen as specific instantiations under our framework.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Moreover, this leads us to propose a novel and effective data augmentation algorithm.",2.3 Diverse and Smooth Augmentation,[0],[0]
Word Dropout.,2.4 Existing and New Algorithms,[0],[0]
"In the context of machine translation, Sennrich et al. (2016a) propose to randomly choose some words in the source and/or target sentence, and set their embeddings to 0 vectors.",2.4 Existing and New Algorithms,[0],[0]
"Intuitively, it regards every new data pair generated by this procedure as similar enough and then includes them in the augmented training set.",2.4 Existing and New Algorithms,[0],[0]
"Formally, word dropout can be seen as an instantiation of our framework with a particular similarity function s(x̂, ŷ;x, y) (see Appendix A.1).
RAML.",2.4 Existing and New Algorithms,[0],[0]
"From the perspective of reinforcement learning, Norouzi et al. (2016) propose to train the model distribution to match a target distribution proportional to an exponentiated reward.",2.4 Existing and New Algorithms,[0],[0]
"Despite the difference in motivation, it can be shown (c.f. Appendix A.2) that RAML can be viewed as an instantiation of our generic framework, where the similarity measure is s(bx, by;x, y) = r(by; y) if bx = x and 1 otherwise.",2.4 Existing and New Algorithms,[0],[0]
"Here, r is a task-specific reward function which measures the similarity between by and y. Intuitively, this means that RAML only exploits the smoothness property on the target side while keeping the source side intact.
SwitchOut.",2.4 Existing and New Algorithms,[0],[0]
"After reviewing the two existing augmentation schemes, there are two immediate
insights.",2.4 Existing and New Algorithms,[0],[0]
"Firstly, augmentation should not be restricted to only the source side or the target side.",2.4 Existing and New Algorithms,[0],[0]
"Secondly, being able to incorporate prior knowledge, such as the task-specific reward function r in RAML, can lead to a better similarity measure.
",2.4 Existing and New Algorithms,[0],[0]
"Motivated by these observations, we propose to perform augmentation in both source and target domains.",2.4 Existing and New Algorithms,[0],[0]
"For simplicity, we separately measure the similarity between the pair (bx, x) and the pair (by, y) and then sum them together, i.e.
s(bx, by;x, y)/⌧ ⇡ r x (bx, x)/⌧ x + r y (by, y)/⌧ y , (3)
where r x and r y are domain specific similarity functions and ⌧
x , ⌧ y are hyper-parameters that absorb the temperature parameter ⌧ .",2.4 Existing and New Algorithms,[0],[0]
"This allows us to factor q⇤(bx, by|x, y) into:
q⇤(bx, by|x, y) = exp {rx(bx, x)/⌧x}P bx0 exp {rx(bx0, x)/⌧x}
⇥",2.4 Existing and New Algorithms,[0],[0]
"exp {ry(by, y)/⌧y}P by0 exp {ry(by0, y)/⌧y}
(4)
",2.4 Existing and New Algorithms,[0],[0]
"In addition, notice that this factored formulation allows bx and by to be sampled independently.
",2.4 Existing and New Algorithms,[0],[0]
Sampling Procedure.,2.4 Existing and New Algorithms,[0],[0]
"To complete our method, we still need to define r
x and r y , and then design a practical sampling scheme from each factor in (4).",2.4 Existing and New Algorithms,[0],[0]
"Though non-trivial, both problems have been (partially) encountered in RAML (Norouzi et al., 2016; Ma et al., 2017).",2.4 Existing and New Algorithms,[0],[0]
"For simplicity, we follow previous work to use the negative Hamming distance for both r
x and r y .",2.4 Existing and New Algorithms,[0],[0]
"For a more parallelized implementation, we sample an augmented sentence bs from a true sentence s as follows:
1.",2.4 Existing and New Algorithms,[0],[0]
"Sample bn 2 {0, 1, ..., |s|} by p(bn) /",2.4 Existing and New Algorithms,[0],[0]
"e bn/⌧ .
",2.4 Existing and New Algorithms,[0],[0]
2.,2.4 Existing and New Algorithms,[0],[0]
"For each i 2 {1, 2, ..., |s|}, with probability bn/ |s|, we can replace s
i by a uniform bs",2.4 Existing and New Algorithms,[0],[0]
"i 6= s i .
",2.4 Existing and New Algorithms,[0],[0]
"This procedure guarantees that any two sentences bs1 and bs2 with the same Hamming distance to s have the same probability, but slightly changes the relative odds of sentences with different Hamming distances to s from the true distribution by negative Hamming distance, and thus is an approximation of the actual distribution.",2.4 Existing and New Algorithms,[0],[0]
"However, this efficient sampling procedure is much easier to implement while achieving good performance.
",2.4 Existing and New Algorithms,[0],[0]
"Algorithm 1 illustrates this sampling procedure, which can be applied independently and in parallel for each batch of source sentences and target
sentences.",2.4 Existing and New Algorithms,[0],[0]
"Additionally, we open source our implementation in TensorFlow and in PyTorch (respectively in Appendix A.5 and A.6).
",2.4 Existing and New Algorithms,[0],[0]
Algorithm 1: Sampling with SwitchOut.,2.4 Existing and New Algorithms,[0],[0]
"Input : s: a sentence represented by vocab integral ids,
⌧ : the temperature, V : the vocabulary Output : bs: a sentence with words replaced
1 Function HammingDistanceSample(s, ⌧ , |V |): 2 Let Z(⌧) P|s| n=0 e
n/⌧ be the partition function.",2.4 Existing and New Algorithms,[0],[0]
"3 Let p(n) e n/⌧/Z(⌧) for n = 0, 1, ..., |s|.",2.4 Existing and New Algorithms,[0],[0]
4 Sample bn ⇠ p(n).,2.4 Existing and New Algorithms,[0],[0]
5,2.4 Existing and New Algorithms,[0],[0]
"In parallel, do: 6 Sample a
i ⇠ Bernoulli(bn/ |s|).",2.4 Existing and New Algorithms,[0],[0]
7,2.4 Existing and New Algorithms,[0],[0]
"if a
i = 1 then 8 bs
i Uniform(V \{s i })",2.4 Existing and New Algorithms,[0],[0]
.,2.4 Existing and New Algorithms,[0],[0]
"9 else
10 bs",2.4 Existing and New Algorithms,[0],[0]
i s i .,2.4 Existing and New Algorithms,[0],[0]
11 end 12 return bs,2.4 Existing and New Algorithms,[0],[0]
Datasets.,3 Experiments,[0],[0]
We benchmark SwitchOut on three translation tasks of different scales: 1) IWSLT 2015 English-Vietnamese (en-vi); 2) IWSLT 2016 German-English (de-en); and 3) WMT 2015 English-German (en-de).,3 Experiments,[0],[0]
All translations are wordbased.,3 Experiments,[0],[0]
"These tasks and pre-processing steps are standard, used in several previous works.",3 Experiments,[0],[0]
"Detailed statistics and pre-processing schemes are in Appendix A.3.
",3 Experiments,[0],[0]
Models and Experimental Procedures.,3 Experiments,[0],[0]
"Our translation model, i.e. p
✓ (y|x), is a Transformer network (Vaswani et al., 2017).",3 Experiments,[0],[0]
"For each dataset, we first train a standard Transformer model without SwitchOut and tune the hyper-parameters on the dev set to achieve competitive results.",3 Experiments,[0],[0]
(w.r.t.,3 Experiments,[0],[0]
Luong and Manning (2015); Gu et al. (2018); Vaswani et al. (2017)),3 Experiments,[0],[0]
.,3 Experiments,[0],[0]
"Then, fixing all hyper-parameters, and fixing ⌧
y = 0, we tune the ⌧ x rate, which controls how far we are willing to let bx deviate from x.",3 Experiments,[0],[0]
"Our hyper-parameters are listed in Appendix A.4.
Baselines.",3 Experiments,[0],[0]
"While the Transformer network without SwitchOut is already a strong baseline, we also compare SwitchOut against two other baselines that further use existing varieties of data augmentation: 1) word dropout on the source side with the dropping probability of word = 0.1; and 2) RAML on the target side, as in Section 2.4.",3 Experiments,[0],[0]
"Additionally, on the en-de task, we compare SwitchOut against back-translation (Sennrich et al., 2016b).
",3 Experiments,[0],[0]
SwitchOut vs. Word Dropout and RAML.,3 Experiments,[0],[0]
"We report the BLEU scores of SwitchOut, word dropout, and RAML on the test sets of the tasks in Table 1.",3 Experiments,[0],[0]
"To account for variance, we run each experiment multiple times and report the median BLEU.",3 Experiments,[0],[0]
"Specifically, each experiment without SwitchOut is run for 4 times, while each experiment with SwitchOut is run for 9 times due to its inherently higher variance.",3 Experiments,[0],[0]
"We also conduct pairwise statistical significance tests using paired bootstrap (Clark et al., 2011), and record the results in Table 1.",3 Experiments,[0],[0]
"For 4 of the 6 settings, SwitchOut delivers significant improvements over the best baseline without SwitchOut.",3 Experiments,[0],[0]
"For the remaining two settings, the differences are not statistically significant.",3 Experiments,[0],[0]
The gains in BLEU with SwitchOut over the best baseline on WMT 15 en-de are all significant (p < 0.0002).,3 Experiments,[0],[0]
"Notably, SwitchOut on the source demonstrates as large gains as these obtained by RAML on the target side, and SwitchOut delivers further improvements when combined with RAML.
SwitchOut vs. Back Translation.",3 Experiments,[0],[0]
"Traditionally, data-augmentation is viewed as a method to enlarge the training datasets (Krizhevsky et al., 2012; Szegedy et al., 2014).",3 Experiments,[0],[0]
"In the context of neural MT, Sennrich et al. (2016b) propose to use artificial data generated from a weak back-translation model, effectively utilizing monolingual data to enlarge the bilingual training datasets.",3 Experiments,[0],[0]
"In connection, we compare SwitchOut against back translation.",3 Experiments,[0],[0]
"We only compare SwitchOut against back translation on the en-de task, where the amount of bilingual training data is already sufficiently large2.",3 Experiments,[0],[0]
"The
2We add the extra monolingual data from http://data.statmt.org/rsennrich/wmt16_ backtranslations/en-de/
BLEU scores with back-translation are reported in Table 2.",3 Experiments,[0],[0]
These results provide two insights.,3 Experiments,[0],[0]
"First, the gain delivered by back translation is less significant than the gain delivered by SwitchOut.",3 Experiments,[0],[0]
"Second, SwitchOut and back translation are not mutually exclusive, as one can additionally apply SwitchOut on the additional data obtained from back translation to further improve BLEU scores.
",3 Experiments,[0],[0]
Effects of ⌧ x and ⌧ y .,3 Experiments,[0],[0]
We empirically study the effect of these temperature parameters.,3 Experiments,[0],[0]
"During the tuning process, we translate the dev set of the tasks and report the BLEU scores in Figure 1.",3 Experiments,[0],[0]
"We observe that when fixing ⌧
y , the best performance is always achieved with a non-zero ⌧
.
",3 Experiments,[0],[0]
Where does SwitchOut Help the Most?,3 Experiments,[0],[0]
"Intuitively, because SwitchOut is expanding the support of the training distribution, we would expect that it would help the most on test sentences that are far from those in the training set and would thus benefit most from this expanded support.",3 Experiments,[0],[0]
"To test this hypothesis, for each test sentence we find its most similar training sample (i.e. nearest neighbor), then bucket the instances by the distance to their
nearest neighbor and measure the gain in BLEU afforded by SwitchOut for each bucket.",3 Experiments,[0],[0]
"Specifically, we use (negative) word error rate (WER) as the similarity measure, and plot the bucket-by-bucket performance gain for each group in Figure 2.",3 Experiments,[0],[0]
"As we can see, SwitchOut improves increasingly more as the WER increases, indicating that SwitchOut is indeed helping on examples that are far from the sentences that the model sees during training.",3 Experiments,[0],[0]
This is the desirable effect of data augmentation techniques.,3 Experiments,[0],[0]
"In this paper, we propose a method to design data augmentation algorithms by solving an optimization problem.",4 Conclusion,[0],[0]
"These solutions subsume a few existing augmentation schemes and inspire a novel augmentation method, SwitchOut.",4 Conclusion,[0],[0]
SwitchOut delivers improvements over translation tasks at different scales.,4 Conclusion,[0],[0]
"Additionally, SwitchOut is efficient and easy to implement, and thus has the potential for wide application.",4 Conclusion,[0],[0]
"We thank Quoc Le, Minh-Thang Luong, Qizhe Xie, and the anonymous EMNLP reviewers, for their suggestions to improve the paper.
",Acknowledgements,[0],[0]
This material is based upon work supported in part by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) Low Resource Languages for Emergent Incidents (LORELEI) program under Contract No. HR0011-15-C0114.,Acknowledgements,[0],[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.",Acknowledgements,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.,Acknowledgements,[0],[0]
"In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT).",abstractText,[0],[0]
"We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution.",abstractText,[0],[0]
"This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies.",abstractText,[0],[0]
We name this method SwitchOut.,abstractText,[0],[0]
"Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a).",abstractText,[0],[0]
Code to implement this method is included in the appendix.,abstractText,[0],[0]
SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3772–3782 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3772",text,[0],[0]
"As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited.",1 Introduction,[0],[0]
"Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014).",1 Introduction,[0],[0]
"Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017).
",1 Introduction,[0],[0]
"Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization.",1 Introduction,[0],[0]
"We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing?
",1 Introduction,[0],[0]
"We propose a multitask learning approach to incorporating syntactic information into learned
representations of neural semantics models (§2).",1 Introduction,[0],[0]
"Our approach, the syntactic scaffold, minimizes an auxiliary supervised loss function, derived from a syntactic treebank.",1 Introduction,[0],[0]
"The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling.",1 Introduction,[0],[0]
"We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications)",1 Introduction,[0],[0]
the semantic analyzer has no additional cost over a syntax-free baseline.,1 Introduction,[0],[0]
"Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task.
",1 Introduction,[0],[0]
"Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
These spans are usually syntactic constituents (cf.,1 Introduction,[0],[0]
"PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold.",1 Introduction,[0],[0]
See Figure 1 for an example sentence with syntactic and semantic annotations.,1 Introduction,[0],[0]
"Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree.",1 Introduction,[0],[0]
"This means we never need to run a syntactic parsing algorithm.
",1 Introduction,[0],[0]
Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for two SRL tasks (§5) and coreference resolution (§6).,1 Introduction,[0],[0]
"Our models use the strongest available neural network architectures for these tasks, integrating deep representation learning (He et al., 2017) and structured prediction at the level of spans (Kong et al., 2016).",1 Introduction,[0],[0]
"For SRL, the base-
line itself is a novel globally normalized structured conditional random field, which outperforms the previous state of the art.1 Syntactic scaffolds result in further improvements over prior work— 3.6 absolute F1 in FrameNet SRL, 1.1 absolute F1 in PropBank SRL, and 0.6 F1 in coreference resolution (averaged across three standard scores).",1 Introduction,[0],[0]
Our code is open source and available at https: //github.com/swabhs/scaffolding.,1 Introduction,[0],[0]
"Multitask learning (Caruana, 1997) is a collection of techniques in which two or more tasks are learned from data with at least some parameters shared.",2 Syntactic Scaffolds,[0],[0]
"We assume there is only one task about whose performance we are concerned, denoted T1 (in this paper, T1 is either SRL or coreference resolution).",2 Syntactic Scaffolds,[0],[0]
"We use the term “scaffold” to refer to a second task, T2, that can be combined with T1 during multitask learning.",2 Syntactic Scaffolds,[0],[0]
"A scaffold task is only used during training; it holds no intrinsic interest beyond biasing the learning of T1, and after learning is completed, the scaffold is discarded.
",2 Syntactic Scaffolds,[0],[0]
"A syntactic scaffold is a task designed to steer the (shared) model toward awareness of syntactic
1This excludes models initialized with deep, contextualized embeddings (Peters et al., 2018), an approach orthogonal to ours.
structure.",2 Syntactic Scaffolds,[0],[0]
It could be defined through a syntactic parser that shares some parameters with T1’s model.,2 Syntactic Scaffolds,[0],[0]
"Since syntactic parsing is costly, we use simpler syntactic prediction problems (discussed below) that do not produce whole trees.
",2 Syntactic Scaffolds,[0],[0]
"As with multitask learning in general, we do not assume that the same data are annotated with outputs for T1 and T2.",2 Syntactic Scaffolds,[0],[0]
"In this work, T2 is defined using phrase-structure syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).",2 Syntactic Scaffolds,[0],[0]
We experiment with three settings: one where the corpus for T2 does not overlap with the training datasets for T1 (frame-SRL) and two where there is a complete overlap (PropBank SRL and coreference).,2 Syntactic Scaffolds,[0],[0]
"Compared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T1 and T2 output.",2 Syntactic Scaffolds,[0],[0]
"We briefly contrast the syntactic scaffold with existing alternatives.
Pipelines.",3 Related Work,[0],[0]
"In a typical pipeline, T1 and T2 are separately trained, with the output of T2 used to define the inputs to T1 (Wolpert, 1992).",3 Related Work,[0],[0]
"Using syntax as T2 in a pipeline is perhaps the most
common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T2’s mistakes affect the performance, and perhaps the training, of T1; He et al., 2013).",3 Related Work,[0],[0]
"To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006).",3 Related Work,[0],[0]
"A syntactic scaffold is quite different from a pipeline since the output of T2 is never explicitly used.
",3 Related Work,[0],[0]
Latent variables.,3 Related Work,[0],[0]
Another solution is to treat the output of T2 as a (perhaps structured) latent variable.,3 Related Work,[0],[0]
This approach obviates the need of supervision for T2 and requires marginalization (or some approximation to it) in order to reason about the outputs of T1.,3 Related Work,[0],[0]
Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012).,3 Related Work,[0],[0]
"Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T2, and it need not overlap the T1 training data.
",3 Related Work,[0],[0]
Joint learning of syntax and semantics.,3 Related Work,[0],[0]
"The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Lluı́s and Màrquez, 2008; Lluı́s et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016).",3 Related Work,[0],[0]
"This typically requires joint prediction of the outputs of T1 and T2, which tends to be computationally expensive at both training and test time.
",3 Related Work,[0],[0]
Part of speech scaffolds.,3 Related Work,[0],[0]
"Similar to our work, there have been multitask models that use partof-speech tagging as T2, with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T1.",3 Related Work,[0],[0]
Both of the above approaches assumed parallel input data and used both tasks as supervision.,3 Related Work,[0],[0]
"Notably, we simplify our T2, throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with.",3 Related Work,[0],[0]
"While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations.",3 Related Work,[0],[0]
"In-
2",3 Related Work,[0],[0]
"There has been some recent work on SRL which completely forgoes syntactic processing (Zhou and Xu, 2015), however it has been shown that incorporating syntactic information still remains useful (He et al., 2017).
",3 Related Work,[0],[0]
"stead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks.",3 Related Work,[0],[0]
"Additionally, these methods explore architectural variations in RNN layers for including supervision, whereas we focus on incorporating supervision with minimal changes to the baseline architecture.",3 Related Work,[0],[0]
"To the best of our knowledge, such simplified syntactic scaffolds have not been tried before.
",3 Related Work,[0],[0]
Word embeddings.,3 Related Work,[0],[0]
"Our definition of a scaffold task almost includes stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018).",3 Related Work,[0],[0]
"After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings.",3 Related Work,[0],[0]
"A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T1 through a multitask objective.
",3 Related Work,[0],[0]
Multitask learning.,3 Related Work,[0],[0]
"Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017).",3 Related Work,[0],[0]
"In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018).",3 Related Work,[0],[0]
"Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013).",3 Related Work,[0],[0]
"Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task.",3 Related Work,[0],[0]
"We assume two sources of supervision: a corpusD1 with instances x annotated for the primary task’s outputs y (semantic role labeling or coreference resolution), and a treebankD2 with sentences x, each with a phrase-structure tree z.",4 Syntactic Scaffold Model,[0],[0]
"Each task has an associated loss, and we seek to minimize the combination of task losses,∑
(x,y)∈D1 L1(x, y) + δ ∑ (x,z)∈D2 L2(x, z) (1)
with respect to parameters, which are partially shared, where δ is a tunable hyperparameter.",4.1 Loss,[0],[0]
"In
the rest of this section, we describe the scaffold task.",4.1 Loss,[0],[0]
"We define the primary tasks in Sections 5–6.
",4.1 Loss,[0],[0]
"Each input is a sequence of tokens, x = 〈x1, x2, . . .",4.1 Loss,[0],[0]
", xn〉, for some n. We refer to a span of contiguous tokens in the sentence as xi: j = 〈xi, xi+1, . . .",4.1 Loss,[0],[0]
", x",4.1 Loss,[0],[0]
"j〉, for any 1 6 i 6 j 6 n. In our experiments we consider only spans up to a maximum length D, resulting in O(nD) spans.
",4.1 Loss,[0],[0]
"Supervision comes from a phrase-syntactic tree z for the sentence, comprising a syntactic category zi: j ∈ C for every span xi: j in x (many spans are given a null label).",4.1 Loss,[0],[0]
"We experiment with different sets of labels C (§4.2).
",4.1 Loss,[0],[0]
"In our model, every span xi: j is represented by an embedding vector vi: j (see details in §5.3).",4.1 Loss,[0],[0]
"A distribution over the category assigned to zi: j is derived from vi: j:
p(zi: j = c | xi: j) = softmax c wc · vi: j (2)
where wc is a parameter vector associated with category c.",4.1 Loss,[0],[0]
"We sum the log loss terms for all the spans in a sentence to give its loss:
L2(x, z) =",4.1 Loss,[0],[0]
"− ∑
16i6 j6n j−i6D
log p(zi: j | xi: j).",4.1 Loss,[0],[0]
(3),4.1 Loss,[0],[0]
"Different kinds of syntactic labels can be used for learning syntactically-aware span representations: • Constituent identity: C = {0, 1}; is a span a
constituent, or not?",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"• Non-terminal: c is the category of a span,
including a null for non-constituents.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"• Non-terminal and parent: c is the category
of a span, concatenated with the category of its immediate ancestor.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"null is used for nonconstituents, and for empty ancestors.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"• Common non-terminals: Since a majority
of semantic arguments and entity mentions are labeled with a small number of syntactic categories,3 we experiment with a threeway classification among (i) noun phrase (or prepositional phrase, for frame SRL); (ii) any other category; and (iii) null.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"In Figure 1, for the span “encouraging them”, the constituent identity scaffold label is 1, the nonterminal label is S|VP, the non-terminal and parent label is S|VP+par=PP, and the common nonterminals label is set to OTHER.
",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"3In the OntoNotes corpus, which includes both syntactic and semantic annotations, 44% of semantic arguments are noun phrases and 13% are prepositional phrases.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
We contribute a new SRL model which contributes a strong baseline for experiments with syntactic scaffolds.,5 Semantic Role Labeling,[0],[0]
"The performance of this baseline itself is competitive with state-of-the-art methods (§7).
",5 Semantic Role Labeling,[0],[0]
FrameNet.,5 Semantic Role Labeling,[0],[0]
"In the FrameNet lexicon (Baker et al., 1998), a frame represents a type of event, situation, or relationship, and is associated with a set of semantic roles, called frame elements.",5 Semantic Role Labeling,[0],[0]
"A frame can be evoked by a word or phrase in a sentence, called a target.",5 Semantic Role Labeling,[0],[0]
"Each frame element of an evoked frame can then be realized in the sentence as a sentential span, called an argument (or it can be unrealized).",5 Semantic Role Labeling,[0],[0]
"Arguments for a given frame do not overlap.
",5 Semantic Role Labeling,[0],[0]
PropBank.,5 Semantic Role Labeling,[0],[0]
PropBank similarly disambiguates predicates and identifies argument spans.,5 Semantic Role Labeling,[0],[0]
"Targets are disambiguated to lexically specific senses rather than shared frames, and a set of generic roles is used for all targets, reducing the argument label space by a factor of 17.",5 Semantic Role Labeling,[0],[0]
"Most importantly, the arguments were annotated on top of syntactic constituents, directly coupling syntax and semantics.",5 Semantic Role Labeling,[0],[0]
"A detailed example for both formalisms is provided in Figure 1.
",5 Semantic Role Labeling,[0],[0]
"Semantic structure prediction is the task of identifying targets, labeling their frames or senses, and labeling all their argument spans in a sentence.",5 Semantic Role Labeling,[0],[0]
"Here we assume gold targets and frames, and consider only the SRL task.
",5 Semantic Role Labeling,[0],[0]
"Formally, a single input instance for argument identification consists of: an n-word sentence x = 〈x1, x2, . . .",5 Semantic Role Labeling,[0],[0]
", xn〉, a single target span t = 〈tstart, tend〉, and its evoked frame, or sense, f .",5 Semantic Role Labeling,[0],[0]
"The argument labeling task is to produce a segmentation of the sentence: s = 〈s1, s2, . . .",5 Semantic Role Labeling,[0],[0]
", sm〉 for each input x.",5 Semantic Role Labeling,[0],[0]
A segment s = 〈,5 Semantic Role Labeling,[0],[0]
"i, j, yi: j〉 corresponds to a labeled span of the sentence, where the label yi: j ∈ Y f ∪ {null} is either a role that the span fills, or null if the span does not fill any role.",5 Semantic Role Labeling,[0],[0]
"In the case of PropBank, Y f consists of all possible roles.",5 Semantic Role Labeling,[0],[0]
The segmentation is constrained so that argument spans cover the sentence and do not overlap (ik+1 = 1 + jk for sk; i1 = 1; jm = n).,5 Semantic Role Labeling,[0],[0]
Segments of length 1 such that i = j are allowed.,5 Semantic Role Labeling,[0],[0]
A separate segmentation is predicted for each target annotation in a sentence.,5 Semantic Role Labeling,[0],[0]
"In order to model the non-overlapping arguments of a given target, we use a semi-Markov conditional random field (semi-CRF; Sarawagi et al., 2004).",5.1 Semi-Markov CRF,[0],[0]
"Semi-CRFs define a conditional distribution over labeled segmentations of an input sequence, and are globally normalized.",5.1 Semi-Markov CRF,[0],[0]
A single target’s arguments can be neatly encoded as a labeled segmentation by giving the spans in between arguments a reserved null label.,5.1 Semi-Markov CRF,[0],[0]
"Semi-Markov models are more powerful than BIO tagging schemes, which have been used successfully for PropBank SRL (Collobert et al., 2011; Zhou and Xu, 2015, inter alia), because the semi-Markov assumption allows scoring variable-length segments, rather than fixed-length label n-grams as under an (n − 1)-order Markov assumption.",5.1 Semi-Markov CRF,[0],[0]
Computing the marginal likelihood with a semi-CRF can be done using dynamic programming in O(n2) time (§5.2).,5.1 Semi-Markov CRF,[0],[0]
"By filtering out segments longer than D tokens, this is reduced to O(nD).
",5.1 Semi-Markov CRF,[0],[0]
"Given an input x, a semi-CRF defines a conditional distribution p(s | x).",5.1 Semi-Markov CRF,[0],[0]
Every segment s = 〈,5.1 Semi-Markov CRF,[0],[0]
"i, j, yi: j〉 is given a real-valued score, ψ(〈i, j, yi: j = r〉, xi: j) = wr · vi: j, where vi: j is an embedding of the span (§5.3) and wr is a parameter vector corresponding to its label.",5.1 Semi-Markov CRF,[0],[0]
"The score of the entire segmentation s is the sum of the scores of its segments: Ψ(x, s) =",5.1 Semi-Markov CRF,[0],[0]
"∑m k=1 ψ(sk, xik: jk ).",5.1 Semi-Markov CRF,[0],[0]
These scores are exponentiated and normalized to define the probability distribution.,5.1 Semi-Markov CRF,[0],[0]
The sum-product variant of the semi-Markov dynamic programming algorithm is used to calculate the normalization term (required during learning).,5.1 Semi-Markov CRF,[0],[0]
"At test time, the maxproduct variant returns the most probable segmentation, ŝ = arg max sΨ(s, x).
",5.1 Semi-Markov CRF,[0],[0]
The parameters of the semi-CRF are learned to maximize a criterion related to the conditional loglikelihood of the gold-standard segments in the training corpus (§5.2).,5.1 Semi-Markov CRF,[0],[0]
"The learner evaluates and adjusts segment scores ψ(sk, x) for every span in the sentence, which in turn involves learning embedded representations for all spans (§5.3).",5.1 Semi-Markov CRF,[0],[0]
Typically CRF and semi-CRF models are trained to maximize a conditional log-likelihood objective.,5.2 Softmax-Margin Objective,[0],[0]
"In early experiments, we found that incorporating a structured cost was beneficial; we do so by using a softmax-margin training objective (Gimpel and Smith, 2010), a “cost-aware” variant
of log-likelihood:
L1 = − ∑
(x,s∗)∈D1 log
exp Ψ(s∗, x) Z(x, s∗) , (4)
Z(x, s∗) =",5.2 Softmax-Margin Objective,[0],[0]
"∑
s exp {Ψ(s, x) +",5.2 Softmax-Margin Objective,[0],[0]
"cost(s, s∗)}.",5.2 Softmax-Margin Objective,[0],[0]
"(5)
We design the cost function so that it factors by predicted span, in the same way Ψ does:
cost(s, s∗)",5.2 Softmax-Margin Objective,[0],[0]
"= ∑ s∈s cost(s, s∗) = ∑ s∈s I(s < s∗).",5.2 Softmax-Margin Objective,[0],[0]
"(6)
The softmax-margin criterion, like log-likelihood, is globally normalized over all of the exponentially many possible labeled segmentations.",5.2 Softmax-Margin Objective,[0],[0]
"The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function:
α j = ∑
s=〈i, j,yi: j〉 j−i6D
αi−1 exp{Ψ(s, x) + cost(s, s∗)}, (7)
where Z = αn, under the base case α0 = 1.",5.2 Softmax-Margin Objective,[0],[0]
"The prediction under the model can be calculated using a similar dynamic program with the following recurrence where γ0 = 1:
γ j = max s=〈i, j,yi: j〉
j−i6D
γi−1 exp Ψ(s, x).",5.2 Softmax-Margin Objective,[0],[0]
"(8)
Our model formulation enforces that arguments do not overlap.",5.2 Softmax-Margin Objective,[0],[0]
"We do not enforce any other SRL constraints, such as non-repetition of core frame elements (Das et al., 2012).",5.2 Softmax-Margin Objective,[0],[0]
"This section describes the neural architecture used to obtain the span embedding, vi: j, corresponding to a span xi: j and the target in consideration, t = 〈tstart, tend〉.",5.3 Input Span Representation,[0],[0]
"For the scaffold task, since the syntactic treebank does not contain annotations for semantic targets, we use the last verb in the sentence as a placeholder target, wherever target features are used.",5.3 Input Span Representation,[0],[0]
"If there are no verbs, we use the first token in the sentence as a placeholder target.",5.3 Input Span Representation,[0],[0]
"The parameters used to learn v are shared between the tasks.
",5.3 Input Span Representation,[0],[0]
"We construct an embedding for the span using • hi and h j: contextualized embeddings for the
words at the span boundary (§5.3.1), • ui: j: a span summary that pools over the con-
tents of the span (§5.3.2), and
• ai: j: and a hand-engineered feature vector for the span (§5.3.3).
",5.3 Input Span Representation,[0],[0]
"This embedding is then passed to a feedforward layer to compute the span representation, vi: j.",5.3 Input Span Representation,[0],[0]
"To obtain contextualized embeddings of each token in the input sequence, we run a bidirectional LSTM (Graves, 2012) with ` layers over the full input sequence.",5.3.1 Contextualized Token Embeddings,[0],[0]
"To indicate which token is a predicate, a linearly transformed one-hot embedding v is used, following Zhou and Xu (2015) and He et al. (2017).",5.3.1 Contextualized Token Embeddings,[0],[0]
The input vector representing the token at position q in the sentence is the concatenation of a fixed pretrained embedding xq and vq.,5.3.1 Contextualized Token Embeddings,[0],[0]
"When given as input to the bidirectional LSTM, this yields a hidden state vector hq representing the qth token in the context of the sentence.",5.3.1 Contextualized Token Embeddings,[0],[0]
Tokens within a span might convey different amounts of information necessary to label the span as a semantic argument.,5.3.2 Span Summary,[0],[0]
"Following Lee et al. (2017), we use an attention mechanism (Bahdanau et al., 2014) to summarize each span.",5.3.2 Span Summary,[0],[0]
"Each contextualized token in the span is passed through a feed-forward network to obtain a weight, normalized to give σk = softmax
i6k6 j whead · hk, where whead
is a learned parameter.",5.3.2 Span Summary,[0],[0]
"The weights σ are then used to obtain a vector that summarizes the span, ui: j = ∑ i6k6 j; j−i<D σk · hk.",5.3.2 Span Summary,[0],[0]
"We use the following three features for each span: • width of the span in tokens (Das et al., 2014) • distance (in tokens) of the span from the tar-
get (Täckström et al., 2015) • position of the span with respect to the tar-
get (before, after, overlap) (Täckström et al., 2015)
",5.3.3 Span Features,[0],[0]
"Each of these features is encoded as a one-hotembedding and then linearly transformed to yield a feature vector, ai: j.",5.3.3 Span Features,[0],[0]
Coreference resolution is the task of determining clusters of mentions that refer to the same entity.,6 Coreference Resolution,[0],[0]
"Formally, the input is a document x = x1, x2, . . .",6 Coreference Resolution,[0],[0]
", xn consisting of n words.",6 Coreference Resolution,[0],[0]
"The goal is to predict a set of clusters c = {c1, c2, . . .",6 Coreference Resolution,[0],[0]
"}, where each cluster c = {s1, s2, .",6 Coreference Resolution,[0],[0]
. .,6 Coreference Resolution,[0],[0]
"} is a set of spans and
each span s = 〈i, j〉 is a pair of indices such that 1 6 i 6 j 6 n.
As a baseline, we use the model of Lee et al. (2017), which we describe briefly in this section.",6 Coreference Resolution,[0],[0]
This model decomposes the prediction of coreference clusters into a series of span classification decisions.,6 Coreference Resolution,[0],[0]
"Every span s predicts an antecedent ws ∈ Y(s) = {null, s1, s2, . . .",6 Coreference Resolution,[0],[0]
", sm}.",6 Coreference Resolution,[0],[0]
"Labels s1 to sm indicate a coreference link between s and one of the m spans that precede it, and null indicates that s does not link to anything, either because it is not a mention or it is in a singleton cluster.",6 Coreference Resolution,[0],[0]
"The predicted clustering of the spans can be recovered by aggregating the predicted links.
",6 Coreference Resolution,[0],[0]
"Analogous to the SRL model (§5), every span s is represented by an embedding vs, which is central to the model.",6 Coreference Resolution,[0],[0]
"For each span s and a potential antecedent a ∈ Y(s), pairwise coreference scores Ψ(vs, va, φ(s, a)) are computed via feedforward networks with the span embeddings as input.",6 Coreference Resolution,[0],[0]
"φ(s, a) are pairwise discrete features encoding the distance between span s and span a and metadata, such as the genre and speaker information.",6 Coreference Resolution,[0],[0]
"We refer the reader to Lee et al. (2017) for the details of the scoring function.
",6 Coreference Resolution,[0],[0]
"The scores from Ψ are normalized over the possible antecedents Y(s) of each span to induce a probability distribution for every span:
p(ws = a) = softmax a∈Y(s) Ψ(vs, va, φ(s, a))",6 Coreference Resolution,[0],[0]
"(9)
In learning, we minimize the negative loglikelihood marginalized over the possibly correct antecedents:
L1 = − ∑ s∈D log ∑ a∗∈G(s)∩Y(s) p(ws = a∗) (10)
whereD is the set of spans in the training dataset, and G(s) indicates the gold cluster of s if it belongs to one and {null} otherwise.
",6 Coreference Resolution,[0],[0]
"To operate under reasonable computational requirements, inference under this model requires a two-stage beam search, which reduces the number of span pairs considered.",6 Coreference Resolution,[0],[0]
"We refer the reader to Lee et al. (2017) for details.
",6 Coreference Resolution,[0],[0]
Input span representation.,6 Coreference Resolution,[0],[0]
"The input span embedding, vs for coreference resolution and its syntactic scaffold follow the definition used in §5.3, with the key difference of using no target features.",6 Coreference Resolution,[0],[0]
"Since there is a complete overlap of input sentences betweenDsc andDpr as the coreference annotations are also from OntoNotes (Pradhan et al.,
2012), we reuse the v for the scaffold task.",6 Coreference Resolution,[0],[0]
"Additionally, instead of the entire document, each sentence in it is independently given as input to the bidirectional LSTMs.",6 Coreference Resolution,[0],[0]
We evaluate our models on the test set of FrameNet 1.5 for frame SRL and on the test set of OntoNotes for both PropBank SRL and coreference.,7 Results,[0],[0]
"For the syntactic scaffold in each case, we use syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).4 Further details on experimental settings and datasets have been elaborated in the supplemental material.
",7 Results,[0],[0]
Frame SRL.,7 Results,[0],[0]
Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold.,7 Results,[0],[0]
"We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007).
",7 Results,[0],[0]
"Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; Täckström",7 Results,[0],[0]
"et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015).
",7 Results,[0],[0]
The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017).,7 Results,[0],[0]
"In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer.",7 Results,[0],[0]
"In their relational model (Rel), they treat the same problem as a span classification problem.",7 Results,[0],[0]
"Finally, they introduce an ensemble to integerate both models, and use an integer linear program for inference satisfying SRL constraints.",7 Results,[0],[0]
"Though their model does not do any syntactic pruning, it does use syntactic features for argument identification and labeling.5
Notably, all prior systems for frame SRL listed in Table 1 use a pipeline of syntax and semantics.",7 Results,[0],[0]
"Our semi-CRF baseline outperforms all prior work, without any syntax.",7 Results,[0],[0]
"This highlights the ben-
4http://cemantix.org/data/ontonotes.html 5Yang and Mitchell (2017) also evaluated on the full frame-semantic parsing task, which includes frame-SRL as well as identifying frames.",7 Results,[0],[0]
"Since our frame SRL performance improves over theirs, we expect that incorporation into a full system (e.g., using their frame identification module) would lead to overall benefits as well; this experiment is left to future work.
",7 Results,[0],[0]
"efits of modeling spans and of global normalization.
",7 Results,[0],[0]
"Turning to scaffolds, even the most coarsegrained constituent identity scaffold improves the performance of our syntax-agnostic baseline.",7 Results,[0],[0]
"The nonterminal and nonterminal and parent scaffolds, which use more detailed syntactic representations, improve over this.",7 Results,[0],[0]
"The greatest improvements come from the scaffold model predicting common nonterminal labels (NP and PP, which are the most common syntactic categories of semantic arguments, vs. others): 3.6% absolute improvement in F1 measure over prior work.
",7 Results,[0],[0]
"Contemporaneously with this work, Peng et al. (2018) proposed a system for joint frame-semantic and semantic dependency parsing.",7 Results,[0],[0]
"They report results for joint frame and argument identification, and hence cannot be directly compared in Table 1.",7 Results,[0],[0]
"We evaluated their output for argument identification only; our semi-CRF baseline model exceeds their performance by 1 F1, and our common nonterminal scaffold by 3.1 F1.6
6This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set.
",7 Results,[0],[0]
PropBank SRL.,7 Results,[0],[0]
"We use the OntoNotes data from the CoNLL shared task in 2012 (Pradhan et al., 2013) for Propbank SRL.",7 Results,[0],[0]
"Table 2 reports results using gold predicates.
",7 Results,[0],[0]
"Recent competitive systems for PropBank SRL follow the approach of Zhou and Xu (2015), employing deep architectures, and forgoing the use of any syntax.",7 Results,[0],[0]
"He et al. (2017) improve on those results, and in analysis experiments, show that constraints derived using syntax may further improve performance.",7 Results,[0],[0]
Tan et al. (2018) employ a similar approach but use feed-forward networks with selfattention.,7 Results,[0],[0]
"He et al. (2018a) use a span-based classification to jointly identify and label argument spans.
",7 Results,[0],[0]
"Our syntax-agnostic semi-CRF baseline model improves on prior work (excluding ELMo), showing again the value of global normalization in semantic structure prediction.",7 Results,[0],[0]
We obtain further improvement of 0.8 absolute F1 with the best syntactic scaffold from the frame SRL task.,7 Results,[0],[0]
"This indicates that a syntactic inductive bias is beneficial even when using sophisticated neural architectures.
",7 Results,[0],[0]
"He et al. (2018a) also provide a setup where initialization was done with deep contextualized embeddings, ELMo (Peters et al., 2018), resulting in 85.5 F1 on the OntoNotes test set.",7 Results,[0],[0]
"The improvements from ELMo are methodologically orthogonal to syntactic scaffolds.
",7 Results,[0],[0]
"Since the datasets for learning PropBank semantics and syntactic scaffolds completely overlap, the performance improvement cannot be attributed to a larger training corpus (or, by extension, a larger vocabulary), though that might be a factor for frame SRL.
",7 Results,[0],[0]
"A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017).",7 Results,[0],[0]
"This, along with other recent ap-
proaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL.
Coreference.",7 Results,[0],[0]
"We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3.",7 Results,[0],[0]
"Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax.
",7 Results,[0],[0]
"Our baseline is the model from Lee et al. (2017), described in §6.",7 Results,[0],[0]
"Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax.
",7 Results,[0],[0]
We experiment with the best syntactic scaffold from the frame SRL task.,7 Results,[0],[0]
"We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases.",7 Results,[0],[0]
The syntactic scaffold outperforms the baseline by 0.6 absolute F1.,7 Results,[0],[0]
"Contemporaneously, Lee et al. (2018) proposed a model which takes in account higher order inference and more aggressive pruning, as well as initialization with ELMo embeddings, resulting in 73.0 average F1.",7 Results,[0],[0]
"All the above are orthogonal to our approach, and could be incorporated to yield higher gains.",7 Results,[0],[0]
"To investigate the performance of the syntactic scaffold, we focus on the frame SRL results, where we observed the greatest improvement with respect to a non-syntactic baseline.
",8 Discussion,[0],[0]
"We consider a breakdown of the performance by the syntactic phrase types of the arguments, provided in FrameNet7 in Figure 2.",8 Discussion,[0.9518085178836716],['We show two examples of queries and the probabilities of each candidate term of being selected by the RL-CNN model in Fig.']
"Not surpris-
7We used FrameNet syntactic phrase annotations for analysis only, and not in our models, since they are annotated only for the gold arguments.
",8 Discussion,[0],[0]
"ingly, we observe large improvements in the common nonterminals used (NP and PP).",8 Discussion,[0],[0]
"However, the phrase type annotations in FrameNet do not correspond exactly to the OntoNotes phrase categories.",8 Discussion,[0],[0]
"For instance, FrameNet annotates nonmaximal (A) and standard adjective phrases (AJP), while OntoNotes annotations for noun-phrases are flat, ignore the underlying adjective phrases.",8 Discussion,[0],[0]
"This explains why the syntax-agnostic baseline is able to recover the former while the scaffold is not.
",8 Discussion,[0],[0]
"Similarly, for frequent frame elements, scaffolding improves performance across the board, as shown in Fig. 3.",8 Discussion,[0],[0]
"The largest improvements come for Theme and Goal, which are predominantly realized as noun phrases and prepositional phrases.",8 Discussion,[0],[0]
"We introduced syntactic scaffolds, a multitask learning approach to incorporate syntactic bias into semantic processing tasks.",9 Conclusion,[0],[0]
"Unlike pipelines and approaches which jointly model syntax and semantics, no explicit syntactic processing is required at runtime.",9 Conclusion,[0],[0]
"Our method improves the performance of competitive baselines for semantic role labeling on both FrameNet and PropBank, and for coreference resolution.",9 Conclusion,[0],[0]
"While our focus was on span-based tasks, syntactic scaffolds could be applied in other settings (e.g., dependency and graph representations).",9 Conclusion,[0],[0]
"Moreover, scaffolds need not be syntactic; we can imagine, for example, semantic scaffolds being used to improve NLP applications with limited annotated data.",9 Conclusion,[0],[0]
"It remains an open empirical question to determine the relative merits of different kinds of scaffolds and multitask learners, and how they can be most produc-
tively combined.",9 Conclusion,[0],[0]
Our code is publicly available at https://github.com/swabhs/scaffolding.,9 Conclusion,[0],[0]
"We thank several members of UW-NLP, particularly Luheng He, as well as David Weiss and Emily Pitler for thoughtful discussions on prior versions of this paper.",Acknowledgments,[0],[0]
We also thank the three anonymous reviewers for their valuable feedback.,Acknowledgments,[0],[0]
This work was supported in part by NSF grant IIS1562364 and by the NVIDIA Corporation through the donation of a Tesla GPU.,Acknowledgments,[0],[0]
"We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks.",abstractText,[0],[0]
"Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective.",abstractText,[0],[0]
"We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.",abstractText,[0],[0]
Syntactic Scaffolds for Semantic Structures,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2061–2071 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2061",text,[0],[0]
"Semantic role labeling (SRL), namely semantic parsing, is a shallow semantic parsing task, which aims to recognize the predicate-argument structure of each predicate in a sentence, such as who did what to whom, where and when, etc.",1 Introduction,[0],[0]
"Specifically, we seek to identify arguments and label their semantic roles given a predicate.",1 Introduction,[0],[0]
"SRL is an impor-
∗ These authors made equal contribution.† Corresponding author.",1 Introduction,[0],[0]
"This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15- ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04).
tant method to obtain semantic information beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016), question answering (Berant et al., 2013; Yih et al., 2016) and discourse relation sense classification (Mihaylov and Frank, 2016).
",1 Introduction,[0],[0]
"There are two formulizations for semantic predicate-argument structures, one is based on constituents (i.e., phrase or span), the other is based on dependencies.",1 Introduction,[0],[0]
"The latter proposed by the CoNLL-2008 shared task (Surdeanu et al., 2008) is also called semantic dependency parsing, which annotates the heads of arguments rather than phrasal arguments.",1 Introduction,[0],[0]
"Generally, SRL is decomposed into multi-step classification subtasks in pipeline systems, consisting of predicate identification and disambiguation, argument identification and classification.
",1 Introduction,[0],[0]
"In prior work of SRL, considerable attention has been paid to feature engineering that struggles to capture sufficient discriminative information, while neural network models are capable of extracting features automatically.",1 Introduction,[0],[0]
"In particular, syntactic information, including syntactic tree feature, has been show extremely beneficial to SRL since a larger scale of empirical verification of Punyakanok et al. (2008).",1 Introduction,[0],[0]
"However, all the work had to take the risk of erroneous syntactic input, leading to an unsatisfactory performance.
",1 Introduction,[0],[0]
"To alleviate the above issues, Marcheggiani et al. (2017) propose a simple but effective model for dependency SRL without syntactic input.",1 Introduction,[0],[0]
"It seems that neural SRL does not have to rely on syntactic features, contradicting with the belief that syntax is a necessary prerequisite for SRL as early as Gildea and Palmer (2002).",1 Introduction,[0],[0]
"This dramatic contradiction motivates us to make a thorough exploration on syntactic contribution to SRL.
",1 Introduction,[0],[0]
"This paper will focus on semantic dependency parsing and formulate SRL as one or two se-
quence tagging tasks with predicate-specific encoding.",1 Introduction,[0],[0]
"With the help of the proposed k-order argument pruning algorithm over syntactic tree, our model obtains state-of-the-art scores on the CoNLL benchmarks for both English and Chinese.
",1 Introduction,[0],[0]
"In order to quantitatively evaluate the contribution of syntax to SRL, we adopt the ratio between labeled F1 score for semantic dependencies (Sem-F1) and the labeled attachment score (LAS) for syntactic dependencies introduced by CoNLL2008 Shared Task1 as evaluation metric.",1 Introduction,[0],[0]
"Considering that various syntactic parsers contribute different syntactic inputs with various range of quality levels, the ratio provides a fairer comparison between syntactically-driven SRL systems, which will be surveyed by our empirical study.",1 Introduction,[0],[0]
"To fully disclose the predicate-argument structure, typical SRL systems have to step by step perform four subtasks.",2 Model,[0],[0]
"Since the predicates in CoNLL2009 (Hajič et al., 2009) corpus have been preidentified, we need to tackle three other subtasks, which are formulized into two-step pipeline in this work, predicate disambiguation and argument labeling.",2 Model,[0],[0]
"Namely, we do the work of argument identification and classification in one model.
",2 Model,[0],[0]
Argument structure for each known predicate will be disclosed by our argument labeler over a sequence including possible arguments (candidates).,2 Model,[0],[0]
"There are two ways to determine the sequence, one is to simply input the entire sentence as a syntax-agnostic SRL system does, the other is to select words according to syntactic parse tree around the predicate as most previous SRL systems did.",2 Model,[0],[0]
The latter strategy usually works through a syntactic tree based argument pruning algorithm.,2 Model,[0],[0]
"We will use the proposed k-order argument pruning algorithm (Section 2.1) to get a sequence w = (w1, . . .",2 Model,[0],[0]
", wn) for each predicate.",2 Model,[0],[0]
"Then, we represent each word wi ∈ w as xi (Section 2.2).",2 Model,[0],[0]
"Eventually, we obtain contextual features with sequence encoder (Section 2.3).",2 Model,[0],[0]
The overall role labeling model is depicted in Figure 1.,2 Model,[0],[0]
"As pointed out by Punyakanok et al. (2008), syntactic information is most relevant in identifying
1CoNLL-2008 is an English-only task, while CoNLL2009 extends to a multilingual one.",2.1 Argument Pruning,[0],[0]
"Their main difference is that predicates have been beforehand indicated for the latter.
",2.1 Argument Pruning,[0],[0]
"the arguments, and the most crucial contribution of full parsing is in the pruning stage.",2.1 Argument Pruning,[0],[0]
"In this paper, we propose a k-order argument pruning algorithm inspired by Zhao et al. (2009b).",2.1 Argument Pruning,[0],[0]
"First of all, for node n and its descendant nd in a syntactic dependency tree, we define the order to be the distance between the two nodes, denoted as D(n, nd).",2.1 Argument Pruning,[0],[0]
"Then we define k-order descendants of given node satisfying D(n, nd) = k, and k-order traversal that visits each node from the given node to its descendant nodes within k-th order.",2.1 Argument Pruning,[0],[0]
"Note that the definition of k-order traversal is somewhat different from tree traversal in terminology.
",2.1 Argument Pruning,[0],[0]
A brief description of the proposed k-order pruning algorithm is given as follow.,2.1 Argument Pruning,[0],[0]
"Initially, we set a given predicate as the current node in a syntactic dependency tree.",2.1 Argument Pruning,[0],[0]
"Then, collect all its argument candidates by the strategy of k-order traversal.",2.1 Argument Pruning,[0],[0]
"Afterwards, reset the current node to its syntactic head and repeat the previous step till the root of the tree.",2.1 Argument Pruning,[0],[0]
"Finally, collect the root and stop.",2.1 Argument Pruning,[0],[0]
The k-order argument algorithm is presented in Algorithm 1 in detail.,2.1 Argument Pruning,[0],[0]
"An example of a syntactic dependency tree for sentence She began to trade the art for money is shown in Figure 2.
",2.1 Argument Pruning,[0],[0]
"The main reasons for applying the extended korder argument pruning algorithm are two-fold.
",2.1 Argument Pruning,[0],[0]
Algorithm 1 k-order argument pruning algorithm,2.1 Argument Pruning,[0],[0]
"Input: A predicate p, the root node r given a syn-
tactic dependency tree T , the order k Output:",2.1 Argument Pruning,[0],[0]
"The set of argument candidates S
1: initialization set p as current node c, c = p 2: for each descendant ni of c in T do 3: if D(c, ni) ≤ k",2.1 Argument Pruning,[0],[0]
"and ni /∈ S then 4: S = S + ni 5: end if 6: end for 7: find the syntactic head ch of c, and let c = ch 8: if c = r then 9: S = S + r
10: else 11: goto step 2 12: end if 13: return argument candidates set S
First, previous standard pruning algorithm may hurt the argument coverage too much, even though indeed arguments usually tend to surround their predicate in a close distance.",2.1 Argument Pruning,[0],[0]
"As a sequence tagging model has been applied, it can effectively handle the imbalanced distribution between arguments and non-arguments, which is hardly tackled by early argument classification models that commonly adopt the standard pruning algorithm.",2.1 Argument Pruning,[0],[0]
"Second, the extended pruning algorithm provides a better trade-off between computational cost and performance by carefully tuning k.",2.1 Argument Pruning,[0],[0]
"We produce a predicate-specific word representation xi for each word wi, where i stands for the word position in an input sequence, following Marcheggiani et al. (2017).",2.2 Word Representation,[0],[0]
"However, we differ by (1) leveraging a predicate-specific indicator embedding, (2) using deeper refined representation, including character and dependency relation embeddings, and (3) applying recent advances in RNNs, such as highway connections (Srivastava et al., 2015).
",2.2 Word Representation,[0],[0]
"In this work, word representation xi is the concatenation of four types of features: predicatespecific feature, character-level, word-level and linguistic features.",2.2 Word Representation,[0],[0]
"Unlike previous work, we leverage a predicate-specific indicator embedding xiei rather than directly using a binary flag either 0 or 1.",2.2 Word Representation,[0],[0]
"At character level, we exploit convolutional neural network (CNN) with bidirectional LSTM (BiLSTM) to learn character embedding
xcei .",2.2 Word Representation,[0],[0]
"As shown in Figure 1, the representation calculated by the CNN is fed as input to BiLSTM.",2.2 Word Representation,[0],[0]
"At word level, we use a randomly initialized word embedding xrei and a pre-trained word embedding xpei .",2.2 Word Representation,[0],[0]
"For linguistic features, we employ a randomly initialized lemma embedding xlei and a randomly initialized POS tag embedding xposi .",2.2 Word Representation,[0],[0]
"In order to incorporate more syntactic information, we adopt an additional feature, the dependency relation to syntactic head.",2.2 Word Representation,[0],[0]
"Likewise, it is a randomly initialized embedding xdei .",2.2 Word Representation,[0],[0]
The resulting word representation is concatenated as xi =,2.2 Word Representation,[0],[0]
"[x ie i , x ce i , x re i , x pe",2.2 Word Representation,[0],[0]
"i , x le i , x pos",2.2 Word Representation,[0],[0]
"i , x de i ].",2.2 Word Representation,[0],[0]
"As Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have shown significant representational effectiveness to NLP tasks, we thus use BiLSTM as the sentence encorder.",2.3 Sequence Encoder,[0],[0]
"Given an input sequence x = (x1, . . .",2.3 Sequence Encoder,[0],[0]
", xn), BiLSTM processes the sequence in both forward and backward direction to obtain two separated hidden states, −→ h i which handles data from x1 to xi and ←− h i which tackles data from xn to xi for each word representation.",2.3 Sequence Encoder,[0],[0]
"Finally, we get a contextual representation hi =",2.3 Sequence Encoder,[0],[0]
"[ −→ h i, ←− h",2.3 Sequence Encoder,[0],[0]
i] by concatenating the states of BiLSTM networks.,2.3 Sequence Encoder,[0],[0]
"To get the final predicted semantic roles, we exploit a multi-layer perceptron (MLP) with highway connections on the top of BiLSTM networks, which takes as input the hidden representation hi
of all time steps.",2.3 Sequence Encoder,[0],[0]
The MLP network consists of 10 layers with highway connections and we employ ReLU activations for the hidden layers.,2.3 Sequence Encoder,[0],[0]
"Finally, we use a softmax layer over the outputs to maximize the likelihood of labels.",2.3 Sequence Encoder,[0],[0]
"Although predicates have been identified given a sentence, predicate disambiguation is an indispensable task, which aims to determine the predicate-argument structure for an identified predicate in a particular context.",2.4 Predicate Disambiguation,[0],[0]
"Here, we also use the identical model (BiLSTM composed with MLP) for predicate disambiguation, in which the only difference is that we remove the syntactic dependency relation feature in corresponding word representation (Section 2.2).",2.4 Predicate Disambiguation,[0],[0]
"Exactly, given a predicate p, the resulting word representation is pi =",2.4 Predicate Disambiguation,[0],[0]
"[p ie i , p ce",2.4 Predicate Disambiguation,[0],[0]
"i , p re i , p pe",2.4 Predicate Disambiguation,[0],[0]
"i , p le i , p pos",2.4 Predicate Disambiguation,[0],[0]
i ].,2.4 Predicate Disambiguation,[0],[0]
"Our model2 is evaluated on the CoNLL-2009 shared task both for English and Chinese datasets, following the standard training, development and test splits.",3 Experiments,[0],[0]
"The hyperparameters in our model were selected based on the development set, and are summarized in Table 1.",3 Experiments,[0],[0]
Note that the parameters of predicate model are the same as these in argument model.,3 Experiments,[0],[0]
"All real vectors are randomly initialized, and the pre-trained word embeddings for English are GloVe vectors (Pennington et al., 2014).",3 Experiments,[0],[0]
"For Chinese, we exploit Wikipedia documents to train Word2Vec embeddings (Mikolov
2The code is available at https://github.com/ bcmi220/srl_syn_pruning.
",3 Experiments,[0],[0]
"et al., 2013).",3 Experiments,[0],[0]
"During training procedures, we use the categorical cross-entropy as objective, with Adam optimizer (Kingma and Ba, 2015).",3 Experiments,[0],[0]
We train models for a maximum of 20 epochs and obtain the nearly best model based on development results.,3 Experiments,[0],[0]
"For argument labeling, we preprocess corpus with k-order argument pruning algorithm.",3 Experiments,[0],[0]
"In addition, we use four CNN layers with singlelayer BiLSTM to induce character representations derived from sentences.",3 Experiments,[0],[0]
"For English3, to further enhance the representation, we adopt CNNBiLSTM character embedding structure from AllenNLP toolkit (Peters et al., 2018).",3 Experiments,[0],[0]
"During the pruning of argument candidates, we use the officially predicted syntactic parses provided by CoNLL-2009 shared-task organizers on both English and Chinese.",3.1 Preprocessing,[0],[0]
Figure 3 shows changing curves of coverage and reduction following k on the English train set.,3.1 Preprocessing,[0],[0]
"According to our statistics, the number of non-arguments is ten times more than that of arguments, where the data distribution is fairly unbalanced.",3.1 Preprocessing,[0],[0]
"However, a proper pruning strategy could alleviate this problem.",3.1 Preprocessing,[0],[0]
"Accordingly, the first-order pruning reduces more than 50% candidates at the cost of missing 5.5% true ones on average, and the second-order prunes about 40% candidates with nearly 2.0% loss.",3.1 Preprocessing,[0],[0]
"The coverage of third-order has achieved 99% and it reduces approximately 1/3 corpus size.
",3.1 Preprocessing,[0],[0]
"It is worth noting that as k is larger than 19,
3For Chinese, we do not use character embedding.
",3.1 Preprocessing,[0],[0]
"there will come full coverage on all argument candidates for English training set, which let our high order pruning algorithm degrade into a syntaxagnostic setting.",3.1 Preprocessing,[0],[0]
"In this work, we use the tenthorder pruning for pursuing the best performance.",3.1 Preprocessing,[0],[0]
"Our system performance is measured with the official script from CoNLL-2009 benchmarks, combining the output of our predicate disambiguation with our semantic role labeling.",3.2 Results,[0],[0]
"Our predicate disambiguation model achieves the accuracy of 95.01% and 95.58%4 on development and test sets, respectively.",3.2 Results,[0],[0]
"We compare our model performance with the state-of-the-art models for dependency SRL.5 Noteworthily, our model is local and single without reranking, which neither includes global inference nor combines multiple models.",3.2 Results,[0],[0]
"The experimental results on the English in-domain (WSJ) and out-of-domain (Brown) test sets are shown in Tables 2 and 3, respectively.
",3.2 Results,[0],[0]
"For English, our syntax-aware model outperforms previously published best single model, scoring 89.5% F1 with 1.5% absolute improvement on the in-domain (WSJ) test data.",3.2 Results,[0],[0]
"Compared
4Note that we give a slightly better predicate model than Roth and Lapata (2016), with 94.77% and 95.47% accuracy on development and test sets, respectively.
5Here, we do not compare against span-based SRL models, which annotate roles for entire argument spans instead of semantic dependencies.
with ensemble models, our single model even provides better performance (+0.4% F1) than the system (Marcheggiani and Titov, 2017), and significantly surpasses all the rest models.",3.2 Results,[0],[0]
"In the syntaxagnostic setting (without pruning and dependency relation embedding), we also reach the new stateof-the-art, achieving a performance gain of 1% F1.
",3.2 Results,[0],[0]
"On the out-of-domain (Brown) test set, we achieve the new best results of 79.3% (syntaxaware) and 78.8% (syntax-agnostic) in F1 scores.",3.2 Results,[0],[0]
"Moreover, our syntax-aware model performs better than the syntax-agnostic one.
",3.2 Results,[0],[0]
Table 4 presents the results on Chinese test set.,3.2 Results,[0],[0]
"Even though we use the same parameters as for English, our model also outperforms the best reported results by 0.3% (syntax-aware) and 0.6% (syntax-agnostic) in F1 scores.",3.2 Results,[0],[0]
"To evaluate the contributions of key factors in our method, a series of ablation studies are performed on the English development set.
",3.3 Analysis,[0],[0]
"In order to demonstrate the effectiveness of our k-order pruning algorithm, we report the SRL performance excluding predicate senses in evaluation, eliminating the performance gain from predicate disambiguation.",3.3 Analysis,[0],[0]
Table 5 shows the results from our syntax-aware model with lower order argument pruning.,3.3 Analysis,[0],[0]
"Compared to the best previous model, our system still yields an increment in recall by more than 1%, leading to improvements in F1 score.",3.3 Analysis,[0],[0]
"It demonstrates that refining syntactic parser tree based candidate pruning does help in argument recognition.
",3.3 Analysis,[0],[0]
"Table 6 presents the performance of our syntaxagnostic SRL system with a basic configuration, which removes components, including indicator and character embeddings.",3.3 Analysis,[0],[0]
"Note that the first row is the results of BiLSTM (removing MLP from basic model), whose encoding is the same as Marcheggiani et al. (2017).",3.3 Analysis,[0],[0]
"Experiments show that both enhanced representations improve over our basic model, and our adopted labeling model is superior to the simple BiLSTM.
Figure 4 shows F1 scores in different k-order pruning together with our syntax-agnostic model.",3.3 Analysis,[0],[0]
"It also indicates that the least first-order pruning fails to give satisfactory performance, the best performing setting coming from a moderate setting of k = 10, and the largest k shows that our argu-
ment pruning falls back to syntax-agnostic type.",3.3 Analysis,[0],[0]
"Meanwhile, from the best k setting to the lower order pruning, we receive a much faster performance drop, compared to the higher order pruning until the complete syntax-agnostic case.",3.3 Analysis,[0],[0]
"The proposed k-order pruning algorithm always works even it reaches the syntax-agnostic setting, which empirically explains why the current syntax-aware and syntax-agnostic SRL models hold little performance difference, as maximum k-order pruning actually removes few words just like syntaxagnostic model.",3.3 Analysis,[0],[0]
"In this work, we consider additional model that integrates predicate disambiguation and argument labeling into one sequence labeling model.",3.4 End-to-end SRL,[0],[0]
"In order to implement an end-to-end model, we introduce a virtual root (VR) for predicate disambiguation similar to Zhao et al. (2013) who handled the entire SRL task as word pair classification.",3.4 End-to-end SRL,[0],[0]
"Concretely, we add a predicate sense feature to the input sequence by concatenating a VR.",3.4 End-to-end SRL,[0],[0]
The word representation of VR is randomly initialized during training.,3.4 End-to-end SRL,[0],[0]
"In Figure 5, we give an example sequence with the labels for the given sentence.
",3.4 End-to-end SRL,[0],[0]
We also report results of our end-to-end model on CoNLL-2009 test set with syntax-aware and syntax-agnostic settings.,3.4 End-to-end SRL,[0],[0]
"As shown in Table 7, our end-to-end model yields slightly weaker performance compared with our pipeline.",3.4 End-to-end SRL,[0],[0]
"A reasonable account for performance degradation is that the training data has completely different genre distributions over predicate senses and argument roles, which may be somewhat confusing for integrative model to make classification decisions.",3.4 End-to-end SRL,[0],[0]
"For a full SRL task, the predicate identification subtask is also indispensable, which has been included in CoNLL-2008 shared task.",3.5 CoNLL-2008 SRL Setting,[0],[0]
"We thus evaluate our model in terms of data and setting of the CoNLL-2008 benchmark (WSJ).
",3.5 CoNLL-2008 SRL Setting,[0],[0]
"To identify predicates, we train the BiLSTMMLP sequence labeling model with same parameters in Section 2.4 to tackle the predicate identification and disambiguation subtasks in one shot, and the only difference is that we remove the predicate-specific indicator feature.",3.5 CoNLL-2008 SRL Setting,[0],[0]
The F1 score of our predicate labeling model is 90.53% on indomain (WSJ) data.,3.5 CoNLL-2008 SRL Setting,[0],[0]
"Compared with the best reported results, we observe absolute improvements in semantic F1 of 0.8% (in Table 8).",3.5 CoNLL-2008 SRL Setting,[0],[0]
"Note that as predicate identification is introduced, our same model shows about 6% performance loss for either syntax-agnostic or syntax-aware case, which indicates that predicate identification should be carefully handled, as it is very needed in a complete practical SRL system.",3.5 CoNLL-2008 SRL Setting,[0],[0]
Syntactic information plays an informative role in semantic role labeling.,4 Syntactic Contribution,[0],[0]
"However, few studies were done to quantitatively evaluate the syntactic contribution to SRL.",4 Syntactic Contribution,[0],[0]
"Furthermore, we observe that most of the above compared neural SRL systems took the syntactic parser of (Björkelund et al., 2010) as syntactic inputs instead of the one from CoNLL-2009 shared task, which adopted a much weaker syntactic parser.",4 Syntactic Contribution,[0],[0]
"Especially (Marcheggiani and Titov, 2017), adopted an external syntactic
parser with even higher parsing accuracy.",4 Syntactic Contribution,[0],[0]
"Contrarily, our SRL model is based on the automatically predicted parse with moderate performance provided by CoNLL-2009 shared task, but outperforms their models.
",4 Syntactic Contribution,[0],[0]
This section thus attempts to explore how much syntax contributes to dependency-based SRL in deep learning framework and how to effectively evaluate relative performance of syntax-based SRL.,4 Syntactic Contribution,[0],[0]
"To this end, we conduct experiments for empirical analysis with different syntactic inputs.
",4 Syntactic Contribution,[0],[0]
"Syntactic Input In order to obtain different syntactic inputs, we design a faulty syntactic tree generator (refer to STG hereafter), which is able to produce random errors in the output parse tree like a true parser does.",4 Syntactic Contribution,[0],[0]
"To simplify implementation, we construct a new syntactic tree based on the gold standard parse tree.",4 Syntactic Contribution,[0],[0]
"Given an input error probability distribution estimated from a true parser output, our algorithm presented in Algorithm 2 stochastically modifies the syntactic heads of nodes on the premise of a valid tree.
",4 Syntactic Contribution,[0],[0]
"Evaluation Measure For SRL task, the primary evaluation measure is the semantic labeled F1 score.",4 Syntactic Contribution,[0],[0]
"However, the score is influenced by the quality of syntactic input to some extent, leading to unfaithfully reflecting the competence of syntax-based SRL system.",4 Syntactic Contribution,[0],[0]
"Namely, this is not the outcome of a true and fair quantitative comparison for these types of SRL models.",4 Syntactic Contribution,[0.9565233505339675],['This is a consequence of our choice of a reward that does not penalize the selection of neutral terms.']
"To normalize the semantic score relative to syntactic parse, we take into account additional evaluation measure to estimate the actual overall performance of SRL.",4 Syntactic Contribution,[0],[0]
"Here, we use the ratio between labeled F1 score for semantic dependencies (Sem-F1) and the labeled attachment score (LAS) for syntactic dependencies
Algorithm 2 Faulty Syntactic Tree Generator Input: A gold standard syntactic tree GT , the
specific error probability p Output: The new generative syntactic tree NT
1: N denotes the number of nodes in GT 2: for each node n ∈ GT do 3: r = random(0, 1), a random number 4: if r < p then 5: h = random(0, N ), a random integer 6: find the syntactic head nh of n in GT 7: modify nh = h, and get a new tree NT 8: if NT is a valid tree then 9: break
10: else 11: goto step 5 12: end if 13: end if 14: end for 15: return the new generative tree NT
proposed by Surdeanu et al. (2008) as evaluation metric.6 The benefits of this measure are twofold: quantitatively evaluating syntactic contribution to SRL and impartially estimating the true performance of SRL, independent of the performance of the input syntactic parser.
",4 Syntactic Contribution,[0],[0]
Table 9 reports the performance of existing models7 in term of Sem-F1/LAS ratio on CoNLL2009 English test set.,4 Syntactic Contribution,[0],[0]
"Interestingly, even though our system has significantly lower scores than others by 3.8% LAS in syntactic components, we
6The idea of ratio score in Surdeanu et al. (2008) actually was from author of this paper, Hai Zhao, which has been indicated in the acknowledgement part of Surdeanu et al. (2008).
7Note that several SRL systems without providing syntactic information are not listed in the table.
obtain the highest results both on Sem-F1 and the Sem-F1/LAS ratio, respectively.",4 Syntactic Contribution,[0],[0]
These results show that our SRL component is relatively much stronger.,4 Syntactic Contribution,[0],[0]
"Moreover, the ratio comparison in Table 9 also shows that since the CoNLL-2009 shared task, most SRL works actually benefit from the enhanced syntactic component rather than the improved SRL component itself.",4 Syntactic Contribution,[0],[0]
"All post-CoNLL SRL systems, either traditional or neural types, did not exceed the top systems of CoNLL-2009 shared task, (Zhao et al., 2009c) (SRL-only track using the provided predicated syntax) and (Zhao et al., 2009a)",4 Syntactic Contribution,[0],[0]
(Joint track using self-developed parser).,4 Syntactic Contribution,[0],[0]
"We believe that this work for the first time reports both higher Sem-F1 and higher Sem-F1/LAS ratio since CoNLL-2009 shared task.
",4 Syntactic Contribution,[0],[0]
"We also perform our first and tenth order pruning models with different erroneous syntactic inputs generated from STG and evaluate their per-
formance using the Sem-F1/LAS ratio.",4 Syntactic Contribution,[0],[0]
Figure 6 shows Sem-F1 scores at different quality of syntactic parse inputs on the English test set whose LAS varies from 85% to 100%.,4 Syntactic Contribution,[0],[0]
"Compared to previous state-of-the-arts (Marcheggiani and Titov, 2017).",4 Syntactic Contribution,[0],[0]
"Our tenth-order pruning model gives quite stable SRL performance no matter the syntactic input quality varies in a broad range, while our firstorder pruning model yields overall lower results (1-5% F1 drop), owing to missing too many true arguments.",4 Syntactic Contribution,[0],[0]
These results show that high-quality syntactic parses may indeed enhance dependency SRL.,4 Syntactic Contribution,[0],[0]
"Furthermore, it indicates that our model with an accurate enough syntactic input as Marcheggiani and Titov (2017), namely, 90% LAS, will give a Sem-F1 exceeding 90% for the first time in the research timeline of semantic role labeling.",4 Syntactic Contribution,[0],[0]
Semantic role labeling was pioneered by Gildea and Jurafsky (2002).,5 Related Work,[0],[0]
"Most traditional SRL models rely heavily on feature templates (Pradhan et al., 2005; Zhao et al., 2009b; Björkelund et al., 2009).",5 Related Work,[0],[0]
"Among them, Pradhan et al. (2005) combined features derived from different syntactic parses based on SVM classifier, while Zhao et al. (2009b) presented an integrative approach for dependency SRL by greedy feature selection algorithm.",5 Related Work,[0],[0]
"Later, Collobert et al. (2011) proposed a convolutional neural network model of inducing word embeddings substituting for hand-crafted features, which was a breakthrough for SRL task.
",5 Related Work,[0],[0]
"With the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017), a series of neural SRL systems have been proposed.",5 Related Work,[0],[0]
"Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach.",5 Related Work,[0],[0]
"Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning.",5 Related Work,[0],[0]
Roth and Lapata (2016) introduced dependency path embedding to model syntactic information and exhibited a notable success.,5 Related Work,[0],[0]
Marcheggiani and Titov (2017) leveraged the graph convolutional network to incorporate syntax into neural models.,5 Related Work,[0],[0]
"Differently, Marcheggiani et al. (2017) proposed a
syntax-agnostic model using effective word representation for dependency SRL, which for the first time achieves comparable performance as stateof-the-art syntax-aware SRL models.
",5 Related Work,[0],[0]
"However, most neural SRL works seldom pay much attention to the impact of input syntactic parse over the resulting SRL performance.",5 Related Work,[0],[0]
"This work is thus more than proposing a high performance SRL model through reviewing the highlights of previous models, and presenting an effective syntactic tree based argument pruning.",5 Related Work,[0],[0]
"Our work is also closely related to (Punyakanok et al., 2008; He et al., 2017).",5 Related Work,[0],[0]
"Under the traditional methods, Punyakanok et al. (2008) investigated the significance of syntax to SRL system and shown syntactic information most crucial in the pruning stage.",5 Related Work,[0],[0]
"He et al. (2017) presented extensive error analysis with deep learning model for span SRL, including discussion of how constituent syntactic parser could be used to improve SRL performance.",5 Related Work,[0],[0]
"This paper presents a simple and effective neural model for dependency-based SRL, incorporating syntactic information with the proposed extended k-order pruning algorithm.",6 Conclusion and Future Work,[0],[0]
"With a large enough setting of k, our pruning algorithm will result in a syntax-agnostic setting for the argument labeling model, which smoothly unifies syntax-aware and syntax-agnostic SRL in a consistent way.",6 Conclusion and Future Work,[0],[0]
"Experimental results show that with the help of deep enhanced representation, our model outperforms the previous state-of-the-art models in both syntaxaware and syntax-agnostic situations.
",6 Conclusion and Future Work,[0],[0]
"In addition, we consider the Sem-F1/LAS ratio as a mean of evaluating syntactic contribution to SRL, and true performance of SRL independent of the quality of syntactic parser.",6 Conclusion and Future Work,[0],[0]
"Though we again confirm the importance of syntax to SRL with empirical experiments, we are aware that since (Pradhan et al., 2005), the gap between syntax-aware and syntax-agnostic SRL has been greatly reduced, from as high as 10% to only 1-2% performance loss in this work.",6 Conclusion and Future Work,[0],[0]
"However, maybe we will never reach a satisfying conclusion, as whenever one proposes a syntax-agnostic SRL system which can outperform all syntax-aware ones at then, always there comes argument that you have never fully explored creative new method to effectively exploit the syntax input.",6 Conclusion and Future Work,[0],[0]
Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence.,abstractText,[0],[0]
Previous studies have shown syntactic information has a remarkable contribution to SRL performance.,abstractText,[0],[0]
"However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone.",abstractText,[0],[0]
This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework.,abstractText,[0],[0]
We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information.,abstractText,[0],[0]
"Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.",abstractText,[0],[0]
"Syntax for Semantic Role Labeling, To Be, Or Not To Be",title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 55–64, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Simultaneous interpretation is challenging because it demands both quality and speed.,1 Introduction,[0],[0]
Conventional batch translation waits until the entire sentence is completed before starting to translate.,1 Introduction,[0],[0]
This merely optimizes translation quality and often introduces undesirable lag between the speaker and the audience.,1 Introduction,[0],[0]
Simultaneous interpretation instead requires a tradeoff between quality and speed.,1 Introduction,[0],[0]
A common strategy is to translate independently translatable segments as soon as possible.,1 Introduction,[0],[0]
"Various segmentation methods (Fujita et al., 2013; Oda et al., 2014) reduce translation delay; they are limited, however, by the unavoidable word reordering between languages with drastically different word orders.",1 Introduction,[0],[0]
We show an example of Japanese-English translation in Figure 1.,1 Introduction,[0],[0]
"Consider the batch translation: in English, the verb change comes immediately after the subject",1 Introduction,[0],[0]
"We, whereas in Japanese it comes at the end
of the sentence; therefore, to produce an intelligible English sentence, we must translate the object after the final verb is observed, resulting in one large and painfully delayed segment.
",1 Introduction,[0],[0]
"To reduce structural discrepancy, we can apply syntactic transformations to make the word order of one language closer to the other.",1 Introduction,[0],[0]
Consider the monotone translation in Figure 1.,1 Introduction,[0],[0]
"By passivizing the English sentence, we can cache the subject and begin translating before observing the final verb.",1 Introduction,[0],[0]
"Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction.",1 Introduction,[0],[0]
"These transformations enable us to divide the input into shorter segments, thus reducing translation delay.
",1 Introduction,[0],[0]
"To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order.",1 Introduction,[0],[0]
Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff.,1 Introduction,[0],[0]
"However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010).",1 Introduction,[0],[0]
"In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compared to human translations done at the translator’s leisure, allowing for more introspection and precise word choice.
",1 Introduction,[0],[0]
We aim to address the data scarcity problem and combine translators’ lexical precision and interpreters’ syntactic flexibility.,1 Introduction,[0],[0]
"We propose to rewrite the reference translation in a way that uses the original lexicon, obeys standard grammar rules of
55
the target language, preserves the original semantics, and yields more monotonic translations.",1 Introduction,[0],[0]
We then train the MT system with the rewritten references so that it learns how to produce low-latency translations from the data.,1 Introduction,[0],[0]
A data-driven approach to learning these rewriting rules is hampered by the dearth of parallel data: we have few examples of text that have been both interpreted and translated.,1 Introduction,[0],[0]
"Therefore, we design syntactic transformation rules based on linguistic analysis of the source and the target languages.",1 Introduction,[0],[0]
We apply these rules to parsed text and decide whether to accept the rewritten sentence based on the amount of delay reduction.,1 Introduction,[0],[0]
"In this work, we focus on Japanese to English translation, because (i) Japanese and English have significantly different word orders (SOV vs. SVO); and consequently, (ii) the syntactic constituents required earlier by an English sentence often come late in the corresponding Japanese sentence.
",1 Introduction,[0],[0]
We evaluate our approach using standard machine translation data (the Reuters newsfeed Japanese-English corpus) in a simultaneous translation setting.,1 Introduction,[0],[0]
Our experimental results show that including the rewritten references into the learning of a phrase-based MT system results in a better speed-accuracy tradeoff against both the original and the rewritten reference translations.,1 Introduction,[0],[0]
Simultaneous interpretation has two goals: producing good translations and producing them promptly.,2 The Problem of Delay Reduction,[0],[0]
"However, most existing parallel corpora and MT systems do not address the issue of delay during translation.",2 The Problem of Delay Reduction,[0],[0]
We explicitly adapt the training data by rewriting rules to reduce delay.,2 The Problem of Delay Reduction,[0],[0]
We first define translation delay and describe—in general terms— our rewriting rules.,2 The Problem of Delay Reduction,[0],[0]
"In the next section, we describe the rules in more detail.
",2 The Problem of Delay Reduction,[0],[0]
"While we are motivated by real-time interpretation, to simplify our problem, we assume that we have perfect text input.",2 The Problem of Delay Reduction,[0],[0]
"Given this constraint, a typical simultaneous interpretation system (Sridhar et al., 2013; Fujita et al., 2013; Oda et al., 2014) produces partial translations of consecutive segments in the source sentence and concatenates them to produce a complete translation.",2 The Problem of Delay Reduction,[0],[0]
We define the translation delay of a sentence as the average number of tokens the system has to observe between translation of two consecutive segments (denoted by # words/seg).1,2 The Problem of Delay Reduction,[0],[0]
"For instance, the minimum delay of 1 word/seg is achieved when we translate immediately upon hearing a word.",2 The Problem of Delay Reduction,[0],[0]
"At test time, when the input is segmented, the delay is the average segment length.",2 The Problem of Delay Reduction,[0],[0]
"During the data preprocessing step of rewriting, we calculate delay from word alignments (Section 4).
",2 The Problem of Delay Reduction,[0],[0]
"Given a reference batch translation x, we apply a set of rewriting rulesR to x to minimize its delay.",2 The Problem of Delay Reduction,[0],[0]
"A rewriting rule r ∈ R is a mapping that takes the constituent parse tree of x as input and outputs a modified parse tree, which specifies a rewritten sentence x′.",2 The Problem of Delay Reduction,[0],[0]
"The tree-editing operation includes node deletion, insertion, and swapping, as well as induced changes of word form and node label.",2 The Problem of Delay Reduction,[0],[0]
"A valid transformation rule should rearrange constituents in x to follow the word order of the input sentence as closely as possible, subject to grammatical constraints and preservation of the original meaning.
",2 The Problem of Delay Reduction,[0],[0]
"1Ideally, delay should be based on time lapse.",2 The Problem of Delay Reduction,[0],[0]
"However, timestamping is not applicable to typical MT corpus, therefore we approximate it by number of tokens and ignore decoding time.",2 The Problem of Delay Reduction,[0],[0]
We design a variety of syntactic transformation rules for Japanese-English translation motivated by their structural differences.,3 Transformation Rules,[0],[0]
"Our rules cover verb, noun, and clause reordering.",3 Transformation Rules,[0],[0]
"While we specifically focus on Japanese to English, many rules are broadly applicable to SOV to SVO languages.",3 Transformation Rules,[0],[0]
The most significant difference between Japanese and English is that the head of a verb phrase comes at the end of Japanese sentences.,3.1 Verb Phrases,[0],[0]
"In English, it occupies one of the initial positions.",3.1 Verb Phrases,[0],[0]
"We now introduce rules that can postpone a head verb.
",3.1 Verb Phrases,[0],[0]
Passivization and Activization,3.1 Verb Phrases,[0],[0]
"In Japanese, the standard structure of a sentence is NP1 NP2 verb, where case markers following the verb indicate the voice of the sentence.",3.1 Verb Phrases,[0],[0]
"However, in English, we have NP1 verb NP2, where the form of the verb indicates its voice.",3.1 Verb Phrases,[0],[0]
Changing the voice is particularly useful when NP2 (object in an active-voice sentence and subject in a passive-voice sentence) is long.,3.1 Verb Phrases,[0],[0]
"By reversing positions of verb and NP2, we are not held back by the upcoming verb and can start to translate NP2 immediately.",3.1 Verb Phrases,[0],[0]
"Figure 1 shows an example in which passive voice can help make the target and source word orders more compatible, but it is not the case that passivizing every sentence would be a good idea; sometimes making a passive sentence active makes the word orders more compatible if the objects are relatively short:
",3.1 Verb Phrases,[0],[0]
O: The talk was denied by the boycott group spokesman.,3.1 Verb Phrases,[0],[0]
R:,3.1 Verb Phrases,[0],[0]
"The boycott group spokesman denied the talk.
",3.1 Verb Phrases,[0],[0]
"Quotative Verbs Quotative verbs are verbs that, syntactically and semantically, resemble said and often start an independent clause.",3.1 Verb Phrases,[0],[0]
"Such verbs are frequent, especially in news, and can be moved to the end of a sentence:
",3.1 Verb Phrases,[0],[0]
O: They announced that the president will restructure the division.,3.1 Verb Phrases,[0],[0]
R:,3.1 Verb Phrases,[0],[0]
"The president will restructure the division, they announced.
",3.1 Verb Phrases,[0],[0]
"In addition to quotative verbs, candidates typically include factive (e.g., know, realize, observe), factive-like (e.g., announce, determine), belief (e.g., believe, think, suspect), and antifactive (e.g., doubt, deny) verbs.",3.1 Verb Phrases,[0],[0]
"When these verbs are followed by a
clause (S or SBAR), we move the verb and its subject to the end of the clause.
",3.1 Verb Phrases,[0],[0]
"While some exploratory work automatically extracts factive verbs, to our knowledge, an exhaustive list does not exist.",3.1 Verb Phrases,[0],[0]
"To obtain a list with reasonable coverage, we exploit the fact that Japanese has an unambiguous quotative particle, to, that precedes such verbs.2 We identify all of the verbs in the Kyoto corpus (Neubig, 2011) marked by the quotative particle and translate them into English.",3.1 Verb Phrases,[0],[0]
We then use these as our quotative verbs.3,3.1 Verb Phrases,[0],[0]
Another difference between Japanese and English lies in the order of adjectives and the nouns they modify.,3.2 Noun Phrases,[0],[0]
"We identify two situations where we can take advantage of the flexibility of English grammar to favor sentence structures consistent with positions of nouns in Japanese.
",3.2 Noun Phrases,[0],[0]
"Genitive Reordering In Japanese, genitive constructions always occur in the form of X no Y, where Y belongs to X.",3.2 Noun Phrases,[0],[0]
"In English, however, the order may be reversed through the of construction.",3.2 Noun Phrases,[0],[0]
"Therefore, we transform constructions NP1 of NP2 to possessives using the apostrophe-s, NP2’(s) NP1 (Figure 1).",3.2 Noun Phrases,[0],[0]
We use simple heuristics to decide if such a transformation is valid.,3.2 Noun Phrases,[0],[0]
"For example, when X / Y contains proper nouns (e.g., the City of New York), numbers (e.g., seven pounds of sugar), or pronouns (e.g., most of them), changing them to the possessive case is not legal.
that Clause In English, clauses are often modified through a pleonastic pronoun.",3.2 Noun Phrases,[0],[0]
"E.g., It is ADJP to/that SBAR/S.",3.2 Noun Phrases,[0],[0]
"In Japanese, however, the subject (clause) is usually put at the beginning.",3.2 Noun Phrases,[0],[0]
"To be consistent with the Japanese word order, we move the modified clause to the start of the sentence: To S/SBAR is ADJP.",3.2 Noun Phrases,[0],[0]
"The rewritten English sentence is still grammatical, although its structure is less frequent in common English usage.",3.2 Noun Phrases,[0],[0]
"For example,
O: It is important to remain watchful.",3.2 Noun Phrases,[0],[0]
"R: To remain watchful is important.
",3.2 Noun Phrases,[0],[0]
2We use a morphological analyzer to distinguish between the conjunction and quotative particles.,3.2 Noun Phrases,[0],[0]
"Examples of words marked by this particle include 見られる (expect), 言う (say), 思われる (seem), する (assume), 信じる (believe) and so on.
",3.2 Noun Phrases,[0],[0]
3We also include the phrase It looks like.,3.2 Noun Phrases,[0],[0]
"In Japanese, clausal conjunctions are often marked at the end of the initial clause of a compound sentence.",3.3 Conjunction Clause,[0],[0]
"In English, however, the order of clauses is more flexible.",3.3 Conjunction Clause,[0],[0]
We can therefore reduce delay by reordering the English clauses to mirror how they typically appear in Japanese.,3.3 Conjunction Clause,[0],[0]
"Below we describe rules reversing the order of clauses connected by these conjunctions:
• Clausal conjunctions: because (of), in order to • Contrastive conjunctions: despite, even though, although • Conditionals: (even) if, as a result (of) •",3.3 Conjunction Clause,[0],[0]
"Misc: according to
In standard Japanese, such conjunctions include no de, kara, de mo and so on.",3.3 Conjunction Clause,[0],[0]
"The sentence often appears in the form of S2 conj, S1.",3.3 Conjunction Clause,[0],[0]
"In English, however, two common constructions are
S1 conj S2: We should march because winter is coming.",3.3 Conjunction Clause,[0],[0]
"conj S2, S1: Because winter is coming, we should march.
",3.3 Conjunction Clause,[0],[0]
"To follow the Japanese clause order, we adapt the above two constructions to
S2, conj’ S1: Winter is coming, because of this, we should march.
",3.3 Conjunction Clause,[0],[0]
Here conj’ represents the original conjunction word appended with simple pronouns/phrases to refer to S2.,3.3 Conjunction Clause,[0],[0]
"For example, because → because of this, even if → even if this is the case.",3.3 Conjunction Clause,[0],[0]
We now turn our attention to the implementation of the syntactic transformation rules described above.,4 Sentence Rewriting Process,[0],[0]
"Applying a transformation consists of three steps:
1.",4 Sentence Rewriting Process,[0],[0]
Detection:,4 Sentence Rewriting Process,[0],[0]
Identify nodes in the parse tree for which the transformation is applicable; 2.,4 Sentence Rewriting Process,[0],[0]
Modification: Transform nodes and labels; 3.,4 Sentence Rewriting Process,[0],[0]
"Evaluation: Compute delay reduction, and
decide whether to accept the rewritten sentence.
",4 Sentence Rewriting Process,[0],[0]
Figure 2 illustrates the process using passivization as an example.,4 Sentence Rewriting Process,[0],[0]
"In the detection step, we find the subtree that satisfies the condition of applying a rule.",4 Sentence Rewriting Process,[0],[0]
"In this case, we look for an S node whose children include an NP (denoted by NP1), the subject, and a VP to its right, such that the VP node has a leaf VB*, the main verb,4 followed by another NP (denoted by NP2), the object.",4 Sentence Rewriting Process,[0],[0]
We allow the parent nodes (S and VP) to have additional children besides the matched ones.,4 Sentence Rewriting Process,[0],[0]
They are not affected during the transformation.,4 Sentence Rewriting Process,[0],[0]
"In the modification step, we swap the subject node and object node; we add the verb be in its correct form by checking the tense of the verb and the form of NP2;5and we add the preposition by before the subject.",4 Sentence Rewriting Process,[0],[0]
"The process is executed recursively throughout the parse tree.
",4 Sentence Rewriting Process,[0],[0]
"4The main verb excludes be and have when it indicates tense (e.g., have done).
",4 Sentence Rewriting Process,[0],[0]
"5We use the Nodebox linguistic library (https://www. nodebox.net/code) to detect and modify word forms.
",4 Sentence Rewriting Process,[0],[0]
"Although our rules are designed to minimize long range reordering, there are exceptions.6 Thus applying a rule does not always reduce delay.",4 Sentence Rewriting Process,[0],[0]
"In the evaluation step, we compare translation delay before and after applying the rule.",4 Sentence Rewriting Process,[0],[0]
"We accept a rewritten sentence if its delay is reduced; otherwise, we revert to the input sentence.",4 Sentence Rewriting Process,[0],[0]
"Since we do not segment sentences during rewriting, we must estimate the delay.
",4 Sentence Rewriting Process,[0],[0]
"To estimate the delay, we use word alignments.",4 Sentence Rewriting Process,[0],[0]
Figure 2c shows the source Japanese sentence in its word-for-word English translation and alignments from the target words to the source words.,4 Sentence Rewriting Process,[0],[0]
"The first English word, We, is aligned to the first Japanese word; it can thus be treated as an independent segment and translated immediately.",4 Sentence Rewriting Process,[0],[0]
"The second English word, love, is aligned to the last Japanese word, which means the system cannot start to translate until four more Japanese words are revealed.",4 Sentence Rewriting Process,[0],[0]
This alignment therefore forms a segment with delay of four words/seg.,4 Sentence Rewriting Process,[0],[0]
"Alignments of the following words come before the source word aligned to love; hence, they are already translated in the previous segment and we do not double count their delay.",4 Sentence Rewriting Process,[0],[0]
"In this example, the delay of the original sentence is 2.5 word/seg; after rewriting, it is reduced to 1.7 word/seg.",4 Sentence Rewriting Process,[0],[0]
"Therefore, we accept the rewritten sentence.",4 Sentence Rewriting Process,[0],[0]
"However, when the subject phrase is long and the object phrase is short, a swap may not reduce delay.
",4 Sentence Rewriting Process,[0],[0]
We can now formally define the delay.,4 Sentence Rewriting Process,[0],[0]
Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to.,4 Sentence Rewriting Process,[0],[0]
"We define the delay of ei as di = max(0, ai−maxj<i aj).",4 Sentence Rewriting Process,[0],[0]
The delay of x is then ∑N i=1,4 Sentence Rewriting Process,[0],[0]
"di/N , where the sum is over all aligned words except punctuation and stopwords.
",4 Sentence Rewriting Process,[0],[0]
"Given a set of rules, we need to decide which rules to apply and in what order.",4 Sentence Rewriting Process,[0],[0]
"Fortunately, our rules have little interaction with each other, and the order of application has a negligible effect.",4 Sentence Rewriting Process,[0],[0]
"We apply the rules, roughly, sequentially in order of complexity: if the output of current rule is not accepted, the sentence is reverted to the last accepted version.",4 Sentence Rewriting Process,[0],[0]
"We evaluate our method on the Reuters JapaneseEnglish corpus of news articles (Utiyama and Isahara, 2003).",5 Experiments,[0],[0]
"For training the MT system, we also include the EIJIRO dictionary entries and the accompanying example sentences.7 Statistics of the dataset are shown in Table 1.",5 Experiments,[0],[0]
"The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents.
",5 Experiments,[0],[0]
"We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences.",5 Experiments,[0],[0]
"Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings.",5 Experiments,[0],[0]
We use GIZA++,5 Experiments,[0],[0]
"(Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning.",5 Experiments,[0],[0]
"The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model.",5 Experiments,[0],[0]
"After applying the rewriting rules (Section 4), Table 2 shows the percentage of sentences that are candidates and how many rewrites are accepted.",5.1 Quality of Rewritten Translations,[0],[0]
The most generalizable rules are passivization and delaying quotative verbs.,5.1 Quality of Rewritten Translations,[0],[0]
"We rewrite 32.2% of sentences, reducing the delay from 9.9 words/seg to 6.3 words/seg per segment for rewritten sentences and from 7.8 words/seg to 6.7 words/seg overall.
",5.1 Quality of Rewritten Translations,[0],[0]
"6For example, in clause transformation, the Japanese conjunction moshi, which is clause initial, may appear at the beginning of a sentence to emphasize conditionals, although its appearance is relatively rare.
",5.1 Quality of Rewritten Translations,[0],[0]
"7Available at http://eijiro.jp 8Available at http://www.atilika.org/ 9In contrast to BLEU, RIBES is an order-sensitive metric commonly used for translation between Japanese and English.
",5.1 Quality of Rewritten Translations,[0],[0]
We evaluate the quality of our rewritten sentences from two perspectives: grammaticality and preserved semantics.,5.1 Quality of Rewritten Translations,[0],[0]
"To examine how close the rewritten sentences are to standard English, we train a 5-gram language model using the English data from the Europarl corpus, consisting of 46 million words, and use it to compute perplexity.",5.1 Quality of Rewritten Translations,[0],[0]
Rewriting references increases the perplexity under the language model only slightly: from 332.0 to 335.4.,5.1 Quality of Rewritten Translations,[0],[0]
"To ensure that rewrites leave meaning unchanged, we use the SEMAFOR semantic role labeler (Das et al., 2014) on the original and modified sentence; for each role-labeled token in the reference sentence, we examine its corresponding role in the rewritten sentence and calculate the average accuracy acrosss all sentences.",5.1 Quality of Rewritten Translations,[0],[0]
"Even ignoring benign lexical changes—for example, he becoming him in a passivized sentence—95.5% of the words retain their semantic roles in the rewritten sentences.
",5.1 Quality of Rewritten Translations,[0],[0]
"Although our rules are conservative to minimize corruption, some errors are unavoidable propagation of parser errors.",5.1 Quality of Rewritten Translations,[0],[0]
"For example, the sentence the London Stock Exchange closes at 1230 GMT today is parsed as:10 (S (NP the London Stock Exchange) (VP (VBZ closes)
(PP at 1230) (NP GMT today)))
",5.1 Quality of Rewritten Translations,[0],[0]
GMT today is separated from the PP as an NP and is mistaken as the object.,5.1 Quality of Rewritten Translations,[0],[0]
The passive version is then GMT today is closed at 1230 by the London Stock Exchange.,5.1 Quality of Rewritten Translations,[0],[0]
"Such errors could be reduced by skipping nodes with low inside/outside scores given by the parser, or skipping low-frequency patterns.",5.1 Quality of Rewritten Translations,[0],[0]
"However, we leave this for future work.",5.1 Quality of Rewritten Translations,[0],[0]
"At test time, we use right probability (Fujita et al., 2013, RP) to decide when to start translating a
10For simplicity we show the shallow parse only.
sentence.",5.2 Segmentation,[0],[0]
"As we read in the source Japanese sentence, if the input segment matches an entry in the learned phrase table, we query the RP of the Japanese/English phrase pair.",5.2 Segmentation,[0],[0]
A higher RP indicates that the English translation of this Japanese phrase will likely be followed by the translation of the next Japanese phrase.,5.2 Segmentation,[0],[0]
"In other words, translation of the two consecutive Japanese phrases is monotonic, thus, we can begin translating immediately.",5.2 Segmentation,[0],[0]
"Following (Fujita et al., 2013), if the RP of the current phrase is lower than a fixed threshold, we cache the current phrase and wait for more words from the source sentence; otherwise, we translate all cached phrases.",5.2 Segmentation,[0],[0]
"Finally, translations of segments are concatenated to form a complete translation of the input sentence.",5.2 Segmentation,[0],[0]
"To show the effect of rewritten references, we compare the following MT systems:
• GD: only gold reference translations; • RW: only rewritten reference translations; • RW+GD: both gold and the rewritten refer-
ences; and • RW-LM+GD: using gold reference transla-
tions but using the rewritten references for training the LM and for tuning.
",5.3 Speed/Accuracy Trade-off,[0],[0]
"For RW+GD and RW-LM+GD, we interpolate the language models of GD and RW.",5.3 Speed/Accuracy Trade-off,[0],[0]
The interpolating weight is tuned with the rewritten sentences.,5.3 Speed/Accuracy Trade-off,[0],[0]
"For RW+GD, we combine the translation models (phrase tables and reordering tables) of RW and GD by fill-up combination (Bisazza et al., 2011), where all entries in the tables of RW are preserved and entries from the tables of GD are added if new.
Increasing the RP threshold increases interpretation delay but improves the quality of the translation.",5.3 Speed/Accuracy Trade-off,[0],[0]
"We set the RP threshold at 0.0, 0.2, 0.4, 0.8 and finally 1.0 (equivalent to batch translation).",5.3 Speed/Accuracy Trade-off,[0],[0]
Figure 3 shows the BLEU/RIBES scores vs. the number of words per segement as we increase the threshold.,5.3 Speed/Accuracy Trade-off,[0],[0]
Rewritten sentences alone do not significantly improve over the baseline.,5.3 Speed/Accuracy Trade-off,[0],[0]
"We suspect this is because the transformation rules sometimes generate ungrammatical sentences due to parsing errors, which impairs learning.",5.3 Speed/Accuracy Trade-off,[0],[0]
"However, combining RW and GD results in a better speed-accuracy tradeoff: the RW+GD curve completely dominates other curves in Figure 3a, 3c.",5.3 Speed/Accuracy Trade-off,[0],[0]
"Thus, using more monotone translations improves simultaneous machine translation, and because RW-LM+GD is about
0 5 10 15 20 25 30 35
Average # of words per segment
11
12
13
14
15
16
17
18
B LE
U
RW+GD RW-LM+GD RW GD
(a) BLEU w.r.t.",5.3 Speed/Accuracy Trade-off,[0],[0]
"gold ref
0 5 10 15 20 25 30 35
Average # of words per segment
59.5
60.0
60.5
61.0
61.5
62.0
62.5
R IB
E S
RW+GD RW-LM+GD RW GD
(b) RIBES w.r.t.",5.3 Speed/Accuracy Trade-off,[0],[0]
"gold ref
0 5 10 15 20 25 30 35
Average # of words per segment
10
11
12
13
14
15
16
17
18
B LE
U
RW+GD RW-LM+GD RW GD
(c) BLEU",5.3 Speed/Accuracy Trade-off,[0],[0]
w.r.t.,5.3 Speed/Accuracy Trade-off,[0],[0]
"rewritten ref
0 5 10 15 20 25 30 35
Average # of words per segment
59.5
60.0
60.5
61.0
61.5
62.0
62.5
R IB
E S
RW+GD",5.3 Speed/Accuracy Trade-off,[0],[0]
"RW-LM+GD RW GD
(d) RIBES w.r.t.",5.3 Speed/Accuracy Trade-off,[0],[0]
"rewritten ref
Figure 3: Speed/accuracy tradeoff curves: BLEU (left) /",5.3 Speed/Accuracy Trade-off,[0],[0]
"RIBES (right) versus translation delay (average number of words per segment).
",5.3 Speed/Accuracy Trade-off,[0],[0]
"the same as GD, the major improvement likely comes from the translation model from rewritten sentences.
",5.3 Speed/Accuracy Trade-off,[0],[0]
The right two plots recapitulate the evaluation with the RIBES metric.,5.3 Speed/Accuracy Trade-off,[0],[0]
"This result is less clear, as MT systems are optimized for BLEU and RIBES penalizes word reordering, making it difficult to compare systems that intentionally change word order.",5.3 Speed/Accuracy Trade-off,[0],[0]
"Nevertheless, RW is comparable to GD on gold references and superior to the baseline on rewritten references.",5.3 Speed/Accuracy Trade-off,[0],[0]
"Rewriting training data not only creates lower latency simultaneous translations, but it also improves batch translation.",5.4 Effect on Verbs,[0],[0]
One reason is that SOV to SVO translation often drops the verb because of long range reordering.,5.4 Effect on Verbs,[0],[0]
"(We see this for Japanese here, but this is also true for German.)",5.4 Effect on Verbs,[0],[0]
"Similar word orders in the source and target results in less reordering and improves phrase-based MT (Collins
et al., 2005; Xu et al., 2009).",5.4 Effect on Verbs,[0],[0]
"Table 3 shows the number of verbs in the translations of the test sentences produced by GD, RW, RW+GD, as well as the number in the gold reference translation.",5.4 Effect on Verbs,[0],[0]
"Both RW and RW+GD produce more verbs (a statistically significant result), although RW+GD captures the most verbs.",5.4 Effect on Verbs,[0],[0]
Table 4 compares translations by GD and RW.,5.5 Error Analysis,[0],[0]
"RW correctly puts the verb said at the end, while GD drops the final verb.",5.5 Error Analysis,[0],[0]
"However, RW still produces he at the beginning (also the first word in the Japanese source sentence).",5.5 Error Analysis,[0],[0]
This is because our current segmentation strategy do not preserve words for later translation—a note-taking strategy used by human interpreters.,5.5 Error Analysis,[0],[0]
Previous approaches to simultaneous machine translation have employed explicit interpretation strategies for coping with delay.,6 Related Work,[0],[0]
"Two major approaches are segmentation and prediction.
",6 Related Work,[0],[0]
"Most segmentation strategies are based on heuristics, such as pauses in speech (Fügen et al., 2007; Bangalore et al., 2009), comma prediction (Sridhar et al., 2013) and phrase reordering probability (Fujita et al., 2013).",6 Related Work,[0],[0]
Learning-based methods have also been proposed.,6 Related Work,[0],[0]
Oda et al. (2014) find segmentations that maximize the BLEU score of the final concatenated translation by dynamic programming.,6 Related Work,[0],[0]
Grissom II et al. (2014) formulate simultaneous translation as a sequential decision making problem and uses reinforcement learning to decide when to translate.,6 Related Work,[0],[0]
"One limitation of these methods is that when learning with standard batch MT corpus, their gain can be restricted by natural word reordering between the source and the target sentences, as explained in Section 1.
",6 Related Work,[0],[0]
"In an SOV-SVO context, methods to predict unseen words are proposed to alleviate the above restriction.",6 Related Work,[0],[0]
Matsubara et al. (1999) predict the English verb in the target sentence and integrates it syntactically.,6 Related Work,[0],[0]
"Grissom II et al. (2014) predict the final verb in the source sentence and decide when to use the predicted verb with reinforcement learning.
",6 Related Work,[0],[0]
"Nevertheless, unless the predictor considers contextual and background information, which human interpreters often rely on for prediction (Hönig, 1997; Camayd-Freixas, 2011), such a prediction task is inherently hard.
",6 Related Work,[0],[0]
"Unlike previous approaches to simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one.",6 Related Work,[0],[0]
"We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system.",6 Related Work,[0],[0]
"In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined.
",6 Related Work,[0],[0]
"This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders.",6 Related Work,[0],[0]
"However, our problem is different in several ways.",6 Related Work,[0],[0]
"First, while the approaches resemble each other, our motivation is to reduce translation delay.",6 Related Work,[0],[0]
"Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed.",6 Related Work,[0],[0]
"Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences.",6 Related Work,[0],[0]
Training MT systems with more monotonic (interpretation-like) sentences improves the speedaccuracy tradeoff for simultaneous machine translation.,7 Conclusion,[0],[0]
"By designing syntactic transformations and rewriting batch translations into more monotonic translations, we reduce the translation delay.",7 Conclusion,[0],[0]
"MT systems trained on the rewritten reference translations learn interpretation strategies implicitly from the data.
",7 Conclusion,[0],[0]
Our rewrites are based on linguistic knowledge and inspired by techniques used by human interpreters.,7 Conclusion,[0],[0]
"They cover a wide range of reordering phenomena between Japanese and English, and more generally, between SOV and SVO languages.",7 Conclusion,[0],[0]
A natural extension is to automatically extract such rules from parallel corpora.,7 Conclusion,[0],[0]
"While there exist approaches that extract syntactic tree transformation rules automatically, one of the difficulties is that most parallel corpora is dominated by lexical paraphrasing instead of syntactic paraphrasing.",7 Conclusion,[0],[0]
This work was supported by NSF grant IIS1320538.,Acknowledgments,[0],[0]
Boyd-Graber is also partially supported by NSF grants CCF-1409287 and NCSE-1422492.,Acknowledgments,[0],[0]
Daumé III,Acknowledgments,[0],[0]
and He are also partially supported by NSF grant IIS-0964681.,Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.",Acknowledgments,[0],[0]
Divergent word order between languages causes delay in simultaneous machine translation.,abstractText,[0],[0]
We present a sentence rewriting method that generates more monotonic translations to improve the speedaccuracy tradeoff.,abstractText,[0],[0]
We design grammaticality and meaning-preserving syntactic transformation rules that operate on constituent parse trees.,abstractText,[0],[0]
We apply the rules to reference translations to make their word order closer to the source language word order.,abstractText,[0],[0]
"On Japanese-English translation (two languages with substantially different structure), incorporating the rewritten, more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline system that only uses gold reference translations.",abstractText,[0],[0]
Syntax-based Rewriting for Simultaneous Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1325–1337 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1325",text,[0],[0]
Dependency parsing is a core task in natural language processing (NLP).,1 Introduction,[0],[0]
"Given a sentence, a dependency parser produces a dependency tree, which specifies the typed head-modifier relations between pairs of words.",1 Introduction,[0],[0]
"While supervised dependency parsing has been successful (McDonald and Pereira, 2006; Nivre, 2008; Kiperwasser and Goldberg, 2016), unsupervised parsing can hardly produce useful parses (Mareček, 2016).",1 Introduction,[0],[0]
So it is extremely helpful to have some treebank of supervised parses for training purposes.,1 Introduction,[0],[0]
"Unfortunately, manually constructing a treebank for a new target language is expensive (Böhmová et al., 2003).",1.1 Past work: Cross-lingual transfer,[0],[0]
"As an alternative, cross-lingual transfer parsing (McDonald et al., 2011) is sometimes possible, thanks to the recent development of multi-lingual treebanks (McDonald et al., 2013; Nivre et al., 2015; Nivre et al., 2017).",1.1 Past work: Cross-lingual transfer,[0],[0]
The idea is to parse the sentences of the target language with a supervised parser trained on the treebanks of one or more source languages.,1.1 Past work: Cross-lingual transfer,[0],[0]
"Although the parser cannot be expected to know the words of the target language, it can make do with parts of
speech (POS) (McDonald et al., 2011; Täckström",1.1 Past work: Cross-lingual transfer,[0],[0]
"et al., 2013; Zhang and Barzilay, 2015) or crosslingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016).",1.1 Past work: Cross-lingual transfer,[0],[0]
"A more serious challenge is that the parser may not know how to handle the word order of the target language, unless the source treebank comes from a closely related language (e.g., using German to parse Luxembourgish).",1.1 Past work: Cross-lingual transfer,[0],[0]
"Training the parser on trees from multiple source languages may mitigate this issue (McDonald et al., 2011) because the parser is more likely to have seen target part-of-speech sequences somewhere in the training data.",1.1 Past work: Cross-lingual transfer,[0],[0]
"Some authors (Rosa and Žabokrtský, 2015a,b; Wang and Eisner, 2016) have shown additional improvements by preferring source languages that are “close” to the target language, where the closeness is measured by distance between POS language models trained on the source and target corpora.",1.1 Past work: Cross-lingual transfer,[0],[0]
"We will focus on delexicalized dependency parsing, which maps an input POS tag sequence to a dependency tree.",1.2 This paper: Tailored synthetic data,[0],[0]
"We evaluate single-source transfer—train a parser on a single source language, and evaluate it on the target language.",1.2 This paper: Tailored synthetic data,[0],[0]
"This is the setup of Zeman and Resnik (2008) and Søgaard (2011a).
",1.2 This paper: Tailored synthetic data,[0],[0]
"Our novel ingredient is that rather than seek a close source language that already exists, we create one.",1.2 This paper: Tailored synthetic data,[0],[0]
How?,1.2 This paper: Tailored synthetic data,[0],[0]
"Given a dependency treebank of a possibly distant source language, we stochastically permute the children of each node, according to some distribution that makes the permuted language close to the target language.
",1.2 This paper: Tailored synthetic data,[0],[0]
And how do we find this distribution?,1.2 This paper: Tailored synthetic data,[0],[0]
We adopt the tree-permutation model of Wang and Eisner (2016).,1.2 This paper: Tailored synthetic data,[0],[0]
"We design a dynamic programming algorithm which, for any given distribution p in Wang and Eisner’s family, can compute the expected counts of all POS bigrams in the permuted source treebank.",1.2 This paper: Tailored synthetic data,[0],[0]
"This allows us to evaluate p by computing the divergence between the bigram POS language model formed by these expected counts,
and the one formed by the observed counts of POS bigrams in the unparsed target language.",1.2 This paper: Tailored synthetic data,[0],[0]
"In order to find a p that locally minimizes this divergence, we adjust the model parameters by stochastic gradient descent (SGD).",1.2 This paper: Tailored synthetic data,[0],[0]
Better measures of surface closeness between two languages might be devised.,1.3 Key limitations in this paper,[0],[0]
"However, even counting the expected POS N -grams is moderately expensive, taking time exponential in N if done exactly.",1.3 Key limitations in this paper,[0],[0]
"So we compute only these local statistics, and only for N = 2.",1.3 Key limitations in this paper,[0],[0]
We certainly need N > 1 because the 1-gram distribution is not affected by permutation at all.,1.3 Key limitations in this paper,[0],[0]
"N = 2 captures useful bigram statistics: for example, to mimic a verb-final language with prenominal modifiers, we would seek constituent permutations that result in matching its relatively high rate of VERB–PUNCT and ADJ–NOUN bigrams.",1.3 Key limitations in this paper,[0],[0]
"While N > 2 might have improved the results, it was too slow for our large-scale experimental design.",1.3 Key limitations in this paper,[0],[0]
"§7 discusses how richer measures could be used in the future.
",1.3 Key limitations in this paper,[0],[0]
"We caution that throughout this paper, we assume that our corpora are annotated with gold POS tags, even in the target language (which lacks any gold training trees).",1.3 Key limitations in this paper,[0],[0]
This is an idealized setting that has often been adopted in work on unsupervised and cross-lingual transfer.§7 discusses a possible avenue for doing without gold tags.,1.3 Key limitations in this paper,[0],[0]
We begin by motivating the idea of tree permutation.,2 Modeling Surface Realization,[0],[0]
Let us suppose that the dependency tree for a sentence starts as a labeled graph—a tree in which siblings are not yet ordered with respect to their parent or one another.,2 Modeling Surface Realization,[0],[0]
Each language has some systematic way to realize its unordered trees as surface strings:1 it imposes a particular order on the tree’s word tokens.,2 Modeling Surface Realization,[0],[0]
"More precisely, a language specifies a distribution p(string | unordered tree) over a tree’s possible realizations.
",2 Modeling Surface Realization,[0],[0]
"As an engineering matter, we now make the strong assumption that the unordered dependency trees are similar across languages.",2 Modeling Surface Realization,[0],[0]
"That is, we suppose that different languages use similar underlying syntactic/semantic graphs, but differ in how they realize this graph structure on the surface.
",2 Modeling Surface Realization,[0],[0]
"1Modeling this process was the topic of the recent Surface Realization Shared Task (Mille et al., 2018).",2 Modeling Surface Realization,[0],[0]
"Most relevant is work on tree linearization (Filippova and Strube, 2009; Futrell and Gibson, 2015; Puzikov and Gurevych, 2018).
",2 Modeling Surface Realization,[0],[0]
"Thus, given a gold POS corpus u of the unknown target language, we may hope to explain its distribution of surface POS bigrams as the result of applying some target-language surface realization model to the distribution of cross-linguistically “typical” unordered trees.",2 Modeling Surface Realization,[0],[0]
"To obtain samples of the latter distribution, we use the treebanks of one or more other languages.",2 Modeling Surface Realization,[0],[0]
The present paper evaluates our method when only a single source treebank is used.,2 Modeling Surface Realization,[0],[0]
"In the future, we could try tuning a mixture of all available source treebanks.",2 Modeling Surface Realization,[0],[0]
We presume that the target language applies the same stochastic realization model to all trees.,2.1 Realization is systematic,[0],[0]
All that we can optimize is the parameter vector of this model.,2.1 Realization is systematic,[0],[0]
"Thus, we deny ourselves the freedom to realize each individual tree in an ad hoc way.",2.1 Realization is systematic,[0],[0]
"To see why this is important, suppose the target language is French, whose corpus u contains many NOUN–ADJ bigrams.",2.1 Realization is systematic,[0],[0]
"We could achieve such a bigram from the unordered source tree
DET NOUN VERB PROPN ADJ
the cake made Sue sleepy
det nsubj dobj xcomp
by ordering
it to yield DET NOUN ADJ VERB PROPN the cake sleepy made Sue
det dobjxcomp nsubj
.",2.1 Realization is systematic,[0],[0]
"However, that realization is not in fact appropriate for French, so that ordered tree would not be a useful training tree for French.",2.1 Realization is systematic,[0],[0]
"Our approach should disprefer this tempting but incorrect realization, because any model with a high probability of this realization would, if applied systematically over the whole corpus, also yield sentences like He sleepy made Sue, with unwanted PRON–ADJ bigrams that would not match the surface statistics of French.",2.1 Realization is systematic,[0],[0]
"We hope our approach will instead choose the realization model that is correct for French, in which the NOUN–ADJ bigrams arise instead from source trees where the ADJ is a dependent of the NOUN, yielding (e.g.)
DET NOUN ADJ VERB PROPN the cake tasty pleased Sue
dobjdet amod nsubj
.",2.1 Realization is systematic,[0],[0]
"This has the same POS sequence as the example above (as it happens), but now assigns the correct tree to it.",2.1 Realization is systematic,[0],[0]
"As our family of realization distributions, we adopt the log-linear model used for this purpose by Wang and Eisner (2016).",2.2 A parametric realization model,[0],[0]
"The model assumes that the root node a of the unordered dependency tree selects an ordering π(a) of the na nodes consisting
of a and its na − 1 dependent children.",2.2 A parametric realization model,[0],[0]
The procedure is repeated recursively at the child nodes.,2.2 A parametric realization model,[0],[0]
"This method can produce only projective trees.
",2.2 A parametric realization model,[0],[0]
"Each node a draws its ordering π(a) independently according to
pθ(π | a) = 1
Z(a) exp ∑ 1≤i<j≤na θ · f(π, i, j) (1)
which is a distribution over the na! possible orderings.",2.2 A parametric realization model,[0],[0]
Z(a) is a normalizing constant.,2.2 A parametric realization model,[0],[0]
"f is a feature vector extracted from the ordered pair of nodes πi, πj , and θ is the model’s parameter vector of feature weights.",2.2 A parametric realization model,[0],[0]
"See Appendix A for the feature templates, which are a subset of those used by Wang and Eisner (2016).",2.2 A parametric realization model,[0],[0]
These features are able to examine the tree’s node labels (POS tags) and edge labels (dependency relations).,2.2 A parametric realization model,[0],[0]
"Thus, when a is a verb, the model can assign a positive weight to “subject precedes verb” or “subject precedes object,” thus preferring orderings with these features.
",2.2 A parametric realization model,[0],[0]
"Following Wang and Eisner (2016, §3.1), we choose new orderings for the noun and verb nodes only,2 preserving the source treebank’s order at all other nodes a.",2.2 A parametric realization model,[0],[0]
"Given a source treebank B and some parameters θ, we can use equation (1) to randomly sample realizations of the trees inB.",2.3 Generating training data,[0],[0]
The effect is to reorder dependent phrases within those trees.,2.3 Generating training data,[0],[0]
The resulting permuted treebank B′ can be used to train a parser for the target language.,2.3 Generating training data,[0],[0]
So how do we choose θ that works for the target language?,2.4 Choosing parameters θ,[0],[0]
"Suppose u is a corpus of targetlanguage POS sequences, using the same set of POS tags as B. We evaluate parameters θ according to whether POS tag sequences in B′ will be distributed like POS tag sequences in u.
To do this, first we estimate a bigram language model q̂ from the actual distribution q of POS sequences observed in u. Second, let pθ denote the distribution of POS sequences that we expect to see in B′, that is, POS sequences obtained by
2Specifically, the 93% of nodes tagged with NOUN, PROPN, PRON or VERB in Universal Dependencies format.",2.4 Choosing parameters θ,[0],[0]
"In retrospect, this restriction was unnecessary in our setting, but it skipped only 4.4% of nodes on average (from 2% to 11% depending on language).",2.4 Choosing parameters θ,[0],[0]
"The remaining nodes were nouns, verbs, or childless.
stochastically realizing observed trees in B according to θ.",2.4 Choosing parameters θ,[0],[0]
"We estimate another bigram model p̂θ from this distribution pθ.
",2.4 Choosing parameters θ,[0],[0]
"We then try to set θ, using SGD, to minimize a divergence D(p̂θ, q̂) that we will define below.",2.4 Choosing parameters θ,[0],[0]
"Estimating q̂ is straightforward: q̂(t | s) = cq(st)/cq(s), where cq(st) is the count of POS bigram st in the average3 sentence of u and cq(s) =∑
t′ cq(st ′).",2.4.1 Estimation of bigram models,[0],[0]
"We estimate p̂θ in the same way, where cp(st) denotes the expected count of st in a random POS sequence y ∼ pθ.",2.4.1 Estimation of bigram models,[0],[0]
"This is equivalent to choosing q̂, p̂θ to minimize the KL-divergences KL(q ||",2.4.1 Estimation of bigram models,[0],[0]
"q̂),KL(pθ || p̂θ).",2.4.1 Estimation of bigram models,[0],[0]
"It ensures that each model’s expected bigram counts match those in the POS sequences.
",2.4.1 Estimation of bigram models,[0],[0]
"However, these maximum-likelihood estimates might overfit on our finite data, u and B. We therefore smooth both models by first adding λ = 0.1 to all bigram counts cq(st) and cp(st).4",2.4.1 Estimation of bigram models,[0],[0]
We need a metric to evaluate θ.,2.4.2 Divergence of bigram models,[0],[0]
"If p and q are bigram language models over POS sequences y (sentences), their Kullback-Leibler divergence is
KL(p || q) def=",2.4.2 Divergence of bigram models,[0],[0]
"Ey∼p[log p(y)− log q(y)] (2) = ∑ s,t cp(st) (3)
· (log p(t | s)− log q(t | s))
where y ranges over POS sequences and st ranges over POS bigrams.",2.4.2 Divergence of bigram models,[0],[0]
"These include bigrams where s = BOS (“beginning of sequence”) or t = EOS (“end of sequence”), which are boundary tags that we take to surround y.
All quantities in equation (3) can be determined directly from the (expected) bigram counts given by cp and cq.",2.4.2 Divergence of bigram models,[0],[0]
"No other model estimation is needed.
",2.4.2 Divergence of bigram models,[0],[0]
A concern about equation (3) is that a single bigram st that is badly underrepresented in q may contribute an arbitrarily large term log p(t|s)q(t|s) .,2.4.2 Divergence of bigram models,[0],[0]
"To limit this contribution to at most log 1α , for some small α ∈ (0, 1), we define KLα(p || q) by a variant of equation (3) in which q(t | s) has been replaced by q̃(t | s) def= αp(t",2.4.2 Divergence of bigram models,[0],[0]
"| s) + (1− α)q(t | s).5
3A more familiar definition of cq would use the total count in u. Our definition, which yields the same bigram probabilities, is analogous to our definition of cp.",2.4.2 Divergence of bigram models,[0],[0]
"This cp is needed for KL(p || q) in (3), and cq symmetrically for KL(q || p).
",2.4.2 Divergence of bigram models,[0],[0]
"4Ideally one should tune λ to minimize the language model perplexity on held-out data (e.g., by cross-validation).
",2.4.2 Divergence of bigram models,[0],[0]
"5This is inspired by the α-skew divergence of Lee (1999,
Our final divergence metric D(p̂θ, q̂) defines D as a linear combination of exclusive and inclusive KLα divergences, which respectively emphasize pθ’s precision and recall at matching q’s bigrams:
D(p, q) =",2.4.2 Divergence of bigram models,[0],[0]
(1−β)·KLα1(p || q) Ey∼p[ |y| ] +β·KLα2(q ||,2.4.2 Divergence of bigram models,[0],[0]
"p) Ey∼q[ |y| ] (4) where β, α1, α2 are tuned by cross-validation to maximize the downstream parsing performance.",2.4.2 Divergence of bigram models,[0],[0]
"The division by average sentence length converts KL from nats per sentence to nats per word,6 so that the KL values have comparable scale even if B has much longer or shorter sentences than u.",2.4.2 Divergence of bigram models,[0],[0]
"We now present a polynomial-time algorithm for computing the expected bigram counts cp under pθ (or equivalently p̂θ), for use above.",3.1 Efficiently computing expected counts,[0],[0]
"This averages expected counts from each unordered tree x ∈ B. Algorithm 1 in the supplement gives pseudocode.
",3.1 Efficiently computing expected counts,[0],[0]
"The insight is that rather than sampling a single realization of x (as B′ does), we can use dynamic programming to sum efficiently over all of its exponentially many realizations.",3.1 Efficiently computing expected counts,[0],[0]
This gives an exact answer.,3.1 Efficiently computing expected counts,[0],[0]
"It algorithmically resembles tree-to-string machine translation, which likewise considers the possible reorderings of a source tree and incorporates a language model by similarly tracking their surface N -grams (Chiang, 2007, §5.3.2).
",3.1 Efficiently computing expected counts,[0],[0]
"For each node a of the tree x, let the POS string ya be the realization of the subtree rooted at a. Let ca(st) be the expected count of bigram st in ya, whose distribution is governed by equation (1).",3.1 Efficiently computing expected counts,[0],[0]
"We allow s = BOS or t = EOS as defined in §2.4.2.
",3.1 Efficiently computing expected counts,[0],[0]
The ca function can be represented as a sparse map from POS bigrams to reals.,3.1 Efficiently computing expected counts,[0],[0]
We compute ca at each node a of x in a bottom-up order.,3.1 Efficiently computing expected counts,[0],[0]
"The final step computes croot, giving the expected bigram counts in x’s realization y",3.1 Efficiently computing expected counts,[0],[0]
"(that is, cp in §2.4).
",3.1 Efficiently computing expected counts,[0],[0]
We find ca as follows.,3.1 Efficiently computing expected counts,[0],[0]
"Let n = na and recall from §2.2 that π(a) is an ordering of a1, . . .",3.1 Efficiently computing expected counts,[0],[0]
", an, where a1, . . .",3.1 Efficiently computing expected counts,[0],[0]
", an−1 are the child nodes of a, and an is a dummy node representing a’s head token.
2001).",3.1 Efficiently computing expected counts,[0],[0]
"Indeed, we may regard KLα(p || q) as the α-skew divergence between the unigram distributions p(· | s) and q(· | s), averaged over all s in proportion to cp(s).",3.1 Efficiently computing expected counts,[0],[0]
"In principle, we could have used the α-skew divergence between the distributions p(·) and q(·) over POS sequences y, but computing that would have required a sampling-based approximation (§7).
6Recall that the units of negated log-probability are called bits for log base 2, but nats for log base e.
Also, let a0 and an+1 be dummy nodes that always appear at the start and end of any ordering.
",3.1 Efficiently computing expected counts,[0],[0]
For all 0 ≤,3.1 Efficiently computing expected counts,[0],[0]
i ≤ n,3.1 Efficiently computing expected counts,[0],[0]
and 1 ≤ j ≤ n,3.1 Efficiently computing expected counts,[0],[0]
"+ 1, let pa(i, j) denote the expected count of the aiaj node bigram—the probability that π(a) places node ai immediately before node aj .",3.1 Efficiently computing expected counts,[0],[0]
"These node bigram probabilities can be obtained by enumerating all possible orderings π, a matter we return to below.
",3.1 Efficiently computing expected counts,[0],[0]
"It is now easy to compute ca:
ca(st) =",3.1 Efficiently computing expected counts,[0],[0]
"c within a (st) + c between a (st) (5)
",3.1 Efficiently computing expected counts,[0],[0]
"cwithina (st) =
{∑n i=1 cai(st)",3.1 Efficiently computing expected counts,[0],[0]
"if s 6= BOS, t 6=",3.1 Efficiently computing expected counts,[0],[0]
"EOS
0 otherwise
cacrossa (st) = n∑ i=0 n+1∑ j=1 pa(i, j)cai(s EOS)caj (BOS t)
",3.1 Efficiently computing expected counts,[0],[0]
"That is, ca inherits all non-boundary bigrams st that fall within its child constituents (via cwithina ).",3.1 Efficiently computing expected counts,[0],[0]
"It also counts bigrams st that cross the boundary between consecutive nodes (via cacrossa ), where nodes ai and aj are consecutive with probability pa(i, j).
",3.1 Efficiently computing expected counts,[0],[0]
"When computing ca via (5), we will have already computed ca1 , . . .",3.1 Efficiently computing expected counts,[0],[0]
", can−1 bottom-up.",3.1 Efficiently computing expected counts,[0],[0]
"As for the dummy nodes, an is realized by the length-1 string hwhere h is the head token of node a, while a0 and an+1 are each realized by the empty string.",3.1 Efficiently computing expected counts,[0],[0]
"Thus, can simply assigns count 1 to the bigrams BOS h and h EOS, and ca0 and can+1 each assign expected count 1 to BOS EOS.",3.1 Efficiently computing expected counts,[0],[0]
"(Notice that thus, cacrossa (st) counts ya’s boundary bigrams—the bigrams stwhere s = BOS or t = EOS—when i = 0 or j = n+ 1 respectively.)",3.1 Efficiently computing expected counts,[0],[0]
"The main challenge above is computing the node bigram probabilities pa(i, j).",3.2 Efficient enumeration over permutations,[0],[0]
"These are marginals of p(π | a) as defined by (1), which unfortunately is intractable to marginalize: there is no better way than enumerating all n! permutations.
",3.2 Efficient enumeration over permutations,[0],[0]
"That said, there is a particularly efficient way to enumerate the permutations.",3.2 Efficient enumeration over permutations,[0],[0]
"The SteinhausJohnson-Trotter (SJT) algorithm (Sedgewick, 1977) does so in O(1) time per permutation, obtaining each permutation by applying a single swap to the previous one.",3.2 Efficient enumeration over permutations,[0],[0]
Only the features that are affected by this swap need to be recomputed.,3.2 Efficient enumeration over permutations,[0],[0]
"For our features (Appendix A), this cuts the runtime per permutation from O(n2) to O(n).
",3.2 Efficient enumeration over permutations,[0],[0]
"Furthermore, the single swap of adjacent nodes only changes 3 bigrams (possibly including boundary bigrams).",3.2 Efficient enumeration over permutations,[0],[0]
"As a result, it is possible to
obtain the marginal probabilities with O(1) additional work per permutation.",3.2 Efficient enumeration over permutations,[0],[0]
"When a node bigram is destroyed, we increment its marginal probability by the total probability of permutations encountered since the node bigram was last created.",3.2 Efficient enumeration over permutations,[0],[0]
This can be found as a difference of partial sums.,3.2 Efficient enumeration over permutations,[0],[0]
"The final partial sum is the normalizing constant Z(a), which can be applied at the end.",3.2 Efficient enumeration over permutations,[0],[0]
"Pseudocode is given in supplementary material as Algorithm 2.
",3.2 Efficient enumeration over permutations,[0],[0]
"When we train the parameters θ (§2.4), we must back-propagate through the whole computation of equation (4), which depends on tag bigram counts ca(st), which depend via (5) on expected node bigram counts pa(i, j), which depend via Algorithm 2 on the permutation probabilities p(π | a), which depend via (1) on the feature weights θ.",3.2 Efficient enumeration over permutations,[0],[0]
"As a further speedup, we only train on trees with number of words < 40 and maxa na ≤ 5, so na!",4.1 Pruning high-degree trees,[0],[0]
≤,4.1 Pruning high-degree trees,[0],[0]
120.7 We then produce the synthetic treebank B′,4.1 Pruning high-degree trees,[0],[0]
(§2.3) by drawing a single realization of each tree in B for which maxa na ≤ 7.,4.1 Pruning high-degree trees,[0],[0]
"This requires sampling from up to 7! = 5040 candidates per node, again using SJT.8
That is, in this paper we run exact algorithms (§3), but only on a subset of B. The subset is not necessarily representative.",4.1 Pruning high-degree trees,[0],[0]
"An improvement would use importance sampling, with a proposal distribution that samples the slower trees less often during SGD but upweights them to compensate.",4.1 Pruning high-degree trees,[0],[0]
"§7 suggests a future strategy that would run on all trees in B via approximate, sampling-based algorithms.",4.1 Pruning high-degree trees,[0],[0]
The exact methods would remain useful for calibrating the approximation quality.,4.1 Pruning high-degree trees,[0],[0]
"To minimize (4), we use the Adam variant of SGD (Kingma and Ba, 2014), with learning rate 0.01 chosen by cross-validation (§5.1).
",4.2 Minibatch estimation of cp,[0],[0]
SGD requires a stochastic estimate of the gradient of the training objective.,4.2 Minibatch estimation of cp,[0],[0]
"Ordinarily this is done by replacing an expectation over the entire training set with an expectation over a minibatch.
",4.2 Minibatch estimation of cp,[0],[0]
"7We found that this threshold worked much better than ≤ 4 and about as well as the much slower ≤ 6.
8This pruning heuristic retains 36.1% of the trees (averaging over the 20 development treebanks (§5.1)) for training, and 66.6% for actual realization.",4.2 Minibatch estimation of cp,[0],[0]
"The latter restriction follows Wang and Eisner (2016, §4.2): they too discarded trees with nodes having na ≥ 8.
",4.2 Minibatch estimation of cp,[0],[0]
Equation (2) with p = p̂θ is indeed an expectation over sentences of B. It can be stochastically estimated as (3) where cp gives the expected bigram counts averaged over only the sentences in a minibatch of B. These are found using §3’s algorithms with the current θ.,4.2 Minibatch estimation of cp,[0],[0]
"Unfortunately, the term log p(t | s) depends on bigram counts that should be derived from the entire corpus B in the same way.",4.2 Minibatch estimation of cp,[0],[0]
Our solution is to simply reuse the minibatch estimate of cp for the latter counts.,4.2 Minibatch estimation of cp,[0],[0]
"We use a large minibatch of 500 sentences from B so that this drop-in estimate does not introduce too much bias into the stochastic gradient: after all, we only need to estimate bigram statistics on 17 POS types.9
By contrast, the cq values that are used for the expectation in the second term of (4) and in log q(t | s) do not change during optimization, so we simply compute them once from all of u.",4.2 Minibatch estimation of cp,[0],[0]
"Unfortunately the objective (4) is not convex, so the optimizer is sensitive to initialization (see §5.3 below for empirical discussion).",4.3 Informed initialization,[0],[0]
Initializing θ = 0,4.3 Informed initialization,[0],[0]
(so that p(π | a) is uniform),4.3 Informed initialization,[0],[0]
gave poor results in pilot experiments.,4.3 Informed initialization,[0],[0]
"Instead, we initially choose θ to be the realization parameters of the source language, as estimated from the source",4.3 Informed initialization,[0],[0]
treebank B.,4.3 Informed initialization,[0],[0]
"This is at least a linguistically realistic θ, although it may not be close to the target language.10
For this initial estimation, we follow Wang and Eisner (2016) and perform supervised training on B of the log-linear realization model (1), by maximizing the conditional log-likelihood of B, namely ∑ (x,t)∈B log pθ(t | x), where (x, t) are an unordered tree and its observed ordering in B. This initial objective is convex.11",4.3 Informed initialization,[0],[0]
We performed a large-scale experiment requiring hundreds of thousands of CPU-hours.,5 Experiments,[0],[0]
"To our knowledge, this is the largest study of parsing transfer yet attempted.
",5 Experiments,[0],[0]
"9We also used the minibatch to estimate the average sentence length Ey∼p[ |y| ] in (4), although here we could have simply used all of B since this value does not change.
",5 Experiments,[0],[0]
"10As an improvement, one could also try initial realization parameters for B that are estimated from treebanks of other languages.",5 Experiments,[0],[0]
"Concretely, the optimizer could start by selecting a “galactic” treebank from Wang and Eisner (2016) that is already close to the target language, according to (4), and try to make it even closer.",5 Experiments,[0],[0]
"We leave this to future work.
",5 Experiments,[0],[0]
"11Unfortunately, we did not regularize it, which probably resulted in initializing some parameters too close to ±∞ for the optimizer to change them meaningfully.",5 Experiments,[0],[0]
"As our main dataset, we use Universal Dependencies version 1.2 (Nivre et al., 2015)—a set of 37 dependency treebanks for 33 languages, with a unified POS-tag set and relation label set.
",5.1 Data and setup,[0],[0]
Our evaluation metric was unnormalized attachment score (UAS) when parsing a target treebank with a parser trained on a (possibly permuted) source treebank.,5.1 Data and setup,[0],[0]
"For both evaluation and training, we used only the training portion of each treebank.
",5.1 Data and setup,[0],[0]
"Our parser was Yara (Rasooli and Tetreault, 2015), a fast and accurate transition-based dependency parser that can be rapidly retrained.",5.1 Data and setup,[0],[0]
We modified Yara to ignore the input words and use only the input gold POS tags (see §1.3).,5.1 Data and setup,[0],[0]
"To train the Yara parser on a (possibly permuted) source treebank, we first train on 80% of the trees and use the remaining 20% to tune Yara’s hyperparameters.",5.1 Data and setup,[0],[0]
"We then retrain Yara on 100% of the source trees and evaluate it on the target treebank.
Similar to Wang and Eisner (2017), we use 20 treebanks (18 distinct languages) as development data, and hold out the remaining 17 treebanks for the final evaluation.",5.1 Data and setup,[0],[0]
"We chose the hyperparameters (α1, α2, β) of (4) to maximize the target-language UAS, averaged over all 376 transfer experiments where the source and target treebanks were development treebanks of different languages.12 (See Appendix C for details.)
",5.1 Data and setup,[0],[0]
The next few sections perform some exploratory analysis on these 376 experiments.,5.1 Data and setup,[0],[0]
"Then, for the final test in §5.4, we will evaluate UAS on all 337 transfer experiments where the source is a development treebank and the target is a test treebank of a different language.13",5.1 Data and setup,[0],[0]
We have assumed that a smaller divergence between source and target treebanks results in better transfer parsing accuracy.,5.2 Exploratory analysis,[0],[0]
"Figure 1 shows that these quantities are indeed correlated, both for the original source treebanks and for their “made to order” permuted versions.
",5.2 Exploratory analysis,[0],[0]
"12We have 19*20=380 pairs in total, minus the four excluded pairs (grc, grc proiel), (grc proiel, grc), (la proiel, la itt) and (la itt, la proiel).",5.2 Exploratory analysis,[0],[0]
"Unlike Wang and Eisner (2017), we exclude duplicated languages in development and testing.
",5.2 Exploratory analysis,[0],[0]
"13Specifically, there are 3 duplicated sets: {grc, grc proiel}, {la, la proiel, la itt}, and {fi, fi ftb}.",5.2 Exploratory analysis,[0],[0]
"Whenever one treebank is used as the target language, we exclude the other treebanks in the same set.
",5.2 Exploratory analysis,[0],[0]
"15According to the family (and sub-family) information at http://universaldependencies.org.
",5.2 Exploratory analysis,[0],[0]
"Thus, we hope that the optimizer will find a systematic permutation that reduces the divergence.",5.2 Exploratory analysis,[0],[0]
Does it?,5.2 Exploratory analysis,[0],[0]
"Yes: Figures 5 and 6 in the supplementary material show that the optimizer almost always manages to reduce the objective on training data, as expected.
",5.2 Exploratory analysis,[0],[0]
"One concern is that our divergence metric might misguide us into producing dysfunctional languages whose trees cannot be easily recovered from their surface strings, i.e., they have no good parser.",5.2 Exploratory analysis,[0],[0]
"In such a language, the word order might be extremely free (e.g., θ = 0), or common constructions might be syntactically ambiguous.",5.2 Exploratory analysis,[0],[0]
"Fortunately, Appendix D shows that our synthetic languages appear natural with respect to their their parsability.
",5.2 Exploratory analysis,[0],[0]
The above findings are promising.,5.2 Exploratory analysis,[0],[0]
So does permuting the source language in fact result in better transfer parsing of the target language?,5.2 Exploratory analysis,[0],[0]
"We experiment on the 376 development pairs.
",5.2 Exploratory analysis,[0],[0]
"The solid lines in Figure 2 show our improvements on the dev data, with a simpler scatterplot given by in Figure 7 in the supplementary material.",5.2 Exploratory analysis,[0],[0]
The upshot is that the synthetic source treebanks yield a transfer UAS of 52.92 on average.,5.2 Exploratory analysis,[0],[0]
This is not yet a result on held-out test data: recall that 52.92 was the best transfer UAS achieved by any hyperparameter setting.,5.2 Exploratory analysis,[0],[0]
"That said, it is 1.00 points better than transferring from the original source treebanks, a significant difference (paired permutation test by language pair, p < 0.01).
",5.2 Exploratory analysis,[0],[0]
"Figure 2 shows that this average improvement is mainly due to the many cases where the source and target languages come from different families.
",5.2 Exploratory analysis,[0],[0]
Permutation tends to improve source languages that were doing badly to start with.,5.2 Exploratory analysis,[0],[0]
"However, it tends to hurt a source language that is already in the target language family.
",5.2 Exploratory analysis,[0],[0]
A hypothetical experiment shows that permuting the source does have good potential to help (or at least not hurt) in both cases.,5.2 Exploratory analysis,[0],[0]
"The dashed lines in Figure 2—and the scatterplot in Figure 8— show the potential of the method, by showing the improvement we would get from permuting each source treebank using an “oracle” realization policy—the supervised realization parameters θ that are estimated from the actual target treebank.",5.2 Exploratory analysis,[0],[0]
"The usefulness of this oracle-permuted source varies depending on the source language, but it is usually much better than the automaticallypermuted version of the same source.
",5.2 Exploratory analysis,[0],[0]
This shows that large improvements would be possible if we could only find the best permutation policy allowed by our model family.,5.2 Exploratory analysis,[0],[0]
"The question for future work is whether such gains can be achieved by a more sensitive permutation model than (1), a better divergence objective than (4), or a better search algorithm than §4.2.",5.2 Exploratory analysis,[0],[0]
"Identifying the best available source treebank, or the best mixture of all source treebanks, would also help greatly.",5.2 Exploratory analysis,[0],[0]
Figure 2 makes clear that performance of the synthetic source treebanks is strongly correlated with that of their original versions.,5.3 Sensitivity to initializer,[0],[0]
Most points in Figure 7 lie near the diagonal (Kendall’s τ = 0.85).,5.3 Sensitivity to initializer,[0],[0]
"Even with oracle permutation in Figure 8, the correlation remains strong (τ = 0.59), suggesting that the choice of source treebank is important even beyond its effect on search initialization.
",5.3 Sensitivity to initializer,[0],[0]
"We suspected that when “made to order” source treebanks (more than the oracle versions) have performance close to their original versions, this is in part because the optimizer can get stuck near the initializer (§4.3).",5.3 Sensitivity to initializer,[0],[0]
"To examine this, we experimented with random restarts, as follows.",5.3 Sensitivity to initializer,[0],[0]
"In addition to informed initialization (§4.3), we optimized from 5 other starting points θ ∼ N (0, I).",5.3 Sensitivity to initializer,[0],[0]
"From these 6 runs, we selected the final parameters that achieved the best divergence (4).",5.3 Sensitivity to initializer,[0],[0]
"As shown by
Figure 9 in the supplement, greater gains appear to be possible with more aggressive search methods of this sort, which we leave to future work.",5.3 Sensitivity to initializer,[0],[0]
"We could also try non-random restarts based on the realization parameters of other languages, as suggested in footnote 10.",5.3 Sensitivity to initializer,[0],[0]
"For our final evaluation (§5.1), we use the same hyperparameters (Appendix C) and report on single-source transfer to the 17 held-out treebanks.
",5.4 Final evaluation on the test languages,[0],[0]
The development results hold up in Figure 3.,5.4 Final evaluation on the test languages,[0],[0]
"Using the synthetic languages yields 50.36 UAS on average—1.75 points over the baseline, which is significant (paired permutation test, p < 0.01).
",5.4 Final evaluation on the test languages,[0],[0]
"In the supplementary material (Appendix E), we include some auxiliary experiments on multisource transfer.",5.4 Final evaluation on the test languages,[0],[0]
"Unsupervised parsing has remained challenging for decades (Mareček, 2016).",6.1 Unsupervised parsing,[0],[0]
"Classical grammar induction approaches (Lari and Young, 1990; Carroll and Charniak, 1992; Klein and Manning, 2004; Headden III et al., 2009; Naseem et al., 2010) estimate a generative grammar to explain the sentences, for example by the ExpectationMaximization (EM) algorithm, and then use it to parse.",6.1 Unsupervised parsing,[0],[0]
Some such approaches try to improve the grammar model.,6.1 Unsupervised parsing,[0],[0]
"For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012).",6.1 Unsupervised parsing,[0],[0]
"Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareček and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013).
",6.1 Unsupervised parsing,[0],[0]
"The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences.",6.1 Unsupervised parsing,[0],[0]
McDonald et al. (2011) showed a significant improvement over grammar induction by simply using the delexicalized parser trained on other language(s).,6.1 Unsupervised parsing,[0],[0]
"Subsequent improvements have come from re-weighting source languages (Søgaard, 2011b; Rosa and Žabokrtský, 2015a,b; Wang and Eisner, 2016), adapting the model to the target language using WALS (Dryer and Haspelmath, 2013) features (Naseem et al., 2012; Täckström",6.1 Unsupervised parsing,[0],[0]
"et al., 2013;
Zhang and Barzilay, 2015; Ammar et al., 2016), and improving the lexical representations via multilingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016) and synthetic data generation (§6.2).",6.1 Unsupervised parsing,[0],[0]
Our novel proposal ties into the recent interest in data augmentation in supervised machine learning.,6.2 Synthetic data generation,[0],[0]
"In unsupervised parsing, the most widely
adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation.",6.2 Synthetic data generation,[0],[0]
"Of course, this requires bilingual corpora as an additional resource.",6.2 Synthetic data generation,[0],[0]
"Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014).",6.2 Synthetic data generation,[0],[0]
"Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agić et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016).
",6.2 Synthetic data generation,[0],[0]
"On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages.",6.2 Synthetic data generation,[0],[0]
"They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages.",6.2 Synthetic data generation,[0],[0]
"Their idea was that with a large set of synthetic languages, they could use them as supervised examples to train an unsupervised structure discovery system that could analyze any new language.",6.2 Synthetic data generation,[0],[0]
"Systems built with this dataset were competitive in single-source parser transfer (Wang and Eisner, 2016), typology
prediction (Wang and Eisner, 2017), and parsing unknown languages (Wang and Eisner, 2018).
",6.2 Synthetic data generation,[0],[0]
Our work in this paper differs in that our synthetic treebanks are “made to order.”,6.2 Synthetic data generation,[0],[0]
"Rather than combine aspects of different treebanks and hope to get at least one combination that is close to the target language, we “combine” the source treebank with a POS corpus of the target language, which guides our customized permutation of the source.
",6.2 Synthetic data generation,[0],[0]
"Beyond unsupervised parsing, synthetic data has been used for several other tasks.",6.2 Synthetic data generation,[0],[0]
"In NLP, it has been used for complex tasks such as questionanswering (QA) (Serban et al., 2016) and machine reading comprehension (Weston et al., 2016; Hermann et al., 2015; Rajpurkar et al., 2016), where highly expressive neural models are used and not enough real data is available to train them.",6.2 Synthetic data generation,[0],[0]
"In the playground of supervised parsing, Gulordava and Merlo (2016) conduct a controlled study on the parsibility of languages by generating treebanks with short dependency length and low variability of word order.",6.2 Synthetic data generation,[0],[0]
We have shown how cross-lingual transfer parsing can be improved by permuting the source treebank to better resemble the target language on the surface (in its distribution of gold POS bigrams).,7 Conclusion & Future Work,[0],[0]
The code is available at https://github. com/wddabc/ordersynthetic.,7 Conclusion & Future Work,[0],[0]
"Our work is grounded in the notion that by trying to explain the POS bigram counts in a target corpus, we can discover a stochastic realization policy for the target language, which correctly “translates” the source trees into appropriate target trees.
",7 Conclusion & Future Work,[0],[0]
"We formulated an objective for evaluating such a policy, based on KL-divergence between bigram models.",7 Conclusion & Future Work,[0],[0]
"We showed that the objective could be computed efficiently by dynamic programming, thanks to the limitation to bigram statistics.
",7 Conclusion & Future Work,[0],[0]
"Experimenting on the Universal Dependencies treebanks v1.2, we showed that the synthetic treebanks were—on average—modestly but significantly better than the corresponding real treebanks for single-source transfer (and in Appendix E, on multi-source transfer).
",7 Conclusion & Future Work,[0],[0]
"On the downside, Figure 7 shows that with our current method, permuting the source language to be more like the target language is helpful (on average) only when the source language is from a different language family.",7 Conclusion & Future Work,[0],[0]
"This contrast would be
even more striking if we had a better optimizer:
Figure 9 shows that SGD’s initialization bias limits permutation’s benefit for cross-family training, as well as its harm for within-family training.
",7 Conclusion & Future Work,[0],[0]
Several opportunities for future work have already been mentioned throughout the paper.,7 Conclusion & Future Work,[0],[0]
"We are also interested in experimenting with richer families of permutation distributions, as well as “conservative” distributions that tend to prefer the original source order.",7 Conclusion & Future Work,[0.9523176074770734],"['Notice that terms that are more related to the query have higher probabilities, although common words such as ”the” are also selected.']"
"We could use entropy regularization (Grandvalet and Bengio, 2005) to encourage more “deterministic” patterns of realization in the synthetic languages.
",7 Conclusion & Future Work,[0],[0]
"We would also like to consider more sensitive divergence measures that go beyond bigrams, for example using recurrent neural network language models (RNNLMs) for q̂ and p̂θ.",7 Conclusion & Future Work,[0],[0]
"This means abandoning our exact dynamic programming methods; we would also like to abandon exact exhaustive enumeration in order to drop §4.1’s bounds on n. Fortunately, there exist powerful MCMC methods (Eisner and Tromble, 2006) that can sample from interesting distributions over the space of n!",7 Conclusion & Future Work,[0],[0]
"permutations, even for large n. Thus, we could approximately sample from pθ by drawing permuted versions of each tree in B.
Given this change, a very interesting direction would be to graduate from POS language models to word language models, using cross-lingual unsupervised word embeddings (Ruder et al., 2017).",7 Conclusion & Future Work,[0],[0]
This would eliminate the need for the gold POS tags that we unrealistically assumed in this paper (which are typically unavailable for a low-resource target language).,7 Conclusion & Future Work,[0],[0]
"Furthermore, it would enable us to harness richer lexical information beyond the 17 UD POS tags.",7 Conclusion & Future Work,[0],[0]
"After all, even a (gold) POS corpus might not be sufficient to determine the word order of the target language: “NOUN VERB NOUN” could be either subject-verb-object or object-verbsubject.",7 Conclusion & Future Work,[0],[0]
"However, “water drink boy” is presumably object-verb-subject.",7 Conclusion & Future Work,[0],[0]
"Thus, using crosslingual embeddings, we would try to realize the unordered source trees so that their word strings, with few edits, can achieve high probability under a neural language model of the target.",7 Conclusion & Future Work,[0],[0]
This work was supported by National Science Foundation Grants 1423276 & 1718846.,Acknowledgements,[0],[0]
"We are grateful to the state of Maryland for the Maryland Advanced Research Computing Center, a crucial resource.",Acknowledgements,[0],[0]
"We thank Shijie Wu and Adithya Renduchintala for early discussion, Argo lab members for further discussion, and the 3 reviewers for quality comments.",Acknowledgements,[0],[0]
"To approximately parse an unfamiliar language, it helps to have a treebank of a similar language.",abstractText,[0],[0]
But what if the closest available treebank still has the wrong word order?,abstractText,[0],[0]
We show how to (stochastically) permute the constituents of an existing dependency treebank so that its surface part-of-speech statistics approximately match those of the target language.,abstractText,[0],[0]
The parameters of the permutation model can be evaluated for quality by dynamic programming and tuned by gradient descent (up to a local optimum).,abstractText,[0],[0]
This optimization procedure yields trees for a new artificial language that resembles the target language.,abstractText,[0],[0]
We show that delexicalized parsers for the target language can be successfully trained using such “made to order” artificial languages.,abstractText,[0],[0]
Synthetic Data Made to Order: The Case of Parsing,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752–757 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
752",text,[0],[0]
"Story comprehension has been one of the longestrunning ambitions in artificial intelligence (Dijk, 1980; Charniak, 1972).",1 Introduction,[0],[0]
One of the challenges in expanding the field had been the lack of a solid evaluation framework and datasets on which comprehension models can be trained and tested.,1 Introduction,[0],[0]
"Mostafazadeh et al. (2016) introduced the Story Cloze Test (SCT) evaluation framework to address
*",1 Introduction,[0],[0]
"This work was performed at University of Rochester.
this issue.",1 Introduction,[0],[0]
"This test evaluates a story comprehension system where the system is given a foursentence short story as the ‘context’ and two alternative endings and to the story, labeled ‘right ending’ and ’wrong ending.’",1 Introduction,[0],[0]
"Then, the system’s task is to choose the right ending.",1 Introduction,[0],[0]
"In order to support this task, Mostafazadeh et al. also provide the ROC Stories dataset, which is a collection of crowd-sourced complete five sentence stories through Amazon Mechanical Turk (MTurk).",1 Introduction,[0],[0]
"Each story follows a character through a fairly simple series of events to a conclusion.
",1 Introduction,[0],[0]
"Several shallow and neural models, including the state-of-the-art script learning approaches, were presented as baselines (Mostafazadeh et al., 2016) for tackling the task, where they show that all their models perform only slightly better than a random baseline suggesting that richer models are required for tackling this task.",1 Introduction,[0],[0]
"A variety of new systems were proposed (Mihaylov and Frank, 2017; Schenk and Chiarcos, 2017; Schwartz et al., 2017b; Roemmele et al., 2017) as a part of the first shared task on SCT at LSDSem’17 workshop (Mostafazadeh et al., 2017).",1 Introduction,[0],[0]
"Surprisingly, one of the models made a staggering improvement of 15% to the accuracy, partially due to using stylistic features isolated in the ending choices (Schwartz et al., 2017b), discarding the narrative context.",1 Introduction,[0],[0]
"Clearly, this success does not seem to reflect the intent of the original task, where the systems should leverage narrative understanding as opposed to the statistical biases in the data.",1 Introduction,[0],[0]
"In this paper, we study the effect of such biases between the ending choices and present a new scheme to reduce such stylistic artifacts.
",1 Introduction,[0],[0]
"The contribution of this paper is threefold: (1) we provide an extensive analysis of the SCT dataset to shed some light on the ending data characteristics (Section 3) (2) we develop a new strong classifier for tackling the SCT that uses a variety
of features inspired by all the top-performing systems on the task (Section 4) (3) we design a new crowd-sourcing scheme that yields a new SCT dataset; we benchmark various models on the new dataset (Section 5).",1 Introduction,[0],[0]
"The results show that the topperforming SCT system on the the leaderboard1 (Chaturvedi et al., 2017) fails to keep up the performance on our new dataset.",1 Introduction,[0],[0]
We discuss the implications of this experiment to the greater research community in terms of data collection and benchmarking practices in Section 6.,1 Introduction,[0],[0]
All the code and datasets for this paper will be released to the public.,1 Introduction,[0],[0]
We hope that the availability of the new evaluation set can further support the continued research on story understanding.,1 Introduction,[0],[0]
"This paper mainly extends the work on creating the Story Cloze Test set (Mostafazadeh et al., 2016), hereinafter SCT-v1.0.",2 Related Work,[0],[0]
"The SCT-v1.0 dataset was created as follows: full five-sentence stories from the ROC Stories corpus were sampled, then, the initial four sentences were shown to a set of MTurk2 crowd workers who were prompted to author ‘right’ and ‘wrong’ endings.",2 Related Work,[0],[0]
"Mostafazadeh et al. (Mostafazadeh et al., 2016) give special care to make sure there were no boundary cases for ‘right’ and ‘wrong’ endings by implementing extra rounds of data filtering.",2 Related Work,[0],[0]
"The resulting SCT-v1.0 dataset had a validation (hereinafter, SCT-v1.0 Val) and a test set (SCT-v1.0 test), each with 1,871 cases.",2 Related Work,[0],[0]
Table 1 shows two example story cloze test cases from SCT-v1.0 corpus.,2 Related Work,[0],[0]
"As for positive training data, they had provided a collection of 100K five sentence stories.",2 Related Work,[0],[0]
"Human performance is reported to be 100% on SCT-v1.0.
",2 Related Work,[0],[0]
"Mostafazadeh et al. (2016) provide a variety of baseline models for SCT-v1.0, with the best model performing with an accuracy of 59%.",2 Related Work,[0],[0]
"The first
1As of 15th February 2018.",2 Related Work,[0],[0]
"2http://mturk.com
shared task on SCT-v1.0 was conducted at the LSDSem’17 workshop (Mostafazadeh et al., 2017), where most of the models performed with 60- 70% accuracy.",2 Related Work,[0],[0]
"One of the top-performing models, msap (Schwartz et al., 2017b,a), built a classifier using linguistic features that have been previously useful in authorship style detection, using only the ending sentences.",2 Related Work,[0],[0]
"They used stylistic features such as sentence length, word, and character level n-grams for each ending (fully discarding the context), achieving an accuracy of 72%.",2 Related Work,[0],[0]
"In conjunction with their work, Cai et al., (Cai et al., 2017) reported similar observations separately, exposing that features such as sentiment, negation, and length are different between the right and wrong endings.",2 Related Work,[0],[0]
"The best model on SCT-v1.0 to this date is cogcomp, which is a linear model that uses event sequences, sentiment trajectory, and topical consistency as features, and performs with an accuracy of 77.6%.
",2 Related Work,[0],[0]
This paper takes all their analysis further and introduces a model aggregating all the pinpointed features to shed more light into the stylistic biases isolated in SCT-v1.0 endings.,2 Related Work,[0],[0]
"Despite all the efforts made in the original SCT paper, there was never an extensive analysis of the features isolated in the endings of the stories.",3 Stylistic Feature Analysis,[0],[0]
"We explored the differences among stylistic features such as word-token count, sentiment, and the sentence complexity between the endings, to determine a composite score for identifying sources of bias.",3 Stylistic Feature Analysis,[0],[0]
"For determining the sentiment, we used Stanford CoreNLP",3 Stylistic Feature Analysis,[0],[0]
"(Manning et al., 2014) and the VADER sentiment analyzer (Hutto and Gilbert, 2014).",3 Stylistic Feature Analysis,[0],[0]
"For measuring the syntactic complexity, we used Yngve and Frazier metrics (Yngve, 1960; Frazier, 1985).",3 Stylistic Feature Analysis,[0],[0]
Table 2 compares these statistics between the right and wrong endings in the SCTv1.0 dataset.,3 Stylistic Feature Analysis,[0],[0]
"The feature distribution plots can be found in the supplementary material.
",3 Stylistic Feature Analysis,[0],[0]
"Furthermore, we conducted an extensive ngram analysis, using word tokens, characters, partof-speech, and token-POS (similar to Schwartz et al. (Schwartz et al., 2017b))",3 Stylistic Feature Analysis,[0],[0]
as features.,3 Stylistic Feature Analysis,[0],[0]
"We see char-grams such as “sn’t” and “not” appear more commonly in the ‘wrong endings’, suggesting heavy negation.",3 Stylistic Feature Analysis,[0],[0]
"In ‘right endings’, pronouns are used more frequently versus proper nouns used in ‘wrong endings’.",3 Stylistic Feature Analysis,[0],[0]
"Artifacts such as ‘pizza’ are common in ’wrong endings,’ which could suggest that for a given topic, the authors may replace an object in a right ending with a wrong one and quickly think up a common item such as pizza to create a ‘wrong’ one.",3 Stylistic Feature Analysis,[0],[0]
"An extensive analysis of these features, including the n-gram analysis, can be found in the supplementary material.",3 Stylistic Feature Analysis,[0],[0]
"Following the analysis above, we developed a Story Cloze model, hereinafter EndingReg, that only uses the ending features while disregarding the story context for choosing the right ending.",4 Model,[0],[0]
We expanded each Story Cloze Test case’s ending options into a set of two single sentences.,4 Model,[0],[0]
"Then, for each sentence, we created the following features:
1.",4 Model,[0],[0]
Number of tokens 2.,4 Model,[0],[0]
VADER composite sentiment score 3.,4 Model,[0],[0]
Yngve complexity score 4.,4 Model,[0],[0]
Token-POS n-grams 5.,4 Model,[0],[0]
POS n,4 Model,[0],[0]
-grams 6.,4 Model,[0],[0]
"Four length character-grams
All n-gram features needed to appear at least five times throughout the dataset.",4 Model,[0],[0]
The features were collected for each five-sentence story and then fed into a logistic regression classifier.,4 Model,[0],[0]
"As an initial experiment, we trained this model using the SCTv1.0 validation set and tested on the SCT-v1.0 test set.",4 Model,[0],[0]
"An L2 regularization penalty was used to enforce a Gaussian prior on the feature-space, where a grid search was conducted for hyper-parameter tuning.",4 Model,[0],[0]
This model achieves an accuracy of 71.5% on the SCT-v1.0 dataset which is on par with the highest score achieved by any model using only the endings.,4 Model,[0],[0]
"Table 3 shows the accuracies ob-
tained by models using only those particular features.",4 Model,[0],[0]
"We achieve minimal but sometimes important classification using token count, VADER, and Yngve in combination alone, better classification using POS or char-grams alone, and best classification using n-grams alone.",4 Model,[0],[0]
By combining all of them we achieve the overall best results.,4 Model,[0],[0]
"Based on the findings above, a new test set for the SCT was deemed necessary.",5 Data Collection,[0],[0]
"The premise of predicting an ending to a short story, as opposed to predicting say a middle sentence, enables a more systematic evaluation where human can agree on the cases 100%.",5 Data Collection,[0],[0]
"Hence, our goal was to come up with a data collection scheme that overcomes the data collection biases, while keeping the original evaluation format.",5 Data Collection,[0],[0]
"As the data analysis revealed, the token count, sentiment, and the complexity are not as important features for classification as the ending n-grams are.",5 Data Collection,[0],[0]
We set the following goals for sourcing the new ‘right’ and ‘wrong’ endings.,5 Data Collection,[0],[0]
"They both should:
1.",5 Data Collection,[0],[0]
Contain a similar number of tokens 2.,5 Data Collection,[0],[0]
"Have similar distributions of token n-grams
and char-grams 3.",5 Data Collection,[0],[0]
"Occur as standalone events with the same
likelihood to occur, with topical, sentimental, or emotion consistencies when applicable.
",5 Data Collection,[0],[0]
"First, we crowdsourced 5,000 new five-sentence stories through Amazon Mechanical Turk.",5 Data Collection,[0],[0]
We prompted the users in the same manner described in Mostafazadeh et al. (2016).,5 Data Collection,[0],[0]
"In order to source new ‘wrong’ endings, we tried two different methods.",5 Data Collection,[0],[0]
"In Method #1, we kept the original ending sourcing format of Mostafazadeh et al., but imposed some further restrictions.",5 Data Collection,[0],[0]
"This was done
by taking the first four sentences of the newly collected stories and asking an MTurker to write a ‘right’ and ‘wrong’ ending for each.",5 Data Collection,[0],[0]
"The new restrictions were: ‘Each sentence should stay within the same subject area of the story,’ and ‘The number of words in the Right and Wrong sentences should not differ by more than 2 words,’ and ‘When possible, the Right and Wrong sentences should try to keep a similar tone/sentiment as one another.’",5 Data Collection,[0],[0]
"The motivation behind this technique was to reduce the statistical differences by asking the user to be mindful of considerations.
",5 Data Collection,[0],[0]
"In Method #2, we took the five sentences stories and prompted a second set of MTurk workers to modify the fifth sentence in order to make a resulting five-sentence story non-sensible.",5 Data Collection,[0],[0]
"Here, the prompt instructs the workers to make sure the new ‘wrong ending’ sentence makes sense standalone, that it does not differ in the number of words from the original sentence by more than three words, and that the changes cannot be as simple as e.g., putting the word ‘not’ in front of a description or a verb.",5 Data Collection,[0],[0]
"As a result, the workers had much less flexibility for changing the underlying linguistic structures which can help tackle the authorship style differences between the ‘right’ and ‘wrong’ endings.
",5 Data Collection,[0],[0]
"The results in Table 4, which show classification accuracy when using EndingReg on the two new data sources, show that Method #2 is a slightly better data sourcing scheme in reducing the bias, since the EndingReg model’s performance is slightly worse.",5 Data Collection,[0],[0]
The set was further filtered through human verification similar to Mostafazadeh et al. (2016).,5 Data Collection,[0],[0]
"The filtering was done by splitting each SCT-v1.0’s two alternative endings into two independent five-sentence stories and asking three different MTurk users to categorize the story as either: one where the story made complete sense, one where the story made sense until the last sentence and one where the story does not make sense for another reason.",5 Data Collection,[0],[0]
Stories were only selected if all the three MTurk users verified that the story with the ‘right ending’ and the corresponding story with the ‘wrong ending’ were verified to be indeed right and wrong respectively.,5 Data Collection,[0],[0]
This ensured a higher quality of data and eliminating boundary cases.,5 Data Collection,[0],[0]
"This entire process resulted in creating the Story Cloze Test v1.5 (SCT-v1.5) dataset, consisting of 1,571 stories for each validation and test sets.",5 Data Collection,[0],[0]
"In order to test the decrease in n-gram bias, which was the most salient feature for the classification task using only the endings, we compare the variance between the n-gram counts from SCT-v1.0 to SCT-v1.5.",6 Results,[0],[0]
"The results are presented in Table 5, which indicates the drop in the standard deviations in our new dataset.",6 Results,[0],[0]
Table 6 shows the classification results of various models on SCT-v1.5.,6 Results,[0],[0]
"The drop in accuracy of the EndingReg model between the SCT-v1.0 and SCT-v1.5 shows a significant improvement on the statistical weight of the stylistic features generated by the model.
",6 Results,[0],[0]
"Since the main features used are the token length and the various n-grams, this suggests that the new ‘right endings’ and ‘wrong endings’ have much more similar token n-gram, pos n-gram, postoken n-gram and char-gram overlap.",6 Results,[0],[0]
"Furthermore, the CogComp model’s performance has significantly dropped on SCT-v1.5.",6 Results,[0],[0]
"Although this model seems to be using story comprehension features such as event sequencing, since the endings are included in the sequences, the biases within the endings have influenced the predictions and the weak performance of the model in SCT-v1.5 suggest that this model had picked up on the biases of SCT-v1.0 as opposed to really understanding the context.",6 Results,[0],[0]
"In particular, the posterior probabilities for each ending choice using their features are quite similar on the SCT-v1.5.",6 Results,[0],[0]
"These results place the classification accuracy of this top performing model on par with or worse than the models that did not use the ending features of the old SCT-v1.0 dataset (Mostafazadeh et al., 2017), which suggest that the gap that once was held by models using the ending biases seems to be corrected for.",6 Results,[0],[0]
"Al-
though we did not get to test all the other models published on SCT-v1.0 directly, we predict similar trends.
",6 Results,[0],[0]
It is important to point out that the 64.4% performance attained by our EndingReg model is still high for a model which completely discards the context.,6 Results,[0],[0]
"This indicates that although we could correct for some of the stylistic biases, there are some other hidden patterns in the new endings that would not have been accounted for without having the EndingReg baseline.",6 Results,[0],[0]
"This showcases the importance of maintaining benchmarks that evolve and improve over time, where systems should not be optimized for particular narrow test sets.",6 Results,[0],[0]
"We propose the community to report accuracies on both SCT-v1.0 and SCT-v1.5, both of which still have a huge gap between the best system and the human performance.",6 Results,[0],[0]
"In this paper, we presented a comprehensive analysis of the stylistic features isolated in the endings of the original Story Cloze Test (SCT-v1.0).",7 Conclusion,[0],[0]
"Using that analysis, along with a classifier we developed for testing new data collection schemes, we created a new SCT dataset, SCT-v1.5, which overcomes some of the biases.",7 Conclusion,[0],[0]
"Based on the results presented in this paper, we believe that our SCT-v1.5 is a better benchmark for story comprehension.",7 Conclusion,[0],[0]
"However, as shown in multiple AI tasks (Ettinger et al., 2017; Antol et al., 2015; Jabri et al., 2016; Poliak et al., 2018), no collected dataset is entirely without its inherent biases and often the biases in datasets go undiscovered.",7 Conclusion,[0],[0]
We believe that evaluation benchmarks should evolve and improve over time and we are planning to incrementally update the Story Cloze Test benchmark.,7 Conclusion,[0],[0]
"All the new versions, along with a leader-board showcasing the stateof-the-art results, will be tracked via CodaLab
https://competitions.codalab.org/ competitions/15333.
",7 Conclusion,[0],[0]
The success of our modified data collection method shows how extreme care must be given for sourcing new datasets.,7 Conclusion,[0],[0]
"We suggest the next SCT challenges to be completely blind, where the participants cannot deliberately leverage any particular data biases.",7 Conclusion,[0],[0]
"Along with this paper, we are releasing the datasets and the developed models to the community.",7 Conclusion,[0],[0]
"All the announcements, new supplementary material, and datasets can be accessed through http://cs.",7 Conclusion,[0],[0]
rochester.edu/nlp/rocstories/.,7 Conclusion,[0],[0]
We hope that this work ignites further interest in the community for making progress on story understanding.,7 Conclusion,[0],[0]
We would like to thank Roy Schwartz for his valuable feedback regarding some of the experiments.,Acknowledgement,[0],[0]
"We also thank the amazing crowd workers, without the work of whom this work would have been impossible.",Acknowledgement,[0],[0]
This work was supported in part by grant W911NF15-1-0542 with the US Defense Advanced Research Projects Agency (DARPA) as a part of the Communicating with Computers (CwC) program.,Acknowledgement,[0],[0]
The Story Cloze Test (SCT) is a recent framework for evaluating story comprehension and script learning.,abstractText,[0],[0]
There have been a variety of models tackling the SCT so far.,abstractText,[0],[0]
"Although the original goal behind the SCT was to require systems to perform deep language understanding and commonsense reasoning for successful narrative understanding, some recent models could perform significantly better than the initial baselines by leveraging human-authorship biases discovered in the SCT dataset.",abstractText,[0],[0]
"In order to shed some light on this issue, we have performed various data analysis and analyzed a variety of top performing models presented for this task.",abstractText,[0],[0]
"Given the statistics we have aggregated, we have designed a new crowdsourcing scheme that creates a new SCT dataset, which overcomes some of the biases.",abstractText,[0],[0]
We benchmark a few models on the new dataset and show that the topperforming model on the original SCT dataset fails to keep up its performance.,abstractText,[0],[0]
Our findings further signify the importance of benchmarking NLP systems on various evolving test sets.,abstractText,[0],[0]
Tackling the Story Ending Biases in The Story Cloze Test,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2026–2031, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Online discussion forums are a popular platform for people to share their views about current events and learn about issues of concern to them.,1 Introduction,[0],[0]
"Discussion forums tend to specialize on different topics, and people participating in them form communities of interest.",1 Introduction,[0],[0]
The reaction of people within a community to comments posted provides an indication of community endorsement of opinions and value of information.,1 Introduction,[0],[0]
"In most discussions, the vast majority of comments spawn little reaction.",1 Introduction,[0],[0]
"In this paper, we look at whether (and how) language use affects the reaction, compared to the relative importance of the author and timing of the post.
",1 Introduction,[0],[0]
"Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect).",1 Introduction,[0],[0]
"Judging by differences in popularity of various discussion forums, topic is clearly important.",1 Introduction,[0],[0]
"Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014;
Tan et al., 2014).",1 Introduction,[0],[0]
"Teasing these different factors apart, however, is a challenge.",1 Introduction,[0],[0]
The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest.,1 Introduction,[0],[0]
"Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion.
",1 Introduction,[0],[0]
"The primary contributions of this work include findings about the role of author reputation and variation across communities in terms of aspects of language use that matter, as well as the problem formulation, associated data collection, and development of a variety of features for characterizing informativeness, community response, relevance and mood.",1 Introduction,[0],[0]
"Reddit1 is the largest public online discussion forum with a wide variety of subreddits, which makes it a good data source for studying how textual content in a discussion impacts the response of the crowd.",2 Data,[0],[0]
"On Reddit, people initiate a discussion thread with a post (a question, a link to a news item, etc.), and others respond with comments.",2 Data,[0],[0]
Registered users vote on which posts and comments are important.,2 Data,[0],[0]
"The total amount of up votes minus the down votes (roughly) is called karma; it provides an indication of community endorsement and popularity of a comment, as used in (Lakkaraju et al., 2013).",2 Data,[0],[0]
"Karma is valued as it impacts the order in which the posts or comments are displayed, with the high karma content rising to the top.",2 Data,[0],[0]
"Karma points are also accumulated by members of the discussion forum as a function of the karma associated with their comments.
",2 Data,[0],[0]
"1http://www.reddit.com
2026
The Reddit data is highly skewed.",2 Data,[0],[0]
"Although there are thousands of active communities, only a handful of them are large.",2 Data,[0],[0]
"Similarly, out of the more than a million comments made per day2, most of them receive little to no attention; the distributions of positive comment karma and author karma are Zipfian.",2 Data,[0],[0]
"Slightly more than half of all comments have exactly one karma point (no votes beyond the author), and only 5% of comments have less than one karma point.
",2 Data,[0],[0]
"For this study, we downloaded all the posts and associated comments made to six subreddits over a few weeks, as summarized in Table 1, as well as karma of participants in the discussion3.",2 Data,[0],[0]
All available comments on each post were downloaded at least 48 hours after the post was made.4,2 Data,[0],[0]
"Factors other than the language use that influence whether a comment will have uptake from the community include the topic, the timing of the message, and the messenger.",3 Uptake Factors,[0],[0]
These factors are all evident in the Reddit discussions.,3 Uptake Factors,[0],[0]
"Some subreddits are more popular and thus have higher karma comments than others, reflecting the influence of topic.",3 Uptake Factors,[0],[0]
"Comments that are posted early in the discussion are more likely to have high karma, since they have more potential responses.
",3 Uptake Factors,[0],[0]
"Previous studies on Twitter show that the reputation of the author substantially increases the chances of the retweet (Suh et al., 2010; Cha et al., 2010), and reputation is also raised as a factor in Slashdot (Lampe and Resnick, 2004).",3 Uptake Factors,[0],[0]
"On Reddit most users are anonymous, but it is possible that members of a forum become familiar with particular usernames associated with high karma comments.",3 Uptake Factors,[0],[0]
"In order to see how important per-
2http://www.redditblog.com/2014/12/reddit-in-2014.html 3Our data collection is available online at https://ssli.ee.washington.edu/tial/data/reddit 4Based on our initial look at the data, we noticed that most posts receive all of their comments within 48 hours.",3 Uptake Factors,[0],[0]
"Some comments are deleted before we are able to download them.
",3 Uptake Factors,[0],[0]
"sonal reputation is, we looked at how often the top karma comments are associated with the top karma participants in the discussion.",3 Uptake Factors,[0],[0]
"Since an individual’s karma can be skewed by a few very popular posts, we measure reputation instead using a measure we call the k-index, defined to be equal to the number of comments in each user’s history that have karma ≥ k.",3 Uptake Factors,[0],[0]
"The k-index is analgous to the h-index (Hirsch, 2005) and arguably a better indicator of extended impact than total karma.
",3 Uptake Factors,[0],[0]
The results in Table 2 address the question of whether the top karma comments always come from the top karma person.,3 Uptake Factors,[0],[0]
The Top1 column shows the percentage of threads where the top karma comment in a discussion happens to be made by the highest k-index person participating in the discussion; the next column shows the percentage of threads where the comment comes from any one of the top 3 k-index people.,3 Uptake Factors,[0],[0]
"We find that, in fact, the highest karma comment in a discussion is rarely from the highest k-index people.",3 Uptake Factors,[0],[0]
"The highest percentage is in ASKSCIENCE, where expertise is more highly valued.",3 Uptake Factors,[0],[0]
"If we consider whether any one of the multiple comments that the top k-index person made is the top karma comment in the discussion, then the frequency is even lower.",3 Uptake Factors,[0],[0]
"Having shown that the reputation of the author of a post is not a dominating factor in predicting high karma comments, we propose to control for topic and timing by ranking a set of 10 comments that were made consecutively in a short window of time within one discussion thread according to the karma they finally received.",4.1 Tasks,[0],[0]
The ranking has access to the comment history about these posts.,4.1 Tasks,[0],[0]
"This simulates the view of an early reader of these posts, i.e., without influence of the ratings of oth-
ers, so that the language content of the post is more likely to have an impact.",4.1 Tasks,[0],[0]
"Very long threads are sampled, so that these do not dominate the set of lists.",4.1 Tasks,[0],[0]
"Approximately 75% of the comment lists are designated for training and the rest is for testing, with splits at the discussion thread level.",4.1 Tasks,[0],[0]
"Here, feature selection is based on mean precision of the top-ranked comment (P@1), so as to emphasize learning the rare high karma events.",4.1 Tasks,[0],[0]
(Note that P@1 is equivalent to accuracy but allows for any top-ranking comment to count as correct in the case of ties.),4.1 Tasks,[0],[0]
"The system performance is evaluated using both P@1 and normalized discounted cumulative gain (NDCG) (Burges et al., 2005), which is a standard criterion for ranking evaluation when the samples to be ranked have meaningful differences in scores, as is the case for karma of the comments.
",4.1 Tasks,[0],[0]
"In addition, for analysis purposes, we report results for three surrogate tasks that can be used in the ranking problem: i) the binary ranker trained on all comment pairs within each list, in which low karma comments dominate, ii) a positive vs. negative karma classifier, and iii) a high vs. medium karma classifier.",4.1 Tasks,[0],[0]
"All use class-balanced data; the second two are trained and tested on a biased sampling of the data, where the pairs need not be from the same discussion thread.",4.1 Tasks,[0],[0]
"We use the support vector machine (SVM) rank algorithm (Joachims, 2002) to predict a rank order for each list of comments.",4.2 Classifier,[0],[0]
The SVM is trained to predict which of a pair of comments has higher karma.,4.2 Classifier,[0],[0]
"The error term penalty parameter is tuned to maximize P@1 on a held-out validation set (20% of the training samples).
",4.2 Classifier,[0],[0]
"Since much of the data includes low-karma comments, there will be a tendancy for the learning to emphasize features that discriminate comments at the lower end of the scale.",4.2 Classifier,[0],[0]
"In order to learn features that improve P@1, and to understand the relative importance of different features, we use a greedy automatic feature selection process that incrementally adds one feature whose resulting feature set achives the highest P@1 on the validation set.",4.2 Classifier,[0],[0]
"Once all features have been used, we select the model with the subset of features that obtains the best P@1 on the validation set.",4.2 Classifier,[0],[0]
The features are designed to capture several key attributes that we hypothesize are predictive of comment karma motivated by related work.,4.3 Features,[0],[0]
"The features are categorized in groups as summarized below, with details in supplementary material.",4.3 Features,[0],[0]
"• Graph and Timing (G&T): A baseline that
captures discourse history (response structure) and comment timing, but no text content.",4.3 Features,[0],[0]
"• Authority and Reputation (A&R): K-index,
whether the commenter was the original poster, and in some subreddits “flair” (display next to a comment author’s username that is subject to a cursory verification by moderators).",4.3 Features,[0],[0]
•,4.3 Features,[0],[0]
Informativeness (Info.):,4.3 Features,[0],[0]
"Different indicators
suggestive of informative content and novelty, including various word counts, named entity counts, urls, and unseen n-grams.",4.3 Features,[0],[0]
• Lexical Unigrams (Lex.):,4.3 Features,[0],[0]
"Miscellaneous word
class indicators, puncutation, and part-ofspeech counts • Predicted Community Response (Resp.):
",4.3 Features,[0],[0]
"Probability scores from surrogate classification tasks (reply vs. no reply, positive vs. negative sentiment) to measure the community response of a comment using bag-of-words predictors.",4.3 Features,[0],[0]
• Relevance (Rel.):,4.3 Features,[0],[0]
"Comment similarity to the
parent, post and title in terms of topic, computed with three methods: i) a distributed vector representation of topic using a non-negative matrix factorization (NMF) model (Xu et al., 2003), ii)",4.3 Features,[0],[0]
"the average of skip-gram word embeddings (Mikolov et al., 2013), and iii) word set Jaccard similarity (Strehl et al., 2000).",4.3 Features,[0],[0]
•,4.3 Features,[0],[0]
Mood: Mean and std.,4.3 Features,[0],[0]
"deviation of sentence sen-
timent in the comment; word list indicators for politeness, argumentativeness and profanity.",4.3 Features,[0],[0]
• Community Style (Comm.):,4.3 Features,[0],[0]
"Posterior proba-
bility of each subreddit given the comment using a bag-of-words model.",4.3 Features,[0],[0]
The various word lists are motivated by feature exploration studies in surrogate tasks.,4.3 Features,[0],[0]
"For example, projecting words to a two dimensional space of positive vs. negative and likelihood of reply showed that self-oriented pronouns were more likely to have no response and secondperson pronouns were more likely to have a negative response.",4.3 Features,[0],[0]
"The politeness and argumentativeness/profanity lists are generated by starting with hand-specified seed lists used to train an SVM to classify word embeddings (Mikolov et al., 2013)
into these categories, and expanding the lists with 500 words farthest from the decision boundary.
",4.3 Features,[0],[0]
"Both the NMF and the skip-gram topic models use a cosine distance to determine topic similarity, with 300 as the word embedding dimension.",4.3 Features,[0],[0]
Both are trained on approximately 2 million comments in high karma posts taken across a wide variety of subreddits.,4.3 Features,[0],[0]
"We use topic models in various measures of comment relevance to the discussion, but we do not use topic of the comment on its own since topic is controlled for by ranking within a thread.",4.3 Features,[0],[0]
"We present three sets of experiments on comment karma ranking, all of which show very different behavior for the different subreddits.",5 Ranking Experiments,[0],[0]
Fig. 1 shows the relative gain in P@1 over the G&T baseline associated with using different feature groups.,5 Ranking Experiments,[0],[0]
The importance of the different features reflect the nature of the different communities.,5 Ranking Experiments,[0],[0]
"The authority/reputation features help most for ASKSCIENCE, consistent with our k-index study.",5 Ranking Experiments,[0],[0]
Informativeness and relevance help all subreddits except ASKMEN and WORLDNEWS.,5 Ranking Experiments,[0],[0]
"Lexical, mood and community style features are useful in some cases, but hurt others.",5 Ranking Experiments,[0],[0]
"The predicted probability of a reply was least useful, possibly because of the low-karma training bias.
",5 Ranking Experiments,[0],[0]
Tables 3 and 4 summarize the results for the P@1 and NDCG criteria using the greedy selection procedure (which optimizes P@1) compared to a random baseline and the G&T baseline.,5 Ranking Experiments,[0],[0]
The random baseline for P@1 is greater than 10% because of ties.,5 Ranking Experiments,[0],[0]
"The G&T baseline results show that the graph and timing features alone obtain 21-32%
of top karma comments depending on subreddits.",5 Ranking Experiments,[0],[0]
Adding the textual features gives an improvement in P@1 performance over the G&T baseline for all subreddits except ASKMEN and WORLDNEWS.,5 Ranking Experiments,[0],[0]
"The trends for performance measured with NDCG are similar, but the benefit from textual features is smaller.",5 Ranking Experiments,[0],[0]
"The results in both tables show different ways of reporting performance of the same system, but the system has been optimized for P@1 in terms of feature selection.",5 Ranking Experiments,[0],[0]
"In initial exploratory experiments, this seems to have a small impact: when optimizing for NDCG in feature selection we obtain 0.61 vs. 0.60 with the P@1-optimized features.
",5 Ranking Experiments,[0],[0]
"A major challenge with identifying high karma comments (and negative karma comments) is that
they are so rare.",5 Ranking Experiments,[0],[0]
"Although our feature selection tunes for high rank precision, it is possible that the low-karma data dominate the learning.",5 Ranking Experiments,[0],[0]
"Alternatively, it may be that language cues are mainly useful for identifying distinguishing the negative or mid-level karma comments, and that the very high karma comments are a matter of timing.",5 Ranking Experiments,[0],[0]
"To better understand the role of language for these different types, we trained classifiers on balanced data for positive vs. negative karma and high vs. mid levels of karma.",5 Ranking Experiments,[0],[0]
"For these models, the training pairs could come from different threads, but topic is controlled for in that all topic features are relative (similarity to original post, parent, etc.).",5 Ranking Experiments,[0],[0]
"We compared the results to the binary classifier used in ranking, where all pairs are considered.",5 Ranking Experiments,[0],[0]
"In all three cases, random chance accuracy is 50%.
",5 Ranking Experiments,[0],[0]
Table 5 shows the pairwise accuracy of these classifiers.,5 Ranking Experiments,[0],[0]
"We find that distinguishing positive from negative classes is fairly easy, with the notable exception of the more information-oriented subreddit ASKSCIENCE.",5 Ranking Experiments,[0],[0]
"Averaging across the different subreddits, the high vs. mid task is slightly easier than the general ranking task, but the variation across subreddits is substantial.",5 Ranking Experiments,[0],[0]
"The high vs. mid distinction for FITNESS falls below chance (likely overtraining), whereas it seems to be an easier task for the ASKWOMEN, ASKMEN, and WORLDNEWS.",5 Ranking Experiments,[0],[0]
"Interest in social media is rapidly growing in recent years, which includes work on predicting the popularity of posts, comments and tweets.",6 Related Work,[0],[0]
Danescu-Niculescu-Mizil et al. (2012) investigate phrase memorability in the movie quotes.,6 Related Work,[0],[0]
Cheng et al. (2014) explore prediction of information cascades on Facebook.,6 Related Work,[0],[0]
"Weninger et al. (2013) analyze the hierarchy of the Reddit discussions, topic shifts, and popularity of the comment, using among the others very simple language analysis.",6 Related Work,[0],[0]
"Lampos et al. (2014) study the problem of predicting a Twitter user impact score (determined by combining the numbers of user’s followers, followees, and listings) using text-based and nontextual features, showing that performance improves when user participation in particular topics is included.
",6 Related Work,[0],[0]
Most relevant to this paper are studies of the effect of language in popularity predictions.,6 Related Work,[0],[0]
"Tan et al. (2014) study how word choice affects the pop-
ularity of Twitter messages.",6 Related Work,[0],[0]
"As in our work, they control for topic, but they also control for the popularity of the message authors.",6 Related Work,[0],[0]
"On Reddit, we find that celebrity status is less important than it is on Twitter since on Reddit almost everyone is anonymous.",6 Related Work,[0],[0]
Lakkaraju et al. (2013) study how timing and language affect the popularity of posting images on Reddit.,6 Related Work,[0],[0]
They control for content by only making comparisons between reposts of the same image.,6 Related Work,[0],[0]
"Our focus is on studying comments within a discussion instead of standalone posts, and we analyze a vast majority of language features.",6 Related Work,[0],[0]
Althoff et al. (2014) use deeper language analysis on Reddit to predict the success of receiving a pizza in the Random Acts of Pizza subreddit.,6 Related Work,[0],[0]
"To our knowledge, this is the first work on ranking comments in terms of community endorsement.",6 Related Work,[0],[0]
This paper addresses the problem of how language affects the reaction of community in Reddit comments.,7 Conclusion,[0],[0]
We collect a new dataset of six subredit discussion forums.,7 Conclusion,[0],[0]
"We introduce a new task of ranking comments based on karma in Reddit discussions, which controls for topic and timing of comments.",7 Conclusion,[0],[0]
Our results show that using language features improve the comment ranking task in most of the subreddits.,7 Conclusion,[0],[0]
"Informativeness and relevance are the most broadly useful feature categories; reputation matters for ASKSCIENCE, and other categories could either help or hurt depending on the community.",7 Conclusion,[0],[0]
Future work involves improving the classification algorithm by using new approaches to learning about rare events.,7 Conclusion,[0],[0]
"This paper addresses the question of how language use affects community reaction to comments in online discussion forums, and the relative importance of the message vs. the messenger.",abstractText,[0],[0]
"A new comment ranking task is proposed based on community annotated karma in Reddit discussions, which controls for topic and timing of comments.",abstractText,[0],[0]
Experimental work with discussion threads from six subreddits shows that the importance of different types of language features varies with the community of interest.,abstractText,[0],[0]
Talking to the crowd: What do people react to in online discussions?,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 896–905 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1083
Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation. In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008). However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012).
Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated
topics, making topic models less of a “take it or leave it” proposition. Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis.
The downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009). We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models.
The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1). This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections.
A drawback of the anchor method is that anchor words—words that have high probability of being in a single topic—are not intuitive. We extend the anchor algorithm to use multiple anchor words in tandem (Section 2). Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive.
For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3). Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4). Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.
896",text,[0],[0]
"The anchor algorithm computes the topic matrix A, where Av,k is the conditional probability of observing word v given topic k, e.g., the probability of seeing the word “lens” given the camera topic in a corpus of Amazon product reviews.",1 Vanilla Anchor Algorithm,[0],[0]
Arora et al. (2012a) find these probabilities by assuming that every topic contains at least one ‘anchor’ word which has a non-zero probability only in that topic.,1 Vanilla Anchor Algorithm,[0],[0]
"Anchor words make computing the topic matrix A tractable because the occurrence pattern of the anchor word mirrors the occurrence pattern of the topic itself.
",1 Vanilla Anchor Algorithm,[0],[0]
"To recover the topic matrix A using anchor words, we first compute a V × V cooccurrence matrix Q, where Qi,j is the conditional probability p(wj |wi) of seeing word type wj after having seen wi in the same document.",1 Vanilla Anchor Algorithm,[0],[0]
A form of the Gram-Schmidt process on Q finds anchor words {g1 . . .,1 Vanilla Anchor Algorithm,[0],[0]
"gk} (Arora et al., 2013).
",1 Vanilla Anchor Algorithm,[0],[0]
"Once we have the set of anchor words, we can compute the probability of a topic given a word (the inverse of the conditioning in A).",1 Vanilla Anchor Algorithm,[0],[0]
"This coefficient matrix C is defined row-wise for each word i
C∗i,· = argmin Ci,· DKL
( Qi,· ∥∥∥∥",1 Vanilla Anchor Algorithm,[0],[0]
"K∑
k=1
Ci,kQgk,·
) ,
(1) which gives the best reconstruction (based on Kullback-Leibler divergence DKL) of non-anchor words given anchor words’ conditional probabilities.",1 Vanilla Anchor Algorithm,[0],[0]
"For example, in our product review data, a word such as “battery” is a convex combination of the anchor words’ contexts (Qgk,·) such as “camera”, “phone”, and “car”.",1 Vanilla Anchor Algorithm,[0],[0]
Solving each row of C is fast and is embarrassingly parallel.,1 Vanilla Anchor Algorithm,[0],[0]
"Finally, we apply Bayes’ rule to recover the topic matrix A from the coefficient matrix",1 Vanilla Anchor Algorithm,[0],[0]
"C.
The anchor algorithm can be orders of magnitude faster than probabilistic inference (Arora et al., 2013).",1 Vanilla Anchor Algorithm,[0],[0]
The construction of Q has a runtime of O(DN2) where D is the number of documents and N is the average number of tokens per document.,1 Vanilla Anchor Algorithm,[0],[0]
This computation requires only a single pass over the data and can be pre-computed for interactive use-cases.,1 Vanilla Anchor Algorithm,[0],[0]
"Once Q is constructed, topic recovery requires O(KV 2 +K2V I), where K is the number of topics, V is the vocabulary size, and I is the average number of iterations (typically 100-1000).",1 Vanilla Anchor Algorithm,[0],[0]
"In contrast, traditional topic
Anchor Top Words in Topics backpack backpack camera lens bag room carry fit cameras equipment comfortable camera camera lens pictures canon digital lenses batteries filter mm photos bag bag camera diaper lens bags genie smell
room diapers odor
Table 1: Three separate attempts to construct a topic concerning camera bags in Amazon product reviews with single word anchors.",1 Vanilla Anchor Algorithm,[0],[0]
This example is drawn from preliminary experiments with an author as the user.,1 Vanilla Anchor Algorithm,[0],[0]
The term “backpack” is a good anchor because it uniquely identifies the topic.,1 Vanilla Anchor Algorithm,[0],[0]
"However, both “camera” and “bag” are poor anchors for this topic.
model inference typically requires multiple passes over the entire data.",1 Vanilla Anchor Algorithm,[0],[0]
"Techniques such as Online LDA (Hoffman et al., 2010) or Stochastic Variation Inference (Hoffman et al., 2013) improves this to a single pass over the entire data.",1 Vanilla Anchor Algorithm,[0],[0]
"However, from Heaps’ law (Heaps, 1978)",1 Vanilla Anchor Algorithm,[0],[0]
"it follows that V 2 DN for large datasets, leading to much faster inference times for anchor methods compared to probabilistic topic modeling.",1 Vanilla Anchor Algorithm,[0],[0]
"Further, even if online were to be adapted to incorporate human guidance, a single pass is not tractable for interactive use.",1 Vanilla Anchor Algorithm,[0],[0]
Single word anchors can be opaque to users.,2 Tandem Anchor Extension,[0],[0]
"For an example of bewildering anchor words, consider a camera bag topic from a collection of Amazon product reviews (Table 1).",2 Tandem Anchor Extension,[0],[0]
The anchor word “backpack” may seem strange.,2 Tandem Anchor Extension,[0],[0]
"However, this dataset contains nothing about regular backpacks; thus, “backpack” is unique to camera bags.",2 Tandem Anchor Extension,[0],[0]
"Bizarre, low-to-mid frequency words are often anchors because anchor words must be unique to a topic; intuitive or high-frequency words cannot be anchors if they have probability in any other topic.
",2 Tandem Anchor Extension,[0],[0]
The anchor selection strategy can mitigate this problem to some degree.,2 Tandem Anchor Extension,[0],[0]
"For example, rather than selecting anchors using an approximate convex hull in high-dimensional space, we can find an exact convex hull in a low-dimensional embedding (Lee and Mimno, 2014).",2 Tandem Anchor Extension,[0],[0]
"This strategy will produce more salient topics but still makes it difficult for users to manually choose unique anchor words for interactive topic modeling.
",2 Tandem Anchor Extension,[0],[0]
"If we instead ask users to give us representative
words for this topic, we would expect combinations of words like “camera” and “bag.”",2 Tandem Anchor Extension,[0],[0]
"However, with single word anchors we must choose a single word to anchor each topic.",2 Tandem Anchor Extension,[0],[0]
"Unfortunately, because these words might appear in multiple topics, individually they are not suitable as anchor words.",2 Tandem Anchor Extension,[0],[0]
"The anchor word “camera” generates a general camera topic instead of camera bags, and the topic anchored by “bag” includes bags for diaper pails (Table 1).
",2 Tandem Anchor Extension,[0],[0]
"Instead, we need to use sets of representative terms as an interpretable, parsimonious description of a topic.",2 Tandem Anchor Extension,[0],[0]
This section discusses strategies to build anchors from multiple words and the implications of using multiword anchors to recover topics.,2 Tandem Anchor Extension,[0],[0]
This extension not only makes anchors more interpretable but also enables users to manually construct effective anchors in interactive topic modeling settings.,2 Tandem Anchor Extension,[0],[0]
We first need to turn words into an anchor.,2.1 Anchor Facets,[0],[0]
"If we interpret the anchor algorithm geometrically, each row of Q represents a word as a point in V -dimensional space.",2.1 Anchor Facets,[0],[0]
We then model each point as a convex combination of anchor words to reconstruct the topic matrix A (Equation 1).,2.1 Anchor Facets,[0],[0]
"Instead of individual anchor words (one anchor word per topic), we use anchor facets, or sets of words that describe a topic.",2.1 Anchor Facets,[0],[0]
"The facets for each anchor form a new pseudoword, or an invented point in V -dimensional space (described in more detail in Section 2.2).
",2.1 Anchor Facets,[0],[0]
"While these new points do not correspond to words in the vocabulary, we can express nonanchor words as convex combinations of pseudowords.",2.1 Anchor Facets,[0],[0]
"To construct these pseudowords from their facets, we combine the co-occurrence profiles of the facets.",2.1 Anchor Facets,[0],[0]
These pseudowords then augment the original cooccurrence matrix Q with K additional rows corresponding to synthetic pseudowords forming each of K multiword anchors.,2.1 Anchor Facets,[0],[0]
"We refer to this augmented matrix as S. The rest of the anchor algorithm proceeds unmodified.
",2.1 Anchor Facets,[0],[0]
Our augmented matrix S is therefore a (V + K) × V matrix.,2.1 Anchor Facets,[0],[0]
"As before, V is the number of token types in the data and K is the number of topics.",2.1 Anchor Facets,[0],[0]
"The first V rows of S correspond to the V token types observed in the data, while the additionalK rows correspond to the pseudowords constructed from anchor facets.",2.1 Anchor Facets,[0],[0]
"Each entry of S en-
codes conditional probabilities so that Si,j is equal to p(wi |wj).",2.1 Anchor Facets,[0],[0]
"For the additionalK rows, we invent a cooccurrence pattern that can effectively explain the other words’ conditional probabilities.
",2.1 Anchor Facets,[0],[0]
"This modification is similar in spirit to supervised anchor words (Nguyen et al., 2015).",2.1 Anchor Facets,[0],[0]
This supervised extension of the anchor words algorithm adds columns corresponding to conditional probabilities of metadata values after having seen a particular word.,2.1 Anchor Facets,[0],[0]
"By extending the vector-space representation of each word, anchor words corresponding to metadata values can be found.",2.1 Anchor Facets,[0],[0]
"In contrast, our extension does not add dimensions to the representation, but simply places additional points corresponding to pseudoword words in the vectorspace representation.",2.1 Anchor Facets,[0],[0]
We now describe more concretely how to combine an anchor facets to describe the cooccurrence pattern of our new pseudoword anchor.,2.2 Combining Facets into Pseudowords,[0],[0]
"In tandem anchors, we create vector representations that combine the information from anchor facets.",2.2 Combining Facets into Pseudowords,[0],[0]
Our anchor facets are G1 . .,2.2 Combining Facets into Pseudowords,[0],[0]
.GK,2.2 Combining Facets into Pseudowords,[0],[0]
", where Gk is a set of anchor facets which will form the kth pseudoword anchor.",2.2 Combining Facets into Pseudowords,[0],[0]
The pseudowords are g1 . . .,2.2 Combining Facets into Pseudowords,[0],[0]
"gK , where gk is the pseudoword from Gk.",2.2 Combining Facets into Pseudowords,[0],[0]
"These pseudowords form the new rows of S. We give several candidates for combining anchors facets into a single multiword anchor; we compare their performance in Section 3.
",2.2 Combining Facets into Pseudowords,[0],[0]
Vector Average An obvious function for computing the central tendency is the vector average.,2.2 Combining Facets into Pseudowords,[0],[0]
"For each anchor facet,
Sgk,j = ∑
i∈Gk
Si,j |Gk| , (2)
where |Gk| is the cardinality of Gk.",2.2 Combining Facets into Pseudowords,[0],[0]
"Vector average makes the pseudoword Sgk,j more central, which is intuitive but inconsistent with the interpretation from Arora et al. (2013) that anchors should be extreme points whose linear combinations explain more central words.
",2.2 Combining Facets into Pseudowords,[0],[0]
Or-operator An alternative approach is to consider a cooccurrence with any anchor facet in Gk.,2.2 Combining Facets into Pseudowords,[0],[0]
"For word j, we use De Morgan’s laws to set
Sgk,j = 1− ∏
i∈Gk (1− Si,j).",2.2 Combining Facets into Pseudowords,[0],[0]
"(3)
Unlike the average, which pulls the pseudoword inward, this or-operator pushes the word outward,
increasing each of the dimensions.",2.2 Combining Facets into Pseudowords,[0],[0]
"Increasing the volume of the simplex spanned by the anchors explains more words.
",2.2 Combining Facets into Pseudowords,[0],[0]
Element-wise Min Vector average and oroperator are both sensitive to outliers and cannot account for polysemous anchor facets.,2.2 Combining Facets into Pseudowords,[0],[0]
"Returning to our previous example, both “camera” and “bag” are bad anchors for camera bags because they appear in documents discussing other products.",2.2 Combining Facets into Pseudowords,[0],[0]
"However, if both “camera” and “bag” are anchor facets, we can look at an intersection of their contexts: words that appear with both.",2.2 Combining Facets into Pseudowords,[0],[0]
"Using the intersection, the cooccurrence pattern of our anchor facet will only include terms relevant to camera bags.
",2.2 Combining Facets into Pseudowords,[0],[0]
"Mathematically, this is an element-wise min operator,
Sgk,j = min i∈Gk Si,j .",2.2 Combining Facets into Pseudowords,[0],[0]
"(4)
This construction, while perhaps not as simple as the previous two, is robust to words which have cooccurrences which are not unique to a single topic.
",2.2 Combining Facets into Pseudowords,[0],[0]
"Harmonic Mean Leveraging the intuition that we should use a combination function which is both centralizing (like vector average) and ignores large outliers (like element-wise min), the final combination function is the element-wise harmonic mean.",2.2 Combining Facets into Pseudowords,[0],[0]
"Thus, for each anchor facet
Sgk,j = ∑
i∈Gk
( S−1i,j |Gk| )−1 .",2.2 Combining Facets into Pseudowords,[0],[0]
"(5)
Since the harmonic mean tends towards the lowest values in the set, it is not sensitive to large outliers, giving us robustness to polysemous words.",2.2 Combining Facets into Pseudowords,[0],[0]
"After constructing the pseudowords of S we then need to find the coefficients Ci,k which describe each word in our vocabulary as a convex combination of the multiword anchors.",2.3 Finding Topics,[0],[0]
"Like standard anchor methods, we solve the following for each token type:
C∗i,· = argmin Ci,· DKL
( Si,· ∥∥∥∥ K∑
k=1
Ci,kSgk,·
) .
",2.3 Finding Topics,[0],[0]
"(6) Finally, we appeal to Bayes’ rule, we recover the topic-word matrix A from the coefficients of C.
The correctness of the topic recovery algorithm hinges upon the assumption of separability.",2.3 Finding Topics,[0],[0]
"Separability means that the occurrence pattern across
documents of the anchor words across the data mirrors that of the topics themselves.",2.3 Finding Topics,[0],[0]
"For single word anchors, this has been observed to hold for a wide variety of data (Arora et al., 2012b).",2.3 Finding Topics,[0],[0]
"With our tandem anchor extension, we make similar assumptions as the vanilla algorithm, except with pseudowords constructed from anchor facets.",2.3 Finding Topics,[0],[0]
"So long as the occurrence pattern of our tandem anchors mirrors that of the underlying topics, we can use the same reasoning as Arora et al. (2012a) to assert that we can provably recover the topic-word matrix A with all of the same theoretical guarantees of complexity and robustness.",2.3 Finding Topics,[0],[0]
"Furthermore, we runtime analysis given by Arora et al. (2013) applies to tandem anchors.
",2.3 Finding Topics,[0],[0]
"If desired, we can also add further robustness and extensibility to tandem anchors by adding regularization to Equation 6.",2.3 Finding Topics,[0],[0]
"Regularization allows us to add something which is mathematically similar to priors, and has been shown to improve the vanilla anchor word algorithm (Nguyen et al., 2014).",2.3 Finding Topics,[0],[0]
"We leave the question of the best regularization for tandem anchors as future work, and focus our efforts on solving the problem of interactive topic modeling.",2.3 Finding Topics,[0],[0]
"Before addressing interactivity, we apply tandem anchors to real world data, but with anchors gleaned from metadata.",3 High Water Mark for Tandem Anchors,[0],[0]
Our purpose is twofold.,3 High Water Mark for Tandem Anchors,[0],[0]
"First, we determine which combiner from Section 2.2 to use in our interactive experiments in Section 4 and second, we confirm that well-chosen tandem anchors can improve topics.",3 High Water Mark for Tandem Anchors,[0],[0]
"In addition, we examine the runtime of tandem anchors and compare to traditional model-based interactive topic modeling techniques.",3 High Water Mark for Tandem Anchors,[0],[0]
"We cannot assume that we will have metadata available to build tandem anchors, but we use them here because they provide a high water mark without the variance introduced by study participants.",3 High Water Mark for Tandem Anchors,[0],[0]
"We use the well-known 20 Newsgroups dataset (20NEWS) used in previous interactive topic modeling work: 18,846 Usenet postings from 20 different newgroups in the early 1990s.1 We remove the newsgroup headers from each message, which contain the newsgroup names, but otherwise left messages intact with any footers or quotes.",3.1 Experimental Setup,[0],[0]
"We
1http://qwone.com/˜jason/20Newsgroups/
then remove stopwords and words which appear in fewer than 100 documents or more than 1,500 documents.
",3.1 Experimental Setup,[0],[0]
"To seed the tandem anchors, we use the titles of newsgroups.",3.1 Experimental Setup,[0],[0]
"To build each multiword anchor facet, we split the title on word boundaries and expand any abbreviations or acronyms.",3.1 Experimental Setup,[0],[0]
"For example, the newsgroup title ‘comp.os.mswindows.misc’ becomes {“computer”, “operating”, “system”, “microsoft”, “windows”, “miscellaneous”}.",3.1 Experimental Setup,[0],[0]
"We do not fully specify the topic; the title gives some intuition, but the topic modeling algorithm must still recover the complete topic-word distributions.",3.1 Experimental Setup,[0],[0]
This is akin to knowing the names of the categories used but nothing else.,3.1 Experimental Setup,[0],[0]
"Critically, the topic modeling algorithm has no knowledge of document-label relationships.",3.1 Experimental Setup,[0],[0]
Our first evaluation is a classification task to predict documents’ newsgroup membership.,3.2 Experimental Results,[0],[0]
"Thus, we do not aim for state-of-the-art accuracy,2 but the experiment shows title-based tandem anchors yield topics closer to the underlying classes than Gram-Schmidt anchors.",3.2 Experimental Results,[0],[0]
"After randomly splitting the data into test and training sets we learn topics from the test data using both the title-based tandem anchors and the Gram-Schmidt single word anchors.3 For multiword anchors, we use each of the combiner functions from Section 2.2.",3.2 Experimental Results,[0],[0]
"The anchor algorithm only gives the topic-word distributions and not word-level topic assignments, so we infer token-level topic assignments using LDA Latent Dirichlet Allocation (Blei et al., 2003) with fixed topics discovered by the anchor method.",3.2 Experimental Results,[0],[0]
We use our own implementation of Gibbs sampling with fixed topics and a symmetric documenttopic Dirichlet prior with concentration α = .01.,3.2 Experimental Results,[0],[0]
"Since the topics are fixed, this inference is very fast and can be parallelized on a per-document basis.",3.2 Experimental Results,[0],[0]
We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit4 with topic-word pairs as features.,3.2 Experimental Results,[0],[0]
"Finally, we infer topic assignments in the test data and evaluate the classification using those topic-word features.",3.2 Experimental Results,[0],[0]
"For both training and test, we exclude words outside
2The best system would incorporate topic features with other features, making it harder to study and understand the topical trends in isolation.
3With fixed anchors and data the anchor algorithm is deterministic, so we use random splits instead of the standard train/test splits so that we can compute variance.
",3.2 Experimental Results,[0],[0]
"4http://hunch.net/˜vw/
the LDA vocabulary.",3.2 Experimental Results,[0],[0]
The topics created from multiword anchor facets are more accurate than Gram-Schmidt topics (Figure 1).,3.2 Experimental Results,[0],[0]
This is true regardless of the combiner function.,3.2 Experimental Results,[0],[0]
"However, harmonic mean is more accurate than the other functions.5
Since 20NEWS has twenty classes, accuracy alone does not capture confusion between closely related newsgroups.",3.2 Experimental Results,[0],[0]
"For example, accuracy penalizes a classifier just as much for labeling a document from ‘rec.sport.baseball’ with ‘rec.sport.hockey’ as with ‘alt.atheism’ despite the similarity between sports newsgroups.",3.2 Experimental Results,[0],[0]
"Consequently, after building a confusion matrix between the predicted and true classes, external clustering metrics reveal confusion between classes.
",3.2 Experimental Results,[0],[0]
"The first clustering metric is the adjusted Rand index (Yeung and Ruzzo, 2001), which is akin to accuracy for clustering, as it gives the percentage of correct pairing decisions from a reference clustering.",3.2 Experimental Results,[0],[0]
Adjusted Rand index (ARI) also accounts for chance groupings of documents.,3.2 Experimental Results,[0],[0]
"Next we use F-measure, which also considers pairwise groups, balancing the contribution of false negatives, but without the true negatives.",3.2 Experimental Results,[0],[0]
"Finally, we use variation of information (VI).",3.2 Experimental Results,[0],[0]
"This metric measures the amount of information lost by switching from the gold standard labels to the predicted labels (Meilă, 2003).",3.2 Experimental Results,[0],[0]
"Since we are measuring the amount of information lost, lower variation of information is better.
",3.2 Experimental Results,[0],[0]
"Based on these clustering metrics, tandem anchors can yield superior topics to those created using single word anchors (Figure 1).",3.2 Experimental Results,[0],[0]
"As with accuracy, this is true regardless of which combination function we use.",3.2 Experimental Results,[0],[0]
"Furthermore, harmonic mean produces the least confusion between classes.5
The final evaluation is topic coherence by Newman et al. (2010), which measures whether the topics make sense, and correlates with human judgments of topic quality.",3.2 Experimental Results,[0],[0]
"Given V , the set of the n most probable words of a topic, coherence is
∑
v1,v2∈V log
D(v1, v2) +
D(v2) (7)
where D(v1, v2) is the co-document frequency of
5Significant at p < 0.01/4 when using two-tailed t-tests with a Bonferroni correction.",3.2 Experimental Results,[0],[0]
"For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.
word types v1 and v2, and D(v2) is the document frequency of word type v2.",3.2 Experimental Results,[0],[0]
"A smoothing parameter prevents zero logarithms.
",3.2 Experimental Results,[0],[0]
Figure 1 also shows topic coherence.,3.2 Experimental Results,[0],[0]
"Although title-based anchor facets produce better classification features, topics from Gram-Schmidt anchors have better coherence than title-based anchors with the vector average or the or-operator.",3.2 Experimental Results,[0],[0]
"However, when using the harmonic mean combiner, title-based anchors produce the most human interpretable topics.6
Harmonic mean beats other combiner functions because it is robust to ambiguous or irrelevant term cooccurrences an anchor facet.",3.2 Experimental Results,[0],[0]
"Both the vector average and the or-operator are swayed by large outliers, making them sensitive to ambiguous terms in an anchor facet.",3.2 Experimental Results,[0],[0]
"Element-wise min also has this robustness, but harmonic mean is also able to better characterize anchor facets as it has more centralizing tendency than the min.",3.2 Experimental Results,[0],[0]
Tandem anchors will enable users to direct topic inference to improve topic quality.,3.3 Runtime Considerations,[0],[0]
"However, for the algorithm to be interactive we must also consider runtime.",3.3 Runtime Considerations,[0],[0]
Cook and Thomas (2005) argue that for interactive applications with user-initiated actions like ours the response time should be less than ten seconds.,3.3 Runtime Considerations,[0],[0]
"Longer waits can increase the cognitive load on the user and harm the user interaction.
6Significant at p < 0.01/4 when using two-tailed t-tests with a Bonferroni correction.",3.3 Runtime Considerations,[0],[0]
"For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.
",3.3 Runtime Considerations,[0],[0]
"Fortunately, the runtime of tandem anchors is amenable to interactive topic modeling.",3.3 Runtime Considerations,[0],[0]
"On 20NEWS, interactive updates take a median time of 2.13 seconds.",3.3 Runtime Considerations,[0],[0]
This result was obtained using a single core of an AMD Phemon II X6 1090T processor.,3.3 Runtime Considerations,[0],[0]
"Furthermore, larger datasets typically have a sublinear increase in distinct word types, so we can expect to see similar run times, even on much larger datasets.
",3.3 Runtime Considerations,[0],[0]
"Compared to other interactive topic modeling algorithms, tandem anchors has a very attractive run time.",3.3 Runtime Considerations,[0],[0]
"For example, using an optimized version of the sampler for the Interactive Topic Model described by Hu and Boyd-Graber (2012), and the recommended 30 iterations of sampling, the Interactive Topic Model updates with a median time of 24.8 seconds (Hu and Boyd-Graber, 2012), which is well beyond our desired update time for interactive use and an order of magnitude slower than tandem anchors.
",3.3 Runtime Considerations,[0],[0]
"Another promising interactive topic modeling approach is Utopian (Choo et al., 2013), which uses non-negative factorization, albeit without the benefit of anchor words.",3.3 Runtime Considerations,[0],[0]
Utopian is much slower than tandem anchors.,3.3 Runtime Considerations,[0],[0]
"Even on the small InfoVisVAST dataset which contains only 515 documents, Utopian takes 48 seconds to converge.",3.3 Runtime Considerations,[0],[0]
"While the times are not strictly comparable due to differing datasets, Utopian scales linearly with the size of the data, we can intuit that even for moderately sized datasets such as 20NEWS, Utopian is infeasible for interactive topic modeling due to run time.
",3.3 Runtime Considerations,[0],[0]
"While each of these interactive topic modeling algorithms do achieve reasonable topics, only our algorithm fits the run time requirements for inter-
activity.",3.3 Runtime Considerations,[0],[0]
"Furthermore, since tandem anchors scales with the size of the vocabulary rather than the size of the data, this trend will only become more pronounced as we increase the amount of data.",3.3 Runtime Considerations,[0],[0]
"Given high quality anchor facets, the tandem anchor algorithm can produce high quality topic models (particularly when the harmonic mean combiner is used).",4 Interactive Anchor Words,[0],[0]
"Moreover, the tandem anchor algorithm is fast enough to be interactive (as opposed to model-based approaches such as the Interactive Topic Model).",4 Interactive Anchor Words,[0],[0]
We now turn our attention to our main experiment: tandem anchors applied to the problem of interactive topic modeling.,4 Interactive Anchor Words,[0],[0]
We compare both single word and tandem anchors in our study.,4 Interactive Anchor Words,[0],[0]
"We do not include the Interactive Topic Model or Utopian, as their run times are too slow for our users.",4 Interactive Anchor Words,[0],[0]
"To show that interactive tandem anchor words are fast, effective, and intuitive, we ask users to understand a dataset using the anchor word algorithm.",4.1 Interface and User Study,[0],[0]
"For this user study, we recruit twenty participants drawn from a university student body.",4.1 Interface and User Study,[0],[0]
The student median age is twenty-two.,4.1 Interface and User Study,[0],[0]
"Seven are female, and thirteen are male.",4.1 Interface and User Study,[0],[0]
"None of the students had any prior familiarity with topic modeling or the 20NEWS dataset.
",4.1 Interface and User Study,[0],[0]
Each participant sees a simple user interface (Figure 2) with topic given as a row with two columns.,4.1 Interface and User Study,[0],[0]
The left column allows users to view and edit topics’ anchor words; the right column lists the most probable words in each topic.7,4.1 Interface and User Study,[0],[0]
"The user can remove an anchor word or drag words from
7While we use topics generated using harmonic mean for our final analysis, users were shown topics generated using the min combiner.",4.1 Interface and User Study,[0],[0]
"However, this does not change our result.
",4.1 Interface and User Study,[0],[0]
the topic word lists (right column) to become an anchor word.,4.1 Interface and User Study,[0],[0]
Users can also add additional topics by clicking the “Add Anchor” to create additional anchors.,4.1 Interface and User Study,[0],[0]
"If the user wants to add a word to a tandem anchor set that does not appear in the interface, they manually type the word (restricted to the model’s vocabulary).",4.1 Interface and User Study,[0],[0]
"When the user wants to see the updated topics for their newly refined anchors, they click “Update Topics”.
",4.1 Interface and User Study,[0],[0]
We give each a participant a high level overview of topic modeling.,4.1 Interface and User Study,[0],[0]
"We also describe common problems with topic models including intruding topic words, duplicate topics, and ambiguous topics.",4.1 Interface and User Study,[0],[0]
Users are instructed to use their best judgement to determine if topics are useful.,4.1 Interface and User Study,[0],[0]
The task is to edit the anchor words to improve the topics.,4.1 Interface and User Study,[0],[0]
"We asked that users spend at least twenty minutes, but no more than thirty minutes.",4.1 Interface and User Study,[0],[0]
"We repeat the task twice: once with tandem anchors, and once with single word anchors.8",4.1 Interface and User Study,[0],[0]
"We now validate our main result that for interactive topic modeling, tandem anchors yields better topics than single word anchors.",4.2 Quantitative Results,[0],[0]
"Like our titlebased experiments in Section 3, topics generated from users become features to train and test a classifier for the 20NEWS dataset.",4.2 Quantitative Results,[0],[0]
We choose this dataset for easier comparison with the Interactive Topic Modeling result of Hu et al. (2014).,4.2 Quantitative Results,[0],[0]
"Basedsie on our results with title-based anchors, we use the harmonic mean combiner in our analysis.",4.2 Quantitative Results,[0],[0]
"As before, we report not only accuracy, but also multiple clustering metrics using the confusion matrix from the classification task.",4.2 Quantitative Results,[0],[0]
"Finally, we report topic coherence.
",4.2 Quantitative Results,[0],[0]
Figure 3 summarizes the results of our quantitative evaluation.,4.2 Quantitative Results,[0],[0]
"While we only compare user generated anchors in our analysis, we include the unsupervised Gram-Schmidt anchors as a baseline.",4.2 Quantitative Results,[0],[0]
Some of the data violate assumptions of normality.,4.2 Quantitative Results,[0],[0]
"Therefore, we use Wilcoxon’s signed-rank test (Wilcoxon, 1945) to determine if the differences between multiword anchors and single word anchors are significant.
Topics from user generated multiword anchors yield higher classification accuracy (Figure 3).",4.2 Quantitative Results,[0],[0]
"Not only is our approach more scalable than the Interactive Topic Model, but we also achieve
8The order in which users complete these tasks is counterbalanced.
higher classification accuracy than Hu et al. (2014).9 Tandem anchors also improve clustering metrics.10
While user selected tandem anchors produce better classification features than single word anchors, users selected single word anchors produce topics with similar topic coherence scores.11
To understand this phenomenon, we use quality metrics (AlSumait et al., 2009) for ranking topics by their correspondence to genuine themes in the data.",4.2 Quantitative Results,[0],[0]
"Significant topics are likely skewed towards a few related words, so we measure the distance of each topic-word distribution from the uniform distribution over words.",4.2 Quantitative Results,[0],[0]
"Topics which are close to the underlying word distribution of the entire data are likely to be vacuous, so we also measure the distance of each topic-word distribution from the underlying word distribution.",4.2 Quantitative Results,[0],[0]
"Finally, background topics are likely to appear in a wide range of documents, while meaningful topics will appear in a smaller subset of the data.
",4.2 Quantitative Results,[0],[0]
Figure 4 reports our topic significance findings.,4.2 Quantitative Results,[0],[0]
"For all three significance metrics, multiword anchors produce more significant topics than single word anchors.10 Topic coherence is based solely on the top n words of a topic, while both accuracy and topic significance depend on the entire topicword distributions.",4.2 Quantitative Results,[0],[0]
"With single word anchors, topics with good coherence may still be too general.",4.2 Quantitative Results,[0],[0]
Tandem anchors enables users to produce topics with more specific word distributions which are better features for classification.,4.2 Quantitative Results,[0],[0]
We examine the qualitative differences between how users select multiword anchor facets versus single word anchors.,4.3 Qualitative Results,[0],[0]
Table 2 gives examples of topics generated using different anchor strategies.,4.3 Qualitative Results,[0],[0]
"In a follow-up survey with our users, 75% find it easier to affect individual changes in the topics using tandem anchors compared to single word anchors.",4.3 Qualitative Results,[0],[0]
"Users who prefer editing multiword anchors over single word anchors often report that
9However, the values are not strictly comparable, as Hu et al. (2014) use the standard chronological test/train fold, and we use random splits.
10Significant at p < 0.01 when using Wilcoxon’s signedrank test.
",4.3 Qualitative Results,[0],[0]
"11The difference between coherence scores was not statistically significant using Wilcoxon’s signed-rank test.
",4.3 Qualitative Results,[0],[0]
multiword anchors make it easier to merge similar topics into a single focused topic by combining anchors.,4.3 Qualitative Results,[0],[0]
"For example, by combining multiple words related to Christianity, users were able to create a topic which is highly specific, and differentiated from general religion themes which included terms about Atheism and Judaism.
",4.3 Qualitative Results,[0],[0]
"While users find that use tandem anchors is easier, only 55% of our users say that they prefer the final topics produced by tandem anchors compared to single word anchors.",4.3 Qualitative Results,[0],[0]
"This is in harmony with our quantitative measurements of topic coherence, and may be the result of our stopping criteria: when users judged the topics to be useful.
",4.3 Qualitative Results,[0],[0]
"However, 100% of our users feel that the topics created through interaction were better than those generated from Gram-Schmidt anchors.",4.3 Qualitative Results,[0],[0]
"This was true regardless of whether we used tandem anchors or single word anchors.
",4.3 Qualitative Results,[0],[0]
Our participants also produce fewer topics when using multiword anchors.,4.3 Qualitative Results,[0],[0]
The mean difference between topics under single word anchors and multiple word anchors is 9.35.,4.3 Qualitative Results,[0],[0]
"In follow up interviews, participants indicate that the easiest way to resolve an ambiguous topic with single word anchors was to create a new anchor for each of the ambiguous terms, thus explaining the proliferation of topics for single word anchors.",4.3 Qualitative Results,[0],[0]
"In contrast, fixing an ambiguous tandem anchor is simple: users just add more terms to the anchor facet.",4.3 Qualitative Results,[0],[0]
Tandem anchors extend the anchor words algorithm to allow multiple words to be combined into anchor facets.,5 Conclusion,[0],[0]
"For interactive topic modeling, using anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use.",5 Conclusion,[0],[0]
"Furthermore, our approach scales much better than existing interactive topic modeling techniques, allowing interactivity on large
datasets for which interactivity was previous impossible.",5 Conclusion,[0],[0]
This work was supported by the collaborative NSF Grant IIS-1409287 (UMD) and,Acknowledgements,[0],[0]
IIS- 1409739 (BYU).,Acknowledgements,[0],[0]
Boyd-Graber is also supported by NSF grants IIS-1320538 and NCSE-1422492.,Acknowledgements,[0],[0]
Interactive topic models are powerful tools for understanding large collections of text.,abstractText,[0],[0]
"However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets.",abstractText,[0],[0]
"Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for userfacing applications.",abstractText,[0],[0]
"We propose combinations of words as anchors, going beyond existing single word anchor algorithms— an approach we call “Tandem Anchors”.",abstractText,[0],[0]
We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and noninteractive approaches.,abstractText,[0],[0]
Tandem anchors are faster and more intuitive than existing interactive approaches.,abstractText,[0],[0]
"Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation.",abstractText,[0],[0]
"In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008).",abstractText,[0],[0]
"However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012).",abstractText,[0],[0]
"Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated topics, making topic models less of a “take it or leave it” proposition.",abstractText,[0],[0]
"Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis.",abstractText,[0],[0]
"The downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009).",abstractText,[0],[0]
We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models.,abstractText,[0],[0]
"The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1).",abstractText,[0],[0]
"This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections.",abstractText,[0],[0]
A drawback of the anchor method is that anchor words—words that have high probability of being in a single topic—are not intuitive.,abstractText,[0],[0]
We extend the anchor algorithm to use multiple anchor words in tandem (Section 2).,abstractText,[0],[0]
"Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive.",abstractText,[0],[0]
"For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3).",abstractText,[0],[0]
Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4).,abstractText,[0],[0]
"Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.",abstractText,[0],[0]
Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,title,[0],[0]
"Applications using machine learning techniques have exploded during the recent years, with “deep learning” techniques being applied on a wide variety of tasks that had hitherto proved challenging.",1. Introduction,[0],[0]
"Training highly accurate machine learning models requires large quantities of (high quality) data, technical expertise and computational resources.",1. Introduction,[0],[0]
"An important recent paradigm is prediction as a service, whereby a service provider with expertise and resources can make predictions for clients.",1. Introduction,[0],[0]
"However, this approach requires trust between service provider and client; there are several instances where clients may be unwilling or unable to provide data to service providers due to privacy
1University of Oxford, Oxford, UK 2The Alan Turing Institute, London, UK 3University of Warwick, Coventry, UK.",1. Introduction,[0],[0]
"Correspondence to: Amartya Sanyal <amartya.sanyal@cs.ox.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
concerns.",1. Introduction,[0],[0]
"Examples include assisting in medical diagnoses (Kononenko, 2001; Blecker et al., 2017), detecting fraud from personal finance data (Ghosh & Reilly, 1994), and detecting online communities from user data (Fortunato, 2010).",1. Introduction,[0],[0]
"The ability of a service provider to predict on encrypted data can alleviate concerns of data leakage.
",1. Introduction,[0],[0]
The framework of fully homomorphic encryption (FHE) is ideal for this paradigm.,1. Introduction,[0],[0]
Fully homomorphic encryption schemes support arbitrary computations to be performed directly on encrypted data without prior decryption.,1. Introduction,[0],[0]
"The first fully homomorphic encryption system was developed just 10 years ago by Gentry (2009), after being an open question for 30 years (Rivest et al., 1978).",1. Introduction,[0],[0]
"Since then several other schemes have been proposed (Gentry et al., 2012; 2013; Brakerski & Vaikuntanathan, 2014; Ducas & Micciancio, 2015; Chillotti et al., 2016).",1. Introduction,[0],[0]
"However, without significant changes to machine learning models and improved algorithmic tools, homomorphic encryption does not scale to real-world machine learning applications.
",1. Introduction,[0],[0]
"Indeed, already there have been several recent works trying to accelerate predictions of machine learning models on fully homomorphic encrypted data.",1. Introduction,[0],[0]
"In general, the approach has been to approximate all or parts of a machine learning model to accommodate the restrictions of an FHE framework.",1. Introduction,[0],[0]
"Often, certain kind of FHE schemes are preferred because they allow for “batched” parallel encrypted computations, called SIMD operations (Smart & Vercauteren, 2014).",1. Introduction,[0],[0]
"This technique is exemplified by the CryptoNets model (Gilad-Bachrach et al., 2016).",1. Introduction,[0],[0]
"While these models allow for high-throughput (via SIMD), they are not particularly suited for the prediction as a service framework for individual users, as single predictions are slow.",1. Introduction,[0],[0]
"Further, because they employ a leveled homomorphic encryption scheme, they are unable to perform many nested multiplications, a requirement for state-of-the-art deep learning models (He et al., 2016; Huang et al., 2017).
",1. Introduction,[0],[0]
"Our solution demonstrates that existing work on Binary Neural Networks (BNNs) (Kim & Smaragdis, 2015; Courbariaux et al., 2016) can be adapted to produce efficient and highly accurate predictions on encrypted data.",1. Introduction,[0],[0]
"We show that a recent FHE encryption scheme (Chillotti et al., 2016) which only supports operations on binary data can be leveraged to compute all of the operations of BNNs.",1. Introduction,[0],[0]
"To do so,
we develop specialized circuits for fully-connected, convolutional, and batch normalization layers (Ioffe & Szegedy, 2015).",1. Introduction,[0],[0]
"Additionally we design tricks to sparsify encrypted computation that reduce computation time even further.
",1. Introduction,[0],[0]
Most similar to our work is Bourse et al. (2017) who use neural networks with signed integer weights and binary activations to perform encrypted prediction.,1. Introduction,[0],[0]
"However, this model is only evaluated on MNIST, with modest accuracy results, and the encryption scheme parameters depend on the structure of the model, potentially requiring clients to re-encrypt their data if the service provider updates their model.",1. Introduction,[0],[0]
"Our framework allows the service provider to update their model at anytime, and allows one to use binary neural networks of Courbariaux et al. (2016) which, in particular, achieve high accuracy on MNIST (99.04%).",1. Introduction,[0],[0]
Another closely related work is Meehan et al. (2018) who design encrypted adder and multiplier circuits so that they can implement machine learning models on integers.,1. Introduction,[0],[0]
"This can be seen as complementary to our work on binary networks: while they achieve improved accuracy because of greater precision, they are less efficient than our methods (however on MNIST we achieve the same accuracy with a 29× speedup, via our sparsification and parallelization tricks).
",1. Introduction,[0],[0]
Private training.,1. Introduction,[0],[0]
"In this work, we do not address the question of training machine learning models with encrypted data.",1. Introduction,[0],[0]
"There has been some recent work in this area (Hardy et al., 2017; Aono et al., 2017).",1. Introduction,[0],[0]
"However, as of now it appears possible only to train very small models using fully homomorphic encryption.",1. Introduction,[0],[0]
We leave this for future work.,1. Introduction,[0],[0]
"In this work, our focus is on achieving speed-ups when using complex models on fully homomorphic encrypted data.",1.1. Our contributions,[0],[0]
"In order to achieve these speed-ups, we propose several methods to modify the training and design of neural networks, as well as algorithmic tricks to parallelize and accelerate computation on encrypted data:
• We propose two types of circuits for performing inner products between unencrypted and encrypted data: reduce tree circuits and sorting networks.",1.1. Our contributions,[0],[0]
"We give a runtime comparison of each method.
",1.1. Our contributions,[0],[0]
"• We introduce an easy trick, which we call the +1 trick to sparsify encrypted computations.
",1.1. Our contributions,[0],[0]
"• We demonstrate that our techniques are easily parallelizable and we report timing for a variety of computation settings on real world datasets, alongside classification accuracies.",1.1. Our contributions,[0],[0]
In this section we describe our Encrypted Prediction as a Service (EPAAS) paradigm.,2. Encrypted Prediction as a Service,[0],[0]
We then detail our privacy and computational guarantees.,2. Encrypted Prediction as a Service,[0],[0]
"Finally, we discuss how different related work is suited to this paradigm and propose a solution.
",2. Encrypted Prediction as a Service,[0],[0]
"In the EPAAS setting we have any number of clients, say C1, . . .",2. Encrypted Prediction as a Service,[0],[0]
", Cn that have data x1, . . .",2. Encrypted Prediction as a Service,[0],[0]
",xn.",2. Encrypted Prediction as a Service,[0],[0]
The clients would like to use a highly-accurate model f provided by a server S to predict some outcome.,2. Encrypted Prediction as a Service,[0],[0]
"In cases where data x is not sensitive there are already many solutions for this such as BigML, Wise.io, Google Cloud AI, Amazon Machine Learning, among others.",2. Encrypted Prediction as a Service,[0],[0]
"However, if the data is sensitive so that the clients would be uncomfortable giving the raw data to the server, none of these systems can offer the client a prediction.",2. Encrypted Prediction as a Service,[0],[0]
"If data x is sensitive (e.g., x may be the health record of client C, and f(x) may be the likelihood of heart disease), then we would like to have the following privacy guarantees:
P1.",2.1. Privacy and computational guarantees,[0],[0]
"Neither the server S, or any other party, learn anything about client data x, other than its size (privacy of the data).
P2.",2.1. Privacy and computational guarantees,[0],[0]
"Neither the client C, or any other party, learn anything about model f , other than the prediction f(x) given client data x (and whatever can be deduced from it) (privacy of the model).
",2.1. Privacy and computational guarantees,[0],[0]
"Further, the main attraction of EPAAS is that the client is involved as little as possible.",2.1. Privacy and computational guarantees,[0],[0]
"More concretely, we wish to have the following computational guarantees:
C1.",2.1. Privacy and computational guarantees,[0],[0]
"No external party is involved in the computation.
",2.1. Privacy and computational guarantees,[0],[0]
C2.,2.1. Privacy and computational guarantees,[0],[0]
"The rounds of communication between client and server should be limited to 2 (send data & receive prediction).
",2.1. Privacy and computational guarantees,[0],[0]
C3.,2.1. Privacy and computational guarantees,[0],[0]
Communication and computation at the client side should be independent of model f .,2.1. Privacy and computational guarantees,[0],[0]
"In particular, (i) the server should be able to update f without communicating with any client, and (ii) clients should not need to be online during the computation of f(x).
",2.1. Privacy and computational guarantees,[0],[0]
Note that these requirements rule out protocols with preprocessing stages or that involve third parties.,2.1. Privacy and computational guarantees,[0],[0]
"Generally speaking, a satisfactory solution based on FHE would proceed as follows: (1) a client generates encryption parameters, encrypts their data x using the private key, and sends the resulting encryption x̃, as well as the public key to the server.",2.1. Privacy and computational guarantees,[0],[0]
"(2) The server evaluates f on x̃ leveraging the homomorphic properties of the encryption, to obtain an encryption f̃(x) without learning anything whatsoever about x, and sends f̃(x) to the client.",2.1. Privacy and computational guarantees,[0],[0]
"(3) Finally, the client decrypts and recovers the prediction f(x) in the clear.",2.1. Privacy and computational guarantees,[0],[0]
A high level depiction of these steps is shown in Figure 1.,2.1. Privacy and computational guarantees,[0],[0]
Table 1 describes whether prior work satisfy the above privacy and computational guarantees.,2.2. Existing approaches,[0],[0]
"First, note that Cryptonets (Gilad-Bachrach et al., 2016) violates C3(i) and P2.",2.2. Existing approaches,[0],[0]
"This is because the clients would have to generate parameters for the encryption according to the structure of f , so we are able to make inferences about the model (violating P2) and the client is not allowed to change the model f without telling the client (violating C3(i)).",2.2. Existing approaches,[0],[0]
The same holds for the work of Chabanne et al. (2017).,2.2. Existing approaches,[0],[0]
"The approach of Bourse et al. (2017) requires the server to calibrate the parameters of the encryption scheme according to the magnitude of intermediate values, thus C3(i) is not necessarily satisfied.",2.2. Existing approaches,[0],[0]
"Closely related to our work is that of Meehan et al. (2018)
which satisfies our privacy and computational requirements.",2.2. Existing approaches,[0],[0]
"We will show that our method is significantly faster than this method, with very little sacrifice in accuracy.
",2.2. Existing approaches,[0],[0]
Multi-Party Computation (MPC).,2.2. Existing approaches,[0],[0]
"It is important to distinguish between approaches purely based on homomorphic encryption (described above), and those involving MultiParty Computation (MPC) techniques, such as (Mohassel & Zhang, 2017; Liu et al., 2017; Rouhani et al., 2017; Riazi et al., 2017; Chase et al.;",2.2. Existing approaches,[0],[0]
"Juvekar et al., 2018).",2.2. Existing approaches,[0],[0]
"While generally MPC approaches are faster, they crucially rely on all parties being involved in the whole computation, which is in conflict with requirement C3(ii).",2.2. Existing approaches,[0],[0]
"Additionally, in MPC the structure of the computation is public to both parties, which means that the server would have to communicate basic information such as the number of layers of f .",2.2. Existing approaches,[0],[0]
"This is conflict with requirements P1, C2, and C3(i).
",2.2. Existing approaches,[0],[0]
"In this work, we propose to use a very tailored homomorphic encryption technique to guarantee all privacy and computational requirements.",2.2. Existing approaches,[0],[0]
In the next section we give background on homomorphic encryption.,2.2. Existing approaches,[0],[0]
"Further, we motivate the encryption protocol and the machine learning model class we use to satisfy all guarantees.",2.2. Existing approaches,[0],[0]
All cryptosystems define two functions: 1. an encryption function E(·) that maps data (often called plaintexts) to encrypted data (ciphertexts); 2.,3. Background,[0],[0]
a decryption function D(·) that maps ciphertexts back to plaintexts.,3. Background,[0],[0]
"In public-key cryptosystems, to evaluate the encryption function E , one needs to hold a public key kPUB, so the encryption of data x is E(x, kPUB).",3. Background,[0],[0]
"Similarly, to compute the decryption function D(·) one needs to hold a secret key kSEC which allows us to recover: D(E(x, kPUB), kSEC)",3. Background,[0],[0]
"= x.
A cryptosystem is homomorphic in some operation if it is possible to perform another (possibly different) operation such that: E(x, kPUB) E(x, kPUB) = E(x y, kPUB).",3. Background,[0],[0]
"Finally, in this work we assume all data to be binary ∈ {0, 1}.",3. Background,[0],[0]
"For more detailed background on FHE beyond what is described below, see the excellent tutorial of Halevi (2017).",3. Background,[0],[0]
"In 1978, cryptographers posed the question: Does an encryption scheme exist that allows one to perform arbitrary computations on encrypted data?",3.1. Fully Homomorphic Encryption,[0],[0]
"The implications of this, called a Fully homomorphic encryption (FHE) scheme, would enable clients to send computations to the cloud while retaining control over the secrecy of their data.",3.1. Fully Homomorphic Encryption,[0],[0]
This was still an open problem however 30 years later.,3.1. Fully Homomorphic Encryption,[0],[0]
"Then, in 2009, a cryptosystem (Gentry, 2009) was devised that could, in principle, perform such computations on encrypted data.",3.1. Fully Homomorphic Encryption,[0],[0]
"Similar to previous approaches, in each computation, noise is introduced into the encrypted data.",3.1. Fully Homomorphic Encryption,[0],[0]
"And after a certain number of computations, the noise grows too large so that the encryptions can no longer be decrypted.",3.1. Fully Homomorphic Encryption,[0],[0]
"The key innovation was a technique called bootstrapping, which allows one to reduce the noise to its original level without decrypting.",3.1. Fully Homomorphic Encryption,[0],[0]
"That result constituted a massive breakthrough, as it established, for the first time, a fully homomorphic encryption scheme (Gentry, 2009).",3.1. Fully Homomorphic Encryption,[0],[0]
"Unfortunately, the original bootstrapping procedure was highly impractical.",3.1. Fully Homomorphic Encryption,[0],[0]
"Consequently, much of the research since the first FHE scheme has been devoted to reducing the growth of noise so that the scheme never has to perform bootstrapping.",3.1. Fully Homomorphic Encryption,[0],[0]
"Indeed, even in recent FHE schemes bootstrapping is slow (roughly six minutes in a highly-optimized implementation of a recent popular scheme (Halevi & Shoup, 2015)) and bootstrapping many times increases the memory requirements of encrypted data.",3.1. Fully Homomorphic Encryption,[0],[0]
"Thus, one common technique to implement encrypted prediction was to take an existing ML algorithm and approximate it with as few operations as possible, in order to never have to bootstrap.",3.1.1. ENCRYPTED PREDICTION WITH LEVELED HE,[0],[0]
"This involved careful parameter tuning to ensure that the security of the encryption scheme was sufficient, that it didn’t require too much memory, and that it ran in a reasonable amount of time.",3.1.1. ENCRYPTED PREDICTION WITH LEVELED HE,[0],[0]
"One prominent example of this is Cryptonets (Gilad-Bachrach et al., 2016).",3.1.1. ENCRYPTED PREDICTION WITH LEVELED HE,[0],[0]
Recent developments in cryptography call for rethinking this approach.,3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
Ducas & Micciancio (2015) devised a scheme that that could bootstrap a single Boolean gate in under one second with reduced memory.,3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"Recently, Chillotti et al. (2016) introduced optimizations implemented in the TFHE library, which further reduced bootstrapping of to under 0.1 seconds.",3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"In this paper, we demonstrate that this change has a huge impact on designing encrypted machine learning algorithms.",3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"Specifically, encrypted computation is now modular: the cost of adding a few layers to an encrypted neural network is simply the added cost of each layer in isolation.",3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"This is particularly important as recent developments in deep learning such as Residual Networks (He
et al., 2016) and Dense Networks (Huang et al., 2017) have shown that networks with many layers are crucial to achieve state-of-the-art accuracy.",3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"The cryptosystem that we will use in this paper, TFHE, is however restricted to computing binary operations.",3.2. Binary Neural Networks,[0],[0]
"We note that, concurrent to the work that led to TFHE, was the development of neural network models that perform binary operations between binary weights and binary activations.",3.2. Binary Neural Networks,[0],[0]
"These models, called Binary Neural Networks (BNNs), were first devised by Kim & Smaragdis (2015); Courbariaux et al. (2016), and were motivated by the prospect of training and testing deep models on limited memory and limited compute devices, such as mobile phones.
",3.2. Binary Neural Networks,[0],[0]
Technical details.,3.2. Binary Neural Networks,[0],[0]
We now describe the technical details of binary networks that we will aim to replicate on encrypted data.,3.2. Binary Neural Networks,[0],[0]
"In a Binary Neural Network (BNN) every layer maps a binary input x ∈ {−1, 1}d to a binary output z ∈ {−1, 1}p using a set of binary weights W ∈ {−1, 1}(p,d) and a binary activation function sign(·) that is 1 if x ≥ 0 and −1 otherwise.",3.2. Binary Neural Networks,[0],[0]
"Although binary nets don’t typically use a bias term, applying batch-normalization (Ioffe & Szegedy, 2015) when evaluating the model it means that a bias term b ∈",3.2. Binary Neural Networks,[0],[0]
Zp may need to be added before applying the activation function (cf. Sec. 4.1.2).,3.2. Binary Neural Networks,[0],[0]
"Thus, when evaluating the model, a fully connected layer in a BNN implements the following transformation z",3.2. Binary Neural Networks,[0],[0]
:= sign(Wx + b).,3.2. Binary Neural Networks,[0],[0]
"From now on we will call all data represented as {−1, 1} non-standard binary and data represented as {0, 1} as binary.",3.2. Binary Neural Networks,[0],[0]
"Kim & Smaragdis (2015); Courbariaux et al. (2016) were the first to note that the above inner product nonlinearity in BNNs could be implemented using the following steps:
1.",3.2. Binary Neural Networks,[0],[0]
"Transform data and weights from non-standard binary to binary: w,x→ w,x by replacing −1 with 0. n
2.",3.2. Binary Neural Networks,[0],[0]
"Element-wise multiply by applying the logical operator XNOR(w,x) for each element of w and x.
3.",3.2. Binary Neural Networks,[0],[0]
"Sum result of previous step by using popcount operation (which counts the number of 1s), call this S.
4.",3.2. Binary Neural Networks,[0],[0]
"If the bias term is b, check if 2S ≥",3.2. Binary Neural Networks,[0],[0]
d,3.2. Binary Neural Networks,[0],[0]
"− b, if so the activation is positive and return 1, otherwise return −1.
",3.2. Binary Neural Networks,[0],[0]
"Thus we have that,
zi = sign(2 · popcount(XNOR(wi,x))− d + b)
",3.2. Binary Neural Networks,[0],[0]
Related binary models.,3.2. Binary Neural Networks,[0],[0]
"Since the initial work on BNNs there has been a wealth of work on binarizing, ternarizing, and quantizing neural networks Chen et al. (2015); Courbariaux et al. (2015); Han et al. (2016); Hubara et al. (2016);
Zhu et al. (2016); Chabanne et al. (2017); Chen et al. (2017).",3.2. Binary Neural Networks,[0],[0]
Our approach is currently tailored to methods that have binary activations and we leave the implementation of these methods on encrypted data for future work.,3.2. Binary Neural Networks,[0],[0]
"In this work, we make the observation that BNNs can be run on encrypted data by designing circuits in TFHE for computing their operations.",4. Methods,[0],[0]
In this section we consider Boolean circuits that operate on encrypted data and unencrypted weights and biases.,4. Methods,[0],[0]
"We show how these circuits allow us to efficiently implement the three main layers of binary neural networks: fully connected, convolutional, and batch-normalization.",4. Methods,[0],[0]
We then show how a simple trick allows us to sparsify our computations.,4. Methods,[0],[0]
Our techniques can be easily parallelized.,4. Methods,[0],[0]
"During the evaluation of a circuit, gates at the same level in the tree representation of the circuit can be evaluated in parallel.",4. Methods,[0],[0]
"Hence, when implementing a function, “shallow” circuits are preferred in terms of parallelization.",4. Methods,[0],[0]
While parallel computation was often used to justify employing second generation FHE techniques— where parallelization comes from ciphertext packing—we show in the following section that our techniques create dramatic speedups for a state-of-the-art FHE technique.,4. Methods,[0],[0]
We emphasize that a key challenge is that we need to use data oblivious algorithms (circuits) when dealing with encrypted data as the algorithm never discovers the actual value of any query made on the data.,4. Methods,[0],[0]
The three primary circuits we need are for the following tasks: 1. computing the inner product; 2. computing the binary activation function (described in the previous section) and; 3. dealing with the bias.,4.1. Binary OPs,[0],[0]
"As described in the previous section, BNNs can speed up an inner product by computing XNORs (for element-wise multiplication) followed by a POPCOUNT (for summing).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"In our case, we compute an inner product of size d by computing XNORs element-wise between d bits of encrypted data and d bits of unencrypted data, which results in an encrypted d bit output.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"To sum this output, the POPCOUNT operation is useful when weights and data are unencrypted because POPCOUNT is implemented in the instruction set of Intel and AMD processors, but when dealing with encrypted data we simply resort to using shallow circuits.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"We consider two circuits for summation, both with sublinear depth: a reduce tree adder and a sorting network.
",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
Reduce tree adder.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"We implement the sum using a binary tree of half and ripple-carry (RC) adders organized into a reduction tree, as shown in Figure 2 (Left).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"All these structures can be implemented to run on encrypted data because TFHE allows us to compute XNOR, AND, and OR on encrypted data.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"The final number returned by the reduction tree S̃ is the binary representation of the number of 1s resulting from the XNOR, just like POPCOUNT.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Thus, to compute the BNN activation function sign(·) we need to check whether 2S̃ ≥ d− b, where d is the number of bits in S̃ and b is the bias.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
Note that if the bias is zero we simply need to check if S̃ ≥ d/2.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
To do so we can simply return the second-to-last bit of S̃. If it is 1 then S̃ is at least d/2.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"If the bias b is non-zero (because of batch-normalization, described in Section 4.1.2), we can implement a circuit to perform the check 2S̃ ≥ d",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
− b.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"The bias b (which is available in the clear) may be an integer as large as S̃. Let B[(d − b)/2], B[S̃] be the binary representations of b and S̃. Algorithm 1 describes a comparator circuit that returns an encrypted value of 1 if the above condition holds and (encrypted) 0",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"otherwise (where MUX(s, a, b) returns a if s = 1 and b otherwise).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"As encrypted operations dominate
the running time of our computation, in practice this computation essentially corresponds to evaluating d MUX gates.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"This gate has a dedicated implementation in TFHE, which results in a very efficient comparator in our setting.
",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Algorithm 1 Comparator Inputs: Encrypted B[S̃], unencrypted B[(d− b)/2], size d of B[(d− b)/2],B[S̃]",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Output: Result of 2S̃ ≥ d− b
1: o = 0 2: for i = 1, . . .",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
", d do 3: if B[(d− b)/2]i = 0",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"then 4: o = MUX(B[S̃]i, 1̃, o) 5: else 6: o = MUX(B[S̃]i, o, 0̃) 7: end if 8: end for 9: Return: o
Sorting network.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
We do not technically care about the sum of the result of the element-wise XNOR between w̄ and x̄.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"In fact, all we care about is if the result of the comparison: 2S̃ ≥ d",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
− b.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Thus, another idea is to take the output of the (bitwise) XNOR and sort it.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Although this sorting needs to be performed over encrypted data, the rest of the computation does not require any homomorphic operations; after sorting we hold a sequence of encrypted 1s, followed by encrypted 0s.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"To output the correct value, we only need to select one the (encrypted) bit in the correct position and return it.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"If b = 0 we can simply return the encryption of the central bit in the sequence; indeed, if the central bit is 1, then there are more 1s than 0s and thus 2S̃ ≥ d",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
and we return 1.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
If b 6= 0,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
we need to offset the returned index by b in the correct direction depending on the sign of b.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"In order to sort the initial array we implement a sorting network, shown in Figure 2 (Right).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"The sorting network is a sequence of swap gates between individuals bits, where SWAP(a, b) =",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"(OR(a, b), AND(a, b)).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Note that if a ≥ b then SWAP(a, b) =",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"(a, b), and otherwise is (b, a).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"More specifically, we implement Batcher’s sorting network (Batcher, 1968), which consists of O(n log2(n)) swap gates, and has depth O(log2(n)).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
Batch normalization is mainly used during training; however during evaluating a model this requires us scale and translate and scale the input (which is the output of the previous layer).,4.1.2. BATCH NORMALIZATION,[0],[0]
"In practice, when our activation function is the sign function, this only means that we need to update the bias term (the actual change to the bias term is an elementary calculation).",4.1.2. BATCH NORMALIZATION,[0],[0]
"As our circuits are designed to work with a bias term, and the scaling and translation factors are available as
plaintext (as they are part of the model), this operation is easily implemented during test time.",4.1.2. BATCH NORMALIZATION,[0],[0]
"Since we have access to W ∈ {−1, 1}p×d and the bias term b ∈ Zp in the clear (only data x and subsequent activations are encrypted), we can exploit the fact that W always has values±1 to roughly halve the cost computation.",4.2. Sparsification via “+1”-trick,[0],[0]
"We consider w ∈ {−1, 1}d which is a single row of W and observe that:
w>x = (1 + w)>(1 + x)− ∑ i wi − (1 + x)>1,
where 1 denotes the vector in which every entry is 1.",4.2. Sparsification via “+1”-trick,[0],[0]
"Further note that (1 + w) ∈ {0, 2}d which means that the product (1+w)>(1+x) is simply the quantity 4 ∑ i:wi=1
x̄i, where x̄ refers to the standard binary representation of the nonstandard binary x.",4.2. Sparsification via “+1”-trick,[0],[0]
"Assuming at most half of the wis were originally +1, if w ∈ {−1, 1}d, only d/2 encrypted values need be added.",4.2. Sparsification via “+1”-trick,[0],[0]
"We also need to compute the encrypted sum ∑ i xi; however, this latter sum need only be computed once, no matter how many output units the layer has.",4.2. Sparsification via “+1”-trick,[0],[0]
"Thus, this small bit of extra overhead roughly halves the amount of computation required.",4.2. Sparsification via “+1”-trick,[0],[0]
"We note that if w has more −1s than +1s, w>x can be computed using (1−w) and (1− x) instead.",4.2. Sparsification via “+1”-trick,[0],[0]
This guarantees that we never need to sum more than half the inputs for any output unit.,4.2. Sparsification via “+1”-trick,[0],[0]
The sums of encrypted binary values can be calculated as described in Sec. 4.1.,4.2. Sparsification via “+1”-trick,[0],[0]
"The overheads are two additions required to compute (1+x)>1 and (1 − x)>1, and then a subtraction of two log(d)-bit long encrypted numbers.",4.2. Sparsification via “+1”-trick,[0],[0]
"(The multiplication by 2 or 4 as may be sometimes required is essentially free, as bit shifts correspond to dropping bits, and hence do not require homomorphic operations).",4.2. Sparsification via “+1”-trick,[0],[0]
"As our experimental results show this simple trick roughly halves the computation time of one
layer; the actual savings appear to be even more than half as in many instances the number of elements we need to sum over is significantly smaller than half.
",4.2. Sparsification via “+1”-trick,[0],[0]
It is worth emphasizing the advantage for binarizing and then using the above approach to making the sums sparse.,4.2. Sparsification via “+1”-trick,[0],[0]
"By default, units in a neural network compute an affine function to which an activation function is subsequently applied.",4.2. Sparsification via “+1”-trick,[0],[0]
The affine map involves an inner product which involves d multiplications.,4.2. Sparsification via “+1”-trick,[0],[0]
Multiplication under fully homomorphic encryption schemes is however significantly more expensive than addition.,4.2. Sparsification via “+1”-trick,[0],[0]
"By binarizing and applying the above calculation, we’ve replaced the inner product operation by selection (which is done in the clear as W is available in plaintext) and (encrypted) addition.",4.2. Sparsification via “+1”-trick,[0],[0]
"Ternary neural networks use weights in {−1, 0, 1} rather than {−1, 1}; this can alternatively be viewed as dropping connections from a BNN.",4.3. Ternarization (Weight Dropping),[0],[0]
"Using ternary neural networks rather than binary reduces the computation time as encrypted inputs for which the corresponding wi is 0 can be safely dropped from the computation, before the method explained in section 4.2 is applied to the remaining elements.",4.3. Ternarization (Weight Dropping),[0],[0]
Our experimental results show that a binary network can be ternarized to maintain the same level of test accuracy with roughly a quarter of the weights being 0,4.3. Ternarization (Weight Dropping),[0],[0]
(cf. Sec. 5.4).,4.3. Ternarization (Weight Dropping),[0],[0]
In this section we report encrypted binary neural network prediction experiments on a number of real-world datasets.,5. Experimental Results,[0],[0]
"We begin by comparing the efficiency of the two circuits used for inner product, the reduce tree and the sorting network.",5. Experimental Results,[0],[0]
We then describe the datasets and the architecture of the BNNs used for classification.,5. Experimental Results,[0],[0]
"We report the classification timings of these BNNs for each dataset, for different computational settings.",5. Experimental Results,[0],[0]
"Finally, we give accuracies of the BNNs compared to floating point networks.",5. Experimental Results,[0],[0]
"Our code is freely available at (tap, 2018).",5. Experimental Results,[0],[0]
"We show timings of reduce tree and sorting network for different number of input bits, with and without parallelization in Figure 3 (parallelization is over 16 CPUs).",5.1. Reduce tree vs. sorting network,[0],[0]
We notice that the reduce tree is strictly better when comparing parallel or non-parallel timings of the circuits.,5.1. Reduce tree vs. sorting network,[0],[0]
"As such, from now on we use the reduce tree circuit for inner product.
",5.1. Reduce tree vs. sorting network,[0],[0]
"It should be mentioned that at the outset this result was not obvious because while sorting networks have more levels of computation, they have fewer gates.",5.1. Reduce tree vs. sorting network,[0],[0]
"Specifically, the sorting network used for encrypted sorting is the bitonic sorting network which for n bits has O(log2 n) levels of computa-
tion whereas the reduce tree only has O(log n) levels.",5.1. Reduce tree vs. sorting network,[0],[0]
"On the other hand, the reduce tree requires 2 gates for each half adder and 5k gates for each k-bit RC adder, whereas a sorting network only requires 2 gates per SWAP operation.",5.1. Reduce tree vs. sorting network,[0],[0]
"Another factor that may slow down sorting networks is that is that our implementation of sorting networks is recursive, whereas the reduce tree is iterative.",5.1. Reduce tree vs. sorting network,[0],[0]
"We evaluate on four datasets, three of which have privacy implications due to health care information (datasets Cancer and Diabetes) or applications in surveillance (dataset Faces).",5.2. Datasets,[0],[0]
"We also evaluate on the standard benchmark MNIST dataset.
Cancer.",5.2. Datasets,[0],[0]
The Cancer dataset1 contains 569 data points where each point has 30 real-valued features.,5.2. Datasets,[0],[0]
The task is to predict whether a tumor is malignant (cancerous) or benign.,5.2. Datasets,[0],[0]
Similar to Meehan et al. (2018) we divide the dataset into a training set and a test in a 70 : 30 ratio.,5.2. Datasets,[0],[0]
"For every real-valued feature, we divide the range of each feature into three equal-spaced bins and one-hot encode each feature by its bin-membership.",5.2. Datasets,[0],[0]
This creates a 90-dimensional binary vector for each example.,5.2. Datasets,[0],[0]
"We use a single fully connected layer 90→ 1 followed by a batch normalization layer, as is common practice for BNNs (Courbariaux et al., 2016).
",5.2. Datasets,[0],[0]
Diabetes.,5.2. Datasets,[0],[0]
This dataset2 contains data on 100000 patients with diabetes.,5.2. Datasets,[0],[0]
The task is to predict one of three possible labels regarding hospital readmission after release.,5.2. Datasets,[0],[0]
We divide patients into a 80/20 train/test split.,5.2. Datasets,[0],[0]
"As this dataset contains real and categorical features, we bin them as in the Cancer dataset.",5.2. Datasets,[0],[0]
We obtain a 1704 dimensional binary data point for each entry.,5.2. Datasets,[0],[0]
"Our network (selected by cross validation) consists of a fully connected layer 1704→ 10, a batch normalization layer, a SIGN activation function, followed by another fully connected layer 10→ 3, and a batch normalization layer.
",5.2. Datasets,[0],[0]
Faces.,5.2. Datasets,[0],[0]
The Labeled Faces in the Wild-a dataset contains 13233 gray-scale face images.,5.2. Datasets,[0],[0]
We use the binary classification task of gender identification from the images.,5.2. Datasets,[0],[0]
We resize the images to size 50 × 50.,5.2. Datasets,[0],[0]
"Our network architecture (selected by cross-validation) contains 5 convolutional layers, each of which is followed by a batch normalization layer and a SIGN activation function (except the last which has no activation).",5.2. Datasets,[0],[0]
All convolutional layers have unit stride and filter dimensions 10 × 10.,5.2. Datasets,[0],[0]
All layers except the last layer have 32 output channels (the last has a single output channel).,5.2. Datasets,[0],[0]
"The output is flattened and passed through a fully connected layer 25→ 1 and a batch normalization layer.
",5.2. Datasets,[0],[0]
1https://tinyurl.com/gl3yhzb 2https://tinyurl.com/m6upj7y,5.2. Datasets,[0],[0]
"in Table 2 (computed with Intel Xeon CPUs @ 2.40GHz, processor number E5-2673V3).",5.3. Timing,[0],[0]
"We notice that without parallelization over BNN outputs, the predictions on datasets which use fully connected layers: Cancer and Diabetes, finish within seconds or minutes.",5.3. Timing,[0],[0]
"While the for the datasets that use convolutional layers: Faces and MNIST, predictions require multiple days.",5.3. Timing,[0],[0]
The +1-trick cuts the time of MNIST prediction by half and reduces the time of Faces prediction by 200 hours.,5.3. Timing,[0],[0]
"With only a bit of parallelism over outputs (Out 16-Parallel) prediction on the Faces dataset now requires less than 1.5 days and MNIST can be done
3https://tinyurl.com/yc8d79oe
in 2 hours.",5.3. Timing,[0],[0]
With complete parallelism (Out N-Parallel) all methods reduce to under 2 hours.,5.3. Timing,[0],[0]
We wanted to ensure that BNNs can still achieve similar test set accuracies to floating point networks.,5.4. Accuracy,[0],[0]
"To do so, for each dataset we construct similar floating point networks.",5.4. Accuracy,[0],[0]
"For the Cancer dataset we use the same network except we use the original 30 real-valued features, so the fully connected layer is 30 → 1, as was used in Meehan et al. (2018).",5.4. Accuracy,[0],[0]
"For Diabetes and Faces, just like for our BNNs we cross validate to find the best networks (for Faces: 4 convolutional layers, with filter sizes of 5× 5 and 64 output channels; for Diabetes the best network is the same as used in the BNN).",5.4. Accuracy,[0],[0]
"For MNIST we report the accuracy of the best performing method (Wan et al., 2013) as reported4.",5.4. Accuracy,[0],[0]
"Additionally, we report the accuracy of the weight-dropping method described in Section 4.",5.4. Accuracy,[0],[0]
"The results are shown in
Table 3.",5.4. Accuracy,[0],[0]
"We notice that apart from the Faces dataset, the accuracies differ between the floating point networks and BNNs by at most 1.2% (on MNIST).",5.4. Accuracy,[0],[0]
The face dataset uses a different network in floating point which seems to be able to exploit the increased precision to increase accuracy by 5.1%.,5.4. Accuracy,[0],[0]
We also observe that weight dropping by 10% reduces the accuracy by at most 1.2% (on Faces).,5.4. Accuracy,[0],[0]
"Dropping 20% of the weights seem to have small effect on all datasets except Cancer, which has only a single layer and so likely relies more on every individual weight.",5.4. Accuracy,[0],[0]
"In this work, we devised a set of techniques that allow for practical Encrypted Prediction as a Service.",6. Conclusion,[0],[0]
"In future work, we aim to develop techniques for encrypting non-binary quantized neural networks, and well as design methods for encrypted model training.
4https://tinyurl.com/knn2434",6. Conclusion,[0],[0]
The authors would like to thank Nick Barlow and Oliver Strickson for their support in using the SHEEP platform.,Acknowledgments,[0],[0]
"AS acknowledges support from The Alan Turing Institute under the Turing Doctoral Studentship grant TU/C/000023. AG, MK, and VK were supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.",Acknowledgments,[0],[0]
Machine learning methods are widely used for a variety of prediction problems.,abstractText,[0],[0]
Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients.,abstractText,[0],[0]
"However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed.",abstractText,[0],[0]
Equally important is to minimize the amount of computation and communication required between client and server.,abstractText,[0],[0]
"Fully homomorphic encryption offers a possible way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations.",abstractText,[0],[0]
The main drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data.,abstractText,[0],[0]
"We combine ideas from the machine learning literature, particularly work on binarization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.",abstractText,[0],[0]
TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 957–967 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
957",text,[0],[0]
"Aspect sentiment classification (ASC) is a core problem of sentiment analysis (Liu, 2012).",1 Introduction,[0],[0]
"Given an aspect and a sentence containing the aspect, ASC classifies the sentiment polarity expressed in the sentence about the aspect, namely, positive, neutral, or negative.",1 Introduction,[0],[0]
"Aspects are also called opinion targets (or simply targets), which are usually product/service features in customer reviews.",1 Introduction,[0],[0]
"In this paper, we use aspect and target interchangeably.",1 Introduction,[0],[0]
"In practice, aspects can be specified by the user or extracted automatically using an aspect extraction technique (Liu, 2012).",1 Introduction,[0],[0]
"In this work, we assume the aspect terms are given and only focus on the classification task.
",1 Introduction,[0],[0]
"Due to their impressive results in many NLP tasks (Deng et al., 2014), neural networks have been applied to ASC (see the survey (Zhang et al., 2018)).",1 Introduction,[0],[0]
"Memory networks (MNs), a type of neural networks which were first proposed for question answering (Weston et al., 2015; Sukhbaatar et al., 2015), have achieved the state-of-the-art results in ASC (Tang et al., 2016).",1 Introduction,[0],[0]
A key factor for their success is the attention mechanism.,1 Introduction,[0],[0]
"However, we found that using existing MNs to deal with ASC has an important problem and simply relying on attention modeling cannot solve it.",1 Introduction,[0],[0]
"That is, their performance degrades when the sentiment of a context word is sensitive to the given target.
",1 Introduction,[0],[0]
"Let us consider the following sentences:
(1)",1 Introduction,[0],[0]
The screen resolution is excellent but the price is ridiculous.,1 Introduction,[0],[0]
(2) The screen resolution is excellent but the price is high.,1 Introduction,[0],[0]
(3) The price is high.,1 Introduction,[0],[0]
"(4) The screen resolution is high.
",1 Introduction,[0],[0]
"In sentence (1), the sentiment expressed on aspect screen resolution (or resolution for short) is positive, whereas the sentiment on aspect price is negative.",1 Introduction,[0],[0]
"For the sake of predicting correct sentiment, a crucial step is to first detect the sentiment context about the given aspect/target.",1 Introduction,[0],[0]
We call this step targeted-context detection.,1 Introduction,[0],[0]
Memory networks (MNs) can deal with this step quite well because the sentiment context of a given aspect can be captured by the internal attention mechanism in MNs.,1 Introduction,[0],[0]
"Concretely, in sentence (1) the word “excellent” can be identified as the sentiment context when resolution is specified.",1 Introduction,[0],[0]
"Likewise, the context word “ridiculous” will be placed with a high attention when price is the target.",1 Introduction,[0],[0]
"With the correct targeted-context detected, a trained MN, which recognizes “excellent” as positive sentiment and “ridiculous” as negative sentiment, will infer correct sentiment polarity for the given target.",1 Introduction,[0.9501974084334118],"['The model trained on TREC-CAR selects terms that are similar to the ones in the original query, such as “serves” and “accreditation”.']"
"This
is relatively easy as “excellent” and “ridiculous” are both target-independent sentiment words, i.e., the words themselves already indicate clear sentiments.
",1 Introduction,[0],[0]
"As illustrated above, the attention mechanism addressing the targeted-context detection problem is very useful for ASC, and it helps classify many sentences like sentence (1) accurately.",1 Introduction,[0],[0]
This also led to existing and potential research in improving attention modeling (discussed in Section 5).,1 Introduction,[0],[0]
"However, we observed that simply focusing on tackling the target-context detection problem and learning better attention are not sufficient to solve the problem found in sentences (2), (3) and (4).
",1 Introduction,[0],[0]
Sentence (2) is similar to sentence (1) except that the (sentiment) context modifying aspect/target price is “high”.,1 Introduction,[0],[0]
"In this case, when “high” is assigned the correct attention for the aspect price, the model also needs to capture the sentiment interaction between “high” and price in order to identify the correct sentiment polarity.",1 Introduction,[0],[0]
This is not as easy as sentence (1) because “high” itself indicates no clear sentiment.,1 Introduction,[0],[0]
"Instead, its sentiment polarity is dependent on the given target.
",1 Introduction,[0],[0]
"Looking at sentences (3) and (4), we further see the importance of this problem and also why relying on attention mechanism alone is insufficient.",1 Introduction,[0],[0]
"In these two sentences, sentiment contexts are both “high” (i.e., same attention), but sentence (3) is negative and sentence (4) is positive simply because their target aspects are different.",1 Introduction,[0],[0]
"Therefore, focusing on improving attention will not help in these cases.",1 Introduction,[0],[0]
"We will give a theoretical insight about this problem with MNs in Section 3.
",1 Introduction,[0],[0]
"In this work, we aim to solve this problem.",1 Introduction,[0],[0]
"To distinguish it from the aforementioned targetedcontext detection problem as shown by sentence (1), we refer to the problem in (2), (3) and (4) as the target-sensitive sentiment (or target-dependent sentiment) problem, which means that the sentiment polarity of a detected/attended context word is conditioned on the target and cannot be directly inferred from the context word alone, unlike “excellent” and “ridiculous”.",1 Introduction,[0],[0]
"To address this problem, we propose target-sensitive memory networks (TMNs), which can capture the sentiment interaction between targets and contexts.",1 Introduction,[0],[0]
We present several approaches to implementing TMNs and experimentally evaluate their effectiveness.,1 Introduction,[0],[0]
"This section describes our basic memory network for ASC, also as a background knowledge.",2 Memory Network for ASC,[0],[0]
"It does not include the proposed target-sensitive sentiment solutions, which are introduced in Section 4.",2 Memory Network for ASC,[0],[0]
"The model design follows previous studies (Sukhbaatar et al., 2015; Tang et al., 2016) except that a different attention alignment function is used (shown in Eq. 1).",2 Memory Network for ASC,[0],[0]
Their original models will be compared in our experiments as well.,2 Memory Network for ASC,[0],[0]
"The definitions of related notations are given in Table 1.
",2 Memory Network for ASC,[0],[0]
Input Representation:,2 Memory Network for ASC,[0],[0]
"Given a target aspect t, an embedding matrix A is used to convert t into a vector representation, vt (vt = At).",2 Memory Network for ASC,[0],[0]
"Similarly, each context word (non-aspect word in a sentence) xi ∈ {x1, x2, ...xn} is also projected to the continuous space stored in memory, denoted by mi (mi = Axi) ∈ {m1,m2, ...mn}.",2 Memory Network for ASC,[0],[0]
Here n is the number of words in a sentence and i is the word position/index.,2 Memory Network for ASC,[0],[0]
Both t and xi are one-hot vectors.,2 Memory Network for ASC,[0],[0]
"For an aspect expression with multiple words, its aspect representation vt is the averaged vector of those words (Tang et al., 2016).
",2 Memory Network for ASC,[0],[0]
Attention:,2 Memory Network for ASC,[0],[0]
Attention can be obtained based on the above input representation.,2 Memory Network for ASC,[0],[0]
"Specifically, an attention weight αi for the context word xi is computed based on the alignment function:
αi = softmax(v T t Mmi) (1)
where M ∈ Rd×d is the general learning matrix suggested by Luong et al. (2015).",2 Memory Network for ASC,[0],[0]
"In this manner, attention α = {α1, α2, ..αn} is represented as a vector of probabilities, indicating the weight/importance of context words towards a given target.",2 Memory Network for ASC,[0],[0]
Note that αi ∈,2 Memory Network for ASC,[0],[0]
"(0, 1) and
∑ i αi = 1.
",2 Memory Network for ASC,[0],[0]
Output Representation: Another embedding matrixC is used for generating the individual (output) continuous vector ci (ci = Cxi) for each context word xi.,2 Memory Network for ASC,[0],[0]
"A final response/output vector o is produced by summing over these vectors weighted with the attention α, i.e., o =
∑ i αici.
",2 Memory Network for ASC,[0],[0]
"Sentiment Score (or Logit): The aspect sentiment scores (also called logits) for positive, neutral, and negative classes are then calculated, where a sentiment-specific weight matrix W ∈ RK×d is used.",2 Memory Network for ASC,[0],[0]
"The sentiment scores are represented in a vector s ∈ RK×1, whereK is the number of (sentiment) classes, which is 3 in ASC.
s =W (o+ vt) (2)
",2 Memory Network for ASC,[0],[0]
"The final sentiment probability y is produced with a softmax operation, i.e., y = softmax(s).",2 Memory Network for ASC,[0],[0]
This section analyzes the problem of targetsensitive sentiment in the above model.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
The analysis can be generalized to many existing MNs as long as their improvements are on attention α only.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"We first expand the sentiment score calculation from Eq. 2 to its individual terms:
s =W (o+ vt) =W ( ∑ i αici + vt)
",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"= α1Wc1 + α2Wc2 + ...αnWcn +Wvt
(3)
where “+” denotes element-wise summation.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"In Eq. 3, αiWci can be viewed as the individual sentiment logit for a context word and Wvt is the sentiment logit of an aspect.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
They are linearly combined to determine the final sentiment score s. This can be problematic in ASC.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"First, an aspect word often expresses no sentiment, for example, “screen”.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"However, if the aspect term vt is simply removed from Eq. 3, it also causes the problem that the model cannot handle target-dependent sentiment.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For instance, the sentences (3) and (4) in Section 1 will then be treated as identical if their aspect words are not considered.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Second, if an aspect word is considered and it directly bears some positive or negative sentiment, then when an aspect word occurs with different context words for expressing opposite sentiments, a contradiction can be resulted from them, especially in the case that the context word is a target-sensitive sentiment word.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"We explain it as follows.
",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Let us say we have two target words price and resolution (denoted as p and r).,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
We also have two possible context words “high” and “low” (denoted as h and l).,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"As these two sentiment words can modify both aspects, we can construct four snippets “high price”, “low price”, “high resolution” and “low resolution”.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Their sentiments are negative, positive, positive, and negative respectively.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Let us set W to R1×d so that s becomes a 1-dimensional sentiment score indicator.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
s > 0 indicates a positive sentiment and s < 0 indicates a negative sentiment.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Based on the above example snippets or phrases we have four corresponding inequalities: (a) W (αhch + vp) < 0, (b) W (αlcl+ vp) > 0, (c) W (αhch+ vr) > 0",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
and (d) W (αlcl + vr) < 0.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"We can drop all α terms here as they all equal to 1, i.e., they are the only context word in the snippets to attend to (the target words are not contexts).",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
From (a) and (b) we can infer (e),3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Wch < −Wvp,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
<,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Wcl.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
From (c) and (d) we can infer (f),3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Wcl < −Wvr < Wch.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
From (e) and (f) we have (g),3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Wch < Wcl < Wch, which is a contradiction.
",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
This contradiction means that MNs cannot learn a set of parameters W and C to correctly classify the above four snippets/sentences at the same time.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
This contradiction also generalizes to realworld sentences.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"That is, although real-world review sentences are usually longer and contain more words, since the attention mechanism makes MNs focus on the most important sentiment context (the context with high αi scores), the problem is essentially the same.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For example, in sentences (2) and (3) in Section 1, when price is targeted, the main attention will be placed on “high”.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For MNs, these situations are nearly the same as that for classifying the snippet “high price”.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"We will also show real examples in the experiment section.
",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"One may then ask whether improving attention can help address the problem, as αi can affect the final results by adjusting the sentiment effect of the context word via αiWci.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"This is unlikely, if not impossible.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"First, notice that αi is a scalar ranging in (0,1), which means it essentially assigns higher or lower weight to increase or decrease the sentiment effect of a context word.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"It cannot change the intrinsic sentiment orientation/polarity of the context, which is determined by Wci.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For example, if Wci assigns the context word “high” a positive sentiment (Wci > 0), αi will not make it negative (i.e., αiWci < 0 cannot be achieved by chang-
ing αi).",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Second, other irrelevant/unimportant context words often carry no or little sentiment information, so increasing or decreasing their weights does not help.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For example, in the sentence “the price is high”, adjusting the weights of context words “the” and “is” will neither help solve the problem nor be intuitive to do so.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"This section introduces six (6) alternative targetsensitive memory networks (TMNs), which all can deal with the target-sensitive sentiment problem.",4 The Proposed Approaches,[0],[0]
"Each of them has its characteristics.
",4 The Proposed Approaches,[0],[0]
Non-linear Projection (NP): This is the first approach that utilizes a non-linear projection to capture the interplay between an aspect and its context.,4 The Proposed Approaches,[0],[0]
"Instead of directly following the common linear combination as shown in Eq. 3, we use a non-linear projection (tanh) as the replacement to calculate the aspect-specific sentiment score.
",4 The Proposed Approaches,[0],[0]
s =W · tanh( ∑,4 The Proposed Approaches,[0],[0]
"i αici + vt) (4)
",4 The Proposed Approaches,[0],[0]
"As shown in Eq. 4, by applying a non-linear projection over attention-weighted ci and vt, the context and aspect information are coupled in a way that the final sentiment score cannot be obtained by simply summing their individual contributions (compared with Eq. 3).",4 The Proposed Approaches,[0],[0]
This technique is also intuitive in neural networks.,4 The Proposed Approaches,[0],[0]
"However, notice that by using the non-linear projection (or adding more sophisticated hidden layers) over them in this way, we sacrifice some interpretability.",4 The Proposed Approaches,[0],[0]
"For example, we may have difficulty in tracking how each individual context word (ci) affects the final sentiment score s, as all context and target representations are coupled.",4 The Proposed Approaches,[0],[0]
"To avoid this, we can use the following five alternative techniques.
",4 The Proposed Approaches,[0],[0]
"Contextual Non-linear Projection (CNP): Despite the fact that it also uses the non-linear projection, this approach incorporates the interplay between a context word and the given target into its (output) context representation.",4 The Proposed Approaches,[0],[0]
"We thus name it Contextual Non-linear Projection (CNP).
",4 The Proposed Approaches,[0],[0]
s,4 The Proposed Approaches,[0],[0]
"=W ∑ i αi · tanh(ci + vt) (5)
",4 The Proposed Approaches,[0],[0]
"From Eq. 5, we can see that this approach can keep the linearity of attention-weighted context aggregation while taking into account the aspect information with non-linear projection, which works
in a different way compared to NP.",4 The Proposed Approaches,[0],[0]
"If we define c̃i = tanh(ci + vt), c̃i can be viewed as the target-aware context representation of context xi and the final sentiment score is calculated based on the aggregation of such c̃i.",4 The Proposed Approaches,[0],[0]
"This could be a more reasonable way to carry the aspect information rather than simply summing the aspect representation (Eq. 3).
",4 The Proposed Approaches,[0],[0]
"However, one potential disadvantage is that this setting uses the same set of vector representations (learned by embeddings C) for multiple purposes, i.e., to learn output (context) representations and to capture the interplay between contexts and aspects.",4 The Proposed Approaches,[0],[0]
"This may degenerate its model performance when the computational layers in memory networks (called “hops”) are deep, because too much information is required to be encoded in such cases and a sole set of vectors may fail to capture all of it.
",4 The Proposed Approaches,[0],[0]
"To overcome this, we suggest the involvement of an additional new set of embeddings/vectors, which is exclusively designed for modeling the sentiment interaction between an aspect and its context.",4 The Proposed Approaches,[0],[0]
"The key idea is to decouple different functioning components with different representations, but still make them work jointly.",4 The Proposed Approaches,[0],[0]
"The following four techniques are based on this idea.
",4 The Proposed Approaches,[0],[0]
Interaction Term (IT): The third approach is to formulate explicit target-context sentiment interaction terms.,4 The Proposed Approaches,[0],[0]
"Different from the targeted-context detection problem which is captured by attention (discussed in Section 1), here the targetcontext sentiment (TCS) interaction measures the sentiment-oriented interaction effect between targets and contexts, which we refer to as TCS interaction (or sentiment interaction) for short in the rest of this paper.",4 The Proposed Approaches,[0.9552575768110266],"['In the future, more research is necessary in the directions of (1) iterative reformulation under the proposed framework, (2) using information from modalities other than text, and (3) better reinforcement learning algorithms for a partially-observable environment.']"
"Such sentiment interaction is captured by a new set of vectors, and we thus also call such vectors TCS vectors.
s = ∑ i αi(Wsci + wI〈di, dt〉) (6)
",4 The Proposed Approaches,[0],[0]
"In Eq. 6, Ws ∈ RK×d and wI ∈ RK×1 are used instead of W in Eq. 3.",4 The Proposed Approaches,[0],[0]
Ws models the direct sentiment effect from ci while wI works with di and dt together for learning the TCS interaction.,4 The Proposed Approaches,[0],[0]
"di and dt are TCS vector representations of context xi and aspect t, produced from a new embedding matrix D,",4 The Proposed Approaches,[0],[0]
"i.e., di = Dxi, dt = Dt (D ∈ Rd×V and di, dt ∈ Rd×1).
",4 The Proposed Approaches,[0],[0]
"Unlike input and output embeddings A and C, D is designed to capture the sentiment interac-
tion.",4 The Proposed Approaches,[0],[0]
"The vectors fromD affect the final sentiment score through wI〈di, dt〉, where wI is a sentimentspecific vector and 〈di, dt〉 ∈ R denotes the dot product of the two TCS vectors di and dt.",4 The Proposed Approaches,[0],[0]
"Compared to the basic MNs, this model can better capture target-sensitive sentiment because the interactions between a context word h and different aspect words (say, p and r) can be different, i.e., 〈dh, dp〉 6=",4 The Proposed Approaches,[0],[0]
"〈dh, dr〉.
",4 The Proposed Approaches,[0],[0]
The key advantage is that now the sentiment effect is explicitly dependent on its target and context.,4 The Proposed Approaches,[0],[0]
"For example, 〈dh, dp〉 can help shift the final sentiment to negative and 〈dh, dr〉 can help shift it to positive.",4 The Proposed Approaches,[0],[0]
Note that α is still needed to control the importance of different contexts.,4 The Proposed Approaches,[0],[0]
"In this manner, targeted-context detection (attention) and TCS interaction are jointly modeled and work together for sentiment inference.",4 The Proposed Approaches,[0],[0]
The proposed techniques introduced below also follow this core idea but with different implementations or properties.,4 The Proposed Approaches,[0],[0]
"We thus will not repeat similar discussions.
",4 The Proposed Approaches,[0],[0]
Coupled Interaction (CI): This proposed technique associates the TCS interaction with an additional set of context representation.,4 The Proposed Approaches,[0],[0]
"This representation is for capturing the global correlation between context and different sentiment classes.
",4 The Proposed Approaches,[0],[0]
"s = ∑ i αi(Wsci +WI〈di, dt〉ei) (7)
",4 The Proposed Approaches,[0],[0]
"Specifically, ei is another output representation for xi, which is coupled with the sentiment interaction factor 〈di, dt〉.",4 The Proposed Approaches,[0],[0]
"For each context word xi, ei is generated as ei = Exi whereE ∈ Rd×V is an embedding matrix.",4 The Proposed Approaches,[0],[0]
"〈di, dt〉 and ei function together as a target-sensitive context vector and are used to produce sentiment scores with WI (WI ∈ RK×d).
",4 The Proposed Approaches,[0],[0]
"Joint Coupled Interaction (JCI): A natural variant of the above model is to replace ei with ci, which means to learn a joint output representation.",4 The Proposed Approaches,[0],[0]
"This can also reduce the number of learning parameters and simplify the CI model.
",4 The Proposed Approaches,[0],[0]
"s = ∑ i αi(Wsci +WI〈di, dt〉ci) (8)
Joint Projected Interaction (JPI): This model also employs a unified output representation like JCI, but a context output vector ci will be projected to two different continuous spaces before sentiment score calculation.",4 The Proposed Approaches,[0],[0]
"To achieve the goal, two projection matrices W1, W2 and the non-linear projection function tanh are used.",4 The Proposed Approaches,[0],[0]
"The intuition is
that, when we want to reduce the (embedding) parameters and still learn a joint representation, two different sentiment effects need to be separated in different vector spaces.",4 The Proposed Approaches,[0],[0]
"The two sentiment effects are modeled as two terms:
s = ∑ i αiWJ tanh(W1ci)
+ ∑",4 The Proposed Approaches,[0],[0]
"i αiWJ〈di, dt〉 tanh(W2ci) (9)
where the first term can be viewed as learning target-independent sentiment effect while the second term captures the TCS interaction.",4 The Proposed Approaches,[0],[0]
"A joint sentiment-specific weight matrix WJ(WJ ∈ RK×d) is used to control/balance the interplay between these two effects.
",4 The Proposed Approaches,[0],[0]
"Discussions: (a) In IT, CI, JCI, and JPI, their first-order terms are still needed, because not in all cases sentiment inference needs TCS interaction.",4 The Proposed Approaches,[0],[0]
"For some simple examples like “the battery is good”, the context word “good” simply indicates clear sentiment, which can be captured by their first-order term.",4 The Proposed Approaches,[0],[0]
"However, notice that the modeling of second-order terms offers additional help in both general and target-sensitive scenarios.",4 The Proposed Approaches,[0],[0]
(b) TCS interaction can be calculated by other modeling functions.,4 The Proposed Approaches,[0],[0]
"We have tried several methods and found that using the dot product 〈di, dt〉 or dTi Wdt (with a projection matrix W ) generally produces good results.",4 The Proposed Approaches,[0],[0]
"(c) One may ask whether we can use fewer embeddings or just use one universal embedding to replace A, C and D (the definition of D can be found in the introduction of IT).",4 The Proposed Approaches,[0],[0]
We have investigated them as well.,4 The Proposed Approaches,[0],[0]
We found that merging A and C is basically workable.,4 The Proposed Approaches,[0],[0]
But merging D and A/C produces poor results because they essentially function with different purposes.,4 The Proposed Approaches,[0],[0]
"While A and C handle targeted-context detection (attention), D captures the TCS interaction.",4 The Proposed Approaches,[0],[0]
(d),4 The Proposed Approaches,[0],[0]
"Except NP, we do not apply non-linear projection to the sentiment score layer.",4 The Proposed Approaches,[0],[0]
"Although adding non-linear transformation to it may further improve model performance, the individual sentiment effect from each context will become untraceable, i.e., losing some interpretability.",4 The Proposed Approaches,[0],[0]
"In order to show the effectiveness of learning TCS interaction and for analysis purpose, we do not use it in this work.",4 The Proposed Approaches,[0],[0]
"But it can be flexibly added for specific tasks/analyses that do not require strong interpretability.
",4 The Proposed Approaches,[0],[0]
Loss function: The proposed models are all trained in an end-to-end manner by minimizing the cross entropy loss.,4 The Proposed Approaches,[0],[0]
"Let us denote a sentence and a
target aspect as x and t respectively.",4 The Proposed Approaches,[0],[0]
"They appear together in a pair format (x, t) as input and all such pairs construct the dataset H .",4 The Proposed Approaches,[0],[0]
"g(x,t) is a one-hot vector and gk(x,t) ∈ {0, 1} denotes a gold sentiment label, i.e., whether (x, t) shows sentiment k. yx,t is the model-predicted sentiment distribution for (x, t).",4 The Proposed Approaches,[0],[0]
"ykx,t denotes its probability in class k.",4 The Proposed Approaches,[0],[0]
"Based on them, the training loss is constructed as:
loss = − ∑
(x,t)∈H ∑ k∈K gk(x,t) log y k",4 The Proposed Approaches,[0],[0]
"(x,t) (10)",4 The Proposed Approaches,[0],[0]
"Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)).",5 Related Work,[0],[0]
"Later on, the seminal work of using attention mechanism for neural machine translation (Bahdanau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Hermann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC.
Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015), and they can be applied to ASC.",5 Related Work,[0],[0]
Tang et al. (2016) proposed an MN variant to ASC and achieved the state-of-the-art performance.,5 Related Work,[0],[0]
"Another common neural model using attention mechanism is the RNN/LSTM (Wang et al., 2016).
",5 Related Work,[0],[0]
"As discussed in Section 1, the attention mechanism is suitable for ASC because it effectively addresses the targeted-context detection problem.",5 Related Work,[0],[0]
"Along this direction, researchers have studied more sophisticated attentions to further help the ASC task (Chen et al., 2017; Ma et al., 2017; Liu and Zhang, 2017).",5 Related Work,[0],[0]
Chen et al. (2017) proposed to use a recurrent attention mechanism.,5 Related Work,[0],[0]
"Ma et al. (2017) used multiple sets of attentions, one for modeling the attention of aspect words and one for modeling the attention of context words.",5 Related Work,[0],[0]
"Liu and Zhang (2017) also used multiple sets of attentions, one obtained from the left context and one obtained from the right context of a given target.",5 Related Work,[0],[0]
Notice that our work does not lie in this direction.,5 Related Work,[0],[0]
"Our goal is to solve the target-sensitive sen-
timent and to capture the TCS interaction, which is a different problem.",5 Related Work,[0],[0]
"This direction is also finergrained, and none of the above works addresses this problem.",5 Related Work,[0],[0]
"Certainly, both directions can improve the ASC task.",5 Related Work,[0],[0]
"We will also show in our experiments that our work can be integrated with an improved attention mechanism.
",5 Related Work,[0],[0]
"To the best of our knowledge, none of the existing studies addresses the target-sensitive sentiment problem in ASC under the purely data-driven and supervised learning setting.",5 Related Work,[0],[0]
"Other concepts like sentiment shifter (Polanyi and Zaenen, 2006) and sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2013) are also related, but they are not learned automatically and require rule/patterns or external resources (Liu, 2012).",5 Related Work,[0],[0]
"Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013).",5 Related Work,[0],[0]
"We perform experiments on the datasets of SemEval Task 2014 (Pontiki et al., 2014), which contain online reviews from domain Laptop and Restaurant.",6 Experiments,[0],[0]
"In these datasets, aspect sentiment polarities are labeled.",6 Experiments,[0],[0]
The training and test sets have also been provided.,6 Experiments,[0],[0]
Full statistics of the datasets are given in Table 2.,6 Experiments,[0],[0]
MN:,6.1 Candidate Models for Comparison,[0],[0]
"The classic end-to-end memory network (Sukhbaatar et al., 2015).",6.1 Candidate Models for Comparison,[0],[0]
"AMN: A state-of-the-art memory network used for ASC (Tang et al., 2016).",6.1 Candidate Models for Comparison,[0],[0]
"The main difference from MN is in its attention alignment function, which concatenates the distributed representations of the context and aspect, and uses an additional weight matrix for attention calculation, following the method introduced in (Bahdanau et al., 2015).",6.1 Candidate Models for Comparison,[0],[0]
"BL-MN: Our basic memory network presented in Section 2, which does not use the proposed techniques for capturing target-sensitive sentiments.",6.1 Candidate Models for Comparison,[0],[0]
AE-LSTM: RNN/LSTM is another popular attention based neural model.,6.1 Candidate Models for Comparison,[0],[0]
"Here we compare
with a state-of-the-art attention-based LSTM for ASC, AE-LSTM (Wang et al., 2016).",6.1 Candidate Models for Comparison,[0],[0]
ATAE-LSTM:,6.1 Candidate Models for Comparison,[0],[0]
"Another attention-based LSTM for ASC reported in (Wang et al., 2016).",6.1 Candidate Models for Comparison,[0],[0]
"Target-sensitive Memory Networks (TMNs): The six proposed techniques, NP, CNP, IT, CI, JCI, and JPI give six target-sensitive memory networks.
",6.1 Candidate Models for Comparison,[0],[0]
"Note that other non-neural network based models like SVM and neural models without attention mechanism like traditional LSTMs have been compared and reported with inferior performance in the ASC task (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016), so they are excluded from comparisons here.",6.1 Candidate Models for Comparison,[0],[0]
"Also, note that non-neural models like SVMs require feature engineering to manually encode aspect information, while this work aims to improve the aspect representation learning based approaches.",6.1 Candidate Models for Comparison,[0],[0]
"Since we have a three-class classification task (positive, negative and neutral) and the classes are imbalanced as shown in Table 2, we use F1-score as our evaluation measure.",6.2 Evaluation Measure,[0],[0]
We report both F1Macro over all classes and all individual classbased F1 scores.,6.2 Evaluation Measure,[0],[0]
"As our problem requires finegrained sentiment interaction, the class-based F1 provides more indicative information.",6.2 Evaluation Measure,[0],[0]
"In addition, we report the accuracy (same as F1-Micro), as it is used in previous studies.",6.2 Evaluation Measure,[0],[0]
"However, we suggest using F1-score because accuracy biases towards the majority class.",6.2 Evaluation Measure,[0],[0]
We use the open-domain word embeddings1 for the initialization of word vectors.,6.3 Training Details,[0],[0]
"We initialize other model parameters from a uniform distribution U (-0.05, 0.05).",6.3 Training Details,[0],[0]
The dimension of the word embedding and the size of the hidden layers are 300.,6.3 Training Details,[0],[0]
The learning rate is set to 0.01 and the dropout rate is set to 0.1.,6.3 Training Details,[0],[0]
Stochastic gradient descent is used as our optimizer.,6.3 Training Details,[0],[0]
"The position encoding is also used (Tang et al., 2016).",6.3 Training Details,[0],[0]
"We also compare the memory networks in their multiple computational layers version (i.e., multiple hops) and the number of hops is set to 3 as used in the mentioned previous studies.",6.3 Training Details,[0],[0]
"We implemented all models in the TensorFlow environment using same input, embedding size, dropout rate, optimizer, etc.
",6.3 Training Details,[0],[0]
"1https://github.com/mmihaltz/word2vec-GoogleNewsvectors
so as to test our hypotheses, i.e., to make sure the achieved improvements do not come from elsewhere.",6.3 Training Details,[0],[0]
"Meanwhile, we can also report all evaluation measures discussed above2.",6.3 Training Details,[0],[0]
10% of the training data is used as the development set.,6.3 Training Details,[0],[0]
We report the best results for all models based on their F-1 Macro scores.,6.3 Training Details,[0],[0]
The classification results are shown in Table 3.,6.3.1 Result Analysis,[0],[0]
"Note that the candidate models are all based on classic/standard attention mechanism, i.e., without sophisticated or multiple attentions involved.",6.3.1 Result Analysis,[0],[0]
We compare the 1-hop and 3-hop memory networks as two different settings.,6.3.1 Result Analysis,[0],[0]
The top three F1-Macro scores are marked in bold.,6.3.1 Result Analysis,[0],[0]
"Based on them, we have the following observations:
1.",6.3.1 Result Analysis,[0],[0]
"Comparing the 1-hop memory networks (first nine rows), we see significant performance gains achieved by CNP, CI, JCI, and JPI on both datasets, where each of them has p < 0.01 over the strongest baseline (BL-MN) from paired t-test using F1-Macro.",6.3.1 Result Analysis,[0],[0]
IT also outperforms the other baselines while NP has similar performance to BL-MN.,6.3.1 Result Analysis,[0],[0]
"This indicates that TCS interaction is very useful, as BL-MN and NP do not model it.",6.3.1 Result Analysis,[0],[0]
2.,6.3.1 Result Analysis,[0],[0]
"In the 3-hop setting, TMNs achieve much better results on Restaurant.",6.3.1 Result Analysis,[0],[0]
"JCI, IT, and CI achieve the best scores, outperforming the strongest baseline AMN by 2.38%, 2.18%, and 2.03%.",6.3.1 Result Analysis,[0],[0]
"On Laptop, BL-MN and most TMNs (except CNP and JPI) perform similarly.",6.3.1 Result Analysis,[0],[0]
"However, BL-MN performs poorly on Restaurant (only better than two models) while TMNs show more stable performance.",6.3.1 Result Analysis,[0],[0]
3.,6.3.1 Result Analysis,[0],[0]
"Comparing all TMNs, we see that JCI works the best as it always obtains the top-three scores on two datasets and in two settings.",6.3.1 Result Analysis,[0],[0]
CI and JPI also perform well in most cases.,6.3.1 Result Analysis,[0],[0]
"IT, NP, and CNP can achieve very good scores in some cases but are less stable.",6.3.1 Result Analysis,[0],[0]
We also analyzed their potential issues in Section 4. 4.,6.3.1 Result Analysis,[0],[0]
It is important to note that these improvements are quite large because in many cases sentiment interactions may not be necessary (like sentence (1) in Section 1).,6.3.1 Result Analysis,[0],[0]
"The overall good results obtained by TMNs demonstrate their capability of handling both general and target-sensitive sentiments, i.e., the proposed
2Most related studies report accuracy only.
",6.3.1 Result Analysis,[0],[0]
techniques do not bring harm while capturing additional target-sensitive signals.,6.3.1 Result Analysis,[0],[0]
5.,6.3.1 Result Analysis,[0],[0]
"Micro-F1/accuracy is greatly affected by the majority class, as we can see the scores from Pos. and Micro are very consistent.",6.3.1 Result Analysis,[0],[0]
"TMNs, in fact, effectively improve the minority classes, which are reflected in Neg.",6.3.1 Result Analysis,[0],[0]
"and Neu., for example, JCI improves BL-MN by 3.78% in Neg.",6.3.1 Result Analysis,[0],[0]
on Restaurant,6.3.1 Result Analysis,[0],[0]
.,6.3.1 Result Analysis,[0],[0]
This indicates their usefulness of capturing fine-grained sentiment signals.,6.3.1 Result Analysis,[0],[0]
"We will give qualitative examples in next section to show their modeling superiority for identifying target-sensitive sentiments.
Integration with Improved Attention: As discussed, the goal of this work is not for learning better attention but addressing the targetsensitive sentiment.",6.3.1 Result Analysis,[0],[0]
"In fact, solely improving attention does not solve our problem (see Sections 1 and 3).",6.3.1 Result Analysis,[0],[0]
"However, better attention can certainly help achieve an overall better performance for the ASC task, as it makes the targeted-context detection more accurate.",6.3.1 Result Analysis,[0],[0]
"Here we integrate our pro-
posed technique JCI with a state-of-the-art sophisticated attention mechanism, namely, the recurrent attention framework, which involves multiple attentions learned iteratively (Kumar et al., 2016; Chen et al., 2017).",6.3.1 Result Analysis,[0],[0]
We name our model with this integration as Target-sensitive Recurrent-attention Memory Network (TRMN) and the basic memory network with the recurrent attention as Recurrentattention Memory Network (RMN).,6.3.1 Result Analysis,[0],[0]
Their results are given in Table 4.,6.3.1 Result Analysis,[0],[0]
TRMN achieves significant performance gain with p < 0.05 in paired t-test.,6.3.1 Result Analysis,[0],[0]
"We now give some real examples to show the effectiveness of modeling TCS interaction for identifying target-sensitive sentiments, by comparing a regular MN and a TMN.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Specifically, BL-MN and JPI are used.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Other MNs/TMNs have similar performances to BL-MN/JPI qualitatively, so we do not list all of them here.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"For BL-MN and JPI, their sentiment scores of a single context word i are calculated by αiWci (from Eq. 3) and αiWJ tanh(W1ci) + αiWJ〈di, dt〉tanh(W2ci) (from Eq. 9), each of which results in a 3-dimensional vector.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
Illustrative Examples: Table 5 shows two records in Laptop.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"In record 1, to identify the sentiment of target price in the presented sentence, the sentiment interaction between the context word “higher” and the target word price is the key.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"The
specific sentiment scores of the word “higher” towards negative, neutral and positive classes in both models are reported.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
We can see both models accurately assign the highest sentiment scores to the negative class.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"We also observe that in MN the negative score (0.3641) in the 3-dimension vector {0.3641,−0.3275,−0.0750} calculated by αiWci is greater than neutral (−0.3275) and positive (−0.0750) scores.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Notice that αi is always positive (ranging in (0, 1)), so it can be inferred that the first value in vector Wci is greater than the other two values.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Here ci denotes the vector representation of “higher” so we use chigher to highlight it and we have {Wchigher}Negative > {Wchigher}Neutral/Positive as an inference.
",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"In record 2, the target is resolution and its sentiment is positive in the presented sentence.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Although we have the same context word “higher”, different from record 1, it requires a positive sentiment interaction with the current target.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Looking at the results, we see TMN assigns the highest sentiment score of word “higher” to positive class correctly, whereas MN assigns it to negative class.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
This error is expected if we consider the above inference {Wchigher}Negative > {Wchigher}Neutral/Positive in MN.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
The cause of this unavoidable error is that Wci is not conditioned on the target.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"In contrast, WJ〈di, ·dt〉tanh(W2ci) can change the sentiment polarity with the aspect vector dt encoded.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Other TMNs also achieve it (like WI〈di, dt〉ci in JCI).
",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
One may notice that the aspect information (vt) is actually also considered in the form of αiWci+ Wvt in MNs and wonder whether Wvt may help address the problem given different vt.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Let us assume it helps, which means in the above example an MN makes Wvresolution favor the positive class and Wvprice favor the negative class.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"But then we will have trouble when the context word is “lower”, where it requires Wvresolution to favor the negative class and Wvprice to favor the positive class.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"This contradiction reflects the theoretical problem discussed in Section 3.
",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Other Examples: We also found other interesting target-sensitive sentiment expressions like “large bill” and “large portion”, “small tip” and “small portion” from Restaurant.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
Notice that TMNs can also improve the neutral sentiment (see Table 3).,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"For instance, TMN generates a sentiment score vector of the context “over” for target aspect price: {0.1373, 0.0066, -0.1433} (negative) and for target aspect dinner: {0.0496, 0.0591, - 0.1128} (neutral) accurately.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"But MN produces both negative scores {0.0069, 0.0025, -0.0090} (negative) and {0.0078, 0.0028, -0.0102} (negative) for the two different targets.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
The latter one in MN is incorrect.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"In this paper, we first introduced the targetsensitive sentiment problem in ASC.",7 Conclusion and Future Work,[0],[0]
"After that, we discussed the basic memory network for ASC and analyzed the reason why it is incapable of capturing such sentiment from a theoretical perspective.",7 Conclusion and Future Work,[0],[0]
We then presented six techniques to construct target-sensitive memory networks.,7 Conclusion and Future Work,[0],[0]
"Finally, we reported the experimental results quantitatively and qualitatively to show their effectiveness.
",7 Conclusion and Future Work,[0],[0]
"Since ASC is a fine-grained and complex task, there are many other directions that can be further explored, like handling sentiment negation, better embedding for multi-word phrase, analyzing sentiment composition, and learning better attention.",7 Conclusion and Future Work,[0],[0]
We believe all these can help improve the ASC task.,7 Conclusion and Future Work,[0],[0]
"The work presented in this paper lies in the direction of addressing target-sensitive sentiment, and we have demonstrated the usefulness of capturing this signal.",7 Conclusion and Future Work,[0],[0]
We believe that there will be more effective solutions coming in the near future.,7 Conclusion and Future Work,[0],[0]
This work was partially supported by National Science Foundation (NSF) under grant nos.,Acknowledgments,[0],[0]
"IIS1407927 and IIS-1650900, and by Huawei Technologies Co. Ltd with a research gift.",Acknowledgments,[0],[0]
Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis.,abstractText,[0],[0]
"Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence.",abstractText,[0],[0]
Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results.,abstractText,[0],[0]
"In MNs, attention mechanism plays a crucial role in detecting the sentiment context for the given target.",abstractText,[0],[0]
"However, we found an important problem with the current MNs in performing the ASC task.",abstractText,[0],[0]
Simply improving the attention mechanism will not solve it.,abstractText,[0],[0]
"The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it cannot be inferred from the context alone.",abstractText,[0],[0]
"To tackle this problem, we propose the targetsensitive memory networks (TMNs).",abstractText,[0],[0]
Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.,abstractText,[0],[0]
Target-Sensitive Memory Networks for Aspect Sentiment Classification,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1192",text,[0],[0]
A language model (LM) defines a probability distribution over sequences of words.,1 Introduction,[0],[0]
Recent technological advances have led to an explosion of neural network-based LM architectures.,1 Introduction,[0],[0]
"The most popular ones are based on recurrent neural networks (RNNs) (Elman, 1990; Mikolov et al., 2010), in particular Long Short-Term Memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997).",1 Introduction,[0],[0]
"While a large number of alternative architectures have been proposed in the past few years, LSTMs are still highly competitive (Melis et al., 2018).
",1 Introduction,[0],[0]
Language models are typically evaluated using perplexity: it is considered desirable for an LM to assign a high probability to held-out data from the same corpus as the training data.,1 Introduction,[0],[0]
"This measure conflates multiple sources of success (or failure) in predicting the next word: common collo-
cations, semantics, pragmatics, syntax, and so on.",1 Introduction,[0],[0]
"The quality of the syntactic predictions made by the LM is arguably particularly difficult to measure using perplexity: since most sentences are grammatically simple and most words can be predicted from their local context, perplexity rewards LMs primarily for collocational and semantic predictions.
",1 Introduction,[0],[0]
We propose to supplement perplexity with a metric that assesses whether the probability distribution defined by the model conforms to the grammar of the language.,1 Introduction,[0],[0]
"Following previous work (Lau et al., 2017; Linzen et al., 2016; Gulordava et al., 2018), we suggest that given two sentences that differ minimally from each other, one of which is grammatical and the other which is not, it is desirable for the model to assign a higher probability to the grammatical one.
",1 Introduction,[0],[0]
"The value of this approach can be illustrated with a recent study by Tran et al. (2018), where a standard LSTM language model was compared to an attention-only LM without recurrence (Vaswani et al., 2017).",1 Introduction,[0],[0]
"Although the attention-only model had somewhat better perplexity on the validation set, when the models were tested specifically on challenging subject-verb agreement dependencies, the attention-only model made three times as many errors as the LSTM.",1 Introduction,[0],[0]
"In other words, the LSTM learned more robust syntactic representations, but this advantage was not reflected in its average perplexity on the corpus, since syntactically challenging sentences are relatively infrequent.
",1 Introduction,[0],[0]
"Previous work on targeted syntactic evaluation of language models has identified syntactically challenging sentences in corpora (Linzen et al., 2016; Gulordava et al., 2018).",1 Introduction,[0],[0]
"While evaluation on naturally occurring examples is appealing, this approach has its limitations (see Section 2).",1 Introduction,[0],[0]
"In particular, syntactically challenging examples are sparsely represented in a corpus, their identifica-
tion requires a clean parsed corpus, and naturally occurring sentences are difficult to control for confounds.",1 Introduction,[0],[0]
"We contrast the naturalistic approach with a constructed dataset, which allows us to examine a much larger range of specific grammatical phenomena than has been possible before.",1 Introduction,[0],[0]
"We use templates to automatically create our test sentences, making it possible to generate a large test set while maintaining experimental control over our materials as well as a balanced number of examples of each phenomenon.
",1 Introduction,[0],[0]
"We test three LMs on the data set we develop: an n-gram baseline, an RNN LM trained on an unannotated corpus, and an RNN LM trained on a multitask objective: language modeling and Combinatory Categorial Grammar (CCG) supertagging (Bangalore and Joshi, 1999).",1 Introduction,[0],[0]
We also conduct a human experiment using the same materials.,1 Introduction,[0],[0]
"The n-gram baseline largely performed at chance, suggesting that good performance on the task requires syntactic representations.",1 Introduction,[0],[0]
"The RNN LMs performed well on simple cases, but struggled on more complex ones.",1 Introduction,[0],[0]
"Multi-task training with a supervised syntactic objective improved the performance of the RNN, but it was still much weaker than humans.",1 Introduction,[0],[0]
"This suggests that our data set is challenging, especially when explicit syntactic supervision is not available, and can therefore motivate richer language modeling architectures.",1 Introduction,[0],[0]
How should grammaticality be captured in the probability distribution defined by an LM?,2.1 Grammaticality and LM probability,[0],[0]
The most extreme position would be that a language model should assign a probability of zero to ungrammatical sentences.,2.1 Grammaticality and LM probability,[0],[0]
"For most applications, some degree of error tolerance is desirable, and it is not practical to assign a sentence a probability of exactly zero.1 Following Linzen et al. (2016) and Gulordava et al. (2018), our desideratum for the language model is more modest: if two closely matched sentence differ only in their grammaticality, the probability of the grammatical sentence should be higher than the probability of the ungrammatical one.",2.1 Grammaticality and LM probability,[0],[0]
"For example, the following minimal pair illustrates the fact that third-
1Nor is it possible to have a threshold such that all grammatical sentences have probability higher than and all ungrammatical sentences have probability lower than , for the simple reason that there is an infinite number of grammatical sentences (Lau et al., 2017).
person present English verbs agree with the number of their subject:
(1) Simple agreement:",2.1 Grammaticality and LM probability,[0],[0]
a.,2.1 Grammaticality and LM probability,[0],[0]
The author laughs.,2.1 Grammaticality and LM probability,[0],[0]
b. *,2.1 Grammaticality and LM probability,[0],[0]
"The author laugh.
",2.1 Grammaticality and LM probability,[0],[0]
We expect the probability of (1a) to be higher than the probability of (1b).,2.1 Grammaticality and LM probability,[0],[0]
Previous work has simplified this setting further by comparing the probability that the LM assigns to a single word that is the locus of ungrammaticality.,2.1 Grammaticality and LM probability,[0],[0]
"In (1), for example, the LM would be fed the first two words of the sentence, and would be considered successful on the task if it predicts P (laughs) >",2.1 Grammaticality and LM probability,[0],[0]
"P (laugh).
",2.1 Grammaticality and LM probability,[0],[0]
"The prediction setting is only applicable when the locus of ungrammaticality is a single word, rather than, say, the interaction between two words; moreover, the information needed to make the grammaticality decision needs to be available in the left context of the locus of grammaticality.",2.1 Grammaticality and LM probability,[0],[0]
These conditions do not always hold.,2.1 Grammaticality and LM probability,[0],[0]
"Negative polarity items (NPIs), for example, are words like any and ever that can only be used in the scope of negation.2 The grammaticality of placing a particular quantifier in the beginning of the sentences in (2) depends on whether the sentence contains an NPI later on:
(2) Simple NPI:",2.1 Grammaticality and LM probability,[0],[0]
a. No students have ever lived here.,2.1 Grammaticality and LM probability,[0],[0]
b. *,2.1 Grammaticality and LM probability,[0],[0]
"Most students have ever lived here.
",2.1 Grammaticality and LM probability,[0],[0]
It would not be possible to compare these two sentences using the prediction task.,2.1 Grammaticality and LM probability,[0],[0]
"In the current paper, we use the more general setting and compare the probability of the two complete sentences.",2.1 Grammaticality and LM probability,[0],[0]
Previous work has used syntactically complex sentences identified from a parsed corpus.,2.2 Data set construction,[0],[0]
This approach has several limitations.,2.2 Data set construction,[0],[0]
"If the corpus is automatically parsed, the risk of a parse error increases with the complexity of the construction (Bender et al., 2011).",2.2 Data set construction,[0],[0]
"If the test set is restricted to sentences with gold parses, it can be difficult or impossible to find a sufficient number of examples of syntactically challenging cases.",2.2 Data set construction,[0],[0]
"Moreover, using naturally occurring sentences can introduce
2In practice, the conditions that govern the distribution of NPIs are much more complicated, but this first approximation will suffice for the present purposes.",2.2 Data set construction,[0],[0]
"For a review, see Giannakidou (2011).
",2.2 Data set construction,[0],[0]
"confounds that may complicate the interpretation of the experiments (Ettinger et al., 2018).
",2.2 Data set construction,[0],[0]
"To circumvent these issues, we use templates to automatically construct a large number of English sentence pairs (∼350,000).",2.2 Data set construction,[0],[0]
"Our data set includes three phenomena that linguists consider to be sensitive to hierarchical syntactic structure (Everaert et al., 2015; Xiang et al., 2009): subjectverb agreement (described in detail in Sections 4.1 and 4.2), reflexive anaphora (Section 4.3) and negative polarity items (Section 4.4).
",2.2 Data set construction,[0],[0]
The templates can be described using nonrecursive context-free grammars.,2.2 Data set construction,[0],[0]
We specify the preterminal symbols that make up a syntactic construction and have different terminal symbols that those preterminals could be mapped to.,2.2 Data set construction,[0],[0]
"For example, the template for the simple agreement construction illustrated in (1) consists of the following rules:
(3) a. Simple agreement→ D MS MV b. D→ the c. MS→ {author, pilot, . . .}",2.2 Data set construction,[0],[0]
"d. MV→ {laughs, smiles, . . .",2.2 Data set construction,[0],[0]
"}
We generate all possible combinations of the terminals.",2.2 Data set construction,[0],[0]
"The Supplementary Materials provide a full description of all our templates.3
While these examples are somewhat artificial, our goal is to isolate the syntactic capabilities of the model; it is in fact beneficial to minimize the semantic or collocational cues that can be used to identify the grammatical sentence.",2.2 Data set construction,[0],[0]
Gulordava et al. took this approach further and constructed “colorless green ideas” test cases by substituting random content words into sentences from a corpus.,2.2 Data set construction,[0],[0]
"We take a more moderate position and avoid combinations that are very implausible or violate selectional restrictions (e.g., the apple laughs).",2.2 Data set construction,[0],[0]
We do this by having separate templates for animate and inanimate subjects and verbs so that the resulting sentences are always reasonably plausible.,2.2 Data set construction,[0],[0]
"Targeted evaluation: LM evaluation data sets using challenging prediction tasks have been proposed in the context of semantics and discourse comprehension (Zweig and Burges, 2011; Paperno et al., 2016).",3 Related work,[0],[0]
"Evaluation sets consisting of chal-
3The code, the data set and the Supplementary Materials can be found at https://github.com/ BeckyMarvin/LM_syneval.
",3 Related work,[0],[0]
"lenging syntactic constructions have been constructed for parser evaluation (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011), and minimal pair approaches have been proposed for evaluating image captioning (Shekhar et al., 2017) and machine translation systems (Sennrich, 2017), but no data sets exist that target a range of syntactic constructions for language model evaluation.
",3 Related work,[0],[0]
Acceptability judgments: Lau et al. (2017) compared the ability of different LMs to predict graded human acceptability judgments.,3 Related work,[0],[0]
"The forced-choice approach used in the current paper has been shown to be effective in human acceptability judgment experiments (Sprouse and Almeida, 2017).",3 Related work,[0],[0]
"In some early work, neural networks were trained explicitly to predict acceptability judgments (Lawrence et al., 1996; Allen and Seidenberg, 1999); Post (2011) likewise trained a classifier on top of a parser to predict grammaticality.",3 Related work,[0],[0]
"Warstadt et al. (2018) use a transfer learning approach, where an unsupervised model is finetuned on acceptability prediction.",3 Related work,[0],[0]
"Our work differs from those studies in that we do not advocate providing any explicit grammaticality signal to the LM at any point (“no negative evidence”).
",3 Related work,[0],[0]
"Syntax in LMs: There have been several proposals over the years to incorporate explicit syntax into LMs to overcome the inability of n-gram LMs to model long-distance dependencies (Jurafsky et al., 1995; Roark, 2001; Pauls and Klein, 2012).",3 Related work,[0],[0]
"While RNN language models can in principle model longer dependencies (Mikolov et al., 2010; Linzen et al., 2016), in practice it can still be beneficial to inject syntax into the model.",3 Related work,[0],[0]
"This can be done by combining it with a supervised parser (Dyer et al., 2016) or other multi-task learning objectives (Enguehard et al., 2017).",3 Related work,[0],[0]
"Our work is orthogonal to this area of research, but can be seen as providing a potential opportunity to underscore the advantage of such syntax-infused models.",3 Related work,[0],[0]
"This section describes all of the types of sentence pairs included in our data set, which include examples of subject-verb agreement (Sections 4.1 and 4.2), reflexive anaphoras (Section 4.3) and negative polarity items (Section 4.4).",4 Data set composition,[0],[0]
"Determining the correct number of the verb is trivial in examples such as (1) above, in which the sentence only contains a single noun.",4.1 Subject-verb agreement,[0],[0]
"By contrast, in cases where there are multiple nouns in the sentence, identifying which of them is the subject of a given verb requires understanding the structure of the sentence.",4.1 Subject-verb agreement,[0],[0]
"In particular, the relevant subject is not necessarily the first noun of the sentence:
(4) Agreement in a sentential complement:",4.1 Subject-verb agreement,[0],[0]
a. The bankers knew the officer smiles.,4.1 Subject-verb agreement,[0],[0]
b. *,4.1 Subject-verb agreement,[0],[0]
"The bankers knew the officer smile.
",4.1 Subject-verb agreement,[0],[0]
Here the verb smiles needs to agree with the embedded subject officer rather than the main clause subject bankers.,4.1 Subject-verb agreement,[0],[0]
"The subject is also not necessarily the most recent noun before the verb: when the subject is modified by a phrase, a distracting noun (“attractor”) often intervenes in the linear order of the sentence between the head of the subject and the verb.",4.1 Subject-verb agreement,[0],[0]
"Two examples of such modifiers are prepositional phrases and relative clauses (RCs):
(5) Agreement across a prepositional phrase: a. The farmer near the parents smiles.",4.1 Subject-verb agreement,[0],[0]
b.,4.1 Subject-verb agreement,[0],[0]
*,4.1 Subject-verb agreement,[0],[0]
"The farmer near the parents smile.
",4.1 Subject-verb agreement,[0],[0]
(6) Agreement across a subject relative clause:,4.1 Subject-verb agreement,[0],[0]
a. The officers that love the skater smile.,4.1 Subject-verb agreement,[0],[0]
b. *,4.1 Subject-verb agreement,[0],[0]
"The officers that love the skater smiles.
",4.1 Subject-verb agreement,[0],[0]
"We include all four possible configurations of noun number for each type of minimal pair; for (5), these would be:4
(7) a.",4.1 Subject-verb agreement,[0],[0]
The farmer near the parent smiles/*smile.,4.1 Subject-verb agreement,[0],[0]
b.,4.1 Subject-verb agreement,[0],[0]
The farmer near the parents smiles/*smile.,4.1 Subject-verb agreement,[0],[0]
c.,4.1 Subject-verb agreement,[0],[0]
The farmers near the parent smile/*smiles.,4.1 Subject-verb agreement,[0],[0]
d.,4.1 Subject-verb agreement,[0],[0]
"The farmers near the parents
smile/*smiles.
",4.1 Subject-verb agreement,[0],[0]
"Sentences where the two nouns conflict in number are expected to be more challenging, but interpretable errors may certainly occur even when they do not.",4.1 Subject-verb agreement,[0],[0]
"For example, the model may use the heuristic that sentences with multiple nouns are likely to have a plural verb (a heuristic that
4The slash notation indicates the word that differs between the grammatical and ungrammatical sentence; for example, in (7a), the full sentence pair would be:
(i) a.",4.1 Subject-verb agreement,[0],[0]
The farmer near the parent smiles.,4.1 Subject-verb agreement,[0],[0]
b.,4.1 Subject-verb agreement,[0],[0]
*,4.1 Subject-verb agreement,[0],[0]
"The farmer near the parent smile.
would be effective for coordination); alternatively, it might prefer singular verbs to plural ones regardless of whether the subject is singular or plural, simply because the singular form of the verb is more frequent.
",4.1 Subject-verb agreement,[0],[0]
"Next, in verb phrase (VP) coordination, both of the verbs need to agree with the subject:
(8) Short VP coordination: a.",4.1 Subject-verb agreement,[0],[0]
The senator smiles and laughs.,4.1 Subject-verb agreement,[0],[0]
b. *,4.1 Subject-verb agreement,[0],[0]
"The senator smiles and laugh.
",4.1 Subject-verb agreement,[0],[0]
We had both singular and plural subjects.,4.1 Subject-verb agreement,[0],[0]
The number of the verb immediately adjacent to the subject was always grammatical.,4.1 Subject-verb agreement,[0],[0]
"This problem can in principle be solved with a trigram model (smiles and laughs is likely to be a more frequent trigram than smiles and laugh); to address this potential concern, we also included a coordination condition with a longer dependency:
(9) Long VP coordination: The manager writes in a journal every day and likes/*like to watch television shows.",4.1 Subject-verb agreement,[0],[0]
"We go into greater depth in object relative clauses, which most clearly require a hierarchical representation.",4.2 Agreement and object relative clauses,[0],[0]
"In (10) and (11), the model needs to be able to distinguish the embedded subject (parents) from the main clause subject (farmer) when making its predictions:
(10) Agreement across an object relative clause: a. The farmer that the parents love swims.",4.2 Agreement and object relative clauses,[0],[0]
b. *,4.2 Agreement and object relative clauses,[0],[0]
"The farmer that the parents love swim.
",4.2 Agreement and object relative clauses,[0],[0]
(11) Agreement in an object relative clause:,4.2 Agreement and object relative clauses,[0],[0]
a. The farmer that the parents love swims.,4.2 Agreement and object relative clauses,[0],[0]
b. *,4.2 Agreement and object relative clauses,[0],[0]
"The farmer that the parents loves swims.
",4.2 Agreement and object relative clauses,[0],[0]
"In keeping with the minimal pair approach, we never introduce two agreement errors at the same time: either the embedded verb or the main verb is incorrectly inflected, but not both.
",4.2 Agreement and object relative clauses,[0],[0]
We include a number of variations on the pattern in (11).,4.2 Agreement and object relative clauses,[0],[0]
"First, we delete the relativizer that, with the hypothesis that the absence of an overt cue to structure will make the task more difficult:
(12) The farmer the parents love/*loves swims.
",4.2 Agreement and object relative clauses,[0],[0]
"In another condition, we replace the main subject with an inanimate noun and keep the embed-
ded subject animate.",4.2 Agreement and object relative clauses,[0],[0]
"We base this manipulation on human experimental work showing that similar nouns (for example, two animate nouns) are more likely to cause confusion during comprehension than dissimilar nouns, such as an animate and an inanimate noun (Van Dyke, 2007):
(13) The movies that the author likes are/*is good.
",4.2 Agreement and object relative clauses,[0],[0]
"For a complete list of all the types of minimal pairs we include, see the Supplementary Materials.",4.2 Agreement and object relative clauses,[0],[0]
A reflexive pronoun such as himself needs to have an antecedent from which it derives its interpretation.,4.3 Reflexive anaphora,[0],[0]
"The pronoun needs to agree in number (and gender) with its antecedent:
(14)",4.3 Reflexive anaphora,[0],[0]
Simple reflexive:,4.3 Reflexive anaphora,[0],[0]
a.,4.3 Reflexive anaphora,[0],[0]
The senators embarrassed themselves.,4.3 Reflexive anaphora,[0],[0]
b. *,4.3 Reflexive anaphora,[0],[0]
"The senators embarrassed herself.
",4.3 Reflexive anaphora,[0],[0]
There are structural conditions on the nouns to which a reflexive pronoun can be bound.,4.3 Reflexive anaphora,[0],[0]
One of these conditions requires the antecedent to be in the same clause as the reflexive pronoun.,4.3 Reflexive anaphora,[0],[0]
"For example, (15b) cannot refer to a context in which the pilot embarrassed the bankers:
(15) Reflexive in a sentential complement: a.",4.3 Reflexive anaphora,[0],[0]
"The bankers thought the pilot embar-
rassed himself.",4.3 Reflexive anaphora,[0],[0]
b. *,4.3 Reflexive anaphora,[0],[0]
"The bankers thought the pilot embar-
rassed themselves.
",4.3 Reflexive anaphora,[0],[0]
"Likewise, in the following minimal pair, sentence (16b) is ungrammatical, because the reflexive pronoun themselves, which is part of the main clause, cannot be bound to the noun phrase the architects, which is inside an embedded clause:
(16) Reflexive across an object relative clause:",4.3 Reflexive anaphora,[0],[0]
"a. The manager that the architects like
doubted himself.",4.3 Reflexive anaphora,[0],[0]
b.,4.3 Reflexive anaphora,[0],[0]
*,4.3 Reflexive anaphora,[0],[0]
"The manager that the architects like
doubted themselves.",4.3 Reflexive anaphora,[0],[0]
"Negative polarity items, introduced in example (2) above, are words that (to a first approximation) need to occur in the context of negation.",4.4 Negative polarity items,[0],[0]
"Crucially for the purposes of the present work, the scope of negation is structurally defined.",4.4 Negative polarity items,[0],[0]
"In particular
the negative noun phrase needs to c-command the NPI: the syntactic non-terminal node that dominates the negative noun phrase must also dominate the NPI.",4.4 Negative polarity items,[0],[0]
"This is the case in (17a), but not in (17b), where the negative noun phrase is too deep in the tree to c-command the NPI ever (Xiang et al., 2009; Everaert et al., 2015).
",4.4 Negative polarity items,[0],[0]
(17) NPI across a relative clause:,4.4 Negative polarity items,[0],[0]
"a. No authors that the security guards like
have ever been famous.",4.4 Negative polarity items,[0],[0]
b. *,4.4 Negative polarity items,[0],[0]
"The authors that no security guards like
have ever been famous.
",4.4 Negative polarity items,[0],[0]
All of the nouns and verbs in the NPI cases were plural.,4.4 Negative polarity items,[0],[0]
"As in some of the agreement cases, we included a variant of (17) in which the subject was inanimate.",4.4 Negative polarity items,[0],[0]
"To show how our challenge set can be used to evaluate the syntactic performance of LMs, we trained three LMs with increasing levels of syntactic sophistication.",5 Experimental setup,[0],[0]
"All of the LMs were trained on a 90 million word subset of Wikipedia (Gulordava et al., 2018).",5 Experimental setup,[0],[0]
Our n-gram LM and LSTM LM do not require annotated data.,5 Experimental setup,[0],[0]
"The third model is also an LSTM LM, but it requires syntactically annotated data (CCG supertags).
N-gram model:",5 Experimental setup,[0],[0]
"We trained a 5-gram model on the same 90M word corpus using the SRILM toolkit (Stolcke, 2002) which backs off to smaller n-grams using Kneser-Ney smoothing.
",5 Experimental setup,[0],[0]
Single-task RNN:,5 Experimental setup,[0],[0]
"The RNN LM had two layers of 650 LSTM units, a batch size of 128, a dropout rate of 0.2, and a learning rate of 20.0, and was trained for 40 epochs (following the hyperparameters of Gulordava et al. 2018).
",5 Experimental setup,[0],[0]
Multi-task RNN:,5 Experimental setup,[0],[0]
"In multi-task learning, the system is trained to optimize an objective function that combines the objective functions of several tasks.",5 Experimental setup,[0],[0]
"We combine language modeling with CCG supertagging, a task that predicts for each word in the sentence its CCG supertag (Bangalore and Joshi, 1999; Lewis et al., 2016).",5 Experimental setup,[0],[0]
"We simply sum the two objective functions with equal weights (Enguehard et al., 2017).",5 Experimental setup,[0],[0]
Early stopping in this model is based on the combined loss on language modeling and supertagging.,5 Experimental setup,[0],[0]
"Supertags provide a large amount of syntactic information
about the word; the sequence of supertags of a sentence strongly constrains the possible parses of the sentence.",5 Experimental setup,[0],[0]
"We use supertagging as a “scaffold” task (Swayamdipta et al., 2017): our goal is not to produce a competitive supertagger, but to induce better syntactic representations, which would then lead to improved language modeling.",5 Experimental setup,[0],[0]
"We used CCG-Bank (Hockenmaier and Steedman, 2007) as our CCG corpus.
Human evaluation: We designed a human experiment on Amazon Mechanical Turk that mirrored the task that was given to the LMs: both versions of a minimal pair were shown on the screen at the same time, and participants were asked to judge which one of them was more acceptable (for details, see the Supplementary Materials).",5 Experimental setup,[0],[0]
We emphasize that we do not see human performance on complex syntactic dependencies as setting an upper bound on the performance that we should expect from an LM.,5 Experimental setup,[0],[0]
"There is a rich literature showing that humans make mistakes such as subject-verb agreement errors; in fact, most of the phenomena we test were inspired by work in psycholinguistics that studies these errors (Bock and Miller, 1991; Phillips et al., 2011).",5 Experimental setup,[0],[0]
"At the same time, while we do not see a reason not to aspire for 100% accuracy, we are interested in comparing LM and human errors: if the errors are similar, the two systems may be using similar representations.",5 Experimental setup,[0],[0]
Local agreement: The overall accuracy per condition can be seen in Table 1.,6 Results,[0],[0]
"The n-gram LM’s accuracy was only 79% for simple agreement and agreement in a sentential complement, both of which can be solved entirely using local context.",6 Results,[0],[0]
"This is because not all subject and verb combinations in our materials appeared verbatim in the 90M word training corpus; for those combinations, the model fell back on unigram probabilities, which in this context amounts to selecting the more frequent form of the verb.
",6 Results,[0],[0]
"Both RNNs performed much better than the n-gram model on the simple agreement case (single-task: 94%; multi-task: 100%), reflecting these models’ ability to generalize beyond the specific bigrams that occurred in the corpus.",6 Results,[0],[0]
Accuracy on agreement in a sentential complement was also very high (single-task: 99%; multi-task: 93%).,6 Results,[0],[0]
"This indicates that the RNNs do not rely on the heuristic whereby the first noun of the sentence
is likely to be its subject.",6 Results,[0],[0]
"They did slightly worse but still very well on short VP coordination (both 90%); this dependency is also local, albeit across the word and.
",6 Results,[0],[0]
Non-local agreement: The accuracy of the n-gram model on non-local dependencies (long VP coordination and agreement across a phrase or a clause) was very close to 50%.,6 Results,[0],[0]
This suggests that local collocational information is not useful in these conditions.,6 Results,[0],[0]
"The single-task RNN also performed much more poorly on these conditions than on the local agreement conditions, though for the most part its accuracy was better than chance.",6 Results,[0],[0]
"Humans did worse on these dependencies as well, but their accuracy did not drop as sharply as the RNNs’ (human accuracies ranged from 82% to 88%).",6 Results,[0],[0]
"In most of these cases, multitask learning was very helpful; for example, accuracy in long VP coordination increased from 61% to 81%.",6 Results,[0],[0]
"Still, both RNNs performed poorly on agreement across an object RC, especially without that, whereas humans performed comparably on all non-local dependencies.
",6 Results,[0],[0]
"Agreement inside an object RC: This case is particularly interesting, because this dependency is purely local (see (11)), and the interference is from the distant sentence-initial noun.",6 Results,[0],[0]
"Although this configuration is similar to the sentential complement case, performance was worse both in RNNs and humans.",6 Results,[0],[0]
"However, RNNs performed better than humans, at least when the sentence included the overt relativizer that.",6 Results,[0],[0]
"This suggests that interference is sensitive to proximity in RNNs but to syntactic status in humans — humans appear to be confusing the main clause subject and the embedded subject (Wagers et al., 2009).
",6 Results,[0],[0]
Reflexive anaphora:,6 Results,[0],[0]
"The RNNs’ performance was significantly worse on simple reflexives (83%) than on simple agreement (94%), and did not differ between the single-task and multi-task models.",6 Results,[0],[0]
"By contrast, human performance did not differ between subject-verb agreement and reflexive anaphoras.",6 Results,[0],[0]
"The surprisingly poor performance for this adjacent dependency seems to be due to an asymmetry in accuracy between himself and themselves on the one hand (100% accuracy in the multi-task RNN) and herself on the other hand (49% accuracy).5 Accuracy was very low for all
5This may be because himself and themselves are significantly more frequent than herself, and consequently the num-
pronouns in the structurally complex case in which the dependency was across a relative clause (55% compared to 87% in humans).
",6 Results,[0],[0]
NPIs:,6 Results,[0],[0]
"The dependency in simple NPIs spans only four words, so the n-gram model could in principle capture it.",6 Results,[0],[0]
"In practice, the n-gram model systematically selected the wrong answer, suggesting that it backed off to comparing the bigrams no students and most students, the first of which is presumably less frequent.",6 Results,[0],[0]
"Surprisingly, the n-gram model’s accuracy was higher than 50% on NPIs across a relative clause, a dependency that spans more than five words.",6 Results,[0],[0]
"In this case, the bigrams that the and the chef (for example) happen to be more frequent than the that no and no chef.",6 Results,[0],[0]
"This difference was apparently strong enough to make up for the low-frequency bigram at the start of the sentence.
",6 Results,[0],[0]
The RNNs did poorly on this task.,6 Results,[0],[0]
The accuracy of the single-task model was around 40%.,6 Results,[0],[0]
The multi-task did somewhat better on the simple NPIs (48%) and much better on the NPIs across a relative clause (73%).,6 Results,[0],[0]
"At the same time, an examination of the plot of log probability of each word in a sentence (Figure A.1 in the Supplementary Materials) suggests that the single-task RNN is in
ber representation learned for herself was not robust.",6 Results,[0],[0]
"Another possibility is that gender bias reduces the probability of an anaphoric relation between herself and words such as surgeon (Rudinger et al., 2018).
",6 Results,[0],[0]
"fact able to differentiate between the grammatical and ungrammatical sentences when it reaches the NPI, but this difference does not offset the overall probability advantage of the ungrammatical sentence (which is likely due to non-grammatical collocational factors).",6 Results,[0],[0]
"In any case, the fact that the n-gram baseline did not perform at chance suggests that there are non-syntactic cues to this task, complicating the interpretation of the performance of other LMs.
",6 Results,[0],[0]
"Perplexity: The perplexity of the n-gram model on the Wikipedia test data was 157.5, much higher than the perplexity of the single-task RNN (78.65) and the multi-task RNN (61.10).",6 Results,[0],[0]
"In other words, perplexity tracked accuracy on our syntactic data set – an unsatisfying outcome given our goal of dissociating perplexity and our syntactic evaluation method, but an expected one given that each model was conditioned on richer information than the previous one.",6 Results,[0],[0]
"In previous work, perplexity and syntactic judgment accuracy have been found to be partly dissociable (Kuncoro et al., 2018; Tran et al., 2018).
",6 Results,[0],[0]
Lexical variation and frequency: There was considerable lexical variation in the results; we have mentioned the surprising asymmetry between himself and herself above.,6 Results,[0],[0]
"As another case study, we examine variation in the results of the simple agreement condition in the single-
task RNN.",6 Results,[0],[0]
"Accuracy varied by verb, ranging from is and are, which had 100% accuracy, to swims, where accuracy was only 60% (recall that average accuracy was 94%).",6 Results,[0],[0]
"This may be a frequency effect: either the LM is learning less robust number representations for infrequent verbs, or the tail of the distribution over the vocabulary is more fragile during word prediction.",6 Results,[0],[0]
Pauls and Klein (2012) propose normalizing for unigram frequency when deriving acceptability judgments from an LM.,6 Results,[0],[0]
"Our preliminary experiments with this method did not significantly improve overall performance; regardless of the effectiveness of this method, such corrections should arguably not be necessary in an LM that adequately captures grammaticality.",6 Results,[0],[0]
The overall results in Table 1 were averaged over all of the possible number configurations within each condition.,7 Case study: agreement and object relative clauses,[0],[0]
"In this section, we take a closer look at agreement in sentences with an object RC (see Table 2).",7 Case study: agreement and object relative clauses,[0],[0]
"This kind of finer-grained analysis helps explain the cases in which the LMs are failing, and might reveal some of the patterns or heuristics the LMs are using.
",7 Case study: agreement and object relative clauses,[0],[0]
Performance in agreement across an object RC was poor.,7 Case study: agreement and object relative clauses,[0],[0]
Both RNNs made attraction errors: they often preferred the verb that agreed in number with the irrelevant embedded subject to the verb that agreed with the correct main subject.,7 Case study: agreement and object relative clauses,[0],[0]
"The multitask RNN showed greater symmetry between the simpler singular/singular and plural/plural cases, whereas the single-task RNN performed poorly even in these cases, often preferring a singular
verb when both subjects were plural.",7 Case study: agreement and object relative clauses,[0],[0]
"This default preference for singular verbs matches the behavior of younger children (Franck et al., 2004).
",7 Case study: agreement and object relative clauses,[0],[0]
"Performance in agreement within an object RC was better; still, the single-task RNN made the most errors when both subjects were singular, perhaps due to a heuristic in which a sentence with multiple subjects is likely to have a plural verb (as in coordination sentences).",7 Case study: agreement and object relative clauses,[0],[0]
"By contrast, the multitask model seemed to have a general bias towards singular subjects in this condition.",7 Case study: agreement and object relative clauses,[0],[0]
"Incidentally, the human results with object RCs were also unexpected: while attraction errors when the two subjects differ in number are to be expected (Wagers et al., 2009), our participants made a sizable number of errors even when both subjects were plural.
",7 Case study: agreement and object relative clauses,[0],[0]
"Despite the generally poor performance in object RCs, Figures A.2 and A.3 in the Supplementary Materials show that the single-task RNN is typically assigning a higher probability to the grammatical word of a minimal pair than to the ungrammatical word.",7 Case study: agreement and object relative clauses,[0],[0]
We have described a template-based data set for targeted syntactic evaluation of language models.,8 Discussion,[0],[0]
"The data set consists of pairs of sentences that are matched except for their grammaticality; we consider a language model to capture the relevant aspects of the grammar of the language if it assigns a higher probability to the grammatical sentence than to the ungrammatical one.
",8 Discussion,[0],[0]
"An RNN language model performed very well on local subject-verb agreement dependencies, significantly outperforming an n-gram baseline.
",8 Discussion,[0],[0]
This suggests that the task is a viable evaluation strategy.,8 Discussion,[0],[0]
"Even on simple cases, however, the RNN’s accuracy was sensitive to the particular lexical items that occurred in the sentence; this would not be expected if its syntactic representations were fully abstract.",8 Discussion,[0],[0]
"The RNN’s performance degraded markedly on non-local dependencies, approaching chance levels on agreement across an object relative clause.",8 Discussion,[0],[0]
Multi-task training with a syntactic objective (CCG supertagging) mitigated this drop in performance for some but not all of the dependencies we tested.,8 Discussion,[0],[0]
"We conjecture that the benefits of the inductive bias conferred by multi-task learning will be amplified when the amount of training data is limited.
",8 Discussion,[0],[0]
"Our results contrast with the results of Gulordava et al. (2018), who obtained a prediction accuracy of 81% on English sentences from their test corpus and 74% on constructed sentences modeled after sentences from the corpus.",8 Discussion,[0],[0]
"It is likely that our sentences are more syntactically challenging than the ones they were able to find in the relatively small manually annotated treebank they used.
",8 Discussion,[0],[0]
One limitation of our approach is that it is not always clear what constitutes a minimal grammaticality contrast.,8 Discussion,[0],[0]
"In the subject-verb agreement case, the contrast was clear: the two present-tense forms of the verb, e.g., laugh vs. laughs.",8 Discussion,[0],[0]
"Our NPI manipulations, on the other hand, were less successful: the members of the contrasts differed not only in their syntactic structure but also in low-level n-gram probabilities, making the performance on this particular contrast harder to interpret.
",8 Discussion,[0],[0]
"We emphasize that the goal of this article was not to advocate for LSTMs in particular as an effective architecture for modeling syntax; indeed, our results show that LSTM language models are far from matching naive annotators’ performance on this task, let alone performing at 100% accuracy.",8 Discussion,[0],[0]
"We hope that our data set, and future extensions to other phenomena and languages, will make it possible to measure progress in syntactic language modeling and will lead to better understanding of the syntactic generalizations captured by language models.",8 Discussion,[0],[0]
We would like to thank Ming Xiang for sharing materials from human experiments that inspired many of our test cases.,9 Acknowledgments,[0],[0]
"We also thank Brian Roark and the JHU Computational Psycholinguistics lab
for discussion, and Brian Leonard for help conducting the human experiment.",9 Acknowledgments,[0],[0]
We present a dataset for evaluating the grammaticality of the predictions of a language model.,abstractText,[0],[0]
"We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence.",abstractText,[0],[0]
"The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items.",abstractText,[0],[0]
We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one.,abstractText,[0],[0]
"In an experiment using this data set, an LSTM language model performed poorly on many of the constructions.",abstractText,[0],[0]
"Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM’s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online.",abstractText,[0],[0]
This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.,abstractText,[0],[0]
Targeted Syntactic Evaluation of Language Models,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 574–583 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
Search engines help us find what we need among the vast array of available data.,1 Introduction,[0],[0]
"When we request some information using a long or inexact description of it, these systems, however, often fail to deliver relevant items.",1 Introduction,[0],[0]
"In this case, what typically follows is an iterative process in which we try to express our need differently in the hope that the system will return what we want.",1 Introduction,[0],[0]
This is a major issue in information retrieval.,1 Introduction,[0],[0]
"For instance, Huang and Efthimiadis (2009) estimate that 28-52% of all the web queries are modifications of previous ones.
",1 Introduction,[0],[0]
"To a certain extent, this problem occurs because search engines rely on matching words in the query with words in relevant documents, to
perform retrieval.",1 Introduction,[0],[0]
"If there is a mismatch between them, a relevant document may be missed.
",1 Introduction,[0],[0]
One way to address this problem is to automatically rewrite a query so that it becomes more likely to retrieve relevant documents.,1 Introduction,[1.0],['One way to address this problem is to automatically rewrite a query so that it becomes more likely to retrieve relevant documents.']
This technique is known as automatic query reformulation.,1 Introduction,[0],[0]
"It typically expands the original query by adding terms from, for instance, dictionaries of synonyms such as WordNet (Miller, 1995), or from the initial set of retrieved documents (Xu and Croft, 1996).",1 Introduction,[0],[0]
"This latter type of reformulation is known as pseudo (or blind) relevance feedback (PRF), in which the relevance of each term of the retrieved documents is automatically inferred.
",1 Introduction,[0],[0]
"The proposed method is built on top of PRF but differs from previous works as we frame the query
574
reformulation problem as a reinforcement learning (RL) problem.",1 Introduction,[0],[0]
"An initial query is the natural language expression of the desired goal, and an agent (i.e. reformulator) learns to reformulate an initial query to maximize the expected return (i.e. retrieval performance) through actions (i.e. selecting terms for a new query).",1 Introduction,[0],[0]
The environment is a search engine which produces a new state (i.e. retrieved documents).,1 Introduction,[1.0],['The environment is a search engine which produces a new state (i.e. retrieved documents).']
"Our framework is illustrated in Fig. 1.
",1 Introduction,[0],[0]
The most important implication of this framework is that a search engine is treated as a black box that an agent learns to use in order to retrieve more relevant items.,1 Introduction,[0],[0]
This opens the possibility of training an agent to use a search engine for a task other than the one it was originally intended for.,1 Introduction,[0],[0]
"To support this claim, we evaluate our agent on the task of question answering (Q&A), citation recommendation, and passage/snippet retrieval.
",1 Introduction,[1.0000000104415157],"['To support this claim, we evaluate our agent on the task of question answering (Q&A), citation recommendation, and passage/snippet retrieval.']"
"As for training data, we use two publicly available datasets (TREC-CAR and Jeopardy) and introduce a new one (MS Academic) with hundreds of thousands of query/relevant document pairs from the academic domain.
",1 Introduction,[1.0000000091167751],"['As for training data, we use two publicly available datasets (TREC-CAR and Jeopardy) and introduce a new one (MS Academic) with hundreds of thousands of query/relevant document pairs from the academic domain.']"
"Furthermore, we present a method to estimate the upper bound performance of our RL-based model.",1 Introduction,[0],[0]
"Based on the estimated upper bound, we claim that this framework has a strong potential for future improvements.
",1 Introduction,[0],[0]
"Here we summarize our main contributions:
•",1 Introduction,[0],[0]
"A reinforcement learning framework for automatic query reformulation.
",1 Introduction,[0],[0]
"• A simple method to estimate the upper-bound performance of an RL-based model in a given environment.
",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
A new large dataset with hundreds of thousands of query/relevant document pairs.1,1 Introduction,[0],[0]
"In this section we describe the proposed method, illustrated in Fig. 2.
",2.1 Model Description,[0],[0]
"The inputs are a query q0 consisting of a sequence of words (w1, ..., wn) and a candidate term ti with some context words (ti−k, ..., ti+k), where k ≥ 0 is the context window size.",2.1 Model Description,[0],[0]
"Candidate terms
1The dataset and code to run the experiments are available at https://github.com/nyu-dl/ QueryReformulator.
are from q0 ∪ D0, the union of the terms in the original query and those from the documents D0 retrieved using q0.
",2.1 Model Description,[0.9581030772354566],"['Therefore, the numbers reported in the results section were all obtained from models running two rounds of search and reformulation.']"
"We use a dictionary of pretrained word embeddings (Mikolov et al., 2013) to convert the symbolic terms wj",2.1 Model Description,[0],[0]
"and ti to their vector representations vj and ei ∈ Rd, respectively.",2.1 Model Description,[0],[0]
"We map outof-vocabulary terms to an additional vector that is learned during training.
",2.1 Model Description,[0],[0]
"We convert the sequence {vj} to a fixed-size vector φa(v) by using either a Convolutional Neural Network (CNN) followed by a max pooling operation over the entire sequence (Kim, 2014) or by using the last hidden state of a Recurrent Neural Network (RNN).2
Similarly, we fed the candidate term vectors ei to a CNN or RNN to obtain a vector representation φb(ei) for each term ti.",2.1 Model Description,[1.0000000047852895],"['We convert the sequence {vj} to a fixed-size vector φa(v) by using either a Convolutional Neural Network (CNN) followed by a max pooling operation over the entire sequence (Kim, 2014) or by using the last hidden state of a Recurrent Neural Network (RNN).2 Similarly, we fed the candidate term vectors ei to a CNN or RNN to obtain a vector representation φb(ei) for each term ti.']"
"The convolutional/recurrent layers serve an important role of capturing context information, especially for outof-vocabulary and rare terms.",2.1 Model Description,[0],[0]
"CNNs can process candidate terms in parallel, and, therefore, are faster for our application than RNNs.",2.1 Model Description,[0],[0]
"RNNs, on the other hand, can encode longer contexts.
",2.1 Model Description,[0],[0]
"Finally, we compute the probability of selecting
2To deal with variable-length inputs in a mini-batch, we pad smaller ones with zeros on both ends so they end up as long as the largest sample in the mini-batch.
ti as:
P (ti|q0) = σ(UT tanh(W (φa(v)‖φb(ei))",2.1 Model Description,[0],[0]
"+ b)), (1)
where σ is the sigmoid function, ‖ is the vector concatenation operation,",2.1 Model Description,[0],[0]
"W ∈ Rd×2d and U ∈ Rd are weights, and b ∈ R is a bias.
",2.1 Model Description,[0],[0]
"At test time, we define the set of terms used in the reformulated query as T = {ti | P (ti|q0) > }, where is a hyper-parameter.",2.1 Model Description,[0],[0]
"At training time, we sample the terms according to their probability distribution, T = {ti | α = 1∧α ∼ P (ti|q0)}.",2.1 Model Description,[0],[0]
"We concatenate the terms in T to form a reformulated query q′, which will then be used to retrieve a new set of documents D′.",2.1 Model Description,[1.0],"['We concatenate the terms in T to form a reformulated query q′, which will then be used to retrieve a new set of documents D′.']"
One problem with the method previously described is that terms are selected independently.,2.2 Sequence Generation,[0],[0]
This may result in a reformulated query that contains duplicated terms since the same term can appear multiple times in the feedback documents.,2.2 Sequence Generation,[0],[0]
"Another problem is that the reformulated query can be very long, resulting in a slow retrieval.
",2.2 Sequence Generation,[0],[0]
"To solve these problems, we extend the model to sequentially generate a reformulated query, as proposed by Buck et al. (2017).",2.2 Sequence Generation,[0],[0]
We use a Recurrent Neural Network (RNN) that selects one term at a time from the pool of candidate terms and stops when a special token is selected.,2.2 Sequence Generation,[0],[0]
The advantage of this approach is that the model can remember the terms previously selected through its hidden state.,2.2 Sequence Generation,[0],[0]
"It can, therefore, produce more concise queries.
",2.2 Sequence Generation,[0],[0]
"We define the probability of selecting ti as the k-th term of a reformulated query as:
P (tki |q0) ∝",2.2 Sequence Generation,[0],[0]
"exp(φb(ei)Thk), (2) where hk is the hidden state vector at the k-th step, computed as:
hk = tanh(Waφa(v)",2.2 Sequence Generation,[0],[0]
"+Wbφb(tk−1) +Whhk−1), (3) where tk−1 is the term selected in the previous step and Wa ∈ Rd×d, Wb ∈ Rd×d, and Wh ∈ Rd×d are weight matrices.",2.2 Sequence Generation,[0],[0]
"In practice, we use an LSTM (Hochreiter and Schmidhuber, 1997) to encode the hidden state as this variant is known to perform better than a vanilla RNN.
",2.2 Sequence Generation,[0],[0]
We avoid normalizing over a large vocabulary by using only terms from the retrieved documents.,2.2 Sequence Generation,[0],[0]
"This makes inference faster and training practical since learning to select words from the whole
vocabulary might be too slow with reinforcement learning, although we leave this experiment for a future work.",2.2 Sequence Generation,[0],[0]
"We train the proposed model using REINFORCE (Williams, 1992) algorithm.",2.3 Training,[0],[0]
"The perexample stochastic objective is defined as
Ca = (R− R̄) ∑ t∈T",2.3 Training,[0],[0]
"− logP (t|q0), (4)
where R is the reward and R̄ is the baseline, computed by the value network as:
R̄ = σ(ST tanh(V (φa(v)‖ē) + b)), (5) where ē = 1N ∑N i=1 φb(ei), N = |q0 ∪ D0|, V ∈ Rd×2d and S ∈ Rd are weights and b ∈ R is a bias.",2.3 Training,[0],[0]
"We train the value network to minimize
Cb = α||R− R̄||2, (6) where α is a small constant (e.g. 0.1) multiplied to the loss in order to stabilize learning.",2.3 Training,[0],[0]
We conjecture that the stability is due to the slowly evolving value network which directly affects the learning of the policy.,2.3 Training,[0],[0]
"This effectively prevents the value network to fit extreme cases (unexpectedly high or low reward.)
",2.3 Training,[0],[0]
"We minimize Ca and Cb using stochastic gradient descent (SGD) with the gradient computed by backpropagation (Rumelhart et al., 1988).",2.3 Training,[0],[0]
"This allows the entire model to be trained end-to-end directly to optimize the retrieval performance.
",2.3 Training,[0],[0]
Entropy Regularization We observed that the probability distribution in Eq.(1) became highly peaked in preliminary experiments.,2.3 Training,[0],[0]
This phenomenon led to the trained model not being able to explore new terms that could lead to a betterreformulated query.,2.3 Training,[0],[0]
We address this issue by regularizing the negative entropy of the probability distribution.,2.3 Training,[1.0],['We address this issue by regularizing the negative entropy of the probability distribution.']
We add the following regularization term to the original cost function in Eq.,2.3 Training,[1.0],['We add the following regularization term to the original cost function in Eq.']
"(4):
CH = −λ ∑
t∈q0∪D0 P (t|q0) logP (t|q0), (7)
where λ is a regularization coefficient.",2.3 Training,[0],[0]
"Query reformulation techniques are either based on a global method, which ignores a set of documents returned by the original query, or a local
method, which adjusts a query relative to the documents that initially appear to match the query.",3 Related Work,[0],[0]
"In this work, we focus on local methods.
",3 Related Work,[0],[0]
"A popular instantiation of a local method is the relevance model, which incorporates pseudo-relevance feedback into a language model form (Lavrenko and Croft, 2001).",3 Related Work,[0],[0]
The probability of adding a term to an expanded query is proportional to its probability of being generated by the language models obtained from the original query and the document the term occurs in.,3 Related Work,[0],[0]
"This framework has the advantage of not requiring query/relevant documents pairs as training data since inference is based on word co-occurrence statistics.
",3 Related Work,[0],[0]
"Unlike the relevance model, algorithms can be trained with supervised learning, as proposed by Cao et al. (2008).",3 Related Work,[0],[0]
A training dataset is automatically created by labeling each candidate term as relevant or not based on their individual contribution to the retrieval performance.,3 Related Work,[0],[0]
Then a binary classifier is trained to select expansion terms.,3 Related Work,[0],[0]
"In Section 4, we present a neural network-based implementation of this supervised approach.
",3 Related Work,[0],[0]
A generalization of this supervised framework is to iteratively reformulate the query by selecting one candidate term at each retrieval step.,3 Related Work,[0],[0]
"This can be viewed as navigating a graph where the nodes represent queries and associated retrieved results and edges exist between nodes whose queries are simple reformulations of each other (Diaz, 2016).",3 Related Work,[0],[0]
"However, it can be slow to reformulate a query this way as the search engine must be queried for each newly added term.",3 Related Work,[0],[0]
"In our method, on the contrary, the search engine is queried with multiple new terms at once.
",3 Related Work,[0],[0]
"An alternative technique based on supervised learning is to learn a common latent representation of queries and relevant documents terms by using a click-through dataset (Sordoni et al., 2014).",3 Related Work,[0],[0]
Neighboring document terms of a query in the latent space are selected to form an expanded query.,3 Related Work,[0],[0]
"Instead of using a click-through dataset, which is often proprietary, it is possible to use an alternative dataset consisting of anchor text/title pairs.",3 Related Work,[0],[0]
"In contrast, our approach does not require a dataset of paired queries as it learns term selection strategies via reinforcement learning.
",3 Related Work,[0],[0]
"Perhaps the closest work to ours is that by Narasimhan et al. (2016), in which a reinforcement learning based approach is used to reformu-
late queries iteratively.",3 Related Work,[0],[0]
A key difference is that in their work the reformulation component uses domain-specific template queries.,3 Related Work,[0],[0]
"Our method, on the other hand, assumes open-domain queries.",3 Related Work,[0],[0]
"In this section we describe our experimental setup, including baselines against which we compare the proposed method, metrics, reward for RL-based models, datasets and implementation details.",4 Experiments,[0],[0]
Raw: The original query is given to a search engine without any modification.,4.1 Baseline Methods,[0],[0]
"We evaluate two search engines in their default configuration: Lucene3 (Raw-Lucene) and Google Search4 (Raw-Google).
",4.1 Baseline Methods,[0],[0]
Pseudo Relevance Feedback (PRF-TFIDF):,4.1 Baseline Methods,[0],[0]
A query is expanded with terms from the documents retrieved by a search engine using the original query.,4.1 Baseline Methods,[0],[0]
"In this work, the top-N TF-IDF terms from each of the top-K retrieved documents are added to the original query, where N and K are selected by a grid search on the validation data.
",4.1 Baseline Methods,[1.0000000590458926],"['In this work, the top-N TF-IDF terms from each of the top-K retrieved documents are added to the original query, where N and K are selected by a grid search on the validation data.']"
PRF-Relevance Model (PRF-RM):,4.1 Baseline Methods,[0],[0]
This is a popular relevance model for query expansion by Lavrenko and Croft (2001).,4.1 Baseline Methods,[0],[0]
"The probability of using a term t in an expanded query is given by:
P (t|q0) =",4.1 Baseline Methods,[0],[0]
(1− λ)P ′(t|q0),4.1 Baseline Methods,[0],[0]
"+ λ ∑ d∈D0 P (d)P (t|d)P (q0|d), (8)
where P (d) is the probability of retrieving the document d, assumed uniform over the set, P (t|d) and P (q0|d) are the probabilities assigned by the language model obtained from d to t and q0, respectively.",4.1 Baseline Methods,[0],[0]
"P ′(t|q0) = tf(t∈q)|q| , where tf(t, d) is the term frequency of t in d. We set the interpolation parameter λ to 0.5, following Zhai and Lafferty (2001).
",4.1 Baseline Methods,[0],[0]
"We use a Dirichlet smoothed language model (Zhai and Lafferty, 2001) to compute a language model from a document d ∈ D0:
P (t|d) = tf(t, d) + uP (t|C)|d|+ u , (9)
3https://lucene.apache.org/ 4https://cse.google.com/cse/
where u is a scalar constant (u = 1500 in our experiments), and P (t|C) is the probability of t occurring in the entire corpus C.
We use the N terms with the highest P (t|q0) in an expanded query, whereN is a hyper-parameter.
",4.1 Baseline Methods,[0],[0]
"Embeddings Similarity: Inspired by the methods proposed by Roy et al. (2016) and Kuzi et al. (2016), the top-N terms are selected based on the cosine similarity of their embeddings against the original query embedding.",4.1 Baseline Methods,[0],[0]
"Candidate terms come from documents retrieved using the original query (PRF-Emb), or from a fixed vocabulary (Vocab-Emb).",4.1 Baseline Methods,[0],[0]
"We use pretrained embeddings from Mikolov et al. (2013), and it contains 374,000 words.",4.1 Baseline Methods,[0],[0]
Supervised Learning (SL): Here we detail a deep learning-based variant of the method proposed by Cao et al. (2008).,4.2 Proposed Methods,[0],[0]
It assumes that query terms contribute independently to the retrieval performance.,4.2 Proposed Methods,[0],[0]
We thus train a binary classifier to select a term if the retrieval performance increases beyond a preset threshold when that term is added to the original query.,4.2 Proposed Methods,[0],[0]
"More specifically, we mark a term as relevant if (R′ −R)/R > 0.005, where R and R′ are the retrieval performances of the original query and the query expanded with the term, respectively.
",4.2 Proposed Methods,[0],[0]
"We experiment with two variants of this method: one in which we use a convolutional network for both original query and candidate terms (SL-CNN), and the other in which we replace the convolutional network with a single hidden layer feed-forward neural network (SL-FF).",4.2 Proposed Methods,[0],[0]
"In this variant, we average the output vectors of the neural network to obtain a fixed size representation of q0.
",4.2 Proposed Methods,[0],[0]
Reinforcement Learning (RL): We use multiple variants of the proposed RL method.,4.2 Proposed Methods,[1.0],['Reinforcement Learning (RL): We use multiple variants of the proposed RL method.']
"RL-CNN and RL-RNN are the models described in Section 2.1, in which the former uses CNNs to encode query and term features and the latter uses RNNs (more specifically, bidirectional LSTMs).",4.2 Proposed Methods,[0],[0]
RL-FF is the model in which term and query vectors are encoded by single hidden layer feed-forward neural networks.,4.2 Proposed Methods,[0],[0]
"In the RL-RNN-SEQ model, we add the sequential generator described in Section 2.2 to the RL-RNN variant.",4.2 Proposed Methods,[0],[0]
"We summarize in Table 1 the datasets.
",4.3 Datasets,[0],[0]
TREC - Complex Answer Retrieval (TRECCAR),4.3 Datasets,[0],[0]
"This is a publicly available dataset automatically created from Wikipedia whose goal is to encourage the development of methods that respond to more complex queries with longer answers (Dietz and Ben, 2017).",4.3 Datasets,[0],[0]
A query is the concatenation of an article title and one of its section titles.,4.3 Datasets,[1.0],['A query is the concatenation of an article title and one of its section titles.']
The ground-truth documents are the paragraphs within that section.,4.3 Datasets,[1.0],['The ground-truth documents are the paragraphs within that section.']
"For example, a query is “Sea Turtle, Diet” and the ground truth documents are the paragraphs in the section “Diet” of the “Sea Turtle” article.",4.3 Datasets,[0],[0]
"The corpus consists of all the English Wikipedia paragraphs, except the abstracts.",4.3 Datasets,[0],[0]
"The released dataset has five predefined folds, and we use the first three as the training set and the remaining two as validation and test sets, respectively.
",4.3 Datasets,[0],[0]
Jeopardy This is a publicly available Q&A dataset introduced by Nogueira and Cho (2016).,4.3 Datasets,[0],[0]
A query is a question from the Jeopardy! TV Show and the corresponding document is a Wikipedia article whose title is the answer.,4.3 Datasets,[0],[0]
"For example, a query is “For the last eight years of his life, Galileo was under house arrest for espousing this mans theory” and the answer is the Wikipedia article titled “Nicolaus Copernicus”.",4.3 Datasets,[0],[0]
"The corpus consists of all the articles in the English Wikipedia.
",4.3 Datasets,[0],[0]
Microsoft Academic (MSA),4.3 Datasets,[0],[0]
This dataset consists of academic papers crawled from Microsoft Academic API.5,4.3 Datasets,[0],[0]
"The crawler started at the paper Silver et al. (2016) and traversed the graph of references until 500,000 papers were crawled.",4.3 Datasets,[0.9691215332815883],"['Microsoft Academic (MSA) This dataset consists of academic papers crawled from Microsoft Academic API.5 The crawler started at the paper Silver et al. (2016) and traversed the graph of references until 500,000 papers were crawled.']"
We then removed papers that had no reference within or whose abstract had less than 100 characters.,4.3 Datasets,[0],[0]
"We ended up with 480,000 papers.
",4.3 Datasets,[0],[0]
"A query is the title of a paper, and the groundtruth answer consists of the papers cited within.",4.3 Datasets,[0],[0]
Each document in the corpus consists of its title and abstract.6,4.3 Datasets,[0],[0]
"Three metrics are used to evaluate performance:
Recall@K:",4.4 Metrics and Reward,[0],[0]
"Recall of the top-K retrieved documents:
R@K = |DK",4.4 Metrics and Reward,[0],[0]
"∩D∗| |D∗| , (10)
5https://www.microsoft.com/cognitive-services/enus/academic-knowledge-api
6This was done to avoid a large computational overhead for indexing full papers.
where DK are the top-K retrieved documents and D∗ are the relevant documents.",4.4 Metrics and Reward,[0],[0]
"Since one of the goals of query reformulation is to increase the proportion of relevant documents returned, recall is our main metric.
",4.4 Metrics and Reward,[0],[0]
"Precision@K: Precision of the top-K retrieved documents:
P@K = |DK",4.4 Metrics and Reward,[0],[0]
∩D∗| |DK,4.4 Metrics and Reward,[0],[0]
"| (11)
Precision captures the proportion of relevant documents among the returned ones.",4.4 Metrics and Reward,[0],[0]
"Despite not being the main goal of a reformulation method, improvements in precision are also expected with a good query reformulation method.",4.4 Metrics and Reward,[0],[0]
"Therefore, we include this metric.
",4.4 Metrics and Reward,[0],[0]
Mean Average Precision:,4.4 Metrics and Reward,[0],[0]
"The average precision of the top-K retrieved documents is defined as:
AP@K = ∑K
k=1 P@k × rel(k) |D∗| , (12)
where
rel(k) = { 1, if the k-th document is relevant; 0, otherwise.
",4.4 Metrics and Reward,[0],[0]
"(13)
The mean average precision of a set of queries Q is then:
MAP@K = 1 |Q| ∑ q∈Q AP@Kq, (14)
where AP@Kq is the average precision at K for a query q.",4.4 Metrics and Reward,[0],[0]
"This metric values the position of a relevant document in a returned list and is, therefore, complementary to precision and recall.
",4.4 Metrics and Reward,[0],[0]
"Reward We use R@K as a reward when training the proposed RL-based models as this metric has shown to be effective in improving the other metrics as well.
",4.4 Metrics and Reward,[0],[0]
"SL-Oracle In addition to the baseline methods and proposed reinforcement learning approach, we report two oracle performance bounds.",4.4 Metrics and Reward,[0],[0]
The first oracle is a supervised learning oracle (SLOracle).,4.4 Metrics and Reward,[0],[0]
It is a classifier that perfectly selects terms that will increase performance according to the procedure described in Section 4.2.,4.4 Metrics and Reward,[0],[0]
This measure serves as an upper-bound for the supervised methods.,4.4 Metrics and Reward,[0],[0]
Notice that this heuristic assumes that each term contributes independently from all the other terms to the retrieval performance.,4.4 Metrics and Reward,[0],[0]
"There may be, however, other ways to explore the dependency of terms that would lead to a higher performance.
",4.4 Metrics and Reward,[0],[0]
"RL-Oracle Second, we introduce a reinforcement learning oracle (RL-Oracle) which estimates a conservative upper-bound performance for the RL models.",4.4 Metrics and Reward,[0],[0]
"Unlike the SL-Oracle, it does not assume that each term contributes independently to the retrieval performance.",4.4 Metrics and Reward,[0],[0]
"It works as follows: first, the validation or test set is divided into N small subsets {Ai}Ni=1 (each with 100 examples, for instance).",4.4 Metrics and Reward,[0],[0]
"An RL model is trained on each subset Ai until it overfits, that is, until the reward R∗i stops increasing or an early stop mechanism ends training.7",4.4 Metrics and Reward,[0],[0]
"Finally, we compute the oracle performance R∗ as the average reward over all the subsets: R∗ = 1N ∑N i=1R ∗",4.4 Metrics and Reward,[0],[0]
"i .
",4.4 Metrics and Reward,[0],[0]
"This upper bound by the RL-Oracle is, however, conservative since there might exist better reformulation strategies that the RL model was not able to discover.",4.4 Metrics and Reward,[0],[0]
"Search engine We use Lucene and BM25 as the search engine and the ranking function, respectively, for all PRF, SL and RL methods.",4.5 Implementation Details,[0],[0]
"For RawGoogle, we restrict the search to the wikipedia.org domain when evaluating its performance on the Jeopardy dataset.",4.5 Implementation Details,[0],[0]
"We could not apply the same restriction to the two other datasets as Google does not index Wikipedia paragraphs, and as it is not trivial to match papers from MS Academic to the ones returned by Google Search.
Candidate terms We use Wikipedia articles as a source for candidate terms since it is a well curated, clean corpus, with diverse topics.
",4.5 Implementation Details,[0],[0]
"At training and test times of SL methods, and at test time of RL methods, the candidate terms are from the first M words of the top-K Wikipedia articles retrieved.",4.5 Implementation Details,[0],[0]
"We select M and K using grid search on the validation set over {50, 100, 200, 300} and {1, 3, 5, 7}, respectively.",4.5 Implementation Details,[0],[0]
The best values are M = 300 and K = 7.,4.5 Implementation Details,[0],[0]
"These correspond to the maximum number of terms we could fit in a single GPU.
7The subset should be small enough, or the model should be large enough so it can overfit.
",4.5 Implementation Details,[0],[0]
"At training time of an RL model, we use only one document uniformly sampled from the top-K retrieved ones as a source for candidate terms, as this leads to a faster learning.
",4.5 Implementation Details,[0],[0]
"For the PRF methods, the top-M terms according to a relevance metric (i.e., TF-IDF for PRF-TFIDF, cosine similarity for PRF-Emb, and conditional probability for PRF-RM) from each of the top-K retrieved documents are added to the original query.",4.5 Implementation Details,[1.0],"['For the PRF methods, the top-M terms according to a relevance metric (i.e., TF-IDF for PRF-TFIDF, cosine similarity for PRF-Emb, and conditional probability for PRF-RM) from each of the top-K retrieved documents are added to the original query.']"
"We select M and K using grid search over {10, 50, 100, 200, 300, 500} and {1, 3, 5, 9, 11}, respectively.",4.5 Implementation Details,[0],[0]
"The best values are M = 300 and K = 9.
",4.5 Implementation Details,[0],[0]
"Multiple Reformulation Rounds Although our framework supports multiple rounds of search and reformulation, we did not find any significant improvement in reformulating a query more than once.",4.5 Implementation Details,[0],[0]
"Therefore, the numbers reported in the results section were all obtained from models running two rounds of search and reformulation.
",4.5 Implementation Details,[0],[0]
"Neural Network Setup For SL-CNN and RLCNN variants, we use a 2-layer convolutional network for the original query.",4.5 Implementation Details,[0],[0]
Each layer has a window size of 3 and 256 filters.,4.5 Implementation Details,[0],[0]
"We use a 2-layer convolutional network for candidate terms with window sizes of 9 and 3, respectively, and 256 filters in each layer.",4.5 Implementation Details,[1.0],"['We use a 2-layer convolutional network for candidate terms with window sizes of 9 and 3, respectively, and 256 filters in each layer.']"
"We set the dimension d of the weight matrices W,S,U , and V to 256.",4.5 Implementation Details,[0],[0]
"For the optimizer, we use ADAM (Kingma and Ba, 2014) with α = 10−4, β1 = 0.9, β2 = 0.999, and = 10−8.",4.5 Implementation Details,[0],[0]
"We set the entropy regularization coefficient λ to 10−3.
",4.5 Implementation Details,[0],[0]
"For RL-RNN and RL-RNN-SEQ, we use a 2- layer bidirectional LSTM with 256 hidden units in each layer.",4.5 Implementation Details,[0],[0]
We clip the gradients to unit norm.,4.5 Implementation Details,[0],[0]
"For RL-RNN-SEQ, we set the maximum possible
number of generated terms to 50 and we use beam search of size four at test time.
",4.5 Implementation Details,[0],[0]
"We fix the dictionary of pre-trained word embeddings during training, except the vector for outof-vocabulary words.",4.5 Implementation Details,[0],[0]
We found that this led to faster convergence and observed no difference in the overall performance when compared to learning embeddings during training.,4.5 Implementation Details,[0],[0]
Table 2 shows the main result.,5 Results and Discussion,[0],[0]
"As expected, reformulation based methods work better than using the original query alone.",5 Results and Discussion,[0],[0]
"Supervised methods (SL-FF and SL-CNN) have in general a better performance than unsupervised ones (PRF-TFIDF, PRF-RM, PRF-Emb, and Emb-Vocab), but perform worse than RL-based models (RL-FF, RLCNN, RL-RNN, and RL-RNN-SEQ).
",5 Results and Discussion,[0],[0]
"RL-RNN-SEQ performs slightly worse than RL-RNN but produces queries that are three times shorter, on average (15 vs 47 words).",5 Results and Discussion,[1.0],"['RL-RNN-SEQ performs slightly worse than RL-RNN but produces queries that are three times shorter, on average (15 vs 47 words).']"
"Thus, RLRNN-SEQ is faster in retrieving documents and therefore might be a better candidate for a production implementation.
",5 Results and Discussion,[0],[0]
"The performance gap between the oracle and best performing method (Table 2, RL-Oracle vs. RL-RNN) suggests that there is a large room for improvement.",5 Results and Discussion,[0],[0]
"The cause for this gap is unknown but we suspect, for instance, an inherent difficulty in learning a good selection strategy and the partial observability from using a black box search engine.",5 Results and Discussion,[0],[0]
The proportion of relevant terms selected by the SL- and RL-Oracles over the total number of candidate terms (Table 3) indicates that only a small subset of terms are useful for the reformulation.,5.1 Relevant Terms per Document,[0],[0]
"Thus, we may conclude that the proposed method was able to learn an efficient term selection strategy in an environment where relevant terms are infrequent.",5.1 Relevant Terms per Document,[0],[0]
Fig. 3 shows the improvement in recall as more candidate terms are provided to a reformulation method.,5.2 Scalability: Number of Terms vs Recall,[0],[0]
"The RL-based model benefits from more candidate terms, whereas the classical PRF method quickly saturates.",5.2 Scalability: Number of Terms vs Recall,[0],[0]
"In our experiments, the best performing RL-based model uses the maximum number of candidate terms that we could fit
on a single GPU.",5.2 Scalability: Number of Terms vs Recall,[0],[0]
"We, therefore, expect further improvements with more computational resources.",5.2 Scalability: Number of Terms vs Recall,[0],[0]
"We show two examples of queries and the probabilities of each candidate term of being selected by the RL-CNN model in Fig. 4.
Notice that terms that are more related to the query have higher probabilities, although common words such as ”the” are also selected.",5.3 Qualitative Analysis,[0],[0]
"This is a consequence of our choice of a reward that does
not penalize the selection of neutral terms.",5.3 Qualitative Analysis,[0],[0]
"In Table 4 we show an original and reformulated query examples extracted from the MS Academic and TREC-CAR datasets, and their top-3 retrieved documents.",5.3 Qualitative Analysis,[1.0],"['In Table 4 we show an original and reformulated query examples extracted from the MS Academic and TREC-CAR datasets, and their top-3 retrieved documents.']"
Notice that the reformulated query retrieves more relevant documents than the original one.,5.3 Qualitative Analysis,[1.0],['Notice that the reformulated query retrieves more relevant documents than the original one.']
"As we conjectured earlier, we see that a search engine tends to return a document simply with the largest overlap in the text, necessitating the reformulation of a query to retrieve semantically relevant documents.
",5.3 Qualitative Analysis,[0],[0]
"Same query, different tasks We compare in Table 5 the reformulation of a sample query made by models trained on different datasets.",5.3 Qualitative Analysis,[1.0],"['Same query, different tasks We compare in Table 5 the reformulation of a sample query made by models trained on different datasets.']"
"The model trained on TREC-CAR selects terms that are similar to the ones in the original query, such as “serves” and “accreditation”.",5.3 Qualitative Analysis,[0],[0]
These selections are expected for this task since similar terms can be effective in retrieving similar paragraphs.,5.3 Qualitative Analysis,[0],[0]
"On the other hand, the model trained on Jeopardy prefers to select proper nouns, such as “Tunxis”, as these have a higher chance of being an answer to the question.",5.3 Qualitative Analysis,[1.0],"['On the other hand, the model trained on Jeopardy prefers to select proper nouns, such as “Tunxis”, as these have a higher chance of being an answer to the question.']"
"The model trained on MSA selects terms that cover different aspects of the entity being queried, such as “arts center” and “library”, since retrieving a diverse set of documents is necessary for the task the of citation recommendation.",5.3 Qualitative Analysis,[0],[0]
"Our best model, RL-RNN, takes 8-10 days to train on a single K80 GPU.",5.4 Training and Inference Times,[0],[0]
"At inference time, it takes
approximately one second to reformulate a batch of 64 queries.",5.4 Training and Inference Times,[0],[0]
Approximately 40% of this time is to retrieve documents from the search engine.,5.4 Training and Inference Times,[0],[0]
We introduced a reinforcement learning framework for task-oriented automatic query reformulation.,6 Conclusion,[0],[0]
An appealing aspect of this framework is that an agent can be trained to use a search engine for a specific task.,6 Conclusion,[0],[0]
The empirical evaluation has confirmed that the proposed approach outperforms strong baselines in the three separate tasks.,6 Conclusion,[0],[0]
The analysis based on two oracle approaches has revealed that there is a meaningful room for further development.,6 Conclusion,[0],[0]
"In the future, more research is necessary in the directions of (1) iterative reformulation under the proposed framework, (2) using information from modalities other than text, and (3) better reinforcement learning algorithms for a partially-observable environment.",6 Conclusion,[0],[0]
RN is funded by Coordenao de Aperfeioamento de Pessoal de Nvel Superior (CAPES).,Acknowledgements,[0],[0]
"KC thanks support by Facebook, Google and NVIDIA.",Acknowledgements,[0],[0]
This work was partly funded by the Defense Advanced Research Projects Agency (DARPA) D3M program.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.",Acknowledgements,[0],[0]
Search engines play an important role in our everyday lives by assisting us in finding the information we need.,abstractText,[0],[0]
"When we input a complex query, however, results are often far from satisfactory.",abstractText,[0],[0]
"In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned.",abstractText,[0],[0]
We train this neural network with reinforcement learning.,abstractText,[0],[0]
"The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall.",abstractText,[0],[0]
We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20% in terms of recall.,abstractText,[0],[0]
"Furthermore, we present a simple method to estimate a conservative upperbound performance of a model in a particular environment and verify that there is still large room for improvements.",abstractText,[0],[0]
Task-Oriented Query Reformulation with Reinforcement Learning,title,[0],[0]
