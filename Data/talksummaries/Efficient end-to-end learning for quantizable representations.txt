1	91	To this end, deep metric learning methods (Hadsell et al., 2006; Weinberger et al., 2006; Schroff et al., 2015; Song et al., 2016; Sohn, 2016; Song et al., 2017; Bell & Bala, 2015; Sener et al., 2016) aim to learn an embedding representation space such that similar data are close to each other and vice versa for dissimilar data.
3	19	Despite recent advances in deep metric learning methods, deploying the learned embedding representation in large scale applications poses great challenges in terms of the inference efficiency and scalability.
33	29	Concretely, if we replace f(x;θ) in Equation (1) with the negative distances of the input item x with respect to all d prototypes or centroids, [−||x− c1||2, .
34	48	,−||x− cd||2]ᵀ, then the corresponding hash function r(x) can be used to build the hash table.
36	25	We explicitly maintain the sparsity constraint on the hash code in Equation (1) throughout our optimization without continuous relaxations to inherit the efficiency aspect of quantization based hashing and this is one of the key attributes of the algorithm.
41	33	Embedding representations are required to infer which k activation dimensions to set in the corresponding binary hash code, but the binary hash codes are needed to adjust the embedding representations indexed at the activated bits so that similar items get hashed to the same buckets and vice versa.
45	19	We solve this optimization problem via alternating minimization through iterating over solving for k-sparse binary hash codes h1, .
47	15	Following subsections discuss these two steps in detail.
48	19	Given a set of continuous embedding representations {f(xi;θ)}ni=1, we seek to solve the following subproblem in Equation (3) where the task is to (unary) select k as large elements of the each embedding vector as possible, while (pairwise) selecting as orthogonal elements as pos- sible across different classes.
55	14	Here, w.l.o.g we assume each class has m number of data in the mini-batch (i.e. Npairs (Sohn, 2016) mini-batch construction).
58	27	Furthermore, since this value corresponds to the maximum deviation of an embedding vector from its class mean of the embedding, the bound gap decreases over iterations as we update the network parameter θ to attract similar pairs of data and vice versa for dissimilar pairs in the other embedding subproblem (more details in Section 4.4).
64	26	In practice, we use the efficient implementations from OR-Tools (Google Optimization Tools for combinatorial optimization problems) (OR-tools, 2018) to solve the minimum cost flow problem per each mini-batch.
81	28	Now denote {fo(e)}e∈E′ as the minimum cost flow solution of the flow network G′ which minimizes the total cost ∑ e∈E′ v(e)fo(e).
83	14	By the optimality of the flow {fo(e)}e∈E′ , we have that ∑ e∈E′ v(e)fo(e) ≤ ∑ e∈E′ v(e)fz(e).
85	13	Also, by Lemma 2, the rhs is equal to ∑ p−cᵀpzp + ∑ p1 6=p2 z ᵀ p1Pzp2 .
87	35	Thus, we have proved that finding the minimum cost flow solution on the flow network G′ and translating the flows between each vertices between A and B as {z′p}, we can find the optimal solution to the optimization problem in Equation (5).
94	108	Time complexity For λq nc, note that the worst case time complexity of finding the minimum cost flow (MCF) solution in the network G′ is O ( (nc + d) 2 ncd log (nc + d) ) (Goldberg & Tarjan, 1990).
97	24	We benchmarked the wall clock running time of the method at varying sizes of nc and d and observed approximately linear time complexity in nc and d. Figure 1 shows the benchmark wall clock run time results.
98	19	As the hash codes become more and more sparse, it becomes increasingly likely for hamming distances defined on binary codes (Norouzi et al., 2012; Zhao et al., 2015) to become zero regardless of whether the input pair of data is similar or dissimilar.
99	15	This phenomenon can be problematic when trained in a deep network because the back-propagation gradient would become zero and thus the embedding representations would not be updated at all.
101	33	This parameterization outputs zero distance only if the embedding representations of the two input data are identical at all the hash code activations.
120	15	For a given hash code hq, the number of retrieved data is Nq = ∑ i 6=q 1(h ᵀ i hq 6= 0).
126	14	Algorithm 1 summarizes the training procedure in detail.
130	18	Once we retrieve the union of all bucket items indexed at the k set bits in the hash code, we apply a reranking procedure (Wang et al., 2016) based on the euclidean distance in the embedding representation space.
131	22	Evaluation metrics We report our accuracy results using precision@k (Pr@k) and normalized mutual information (NMI) (Manning et al., 2008) metrics.
151	36	The results show that our method not only outperforms search accuracies of the state of the art deep metric learning base models but also provides up to 98× speed up over exhaustive search.
159	23	Our results in Table 3 and Table 4 show that our method outperforms the state of the art deep metric learning base models in search accuracy while providing up to 478× speed up over exhaustive linear search.
161	13	We have presented a novel end-to-end optimization algorithm for jointly learning a quantizable embedding representation and the sparse binary hash code which then can be used to construct a hash table for efficient inference.
162	19	We also show an interesting connection between finding the optimal sparse binary hash code and solving a minimum cost flow problem.
163	30	Our experiments show that the proposed algorithm not only achieves the state of the art search accuracy outperforming the previous state of the art deep metric learning approaches (Schroff et al., 2015; Sohn, 2016) but also provides up to 98× and 478× search speedup on Cifar-100 and ImageNet datasets respectively.
