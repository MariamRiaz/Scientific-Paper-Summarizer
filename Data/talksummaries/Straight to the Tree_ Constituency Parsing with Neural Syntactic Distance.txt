3	15	Generally speaking, either these approaches produce the parse tree sequentially, by governing the sequence of transitions in a transition-based parser (Nivre, 2004; Zhu et al., 2013; Chen and Manning, 2014; Cross and Huang, 2016), or use a chart-based approach by estimating non-linear potentials and performing exact structured inference by dynamic programming (Finkel et al., 2008; Durrett and Klein, 2015; Stern et al., 2017a).
5	17	This enables fast greedy decoding but also leads to compounding errors because the model is never exposed to its own mistakes during training (Daumé et al., 2009).
6	16	Solutions to this problem usually complexify the training procedure by using structured training through beamsearch (Weiss et al., 2015; Andor et al., 2016) and dynamic oracles (Goldberg and Nivre, 2012; Cross and Huang, 2016).
7	34	On the other hand, chartbased models can incorporate structured loss functions during training and benefit from exact inference via the CYK algorithm but suffer from higher computational cost during decoding (Durrett and Klein, 2015; Stern et al., 2017a).
8	98	In this paper, we propose a novel, fully-parallel model for constituency parsing, based on the concept of “syntactic distance”, recently introduced by (Shen et al., 2017) for language modeling.
11	26	The order induced by the syntactic distances fully specifies the order in which the sentence needs to be recursively split into smaller constituents (Figure 1): in case of a binary tree, there exists a oneto-one correspondence between the ordering and the tree.
23	18	The height of the lowest common ancestor for two leaves (wi, wj) is noted as d̃ij .
30	117	Algorithm 1 provides a way to convert it to a tuple (d, c, t), where d contains the height of the inner nodes in the tree following a left-to-right (in order) traversal, c the constituent labels for each node in the same order and t the part-of-speech Algorithm 2 Distance to Binary Parse Tree 1: function TREE(d,c,t) 2: if d = [] then 3: node← Leaf(t) 4: else 5: i← argmaxi(d) 6: childl ← Tree(d<i, c<i, t<i) 7: childr ← Tree(d>i, c>i, t≥i) 8: node← Node(childl, childr, ci) 9: end if 10: return node 11: end function (POS) tags of each word in the left-to-right order.
31	15	d is a valid vector of syntactic distances satisfying Definition 2.1.
32	20	Once a model has learned to predict these variables, Algorithm 2 can reconstruct a unique binary tree from the output of the model (d̂, ĉ, t̂).
34	34	The algorithm simply chooses the split point i with the maximum d̂i, and assigns to the span the predicted label ĉi.
56	13	The syntactic distances d and constituent labels c are predicted using a neural network architecture that stacks recurrent (LSTM (Hochreiter and Schmidhuber, 1997)) and convolutional layers.
58	22	To predict the constituent labels for each word, we pass the hidden states representations hw0 , ...,h w n through a 2-layer network FF w c , with softmax output: p(cwi |w) = softmax(FFwc (hwi )) (3) To compose the necessary information for inferring the syntactic distances and the constituency label information, we perform an additional convolution: gs1, .
72	27	Given that only the ranking induced by the ground-truth distances in d is important, as opposed to the absolute values themselves, using an MSE loss over-penalizes the model by ignoring ranking equivalence between different predictions.
74	45	We define our loss as a variant of the hinge loss as: Lrankdist = ∑ i,j>i [1− sign(di − dj)(d̂i − d̂j)]+, (9) where [x]+ is defined as max(0, x).
75	16	This loss encourages the model to reproduce the full ranking order induced by the ground-truth distances.
76	63	The final loss for the overall model is just the sum of individual losses L = Llabel + Lrankdist .
95	26	Our best model obtains a labeled F1 score of 91.8 on the test set (Table 1).
103	12	We use the Chinese Treebank 5.1 dataset, with articles 001-270 and 440-1151 for training, articles 301-325 as development set, and articles 271-300 for test set.
120	43	We also exper- imented by using 300D GloVe (Pennington et al., 2014) embedding for the input layer but this didn’t yield improvements over the model’s best performance.
121	21	Unsurprisingly, the model trained with MSE loss underperforms considerably a model trained with the rank loss.
123	107	The distance to tree conversion is a O(n log n) (n stand for the number of words in the input sentence) divide-and-conquer algorithm.
125	78	As the syntactic distance computation can be performed in parallel within a GPU, we first compute the distances in a batch, then we iteratively decode the tree with Algorithm 2.
127	52	We couldn’t find the source code to re-run them on our hardware, to give a fair enough comparison.
128	34	In our setting, we use an NVIDIA TITAN Xp graphics card for running the neural network part, and the distance to tree inference is run on an Intel Core i7-6850K CPU, with 3.60GHz clock speed.
145	28	We presented a novel constituency parsing scheme based on predicting real-valued scalars, named syntactic distances, whose ordering identify the sequence of top-down split decisions.
146	59	We employ a neural network model that predicts the distances d and the constituent labels c. Given the algorithms presented in Section 2, we can build an unambiguous mapping between each (d, c, t) and a parse tree.
147	118	One peculiar aspect of our model is that it predicts split decisions in parallel.
148	26	Our experiments show that our model can achieve strong performance compare to previous models, while being significantly more efficient.
149	58	Since the architecture of model is no more than a stack of standard recurrent and convolution layers, which are essential components in most academic and industrial deep learning frameworks, the deployment of this method would be straightforward.
