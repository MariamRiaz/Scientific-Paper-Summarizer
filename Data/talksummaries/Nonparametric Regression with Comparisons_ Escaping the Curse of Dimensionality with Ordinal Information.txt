42	1	Our experiments with the age-estimation data also show the practicality of R2.
43	1	We consider a non-parametric regression model with random design, i.e. we suppose first that we are given access to an unlabeled set U = {X1, .
47	2	We obtain two forms of supervision: 1.
64	1	1: Order elements in U as (Xπ̂(1), ..., Xπ̂(n)).
96	1	Without loss of generality suppose f ( Xt1 ) ≤ · · · f ( Xtm ) .
118	1	For constants C1, C2 > 0, the MSE of the R2 estimate f̂ is bounded by E[(f̂(X)− f(X))2] ≤ C1 ( log2 n logm ( m−2/3 + √ ν )) + C2n −2s/d.
120	1	We investigate the optimality of the √ ν-dependence in the next section.
121	1	(3) Finally, in settings where ν is large R2 can be led astray by the ordinal information, and a standard non-parametric regressor can achieve the (possibly faster) O ( m− 2s 2s+d ) rate by ignoring the ordinal information.
124	1	The cross validation process is standard and computationally efficient: we estimate the regression function twice, once using R2 and once using k-nearest neighbors, and choose the regression function that performs better on a held-out validation set.
126	1	When using an estimated permutation π̂ the true function of interest f is no longer an increasing (isotonic) function with respect to π̂, and this results in a modelmisspecification bias.
129	1	For any permutation π̂ satisfying the condition in (1) n∑ i=1 (f(Xπ−1(i))− f(Xπ̂−1(i)))2 ≤ 8M2 √ 2νn.
131	2	We denote this error by ∆, and using Lemma 4 we show that in expectation (over the random choice of the labeled set) E[∆] ≤ 8M2 √ 2νm.
133	1	In this case, the first term for some constant C > 0 is bounded as: E ( m∑ k=1 ( f̂(Xtk)− f(Xtk) )2) ≤ 2E[∆] + Cm1/3, where the first term corresponds to the modelmisspecification bias and the second corresponds to the usual isotonic regression rate.
135	1	In this section we turn our attention to lower bounds in the setting with noisy ordinal information.
147	2	So the Kendall-Tau error of the comparison oracle is (td)2 × ((1/u)2 × u) = ut2d.
166	1	We compare R2 with k-NN algorithms in all experiments.
170	1	We repeat each experiment 20 times and report the average MSE2.
173	1	We rescale f(x) so that it has 0 mean and unit variance.
176	1	At test time, we compute the MSE 1n ∑n i=1(f(X test i ) − f̂(X testi )) 2 for all test data X test1 , ..., X test n .
177	1	We consider two variants of R2.
180	1	However, we find that using 5-NN improves our estimator empirically.
182	1	Since R2 uses ordinal data in addition to labels, it should have lower MSE than 1-NN and 5-NN.
190	1	This eliminates the need for isotonic regression in Algorithm 1, but we find that the ranking still provides useful information for the unlabeled samples.
196	1	As σ goes up, the error of both variants of the R2 algorithm increases as expected.
198	1	We use the APPA- REAL dataset (E Agustsson, 2017), which contains 7,591 images, where each image is associated a biological age and an apparent age.
229	2	Since ordinal information is typically easier to obtain than direct labels, one might expect in these favorable settings the R2 algorithm to have lower effective cost than an algorithm based purely on direct supervision.
231	3	Another possible direction is to consider partial orders, where we have several subsets of unlabeled data ranked, but the relation between these subsets is unknown.
232	19	It would also be interesting to consider other models for ordinal information and to more broadly understand settings where indirect feedback is beneficial.
233	197	Also, several recent papers (Bellec & Tsybakov, 2015; Bellec, 2018; Han et al., 2017) demonstrate the adaptivity (to complexity of the unknown parameter) of the MLE in shape-constrained problems.
234	187	Understanding precise assumptions on the underlying smooth function which would induce a low-complexity isotonic regression problem is an interesting avenue for future work.
