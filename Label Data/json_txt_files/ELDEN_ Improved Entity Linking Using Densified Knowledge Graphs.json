{"sections": [{"text": "Proceedings of NAACL-HLT 2018, pages 1844\u20131853 New Orleans, Louisiana, June 1 - 6, 2018. c\u00a92018 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Entity Linking (EL) is the task of mapping mentions of an entity in text to the corresponding entity in Knowledge Graph (KG) (Hoffart et al., 2011; Dong et al., 2014; Chisholm and Hachey, 2015). EL systems primarily exploit two types of information: (1) similarity of the mention to the candidate entity string, and (2) coherence between the candidate entity and other entities mentioned in the vicinity of the mention in text. Coherence essentially measures how well the candidate entity is connected, either directly or indirectly, with other KG entities mentioned in the vicinity (Milne and Witten, 2008; Globerson et al., 2016). In the state-\nof-the-art EL system by (Yamada et al., 2016), coherence is measured as distance between embeddings of entities. This system performs well on entities which are densely-connected in KG, but not so well on sparsely-connected entities in the KG.\nWe demonstrate this problem using the example sentence in Figure 1. This sentence has two mentions: Andrei Broder and WWW. The figure also shows mention-entity linkages, i.e., mentions and their candidate entities in KG. Using a conventional EL system, the first mention Andrei Broder1 can be easily linked to Andrei Broder using string similarity between the mention and candidate entity strings. String similarity works well in this case as this mention is unambiguous in the given setting. However, the second mention\n1We use italics to denote textual mentions and typewriter to indicate an entity in KG.\n1844\nWWW has two candidates, World Wide Web and WWW conference, and hence is ambiguous. In such cases, coherence measure between the candidate entity and other unambiguously linked entity(ies) is used for disambiguation.\nState-of-the-art EL systems measure coherence as similarity between embeddings of entities. The entity embeddings are trained based on the number of common edges in KG2. In our example, common edges are edges World Wide Web shares with Andrei Broder and edges WWW conference shares with Andrei Broder. But WWW conference has less number of edges (it is a sparsely-connected entity) compared to World Wide Web. This leads to poor performance3 whereby WWW is erroneously linked to World Wide Web instead of linking to WWW conference.\nIn this paper, we propose ELDEN, an EL system which increases nodes and edges of the KG by using information available on the web about entities and pseudo entities. Pseudo Entities are words and phrases that frequently occur in Wikipedia, and co-occur with mentions of KG entities in the web corpus. Thus ELDEN uses a web corpus to find pseudo entities and refines the cooccurrences with Pointwise Mutual Information (PMI) (Church and Hanks, 1989) measure. ELDEN then adds edges to the entity from pseudo entities. In Figure 1, pseudo entity Program Committee co-occurs with mentions of Andrei Broder and WWW conference in web corpus and has a positive PMI value with both. So ELDEN adds edges from Program Committee to Andrei Broder and WWW conference, densifying neighborhood of the entities. Coherence, now measured as similarity between entity embeddings where embeddings are trained on densified KG, leads to improved EL performance.\nDensity (number of KG edges) of candidate entity affects EL performance. In our analysis of density and number of entities having that density in the Wikipedia KG, we find that entities with 500 edges or less make up more than 90%. Thus, creating an EL system that performs well on densely as well as sparsely-connected entities is a challenging, yet unavoidable problem.\n2Wikipedia Link based Measure (WLM) (Milne and Witten, 2008) used in Yamada et al.\u2019s system is based on number of common edges in KG.\n3This paper focuses on mention disambiguation. We assume mention and candidate entities are detected already.\nWe make the following contributions:\n\u2022 ELDEN presents a simple yet effective graph densification method which may be applied to improve EL involving any KG.\n\u2022 By using pseudo entities and unambiguous mentions of entity in a corpus, we demonstrate how non-entity-linked corpus can be used to improve EL performance.\n\u2022 We have made ELDEN\u2019s code and data publicly available4."}, {"heading": "2 Related Work", "text": "Entity linking: Most EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions. We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016). We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC. The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013). WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants. Entity embedding similarity (Yamada et al., 2016) is reported to give highest EL6 performance and is the baseline of ELDEN. Enhancing entity disambiguation: Among methods proposed in literature to enhance entity disambiguation utilizing KG (Bhattacharya and Getoor) uses additional relational information between database references; (Han and Zhao, 2010) uses semantic relatedness between entities in other KGs; and (Shen et al., 2018) uses paths consisting of defined relations between entities in the KG (IMDB and DBLP). All these methods utilize structured information, while our method shows how unstructured data (web corpus about the entity to be linked) can be effectively used for entity disambiguation. Entity Embeddings: ELDEN presents a method to enhance embedding of entities and words in a\n4https://github.com/ priyaradhakrishnan0/ELDEN\n5 (Shen et al., 2015) presents a survey of EL systems. 6Named Entity Disambiguation (NED) and EL are syn-\nonymous terms in research (Hoffart et al., 2011)\ncommon vector space. Word embedding methods like word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) have been extended to entities in EL (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015). These methods use data about entity-entity co-occurrences to improve the entity embeddings. In ELDEN, we improve it with web corpus cooccurrence statistics. Ganea and Hofmann (2017) present a very interesting neural model for jointly learning entity embedding along with mentions and contexts. KG densification with pseudo entities: KG densification using external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015). Densifying edge graph is also studied as \u2018link prediction\u2019 in literature (Mart\u0131\u0301nez et al., 2016). Kotnis et al. augment paths between KG nodes using \u2018bridging entities\u2019 which are noun phrases mined from an external corpus. ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entities. However, densification is used for relation inference in the former methods whereas it is used for entity coherence measurement in ELDEN. Word co-occurrence measures: Chaudhari et al. (2011) survey several co-occurrence measures for word association including PMI, Jaccard (Dice, 1945) and Co-occurrence Significance Ratio (CSR). Damani (2013) proves that considering corpus level significant co-occurrences, PMI is better than others. Budiu et al. (2007) compare Latent Semantic Analysis (LSA), PMI and Generalized Latent Semantic Analysis (GLSA) and conclude that for large corpora like web corpus, PMI works best on word similarity tests. Hence, we chose PMI to refine co-occurring mentions of entities in web corpus."}, {"heading": "3 Definitions and Problem Formulation", "text": "In this section, we present a few definitions and formulate the EL problem. Knowledge Graph (KG): A Knowledge Graph is defined as G = (E,F ) with entities E as nodes and F as edges. In allegiance to EL literature and baselines (Milne and Witten, 2008; Globerson et al., 2016), we use the Wikipedia hyperlink graph as the KG in this paper, where nodes correspond to Wikipedia articles and edges are incoming links from one Wikipedia article to another. ELDEN ultimately uses a densified version of this\nWikipedia KG, as described in Section 4. Sparsely connected entities: Following Hoffart et al. (2012), we define an entity to be a sparsely connected entity, if the number of edges incident on the entity node in the KG is less than threshold \u03b77. Otherwise, the entity is called a denselyconnected entity. Entity Linking (EL): Given a set of mentions MD = {m1, ...,mn} in a document D, and a knowledge graph G = (E,F ), the problem of entity linking is to find the assignment \u039b : MD \u2192 ED, where ED is the set of entities linked to mentions in document D such that ED \u2286 E.\nFor mention mi \u2208 MD, let the set of possible entities it can link to (candidate entities) be Ci. Then, the solution to the EL problem is an assignment \u039b where,\n\u039b(mi) = arg max e\u2208Ci [\u03c6(mi, e) + \u03b2 \u00b7 \u03c8(e, ED)] (1) Here, \u03c6(mi, e) \u2208 [0, 1] measures the contextual compatibility of mentionmi and entity e. \u03c6(mi, e) is obtained by combining prior probability and context similarity. \u03c8(e, ED) measures the coherence of e with entities in ED. \u03b2 is a variable controlling inclusion of \u03c8 in the assignment \u039b. Problem Formulation : (Yamada et al., 2016) is a recently proposed state-of-the-art EL system. We consider it as a representative EL system and use it as the main baseline for the experiments in this paper. In this section, we briefly describe Yamada et al. (2016)\u2019s two-step approach that solves the EL problem presented above.\nStep 1: A mention mi \u2208 MD is defined to be unambiguous if \u2203e \u2208 Ci such that \u03c6(mi, e) \u2265 \u03b3. Let M (u)D \u2286 MD be the set of such unambiguous mentions in document D. For all unambiguous mentions m \u2208 M (u)D , Yamada et al. freeze the assignment by solving Equation 1 after setting \u03b2 = 0. In other words, \u03c8(e, ED) is not used while assigning entities to unambiguous mentions. Assigning entities first to unambiguous mentions has also been found to be helpful in prior research (Milne and Witten, 2008; Guo and Barbosa, 2014). Let AD be the set of entities linked to in this step. In Figure 1, mention Andrei Broder is unambiguous8.\n7Like (Hoffart et al., 2012) we set \u03b7 = 500 for the experiments in this paper.\n8Between mention Andrei Broder and entity Andrei Broder, string similarity and prior probability are 1.0. In the experiments we use a \u03b3 value of 0.95.\nStep 2: In this step, Yamada et al. links all ambiguous mentions by solving Equation 2.\n\u039b(mi) =\narg max e\u2208Ci\n \u03c6(mi, e) + 1\n|AD| \u2211\nej\u2208AD ve \u00b7 vej\n \n\u2200mi \u2208MD \\M (u)D (2)\nwhere ve, vej \u2208 Rd are d-dimensional embeddings of entities e and ej respectively. Please note that the equation above is a reformulation of Equation 1 with \u03b2 = 1 and \u03c8(e, ED) = 1 |AD| \u2211 ej\u2208AD ve \u00b7 vej , where AD is derived from ED as described in Step 1. As coherence \u03c8(e, ED) is applied only in disambiguation of ambiguous mentions, we apply densification to only selective nodes of KG.\nEmbeddings of entities are generated using word2vec model and trained using WLM (Details in Section 5). Given a graph G = (E,F ), the WLM coherence measure \u03c8wlm(ei, ej) between two entities ei and ej is defined in Equation 3 where Ce is the set of entities with edge to entity e.\n\u03c8wlm(ei, ej) = 1\u2212 log(max(|Cei |, |Cej |))\u2212 log(|Cei \u2229 Cej )|)\nlog(|E|)\u2212 log(min(|Cei |, |Cej |)) (3)"}, {"heading": "4 Our Approach: ELDEN", "text": "In this section, we present ELDEN, our proposed approach. ELDEN extends (Yamada et al., 2016), with one important difference: rather than working with the input KG directly, ELDEN works with the densified KG, created by selective densification of the KG with co-occurrence statistics extracted from a large corpus. Even though this\nis a simple change, this results in improved EL performance. The method performs well even for sparsely connected entities. Overview : Overview of the ELDEN system is shown in Figure 2. ELDEN starts off with densification of the input KG, using statistics from web corpus. Embeddings of entities are then learned utilizing the densified KG in the next step. Embedding similarity estimated using the learned entity embeddings is used in calculating coherence measure in subsequent EL. Notation used is summarized in Table 1. (i)KG Densification Figure 2 depicts densification of KG in \u2018Input KG\u2019 and \u2018Densified KG\u2019. It shows two Wikipedia titles Andrei Broder and WWW conference from our running example (Figure 1). There are no edges common between Andrei Broder and WWW conference. In a web corpus, mentions of Andrei Broder and WWW conference co-occur with Program committee and it has a positive PMI value with both the entities. So ELDEN adds an edge from Program committee to both the entities. Here Program committee is a pseudo entity. Thus, ELDEN densifies the KG by adding edges from pseudo entities when the mentions of Wikipedia entity and pseudo entity co-occur in a web corpus and the pseudo entity has a positive PMI value with given entity.\nTaking a closer look, KG densification process starts from \u2018input KG\u2019 which is Wikipedia hyperlink graph G = (E,F ), where the nodes are Wikipedia titles (E) and edges are hyperlinks (F ). ELDEN processes Wikipedia text corpus and identifies phrases (unigrams and bi-grams) that occur frequently, i.e. more than 10 times in it. We denote these phrases as pseudo entities (S) and add them as nodes to the KG. Let E+ = E \u222a S be the resulting set of nodes.\nELDEN then adds edges connecting entities in E+ to entities in E. This is done by processing a web text corpus looking for mentions of entities in E+, and linking the mentions to entities in KG G \u2032 = (E+, F ). ELDEN uses Equation 1 with \u03b2 = 0 for this entity linking, i.e. only mentionentity similarity \u03c6(m, e) is used during this linking9. Based on this entity linked corpus, a cooccurrence matrix M of size |E+| \u00d7 |E+| is constructed. Each cell Mi,j is set to the PMI between\n9Since prior probabilities of pseudo entities are not available, only mention-entity similarity component of \u03c6(m, s) is used while linking a mention m to a pseudo entity s \u2208 S.\nInput KG Densified KG\nEntity Embeddings (V)\nthe entities e and e \u2032 .\nMe,e\u2032 = PMI(e, e \u2032 ) = log\nf(e, e \u2032 )\u00d7N\nf(e)\u00d7 f(e\u2032) where f(e) is the frequency of entity e in web corpus, f(e, e \u2032 ) is the sentence-constrained pair frequency of the entity pair (e, e \u2032 ) in web corpus, and N = \u2211\ne,e\u2032\u2208E+ f(e, e \u2032 ). Please note that PMI, and\nthere by M , are symmetric. The expanded set of edges, F+, is now defined as\nF+ = F\u222a{(e, e \u2032 ), (e \u2032 , e) | e\u2032 \u2208 E+, e \u2208 E,Me,e\u2032 > 0}\nIn other words, we augment the set of initial edges F with additional edges connecting entities in E+ with entities in E such that PMI between the entities is positive.\nELDEN now constructs the KG Gdense = (E+, F+), which is a densified version of the input KG G = (E,F ). ELDEN uses this densified KG Gdense for subsequent processing and entity linking. (ii)Learning Embeddings of Densified KG Entities ELDEN derives entity embeddings using the same setup, corpus and Word2vec skip-gram with negative sampling model as in Yamada et al., However, instead of training embeddings over the input KG, ELDEN trains embeddings of entities in the densified KG Gdense. Let V be the word2vec matrix containing embeddings of entities in E+ where V \u2208 Rk\u2217d. ve is the embedding of entity e in E+ with dimension 1 \u2217 d.\nIn word2vec model, entities in context are used\nto predict the target entity. ELDEN maximizes the objective function (Goldberg and Levy, 2014) of word2vec skip-gram model with negative sampling, L = \u2211 (t,c)\u2208P Lt,c where\nLt,c = log \u03b8(vc \u00b7 vt) + \u2211\nn\u2208N(t,c) log \u03b8(\u2212vn \u00b7 vt)\nHere vt and vc are the entity embeddings of target entity t and context entity c. P is the set of target-context entity pairs considered by the model. N(t,c) is a set of randomly sampled entities used as negative samples with pair (t, c). This objective is maximized with respect to variables vt\u2019s and vc\u2019s, where \u03b8(x) = 11+e\u2212x . P and N are derived usingGdense. t and c are entities inE+ such that c shares a common edge with t. vn is randomly sampled from V, for entities that do not share a common edge with t. Entity embedding similarity measured using V trained this way on Gdense is \u03c8ELDEN. Embedding similarity is measured as cosine distance between ves. Embeddings of S are trained using positive and negative word contexts derived using context length. (iii) Bringing it All Together: ELDEN\nELDEN is a supervised EL system which uses two sets of features: (1) contextual compatibility \u03c6(m, e); and (2) coherence \u03c8(ei, ej). These features are summarized in Table 2. Similarity between entity embeddings is measured as cosine similarity between ves."}, {"heading": "5 Experiments", "text": "In this section, we evaluate the following:\n\u2022 Is ELDEN\u2019s corpus co-occurrence statisticsbased densification helpful in disambiguating entities better? (Sec. 6.1)\n\u2022 Where does ELDEN\u2019s selective densification of KG nodes link entities better? (Sec. 6.3)\nSetup : ELDEN is implemented using Random Forest ensemble 10 (Breiman, 1998). Parameter values were set using CoNLL development set. Feature limit of 3 with number of estimators as 100 yielded best performance. Knowledge Graph: Wikipedia Following prior EL literature, we use Wikipedia hypergraph as our KG (Milne and Witten, 2008; Globerson et al., 2016). This KG is enhanced with pseudo entities as explained in Section 4. We process\n10http://scikit-learn.org\nthe Wikipedia corpus following the same procedure as in Yamada et al. (2016). We cleaned 11 Wikipedia dump dated Nov 2015. We then parsed the Wikipedia article text to identify pseudo entities. More details on KG and parameters used for training embeddings are in Table 3. Training took 4 days on gpu with 2 cores. Preprocessing: Web corpus and Densified KG For our experiments, we created a web corpus by querying Google12. Candidate entities of all mentions in the dataset are queried in Google and top ten search results are considered for unigram and bigram frequencies. This corpus occupied 6.8GB for candidate entities of TAC and CoNLL dataset mentions (54336 entities). Even for sparsely connected entities, an average corpus size of 670 lines or more13 was collected. We note that though some of the entities mentioned in this dataset are ten or more years old, we are able to collect, on an average more than 670 lines of web content. Thus corpus proves to be a good source of additional links for densification, for both common and rare entities. As Taneva and Weikum (2013) also note, it is not hard to find content about sparsely connected entities on the web.\nThe web corpus is analyzed for mentions and pseudo entities. Co-occurrence matrix M is created14 for mention and pseudo entities occurring within window of size 10 for PMI calculation15. Edges are added from pseudo entities with positive PMI to mention of given entity. In experiments we add edges from top 10 pseudo entities ordered by\n11by removing disambiguation, navigation, maintenance and discussion pages.\n12https://www.google.com/ 13A detailed analysis of knowledge gained from crawling for common versus less common entities is present in Figure 1 of supplementary material.\n14This co-occurrence matrix is downloadable with source code.\n15We experimented with window sizes 10, 25 and 50. We chose 10 that gave best results\nPMI values16. Evaluation Dataset: In line with prior work on EL, we test the performance of ELDEN on CoNLL and TAC datasets. As this paper focuses on entity disambiguation, we tested ELDEN against datasets and baseline methods for disambiguation. We note that the entity disambiguation evaluation part of other recent datasets like ERD 2014 and TAC 2015 is exactly same as the TAC 2010 evaluation (Ellis et al., 2014) 17. Training: ELDEN\u2019s parameters were tuned using training (development) sets of CoNLL and TAC datasets. CoNLL and TAC datasets consist of documents where mentions are marked and entity to which the mention links to, is specified. We use only mentions that link to a valid Wikipedia title (non NIL entities) and report performance on test set. Some aspects of these datasets relevant to our experiments are provided below. CoNLL: In CoNLL test set (5267 mentions), we report Precision of topmost candidate entity, aggregated over all mentions (P-micro) and aggregated over all documents (P-macro), i.e., if tp, fp and p are the individual true positives, false positives and precision for each document in a dataset of \u03b4 documents, then\nPmicro =\n\u03b4\u2211 i=1 tpi\n\u03b4\u2211 i=1 tpi+ \u03b4\u2211 i=1 fpi\nand Pmacro =\n\u03b4\u2211 i=1 pi\n\u03b4 .\nFor CoNLL candidate entities, we use (Pershina et al., 2015) dataset18. TAC: In TAC dataset, we report P-micro of topranked candidate entity on 1,020 mentions. Pmacro is not applicable to TAC as most documents have only one mention as query mention ( or \u2019mention to be linked\u2019). For TAC candidate entities, we index the Wikipedia word tokens and titles using solr19. We index terms in (1) title of the entity, (2) title of another entity redirecting to the entity, and (3) names of anchors that point to the entity, in line with baselines. We are making this TAC candidate set publicly available. Baseline: Yamada16 Our baseline is the Yamada et al. system explained in Section 3. Entity embedding distance measured using ve trained on the\n16This is a tunable parameter. 17These recent datasets consist of other evaluations, e.g., mention detection, multilinguality etc. which is beyond the scope of the paper and hence we didnt focus on them in the paper.\n18https://github.com/masha-p/PPRforNED 19http://lucene.apache.org/solr/\ninput KG G is \u03c8Yamada."}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Does ELDEN\u2019s selective densification help in disambiguation in EL?", "text": "In Table 4, we compare ELDEN\u2019s EL performance with results of other recently proposed state-ofthe-art EL methods that use coherence models. We see that ELDEN results matches best results on CoNLL and outperforms state-of-the-art in TAC dataset. In the table, the last four rows uses the Pershina et al. (2015) candidate set and hence, we provide a comparison of their disambiguation performance. Improved results of ELDEN over baseline is attributed to the improved disambiguation due to KG densification."}, {"heading": "6.2 Why does ELDEN\u2019s selective densification work?", "text": "We conduct ablation analysis using various feature and feature combinations and present performance of ELDEN and baseline in Table 5. Starting with base features, we add various features to ELDEN incrementally and report their impact on performance. The results when using base feature group alone, and base and string similarity groups together (\u03c6) are presented in first and second rows for each dataset. We compare \u03c8ELDEN to three coherence measures: \u03c8wlm, \u03c8Yamada and \u03c8dense, details of which are provided in Table 2. The performance improvement from each of the four coherence measures are in the next four rows. Performance of ELDEN from using all four coherence features is given in \u03c8ELDEN++ row.\nOn CoNLL dataset, \u03c8dense combined with \u03c6,\ngave an improvement of 2.0 and 1.9 (P-micro and P-macro) over Yamada16 results. We note that Yamada16 results are from our re-implementation of (Yamada et al., 2016) system 20 and we are able to almost reproduce the baseline results. We also present the results combining baseline\u2019s \u03c8Yamada and \u03c8wlm versus ELDEN\u2019s \u03c8ELDEN and \u03c8dense in next two rows. We find the ELDEN\u2019s KG densification features perform better than baselines.\nOn TAC dataset also, combined with \u03c6, \u03c8dense is found to do better than \u03c8wlm and \u03c8ELDEN gives a significant P-micro improvement of 4.2 over \u03c8Yamada. The \u03c8ELDEN++ P-micro in TAC dataset is statistically significant21. In short, we find the KG densification features, \u03c8dense and \u03c8ELDEN, as the features causing better performance of ELDEN on both datasets."}, {"heading": "6.3 Where does ELDEN\u2019s selective densification work better?", "text": "While most EL systems give higher precision on CoNLL dataset than TAC dataset, ELDEN performs with high precision on TAC dataset too.\n20We have re-implemented the Yamada et al system using hyper-parameters specified in the paper and these are our best-effort results.\n21We performed two tailed t-test, with 2-tail 95% value of 1.96.\nThis is explained by analyzing distribution of densely-connected and sparsely connected entities in TAC and CoNLL datasets as presented in Table 6. We see that CoNLL test set has almost half as densely-connected and half as sparsely connected entities, whereas in TAC test set, 63.6% are sparsely connected entities. This higher constitution of sparsely connected entities in TAC, explains ELDEN\u2019s better results in TAC relative to CoNLL dataset. As the number of sparsely connected entities is more than the number of denselyconnected entities in most KGs (Reinanda et al., 2016), our method is expected to be of significance for most KGs."}, {"heading": "6.4 What type of EL errors are best fixed with ELDEN\u2019s selective densification ?", "text": "We analyzed errors fixed by ELDEN on TAC dataset. We categorize the errors into four classes in line with error classes of Ling et al. (2015). We\nmanually analyzed 240 wrong predictions of Yamada16 and compared it with that of ELDEN, and the results are presented in Figure 3. We found errors to reduce with use of KG densification features and most of the errors eliminated were in \u201cSpecific label\u201d class. Errors in this class called for better modeling of mention\u2019s context and linkbased similarity (Ling et al., 2015).(More details of this analysis in the supplementary document.)"}, {"heading": "7 Conclusion", "text": "We started this study by analyzing the performance of a state-of-the-art Entity Linking (EL) system and found that its performance was low when linking entities sparsely-connected in the KG. We saw that this can be addressed by densifying the KG with respect to the given entity. We proposed ELDEN, which densifies edge graph of entities using pseudo entities and mentions of entities in a large web corpus. Through our experiments, we find that ELDEN outperforms state-ofthe-art baseline on benchmark datasets.\nWe believe that ELDENs combination of KG densification and entity embeddings is novel. Poor performance of EL systems on sparsely connected entities has been recognized as one of the open challenges by prior research. ELDEN performs well on sparsely connected entities too, as a validation of our method of combining KG densification followed by embedding. Our approach may be applied to any KG as the densification is performed with the help of unstructured data, and not any specific KG. We hope the simple graph densification method utilized in ELDEN will be of much interest to the research community.\nPseudo entities can be looked at as entity candidates for KG expansion, as also noted by Farid et al. (2016). In future, we plan to enhance ELDEN using EL of pseudo entities to estimate entity prior of entities not present in KG. We also plan to explore entity embeddings obtained using other graph densifying methods."}, {"heading": "8 Acknowledgments", "text": "We thank the Microsoft Research India Travel Grants for generous travel funds to attend and present this paper at NAACL."}], "year": 2018, "references": [{"title": "Alleviating poor context with background knowledge for named entity disambiguation", "authors": ["Ander Barrena", "Aitor Soroa", "Eneko Agirre."], "venue": "ACL (1). The Association for Computer Linguistics.", "year": 2016}, {"title": "Arcing classifier (with discussion and a rejoinder by the author)", "authors": ["Leo Breiman."], "venue": "Ann. Statist. 26(3):801\u2013849. https://doi.org/10.1214/aos/1024691079.", "year": 1998}, {"title": "Modeling information scent: A comparison of lsa, pmi and glsa similarity measures on common tests and corpora", "authors": ["Raluca Budiu", "Christiaan Royer", "Peter Pirolli."], "venue": "Large Scale Semantic Access to Content", "year": 2007}, {"title": "Lexical co-occurrence, statistical significance, and word association", "authors": ["Dipak Chaudhari", "Om P. Damani", "Srivatsan Laxman."], "venue": "CoRR abs/1008.5287.", "year": 2011}, {"title": "Relational inference for wikification", "authors": ["Xiao Cheng", "Dan Roth."], "venue": "EMNLP. ACL, pages 1787\u20131796.", "year": 2013}, {"title": "Entity disambiguation with web links", "authors": ["Andrew Chisholm", "Ben Hachey."], "venue": "Transactions of the Association for Computational Linguistics 3:145\u2013 156. https://tacl2013.cs.columbia.edu/ ojs/index.php/tacl/article/view/494.", "year": 2015}, {"title": "Word association norms, mutual information, and lexicography", "authors": ["Kenneth Ward Church", "Patrick Hanks."], "venue": "ACL \u201989, pages 76\u201383. https://doi.org/10. 3115/981623.981633.", "year": 1989}, {"title": "Improving pointwise mutual information (PMI) by incorporating significant co-occurrence", "authors": ["Om P. Damani."], "venue": "CoRR abs/1307.0596.", "year": 2013}, {"title": "Measures of the amount of ecologic association between species", "authors": ["Lee R. Dice."], "venue": "Ecology 26(3):297\u2013302. https://doi.org/10.2307/1932409.", "year": 1945}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "authors": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang."], "venue": "ACM, New York, NY, USA, KDD \u201914, pages 601\u2013610. https:", "year": 2014}, {"title": "Overview of linguistic resource for the tac kbp 2014 evaluations: Planning, execution, and results", "authors": ["Joe Ellis", "Jeremy Getman", "Stephanie Strassel."], "venue": "Linguistic Data Consortium, University of Pennsylvania.", "year": 2014}, {"title": "Entity disambiguation by knowledge and text jointly embedding", "authors": ["Weiyi Fang", "Jianwen Zhang", "Dilin Wang", "Zheng Chen", "Ming Li."], "venue": "CoNLL.", "year": 2016}, {"title": "Lonlies: Estimating property values for long tail entities", "authors": ["Mina Farid", "Ihab F. Ilyas", "Steven Euijong Whang", "Cong Yu."], "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, New York, NY, USA, SI-", "year": 2016}, {"title": "Deep joint entity disambiguation with local neural attention", "authors": ["Octavian-Eugen Ganea", "Thomas Hofmann."], "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 2609\u20132619. http:", "year": 2017}, {"title": "Collective entity resolution with multifocal attention", "authors": ["Amir Globerson", "Nevena Lazic", "Soumen Chakrabarti", "Amarnag Subramanya", "Michael Ringaard", "Fernando Pereira."], "venue": "ACL.", "year": 2016}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling wordembedding method", "authors": ["Yoav Goldberg", "Omer Levy"], "venue": "CoRR abs/1402.3722", "year": 2014}, {"title": "To link or not to link? a study on end-toend tweet entity linking", "authors": ["Stephen Guo", "Ming-Wei Chang", "Emre Kiciman."], "venue": "NAACL-HLT 2013. http://research.microsoft.com/apps/ pubs/default.aspx?id=183909.", "year": 2013}, {"title": "Robust entity linking via random walks", "authors": ["Zhaochen Guo", "Denilson Barbosa."], "venue": "ACM, New York, NY, USA, CIKM \u201914, pages 499\u2013508. https://doi.org/10. 1145/2661829.2661887.", "year": 2014}, {"title": "Structural semantic relatedness: A knowledge-based method to named entity disambiguation", "authors": ["Xianpei Han", "Jun Zhao."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Stroudsburg, PA,", "year": 2010}, {"title": "Efficient collective entity linking with stacking", "authors": ["Zhengyan He", "Shujie Liu", "Yang Song", "Mu Li", "Ming Zhou", "Houfeng Wang."], "venue": "EMNLP. http://research.microsoft.com/apps/ pubs/default.aspx?id=202249.", "year": 2013}, {"title": "An entitycentric approach for overcoming knowledge graph sparsity", "authors": ["Manjunath Hegde", "Partha P. Talukdar."], "venue": "Association for Computational Linguistics, Lisbon, Portugal, EMNLP \u201915, pages 530\u2013535. http: //aclweb.org/anthology/D15-1061.", "year": 2015}, {"title": "Kore: Keyphrase overlap relatedness for entity disambiguation", "authors": ["Johannes Hoffart", "Stephan Seufert", "Dat Ba Nguyen", "Martin Theobald", "Gerhard Weikum."], "venue": "ACM, New York, NY, USA, CIKM \u201912, pages 545\u2013554. https: //doi.org/10.1145/2396761.2396832.", "year": 2012}, {"title": "Robust disambiguation of named entities in text", "authors": ["Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum."], "venue": "Association for Computational Linguistics, Stroudsburg, PA, USA,", "year": 2011}, {"title": "Leveraging deep neural networks and knowledge graphs for entity disambiguation", "authors": ["Hongzhao Huang", "Larry Heck", "Heng Ji."], "venue": "CoRR abs/1504.07678. http: //arxiv.org/abs/1504.07678.", "year": 2015}, {"title": "Overview of the tac 2010 knowledge base population track", "authors": ["Heng Ji", "Ralph Grishman", "Hoa Trang Dang", "Kira Griffitt", "Joe Ellis."], "venue": "In Third Text Analysis Conference (TAC).", "year": 2010}, {"title": "Knowledge base inference using bridging entities", "authors": ["Bhushan Kotnis", "Pradeep Bansal", "Partha P. Talukdar."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015. pages 2038\u2013", "year": 2015}, {"title": "Design challenges for entity linking", "authors": ["Xiao Ling", "Sameer Singh", "Dan Weld."], "venue": "Transactions of the Association for Computational Linguistics (TACL) 3.", "year": 2015}, {"title": "A survey of link prediction in complex networks", "authors": ["V\u0131\u0301ctor Mart\u0131\u0301nez", "Fernando Berzal", "Juan-Carlos Cubero"], "venue": "ACM Comput. Surv. 49(4):69:1\u201369:33. https://doi", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "authors": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean."], "venue": "NIPS 2013. pages 3111\u20133119.", "year": 2013}, {"title": "Learning to Link with Wikipedia", "authors": ["David Milne", "Ian H. Witten."], "venue": "ACM, CIKM\u201908, pages 509\u2013518.", "year": 2008}, {"title": "Glove: Global vectors for word representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u20131543. http://www. aclweb.org/anthology/D14-1162.", "year": 2014}, {"title": "Personalized page rank for named entity disambiguation, Association for Computational Linguistics (ACL), pages 238\u2013243", "authors": ["Maria Pershina", "Yifan He", "Ralph Grishman."], "venue": "NAACL HLT 2015.", "year": 2015}, {"title": "Document filtering for long-tail entities", "authors": ["Ridho Reinanda", "Edgar Meij", "Maarten de Rijke."], "venue": "ACM, CIKM\u201916.", "year": 2016}, {"title": "Shine+: A general framework for domain-specific entity linking with heterogeneous information networks", "authors": ["W. Shen", "J. Han", "J. Wang", "X. Yuan", "Z. Yang."], "venue": "IEEE Transactions on Knowledge and Data Engineering 30(2):353\u2013366. https://doi.org/10.1109/", "year": 2018}, {"title": "Entity linking with a knowledge base: Issues, techniques, and solutions", "authors": ["W. Shen", "J. Wang", "J. Han."], "venue": "IEEE Transactions on Knowledge and Data Engineering 27(2):443\u2013460. https://doi.org/10.1109/ TKDE.2014.2327028.", "year": 2015}, {"title": "Modeling mention, context and entity with neural networks for entity disambiguation", "authors": ["Yaming Sun", "Lei Lin", "Duyu Tang", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang."], "venue": "Qiang Yang and Michael Wooldridge, editors, IJCAI. AAAI Press, pages 1333\u2013", "year": 2015}, {"title": "Gem-based entity-knowledge maintenance", "authors": ["Bilyana Taneva", "Gerhard Weikum."], "venue": "ACM, New York, NY, USA, CIKM \u201913, pages 149\u2013158. https://doi. org/10.1145/2505515.2505715.", "year": 2013}, {"title": "Joint learning of the embedding of words and entities for named entity disambiguation", "authors": ["Ikuya Yamada", "Hiroyuki Shindo", "Hideaki Takeda", "Yoshiyasu Takefuji."], "venue": "CoNLL 2016, pages 250\u2013259. http://aclweb. org/anthology/K/K16/K16-1025.pdf.", "year": 2016}, {"title": "Robust and collective entity disambiguation through semantic embeddings", "authors": ["Stefan Zwicklbauer", "Christin Seifert", "Michael Granitzer."], "venue": "Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, New", "year": 2016}], "id": "SP:03aa3c66aad11069e79d73108a92eeef3a43b40a", "authors": [{"name": "Priya Radhakrishnan", "affiliations": []}, {"name": "Partha Talukdar", "affiliations": []}, {"name": "Vasudeva Varma", "affiliations": []}, {"name": "Andrei Broder", "affiliations": []}], "abstractText": "Entity Linking (EL) systems aim to automatically map mentions of an entity in text to the corresponding entity in a Knowledge Graph (KG). Degree of connectivity of an entity in the KG directly affects an EL system\u2019s ability to correctly link mentions in text to the entity in KG. This causes many EL systems to perform well for entities well connected to other entities in KG, bringing into focus the role of KG density in EL. In this paper, we propose Entity Linking using Densified Knowledge Graphs (ELDEN). ELDEN is an EL system which first densifies the KG with co-occurrence statistics from a large text corpus, and then uses the densified KG to train entity embeddings. Entity similarity measured using these trained entity embeddings result in improved EL. ELDEN outperforms stateof-the-art EL system on benchmark datasets. Due to such densification, ELDEN performs well for sparsely connected entities in the KG too. ELDEN\u2019s approach is simple, yet effective. We have made ELDEN\u2019s code and data publicly available.", "title": "ELDEN: Improved Entity Linking Using Densified Knowledge Graphs"}