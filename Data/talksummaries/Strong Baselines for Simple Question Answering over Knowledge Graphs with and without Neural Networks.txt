0	36	There has been significant recent interest in simple question answering over knowledge graphs, where a natural language question such as “Where was Sasha Vujacic born?” can be answered via the lookup of a simple fact—in this case, the “place of birth” property of the entity “Sasha Vujacic”.
2	40	Most recent work on the simple QA task involves increasingly complex neural network (NN) architectures that yield progressively smaller gains over the previous state of the art (see §2 for more details).
5	25	To give two related examples: Melis et al. (2017) reported that standard LSTM architectures, when properly tuned, outperform some more recent models; Vaswani et al. (2017) showed that the dominant approach to sequence transduction using complex encoder–decoder networks with attention mechanisms work just as well with the attention module only, yielding networks that are far simpler and easier to train.
6	77	In line with an emerging thread of research that aims to improve empirical rigor in our field by focusing on knowledge and insights, as opposed to simply “winning” (Sculley et al., 2018), we take the approach of peeling away unnecessary complexity until we arrive at the simplest model that works well.
23	25	We begin with minimal preprocessing on questions: downcasing and tokenizing based on the Penn TreeBank.
24	25	As is common in the literature, we decompose the simple QA problem into four tasks: entity detection, entity linking, relation prediction, and evidence integration, detailed below.
34	22	The output of entity detection is a sequence of tokens representing a candidate entity.
35	15	This still needs to be linked to an actual node in the knowledge graph.
37	16	Our formulation treats this problem as fuzzy string matching and does not use neural networks.
38	18	For all the entities in the knowledge graph (Freebase), we pre-built an inverted index over ngrams n ∈ {1, 2, 3} in an entity’s name.
39	18	At linking time, we generate all corresponding n-grams from the candidate entity and look them up in the inverted index for all matches.
43	17	The goal of relation prediction is to identify the relation being queried.
46	20	Since relation prediction is over the entire question, we base the classification decision only on the hidden states (forward and backward passes) of the final token, but otherwise the model architecture is the same as for entity detection.
48	13	We adopt the model of Kim (2014), albeit slightly simplified in that we use a single static channel instead of multiple channels.
52	45	We experimented with two feature sets over the questions: (1) tf-idf on unigrams and bigrams and (2) word embeddings + relation words.
55	21	Given the top m entities and r relations from the previous components, the final task is to integrate evidence to arrive at a single (entity, relation) prediction.
58	25	After pruning, we observe many scoring ties, which arise from nodes in the knowledge graph that share the exact same label, e.g., all persons with the name “Adam Smith”.
59	12	We break ties by favoring more “popular” entities, using the number of incoming edges to the entity in the knowledge graph (i.e., entity in-degree) as a simple proxy.
69	15	For both entity linking and relation prediction, we evaluate recall at N (R@N ), i.e., whether the correct answer appears in the top N results.
70	53	For end-to-end evaluation, we follow the approach of Bordes et al. (2015) and mark a prediction as correct if both the entity and the relation exactly match the ground truth.
74	21	We used negative log likelihood loss to optimize model parameters using Adam, with an initial learning rate of 0.0001.
85	72	Following Reimers and Gurevych (2017), and due to questions about assumptions of normality, we simply report the mean as well as the minimum and maximum scores achieved in square brackets.
86	20	For entity detection, on the validation set, the BiLSTM (which outperforms the BiGRU) achieves 93.1 [92.8 93.4] F1, compared to the CRF at 90.2.
87	74	Entity linking results (R@N ) are shown in Table 1 for both the BiLSTM and the CRF.
88	23	We see that entity linking using the CRF achieves comparable accuracy, even though the CRF performs slightly worse on entity detection alone; entity linking appears to be the bottleneck.
91	23	Ture and Jojic (2017) conducted the same component-level evaluation, the results of which we report (but none else that we could find).
93	21	Interestingly, we see that the CNN slightly outperforms the BiGRU (which beats the BiLSTM slightly; not shown) on R@1, but both give essentially the same results for R@5.
96	64	We found that crossing 50 candidate entities with five candidate relations works the best.
97	14	To compute the [min, max] scores, we crossed 10 randomlyselected entity models with 10 relation models.
98	37	The best model combination is BiLSTM (for entity detection/linking) and BiGRU (for relation prediction), which achieves an accuracy of 74.9, competitive with a cluster of recent top results.
