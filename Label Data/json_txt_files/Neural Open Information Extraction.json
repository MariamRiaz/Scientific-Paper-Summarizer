{"sections": [{"heading": "1 Introduction", "text": "Open Information Extraction (Open IE) involves generating a structured representation of information in text, usually in the form of triples or n-ary propositions. An Open IE system not only extracts arguments but also relation phrases from the given text, which does not rely on pre-defined ontology schema. For instance, given the sentence \u201cdeep learning is a subfield of machine learning\u201d, the triple (deep learning; is a subfield of ; machine learning) can be extracted, where the relation phrase \u201cis a subfield of \u201d indicates the semantic relationship between two arguments. Open IE plays a key role in natural language understanding and fosters many downstream NLP applications such as knowledge base construction, question answering, text comprehension, and others.\nThe Open IE system was first introduced by TEXTRUNNER (Banko et al., 2007), followed by several popular systems such as REVERB (Fader et al., 2011), OLLIE (Mausam\net al., 2012), ClausIE (Del Corro and Gemulla, 2013) Stanford OPENIE (Angeli et al., 2015), PropS (Stanovsky et al., 2016) and most recently OPENIE41 (Mausam, 2016) and OPENIE52. Although these systems have been widely used in a variety of applications, most of them were built on hand-crafted patterns from syntactic parsing, which causes errors in propagation and compounding at each stage (Banko et al., 2007; Gashteovski et al., 2017; Schneider et al., 2017). Therefore, it is essential to solve the problems of cascading errors to alleviate extracting incorrect tuples.\nTo this end, we propose a neural Open IE approach with an encoder-decoder framework. The encoder-decoder framework is a text generation technique and has been successfully applied to many tasks, such as machine translation (Cho et al., 2014; Bahdanau et al., 2014; Sutskever et al., 2014; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017), image caption (Vinyals et al., 2014), abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) and recently keyphrase extraction (Meng et al., 2017). Generally, the encoder encodes the input sequence to an internal representation called \u2018context vector\u2019 which is used by the decoder to generate the output sequence. The lengths of input and output sequences can be different, as there is no one on one relation between the input and output sequences. In this work, Open IE is cast as a sequence-to-sequence generation problem, where the input sequence is the sentence and the output sequence is the tuples with special placeholders. For instance, given the input sequence \u201cdeep learning is a subfield of machine learning\u201d, the output sequence will be \u201c\u3008arg1\u3009 deep learning \u3008/arg1\u3009 \u3008rel\u3009 is a subfield of \u3008/rel\u3009 \u3008arg2\u3009 machine\n1https://github.com/allenai/openie-standalone 2https://github.com/dair-iitd/OpenIE-standalone\nar X\niv :1\n80 5.\n04 27\n0v 1\n[ cs\n.C L\n] 1\n1 M\nay 2\n01 8\nlearning \u3008/arg2\u3009\u201d. We obtain the input and output sequence pairs from highly confident tuples bootstrapped from a state-of-the-art Open IE system. Experiment results on a large benchmark dataset illustrate that the neural Open IE approach is significantly better than others in precision and recall, while also reducing the dependencies on other NLP tools.\nThe contributions of this paper are threefold. First, the encoder-decoder framework learns the sequence-to-sequence task directly, bypassing other hand-crafted patterns and alleviating error propagation. Second, a large number of highquality training examples can be bootstrapped from state-of-the-art Open IE systems, which is released for future research. Third, we conduct comprehensive experiments on a large benchmark dataset to compare different Open IE systems to show the neural approach\u2019s promising potential."}, {"heading": "2 Methodology", "text": ""}, {"heading": "2.1 Problem Definition", "text": "Let (X,Y ) be a sentence and tuples pair, where X = (x1, x2, ..., xm) is the word sequence and Y = (y1, y2, ..., yn) is the tuple sequence extracted from X . The conditional probability of P (Y |X) can be decomposed as:\nP (Y |X) = P (Y |x1, x2, ..., xm)\n= n\u220f i=1 p(yi|y1, y2, ..., yi\u22121;x1, x2, ...xm)\n(1)\nIn this work, we only consider the binary extractions from sentences, leaving n-ary extractions and\nnested extractions for future research. In addition, we ensure that both the argument and relation phrases are sub-spans of the input sequence. Therefore, the output vocabulary equals the input vocabulary plus the placeholder symbols."}, {"heading": "2.2 Encoder-Decoder Model Architecture", "text": "The encoder-decoder framework takes a variable length input sequence to a compressed representation vector that is used by the decoder to generate the output sequence. In this work, both the encoder and decoder are implemented using Recurrent Neural Networks (RNN) and the model architecture is shown in Figure 1.\nThe encoder uses a 3-layer stacked Long ShortTerm Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network to covert the input sequence X = (x1, x2, ...xm) into a set of hidden representations h = (h1, h2, ..., hm), where each hidden state is obtained iteratively as follows:\nht = LSTM(xt, ht\u22121) (2)\nThe decoder also uses a 3-layer LSTM network to accept the encoder\u2019s output and generate a variable-length sequence Y as follows:\nst = LSTM(yt\u22121, st\u22121, c) p(yt) = softmax(yt\u22121, st, c)\n(3)\nwhere st is the hidden state of the decoder LSTM at time t, c is the context vector that is introduced later. We use the softmax layer to calculate the output probability of yt and select the word with the largest probability.\nAn attention mechanism is vital for the encoderdecoder framework, especially for our neural\nOpen IE system. Both the arguments and relations are sub-spans that correspond to the input sequence. We leverage the attention method proposed by Bahdanau et al. to calculate the context vector c as follows:\nci = n\u2211\nj=1\n\u03b1ijhj\n\u03b1ij = exp(eij)\u2211n k=1 exp(eik)\neij = a(si\u22121, hj)\n(4)\nwhere a is an alignment model that scores how well the inputs around position j and the output at position i match, which is measured by the encoder hidden state hj and the decoder hidden state si\u22121. The encoder and decoder are jointly optimized to maximize the log probability of the output sequence conditioned on the input sequence."}, {"heading": "2.3 Copying Mechanism", "text": "Since most encoder-decoder methods maintain a fixed vocabulary of frequent words and convert a large number of long-tail words into a special symbol \u201c\u3008unk\u3009\u201d, the copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; See et al., 2017; Meng et al., 2017) is designed to copy words from the input sequence to the output sequence, thus enlarging the vocabulary and reducing the proportion of generated unknown words. For the neural Open IE task, the copying mechanism is more important because the output vocabulary is directly from the input vocabulary except for the placeholder symbols. We simplify the copying method in (See et al., 2017), the probability of generating the word yt comes from two parts as follows:\np(yt) = { p(yt|y1, y2, ..., yt\u22121;X) if yt \u2208 V\u2211\ni:xi=yt ati otherwise\n(5) where V is the target vocabulary. We combine the sequence-to-sequence generation and attentionbased copying together to derive the final output."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Data", "text": "For the training data, we used Wikipedia dump 201801013 and extracted all the sentences that are 40 words or less. OPENIE4 is used to analyze the sentences and extract all the tuples\n3https://dumps.wikimedia.org/enwiki/20180101/\nwith binary relations. To further obtain highquality tuples, we only kept the tuples whose confidence score is at least 0.9. Finally, there are a total of 36,247,584 \u3008sentence, tuple\u3009 pairs extracted. The training data is released for public use at https://1drv.ms/u/s!ApPZx_ TWwibImHl49ZBwxOU0ktHv.\nFor the test data, we used a large benchmark dataset (Stanovsky and Dagan, 2016) that contains 3,200 sentences with 10,359 extractions4. We compared with several state-of-the-art baselines including OLLIE, ClausIE, Stanford OPENIE, PropS and OPENIE4. The evaluation metrics are precision and recall."}, {"heading": "3.2 Parameter Settings", "text": "We implemented the neural Open IE model using OpenNMT (Klein et al., 2017), which is an open source encoder-decoder framework. We used 4 M60 GPUs for parallel training, which takes 3 days. The encoder is a 3-layer bidirectional LSTM and the decoder is another 3-layer LSTM. Our model has 256-dimensional hidden states and 256-dimensional word embeddings. A vocabulary of 50k words is used for both the source and target sides. We optimized the model with SGD and the initial learning rate is set to 1. We trained the model for 40 epochs and started learning rate decay from the 11th epoch with a decay rate 0.7. The dropout rate is set to 0.3. We split the data into 20 partitions and used data sampling in OpenNMT to train the model. This reduces the length of the epochs for more frequent learning rate updates and validation perplexity computation."}, {"heading": "3.3 Results", "text": "We used the script in (Stanovsky and Dagan, 2016)5 to evaluate the precision and recall of different baseline systems as well as the neural Open IE system. The precision-recall curve is shown in Figure 2. It is observed that the neural Open IE system performs best among all tested systems. Furthermore, we also calculated the Area under Precision-Recall Curve (AUC) for each system. The neural Open IE system with top-5 outputs achieves the best AUC score 0.473, which is significantly better than other systems. Although the\n4https://github.com/gabrielStanovsky/ oie-benchmark\n5The absolute scores are different from the original paper because the authors changed the matching function in their GitHub Repo, but did not change the relative performance.\nneural Open IE is learned from the bootstrapped outputs of OPENIE4\u2019s extractions, only 11.4% of the extractions from neural Open IE agree with the OPENIE4\u2019s extractions, while the AUC score is even better than OPENIE4\u2019s result. We believe this is because the neural approach learns arguments and relations across a large number of highly confident training instances. This also indicates that the generalization capability of the neural approach is better than previous methods. We observed many cases in which the neural Open IE is able to correctly identify the boundary of arguments but OpenIE4 cannot, for instance:\nInput Instead , much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors . Gold much of numerical analysis ||| concerned ||| with obtaining approximate solutions while maintaining reasonable bounds on errors OpenIE4 much of numerical analysis ||| is concerned with ||| obtaining approximate solutions Neural Open IE much of numerical analysis ||| is concerned ||| with obtaining approximate solutions while maintaining reasonable bounds on errors\nThis case illustrates that the neural approach reduces the limitation of hand-crafted patterns from other NLP tools. Therefore, it reduces the error propagation effect and performs better than other systems especially for long sentences.\nWe also investigated the computational cost of different systems. For the baseline systems, we obtained the Open IE extractions using a Xeon 2.4 GHz CPU. For the neural Open IE, we evaluated performance based on an M60 GPU. The\nrunning time was calculated by extracting Open IE tuples from the test dataset that contains a total of 3,200 sentences. The results are shown in Table 1. Among the aforementioned conventional systems, Ollie is the most efficient approach which takes around 160s to finish the extraction. By using GPU, the neural approach takes 172s to extract the tuples from the test data, which is comparable with conventional approaches. As the neural approach does not depend on other NLP tools, we can further optimize the computational cost in future research efforts."}, {"heading": "4 Related Work", "text": "The development of Open IE systems has witnessed rapid growth during the past decade (Mausam, 2016). The Open IE system was introduced by TEXTRUNNER (Banko et al., 2007) as the first generation. It casts the argument and relation extraction task as a sequential labeling problem. The system is highly scalable and extracts facts from large scale web content. REVERB (Fader et al., 2011) improved over TEXTRUNNER with syntactic and lexical constraints on binary relations expressed by verbs, which more than doubles the area under the\nprecision-recall curve. Following these efforts, the second generation known as R2A2 (Etzioni et al., 2011) was developed based on REVERB and an argument identifier, ARGLEARNER, to better extract the arguments for the relation phrases. The first and second generation Open IE systems extract only relations that are mediated by verbs and ignore contexts. To alleviate these limitations, the third generation OLLIE (Mausam et al., 2012) was developed, which achieves better performance by extracting relations mediated by nouns, adjectives, and more. In addition, contextual information is also leveraged to improve the precision of extractions. All the three generations only consider binary extractions from the text, while binary extractions are not always enough for their semantics representations. Therefore, SRLIE (Christensen et al., 2010) was developed to include an attribute context with a tuple when it is available. OPENIE4 was built on SRLIE with a rule-based extraction system RELNOUN (Pal and Mausam, 2016) for extracting noun-mediated relations. Recently, OPENIE5 improved upon extractions from numerical sentences (Saha et al., 2017) and broke conjunctions in arguments to generate multiple extractions. During this period, there were also some other Open IE systems emerged and successfully applied in different scenarios, such as ClausIE (Del Corro and Gemulla, 2013) Stanford OPENIE (Angeli et al., 2015), PropS (Stanovsky et al., 2016), and more.\nThe encoder-decoder framework was introduced by Cho et al. and Sutskever et al., where a multi-layered LSTM/GRU is used to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM/GRU to decode the target sequence from the vector. Bahdanau et al. and Luong et al. further improved the encoder-decoder framework by integrating an attention mechanism so that the model can automatically find parts of a source sentence that are relevant to predicting a target word. To improve the parallelization of model training, convolutional sequence-to-sequence (ConvS2S) framework (Gehring et al., 2016, 2017) was proposed to fully parallelize the training since the number of non-linearities is fixed and independent of the input length. Recently, the transformer framework (Vaswani et al., 2017) further improved over the vanilla S2S model and ConvS2S in both accuracy and training time.\nIn this paper, we use the LSTM-based S2S approach to obtain binary extractions for the Open IE task. To the best of our knowledge, this is the first time that the Open IE task is addressed using an end-to-end neural approach, bypassing the handcrafted patterns and alleviating error propagation."}, {"heading": "5 Conclusion and Future Work", "text": "We proposed a neural Open IE approach using an encoder-decoder framework. The neural Open IE model is trained with highly confident binary extractions bootstrapped from a state-of-the-art Open IE system, therefore it can generate highquality tuples without any hand-crafted patterns from other NLP tools. Experiments show that our approach achieves very promising results on a large benchmark dataset.\nFor future research, we will further investigate how to generate more complex tuples such as nary extractions and nested extractions with the neural approach. Moreover, other frameworks such as convolutional sequence-to-sequence and transformer models could apply to achieve better performance."}, {"heading": "Acknowledgments", "text": "We are grateful to the anonymous reviewers for their insightful comments and suggestions."}], "year": 2018, "references": [{"title": "Leveraging linguistic structure for open domain information extraction", "authors": ["Gabor Angeli", "Melvin Jose Johnson Premkumar", "Christopher D. Manning."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computa-", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "year": 2014}, {"title": "Open information extraction from the web", "authors": ["Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matt Broadhead", "Oren Etzioni."], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence.", "year": 2007}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "authors": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "year": 2014}, {"title": "Semantic role labeling for open information extraction", "authors": ["Janara Christensen", "Mausam", "Stephen Soderland", "Oren Etzioni."], "venue": "Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learn-", "year": 2010}, {"title": "Clausie: Clause-based open information extraction", "authors": ["Luciano Del Corro", "Rainer Gemulla."], "venue": "Proceedings of the 22Nd International Conference on World Wide Web. ACM, New York, NY, USA, WWW \u201913, pages 355\u2013366.", "year": 2013}, {"title": "Open information extraction: The second generation", "authors": ["Oren Etzioni", "Anthony Fader", "Janara Christensen", "Stephen Soderland", "Mausam Mausam."], "venue": "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelli-", "year": 2011}, {"title": "Identifying relations for open information extraction", "authors": ["Anthony Fader", "Stephen Soderland", "Oren Etzioni."], "venue": "Proceedings of the Conference of Empirical Methods in Natural Language Processing (EMNLP \u201911). Edinburgh, Scotland, UK.", "year": 2011}, {"title": "Minie: Minimizing facts in open information extraction", "authors": ["Kiril Gashteovski", "Rainer Gemulla", "Luciano Del Corro."], "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "year": 2017}, {"title": "A Convolutional Encoder Model for Neural Machine Translation", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N Dauphin."], "venue": "ArXiv eprints .", "year": 2016}, {"title": "Convolutional Sequence to Sequence Learning", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin."], "venue": "ArXiv e-prints .", "year": 2017}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "authors": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "year": 2016}, {"title": "Pointing the unknown words", "authors": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume", "year": 2016}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780. https://doi.org/10.1162/neco.1997.9.8.1735.", "year": 1997}, {"title": "Opennmt: Open-source toolkit for neural machine translation", "authors": ["Guillaume Klein", "Yoon Kim", "Yuntian Deng", "Jean Senellart", "Alexander M. Rush."], "venue": "CoRR abs/1701.02810. http://arxiv.org/abs/1701.02810.", "year": 2017}, {"title": "Effective approaches to attention-based neural machine translation", "authors": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Compu-", "year": 2015}, {"title": "Open information extraction systems and downstream applications", "authors": ["Mausam."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence. AAAI Press, IJCAI\u201916, pages 4074\u20134077.", "year": 2016}, {"title": "Open language learning for information extraction", "authors": ["Mausam", "Michael Schmitz", "Robert Bart", "Stephen Soderland", "Oren Etzioni."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing and Computational Natu-", "year": 2012}, {"title": "Deep keyphrase generation", "authors": ["Rui Meng", "Sanqiang Zhao", "Shuguang Han", "Daqing He", "Peter Brusilovsky", "Yu Chi."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long", "year": 2017}, {"title": "Abstractive text summarization using sequence-tosequence rnns and beyond", "authors": ["Ramesh Nallapati", "Bowen Zhou", "C\u0131\u0301cero Nogueira dos Santos", "aglar G\u00fclehre", "Bing Xiang"], "venue": "In CoNLL", "year": 2016}, {"title": "Demonyms and compound relational nouns in nominal open ie", "authors": ["Harinder Pal", "Mausam."], "venue": "Proceedings of the 5th Workshop on Automated Knowledge Base Construction. Association for Computational Linguistics, San Diego, CA, pages 35\u201339.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "authors": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computa-", "year": 2015}, {"title": "Bootstrapping for numerical open ie", "authors": ["Swarnadeep Saha", "Harinder Pal", "Mausam."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational", "year": 2017}, {"title": "Analysing errors of open information extraction systems", "authors": ["Rudolf Schneider", "Tom Oberhauser", "Tobias Klatt", "Felix A. Gers", "Alexander L\u00f6ser."], "venue": "Proceedings of the First Workshop on Building Linguistically Generalizable", "year": 2017}, {"title": "Get to the point: Summarization with pointergenerator networks", "authors": ["Abigail See", "Peter J. Liu", "Christopher D. Manning."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association", "year": 2017}, {"title": "Creating a large benchmark for open information extraction", "authors": ["Gabriel Stanovsky", "Ido Dagan."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational", "year": 2016}, {"title": "Getting more out of syntax with props", "authors": ["Gabriel Stanovsky", "Jessica Ficler", "Ido Dagan", "Yoav Goldberg."], "venue": "CoRR abs/1603.01648. http://arxiv.org/abs/1603.01648.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR abs/1409.3215. http://arxiv.org/abs/1409.3215.", "year": 2014}, {"title": "Attention is all you need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "\u0141 ukasz Kaiser", "Illia Polosukhin."], "venue": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and", "year": 2017}, {"title": "Show and tell: A neural image caption generator", "authors": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "CoRR abs/1411.4555. http://arxiv.org/abs/1411.4555.", "year": 2014}], "id": "SP:7244c53f2f18833b652533ba6fe1163af27918bc", "authors": [{"name": "Lei Cui", "affiliations": []}, {"name": "Furu Wei", "affiliations": []}, {"name": "Ming Zhou", "affiliations": []}], "abstractText": "Conventional Open Information Extraction (Open IE) systems are usually built on hand-crafted patterns from other NLP tools such as syntactic parsing, yet they face problems of error propagation. In this paper, we propose a neural Open IE approach with an encoder-decoder framework. Distinct from existing methods, the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system. An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several baselines, while maintaining comparable computational efficiency.", "title": "Neural Open Information Extraction"}