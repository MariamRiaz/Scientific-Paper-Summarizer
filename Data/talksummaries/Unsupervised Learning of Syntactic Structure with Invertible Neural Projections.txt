20	51	In experiments, we instantiate our approach using both a Markov-structured syntax model and a tree-structured syntax model – specifically, the DMV.
22	61	Experimental results on the Penn Treebank (Marcus et al., 1993) demonstrate that our approach improves the basic HMM and DMV by a large margin, leading to the state-of-the-art results on POS induction, and state-of-the-art results on unsupervised dependency parsing in the difficult training scenario where neither gold POS annotation nor punctuation-based constraints are available.
23	19	As an illustrative example, we first present a baseline model for Markov syntactic structure (POS induction) that treats a sequence of pre-trained word embeddings as observations.
28	26	The joint distribution of data and latent variables factors as: p(z,x;θ,η) = ∏` i=1 pθ(zi|zi−1)pη(xi|zi), (1) where pθ(zi|zi−1) is the multinomial transition probability and pη(xi|zi) is the multivariate Gaussian emission probability.
29	142	While the observed word embeddings do inform this model with a notion of word similarity – lacking in the basic multinomial HMM – the Gaussian emissions may not be sufficiently flexible to separate some syntactic categories in the complex pretrained embedding space – for example the skipgram embedding space as visualized in Figure 1(a) where different POS categories overlap.
31	35	To flexibly model observed embeddings and yield a new representation space that is more suitable for the syntax model, we propose to cascade a neural network as a projection function, deterministically transforming the simple space defined by the Gaussian HMM to the observed embedding space.
33	24	In the case of sequential Markov structure, our new model corresponds to the following generative process: For each time step i = 1, 2, · · · , `, • Draw the latent state zi ∼ pθ(zi|zi−1) • Draw the latent embedding ei ∼ N (µzi ,Σzi) • Deterministically produce embedding xi = fφ(ei) The graphical model is depicted in Figure 2.
34	23	The deterministic projection can also be viewed as sampling each observation from a point mass at fφ(ei).
38	25	, z` are conditioned to generate e1, e2, .
41	24	The first is Markov-structured, which we use for POS induction, and the second is DMV-structured, which we use to learn dependency parses without supervision.
42	21	The marginal data likelihood of our model is: p(x) = ∑ z ( psyntax(z;θ) · ∏` i=1 [ ∫ ei pη(ei|zi)pφ(xi|ei)dei︸ ︷︷ ︸ p(xi|zi) ]) .
43	27	(5) While the discrete variables z can be marginalized out with dynamic program in many cases, it is generally intractable to marginalize out the latent continuous variables, ei, for an arbitrary projection f in Eq.
51	18	Then, we present the architecture of a neural projector we use in experiments: a volume-preserving invertible neural network proposed by Dinh et al. (2014) for independent components estimation.
53	26	(5), the optimization challenge in our approach comes from the intractability of the marginalized emission factor p(xi|zi).
56	36	By using the change of variable rule to the integration, which allows the integration variable ei to be replaced by x′i = fφ(ei), the marginal emission factor can be computed in closed-form when the invertibility condition is satisfied: p(xi|zi;η,φ) = ∫ x′i pη(f −1 φ (x ′ i)|zi)δ(xi − x′i) ∣∣∣det∂f−1φ ∂x′i ∣∣∣dx′i = pη(f −1 φ (xi)|zi) ∣∣∣det∂f−1φ ∂xi ∣∣∣, (6) where pη(·|z) is a conditional Gaussian distribution, ∂f−1φ ∂xi is the Jacobian matrix of function f−1φ at xi, and ∣∣det∂f−1φ∂xi ∣∣ represents the absolute value of its determinant.
60	23	(7) shows that the training objective of our model is simply the Gaussian HMM log likelihood with an additional Jacobian regularization term.
63	25	The Jacobian regularization term accounts for the volume expansion or contraction behavior of the projection.
66	35	Such “information preserving” regularization is crucial during optimization, otherwise the trivial solution of always projecting data to the same single point to maximize likelihood is viable.2 More generally, for an arbitrary syntax model the data likelihood of our approach is: p(x) = ∑ z ( psyntax(z) · ∏` i=1 pη(f −1 φ (xi)|zi) ∣∣∣det∂f−1φ ∂xi ∣∣∣).
86	20	In this section, we first describe our datasets and experimental setup.
99	18	We use 45 tag clusters, the number of POS tags that appear in WSJ corpus.
110	16	Through the introduced latent embeddings and additional neural projection, our approach improves over the Gaussian HMM by 5.4 points in M-1 and 5.6 points in VM.
114	22	Moreover, our method outperforms the best published result that benefits from hand-engineered features (Yatbaz et al., 2012) by 2.0 points on VM.
137	27	However, to measure how these systems might perform without gold tags, we run three recent state-of-theart systems in our experimental setting: URA E-DMV (Tu and Honavar, 2012), Neural EDMV (Jiang et al., 2016), and CRF Autoencoder (CRFAE) (Cai et al., 2017).5 We use unsupervised POS tags (induced from our Markov-structured model) in place of gold tags.6 We also train basic DMV on gold tags and include several stateof-the-art results on gold tags as reference points.
140	20	DMV, UR-A EDMV, Neural E-DMV, and CRFAE suffer a large decrease in performance when trained on unsupervised tags – an effect also seen in previous work (Spitkovsky et al., 2011a; Cohen et al., 2011).
165	30	For our Markov-structured model, we have displayed the embedding space in Figure 1(b), where the gold POS clusters are well-formed.
170	54	However, two clusters of singular and plural nouns are actually separated.
171	39	We inspect the two clusters and the overlapping region in Figure 5, it turns out that the nouns in the separated clusters are words that can appear as subjects and, therefore, for which verb agreement is important to model.
174	34	Some previous work has deliberately created embeddings to capture different notions of similarity (Levy and Goldberg, 2014; Cotterell and Schütze, 2015), while they use extra morphology or dependency annotations to guide the embedding learning, our approach provides a potential alternative to create new embeddings that are guided by structured syntax model, only using unlabeled text corpora.
182	46	Experiments on both POS induction and unsupervised dependency parsing tasks demonstrate the effectiveness of our proposed approach.
183	82	Future work might explore more sophisticated invertible projections, or recurrent projections that jointly transform the entire input sequence.
