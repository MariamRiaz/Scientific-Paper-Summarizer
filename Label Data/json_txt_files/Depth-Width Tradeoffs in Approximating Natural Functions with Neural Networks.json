{"sections": [{"heading": "1. Introduction", "text": "Deep learning, in the form of artificial neural networks, has seen a dramatic resurgence in the past recent years, achieving great performance improvements in various fields of artificial intelligence such as computer vision and speech recognition. While empirically successful, our theoretical understanding of deep learning is still limited at best.\nAn emerging line of recent works has studied the expressive power of neural networks: What functions can and cannot be represented by networks of a given architecture (see related work section below). A particular focus has been the trade-off between the network\u2019s width and depth: On the one hand, it is well-known that large enough networks of depth 2 can already approximate any continuous target function on [0, 1]d to arbitrary accuracy (Cybenko, 1989; Hornik, 1991). On the other hand, it has long been evident that deeper networks tend to perform better than shallow ones, a phenomenon supported by the intuition that depth, providing compositional expressibility, is necessary for efficiently representing some functions. Moreover, re-\n1Weizmann Institute of Science, Rehovot, Israel. Correspondence to: Itay Safran <itay.safran@weizmann.ac.il>, Ohad Shamir <ohad.shamir@weizmann.ac.il>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ncent empirical evidence suggests that standard feedforward deep networks are harder to optimize than shallower networks which lead to worse training error and testing error (He et al., 2015).\nTo demonstrate the power of depth in neural networks, a clean and precise approach is to prove the existence of functions which can be expressed (or well-approximated) by moderately-sized networks of a given depth, yet cannot be approximated well by shallower networks, even if their size is much larger. However, the mere existence of such functions is not enough: Ideally, we would like to show such depth separation results using natural, interpretable functions, of the type we may expect neural networks to successfully train on. Proving that depth is necessary for such functions can give us a clearer and more useful insight into what various neural network architectures can and cannot express in practice.\nIn this paper, we provide several contributions to this emerging line of work. We focus on standard, vanilla feedforward networks (using some fixed activation function, such as the popular ReLU), and measure expressiveness directly in terms of approximation error, defined as the expected squared loss with respect to some distribution over the input domain. In this setting, we show the following:\n\u2022 We prove that the indicator of the Euclidean unit ball, x 7\u2192 1 (\u2016x\u2016 \u2264 1) in Rd, which can be easily approximated to accuracy using a 3-layer network with O(d2/ ) neurons, cannot be approximated to an accuracy higher than O(1/d4) using a 2-layer network, unless its width is exponential in d. In fact, we show the same result more generally, for any indicator of an ellipsoid x 7\u2192 1 (\u2016Ax + b\u2016 \u2264 r) (where A is a non-singular matrix and b is a vector). The proof is based on a reduction from the main result of (Eldan & Shamir, 2016), which shows a separation between 2- layer and 3-layer networks using a more complicated and less natural radial function.\n\u2022 We prove that any L1 radial function x 7\u2192 f(\u2016x\u20161), where x \u2208 Rd and f : R \u2192 R is piecewise-linear, cannot be approximated to accuracy by a depth 2 ReLU network of width less than\n\u2126\u0303(min{1/ , exp(\u2126(d))}). In contrast, such functions can be represented exactly by 3-layer ReLU networks.\n\u2022 We show that this depth/width trade-off can also be observed experimentally: Specifically, that when using standard backpropagation to learn the indicators of the L1 and L2 unit balls, 3-layer nets give significantly better performance compared to 2-layer nets (even if much larger). Our theoretical results indicate that this gap in performance is due to approximation error issues. This experiment also highlights the fact that our separation result is for a natural function that is not just well-approximated by some 3-layer network, but can also be learned well from data using standard methods.\n\u2022 Finally, we prove that any member of a wide family of non-linear and twice-differentiable functions (including for instance x 7\u2192 x2 in [0, 1]), which can be approximated to accuracy using ReLU networks of depth and width O(poly(log(1/ ))), cannot be approximated to similar accuracy by constantdepth ReLU networks, unless their width is at least \u2126(poly(1/ )). We note that a similar result appeared online concurrently and independently of ours in (Yarotsky, 2016; Liang & Srikant, 2016), but the setting is a bit different (see related work below for more details).\nRELATED WORK\nThe question of studying the effect of depth in neural network has received considerable attention recently, and studied under various settings. Many of these works consider a somewhat different setting than ours, and hence are not directly comparable. These include networks which are not plain-vanilla ones (e.g. (Cohen et al., 2016; Delalleau & Bengio, 2011; Martens & Medabalimi, 2014)), measuring quantities other than approximation error (e.g. (Bianchini & Scarselli, 2014; Poole et al., 2016)), focusing only on approximation upper bounds (e.g. (Shaham et al., 2016)), or measuring approximation error in terms of L\u221e-type bounds, i.e. supx |f(x) \u2212 f\u0303(x))| rather than L2-type bounds Ex(f(x) \u2212 f\u0303(x))2 (e.g. (Yarotsky, 2016; Liang & Srikant, 2016)). We note that the latter distinction is important: Although L\u221e bounds are more common in the approximation theory literature, L2 bounds are more natural in the context of statistical machine learning problems (where we care about the expected loss over some distribution). Moreover, L2 approximation lower bounds are stronger, in the sense that an L2 lower bound easily translates to a lower bound on L\u221e lower bound, but not vice versa1.\n1To give a trivial example, ReLU networks always express continuous functions, and therefore can never approximate a dis-\nA noteworthy paper in the same setting as ours is (Telgarsky, 2016), which proves a separation result between the expressivity of ReLU networks of depth k and depth o (k/ log (k)) (for any k). This holds even for onedimensional functions, where a depth k network is shown to realize a saw-tooth function with exp(O(k)) oscillations, whereas any network of depth o (k/ log (k)) would require a width super-polynomial in k to approximate it by more than a constant. In fact, we ourselves rely on this construction in the proofs of our results in section 5. On the flip side, in our paper we focus on separation in terms of the accuracy or dimension, rather than a parameter k. Moreover, the construction there relies on a highly oscillatory function, with Lipschitz constant exponential in k almost everywhere. In contrast, in our paper we focus on simpler functions, of the type that are likely to be learnable from data using standard methods.\nOur separation results in Sec. 5 (for smooth non-linear functions) are closely related to those of (Yarotsky, 2016; Liang & Srikant, 2016), which appeared online concurrently and independently of our work, and the proof ideas are quite similar. However, these papers focused on L\u221e bounds rather than L2 bounds. Moreover, (Yarotsky, 2016) considers a class of functions different than ours in their positive results, and (Liang & Srikant, 2016) consider networks employing a mix of ReLU and threshold activations, whereas we consider a purely ReLU network.\nAnother relevant and insightful work is (Poggio et al., 2016), which considers width vs. depth and provide general results on expressibility of functions with a compositional nature. However, the focus there is on worse-case approximation over general classes of functions, rather than separation results in terms of specific functions as we do here, and the details and setting is somewhat orthogonal to ours."}, {"heading": "2. Preliminaries", "text": "In general, we let bold-faced letters such as x = (x1, . . . , xd) denote vectors, and capital letters denote matrices or probabilistic events. \u2016\u00b7\u2016 denotes the Euclidean norm, and \u2016\u00b7\u20161 the 1-norm. 1 (\u00b7) denotes the indicator function. We use the standard asymptotic notationO(\u00b7) and \u2126(\u00b7) to hide constants, and O\u0303(\u00b7) and \u2126\u0303(\u00b7) to hide constants and factors logarithmic in the problem parameters.\nNeural Networks. We consider feed-forward neural networks, computing functions from Rd to R. The network is composed of layers of neurons, where each neuron computes a function of the form x 7\u2192 \u03c3(w>x + b), where w\ncontinuous function such as x 7\u2192 1 (x \u2265 0) in an L\u221e sense, yet can easily approximate it in an L2 sense given any continuous distribution.\nis a weight vector, b is a bias term and \u03c3 : R 7\u2192 R is a non-linear activation function, such as the ReLU function \u03c3(z) = [z]+ = max{0, z}. Letting \u03c3(Wx+b) be a shorthand for ( \u03c3(w>1 x + b1), . . . , \u03c3(w > n x + bn) ) , we define a layer of n neurons as x 7\u2192 \u03c3(Wx + b). By denoting the output of the ith layer as Oi, we can define a network of arbitrary depth recursively by Oi+1 = \u03c3(Wi+1Oi+bi+1), where Wi,bi represent the matrix of weights and bias of the ith layer, respectively. Following a standard convention for multi-layer networks, the final layer h is a purely linear function with no bias, i.e. Oh = Wh \u00b7Oh\u22121. We define the depth of the network as the number of layers l, and denote the number of neurons ni in the ith layer as the size of the layer. We define the width of a network as maxi\u2208{1,...,l} ni. Finally, a ReLU network is a neural network where all the non-linear activations are the ReLU function. We use \u201c2- layer\u201d and \u201c3-layer\u201d to denote networks of depth 2 and 3. In particular, in our notation a 2-layer ReLU network has the form\nx 7\u2192 n1\u2211 i=1 vi \u00b7 [w>i x + bi]+\nfor some parameters v1, b1, . . . , vn1 , bn1 and ddimensional vectors w1, . . . ,wn1 . Similarly, a 3-layer ReLU network has the form\nn2\u2211 i=1 ui  n1\u2211 j=1 vi,j [ w>i,jx + bi,j ] + + ci  +\nfor some parameters {ui, vi,j , bi,j , ci,wi,j}.\nApproximation error. Given some function f on a domain X endowed with some probability distribution (with density function \u00b5), we define the quality of its approximation by some other function f\u0303 as \u222b X (f(x) \u2212 f\u0303(x)) 2\u00b5(x)dx = Ex\u223c\u00b5[(f(x) \u2212 f\u0303(x))2]. We refer to this as approximation in the L2-norm sense. In one of our results (Thm. 6), we also consider approximation in the L\u221e-norm sense, defined as supx\u2208X |f(x)\u2212 f\u0303(x)|. Clearly, this upper-bounds the (square root of the) L2 approximation error defined above, so as discussed in the introduction, lower bounds on the L2 approximation error (w.r.t. any distribution) are stronger than lower bounds on theL\u221e approximation error."}, {"heading": "3. Indicators of L2 Balls and Ellipsoids", "text": "We begin by considering one of the simplest possible function classes on Rd, namely indicators of L2 balls (and more generally, ellipsoids). The ability to compute such functions is necessary for many useful primitives, for example determining if the distance between two points in Euclidean space is below or above some threshold (either with respect to the Euclidean distance, or a more general Mahalanobis distance). In this section, we show a depth separation result for such functions: Although they can be easily\napproximated with 3-layer networks, no 2-layer network can approximate it to high accuracy w.r.t. any distribution, unless its width is exponential in the dimension. This is formally stated in the following theorem:\nTheorem 1 (Inapproximability with 2-layer networks). The following holds for some positive universal constants c1, c2, c3, c4, and any network employing an activation function satisfying Assumptions 1 and 2 in Eldan & Shamir (2016): For any d > c1, and any non-singular matrix A \u2208 Rd\u00d7d, b \u2208 Rd and r \u2208 (0,\u221e), there exists a continuous probability distribution \u03b3 on Rd, such that for any function g computed by a 2-layer network of width at most c3 exp(c4d), and for the function f(x) = 1 (\u2016Ax + b\u2016 \u2264 r), we have\u222b\nRd (f(x)\u2212 g(x))2 \u00b7 \u03b3(x)dx \u2265 c2 d4 .\nWe note that the assumptions from (Eldan & Shamir, 2016) are very mild, and apply to all standard activation functions, including ReLU, sigmoid and threshold. For completeness, the fully stated assumptions are presented in Subsection A.1\nThe formal proof of Thm. 1 (provided below) is based on a reduction from the main result of (Eldan & Shamir, 2016), which shows the existence of a certain radial function (depending on the input x only through its norm) and a probability distribution which cannot be expressed by a 2-layer network, whose width is less than exponential in the dimension d to more than constant accuracy. A closer look at the proof reveals that this function (denoted as g\u0303) can be expressed as a sum of \u0398(d2) indicators of L2 balls of various radii. We argue that if we could have accurately approximated a given L2 ball indicator with respect to all distributions, then we could have approximated all the indicators whose sum add up to g\u0303, and hence reach a contradiction. By a linear transformation argument, we show the same contradiction would have occured if we could have approximated the indicators of an non-degenerate ellipse with respect to any distribution. The formal proof is provided below:\nProof of Thm. 1. Assume by contradiction that for f as described in the theorem, and for any distribution \u03b3, there exists a 2-layer network f\u0303\u03b3 of width at most c3 exp(c4d), such that\u222b\nx\u2208Rd\n( f(x)\u2212 f\u0303\u03b3(x) )2 \u03b3(x)dx \u2264 \u2264 c2\nd4 .\nLet A\u0302 and b\u0302 be a d \u00d7 d non-singular matrix and vector respectively, to be determined later. We begin by performing a change of variables, y = A\u0302x+b\u0302 \u21d0\u21d2 x = A\u0302\u22121(y\u2212b\u0302),\ndx = \u2223\u2223\u2223det(A\u0302\u22121)\u2223\u2223\u2223 \u00b7 dy, which yields\u222b\ny\u2208Rd\n( f ( A\u0302\u22121 ( y \u2212 b\u0302 )) \u2212 f\u0303\u03b3 ( A\u0302\u22121 ( y \u2212 b\u0302 )))2 \u00b7 \u03b3 ( A\u0302\u22121 ( y \u2212 b\u0302 )) \u00b7 \u2223\u2223\u2223det(A\u0302\u22121)\u2223\u2223\u2223 \u00b7 dy \u2264 . (1)\nIn particular, let us choose the distribution \u03b3 defined as \u03b3(z) = |det(A\u0302)| \u00b7 \u00b5(A\u0302z + b\u0302), where \u00b5 is the (continuous) distribution used in the main result of (Eldan & Shamir, 2016) (note that \u03b3 is indeed a distribution, since \u222b z \u03b3 (z) =\n(det(A\u0302)) \u222b z \u00b5(A\u0302z+b\u0302)dz, which by the change of variables\nx = A\u0302z + b\u0302, dx = |det(A\u0302)|dz equals \u222b x \u00b5(x)dx = 1). Plugging the definition of \u03b3 in Eq. (1), and using the fact that |det(A\u0302\u22121)| \u00b7 | det(A\u0302)| = 1, we get\u222b\ny\u2208Rd\n( f ( A\u0302\u22121 ( y \u2212 b\u0302 )) \u2212 f\u0303\u03b3 ( A\u0302\u22121 ( y \u2212 b\u0302 )))2 \u00b7 \u00b5 (y) dy \u2264 . (2)\nLetting z > 0 be an arbitrary parameter, we now pick A\u0302 = z rA and b\u0302 = z rb. Recalling the definition of f as x 7\u2192 1 (\u2016Ax + b\u2016 \u2264 r), we get that\u222b y\u2208Rd ( 1 (\u2016y\u2016 \u2264 z)\u2212 f\u0303\u03b3 (r z A\u22121 ( y \u2212 z r b )))2\n\u00b7 \u00b5 (y) dy \u2264 . (3) Note that f\u0303\u03b3 ( r zA \u22121 (y \u2212 zrb)) expresses a 2-layer network composed with a linear transformation of the input, and hence can be expressed in turn by a 2-layer network (as we can absorb the linear transformation into the parameters of each neuron in the first layer). Therefore, letting \u2016f\u2016L2(\u00b5) = \u221a\u222b y f2(y)dy denote the norm in L2(\u00b5) function space, we showed the following: For any z > 0, there exists a 2-layer network f\u0303z such that\u2225\u2225\u2225(1 (\u2016\u00b7\u2016 \u2264 z)\u2212 f\u0303z (\u00b7))\u2225\u2225\u2225\nL2(\u00b5) \u2264 \u221a . (4)\nWith this key result in hand, we now turn to complete the proof. We consider the function g\u0303 from (Eldan & Shamir, 2016), for which it was proven that no 2-layer network can approximate it w.r.t. \u00b5 to better than constant accuracy, unless its width is exponential in the dimension d. In particular g\u0303 can be written as\ng\u0303(x) = n\u2211 i=1 i \u00b7 1 (\u2016x\u2016 \u2208 [ai, bi]) ,\nwhere [ai, bi] are disjoint intervals, i \u2208 {\u22121,+1}, and n = \u0398(d2) where d is the dimension. Since g\u0303 can also be written as\nn\u2211 i=1 i (1 (\u2016x\u2016 \u2264 bi)\u2212 1 (\u2016x\u2016 \u2264 ai)) ,\nwe get by Eq. (4) and the triangle inequality that\u2225\u2225\u2225\u2225\u2225g\u0303(\u00b7)\u2212 n\u2211 i=1 i \u00b7 (f\u0303bi(\u00b7)\u2212 f\u0303ai(\u00b7) \u2225\u2225\u2225\u2225\u2225 L2(\u00b5)\n\u2264 n\u2211 i=1 | i| (\u2225\u2225\u2225(1 (\u2016\u00b7\u2016 \u2264 bi)\u2212 f\u0303bi)\u2225\u2225\u2225 L2(\u00b5)\n+ \u2225\u2225\u22251 (\u2016\u00b7\u2016 \u2264 ai)\u2212 f\u0303ai(\u00b7)\u2225\u2225\u2225\nL2(\u00b5) ) \u2264 2n \u221a .\nHowever, since a linear combination of 2n 2-layer neural networks of width at most w is still a 2-layer network, of width at most 2nw, we get that \u2211n i=1 i \u00b7 (f\u0303bi(\u00b7) \u2212 f\u0303ai(\u00b7)) is a 2-layer network, of width at most \u0398(d2) \u00b7 c3 exp(c4d), which approximates g\u0303 to an accuracy of less than 2n \u221a =\n\u0398(d2) \u00b7 \u221a c2/d4 = \u0398(1) \u00b7 \u221a c2. Hence, by picking c2, c3, c4 sufficiently small, we get a contradiction to the result of (Eldan & Shamir, 2016), that no 2-layer network of width smaller than c exp(cd) (for some constant c) can approximate g\u0303 to more than constant accuracy, for a sufficiently large dimension d.\nTo complement Thm. 1, we also show that such indicator functions can be easily approximated with 3-layer networks. The argument is quite simple: Using an activation such as ReLU or Sigmoid, we can use one layer to approximate any Lipschitz continuous function on any bounded interval, and in particular x 7\u2192 x2. Given a vector x \u2208 Rd, we can apply this construction on each coordinate xi seperately, hence approximating x 7\u2192 \u2016x\u20162 = \u2211d i=1 x 2 i . Similarly, we can approximate x 7\u2192 \u2016Ax + b\u2016 for arbitrary fixed matrices A and vectors b. Finally, with a 3-layer network, we can use the second layer to compute a continuous approximation to the threshold function z 7\u2192 1 (z \u2264 r). Composing these two layers, we get an arbitrarily good approximation to the function x 7\u2192 1 (\u2016Ax + b\u2016 \u2264 r) w.r.t. any continuous distribution, with the network size scaling polynomially with the dimension d and the required accuracy. In the theorem below, we formalize this intuition, where for simplicity we focus on approximating the indicator of the unit ball:\nTheorem 2 (Approximability with 3-layer networks). Given \u03b4 > 0, for any activation function \u03c3 satisfying Assumption 1 in Eldan & Shamir (2016) and any continuous probability distribution \u00b5 on Rd, there exists a constant c\u03c3 dependent only on \u03c3, and a function g expressible by a 3- layer network of width at most max { 8c\u03c3d 2/\u03b4, c\u03c3 \u221a 1/2\u03b4 }\n, such that the following holds:\u222b\nRd (g (x)\u2212 1 (\u2016x\u20162 \u2264 1)) 2 \u00b5 (x) dx \u2264 \u03b4,\nwhere c\u03c3 is a constant depending solely on \u03c3.\nThe proof of the theorem appears in the supplementary material"}, {"heading": "3.1. An Experiment", "text": "In this subsection, we empirically demonstrate that indicator functions of L2 balls are indeed easier to learn with a 3-layer network, compared to a 2-layer network (even if the 2-layer network is significantly larger). This indicates that the depth/width trade-off for indicators of balls, predicted by our theory, can indeed be observed experimentally. Moreover, it highlights the fact that our separation result is for simple natural functions, that can be learned reasonably well from data using standard methods.\nFor our experiment, we sampled 5 \u00b7 105 data instances in R100, with a direction chosen uniformly at random and a norm drawn uniformly at random from the interval [0, 2]. To each instance, we associated a target value computed according to the target function f(x) = 1 (\u2016x\u20162 \u2264 1). Another 5 \u00b7 104 examples were generated in a similar manner and used as a validation set.\nWe trained 5 ReLU networks on this dataset:\n\u2022 One 3-layer network, with a first hidden layer of size 100, a second hidden layer of size 20, and a linear output neuron.\n\u2022 Four 2-layer networks, with hidden layer of sizes 100, 200, 400 and 800, and a linear output neuron.\nTraining was performed with backpropagation, using the TensorFlow library. We used the squared loss `(y, y\u2032) = (y \u2212 y\u2032)2 and batches of size 100. For all networks, we chose a momentum parameter of 0.95, and a learning rate starting at 0.1, decaying by a multiplicative factor of 0.95 every 1000 batches, and stopping at 10\u22124.\nThe results are presented in Fig. 1. As can be clearly seen, the 3-layer network achieves significantly better performance than the 2-layer networks. This is true even though some of these networks are significantly larger and with more parameters (for example, the 2-layer, width 800 network has \u02dc80K parameters, vs. \u02dc10K parameters for the 3- layer network). This gap in performance is the exact opposite of what might be expected based on parameter counting alone. Moreover, increasing the width of the 2-layer networks exhibits diminishing returns: The performance improvement in doubling the width from 100 to 200 is much larger than doubling the width from 200 to 400 or 400 to 800. This indicates that one would need a much larger 2- layer network to match the 3-layer, width 100 network\u2019s performance. Thus, we conclude that the network\u2019s depth indeed plays a crucial role, and that 3-layer networks are inherently more suitable to express indicator functions of the type we studied."}, {"heading": "4. L1 Radial Functions; ReLU Networks", "text": "Having considered functions depending on the L2 norm, we now turn to consider functions depending on the L1 norm. Focusing on ReLU networks, we will show a certain separation result holding for any non-linear function, which depends on the input x only via its 1-norm \u2016x\u20161. Theorem 3. Let f : [0,\u221e) 7\u2192 R be a function such that for some r, \u03b4 > 0 and \u2208 (0, 1/2),\ninf a,b\u2208R\nEx uniform on [r,(1+ )r][(f(x)\u2212 (ax\u2212 b))2] > \u03b4 .\nThen there exists a distribution \u03b3 over {x : \u2016x\u20161 \u2264 (1 + )r}, such that if a 2-layer ReLU network F (x) satisfies\u222b\nx\n(f(\u2016x\u20161)\u2212 F (x)) 2 \u03b3(x)dx \u2264 \u03b4/2,\nthen its width must be at least \u2126\u0303(min {1/ , exp(\u2126(d))}) (where the \u2126\u0303 notation hides constants and factors logarithmic in , d).\nThe proof appears in the supplementary material. We note that \u03b4 controls how \u2018linearly inapproximable\u2019 is f in a narrow interval (of width ) around r, and that \u03b4 is generally dependent on . To give a concrete example, suppose that f(z) = [z \u2212 1]+, which cannot be approximated by a linear function to an accuracy better than O( 2) in an - neighborhood of 1. By taking r = 1\u2212 2 and \u03b4 = O(\n2), we get that no 2-layer network can approximate the function\n[\u2016x\u20161\u22121]+ (at least with respect to some distribution), unless its width is \u2126\u0303(min {1/ , exp(\u2126(d))}). On the flip side, f(\u2016x\u20161) can be expressed exactly by a 3-layer, width 2d ReLU network: x 7\u2192 [ \u2211d i=1([xi]++[\u2212xi]+)\u22121]+, where the output neuron is simply the identity function. The same argument would work for any piecewise-linear f . More generally, the same kind of argument would work for any function f exhibiting a non-linear behavior at some points: Such functions can be well-approximated by 3-layer networks (by approximating f with a piecewise-linear function), yet any approximating 2-layer network will have a lower bound on its size as specified in the theorem.\nIntuitively, the proof relies on showing that any good 2- layer approximation of f(\u2016x\u20161) must capture the nonlinear behavior of f close to \u201cmost\u201d points x satisfying \u2016x\u20161 \u2248 r. However, a 2-layer ReLU network x 7\u2192\u2211N j=1 aj [\u3008wj ,x\u3009+ bj ]+ is piecewise linear, with nonlinearities only at the union of the N hyperplanes \u222aj{x : \u3008wj ,x\u3009 + bj = 0}. This implies that \u201cmost\u201d points x s.t. \u2016x\u20161 \u2248 r must be -close to a hyperplane {x : \u3008wj ,x\u3009 + bj = 0}. However, the geometry of the L1 ball {x : \u2016x\u2016 = r} is such that the neighborhood of any single hyperplane can only cover a \u201csmall\u201d portion of that ball, yet we need to cover most of the L1 ball. Using this and an appropriate construction, we show that required number of hyperplanes is at least 1/ , as long as > exp(\u2212O(d)) (and if is smaller than that, we can simply use one neuron/hyperplane for each of the 2d facets of the L1 ball, and get a covering using 2d neurons/hyperplanes). The formal proof appears in the supplementary material.\nWe note that the bound in Thm. 3 is of a weaker nature than the bound in the previous section, in that the lower bound is only polynomial rather than exponential (albeit w.r.t. different problem parameters: vs. d). Nevertheless, we believe this does point out that L1 balls also pose a geometric difficulty for 2-layer networks, and conjecture that our lower bound can be considerably improved: Indeed, at the moment we do not know how to approximate a function such as x 7\u2192 [\u2016x\u20161 \u2212 1]+ with 2-layer networks to better than constant accuracy, using less than \u2126(2d) neurons.\nFinally, we performed an experiment similar to the one presented in Subsection 3.1, where we verified that the bounds we derived are indeed reflected in differences in empirical performance, when training 2-layer nets versus 3-layer nets. The reader is referred to Sec. B for the full details of the experiment and its results."}, {"heading": "5. C2 Nonlinear Functions; ReLU Networks", "text": "In this section, we establish a depth separation result for approximating continuously twice-differentiable (C2) functions using ReLU neural networks. Unlike the previous re-\nsults in this paper, the separation is for depths which can be larger than 3, depending on the required approximation error. Also, the results will all be with respect to the uniform distribution \u00b5d over [0, 1]d. As mentioned earlier, the results and techniques in this section are closely related to the independent results of (Yarotsky, 2016; Liang & Srikant, 2016), but our emphasis is on L2 rather than L\u221e approximation bounds, and we focus on somewhat different network architectures and function classes.\nClearly, not all C2 functions are difficult to approximate (e.g. a linear function can be expressed exactly with a 2- layer network). Instead, we consider functions which have a certain degree of non-linearity, in the sense that its Hessians are non-zero along some direction, on a significant portion of the domain. Formally, we make the following definition:\nDefinition 1. Let \u00b5d denote the uniform distribution on [0, 1]\nd. For a function f : [0, 1]d \u2192 R and some \u03bb > 0, denote\n\u03c3\u03bb (f) = sup v\u2208Sd\u22121, U\u2208U s.t. v>H(f)(x)v\u2265\u03bb \u2200x\u2208U \u00b5d (U) ,\nwhere Sd\u22121 = {x : \u2016x\u20162 = 1} is the d-dimensional unit hypersphere, and U is the set of all connected and measurable subsets of [0, 1]d.\nIn words, \u03c3\u03bb (f) is the measure (w.r.t. the uniform distribution on [0, 1]d) of the largest connected set in the domain of f , where at any point, f has curvature at least \u03bb along some fixed direction v. The \u201cprototypical\u201d functions f we are interested in is when \u03c3\u03bb(f) is lower bounded by a constant (e.g. it is 1 if f is strongly convex). We stress that our results in this section will hold equally well by considering the condition v>H(f)(x)v \u2264 \u2212\u03bb as well, however for the sake of simplicity we focus on the former condition appearing in Def. 1. Our goal is to show a depth separation result inidividually for any such function (that is, for any such function, there is a gap in the attainable error between deeper and shallower networks, even if the shallow network is considerably larger).\nAs usual, we start with an inapproximability result. Specifically, we prove the following lower bound on the attainable approximation error of f , using a ReLU neural network of a given depth and width:\nTheorem 4. For any C2 function f : [0, 1]d \u2192 R, any \u03bb > 0, and any function g on [0, 1]d expressible by a ReLU network of depth l and maximal width m, it holds that\u222b\n[0,1]d (f(x)\u2212 g(x)2\u00b5d (x) dx \u2265 c \u00b7 \u03bb2 \u00b7 \u03c35\u03bb (2m)4l ,\nwhere c > 0 is a universal constant.\nThe theorem conveys a key tradeoff between depth and width when approximating a C2 function using ReLU networks: The error cannot decay faster than polynomially in the widthm, yet the bound deteriorates exponentially in the depth l. As we show later on, this deterioration does not stem from the looseness in the bound: For well-behaved f , it is indeed possible to construct ReLU networks, where the approximation error decays exponentially with depth.\nThe proof of Thm. 4 appears in the supplementary material, and is based on a series of intermediate results. First, we show that any strictly curved function (in a sense similar to Definition 1) cannot be well-approximated in an L2 sense by piecewise linear functions, unless the number of linear regions is large. To that end, we first establish some necessary tools based on Legendre polynomials. We then prove a result specific to the one-dimensional case, including an explicit lower bound if the target function is quadratic (Thm. 9) or strongly convex or concave (Thm. 10). We then expand the construction to get an error lower bound in general dimension d, depending on the number of linear regions in the approximating piecewiselinear function. Finally, we note that any ReLU network induces a piecewise-linear function, and bound the number of linear regions induced by a ReLU network of a given width and depth (using a lemma borrowed from (Telgarsky, 2016)). Combining this with the previous lower bound yields Thm. 4.\nWe now turn to complement this lower bound with an approximability result, showing that with more depth, a wide family of functions to which Thm. 4 applies can be approximated with exponentially high accuracy. Specifically, we consider functions which can be approximated using a moderate number of multiplications and additions, where the values of intermediate computations are bounded (for example, a special case is any function approximable by a moderately-sized Boolean circuit, or a polynomial).\nThe key result to show this is the following, which implies that the multiplication of two (bounded-size) numbers can be approximated by a ReLU network, with error decaying exponentially with depth:\nTheorem 5. Let f : [\u2212M,M ]2 \u2192 R, f (x, y) = x \u00b7 y and let > 0 be arbitrary. Then exists a ReLU neural network g of width 4 \u2308 log ( M )\u2309 + 13 and depth \u2308 2 log ( M )\u2309 + 9 satisfying\nsup (x,y)\u2208[\u2212M,M ]2\n|f (x, y)\u2212 g (x, y)| \u2264 .\nThe idea of the construction is that depth allows us to compute highly-oscillating functions, which can extract highorder bits from the binary representation of the inputs. Given these bits, one can compute the product by a procedure resembling long multiplication, as shown in Fig. 2,\nand formally proven as follows:\nProof of Thm. 5. We begin by observing that by using a simple linear change of variables on x, we may assume without loss of generality that x \u2208 [0, 1], as we can just rescale x to the interval [0, 1], and then map it back to its original domain [\u2212M,M ], where the error will multiply by a factor of 2M . Then by requiring accuracy 2M instead of , the result will follow.\nThe key behind the proof is that performing bit-wise operations on the first k bits of x \u2208 [0, 1] yields an estimation of the product to accuracy 21\u2212kM . Let x = \u2211\u221e i=1 2\n\u2212ixi be the binary representation of x where xi is the ith bit of x, then\nx \u00b7 y = \u221e\u2211 i=1 2\u2212ixi \u00b7 y\n= k\u2211 i=1 2\u2212ixi \u00b7 y + \u221e\u2211 i=k+1 2\u2212ixi \u00b7 y. (5)\nBut since\u2223\u2223\u2223\u2223\u2223 \u221e\u2211\ni=k+1\n2\u2212ixi \u00b7 y \u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223 \u221e\u2211\ni=k+1\n2\u2212i \u00b7 y \u2223\u2223\u2223\u2223\u2223 = 2\u2212k |y| \u2264 21\u2212kM, Eq. (5) implies\u2223\u2223\u2223\u2223\u2223x \u00b7 y \u2212 k\u2211 i=1 2\u2212ixi \u00b7 y\n\u2223\u2223\u2223\u2223\u2223 \u2264 21\u2212kM. Requiring that 22\u2212kM \u2264 2M , it suffices to show the existence of a network which approximates the function\u2211k i=1 2 \u2212ixi \u00b7 y to accuracy 2 , where k = 2 \u2308 log ( 8M )\u2309 .\nThis way both approximations will be at most 2 , resulting in the desired accuracy of .\nBefore specifying the architecture which extracts the ith bit of x, we first describe the last 2 layers of the network. Let the penultimate layer comprise of k neurons, each receiving both y and xi as input, and having the set of weights( 2\u2212i, 1,\u22121 ) . Thus, the output of the ith neuron in the penultimate layer is[ 2\u2212iy + xi \u2212 1 ] + = 2\u2212ixiy.\nLet the final single output neuron have the set of weights (1, . . . , 1, 0) \u2208 Rk+1, this way, the output of the network will be \u2211k i=1 2 \u2212ixi \u00b7 y as required.\nWe now specify the architecture which extracts the first most significant k bits of x. In Telgarsky (2016), the author demonstrates how the composition of the function\n\u03d5 (x) = [2x]+ \u2212 [4x\u2212 2]+\nwith itself i times, \u03d5i, yields a highly oscillatory triangle wave function in the domain [0, 1]. Furthermore, we observe that \u03d5 (x) = 0 \u2200x \u2264 0, and thus \u03d5i (x) = 0 \u2200x \u2264 0. Now, a linear shift of the input of \u03d5i by 2\u2212i\u22121, and composing the output with\n\u03c3\u03b4 (x) =\n[ 1\n2\u03b4 x\u2212 1 4\u03b4 + 1 2 ] + \u2212 [ 1 2\u03b4 x\u2212 1 4\u03b4 \u2212 1 2 ] + ,\nwhich converges to 1[x\u22650.5] (x) as \u03b4 \u2192 0, results in an approximation of x 7\u2192 xi: \u03c3\u03b4 ( \u03d5i ( x\u2212 2\u2212i\u22121 )) . We stress that choosing \u03b4 such that the network approximates the bitwise product to accuracy 2 will require \u03b4 to be of magnitude 1 , but this poses no problem as representing such a number requires log ( 1 ) bits, which is also the magnitude of the size of the network, as suggested by the following analysis.\nNext, we compute the size of the network required to implement the above approximation. To compute \u03d5 only two neurons are required, therefore \u03d5i can be computed using i layers with 2 neurons in each, and finally composing this with \u03c3\u03b4 requires a subsequent layer with 2 more neurons. To implement the ith bit extractor we therefore require a network of size 2\u00d7(i+ 1). Using dummy neurons to propagate the ith bit for i < k, the architecture extracting the k most significant bits of x will be of size 2k \u00d7 (k + 1). Adding the final component performing the multiplication estimation will require 2 more layers of width k and 1 respectively, and an increase of the width by 1 to propagate y to the penultimate layer, resulting in a network of size (2k + 1)\u00d7 (k + 1).\nThm. 5 shows that multiplication can be performed very accurately by deep networks. Moreover, additions can be\ncomputed by ReLU networks exactly, using only a single layer with 4 neurons: Let \u03b1, \u03b2 \u2208 R be arbitrary, then (x, y) 7\u2192 \u03b1 \u00b7x+\u03b2 \u00b7y is given in terms of ReLU summation by\n\u03b1 [x]+ \u2212 \u03b1 [\u2212x]+ + \u03b2 [y]+ \u2212 \u03b2 [\u2212y]+ .\nRepeating these arguments, we see that any function which can be approximated by a bounded number of operations involving additions and multiplications, can also be approximated well by moderately-sized networks. This is formalized in the following theorem, which provides an approximation error upper bound (in the L\u221e sense, which is stronger than L2 for upper bounds):\nTheorem 6. Let Ft,M, be the family of functions on the domain [0, 1]d with the property that f \u2208 Ft,M, is approximable to accuracy with respect to the infinity norm, using at most t operations involving weighted addition, (x, y) 7\u2192 \u03b1 \u00b7 x+ \u03b2 \u00b7 y, where \u03b1, \u03b2 \u2208 R are fixed; and multiplication, (x, y) 7\u2192 x \u00b7 y, where each intermediate computation stage is bounded in the interval [\u2212M,M ]. Then there exists a universal constant c, and a ReLU network g of width and depth at most c ( t log ( 1 ) + t2 log (M) ) , such that sup\nx\u2208[0,1]d |f (x)\u2212 g (x)| \u2264 2 .\nAs discussed in Sec. 2, this type of L\u221e approximation bound implies an L2 approximation bound with respect to any distribution. The proof of the theorem appears in Sec. A.\nCombining Thm. 4 and Thm. 6, we can state the following corollary, which formally shows how depth can be exponentially more valuable than width as a function of the target accuracy :\nCorollary 1. Suppose f \u2208 C2\u2229Ft( ),M( ), , where t ( ) = O (poly (log (1/ ))) and M ( ) = O (poly (1/ )). Then approximating f to accuracy in the L2 norm using a fixed depth ReLU network requires width at least poly(1/ ), whereas there exists a ReLU network of depth and width at most p (log (1/ )) which approximates f to accuracy in the infinity norm, where p is a polynomial depending solely on f .\nProof. The lower bound follows immediately from Thm. 4. For the upper bound, observe that Thm. 6 implies an approximation by a network of width and depth at most\nc ( t ( /2) log (2/ ) + (t ( /2)) 2 log (M ( /2)) ) ,\nwhich by the assumption of Corollary 1, can be bounded by p (log (1/ )) for some polynomial p which depends solely on f ."}, {"heading": "Acknowledgements", "text": "This research is supported in part by an FP7 Marie Curie CIG grant, Israel Science Foundation grant 425/13, and the Intel ICRI-CI Institute. We would like to thank Shai Shalev-Shwartz for some illuminating discussions, and Eran Amar for his valuable help with the experiments."}], "year": 2017, "references": [{"title": "On the complexity of shallow and deep neural network classifiers", "authors": ["M. Bianchini", "F. Scarselli"], "venue": "In ESANN,", "year": 2014}, {"title": "On the expressive power of deep learning: A tensor analysis", "authors": ["Cohen", "Nadav", "Sharir", "Or", "Shashua", "Amnon"], "venue": "In 29th Annual Conference on Learning Theory,", "year": 2016}, {"title": "Approximation by superpositions of a sigmoidal function", "authors": ["Cybenko", "George"], "venue": "Mathematics of control, signals and systems,", "year": 1989}, {"title": "Shallow vs. deep sum-product networks", "authors": ["O. Delalleau", "Y. Bengio"], "venue": "In NIPS, pp", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "authors": ["Eldan", "Ronen", "Shamir", "Ohad"], "venue": "In 29th Annual Conference on Learning Theory,", "year": 2016}, {"title": "Deep residual learning for image recognition", "authors": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "arXiv preprint arXiv:1512.03385,", "year": 2015}, {"title": "Approximation capabilities of multilayer feedforward networks", "authors": ["Hornik", "Kurt"], "venue": "Neural networks,", "year": 1991}, {"title": "Why deep neural networks", "authors": ["Liang", "Shiyu", "R. Srikant"], "venue": "arXiv preprint arXiv:1610.04161,", "year": 2016}, {"title": "On the expressive efficiency of sum product networks", "authors": ["J. Martens", "V. Medabalimi"], "venue": "arXiv preprint arXiv:1411.7717,", "year": 2014}, {"title": "Why and when can deep\u2013but not shallow\u2013networks avoid the curse of dimensionality: a review", "authors": ["Poggio", "Tomaso", "Mhaskar", "Hrushikesh", "Rosasco", "Lorenzo", "Miranda", "Brando", "Liao", "Qianli"], "venue": "arXiv preprint arXiv:1611.00740,", "year": 2016}, {"title": "Exponential expressivity in deep neural networks through transient chaos", "authors": ["Poole", "Ben", "Lahiri", "Subhaneil", "Raghu", "Maithreyi", "SohlDickstein", "Jascha", "Ganguli", "Surya"], "venue": "In Advances In Neural Information Processing Systems,", "year": 2016}, {"title": "Provable approximation properties for deep neural networks", "authors": ["Shaham", "Uri", "Cloninger", "Alexander", "Coifman", "Ronald R"], "venue": "Applied and Computational Harmonic Analysis,", "year": 2016}, {"title": "Benefits of depth in neural networks", "authors": ["Telgarsky", "Matus"], "venue": "arXiv preprint arXiv:1602.04485,", "year": 2016}, {"title": "Error bounds for approximations with deep relu networks", "authors": ["Yarotsky", "Dmitry"], "venue": "arXiv preprint arXiv:1610.01145,", "year": 2016}], "id": "SP:69b468459daa2b2e5c096c0cf6da8735dba26a4a", "authors": [{"name": "Itay Safran", "affiliations": []}, {"name": "Ohad Shamir", "affiliations": []}], "abstractText": "We provide several new depth-based separation results for feed-forward neural networks, proving that various types of simple and natural functions can be better approximated using deeper networks than shallower ones, even if the shallower networks are much larger. This includes indicators of balls and ellipses; non-linear functions which are radial with respect to the L1 norm; and smooth non-linear functions. We also show that these gaps can be observed experimentally: Increasing the depth indeed allows better learning than increasing width, when training neural networks to learn an indicator of a unit ball.", "title": "Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks"}