0	22	Generative machine learning models have been used recently to produce extraordinary results, from realistic musical improvisation (Jaques et al., 2016), to changing facial expressions in images (Radford et al., 2015; Upchurch et al., 2016), to creating realistic looking artwork (Gatys et al., 2015).
1	13	In large part, these generative models have been successful at representing data in continuous domains.
3	23	To train generative models for these tasks, these objects are often first represented as strings.
6	174	This model is able to encode and decode molecules to and from a continuous latent space, allowing one to search this space for new molecules with desirable properties (Gómez-Bombarelli et al., 2016b).
7	28	However, one immediate difficulty in using strings to represent discrete objects is that the representation is very brittle: small changes in the string can lead to completely different objects, or often do not correspond to valid objects at all.
8	71	Specifically, Gómez-Bombarelli et al. (2016b) described that while searching for new molecules, the probabilistic decoder — the distribution which maps from the continuous latent space into the space of molecular structures — would sometimes accidentally put high probability on strings which are not valid SMILES strings or do not encode plausible molecules.
11	21	For instance the set of syntactically valid SMILES strings is described using a context free grammar, which can be used for parsing and validation1.
60	11	These 1-hot vectors are concatenated into the rows of a matrix X of dimension T (X)⇥K, where K is the number of production rules in the SMILES grammar, and T (X) is the number of production rules used to generate X.
64	31	Crucially we construct the decoder so that, at any time while we are decoding a sequence, the decoder will only be allowed to select a subset of production rules that are ‘valid’.
77	14	In the previous example, the only production rule in the grammar beginning with smiles is the first so we maskout every dimension except the first, shown in Figure 2, box 4 .
83	16	We then sample a valid rule from this logit vector: chain!
84	12	Just as before we push the non-terminals on the right-hand side of this rule onto the stack, adding the individual non-terminals in from right to left, such that the leftmost non-terminal is on the top of the stack.
85	11	For the Algorithm 1 Sampling from the decoder Input: Deterministic decoder output F 2 RTmax⇥K , masks m ↵ for each production rule ↵ Output: Sampled productions X from p(X|z) 1: Initialize empty stack S , and push the start symbol S onto the top; set t = 0 2: while S is nonempty do 3: Pop the last-pushed non-terminal ↵ from the stack S 4: Use Eq.
86	20	(2) to sample a production rule R 5: Let x t be the 1-hot vector corresponding to R 6: Let RHS(R) denote all non-terminals on the righthand side of rule R, ordered from right to left 7: for non-terminal in RHS(R) do 8: Push on to the stack S 9: end for 10: Set X [X>,x t ] > 11: Set t t+ 1 12: end while next state we again pop the last rule placed on the stack and mask the current logit, etc.
95	23	There is no stack or masking operation.
96	10	The grammar VAE however is constrained to select syntactically-valid sequences.
109	13	We pad any remaining timesteps after T (X) up Algorithm 2 Training the Grammar VAE Input: Dataset {X(i)}N i=1 Output: Trained VAE model p ✓ (X|z), q (z|X) 1: while VAE not converged do 2: Select element: X 2 {X(i)}N i=1 (or minibatch) 3: Encode: z ⇠ q (z|X) 4: Decode: given z, compute logits F 2 RTmax⇥K 5: for t in [1, .
118	45	We show the usefulness of our proposed grammar variational autoencoder (GVAE)2 on two sequence optimization problems: 1) searching for an arithmetic expression that best fits a dataset and 2) finding new drug molecules.
119	47	We begin by showing the latent space of the GVAE and a character variational autoencoder (CVAE), similar to that of Gómez-Bombarelli et al. (2016b)3, on each of the problems.
120	26	We demonstrate that the GVAE learns a smooth, meaningful latent space for arithmetic equations and molecules.
121	15	Given this we perform optimization in this latent space using Bayesian optimization, inspired by the technique of Gómez-Bombarelli et al. (2016b).
130	37	We propose to perform optimization in this latent space of expressions to find an expression that best fits a fixed dataset.
132	10	In the generated expressions, the presence of exponential functions can result in very large MSE values.
133	100	For this reason, we use as target variable log(1 + MSE) instead of MSE.
135	10	Our goal is to maximize the water-octanol partition coefficient (logP), an important metric in drug design that characterizes the drug-likeness of a molecule.
137	49	The training data for the CVAE and GVAE models are 250,000 SMILES strings (Weininger, 1988) extracted at random from the ZINC database by Gómez-Bombarelli et al. (2016b).
138	44	We describe the context-free grammar for SMILES strings that we use to train our GVAE in the supplementary material.
140	144	This is done by encoding two equations and then performing linear interpolation in the latent space.
141	25	Results comparing the character and grammar VAEs are shown in Table 1.
143	10	In contrast, the grammar VAE also provides smooth interpolation and produces valid equations for any location in the latent space.
