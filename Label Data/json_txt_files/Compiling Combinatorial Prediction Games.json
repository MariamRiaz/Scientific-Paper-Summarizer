{"sections": [{"heading": "1. Introduction", "text": "Combinatorial optimization is a important topic of computer science and discrete mathematics, with a wide spectrum of applications ranging from resource allocation and job scheduling, to automated planning and configuration softwares. A common problem is to minimize a modular loss function ` over a discrete space S \u2286 {0, 1}d of feasible solutions represented in a concise manner by a set of combinatorial constraints. In the offline version of this problem, all information necessary to define the optimization task is available beforehand, and the challenge is to develop algorithms which are provably or practically better than enumerating all feasible solutions. Contrastingly, in the online version of this problem (Audibert et al., 2014), the objective function ` is subject to change over time. The challenge here is more acute, since the optimization algorithm is required to perform repeated choices on S so as to minimize their average cost in the long run.\n*Equal contribution 1CRIL, CNRS UMR 8188, Universite\u0301 d\u2019Artois, France. Correspondence to: Frederic Koriche <koriche@cril.fr>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nConceptually, an online combinatorial optimization problem can be cast as a repeated prediction game between a learning algorithm and its environment (Audibert et al., 2011; 2014). During each trial t, the learner chooses a feasible solution st from its decision set S and, simultaneously, the environment selects a loss vector `t \u2208 [0, 1]d. Then, the learner incurs the loss \u3008`t, st\u3009 = \u2211d i=1 `t(i)st(i) and, in light of the feedback provided by its environment, updates its strategy in order to improve the chance of selecting better solutions on subsequent trials.\nSeveral classes of combinatorial prediction games can be distinguished, depending on the type of decision set, and the type of observed feedback. In this paper, we focus on full information games in which it is assumed that the feedback supplied at trial t by the environment is the entire vector `t. On the other hand, we make very few assumptions about the decision set: S may be described by an arbitrary SAT formula, that is, any set of combinatorial constraints representable by Boolean clauses. As SAT encodings of discrete solution spaces are frequently used in academic and industrial applications (Biere et al., 2009), our setting covers an important class of combinatorial prediction games.\nAs usual, the performance of an online learning algorithm is measured according to two metrics. The first, called regret, measures the difference in cumulative loss between the algorithm and the best solution in hindsight. In this study, we make no assumption about the sequence of loss vectors; in particular `t may depend on the previous decisions s1, \u00b7 \u00b7 \u00b7 , st\u22121 made by the learner. In such non-oblivious or adversarial environments, the learner is generally allowed to make decisions in a randomized way, and its predictive performance is measured by the expected regret:\nRT = E [ T\u2211 t=1 \u3008`t, st\u3009 ] \u2212min s\u2208S T\u2211 t=1 \u3008`t, s\u3009\nThe second metric is computational complexity, i.e. the amount of resources required to compute st at each round t, given the sequence of feedbacks observed so far.\nRelated Work. In the literature of combinatorial prediction games, three main strategies have been proposed to attain an expected regret that is sublinear in the game horizon T and polynomial in the input dimension d. The\nfirst, and arguably simplest strategy, is to Follow the Perturbed Leader (FPL): on each trial t, the learner draws at random a perturbation vector zt \u2208 Rd, and then selects in S a minimizer of \u03b7Lt + zt, where \u03b7 \u2208 (0, 1] is a step-size parameter, andLt is the cumulative lossLt = `1 + \u00b7 \u00b7 \u00b7+`t\u22121. Based on the pioneering work of Hannan (1957), refined in (Hutter & Poland, 2005; Kalai & Vempala, 2005), the FPL algorithm achieves an expected regret of O(d 32 \u221a T ).\nThe second strategy is based on the popular exponentially weighted average forecaster in the framework of prediction with expert advice (Cesa-Bianchi & Lugosi, 2006). The overall idea is to maintain a weight for each feasible solution s \u2208 S, which decays exponentially according to the estimated cumulative loss of s. Specifically, on each trial t, the learner draws a solution st \u2208 S at random from the exponential family pt(s) \u223c exp(\u2212\u03b7\u3008Lt, s\u3009). This strategy, referred to as Expanded Hedge (EH) in (Koolen et al., 2010), attains an expected regret of O(d 32 \u221a T ).\nFinally, the third strategy is to Follow the Regularized Leader, a paradigm often advocated in online convex optimization (Hazan, 2016). Here, the learner operates on the convex hull of S, denoted conv(S). On each trial t, the learner starts by choosing a point pt \u2208 conv(S) that minimizes \u03b7\u3008L\u0302t,p\u3009 + F (p), where F is a regularization function. Next, pt is decomposed as a convex composition of feasible solutions in S, and then, a decision st is picked at random according to the resulting distribution. For modular loss functions, this strategy is equivalent to the Online Stochastic Mirror Descent (OSMD) algorithm (Audibert et al., 2014; Rajkumar & Agarwal, 2014), which iteratively performs a gradient descent in the dual space of conv(S) under F , and projects back to the primal space according to the Bregman divergence defined from F . Notably, when F is the Euclidean regularizer, OSMD coincides with the popular stochastic gradient descent (SGD) algorithm (Robbins & Monro, 1951). Alternatively, when F is the entropic regularizer, OSMD corresponds to the Component Hedge (CH) algorithm (Koolen et al., 2010), which achieves an optimal expected regret of O(d \u221a T ).\nFrom the viewpoint of regret, the results outlined above indicate that few improvements remain to be made in full information games. However, we get a different picture if computational considerations are taken into account: all aforementioned algorithms rely on powerful oracles for making decisions in spaces S represented by combinatorial constraints. Namely, the EH algorithm is required, at each iteration, to sample a solution according to an exponential family over S , a problem which is generally #P-hard (Dyer et al., 2009). Similarly, the FPL strategy has to repeatedly solve a linear optimization task over S, which is generally NP-hard (Creignou et al., 2001). For the OSMD algorithm, and its specializations SGD and CH, the computational issue\nis exacerbated by the fact that, even if the learner has access to a linear optimization oracle, it still has to perform, at each trial, a Bregman projection step for which the best known algorithms run in O(d6) time (Suehiro et al., 2012).\nAlthough combinatorial prediction games are generally intractable, efficient implementations of sampling and optimization oracles may be obtained for several decision sets S. For example, when the feasible solutions in S coincide with the bases of a binary matroid, or the perfect matchings of a bipartite graph, linear optimization can be performed in polynomial time, and tractable forms of FPL and OSMD may be derived (Helmbold & Warmuth, 2009; Koolen et al., 2010; Takimoto & Hatano, 2013; Rajkumar & Agarwal, 2014). On the other hand, when the feasible solutions in S correspond to the paths or multi-paths of a rooted Directed Acyclic Graph (DAG), the sampling oracle may be implemented by the weight pushing technique (Mohri, 1998), that recursively evaluates the partition function of an exponential family over the edges of the input DAG. Based on this technique, tractable forms of EH can be derived (Takimoto & Warmuth, 2003; Rahmanian & Warmuth, 2017).\nOur Results. Viewing feasible solutions as paths in a DAG is only one of many abstractions that have been proposed in the literature of circuit complexity for representing combinatorial spaces. In the related field of knowledge compilation (Darwiche & Marquis, 2002), various classes of Boolean circuits have been identified, each associated with a set of inference tasks which can be performed in polynomial time. These theoretical results naturally motivate the following question: can we compile a set of constraints representing a combinatorial space S into a compact and Boolean circuit for which both solution sampling and linear optimization are tractable? By viewing the compilation process as a \u201cpre-processing step\u201d, we may get for free efficient implementations of sampling and optimization oracles, provided that the size of the resulting circuit is not too large.\nThe present study aims at solving combinatorial prediction games, by compiling decision sets into deterministic Decomposable Negation Normal Form (dDNNF) circuits (Darwiche, 2001). This class comes with generic compilers which take as input a SAT formula representing a decision set S, and return a dDNNF circuit C that encodes S (Darwiche, 2002; Lagniez & Marquis, 2017). Although the size of C may grow exponentially in the treewidth of the input formula, it is usually much smaller in practice; existing compilers are able to compress combinatorial spaces defined over thousands of variables and constraints.\nWith these compilation tools in hand, our contributions are threefold: (i) we show that for dDNNF circuits, the sampling oracle in EH and the linear optimization oracle in FPL, run in linear time using a simple variant of the weight-\npushing technique; (ii) for the SGD and CH strategies, we develop a Bregman projection-decomposition method that uses O(d2 ln(dT )) calls to the linear optimization oracle; (iii) we experimentally show on online configuration and planning tasks that EH and FPL are fast, but our variants of SGD and CH are more efficient to minimize the empirical regret.\nBefore proceeding to the core of the paper, we emphasize that the compilation approach to online optimization is not entirely new. Recently, Sakaue et. al. (2018) used the class of Ordered Binary Decision Diagrams (OBDDs) (Bryant, 1986) for implementing the EWA forecaster in combinatorial bandits. Here, S is described by a graph over d edges, together with a constraint specifying the type of objects we desire (e.g. paths or cliques). By contrast, our study assumes that S is described with an arbitrary set of Boolean constraints. So, both studies are targeting different classes of combinatorial prediction games. Moreover, it is known that dDNNF is strictly more succinct than OBDD (Darwiche & Marquis, 2002). Namely, any OBDD can be transformed in linear time and space into an equivalent dDNNF circuit, but the converse is not true: dDNNF includes simple circuits which require an exponential size representation in OBDD. In fact, the key point of compiling combinatorial prediction games is to use both tractable and succinct languages, for allowing prediction strategies to be efficient on a wide variety of combinatorial domains."}, {"heading": "2. Tractable Inference via Compilation", "text": "For the combinatorial prediction games considered in this paper, we assume that the input decision space S is defined from a set of n binary-valued attributes, and we use X = {x1, \u00b7 \u00b7 \u00b7 , xd}, where d = 2n, to denote the set of all \u201cattribute-value\u201d pairs, called literals. A solution is a vector s \u2208 {0, 1}d such that s(i) + s(j) = 1 for every pair of distinct literals xi, xj \u2208 X defined on the same attribute. Thus, \u2016s\u20161 = n for any feasible solution s \u2208 S.\nAn NNF circuit over X is a rooted DAG, whose internal nodes are labeled by \u2228 (or-node) or \u2227 (and-node), and whose leaves are labeled by either a literal in X , or a constant in {0, 1}. The size of C, denoted |C|, is given by the number of its edges. The set of attributes occurring in the subgraph of C rooted at some node c is denoted att(c).\nFor the sake of clarity, we assume that any NNF circuit C satisfies two basic properties, namely (i) any internal node c in C has exactly two children, denoted cl and cr, and (ii) att(cl) = att(cr) 6= \u2205 for any or-node c of C. An NNF circuit satisfying both conditions is called smooth. As shown in (Darwiche, 2001), any Boolean circuitC can be transformed in to an equivalent smooth NNF circuit of size linear in |C|.\nBy viewing literals as \u201cinput gates\u201d, and nodes as \u201coutput gates\u201d, we may specify various inference tasks on Boolean\ncircuits, depending on the type of input values and the semantics of nodes. As suggested by Friesen & Domingos for sum-product functions (2016), inference tasks can be captured through semiring operations. To this point, recall that a commutative semiring is a tuple (R,\u2295,\u2297,\u22a5,>) such that R is a set including the elements \u22a5 and >, \u2295 is a associative and commutative binary operation on R with identity element \u22a5, \u2297 is an associative binary operation on R with identity element > and absorbing element \u22a5, and the operator \u2297 left and right distributes over the operator \u2295.\nInference tasks on an NNF circuit C are defined using a commutative semiring Q = (R,\u2295,\u2297,\u22a5,>) and an input vector w \u2208 Rd. The output of a node c in C for Q given w is denoted Q(c |w), and recursively defined by\nQ(c |w) =  w(i) if c is the literal xi, > if c is the constant 1, \u22a5 if c is the constant 0, Q(cl |w)\u2295Q(cr |w) if c is a node \u2228, and Q(cl |w)\u2297Q(cr |w) if c is a node \u2227\nBy Q(C |w), we denote the output of the root of C for Q givenw. Of particular interest in this study are the semirings described in Table 1; maxmin, minsum, and sumprod, and used to capture the inference tasks of model checking, linear optimization, and model sampling, respectively."}, {"heading": "2.1. Model Checking", "text": "Given an NNF circuit C over X , the task of model checking is to decide whether a Boolean input s \u2208 {0, 1}d is true in C according to the propositional semantics of nodes. Obviously, s is a model of C iff maxmin(C | s) = 1, which can be determined in O(|C|) time. An NNF circuit C is called a representation of a set of feasible solutions S \u2286 {0, 1}d if sol(C) = S, where sol(C) is the set of models of C.\nApart from model checking, virtually all inference tasks in NNF circuits are NP-hard. Indeed, the NNF language covers the class of SAT formulas. So, we need to refine this class in order to get tractable forms of optimization and sampling."}, {"heading": "2.2. Decomposability and Optimization", "text": "A Boolean circuit C is decomposable if for every and-node c of C, we have att(cl) \u2229 att(cr) = \u2205. The class of decomposable NNF circuits is denoted DNNF. For such circuits, which are similar to Boolean sum-product networks (Poon & Domingos, 2011), we can get an efficient implementation of the linear optimization oracle.\nProposition 1. Let S \u2286 {0, 1}d be a (nonempty) decision set represented by a DNNF circuit C, and let w \u2208 Rd be a modular objective. Then, finding a minimizer of w in S can be done in O(|C|) time.\nProof. Based on the minsum semiring, we have\nmin s\u2208S \u3008w, s\u3009 = min s\u2208sol(C) \u3008w, s\u3009 = minsum(C |w)\nThis observation suggests a two-pass weight pushing method for finding a minimizer s of w in S in O(|C|) time. Given a topological ordering of C, the first pass stores the value Q(c |w) of each node c \u2208 C, using Q = minsum. The second pass performs a top-down search over C, by selecting all children of a visited and-node, and by selecting exactly one child c\u2032 \u2208 {cl, cr} of a visited or-node c such that Q(c\u2032 | w) = Q(c | w). Let T be the corresponding search tree, and let s \u2208 {0, 1}d be the indicator vector of the set of literals occurring in T . By construction, we have Q(T |w) = Q(C |w), which implies that s is a minimizer ofw. Since S is not empty, we know that Q(C |w) < +\u221e. This, together with the fact that minmax(T | s) = 1 whenever Q(T |w) < +\u221e, implies that s \u2208 S."}, {"heading": "2.3. Determinism and Sampling", "text": "As the problem of counting the number of models in a DNNF circuit is #P-hard (Darwiche & Marquis, 2002), we need to refine this class in order to get an efficient implementation of the sampling oracle. To this end, an NNF circuit C is called deterministic if minmax(cl | s) + minmax(cr | s) \u2264 1 for every or-node c \u2208 C and every feasible solution s. The class of deterministic DNNF circuits is denoted dDNNF.\nProposition 2. Let S \u2286 {0, 1}d be a decision set represented by a dDNNF circuit C, and for a vector w \u2208 Rd, let Pw be the exponential family on S given by:\nPw(s) = exp\u3008w, s\u3009\u2211\ns\u2032\u2208S exp\u3008w, s\u2032\u3009\nThen, sampling s \u223c Pw can be done in O(|C|) time.\nProof. Based on the sumprod semiring, we have\nPw(s) = exp\u3008w, s\u3009\u2211\ns\u2032\u2208sol(C) exp\u3008w, s\u2032\u3009 =\nexp\u3008w, s\u3009 sumprod(C |w\u2032)\nwherew\u2032 = (ew(1), \u00b7 \u00b7 \u00b7 , ew(d)). Again, such an equivalence suggests a two-pass weight pushing method for sampling a solution s according to Pw in O(|C|) time. Using a topological ordering of C, the first pass stores the values Q(c |w\u2032), where Q = sumprod. The second pass performs a top-down randomized search over C, by selecting all children of a visited and-node, and by drawing at random one of the children of a visited or-node c according to the distribution p(cl) = Q(cl | w\u2032)/Q(c | w\u2032) and p(cr) = 1\u2212 p(cl). Let T be the tree of visited nodes, and s be the indicator vector of the literals in T . Since S 6= \u2205, we must have Q(C |w) > 0. Thus, each Bernoulli test performed in T is valid, and hence, s \u2208 S . For any literal xi occurring in T , let p(xi) denote the probability of the (unique) path connecting the root to xi. By a telescoping product of Bernoulli distributions, we get that p(xi) = ew(i)/Q(C |w\u2032). Therefore, p(s) = \u220f i:s(i)=1 p(xi) = Pw(s), as desired.\nWe close this section by highlighting some interesting subclasses of dDNNF. A decision node is an or-node of the form (xi \u2227 c\u2032l) \u2228 (xi \u2227 c\u2032r), where xi and xi are opposite literals, and c\u2032l and c \u2032 r are arbitrary nodes. The class of Free Binary Decision Diagrams (FBDD) is the subset of dDNNF in which every or-node is a decision node, and at least one child of any and-node is a literal (Wegener, 2000). For example, if in the dDNNF circuit of Figure 1, we replace the or-node (in blue) by a simple literal, say x3, then we get an FBDD circuit. The family of Ordered Binary Decision Diagrams (OBDD) is the subclass of FBDD obtained by imposing a fixed ordering on the decision variables. Alternatively, the well-known family of (Binary) Decision Trees (DT) is the subclass of FBDD circuits for which the primal graph is cycle-free. Since all these classes are (strict) subsets of dDNNF, they admit linear-time algorithms for linear optimization and model sampling."}, {"heading": "3. Tractable Prediction via Compilation", "text": "After an excursion into compilation languages, we are now ready to provide efficient characterizations of combinatorial prediction strategies. Our results are summarized in Table 2.\nNotably, using the fact that \u2016s\u20161 = d/2, the regret bounds for EH and FPL can easily be derived from (Audibert et al., 2011) and (Hutter & Poland, 2005), respectively. Both strategies are straightforward to implement on dDNNF circuits. Indeed, recall that EH draws, at each trial t, a feasible solution st \u2208 S at random according to the distribution P\u2212\u03b7Lt , where Lt = `1 + \u00b7 \u00b7 \u00b7+ `t\u22121. So, by direct application of Proposition 2, this strategy runs in O(|C|) time per round, using a dDNNF representation C of the decision set S. For the FPL strategy, each round t is performed by choosing a minimizer st \u2208 S of the objective function \u03b7Lt \u2212 zt, where zt \u2208 Rd is a perturbation vector whose components are independent exponentially distributed random variables. By Proposition 1, the FPL strategy also runs inO(|C|) time per round, using a dDNNF encoding C of S, and the fact that |C| is in \u2126(d).\nHowever, the OSMD strategy and its specializations, SGD and CH, require more attention, due to the projectiondecomposition step involved at each iteration."}, {"heading": "3.1. Online Stochastic Mirror Descent", "text": "The overall idea of Online Mirror Descent (OMD) is to \u201cfollow the regularized leader\u201d through a primal-dual approach (Nemirovski & Yudin, 1983; Beck & Teboulle, 2003). Let K be a convex set, and let int(K) denotes its interior. Given a regularization function F defined on K, OMD iteratively performs a gradient descent in the interior of the dual space K\u2217, and projects back the dual point into the primal space K. The connection between K and K\u2217 is ensured using the gradients \u2207F and \u2207F \u2217, where F \u2217 is the convex conjugate of F , defined on K\u2217. The projection step is captured by the Bregman divergence of F , which is a function BF : K \u00d7 int(K)\u2192 R given by:\nBF (p, q) = F (p)\u2212 F (q)\u2212 \u3008\u2207F (q),p\u2212 q\u3009\nIn the stochastic variant of OMD, introduced by Audibert et. al. (2011; 2014), and specified in Algorithm 1, each projection is performed onto the subset conv(S) of K, and the resulting point pt is decomposed into a convex combination of feasible solutions in S, from which one is picked at random for the prediction task.\nAlgorithm 1 OSMD\nInput: decision set S \u2286 {0, 1}d, horizon T \u2208 Z+ Parameters: regularizer F on K \u2287 conv(S), step-size \u03b7 \u2208 (0, 1]\nset u1 = 0 for t = 1 to T do\nset pt \u2208 Argminp\u2208conv(S)BF (p,\u2207F \u2217(ut)) play st \u223c pt and observe `t set ut+1 = \u2207F (pt)\u2212 \u03b7`t\nend for\nFor common regularizers, the gradient\u2207F (pt) and its dual \u2207F \u2217(ut) are easily calculable, and we shall assume that the time spent for their construction is negligible compared with the running time of the linear optimization oracle. In fact, the computational bottleneck of OSMD is to find a minimizer pt of BF (p,\u2207F \u2217(ut)) in the convex hull of S, and to decompose pt into a convex combination of solutions in S. Fortunately, under reasonable assumptions about the curvature of BF , this projection-decomposition step can be efficiently computed, using recent results in projection-free convex optimization algorithms.\nTo this end, we need additional definitions. For a convex set K, a differentiable function f : K \u2192 R is called \u03b1-strongly convex with respect to a norm \u2016 \u00b7 \u2016 if\nf(p\u2032)\u2212 f(p) \u2265 \u3008\u2207f(p),p\u2032 \u2212 p\u3009+ \u03b1 2 \u2016p\u2032 \u2212 p\u20162\nFurthermore, f is called \u03b2-smooth1 with respect to \u2016 \u00b7 \u2016 if\nf(p\u2032)\u2212 f(p) \u2264 \u3008\u2207f(p),p\u2032 \u2212 p\u3009+ \u03b2 2 \u2016p\u2032 \u2212 p\u20162\nBased on these notions, we say that a Bregman divergence BF has the condition number \u03b2/\u03b1 if BF is both \u03b1-strongly convex and \u03b2-smooth with respect to the Euclidean norm \u2016 \u00b7 \u20162 in its first argument. For such regularizers, the next result states that the projection-decomposition step can be approximated in low polynomial time, by exploiting the Pairwise Conditional Gradient (PCG) method, a variant of the Frank-Wolfe convex optimization algorithm, whose convergence rate has been analyzed in (Lacoste-Julien & Jaggi, 2015; Garber & Meshi, 2016; Bashiri & Zhang, 2017).\nLemma 1. Let S \u2286 {0, 1}d be a decision set represented by a dDNNF circuit C, and F be a regularizer onK \u2287 conv(S) such that BF has condition number \u03b2/\u03b1. Then, for any q \u2208 int(K) and \u2208 (0, 1), one can find inO(\u03b2\u03b1d\n2|C|ln \u03b2d ) time a convex decomposition of p \u2208 conv(S) such that\nBF (p, q)\u2212 min p\u2032\u2208conv(C) BF (p \u2032, q) \u2264\n1This notion of geometric smoothness should not be confused with the structural smoothness of NNF circuits in Section 2.\nAlgorithm 2 PCG\nInput: S \u2286 {0, 1}d, f : K \u2192 R, m \u2208 Z+ Parameters: step-sizes {\u03b7j}mj=1\nlet p1 be some point in S for j = 1 to m do\nlet \u2211j i=1 \u03b1isi be the convex decomposition of pj set s+j \u2208 Argminp\u2208conv(S)\u3008\u2207f(pj),p\u3009 set s\u2212j \u2208 Argmins\u2208{s1,\u00b7\u00b7\u00b7,sj}\u3008\u2212\u2207f(pj), s\u3009 set pj+1 = pj + \u03b7j(s+j \u2212 s \u2212 j )\nend for\nProof. Observe that conv(S) is a simplex-like polytope (Bashiri & Zhang, 2017), defined by the linear constraints p \u2265 0, \u2211N i=1 \u03b1isi = p, \u03b1 \u2265 0, and \u2211N i=1 \u03b1i = 1, where N = |S|. So, conv(S) and BF satisfy the conditions of Theorem 1 in (Garber & Meshi, 2016), and using the step-sizes advocated by the authors, we get that\nBF (pm, q)\u2212BF (p\u2217, q) \u2264 \u03b2d\n2 exp\n( \u2212 \u03b1\n8\u03b2d2 m ) where pm is the point obtained at the last iteration of PCG, and p\u2217 is the (unique) minimizer of BF (p, q) on conv(S). Therefore, after m \u2265 (8d2\u03b2/\u03b1) ln(\u03b2d/(2 )) iterations, we haveBF (pm, q)\u2212BF (p\u2217, q) \u2264 . Finally, since each iteration of PCG makes one call to the linear optimization oracle, the runtime complexity follows from Proposition 1.\nBy OSMD+PCG, we denote the refined version of the OSMD algorithm that uses the PCG method at each trial t in order to approximate the Bregman projection-decomposition step. In addition to a regularizer F and a step-size \u03b7, OSMD+PCG takes as parameters a sequence { t}Tt=1 such that\nBF (pt, qt)\u2212BF (p\u2217t , qt) \u2264 t\nwhere pt is the point returned by PCG, qt = \u2207F \u2217(ut), and p\u2217t is the minimizer of BF (p, qt) over conv(S).\nTheorem 1. Suppose that OSMD+PCG takes as input a dDNNF representation C of a decision set S \u2286 {0, 1}d, and a horizon T , and uses a regularizer F on K \u2287 conv(S) such thatBF has condition number \u03b2/\u03b1, together with a stepsize \u03b7 \u2208 (0, 1] and a sequence of { t}Tt=1 such that t = \u03b3/t2 for \u03b3 > 0. Then, OSMD+PCG attains the expected regret\n(1) RT \u2264\n\u221a 2\u03b3d\n\u03b1 (lnT + 1) +\n1 \u03b7 max s\u2208S BF (s,p \u2217 1)\n+ 1\n\u03b7 T\u2211 t=1 BF\u2217(\u2207F (p\u2217t )\u2212 \u03b7`t,\u2207F (pt))\nwith a per-round running time in O ( \u03b2\n\u03b1 d2|C|ln \u03b2dT \u03b3\n) .\nProof. Let s\u2217 \u2208 S be the optimal solution chosen with the benefit of hindsight. By decomposing the regret, we have\nRT = T\u2211 t=1 \u3008`t,p\u2217t \u2212 s\u2217\u3009+ T\u2211 t=1 E\u3008`t, st \u2212 p\u2217t \u3009 (2)\nBy Theorem 2 in (Audibert et al., 2014), the first term in (2) is bounded by the last two terms in (1). For the second term in (2), we get from the Cauchy-Schwarz inequality that\nE\u3008`t, st \u2212 p\u2217t \u3009 \u2264 \u2016`t\u20162\u2016pt \u2212 p\u2217t \u20162\u2264 \u221a d\u2016pt \u2212 p\u2217t \u20162\nMoreover, by applying the Generalized Pythagorean Theorem (Cesa-Bianchi & Lugosi, 2006), we know that BF (p, qt) \u2265 BF (p,p\u2217t ) + BF (p\u2217t , qt), for any p \u2208 conv(S). Using p = pt and rearranging,\nBF (pt,p \u2217 t ) \u2264 BF (pt, qt)\u2212BF (p\u2217t , qt) \u2264 t (3)\nSince BF is \u03b1-strongly convex with respect to \u2016 \u00b7 \u20162 in its first argument, we also have \u03b12 \u2016pt \u2212 p \u2217 t \u201622\u2264 BF (pt,p\u2217t ). Thus by plugging this inequality into (3), we get that E\u3008`t, st \u2212 p\u2217t \u3009 \u2264 \u221a 2d t/\u03b1. Finally, by substituting t with \u03c1/t2, summing other T , and applying the logarithmic bound on harmonic series, we obtain the desired result."}, {"heading": "3.2. Stochastic Gradient Descent", "text": "The (online) SGD algorithm is derived from OSMD using the Euclidean regularizer F (p) = 12 \u2016p\u2016 2 2. In this simple framework, the primal and dual spaces coincide with Rd, and hence, F \u2217(u) = u, \u2207F (p) = p, and \u2207F \u2217(u) = u. Furthermore, BF has the condition number 1/1, since BF (p, q) = 1 2 \u2016p\u2212 q\u2016 2 2. We denote by SGD+PCG the instance of OSMD+PCG defined on the Euclidean regularizer.\nProposition 3. The SGD+PCG algorithm achieves an expected regret bounded by d( \u221a T + lnT + 1) with a per-round runtime complexity in O(d2|C|ln(dT )) using \u03b7 = 1/ \u221a T and \u03b3 = d/2.\nProof. This simply follows from Theorem 1, together with the fact that maxs\u2208S BF (s,p\u22171) \u2264 d and \u2016`t\u201622\u2264 d."}, {"heading": "3.3. Component Hedge", "text": "The CH algorithm is derived from OSMD using the entropic regularizer F (p) = \u2211d i=1 p(i)(ln p(i)\u2212 1), for which the\nconjugate is F \u2217(u) = \u2211d i=1 expu(i). Here, we cannot find a finite condition number for the associated divergence BF (p, q) = \u2211d i=1 p(i) ln p(i) q(i) \u2212 (p(i) \u2212 q(i)), since its gradient is unbounded. This issue may, however, be circumvented using a simple trick advocated in (Krichene et al., 2015), which consists in replacing the entropic regularizer with the function F\u03b4(p) = F (p + \u03b4), where\n\u03b4 \u2208 (0, 1) and \u03b4 = (\u03b4, \u00b7 \u00b7 \u00b7 , \u03b4). For this function, the primal space is (\u2212\u03b4,+\u221e), and since F \u2217\u03b4 (u) = F \u2217(u) \u2212 \u3008u, \u03b4\u3009, the dual space is Rd. It is easy to show that\n\u2202F\u03b4(p)\n\u2202p(i) = ln(p(i) + \u03b4)\n\u2202F \u2217\u03b4 (u)\n\u2202u(i) = eu(i) \u2212 \u03b4\nBF\u03b4(p, q) = BF (p+\u03b4, q+\u03b4) B \u2217 F\u03b4 (u,v) = B\u2217F (u,v)\nwhere B\u2217F (u,v) = \u2211d i=1 e\nv(i)(ev(i)\u2212u(i) +v(i)\u2212u(i)\u22121). Furthermore, since the first and second order partial derivatives of B\u2217F\u03b4(p, q) at the coordinate p(i) are\n\u2202BF\u03b4(p, q)\n\u2202p(i) = ln\np(i) + \u03b4\nq(i) + \u03b4\n\u22022BF\u03b4(p, q)\n\u22022p(i) = ln\n1\np(i) + \u03b4\nit follows that BF\u03b4 has the condition number 1+\u03b4/\u03b4. Indeed, given an arbitrary point q \u2208 int(\u2212\u03b4,+\u221e), let Hq(p) denote the the Hessian matrix of BF\u03b4(p, q) at p \u2208 conv(S). Then, for any z \u2208 Rd, the diagonal entries of Hq(p) satisfy\n1 1 + \u03b4 \u2264 \u2202\n2BF (p, q)\n\u22022p(i) z(i)2 \u2264 1 \u03b4\nusing the fact that p(i) \u2208 [0, 1]. Thus, \u03b1I 4 Hq(p) 4 \u03b2I for \u03b1 = 1/1+\u03b4 and \u03b2 = 1/\u03b4. In what follows, the instance of OSMD+PCG that uses F\u03b4 as regularizer is called \u03b4-CH+PCG.\nProposition 4. The \u03b4-CH+PCG algorithm achieves an expected regret bounded by d(1 + 2\u03b4)( \u221a T + lnT + 1)\nwith a per-round runtime complexity in O ( d2|C|/\u03b4 ln dT/\u03b4 ) using \u03b7 = 1/\u221aT and \u03b3 = 2d(1/2 + \u03b4)/(1 + \u03b4).\nProof. The runtime complexity simply follows from Theorem 1. The regret bound is obtained by bounding the second and third terms of (1), and using the above values for \u03b7 and \u03b3. Using s\u22171 as a maximizer of the second term of (1), we have BF\u03b4(s \u2217 1,p \u2217 1) = F\u03b4(s \u2217 1) \u2212 F\u03b4(p\u22171). Using the notation p\u03031 = p\u22171 + \u03b4 and r = d(1/2 + \u03b4), we get that\nF\u03b4(s \u2217 1)\u2212 F\u03b4(p\u22171) \u2264 d\u2211 i=1 p\u03031(i) ln 1 p\u03031(i) \u2264 r ln d r\nwhich is bounded by r. For the third term of (1), observe that F\u03b4 is 1(1+\u03b4)d -strongly convex with respect to the norm \u2016 \u00b7 \u20161, since \u2016p \u2212 p\u2032\u201621\u2264 d\u2016p \u2212 p\u2032\u201622. By Theorem 3 in (Kakade et al., 2012), it follows that F \u2217\u03b4 is (1 + \u03b4)d-smooth with respect to the norm \u2016 \u00b7 \u2016\u221e. Therefore,\n1 \u03b7 BF\u2217(\u2207F (p\u2217t )\u2212 \u03b7`t,\u2207F (pt)) \u2264 \u03b7 2 d(1 + \u03b4)\u2016`t\u20162\u221e\nwhich is bounded by \u03b7r."}, {"heading": "4. Experiments", "text": "In order to evaluate the performance of the different online combinatorial optimization strategies examined in Section 3, we have considered 16 instances of the SAT Library,2 described in Table 3. Namely, the first six rows of the table are (car) configuration tasks, while the remaining rows are planning problems. In the first four columns of the table are reported the name of the SAT instance, the number of attributes (d/2), the number of constraints (|SAT|), and the number |S| of feasible solutions. We have used the recent D4 compiler 3 (Lagniez & Marquis, 2017) for transforming SAT instances into dDNNF circuits. The size |C| of the compiled circuit is reported in the fifth column.\nIn order to simulate combinatorial prediction games, we have used the following protocol. Suppose that the set X = {x1, \u00b7 \u00b7 \u00b7 , xd} of literals is sorted in a lexicographic way, so that for each odd integer i, the pair (xi, xi+1) encodes both configurations of the same binary attribute. First, we construct a vector \u00b50 of d/2 independent Bernoulli variables. At each round t \u2208 {1, \u00b7 \u00b7 \u00b7 , T}, \u00b5t is set to \u00b5t\u22121 with probability 0.9, or picked uniformly at random from [0, 1]d/2 with probability 0.1. Then, the feedback supplied to the learner is a vector `t \u2208 {0, 1}d such that `t(i) + `t(i + 1) = 1, and `t(i) = 1 with probability \u00b5t(i+1/2) for each odd integer i. So, `t(i + 1) = 1 with probability 1 \u2212 \u00b5t(i+1/2). Although this protocol is essentially stochastic, the environment secretly resets \u00b5t with probability 0.1 at each round to foil the learner.\nThe combinatorial prediction strategies were implemented in C++ and tested on a six-core Intel i7-5930K with 32 GiB RAM.4 For the FPL and EH algorithms, we used the step-size \u03b7 reported in (Audibert et al., 2011) and (Hutter & Poland, 2005), respectively. Concerning the SGD+PCG and \u03b4-CH+PCG algorithms, we used for \u03b7 and \u03b3 the values determined by our theoretical analysis; the step-sizes {\u03b7t} of PCG were computed from binary search as advocated by Garber & Meshi (2016) in their experiments, and the value of \u03b4 was fixed to 1/ln d in order to keep a quadratic runtime complexity for \u03b4-CH+PCG. Finally, the horizon T was set to 103, and a timeout of one day was fixed for learning.\nIn our experiments, the regret is measured by the difference in cumulative loss between the algorithm and the best feasible solution in hindsight, which is obtained using the linear optimization oracle at horizon T . This measure is averaged on 10 simulations, and divided by T to yield an average empirical regret. Similarly, the per-round runtimes (in seconds) are averaged on 10 simulations. The corresponding results are reported in the last four columns of Table 3.\n2www.cs.ubc.ca/\u02dchoos/SATLIB/ 3www.cril.univ-artois.fr/KC/d4.html 4www.github.com/frederic-koriche/ccpg.git\nHere, the symbol \u201c\u2212\u201d indicates that the learner was not able to perform the T rounds in one day. From the viewpoint of regret, SGD+PCG and \u03b4-CH+PCG outperform EH and FPL, which confirms our theoretical results. We mention in passing that SGD+PCG and \u03b4-CH+PCG are remarkably stable. Contrastingly, FPL exhibits a larger variance.\nConcerning runtimes, EH and FPL are unsurprisingly faster than SGD+PCG and \u03b4-CH+PCG. Notably, for the hard-to-compile instances c140-fc and c163-fw, both EH and FPL were able to perform each trial in few tens of seconds, while OSMD+PCG algorithms took several minutes per-round (and hence, they were unable to process 103 rounds in one day), due to the time spent in approximating the Bregman projection step. Yet, it is important to emphasize that the convergence rate of PCG is, in practice, much faster than the theoretical bound of O\u0303(d2|C|). Both SGD+PCG and \u03b4-CH+PCG were able to process nearly all instances in few seconds per round. For circuits of moderate size, all algorithms run in less than one second per trial. We also observed that SGD+PCG is slightly faster than \u03b4-CH+PCG, especially for large domains where small values of \u03b4 have a significant impact on the the runtime complexity. In essence, SGD+PCG offers the best compromise between predictive performance and running time; since all feasible solutions are dense (\u2016s\u20161= d/2), there is no significant difference in accuracy between SGD+PCG and \u03b4-CH+PCG."}, {"heading": "5. Conclusions", "text": "We have proposed a general framework for compiling online combinatorial optimization problems, whose space of feasible solutions is described using a set of Boolean constraints. Namely, we have focused on the class of dDNNF circuits which is endowed with fast inference algorithms for the linear optimization oracle and the sampling oracle. Based on\nthis framework, we have shown than both EH and FPL admit fast implementation for tackling large scale online combinatorial problems. A particular attention was devoted to the generic OSMD strategy, which involves a computationally expensive projection-decomposition step at each iteration. To this point, we made use of projection-free algorithms, and in particular the PCG method, for approximating this operation. The resulting algorithms, SGD+PCG and \u03b4-CH-PCG, are inevitably slower than EH and FPL, but achieve a better regret performance, as corroborated by our experiments.\nWe conclude with a few remarks. In light of the current results, a natural perspective of research is to extend our framework to other classes of combinatorial prediction games. Notably, the semi-bandit setting seems within reach. Indeed, the semi-bandit variant of EH, often referred to as EXP2 (Audibert et al., 2014), uses importance weights for estimating the loss at each iteration. By simple adaptation of Proposition 2, such weights can be computed in linear time. Similarly, the semi-bandit extension of FPL exploits the geometric sampling method for estimating loss vectors (Neu & Barto\u0301k, 2016). Again, this iterative method can be implemented in linear time (per iteration) using Proposition 1. Less obvious, however, is the extension of OSMD to semi-bandits: although the extension of CH achieves an optimal expected regret in this setting, its practical use remain limited due to projection-decomposition step. An interesting open question is to determine whether a combination of CH with PCG is able, in the semi-bandit case, to achieve a quasi-optimal regret in low-polynomial time. Of course, the bandit setting is even more challenging. To this point, Sakaue et. al. (2018) have paved the way using OBDDs for an efficient implementation of the COMBBAND algorithm (Cesa-Bianchi & Lugosi, 2012), Extending their approach to dDNNF, which is more succinct than OBDD, is a promising direction of future research."}], "year": 2018, "references": [{"title": "Minimax policies for combinatorial prediction games", "authors": ["Audibert", "J-Y", "S. Bubeck", "G. Lugosi"], "venue": "In Proceedings of the 24th Annual Conference on Learning Theory (COLT", "year": 2011}, {"title": "Regret in online combinatorial optimization", "authors": ["Audibert", "J-Y", "S. Bubeck", "G. Lugosi"], "venue": "Mathematics of Operations Research,", "year": 2014}, {"title": "Decomposition-invariant conditional gradient for general polytopes with line search", "authors": ["M.A. Bashiri", "X. Zhang"], "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "authors": ["A. Beck", "M. Teboulle"], "venue": "Operational Research Letters,", "year": 2003}, {"title": "Handbook of Satisfiability", "authors": ["A. Biere", "M. Heule", "H. van Maaren", "T. Walsh"], "year": 2009}, {"title": "Graph-based algorithms for Boolean function manipulation", "authors": ["R. Bryant"], "venue": "IEEE Transactions on Computers,", "year": 1986}, {"title": "Prediction, Learning, and Games", "authors": ["N. Cesa-Bianchi", "G. Lugosi"], "year": 2006}, {"title": "Combinatorial bandits", "authors": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Journal of Computer and System Sciences,", "year": 2012}, {"title": "Complexity Classification of Boolean Constraint Satisfaction Problems", "authors": ["N. Creignou", "S. Khanna", "M. Sudan"], "venue": "SIAM Monographs on Discrete Mathematics and Applications,", "year": 2001}, {"title": "Decomposable negation normal form", "authors": ["A. Darwiche"], "venue": "Journal of the ACM,", "year": 2001}, {"title": "A compiler for deterministic, decomposable negation normal form", "authors": ["A. Darwiche"], "venue": "In Proceedings of the 18th National Conference on Artificial Intelligence,", "year": 2002}, {"title": "A knowledge compilation map", "authors": ["A. Darwiche", "P. Marquis"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "year": 2002}, {"title": "The complexity of weighted Boolean #CSP", "authors": ["M.E. Dyer", "L.A. Goldberg", "M. Jerrum"], "venue": "SIAM Journal of Computing,", "year": 1970}, {"title": "The sum-product theorem: A foundation for learning tractable models", "authors": ["A.L. Friesen", "P.M. Domingos"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "year": 2016}, {"title": "Linear-memory and decomposition-invariant linearly convergent conditional gradient algorithm for structured polytopes", "authors": ["D. Garber", "O. Meshi"], "venue": "In Advances in Neural Information Processing Systems", "year": 2016}, {"title": "Approximation to Bayes risk in repeated play", "authors": ["J. Hannan"], "venue": "Contributions to the Theory of Games,", "year": 1957}, {"title": "Introduction to online convex optimization", "authors": ["E. Hazan"], "venue": "Foundations and Trends in Optimization,", "year": 2016}, {"title": "Learning permutations with exponential weights", "authors": ["D.P. Helmbold", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research,", "year": 2009}, {"title": "Adaptive online prediction by following the perturbed leader", "authors": ["M. Hutter", "J. Poland"], "venue": "Journal of Machine Learning Research,", "year": 2005}, {"title": "Regularization techniques for learning with matrices", "authors": ["S.M. Kakade", "S. Shalev-Shwartz", "A. Tewari"], "venue": "Journal of Machine Learning Research,", "year": 2012}, {"title": "Efficient algorithms for online decision problems", "authors": ["A.T. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "year": 2005}, {"title": "Hedging structured concepts", "authors": ["W.M. Koolen", "M.K. Warmuth", "J. Kivinen"], "venue": "In Proceedings of the 23rd Conference on Learning Theory (COLT", "year": 2010}, {"title": "Efficient bregman projections onto the simplex", "authors": ["W. Krichene", "S. Krichene", "A.M. Bayen"], "venue": "In Proceedings of the 54th IEEE Conference on Decision and Control,", "year": 2015}, {"title": "On the global linear convergence of frank-wolfe optimization variants", "authors": ["S. Lacoste-Julien", "M. Jaggi"], "venue": "In Advances in Neural Information Processing Systems", "year": 2015}, {"title": "An improved decision-dnnf compiler", "authors": ["Lagniez", "J-M", "P. Marquis"], "venue": "In Proceedings of the 26th International Joint Conference on Artificial Intelligence,", "year": 2017}, {"title": "General algebraic frameworks and algorithms for shortest-distance problems", "authors": ["M. Mohri"], "venue": "Technical Report 981210-10TM, AT & T Labs-Research,", "year": 1998}, {"title": "Problem Complexity and Method Efficiency in Optimization", "authors": ["Nemirovski", "A. S", "D.B. Yudin"], "venue": "J. Wiley and Sons,", "year": 1983}, {"title": "Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits", "authors": ["G. Neu", "G. Bart\u00f3k"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "Sum-product networks: A new deep architecture", "authors": ["H. Poon", "P.M. Domingos"], "venue": "In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence (UAI", "year": 2011}, {"title": "Online dynamic programming", "authors": ["H. Rahmanian", "M.K. Warmuth"], "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"title": "Online decision-making in general combinatorial spaces", "authors": ["A. Rajkumar", "S. Agarwal"], "venue": "In Advances in Neural Information Processing Systems 27,", "year": 2014}, {"title": "A stochastic approximation method", "authors": ["H. Robbins", "S. Monro"], "venue": "The Annals of Mathematical Statistics,", "year": 1951}, {"title": "Efficient bandit combinatorial optimization algorithm with zero-suppressed binary decision diagrams", "authors": ["S. Sakaue", "M. Ishihata", "Minato", "S-I"], "venue": "In Proceedings of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS", "year": 2018}, {"title": "Online prediction under submodular constraints", "authors": ["D. Suehiro", "K. Hatano", "S. Kijima", "E. Takimoto", "K. Nagano"], "venue": "In Proceedings of the 23rd International Conference on Algorithmic Learning Theory (ALT", "year": 2012}, {"title": "Efficient algorithms for combinatorial online prediction", "authors": ["E. Takimoto", "K. Hatano"], "venue": "In Proceedings of the24th International Conference on Algorithmic Learning Theory (ALT", "year": 2013}, {"title": "Path kernels and multiplicative updates", "authors": ["E. Takimoto", "M.K. Warmuth"], "venue": "Journal of Machine Learning Research,", "year": 2003}, {"title": "Branching Programs and Binary Decision Diagrams: Theory and Applications. Discrete mathematics and applications", "authors": ["I. Wegener"], "venue": "SIAM Monographs on Discrete Mathematics and Applications,", "year": 2000}], "id": "SP:f2b9c2099b98f12fb2b8ca486ff3af453578e09f", "authors": [{"name": "Frederic Koriche", "affiliations": []}], "abstractText": "In online optimization, the goal is to iteratively choose solutions from a decision space, so as to minimize the average cost over time. As long as this decision space is described by combinatorial constraints, the problem is generally intractable. In this paper, we consider the paradigm of compiling the set of combinatorial constraints into a deterministic and Decomposable Negation Normal Form (dDNNF) circuit, for which the tasks of linear optimization and solution sampling take linear time. Based on this framework, we provide efficient characterizations of existing combinatorial prediction strategies, with a particular attention to mirror descent techniques. These strategies are compared on several real-world benchmarks for which the set of Boolean constraints is preliminarily compiled into a dDNNF circuit.", "title": "Compiling Combinatorial Prediction Games"}